[{"type": "text", "text": "Jiaqi Tang1,2,3\u2217 Hao $\\mathbf{L}\\mathbf{u}^{1,2*}$ Ruizheng $\\mathbf{W}\\mathbf{u}^{4}$ Xiaogang $\\mathbf{X}\\mathbf{u}^{5,6}$ Ke Ma7 Cheng Fang7 Bin Guo7 Jiangbo $\\mathbf{L}\\mathbf{u}^{3,4}$ Qifeng Chen 2 Ying-Cong Chen1,2,3\u2020 ", "page_idx": 0}, {"type": "text", "text": "1The Hong Kong University of Science and Technology (Guangzhou) 2The Hong Kong University of Science and Technology 3HKUST(GZ) \u2013 SmartMore Joint Lab 4SmartMore Corporation 5The Chinese University of Hong Kong 6Zhejiang University 7Northwestern Polytechnical University {jtang092, hlu585}@connect.hkust-gz.edu.cn {ruizheng.wu, jiangbo}@smartmore.com xiaogangxu00@gmail.com {2544552413, sura}@mail.nwpu.edu.cn guob@nwpu.edu.cn cqf@ust.hk yingcongchen@hkust-gz.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Video Anomaly Detection (VAD) systems can autonomously monitor and identify disturbances, reducing the need for manual labor and associated costs. However, current VAD systems are often limited by their superficial semantic understanding of scenes and minimal user interaction. Additionally, the prevalent data scarcity in existing datasets restricts their applicability in open-world scenarios. In this paper, we introduce HAWK, a novel framework that leverages interactive large Visual Language Models (VLM) to interpret video anomalies precisely. Recognizing the difference in motion information between abnormal and normal videos, HAWK explicitly integrates motion modality to enhance anomaly identification. To reinforce motion attention, we construct an auxiliary consistency loss within the motion and video space, guiding the video branch to focus on the motion modality. Moreover, to improve the interpretation of motion-to-language, we establish a clear supervisory relationship between motion and its linguistic representation. Furthermore, we have annotated over 8,000 anomaly videos with language descriptions, enabling effective training across diverse open-world scenarios, and also created 8,000 question-answering pairs for users\u2019 open-world questions. The final results demonstrate that HAWK achieves SOTA performance, surpassing existing baselines in both video description generation and question-answering. Our codes/dataset/demo will be released at https://github.com/jqtangust/hawk. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "\"Have eyes like a HAWK!\" \u2013 Longman Dictionary ", "page_idx": 0}, {"type": "text", "text": "In recent years, the deployment of Video Anomaly Detection (VAD) systems has seen a significant uptick across a diverse array of domains, including but not limited to, autonomous driving [45, 25], surveillance [6, 23], and crime scene analysis [33]. The inherent capability of these systems to autonomously monitor and identify disturbances within a scene has markedly diminished the reliance on manual labor, thereby streamlining operational efficiency and reducing associated costs. ", "page_idx": 0}, {"type": "image", "img_path": "vBKoEZ1PG3/tmp/632aac6c1feb2cd56d33abcd3fdd54300f785321bfb373fd9c3d5e41d2dfc438.jpg", "img_caption": ["(B) Video Anomaly Detection & Classification ", "(D) Video Understanding for Anomaly (Ours) "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Different framework in video anomaly detection. (A) shows traditional video anomaly detection methods, which use binary classifiers to detect anomalies. (B), following (A), introduces a multi-class classifier for integrating semantic information, allowing users to obtain different types of anomaly information. Neither (A) nor (B) can interact with users. (C) is a previous video understanding framework that can interactively provide richer semantic information for users, but cannot specifically locate video anomalies. Our framework (D) enhances the anomaly understanding capability and provides annotated labels with rich semantic information. ", "page_idx": 1}, {"type": "text", "text": "Despite the extensive focus on anomaly detection in most existing VAD systems [23, 44, 33, 31, 9, 13, 19, 34, 40, 48, 52] (as shown in Fig. 1 (A)), there is often a lack of deeper semantic understanding of the scenes and insufficient interaction with users. While Pu et al. [31] and Wu et al. [42] incorporated semantic information for video anomaly detection, their frameworks are limited as multiple-class classifiers (as displayed in Fig. 1 (B)). Consequently, the functionality of these systems is confined to the detection of anomalous frames, necessitating further manual analysis by users to analyze the detected anomalies comprehensively. Although Lv et al. [27] has pioneered the development of a large language model for the video anomaly explanation, their approach primarily relies on pseudo labels for training. The lack of robust training data severely constrains its practical applicability. Besides, such a method focuses more on acquiring long-range context information rather than anomaly-related features on anomaly understanding (as exhibited in Fig. 1 (C)). ", "page_idx": 1}, {"type": "text", "text": "To solve the above challenges, we propose an interactive large visual-language model [21, 18, 29], HAWK, for precisely understanding video anomalies (as illustrated in Fig. 1 (D)). Considering that the motion in normal and abnormal videos is significantly different [44, 52], we explicitly integrate motion modality by a dual-branch framework in HAWK to enhance the understanding of anomalies (Section 4.1). Besides, to reinforce motion attention, we construct an auxiliary consistency loss based on the mutual information between the original video (appearance feature) and its motion in tight space (Section 4.2), to implicitly guide the video branch to focus on motion-related features. However, the interpretation of motion to the corresponding language remains unclear. Therefore, we extract the motion-related language (verbs and their entities) from the original description to directly supervise the visual and linguistic representations of motion, for accurately enhancing the interpretation of video anomaly in HAWK (Section 4.3). ", "page_idx": 1}, {"type": "text", "text": "Furthermore, we also collect seven video anomaly datasets from various scenarios and generate language descriptions for each video. Besides, to address the open-ended questions raised by users, we utilize language descriptions of the videos to generate potential question-answer pairs for training. Since these datasets cover a range of scenarios (Section 3), including crime (UCF-Crime [33]), campus environments (ShanghaiTech [22] and CUHK Avenue [23]), pedestrian walkways (UCSD Ped1 [6] and Ped2 [37]), traffic situations (DoTA [45]), and human behavior (UBnormal [2]), and finally, the model tends to generalize to open-world scenarios. ", "page_idx": 1}, {"type": "text", "text": "To train our framework, we initially pre-train it on WebVid [3] to equip it with the capability to understand general videos. Then, we fine-tuned it on our proposed video anomaly dataset to enhance its understanding of video anomalies across multiple scenarios. Compared to other baselines, our model achieves SOTA performance in both Text-Level and GPT-Guided Metrics. Our contributions are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "\u2022 We propose a novel video-language framework, HAWK, aiming at understanding video anomalies, which incorporates motion modality to enhance its capability.   \n\u2022 We generate rich language descriptions for seven different video anomaly datasets. Meanwhile, considering the diversity of open-world problems, we also generate question-answer pairs to tackle potential user inquiries.   \n\u2022 Compared to other large video models, our framework demonstrates SOTA performance for video anomaly understanding and question-answering across multiple scenarios, which will help open-world anomaly understanding in the future. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Video Anomaly Detection Video Anomaly Detection (VAD) usually focuses on identifying unexpected events from the video and it has been widely applied in various fields, including autonomous driving [45], public surveillance [6, 23], and crime scene analysis [33] etc. Previous VAD methods [27, 33, 23, 44, 9, 13, 19, 34, 40, 48, 52] are designed in numerous pathways. Lu et al. [23] designed to learn video features only from normal videos, and hand-craft features or deep-learningbased features are leveraged. Sultani et al. [33] proposed multiple instance learning (MIL), which is the main paradigm for many weakly-supervised learning methods. Recently, Lv et al. [27] first proposed video-based large language models in the framework of VAD. ", "page_idx": 2}, {"type": "text", "text": "However, these methods lack sufficient semantic comprehension of scenes and offer inadequate user interaction. Several approaches [31, 42] have introduced multi-class classifiers to integrate semantic information with various types of anomaly information. Nevertheless, their output is still limited. In contrast, our framework not only integrates more comprehensive semantic information as a general video understanding system but also provides advanced interaction capabilities for users. ", "page_idx": 2}, {"type": "text", "text": "Large Model in Video Understanding Recent studies have demonstrated the reliable capabilities of large models in video understanding. Beyond powerful vision-language models [16, 51, 21, 24], recent research has increasingly explored more modalities [27, 18, 28, 46, 26]. Bain et al.[3] introduced a large-scale dataset with general video content descriptions. Several LLM-based works[18, 28, 46, 26] aim to comprehend visual content. Additionally, Video-LLaMa [49] extends comprehension to both auditory and visual information, while Su et al.[32] utilize multi-modal encoders to understand across six modalities. Recently, Lv et al.[27] proposed video-based large language models for VAD tasks in a weakly supervised framework. In this paper, we introduce the motion modality in our proposed vision-language model, which enhances the model\u2019s ability to locate anomalies by prioritizing relevant video content. ", "page_idx": 2}, {"type": "text", "text": "3 Data Engineering ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Previous datasets are inadequate for addressing our problem. Most existing VAD datasets, such as UBnormal [2] and DoTA [45], only contain simple video category labels and lack detailed language descriptions. This results in video understanding models lacking accurate and comprehensive supervision, creating a significant obstacle to identifying anomalies in videos. Recently, Lv et al.[27] attempted to create pseudo language descriptions for anomaly videos. However, these descriptions are naive combinations of labels and fixed text, relying on a rigid format that offers only limited information. Other datasets, like WebVid[3], include only general descriptions of video content, which may not direct the model\u2019s focus on anomalies. ", "page_idx": 2}, {"type": "text", "text": "Our Principle To tackle the above problems, we annotate detailed language descriptions specifically for anomaly scenes in seven different existing <VIDEO> datasets. These seven datasets include a variety of anomalous scenarios such as crime (UCF-Cirme [33]), campus (ShanghaiTech [22] and CUHK Avenue [23]), pedestrian walkways (UCSD Ped1 [6] and Ped2 [37]), traffic (DoTA [45]), and human behavior (UBnormal [2]). With the support of these visual scenarios, we can perform comprehensive fine-tuning for various abnormal scenarios, being closer to open-world scenarios. ", "page_idx": 2}, {"type": "image", "img_path": "vBKoEZ1PG3/tmp/1df0c759f453a210b06cdedaa921150d268c35b1aa63be12a24ec147c77f68ae.jpg", "img_caption": ["Figure 2: Generation pipeline of our dataset. In the first line, we first segment videos into clips and generate dense captions for each segment, including a comprehensive description of the video content. Then, we use GPT-4 to guide the generation of corresponding anomalous video descriptions based on these descriptions, which are then manually checked to reduce mistakes. In the second line, to generate user-centered QA pairs, we first use GPT-4 to generate open-ended questions based on the proposed two principles. Then, the questions and video descriptions are jointly input into GPT-4 to provide possible answers. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Moreover, to better account for real-world user situations, we believe that language descriptions should not only include descriptions of the video anomalies themselves, but also address open questions asked by users. Therefore, we construct open-ended question-answer pairs for each scenario to further enhance model\u2019s practical ability to answer users\u2019 varying questions. The procedure for answering users\u2019 questions is shown in Fig. 2. The data format of can be described by the Eq. (1), <VIDEO>: {DIS: <DESCRIPTION> | QA: <QUESTION> $\\rightarrow$ <ANSWERING>}. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Anomaly Video Description Generation To construct natural language descriptions <DESCRIPTION> for anomalous video datasets, we refer to previous research such as LLaVa [21] and VideoChat [18], and employ GPT-4 [1] as an assistant. We first split the video into dense clips to ensure key information is captured. Following VideoChat [18], we use perception tools (InternVideo [38], Tag2Text [14], or GRiT [39]) to automatically generate captions for each key clip, obtaining a dense representation of the videos (except for the UCF-Crime dataset, which already has a dense representation built in [47]). Next, we use GPT-4 [1] to generate anomaly-related descriptions based on the captions for each video. Unlike other general video understanding datasets [21, 18], we provide prompts for GPT-4 to generate specific descriptions closely related to video anomalies. Finally, due to varying quality of dense captions, some videos may have incorrect annotations. Thus, we manually recheck the final generated video anomaly descriptions to ensure label accuracy. ", "page_idx": 3}, {"type": "text", "text": "Human-Centric Question-Answering Pairs Generation So far, we have obtained nearly accurate descriptions of anomaly videos. However, our framework may still face challenges with more openended questions from users. Therefore, anomaly-related question-answering is a significant practical requirement. Given the diversity of open-world scenes, users may ask questions involving various pronouns. Thus, we mainly consider these two principles: $\\textcircled{1}$ Anomaly-related, our questions should be strongly related to the anomaly in the video. $\\textcircled{2}$ 5W2H, we introduce seven different question pronouns (What, Who, Where, When, How, How much, and Why) to simulate various question formats that users may employ. This enables us to address a wide range of open questions related to video anomalies. We input these two principles into GPT-4 [1] to generate open questions for anomaly videos. We then manually review and select the 100 most suitable questions, which are randomly assigned to each video. Finally, GPT-4 [1] will generate ${<}\\mathrm{ANSWERS}{>}$ to these <QUESTIONS>. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Our data is more practical compared to previous ones: it not only understands multiple anomalies in videos but also supports question-answering in open scenarios (More details in Appendix D). ", "page_idx": 4}, {"type": "text", "text": "4 Methodology ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To construct a practical framework for understanding video anomalies, our goal is to accurately interpret these anomalies into natural language. However, most previous studies [18, 49, 29, 20, 27] focus on enhancing general video understanding capabilities while neglecting video anomalies. This oversight results in equal attention being given to all parts of the video, such as the background and human appearances, often at the expense of key anomaly features, as shown in Fig. 1 (C). Consequently, these approaches are not effective in accurately focusing on anomaly-related features. ", "page_idx": 4}, {"type": "text", "text": "Overview of Solution The core of our solution is guiding visual instruction to focus on anomalies. Previous studies in video anomaly detection [44, 52] have demonstrated that motion-related feature help identify multiple anomalies. Therefore, in Section 4.1, we first explicitly integrate a motion modality into our proposed framework to target anomaly-related features. Subsequently, in Section 4.2, we maintain mutual information consistency between the appearance and motion modalities within a tight feature space, implicitly guiding the appearance branch to reinforce motion attention. Finally, in Section 4.3, to improve the interpretation of motion-to-language, we extract motion-related language descriptions to directly match the motion and its corresponding motion-related language. ", "page_idx": 4}, {"type": "text", "text": "4.1 Explicit Motion Modality Integration ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To enhance the capability of interpreting anomalies, we build a framework, HAWK, to explicitly integrate motion modality. HAWK has a dual-branch architecture, with $f_{v}$ as the original video understanding network and $f_{m}$ for motion understanding. Inspired by Video-LLaMA [49], $f_{v}$ and $f_{m}$ share the same architecture but separate parameters in Fig. 3. Eq. (2) denotes our framework as, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{Y}=\\mathrm{LLAMA}\\left(\\left[P_{v}(f_{v}(\\mathbf{X_{v}})),P_{m}(f_{m}(\\mathbf{X_{m}}))\\right]\\oplus f_{t}(\\mathbf{T})\\right),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbf{X_{v}}\\in\\operatorname{R}^{T\\times C\\times H\\times W}$ represents the ${<}\\mathrm{VIDEO}{>}$ input for extracting appearance feature, and $T$ denotes the temporal dimension. $\\mathbf{X_{m}}=M(\\mathbf{X_{v}})$ , with $M(\\cdot)$ being the motion extractor. ", "page_idx": 4}, {"type": "text", "text": "$f_{v}(\\cdot)$ and $f_{m}(\\cdot)$ are the frozen pre-trained video encoders from BLIP-2 [17], which consist of one EVA-CLIP [10] and one pre-trained Video Q-Former to output embeddings. Then, the output embeddings from $f_{v}(\\cdot)$ and $f_{m}(\\cdot)$ are passed through learnable projection networks for video and motion, $P_{v}(\\cdot)$ and $P_{m}(\\cdot)$ , respectively. These networks aim to project visual (video and motion) embedding into the language feature space for interpreting. $f_{t}(\\cdot)$ is the frozen text token to embedding projection, that makes textual information can be inputted into LLaMA-2 [35]. $\\bigoplus$ is for combining our input prompt, we define our prompt as: \u201cHere is the input video embedding: <VIDEO_EMBEDDING> and motion embedding <MOTION_EMBEDDING> in different frames, please help me to <DESCRIBE_VIDEO> | $<$ QUESTION>.\u201d. <DESCRIBE_VIDEO> and <QUESTION> are the question classes for video description generation and video question answering respectively (Details see Appendix D). By combining the visual token embedding with the textual embedding, $f_{t}(\\mathbf{T})$ , LLaMA2 [35], is employed to generate the final language response, Y. This framework explicitly integrates the motion modality during visual instruction tuning, significantly targeting anomaly-related features. ", "page_idx": 4}, {"type": "text", "text": "4.2 Implicitly Motion Attention Reinforcement ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Although we integrate the motion modality to facilitate fine-tuning of HAWK, motion and video branches operate independently. Therefore, we cannot expect the original video branch to extract appearance features that focus on the region where the anomaly occurred (i.e., motion). To help HAWK focus more on these regions, we observed the containment relationship in mutual information between motion and the original video. We use this relationship to construct an auxiliary consistency loss function, implicitly reinforcing the motion attention (Fig. $4\\,(2)$ . ", "page_idx": 4}, {"type": "image", "img_path": "vBKoEZ1PG3/tmp/fe34df02b63032ab7fd88eed9c826722da28abb312fc372b78678e42cda7e8a2.jpg", "img_caption": ["Figure 3: Overview of HAWK. During training (Black and $\\mathbf{Gray}$ path), we aim to optimize for videolanguage matching loss, along with Video-Motion Consistency and Motion-Language Matching. During inference (only Gray path), we generate language descriptions using video, motion, and text. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Extract Motion Specifically, to obtain motion, we employ a motion describer $M(\\cdot)$ , which generates motion between two successive frames as shown in Eq. (3), ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{X_{Motion}^{(t)}}=M^{(\\mathbf{t})}(\\mathbf{X_{v}^{(t)}},\\mathbf{X_{v}^{(t-1)}}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $M^{(\\mathbf{t})}(\\cdot)$ is the motion describer at the time step $\\mathbf{t}$ , we currently use Gunnar Farneback\u2019s algorithm [11], and $\\mathbf{X_{v}^{(t)}},\\mathbf{X_{v}^{(t-1)}}\\in\\mathrm{R}^{1\\times C\\times H\\times W}$ denote the video frames at time steps t and $\\mathbf{t}-\\mathbf{1}$ . $\\mathbf{X_{Motion}^{(t)}}\\ \\in\\ \\mathbb{R}^{2\\times H\\times W}$ includes two channels motion vector in $\\mathbf{X}$ (horizontal) and $\\mathbf{Y}$ (vertical) directions. We use the optical flow magnitude from these channels as a Mask, normalized to $[0,1]$ and multiplied with the original video appearance, to hide other non-motion regions, as Eq. (4), ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{X_{m}^{(t)}}=\\underbrace{\\mathrm{Nopend}(\\sqrt{(\\mathbf{X}_{\\mathrm{Motion}}^{(\\mathrm{t})}(\\mathbf{X}))^{2}+(\\mathbf{X}_{\\mathrm{Motion}}^{(\\mathrm{t})}(\\mathbf{Y}))^{2}})}_{\\mathrm{Mask}}\\times\\mathbf{X}_{\\mathbf{v}}^{(\\mathrm{t})},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\times$ is the operator of pixel-wise multiplication. $\\mathbf{X}_{\\mathbf{v}}^{\\left(\\mathbf{t}\\right)}$ , $\\mathbf{X_{m}^{(t)}}\\in\\mathrm{R}^{1\\times C\\times H\\times W}$ donate the original video and our input motion information at time step $\\mathbf{t}$ , respectively. We usually extract $T$ frames as motion input $\\mathbf{X_{m}^{\\mathcal{\\bar{\\ }}}}\\in\\mathrm{R}^{T\\times C\\times H\\times W}$ , same as $\\mathbf{X_{v}}$ . ", "page_idx": 5}, {"type": "text", "text": "Build ${\\mathcal{L}}_{M V}$ Loss Then, we consider that $\\mathbf{X_{m}}$ only contains key information for anomaly and it is contained in $\\mathbf{X_{v}}$ , and feature space from $\\mathbf{X_{v}}$ is more sparse. Therefore, we compact features from $\\mathbf{X_{m}}$ and $\\mathbf{X_{v}}$ into a tight space. At this space, we aim to maintain the mutual information between $\\mathbf{X_{m}}$ and $\\mathbf{X_{v}}$ consistency, and in this way, the appearance feature can be focused on the motion region. Therefore, we construct an auxiliary loss to promote $\\mathbf{X_{v}}$ \u2019s motion attention, as in Eq. (5), ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{M V}=1-\\mathrm{Cos}_{-}\\mathrm{SIM}(\\mathbf{X_{m}^{c}},\\mathbf{X_{v}^{c}})=1-\\frac{\\mathbf{X_{m}^{c}}\\cdot\\mathbf{X_{v}^{c}}}{\\lVert\\mathbf{X_{m}^{c}}\\rVert\\lVert\\mathbf{X_{v}^{c}}\\rVert},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathbf{X_{v}^{c}}=C_{v}(f_{v}(\\mathbf{X_{v}}))$ and $\\mathbf{X_{m}^{c}}=C_{m}(f_{m}(\\mathbf{X_{m}}))$ denote the tightly compressed representations of $\\mathbf{X_{v}}$ and $\\mathbf{X_{m}}$ , respectively, by the compression functions $C_{v}$ and $C_{m}$ . $C_{v}$ and $C_{m}$ share some initial shallow layer parameters with $P_{v}$ and $P_{m}$ (as Fig. 3). Then, following a subsequent tight projection to compresses both $\\mathbf{X_{v}}$ and $\\mathbf{X_{m}}$ into a more compacted space. ", "page_idx": 5}, {"type": "image", "img_path": "vBKoEZ1PG3/tmp/b38bbe1203ef9f222838368c5f379daf861b1bcde293f4381027419df17f6110.jpg", "img_caption": ["Figure 4: Visualization of HAWK\u2019s loss. $\\tilde{\\textmd{(1)}}$ is the original video-to-language loss. $\\textcircled{2}$ is the cosine similarity loss for motion modality adaptation. $\\circled{3}$ is the motion-to-language loss. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Finally, with this auxiliary loss, we can reinforce the motion attention in the appearance feature, and HAWK\u2019s feature space will focus on more abnormal related features, which will promote the understanding of anomalies in the whole framework. ", "page_idx": 5}, {"type": "text", "text": "4.3 Interpreting Motion-to-Language ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Although HAWK has already accommodated the motion modality in visual input, the corresponding motion from language is still unclear. This limitation hinders HAWK\u2019s interpretation in motion modality. Hence, to augment this relationship, we aim to reinforce the correspondence between motion and their linguistic representation. ", "page_idx": 6}, {"type": "text", "text": "Extract Motion-related Language Previous studies [5, 36, 43, 15] have proved that the representation of motion in the language is predominantly from verbs and their corresponding entities. Therefore, to extract linguistic representation, the first step is to do dependency parsing for the original sentences, as Eq. (6), ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbf{G_{gt}}=D(\\mathbf{Y_{gt}}),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $D(\\cdot)$ is the dependency parsing and $\\mathbf{Y_{gt}}$ is the ground truth. $\\mathbf{G_{gt}}$ represents the graph of the dependency structure, which symbolizes the syntactic relationships among the words in a sentence. ", "page_idx": 6}, {"type": "text", "text": "Based on this graph, we can extract predicates (verbs) $\\mathbf{V}$ , and also entities closely related to these predicates, such as subjects S, objects $\\mathbf{O}$ , indirect subjects $\\mathbf{S_{i}}$ , and indirect objects $\\mathbf{O_{i}}$ . These elements are then combined to form short phrases representing motion, as in Eq. (7), ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbf{Y_{gt}^{m}}=\\{\\mathbf{V},\\mathbf{S},\\mathbf{O},\\mathbf{S_{i}},\\mathbf{O_{i}}\\}=M_{l}(\\mathbf{G_{gt}}),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $M_{l}(\\cdot)$ is the language motion extraction operator, and $\\mathbf{Y}_{\\mathrm{gt}}^{\\mathbf{m}}$ is the motion-related language. ", "page_idx": 6}, {"type": "text", "text": "Build $\\mathcal{L}_{M L}$ Loss After obtaining motion-related language, we can establish strong supervision between motion in both vision and linguistic representation (as Fig. $4\\,\\textcircled{3})$ , significantly enhancing the ability to interpret motion to language in HAWK. Consequently, we design a motion-language matching as an auxiliary loss, as Eq. (8), ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}_{M L}^{m}(\\mathbf{Y_{m}},\\mathbf{Y_{gt}^{m}})=-\\sum_{i=1}^{N}\\mathbf{Y_{gt}^{m}}^{i}\\log(\\mathbf{Y_{m}}^{i})\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\mathcal{L}_{M L}(\\cdot)$ is the cross-entropy loss, which contains $N$ words. ", "page_idx": 6}, {"type": "text", "text": "Optimization Goal Finally, our total loss $\\mathcal{L}$ shows as, $\\underline{{\\mathcal{L}}}=\\mathbf{t_{0}}\\times\\mathcal{L}_{V L}+\\mathbf{t_{1}}\\times\\mathcal{L}_{M V}+\\mathbf{t_{2}}\\times\\mathcal{L}_{M L}$ , where $\\mathcal{L}_{V L}$ is original video to language loss (as Fig. 4 $\\textcircled{1}$ , and $\\bf{t_{0}},\\bf{t_{1}}$ and $\\mathbf{t_{2}}$ is the hyper-parameter. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we will provide a comprehensive introduction to the experiments, including the processes of training and testing, the establishment of baselines, the methods of evaluations, and the detailed examination of ablation experiments pertaining to HAWK. ", "page_idx": 6}, {"type": "text", "text": "Training & Testing To enhance our framework\u2019s anomaly understanding capabilities, we\u2019ve structured our training and testing process into three stages, as Fig. 5. Stage 1 involves pre-training on the WebVid dataset [3] to acquire a general understanding of video content. In Stage 2, we finetune the model\u2019s focus towards video anomaly understanding by employing a specially curated dataset described in Section 1, consisting of over 8, 000 videos. We use $90\\%$ of these videos for training and allocate the remaining $10\\%$ for testing purposes. We jointly train on two tasks: video <DESCRIPTION> generation and video $<$ QUESTIO $\\mathrm{N}{>}{\\longrightarrow}\\bullet$ <ANSWERING>. In Stage 3, we evaluate these two tasks independently in the testing set to ensure our model\u2019s effectiveness. ", "page_idx": 6}, {"type": "image", "img_path": "vBKoEZ1PG3/tmp/5ddca00805f36e4198817434e16444efef795e4a463de7977eec8b7f67793d52.jpg", "img_caption": ["Figure 5: Training & Testing. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Baselines To evaluate the anomaly understanding performance of our proposed framework, we conduct comparisons with SOTA video understanding baselines. We select five baselines: VideoChatGPT [29], VideoChat [18], Video-LLaMA [49], LLaMA-Adapter [50], and Video-LLaVA [20]. Our comparison aims to determine whether these baselines can fully understand and interpret video ", "page_idx": 6}, {"type": "text", "text": "Table 1: Quantitative performance of (A) anomaly video description generation and (B) video question-answering. Red indicates the best performance, while blue denotes the second best. ", "page_idx": 7}, {"type": "table", "img_path": "vBKoEZ1PG3/tmp/0f6da39aed7e6755ce506f8a9db0fff3a3617644cf71717a7d318cf573fe58b7.jpg", "table_caption": ["(A) Anomaly Video Description Generation "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "vBKoEZ1PG3/tmp/9ef067b72147489c1aebeda9ea13df281ca7386c3de51449441e7a2fdfaab526.jpg", "table_caption": ["(B) Anomaly Video Question-Answering "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "anomalies. To ensure the fairness of our experiments, we employed the baselines with the same size (7B parameters) as the backbone. ", "page_idx": 7}, {"type": "text", "text": "Evaluation Metrics To accurately evaluate our model\u2019s performance in understanding video anomalies, we firstly adopt four Text-Level metrics, from BLEU (Bilingual Evaluation Understudy) [30]-1 to BLEU-4 to measure word overlap between the model-generated text and the ground truth. This approach enables us to objectively assess the similarity and also take into account various levels of granularity at the text-level, thus providing a clear indicator of how well the model understands and describes anomalies. ", "page_idx": 7}, {"type": "text", "text": "Besides, we expand our evaluation framework by incorporating insights from recent research in LLaVa [21] or Video-ChatGPT [29], utilizing GPT-Guided [1] methods to assess the quality of the generated text. GPT [1] serves as a critical evaluator, generating scores for three key aspects of the language produced, with each aspect scored on a scale from 0 to 1. These three aspects are as, ", "page_idx": 7}, {"type": "text", "text": "\u2022 Reasonability: evaluates the logical reasoning and coherence of the generated language.   \n\u2022 Detail: assesses the level of detail and specificity of the generated language.   \n\u2022 Consistency: evaluates the coherence and consistency of the generated language. ", "page_idx": 7}, {"type": "text", "text": "By leveraging GPT [1] as an evaluative tool, we aim to provide a nuanced understanding of the text\u2019s quality, focusing on aspects that traditional metrics may overlook. ", "page_idx": 7}, {"type": "text", "text": "Quantitative Evaluation Table 1 (A) and (B) demonstrate the effectiveness of our model to describe abnormal phenomena. Our proposed model significantly outperforms the previous baselines, achieving SOTA performance in every metric for both Text-level and GPT-guided metrics, thus it can generate text that more closely aligns with actual scenarios. ", "page_idx": 7}, {"type": "text", "text": "Qualitative Evaluation Table 2 (A) and (B) demonstrate that our proposed framework achieves optimal qualitative performance in video description generation and question-answering, respectively. Compared with other baselines, HAWK can accurately understand and focus on video anomalies. For example, in Table 2 (A) - Video-LLaMa [49], it pays more attention to the clothing information from the people (wearing blue and red jacket), while ignoring the motion-related anomaly (slipping). In Table 2 (B) - Video-ChatGPT, it may produce hallucinations (two people... who were hit by the car), which differ from the original video anomaly (car suddenly braking). In contrast, HAWK generates descriptions that are close to the real semantics (driver losing control). ", "page_idx": 7}, {"type": "text", "text": "Table 2: Qualitative performance on (A) anomaly video description generation, and (B) questionanswering. Red texts indicate key semantic inconsistencies, whereas Green texts signify that the generated results are closely aligned with the Ground Truth. [YELLOW] indicates the text problem. ", "page_idx": 8}, {"type": "table", "img_path": "vBKoEZ1PG3/tmp/ed44cefc21dcd3504ca9205192ba8a26a5ea9191ab5e0811473a36190469af86.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Ablation Study We conducted ablation experiments on three key structures proposed in this paper and analyzed their impact on the overall performance in Table 3 (A) and (B). ", "page_idx": 8}, {"type": "text", "text": "\u2022 Effectiveness of Motion Information: We ablate all the motion components, including $f_{m}$ , $P_{m}$ and the motion input $\\mathbf{X_{m}}$ for proving the effectiveness of introducing motion modality. ", "page_idx": 8}, {"type": "table", "img_path": "vBKoEZ1PG3/tmp/0faac27e1b3c5863859ddcd9e10871b71503c740fe4f4b9601f021664f5f6abe.jpg", "table_caption": ["Table 3: Ablation study of (A) anomaly video description generation and (B) video questionanswering. Red indicates the best performance, while blue denotes the second best. ", "(A) Anomaly Video Description Generation "], "table_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "vBKoEZ1PG3/tmp/dcb204c15de0f079881c8f5d17a8d16f486c111bf089ea0c736b667858d3fafb.jpg", "table_caption": ["Table 4: Qualitative evaluation of ablation study. Red/Green texts indicate language semantic inconsistency/consistency with the Ground Truth respectively. [YELLOW] indicates the text problem. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "When explicit motion information is lacking, the model\u2019s ability to describe the motionsrelated anomaly diminishes, leading to inaccurate descriptions or even hallucinations (Table 4 w/o Motion Information), then impedes the overall performance (Table 3). \u2022 Effectiveness of Video-Motion Consistency: The absence of video-motion consistency constraints reduces the generative model\u2019s ability to adapt to the motion modality, causing difficulties in accurately understanding motion scenes (Table 4 w/o Video-Motion Consistency), then impedes the overall performance (Table 3). \u2022 Effectiveness of Motion-Language Matching: Without motion-language matching loss, the correlation between motion and language becomes unclear. This ambiguity leads to the generation of language that includes unspecified motion information (Table 4 w/o Motion-Language Matching), subsequently degrading the overall performance (Table 3). ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In conclusion, we have developed a novel video-language framework for understanding video anomalies across various scenarios. By incorporating motion features and constructing rich linguistic descriptions, our model demonstrates SOTA performance in the open world. It has the potential to benefit practical applications in diverse domains and paves the way for improving the model\u2019s interactivity with users, enabling more efficient and effective communication in addressing userspecific inquiries related to video anomalies. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This paper is supported by Guangdong Provincial Key Lab of Integrated Communication, Sensing and Computation for Ubiquitous Internet of Things (No.2023B1212010007), the Innovation and Technology Fund of HKSAR under grant number GHX/054/21GD, the Natural Science Foundation of Zhejiang Province, China, under No. LD24F020002, and National Science Fund for Distinguished Young Scholars (62025205). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al.: Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023) 4, 5, 8   \n[2] Acsintoae, A., Florescu, A., Georgescu, M.I., Mare, T., Sumedrea, P., Ionescu, R.T., Khan, F.S., Shah, M.: Ubnormal: New benchmark for supervised open-set video anomaly detection. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 20143\u201320153 (2022) 2, 3, 16, 26   \n[3] Bain, M., Nagrani, A., Varol, G., Zisserman, A.: Frozen in time: A joint video and image encoder for end-to-end retrieval. In: IEEE International Conference on Computer Vision (2021) 2, 3, 7, 15   \n[4] Brox, T., Malik, J.: Large displacement optical flow: descriptor matching in variational motion estimation. IEEE transactions on pattern analysis and machine intelligence 33(3), 500\u2013513 (2010) 19   \n[5] Cadiot, P., Lebas, F., Visetti, Y.M.: The semantics of the motion verbs. Space in Languages: Linguistic Systems and Cognitive Categories 66, 175 (2006) 7   \n[6] Chan, A.B., Vasconcelos, N.: Modeling, clustering, and segmenting video with mixtures of dynamic textures. IEEE transactions on pattern analysis and machine intelligence 30(5), 909\u2013926 (2008) 1, 2, 3, 16, 27   \n[7] Dosovitskiy, A., Fischer, P., Ilg, E., Hausser, P., Hazirbas, C., Golkov, V., Van Der Smagt, P., Cremers, D., Brox, T.: Flownet: Learning optical flow with convolutional networks. In: Proceedings of the IEEE international conference on computer vision. pp. 2758\u20132766 (2015) 19   \n[8] Du, H., Zhang, S., Xie, B., Nan, G., Zhang, J., Xu, J., Liu, H., Leng, S., Liu, J., Fan, H., Huang, D., Feng, J., Chen, L., Zhang, C., Li, X., Zhang, H., Chen, J., Cui, Q., Tao, X.: Uncovering what, why and how: A comprehensive benchmark for causation understanding of video anomaly. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2024) 21   \n[9] Dubey, S., Boragule, A., Jeon, M.: 3d resnet with ranking loss function for abnormal activity detection in videos. In: 2019 International Conference on Control, Automation and Information Sciences (ICCAIS). pp. 1\u20136. IEEE (2019) 2, 3   \n[10] Fang, Y., Wang, W., Xie, B., Sun, Q.S., Wu, L.Y., Wang, X., Huang, T., Wang, X., Cao, Y.: Eva: Exploring the limits of masked visual representation learning at scale. 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) pp. 19358\u201319369 (2022) 5   \n[11] Farneback, G.: Fast and accurate motion estimation using orientation tensors and parametric motion models. In: Proceedings 15th International Conference on Pattern Recognition. ICPR2000. vol. 1, pp. 135\u2013139. IEEE (2000) 6   \n[12] Farneb\u00e4ck, G.: Two-frame motion estimation based on polynomial expansion. In: Image Analysis: 13th Scandinavian Conference, SCIA 2003 Halmstad, Sweden, June 29\u2013July 2, 2003 Proceedings 13. pp. 363\u2013370. Springer (2003) 19   \n[13] He, C., Shao, J., Sun, J.: An anomaly-introduced learning method for abnormal event detection. Multimedia Tools and Applications 77, 29573\u201329588 (2018) 2, 3 ", "page_idx": 10}, {"type": "text", "text": "[14] Huang, X., Zhang, Y., Ma, J., Tian, W., Feng, R., Zhang, Y., Li, Y., Guo, Y., Zhang, L.: Tag2text: Guiding vision-language model via image tagging. arXiv preprint arXiv:2303.05657 (2023) 4 ", "page_idx": 11}, {"type": "text", "text": "[15] Langacker, R.W.: Nouns and verbs. Language pp. 53\u201394 (1987) 7   \n[16] Li, J., Li, D., Savarese, S., Hoi, S.: Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597 (2023) 3   \n[17] Li, J., Li, D., Savarese, S., Hoi, S.C.H.: Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In: International Conference on Machine Learning (2023) 5   \n[18] Li, K., He, Y., Wang, Y., Li, Y., Wang, W., Luo, P., Wang, Y., Wang, L., Qiao, Y.: Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355 (2023) 2, 3, 4, 5, 7, 8, 9, 22, 23, 24, 25, 26, 27   \n[19] Li, S., Liu, F., Jiao, L.: Self-training multi-sequence learning with transformer for weakly supervised video anomaly detection. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 36, pp. 1395\u20131403 (2022) 2, 3   \n[20] Lin, B., Zhu, B., Ye, Y., Ning, M., Jin, P., Yuan, L.: Video-llava: Learning united visual representation by alignment before projection. arXiv preprint arXiv:2311.10122 (2023) 5, 7, 8, 9, 22, 23, 24, 25, 26, 27   \n[21] Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning. Advances in neural information processing systems 36 (2024) 2, 3, 4, 8, 10   \n[22] Liu, W., W. Luo, D.L., Gao, S.: Future frame prediction for anomaly detection \u2013 a new baseline. In: 2018 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2018) 2, 3, 15, 25   \n[23] Lu, C., Shi, J., Jia, J.: Abnormal event detection at 150 fps in matlab. In: Proceedings of the IEEE international conference on computer vision. pp. 2720\u20132727 (2013) 1, 2, 3, 15, 23   \n[24] Lu, H., Niu, X., Wang, J., Wang, Y., Hu, Q., Tang, J., Zhang, Y., Yuan, K., Huang, B., Yu, Z., et al.: Gpt as psychologist? preliminary evaluations for gpt-4v on visual affective computing. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) workshop (2024) 3   \n[25] Lu, H., Tang, J., Xu, X., Cao, X., Zhang, Y., Wang, G., Du, D., Chen, H., Chen, Y.: Scaling multi-camera 3d object detection through weak-to-strong eliciting. arXiv (2024) 1   \n[26] Luo, R., Zhao, Z., Yang, M., Dong, J., Qiu, M., Lu, P., Wang, T., Wei, Z.: Valley: Video assistant with large language model enhanced ability. arXiv preprint arXiv:2306.07207 (2023) 3   \n[27] Lv, H., Sun, Q.: Video anomaly detection and explanation via large language models. arXiv preprint arXiv:2401.05702 (2024) 2, 3, 5   \n[28] Maaz, M., Rasheed, H., Khan, S., Khan, F.S.: Video-chatgpt: Towards detailed video understanding via large vision and language models. arXiv preprint arXiv:2306.05424 (2023) 3   \n[29] Muhammad Maaz, Hanoona Rasheed, S.K., Khan, F.: Video-chatgpt: Towards detailed video understanding via large vision and language models. ArXiv 2306.05424 (2023) 2, 5, 7, 8, 9, 22, 23, 24, 25, 26, 27   \n[30] Papineni, K., Roukos, S., Ward, T., Zhu, W.J.: Bleu: a method for automatic evaluation of machine translation. In: Proceedings of the 40th annual meeting of the Association for Computational Linguistics. pp. 311\u2013318 (2002) 8, 10   \n[31] Pu, Y., Wu, X., Wang, S.: Learning prompt-enhanced context features for weakly-supervised video anomaly detection. arXiv preprint arXiv:2306.14451 (2023) 2, 3   \n[32] Su, Y., Lan, T., Li, H., Xu, J., Wang, Y., Cai, D.: Pandagpt: One model to instruction-follow them all. arXiv preprint arXiv:2305.16355 (2023) 3   \n[33] Sultani, W., Chen, C., Shah, M.: Real-world anomaly detection in surveillance videos. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 6479\u20136488 (2018) 1, 2, 3, 15, 24   \n[34] Tian, Y., Pang, G., Chen, Y., Singh, R., Verjans, J.W., Carneiro, G.: Weakly-supervised video anomaly detection with robust temporal feature magnitude learning. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 4975\u20134986 (2021) 2, 3   \n[35] Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al.: Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023) 5   \n[36] Vo, N.P.A., Manotas, I., Sheinin, V., Popescu, O.: Identifying motion entities in natural language and a case study for named entity recognition. In: Proceedings of the 28th International Conference on Computational Linguistics. pp. 5250\u20135258 (2020) 7   \n[37] Wang, S., Miao, Z.: Anomaly detection in crowd scene. In: IEEE 10th International Conference on Signal Processing Proceedings. pp. 1220\u20131223. IEEE (2010) 2, 3, 16   \n[38] Wang, Y., Li, K., Li, Y., He, Y., Huang, B., Zhao, Z., Zhang, H., Xu, J., Liu, Y., Wang, Z., et al.: Internvideo: General video foundation models via generative and discriminative learning. arXiv preprint arXiv:2212.03191 (2022) 4   \n[39] Wu, J., Wang, J., Yang, Z., Gan, Z., Liu, Z., Yuan, J., Wang, L.: Grit: A generative region-to-text transformer for object understanding. arXiv preprint arXiv:2212.00280 (2022) 4   \n[40] Wu, P., Liu, J.: Learning causal temporal relation and feature discrimination for anomaly detection. IEEE Transactions on Image Processing 30, 3513\u20133527 (2021) 2, 3   \n[41] Wu, P., Liu, j., Shi, Y., Sun, Y., Shao, F., Wu, Z., Yang, Z.: Not only look, but also listen: Learning multimodal violence detection under weak supervision. In: European Conference on Computer Vision (ECCV) (2020) 20   \n[42] Wu, P., Zhou, X., Pang, G., Sun, Y., Liu, J., Wang, P., Zhang, Y.: Open-vocabulary video anomaly detection. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2024) 2, 3   \n[43] Wunderlich, D.: Cause and the structure of verbs. Linguistic inquiry pp. 27\u201368 (1997) 7   \n[44] Xu, D., Ricci, E., Yan, Y., Song, J., Sebe, N.: Learning deep representations of appearance and motion for anomalous event detection. arXiv preprint arXiv:1510.01553 (2015) 2, 3, 5   \n[45] Yao, Y., Wang, X., Xu, M., Pu, Z., Wang, Y., Atkins, E., Crandall, D.J.: Dota: unsupervised detection of traffic anomaly in driving videos. IEEE transactions on pattern analysis and machine intelligence 45(1), 444\u2013459 (2022) 1, 2, 3, 16, 22   \n[46] Ye, Q., Xu, H., Xu, G., Ye, J., Yan, M., Zhou, Y., Wang, J., Hu, A., Shi, P., Shi, Y., et al.: mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178 (2023) 3   \n[47] Yuan, T., Zhang, X., Liu, K., Liu, B., Chen, C., Jin, J., Jiao, Z.: Towards surveillance video-andlanguage understanding: New dataset, baselines, and challenges (2023) 4   \n[48] Zaheer, M.Z., Mahmood, A., Astrid, M., Lee, S.I.: Claws: Clustering assisted weakly supervised learning with normalcy suppression for anomalous event detection. In: Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XXII 16. pp. 358\u2013376. Springer (2020) 2, 3   \n[49] Zhang, H., Li, X., Bing, L.: Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858 (2023) 3, 5, 7, 8, 9, 22, 23, 24, 25, 26, 27   \n[50] Zhang, R., Han, J., Liu, C., Gao, P., Zhou, A., Hu, X., Yan, S., Lu, P., Li, H., Qiao, Y.: Llama-adapter: Efficient fine-tuning of language models with zero-init attention. arXiv preprint arXiv:2303.16199 (2023) 7, 8, 9, 22, 23, 24, 25, 26, 27   \n[51] Zhu, D., Chen, J., Shen, X., Li, X., Elhoseiny, M.: MiniGPT-4: Enhancing vision-language understanding with advanced large language models. In: The Twelfth International Conference on Learning Representations (2024) 3   \n[52] Zhu, Y., Newsam, S.: Motion-aware feature for improved video anomaly detection. arXiv preprint arXiv:1907.10211 (2019) 2, 3, 5 ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Summary of Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "This appendix provides supplementary information that was not included in the main paper. Firstly, we address the security statement of our study, ensuring the confidentiality and integrity of the data used. Additionally, we provide detailed explanations of the training and testing resources utilized, including information on the hardware and software configurations. We also present statistics and distribution of the training data, along with the costs associated with human resources involved in the study. Furthermore, we describe the evaluation metrics employed to assess the performance of our method. Moreover, we present additional qualitative results comparisons, showcasing the effectiveness of our approach. Additionally, we provide an open-world demo, demonstrating the real-world applicability of our method. Finally, we discuss the existing limitations of our paper and propose potential avenues for future research. ", "page_idx": 14}, {"type": "text", "text": "B Security Statement ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "To prevent any potential misuse and ensure responsible use, we have strictly limited the application scope of our proposed method, HAWK. Unless authorized, HAWK is only permitted for use in research domains. ", "page_idx": 14}, {"type": "text", "text": "Additionally, access to the proposed dataset is restricted to qualified institutions and organizations, who must provide a clear purpose for its use. We explicitly prohibit the application of the dataset in situations that may cause potential danger or have a significant social impact. ", "page_idx": 14}, {"type": "text", "text": "These measures are in place to ensure the ethical and responsible use of our research. ", "page_idx": 14}, {"type": "text", "text": "C Details in Training and Testing ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Computational Resource During the pre-training phase, we utilized four Nvidia GTX A6000 GPUs\\* to train on the WebVid dataset [3] for approximately 120 hours. In the fine-tuning phase, we employed two Nvidia GTX A6000 GPUs to fine-tune on our proposed dataset for about 80 hours. ", "page_idx": 14}, {"type": "text", "text": "Efficiency During testing, the average model response time for each round of conversation with HAWK is approximately 2ms. Additionally, considering the available graphics memory, the model can handle video clips of up to 32 frames. Therefore, it is necessary to extract different frames from longer videos. ", "page_idx": 14}, {"type": "text", "text": "Hyper-parameters In the loss function, $\\mathbf{t_{0}}$ is set to 1 for our main task, video-to-language, and $\\mathbf{t_{1}}$ and $\\mathbf{t_{2}}$ are set to 0.1, as two auxiliary tasks for balancing different loss values. ", "page_idx": 14}, {"type": "text", "text": "D Details in Dataset ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Dataset Introduction and Statistics Our study utilizes seven video anomaly datasets, each encompassing different scenes. The detailed statistics and introduction of these datasets are as follows: ", "page_idx": 14}, {"type": "text", "text": "\u2022 UCF-Cirme [33]: The UCF-Crime dataset comprises an extensive collection of 128 hours of video. It consists of 1,900 long and untrimmed real-world surveillance videos, featuring 13 distinct classes of realistic anomalies. These anomalies are carefully chosen due to their notable implications for public safety. \u2022 ShanghaiTech [22]: The ShanghaiTech Campus dataset comprises 13 scenes characterized by complex light conditions and varied camera angles. It encompasses 130 instances of abnormal events and encompasses over 270,000 training frames. Notably, this dataset includes annotations for both frame-level and pixel-level ground truth of abnormal events, providing comprehensive insight into anomaly detection and localization tasks. \u2022 CUHK Avenue [23]: The CUHK Avenue Dataset comprises 16 training and 21 testing video clips designed for abnormal event detection. Captured within the CUHK campus avenue, these videos encompass a total of 30,652 frames, divided into 15,328 frames for training and 15,324 frames for testing. The training videos capture normal situations, while the testing videos include both normal and abnormal events. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "\u2022 UCSD Dataset [6, 37]: The UCSD Anomaly Detection Dataset was captured using a stationary camera positioned at an elevation, providing an overhead view of pedestrian walkways. The crowd density within these walkways exhibits variability, spanning from sparsely populated areas to densely crowded environments. It is split into 2 subsets, each corresponding to a different scene. Ped1 [6] includes a total of 34 training video samples and 36 testing video samples, while Ped2 [37] consists of 16 training video samples and 12 testing video samples.   \n\u2022 DoTA [45]: The Detection of Traffic Anomaly (DOTA) Dataset introduces the When-WhereWhat pipeline with temporal, spatial, and categorical annotations. It contains 4677 videos, all with a resolution of $1280\\mathrm{~x~}720$ pixels. Notably, the original videos were extracted at a frame rate of 10 fps in this dataset.   \n\u2022 UBnormal [2]: The UBnormal dataset is a supervised open-set benchmark designed explicitly for video anomaly detection, comprising diverse virtual scenes. It introduces abnormal events annotated at the pixel level during training, which enables the utilization of fullysupervised learning techniques for abnormal event detection. ", "page_idx": 15}, {"type": "text", "text": "In our study, we extend upon these existing datasets by implementing our data engineering pipeline. This pipeline generates comprehensive descriptions of video anomalies and formulates open questions derived from these anomalies. ", "page_idx": 15}, {"type": "text", "text": "Data Distribution To demonstrate the applicability of our data in an open-world scenario, we conducted a statistical analysis of the data distribution. Figure 6 illustrates the data distribution of all the datasets we utilized, indicating that our method can effectively support various open-world datasets. Besides, we acknowledge the need to expand our dataset further to enhance the model\u2019s applicability in this task. ", "page_idx": 15}, {"type": "image", "img_path": "vBKoEZ1PG3/tmp/e6ea87305ca193602fdc7ca71f74f68918646d4bc915dd39eff58793911ab194.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Figure 6: Violin plot of data distribution. We use PCA dimensional reduction to measure the feature distribution of different datasets, where there are significant differences in the feature distribution. ", "page_idx": 15}, {"type": "text", "text": "Manual Checking Before conducting the experiments, we manually checked the textual descriptions generated for the videos. Specifically, we consider the following aspects: ", "page_idx": 15}, {"type": "text", "text": "1. Error Correction: We removed text descriptions that contained obvious errors about the video content and supplemented the correct object, behavior, and scene information. (For instance, GPT tends to misidentify dogs in videos, describe running pedestrians as skateboards and motorcycles, and mistake scenes containing water as rainy days.)   \n2. Detail Enhancement: We provided more detailed textual descriptions of anomalies in the video (such as pedestrians lingering or jumping in the middle of the road).   \n3. Human Resource Cost: We formed a team of five annotators to conduct Manual Checking on all the videos. Since most of the videos already had automatically generated annotations, each annotator invested approximately 30 hours of work during the labeling process, processing about 1700 videos. ", "page_idx": 16}, {"type": "text", "text": "Table 5 below provides an example of before and after manual checking. ", "text_level": 1, "page_idx": 16}, {"type": "table", "img_path": "vBKoEZ1PG3/tmp/4831330b7ab2717701e6f3eed2a3fc2105ae65b7a8a5787e4887e41080c7a5ee.jpg", "table_caption": ["Table 5: An Example of Manual Checking. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "In this example, the caption generated by GPT-4 included hallucinations (such as \"a man walking by with a shopping bag\" or \"a young boy in a red jacket\" ), which were corrected after Manual Checking. Additionally, GPT-4\u2019s description was inaccurate (for instance, \"bus stop\" should have been more accurately described as \"subway entrance\"). ", "page_idx": 16}, {"type": "text", "text": "<DESCRIBE_VIDEO> and Generated Open-World <QUESTION> We set 20 problems for <DESCRIBE_VIDEO>, and during each iteration in training, we randomly select one of them. ", "page_idx": 16}, {"type": "image", "img_path": "vBKoEZ1PG3/tmp/cc1c16003fdc9251bfcbc4f38cb4d0f3e92260b56db9988984650edb6f40282d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "We have also generated $100<$ QUESTIONS> for open-world anomalies. To mimic user behavior, some of these questions are closely related to the video scene, while others are less closely related. However, all of these questions are potential inquiries in an open-world scenario. ", "page_idx": 16}, {"type": "text", "text": "1. Who is causing the disturbance in the video?   \n2. What is the unusual activity happening in the video?   \n3. When did the anomaly occur in the video?   \n4. Where is the strange event taking place in the video?   \n5. Why is the object in the video behaving abnormally?   \n6. How is the anomaly in the video affecting the surroundings?   \n7. How much damage was caused by the incident in the video?   \n8. Who is the main person involved in the unusual event?   \n9. What is the cause of the sudden change in the video? ", "page_idx": 16}, {"type": "text", "text": "11.12.13.14.15.16.17.18.19.20.21.22.23.24.25.26.27.28.29.30.31.32.33.34.35.36.37.38.39.40.41.42.43.44.45.46.47.48.49.50.51.52. 1101.. When does the suspicious activity start in the video? Why are the people in the video reacting in that way? Where can I find more information about the incident in the video? How can I identify the source of the problem in the video? How much time does the abnormal event last in the video? Who are the other people affected by the anomaly in the video? What actions were taken to address the issue in the video? When was the video recorded, and is it a recent event? Where else can I find similar incidents in other videos? Why is the vehicle in the video moving erratically? How can I prevent such anomalies from occurring in the future? How much impact does the abnormal event have on the overall situation? Who should I contact if I notice a similar anomaly in another video? What steps can I take to investigate the issue further? When is the best time to report an unusual event in a video? Where can I find resources to help me understand the anomaly better? Why did the equipment in the video malfunction? How can I differentiate between normal and abnormal behavior in a video? How much does it cost to implement a system that detects anomalies in videos? Who can provide expert advice on handling video anomalies? What is the most common type of anomaly found in videos? When should I be concerned about an anomaly in a video? Where can I find a list of known video anomalies and their descriptions? Why is it important to detect and analyze anomalies in videos? How can I improve my ability to spot anomalies in videos? How much training is required to become proficient in detecting video anomalies? Who can I collaborate with to better understand video anomalies? What are the potential consequences of ignoring an anomaly in a video? When did the trend of analyzing anomalies in videos begin? Where can I find examples of successfully resolved video anomaly cases? Why do some anomalies in videos go unnoticed? How can I report a video anomaly to the appropriate authorities? How much time is needed to thoroughly analyze a video anomaly? Who is responsible for monitoring and addressing video anomalies? What are the best tools to use for detecting anomalies in videos? When is it necessary to escalate a video anomaly for further investigation? Where can I find guidelines on how to handle video anomalies? Why do some video anomalies lead to serious consequences? How can I ensure the accuracy of my video anomaly detection system? How much effort is needed to maintain a video anomaly detection system? Who should be informed when a video anomaly is detected? What are the signs that indicate a potential anomaly in a video? 53.54.55.56.57.58.59.60.61.62.63.64.65.66.67.68.69.70. When should I perform a follow-up analysis on a detected video anomaly? Where can I find support for dealing with video anomalies? Why is it crucial to act quickly when a video anomaly is detected? How can I improve the efficiency of my video anomaly detection process? How much data is needed to accurately detect anomalies in videos? Who can help me fine-tune my video anomaly detection system? What are the key factors to consider when analyzing video anomalies? When should I update my video anomaly detection system? Where can I find the latest research on video anomaly detection techniques? Why is it necessary to have a video anomaly detection system in place? How can I minimize false alarms in my video anomaly detection system? How much does it cost to maintain a video anomaly detection system? Who can I consult if I encounter difficulties with my video anomaly detection system? What are the best practices for dealing with video anomalies? When is it appropriate to involve law enforcement in a video anomaly case? Where can I find a community of professionals who specialize in video anomaly detection? Why do some video anomalies require immediate attention? How can I enhance the performance of my video anomaly detection system? 71.72.73.74.75.76.77.78.79.80. How much should I invest in a video anomaly detection system? Who can provide training on how to detect and analyze video anomalies? What are the most effective methods for detecting anomalies in videos? When should I seek external help for a video anomaly case? Where can I find a comprehensive database of video anomalies? Why is it important to continuously monitor videos for anomalies? How can I validate the results of my video anomaly detection system? How much influence do external factors have on video anomalies? Who can I reach out to for assistance with a complex video anomaly case? 81.82.83.84.85.86.87.88.89.90. What are the main challenges in detecting and analyzing video anomalies? When is it necessary to involve other stakeholders in a video anomaly case? Where can I find case studies on successful video anomaly detection projects? Why is it essential to have a systematic approach to video anomaly detection? How can I optimize my video anomaly detection system for different scenarios? How much storage is needed to archive video anomalies for future analysis? Who should be held accountable for undetected video anomalies? What are the most common reasons for video anomalies to occur? When should I reevaluate my video anomaly detection system? Where can I find information on the latest video anomaly detection technologies? Why is it beneficial to collaborate with others in the field of video anomaly detection? How can I ensure the confidentiality of video anomaly cases? ", "page_idx": 17}, {"type": "table", "img_path": "vBKoEZ1PG3/tmp/1b84abcc2f3385d7ef3639cef1871e08115741d15cfbcf03ae8e1e0ba09cb033.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Demonstrating that Our model outperforms GPT-4-based Data Generation. To demonstrate that our model has better detection capabilities than GPT-4, we compared our model\u2019s results with the unchecked labels generated by GPT-4 on the same testset, as shown in the following Table 6. ", "page_idx": 18}, {"type": "table", "img_path": "vBKoEZ1PG3/tmp/1e172d1d21e51ff1c8174eb00d84c104665954509a5d517746ead63a47b4e9a3.jpg", "table_caption": ["Table 6: Comparison with GPT-based Data Engineering Pipeline "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Clearly, the results indicate that our model can better assist in understanding video anomalies compared to GPT-4, achieving state-of-the-art (SOTA) performance in both text-level and GPTguided evaluations. ", "page_idx": 18}, {"type": "text", "text": "E Efficiency of Gunnar Farneback\u2019s Algorithm for Motion Modality Extraction ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Firstly, Gunnar Farneback\u2019s algorithm demonstrates remarkable efficiency in generating video optical flows\u2014even on CPU platforms. For each frame, the efficiency of this algorithm surpasses that of other widely deployed methods, as illustrated in the table below: ", "page_idx": 18}, {"type": "table", "img_path": "vBKoEZ1PG3/tmp/4f89883aa66d2e87a0f0d182b8f6080bd4588884341c4d8bfbcf4ec3f9887907.jpg", "table_caption": ["Table 7: Performance Comparison of Different Optical Flow Methods. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Secondly, for processing one video, the motion of multiple rounds of dialogue necessitates just a single iteration of video motion extraction. Consequently, the response time for processing one video (about 0.72 seconds) is significantly shorter than the time required to generate a single round of dialogue (about 1.5 seconds). This cost is deemed acceptable for a practical anomaly detection system for users. ", "page_idx": 18}, {"type": "text", "text": "Certainly, we concur that future research into more efficient methods for optical flow extraction, including end-to-end optical flow extraction strategies, will likely further augment the efficiency of our system. ", "page_idx": 18}, {"type": "text", "text": "F Details in GPT-Guided Metrics ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In the GPT-Guided metrics, we employ GPT-4 as an auxiliary tool to evaluate the generated response of HAWK. Our evaluation focuses on three primary dimensions: Reasonability, Detail, and Consistency. ", "page_idx": 18}, {"type": "text", "text": "We first set the system prompt as follows: Initially, we establish the system prompt as shown below: ", "page_idx": 18}, {"type": "text", "text": "{\"role\": \"system\", \"content\":   \n\"You are an intelligent chatbot designed for evaluating the generative $\\hookrightarrow$ outputs for video-based pairs. you will be given two answers, one $\\hookrightarrow$ reference ground truth and one our generated, but this does not mean $\\hookrightarrow$ that the reference GT is the only answer. Your task is to give the $\\hookrightarrow$ score of the predicted answers.\"} ", "page_idx": 18}, {"type": "text", "text": "Our system prompt is designed to compare the degree of matching between image pairs. However, this does not imply fine-grained matching at the text level. Instead, it emphasizes the semantic information-related aspects. ", "page_idx": 19}, {"type": "text", "text": "To assess a particular dimension of the metric, we employ the following prompt: ", "page_idx": 19}, {"type": "text", "text": "{\"role\": \"user\", \"content\":   \n\"### Video Description Generation   \nPlease evaluate the following video-based video description pair:   \nReference: <DESCRIPTION_GT>   \nOurs: <DESCRIPTION_Ours>   \n### Video Question-Answering   \nPlease evaluate the following video-based video question-answer pair:   \nQuestion: <QUESTION>   \nReference: <ANSWER_GT>   \nOurs: <ANSWER_Ours> ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "Provide your evaluation only as a <Reasonability|Detail|Consistency> score $\\hookrightarrow$ where the <Reasonability|Detail|Consistency> score is a FLOAT value $\\hookrightarrow$ between 0 and 1, with 1 indicating the highest level of $\\hookrightarrow$ <Reasonability|Detail|Consistency>. Please generate the response in the $\\hookrightarrow$ form of a Python dictionary string with key 'score', where its value is $\\hookrightarrow$ the <Reasonability|Detail|Consistency> score in FLOAT, not STRING. DO $\\hookrightarrow$ NOT PROVIDE ANY OTHER OUTPUT TEXT OR EXPLANATION. Only provide the $\\hookrightarrow$ Python dictionary string. For example, your response should look like $\\hookrightarrow$ this: {'score': 0.675}.\"} ", "page_idx": 19}, {"type": "text", "text": "We have developed distinct prompts for two tasks: Video Description Generation and Video QuestionAnswering. The primary difference is the addition of the <QUESTION> field in Video QuestionAnswering, which indicates what kind of question the model should answer. <DESCRIPTION_GT> and <DESCRIPTION_Ours> represent the Ground Truth and our generated video description, respectively. Similarly, <ANSWER_GT> and <ANSWER_Ours> signify the Ground Truth and our generated video answers, respectively. $<$ Reasonability | Detail | Consistency $>$ represents the three dimensions we aim to evaluate. Lastly, besides the essential reminders, we have constrained GPT\u2019s output format to {\u2018score\u2019: 0.675}. ", "page_idx": 19}, {"type": "text", "text": "G More Results ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Table (A), (B), (C), (D), (E), and (F) below present additional qualitative results from different datasets. In the tables, red texts indicate key semantic inconsistencies with the Ground Truth, while green texts signify that the generated results closely align with the Ground Truth. ", "page_idx": 19}, {"type": "text", "text": "H Open-World Video Anomaly Understanding Demo ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We present a demo showcasing the use of HAWK in an open-world scenario, using XD-Violence [41] (which is not included in our dataset). The practical capability of the system in an unknown scenario in the open world is depicted in Fig.7 and Fig.8. Furthermore, HAWK can provide accurate answers to users\u2019 questions and engage in long dialogues in the open world. ", "page_idx": 19}, {"type": "text", "text": "I Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Hallucination Although most of the hallucinations can be decreased through motion, some error motion may still also cause hallucinations. Future work may need to consider the connection between the hallucination and the abnormal region more precisely. ", "page_idx": 19}, {"type": "text", "text": "Background Information Leveraging background information can provide a robust prior for understanding video anomalies. We will try to integrate information related to scenes, backgrounds, and objects in a large-model-based video anomaly understanding model. ", "page_idx": 20}, {"type": "text", "text": "Video-level v.s. Streaming Data The goal of this paper is video-level video anomaly understanding. However, for a video anomaly detection system, anomaly detection in streaming is essential, so to increase the practical application ability, we need to design a more practical system for streaming data. ", "page_idx": 20}, {"type": "text", "text": "Data Limitations While our dataset includes multiple anomaly scenarios and our framework is designed for an open-world setting, the limitations of our data make it difficult to fully support open-world scenarios. This is a significant drawback of our study. To address this limitation, we recommend building larger and more diverse open datasets. ", "page_idx": 20}, {"type": "text", "text": "Keyframes First, our current methodology for dataset construction involves sampling the video at consistent one-second intervals. This technique is strategically chosen to ensure that all possible anomalies within the videos are comprehensively captured (Some anomalies only happened in 1-2 seconds), thereby significantly reducing the likelihood of missing critical accidents. While we realize that this may lead to a degree of caption redundancy, we still prioritize the facilitation of thorough annotation to ensure that all anomalies are detected. ", "page_idx": 20}, {"type": "text", "text": "In addition, we have leveraged the capabilities of GPT-4 for generating captions, especially for anomalous events. Due to GPT-4\u2019s advanced text generation and summarization abilities, it serves as an effective tool in minimizing redundancy, ensuring that the extracted captions are both high-quality and succinct. ", "page_idx": 20}, {"type": "text", "text": "After the initial processing with GPT-4, we also undertake a manual checking process. This step is crucial for further reducing any residual redundancy and correcting possible errors within the captions, thereby ensuring the quality and accuracy of our dataset. ", "page_idx": 20}, {"type": "text", "text": "Certainly, we agree that the utility of keyframes is an effective strategy, especially for much longer videos, and believe its potential to significantly enhance data annotation efficiency. This will be the future work. ", "page_idx": 20}, {"type": "text", "text": "J Future Work ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we discuss potential avenues for future research to build upon this paper. ", "page_idx": 20}, {"type": "text", "text": "Expanding Applicability One possible direction for future work is to expand the applicability of the model to a wider range of scenarios (like most recent research by Du et al. [8]). While our current dataset includes multiple anomaly scenarios, there are still many diverse scenarios the model may not handle. We could explore ways to incorporate additional data sources or create new datasets that cover a broader range of scenarios. Additionally, we could investigate ways to deploy the model in cloud, edge, and hybrid computing environments to support more diverse deployment scenarios. ", "page_idx": 20}, {"type": "text", "text": "Enriching Task Capabilities While our current model is able to detect anomalies in videos, it does not provide location information where the anomaly occurs. This information could be valuable for certain applications, such as security monitoring or surveillance. Therefore, a valuable direction for future work would be to investigate ways to improve the model\u2019s ability to localize anomalies. By improving the model\u2019s ability to locate the scene of an anomaly, we could further enhance its usability and expand its range of applications. ", "page_idx": 20}, {"type": "table", "img_path": "vBKoEZ1PG3/tmp/7900ebd2b297d4242fecb50153af35487f60efa45107cef0f9f5d526dd540110.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "vBKoEZ1PG3/tmp/1401d13180f037b240b4b519097ee5f5821556e78bd156921ea4c93b6e39f0f2.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "vBKoEZ1PG3/tmp/a0c6f7c45ba2f34ebd74e396c5b7365d1bcdaee32469d63b777ce7522d931892.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "vBKoEZ1PG3/tmp/a88073e650eb1a46c521f46ab9cd58b0a0361e81e7496bca22feacebb1ba8393.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "vBKoEZ1PG3/tmp/13765cd542d8ff37d6f0103effbbec49da41103d1bf15326a80bec966ea83569.jpg", "table_caption": [], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "vBKoEZ1PG3/tmp/276cb62df9a0b43465a385637507483df7d79e3ffc079f4d53f3db9fd24643f1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "HAWK ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Of course, I'd be happy to help you understand a video! Can you please provide more information about the video, such as a description, the context, or a specific part you're trying to understand? Additionally, please let me know what kind of help you need \u2013 is it an analysis of the content, a breakdown of the video's meaning, or something else? ", "page_idx": 27}, {"type": "text", "text": "What anomaly can you identify in the video? ", "text_level": 1, "page_idx": 27}, {"type": "image", "img_path": "vBKoEZ1PG3/tmp/7d5a0d0f88e1247c1b5303479a7a48b76aca5433cec7b9dffd80f5fba1a67308.jpg", "img_caption": [], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Based on the description you provided, there are several anomalies that can be identified in the video:   \n1. Car fire: The most obvious anomaly is the car fire that occurs near the end of the video. A car catching fire without any prior indication of a malfunction or mechanical issue is unusual and potentially dangerous.   \n2. Lights flickering: The lights on the car and the building nearby flicker, which could be indicative of an electrical issue or a malfunction in the camera equipment.   \n3. \u2026\u2026. ", "page_idx": 27}, {"type": "text", "text": "How can we solve this problem? ", "page_idx": 27}, {"type": "text", "text": "To solve the problem of the car fire and the sudden stop in the middle of the road, it's important to gather more information and context. Here are some steps that can be taken: ", "page_idx": 27}, {"type": "image", "img_path": "vBKoEZ1PG3/tmp/d16c0b501f6751d18ac5b439395c2de2a7166c1715f3c8270379741ecac3d027.jpg", "img_caption": ["Figure 8: Demo on Open-World Video Anomaly Understanding. (Part 2) "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: The abstract and title accurately reflect the content of the article. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 29}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: We discuss the paper\u2019s limitations in the Appendix ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 29}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: Our paper does not require mathematical theory proof. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 30}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: We will release all our code, data, and demo and most of the key information is included in the paper. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 30}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We will make the data and code publicly available, but to prevent misuse, our data is subject to requests from eligible research institutions. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 31}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We present most of the key parameters in the paper, and the remaining details will be open-sourced. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 31}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: Our paper does not include the results of significance statistics, we used a given random seed to maintain a random initialization, and the results are fixed. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Details in Appendix. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 32}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: Details in Appendix. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 32}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Details in Appendix. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: Details in Appendix. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 33}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We strictly follow the principle of open data. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 33}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 34}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: Details in Appendix. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 34}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: These are not included in our research. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 34}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: These are not included in our research. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 34}]