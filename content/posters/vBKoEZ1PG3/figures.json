[{"figure_path": "vBKoEZ1PG3/figures/figures_1_1.jpg", "caption": "Figure 1: Different framework in video anomaly detection. (A) shows traditional video anomaly detection methods, which use binary classifiers to detect anomalies. (B), following (A), introduces a multi-class classifier for integrating semantic information, allowing users to obtain different types of anomaly information. Neither (A) nor (B) can interact with users. (C) is a previous video understanding framework that can interactively provide richer semantic information for users, but cannot specifically locate video anomalies. Our framework (D) enhances the anomaly understanding capability and provides annotated labels with rich semantic information.", "description": "This figure illustrates the evolution of video anomaly detection frameworks. (A) shows a traditional approach using a simple binary classifier. (B) improves upon this by incorporating semantic information through a multi-class classifier, still lacking user interaction. (C) demonstrates a video understanding framework that offers interactive semantic information but cannot pinpoint anomalies.  Finally, (D) presents the proposed HAWK framework, which combines motion and visual information, enabling precise anomaly identification and interaction with users via rich semantic information and annotated labels.", "section": "1 Introduction"}, {"figure_path": "vBKoEZ1PG3/figures/figures_3_1.jpg", "caption": "Figure 2: Generation pipeline of our dataset. In the first line, we first segment videos into clips and generate dense captions for each segment, including a comprehensive description of the video content. Then, we use GPT-4 to guide the generation of corresponding anomalous video descriptions based on these descriptions, which are then manually checked to reduce mistakes. In the second line, to generate user-centered QA pairs, we first use GPT-4 to generate open-ended questions based on the proposed two principles. Then, the questions and video descriptions are jointly input into GPT-4 to provide possible answers.", "description": "This figure illustrates the two-stage process used to create the dataset.  The first stage focuses on generating detailed descriptions of video anomalies.  Videos are segmented, dense captions are created for each segment using perception tools, and then GPT-4 is used to generate comprehensive anomaly descriptions based on these captions. These descriptions undergo manual checking for accuracy. The second stage involves creating question-answer pairs for user interaction.  GPT-4 generates open-ended questions based on the video descriptions and the 5W2H and Anomaly-Related principles.  Finally, GPT-4 generates answers based on both the questions and video descriptions. This process results in a dataset rich in descriptive information and question-answer pairs for various anomaly scenarios.", "section": "3 Data Engineering"}, {"figure_path": "vBKoEZ1PG3/figures/figures_5_1.jpg", "caption": "Figure 3: Overview of HAWK. During training (Black and Gray path), we aim to optimize for video-language matching loss, along with Video-Motion Consistency and Motion-Language Matching. During inference (only Gray path), we generate language descriptions using video, motion, and text.", "description": "This figure illustrates the architecture of HAWK, a dual-branch framework for video anomaly understanding. It integrates both appearance (video) and motion modalities.  The video branch processes video frames to extract appearance features, while the motion branch extracts motion features. Both branches use pre-trained video encoders (EVA-CLIP and Video Q-Former).  A consistency loss in a tight space enforces agreement between the video and motion modalities.  Motion-related language is extracted from the descriptions to supervise motion interpretation. Finally, a large language model (LLaMA-2) integrates the visual (video and motion) and textual embeddings to generate language descriptions or answer questions.", "section": "4 Methodology"}, {"figure_path": "vBKoEZ1PG3/figures/figures_5_2.jpg", "caption": "Figure 3: Overview of HAWK. During training (Black and Gray path), we aim to optimize for video-language matching loss, along with Video-Motion Consistency and Motion-Language Matching. During inference (only Gray path), we generate language descriptions using video, motion, and text.", "description": "This figure illustrates the architecture of the HAWK model, highlighting the dual-branch design for video and motion understanding. It shows how both modalities are integrated with a large language model (LLaMA-2) for video description generation. The training process involves optimizing three loss functions: video-language matching loss, video-motion consistency loss, and motion-language matching loss.  The inference process uses the combined video and motion features to generate descriptions, enabling fine-grained anomaly interpretation.", "section": "4 Methodology"}, {"figure_path": "vBKoEZ1PG3/figures/figures_6_1.jpg", "caption": "Figure 5: Training & Testing.", "description": "This figure illustrates the three-stage training and testing process employed in the paper.  Stage 1 involves pre-training on the WebVid dataset [3] to acquire general video understanding.  Stage 2 fine-tunes the model on a curated anomaly video dataset for video anomaly understanding, using both video description generation and question-answering tasks, with joint training. Stage 3 involves testing the model independently on the two tasks (video description and question-answering) on a held-out 10% of the dataset.", "section": "5 Experiments"}, {"figure_path": "vBKoEZ1PG3/figures/figures_15_1.jpg", "caption": "Figure 3: Overview of HAWK. During training (Black and Gray path), we aim to optimize for video-language matching loss, along with Video-Motion Consistency and Motion-Language Matching. During inference (only Gray path), we generate language descriptions using video, motion, and text.", "description": "This figure shows the architecture of HAWK, a dual-branch framework that integrates motion modality.  The black and gray paths represent the training process, which involves optimizing for video-language matching, video-motion consistency, and motion-language matching losses. The gray path alone represents the inference process, where the model generates language descriptions based on video and motion inputs.  The figure highlights the key components of the framework including video encoders, motion extractors, projection layers, LLaMa-2 language model and loss functions.", "section": "4 Methodology"}, {"figure_path": "vBKoEZ1PG3/figures/figures_16_1.jpg", "caption": "Figure 2: Generation pipeline of our dataset. In the first line, we first segment videos into clips and generate dense captions for each segment, including a comprehensive description of the video content. Then, we use GPT-4 to guide the generation of corresponding anomalous video descriptions based on these descriptions, which are then manually checked to reduce mistakes. In the second line, to generate user-centered QA pairs, we first use GPT-4 to generate open-ended questions based on the proposed two principles. Then, the questions and video descriptions are jointly input into GPT-4 to provide possible answers.", "description": "This figure illustrates the process of creating the dataset used in the HAWK model.  The top half shows how dense captions are generated for video clips, followed by GPT-4 generating more detailed descriptions of anomalies, and then manual checking for accuracy. The bottom half details the generation of question-answer pairs using GPT-4 based on the video descriptions and two principles (5W2H and Anomaly-Related).  These pairs aim to simulate real-world user inquiries about video anomalies.", "section": "3 Data Engineering"}, {"figure_path": "vBKoEZ1PG3/figures/figures_27_1.jpg", "caption": "Figure 7: Demo on Open-World Video Anomaly Understanding. (Part 1)", "description": "This figure shows a demonstration of HAWK's capabilities in understanding open-world video anomalies.  The user provides a video showing a car fire and asks HAWK to identify the anomaly and suggest solutions.  HAWK correctly identifies the car fire as an anomaly and proposes steps for investigation, including checking for mechanical issues, looking for external factors, consulting experts, and conducting a thorough investigation.  The interaction highlights HAWK's ability to interpret complex real-world scenarios and provide actionable insights.", "section": "Open-World Anomaly Understanding Demo"}, {"figure_path": "vBKoEZ1PG3/figures/figures_28_1.jpg", "caption": "Figure 3: Overview of HAWK. During training (Black and Gray path), we aim to optimize for video-language matching loss, along with Video-Motion Consistency and Motion-Language Matching. During inference (only Gray path), we generate language descriptions using video, motion, and text.", "description": "This figure shows the architecture of HAWK, a dual-branch framework for understanding video anomalies. It explicitly integrates motion modality using two branches: one for processing video appearance features and one for motion features.  Both branches share the same architecture but use separate parameters. The video and motion embeddings are then projected into a common language feature space and combined with text tokens (prompt) before being fed into a large language model (LLaMA-2).  During training, video-language matching loss, video-motion consistency loss, and motion-language matching loss are used to optimize the model. During inference, only the gray path is used, generating language descriptions based on video and motion features as well as text prompts.", "section": "4 Methodology"}]