[{"type": "text", "text": "A Surprisingly Simple Approach to Generalized Few-Shot Semantic Segmentation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Tomoya Sakai Haoxiang Qiu Takayuki Katsuki IBM Research - Tokyo IBM Research - Tokyo IBM Research - Tokyo tomoya.sakai2@ibm.com haoxiang.qiu@ibm.com kats@jp.ibm.com ", "page_idx": 0}, {"type": "text", "text": "Daiki Kimura IBM Research - Tokyo daiki@jp.ibm.com ", "page_idx": 0}, {"type": "text", "text": "Takayuki Osogami IBM Research - Tokyo osogami@jp.ibm.com ", "page_idx": 0}, {"type": "text", "text": "Tadanobu Inoue IBM inouet@jp.ibm.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The goal of generalized few-shot semantic segmentation (GFSS) is to recognize novel-class objects through training with a few annotated examples and the baseclass model that learned the knowledge about the base classes. Unlike the classic few-shot semantic segmentation, GFSS aims to classify pixels into both base and novel classes, meaning it is a more practical setting. Current GFSS methods rely on several techniques such as using combinations of customized modules, carefully designed loss functions, meta-learning, and transductive learning. However, we found that a simple rule and standard supervised learning substantially improve the GFSS performance. In this paper, we propose a simple yet effective method for GFSS that does not use the techniques mentioned above. Also, we theoretically show that our method perfectly maintains the segmentation performance of the base-class model over most of the base classes. Through numerical experiments, we demonstrated the effectiveness of our method. It improved in novel-class segmentation performance in the 1-shot scenario by $6.1\\bar{\\%}$ on the PASCAL- $.5^{i}$ dataset, $4.7\\%$ on the PASCAL- $\\cdot10^{i}$ dataset, and $1.0\\%$ on the $\\mathrm{COCO-20^{i}}$ dataset. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Semantic segmentation is a vital task in various visual understanding systems, and the goal is to obtain pixel-level semantic categories [1]. Recent developments in convolutional neural networks [2] and vision transformers [3] have pushed the limits of semantic segmentation. With a large amount of annotated images, we can obtain an accurate model that can recognize objects in the training data. In real-world applications, however, the learned model will encounter novel-class objects that are not classified in base classes, i.e., classes that are not annotated in the training data. ", "page_idx": 0}, {"type": "text", "text": "To solve this problem, few-shot semantic segmentation (FSS) aims to recognize novel-class objects with a few annotated images while using the learned model, which has knowledge about the baseclass information. Although various FSS methods have been proposed [4\u201315], FSS only handles novel-class object recognition, which restricts its applicability since base classes will still appear at inference in practice. ", "page_idx": 0}, {"type": "text", "text": "Generalized FSS (GFSS) aims to recognize both base- and novel-class objects [16] and is regarded as a more practical setting than FSS. Current GFSS methods rely on several techniques such as using combinations of customized modules [16\u201319], carefully designed loss functions [20], meta-learning [16, 18, 19], and transductive learning [20]. These techniques improved GFSS performance at the cost of implementation and computation time. For example, such a customized module is not always ", "page_idx": 0}, {"type": "image", "img_path": "p3nPHMpx04/tmp/8819be12584eb4a78b310b4cdb2d5bcb15db6753ce00d8ad765f6b7a91558b52.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Illustration of base-class mining (BCM) with three novel classes: \u201ctrain\u201d, \u201ctv\u201d, and \u201ccouch\u201d. Base-class model first outputs prediction. If prediction is one of chosen base classes, corresponding model outputs prediction. Otherwise, prediction of baseclass model is used as it is. Simple rule finds which base class is related to novel classes. Models for novel classes are trained by standard supervised learning. ", "page_idx": 1}, {"type": "text", "text": "supported in the target framework, methods based on meta-learning require several hours to train customized modules, and transductive learning optimizes models during inference, which is not suitable for applications that require quick responses. However, we found that a simple rule and standard supervised learning improve GFSS performance. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we propose a simple yet effective GFSS method that does not use the above techniques. As illustrated in Fig. 1, our idea is mining base classes closely related to classifying novel classes. We thus refer to our method as base-class mining (BCM). Surprisingly, BCM perfectly maintains the segmentation performance of the base-class model over a subset of base classes. Since maintaining the segmentation performance of the base-class model is critical in GFSS, BCM will be beneficial, especially when a performance difference from the base-class model confuses users. ", "page_idx": 1}, {"type": "text", "text": "Our contributions are summarized as follows. ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose a simple yet effective GFSS method based on a simple rule and well-known supervised learning techniques, which can be regarded as a strong alternate baseline without transductive learning.   \n\u2022 We theoretically show that the performance of the base-class model for a subset of base classes is perfectly maintained, which is the first theoretical finding about base-class segmentation performances in GFSS, to the best of our knowledge.   \n\u2022 We demonstrated the effectiveness of BCM on the PASCAL- $.5^{i}$ , PASCAL- $\\cdot10^{i}$ , and $\\mathrm{COCO-20^{i}}$ datasets. BCM substantially improved novel-class segmentation performance in the 1-shot scenario by $6.1\\%$ on PASCAL- $\\cdot5^{i}$ and $4.7\\%$ on PASCAL- $10^{i}$ . ", "page_idx": 1}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 GFSS setting ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In GFSS [16], multiple novel classes need to be classified, i.e., multi-class classification, in addition to classifying base classes. This differs from the FSS setting of single novel-class classification, i.e., binary classification. ", "page_idx": 1}, {"type": "text", "text": "We consider the practical GFSS setting [20], in which the available resources are few-shot annotated images for novel classes and the base-class model trained using standard learning methods. The existing setting [16] requires annotated base-class samples for training the base-class model and tuning a GFSS model that can recognize both base and novel classes. For example, the number of base classes is 60 (excluding the background) on $\\mathrm{COCO-20^{i}}$ case, meaning that we need to collect 300 annotated images for tuning in the 5-shot scenario, other than training the base-class model. Such additional samples for base classes are not necessary in the practical GFSS setting, resulting in using those samples for training the base-class model, which is an advantage of the practical GFSS setting. ", "page_idx": 1}, {"type": "text", "text": "2.2 GFSS methods ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "The major challenges with current GFSS methods are i) attaining better recognition performance for novel classes and ii) maintaining the segmentation performance of the base-class model. Contextaware prototype learning (CAPL) [16] enhances prototypes with a few annotated images and employs a balancing mechanism of prototypes for base and novel classes. Base and meta (BAM) [14, 18] designed customized modules to learn knowledge from few-shot data and combined predictions of both base and novel classes on the basis of thresholds. POP [17] and PCN [19] use similar approaches. Distilled information maximization (DIaM) [20] does not depend on customized modules, instead, uses the information maximization principle [21] and designs a loss function on the basis of knowledge distillation [22] to preserve base-class knowledge. DIaM uses the transductive learning approach [23], which is not suitable for applications that require quick responses. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Compared with the above methods, BCM does not rely on carefully customized models, various combinations of loss functions, or transductive learning. ", "page_idx": 2}, {"type": "text", "text": "2.3 Continual semantic segmentation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The GFSS setting relates to another emergence problem setting known as continual semantic segmentation (CSS) [24\u201326] in which the new classes appear in a continual learning manner. The CSS setting is reduced to the GFSS setting if several novel classes appear with a few annotations in a single step. Coincidentally, a previous study [26] empirically found a phenomenon similar to our idea that a novel class is classified as a base class, through qualitative analyses of their method. However, their method was not designed for the few-shot learning setting, meaning that how to improve GFSS performances is unclear. In contrast to their findings, BCM explicitly integrates the idea of the relation between base and novel classes with the architecture. These differences in problem settings and architectures differentiate CSS-based methods from BCM. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Problem settings ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We consider the following practical GFSS setting [20]. ", "page_idx": 2}, {"type": "text", "text": "Let $\\pmb{X}\\,\\in\\,\\mathbb{R}^{H\\times W\\times3}$ denote an RGB image of height $H$ and width $W$ , and $Y\\,\\in\\,\\up y^{H\\times W}$ be its corresponding segmentation map, where $\\mathcal{\\bar{P}}\\subset\\{0,\\bar{1},2,3,\\ldots\\}$ is a set of object classes. Let $[\\,\\cdot\\,]_{j}$ indicate the $j$ -th element of a matrix, where $j\\in\\{1,\\cdot\\cdot\\cdot,H W\\}$ . If $[\\pmb{Y}]_{j}=y$ , the object $y$ exists at the $j$ -th pixel. ", "page_idx": 2}, {"type": "text", "text": "Let ${\\mathcal{V}}_{\\mathrm{b}}$ and ${\\mathcal{V}}_{\\mathrm{n}}$ be the sets of base and novel classes, respectively, such that $y_{\\mathrm{{b}}}\\cap y_{\\mathrm{{n}}}\\,=\\,\\emptyset$ and $\\mathcal{V}_{\\mathrm{all}}=\\mathcal{V}_{\\mathrm{b}}\\cup\\mathcal{V}_{\\mathrm{n}}$ . We use the class $\\acute{\\bullet}$ for background, which is often the case with implementation. For the sake of simplicity, we include the background into $\\mathcal{V}_{\\mathrm{b}}$ , e.g., $\\mathcal{V}_{\\mathrm{b}}=\\{0,1,2,3\\}$ and $\\mathcal{V}_{\\mathrm{n}}=\\{4,5\\}$ . ", "page_idx": 2}, {"type": "text", "text": "We have a learned base-class model $\\widehat{g}_{\\mathrm{b}}$ , which is trained with a large amount of annotated images by using standard semantic-segmentation methods [1]. Given $X,{\\widehat{g}}_{\\mathrm{b}}$ returns a base-class segmentation map $\\widehat{Y}_{\\mathrm{b}}\\in\\mathcal{Y}_{\\mathrm{b}}^{H\\times W}$ . Similarly to the practical GFSS setting [2 0 ], we do not assume the customized archi tecture for $\\widehat{g}_{\\mathrm{b}}$ , enabling us to easily use cutting-edge foundation models [27]. ", "page_idx": 2}, {"type": "text", "text": "A $K$ -shot dataset contains $K$ examples with its ground-truth mask for each novel class $y\\in\\mathcal{V}_{\\mathrm{n}}$ , e.g., if $K=5$ and $|\\mathcal{D}_{\\mathrm{n}}|=5$ , we have 25 annotated images. Note that $K$ examples for base classes are not necessary, as discussed in Sec. 2.1. ", "page_idx": 2}, {"type": "text", "text": "Our goal is to obtain t mentation map $\\widehat{Y}_{\\mathrm{BCM}}\\in\\mathcal{Y}_{\\mathrm{all}}^{H\\times W}$ computed using the prediction model $\\widehat{g}_{\\mathrm{BCM}}$ ", "page_idx": 2}, {"type": "text", "text": "3.2 Evaluation metric ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The mean intersection-over-union (mIoU) is widely used in reporting the performance of segmentation methods [1]. Let us first define the IoU for a class $y^{\\prime}\\in\\mathcal{V}$ as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{loU}_{y^{\\prime}}(\\pmb{Y},\\widehat{\\pmb{Y}}):=\\frac{\\sum_{j=1}^{H W}\\mathrm{I}_{y^{\\prime}}([\\pmb{Y}]_{j},[\\widehat{\\pmb{Y}}]_{j})}{\\sum_{j=1}^{H W}\\mathrm{U}_{y^{\\prime}}([\\pmb{Y}]_{j},[\\widehat{\\pmb{Y}}]_{j})},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\operatorname{I}_{y^{\\prime}}(y,\\widehat{y}):=\\mathbb{I}[y=y^{\\prime}]\\cdot\\mathbb{I}[\\widehat{y}=y^{\\prime}]$ , $\\operatorname{U}_{y^{\\prime}}(y,\\widehat{y}):=\\mathbb{I}[y=y^{\\prime}]+\\mathbb{I}[\\widehat{y}=y^{\\prime}]-\\mathbb{I}[y=y^{\\prime}]\\cdot\\mathbb{I}[\\widehat{y}=y^{\\prime}]$ , and $\\mathbb{I}[\\mathrm{cond}]$ is the indicator function taking 1 if cond is true, 0 otherwise. Here, we consider a single sample for evaluation, but it can be easily extended to multiple samples by adding the summation over ", "page_idx": 2}, {"type": "image", "img_path": "p3nPHMpx04/tmp/0848cdb507c1f29787c6e6e309dc7e0fada2febaeb9e33f3c8af5306b573ccf8.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 2: Illustration of BNM creation. For illustration purpose, image of size $3\\times3$ is used. (a) Count co-occurrences, i.e., (base class, novel class) pairs. There are three $(0,4)$ and one $(2,4)$ co-occurrences. (b) Aggregate co-occurrence counts for all samples and create co-occurrence count table. (c) Create BNM from co-occurrence count table, where top-1 strategy finds base class with largest co-occurrences (shaded cell in Fig. 2b) for each novel class. ", "page_idx": 3}, {"type": "text", "text": "samples to the numerator and denominator in Eq. (1), respectively. Finally, the mIoU is computed by ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname{mIoU}_{\\mathcal{Y}}(\\pmb{Y},\\widehat{\\pmb{Y}}):=\\frac{1}{|\\mathcal{Y}|}\\sum_{y^{\\prime}\\in\\mathcal{Y}}\\operatorname{IoU}_{y^{\\prime}}(\\pmb{Y},\\widehat{\\pmb{Y}}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "For example, if $\\boldsymbol{\\wp}$ is ${\\mathcal{V}}_{\\mathrm{n}}$ , it will be the mIoU over novel classes. ", "page_idx": 3}, {"type": "text", "text": "4 Proposed method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We now present BCM. ", "page_idx": 3}, {"type": "text", "text": "4.1 Training ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Training is divided into two steps: 1) finding the relationship between base and novel classes, and 2) training models for classifying novel classes for each chosen base class. ", "page_idx": 3}, {"type": "text", "text": "Step 1. We input $\\mathbf{\\deltaX}$ into $\\widehat{g}_{\\mathrm{b}}$ and obtain $\\widehat{Y}_{\\mathrm{~b~}}$ . For each pixel of the annotated object, we compare $\\widehat{Y}_{\\mathrm{~b~}}$ and $\\mathbf{Y}$ and record co-occurrences of base and novel classes. We then count the co-occurrences, find the top- $s$ co-occurred base class for each novel class, referred to as the top-s strategy, and obtain chosen base classes denoted as $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ . Finally, we construct the mapping from a base class to novel classes, called base-novel mapping (BNM). Figure 2 illustrates the creation of BNM with the top-1 strategy from the 1-shot dataset. ", "page_idx": 3}, {"type": "text", "text": "Step 2. For each chosen base class $\\beta\\in\\mathcal B$ , we train a model $g_{\\beta}$ with the modified $K$ -shot dataset where labels are converted into $\\beta$ if they are novel classes irrelevant to $\\beta$ or the background. Taking the example in Fig. 2c, when $\\beta=1$ , the irrelevant novel-class labels $\\mathbf{\\Omega}^{\\bullet}4^{\\bullet}$ and $\\mathbf{\\zeta}^{\\bullet}\\mathbf{\\zeta}^{\\bullet}$ and the background label $\\acute{\\bullet}0^{\\bullet}$ are replaced with \u20181\u2019. Then, $\\widehat{g}_{\\beta=1}$ returns either $\"1\"$ or $\\mathbf{\\omega}^{\\bullet}6^{\\bullet}$ as the prediction. ", "page_idx": 3}, {"type": "text", "text": "To obtain the learned model $\\widehat{g}_{\\beta}$ , we can use any learning method, such as minimizing the cross-entropy loss or effective losses used   in the previous studies. ", "page_idx": 3}, {"type": "text", "text": "4.2 Inference ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Inference is analogous to training. For a test image $\\mathbf{\\deltaX}$ , we first obtain the base-class prediction $\\widehat{Y}_{\\mathrm{~b~}}$ . For each pixel $j$ , if $[\\widehat{Y}_{\\mathrm{b}}]_{j}=\\beta$ , we then obtain the prediction of the corresponding model $\\widehat{g}_{\\beta}$ and overwrite $[\\widehat{Y}_{\\mathrm{b}}]_{j}$ with the output of $\\widehat{g}_{\\beta}$ . Figure 3 illustrates how we obtain the segmentation map of BCM. We summarize the flow of BCM in Fig. 4. ", "page_idx": 3}, {"type": "text", "text": "4.3 Preventing catastrophic forgetting ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Maintaining the base-class segmentation performance is crucial in GFSS. We theoretically show that BCM perfectly maintains the segmentation performance of most of the base classes without resorting to, e.g., knowledge distillation [22] for training models. ", "page_idx": 3}, {"type": "image", "img_path": "p3nPHMpx04/tmp/f0b44bb2d7580a36f06477fb5f896906e2c528d8e40b0606197da6f3d1253575.jpg", "img_caption": ["Figure 3: Illustration of inference. $\\widehat{Y}_{\\mathrm{~b~}}^{'}$ , $\\widehat{\\boldsymbol{Y}}_{\\beta=0}$ , and $\\widehat{Y}_{\\mathrm{BCM}}$ are predic- Figure 4: Flow of BCM. For each $\\beta\\in\\mathcal{B}$ , BCM finds tions  of $\\widehat{g}_{\\mathrm{b}}$ , $\\widehat{g}_{\\beta=0}$ , and BCM, re- pixels of $\\widehat{Y}_{\\mathrm{~b~}}=\\beta$ and overwrites $\\widehat{Y}_{\\mathrm{~b~}}$ with $\\widehat{Y}_{\\beta}$ . spective ly . "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "We first formulate the prediction of BCM described in Sec. 4.2. Let $\\widehat{y}_{\\mathrm{b}}$ be the prediction of the base-class model at the $j$ -th pixel and ${\\widehat{y}}_{\\beta}$ be the prediction of $\\widehat{g}_{\\beta}$ at the $j$ - th pixel. The prediction of BCM at the $j$ -th pixel, $\\widehat{y}_{\\mathrm{BCM}}$ , is obtai n ed by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{y}_{\\mathrm{BCM}}=\\left\\{\\widehat{y}_{\\mathrm{b}}\\begin{array}{l l}{\\mathrm{~if~}\\widehat{y}_{\\mathrm{b}}\\not\\in\\mathcal{B},}\\\\ {\\widehat{y}_{\\beta=\\widehat{y}_{\\mathrm{b}}}}&{\\mathrm{otherwise.}}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The above prediction mechanism leads to the following proposition: ", "page_idx": 4}, {"type": "text", "text": "Proposition 4.1. Let $\\widehat{Y}_{\\mathrm{~b~}}$ and $\\widehat{Y}_{\\mathrm{BCM}}$ be the predictions of the base-class model and BCM, respectively. The mIoUs of $\\widehat{Y}_{\\mathrm{~b~}}$ and $\\widehat{Y}_{\\mathrm{BCM}}$ over $\\mathcal{V}_{\\mathrm{b}}\\setminus B$ are the same: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{mIoU}_{\\mathcal{Y}_{\\mathrm{b}}\\backslash B}(\\pmb{Y},\\widehat{\\pmb{Y}}_{\\mathrm{b}})=\\mathrm{mIoU}_{\\mathcal{Y}_{\\mathrm{b}}\\backslash B}(\\pmb{Y},\\widehat{\\pmb{Y}}_{\\mathrm{BCM}}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "${\\cal I}f\\left|B\\right|$ is small, the segmentation performance of most of the base classes is perfectly maintained. ", "page_idx": 4}, {"type": "text", "text": "Proof. For any $y^{\\prime}\\notin{\\boldsymbol{B}}$ , if $[\\widehat{Y}_{\\mathrm{b}}]_{j}=y^{\\prime}$ , then $[\\widehat{Y}_{\\mathrm{BCM}}]_{j}=[\\widehat{Y}_{\\mathrm{b}}]_{j}$ by definition of the BCM prediction. Then, for any $y^{\\prime}\\in\\mathcal{Y}_{\\mathrm{b}}\\setminus B$ , ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{IoU}_{y^{\\prime}}(Y,\\widehat{Y}_{\\mathrm{b}})=\\mathrm{IoU}_{y^{\\prime}}(Y,\\widehat{Y}_{\\mathrm{BCM}}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Taking the average of $\\mathrm{IoU}_{y^{\\prime}}$ over $y_{\\mathrm{b}}\\:\\backslash\\:{\\mathcal{B}}$ , we obtain Eq. (4). ", "page_idx": 4}, {"type": "text", "text": "Intuitively, since BCM uses the prediction of the base-class model as it is for a subset of base classes, the mIoU over those base classes is the same as $\\widehat{g}_{\\mathrm{b}}$ . Proposition 4.1 shows that BCM partially prevents catastrophic forgetting [28, 29]. In our experiments, $|\\beta|$ tended to be small, resulting in the mIoU over base classes being almost maintained. ", "page_idx": 4}, {"type": "text", "text": "4.4 Lightweight implementation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Since the size of training data for novel classes is not large in GFSS, training deep neural networks for $\\widehat{g}_{\\beta}$ is impractical. We thus use the base-class model $\\widehat{g}_{\\mathrm{b}}$ as the feature extractor of $\\widehat{g}_{\\beta}$ and train line a r models as the last layer of $\\widehat{g}_{\\beta}$ with the $K$ -shot data . ", "page_idx": 4}, {"type": "text", "text": "To train linear models, we can use off-the-shelf libraries, such as Scikit-learn [30], meaning that training time will be fast, compared with the end-to-end training on GPU. Since the number of background pixels is much larger than that of objects of interest, we applied sampling techniques for imbalanced data [31] to training data, such as under-sampling. ", "page_idx": 4}, {"type": "text", "text": "Regarding the top-s strategy, we used $s=1$ from the performance and computation time viewpoint.   \nThe effect of $s$ in the top- $s$ strategy is discussed in Sec. 5.6. ", "page_idx": 4}, {"type": "text", "text": "4.5 Further performance improvement ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Since our implementation is to train simple linear models, we can easily use various techniques to improve GFSS performance. We explain two effective and easy-to-use techniques used in our experiments as follows. ", "page_idx": 4}, {"type": "text", "text": "Pre-processing. We can use Tukey\u2019s ladder of powers transformation [32], known as the effective transformation in few-shot learning [33]. Specifically, let $\\boldsymbol{\\textbf{\\textit{f}}}$ be the $d$ -dimensional feature vector extracted by $\\widehat{g}_{\\mathrm{b}}$ . Tukey\u2019s ladder of powers transformation is defined as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\tilde{\\pmb{f}}=\\binom{\\pmb{f}^{\\tau}}{\\log\\pmb{f}}\\quad\\mathrm{otherwise},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\tau$ is a hyper-parameter, and the power and logarithm operations are element-wise. When $\\tau=1$ , the original feature is used. In a previous study [33], $\\tau=0.5$ was recommended as the default value; thus, we used it in our experiments. ", "page_idx": 5}, {"type": "text", "text": "These pre-processed feature vectors are used for ${\\widehat{g}}_{\\beta}$ only since the change in feature representation for $\\widehat{g}_{\\mathrm{b}}$ without retraining would downgrade performance. Note that to apply similar pre-processing to the existing methods, it is crucial to take into account the adverse effect on total performance. ", "page_idx": 5}, {"type": "text", "text": "Ensemble learning. We can use ensemble learning [34, 35] to improve GFSS performance. Unlike the existing GFSS methods, the computation time of BCM will be short since training a linear model for $g_{\\beta}$ is lighter than tuning deep neural networks in an end-to-end manner. ", "page_idx": 5}, {"type": "text", "text": "We introduce shot-wise ensemble learning to few-shot learning when $K>1$ . This involves first preparing multiple $L$ -shot datasets $(L\\leq K)$ by drawing samples from the $K$ -shot dataset then aggregating outputs of models trained with the $L$ -shot datasets. In our experiments, we split the 5-shot dataset into five 1-shot datasets and obtained six models by using five 1-shot and one 5-shot datasets. In inference, we computed a weighted average of the outputs of the models. The weights can be determined by, e.g., validation data or pre-defined values. In our experiments, we set one for the model with the 1-shot dataset and five for the model with the 5-shot dataset for simplicity. ", "page_idx": 5}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "5.1 Setup", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Datasets. We used three FSS datasets: PASCAL- $\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\!\\,\\,\\!\\!\\,\\!\\!\\,\\,\\!\\!\\,\\!\\!\\,\\,\\!\\!\\,\\,\\!\\!\\,\\,\\!\\!\\,\\,\\!\\!\\,\\,\\!\\!\\,\\,\\!\\!\\,\\,\\!\\!\\,\\,\\!\\!\\,\\,\\!\\!\\,\\,\\!\\!\\,\\,\\!\\!\\,\\,\\!\\!\\,\\,\\!\\!\\,\\,\\!\\!\\,\\,\\!\\!\\,\\,\\!\\!\\,\\,\\!\\!\\,\\,\\!\\!\\,\\,\\!\\!\\,\\,\\!\\!\\,\\,\\!\\!\\,\\,\\!\\!\\,\\,\\!\\!\\,\\,\\!\\!\\,\\,\\!\\!\\,\\,\\!\\!\\,\\,\\!\\!\\,\\,\\!\\!\\,\\,\\!\\!\\,\\,\\!\\!\\,\\,\\!\\!\\,\\,\\!\\!\\,\\,\\!\\!\\,\\,\\!\\!\\,\\,\\!\\!\\,\\,\\!\\!\\,\\,\\!\\!\\,\\,\\!\\!\\,\\,\\!\\!\\,\\,\\!\\!\\,\\,\\!\\!\\,\\,\\!\\!\\,\\,\\!\\!\\,\\,\\!\\!\\,\\,\\!\\!\\,\\,\\!\\!\\,\\,\\!\\!\\,\\,\\!\\,\\!\\!\\,\\,\\!\\!\\,\\,\\!\\,\\!\\!\\,\\,\\!\\,\\!\\!\\,\\,\\!\\,\\!\\!\\,\\,\\!\\,\\!\\,\\!\\,\\,\\!\\,\\!\\!\\,\\,\\!\\,\\!\\,\\,\\!\\,\\!\\,\\!\\,\\,\\!\\,\\!\\,\\!\\,\\,\\!\\,\\!\\,\\!\\,\\,\\!\\,\\!\\,\\!\\,\\,\\!\\,\\!\\,\\!\\,\\,\\!\\,\\!\\,\\!\\,\\,\\!\\,\\!\\,\\!\\,\\,\\!\\,\\!\\,\\!\\,\\,\\!\\,\\!\\,\\!\\,\\,\\!\\,\\!\\,\\!\\,\\!\\,\\,\\!\\,\\!\\,\\!\\,\\!\\,\\,\\!\\,\\!\\,\\!\\,\\!\\,\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\,\\!\\,\\!\\,\\!\\,\\!\\,\\,\\!\\,\\!\\,\\!\\,\\!\\,\\,\\!\\,\\!\\,\\!\\,\\,\\!\\,\\!\\,\\!\\,\\,\\!\\,\\!\\,\\,\\!\\,\\!\\,\\,\\!\\,\\!\\,\\,\\!\\,\\!\\,\\,\\!\\,\\!\\,\\,\\!\\,\\,\\!\\,\\!\\,\\,\\!\\,\\,\\!\\,\\!\\,\\,\\!\\,\\,\\!\\,\\,\\!\\,\\,\\!\\,\\,\\!\\,\\,\\!\\,\\,\\!\\,\\,\\!\\,\\,\\!\\,\\,\\,\\!\\,\\,\\!\\,\\,\\!\\,\\,\\,\\!\\,\\,\\,\\!\\,\\,\\!\\,\\,\\,\\!\\,\\,\\,\\,\\!\\,\\,\\,\\!\\,\\,\\,\\,\\!\\,\\,\\,\\,\\$ [4, 36, 37], PASCAL- $1\\theta^{i}$ [20, 36, 37], and $C O C O-20^{\\,i}$ [6, 38]. The PASCAL- $10^{i}$ dataset was introduced to investigate the impact of increasing the number of novel classes [20]. ", "page_idx": 5}, {"type": "text", "text": "Methods for comparison. We compared BCM with CAPL [16], BAM [14, 18],1 and DIaM [20]. Note that DIaM was regarded as a simple method since it trains the last linear layer only, similarly to the simple methods [39, 40] proposed for few-shot object detection. ", "page_idx": 5}, {"type": "text", "text": "Evaluation. We report the mIoUs over base and novel classes, referred to as the Base and Novel scores, respectively, where the background was not included in the Base score, similarly to a previous study [20]. We also report the average of the Base and Novel scores, called the Mean score. All reported scores are the average of five independent trials. ", "page_idx": 5}, {"type": "text", "text": "Base-class model. We used the publicly available pre-trained model for GFSS,2 pyramid scene parsing network (PSPNet) [41] with the pre-trained ResNet-50 backbone [2]. It was trained with labeled data for base classes by using the stochastic gradient descent optimizer with an initial learning rate of $2.5\\times10^{-4}$ , momentum of 0.9, and weight decay of $10^{-4}$ . The batch size was 12, and number of epochs was 20 for COCO- $20^{i}$ and 100 for PASCAL- $\\cdot5^{i}$ and PASCAL- $\\cdot10^{i}$ . ", "page_idx": 5}, {"type": "text", "text": "Detailed implementation. The implementation of BCM is based on the publicly available DIaM code. We followed the same data-loading and evaluation procedure and replaced the method part with BCM. Specifically, to train novel-class models $g_{\\beta}$ in Sec. 4.1, we used the logistic regression in Scikit-learn [30],3 which uses the L-BFGS- $\\boldsymbol{B}$ [42] method with a line-search strategy as the default solver. The regularization parameter was determined from the five-fold cross-validation from the ten candidates $\\{1\\bar{0}^{-5},\\ldots,10^{\\bar{5}}\\}$ . The default values were used for the other hyper-parameters. ", "page_idx": 5}, {"type": "table", "img_path": "p3nPHMpx04/tmp/e7a1e928931e808e65b8de1a946d6e90940d26042cc43fb44a221e1d8058ab9d.jpg", "table_caption": ["Table 1: Average mIoU over five trials. Base and Novel represent mIoU scores over base and novel classes, respectively. Mean shows average of Base and Novel scores. Results of comparison methods were obtained from [20]. All methods use ResNet-50 as backbone. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "5.2 Main results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Table 1 summarizes the average performance over the five trials for each method in the practical GFSS setting. BCM outperformed the other GFSS methods regarding the Novel and Mean scores. Notably, the Novel scores in the 1-shot PASCAL- $.5^{i}$ and PASCAL- $\\cdot10^{i}$ settings substantially improved with BCM. Regarding the Base score, BCM achieved comparable/best performance thanks to it preventing catastrophic forgetting, as discussed in Sec. 4.3. We discuss these results from the viewpoint of our theory in Sec. 5.4. ", "page_idx": 6}, {"type": "text", "text": "These results indicate that BCM achieved the best performance without resorting to various techniques used with the other methods, such as meta-learning [43, 44], information maximization principle [21], and transductive learning [23]. The implementation of BCM was to train the final linear layer only, as described in Sec. 4.4, but we can use cutting-edge architectures and training techniques in practice, leading to further performance improvement. ", "page_idx": 6}, {"type": "text", "text": "5.3 Ablation study ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We investigated the effect of pre-processing (Tukey\u2019s ladder of powers transformation) and ensemble learning, explained in Sec. 4.5. ", "page_idx": 6}, {"type": "text", "text": "Table 2 shows the performance of four variations of BCM, i.e., with and without pre-processing and ensemble learning. Compared with the results in Tab. 1, BCM without data pre-processing and ensemble learning outperformed the other methods in the 1-shot setting, showing that the simple rule and standard supervised learning improved the GFSS performance. The effectiveness of data pre-processing was much higher when the number of novel classes was small (see the PASCAL- $\\cdot5^{i}$ and PASCAL- $\\mathrm{\\nabla}\\mathrm{10}^{i}$ settings). However, the pre-processing decreased this performance slightly in the 1-shot $\\mathrm{COCO-20^{i}}$ setting. ", "page_idx": 6}, {"type": "text", "text": "Our ensemble-learning approach consistently improved GFSS performance, with a roughly $5\\%$ improvement on all datasets. Note that we can use standard ensemble-learning approaches in the 1-shot setting, meaning that further performance improvement is possible in practice. ", "page_idx": 6}, {"type": "image", "img_path": "p3nPHMpx04/tmp/b9731b755cd0558a961449b9715d92e170b3dbcba00361657cf72363c26609f5.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "5.4 Number of chosen base classes ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Figure 5 shows the frequency of $|\\beta|$ , the number of chosen base classes, over 20 runs (four splits and five trials). Overall, $|\\beta|$ tended to decrease with the increase in $K$ . We hypothesize that noisy pairs appear relatively smaller than frequent pairs when $K=5$ , and the top-1 strategy ignored such a noisy pair. In particular, the median value of the frequency in 5-shot was 1 on PASCAL- $\\cdot5^{i}$ and 2 on $\\mathrm{COCO-20^{i}}$ . ", "page_idx": 7}, {"type": "text", "text": "Another observation is that $|\\beta|$ was much smaller than the number of base classes. For example, the largest $|\\beta|$ was four in the 5-shot $\\mathrm{COCO-20^{i}}$ setting, meaning that less than $7\\%(\\approx4/61)$ of base classes (including background) were chosen in the BCM training step. These results indicate that we do not need to prepare $g_{\\beta}$ for many base classes, and training and inference times do not increase rapidly to the number of base classes. Moreover, $|\\beta|$ tends to be small in practice, so IoUs on most of the base classes are perfectly maintained, as shown in Proposition 4.1. ", "page_idx": 7}, {"type": "text", "text": "5.5 Which classes were chosen? ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We explored which class consists of base- and novel-class pairs in the BNM and recorded pairs in the 5-shot COCO- $20^{i}$ setting. In most cases, the background was chosen, meaning that the base-class model recognized novel-class objects as the background. Sometimes, the pairs summarized in Tab. 3 were chosen, showing that the related classes were chosen with BCM. The results empirically confirm our idea that a novel class is classified as the background or a similar base class with the base-class model. ", "page_idx": 7}, {"type": "image", "img_path": "p3nPHMpx04/tmp/9c1b13c90d93cc285aab9d46cf9b0d481c2a4ecdc56190625c275f8ffe04d497.jpg", "img_caption": ["Figure 8: Effect of $s$ in top- $s$ strategy in 5-shot PASCAL- $\\cdot5^{i}$ setting "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "5.6 Effect of top- $\\pmb{s}$ strategy ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We varied $s$ in the top- $s$ strategy and investigated how $s$ affects the segmentation performance and size of $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ in the 5-shot PASCAL- $\\cdot5^{i}$ setting. The results in other settings can be found in Appendix A.1. ", "page_idx": 8}, {"type": "text", "text": "Figures 8a and 8b respectively illustrate the Base and Novel scores with respect to $s\\in\\{1,2,3,5,10\\}$ , showing that the median and minimum scores decreased as $s$ increased. ", "page_idx": 8}, {"type": "text", "text": "We also show the size of $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ in Fig. 8c. The number of chosen base-classes increased, but it was upper-bounded by a certain value. Even though we increased $s$ , the chosen base-classes might be the same. In addition, the increase in $s$ also led to longer training time. In this sense, smaller $s$ is preferable from the perspective of computation time. ", "page_idx": 8}, {"type": "text", "text": "In summary, a higher $s$ may result in selecting redundant base classes and cause performance degradation, and larger $|\\beta|$ requires many models for $g_{\\beta}$ , resulting in longer computation time for training. We thus recommend $s=1$ since it is the best choice based on GFSS performance and computation time. ", "page_idx": 8}, {"type": "text", "text": "5.7 Computation time ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The computation time was measured on a machine equipped with an NVIDIA\u00ae V100, 16 CPU cores, and 32GB memory. ", "page_idx": 8}, {"type": "text", "text": "Training time. We plot the training time [s] with BCM in Fig. 6. CAPL and DIaM are not shown since CAPL requires hours of training time due to meta-learning, and DIaM, which is based on transductive learning, does not optimize models other than the inference phase. In the 1-shot scenario, training time was less than $1\\ \\mathrm{min}$ . The training time increased as the number of novel classes increased. Although we used ensemble learning in the 5-shot scenario, the training time in the 5-shot $\\mathrm{COCO-20^{i}}$ setting was about 7 min. Note that training was done by CPU computation, meaning that further acceleration is expected by GPU computation. ", "page_idx": 8}, {"type": "text", "text": "Inference time. Figure 7 shows the inference time [ms] of CAPL, DIaM, and BCM. Since DIaM is based on transductive learning, the inference time was slower than the inductive methods, i.e., CAPL and BCM. BCM requires computations of the novel-class models in addition to that of the base-class model, but the total inference time was comparable to that of the end-to-end model, i.e., CAPL. ", "page_idx": 8}, {"type": "text", "text": "Note that BCM has $|\\beta|$ final linear layers for novel classes, leading to a subtle slowdown when switching the layers, unlike the end-to-end computation of CAPL. Our implementation used CPUs for training the final linear layers, as Scikit-learn is used, unlike CAPL on a GPU. This device difference might be another reason for the slowdown. In practice, a more sophisticated implementation will shorten the gap between CAPL and BCM. ", "page_idx": 8}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Summary. We presented a simple yet effective GFSS method called BCM. BCM is based on a mapping between base and novel classes and trains novel-class models by simple supervised learning without resorting to meta-learning and the information maximization principle. Since we can use standard supervised learning, training can be done efficiently using off-the-shelf software. We can use the featurizer and base-class model without modifying their weights, enabling us to use cutting-edge public foundation models with BCM. We theoretically showed that the mIoU over most of the base classes is perfectly maintained. Through numerical experiments, we demonstrated the superior performance of BCM method against state-of-the-art GFSS methods. ", "page_idx": 9}, {"type": "text", "text": "Limitations. BCM has limitations that need to be resolved. First, although the final performance of BCM outperformed the other GFSS methods, the performance improvement in the 5-shot scenario was slight, meaning that there is room for improvement. Second, while BCM perfectly maintains the segmentation performances of most of base-classes, it does not improve such performance using novel-class data. A possible direction to resolve these limitations is to investigate the strategy for creating mapping other than the top- $s$ strategy and use more recent powerful supervised/few-shot learning methods. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The authors would like to thank the anonymous reviewers for their helpful comments and suggestions. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Shervin Minaee, Yuri Boykov, Fatih Porikli, Antonio Plaza, Nasser Kehtarnavaz, and Demetri Terzopoulos. Image segmentation using deep learning: A survey. IEEE TPAMI, 44(7):3523\u20133542, 2022. 1, 3   \n[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. 1, 6   \n[3] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. 1   \n[4] Amirreza Shaban, Shray Bansal, Zhen Liu, Irfan Essa, and Byron Boots. One-shot learning for semantic segmentation. In BMVC, 2017. 1, 6   \n[5] Chi Zhang, Guosheng Lin, Fayao Liu, Rui Yao, and Chunhua Shen. CANet: Class-agnostic segmentation networks with iterative refinement and attentive few-shot learning. In CVPR, 2019.   \n[6] Khoi Nguyen and Sinisa Todorovic. Feature weighting and boosting for few-shot segmentation. In ICCV, 2019. 6   \n[7] Kaixin Wang, Jun Hao Liew, Yingtian Zou, Daquan Zhou, and Jiashi Feng. PANet: Few-shot image semantic segmentation with prototype alignment. In ICCV, 2019.   \n[8] Boyu Yang, Chang Liu, Bohao Li, Jianbin Jiao, and Qixiang Ye. Prototype mixture models for few-shot semantic segmentation. In ECCV, pages 763\u2013778, 2020.   \n[9] Haochen Wang, Xudong Zhang, Yutao Hu, Yandan Yang, Xianbin Cao, and Xiantong Zhen. Few-shot semantic segmentation with democratic attention networks. In ECCV, pages 730\u2013746, 2020.   \n[10] Malik Boudiaf, Hoel Kervadec, Ziko Imtiaz Masud, Pablo Piantanida, Ismail Ben Ayed, and Jose Dolz. Few-shot segmentation without meta-learning: A good transductive inference is all you need? In CVPR, pages 13979\u201313988, 2021.   \n[11] Bingfeng Zhang, Jimin Xiao, and Terry Qin. Self-guided and cross-guided learning for few-shot segmentation. In CVPR, pages 8312\u20138321, 2021.   \n[12] Zhihe Lu, Sen He, Xiatian Zhu, Li Zhang, Yi-Zhe Song, and Tao Xiang. Simpler is better: Few-shot semantic segmentation with classifier weight transformer. In ICCV, pages 8741\u20138750, 2021. ", "page_idx": 9}, {"type": "text", "text": "[13] Zhuotao Tian, Hengshuang Zhao, Michelle Shu, Zhicheng Yang, Ruiyu Li, and Jiaya Jia. Prior guided feature enrichment network for few-shot segmentation. IEEE TPAMI, 44(2):1050\u20131065, 2022. ", "page_idx": 10}, {"type": "text", "text": "[14] Chunbo Lang, Gong Cheng, Binfei Tu, and Junwei Han. Learning what not to segment: A new perspective on few-shot segmentation. In CVPR, pages 8057\u20138067, 2022. 3, 6, 7 ", "page_idx": 10}, {"type": "text", "text": "[15] Nico Catalano and Matteo Matteucci. Few shot semantic segmentation: a review of methodologies and open challenges, 2023. 1 ", "page_idx": 10}, {"type": "text", "text": "[16] Zhuotao Tian, Xin Lai, Li Jiang, Shu Liu, Michelle Shu, Hengshuang Zhao, and Jiaya Jia. Generalized few-shot semantic segmentation. In CVPR, pages 11563\u201311572, 2022. 1, 2, 6, 7 ", "page_idx": 10}, {"type": "text", "text": "[17] Sun-Ao Liu, Yiheng Zhang, Zhaofan Qiu, Hongtao Xie, Yongdong Zhang, and Ting Yao. Learning orthogonal prototypes for generalized few-shot semantic segmentation. In CVPR, pages 11319\u201311328, 2023. 3 ", "page_idx": 10}, {"type": "text", "text": "[18] Chunbo Lang, Gong Cheng, Binfei Tu, Chao Li, and Junwei Han. Base and meta: A new perspective on few-shot segmentation. IEEE TPAMI, 45(9):10669\u201310686, 2023. 1, 3, 6 ", "page_idx": 10}, {"type": "text", "text": "[19] Zhihe Lu, Sen He, Da Li, Yi-Zhe Song, and Tao Xiang. Prediction calibration for generalized few-shot semantic segmentation. IEEE TPAMI, 32:3311\u20133323, 2023. 1, 3 ", "page_idx": 10}, {"type": "text", "text": "[20] Sina Hajimiri, Malik Boudiaf, Ismail Ben Ayed, and Jose Dolz. A strong baseline for generalized few-shot semantic segmentation. In CVPR, pages 11269\u201311278, 2023. 1, 2, 3, 6, 7 ", "page_idx": 10}, {"type": "text", "text": "[21] R. Linsker. Self-organization in a perceptual network. Computer, 21(3):105\u2013117, 1988. 3, 7 [22] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. In NIPS Deep Learning and Representation Learning Workshop, 2015. 3, 4 ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "[23] Olivier Chapelle, Bernhard Schlkopf, and Alexander Zien. Semi-Supervised Learning. The MIT Press, 2010. 3, 7 ", "page_idx": 10}, {"type": "text", "text": "[24] Fabio Cermelli, Massimiliano Mancini, Samuel Rota Bulo, Elisa Ricci, and Barbara Caputo. Modeling the background for incremental learning in semantic segmentation. In CVPR, 2020. 3 ", "page_idx": 10}, {"type": "text", "text": "[25] Ze Yang, Ruibo Li, Evan Ling, Chi Zhang, Yiming Wang, Dezhao Huang, Keng Teck Ma, Minhoe Hur, and Guosheng Lin. Label-guided knowledge distillation for continual semantic segmentation on 2D images and 3D point clouds. In ICCV, pages 18601\u201318612, 2023. ", "page_idx": 10}, {"type": "text", "text": "[26] Beomyoung Kim, Joonsang Yu, and Sung Ju Hwang. ECLIPSE: Efficient continual learning in panoptic segmentation with visual prompt tuning. In CVPR, pages 3346\u20133356, 2024. 3 ", "page_idx": 10}, {"type": "text", "text": "[27] Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri Chatterji, Annie Chen, Kathleen Creel, Jared Quincy Davis, Dora Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren Gillespie, Karan Goel, Noah Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, Omar Khattab, Pang Wei Koh, Mark Krass, Ranjay Krishna, Rohith Kuditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir Mirchandani, Eric Mitchell, Zanele Munyikwa, Suraj Nair, Avanika Narayan, Deepak Narayanan, Ben Newman, Allen Nie, Juan Carlos Niebles, Hamed Nilforoshan, Julian Nyarko, Giray Ogut, Laurel Orr, Isabel Papadimitriou, Joon Sung Park, Chris Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Rob Reich, Hongyu Ren, Frieda Rong, Yusuf Roohani, Camilo Ruiz, Jack Ryan, Christopher R\u00e9, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, Krishnan Srinivasan, Alex Tamkin, Rohan Taori, Armin W. Thomas, Florian Tram\u00e8r, Rose E. Wang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie, Michihiro Yasunaga, Jiaxuan You, Matei Zaharia, Michael Zhang, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, and Percy Liang. On the opportunities and risks of foundation models, 2022. 3 ", "page_idx": 10}, {"type": "text", "text": "[28] Michael McCloskey and Neal J. Cohen. Catastrophic interference in connectionist networks: The sequential learning problem. In Psychology of Learning and Motivation, volume 24, pages 109\u2013165. Academic Press, 1989. 5 ", "page_idx": 10}, {"type": "text", "text": "[29] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences, 114(13):3521\u20133526, 2017. 5   \n[30] Fabian Pedregosa, Ga\u00ebl Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, and \u00c9douard Duchesnay. Scikit-learn: Machine learning in Python. JMLR, 12:2825\u20132830, 2011. 5, 6   \n[31] Nitesh V. Chawla. Data Mining for Imbalanced Datasets: An Overview, pages 853\u2013867. Springer, 2005. 5   \n[32] John Wilder Tukey. Exploratory Data Analysis. Addison-Wesley Publishing Company, 1977. 6   \n[33] Shuo Yang, Lu Liu, and Min Xu. Free lunch for few-shot learning: Distribution calibration. In ICLR, 2021. 6   \n[34] Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of Statistical Learning. Springer New York Inc., 2009. 6   \n[35] Zhi-Hua Zhou. Ensemble Methods: Foundations and Algorithms. Chapman & Hall/CRC, 2012. 6   \n[36] Mark Everingham, SM Eslami, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The PASCAL visual object classes challenge: A retrospective. IJCV, 111(1):98\u2013136, 2015. 6   \n[37] Bharath Hariharan, Pablo Arbel\u00e1ez, Ross Girshick, and Jitendra Malik. Simultaneous detection and segmentation. In ECCV, 2014. 6   \n[38] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft COCO: Common objects in context. In ECCV, pages 740\u2013755, 2014. 6   \n[39] Xin Wang, Thomas Huang, Joseph Gonzalez, Trevor Darrell, and Fisher Yu. Frustratingly simple few-shot object detection. In ICML, volume 119, pages 9919\u20139928, 2020. 6   \n[40] Ze Yang, Chi Zhang, Ruibo Li, Yi Xu, and Guosheng Lin. Efficient few-shot object detection via knowledge inheritance. IEEE Transactions on Image Processing, 32:321\u2013334, 2023. 6   \n[41] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing network. In CVPR, 2017. 6   \n[42] Ciyou Zhu, Richard H. Byrd, Peihuang Lu, and Jorge Nocedal. Algorithm 778: L-BFGS-B: Fortran subroutines for large-scale bound-constrained optimization. ACM Trans. Math. Softw., 23(4):550\u2013560, 1997. 6   \n[43] Jurgen Schmidhuber. Evolutionary principles in self-referential learning. On learning now to learn: The meta-meta-meta...-hook. Diploma thesis, Technische Universitat Munchen, Germany, 1987. 7   \n[44] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In ICML, volume 70, pages 1126\u20131135, 2017. 7 ", "page_idx": 11}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1 Effect of top-s strategy ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Figs. 9 and 10 show the effect of $s$ in the top- ${\\bf\\nabla}^{S}$ strategy in the 1-shot PASCAL- $.5^{i}$ and COCO- $20^{i}$ settings, respectively. We can observe a similar tendency to that discussed in Sec. 5.6. ", "page_idx": 12}, {"type": "image", "img_path": "p3nPHMpx04/tmp/064a2efa3e873ae3128e4ae8c9fe7d44a3edcad33ae0caaf67b94e9539eebc97.jpg", "img_caption": ["Figure 9: Effect of $s$ in top-s strategy in 1-shot PASCAL- $\\cdot5^{i}$ setting "], "img_footnote": [], "page_idx": 12}, {"type": "image", "img_path": "p3nPHMpx04/tmp/2f62aec85ca4d8234307d1714f1fd7315664164f727eb8552432e5fd9a7f321a.jpg", "img_caption": ["Figure 10: Effect of $s$ in top- ${\\boldsymbol{s}}$ strategy in 1-shot COCO- $20^{i}$ setting "], "img_footnote": [], "page_idx": 12}, {"type": "text", "text": "A.2 Example when $s>1$ ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We explain obtaining the prediction when $s>1$ . ", "page_idx": 12}, {"type": "text", "text": "Even when $s>1$ , the inference procedure works as explained in Sec. 4.2 since a single base class is mapped to multiple novel classes in BNM. We show the case when $s=2$ with a tiny example: the base classes are $\\cdot_{0},$ and $\\mathbf{\\dot{\\rho}}_{1},$ , and the novel class is $\\bullet\\,\\bullet\\,\\bullet$ . Suppose that we have the following BNM when $s=2$ in Tab. 4. This table shows when the base class $\\acute{\\bullet}$ is mapped to the novel class \u20182\u2019 and $\\mathbf{\\check{\\rho}}_{1}\\mathbf{\\check{\\rho}}_{}$ is also mapped to $\\mathbf{\\zeta}^{\\bullet}2^{\\bullet}$ . In this case, we have the two models: $g_{\\beta=0}$ returns $\\surd0^{\\circ}$ or $\\bullet\\,\\bullet\\,\\bullet$ , and $g_{\\beta=1}$ returns $\\mathbf{\\dot{\\rho}}_{1},$ or $\\mathbf{\\zeta}^{\\bullet}2^{\\bullet}$ . ", "page_idx": 12}, {"type": "table", "img_path": "p3nPHMpx04/tmp/c4708ac99db2f47f0f71f19b7eeb7e03b4aad9e63466c7cab13064193d4fe95e.jpg", "table_caption": ["Table 4: Example BNM when $s=2$ "], "table_footnote": [], "page_idx": 12}, {"type": "text", "text": "For each pixel, the base-class model outputs either $\\cdot_{0},$ or $\\mathbf{\\dot{\\rho}}_{1},$ . We then compute the prediction of the corresponding model and overwrite it. Since BCM does not need to overwrite the same pixel multiple times, we can straightforwardly combine predictions of $g_{\\beta}$ for the final prediction. ", "page_idx": 12}, {"type": "text", "text": "A.3 Broader impact ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "The idea behind BCM will positively affect future studies on few-shot learning. Our future work will lead to more powerful visual understanding systems. Regarding negative societal impact, we expect BCM will not have a direct path to harmful applications. However, harmful actors may maliciously use visual understanding systems. To prevent such a malicious use of technology, we need to pay attention to events in our society. ", "page_idx": 12}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 13}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 13}, {"type": "text", "text": "Justification: We stated the main claims based on the results of the theoretical analysis and numerical experiments. ", "page_idx": 13}, {"type": "text", "text": "Guidelines: ", "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 13}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 13}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Justification: We discuss the limitations of our work in Sec. 6. ", "page_idx": 13}, {"type": "text", "text": "Guidelines: ", "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 13}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 13}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 13}, {"type": "text", "text": "Justification: We provided the proof of the theoretical result. Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 14}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 14}, {"type": "text", "text": "Justification: We disclosed the details of settings and learning procedures in our experiments. Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 14}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 14}, {"type": "text", "text": "Answer: [No] ", "page_idx": 15}, {"type": "text", "text": "Justification: However, we will release the code after we get our organization\u2019s permission. Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 15}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: We wrote the details. Also, we used the pre-trained models and data loading procedure provided by the publicly-available repository of existing method. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 15}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Justification: Partially yes. We used the box plot to show the deviation of the performance on some experiments. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: We disclosed the computing resources used in our experiments. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 16}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: Our research conforms with the NeurIPS Code of Ethics. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 16}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: We discussed broader impacts in Appendix A.3. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 17}, {"type": "text", "text": "Justification: We will not release data or models. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 17}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: We properly cited the assets used in our paper. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 17}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 18}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: Our paper does not release new assets. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 18}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 18}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 18}]