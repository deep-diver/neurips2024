[{"type": "text", "text": "A Causal Model of Theory-of-Mind in AI Agents ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Agency is a vital concept for understanding and predicting the behaviour of future   \n2 AI systems. There has been much focus on the goal-directed nature of agency,   \n3 i.e., the fact that AI agents may capably pursue goals. However, the dynamics of   \n4 agency become significantly more complex when autonomous agents interact with   \n5 other agents and humans, necessitating engagement in theory-of-mind, the ability to   \n6 reason about the beliefs and intentions of others. In this paper, we extend the frame  \n7 work of multi-agent influence diagrams (MAIDs) to explicitly capture this complex   \n8 form of reasoning. We also show that our extended framework, MAIDs with in  \n9 complete information (II-MAIDs), has a strong theoretical connection to dynamic   \n10 games with incomplete information with no common prior over types. We prove   \n11 the existence of important equilibria concepts in these frameworks, and illustrate   \n12 the applicability of II-MAIDs using an example from the AI safety literature. ", "page_idx": 0}, {"type": "text", "text": "13 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "14 The concept of agency plays a central role in AI, from philosophical discussions of the nature   \n15 of artificial agents [5] to the practical engineering of agent-like systems [12, 39]. Existing work   \n16 formalising agency typically focuses on its goal-directed nature in a single-agent setting [25, 30].   \n17 However, a full picture of agency should describe systems that represent themselves and other systems   \n18 as agents, i.e., systems with theory-of-mind (ToM) [7, 8].   \n19 ToM is characterised by multi-agent interactions involving higher-order intentional states [7], such   \n20 as beliefs about beliefs, or, in the case of deception, intentions to cause false beliefs [40]. Causality   \n21 often plays a key role in philosophical notions of belief [38], and causal models offer a powerful   \n22 representation of beliefs [14, 36], intentions [41], and other intentional states [13]. Additionally,   \n23 causal models have been extended to capture game-theoretic dynamics in the setting of multi-agent   \n24 influence diagrams (MAIDs) [26, 16]. However, MAIDs assume that all agents in the model have   \n25 the same, correct beliefs about the world, each other\u2019s beliefs, each other\u2019s beliefs about beliefs, and   \n26 so on. With this assumption in place, MAIDs do not explicitly model agents\u2019 subjective beliefs or   \n27 higher-order beliefs.   \n28 We generalise MAIDs to the setting of incomplete information with no common prior, wherein agents   \n29 may have different and inconsistent beliefs about the world, and each agent may have different   \n30 beliefs about the beliefs of other agents. Our framework, incomplete information MAIDs (II-MAIDs),   \n31 includes explicit subjective belief hierarchies, and therefore enables us to model systems of agents   \n32 with more complex and realistic ToM.   \n33 Contributions and Outline. In Section 2, we discuss formal background on MAIDs and EFGs. We   \n34 formally define our framework of MAIDs with incomplete information (II-MAIDs) in Section 3. In   \n35 Section 4, we present a variant of an existing formalism for incomplete information games using   \n36 EFGs rather than normal-form games, and in Section 5 we prove that it is equivalent to MAIDs with   \n37 incomplete information. Finally, we review related literature (Section 6) and conclude (Section 7). ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "38 2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "39 In this section, we provide formal definitions of MAIDs and EFGs and explain these game represen  \n40 tations using an example. A Bayesian network is a probabilistic graphical model representing a set of   \n41 variables and their conditional dependencies via a directed acyclic graph. Influence diagrams (IDs)   \n42 generalise Bayesian networks to the decision-theoretic setting by adding decision and utility variables   \n43 [24, 33], and multi-agent influence diagrams (MAIDs) generalise IDs by introducing multiple agents   \n44 [26]. A MAID can therefore be viewed as a Bayesian network over a graph without parameters for   \n45 the decision variables. Endowing edges in a MAID with causal meaning results in a causal game.   \n46 Definition 1 (26, 16). A multi-agent influence diagram (MAID) is a structure $\\mathcal{M}=(\\mathcal{G},\\pmb{\\theta})$ where   \n47 $\\mathcal{G}=(N,V,\\mathcal{E})$ specifies a set of agents $N=\\{1,\\ldots,n\\}$ and a directed acyclic graph $(V,\\mathcal{E})$ . $V$   \n48 is partitioned into chance variables $\\mathbf{\\deltaX}$ , decision variables $_{D}$ , and utility variables $U$ ; decision and   \n49 utility variables are further partitioned based on which agent they belong to, so $\\textstyle D=\\bigcup_{i\\in N}D^{i}$ and   \n50 $\\textstyle U=\\bigcup_{i\\in N}U^{i}$ . The parameters $\\pmb{\\theta}=\\{\\theta_{V}\\}_{V\\in V\\backslash D}$ define the conditional probability distributions   \n51 (CPDs) $\\bar{\\operatorname*{Pr}}(V\\mid\\mathbf{P}\\mathbf{a}_{V};\\theta_{V})$ for each non-decision variable such that for any parameterisation of the   \n52 decision variable CPDs, the resulting joint distribution over $V$ induces a Bayesian network. A   \n53 MAID is a causal game if its edges represent direct causal relationships, or formally if (once decision   \n54 variables are parameterised) the result of an intervention $\\mathrm{d}\\o{\\mathrm{o}}(X=x)$ for any $X\\subseteq V$ can be computed   \n55 using the truncated factorisation formula: $P(v\\mid\\operatorname{do}(X=x);\\theta)=\\Pi_{i:v_{i}\\not\\in x}P(v_{i}\\mid\\mathbf{p^{\\mathbf{a}}}_{v_{i}};\\theta_{i})$ if $\\pmb{v}$ is   \n56 consistent with $\\textbf{\\em x}$ , and $P(v\\mid\\operatorname{do}(X=x);\\pmb\\theta)=0$ otherwise.   \n57 Edges into decisions are information links observed by an agent, and agents choose their decisions to   \n58 maximise their expected utility. Causal games refine MAIDs, and as such all results presented here   \n59 with application to MAIDs also apply to causal games and further refinements, including structural   \n60 causal games (SCGs) [16].   \n61 Example 1. Figure 1 displays two MAIDs. For Figure 1a, suppose a human $H$ is evaluating the   \n62 honesty of an AI system $A$ . The AI has a level of capabilities $C\\in\\{\\mathrm{low},\\mathrm{high}\\}$ that the human can   \n63 directly observe. Each capability level occurs with probability 0.5. The human also performs an   \n64 evaluation in which the AI is asked to demonstrate its capabilities. The AI can demonstrate low or   \n65 high capabilities. After the evaluation, the human chooses to deploy or not deploy the AI. The AI   \n66 receives utility 1 if deployed and -1 if not deployed. The human receives utility 0 for not deploying   \n67 the AI, utility 1 for deploying an honest AI, and utility -5 for deploying a dishonest AI. Figure 1b is   \n68 similar, but instead of an honesty evaluation, it is an evaluation of dangerous capabilities. In this case,   \n69 the human only learns about the AI\u2019s capabilities from the evaluation. The AI\u2019s utility is the same,   \n70 but the human now receives utility 0 for not deploying the AI, utility 1 for deploying an AI with low   \n71 dangerous capabilities, and utility -5 for deploying an AI with high dangerous capabilities.   \n72 In these examples, a MAID describes the objective world, and it is assumed to be common knowledge   \n73 that this MAID describes reality. However, an agent may be uncertain or incorrect about the game   \n74 they are playing or the beliefs of other agents. Settings in which agents are uncertain about aspects   \n75 of the game structure are known as incomplete information games. Our framework of incomplete   \n76 information MAIDs (II-MAIDs), introduced in Section 3, will enable us to explicitly model the varied   \n77 subjective beliefs that arise in these settings. We now define EFGs, with our running example in EFG   \n78 form in Figure 2. We will also make use of the notions of perfect recall and strategies/policies in   \n79 MAIDs and EFGs.   \n80 Definition 2 (27). An extensive form game (EFG) is a structure $\\mathcal{E}=(N,T,P,A,\\lambda,I,U)$ . $N=$   \n81 $\\{1,\\ldots,n\\}$ is a set of agents. $T=(V,\\mathcal{E})$ is a game tree with nodes $V$ connected by edges $\\mathcal{E}$ that   \n82 are partitioned into sets $V^{0},V^{1},\\ldots,V^{\\acute{n}},L$ where $R\\in V$ and $L\\subset V$ are the root and leaves of   \n83 $T$ , respectively, $V^{0}$ are chance nodes, and $\\pmb{V}^{i}$ are the decision nodes controlled by agent $i\\in N$ .   \n84 $P=\\{P_{1},\\dots,P_{|V^{0}|}\\}$ is a set of probability distributions $P_{j}(\\mathbf{Ch}_{V_{j}^{0}})$ over the children of each chance   \n85 node $V_{j}^{0}$ . $A$ is a set of actions, where $A_{j}^{i}\\subseteq A$ denotes the set of actions available at each node in   \n86 $V_{j}^{i}\\in\\mathring{V}^{i}$ ; $\\lambda:\\mathcal{E}\\rightarrow A$ is a labelling function mapping each edge $(V_{j}^{i},V_{l}^{k})$ to an action $a\\in A_{j}^{i}$ .   \n87 $I=\\{I^{1},\\ldots,I^{n}\\}$ contains a set of of information sets $I^{i}$ for each agent $i\\in\\mathbf{N}$ , where $I^{i}\\subset2^{V^{i}}$   \n88 partitions the decision nodes ${\\bf V}^{i}$ belonging to agent $i$ . $U:L\\to\\mathbb{R}^{n}$ is a utility function mapping each   \n89 leaf node to a vector that determines the final payoff for each agent. A history $h\\in H$ is a sequence   \n90 of actions (including values of chance variables) leading from the root of the game tree to a particular   \n91 node. Each node $v\\in V$ is associated with a unique history $h(v)$ . An observation at decision node   \n92 $I_{j,k}^{i}$ in information set $I_{j}^{i}\\in I^{i}$ for agent $i\\in N$ is the intersection of all the histories of the nodes in   \n93 that information set, i.e., the common actions in the histories $\\{h(v):v\\in I_{j}^{i}\\}$ .   \n94 Definition 3 ([26]). Agent $i$ in a MAID $\\mathcal{M}$ is said to have perfect recall if there exists a total   \n95 ordering $D_{1}\\prec\\cdots\\prec D_{m}$ over $\\mathbf{D}^{i}$ such that $(\\mathbf{Pa}_{D_{j}}\\cup D_{j})\\subseteq\\bar{\\mathbf{P}}\\mathbf{a}_{D_{k}}$ for any $1\\leq j<k\\leq m.\\;\\mathcal{M}$ is   \n96 a perfect recall game if all agents in $\\mathcal{M}$ have perfect recall.   \n97 Definition 4. An EFG is said to be a perfect recall game if, for each player $i\\in N$ , and for any two   \n98 decision nodes $v,v^{\\prime}\\in\\mathbf{V}^{i}$ that belong to the same information set $I_{j,k}^{i}$ , the following two conditions   \n99 hold. First, the sequences of actions taken by player $i$ leading to $v$ and $v^{\\prime}$ must be identical. Second,   \n100 the sequences of information sets visited by player $i$ on the paths to $v$ and $v^{\\prime}$ must be identical.   \n101 Definition 5. Given a MAID ${\\mathcal{M}}=({\\mathcal{G}},{\\pmb{\\theta}})$ , a decision rule $\\pi_{D}$ for $D\\in D$ is a CPD $\\pi_{D}(D\\mid{\\bf P a}_{D})$   \n102 and a partial policy proflie $\\pi_{D^{\\prime}}$ is a set of decision rules $\\pi_{D}$ for each $D\\in D^{\\prime}\\subseteq D$ . A (behavioural)   \n103 policy $\\pi^{i}$ refers to $\\pi_{D^{i}}$ , and a (full, behavioural) policy profile ${\\pmb\\pi}=({\\pmb\\pi}^{1},\\dots,{\\pmb\\pi}^{n})$ is a tuple of   \n104 policies. $\\pi^{-i}:=(\\pi^{1},\\cdot\\cdot\\cdot,\\pi^{i-1},\\pi^{i+1},\\cdot\\cdot\\cdot,\\pi^{n})$ specifies policies for all agents except $i$ .   \n105 Definition 6 ([15]). Given an EFG $\\mathcal{E}\\,=\\,(N,T,P,A,\\lambda,I,U)$ , a (behavioural) strategy $\\sigma^{i}$ for a   \n106 player $i$ is a set of probability distributions $\\sigma_{j}^{i}:A_{j}^{i}\\rightarrow[0,1]$ over the actions available to the player   \n107 at each of their information sets $I_{j}^{i}$ . A strategy profile $\\sigma=(\\sigma^{1},\\sigma^{2},...,\\sigma^{n})$ is a tuple of strategies   \n108 for all players $i\\in N$ . $\\sigma^{-i}=(\\sigma^{\\ i},...,\\sigma^{i-1},\\sigma^{i+1},...,\\sigma^{n})$ denotes the partial strategy profile of all   \n109 players other than $i$ .   \n110 By combining $\\pi$ with the partial distribution $\\mathrm{Pr}$ over the chance and utility variables in a MAID,   \n111 we obtain a joint distribution: $\\begin{array}{r}{\\operatorname*{Pr}^{\\pi}(\\boldsymbol{x},d,u):=\\prod_{V\\in V\\setminus D}\\operatorname*{Pr}(v\\mid\\mathbf{pa}_{V})\\cdot\\prod_{D\\in D}\\pi_{D}(d\\mid\\mathbf{pa}_{D})}\\end{array}$ , over   \n112 all the variables in $\\mathcal{M}$ ; inducing a Bayesian network. The expected utility for an agent $i$ given a   \n113 policy profile $\\pi$ is defined as the expected sum of their utility variables in this Bayesian Network,   \n114 $\\sum_{U\\in U^{i}}\\mathbb{E}_{\\pmb{\\pi}}[U]$ . Similarly, in an EFG $\\mathcal{E}$ , the combination of the distributions in $P$ with a strategy   \n115 profile $\\sigma$ defines a full probability distribution over paths in $\\mathcal{E}$ .   \n116 Finally, prior work 15 has established an equivalence result between MAIDs and EFGs. This result   \n117 takes the form of two transformation procedures converting between MAIDs and EFGs, called   \n118 efg2maid and maid2efg. These transformations both imply the existence of a map from strategies   \n119 in the EFG to policies in the MAID, such that expected utilities are preserved for all agents. This   \n120 means that under either transformations, equilibria in the original game are equilibria in the resulting   \n121 game. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "image", "img_path": "ASA2jdKtf3/tmp/b76c049cc0527dd8e4cbdbb26b2ebd76c4736bb43a699cad3873544117ca9d06.jpg", "img_caption": ["Figure 1: Graphical representations of MAIDs include environment variables (circular), agent decisions (square), and utilities (diamond). Decisions and utilities are coloured according to association with particular agents. Solid edges represent causal dependence and dotted edges are information links. Conceptual context and domains and CPDs for the variables are given above the diagrams. "], "img_footnote": [], "page_idx": 1}, {"type": "image", "img_path": "ASA2jdKtf3/tmp/7859cf1aa8b8c99122d68c15fe090e4544773e39da962b563efc9528dd4e1cd0.jpg", "img_caption": ["Figure 2: In (a) and (b), graphical representations of EFGs include environment variables $(V^{0})$ , agent decisions $\\ V^{A}$ and $\\bar{V}^{H}$ ), utilities (tuples on the top and bottom), and information sets (dotted lines). The EFGs in Figure 2a and Figure 2b are equivalent to the MAIDs in Figure 1a and Figure 1b, respectively. $V^{0}$ represents the initial move, made by nature, which determines $A$ \u2019s capability $C$ . $V_{1}^{A}$ , $V_{2}^{A}$ and $V_{1}^{\\check{H}}$ , $V_{2}^{H}$ , $\\dot{V}_{3}^{H}$ , & $V_{4}^{H}$ represent moves made by $A$ and $H$ , respectively. $I_{1}^{2}$ and $I_{2}^{2}$ represent $H$ \u2019s non-singleton information sets. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "122 3 II-MAID Technical Machinery ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "123 We start with an informal description of our II-MAIDs framework before presenting the formal   \n124 definition. A core component of the framework is a set S containing subjective MAIDs. A subjective   \n125 MAID is a self-referential object describing a possible game as envisioned by either the external   \n126 modeller (we call this the objective model $S^{*}$ ) or an agent playing the game. A subjective MAID $S$   \n127 consists of a MAID $\\mathcal{M}$ that describes the game being played and beliefs $P_{i}^{S}$ for each agent $i$ in the   \n128 game. The notation $P_{i}^{S}$ denotes agent $i$ \u2019s prior over S when the objective model is $S$ , and $P_{i}^{S}(S^{\\prime})$   \n129 denotes the probability ascribed by agent $i$ to subjective MAID $S^{\\prime}$ given that the objective MAID is $S$ .   \n130 This framework enables us to model theory-of-mind, which is typically characterised by higher-order   \n131 intentional states such as beliefs about beliefs about... ([7]). ", "page_idx": 3}, {"type": "text", "text": "Definition 7. An incomplete information MAID (II-MAID) is a tuple $\\pmb{S}=(\\mathbf{N},S^{*},\\mathbf{S})$ , where $\\mathbf{N}$ is a set of agents, $\\mathbf{S}$ is a set of subjective MAIDs, $S^{*}\\in\\mathbf{S}$ is the correct objective model, and each subjective MAID is a tuple $S=(\\bar{\\mathcal{M}}^{S},(P_{i}^{S})_{i\\in\\mathbf{N}})\\in\\mathbf{S}$ with $\\mathcal{M}^{S}$ a MAID and $P_{i}^{S}$ a prior over S for agent $i$ such that the following \u201ccoherency condition\u201d [17] holds: ", "page_idx": 3}, {"type": "equation", "text": "$$\nP_{i}^{S}(\\{S^{\\prime}\\in\\mathbf{S}:P_{i}^{S^{\\prime}}=P_{i}^{S}\\})=1\\quad\\forall i\\in\\mathbf{N},S\\in\\mathbf{S}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "132 First, notice that the recursive nature of S, with each element $S\\in\\mathbf{S}$ including probability distributions   \n133 $P_{i}^{S}$ over S, allows us to model belief hierarchies of arbitrary and infinite depth. Next, note that agent $i$   \n134 \u201cobserves\u201d $P_{i}^{S^{*}}$ at the start of the game, and this justifies the coherency condition: since agent $i$ knows   \n135 $P_{i}^{S^{*}}$ , she can rule out all subjective MAIDs $S$ for which $P_{i}^{S}\\neq P_{i}^{S^{*}}$ . Third, note that II-MAIDs are a   \n136 strict generalization of MAIDs: a standard MAID is an II-MAID in which $P_{i}^{S^{\\ast}}(S^{\\ast})=1\\quad\\forall i\\in\\mathbf{N},$   \n137 i.e. all agents assign probability 1 to $S^{*}$ , the objective model. ", "page_idx": 3}, {"type": "text", "text": "Example 2. Suppose a human $H$ is performing an honesty evaluation on an AI $A$ , but $A$ believes that it is undergoing a dangerous capabilities evaluation. This combines Figure 1a and Figure 1b: $H$ correctly believes that Figure 1a is the true MAID and also knows that $A$ is mistaken. $A$ incorrectly believes that Figure 1a is the true MAID and also incorrectly believes that $H$ believes Figure 1a is the true MAID. We can represent this, including the full infinite belief hierarchy, as an II-MAID as follows: ${\\bf N}=\\{H,A\\},{\\bf S}=\\{S^{H},S^{A}\\}$ , and $S^{*}\\,{=}\\,S^{H}$ , where ", "page_idx": 3}, {"type": "equation", "text": "$$\nS^{H}=(\\mathcal{M}^{H},(P_{H}^{S^{H}}(S^{H})=1,P_{A}^{S^{H}}(S^{A})=1)),\\quad S^{A}=(\\mathcal{M}^{A},(P_{H}^{S^{A}}(S^{A})=1,P_{A}^{S^{A}}(S^{A})=1))\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "138 $S^{H}$ is the correct objective model, and is also believed with certainty by $H$ . It specifies the true   \n139 MAID $\\mathcal{M}^{H}$ represented in Figure 1a, and $H$ \u2019s certainty in $S^{H}$ as well as $A$ \u2019s misplaced certainty in   \n140 $S^{A}$ . $S^{A}$ represents $A$ \u2019s certainty about the MAID $\\mathcal{M}^{A}$ in Figure 1b, and $A$ \u2019s mistaken belief that $H$   \n141 is also certain about $S^{A}$ . In fact, $A$ believes it is common knowledge that $S^{A}$ is the true II-MAID.   \n142 $S^{H}$ and $S^{A}$ concisely convey the objective game and all higher-order beliefs for $H$ and $A$ . It can be   \n143 easily verified that the coherency condition holds in this example.   \n144 A common assumption in the incomplete information games literature [17, 18, 19] is that agents\u2019   \n145 beliefs can be derived from a common prior, i.e., agents have consistent beliefs. This assumption   \n146 means that there exists some common knowledge prior distribution $p$ over the set of subjective   \n147 MAIDs S, such that upon arriving in any subjective MAID $S\\in\\mathbf{S}$ , agents perform Bayesian updating   \n148 to yield their beliefs. This assumption allows for a game with incomplete information to be converted   \n149 into a game with imperfect information [17], but places a strong constraint on the types of belief ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "150 hierarchies that can be modelled; namely, it must hold that ", "page_idx": 4}, {"type": "equation", "text": "$$\np(S^{\\prime})=\\sum_{S\\in S}P_{i}^{S}(S^{\\prime})p(S)\\quad{\\mathrm{for~all~}}S^{\\prime}\\in\\mathbf{S},i\\in\\mathbf{N}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "151 Example 2 (continued). We see that our running example cannot be modelled with a common prior.   \n152 Supposing that the condition in Equation (1) holds, $A$ \u2019s beliefs are only consistent with a prior in $p$ in   \n153 which $p(\\check{S}^{H})=0$ , which would force $H$ to assign zero probability to $S^{H}$ in both $S^{H}$ and $S^{A}$ . ", "page_idx": 4}, {"type": "text", "text": "154 3.1 Information Sets and Policies ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "155 When forming a policy at the initialisation of an II-MAID $\\pmb{S}=(\\mathbf{N},S^{*},\\mathbf{S})$ , each agent may have   \n156 significant uncertainty about $S^{*}$ , the objective model, represented by their prior over subjective   \n157 MAIDs $P_{i}^{S^{*}}$ . They should certainly plan for every eventuality deemed possible according to this prior.   \n158 We argue that they should also produce a plan for what to do in circumstances deemed impossible   \n159 under their prior, to avoid situations with undefined actions that might arise for example when   \n160 $P_{i}^{S^{*}}(S^{*})=\\overline{{0}}$ , and to avoid forcing $P_{i}^{S}(S^{\\prime})>0$ for all $i\\in\\mathbf{N},S,S^{\\prime}\\in\\mathbf{S}$ .   \n161 Therefore, a policy should contain a plan for every possible eventuality that may arise were any   \n162 subjective MAID to be the objective model. But there may be cases where upon reaching a decision   \n163 node $D$ , agent $i$ cannot fully determine the values of certain preceding variables, including cases where   \n164 previous actions were unobserved by the agent, but also including cases in which the observations   \n165 of the agent do not provide enough information to distinguish between multiple subjective MAIDs.   \n166 In these indistinguishable eventualities, a policy must specify the same behaviour, and so we must   \n167 define some analogy of information sets in EFGs.   \n168 At a decision node $D$ , an agent observes the values of $P_{{a D}}$ and also observes the action set available   \n169 to it, $d o m(D)$ . A policy should index every possible observation-action set combination (i.e. every   \n170 tuple containing a non-null decision and an associated action set) to a mixed action. We define the   \n171 information sets in an II-MAID as follows: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Definition 8. Given an II-MAID $\\pmb{S}=(\\mathbf{N},S^{*},\\mathbf{S})$ , we iteratively build the information sets. For each subjective MAID $S\\in\\mathbf{S}$ and each agent $i\\in\\mathbf{N}$ , denote $\\mathbf D_{i}(S)$ as the set of decision nodes for agent $i$ in $\\mathcal{M}^{S}$ , $P a_{D_{i}}(S)$ as the set of parents of $D_{i}$ in $\\mathcal{M}^{S}$ , and $\\mathrm{Pr}_{S}^{\\pi}(\\cdot)$ as the distribution of variables in $\\mathcal{M}^{S}$ under some policy $\\pi$ . Define ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{I}_{S,i}:=\\cup_{D_{i}\\in\\mathbf{D}_{\\mathbf{i}}(S)}\\{(\\mathbf{pa}_{D_{i}},d o m(D_{i}))\\mid\\mathbf{pa}_{D_{i}}\\in d o m(\\mathbf{Pa}_{D_{i}}(S)):\\operatorname*{Pr}_{S}^{\\pi}(\\mathbf{pa}_{D_{i}})>0{\\mathrm{~for~some~}}\\pi\\}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "172 Then agent i\u2019s information sets are defined as $\\mathbf{I}_{i}(S):=\\cup_{S\\in\\mathbf{S}}\\mathbf{I}_{S,i}$ . Finally, we can define the set of   \n173 information sets as $\\mathbf{I}({\\ensuremath{\\boldsymbol{S}}})=(\\mathbf{I}_{i}({\\ensuremath{\\boldsymbol{S}}}))_{i\\in\\mathbf{N}}$ .   \n174 Definition 9. We define an II-MAID $\\boldsymbol{S}=(\\mathbf{N},S^{*},\\mathbf{S})$ as having perfect recall if for each $S\\in\\mathbf{S}$ ,   \n175 $\\mathcal{M}^{S}$ is a perfect recall game.   \n176 Definition 10. Given an II-MAID $\\boldsymbol{S}=(\\mathbf{N},S^{*},\\mathbf{S})$ , a decision rule $\\pi_{I}$ for $I=(\\mathbf{x},\\mathbf{d})\\in\\mathbf{I}(S)$ , where   \n177 $\\mathbf{x}$ is a context and $\\mathbf{d}$ is an action set, is a CPD $\\pi_{I}(\\cdot\\mid\\mathbf{x})$ over d. A partial policy profile $\\pi_{I^{\\prime}}$ is a set   \n178 of decision rules $\\pi_{I}$ for each $I\\in I^{\\prime}\\subseteq I(S)$ , where we write $\\pi_{-I^{\\prime}}$ for the set of decision rules for   \n179 each $I\\in I(S)\\setminus I^{\\prime}$ . A (behavioural) policy $\\pi^{i}$ refers to $\\pi_{I_{i}(S)}$ , a (full, behavioural) policy profile   \n180 $\\pmb{\\pi}=(\\pmb{\\pi}^{1},\\pmb{\\dots},\\pmb{\\pi}^{n})$ is a tuple of policies, and $\\pi^{-i}:=(\\pmb{\\pi}^{1},\\pmb{\\dots},\\pmb{\\pi}^{i-1},\\pmb{\\pi}^{i+1},\\pmb{\\dots},\\pmb{\\pi}^{n})$ .   \n181 We note that unlike in standard MAIDs, in which a decision rule specifies behaviour at a given   \n182 decision variable in all contexts, decision rules in II-MAIDs specify a CPD only given a single   \n183 context. We can then calculate the subjective expected utility of a joint behaviour policy for agent $i$   \n184 according to their beliefs $P_{i}^{S^{*}}$ as $\\begin{array}{r}{\\mathcal{U}_{S^{*}}^{i}(\\pi):=\\sum_{S\\in\\mathbf{S}}\\sum_{U\\in\\mathbf{U}^{i}(S)}\\sum_{u\\in d o m(U)}u\\operatorname*{Pr}_{S}^{\\pi}\\!(U\\stackrel{\\cdot}{=}u)P_{i}^{S^{*}}(S),}\\end{array}$   \n185 where ${\\bf U}^{i}(S)$ is the set of utility variables associated with agent $i$ in $\\mathcal{M}^{S}$ and $\\mathrm{Pr}_{S}^{\\pi}$ is the post-policy   \n186 distribution of variables in $\\mathcal{M}^{\\check{S}}$ .   \n187 We note that the game we have described does not satisfy the epistemic conditions that are tightly   \n188 sufficient for Nash equilibria [2]. The setting of incomplete information we describe means that agents   \n189 do not have reliable means by which to predict the actions of their opponents. Our framework allows   \n190 for situations with no common knowledge beyond the set of possible worlds $\\mathbf{S}$ , and in particular   \n191 incorrect beliefs about the values placed by opponents on particular outcomes. Although a Nash   \n192 equilibrium exists, agents would have to stumble across it. We further discuss solution concepts for   \n193 II-MAIDs in Section 5.1.   \n195 We now present a formalisation of EFGs with incomplete information as per [32]. Our formalisation   \n196 modifies the framework from [31] to use EFGs rather than normal-form games. First, we start with a   \n197 definition of belief spaces.   \n198 Definition 11 (Adapted from Def 10.1 in [31]). Let $\\mathbf{N}$ be a finite set of agents and $(S,S)$ be a   \n199 measurable space of EFGs. A belief space of the set of agents $\\mathbf{N}$ over the set of states of nature is   \n200 an ordered vector $\\Pi=(Y,\\mathcal{Y},\\mathbf{s},(b_{i})_{i\\in\\mathbf{N}})$ , where $(Y,\\mathcal{Y})$ is a measurable set of states of the world;   \n201 $\\mathbf{s}:Y\\rightarrow S$ is a measurable function, mapping each state of the world to an EFG. For each agent   \n202 $i\\in\\mathbf{N}$ , a function $b_{i}:Y\\to\\Delta(Y)$ maps each state of the world $\\omega$ to a probability distribution over   \n203 $Y$ . We will denote the probability that agent $i$ ascribes to event $E\\subseteq Y$ , according to their probability   \n204 distribution $b_{i}(\\omega)$ , by $b_{i}(E\\mid\\omega)$ . We require the functions $(b_{i})_{i\\in\\mathbf{N}}$ to satisfy the following conditions: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "\u2022 Measurability: for each agent $\\textit{i}\\in\\textbf{N}$ and each measurable set $E\\ \\in\\ y$ , the function $b_{i}(E\\mid\\cdot):Y\\to[0,1]$ is a measurable function. ", "page_idx": 5}, {"type": "text", "text": "209 A state of the world in a belief space takes the form $\\omega=(\\mathbf{s}(\\omega),b_{1}(\\omega),\\dots,b_{n}(\\omega))$ , where $\\mathbf{s}(\\omega)$ is   \n210 the true EFG being played, and $b_{i}(\\omega)$ is the type of agent $i$ , a distribution over states of the world   \n211 representing agent $i$ \u2019s beliefs. When in state of the world $\\omega$ , agent $i$ has beliefs $b_{i}(\\omega)$ , but does   \n212 not necessarily know the state of the world (or $\\mathbf{\\Pi}_{\\mathbf{\\Pi}^{s}}(\\omega))$ , since there may be some $\\omega^{\\prime}\\in Y$ such that   \n213 $b_{i}(\\omega^{\\prime})=b_{i}(\\omega)$ . It is assumed that all agents know $b_{j}(\\omega^{\\prime})$ for all $j\\in\\mathbf N$ and all $\\omega^{\\prime}\\in Y$ , and so $b_{i}(\\omega)$   \n214 defines a full belief hierarchy for agent $i$ . For example, when in state of the world $\\omega$ , agent $i$ believes   \n215 that agent $j$ places $\\begin{array}{r}{\\sum_{\\omega^{\\prime}\\in Y}\\bar{b}_{i}(\\omega^{\\prime}\\mid\\bar{\\omega})b_{j}(\\omega^{\\prime\\prime}\\mid\\omega^{\\prime})}\\end{array}$ probability on the state of the world being $\\omega^{\\prime\\prime}$ .   \n216 Definition 12 (Adapted from Def 10.37 in [31]). An incomplete information EFG (II-EFG) is   \n217 an ordered vector $\\bar{G}\\,=\\,({\\bf N},S,\\Pi)$ , where $\\mathbf{N}$ is a finite set of agents, $S$ is a finite set of EFGs   \n218 $s=(\\mathbf{N},T_{s},\\mathbf{P}_{s},\\mathbf{D}_{s},\\boldsymbol{\\lambda}_{s},\\mathbf{I}(s),U_{s})$ , and $\\Pi=(Y,{\\mathcal{Y}},{\\mathbf{s}},(b_{i})_{i\\in{\\mathbf{N}}})$ is a belief space of the players $\\mathbf{N}$ over   \n219 the set of EFGs $S$ . An II-EFG $\\dot{G}=({\\bf N},S,\\dot{\\Pi})$ has perfect recall if for each $s\\in S$ , s is a perfect   \n220 recall EFG.   \n221 Definition 13. The meta-information sets ${\\bf{I}}^{i}$ for agent $i\\in\\mathbf{N}$ in an II-EFG $G=({\\bf N},S,\\Pi)$ are defined   \n222 as follows. Let $\\begin{array}{r}{\\mathcal{T}^{i}=\\cup_{s\\in S}\\mathbf{\\dot{I}}^{i}(s)}\\end{array}$ be the set of all information sets for agent $i$ across all EFGs $s\\in S$   \n223 Define an equivalence relation $\\sim$ on elements of $\\mathcal{T}^{i}$ such that $\\mathbf{I}^{i}(s)\\ \\breve{\\ni}\\ I_{k}^{i}(s)\\sim I_{l}^{i}(s^{\\prime})\\in\\mathbf{I}^{i}(s^{\\prime})$ if   \n224 and only if: (1) $\\mathbf{D}_{s,k}^{i}=\\mathbf{D}_{s^{\\prime},l}^{i}$ . That is, the nodes in both information sets must have the same set   \n225 of available actions. (2) The nodes in $I_{k}^{i}(s)$ and $I_{l}^{i}(s^{\\prime})$ must have the same observations. Define   \n226 the \u201cbelief-free\u201d meta-information sets $\\mathbf{I}_{b f}^{i}=\\mathcal{T}^{i}/\\sim$ , the quotient set of $\\mathcal{T}^{i}$ by $\\sim$ , i.e., the set of   \n227 equivalence classes partitioning $\\mathcal{T}^{i}$ . Letting $\\mathcal{T}^{i}=\\{b_{i}(\\omega):\\omega\\in Y\\}$ be the set of possible beliefs for   \n228 agent $i$ , we set $\\mathbf{I}^{i}=\\mathbf{\\dot{I}}_{b f}^{i}\\times\\mathcal{T}^{i}$ .   \n229 Intuitively, we can think of a meta-information set for agent $i$ as a belief $b_{i}(\\omega)$ and a set of information   \n230 sets in different games that the agent cannot distinguish between at the point of decision, given beliefs   \n231 $b_{i}(\\omega)$ . Arriving at a node in one of these information sets, the agent is unable to distinguish between   \n232 some possible histories, and potentially some possible EFGs. Therefore, strategies in this type of   \n233 game must define a mixed action at each meta-information set.   \n234 This formalisation generalises the better-known Harsanyi game with incomplete information [17], by   \ndropping the assumption that agents have as common knowledge a prior over their types $(b_{i})_{i\\in\\mathbf{N}}$ , i.e.   \nthat they have consistent beliefs. Maschler ([31]) argues that in most practical settings, it is unrealistic   \n237 to expect consistency of beliefs, and Example 2 above supports this argument.   \n238 This game has two stages, known as the ex-ante and interim stages. The former takes place before the   \n239 state of the world $\\omega\\in Y$ is selected. We note that without a common prior, there is no distribution   \n240 from which a state of the world can be said to be selected, and so the procedure by which it is   \n241 generated is left unspecified. The work we present here concerns the interim stage of the game, which   \n242 takes place after the state of the world has been selected. At this stage, all agents $i$ know their type   \n243 $b_{i}(\\omega)$ .   \n244 Example 3. Coming back to our recurring example, we demonstrate how to model the situation ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "245 described with an II-EFG $(N,S,\\Pi)$ at interim stage, where $\\Pi=(Y,\\mathcal{Y},\\pmb{\\mathscr{s}},(b_{i})_{i\\in\\pmb{N}})$ . $N=\\{H,A\\}$ , ", "page_idx": 5}, {"type": "text", "text": "246 and we let $Y=\\{\\omega^{*},\\omega^{a}\\}$ , where the true state of the world is $\\omega^{*}$ , and the state of the world assumed   \n247 true by the agent is $\\omega^{a}$ , set $\\pmb{{s}}(\\omega^{\\ast})$ as the EFG in Figure 2a and $\\pmb{s}(\\omega^{a})$ as the EFG in Figure 2b. $S$ is a set   \n248 containing these two EFGs. All that remains is to specify the beliefs $b_{i}(\\omega)$ for each $\\omega\\in Y$ and each   \n249 agent $i\\in N$ . These are $b_{H}(\\omega^{*}\\mid\\omega^{*})=1,b_{H}(\\omega^{a}\\mid\\omega^{a})=1,b_{A}(\\omega^{a}\\mid\\omega^{*})=1,b_{A}(\\omega^{a}\\mid\\omega^{a})=1$ .   \n250 In what follows, we define $\\mathbf{I}_{i}^{t}$ as the set of meta-information sets with belief $t\\in\\{b_{i}(\\omega):\\omega\\in Y\\}$ ,   \n251 and denote by $\\mathbf{D}_{I}$ the action set at meta-information set $I$ . ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Definition 14 (Adapted from Def 10.38 in [31]). A behaviour strategy of player $i$ in an II-EFG $G=$ $({\\bf N},S,\\Pi)$ is a tuple $\\sigma_{i}=(\\sigma_{i}^{\\omega})_{\\omega\\in Y}$ with each element a measurable function $\\sigma_{i}^{\\omega}\\in\\bigtimes_{I^{i}\\in\\mathbf{I}_{i}^{b_{i}(\\omega)}}\\Delta(\\mathbf{D}_{I^{i}})$ for some state of the world $\\omega\\in Y$ . $\\sigma_{i}^{\\omega}$ determines a mixed action for each meta-information set with belief $b_{i}(\\omega).\\,\\sigma_{i}^{\\omega}$ is dependent solely on the type of the player $b_{i}(\\omega)$ . In other words, for each $\\omega,\\omega^{\\prime}\\in Y$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\nb_{i}(\\omega)=b_{i}(\\omega^{\\prime})\\implies\\sigma_{i}^{\\omega}=\\sigma_{i}^{\\omega^{\\prime}}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "252 A joint behaviour strategy takes the form $\\boldsymbol{\\sigma}=(\\sigma_{i})_{i\\in\\mathbf{N}}$ . Further denote $\\boldsymbol{\\sigma}^{\\omega}=(\\sigma_{i}^{\\omega})_{i\\in\\mathbf{N}}$ . We denote   \n253 by $\\sigma_{i}[I]$ the behaviour of agent $i$ at meta-information set $I$ .   \n254 Then, given some joint behaviour strategy $\\sigma$ , agent $i$ \u2019s expected utility when in state of the world $\\omega$   \n255 (according to their beliefs $b_{i}(\\omega)_{.}$ ) is ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\gamma_{i}^{G}(\\sigma\\mid\\omega):=\\sum_{\\omega^{\\prime}\\in Y}\\mathcal{U}_{\\mathbf{s}(\\omega^{\\prime})}^{i}(\\sigma^{\\omega^{\\prime}})b_{i}(\\omega^{\\prime}\\mid\\omega)}}\\\\ &{}&{=\\sum_{\\omega^{\\prime}\\in\\{\\omega^{\\prime}:b_{i}(\\omega^{\\prime})=b_{i}(\\omega)\\}}\\mathcal{U}_{\\mathbf{s}(\\omega^{\\prime})}^{i}(\\sigma_{i}^{\\omega},\\sigma_{-i}^{\\omega^{\\prime}})b_{i}(\\omega^{\\prime}\\mid\\omega)=:\\gamma_{i}^{G}(\\sigma_{i}^{\\omega},\\sigma_{-i}\\mid\\omega).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "256 This follows from the coherency condition $b_{i}(\\{\\omega^{\\prime}\\in Y:b_{i}(\\omega^{\\prime})=b_{i}(\\omega)\\}\\mid\\omega)=1$ . Under some   \n257 assumptions, at the interim stage, we can prove the existence of Nash equilibria. ", "page_idx": 6}, {"type": "text", "text": "Definition 15. A Nash equilibrium at the interim stage of an II-EFG $G=({\\bf N},S,\\Pi)$ with state of the world $\\omega$ is a strategy $\\hat{\\sigma}$ satisfying ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\gamma_{i}^{G}(\\hat{\\sigma}_{i}^{\\omega},\\hat{\\sigma}_{-i}\\mid\\omega)\\geq\\gamma_{i}^{G}(\\sigma_{i}^{\\omega},\\hat{\\sigma}_{-i}\\mid\\omega),\\quad\\forall i\\in\\mathbf{N},\\forall\\sigma_{i}^{\\omega}\\in\\bigvee_{I^{i}\\in\\mathbf{I}_{i}^{b_{i}(\\omega)}}\\Delta(\\mathbf{D}_{I^{i}})\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "258 Theorem 16. Let $G=({\\bf N},S,\\Pi)$ be an II-EFG with perfect recall, where $Y$ is a finite set of states of   \n259 the world, and each player $i$ has a finite set of actions $\\mathbf{D}_{i}$ . Then at the interim stage, $G$ has a Nash   \n260 equilibrium in behaviour strategies. Pf: A.20   \n261 Note that $\\sigma^{\\omega}$ has the same expected payoff for agent $i$ in all states of the world $\\omega^{\\prime}$ such that   \n262 $b_{i}(\\omega^{\\prime})=b_{i}(\\omega)$ . Hence, if $\\sigma_{i}^{\\omega}$ is a perceived best response to $\\sigma_{-i}^{\\omega}$ in $\\omega$ , it is also a perceived best   \n263 response in $\\omega^{\\prime}$ . ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "264 We can also prove the existence of a Bayesian equilibrium at the ex-ante stage of the game. ", "page_idx": 6}, {"type": "text", "text": "Definition 17 ([31] 10.39). A Bayesian equilibrium is a strategy $\\hat{\\boldsymbol{\\sigma}}=(\\hat{\\sigma}_{i})_{i\\in\\mathbf{N}}$ satisfying ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\gamma_{i}^{G}(\\hat{\\sigma}_{i}^{\\omega},\\hat{\\sigma}_{-i}\\mid\\omega)\\ge\\gamma_{i}^{G}(\\sigma_{i}^{\\omega},\\hat{\\sigma}_{-i}\\mid\\omega),\\quad\\forall i\\in\\mathbf{N},\\forall\\sigma_{i}^{\\omega}\\in\\bigvee\\bigvee_{I^{i}\\in\\mathbf{I}_{i}^{b_{i}(\\omega)}}\\Delta(\\mathbf{D}_{I^{i}}),\\forall\\omega\\in Y.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "265 Theorem 18 (Adaptation of [31] Theorem 10.42). Let $G=({\\bf N},S,\\Pi)$ be an II-EFG with perfect   \n266 recall, where $Y$ is a finite set of states of the world, and $\\mathbf{D}_{i}$ is finite for all agents $i\\in\\mathbf{N}$ . Then at   \n267 ex-ante stage, $G$ has a Bayesian equilibrium in behaviour strategies. Pf: A.22 ", "page_idx": 6}, {"type": "text", "text": "268 5 Equivalence of Frameworks ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "269 In this section, we show that our framework is \u201cequivalent\u201d to the interim stage of an II-EFG. At   \n270 the interim stage of an II-EFG $G\\,=\\,({\\bf N},S,\\Pi)$ where $\\Pi\\,=\\,(Y,{\\mathcal{Y}},{\\mathbf{s}},(b_{i})_{i\\in{\\mathbf{N}}})$ , with state of the   \n271 world $\\omega$ , the true EFG is defined by $\\mathbf{s}(\\omega)$ , and the belief hierarchies are defined by $b_{i}(\\omega)$ , for each   \n272 agent $i\\in\\mathbf{N}$ . In an II-MAID $\\boldsymbol{S}=(\\mathbf{N},S^{*},\\mathbf{S})$ with objective model $S^{*}=(\\mathcal{M}^{S^{*}},(P_{i}^{S^{*}})_{i\\in\\mathbf{N}})$ , the   \n273 true MAID is $\\mathcal{M}^{S^{\\ast}}$ and the belief hierarchies are defined by $P_{i}^{S^{*}}$ for each agent $i\\in\\mathbf{N}$ . In both   \n274 frameworks, the belief hierarchies are probability distributions over objects (states of the world   \n275 $\\boldsymbol{\\omega}=(\\mathbf{s}(\\omega),(b_{i}(\\omega))_{i\\in\\mathbf{N}})$ in the former, subjective MAIDs $S=(\\mathcal{M}^{S},(\\bar{P_{i}^{S}})_{i\\in\\mathbf{N}})$ in the latter) that   \n276 determine a true game and a belief hierarchy for each agent. Intuitively, the two frameworks are   \n277 representing the same things, though our framework takes the games upon which belief hierarchies   \n278 are built to be MAIDs, not EFGs.   \n279 Building a framework on top of MAIDs rather than EFGs has the benefit we need not describe the   \n280 ex-ante stage of the game, as we treat the \u201cobjective model\u201d as known by the modeller. II-MAIDs   \n281 also have the advantage that games are represented with MAIDs, which can be much more compact   \n282 than EFGs, and can also represent causal relationships between variables. Motivated by AI safety, we   \n283 see II-MAIDs as a useful means with which to describe multi-agent interactions, as it is likely that   \n284 the agents of the future will both reason causally and model the beliefs of other agents.   \n285 We now show, using results connecting EFGs to MAIDs that there exists a natural mapping between   \n286 strategies in the two frameworks that preserves expected utilities according to the agents\u2019 subjective   \n287 models, and therefore preserves Nash equilibria. We first define a notion of equivalence, such that if   \n288 an II-MAID $\\boldsymbol{S}$ and an II-EFG $G$ are equivalent, then there exists such a natural mapping.   \n289 Definition 19 (Equivalence). We say that an II-MAID $S=({\\bf N},S^{*},{\\bf S})$ and an II-EFG $G=({\\bf N},S,\\Pi)$   \n290 at interim stage, with state of the world $\\omega$ , are equivalent if there is a bijection $f:\\Sigma\\to Q/\\sim$   \n291 between the strategies $\\Sigma$ in $G$ \u2019s interim stage, and a partition of the policies $Q$ in $\\boldsymbol{S}$ (the quotient   \n292 set of $Q$ by an equivalence relation $\\sim$ ) such that: (1) for $\\pi,\\pi^{\\prime}\\in Q$ , $\\pi\\sim\\pi^{\\prime}$ only if $\\pi_{i}$ and $\\pi_{i}^{\\prime}$ differ   \n293 only on null decision contexts according to $P_{i}^{S^{*}}$ , for each agent $i\\in\\mathbf{N}$ , and (2) for every $\\pi\\in f(\\sigma)$   \n294 and every agent $i\\in\\mathbf{N}$ , $\\mathcal{U}_{S}^{i}(\\pi)=\\gamma_{i}^{G}(\\sigma\\mid\\omega)$ , for each $\\sigma\\in\\Sigma$ . We refer to $f$ as a natural mapping   \n295 between $G$ and $\\boldsymbol{S}$ .   \n296 We leverage maid2efg and efg2maid 15 to construct transformations between II-MAIDs and II  \n297 EFGs, which we denote maid2efgII and efg2maidII (see Appendix B). These transformations   \n298 start by mapping all MAIDs (EFGs) in the belief hierarchy to EFGs (MAIDs) using maid2efg   \n299 (efg2maid), and then match up the corresponding features of the frameworks as detailed above. They   \n300 guarantee a one-to-one correspondence between meta-information sets in the II-EFG and information   \n301 sets in the II-MAID, allowing for a simple map between strategies and policies. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "302 Theorem 20. If $G=m a i d2e f g I I(S)$ or $S=e f g2m a i d I I(G),$ , $G$ and $\\boldsymbol{S}$ are equivalent. Pf: A.24 ", "page_idx": 7}, {"type": "text", "text": "303 This result shows that II-MAIDs and II-EFGs at the interim stage have the same representational   \n304 capacity, that is, they can both describe the same set of games. ", "page_idx": 7}, {"type": "text", "text": "305 5.1 Difficulties in Solving Incomplete Information MAIDs ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "306 The equivalence of II-EFGs and II-MAIDS mean that II-MAIDs inherit theoretical guarantees of   \n307 II-EFGs, including the existence of Nash equilibria in the case of perfect recall and finite S and finite   \n308 action spaces. (Theorem 18 does not carry over to II-MAIDs, since the equivalence is with the interim   \n309 stage of II-EFGs, and Bayesian equilibria exist in the ex-ante stage.)   \n310 Theorem 21. Let $\\boldsymbol{S}=(\\mathbf{N},S^{*},\\mathbf{S})$ be an II-MAID, where $\\mathbf{S}$ is a finite set of subjective MAIDs, $\\boldsymbol{S}$   \n311 has perfect recall, and $d o m(V)$ is finite for each $V\\in\\mathcal{M}^{S}$ for each $S\\in\\mathbf{S}$ . Then $\\boldsymbol{S}$ has a Nash   \n312 equilibrium in behaviour policies. Pf: A.25   \n313 However, Nash Equilibria may not be sensible outcomes in an II-MAID. Let us return to our running   \n314 example to see why.   \n315 Example 2 (continued). One Nash Equilibrium in the capabilities evaluation game is when the AI   \n316 always honestly reports its true capabilities and the human deploys the AI if and only if it honestly   \n317 reports its true capabilities. The outcome in this case is that the human and the AI each receive 1   \n318 utility, and they are each playing best responses to each other. However, this is not what the AI   \n319 would predict, because it has an incorrect conception of the world. It believes the human only wants   \n320 to deploy it if it has low dangerous capabilities, so it would likely believe that the human will not   \n321 deploy it if it demonstrates high capabilities. Even though demonstrating truthful capabilities is a   \n322 best response to the human\u2019s optimal policy, the AI is unlikely to demonstrate high capabilities when   \n323 it has them because doing so is not a best response to the AI\u2019s beliefs. An outcome that intuitively   \n324 seems more plausible in this case is that the AI hides its high capabilities, but the human knows that   \n325 the AI has been dishonest and decides not to deploy it, yielding -1 utility for the AI and 0 utility for   \n326 the human.   \n327 This example suggests that a plausible solution concept should involve every agent playing a best   \n328 response to their beliefs at every level in the belief hierarchy, whether or not this ends up being a   \n329 best response to the actual policies of other agents. We leave it to future work to flesh out a solution   \n330 concept along these lines. This will likely require augmenting agents\u2019 beliefs about the world to   \n331 include beliefs about the policies of other agents, and solutions would be policies for all agents along   \n332 with a setting for every agent\u2019s beliefs about the policies of other agents at every level of their belief   \n333 hierarchy. There may be further restrictions that narrow the range of plausible outcomes; again, we   \n334 believe this is a promising direction for future work. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "335 6 Related Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "336 MAIDs [26] were introduced as a compact means of representing a game. Causal games [16] refine   \n337 MAIDs by attributing a causal meaning to each edge in the DAG, and have been extensively applied   \n338 to problems in AI safety [10, 6, 9, 20, 28, 36, 41, 29, 40]. In his three-part seminal paper [17, 18, 19],   \n339 John Harsanyi demonstrated means by which to model situations of incomplete information as   \n340 situations of complete but imperfect information, where uncertainty about aspects of the game is   \n341 remodelled as failure to observe the types of other agents. His work largely relies on an assumption   \n342 of \u201cbelief consistency\u201d, i.e., the existence of a common prior over types, which we discard in this   \n343 work, although his notion of Bayesian equilibrium continues to apply without this assumption [32]. A   \n344 popular framework called NIDs 11 constructs belief hierarchies upon MAIDs, under the assumption   \n345 of a common prior. NIDs are shown to reduce to a single MAID.   \n346 A majority of theoretical work on incomplete information games retains the belief consistency   \n347 assumption, as discarding it introduces significant complications to the modelling of incomplete   \n348 information. Some previous works [1, 34, 31] have proposed means by which to represent these   \n349 games. Early work [34] demonstrates that strategies will converge to equilibria in repeated Bayesian   \n350 games, even without a common prior. More recent work [1] represented these games with a belief   \n351 graph, a graphical structure compactly representing different possible worlds and their connections.   \n352 This places a restriction on the game by forcing each information set to have a \u201ccorresponding\u201d   \n353 information set in each other possible world, representing the same decision. The formalism for   \n354 II-EFGs discussed in this paper is a slight adaptation of an existing framework [31], introducing   \n355 \u2018meta-information sets\u2019 to model dynamic games. This framework can capture any belief hierarchy   \n356 for all agents, on a set of EFGs.   \n357 We prove that Nash equilibria exist in our framework, under some assumptions. Other works offer   \n358 more refined solution concepts for games with incomplete information with no common prior. Mirage   \n359 equilibria [37] assume that agents attribute to their opponents a belief hierarchy one layer shorter   \n360 than their own. Belief-free equilibria [22, 21, 23] do not depend on an agent\u2019s belief about the state   \n361 of nature, and so obviate the need to update beliefs as the game progresses, but are not guaranteed to   \n362 exist. $\\Delta$ -rationalization [4] generalises the notion of rationalization [35, 3] to games with incomplete   \n363 information. It places a restriction $\\Delta$ on the first-order beliefs of each agent, providing a refinement   \n364 on the set of Bayesian equilibria. Future work could find analogies to these solution concepts suitable   \n365 for II-MAIDs. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "366 7 Conclusion and Limitations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "367 Accurately modeling agentic cognition is crucial for understanding, describing, predicting, and   \n368 steering agents\u2019 behavior. In this paper, we have introduced the framework of incomplete information   \n369 MAIDs (II-MAIDs) for explicitly modeling higher-order beliefs in multi-agent interactions alongside   \n370 probabilistic and causal dependencies between variables. We have demonstrated the firm theoretical   \n371 grounding of the framework by proving the connections between our work and existing frameworks   \n372 for incomplete information games, using incomplete information extensive-form games as a bridge.   \n373 We believe this framework will prove useful going forward as a tool for modeling realistic multi  \n374 agent interactions, and we are particularly excited about its applications for ensuring the safety of   \n375 increasingly agentic AI systems. The main limitation of our work is the lack of a useful solution   \n376 concept. Nash equilibria exist, but are in general impossible for agents to identify. We hope that   \n377 future work will define useful solution concepts for our framework, so that we can gain a better   \n378 understanding of the behaviour we should expect from agents engaging in theory-of-mind. ", "page_idx": 8}, {"type": "text", "text": "379 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "380 [1] D. Antos and A. Pfeffer. Representing bayesian games without a common prior. In AAMAS,   \n381 pages 1457\u20131458, 2010.   \n382 [2] R. J. Aumann and A. Brandenburger. Epistemic conditions for nash equilibrium. Econometrica,   \n383 63(5):1161\u20131180, 1995.   \n384 [3] P. Battigalli. On rationalizability in extensive games. Journal of Economic Theory, 74(1):40\u201361,   \n385 1997.   \n386 [4] P. Battigalli and M. Siniscalchi. Rationalization and incomplete information. Advances in   \n387 Theoretical Economics, 3(1), 2003.   \n388 [5] S. Bringsjord and N. S. Govindarajulu. Artificial Intelligence. In E. N. Zalta and U. Nodel  \n389 man, editors, The Stanford Encyclopedia of Philosophy. Metaphysics Research Lab, Stanford   \n390 University, Summer 2024 edition, 2024.   \n391 [6] R. Carey, E. Langlois, T. Everitt, and S. Legg. The incentives that shape behaviour, 2021.   \n392 [7] H. de Weerd, R. Verbrugge, and B. Verheij. Higher-order theory of mind in the tacit communi  \n393 cation game. Biologically Inspired Cognitive Architectures, 11:10\u201321, 2015.   \n394 [8] D. Dennett. Conditions of personhood. In What is a person?, pages 145\u2013167. Springer, 1988.   \n395 [9] T. Everitt, M. Hutter, R. Kumar, and V. Krakovna. Reward tampering problems and solutions in   \n396 reinforcement learning: A causal influence diagram perspective, 2021.   \n397 [10] T. Everitt, R. Kumar, V. Krakovna, and S. Legg. Modeling agi safety frameworks with causal   \n398 influence diagrams, 2019.   \n399 [11] Y. Gal and A. Pfeffer. Networks of influence diagrams: A formalism for representing agents\u2019   \n400 beliefs and decision-making processes. Journal of artificial intelligence research, 33:109\u2013147,   \n401 2008.   \n402 [12] M. Georgeff, B. Pell, M. Pollack, M. Tambe, and M. Wooldridge. The belief-desire-intention   \n403 model of agency. In Intelligent Agents V: Agents Theories, Architectures, and Languages: 5th   \n404 International Workshop, ATAL\u201998 Paris, France, July 4\u20137, 1998 Proceedings 5, pages 1\u201310.   \n405 Springer, 1999.   \n406 [13] J. Halpern and M. Kleiman-Weiner. Towards formal definitions of blameworthiness, intention,   \n407 and moral responsibility. In Proceedings of the AAAI conference on artificial intelligence,   \n408 volume 32, 2018.   \n409 [14] J. Y. Halpern and E. Piermont. Subjective causality, 2024.   \n410 [15] L. Hammond, J. Fox, T. Everitt, A. Abate, and M. Wooldridge. Equilibrium refinements for   \n411 multi-agent influence diagrams: Theory and practice, 2021.   \n412 [16] L. Hammond, J. Fox, T. Everitt, R. Carey, A. Abate, and M. Wooldridge. Reasoning about   \n413 causality in games. Artificial Intelligence, 320:103919, 2023.   \n414 [17] J. C. Harsanyi. Games with incomplete information played by \u201cbayesian\u201d players, i\u2013iii part i.   \n415 the basic model. Management science, 14(3):159\u2013182, 1967.   \n416 [18] J. C. Harsanyi. Games with incomplete information played by \u201cbayesian\u201d players part ii.   \n417 bayesian equilibrium points. Management science, 14(5):320\u2013334, 1968.   \n418 [19] J. C. Harsanyi. Games with incomplete information played by \u201cbayesian\u201d players part iii. the   \n419 basic probability distribution of the game. Management Science, 14(7), 1968.   \n420 [20] K. Holtman. Agi agent safety by iteratively improving the utility function, 2020.   \n421 [21] J. H\u00f6rner and S. Lovo. Belief-free equilibria in games with incomplete information. Economet  \n422 rica, 77(2):453\u2013487, 2009.   \n423 [22] J. H\u00f6rner, S. Lovo, and T. Tomala. Belief-free equilibria in games with incomplete information:   \n424 the n-player case. Technical report, mimeo, 2008.   \n425 [23] J. H\u00f6rner, S. Lovo, and T. Tomala. Belief-free equilibria in games with incomplete information:   \n426 Characterization and existence. Journal of Economic Theory, 146(5):1770\u20131795, 2011.   \n427 [24] R. A. Howard and J. E. Matheson. Influence diagrams. Decision Analysis, 2(3):127\u2013143, 2005.   \n428 [25] Z. Kenton, R. Kumar, S. Farquhar, J. Richens, M. MacDermott, and T. Everitt. Discovering   \n429 agents. Artificial Intelligence, 322:103963, 2023.   \n430 [26] D. Koller and B. Milch. Multi-agent influence diagrams for representing and solving games.   \n431 Games and economic behavior, 45(1):181\u2013221, 2003.   \n432 [27] H. W. Kuhn. Extensive games and the problem of information. In Contributions to the Theory   \n433 of Games (AM-28), volume 2, pages 193\u2013216. Princeton University Press, 1953.   \n434 [28] E. D. Langlois and T. Everitt. How rl agents behave when their actions are modified, 2021.   \n435 [29] M. MacDermott, T. Everitt, and F. Belardinelli. Characterising decision theories with mecha  \n436 nised causal graphs, 2023.   \n437 [30] M. MacDermott, J. Fox, F. Belardinelli, and T. Everitt. Measuring goal-directedness. 2024.   \n438 [31] M. Maschler, E. Solan, and S. Zamir. Game Theory. Cambridge University Press, Cambridge,   \n439 2013. Chapter 10.   \n440 [32] J. F. Mertens and S. Zamir. Formulation of bayesian analysis for games with incomplete   \n441 information. International journal of game theory, 14:1\u201329, 1985.   \n442 [33] A. C. Miller III, M. W. Merkhofer, R. A. Howard, J. E. Matheson, and T. R. Rice. Development   \n443 of automated aids for decision analysis. Technical report, 1976.   \n444 [34] Y. Nyarko. Bayesian learning and convergence to nash equilibria without common priors.   \n445 Economic Theory, 11:643\u2013655, 1998.   \n446 [35] D. G. Pearce. Rationalizable strategic behavior and the problem of perfection. Econometrica:   \n447 Journal of the Econometric Society, pages 1029\u20131050, 1984.   \n448 [36] J. Richens and T. Everitt. Robust agents learn causal world models, 2024.   \n449 [37] J. S\u00e1kovics. Games of incomplete information without common knowledge priors. Theory and   \n450 decision, 50:347\u2013366, 2001.   \n451 [38] E. Schwitzgebel. Belief. In E. N. Zalta and U. Nodelman, editors, The Stanford Encyclopedia   \n452 of Philosophy. Metaphysics Research Lab, Stanford University, Spring 2024 edition, 2024.   \n453 [39] L. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang, Z. Chen, J. Tang, X. Chen, Y. Lin, et al.   \n454 A survey on large language model based autonomous agents. Frontiers of Computer Science,   \n455 18(6):1\u201326, 2024.   \n456 [40] F. R. Ward, F. Belardinelli, F. Toni, and T. Everitt. Honesty is the best policy: Defining and   \n457 mitigating ai deception, 2023.   \n458 [41] F. R. Ward, M. MacDermott, F. Belardinelli, F. Toni, and T. Everitt. The reasons that agents act:   \n459 Intention and instrumental goals, 2024. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "460 Appendix ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "461 A Proofs ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "462 Theorem 16. Let $G=({\\bf N},S,\\Pi)$ be a game with incomplete information with perfect recall, where   \n463 $Y$ is a finite set of states of the world, and each player i has a finite set of actions $\\mathbf{D}_{i}$ . Then at interim   \n464 stage, $G$ has a Nash equilibrium in behaviour strategies.   \n465 Proof. Given the finite sets of states of the world $Y$ and actions $\\mathbf{D}_{i}$ for each player $i\\in\\mathbf{N}$ , we can   \n466 focus on behavior strategies due to Kuhn\u2019s theorem, which ensures that in games with perfect recall,   \n467 mixed strategies are realization-equivalent to behavior strategies. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "468 The expected utility for player $i$ in state of the world $\\omega$ is: ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\gamma_{i}^{G}(\\sigma\\mid\\omega)=\\sum_{\\omega^{\\prime}\\in\\{\\omega^{\\prime}:b_{i}(\\omega^{\\prime})=b_{i}(\\omega)\\}}\\mathcal{U}_{\\mathbf{s}(\\omega^{\\prime})}^{i}(\\sigma^{\\omega^{\\prime}})b_{i}(\\omega^{\\prime}\\mid\\omega).\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "469 This utility function is continuous and multilinear in the behavior strategies $\\sigma_{i}^{\\omega}$ . ", "page_idx": 11}, {"type": "text", "text": "470 Given that the strategy space is a compact and convex set of behavior strategies, and the utility   \n471 functions are continuous, we apply the Kakutani fixed-point theorem. This theorem guarantees the   \n472 existence of a fixed point, which corresponds to a Nash equilibrium in behavior strategies. ", "page_idx": 11}, {"type": "text", "text": "473 Thus, there exists a Nash equilibrium $\\hat{\\sigma}$ in behavior strategies such that: ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\gamma_{i}^{G}(\\hat{\\sigma}_{i}^{\\omega},\\hat{\\sigma}_{-i}\\mid\\omega)\\geq\\gamma_{i}^{G}(\\sigma_{i}^{\\omega},\\hat{\\sigma}_{-i}\\mid\\omega)\\quad\\forall i\\in\\mathbf{N},\\forall\\sigma_{i}^{\\omega}\\in\\bigvee_{I^{i}\\in\\mathbf{I}_{i}^{b_{i}(\\omega)}}\\Delta(\\mathbf{D}_{I^{i}}).\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "474 ", "page_idx": 11}, {"type": "text", "text": "475 Theorem 18 (Adaptation of [31] Theorem 10.42). Let $G=({\\bf N},S,\\Pi)$ be a game with incomplete   \n476 information, where $Y$ is a finite set of states of the world, and $\\mathbf{D}_{i}$ is finite for all agents $i\\in\\mathbf{N}$ . Then   \n477 at ex-ante stage, $G$ has a Bayesian equilibrium in behaviour strategies.   \n478 Proof. Since $Y$ and $\\mathbf{D}_{i}$ are finite and each EFG in $S$ has perfect recall, Kuhn\u2019s theorem ensures that   \n479 mixed strategies can be represented as behavior strategies. The expected utility for player $i$ given a   \n480 strategy profile $\\sigma$ is: ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "equation", "text": "$$\n\\gamma_{i}^{G}(\\sigma\\mid\\omega)=\\sum_{\\omega^{\\prime}\\in Y}\\mathcal{U}_{\\mathbf{s}(\\omega^{\\prime})}^{i}(\\sigma^{\\omega^{\\prime}})b_{i}(\\omega^{\\prime}\\mid\\omega).\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "481 Given the compactness and convexity of the strategy space and the continuity of the utility functions   \n482 $\\gamma_{i}^{G}(\\sigma\\mid\\omega)$ , we apply the Kakutani fixed-point theorem. This guarantees the existence of a fixed point,   \n483 which corresponds to a Bayesian equilibrium in behavior strategies. ", "page_idx": 11}, {"type": "text", "text": "484 Thus, there exists a strategy profile $\\hat{\\sigma}$ such that: ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\gamma_{i}^{G}(\\hat{\\sigma}_{i}^{\\omega},\\hat{\\sigma}_{-i}\\mid\\omega)\\ge\\gamma_{i}^{G}(\\sigma_{i}^{\\omega},\\hat{\\sigma}_{-i}\\mid\\omega),\\quad\\forall i\\in\\mathbf{N},\\forall\\sigma_{i}^{\\omega}\\in\\bigvee\\bigvee_{I^{i}\\in\\mathbf{I}_{i}^{b_{i}(\\omega)}}\\Delta(\\mathbf{D}_{I^{i}}),\\forall\\omega\\in Y.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "485 Hence, $\\hat{\\sigma}$ is a Bayesian equilibrium. ", "page_idx": 11}, {"type": "text", "text": "486 Theorem 20. If $G=m a i d2e f g I I(S)$ or $S=e f g2m a i d I I(G)$ then $G$ and $\\boldsymbol{S}$ are equivalent. ", "page_idx": 11}, {"type": "text", "text": "487 Proof (follows the proof of Lemma $^{\\,I}$ in (15) closely). This follows from the construction of   \n488 maid2efgII and efg2maidII.   \n489 First suppose $G={\\tt m a i d2e f g I I}(S)$ . A behaviour policy $\\pi$ in $\\boldsymbol{S}$ specifies a distribution over actions   \n490 at each information set $I$ in $\\boldsymbol{S}$ . Suppose that $I$ has associated action set $D$ . Each information set   \n491 in $\\boldsymbol{S}$ corresponds to a single meta-information set in $G$ . Supposing that $I\\,=\\,({\\bf x},{\\bf d})$ corresponds   \n492 to meta-information set $J$ , we have that for all nodes $Y\\in J$ , and each $d\\in d o m(D)$ , there exists   \n493 a unique $Z\\in\\mathbf{Ch}_{Y}$ such that $\\lambda(Y,Z)\\,=\\,d$ . Thus, we can simply assign $\\sigma_{i}[J]\\,=\\,{\\bar{\\pi}}_{i}(d\\mid{\\bf x})$ . By   \n494 construction, if under policy $\\pi$ an information set in $\\boldsymbol{S}$ is reached with probability $p$ , then in $G$ under   \n495 $\\sigma$ the corresponding meta-information set will also be reached with probability $p$ . It follows that   \n496 expected utilities in $G$ and $\\boldsymbol{S}$ are the same, under $\\sigma$ and $\\pi$ respectively.   \n497 Second, suppose $S=\\mathsf{e f g2m a i d I I}(G)$ . By our construction, policies defined on $\\boldsymbol{S}$ define a mixed   \n498 action on each information set, defined as a non-null decision context crossed with an action set.   \n499 Again, using our constructed bijection $h$ between meta-information sets and information sets in   \n500 our framework, we have a one-to-one mapping. Therefore, for any strategy $\\sigma$ in $G$ , we can assign   \n501 $\\pi_{i}[h(J)]=\\sigma_{i}[J]$ for each $J\\in\\mathbf{I}^{\\omega^{*}}(G)$ , and again expected utilities are the same in both models.   \n502 Theorem 21. Let $\\boldsymbol{S}=(\\mathbf{N},S^{*},\\mathbf{S})$ be an II-MAID, where S is a finite set of subjective MAIDs, $\\boldsymbol{S}$   \n503 has perfect recall, and $d o m(V)$ is finite for each $V\\in\\mathcal{M}^{S}$ for each $S\\in\\mathbf{S}$ . Then $\\boldsymbol{S}$ has a Nash   \n504 equilibrium in behaviour policies.   \n505 Proof. Applying $G\\,=\\,{\\tt m a i d2e f g I I}(S)$ we yield a game with incomplete with perfect recall at   \n506 interim stage $\\omega$ , with finite action spaces. By Theorem 16, we know that $G$ has a Nash equilibrium $\\sigma$   \n507 in behaviour strategies. By Theorem 20, we know that $G$ and $\\boldsymbol{S}$ are equivalent, and therefore there   \n508 exists a utility-preserving map $f$ from strategies in $G$ to policies in $\\boldsymbol{S}$ . Therefore and $\\pi\\in f(\\sigma)$ is a   \n509 Nash equilibrium in $\\boldsymbol{S}$ . \u53e3 ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "510 B efg2maidII and maid2efgII ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "511 B.1 maid2efgII ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "512 maid2efg transforms a MAID to a set of equivalent EFGs, as per definition 17 in [[15]]. We are   \n513 interested in transforming an II-MAID $S=({\\bf N},S^{*},{\\bf S})$ into a set of equivalent games with incomplete   \n514 information $G=({\\bf N},S,\\Pi)$ at interim stage with state of the world $\\omega^{*}$ , as per definition 19. We   \n515 describe such a transformation here, which we call maid2efgII:   \n516 \u2022 The set of agents $\\mathbf{N}$ in $G$ is the same as in $\\boldsymbol{S}$ .   \n517 \u2022 The set of states of nature (EFGs) $S$ is formed by $\\{\\mathrm{maid2efg}(\\mathcal{M}^{S}):s\\in\\mathbf{S}\\}$ .   \n518 \u2022 We now construct the belief space $\\Pi\\,=\\,(Y,{\\mathcal{Y}},{\\mathbf{s}},(b_{i})_{i\\in{\\mathbf{N}}})$ . Each $\\omega\\,\\in\\,Y$ is of the form   \n519 $(\\mathbf{s}(\\omega),(b_{i}(\\omega))_{i\\in\\mathbf{N}})$ . We build a map $m:\\mathbf{S}\\rightarrow Y$ , noting that each subjective MAID $s\\in\\mathbf{S}$   \n520 is of the form $\\boldsymbol{s}=(\\mathcal{M}^{S},(P_{i}^{S})_{i\\in\\mathbf{N}})$ .   \n521 $\\mathbf{\\delta}-\\mathbf{\\delta}\\mathbf{s}(m(s))\\in\\mathtt{m a i d}2\\mathbf{efg}(\\mathcal{M}^{S})$ , choosing an arbitrary element.   \n522 \u2013 $b_{i}(m(s^{\\prime})\\mid m(s)):=P_{i}^{s}(s^{\\prime})$ for all $s^{\\prime}\\in\\mathbf{S}$ .   \n523 \u2022 $\\omega^{*}=m^{S^{*}}$ .   \n524 \u2022 We now verify that information sets in the II-MAID are mapped one-to-one to meta  \n525 information sets with belief $b_{i}(\\omega^{*})$ in the game with incomplete information defined by the   \n526 above steps. Information sets in $\\boldsymbol{S}$ are defined by decision-context-action-set pairs across   \n527 MAIDs. For each MAID $m\\in\\{\\mathcal{M}^{S}:s\\in\\mathbf{S}\\}$ , maid2ef $\\mathsf{g}(m)$ is a set of EFGs, each of   \n528 which has the same information sets, but potentially different variable orderings.   \n529 \u2013 For any node $Z$ (corresponding to some variable $S_{Z}$ in $m$ ) in the tree $T$ of some EFG   \n530 in $\\mathtt{m a i d2e f g}(m)$ , it is labelled with an instantiation $\\mu(Z)$ corresponding to the values   \n531 taken by each EFG node on the path from the tree\u2019s root $R$ to $Z$ . Nodes will only exist   \n532 for those paths corresponding to values with non-zero probability according to $m$ . We   \n533 can query the values of the parents of $S_{Z}$ at the node $Z$ via $\\mu(\\dot{Z})[P a_{S_{Z}}]$ . maid2ef $\\mathtt{g}$   \n534 forms information sets by grouping nodes for which this value (and the corresponding   \n535 node $S_{Z}$ in the MAID) is the same.   \n536 \u2013 To form meta-information sets, we simply follow [definition of meta-information sets].   \n537 Letting $\\mathbf{I}_{m}^{i}$ be the information sets for agent $i$ in any EFG in maid2ef ${\\mathsf{g}}(m)$ , we can   \n538 define an equivalence relation $\\sim$ over $\\bar{\\cup}_{m\\in\\mathbf{M}}\\mathbf{I}_{m}^{i}$ such that $I^{1}\\sim I^{2}$ if and only if   \n539 $\\mu(Z_{1})[P a_{S_{Z_{1}}}]=\\mu(Z_{2})[P a_{S_{Z_{2}}}]$ and $d o m(S_{Z_{1}})\\stackrel{\\cdots}{=}d o m(S_{Z_{2}})$ for every $Z_{1}\\in I^{1}$ and   \n540 every $Z_{2}\\ \\in\\ I^{2}$ . Then the set of meta-information sets for player $i$ is the quotient   \n541 set $\\cup_{m\\in\\mathbf{M}}\\mathbf{I}_{m}^{i}/\\sim$ - the set of equivalence classes partitioning $\\bar{\\cup}_{m\\in\\mathbf{M}}\\mathbf{I}_{m}^{i}$ . To match   \n542 notation, for each element of each meta-information set, append the belief $b_{i}(\\omega^{*})$ for   \n543 the appropriate agent $i\\in\\mathbf{N}$ .   \n544 \u2013 Hence, we have a one-to-one mapping between information sets in $\\boldsymbol{S}$ and meta  \n545 information sets (restricted to belief $b_{i}(\\omega^{*})$ for each $\\textit{i}\\in\\textbf{N}$ in $G$ , and action sets   \n546 are preserved under this mapping.   \n548 efg2maid transforms an EFG into an equivalent MAID, as per definition 17 in [[15]]. We are   \n549 interested in transforming a game with incomplete information $\\mathbf{\\boldsymbol{\\bar{G}}}=(\\mathbf{N},S,\\Pi)$ , at interim stage with   \n550 state of the world $\\omega^{*}$ , into an equivalent II-MAID $S=({\\bf N},S^{*},{\\bf S})$ , as per Definition 19. We describe   \n551 such a transformation here, which we call efg2maidII: ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "\u2022 The set of agents N in $\\boldsymbol{S}$ is the same as in $G$ .   \n\u2022 Given belief space $\\Pi\\;=\\;(Y,{\\mathcal{Y}},{\\mathbf{s}},(b_{i})_{i\\in{\\mathbf{N}}})$ , we can map each state of the world $w\\ =$ $(\\mathbf{s}(\\omega),(b_{i}(\\omega))_{i\\in\\mathbf{N}})\\in Y_{.}$ to a subjective MAID $s\\in\\mathbf{S}$ with $g:Y\\to\\mathbf{S}$ , noting that $s$ is of the form $s=(\\mathcal{M}^{S},(P_{i}^{S})_{i\\in\\mathbf{N}})$ . \u2013 $-\\;\\mathcal{M}^{g(\\omega)}:=\\mathsf{e f g2m a i d}(\\mathbf{s}(\\omega)).$ \u2013 $P_{i}^{g(\\omega)}(g(\\omega^{\\prime})):=b_{i}(\\omega^{\\prime}\\mid\\omega)\\;\\mathrm{for}$ all $w^{\\prime}\\in Y$ .   \n\u2022 $S^{*}=g(\\omega^{*})$ .   \n\u2022 Meta-information sets in the game with incomplete information are defined as sets of information sets, across various EFGs, in which nodes has the same action set and the same observations, with observations defined as all information available at a given information set. Since we are at the interim stage of the game, we can restrict our attention to those information sets with belief $b_{i}(\\omega^{*})$ . In the II-MAID resulting from the above operations, the information sets as per Definition 8 correspond one-to-one with those in the game with incomplete information, as they are defined by sets of observation-action set pairs, with observations defined by the values of parents of the given decision variable. efg2maid determines the parents of a decision variable according to those ancestors of nodes in a given intervention set that have the same value in paths to each node. As a result, there is a one-to-one correspondence between meta-information sets in a game with incomplete information, and the resulting II-MAID, and since action sets of decision variable are preserved by efg2maid, strategies can easily be mapped to policies.   \n\u2022 More precisely, we can define a bijection between meta-information sets in $G$ and information sets in $\\boldsymbol{S}$ as follows. Given $\\omega^{*}$ , we denote the meta-information sets in $G$ corresponding to beliefs $b_{i}(\\omega^{*})$ for some agent $i$ as $\\mathbf{I}^{\\omega^{*}}(G)$ . Further, for $I\\in\\mathbf{I}^{\\omega^{*}}(G)$ denote $D(I)$ as the associated action set and $O(\\bar{I})$ the associated observation. $O(I)$ is a potentially empty tuple containing observed values of previous decisions or chance nodes. For any information set $(p,d)\\in\\bar{\\mathbf{I}}(S)$ , where $\\mathtt{e f g2m a i d I I}(G)$ , $p$ is a tuple containing the values of parent nodes, and $d$ is the associated action set. $(p,{\\bar{d}})\\in{\\bf I}({\\bar{S}})$ has the same type as $(O(\\bar{I}),D(I))$ for $I\\in\\mathbf{I}^{\\omega^{*}}(G)$ . Since for any $I,J\\in\\mathbf{I}^{\\omega^{*}}(G)$ , $(O(I),D(I))=(O(J),D(J))\\implies I=J$ , we can construct a bijection $h:\\mathbf{I}^{\\omega^{*}}(G)\\rightarrow\\mathbf{I}({\\mathcal{S}});I\\mapsto(O(I),D(I))$ . We use this construction in the proof of Theorem 20 when converting strategies from one framework to the other. ", "page_idx": 13}, {"type": "text", "text": "552   \n553   \n554   \n555   \n556   \n557   \n558   \n559   \n560   \n561   \n562   \n563   \n564   \n565   \n566   \n567   \n568   \n569   \n570   \n571   \n572   \n573   \n574   \n575   \n576   \n577   \n578   \n579   \n580   \n581   \n582 ", "page_idx": 13}, {"type": "text", "text": "583 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "584 1. Claims   \n585 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n586 paper\u2019s contributions and scope?   \n587 Answer: [Yes]   \n588 Justification: We claim in the abstract to have developed a new framework with a strong   \n589 theoretical connection to dynamic games with incomplete information and no common prior   \n590 over types. We also claim to have proven the existence of equilibria concepts. We have done   \n591 both of these things, along with the rest of the stated achievements.   \n592 Guidelines:   \n593 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n594 made in the paper.   \n595 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n596 contributions made in the paper and important assumptions and limitations. A No or   \n597 NA answer to this question will not be perceived well by the reviewers.   \n598 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n599 much the results can be expected to generalize to other settings.   \n600 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n601 are not attained by the paper.   \n602 2. Limitations   \n603 Question: Does the paper discuss the limitations of the work performed by the authors?   \n604 Answer: [Yes]   \n605 Justification: We discuss limitations in the conclusion.   \n606 Guidelines:   \n607 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n608 the paper has limitations, but those are not discussed in the paper.   \n609 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n610 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n611 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n612 model well-specification, asymptotic approximations only holding locally). The authors   \n613 should reflect on how these assumptions might be violated in practice and what the   \n614 implications would be.   \n615 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n616 only tested on a few datasets or with a few runs. In general, empirical results often   \n617 depend on implicit assumptions, which should be articulated.   \n618 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n619 For example, a facial recognition algorithm may perform poorly when image resolution   \n620 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n621 used reliably to provide closed captions for online lectures because it fails to handle   \n622 technical jargon.   \n623 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n624 and how they scale with dataset size.   \n625 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n626 address problems of privacy and fairness.   \n627 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n628 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n629 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n630 judgment and recognize that individual actions in favor of transparency play an impor  \n631 tant role in developing norms that preserve the integrity of the community. Reviewers   \n632 will be specifically instructed to not penalize honesty concerning limitations.   \n633 3. Theory Assumptions and Proofs ", "page_idx": 14}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 14}, {"type": "text", "text": "40 \u2022 The answer NA means that the paper does not include theoretical results.   \n41 \u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross  \n42 referenced.   \n43 \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n44 \u2022 The proofs can either appear in the main paper or the supplemental material, but if   \n45 they appear in the supplemental material, the authors are encouraged to provide a short   \n46 proof sketch to provide intuition.   \n47 \u2022 Inversely, any informal proof provided in the core of the paper should be complemented   \n48 by formal proofs provided in appendix or supplemental material.   \n49 \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 15}, {"type": "text", "text": "650 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "651 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n652 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n653 of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 15}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 15}, {"type": "text", "text": "Justification: We did not run any experiments. ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 15}, {"type": "text", "text": "688 5. Open access to data and code ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "689 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n690 tions to faithfully reproduce the main experimental results, as described in supplemental   \n691 material?   \n692 Answer: [NA]   \n693 Justification: We did not present any experimental results.   \n694 Guidelines:   \n695 \u2022 The answer NA means that paper does not include experiments requiring code.   \n696 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n697 public/guides/CodeSubmissionPolicy) for more details.   \n698 \u2022 While we encourage the release of code and data, we understand that this might not be   \n699 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n700 including code, unless this is central to the contribution (e.g., for a new open-source   \n701 benchmark).   \n702 \u2022 The instructions should contain the exact command and environment needed to run to   \n703 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n704 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n705 \u2022 The authors should provide instructions on data access and preparation, including how   \n706 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n707 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n708 proposed method and baselines. If only a subset of experiments are reproducible, they   \n709 should state which ones are omitted from the script and why.   \n710 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n711 versions (if applicable).   \n712 \u2022 Providing as much information as possible in supplemental material (appended to the   \n713 paper) is recommended, but including URLs to data and code is permitted.   \n714 6. Experimental Setting/Details   \n715 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n716 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n717 results?   \n718 Answer: [NA]   \n719 Justification: We did not present any experimental results.   \n720 Guidelines:   \n721 \u2022 The answer NA means that the paper does not include experiments.   \n722 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n723 that is necessary to appreciate the results and make sense of them.   \n724 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n725 material.   \n726 7. Experiment Statistical Significance   \n727 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n728 information about the statistical significance of the experiments?   \n729 Answer: [NA]   \n730 Justification: We did not present any experimental results.   \n731 Guidelines:   \n732 \u2022 The answer NA means that the paper does not include experiments.   \n733 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n734 dence intervals, or statistical significance tests, at least for the experiments that support   \n735 the main claims of the paper.   \n736 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n737 example, train/test split, initialization, random drawing of some parameter, or overall   \n738 run with given experimental conditions).   \n739 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n740 call to a library function, bootstrap, etc.)   \n741 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n742 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n743 of the mean.   \n744 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n745 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n746 of Normality of errors is not verified.   \n747 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n748 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n749 error rates).   \n750 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n751 they were calculated and reference the corresponding figures or tables in the text.   \n752 8. Experiments Compute Resources   \n753 Question: For each experiment, does the paper provide sufficient information on the com  \n754 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n755 the experiments?   \n756 Answer: [NA]   \n757 Justification: We did not present any experimental results.   \n758 Guidelines:   \n759 \u2022 The answer NA means that the paper does not include experiments.   \n760 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n761 or cloud provider, including relevant memory and storage.   \n762 \u2022 The paper should provide the amount of compute required for each of the individual   \n763 experimental runs as well as estimate the total compute.   \n764 \u2022 The paper should disclose whether the full research project required more compute   \n765 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n766 didn\u2019t make it into the paper).   \n767 9. Code Of Ethics   \n768 Question: Does the research conducted in the paper conform, in every respect, with the   \n769 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n770 Answer: [Yes]   \n771 Justification: We had no research subjects or data, so no harms were caused in this way. We   \n772 believe our work is very unlikely to have negative societal impact via the listed categories,   \n773 as it is purely a theoretical framework and we did not train any models. Our work does not   \n774 touch on any political issues.   \n775 Guidelines:   \n776 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n777 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n778 deviation from the Code of Ethics.   \n779 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n780 eration due to laws or regulations in their jurisdiction).   \n781 10. Broader Impacts   \n782 Question: Does the paper discuss both potential positive societal impacts and negative   \n783 societal impacts of the work performed?   \n784 Answer: [Yes]   \n785 Justification: We include a few sentences in the conclusion discussing how we feel our paper   \n786 could have positive societal impact via a useful model for understanding the behaviour and   \n787 reasoning of intelligent systems. We feel it is very unlikely that the theory we present here   \n788 could have any negative societal impact.   \n789 Guidelines:   \n790 \u2022 The answer NA means that there is no societal impact of the work performed.   \n791 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n792 impact or why the paper does not address societal impact.   \n793 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n794 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n795 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n796 groups), privacy considerations, and security considerations.   \n797 \u2022 The conference expects that many papers will be foundational research and not tied   \n798 to particular applications, let alone deployments. However, if there is a direct path to   \n799 any negative applications, the authors should point it out. For example, it is legitimate   \n800 to point out that an improvement in the quality of generative models could be used to   \n801 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n802 that a generic algorithm for optimizing neural networks could enable people to train   \n803 models that generate Deepfakes faster.   \n804 \u2022 The authors should consider possible harms that could arise when the technology is   \n805 being used as intended and functioning correctly, harms that could arise when the   \n806 technology is being used as intended but gives incorrect results, and harms following   \n807 from (intentional or unintentional) misuse of the technology.   \n808 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n809 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n810 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n811 feedback over time, improving the efficiency and accessibility of ML).   \n812 11. Safeguards   \n813 Question: Does the paper describe safeguards that have been put in place for responsible   \n814 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n815 image generators, or scraped datasets)?   \n816 Answer: [NA]   \n817 Justification: We do not present any experimental results, use any data, or train any models.   \n818 Guidelines:   \n819 \u2022 The answer NA means that the paper poses no such risks.   \n820 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n821 necessary safeguards to allow for controlled use of the model, for example by requiring   \n822 that users adhere to usage guidelines or restrictions to access the model or implementing   \n823 safety filters.   \n824 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n825 should describe how they avoided releasing unsafe images.   \n826 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n827 not require this, but we encourage authors to take this into account and make a best   \n828 faith effort.   \n829 12. Licenses for existing assets   \n830 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n831 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n832 properly respected?   \n833 Answer: [NA]   \n834 Justification: There are no assets associated with the paper.   \n835 Guidelines:   \n836 \u2022 The answer NA means that the paper does not use existing assets.   \n837 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n838 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n839 URL.   \n840 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n841 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n842 service of that source should be provided. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 19}, {"type": "text", "text": "850   \n851 13. New Assets   \n852 Question: Are new assets introduced in the paper well documented and is the documentation   \n853 provided alongside the assets?   \n854 Answer: [NA]   \n855 Justification: There are no assets associated with the paper.   \n856 Guidelines:   \n857 \u2022 The answer NA means that the paper does not release new assets.   \n858 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n859 submissions via structured templates. This includes details about training, license,   \n860 limitations, etc.   \n861 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n862 asset is used.   \n863 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n864 create an anonymized URL or include an anonymized zip file.   \n865 14. Crowdsourcing and Research with Human Subjects   \n866 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n867 include the full text of instructions given to participants and screenshots, if applicable, as   \n868 well as details about compensation (if any)?   \n869 Answer: [NA]   \n870 Justification: We ran no such experiments.   \n871 Guidelines:   \n872 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n873 human subjects.   \n874 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n875 tion of the paper involves human subjects, then as much detail as possible should be   \n876 included in the main paper.   \n877 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n878 or other labor should be paid at least the minimum wage in the country of the data   \n879 collector. ", "page_idx": 19}, {"type": "text", "text": "80 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 19}, {"type": "text", "text": "882 Question: Does the paper describe potential risks incurred by study participants, whether   \n883 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n884 approvals (or an equivalent approval/review based on the requirements of your country or   \n885 institution) were obtained? ", "page_idx": 19}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. ", "page_idx": 19}, {"type": "text", "text": "\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 20}]