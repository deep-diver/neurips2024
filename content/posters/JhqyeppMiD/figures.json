[{"figure_path": "JhqyeppMiD/figures/figures_1_1.jpg", "caption": "Figure 1: Responses of the clean and poisoned LLaVA-1.5 models in a traditional Label Attack (top) and a novel Persuasion Attack task (bottom), with poisoned samples crafted using a different VLM, MiniGPT-v2.", "description": "This figure shows the results of two different attacks using the Shadowcast method. The top part shows a Label Attack, where the model is tricked into misidentifying Donald Trump as Joe Biden. The bottom part shows a Persuasion Attack, where the model is tricked into describing junk food as healthy.  Both attacks demonstrate that Shadowcast is able to successfully manipulate the responses of the model even with visually similar images.", "section": "1 Introduction"}, {"figure_path": "JhqyeppMiD/figures/figures_3_1.jpg", "caption": "Figure 2: Illustration of Shadowcast crafting a poison sample with visually matching image and text.", "description": "The figure illustrates the Shadowcast data poisoning method.  It shows how a poison sample is created by combining a slightly perturbed version of a destination concept image (visually very similar to the original concept image) with a text caption that clearly describes the destination concept. The perturbed image is created by adding imperceptible noise to a clean destination concept image, making it visually similar to a clean original concept image in the latent feature space. This technique makes the poisoned samples nearly indistinguishable from benign samples, enabling stealthy manipulation of the VLM.", "section": "3.2 Overview of Shadowcast"}, {"figure_path": "JhqyeppMiD/figures/figures_6_1.jpg", "caption": "Figure 3: Attack success rate of Label Attack for LLaVA-1.5.", "description": "This figure shows the attack success rate for two label attack tasks (Trump-to-Biden and EngineLight-to-LowFuelLight) against the LLaVA-1.5 model as a function of the percentage of poisoned training samples.  The x-axis represents the percentage of poisoned training samples, and the y-axis represents the attack success rate. The results show that Shadowcast achieves high attack success rates with a small number of poisoned samples.", "section": "4.2 Attack effectiveness on Label Attack"}, {"figure_path": "JhqyeppMiD/figures/figures_6_2.jpg", "caption": "Figure 4: Attack success rate of Persuasion Attack for LLaVA-1.5.", "description": "This figure shows the attack success rate for two different Persuasion Attack tasks against the LLaVA-1.5 model. The x-axis represents the percentage of poisoned training samples used, and the y-axis shows the attack success rate.  The plot indicates that the attack's success increases as the percentage of poisoned training samples increases.  The two tasks are \"JunkFood-to-HealthyFood\" and \"VideoGame-to-PhysicalHealth\", demonstrating Shadowcast's effectiveness in manipulating model responses to present misleading narratives even for innocuous prompts.", "section": "4.3 Attack effectiveness on Persuasion Attack"}, {"figure_path": "JhqyeppMiD/figures/figures_7_1.jpg", "caption": "Figure 5: Human evaluation results of clean and poisoned models on test images depicting the original concepts.", "description": "The figure shows the results of a human evaluation study assessing the coherence and relevance of responses generated by clean and poisoned vision-language models (VLMs).  The evaluation focused on two tasks: JunkFood-to-HealthyFood and VideoGame-to-PhysicalHealth.  For each task, human evaluators rated the coherence and relevance of the VLM's responses to test images using a 1-5 scale. The x-axis represents the percentage of poisoned training samples used to train the model, while the y-axis shows the average rating for coherence and relevance.  The results indicate that the poisoned models maintain high coherence and relevance, suggesting the effectiveness of the Shadowcast attack in subtly manipulating VLM responses.", "section": "Human evaluation"}, {"figure_path": "JhqyeppMiD/figures/figures_7_2.jpg", "caption": "Figure 6: (Generalizability across prompts) Attack success rates when diverse prompts are used.", "description": "This figure demonstrates the robustness of Shadowcast across various prompts.  It shows that the attack's success rate remains consistent even when different questions are used to query the poisoned Vision Language Models (VLMs) during inference. This highlights the pervasive nature of the attack, as it's not limited to specific phrasings.", "section": "4.4 Attack generalizability"}, {"figure_path": "JhqyeppMiD/figures/figures_8_1.jpg", "caption": "Figure 7: (Architecture transferability) Attack success rate for LLaVA-1.5 when InstructBLIP (left) and MiniGPT-v2 (right) are used to craft poison images.", "description": "This figure shows the attack success rate for LLaVA-1.5 when using poison images generated by two different models: InstructBLIP and MiniGPT-v2.  The left panel displays results when poison samples are created using InstructBLIP, while the right panel shows results when using MiniGPT-v2.  The x-axis represents the percentage of poisoned training samples, and the y-axis represents the attack success rate.  Different colored lines represent the different attack tasks from Table 2 (Trump-to-Biden, EngineLight-to-FuelLight, JunkFood-to-HealthyFood, VideoGame-to-PhysicalHealth).  The figure demonstrates the transferability of Shadowcast across different VLM architectures.", "section": "4.4 Attack generalizability"}, {"figure_path": "JhqyeppMiD/figures/figures_8_2.jpg", "caption": "Figure 8: (Robustness to data augmentation) Attack success rate for LLaVA-1.5 trained with data augmentation, when poison images are crafted without (left) and with (right) augmentation.", "description": "This figure shows the results of experiments evaluating the robustness of Shadowcast against data augmentation.  The left panel shows the attack success rate when poison images are crafted without using data augmentation during the training of the LLaVA-1.5 model. The right panel shows the same metric when the poison images are created using the same data augmentation techniques used in training LLaVA-1.5. The results indicate how effectively Shadowcast performs under various data augmentation strategies.", "section": "4.5 Robustness of the attack"}, {"figure_path": "JhqyeppMiD/figures/figures_8_3.jpg", "caption": "Figure 6: (Generalizability across prompts) Attack success rates when diverse prompts are used.", "description": "This figure shows the robustness of Shadowcast across various prompts.  Even when different phrasing is used to query the poisoned vision-language model, the attack remains highly effective. This highlights the pervasive nature of the attack, as it is not limited to specific phrasing.", "section": "4.4 Attack generalizability"}, {"figure_path": "JhqyeppMiD/figures/figures_13_1.jpg", "caption": "Figure 1: Responses of the clean and poisoned LLaVA-1.5 models in a traditional Label Attack (top) and a novel Persuasion Attack task (bottom), with poisoned samples crafted using a different VLM, MiniGPT-v2.", "description": "The figure shows the results of a Label Attack and a Persuasion Attack on the LLaVA-1.5 model.  The Label Attack successfully misidentifies Donald Trump as Joe Biden. The Persuasion Attack manipulates the model's response to describe junk food as healthy.  Both attacks demonstrate the effectiveness of the Shadowcast data poisoning technique.", "section": "1 Introduction"}, {"figure_path": "JhqyeppMiD/figures/figures_16_1.jpg", "caption": "Figure 1: Responses of the clean and poisoned LLaVA-1.5 models in a traditional Label Attack (top) and a novel Persuasion Attack task (bottom), with poisoned samples crafted using a different VLM, MiniGPT-v2.", "description": "This figure shows the results of a data poisoning attack called Shadowcast on the LLaVA-1.5 vision-language model.  The top half demonstrates a Label Attack, where the model is tricked into misidentifying Donald Trump as Joe Biden. The bottom half shows a Persuasion Attack, where the model is manipulated to present junk food as healthy.  Both attacks use poison samples generated by a different model, MiniGPT-v2, highlighting the transferability of the attack across different model architectures.", "section": "1 Introduction"}, {"figure_path": "JhqyeppMiD/figures/figures_17_1.jpg", "caption": "Figure 1: Responses of the clean and poisoned LLaVA-1.5 models in a traditional Label Attack (top) and a novel Persuasion Attack task (bottom), with poisoned samples crafted using a different VLM, MiniGPT-v2.", "description": "This figure shows the results of a Label Attack and a Persuasion Attack on the LLaVA-1.5 vision-language model.  The top half demonstrates a Label Attack, where poisoned samples cause the model to misidentify Donald Trump as Joe Biden. The bottom half shows a Persuasion Attack, where poisoned samples manipulate the model to describe unhealthy food (a hamburger and fries) as healthy.  The poisoned samples were created using MiniGPT-v2, a different VLM than the one being attacked, highlighting the transferability of the attack.", "section": "1 Introduction"}, {"figure_path": "JhqyeppMiD/figures/figures_18_1.jpg", "caption": "Figure 1: Responses of the clean and poisoned LLaVA-1.5 models in a traditional Label Attack (top) and a novel Persuasion Attack task (bottom), with poisoned samples crafted using a different VLM, MiniGPT-v2.", "description": "This figure shows the results of a Label Attack and a Persuasion Attack on the LLaVA-1.5 model.  The top half demonstrates a Label Attack where the model is tricked into misidentifying Donald Trump as Joe Biden. The bottom half shows a Persuasion Attack, where the model is manipulated to describe junk food as healthy.  Poison samples for both attacks were generated using MiniGPT-v2, highlighting the transferability of the attack across different models.", "section": "1 Introduction"}, {"figure_path": "JhqyeppMiD/figures/figures_18_2.jpg", "caption": "Figure 3: Attack success rate of Label Attack for LLaVA-1.5.", "description": "This figure shows the attack success rate as a function of the proportion of poison samples used for poisoning LLaVA-1.5 on two Label Attack tasks (Trump-to-Biden and EngineLight-to-FuelLight). The results show that Shadowcast achieves a significant impact (over 60% attack success rate) with a poison rate of under 1% (or 30 poison samples).  A poison rate larger than 1.4% (or 50 poison samples) results in successful Label Attack over 95% and 80% of the time for the two tasks respectively. This demonstrates the high efficiency of Shadowcast for Label Attack.", "section": "4.2 Attack effectiveness on Label Attack"}, {"figure_path": "JhqyeppMiD/figures/figures_18_3.jpg", "caption": "Figure 3: Attack success rate of Label Attack for LLaVA-1.5.", "description": "This figure shows the attack success rate of Label Attacks on the LLaVA-1.5 model. The x-axis represents the percentage of poisoned training samples, while the y-axis shows the attack success rate. Two Label Attack tasks are presented: Trump-to-Biden and EngineLight-to-FuelLight.  The results demonstrate a significant increase in attack success rate with a small percentage of poisoned samples (less than 1%). For both tasks,  the attack success rate exceeds 95% when the poison rate is approximately 1.4%. This highlights the efficiency and effectiveness of the Shadowcast attack against label attacks.", "section": "4.2 Attack effectiveness on Label Attack"}, {"figure_path": "JhqyeppMiD/figures/figures_23_1.jpg", "caption": "Figure 1: Responses of the clean and poisoned LLaVA-1.5 models in a traditional Label Attack (top) and a novel Persuasion Attack task (bottom), with poisoned samples crafted using a different VLM, MiniGPT-v2.", "description": "This figure shows the results of a Label Attack and a Persuasion Attack on the LLaVA-1.5 vision-language model.  The top half demonstrates a Label Attack, where the model is tricked into misidentifying Donald Trump as Joe Biden, using a poisoned model trained with images subtly altered to resemble Biden. The bottom half displays a Persuasion Attack, in which the model is manipulated to describe junk food as healthy in its text generation, again using images subtly modified to suggest healthiness.  Poison samples were created using a different VLM (MiniGPT-v2) to highlight the transferability of the attack. This demonstrates Shadowcast's ability to manipulate VLM responses through visually indistinguishable poison samples.", "section": "1 Introduction"}, {"figure_path": "JhqyeppMiD/figures/figures_24_1.jpg", "caption": "Figure 1: Responses of the clean and poisoned LLaVA-1.5 models in a traditional Label Attack (top) and a novel Persuasion Attack task (bottom), with poisoned samples crafted using a different VLM, MiniGPT-v2.", "description": "This figure shows the results of a data poisoning attack called Shadowcast on the LLaVA-1.5 vision-language model.  The top half demonstrates a Label Attack, where the model is tricked into misidentifying Donald Trump as Joe Biden.  The bottom half shows a Persuasion Attack, a novel attack type where the model is manipulated into generating a misleading narrative describing junk food as healthy.  Crucially, the poisoned samples used were created using a different model (MiniGPT-v2), showcasing the transferability and potential real-world impact of Shadowcast across different VLMs.", "section": "1 Introduction"}, {"figure_path": "JhqyeppMiD/figures/figures_25_1.jpg", "caption": "Figure 1: Responses of the clean and poisoned LLaVA-1.5 models in a traditional Label Attack (top) and a novel Persuasion Attack task (bottom), with poisoned samples crafted using a different VLM, MiniGPT-v2.", "description": "This figure shows the results of a data poisoning attack called Shadowcast on the LLaVA-1.5 vision-language model.  The top part demonstrates a Label Attack, where the model misidentifies Donald Trump as Joe Biden. The bottom part shows a Persuasion Attack, where the model generates a misleading description of junk food as healthy.  The poisoned samples were created using a different model, MiniGPT-v2, highlighting the transferability of the attack across different models.", "section": "1 Introduction"}]