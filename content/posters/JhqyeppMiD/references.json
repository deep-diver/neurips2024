{"references": [{"fullname_first_author": "Haotian Liu", "paper_title": "Visual instruction tuning", "publication_date": "2023-04-08", "reason": "This paper introduces a novel visual instruction tuning method that is crucial to adapting VLMs to user-oriented tasks and is directly relevant to the poisoning attacks explored in the main paper."}, {"fullname_first_author": "Deyao Zhu", "paper_title": "MiniGPT-4: Enhancing vision-language understanding with advanced large language models", "publication_date": "2023-04-10", "reason": "This paper introduces MiniGPT-4, a VLM architecture used in the main paper's experiments, making it foundational to the paper's empirical evaluation and findings."}, {"fullname_first_author": "Wenliang Dai", "paper_title": "InstructBLIP: Towards general-purpose vision-language models with instruction tuning", "publication_date": "2023-00-00", "reason": "This paper introduces InstructBLIP, another VLM architecture used in the main paper's experiments for evaluating the robustness and transferability of Shadowcast."}, {"fullname_first_author": "Christoph Schuhmann", "paper_title": "LAION-5B: An open large-scale dataset for training next generation image-text models", "publication_date": "2022-10-08", "reason": "This paper introduces LAION-5B, the dataset used for training the VLMs in the main paper, making it essential to understanding the context of the poisoning attacks."}, {"fullname_first_author": "Jonas Geiping", "paper_title": "What doesn't kill you makes you robust (er): How to adversarially train against data poisoning", "publication_date": "2021-02-13", "reason": "This paper is highly relevant because it explores defense mechanisms against data poisoning, which is a crucial consideration in light of the vulnerability demonstrated by the authors."}]}