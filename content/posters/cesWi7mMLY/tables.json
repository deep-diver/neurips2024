[{"figure_path": "cesWi7mMLY/tables/tables_6_1.jpg", "caption": "Table 1: Comparison of AdaptOD with EnergyOE and COCL on six OOD datasets.", "description": "This table compares the performance of the proposed AdaptOD method against two state-of-the-art (SOTA) methods, EnergyOE and COCL, across six different out-of-distribution (OOD) datasets.  The comparison is done using CIFAR10-LT and CIFAR100-LT as the in-distribution (ID) datasets, showcasing the results in terms of AUC, AP-in, AP-out, and FPR metrics for each OOD dataset. This allows for a comprehensive evaluation of AdaptOD's effectiveness in handling OOD detection in long-tailed recognition scenarios.", "section": "4.2 Empirical Results"}, {"figure_path": "cesWi7mMLY/tables/tables_7_1.jpg", "caption": "Table 1: Comparison of AdaptOD with EnergyOE and COCL on six OOD datasets.", "description": "This table compares the performance of the proposed AdaptOD method against two state-of-the-art (SOTA) methods, EnergyOE and COCL, on six different out-of-distribution (OOD) datasets.  The comparison is done using two different long-tailed image recognition datasets (CIFAR10-LT and CIFAR100-LT) as in-distribution (ID) data.  The table shows the Area Under the Curve (AUC), Average Precision (AP) for in-distribution (AP-in) and out-of-distribution (AP-out) samples, and the False Positive Rate (FPR) at 95% true positive rate.  Higher AUC, AP-in, and AP-out values, and lower FPR values, indicate better performance.", "section": "4.2 Empirical Results"}, {"figure_path": "cesWi7mMLY/tables/tables_7_2.jpg", "caption": "Table 3: Comparison to different TTA-based OOD detection methods.", "description": "This table compares the proposed AdaptOD method with several state-of-the-art (SOTA) test-time adaptation (TTA) methods for out-of-distribution (OOD) detection on CIFAR10-LT and CIFAR100-LT datasets. It evaluates the performance of different methods with and without TTA using metrics such as AUC, AP-in, AP-out, and FPR.  The results show the improvement achieved by incorporating the proposed DODA (Dynamic Outlier Distribution Adaptation) component within various training methods.", "section": "4 Experiments"}, {"figure_path": "cesWi7mMLY/tables/tables_8_1.jpg", "caption": "Table 1: Comparison of AdaptOD with EnergyOE and COCL on six OOD datasets.", "description": "This table compares the performance of the proposed AdaptOD method against two state-of-the-art (SOTA) OOD detection methods, EnergyOE and COCL, across six different OOD datasets.  The comparison uses CIFAR10-LT and CIFAR100-LT as the in-distribution (ID) datasets.  The results are presented in terms of AUC (Area Under the Curve), AP-in (Average Precision for in-distribution), AP-out (Average Precision for out-of-distribution), and FPR (False Positive Rate).  The table aims to demonstrate the superior performance of AdaptOD in handling the challenges of OOD detection, particularly within the context of long-tailed recognition.", "section": "4.2 Empirical Results"}, {"figure_path": "cesWi7mMLY/tables/tables_9_1.jpg", "caption": "Table 5: Ablation study results on CIFAR10-LT, CIFAR100-LT and ImageNet-LT.", "description": "This table presents the ablation study of the AdaptOD model, showing the impact of its components (DODA, DNE-C, and DNE-S) on the overall performance.  It compares the results with a baseline (EnergyOE) and an oracle model that has access to ground truth OOD data during test time. The results are shown for three different long-tailed datasets: CIFAR10-LT, CIFAR100-LT, and ImageNet-LT, using metrics such as AUC, AP-in, AP-out, and FPR.", "section": "4.3 Further Analysis of AdaptOD"}, {"figure_path": "cesWi7mMLY/tables/tables_14_1.jpg", "caption": "Table 1: Comparison of AdaptOD with EnergyOE and COCL on six OOD datasets.", "description": "This table compares the performance of the proposed AdaptOD method against two state-of-the-art (SOTA) methods, EnergyOE and COCL, across six different out-of-distribution (OOD) datasets.  The comparison is done using two different ID datasets (CIFAR10-LT and CIFAR100-LT) and evaluates the methods based on AUC, AP-in, AP-out, and FPR metrics.  This allows for a comprehensive assessment of AdaptOD's effectiveness in various scenarios. ", "section": "4.2 Empirical Results"}, {"figure_path": "cesWi7mMLY/tables/tables_15_1.jpg", "caption": "Table 1: Comparison of AdaptOD with EnergyOE and COCL on six OOD datasets.", "description": "This table compares the performance of the proposed AdaptOD method against two state-of-the-art (SOTA) OOD detection methods, EnergyOE and COCL, across six different OOD datasets.  The comparison uses CIFAR10-LT and CIFAR100-LT as the in-distribution (ID) datasets.  The results are presented in terms of AUC, AP-in, AP-out, and FPR, providing a comprehensive evaluation of each method's ability to distinguish between in-distribution and out-of-distribution samples in a long-tailed recognition setting.", "section": "4.2 Empirical Results"}, {"figure_path": "cesWi7mMLY/tables/tables_17_1.jpg", "caption": "Table 1: Comparison of AdaptOD with EnergyOE and COCL on six OOD datasets.", "description": "This table compares the performance of the proposed AdaptOD method against two state-of-the-art (SOTA) methods, EnergyOE and COCL, across six different out-of-distribution (OOD) datasets.  The comparison uses two long-tailed image recognition datasets, CIFAR10-LT and CIFAR100-LT, as the in-distribution (ID) data.  The results are presented in terms of AUC, AP-in, AP-out, and FPR, providing a comprehensive evaluation of the methods' ability to distinguish between in-distribution and out-of-distribution samples in a long-tailed setting.", "section": "4.2 Empirical Results"}, {"figure_path": "cesWi7mMLY/tables/tables_17_2.jpg", "caption": "Table 8: Comparison results on separating head/tail samples from OOD samples.", "description": "This table presents the comparison results of the baseline EnergyOE [24], previous SOTA model COCL [30], and AdaptOD on CIFAR10-LT and CIFAR100-LT. The evaluation metrics include AUC, AP-in, AP-out, and FPR. The results are averaged over the six OOD datasets in the SC-OOD benchmark. The table demonstrates the effectiveness of AdaptOD in distinguishing OOD data from both head and tail samples.", "section": "4.2 Empirical Results"}, {"figure_path": "cesWi7mMLY/tables/tables_17_3.jpg", "caption": "Table 1: Comparison of AdaptOD with EnergyOE and COCL on six OOD datasets.", "description": "This table compares the performance of the proposed AdaptOD method against two state-of-the-art (SOTA) OOD detection methods, EnergyOE and COCL, across six different OOD datasets.  The comparison uses CIFAR10-LT and CIFAR100-LT as the in-distribution (ID) datasets.  The metrics used for comparison include AUC (Area Under the Curve), AP-in (Average Precision for in-distribution), AP-out (Average Precision for out-of-distribution), and FPR (False Positive Rate). Higher AUC, AP-in, and AP-out values indicate better performance, while a lower FPR value is preferred. The results show that AdaptOD consistently outperforms EnergyOE and COCL on all six datasets across all the evaluation metrics.", "section": "4.2 Empirical Results"}, {"figure_path": "cesWi7mMLY/tables/tables_18_1.jpg", "caption": "Table 1: Comparison of AdaptOD with EnergyOE and COCL on six OOD datasets.", "description": "This table compares the performance of the proposed AdaptOD method against two state-of-the-art (SOTA) methods, EnergyOE and COCL, on six different out-of-distribution (OOD) datasets.  The comparison is done using CIFAR10-LT and CIFAR100-LT as in-distribution (ID) datasets.  The metrics used for comparison are AUC, AP-in, AP-out, and FPR.  The results show AdaptOD's superior performance across all datasets and metrics.", "section": "4.2 Empirical Results"}, {"figure_path": "cesWi7mMLY/tables/tables_18_2.jpg", "caption": "Table 11: Comparison results of model structure among EnergyOE [24], COCL [30], and our approach AdaptOD on CIFAR10-LT.", "description": "This table compares the performance of AdaptOD with two state-of-the-art (SOTA) methods, EnergyOE and COCL, using two different backbone models (ResNet18 and ResNet34) on the CIFAR10-LT dataset.  The metrics used for comparison are AUC, AP-in, AP-out, FPR, and ACC, providing a comprehensive evaluation of the models' ability to detect out-of-distribution (OOD) samples and correctly classify in-distribution (ID) samples, especially within the context of long-tailed datasets.", "section": "4 Experiments"}, {"figure_path": "cesWi7mMLY/tables/tables_18_3.jpg", "caption": "Table 1: Comparison of AdaptOD with EnergyOE and COCL on six OOD datasets.", "description": "This table compares the performance of the proposed AdaptOD method against two state-of-the-art (SOTA) methods, EnergyOE and COCL, across six different out-of-distribution (OOD) datasets.  The comparison is performed using two different ID datasets (CIFAR10-LT and CIFAR100-LT) resulting in twelve different experimental conditions. For each condition, the table shows four evaluation metrics: Area Under the Curve (AUC), Average Precision for in-distribution samples (AP-in), Average Precision for out-of-distribution samples (AP-out), and False Positive Rate (FPR). Higher AUC, AP-in, and AP-out values and a lower FPR indicate better performance.", "section": "4.2 Empirical Results"}, {"figure_path": "cesWi7mMLY/tables/tables_19_1.jpg", "caption": "Table 13: Comparison results of training time (seconds) on CIFAR100-LT.", "description": "This table presents a comparison of the training times, measured in seconds, for three different methods: EnergyOE [24], BERL [4], and the proposed AdaptOD method.  The comparison is performed using two different model architectures: ResNet18 and ResNet34. The table allows for a direct assessment of the computational efficiency of each approach on the CIFAR100-LT dataset.", "section": "D More Experimental Results"}, {"figure_path": "cesWi7mMLY/tables/tables_19_2.jpg", "caption": "Table 1: Comparison of AdaptOD with EnergyOE and COCL on six OOD datasets.", "description": "This table compares the performance of the proposed AdaptOD method against two state-of-the-art (SOTA) OOD detection methods, EnergyOE and COCL, across six different OOD datasets.  The comparison uses CIFAR10-LT and CIFAR100-LT as the in-distribution (ID) datasets.  The metrics used for comparison include AUC, AP-in, AP-out, and FPR. This table helps demonstrate AdaptOD's superior performance in handling out-of-distribution detection in long-tailed recognition scenarios.", "section": "4.2 Empirical Results"}]