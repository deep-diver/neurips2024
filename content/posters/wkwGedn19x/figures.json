[{"figure_path": "wkwGedn19x/figures/figures_1_1.jpg", "caption": "Figure 1: (Left) We demonstrate how modifications to the components enhance the performance of the CRATE model. The four models are trained using the same setup: first pre-trained on ImageNet-21K and then fine-tuned on ImageNet-1K. Details are provided in Section 3. (Right). We compare the FLOPs and accuracy on ImageNet-1K of our methods with ViT [9] and CRATE [46]. The values of CRATE-a model correspond to those presented in Table 1. A more detailed comparison between CRATE-a and ViT is included in Appendix A.2.", "description": "The left panel shows the performance gains obtained by sequentially adding three modifications to the CRATE model (overcomplete dictionary, decoupled dictionary, and residual connection). All models were trained using ImageNet-21K for pre-training and ImageNet-1K for fine-tuning.  The right panel compares the computational cost (FLOPs) and ImageNet-1K accuracy of the proposed CRATE-a models against ViT and the original CRATE model.  CRATE-a achieves higher accuracy with comparable FLOPs.", "section": "1 Introduction"}, {"figure_path": "wkwGedn19x/figures/figures_4_1.jpg", "caption": "Figure 2: One layer of the CRATE-a model architecture. MSSA (Multi-head Subspace Self-Attention, defined in (5)) represents the compression block, and ODL (Overcomplete Dictionary Learning, defined in (12)) represents the sparse coding block. A more detailed illustration of the modifications is provided in Fig. 6 in the Appendix.", "description": "This figure shows a detailed architecture of one layer in the CRATE-a model.  It highlights the two main blocks: the Multi-head Subspace Self-Attention (MSSA) block for compression and the Overcomplete Dictionary Learning (ODL) block for sparse coding.  The ODL block incorporates three key modifications compared to the original CRATE model: an overparameterized sparse coding block, a decoupled dictionary, and a residual connection.  These modifications enhance the scalability and performance of the model.", "section": "3 CRATE-a Model"}, {"figure_path": "wkwGedn19x/figures/figures_5_1.jpg", "caption": "Figure 3: Training loss curves of CRATE-a on ImageNet-21K. (Left) Comparing training loss curves across CRATE-a with different model sizes. (Right) Comparing training loss curves across CRATE-\u03b1-Large with different patch sizes.", "description": "This figure displays the training loss curves for the CRATE-a model across various model sizes (left) and patch sizes (right) during training on the ImageNet-21K dataset.  The left panel showcases how training loss changes as the model scales from Tiny to Huge.  The right panel shows loss curves for the large CRATE-a model, highlighting the impact of altering patch size.  Lower loss values generally indicate better model training progress.", "section": "4 Experiments"}, {"figure_path": "wkwGedn19x/figures/figures_7_1.jpg", "caption": "Figure 4: (Left) Comparing training loss curves of CRATE-a-CLIPA with different model sizes on DataComp1B. (Right) Comparing zero-shot accuracy of CRATE-a-B/L/H models and ViT-H on ImageNet-1K.", "description": "This figure presents two graphs showing the results of scaling experiments of CRATE-a models using the CLIPA framework. The left graph displays the training loss curves for CRATE-a-B/16, CRATE-a-L/14, and CRATE-a-H/14 models trained on the DataComp1B dataset. The right graph shows the zero-shot accuracy on ImageNet-1k for the same CRATE-a models and ViT-H/14, comparing pre-training and fine-tuning results. The figure demonstrates the scalability and effectiveness of the CRATE-a architecture, highlighting the improvements in both training efficiency and zero-shot performance with increased model size.", "section": "4.3 Results and Analysis"}, {"figure_path": "wkwGedn19x/figures/figures_9_1.jpg", "caption": "Figure 5: Visualization of segmentation on COCO val2017 [20] with MaskCut [43]. (Top row) Supervised CRATE-a effectively identifies the main objects in the image. Compared with CRATE (Middle row), CRATE-a achieves better segmentation performance in terms of boundary. (Bottom row) Supervised ViT fails to identify the main objects in most images. We mark failed image with.", "description": "This figure compares the zero-shot image segmentation performance of three different models: CRATE-a, CRATE, and ViT.  The top row shows the results from the CRATE-a model, demonstrating accurate segmentation of objects in various images. The middle row presents the results from the CRATE model, which shows less accurate segmentation, particularly around object boundaries. The bottom row shows that the ViT model struggles to identify the main objects accurately in most of the images. This visualization highlights the superior zero-shot segmentation capabilities of CRATE-a compared to the other two models.", "section": "4.3 Results and Analysis"}, {"figure_path": "wkwGedn19x/figures/figures_15_1.jpg", "caption": "Figure 2: One layer of the CRATE-a model architecture. MSSA (Multi-head Subspace Self-Attention, defined in (5)) represents the compression block, and ODL (Overcomplete Dictionary Learning, defined in (12)) represents the sparse coding block. A more detailed illustration of the modifications is provided in Fig. 6 in the Appendix.", "description": "This figure shows a detailed architecture of one layer in the improved CRATE-a model.  The architecture is comprised of two main blocks: a Multi-head Subspace Self-Attention (MSSA) block for compression and an Overcomplete Dictionary Learning (ODL) block for sparse coding. The ODL block incorporates three key modifications for improved scalability: overparameterization, decoupling of the dictionary, and the addition of a residual connection. These modifications are described further in Section 3 and Figure 6 of the Appendix.", "section": "3 CRATE-a Model"}, {"figure_path": "wkwGedn19x/figures/figures_16_1.jpg", "caption": "Figure 7: We visualize the self-attention maps of the CRATE-a Base model using 8 \u00d7 8 patches trained using classification. Similar to the original CRATE [47], our model also demonstrates the capability to automatically capture the structural information of objects. For each row, the original image is displayed on the left, while the corresponding self-attention maps are shown on the right. The number of self-attention maps corresponds to the number of heads in the CRATE-a model.", "description": "This figure visualizes the self-attention maps of the CRATE-a Base model.  Each row shows an input image (left) and its corresponding self-attention maps (right).  The number of self-attention maps equals the number of heads in the CRATE-a model. The figure highlights that the model successfully captures the structural information within the images, similarly to what was observed in the original CRATE model.", "section": "A Additional Experiments and Details"}, {"figure_path": "wkwGedn19x/figures/figures_16_2.jpg", "caption": "Figure 3: Training loss curves of CRATE-a on ImageNet-21K. (Left) Comparing training loss curves across CRATE-a with different model sizes. (Right) Comparing training loss curves across CRATE-\u03b1-Large with different patch sizes.", "description": "This figure shows the training loss curves for the CRATE-a model trained on the ImageNet-21K dataset. The left panel compares the training loss for different model sizes (Tiny, Small, Base, Large), while the right panel compares the training loss for different patch sizes (8, 14, 32) using the large CRATE-a model. The figure illustrates how training loss changes over steps for various configurations of the model, which impacts the model's performance and scalability.", "section": "4 Experiments"}, {"figure_path": "wkwGedn19x/figures/figures_17_1.jpg", "caption": "Figure 3: Training loss curves of CRATE-a on ImageNet-21K. (Left) Comparing training loss curves across CRATE-a with different model sizes. (Right) Comparing training loss curves across CRATE-\u03b1-Large with different patch sizes.", "description": "This figure displays training loss curves for the CRATE-a model trained on the ImageNet-21K dataset.  The left panel shows how the training loss changes with different model sizes (Tiny, Small, Base, Large), demonstrating the model's ability to scale effectively. The right panel illustrates how the training loss is impacted by changes in the image patch size used in the CRATE-a-Large model, offering insights into the model's sensitivity to patch size.", "section": "4 Experiments"}, {"figure_path": "wkwGedn19x/figures/figures_17_2.jpg", "caption": "Figure 10: Comparing training loss curves when using the efficient scaling strategy. The blue curve corresponds to the CRATE-\u03b1-Large/32 model (in the pre-training stage). After pre-training the CRATE-\u03b1-Lage/32, we further fine-tune it with smaller patch sizes (longer token length), including patch size 14 (orange curve) and patch 8 (green curve).", "description": "This figure shows the training loss curves for three different model configurations using an efficient scaling strategy.  Initially, a large CRATE-\u03b1 model (L/32) is pre-trained.  Then, this model's weights are used to initialize smaller models (L/14 and L/8) that are fine-tuned.  The graph illustrates how the training loss progresses for each of these three models, highlighting the efficiency of using this transfer learning approach for scaling.", "section": "4.4 Compute-efficient Scaling Strategy"}]