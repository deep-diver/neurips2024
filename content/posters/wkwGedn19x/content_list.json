[{"type": "text", "text": "Scaling White-Box Transformers for Vision ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jinrui Yang\u22c61 Xianhang Li\u22c61 Druv Pai2 Yuyin Zhou1 Yi Ma2 Yaodong Yu\u20202 Cihang Xie\u20201 \u22c6equal technique contribution, \u2020equal advising 1UC Santa Cruz 2UC Berkeley ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "CRATE, a white-box transformer architecture designed to learn compressed and sparse representations, offers an intriguing alternative to standard vision transformers (ViTs) due to its inherent mathematical interpretability. Despite extensive investigations into the scaling behaviors of language and vision transformers, the scalability of CRATE remains an open question which this paper aims to address. Specifically, we propose CRATE- $\\alpha$ , featuring strategic yet minimal modifications to the sparse coding block in the CRATE architecture design, and a light training recipe designed to improve the scalability of CRATE. Through extensive experiments, we demonstrate that CRATE- $\\alpha$ can effectively scale with larger model sizes and datasets. For example, our CRATE- $\\cdot\\alpha$ -B substantially outperforms the prior best CRATE-B model accuracy on ImageNet classification by $3.7\\%$ , achieving an accuracy of $83.2\\%$ . Meanwhile, when scaling further, our CRATE- $\\cdot\\alpha$ -L obtains an ImageNet classification accuracy of $85.1\\%$ . More notably, these model performance improvements are achieved while preserving, and potentially even enhancing the interpretability of learned CRATE models, as we demonstrate through showing that the learned token representations of increasingly larger trained CRATE- $\\alpha$ models yield increasingly higher-quality unsupervised object segmentation of images. The project page is https://rayjryang.github.io/CRATE-alpha/. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Over the past several years, the Transformer architecture [42] has dominated deep representation learning for natural language processing (NLP), image processing, and visual computing [8, 2, 9, 5, 12]. However, the design of the Transformer architecture and its many variants remains largely empirical and lacks a rigorous mathematical interpretation. This has largely hindered the development of new Transformer variants with improved efficiency or interpretability. The recent white-box Transformer model CRATE [46] addresses this gap by deriving a simplified Transformer block via unrolled optimization on the so-called sparse rate reduction representation learning objective. ", "page_idx": 0}, {"type": "text", "text": "More specifically, layers of the white-box CRATE architecture are mathematically derived and fully explainable as unrolled gradient descent-like iterations for optimizing the sparse rate reduction. The self-attention blocks of CRATE explicitly conduct compression via denoising features against learned low-dimensional subspaces, and the MLP block is replaced by an incremental sparsification (via ISTA [1, 11]) of the features. As shown in previous work [47], besides mathematical interpretability, the learned CRATE models and features also have much better semantic interpretability than conventional transformers, i.e., visualizing features of an image naturally forms a zero-shot image segmentation of that image, even when the model is only trained on classification. ", "page_idx": 0}, {"type": "text", "text": "Scaling model size is widely regarded as a pathway to improved performance and emergent properties [44, 40, 41, 14]. Until now, the deployment of CRATE has been limited to relatively modest scales. The most extensive model described to date is the base model size encompasses $77.6\\mathrm{M}$ parameters (CRATE-Large) [46]. This contrasts sharply with standard Vision Transformers (ViTs [9]), which have been effectively scaled to a much larger model size, namely 22B parameters [5]. ", "page_idx": 0}, {"type": "image", "img_path": "wkwGedn19x/tmp/29b2783d73132a9e790d4b564cdff0776193a0d057977dbb98ad18dc9b06186c.jpg", "img_caption": ["Figure 1: (Left) We demonstrate how modifications to the components enhance the performance of the CRATE model. The four models are trained using the same setup: first pre-trained on ImageNet-21K and then fine-tuned on ImageNet-1K. Details are provided in Section 3. (Right). We compare the FLOPs and accuracy on ImageNet-1K of our methods with ViT [9] and CRATE [46]. The values of CRATE- $\\alpha$ model correspond to those presented in Table 1. A more detailed comparison between CRATE- $\\cdot\\alpha$ and ViT is included in Appendix A.2. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To this end, this paper provides the first exploration of training CRATE at different scales for vision, i.e., Tiny, Small, Base, Large, Huge. Detailed model specifications are given in Table 7 of Appendix A.1. To achieve effective scaling, we make two key changes. First, we identify the vanilla ISTA block within CRATE as a limiting factor that hinders further scaling. To overcome this, we significantly expand the channels, decouple the association matrix, and add a residual connection, resulting in a new model variant \u2014 CRATE- $\\alpha$ . It is worth noting that this architecture change still preserves the mathematical interpretability of the model. Second, we propose an improved training recipe, inspired by previous work [38, 46, 39], for better coping the training with our new CRATE- $\\cdot\\alpha$ architecture. ", "page_idx": 1}, {"type": "text", "text": "We provide extensive experiments supporting the effective scaling of our CRATE- $\\alpha$ models. For example, we scale the CRATE- $\\cdot\\alpha$ model from Base to Large size for supervised image classification on ImageNet-21K [6], achieving $85.I\\%$ top-1 accuracy on ImageNet- $1K$ at the Large model size. We further scale the model size from Large to Huge, utilizing vision-language pre-training with contrastive learning on DataComp1B [10], and achieve a zero-shot top-1 accuracy of $72.3\\%$ on ImageNet- $I K$ at the Huge model size.1 These results demonstrate the strong scalability of the CRATE- $\\cdot\\alpha$ model, shedding light on scaling up mathematically interpretable models for future work. ", "page_idx": 1}, {"type": "text", "text": "The main contributions of this paper are threefold: ", "page_idx": 1}, {"type": "text", "text": "1. We design three strategic yet minimal modifications for the CRATE model architecture to unleash its potential. In Figure 1, we reproduce the results of the CRATE model within our training setup, initially pre-training on ImageNet-21K classification and subsequently fine-tuning on ImageNet1K classification. Compared to the vanilla CRATE model that achieves $68.5\\%$ top-1 classification accuracy on ImageNet-1K, our CRATE- $\\alpha$ -B/32 model significantly improves the vanilla CRATE model by $8\\%$ , which clearly demonstrates the benefits of the three modifications to the existing CRATE model. Moreover, following the settings of the best CRATE model and changing the image patch size from 32 to 8, our CRATE- $\\cdot\\alpha$ -B model attains a top-1 accuracy of $83.2\\%$ on ImageNet-1K, exceeding the previous best CRATE model\u2019s score of $79.5\\%$ by a significant margin of $3.7\\%$ . ", "page_idx": 1}, {"type": "text", "text": "2. Through extensive experiments, we show that one can effectively scale CRATE- $\\alpha$ via model size and data simultaneously. In contrast, when increasing the CRATE model from Base to Large model size, there is a marginal improvement on top-1 classification accuracy $(+0.5\\%$ , from $70.8\\%$ to $71.3\\%$ ) on ImageNet-1K, indicating diminished returns [46]. Furthermore, by scaling the training dataset, we achieved a substantial $1.9\\%$ improvement in top-1 classification accuracy on ImageNet-1K, increasing from $83.2\\%$ to $85.1\\%$ when going from CRATE- $\\alpha$ Base to Large. ", "page_idx": 1}, {"type": "text", "text": "3. We further successfully scale CRATE- $\\alpha$ model from Large to Huge by leveraging vision-language pre-training on DataComp1B. Compared to the Large model, the Huge model (CRATE- $\\alpha$ -H) achieves a zero-shot top-1 classification accuracy of $72.3\\%$ on ImageNet-1K, marking a significant scaling gain of $2.5\\%$ over the Large model. These results indicate that the CRATE architecture has the potential to serve as an effective backbone for vision-language foundation models. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "White-box Transformers. [46, 45] argued that the quality of a learned representation can be assessed through a unified objective function called the sparse rate reduction. Based on this framework, [46, 45] developed a family of transformer-like deep network architectures, named CRATE, which are mathematically fully interpretable. CRATE models has been demonstrably effective on various tasks, including vision self-supervised learning and language modeling [26, 45]. Nevertheless, it remains unclear whether CRATE can scale as effectively as widely used black-box transformers. Previous work [46] suggests that scaling the vanilla CRATE model can be notably challenging. ", "page_idx": 2}, {"type": "text", "text": "Scaling ViT. ViT [9] represents the initial successful applications of Transformers to the image domain on a large scale. Many works [12, 31, 33, 32, 5, 37, 21, 22, 29, 18, 49] have deeply explored various ways of scaling ViTs in terms of model size and data size. From the perspective of selfsupervision, MAE [12] provides a scalable approach to effectively training a ViT-Huge model using only ImageNet-1K. Following the idea of MAE, [31] further scales both model parameters to billions and data size to billions of images. Additionally, CLIP was the first to successfully scale ViT on a larger data scale (i.e., 400M) using natural language supervision. Based on CLIP, [32, 33] further scale the model size to 18 billion parameters, named EVA-CLIP-18B, achieving consistent performance improvements with the scaling of ViT model size. From the perspective of supervised learning, [49, 5] present a comprehensive analysis of the empirical scaling laws for vision transformers on image classification tasks, sharing some similar conclusions with [15]. [49] suggests that the performancecompute frontier for ViT models, given sufficient training data, tends to follow a saturating power law. More recently, [5] scales up ViT to 22 billion parameters. Scaling up different model architectures is non-trivial. [37, 21, 22] have made many efforts to effectively scale up different architectures. In this paper, due to the lack of study on the scalability of white-box models, we explore key architectural modifications to effectively scale up white-box transformers in the image domain. ", "page_idx": 2}, {"type": "text", "text": "2 Background and Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we present the background on white-box transformers proposed in [46], including representation learning objectives, unrolled optimization, and model architecture. We first introduce the notation that will be used in the later presentation. ", "page_idx": 2}, {"type": "text", "text": "Notation. We use notation and problem setup following Yu et al. [46]. We use the matrix-valued random variable ${\\pmb X}=[{\\pmb x}_{1},\\dots,{\\pmb\\dot{\\boldsymbol x_{N}}}]\\in\\mathbb R^{D\\times N}$ to represent the data, where each $\\mathbf{\\boldsymbol{x}}_{i}\\in\\mathbb{R}^{D}$ is a \u201ctoken\u201d, such that each data point is a realization of $\\mathbf{\\deltaX}$ . For instance, $\\mathbf{\\deltaX}$ can represent a collection of image patches for an image, and $\\pmb{x}_{i}$ is the $i$ -th image patch. We use $f\\in\\mathcal{F}\\colon\\mathbb{R}^{D\\times N}\\to\\mathbb{R}^{d\\times N}$ to denote the mapping induced by the transformer, and we let $Z=f(\\pmb{X})=[z_{1},\\dots,z_{N}]\\in\\mathbb{R}^{d\\times N}$ denote the features for input data $\\mathbf{\\deltaX}$ . Specifically, $z_{i}\\in\\mathbb{R}^{d}$ denotes the feature of the $i$ -th input token $\\pmb{x}_{i}$ . The transformer $f$ consists of multiple, say $L$ , layers, and so can be written as $f=f^{\\dot{L}}\\circ\\cdot\\cdot\\cdot\\circ f^{1}\\mathbin{\\circ}f^{\\mathrm{pre}}$ , where $f^{\\ell}\\colon\\bar{\\mathbb{R}^{d\\times N}}\\to\\mathbb{R}^{d\\times N}$ denotes the $\\ell$ -th layer of the transformer, and the pre-processing layer is denoted by $f^{\\mathrm{pre}}=\\mathbb{R}^{D\\times N}\\rightarrow\\mathbb{R}^{d\\times N}$ . The input to the $\\ell_{}$ -th layer $f^{\\ell}$ of the transformer is denoted $\\begin{array}{r}{Z^{\\ell}=\\left[z_{1}^{\\ell},\\bar{\\dots},z_{N}^{\\ell}\\right]\\in\\mathbb{R}^{d\\times N}}\\end{array}$ , so that $f^{\\ell}\\colon\\mathbf{\\dot{Z}}^{\\ell}\\mapsto Z^{\\ell+1}$ . In particular, $Z^{1}=f^{\\mathrm{pre}}(X)\\in\\mathbb{R}^{d\\times N}$ denotes the output of the pre-processing layer and the input to the first layer. ", "page_idx": 2}, {"type": "text", "text": "2.1 Sparse Rate Reduction ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Following the framework proposed in [45], we posit that the goal of representation learning is to learn a feature mapping or representation $\\boldsymbol{f}^{\"}\\in\\mathcal{F}\\colon\\mathbb{R}^{D\\times N}\\rightarrow\\mathbb{R}^{\\tilde{d}\\times N}$ that transforms the input data $\\mathbf{\\deltaX}$ (which may have a nonlinear, multi-modal, and otherwise complicated distribution) into structured and compact features $_{z}$ , such that the token features lie on a union of low-dimensional subspaces, say with orthonormal bases $U_{[K]}=(U_{k})_{k\\in[K]}\\in(\\mathbb{R}^{d\\times p})^{K}$ . [46] proposes the Sparse Rate Reduction (SRR) objective to measure the goodness of such a learned representation: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{f\\in\\mathcal{F}}\\,\\mathbb{E}_{Z=f(X)}\\left[L_{\\mathrm{srr}}(Z)\\right]=\\operatorname*{min}_{f\\in\\mathcal{F}}\\,\\mathbb{E}_{Z=f(X)}\\left[R^{c}(Z\\,|\\,U_{[K]})-R(Z\\,|\\,U_{[K]})+\\lambda\\|Z\\|_{1}\\right],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $~Z~=~f(X)$ denotes the token representation, $\\|Z\\|_{1}$ denotes the $\\ell^{1}$ norm, and $R(Z)$ , $R^{c}(Z\\,|\\,U_{[K]})$ are (estimates for) rate distortions [4, 7], defined as: ", "page_idx": 3}, {"type": "equation", "text": "$$\nR(Z)\\doteq\\frac{1}{2}\\log\\operatorname*{det}\\left(I+\\frac{d}{N\\epsilon^{2}}Z^{\\top}Z\\right),\\qquad R^{c}(Z\\mid U_{[K]})\\doteq\\sum_{k=1}^{K}R(U_{k}^{\\top}Z).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In particular, $R^{c}(Z\\mid U_{[K]})$ (resp. $R(Z))$ provide closed-form estimates for the number of bits required to encode the sample $_{z}$ up to precision $\\epsilon>0$ , conditioned (resp. unconditioned) on the samples being drawn from the subspaces with bases $U_{[K]}$ . Minimizing the term $R^{c}$ improves the compression of the features $_{z}$ against the posited model, and maximizing the term $R$ promotes non-collapsed features. The remaining term $\\lambda\\|Z\\|_{1}$ promotes sparse features. Refer to [45] for more details about the desiderata and objective of representation learning via the rate reduction approach. ", "page_idx": 3}, {"type": "text", "text": "2.2 CRATE: Coding RATE Transformer ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Unrolled optimization. To optimize the learning objective and learn compact and structured representation, one approach is unrolled optimization [11, 36]: each layer of the deep network implements an iteration of an optimization algorithm on the learning objective. For example, one can design the layer $f^{\\ell}$ such that the forward pass is equivalent to a proximal gradient descent step for optimizing learning objective $L(Z)$ , i.e., $\\pmb{Z}^{\\acute{\\ell}+1}=f^{\\acute{\\ell}}(\\pmb{Z}^{\\ell})=\\mathtt{P r o x}[\\pmb{Z}^{\\ell}-\\eta\\cdot\\mathbf{\\check{V}}_{\\pmb{Z}}L(\\pmb{Z}^{\\ell})]$ . Here we use $\\eta$ to denote the step size and $\\operatorname{Prox}[\\cdot]$ to denote the proximal operator [27]. ", "page_idx": 3}, {"type": "text", "text": "One layer of the CRATE model. We now present the design of each layer of the white-box transformer architecture \u2013 Coding RATE Transformer (CRATE) \u2013 proposed in [46]. Each layer of CRATE contains two blocks: the compression block and the sparsification block. These correspond to a two-step alternating optimization procedure for optimizing the sparse rate reduction objective (1). Specifically, the $\\ell$ -th layer of CRATE is defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{Z}^{\\ell+1}=f^{\\ell}(\\pmb{Z}^{\\ell})=\\mathtt{I S T A}(\\pmb{Z}^{\\ell+1/2}\\,|\\,\\pmb{D}^{\\ell}),\\quad\\mathrm{where}\\quad\\pmb{Z}^{\\ell+1/2}=\\pmb{Z}^{\\ell}+\\mathtt{M S S A}(\\pmb{Z}^{\\ell}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Compression block (MSSA). The compression block in CRATE, called Multi-head Subspace SelfAttention block (MSSA), is derived for compressing the token set $Z=[z_{1},\\ldots,z_{N}]$ by optimizing the compression term $R^{c}$ (defined Eq. (1)), i.e., ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{Z}^{\\ell+1/2}=\\pmb{Z}^{\\ell}+\\mathtt{M S S A}(\\pmb{Z}^{\\ell}\\mid\\pmb{U}_{[K]}^{\\ell})\\approx\\pmb{Z}^{\\ell}-\\kappa\\nabla_{\\pmb{Z}}R^{c}(\\pmb{Z}^{\\ell}\\,|\\,\\pmb{U}_{[K]}^{\\ell}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $U_{[K]}^{\\ell}$ denotes the (local) signal model at layer $\\ell$ , and the MSSA operator is defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathtt{M S S A}(Z\\mid U_{[K]})=\\frac{\\kappa p}{N\\epsilon^{2}}\\left[U_{1}\\cdot\\cdot\\cdot U_{K}\\right]\\left[\\overset{\\left(U_{1}^{\\top}Z\\right)\\operatorname{softmax}((U_{1}^{\\top}Z)^{\\top}(U_{1}^{\\top}Z))}{\\vdots}\\right].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Compared with the commonly used attention block in transformer [42], where the $k$ -th attention head is defined as $(V_{k}^{\\top}Z)$ softmax $((Q_{k}^{\\top}Z)^{\\top}(K_{k}^{\\top}Z))$ , MSSA uses only one matrix to obtain the query, key, and value matrices in the attention: that is, $\\dot{U_{k}}=Q_{k}=K_{k}\\stackrel{\\cdot}{=}V_{k}$ . ", "page_idx": 3}, {"type": "text", "text": "Sparse coding block (ISTA). The Iterative Shrinkage-Thresholding Algorithm (ISTA) block is designed to optimize the sparsity term and the global coding rate term, $\\bar{\\lambda}\\|Z\\|_{0}-R(Z\\mid U_{[K]})$ in (1). [46] shows that an optimization strategy for these terms posits a (complete) incoherent dictionary $\\mathbf{\\bar{D}}^{\\ell}\\in\\mathbb{R}^{d\\times d}$ and takes a proximal gradient descent step towards solving the associated LASSO problem $\\begin{array}{r l}{\\small}&{{}\\arg\\operatorname*{min}_{Z\\geq0}[\\frac{1}{2}\\|\\bar{Z}^{\\ell+1/2}-\\bar{D}^{\\ell}Z\\|_{2}^{2}+\\lambda\\|Z\\|_{1}]}\\end{array}$ , obtaining the iteration ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{Z^{\\ell+1}=\\mathrm{ISTA}\\big(Z^{\\ell+1/2}\\,|\\,D^{\\ell}\\big)=\\mathrm{ReLU}\\big(Z^{\\ell+1/2}+\\eta\\,(D^{\\ell})^{\\top}\\big(Z^{\\ell+1/2}-D^{\\ell}Z^{\\ell+1/2}\\big)-\\eta\\lambda\\mathbf{1}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In particular, the ISTA block sparsifies the intermediate iterates $Z^{\\ell+1/2}$ w.r.t. $D^{\\ell}$ to obtain $Z^{\\ell+1}$ ", "page_idx": 3}, {"type": "text", "text": "3 CRATE- $\\alpha$ Model ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we present the CRATE- $\\alpha$ architecture, which is a variant of CRATE [46]. As shown in Fig. 1 $(R i g h t)$ , there is a significant performance gap between the white-box transformer CRATE-B/16 $(70.8\\%)$ and the vision transformer ViT-B/16 $(84.0\\%)$ [9]. One possible reason is that the ISTA block applies a complete dictionary $D\\in\\mathbb{R}^{d\\times d}$ , which may limit its expressiveness. In contrast, the MLP block in the transformer2 applies two linear transformations $W_{1}$ , $\\mathbf{\\Delta}W_{2}\\in\\mathbb{R}^{d\\times4d}$ , leading to the MLP block having 8 times more parameters than the ISTA block. ", "page_idx": 3}, {"type": "image", "img_path": "wkwGedn19x/tmp/4378015561458ab64dd38173be85609c1d3518e378c73a6c9262eebf6b56ce3b.jpg", "img_caption": ["Figure 2: One layer of the CRATE- $\\alpha$ model architecture. MSSA (Multi-head Subspace Self-Attention, defined in (5)) represents the compression block, and ODL (Overcomplete Dictionary Learning, defined in (12)) represents the sparse coding block. A more detailed illustration of the modifications is provided in Fig. 6 in the Appendix . "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Since the ISTA block in CRATE applies a single incremental step to optimize the sparsity objective, applying an orthogonal dictionary can make it ineffective in sparsifying the token representations. Previous work [28] has theoretically demonstrated that overcomplete dictionary learning enjoys a favorable optimization landscape. In this work, we use an overcomplete dictionary in the sparse coding block to promote sparsity in the features. Specifically, instead of using a complete dictionary $D^{\\ell}\\in\\mathbb{R}^{d\\times d}$ , we use an overcomplete dictionary $D^{\\ell}\\in\\mathbb{R}^{d\\times(C d)}$ , where $C>1$ (a positive integer) is the overcompleteness parameter. Furthermore, we explore two additional modifications to the sparse coding block that lead to improved performance for CRATE. We now describe the three variants of the sparse coding block that we use in this paper. ", "page_idx": 4}, {"type": "text", "text": "Modification #1: Overparameterized sparse coding block. For the output of the $\\ell_{}$ -th CRATE attention block $Z^{\\ell+1/2}$ , we propose to sparsify the token representations with respect to an overcomplete dictionary $D^{\\ell}\\in\\mathbb{R}^{d\\times(C d)}$ by optimizing the following LASSO problem, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\boldsymbol{A}^{\\ell}\\approx\\underset{\\boldsymbol{A}\\geq0}{\\arg\\operatorname*{min}}\\left[\\frac{1}{2}\\|\\boldsymbol{Z}^{\\ell+1/2}-\\boldsymbol{D}^{\\ell}\\boldsymbol{A}\\|_{2}^{2}+\\lambda\\|\\boldsymbol{A}\\|_{1}\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "To approximately solve (7), we apply two steps of proximal gradient descent, i.e., ", "page_idx": 4}, {"type": "equation", "text": "$$\nA_{0}^{\\ell}={\\bf0},\\qquad A_{1}^{\\ell}=\\operatorname{Prox}_{\\eta,\\lambda}\\left[A_{0}^{\\ell};D^{\\ell},Z^{\\ell+1/2}\\right],\\qquad A_{2}^{\\ell}=\\operatorname{Prox}_{\\eta,\\lambda}\\left[A_{1}^{\\ell};D^{\\ell},Z^{\\ell+1/2}\\right],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\tt P r o x$ is the proximal operator of the above non-negative LASSO problem (7) and defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{Prox}_{\\eta,\\lambda}[A;D,Z]=\\operatorname{ReLU}(A-\\eta D^{\\top}(D A-Z)-\\eta\\lambda\\mathbf{1}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The output of the sparse coding block is defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{Z}^{\\ell+1}=\\pmb{D}^{\\ell}\\pmb{A}^{\\ell},\\quad\\mathrm{where}\\quad\\pmb{A}^{\\ell}=\\pmb{A}_{2}^{\\ell}\\doteq\\mathtt{I S T A}-\\mathtt{O C}(\\pmb{Z}^{\\ell+1/2}\\mid\\pmb{D}^{\\ell}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Namely, $A^{\\ell}$ is a sparse representation of $Z^{\\ell+1/2}$ with respect to the overcomplete dictionary $D^{\\ell}$ . The original CRATE ISTA tries to learn a complete dictionary $D\\in\\mathbb{R}^{d\\times d}$ to transform and sparsify the features $Z$ . By leveraging more atoms than the ambient dimension, the overcomplete dictionary $D\\,\\in\\,\\mathbb{R}^{d\\times(C d)}$ can provide a redundant yet expressive codebook to identify the salient sparse structures underlying $Z$ . As shown in Fig. 1, the overcomplete dictionary design leads to $5.3\\%$ improvement compared to the vanilla CRATE model. ", "page_idx": 4}, {"type": "image", "img_path": "wkwGedn19x/tmp/90ba0154ea10eddaff3f873cbb908249f12a1dd9aab76e786a4ed7eef1060a8e.jpg", "img_caption": ["Figure 3: Training loss curves of CRATE- $\\alpha$ on ImageNet-21K. (Left) Comparing training loss curves across CRATE- $\\cdot\\alpha$ with different model sizes. (Right) Comparing training loss curves across CRATE- $\\alpha$ - Large with different patch sizes. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Modification #2: Decoupled dictionary. We propose to apply a decoupled dictionary $\\widehat{D}^{\\ell}$ in the last step (defined in (10) of the sparse coding block, $\\begin{array}{r}{\\bar{Z}^{\\ell+1}=\\bar{\\hat{D^{\\ell}}}\\bar{A^{\\ell}}}\\end{array}$ , where $\\widehat{D}^{\\ell}\\in\\mathbb{R}^{d\\times(\\dot{C}d)}$ is a different dictionary compared to $D^{\\ell}$ . By introducing the decouple d dictionary, w e further improve the model performance by $2.0\\%$ , as shown in Fig. 1. We denote this mapping from $Z^{\\ell+1/2}$ to $Z^{\\ell+1}$ as the Overcomplete Dictionary Learning block (ODL), defined as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n{\\mathsf{O D L}}\\big(Z^{\\ell+1/2}\\mid D^{\\ell},\\widehat{D}^{\\ell}\\big)\\doteq\\widehat{D}^{\\ell}\\cdot{\\mathsf{I S T A}}-{\\mathsf{O C}}\\big(Z^{\\ell+1/2}\\mid D^{\\ell}\\big)=\\widehat{D}^{\\ell}A^{\\ell}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Modification #3: Residual connection. Based on the previous two modifications, we further add a residual connection, obtaining the following modified sparse coding block: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\pmb{Z}^{\\ell+1}=\\pmb{Z}^{\\ell+1/2}+0\\mathrm{DL}(\\pmb{Z}^{\\ell+1/2}\\mid\\pmb{D}^{\\ell},\\widehat{\\pmb{D}}^{\\ell}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "An intuitive interpretation of this modified sparse coding block is as follows: instead of directly sparsifying the feature representations $_{z}$ , we first identify the potential sparse patterns present in $Z$ by encoding it over a learned dictionary. Subsequently, we incrementally refine $Z$ by exploiting the sparse codes obtained from the previous encoding step. From Fig. 1, we find that the residual connection leads to a $0.7\\%$ improvement. ", "page_idx": 5}, {"type": "text", "text": "To summarize, to effectively scale white-box transformers, we implement three modifications to the vanilla white-box CRATE model proposed in [46]. Specifically, in our CRATE- $\\cdot\\alpha$ model, we introduce a decoupling mechanism, quadruple the dimension of the dictionary $(4\\times)$ , and incorporate a residual connection in the sparse coding block. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Overall. The experimental section consists of three parts: (1) Scaling study: We thoroughly investigate the scaling behaviors of CRATE- $\\alpha$ from Base to Large size and ultimately to Huge size. (2) Downstream applications: To further verify the broader beneftis of scaling the CRATE- $\\alpha$ model, we conduct additional experiments on real-world downstream tasks and present preliminary exploration results of CRATE- $\\alpha$ on language tasks. (3) Interpretability: In addition to scalability, we study the interpretability of CRATE- $\\alpha$ across different model sizes. ", "page_idx": 5}, {"type": "text", "text": "4.1 Dataset and Evaluation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Scaling Study. For the transition from Base to Large size, we pre-train our model on ImageNet-21K and fine-tune it on ImageNet-1K via supervised learning. When scaling from Large to Huge, we utilize the DataComp1B [10] dataset within a vision-language pre-training paradigm, allowing us to study the effects of scaling the model to a massive size. For evaluation, we evaluate the zero-shot accuracy of these models on ImageNet-1K. ", "page_idx": 5}, {"type": "text", "text": "Table 1: Top-1 accuracy of CRATE- $\\cdot\\alpha$ on ImageNet-1K with different model scales when pre-trained on ImageNet-21K and then fine-tuned on ImageNet-1K. For comparison, we also list the results from the paper [46] which demonstrate the diminished return from CRATE base to large, trained only on ImageNet-1K. \"IN-21K\" refers to ImageNet-21K. $\\ddag$ Results from [46].) ", "page_idx": 6}, {"type": "table", "img_path": "wkwGedn19x/tmp/cdbda9ecc740a183082f437ea303fbba43b547134d0799a7a0713da0ede14aa1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Downstream Applications. We include additional experimental results on four downstream datasets (CIFAR-10/100, Oxford Flowers, and Oxford-IIT Pets). We also examine the dense prediction capability of CRATE- $\\alpha$ by training it on segmentation tasks using the ADE20K dataset [51]. For language tasks, we conduct new experiments with CRATE- $\\alpha$ using autoregressive training on OpenWebText, following the setup in nanoGPT [16]. ", "page_idx": 6}, {"type": "text", "text": "Interpretability. Following the evaluation setup of CRATE as outlined in [46], we apply MaskCut [43] to validate and evaluate the rich semantic information captured by our model in a zero-shot setting, including both qualitative and quantitative results. ", "page_idx": 6}, {"type": "text", "text": "4.2 Training and Fine-tuning Procedures ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Scaling Study. (1) Base to Large size: We initially pre-train the CRATE- $\\alpha$ model on ImageNet-21K and subsequently fine-tune it on ImageNet-1K. During the pre-training phase, we set the learning rate to $8\\times10^{\\bar{-4}}$ , weight decay to 0.1, and batch size to 4096. We apply data augmentation techniques such as Inception crop [35] resized to 224 and random horizontal flipping. In the fine-tuning phase, we adjust the base learning rate to $1.6\\times10^{-4}$ , maintain weight decay at 0.1, and batch size at 4096. We apply label smoothing with a smoothing parameter of 0.1 and apply data augmentation methods including Inception crop, random horizontal flipping, and random augmentation with two transformations (magnitude of 9). For evaluation, we resize the smaller side of an image to 256 while maintaining the original aspect ratio and then crop the central portion to $224\\!\\times\\!224$ . In both the pre-training and fine-tuning phases, we use the AdamW optimizer [24] and incorporate a warm-up strategy, characterized by a linear increase over 10 epochs. Both the pre-training and fine-tuning are conducted for a total of 91 epochs, utilizing a cosine decay schedule. ", "page_idx": 6}, {"type": "text", "text": "(2) Large to Huge size: In the pre-training stage, we utilize an image size of $84\\!\\times\\!84$ , and the maximum token length is 32, with a total of 2.56 billion training samples. During the fine-tuning stage, we increase the image size to $224\\!\\times\\!224$ while maintaining the maximum token length at 32, with a 512 million training samples. Here, the key distinction between the pre-training stage and the fine-tuning stage is the image size. A smaller image size results in a faster training speed. In the configurations of CRATE- $\\alpha$ -CLIPA-B, CRATE- $\\alpha$ -CLIPA-L, and CRATE- $\\alpha$ -CLIPA-H, we use the CRATE- $\\alpha$ model as the vision encoder, and utilize the same pre-trained huge transformer model from CLIPA [18] as the text encoder. For both the pre-training and fine-tuning stages, we freeze the text encoder and only train the vision encoder, i.e., the CRATE- $\\alpha$ model. As we will show in the later results, this setup effectively demonstrates the scaling behaviors of CRATE- $\\cdot\\alpha$ models in the image domain. Detailed hyperparameter settings can be found in Appendix A. ", "page_idx": 6}, {"type": "text", "text": "Downstream Applications. On four downstream datasets, we follow the training setup from [46]. For the segmentation task, we compare the performance of CRATE and CRATE- $\\alpha$ on the ADE20K dataset, mainly following the setup of [30] with minor modifications. Our batch size is set to 128, and the total number of training steps is 5000. For the language task, we conduct experiments with CRATE- $\\alpha$ using autoregressive training on OpenWebText, following the setup in [16]. We compare CRATE- $\\alpha$ models (57M and 120M) with CRATE and GPT-2, using results from CRATE reported in [45]. ", "page_idx": 6}, {"type": "image", "img_path": "wkwGedn19x/tmp/13b63e61013cbe0531be2e61d49700032bc5ed6bed7264cef656d48f2c6204e0.jpg", "img_caption": ["Figure 4: (Left) Comparing training loss curves of CRATE- $\\alpha$ -CLIPA with different model sizes on DataComp1B. (Right) Comparing zero-shot accuracy of CRATE- $\\alpha$ -B/L/H models and ViT-H on ImageNet-1K. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.3 Results and Analysis ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Scaling the CRATE- $\\alpha$ Model from Base to Large. As shown in Table 1, we compare CRATE- $\\alpha$ -B and CRATE- $\\cdot\\alpha$ -L at patch sizes 32, 16, and 8. Firstly, we find our proposed CRATE- $\\cdot\\alpha$ -L consistently achieves significant improvements across all patch sizes. Secondly, compared with the results of the vanilla CRATE (the first row of Table 1), increasing from CRATE-B to CRATE-L results in only a $0.5\\%$ improvement on ImageNet-1K. This indicates a case of diminishing returns. These findings compellingly highlight that the scalability of CRATE- $\\cdot\\alpha$ models significantly outperforms that of the vanilla CRATE. Meanwhile, the training loss in the pre-training stage is presented in Fig. 3; as the model capacity increases, the trend of the training loss improves predictably. This phenomenon is also described in [9]. ", "page_idx": 7}, {"type": "text", "text": "Scaling the CRATE- $\\alpha$ Model from Large to Huge. From the results shown in Fig. 4, we find that: (1) CRATE- $\\alpha$ -CLIPA-L/14 significantly outperforms CRATE- $\\cdot\\alpha$ -CLIPA-B/16 by $11.3\\%$ and $9.0\\%$ in terms of ImageNet-1K zero-shot accuracy during the pre-training and fine-tuning stages, respectively. The substantial benefti suggests that the quality of learned representation may be constrained by the model size. Therefore, increasing the model size effectively leverages larger amounts of data. (2) When continuing to scale up model size, we also observe that CRATE- $\\alpha$ -CLIP-H/14 continues to benefti from larger training datasets, outperforming CRATE- $\\alpha$ -CLIP-L/14 by $3.1\\%$ and $2.5\\%$ in terms of ImageNet-1K zero-shot accuracy during the pre-training and fine-tuning stages, respectively. This demonstrates the strong scalability of the CRATE- $\\cdot\\alpha$ model. To explore the performance ceiling, we train a standard ViT-CLIPA-H/14 from scratch and observe improved performance. ", "page_idx": 7}, {"type": "text", "text": "Downstream Applications. On four downstream datasets, as shown in Table 2, we find that CRATE- $\\alpha$ consistently outperforms CRATE, with both models pre-trained on IN21K, while CRATE- $\\alpha$ demonstrates improved performance as model size increases. For the segmentation task, results in Table 3 show that CRATE- $\\alpha$ consistently outperforms CRATE across all key metrics, with both models pre-trained on IN21K. These findings indicate significant performance gains in vision tasks beyond classification. For the language task, Table 4 shows that CRATE- $\\alpha$ significantly improves over CRATE in language modeling. Due to limited time and resource constraints, we completed $80\\%$ of the total iterations for CRATE- $\\alpha$ -small and $55\\%$ for CRATE- $\\alpha$ -base, compared to the 600K total iterations used for CRATE. Nevertheless, CRATE- $\\cdot\\alpha$ still demonstrated notable improvements. ", "page_idx": 7}, {"type": "text", "text": "Interpretability. As shown in Fig. 5, we provide the segmentation visualization on COCO val2017 [20] for CRATE- $\\cdot\\alpha$ , CRATE, and ViT, respectively. We find that our model preserves and even improves the (semantic) interpretability advantages of CRATE. Moreover, we summarize quantitative evaluation results on COCO val2017 in Table 6. Interestingly, when scaling up model size for CRATE- $\\alpha$ , the Large model improves over the Base model in terms of object detection and segmentation. ", "page_idx": 7}, {"type": "text", "text": "4.4 Compute-efficient Scaling Strategy ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We further explore methods to scale models efficiently in terms of computation. Table 1 demonstrates that the CRATE- $\\alpha$ model scales effectively from the Base model to its larger variants. However, the pre-training computation for the top-performing model, CRATE- $\\alpha$ -L/8, is resource-intensive on ", "page_idx": 7}, {"type": "table", "img_path": "wkwGedn19x/tmp/95477c92656cbe19171f92400aecbabe4e795974b6825d496e1a69bd3b96c6cb.jpg", "table_caption": ["Table 2: The performance comparison between CRATE and CRATE- $\\cdot\\alpha$ across various datasets. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "wkwGedn19x/tmp/a3ef1dbf7917f495c3c9d8461da1ac7be1394064be39d12f45fbe0385392ee28.jpg", "table_caption": ["Table 3: Performance comparison of CRATE models with different configurations. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "wkwGedn19x/tmp/54f317c8dd34217f7cda6e4a595726d1c7d010fb2f1a46cba8c5255fb1cd2276.jpg", "table_caption": ["Table 4: The comparison between CRATE and CRATE- $\\cdot\\alpha$ on the NLP task using the OpenWebText dataset. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "wkwGedn19x/tmp/fbcf99eae47757fd950906970cf452b299081ec0c2bc2bd4beea6db767c5b48f.jpg", "table_caption": ["Table 5: Compute-efficient scaling strategy. To reduce the compute requirements of the pre-training stage, we use a model with a larger patch size. This results in a shorter token length for the same input size. The second and fourth columns indicate the compute requirements for the pre-training and fine-tuning stages, respectively, measured in TPU v3 core-hours. Details are provided in Section 4.4. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "ImageNet-21K. Inspired by CLIPA [18], we aim to reduce computational demands by using reduced image token sequence lengths, while maintaining the same training setup during the fine-tuning stage. The results are summarized in Table 5. ", "page_idx": 8}, {"type": "text", "text": "Results and analysis. (1) When fine-tuning with CRATE- $\\alpha{\\mathrm{-}}{\\mathrm{L}}/14$ and using CRATE- $\\alpha$ -L/32 for pretraining on ImageNet-21K, this approach consumes about $35\\%$ of the TPU v3 core-hours required by CRATE- $\\alpha$ -L/14, yet achieves a promising $83.7\\%$ top-1 accuracy on ImageNet-1K, comparable to the $83.9\\%$ achieved by CRATE- $\\alpha$ -L/14; (2) When fine-tuning with CRATE- $\\alpha$ -L/8 and using CRATE- $\\alpha$ -L/32 for pre-training, this approach consumes just $15\\%$ of the training time required by CRATE- $\\alpha$ -L/8, yet it still achieves a promising $84.2\\%$ top-1 accuracy on ImageNet-1K, compared to $85.1\\%$ when using the CRATE- $\\alpha$ -L/8 model in the pre-training stage; (3) While the total computational cost of CRATE- $\\alpha$ -L/32 + CRATE- $\\cdot\\alpha$ -L/8 is less than that of CRATE- $\\alpha{-}\\mathrm{L}/14+\\mathrm{CRATE{-}}\\alpha{-}\\mathrm{L}/14$ , the performance of the former is slightly better. In summary, we find that this strategy offers a valuable reference for efficiently scaling CRATE- $\\alpha$ models in the future. ", "page_idx": 8}, {"type": "text", "text": "5 Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Limitations. Although we have used some existing compute-efficient training methods (e.g., CLIPA [18]) and have initiated an exploration into compute-efficient scaling strategies for whitebox transformers in Section 4.4, this work still requires a relatively large amount of computational resources, which may not be easily accessible to many researchers. ", "page_idx": 8}, {"type": "text", "text": "Societal impact. A possible broader implication of this research is the energy consumption needed to conduct the experiments in our scaling study. However, there is growing interest in developing white-box transformers for better interpretability and transparency across a wide range of tasks and domains, including image segmentation [46], self-supervised masked autoencoders [26], and integrated sensing and communications [50], etc. Moreover, our results on the scalability of whitebox transformers could also shed light on scaling up a broader class of white-box deep neural networks, such as white-box ISTA networks and their variants [11, 34, 3, 48, 17], designed via unrolled optimization. In summary, we believe that our findings and insights could be helpful for developing white-box transformers for a wide range of applications and tasks, benefiting a broad audience interested in building more interpretable and performant deep learning models and further amortizing the pre-training compute costs. ", "page_idx": 8}, {"type": "image", "img_path": "wkwGedn19x/tmp/64645bf26a276aeccf8e9835ae9d20be4e0bea78c6c89862b9ed7f4a5e526960.jpg", "img_caption": ["Figure 5: Visualization of segmentation on COCO val2017 [20] with MaskCut [43]. (Top row) Supervised CRATE- $\\alpha$ effectively identifies the main objects in the image. Compared with CRATE (Middle row), CRATE- $\\cdot\\alpha$ achieves better segmentation performance in terms of boundary. (Bottom row) Supervised ViT fails to identify the main objects in most images. We mark failed image with . "], "img_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "wkwGedn19x/tmp/49b66d4baaefbcb6f3b681d62b5b68d6b6248e3968cfede9cc742d7736cf8b58.jpg", "table_caption": ["Table 6: Object detection and fine-grained segmentation via MaskCut on COCO val2017 [20]. We evaluate models of various scales and assess their average precision using COCO\u2019s official evaluation metric. Compared with existing models such as CRATE and ViT, CRATE- $\\alpha$ model achieves a notable performance gain. In addition, when scaling CRATE- $\\alpha$ from base to large, it also exhibits the benefit of scalability. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper provides the first exploration of training white-box transformer CRATE at scale for vision tasks. We introduce both principled architectural changes and improved training recipes to unleash the potential scalability of the CRATE type architectures. With these modifications, we successfully scale up the CRATE- $\\alpha$ model along both the dimensions of model size and data size, while preserving, in most cases even improving, the semantic interpretability of the learned white-box transformer models. We believe this work provides valuable insights into scaling up mathematically interpretable deep neural networks, not limited to transformer-like architectures. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work is supported by a gift from Open Philanthropy, TPU Research Cloud (TRC) program, and Google Cloud Research Credits program. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Thomas Blumensath and Mike E Davies. Iterative thresholding for sparse approximations. Journal of Fourier analysis and Applications, 14:629\u2013654, 2008. PF ", "page_idx": 10}, {"type": "text", "text": "[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.   \n[3] Xiaohan Chen, Jialin Liu, Zhangyang Wang, and Wotao Yin. Theoretical linear convergence of unfolded ista and its practical weights and thresholds. Advances in Neural Information Processing Systems, 31, 2018.   \n[4] Thomas M Cover. Elements of information theory. John Wiley & Sons, 1999.   \n[5] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Scaling vision transformers to 22 billion parameters. In International Conference on Machine Learning, pages 7480\u20137512. PMLR, 2023.   \n[6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A largescale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009.   \n[7] Harm Derksen, Yi Ma, Wei Hong, and John Wright. Segmentation of multivariate mixed data via lossy coding and compression. In Visual Communications and Image Processing 2007, volume 6508, pages 170\u2013181. SPIE, 2007.   \n[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.   \n[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.   \n[10] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the next generation of multimodal datasets. Advances in Neural Information Processing Systems, 36, 2024.   \n[11] Karol Gregor and Yann LeCun. Learning fast approximations of sparse coding. In Proceedings of the 27th international conference on international conference on machine learning, pages 399\u2013406, 2010.   \n[12] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16000\u201316009, 2022.   \n[13] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Openclip. July 2021.   \n[14] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.   \n[15] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.   \n[16] Andrej Karpathy. nanogpt, 2022. URL https://github.com/karpathy/nanoGPT. GitHub repository.   \n[17] Mingyang Li, Pengyuan Zhai, Shengbang Tong, Xingjian Gao, Shao-Lun Huang, Zhihui Zhu, Chong You, Yi Ma, et al. Revisiting sparse convolutional model for visual recognition. Advances in Neural Information Processing Systems, 35:10492\u201310504, 2022.   \n[18] Xianhang Li, Zeyu Wang, and Cihang Xie. An inverse scaling law for clip training. Advances in Neural Information Processing Systems, 36, 2024.   \n[19] Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichtenhofer, and Kaiming He. Scaling language-image pre-training via masking. In CVPR, 2023.   \n[20] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740\u2013755. Springer, 2014.   \n[21] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 10012\u201310022, 2021.   \n[22] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11976\u201311986, 2022.   \n[23] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. In ICLR, 2017.   \n[24] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.   \n[25] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2018.   \n[26] Druv Pai, Ziyang Wu, Sam Buchanan, Tianzhe Chu, Yaodong Yu, and Yi Ma. Masked completion via structured diffusion with white-box transformers. In Conference on Parsimony and Learning (Recent Spotlight Track), 2023.   \n[27] Neal Parikh, Stephen Boyd, et al. Proximal algorithms. Foundations and trends\u00ae in Optimization, 1(3):127\u2013239, 2014.   \n[28] Qing Qu, Yuexiang Zhai, Xiao Li, Yuqian Zhang, and Zhihui Zhu. Geometric analysis of nonconvex optimization landscapes for overcomplete learning. In International Conference on Learning Representations, 2019.   \n[29] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[30] Sucheng Ren, Fangyun Wei, Zheng Zhang, and Han Hu. Tinymim: An empirical study of distilling mim pre-trained models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3687\u20133697, 2023.   \n[31] Mannat Singh, Quentin Duval, Kalyan Vasudev Alwala, Haoqi Fan, Vaibhav Aggarwal, Aaron Adcock, Armand Joulin, Piotr Doll\u00e1r, Christoph Feichtenhofer, Ross Girshick, et al. The effectiveness of mae pre-pretraining for billion-scale pretraining. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5484\u20135494, 2023.   \n[32] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. Eva-clip: Improved training techniques for clip at scale. arXiv preprint arXiv:2303.15389, 2023.   \n[33] Quan Sun, Jinsheng Wang, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, and Xinlong Wang. Eva-clip-18b: Scaling clip to 18 billion parameters. arXiv preprint arXiv:2402.04252, 2024.   \n[34] Xiaoxia Sun, Nasser M Nasrabadi, and Trac D Tran. Supervised deep sparse coding networks. In 2018 25th IEEE International Conference on Image Processing (ICIP), pages 346\u2013350. IEEE, 2018.   \n[35] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1\u20139, 2015.   \n[36] Bahareh Tolooshams and Demba Ba. Stable and interpretable unrolled dictionary learning. arXiv preprint arXiv:2106.00058, 2021.   \n[37] Ilya O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, et al. Mlp-mixer: An all-mlp architecture for vision. Advances in neural information processing systems, 34: 24261\u201324272, 2021.   \n[38] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through attention. In International conference on machine learning, pages 10347\u201310357. PMLR, 2021.   \n[39] Hugo Touvron, Matthieu Cord, and Herv\u00e9 J\u00e9gou. Deit iii: Revenge of the vit. In European conference on computer vision, pages 516\u2013533. Springer, 2022.   \n[40] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.   \n[41] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \n[42] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \n[43] Xudong Wang, Rohit Girdhar, Stella X Yu, and Ishan Misra. Cut and learn for unsupervised object detection and instance segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 3124\u20133134, 2023.   \n[44] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. Transactions on Machine Learning Research, 2022.   \n[45] Yaodong Yu, Sam Buchanan, Druv Pai, Tianzhe Chu, Ziyang Wu, Shengbang Tong, Hao Bai, Yuexiang Zhai, Benjamin D Haeffele, and Yi Ma. White-box transformers via sparse rate reduction: Compression is all there is? arXiv preprint arXiv:2311.13110, 2023.   \n[46] Yaodong Yu, Sam Buchanan, Druv Pai, Tianzhe Chu, Ziyang Wu, Shengbang Tong, Benjamin Haeffele, and Yi Ma. White-box transformers via sparse rate reduction. Advances in Neural Information Processing Systems, 36, 2023.   \n[47] Yaodong Yu, Tianzhe Chu, Shengbang Tong, Ziyang Wu, Druv Pai, Sam Buchanan, and Yi Ma. Emergence of segmentation with minimalistic white-box transformers. In Conference on Parsimony and Learning, pages 72\u201393. PMLR, 2024.   \n[48] John Zarka, Louis Thiry, Tom\u00e1s Angles, and St\u00e9phane Mallat. Deep network classification by scattering and homotopy dictionary learning. arXiv preprint arXiv:1910.03561, 2019.   \n[49] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12104\u201312113, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "[50] Bowen Zhang and Geoffrey Ye Li. White-box 3d-omp-transformer for isac. arXiv preprint arXiv:2407.02251, 2024. ", "page_idx": 13}, {"type": "text", "text": "[51] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ade20k dataset. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 633\u2013641, 2017. ", "page_idx": 13}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Additional Experiments and Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Model configuration. ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We provide details about CRATE- $\\alpha$ model configurations in Table 7. ", "page_idx": 14}, {"type": "text", "text": "Table 7: Model configurations for different sizes of CRATE- $\\cdot\\alpha$ , parameter counts, and comparisons to CRATE models. $L$ is depth, $d$ is the hidden size, and $K$ is the number of heads. ", "page_idx": 14}, {"type": "table", "img_path": "wkwGedn19x/tmp/a7668961ddae7280308664aa058bba9c069019d1e6eac51eee0b959c02a8bc60.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "wkwGedn19x/tmp/4b2aab29f31593435a71f518a590ab67c8dab8c7094be0fb5858bbf256703b77.jpg", "table_caption": ["Table 8: The comparison between CRATE- $\\alpha$ and ViT. FLOPs and throughput are calculated based on an input size of $224\\mathrm{x}224$ on an NVIDIA RTX A6000 graphics card. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "A.2 Comparison of model structure with ViT. ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We also compare CRATE- $\\alpha$ to ViT in terms of computational costs, number of parameters, and inference speed. These comparisons are summarized in Table 8, where CRATE- $\\cdot\\alpha$ matches ViT\u2019s efficiency while achieving similar accuracy. With the same number of layers and embedding dimensions, CRATE- $\\cdot\\alpha$ has fewer parameters than ViT, and its FLOPs/Throughput is slightly higher. ", "page_idx": 14}, {"type": "text", "text": "To more accurately compare CRATE- $\\alpha$ and ViT with larger model sizes, we conduct experiments on CRATE- $\\alpha$ -L/16 with an image resolution of 336, nearly matching the setup of ViT-L/16. Both models use a similar amount of FLOPs: 210G for CRATE- $\\alpha$ -L/16 compared to 191G for ViT-L/16. The throughput, or images processed per second, is also comparable at 35.53 for our model versus 35.56 for ViT-L/16. The accuracy of CRATE- $\\alpha$ -L/16 reach $84.6\\%$ , closely approaching ViT\u2019s $85.2\\%$ under similar conditions. Meanwhile, combining the trend from Figure 1 (right) in the main paper, this narrowing performance gap from Base to Large model size suggests that CRATE- $\\alpha$ can nearly matche ViT\u2019s performance in large-scale settings. Besides, CRATE- $\\alpha$ inherits the mathematical interpretability of the white-box models and can also achieve much better semantic interpretability evaluated by zero-shot segmentation. ", "page_idx": 14}, {"type": "text", "text": "A.3 Training details of CRATE- $\\alpha$ -CLIPA models. ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "When employing the CRATE- $\\alpha$ architecture to replace the vision encoder in the CLIPA [18] framework, we essentially follow the original CLIPA training recipe. The setup for the pre-training stage is presented in Table 9. During the fine-tuning stage, we made some modifications: the input image size is set to $224\\times224$ , the warmup steps are set to 800, and the base learning rate is set to 4e-7. When calculating the loss, we use the classification token from the vision encoder as the image feature and the last token from the text encoder as the text feature. ", "page_idx": 14}, {"type": "text", "text": "To explore the performance ceiling, we also train a ViT-CLIPA model from scratch. Most of the hyperparameters remain the same as those in Table 9, but there are some modifications in the pretraining stage. The batch size is set to 65,536, and the text length is set to 8 to speed up training. As with the CLIPA setup, warm-up steps are set to 3,200. Additionally, we add color jitter and grayscale augmentation, and use global average pooling instead of the classification token. These modifications help stabilize training. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "table", "img_path": "wkwGedn19x/tmp/5be2e35f99e9bcb0e5e01bea9eef53990f5f24f5e4b2a7b206499a3960631520.jpg", "table_caption": ["Table 9: Pre-training hyper-parameters for CLIPA. "], "table_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "wkwGedn19x/tmp/90bec483008113eddb316e26198423ac7623981f57e71e925f0b472a4e613f2b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Figure 6: One layer of the CRATE- $\\alpha$ model architecture (with more details for the three modifications described in Section 3. ", "page_idx": 15}, {"type": "text", "text": "Visualization of self-attention maps of CRATE- $\\alpha$ . We provide visualization of attention maps of CRATE- $\\cdot\\alpha$ in Fig. 7. ", "page_idx": 16}, {"type": "image", "img_path": "wkwGedn19x/tmp/7cd94f440ea777b6f8ba4c14f3433a57d9dc57e84e5c12274862220819f5490d.jpg", "img_caption": ["Figure 7: We visualize the self-attention maps of the CRATE- $\\alpha$ Base model using $8\\times8$ patches trained using classification. Similar to the original CRATE [47], our model also demonstrates the capability to automatically capture the structural information of objects. For each row, the original image is displayed on the left, while the corresponding self-attention maps are shown on the right. The number of self-attention maps corresponds to the number of heads in the CRATE- $\\alpha$ model. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Visualization of loss curves. We visualize the training loss curves of the four models, including CRATE and its three variants, in Fig. 8. We visualize the training loss curves of CRATE- $\\alpha$ -Base with different patch sizes in Fig. 9. In Fig. 10, we also visualize the training loss curves of models trained with efficient scaling strategy described in Section 4.4 in the main paper. ", "page_idx": 16}, {"type": "image", "img_path": "wkwGedn19x/tmp/bc22b1862ced122814e81c61fe467c53d58ca35fbc6120bbc0122aba916c9812.jpg", "img_caption": ["Figure 8: Training loss curves of different model architectures (mentioned in Fig. 1 in the main paper) on ImageNet-21K. The patch size is 32 for all four models shown in this figure. ( $^{+0}$ : $^+$ overcomplete dictionary, $+\\mathrm{D}$ : $^+$ decoupled dictionary, $+\\mathbf{R}$ : +residual connection.) "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "wkwGedn19x/tmp/212cbd2a1f7783e0047b6a2038793a8d5384b489522c0ce768f3b68c8818a6c5.jpg", "img_caption": ["Figure 9: Comparing training loss curves across CRATE- $\\alpha$ -Base with different patch sizes. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "wkwGedn19x/tmp/2c7c7b156f1cebd0d98f0d44631a1745a77d0e254c09cfef5beecf46b2725926.jpg", "img_caption": ["Figure 10: Comparing training loss curves when using the efficient scaling strategy. The blue curve corresponds to the CRATE- $\\cdot\\alpha$ -Large/32 model (in the pre-training stage). After pre-training the CRATE- $\\alpha$ -Lage/32, we further fine-tune it with smaller patch sizes (longer token length), including patch size 14 (orange curve) and patch 8 (green curve). "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 18}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 18}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 18}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 18}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 18}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 18}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 18}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: The abstract and introduction clearly outline the primary contributions and scope of the paper. For specific details, refer to abstract and Sections 1, which align with the claims stated. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 18}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: We discuss the limitations of the work in section 5. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 19}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: This paper does not include theoretical results. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 19}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We provide details of reproducing the experimental results in Section 4 and the appendix. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 20}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 20}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: We use public datasets and will also release the code. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 20}, {"type": "text", "text": "\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 21}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We provide details of reproducing the experimental results in Section 4 and the appendix. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 21}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We evaluate our method on a large public validation dataset, which is representative. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 21}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We provide details about the training compute resources in Section 4. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 21}, {"type": "text", "text": "\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 22}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: we follow the NeurIPS Code of Ethics. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 22}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: we discuss them in Section 5. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 22}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: Our work has no potential risk for misuse. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 23}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: All assets used in the paper are properly credited. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 23}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: we haven\u2019t introduced new assets in this work. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 23}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 24}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: This paper does not involve research with human subjects. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}]