[{"figure_path": "Wc0vlQuoLb/tables/tables_5_1.jpg", "caption": "Table 1: Precision (P), Recall (R), and F1-scores for Mistral-7B-v0.1. Our IDK-tuning achieves the best precision with minor decreases in recall, outperforming previous work. Mistral-7B-v0.1 + Confidence Threshold refers to the baseline based on the probability mass of the predicted answer [Yoshikawa and Okazaki, 2023]. Mistral-7B-v0.1 + The Pile refers to the ablation discussed in Section 4.2.", "description": "This table presents the results of the Mistral-7B-v0.1 model on various factual closed-book sentence completion datasets.  It compares the performance of the IDK-tuned Mistral-7B-v0.1 model against several baselines, including a confidence-threshold baseline, a P(True) baseline, a semantic entropy baseline, and a version of the model further trained on The Pile dataset. The metrics used are precision (P), recall (R), and F1-score, showing that IDK-tuning improves precision while maintaining relatively high recall.", "section": "4 Main Results"}, {"figure_path": "Wc0vlQuoLb/tables/tables_5_2.jpg", "caption": "Table 2: Precision (P), Recall (R), and F1-scores of our model on the 1m-eval-harness, compared to baselines.", "description": "This table presents the performance of the proposed IDK-tuning method on the 1m-eval-harness benchmark, a comprehensive evaluation suite for assessing multiple-choice question answering capabilities of language models. The table compares the precision, recall, and F1-score of the IDK-tuned Mistral-7B-v0.1 model against several baseline methods, including a confidence-threshold based approach and the P(True) baseline, showcasing the improved performance of IDK-tuning in terms of factual precision while maintaining reasonable recall on the benchmark.", "section": "4.1 Main Results"}, {"figure_path": "Wc0vlQuoLb/tables/tables_6_1.jpg", "caption": "Table 3: Precision (P), Recall (R), and F1 scores for of our IDK-tuned bert-base-cased on the evaluation benchmarks, compared to baselines.", "description": "This table presents the results of the IDK-tuning method on the bert-base-cased model.  It compares the precision, recall, and F1-score achieved by the bert-base-cased model alone, with the confidence threshold baseline, and with the IDK-tuning method applied.  The results are shown for three different datasets: LAMA Google-RE, LAMA T-Rex, and LAMA SQUAD, each assessing factual knowledge in different ways. The table demonstrates the impact of IDK-tuning on factual precision, showcasing a substantial improvement over the baseline methods at the cost of a slight decrease in recall.", "section": "4.2 Ablations"}, {"figure_path": "Wc0vlQuoLb/tables/tables_8_1.jpg", "caption": "Table 4: RougeL scores on different summarization tasks to measure the impact of IDK-tuning on other language model abilities. Mistral-7B-v0.1 + The Pile refers to the ablation discussed in Section 4.2.", "description": "This table presents the ROUGE-L scores achieved by three different models on three summarization tasks: Legal Plain English, TLDR, and SPEC5G.  The three models are: the base Mistral-7B-v0.1 model, the Mistral-7B-v0.1 model further trained on The Pile dataset (to evaluate impact of additional data), and the Mistral-7B-v0.1 model that underwent IDK-tuning on The Pile.  The results show the effect of IDK-tuning on the model's summarization capabilities, comparing its performance against both the base model and the model trained with additional data.", "section": "4 Main Results"}, {"figure_path": "Wc0vlQuoLb/tables/tables_8_2.jpg", "caption": "Table 5: Error type distribution on 200 failures our IDK-tuned model, using Pythia-70M, 2.8B and 7B.", "description": "This table presents the breakdown of error types observed in 200 incorrect predictions made by the IDK-tuned models across three different sizes: Pythia-70M, Pythia-2.8B, and Mistral-7B.  Each error is categorized into one of four types: No effect (the IDK-tuned model made the same incorrect prediction as the original model), Noise (the original model was correct, but the IDK-tuned model was incorrect), White Noise (both models were incorrect, but they made different predictions), and Abstaining (the IDK-tuned model did not provide a factual answer). The percentages represent the proportion of each error type within the 200 incorrect predictions.", "section": "4.5 Error Analysis"}]