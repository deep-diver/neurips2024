[{"figure_path": "Wc0vlQuoLb/figures/figures_5_1.jpg", "caption": "Figure 2: Average performance on closed-book factual sentence completion benchmarks of IDK-tuned models in terms of their parameter count. 70m to 2.8B are pythia-70m \u2013 2.8B, while 7.0B is Mistral-7B-v0.1.", "description": "This figure shows the relationship between the size of the language model and the performance on factual sentence completion tasks after applying IDK-tuning.  It plots the average precision, recall, and F1-score across multiple datasets for models ranging in size from 70 million parameters (pythia-70m) to 7 billion parameters (Mistral-7B).  The results demonstrate that larger models tend to achieve better overall performance (higher F1-scores) after IDK-tuning, although recall improves more significantly with model size than precision.", "section": "4.1 Main Results"}, {"figure_path": "Wc0vlQuoLb/figures/figures_6_1.jpg", "caption": "Figure 4: Tradeoff between IDK recall and IDK error rate for different parameter combinations. We annotate each data point with its corresponding \u03a0 value.", "description": "This figure shows the trade-off between IDK recall and IDK error rate for different hyperparameter settings (\u03a0, lambda, and LFP-reg) of the IDK-tuning objective.  It demonstrates how the choice of hyperparameters affects the model's ability to correctly identify situations where it lacks knowledge ([IDK] recall), while simultaneously minimizing incorrect [IDK] predictions (IDK error rate). The adaptive lambda results generally yield improved recall and reduced error rates compared to using fixed lambda. The inclusion of LFP-reg further refines this trade-off by lowering the IDK error rate.", "section": "4.2 Ablations"}, {"figure_path": "Wc0vlQuoLb/figures/figures_6_2.jpg", "caption": "Figure 4: Tradeoff between IDK recall and IDK error rate for different parameter combinations. We annotate each data point with its corresponding \u03a0 value.", "description": "This figure shows the trade-off between IDK recall and IDK error rate for different parameter combinations used in the IDK-tuning method.  The x-axis represents the IDK error rate (the proportion of instances where the model incorrectly predicted the [IDK] token when the base model correctly predicted the target), while the y-axis represents the IDK recall (the proportion of instances where the model correctly predicted the [IDK] token when the base model failed to predict the target).  Different lines represent different combinations of hyperparameters: using or not using the LFP-reg regularization and using a fixed or adaptive uncertainty factor (\u03bb). Each data point on the lines is annotated with the value of \u03a0, a hyperparameter that controls the influence of the objective function. The figure helps to understand the impact of the hyperparameters on the model's ability to express uncertainty effectively.", "section": "4.2 Ablations"}]