{"references": [{"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper is foundational for the concept of in-context learning in transformers, a core topic of the current paper."}, {"fullname_first_author": "Kwangjun Ahn", "paper_title": "Transformers learn to implement preconditioned gradient descent for in-context learning", "publication_date": "2024-01-01", "reason": "This paper provides a theoretical analysis linking transformers to optimization algorithms, directly relevant to the current paper's investigation of transformers' learning mechanisms."}, {"fullname_first_author": "Yu Bai", "paper_title": "Transformers as Statisticians: Provable In-Context Learning with In-Context Algorithm Selection", "publication_date": "2023-07-01", "reason": "This paper explores the connection between transformers and statistical algorithms, providing a theoretical framework similar to the current paper's approach."}, {"fullname_first_author": "Johannes von Oswald", "paper_title": "Transformers learn in-context by gradient descent", "publication_date": "2023-05-01", "reason": "This paper offers a mechanistic explanation of in-context learning in transformers, directly relevant to understanding the current paper's focus on multi-head attention and algorithm implementation."}, {"fullname_first_author": "Arvind Mahankali", "paper_title": "One Step of Gradient Descent is Provably the Optimal In-Context Learner with One Layer of Linear Self-Attention", "publication_date": "2023-07-01", "reason": "This paper provides theoretical results regarding the relationship between single-layer transformers and gradient descent, useful for the current paper's analysis of multi-layer models."}]}