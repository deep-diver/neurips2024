[{"figure_path": "xRQxan3WkM/figures/figures_6_1.jpg", "caption": "Figure 1: Normalized l\u221e-margins and l2-margins achieved by GD, GDM, and Adam with/without the stability constant e during training. (a) gives the results of normalized l\u221e-margins, while (b) shows the results of normalized l2-margins.", "description": "This figure compares the convergence of normalized l\u221e-margin and l2-margin for four different optimization algorithms: GD, GDM, Adam with and without stability constant (e). The plot shows that Adam (with or without e) converges towards the maximum l\u221e-margin while GD and GDM converge towards the maximum l2-margin, supporting the paper's claim that Adam has a unique implicit bias towards maximizing the l\u221e-margin.", "section": "Experiments"}, {"figure_path": "xRQxan3WkM/figures/figures_6_2.jpg", "caption": "Figure 1: Normalized l\u221e-margins and l2-margins achieved by GD, GDM, and Adam with/without the stability constant \u03b5 during training. (a) gives the results of normalized l\u221e-margins, while (b) shows the results of normalized l2-margins.", "description": "This figure compares the convergence behavior of four different optimization algorithms (Adam with and without stability constant, GD, and GDM) in terms of both l\u221e-margin and l2-margin.  The plots illustrate how the normalized margins change over the number of iterations.  The l\u221e-maximum margin is shown as a reference line, highlighting the algorithm's implicit bias towards maximizing the l\u221e-margin in linear classification problems.", "section": "Experiments"}, {"figure_path": "xRQxan3WkM/figures/figures_6_3.jpg", "caption": "Figure 1: Normalized l\u221e-margins and l2-margins achieved by GD, GDM, and Adam with/without the stability constant \u03b5 during training. (a) gives the results of normalized l\u221e-margins, while (b) shows the results of normalized l2-margins.", "description": "This figure compares the performance of four optimization algorithms (Gradient Descent (GD), Gradient Descent with Momentum (GDM), Adam with and without stability constant) in terms of normalized l\u221e-margin and normalized l2-margin. The results show that Adam, with or without the stability constant, converges to the maximum l\u221e-margin, while GD and GDM converge to the maximum l2-margin. This difference highlights the unique implicit bias of Adam towards maximizing the l\u221e-margin.", "section": "Experiments"}, {"figure_path": "xRQxan3WkM/figures/figures_6_4.jpg", "caption": "Figure 2: Log-log plots of the normalized l\u221e-margin gaps | mini\u2208[n] (Wt, Yixi)/||wt||\u221e - \u03b3| versus training iterations. (a) presents the results for Adam with the stability constant \u03b5, and (b) presents the results for Adam without the stability constant \u03b5.", "description": "This figure displays the convergence rate of the normalized l\u221e-margin for Adam with different learning rates (\u03b7t = \u0398(t\u207b\u1d43) where a \u2208 {0.3, 0.5, 0.7, 1}). The log-log plots show the relationship between the number of iterations and the margin gap from the maximum l\u221e-margin. It demonstrates polynomial-time convergence (a < 1) and logarithmic convergence (a = 1), distinguishing Adam from (stochastic) gradient descent which converges at a speed of O(1/log t).", "section": "Experiments"}]