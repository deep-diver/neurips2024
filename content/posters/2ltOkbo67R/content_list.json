[{"type": "text", "text": "Contextual Multinomial Logit Bandits with General Value Functions ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Mengxiao Zhang Haipeng Luo University of Iowa University of Southern California mengxiao-zhang@uiowa.edu haipengl@usc.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Contextual multinomial logit (MNL) bandits capture many real-world assortment recommendation problems such as online retailing/advertising. However, prior work has only considered (generalized) linear value functions, which greatly limits its applicability. Motivated by this fact, in this work, we consider contextual MNL bandits with a general value function class that contains the ground truth, borrowing ideas from a recent trend of studies on contextual bandits. Specifically, we consider both the stochastic and the adversarial settings, and propose a suite of algorithms, each with different computation-regret trade-off. When applied to the linear case, our results not only are the first ones with no dependence on a certain problem-dependent constant that can be exponentially large, but also enjoy other advantages such as computational efficiency, dimension-free regret bounds, or the ability to handle completely adversarial contexts and rewards. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "As assortment recommendation becomes ubiquitous in real-world applications such as online retailing and advertising, the multinomial (MNL) bandit model has attracted great interest in the past decade since it was proposed by Rusmevichientong et al. [24]. It involves a learner and a customer interacting for $T$ rounds. At each round, knowing the reward/profit for each of the $N$ available items, the learner selects a subset/assortment of size at most $K$ and recommend it to the customer, who then purchases one of these $K$ items or none of them according to a multinomial logit model specified by the customer\u2019s valuation over the items. The goal of the learner is to learn these unknown valuations over time and select the assortments with high reward. ", "page_idx": 0}, {"type": "text", "text": "To better capture practical applications where there is rich contextual information about the items and customers, a sequence of recent works study a contextual MNL bandit model where the customer\u2019s valuation is determined by the context via an unknown (generalized) linear function [8, 21, 7, 19, 20, 23, 2]. However, there are no studies on general value functions, despite many recent breakthroughs for classic contextual multi-armed bandits using a general value function class with much stronger representation power that enables fruitful results in both theory and practice [1, 10, 27, 11, 25]. ", "page_idx": 0}, {"type": "text", "text": "Contributions. Motivated by this gap, we propose a contextual MNL bandit model with a general value function class that contains the ground truth (a standard realizability assumption), and develop a suite of algorithms for different settings and with different computation-regret trade-off. ", "page_idx": 0}, {"type": "text", "text": "More specifically, in Section 3, we first consider a stochastic setting where the context-reward pairs are i.i.d. samples of an unknown distribution. Following the work by Simchi-Levi and $\\mathrm{Xu}$ [25] for contextual bandits, we reduce the problem to an easier offline log loss regression problem and propose two strategies using an offline regression oracle: one with simple and efficient uniform exploration, and another with more adaptive exploration (and hence improved regret) induced by a novel log-barrier regularized strategy. Our results rely on several new technical findings, including a fast ", "page_idx": 0}, {"type": "text", "text": "Table 1: Comparisons of results for contextual MNL bandits with $T$ rounds, $N$ items, size- $K$ assortments, and a $d_{\\cdot}$ -dimensional linear value function class with norm bounded by $B$ . All previous results depend on a problem-dependent constant $\\kappa$ that is $\\exp(2B)$ in the worst case, while ours (in gray) do not. The notation $\\widetilde O(\\cdot)$ hides logarithmic dependency on all parameters. In the last column, $\\checkmark$ means polynomial runti m e in all parameters; $\\check{\\bf{v}}$ means polynomial only when $K$ is a constant; and $\\pmb{\\chi}$ means not polynomial even for a small $K$ . ", "page_idx": 1}, {"type": "table", "img_path": "2ltOkbo67R/tmp/bed846ffbd1c10457943181f00b0f477f7b57f46091a79fe8fb5ab7a491d6248.jpg", "table_caption": [], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "rate regression result (Lemma 3.1), a \u201creverse Lipschitzness\u201d for the MNL model (Lemma 3.3), and a certain \u201clow-regret-high-dispersion\u201d property of the log-barrier regularized strategy (Lemma 3.6). ", "page_idx": 1}, {"type": "text", "text": "Next, in Section 4, we switch to the more challenging adversarial setting where the context-reward pairs can be arbitrarily chosen. We start by following the idea of [10, 11] for contextual bandits and reducing our problem to online log loss regression, and show that it suffices to find a strategy with a small Decision-Estimation Coefficient (DEC) [10, 14]. We then show that, somewhat surprisingly, the same log-barrier regularized strategy we developed for the stochastic setting leads to a small DEC, despite the fact that it is not the exact DEC minimizer (unlike its counterpart for contextual bandits [13]). We prove this by using the same aforementioned low-regret-high-dispersion property, which to our knowledge is a new way to bound DEC and reveals why log-barrier regularized strategies work in different settings and for different problems. Finally, we also extend the idea of Feel-Good Thompson Sampling [30] and propose a variant for our problem that leads to the best regret bounds in some cases, despite its lack of computational efficiency. ", "page_idx": 1}, {"type": "text", "text": "Throughout the paper, we use two running examples to illustrate the concrete regret bounds our different algorithms achieve: the finite class and the linear class. In particular, for the linear class, this leads to five new results, summarized in Table 1 together with previous results. These results all have their own advantages and disadvantages, but we highlight the following: ", "page_idx": 1}, {"type": "text", "text": "\u2022 While all previous regret bounds depend on a problem-dependent constant $\\kappa$ that can be exponentially large in the norm of the weight vector $B$ , none of our results depends on $\\kappa$ . In fact, our best results (Corollary 4.8) even has only logarithmic dependence on $B$ , a potential doubly-exponential improvement compared to prior works.1 \u2022 The regret bounds of our two algorithms that make use of an online regression oracle are dimension-free, despite not having the optimal $\\sqrt{T}$ -dependence (Corollary 4.4 and Corollary 4.7). \u2022 Our results are the first to handle completely adversarial context-reward pairs.2 ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Related works. The (non-contextual) MNL model was initially studied in [24], followed by a line of improvements [3, 4, 6, 5, 22]. Specifically, Agrawal et al. [3, 5] introduced a UCB-type algorithm achieving $\\widetilde{\\mathcal{O}}(\\sqrt{N T})$ regret and proved a lower bound of $\\Omega({\\sqrt{N T/K}})$ . Subsequently, Chen and Wang [6] e n hanced the lower bound to $\\Omega({\\sqrt{N T}})$ , matching the upper bound up to log factors. ", "page_idx": 2}, {"type": "text", "text": "Cheung and Simchi-Levi [8] first extended MNL bandits to its contextual version and designed a Thompson sampling based algorithm. Follow-up works consider this problem under different settings, including stochastic context [7, 19, 20], adversarial context [21, 2], and uniform reward over items [23]. However, as mentioned, all these works consider (generalized) linear value functions, and our work is the first to consider contextual MNL bandits under a general value function class. ", "page_idx": 2}, {"type": "text", "text": "Our work is also closely related to the recent trend of designing contextual bandits algorithms for a general function class. Due to space limit, we defer the discussion to Appendix A. ", "page_idx": 2}, {"type": "text", "text": "2 Notations and Preliminary ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Notations. Throughout this paper, we denote the set $\\{1,2,\\ldots,N\\}$ for some positive integer $N$ by $[N]$ and $\\{0,1,2,\\ldots,N\\}$ by $[N]_{0}$ . For a vector $u\\in\\mathbb{R}^{N}$ , we use $u_{i}$ to denote its $i$ -th coordinate, and for a matrix $W\\,\\in\\,\\mathbb{R}^{N\\times M}$ , we use $W_{j}$ to denote its $j$ -th column. For a set $\\boldsymbol{S}$ , we denote by $\\Delta(S)$ the set of distributions over $\\boldsymbol{S}$ , and by $\\operatorname{conv}(S)$ the convex hull of $\\boldsymbol{S}$ . Finally, for a distribution $\\mu\\in\\dot{\\Delta}([N]_{0})$ and an outcome $i\\in[N]_{0}$ , the corresponding log loss is $\\ell_{\\mathrm{log}}(\\mu,i)\\stackrel{\\cdot}{=}-\\log\\mu_{i}$ . ", "page_idx": 2}, {"type": "text", "text": "We consider the following contextual MNL bandit problem that proceeds for $T$ rounds. At each round $t$ , the learner receives a context $x_{t}\\,\\in\\,\\mathcal{X}$ for some arbitrary context space $\\mathcal{X}$ and a reward vector $r_{t}\\,\\in\\,[0,1]^{N}$ which specifics the reward of $N$ items. Then, out of these $N$ items, the learner needs to recommend a subset $S_{t}\\subseteq S$ to a customer, where $S\\subseteq2^{[N]}$ is the collection of all subsets of $[N]$ with cardinality at least 1 and at most $K$ for some $K\\leq N$ . Finally, the learner observes the customer purchase decision $i_{t}\\,\\in\\,S_{t}\\cup\\{0\\}$ , where 0 denotes the no-purchase option, and receives reward $\\boldsymbol{r}_{t,i_{t}}$ , where for notational convenience we define $r_{t,0}=0$ for all $t$ (no reward if no purchase). The customer decision $i_{t}$ is assumed to follow an MNL model: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathsf{P r}[i_{t}=i\\mid S_{t},x_{t}]=\\left\\{\\frac{\\frac{f_{i}^{\\star}(x_{t})}{1+\\sum_{j\\in S_{t}}f_{j}^{\\star}(x_{t})}}{\\frac{1}{1+\\sum_{j\\in S_{t}}f_{j}^{\\star}(x_{t})}}\\right.\\mathrm{~if~}i\\in S_{t},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $f^{\\star}:\\mathcal{X}\\to[0,1]^{N}$ is an unknown value function, specifying the costumer\u2019s value for each item under the given context. The MNL model above implicitly assumes a value of 1 for the nopurchase option, making it the most likely outcome. This is a standard assumption that holds in many realistic settings [5, 9, 18]. ", "page_idx": 2}, {"type": "text", "text": "To simplify notation, we define $\\mu\\,:\\,S\\,\\times\\,[0,1]^{N}\\,\\rightarrow\\,\\Delta([N]_{0})$ such that $\\mu_{i}(S,v)\\,\\propto\\,v_{i}\\mathbf{1}[i\\,\\in\\,S\\cup$ $\\{0\\}]$ with the convention $v_{0}\\,=\\,1$ . The purchase decision $i_{t}$ is thus sampled from the distribution $\\bar{\\mu}(\\bar{S}_{t},f^{\\star}(x_{t}))$ . In addition, given a reward vector $r\\,\\in\\,[0,1]^{N}$ (again, with convention ${r}_{0}=0$ ), we further define the expected reward of choosing subset $S\\in S$ under context $x\\in\\mathscr{X}$ as ", "page_idx": 2}, {"type": "equation", "text": "$$\nR(S,v,r)=\\mathbb{E}_{i\\sim\\mu(S,v)}\\left[r_{i}\\right]=\\sum_{i\\in S}\\mu_{i}(S,v)r_{i}=\\frac{\\sum_{i\\in S}r_{i}v_{i}}{1+\\sum_{i\\in S}v_{i}}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The goal of the learner is then to minimize her regret, defined as the expected gap between her total reward and that of the optimal strategy with the knowledge of $f^{\\star}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{Reg}_{\\mathtt{M N L}}=\\mathbb{E}\\left[\\sum_{t=1}^{T}\\operatorname*{max}_{S\\in\\mathcal{S}}R(S,f^{\\star}(x_{t}),r_{t})-\\sum_{t=1}^{T}R(S_{t},f^{\\star}(x_{t}),r_{t})\\right].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "To ensure that no-regret is possible, we make the following assumption, which is standard in the literature of contextual bandits. ", "page_idx": 2}, {"type": "text", "text": "Assumption 1 The learner is given a function class $\\mathcal{F}=\\{f:\\mathcal{X}\\to[0,1]^{N}\\}$ which contains $f^{\\star}$ . ", "page_idx": 2}, {"type": "text", "text": "Our hope is thus to design algorithms whose regret is sublinear in $T$ and polynomial in $N$ and some standard complexity measure of the function class $\\mathcal{F}$ . So far, we have not specified how the ", "page_idx": 2}, {"type": "text", "text": "Input: an offline regression oracle $\\mathsf{A l g}_{\\mathsf{o f f}}$ satisfying Assumption 2   \nDefine: epoch schedule $\\tau_{0}=0$ and $\\bar{\\tau_{m}}=2^{m-1}-1$ for all $m=1,2,\\ldots$ .   \nfor epoch $m=1,2,\\ldots$ do Feed $\\{x_{t},S_{t},i_{t}\\}_{t=\\tau_{m-1}+1}^{\\tau_{m}}$ to $\\mathsf{A l g}_{\\mathsf{o f f}}$ and obtain $f_{m}$ . Define a stochastic policy $q_{m}:\\mathcal{X}\\times[0,1]^{N}\\to\\Delta(S)$ via either Eq. (4) or Eq. (5). for $t=\\tau_{m}+1,\\cdot\\cdot\\cdot\\,,\\tau_{m+1}\\,\\mathrm{~d~}$ do Observe context $x_{t}\\in\\mathscr{X}$ and reward vector $r_{t}\\in[0,1]^{N}$ . Sample $S_{t}\\sim q_{m}(x_{t},r_{t})$ and recommend it to the customer. Observe customer\u2019s purchase decision $i_{t}\\in S_{t}\\cup\\{0\\}$ , drawn according to Eq. (1). ", "page_idx": 3}, {"type": "text", "text": "context $x_{t}$ and the reward $x_{t}$ are chosen. In the next two sections, we will discuss both the easier stochastic case where $(x_{t},r_{t})$ is jointly drawn from some fixed and unknown distribution, and the harder adversarial case where $(x_{t},r_{t})$ can be arbitrarily chosen by an adversary. ", "page_idx": 3}, {"type": "text", "text": "3 Contextual MNL Bandits with Stochastic Contexts and Rewards ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we consider contextual MNL bandits with stochastic contexts and rewards, where at each round $t\\in[T]$ , $x_{t}$ and $r_{t}$ are jointly drawn from a fixed and unknown distribution $\\mathcal{D}$ . Following the literature of contextual bandits, we aim to reduce the problem to an easier and better-studied offline regression problem and only access the function class $\\mathcal{F}$ through some offline regression oracle. Specifically, an offline regression oracle $\\mathsf{A l g}_{\\mathsf{o f f}}$ takes as input a set of i.i.d. context-subsetpurchase tuples and outputs a predictor from $\\mathcal{F}$ with low generalization error in terms of log loss, formally defined as follows. ", "page_idx": 3}, {"type": "text", "text": "Assumption 2 Given n samples $D=\\{(x_{k},S_{k},i_{k})\\}_{k=1}^{n}$ where each $(x_{k},S_{k},i_{k})\\in\\mathcal{X}\\times\\mathcal{S}\\times[N]_{0}$ is an i.i.d. sample of some unknown distribution $\\mathcal{H}$ and the conditional distribution of $i_{k}$ is $\\mu(S_{k},f^{\\star}(x_{k}))$ , with probability at least $1-\\delta$ the offline regression oracle $\\mathsf{A l g}_{\\mathsf{o f f}}$ outputs a function ${\\widehat{f}}_{D}\\in{\\mathcal{F}}$ such that: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{(\\boldsymbol{x},S,i)\\sim\\mathcal{H}}\\left[\\ell_{\\mathrm{log}}(\\mu(S,\\widehat{f}_{D}(\\boldsymbol{x})),i)-\\ell_{\\mathrm{log}}(\\mu(S,f^{\\star}(\\boldsymbol{x})),i)\\right]\\leq\\mathbf{Err}_{\\mathrm{log}}(n,\\delta,\\mathcal{F}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "for some function $\\mathbf{Err}_{\\mathrm{log}}(n,\\delta,\\mathcal{F})$ that is non-increasing in $n$ . ", "page_idx": 3}, {"type": "text", "text": "Given the similarity between MNL and multi-class logistic regression, assuming such a log loss regression oracle is more than natural. Indeed, in the following lemma, we prove that for both the finite class and a certain linear function class, the empirical risk minimizer (ERM) not only satisfies this assumption, but also enjoys a fast $1/n$ rate. The proof is based on the observation that our loss function $\\bar{\\ell_{\\mathrm{log}}}(\\mu(S,f(x)),i)$ , when seen as a function of $f$ , satisfies the so-called strong 1-central condition [17, Definition 7], which might be of independent interest; see Appendix B.1 for details. ", "page_idx": 3}, {"type": "text", "text": "Lemma 3.1 The ERM strategy $\\begin{array}{r}{\\widehat{f}_{D}=\\operatorname*{argmin}_{f\\in\\mathcal{F}}\\sum_{(x,S,i)\\in D}\\ell_{\\log}(\\mu(S,f(x)),i)}\\end{array}$ satisfies Assumption 2 for the following two case s : ", "page_idx": 3}, {"type": "text", "text": "\u2022 (Finite class) $\\mathcal{F}$ is a finite class of functions with image $[\\beta,1]^{N}$ for some $\\beta\\ \\in\\ (0,1)$ and $\\begin{array}{r}{\\mathbf{Err}_{\\mathrm{log}}(n,\\delta,\\mathcal{F})=\\mathcal{O}\\left(\\frac{\\log\\mathcal{K}/\\beta\\log\\vert\\mathcal{F}\\vert/\\delta}{n}\\right)}\\end{array}$   \n\u2022 (Lin $\\begin{array}{r l}&{\\mathit{r e a r c l a s s})\\ X\\subseteq\\{x\\in\\mathbb{R}^{d\\times N}\\mid\\|x_{i}\\|_{2}\\leq1,\\ \\forall i\\in[N]\\},\\mathcal{F}=\\{f_{\\theta,i}(x)=e^{\\theta^{\\top}x_{i}-B}\\mid\\|\\theta\\|_{2}\\leq B\\},}\\\\ &{\\mathrm{'}\\mathbf{Errlog}(n,\\delta,\\mathcal{F})=\\mathcal{O}\\big(\\frac{d B\\log K\\log(B n)\\log\\frac{1}{\\delta}}{n}\\big),\\mathit{f o r}\\,s o m e\\,B>0.}\\end{array}$   \nand ", "page_idx": 3}, {"type": "text", "text": "Due to space limit, we only use these two simple function classes as running examples throughout the paper, but we emphasize that our results can be applied to any class as long as regression is feasible. For additional examples, see Appendix D. ", "page_idx": 4}, {"type": "text", "text": "Given $\\mathsf{A l g}_{\\mathsf{o f f}}$ , we now outline a natural algorithm framework that proceeds in epochs with exponentially increasing length (see Algorithm 1): At the beginning of each epoch $m$ , the algorithm feeds all the context-subset-purchase tuples from the last epoch to the offline regression oracle $\\mathsf{A l g}_{\\mathsf{o f f}}$ and obtains a value predictor $f_{m}$ . Then, it decides in some way using $f_{m}$ a stochastic policy $q_{m}$ , which maps a context $x$ and a reward vector $r\\,\\in\\,[0,1]^{N}$ to a distribution over $\\boldsymbol{S}$ . With such a policy in hand, for every round $t$ within this epoch, the algorithm simply samples a subset $S_{t}$ according to $q_{m}(\\boldsymbol{x}_{t},\\boldsymbol{r}_{t})$ and recommend it to the customer. ", "page_idx": 4}, {"type": "text", "text": "We will specify two concrete stochastic policies $q_{m}$ in the next two subsections. Before doing so, we highlight some key parts of the analysis that shed light on how to design a \u201cgood\u201d $q_{m}$ . The first step is an adaptation of Simchi-Levi and $\\mathrm{Xu}$ [25, Lemma 7], which quantifies the expected reward difference of any policy under the ground-truth value function $f^{\\star}$ versus the estimated value function $f_{m}$ . Specifically, for a deterministic policy $\\pi:\\mathcal{X}\\times[0,1]^{N}\\to\\mathcal{S}$ mapping from a context-reward pair to a subset, we define its true expected reward and its expected reward under $f_{m}$ respectively as (overloading the notation $R$ ): ", "page_idx": 4}, {"type": "equation", "text": "$$\nR(\\pi)=\\mathbb{E}_{(x,r)\\sim\\mathcal{D}}\\left[R(\\pi(x,r),f^{\\star}(x),r)\\right],\\ \\ R_{m}(\\pi)=\\mathbb{E}_{(x,r)\\sim\\mathcal{D}}\\left[R(\\pi(x,r),f_{m}(x),r)\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Moreover, for any $\\rho\\in\\Delta(S)$ , define $w(\\rho)\\in[0,1]^{N}$ such that $\\begin{array}{r}{w_{i}(\\rho)=\\sum_{S\\in S:i\\in S}\\rho(S)}\\end{array}$ is the probability of item $i$ being selected under distribution $\\rho$ , and for any stochas tic policy $q$ , further define a dispersion measure for a deterministic policy $\\pi$ as $\\begin{array}{r}{V(q,\\pi)=\\mathbb{E}_{(x,r)\\sim\\mathcal{D}}\\left[\\sum_{i\\in\\pi(x,r)}\\frac{1}{w_{i}(q(x,r))}\\right]}\\end{array}$ (the smaller $V(q,\\pi)$ is, the more disperse the distribution induced by $q$ is). Using the Lipschitzness (in $v$ ) of the reward function $R(S,\\bar{v_{}},r)$ (Lemma B.1), we prove the following. ", "page_idx": 4}, {"type": "text", "text": "Lemma 3.2 For any deterministic policy $\\pi:\\mathcal{X}\\times[0,1]^{N}\\to\\mathcal{S}$ and any epoch $m\\geq2$ , we have ", "page_idx": 4}, {"type": "equation", "text": "$$\n|R_{m}(\\pi)-R(\\pi)|\\leq{\\sqrt{V(q_{m-1},\\pi)}}\\cdot{\\sqrt{\\mathbb{E}_{(x,r)\\sim{\\mathcal{D}},S\\sim q_{m-1}(x,r)}\\left[\\sum_{i\\in S}{(f_{m,i}(x)-f_{i}^{\\star}(x))}^{2}\\right]}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "If the learner could observe the true value of each item in the selected subset (or its noisy version), then doing squared loss regression on these values would make the squared loss term in Lemma 3.2 small; this is essentially the case in the contextual bandit problem studied by Simchi-Levi and Xu [25]. However, in our problem, only the purchase decisions are observed but not the true values that define the MNL model. Nevertheless, one of our key technical contributions is to show that the offilne log-loss regression, which only relies on observing the purchase decisions, in fact also makes sure that the squared loss above is small. ", "page_idx": 4}, {"type": "text", "text": "Lemma 3.3 For any $S\\in S$ and $v,v^{\\star}\\in[0,1]^{N}$ , we have ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{\\mathfrak{(K+1)}^{4}}\\displaystyle\\sum_{i\\in S}(v_{i}-v_{i}^{\\star})^{2}\\leq\\|\\mu(S,v)-\\mu(S,v^{\\star})\\|_{2}^{2}\\leq2\\mathbb{E}_{i\\sim\\mu(S,v^{\\star})}\\left[\\ell_{\\log}(\\mu(S,v),i)-\\ell_{\\log}(\\mu(S,v^{\\star}),i)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The first equality establishes certain \u201creverse Lipschitzness\u201d of $\\mu$ and is proven by providing a universal lower bound on the minimum singular value of its Jacobian matrix, which is new to our knowledge. It implies that if two value vectors induce a pair of close distributions, then they must be reasonably close as well. The second equality, proven using known facts, further states that to control the distance between two distributions, it suffices to control their log loss difference, which is exactly the job of the offline regression oracle. ", "page_idx": 4}, {"type": "text", "text": "Therefore, combining Lemma 3.2 and Lemma 3.3, we see that to design a good algorithm, it suffices to find a stochastic policy that \u201cmostly\u201d follows $\\operatorname{argmax}_{S}R(S,\\hat{{f_{m}(x_{t})}_{}}r_{t})$ , the best decision according to the oracle\u2019s prediction, and at the same time ensures high dispersion for all $\\pi$ such that the oracle\u2019s predicted reward for any policy is close to its true reward. The design of our two algorithms in the remaining of this section follows exactly this principle. ", "page_idx": 4}, {"type": "text", "text": "3.1 A Simple and Efficient Algorithm via Uniform Exploration ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "As a warm-up, we first introduce a simple but efficient $\\varepsilon$ -greedy-type algorithm that ensures reasonable dispersion by uniformly exploring all the singleton sets. Specifically, at epoch $m$ , given the value predictor $f_{m}$ from $\\mathsf{A l g}_{\\mathsf{o f f}}$ , $q_{m}(x,r)\\in\\Delta(S)$ is defined as follows for some $\\varepsilon_{m}>0$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\nq_{m}(S|x,r)=(1-\\varepsilon_{m})\\mathbb{1}\\left[S=\\operatorname*{argmax}_{S^{\\star}\\in S}R(S^{\\star},f_{m}(x),r)\\right]+\\frac{\\varepsilon_{m}}{N}\\sum_{i=1}^{N}\\mathbb{1}\\left[S=\\{i\\}\\right].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In other words, with probability $1-\\varepsilon$ , the learner picks the subset achieving the maximum reward based on the reward vector $r$ and the predicted value $f_{m}(x)$ ; with the remaining $\\varepsilon$ probability, the tehnes ufroellso $\\begin{array}{r}{V(q_{m},\\pi)\\,\\le\\,\\frac{K N}{\\varepsilon_{m}}}\\end{array}$ nftoere .any $\\pi$ . Base $i~\\in~[N]$ previous analysis, it is straightforward to prove ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.4 Under Assumption 1 and Assumption 2, Algorithm $^{\\,l}$ with $q_{m}$ defined in Eq. (4) and the optimal choice of \u03b5m ensures RegMNL = \u2308mlo=g12 $\\begin{array}{r}{\\mathbf{Reg}_{\\mathsf{M N L}}=\\overset{\\cdot}{\\sum}\\lceil\\overset{\\log_{2}T\\rceil}{m=1}^{T}\\mathcal{O}\\left(2^{m}(N K\\mathbf{Err}_{\\log}(2^{m-1},1/T^{2},\\mathcal{F}))^{\\frac{1}{3}}\\right)}\\end{array}$ . ", "page_idx": 5}, {"type": "text", "text": "To better interpret this regret bound, we consider the finite class and the linear class discussed in Lemma 3.1. Combining it with Theorem 3.4, we immediately obtain the following corollary: ", "page_idx": 5}, {"type": "text", "text": "Corollary 3.5 Under Assumption $^{\\,l}$ , Algorithm $^{\\,l}$ with $q_{m}$ defined in Eq. (4), the optimal choice of $\\varepsilon_{m}$ , and ERM as $\\mathsf{A l g}_{\\mathsf{o f f}}$ ensures $\\begin{array}{r}{\\mathbf{Reg}_{\\mathsf{M N L}}=\\mathcal{O}\\left((N K\\log\\frac{K}{\\beta}\\log(|\\mathcal{F}|T))^{\\frac{1}{3}}T^{\\frac{2}{3}}\\right)}\\end{array}$ for finite class and ${\\bf R e g}_{\\sf M N L}\\,=\\,\\mathcal{O}\\left((d B N K\\log K)^{\\frac13}T^{\\frac23}\\log(B T)\\stackrel{\\cdot}{\\log}T\\right)$ for linear class (see Lemma 3.1 for definitions). ", "page_idx": 5}, {"type": "text", "text": "While these $\\widetilde{\\mathcal{O}}(T^{2/3})$ regret bounds are suboptimal, Theorem 3.4 provides the first computationally efficient algo r ithms for contextual MNL bandits with an offline regression oracle for a general function class. Indeed, computing $\\mathrm{argmax}_{S^{\\star}\\in\\mathcal{S}}\\,R(S^{\\star},f_{m}(x),r)$ can be efficiently done in $\\bar{\\mathcal{O}}(N^{2})$ time according to [24, Section 2.1]. Moreover, for the linear case, the ERM oracle can indeed be efficiently (and approximately) implemented because it is a convex optimization problem over a simple ball constraint. Importantly, previous regret bounds for the linear case all depend on a problemdependent constant $\\kappa=\\operatorname*{max}_{\\parallel{\\boldsymbol{\\theta}}\\parallel\\leq B,S\\in S,i\\in S,t\\in\\left[T\\right]}\\frac{1}{\\mu_{i}(S,f_{\\boldsymbol{\\theta}}(x_{t}))\\mu_{0}(S,f_{\\boldsymbol{\\theta}}(x_{t}))}$ , which is $\\exp(2B)$ in the worst case [7, 20, 23], but ours only has polynomial dependence on $B$ . ", "page_idx": 5}, {"type": "text", "text": "3.2 Better Exploration Leads to Better Regret ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Next, we show that a more sophisticated construction of $q_{m}$ in Algorithm 1 leads to better exploration and consequently improved regret bounds. Specifically, $q_{m}$ is defined as (for some $\\gamma_{m}>0$ ): ", "page_idx": 5}, {"type": "equation", "text": "$$\nq_{m}(x,r)=\\operatorname*{argmax}_{\\rho\\in\\Delta(S)}\\mathbb{E}_{S\\sim\\rho}\\left[R(S,f_{m}(x),r)\\right]-\\frac{(K+1)^{4}}{\\gamma_{m}}\\sum_{i=1}^{N}\\log\\frac{1}{w_{i}(\\rho)}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The first term of the optimization objective above is the expected reward when one picks a subset according to $\\rho$ and the value function is $f_{m}$ , while the second term is a certain log-barrier regularizer applied to $\\rho$ , penalizing it for putting too little mass on any single item. This specific form of regularization ensures that $q_{m}$ enjoys a low-regret-high-dispersion guarantee, as shown below. ", "page_idx": 5}, {"type": "text", "text": "Lemma 3.6 For any $x\\in\\mathscr{X}$ and $r\\in[0,1]^{N}$ , the distribution $q_{m}(x,r)$ defined in Eq. (5) satisfies: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{max}_{S^{\\star}\\in S}R(S^{\\star},f_{m}(x),r)-\\mathbb{E}_{S\\sim q_{m}(x,r)}\\left[R(S,f_{m}(x),r)\\right]\\le\\frac{N(K+1)^{4}}{\\gamma_{m}},}\\\\ {\\forall S\\in S,}&{\\displaystyle\\sum_{i\\in S}\\frac{1}{w_{i}(q_{m}(x,r))}\\le N+\\frac{\\gamma_{m}}{(K+1)^{4}}\\left(\\operatorname*{max}_{S^{\\star}\\in S}R(S^{\\star},f_{m}(x),r)-R(S,f_{m}(x),r)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Eq. (6) states that following $q_{m}(x,r)$ does not incur too much regret compared to the best subset predicted by the oracle, and Eq. (7) states that the dispersion of $q_{m}\\bar{(}x,r\\rangle$ on any subset is controlled by how bad this subset is compared to the best one in terms of their predicted reward \u2014 a good subset has a large dispersion while a bad one can have a smaller dispersion since we do not care about estimating its true reward very accurately. Such a refined dispersion guarantee intuitively provides a much more adaptive exploration scheme compared to uniform exploration. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "This kind of low-regret-high-dispersion guarantees is in fact very similar to the ideas of Simchi-Levi and $\\mathrm{Xu}$ [25] for contextual bandits (which itself is similar to an earlier work by Agarwal et al. [1]). While Simchi-Levi and $\\mathrm{Xu}$ [25] were able to provide a closed-form strategy with such a guarantee for contextual bandits, we do not find a similar closed-form for MNL bandits and instead provide the strategy as the solution of an optimization problem Eq. (5). Unfortunately, we are not aware of an efficient way to solve Eq. (5) with polynomial time complexity, but one can clearly solve it in poly $(|S|)\\,=\\,\\mathrm{poly}(N^{K})$ time since it is a concave problem over $\\Delta(S)$ . Thus, the algorithm is efficient when $K$ is small, which we believe is the case for most real-world applications. ", "page_idx": 6}, {"type": "text", "text": "Combining Lemma 3.2 and Lemma 3.6, we prove the following regret guarantee, which improves the Errl1o/g 3 term in Theorem 3.4 to Errl1o/g2 ( proofs deferred to Appendix B). ", "page_idx": 6}, {"type": "text", "text": "Theorem 3.7 Under Assumption $^{\\,l}$ and Assumption 2, Algorithm $^{\\,l}$ with $q_{m}$ defined in Eq. (5) and the optimal choice of $\\gamma_{m}$ ensures $\\begin{array}{r}{\\mathbf{Reg}_{\\mathrm{MNL}}=\\mathcal{O}\\left(\\sum_{m=1}^{\\lceil\\log_{2}T\\rceil}2^{m}K^{2}\\sqrt{N\\mathbf{Err}_{\\log}(2^{m-1},1/T^{2},\\mathcal{F})}\\right)}\\end{array}$ ", "page_idx": 6}, {"type": "text", "text": "Similar to Section 3.1, we instantiate Theorem 3.7 using the following two concrete classes: ", "page_idx": 6}, {"type": "text", "text": "Corollary 3.8 Under Assumption $^{\\,l}$ , Algorithm 1 with $q_{m}$ defined in Eq. (5), the optimal choice of $\\gamma_{m}$ , and ERM as $\\mathsf{A l g}_{\\mathsf{o f f}}$ ensures $\\begin{array}{r}{\\mathbf{Reg}_{\\mathrm{MNL}}=\\mathcal{O}\\left(K^{2}\\sqrt{T\\log\\frac{K}{\\beta}\\log(|\\mathcal{F}|T)}\\right)}\\end{array}$ for the finite class and ${\\bf R e g}_{\\sf M N L}=\\mathcal{O}\\left(K^{2}\\sqrt{d B N T\\log(B T)\\log T}\\right)$ for the linear class (see Lemma 3.1 for definitions). ", "page_idx": 6}, {"type": "text", "text": "The dependence on $T$ in these $\\mathcal{O}(\\sqrt{T})$ regret bounds is known to be optimal [6, 7]. Once again, in the linear case, we have no exponential dependence on $B$ , unlike previous results. ", "page_idx": 6}, {"type": "text", "text": "4 Contextual MNL Bandits with Adversarial Contexts and Rewards ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we move on to consider the more challenging case where the context $x_{t}$ and the reward vector $r_{t}$ can both be arbitrarily chosen by an adversary. We propose two different approaches leading to three different algorithms, each with its own pros and cons. ", "page_idx": 6}, {"type": "text", "text": "4.1 First Approach: Reduction to Online Regression ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In the first approach, we follow a recent trend of studies that reduces contextual bandits to online regression and only accesses $\\mathcal{F}$ through an online regression oracle [10, 11, 15, 31, 29]. More specifically, we assume access to an online regression oracle $\\mathsf{A l g}_{\\mathsf{o n}}$ that follows the protocol below: at each round $t\\,\\in\\,[T],\\,\\mathsf{A l g}_{\\mathsf{o n}}$ outputs a value predictor $f_{t}\\,\\in\\,\\mathrm{conv}(\\mathcal{F})$ ; then, it receives a context $x_{t}$ , a subset $S_{t}$ , and a purchase decision $i_{t}\\in\\mathbf{\\dot{\\cal{S}}}_{t}\\cup\\{0\\}$ , all chosen arbitrarily, and suffers log loss $\\ell_{\\mathrm{log}}(\\mu(S_{t},f_{t}(x_{t})),i_{t})$ .4 The oracle is assumed to enjoy the following regret guarantee. ", "page_idx": 6}, {"type": "text", "text": "Assumption 3 The predictions made by the online regression oracle $\\mathsf{A l g}_{\\mathsf{o n}}$ ensure: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\sum_{t=1}^{T}\\ell_{\\log}(\\mu(S_{t},f_{t}(x_{t})),i_{t})-\\sum_{t=1}^{T}\\ell_{\\log}(\\mu(S_{t},f^{\\star}(x_{t})),i_{t})\\right]\\leq\\mathbf{Reg}_{\\log}(T,\\mathcal{F}),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "for any $f^{\\star}\\in{\\mathcal{F}}$ and some regret bound $\\mathbf{Reg}_{\\mathrm{log}}(T,\\mathcal{F})$ that is non-decreasing in $T$ . ", "page_idx": 6}, {"type": "text", "text": "While most previous works on contextual bandits assume a squared loss online oracle, log loss is more than natural for our MNL model (it was also used by Foster and Krishnamurthy [11] to achieve first-order regret guarantees for contextual bandits). The following lemma shows that Assumption 3 again holds for the finite class and the linear class. ", "page_idx": 6}, {"type": "text", "text": "Lemma 4.1 For the finite class and the linear class discussed in Lemma 3.1, the following concrete oracles satisfy Assumption 3: ", "page_idx": 7}, {"type": "text", "text": "\u2022 (Finite class) Hedge [16] with $\\begin{array}{r}{\\mathbf{Reg}_{\\mathrm{log}}(T,\\mathcal{F})=\\mathcal{O}(\\sqrt{T\\log\\vert\\mathcal{F}\\vert}\\log\\frac{K}{\\beta});}\\end{array}$ \u2022 (Linear class) Online Gradient Descent [32] with $\\mathbf{Reg}_{\\mathrm{log}}(T,\\mathcal{F})=\\mathcal{O}(B\\sqrt{T}).$ ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Unfortunately, unlike the offline oracle, we are not able to provide a \u201cfast rate\u201d (that is, $\\widetilde O(1)$ regret) for these two cases, because our loss function does not appear to satisfy the standar d  Vovk\u2019s mixability condition or any other sufficient conditions discussed in Van Erven et al. [26]. This is in sharp contrast to the standard multi-class logistic loss [12], despite the similarity between these two models. We leave as an open problem whether fast rates exist for these two classes, which would have immediate consequences to our final MNL regret bounds below. ", "page_idx": 7}, {"type": "text", "text": "With this online regression oracle, a natural algorithm framework works as follows: at each round $t$ , the learner first obtains a value predictor $f_{t}\\in\\mathrm{conv}(\\mathcal{F})$ from the regression oracle $\\mathsf{A l g}_{\\mathsf{o n}}$ ; then, upon seeing context $x_{t}$ and reward vector $r_{t}$ , the learner decides in some way a distribution $q_{t}\\,\\in\\,\\Delta(S)$ based on $f_{t}(\\boldsymbol{x}_{t})$ and $r_{t}$ , and samples $S_{t}$ from $q_{t}$ ; finally, the learner observes the purchase decision $i_{t}$ and feeds the tuple $(x_{t},S_{t},i_{t})$ to the oracle $\\mathsf{A l g}_{\\mathsf{o n}}$ (see Algorithm 2 in Appendix C). To shed light on how to design a good sampling distribution $q_{t}$ , we show a general lemma that holds for any $q_{t}$ . ", "page_idx": 7}, {"type": "text", "text": "Lemma 4.2 Under Assumption $^{\\,l}$ and Assumption $^3$ , Algorithm 2 (with any $q_{t}$ ) ensures ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbf{Reg}_{\\mathsf{M N L}}\\leq\\mathbb{E}\\left[\\sum_{t=1}^{T}\\mathsf{d e c}_{\\gamma}(q_{t};f_{t}(x_{t}),r_{t})\\right]+2\\gamma\\mathbf{Reg}_{\\log}(T,\\mathcal{F})\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "for any $\\gamma>0$ , where $\\mathsf{d e c}_{\\gamma}(q;v,r)$ is the Decision-Estimation Coefficient (DEC) defined as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{v^{\\star}\\in[0,1]^{N}}\\operatorname*{max}_{S^{\\star}\\in S}\\left\\{R(S^{\\star},v^{\\star},r)-\\mathbb{E}_{S\\sim q}\\left[R(S,v^{\\star},r)\\right]-\\gamma\\mathbb{E}_{S\\sim q}\\left[\\|\\mu(S,v)-\\mu(S,v^{\\star})\\|_{2}^{2}\\right]\\right\\}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Our DEC adopts the idea of Foster et al. [14] for general decision making problems: the term $R(S^{\\star},v^{\\star},r)\\,\\stackrel{\\_}{-}\\,\\mathbb{E}_{S\\sim q}\\left[R(S,v^{\\star},r)\\right]$ represents the instantaneous regret of strategy $q$ against the best subset $S^{\\star}$ with respect to reward vector $r$ and the worst-case value vector $v^{\\star}$ , and the term $\\mathbb{E}_{S\\sim q}[\\|\\mu(S,v)-\\mu(S,v^{\\star})\\|_{2}^{2}]$ is the expected squared distance between two distributions induced by $v$ and $v^{\\star}$ , which, in light of the second inequality of Lemma 3.3, lower bounds the instantaneous log loss regret of the online oracle. Therefore, a small DEC makes sure that the learner\u2019s MNL regret is somewhat close to the oracle\u2019s log loss regret $\\mathbf{Reg}_{\\mathrm{log}}$ , formally quantified by Lemma 4.2. With the goal of ensuring a small DEC, we again propose two strategies similar to Section 3. ", "page_idx": 7}, {"type": "text", "text": "Uniform Exploration. We start with a simple uniform exploration approach similar to Eq. (4): ", "page_idx": 7}, {"type": "equation", "text": "$$\nq_{t}(S)=(1-\\varepsilon)\\mathbb{1}\\left[S=\\underset{S^{\\star}\\in S}{\\operatorname{argmax}}\\,R(S^{\\star},f_{t}(x_{t}),r_{t})\\right]+\\frac{\\varepsilon}{N}\\sum_{i=1}^{N}\\mathbb{1}\\left[S=\\{i\\}\\right].\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\varepsilon>0$ is a parameter specifying the probability of uniformly exploring the singleton sets. We prove the following results for this simple algorithm. ", "page_idx": 7}, {"type": "text", "text": "Theorem 4.3 The strategy defined in Eq. (9) guarantees $\\begin{array}{r}{\\mathsf{d e c}_{\\gamma}(q_{t};f_{t}(x_{t}),r_{t})=\\mathcal{O}(\\frac{N K}{\\gamma\\varepsilon}+\\varepsilon)}\\end{array}$ . Consequently, under Assumption 1 and Assumption $^3$ , Algorithm 2 with $q_{t}$ calculated via Eq. (9) and the optimal choice of $\\varepsilon$ and $\\gamma$ ensures $\\mathbf{Reg}_{\\mathrm{MNL}}=\\mathcal{O}\\big((N K\\mathbf{Reg}_{\\mathrm{log}}(T,\\mathcal{F}))^{\\frac{1}{3}}T^{\\frac{2}{3}}\\big)$ . ", "page_idx": 7}, {"type": "text", "text": "Combining this with Lemma 4.1, we immediately obtain the following corollary. ", "page_idx": 7}, {"type": "text", "text": "Corollary 4.4 Under Assumption $^{\\,l}$ , Algorithm 2 with $q_{t}$ defined in Eq. (9) and the optimal choice of $\\varepsilon$ and $\\gamma$ ensures $\\begin{array}{r}{\\mathbf{Reg}_{\\mathrm{MNL}}=\\mathcal{O}\\Big((N K\\log\\frac{K}{\\beta})^{\\frac{1}{3}}T^{\\frac{5}{6}}\\Big)}\\end{array}$ for the finite class (with Hedge as $\\mathsf{A l g}_{\\mathsf{o n}}$ ) and ${\\mathbf{Reg}}_{\\mathrm{MNL}}={\\mathcal{O}}{\\Big(}(N K B)^{{\\frac{1}{3}}}T^{{\\frac{5}{6}}}{\\Big)}$ for the linear class (with Online Gradient Descent as $\\mathsf{A l g}_{\\mathsf{o n}.}$ ). ", "page_idx": 7}, {"type": "text", "text": "While these regret bounds have a large dependence on $T$ , the advantage of this algorithm is its computational efficiency as discussed before. ", "page_idx": 7}, {"type": "text", "text": "Better Exploration. Can we improve the algorithm via a strategy with an even smaller DEC? In particular, what happens if we take the extreme and let $q_{t}$ be the minimizer of $\\mathsf{d e c}_{\\gamma}(q;f_{t}(x_{t}),r_{t})?$ Indeed, this is exactly the approach in several prior works that adopt the DEC framework [13, 29], where the exact minimizer for DEC is characterized and shown to achieve a small DEC value. ", "page_idx": 8}, {"type": "text", "text": "On the other hand, for our problem, it appears quite difficult to analyze the exact DEC minimizer. Somewhat surprisingly, however, we show that the same construction in Eq. (5) for the stochastic environment in fact also achieves a reasonably small DEC for the adversarial case: ", "page_idx": 8}, {"type": "text", "text": "Theorem 4.5 The following distribution satisfies $\\begin{array}{r}{\\mathsf{d e c}_{\\gamma}(q_{t},f_{t}(x_{t}),r_{t})\\le\\mathcal{O}\\left(\\frac{N K^{4}}{\\gamma}\\right):}\\end{array}$ ", "page_idx": 8}, {"type": "equation", "text": "$$\nq_{t}=\\operatorname*{argmax}_{q\\in\\Delta(S)}\\mathbb{E}_{S\\sim q}\\left[R\\big(S,f_{t}(x_{t}),r_{t}\\big)\\right]-\\frac{(K+1)^{4}}{\\gamma}\\sum_{i=1}^{N}\\log\\frac{1}{w_{i}(q)}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "A couple of remarks are in order. First, while for some cases such as the contextual bandit problem studied by Foster et al. [13], this kind of log-barrier regularized strategies is known to be the exact DEC minimizer, one can verify that this is not the case for our DEC. Second, the fact that the same strategy works for both the stochastic and the adversarial environments is similar to the case for contextual bandits where the same inverse gap weighting strategy works for both cases [10, 25], but to our knowledge, the connection between these two cases is unclear since their analysis is quite different. Finally, our proof (in Appendix C) in fact relies on the same low-regret-high-dispersion property of Lemma 3.6, which is a new way to bound DEC as far as we know. More importantly, this to some extent demystifies the last two points: the reason that such log-barrier regularized strategies work regardless whether they are the exact minimizer or not and regardless whether the environment is stochastic or adversarial is all due to their inherent low-regret-high-dispersion property. ", "page_idx": 8}, {"type": "text", "text": "Combining Theorem 4.5 with Lemma 4.2, we obtain the following improved regret. ", "page_idx": 8}, {"type": "text", "text": "Theorem 4.6 Under Assumption $^{\\,l}$ and Assumption $^3$ , Algorithm 2 with $q_{t}$ calculated via Eq. (10) and the optimal choice of $\\gamma$ ensures $\\mathbf{Reg}_{\\mathrm{MNL}}=\\mathcal{O}\\Big(K^{2}\\sqrt{N T\\mathbf{Reg}_{\\mathrm{log}}(T,\\mathcal{F})}\\Big)$ ", "page_idx": 8}, {"type": "text", "text": "Corollary 4.7 Under Assumption $^{\\,l}$ , Algorithm 2 with $q_{t}$ defined in Eq. (10) and the optimal choice of $\\gamma$ ensures $\\begin{array}{r}{\\mathbf{Reg}_{\\mathsf{M N L}}=\\mathcal{O}\\big(K^{2}\\sqrt{N\\log\\frac{K}{\\beta}}T^{\\frac{3}{4}}(\\log|\\mathcal{F}|)^{\\frac{1}{4}}\\big)}\\end{array}$ for the finite class (with Hedge as $\\mathsf{A l g}_{\\mathsf{o n}}$ ) and $\\mathbf{Reg}_{\\mathsf{M N L}}=\\mathcal{O}\\big(K^{2}\\sqrt{N B}T^{\\frac{3}{4}}\\big)$ for the linear class (with Online Gradient Descent as $\\mathsf{A l g}_{\\mathsf{o n}.}$ ). ", "page_idx": 8}, {"type": "text", "text": "We remark that if the \u201cfast rate\u201d discussed after Lemma 4.1 exists, we would have obtained the optimal $\\sqrt{T}$ -regret here. Despite having worse dependence on $T$ , however, our result for the linear case enjoys three advantages compared to prior work [7, 20, 23]: 1) no exponential dependence on $B$ (as in all our other results), 2) no dependence at all on the dimension $d$ , and 3) valid even when contexts and rewards are adversarial. We refer the reader to Table 1 again for detailed comparisons. ", "page_idx": 8}, {"type": "text", "text": "4.2 Second Approach: Feel-Good Thompson Sampling ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The second approach we take is to extend the idea of the Feel-Good Thompson Sampling algorithm of Zhang [30] for contextual bandits. Due to space limit, we defer the algorithm and its analysis to Appendix E, and only state its regret bounds for the finite class and the linear class (a corollary of a more general regret bound in Theorem E.1). ", "page_idx": 8}, {"type": "text", "text": "Corollary 4.8 Under Assumption 1, Algorithm 3 wensures ${\\bf R e g}_{\\mathrm{MNL}}\\,=\\,\\mathcal{O}\\big(K^{2}\\sqrt{N T\\log|\\mathcal{F}|}\\big)$ for the finite class and ${\\bf R e g}_{\\sf M N L}=\\mathcal{O}\\big(K^{2}\\sqrt{d N T\\log(B T K)}\\big)$ for the linear class. ", "page_idx": 8}, {"type": "text", "text": "In terms of the dependence on $T$ , Algorithm 3 achieves the best (and in fact optimal) regret bounds among all our results. For the linear case, it even has only logarithmic dependence on $B$ , a potential doubly-exponential improvement compared to prior works. The caveat is that there is no efficient way to implement the algorithm even for the linear case and even when $K$ is a constant (unlike all our other algorithms). We leave the question of whether there exists a computationally efficient algorithm (even only for small $K$ ) with a $\\sqrt{T}$ -regret bound that has no exponential dependence on $B$ as a key future direction. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion and Future Directions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we consider contextual MNL bandits with a general value function class under a realizability assumption. For both the stochastic and the adversarial settings, we propose a suite of algorithms with different computational-regret trade-off. Notably, none of our regret bounds suffers from the exponentially large dependence on some problem dependent constant in the case with linear value functions. One interesting future direction is to improve the poly $\\langle K,N\\rangle$ dependence in our regret upper bounds, which seems to require new techniques. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The authors were supported by NSF Award IIS-1943607. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert Schapire. Taming the monster: A fast and simple algorithm for contextual bandits. International Conference on Machine Learning, 2014. [2] Priyank Agrawal, Theja Tulabandhula, and Vashist Avadhanula. A tractable online learning algorithm for the multinomial logit contextual bandit. European Journal of Operational Research, 2023. [3] Shipra Agrawal, Vashist Avadhanula, Vineet Goyal, and Assaf Zeevi. A near-optimal exploration-exploitation approach for assortment selection. Proceedings of the ACM Conference on Economics and Computation, 2016. [4] Shipra Agrawal, Vashist Avadhanula, Vineet Goyal, and Assaf Zeevi. Thompson sampling for the mnl-bandit. Conference on Learning Theory, 2017. [5] Shipra Agrawal, Vashist Avadhanula, Vineet Goyal, and Assaf Zeevi. Mnl-bandit: A dynamic learning approach to assortment selection. Operations Research, 67(5), 2019. [6] Xi Chen and Yining Wang. A note on a tight lower bound for capacitated mnl-bandit assortment selection models. Operations Research Letters, 46(5), 2018.   \n[7] Xi Chen, Yining Wang, and Yuan Zhou. Dynamic assortment optimization with changing contextual information. The Journal of Machine Learning Research, 21(1), 2020. [8] Wang Chi Cheung and David Simchi-Levi. Thompson sampling for online personalized assortment optimization problems with multinomial logit choice models. Available at SSRN 3075658, 2017. [9] Kefan Dong, Yingkai Li, Qin Zhang, and Yuan Zhou. Multinomial logit bandit with low switching cost. International Conference on Machine Learning, 2020.   \n[10] Dylan Foster and Alexander Rakhlin. Beyond ucb: Optimal and efficient contextual bandits with regression oracles. International Conference on Machine Learning, 2020.   \n[11] Dylan J Foster and Akshay Krishnamurthy. Efficient first-order contextual bandits: Prediction, allocation, and triangular discrimination. Conference on Advances in Neural Information Processing Systems, 2021.   \n[12] Dylan J Foster, Satyen Kale, Haipeng Luo, Mehryar Mohri, and Karthik Sridharan. Logistic regression: The importance of being improper. Conference On Learning Theory, 2018.   \n[13] Dylan J Foster, Claudio Gentile, Mehryar Mohri, and Julian Zimmert. Adapting to misspecification in contextual bandits. Conference on Neural Information Processing Systems, 2020.   \n[14] Dylan J Foster, Sham M Kakade, Jian Qian, and Alexander Rakhlin. The statistical complexity of interactive decision making. arXiv preprint arXiv:2112.13487, 2021.   \n[15] Dylan J Foster, Alexander Rakhlin, Ayush Sekhari, and Karthik Sridharan. On the complexity of adversarial decision making. Conference on Advances in Neural Information Processing Systems, 2022.   \n[16] Yoav Freund and Robert E Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of computer and system sciences, 55(1), 1997.   \n[17] Peter D Gr\u00fcnwald and Nishant A Mehta. Fast rates for general unbounded loss functions: from erm to generalized bayes. The Journal of Machine Learning Research, 2020.   \n[18] Yanjun Han, Yining Wang, and Xi Chen. Adversarial combinatorial bandits with general nonlinear reward functions. International Conference on Machine Learning, pages 4030\u20134039, 2021.   \n[19] Min-hwan Oh and Garud Iyengar. Thompson sampling for multinomial logit contextual bandits. Conference on Advances in Neural Information Processing Systems, 32, 2019.   \n[20] Min-hwan Oh and Garud Iyengar. Multinomial logit contextual bandits: Provable optimality and practicality. Proceedings of the AAAI conference on artificial intelligence, 35(10), 2021.   \n[21] Mingdong Ou, Nan Li, Shenghuo Zhu, and Rong Jin. Multinomial logit bandit with linear utility functions. Proceedings of the International Joint Conference on Artificial Intelligence, 2018.   \n[22] Yannik Peeters, Arnoud V den Boer, and Michel Mandjes. Continuous assortment optimization with logit choice probabilities and incomplete information. Operations Research, 70(3), 2022.   \n[23] Noemie Perivier and Vineet Goyal. Dynamic pricing and assortment under a contextual mnl demand. Conference on Advances in Neural Information Processing Systems, 35, 2022.   \n[24] Paat Rusmevichientong, Zuo-Jun Max Shen, and David B Shmoys. Dynamic assortment optimization with a multinomial logit choice model and capacity constraint. Operations research, 58(6), 2010.   \n[25] David Simchi-Levi and Yunzong Xu. Bypassing the monster: A faster and simpler optimal algorithm for contextual bandits under realizability. Mathematics of Operations Research, 2021.   \n[26] Tim Van Erven, Peter Grunwald, Nishant A Mehta, Mark Reid, Robert Williamson, et al. Fast rates in statistical and online learning. Journal of Machine Learning Research, 54(6), 2015.   \n[27] Yunbei Xu and Assaf Zeevi. Upper counterfactual confidence bounds: a new optimism principle for contextual bandits. arXiv preprint arXiv:2007.07876, 2020.   \n[28] Mengxiao Zhang and Haipeng Luo. Online learning in contextual second-price pay-per-click auctions. International Conference on Artificial Intelligence and Statistics, 2024.   \n[29] Mengxiao Zhang, Yuheng Zhang, Olga Vrousgou, Haipeng Luo, and Paul Mineiro. Practical contextual bandits with feedback graphs. Conference on Neural Information Processing Systems, 2023.   \n[30] Tong Zhang. Feel-good thompson sampling for contextual bandits and reinforcement learning. SIAM Journal on Mathematics of Data Science, 4(2), 2022.   \n[31] Yinglun Zhu and Paul Mineiro. Contextual bandits with smooth regret: Efficient learning in continuous action spaces. International Conference on Machine Learning, 2022.   \n[32] Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. International Conference on Machine Learning, 2003. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "A Additional Related Works ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "As mentioned, our work is closely related to the recent trend of designing contextual bandits algorithms for a general function class. Specifically, under stochastic context, Xu and Zeevi [27], SimchiLevi and $\\mathrm{Xu}$ [25] designed algorithms based on an offline squared loss regression oracle and achieved optimal regret guarantees. Under adversarial context, there are two lines of works. The first one reduces the contextual bandit problem to online regression [10, 11, 14, 31, 29], while the second one is based on the ability to sample from a certain distribution over the function class using Markov chain Monte Carlo methods [30, 28]. We follow and greatly extend the ideas of all these approaches to design algorithms for contextual MNL bandits. ", "page_idx": 11}, {"type": "text", "text": "B Omitted Details in Section 3 ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "B.1 Offline Regression Oracle ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "We start by proving Lemma 3.1, which shows that ERM strategy satisfies Assumption 2 for the finite class and the linear function class. ", "page_idx": 11}, {"type": "text", "text": "Proof [of Lemma 3.1] We first show that our log loss function $\\ell_{\\log}(\\mu(S,f(x)),i)$ satisfies the socalled strong 1-central condition (Definition 7 of Gr\u00fcnwald and Mehta [17]), which states that there exists $f_{0}\\in\\mathcal{F}$ , such that for any $f\\in\\mathcal F$ , ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{(x,S,i)\\sim\\mathcal{H}}\\left[\\exp(-(\\ell_{\\log}(\\mu(S,f(x)),i)-\\ell_{\\log}(\\mu(S,f_{0}(x)),i)))\\right]\\le1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "Indeed, by picking $f_{0}=f^{\\star}$ , we know that ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{(x,S,i)\\sim\\mathcal{H}}\\left[\\exp(-(\\ell_{\\log}(\\mu(S,f(x),i))-\\ell_{\\log}(S,f^{\\star}(x),i)))\\right]}\\\\ &{=\\mathbb{E}_{(x,S)}\\mathbb{E}_{i\\sim\\mu(S,f^{\\star}(x))}\\left[\\frac{\\mu_{i}(S,f)}{\\mu_{i}(S,f^{\\star})}\\right]}\\\\ &{=\\mathbb{E}_{(x,S)}\\left[\\displaystyle\\sum_{i\\in S\\cup\\{0\\}}\\mu_{i}(S,f)\\right]=1,}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "certifying the strong 1-central condition. ", "page_idx": 11}, {"type": "text", "text": "Now, we first consider the case where $\\mathcal{F}$ is finite. Since $f_{i}(x)\\geq\\beta$ for all $x\\in\\mathscr{X}$ and $i\\in[N]$ , we know that for any $i\\in[N]_{0}$ , we have (defining $f_{0}(x)=1$ ) ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\ell_{\\log}(\\mu(S,f(x)),i)=\\log\\frac{1+\\sum_{j\\in S}f_{j}(x)}{f_{i}(x)}\\leq\\log\\frac{K+1}{\\beta}.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "Therefore, according to Theorem 7.6 of [26], we know that given $n$ i.i.d samples $\\textit{D}=$ $\\{(x_{k},S_{k},i_{k})\\}_{k\\in[n]}$ , ERM predictor $\\widehat{f}_{D}$ guarantees that with probability $1-\\delta$ : ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\mathbb{E}_{(x,S,i)\\sim\\mathcal{D}}\\left[\\ell_{\\log}(\\mu(S,\\widehat{f}_{D}(x)),i)\\right]\\leq\\mathbb{E}_{(x,S,i)\\sim\\mathcal{D}}\\left[\\ell_{\\log}(\\mu(S,f^{\\star}(x)),i)\\right]+\\mathcal{O}\\left(\\frac{\\log\\frac{K}{\\beta}\\log\\frac{|\\mathcal{F}|}{\\delta}}{n}\\right).\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "Next, we consider the linear function class. In this case, we know that $x_{i}^{\\top}\\theta-B\\in[-2B,0]$ for all $x_{i}$ . Therefore, $\\ell_{\\log}(\\mu(S,f(x)),i)$ is bounded by $2B+2\\ln N$ for all $x\\in\\mathscr{X}$ , $f\\,\\in\\,{\\mathcal{F}}$ , $S\\,\\in\\,S$ and $i\\in[N]$ since ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\ell_{\\log}(\\mu(S,f(x)),i)=\\log\\frac{1+\\sum_{j\\in S}\\exp(x_{j}^{\\top}\\theta-B)}{\\exp(x_{i}^{\\top}\\theta-B)}\\le\\log\\frac{1+K}{e^{-2B}}\\le2B+2\\log K,\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "and the same bound clearly holds as well for $i=0$ . Moreover, since ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\left\\|\\nabla_{\\theta}\\log\\frac{1+\\sum_{j\\in S}\\exp(x_{j}^{\\top}\\theta-B)}{\\exp(x_{i}^{\\top}\\theta-B)}\\right\\|_{2}=\\left\\|\\frac{\\sum_{j\\in S}\\exp(\\theta^{\\top}x_{j}-B)x_{j}}{1+\\sum_{j\\in S}\\exp(\\theta^{\\top}x_{j}-B)}-x_{i}\\right\\|_{2}\\le2,\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "we know that the $\\varepsilon$ -covering number of $\\ell_{\\log}\\circ\\mathcal{F}\\triangleq\\{\\ell_{\\log}^{f}:f\\in\\mathcal{F}\\}$ is bounded by $\\left(\\frac{16B}{\\varepsilon}\\right)^{d}$ , where with an abuse of notation, we define $Z\\triangleq(x,S,i)$ and denote $\\ell_{\\log}(\\mu(S,f(x)),i)$ by $\\ell_{\\mathrm{log}}^{f}(Z)$ . Therefore, according to Theorem 7.7 of [26], we know that given $n$ i.i.d samples $D\\,=\\,\\{(\\stackrel{\\smile}{x_{k}},S_{k},i_{k})\\}_{k\\in[n]}$ , ERM predictor $\\widehat{f}_{D}$ guarantees that with probability $1-\\delta$ : ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathfrak{L}_{(x,S,i)\\sim\\mathcal{D}}\\left[\\ell_{\\log}(\\mu(S,\\widehat{f}_{D}(x)),i)\\right]\\leq\\mathbb{E}_{(x,S,i)\\sim\\mathcal{D}}\\left[\\ell_{\\log}(\\mu(S,f^{\\star}(x)),i)\\right]+\\mathcal{O}\\left(\\frac{d B\\log K\\log(B n)\\log\\frac{1}{\\delta}}{n}\\right)\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "B.2 Analysis of Algorithm 1 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We first prove the following lemma, which shows that the expected reward function $R(S,v,r)$ is 1-Lipschitz in the value vector $v$ . ", "page_idx": 12}, {"type": "text", "text": "Lemma B.1 Given $r\\in[0,1]^{N}$ and $S\\subseteq[N],$ , function $\\begin{array}{r}{R(S,v,r)=\\frac{\\sum_{i\\in S}r_{i}v_{i}}{1+\\sum_{i\\in S}v_{i}}}\\end{array}$ satisfies that for any $\\begin{array}{r}{v^{\\prime},v\\in[0,\\infty)^{N},\\,|R(S,v,r)-R(S,v^{\\prime},r)|\\leq\\sum_{i\\in S}|v_{i}-v_{i}^{\\prime}|.}\\end{array}$   \nProof Taking derivative with respect to $v_{j}$ for $j\\in S$ , we know that   \n[ $\\nabla_{v_{j}}R(S,v,r)\\big|=\\left|\\frac{r_{j}(1+\\sum_{i\\in S}v_{i})-\\sum_{j\\in S}r_{j}v_{j}}{(1+\\sum_{i\\in S}v_{i})^{2}}\\right|\\leq\\operatorname*{max}\\left\\{\\frac{r_{j}}{1+\\sum_{i\\in S}v_{i}},\\frac{\\sum_{i\\in S}v_{i}}{(1+\\sum_{i\\in S}v_{i})^{2}}\\right\\}\\leq1,$ ", "page_idx": 12}, {"type": "text", "text": "where both inequalities are because $r_{j}\\in[0,1]$ . This finishes the proof. ", "page_idx": 12}, {"type": "text", "text": "Next, we restate and prove Lemma 3.2. ", "page_idx": 12}, {"type": "text", "text": "Lemma B.2 For any deterministic policy $\\pi:\\mathcal{X}\\times[0,1]^{N}\\to\\mathcal{S}$ and any epoch $m\\geq2$ , we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n|R_{m}(\\pi)-R(\\pi)|\\leq{\\sqrt{V(q_{m-1},\\pi)}}\\cdot{\\sqrt{\\mathbb{E}_{(x,r)\\sim{\\mathcal{D}},S\\sim q_{m-1}(x,r)}\\left[\\sum_{i\\in S}{(f_{m,i}(x)-f_{i}^{\\star}(x))}^{2}\\right]}}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Proof We proceed as: ", "text_level": 1, "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|R_{m}(\\overline{{u}})-R(\\overline{{u}})\\right|}\\\\ &{=\\left|\\overline{{\\mathbf{g}_{\\delta}(\\mathbf{r},\\gamma)}}\\times[R(\\overline{{u}},\\gamma),f_{m}(\\alpha,\\gamma),{\\gamma}-R(\\langle\\mathbf{z},\\pi,\\rangle,f^{\\prime}(\\alpha),\\gamma)]\\right|}\\\\ &{\\leq\\mathbb{E}_{(\\underline{{u}},\\gamma)\\sim\\mathcal{D}}\\left[\\displaystyle\\sum_{i=1}^{N}\\mathbb{E}_{\\left(\\mathbf{z},\\gamma\\right)}\\left[\\|f_{m,{\\alpha}}(\\alpha-f_{i})-f_{i}(\\alpha)\\right|\\right]}\\\\ &{\\leq\\mathbb{E}_{(\\underline{{u}},\\gamma)\\sim\\mathcal{D}}\\left[\\left\\langle\\sum_{i=1}^{N}\\mathbb{E}_{\\left(\\mathbf{z},\\gamma\\right)}\\right\\rangle\\displaystyle\\sum_{i=1}^{N}w_{(i,-1)\\gamma}\\left(f_{m,\\alpha}(x)-f_{i}(\\alpha)\\right)^{2}\\right]}\\\\ &{\\leq\\left|\\mathbb{E}_{(\\underline{{u}},\\gamma)\\sim\\mathcal{D}}\\left[\\displaystyle\\sum_{i=1}^{N}\\mathbb{E}_{\\left(\\mathbf{z},\\gamma\\right)}\\displaystyle\\sum_{i=1}^{N}\\left\\langle\\sum_{s=t,s=t,s=t,s=t,s=t,s=t,s=t,s=t,s=t,s=t,s=t,s=t,s=t,s=t,s=t,s=t,s=t,s=t,s=t,s=t,s}^{\\operatorname*{deang}}\\right\\rangle\\right]}\\\\ &{\\leq\\sqrt{\\mathbb{E}_{(\\underline{{u}},\\gamma)\\sim\\mathcal{D}}\\left[\\displaystyle\\sum_{i=1}^{N}\\mathbb{E}_{\\left(\\underline{{u}},\\gamma\\right)}\\displaystyle\\sum_{i=1}^{N}\\left\\langle\\sum_{s=t,s=t,s=t,s=t,s=t,s=t,s=t,s=t,s=t,s=t,s=t,s=t,s=t,s=t,s=t,s=t,s=t,s=t,s=t,s}^{\\operatorname*{deang}}\\right\\rangle}}\\\\ &{=\\sqrt{\\mathbb{V}(q_{\\delta},\\gamma)}\\left\\langle\\sum_{i=1}^{N}w_{(i,-s)\\sim\\mathcal{D}}\\left[\\displaystyle\\sum_{i=1}^{N}w_{(i,-s)\\sim\\mathcal{D}}(f_{m,\\alpha}(x)-f_{i}(\\alpha))^{2}\\right]}\\right\\rangle}\\\\ &{=\\sqrt{\\mathbb{V}(q_{\\delta},\\gamma)}\\left\\{\n$$", "text_format": "latex", "page_idx": 12}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where the first inequality uses the convexity of the absolute value function and Lemma B.1. ", "page_idx": 12}, {"type": "text", "text": "Next, to prove Lemma 3.3, we first prove the following key technical lemma (where 1 denotes the all-one vector). ", "page_idx": 12}, {"type": "text", "text": "Lemma B.3 Let $\\begin{array}{r}{h(a)=\\frac{a}{1+\\mathbf{1}^{\\top}a}}\\end{array}$ for $a\\in[0,1]^{d}$ . Then, for any $a,b\\in[0,1]^{d}$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n{\\frac{1}{2(d+1)^{4}}}\\|a-b\\|_{2}^{2}\\leq\\|h(a)-h(b)\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof The Jacobian matrix of $h$ is ", "page_idx": 13}, {"type": "equation", "text": "$$\nH(a)={\\frac{1}{1+\\mathbf{1}^{\\top}a}}\\mathbf{I}-{\\frac{\\mathbf{1}a^{\\top}}{(1+\\mathbf{1}^{\\top}a)^{2}}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Therefore, there exists $z\\,\\in\\,\\mathrm{conv}(\\{a,b\\})$ such that $\\|h(a)\\-h(b)\\|_{2}\\,=\\,\\|H(z)(a\\mathrm{~-~}b)\\|_{2}$ . It thus remains to figure out the minimum singular value of $H(z)$ , which is equal to the reciprocal of the spectral norm of $H(z)^{-1}$ . By Sherman-Morrison formula, we know that ", "page_idx": 13}, {"type": "equation", "text": "$$\nH(z)^{-1}=(1+\\mathbf{1}^{\\top}z)(\\mathbf{I}+\\mathbf{1}z^{\\top}).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Therefore, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{H(z)^{-1}H(z)^{-\\top}=(1+\\mathbf{1}^{\\top}z)^{2}(\\mathbf{I}+\\mathbf{1}z^{\\top})(\\mathbf{I}+\\mathbf{1}z^{\\top})^{\\top}}\\\\ &{~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~}=(1+\\mathbf{1}^{\\top}z)^{2}(\\mathbf{I}+\\mathbf{1}z^{\\top}+z\\mathbf{1}^{\\top}+z^{\\top}z\\mathbf{1}\\mathbf{1}^{\\top}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Note that for any $u$ that is perpendicular to the subspace spanned by $\\{z,\\mathbf{1}\\}$ , we have $H(z)^{-1}H(z)^{-\\top}u=(1+1^{\\top}z)^{2}u$ . Therefore, there are $d-2$ identical eigenvalues 1 for the matrix $\\begin{array}{r}{\\frac{1}{(1+\\mathbf{1}^{\\top}z)^{2}}H(z)^{-1}H(z)^{-\\top}}\\end{array}$ . Let the remaining two eigenvalues of (1+11\u22a4z)2 H(z)\u22121H(z)\u2212\u22a4be \u03bb1 and $\\lambda_{2}$ . Note that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\lambda_{1}\\lambda_{2}=\\operatorname*{det}\\big(({\\mathbf I}+{\\mathbf1}z^{\\top})({\\mathbf I}+z{\\mathbf1}^{\\top})\\big)=(1+{\\mathbf1}^{\\top}z)^{2},}\\\\ &{\\lambda_{1}+\\lambda_{2}=\\operatorname{Trace}({\\mathbf I}+{\\mathbf1}z^{\\top}+z{\\mathbf1}^{\\top}+z^{\\top}z{\\mathbf1}{\\mathbf1}^{\\top})-(d-2)}\\\\ &{\\quad\\quad\\quad=2+2{\\mathbf1}^{\\top}z+z^{\\top}z{\\mathbf1}^{\\top}{\\mathbf1}}\\\\ &{\\quad\\quad\\quad=2+2{\\mathbf1}^{\\top}z+d\\cdot z^{\\top}z}\\\\ &{\\leq2+2d+d^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Therefore, we know that $\\operatorname*{max}\\{\\lambda_{1},\\lambda_{2}\\}\\leq\\lambda_{1}+\\lambda_{2}\\leq2+2d+d^{2}$ , meaning that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\|H(z)^{-1}H(z)^{-\\top}\\|_{2}\\le2(1+\\mathbf{1}^{\\top}z)^{2}(1+d+d^{2})\\le2(1+d)^{2}(d^{2}+d+1)\\le2(d+1)^{4}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "This further means that the minimum singular value of $H(z)$ is at least $\\scriptstyle{\\frac{1}{\\sqrt{2}(d+1)^{2}}}$ . Therefore, we can conclude that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\|h(a)-h(b)\\|_{2}\\geq\\frac{1}{\\sqrt{2}(d+1)^{2}}\\|a-b\\|_{2},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "leading to ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\|h(a)-h(b)\\|_{2}^{2}\\geq\\frac{1}{2(d+1)^{4}}\\|a-b\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "\u25a1Next, we restate and prove Lemma 3.3. ", "page_idx": 13}, {"type": "text", "text": "Lemma B.4 For any $S\\in S$ and $v,v^{\\star}\\in[0,1]^{N}$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{1(K+1)^{4}}\\displaystyle\\sum_{i\\in S}(v_{i}-v_{i}^{\\star})^{2}\\leq\\|\\mu(S,v)-\\mu(S,v^{\\star})\\|_{2}^{2}\\leq2\\mathbb{E}_{i\\sim\\mu(S,v^{\\star})}\\left[\\ell_{\\log}(\\mu(S,v),i)-\\ell_{\\log}(\\mu(S,v^{\\star}),i)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof The first inequality follows directly from Lemma B.3 using the fact that $|S|\\,\\le\\,K$ for all $S\\,\\in\\,S$ . Consider the second inequality. For any $\\mu,\\mu^{\\prime}\\,\\in\\,\\Delta([K])$ , by definition of $\\ell_{\\log}(\\mu,i)$ , we know that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{E}_{i\\sim\\mu}\\left[\\ell_{\\log}(\\mu^{\\prime},i)-\\ell_{\\log}(\\mu,i)\\right]=\\mathbb{E}_{i\\sim\\mu}\\left[\\log\\frac{\\mu_{i}}{\\mu_{i}^{\\prime}}\\right]=\\mathrm{KL}(\\mu,\\mu^{\\prime})\\geq\\frac{1}{2}\\|\\mu-\\mu^{\\prime}\\|_{1}^{2}\\geq\\frac{1}{2}\\|\\mu-\\mu^{\\prime}\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where the first inequality is due to Pinsker\u2019s inequality. ", "page_idx": 13}, {"type": "text", "text": "B.3 Omitted Details in Section 3.1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we show omitted details in Section 3.1. For ease of presentation, we assume that the distribution over context-reward pair $\\mathcal{D}$ has finite support. All our results can be directly generalized to the case with infinite support following a similar argument in Appendix A.7 of [25]. Define $\\Psi:\\mathcal{X}\\times[0,1]^{N}\\mapsto\\mathcal{S}$ as the set of all deterministic policy. Following Lemma 3 in [25], we know that for any context $x\\in\\mathscr{X}$ and reward vector $r\\in[0,1]^{N}$ , and any stochastic policy $q:\\bar{\\mathcal{X}}\\times[0,1]^{N}\\mapsto$ $\\Delta(S)$ , there exists an equivalent randomized policy $Q\\in\\Delta(\\Psi)$ such that for all $S\\in S$ , $x\\in\\mathscr{X}$ , and $r\\in[0,1]^{N}$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\nq(S|x,r)=\\sum_{\\pi\\in\\Psi}\\mathbb{1}\\{\\pi(x,r)=S\\}Q(\\pi).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Let $Q_{m}$ be the randomized policy induced by $q_{m}$ . Define $\\operatorname{Reg}(\\pi)$ and $\\mathrm{Reg}_{m}(\\pi)$ as: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{Reg}(\\pi)=R(\\pi_{f^{\\star}})-R(\\pi),\\quad\\mathrm{Reg}_{m}(\\pi)=R_{m}(\\pi_{f_{m}})-R_{m}(\\pi),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $R(\\pi)$ and $R_{m}(\\pi)$ are defined in Eq. (3) and $\\pi_{f}$ is the policy that maps each $(x,r)$ to the one-hot distribution supported on a $\\operatorname{\\mathrm{zgmax}}_{S\\in S}R(S,f({\\bar{x}}),r)$ . ", "page_idx": 14}, {"type": "text", "text": "Following the analysis in [25], we show that to analyze our algorithms expected regret, we only need to analyze the induced randomized policies implicit regret. ", "page_idx": 14}, {"type": "text", "text": "Lemma B.5 Fix any epoch m. For any round $t$ in this epoch, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{E}_{(x_{t},r_{t})\\sim\\mathcal{D},S_{t}\\sim q_{m}(x_{t},r_{t})}\\left[R(\\pi_{f^{\\star}}(x_{t},r_{t}),f^{\\star}(x),r_{t})-R(S_{t},f^{\\star}(x),r_{t})\\right]=\\sum_{\\pi\\in\\Psi}Q_{m}(\\pi)\\mathrm{Reg}(\\pi).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof Direct calculation shows that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{(\\pi,\\pi_{r})\\sim\\mathcal{P},S^{\\pi_{\\theta}}\\sim(\\pi_{r},\\pi_{t})}\\left[R(\\pi_{r^{\\star}}(x,\\pi_{r}),f^{*}(x,\\pi_{t}),r^{\\star}(x),r^{\\star}(x),r^{\\star}(x),r)\\right]}\\\\ &{=\\mathbb{E}_{(\\pi_{r},\\pi_{t})\\sim\\mathcal{P}}\\left[R(\\pi_{r^{\\star}}(x,\\pi_{r}),f^{*}(x),r)\\sim\\sum_{\\underline{{\\pi}}\\in\\mathcal{S}}(\\delta|x_{\\pi},r_{\\pi})R(S_{f}^{*}(x),r_{\\pi})\\right]}\\\\ &{=\\mathbb{E}_{(\\pi_{r},\\pi_{t})\\sim\\mathcal{P}}\\left[R(\\pi_{r^{\\star}}(x,\\pi_{r}),f^{*}(x),r_{\\pi})\\sim\\sum_{\\underline{{\\pi}}\\in\\mathcal{S}}\\sum_{c\\in\\mathcal{C}}\\mathbb{E}_{\\pi}\\{\\pi(x,r_{\\pi})=S\\}Q_{m}(\\pi)R(S_{f}^{*}(x),r_{\\pi})\\right]}\\\\ &{=\\mathbb{E}_{(\\pi_{r})\\sim\\mathcal{P}}\\left[\\sum_{c\\in\\mathcal{S}}\\sum_{1\\in\\mathcal{C}}|c(x,r)-S|Q_{m}(\\pi)(R(\\pi_{r^{\\star}}(x,r),f^{*}(x),r)-R(S_{f}^{*}(x),r))\\right]}\\\\ &{=\\mathbb{E}_{(\\pi_{r})\\sim\\mathcal{P}}\\left[\\sum_{c\\in\\mathcal{C}}(\\pi_{r})(R(\\pi_{r^{\\star}}(x,r),f^{*}(x),r)-R(\\pi(x,r),f^{*}(x),r))\\right]}\\\\ &{=\\sum_{c\\in\\mathcal{C}}Q_{m}(\\pi)\\mathbb{E}_{(\\pi_{r^{\\star}})\\sim\\mathcal{P}}\\left[R(\\pi_{r^{\\star}}(x,r),f^{*}(x),r)-R(\\pi(x,r),f^{*}(x),r)\\right]}\\\\ &{=\\frac{\\sum_{c\\in\\mathcal{C}}Q_{m}(\\pi_{r})\\mathbb{E}_{(\\pi_{r^{\\star}})\\sim\\mathcal{P}}\\left[R(\\pi_{r^{\\star}}(x,r),f^{*}(x),r)-R(\\pi(x \n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which finishes the proof. ", "page_idx": 14}, {"type": "text", "text": "To prove our main results for Algorithm 1, we define the following good event: ", "page_idx": 14}, {"type": "text", "text": "Event 1 For all epoch $m\\geq2$ , $f_{m}$ satisfies ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{(x,r)\\sim\\mathcal{D},S\\sim q_{m-1}(x,r),i\\sim\\mu(S,f^{\\star}(x))}\\left[\\ell_{\\log}(\\mu(S,f_{m}(x)),i)-\\ell_{\\log}(\\mu(S,f^{\\star}(x)),i)\\right]}\\\\ &{\\ \\le\\mathbf{Err}_{\\log}(\\tau_{m}-\\tau_{m-1},1/T^{2},\\mathcal{F}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "According to Assumption 2, Event 1 happens with probability at least $\\textstyle1-{\\frac{1}{T}}$ since there are at most $T$ epochs. ", "page_idx": 14}, {"type": "text", "text": "Although now we have all ingredients to analyze our $\\varepsilon$ -greedy-type algorithm defined Eq. (4), to get the exact result in Theorem 3.4, we will in fact need a refined version of Lemma 3.2, which eventually provides a tighter regret guarantee. ", "page_idx": 14}, {"type": "text", "text": "Lemma B.6 Suppose that Event $^{\\,l}$ holds. Algorithm $^{\\,l}$ with $q_{t}$ defined in Eq. (4) satisfies that for any deterministic policy $\\pi\\in\\Psi$ and any epoch $m\\geq2$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n|R_{m}(\\pi)-R(\\pi)|\\leq8\\sqrt{\\frac{N K}{\\varepsilon_{m-1}}}\\cdot\\sqrt{\\mathbf{Err}_{\\log}(2^{m-2},1/T^{2},\\mathcal{F})}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof Following Eq. (11) in the proof of Lemma 3.2, we know that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle R_{m}(\\pi)-R(\\pi)}}\\\\ {{\\displaystyle\\leq\\mathbb{E}_{(\\alpha,\\gamma)\\sim\\mathcal{D}}\\left[\\sum_{i=1}^{N}\\mathbb{1}\\{i\\in\\pi(x,r)\\}|f_{m,i}(x)-f_{i}^{\\ast}(x)|\\right]}}\\\\ {{\\displaystyle\\leq\\mathbb{E}_{(\\alpha,\\gamma)\\sim\\mathcal{D}}\\left[\\sqrt{\\sum_{i=1}^{N}\\sum_{i=1}^{N}\\{i\\in\\pi(x,r)\\}\\sum_{i=1}^{N}\\frac{\\xi_{\\alpha}}{N}\\frac{\\xi_{\\alpha}-1}{(\\pi_{\\alpha},i)}\\left(f_{m,i}(x)-f_{i}^{\\ast}(x)\\right)^{2}\\right]}}\\\\ {{\\displaystyle\\leq\\sqrt{\\mathbb{E}_{(\\alpha,\\gamma)\\sim\\mathcal{D}}\\left[\\sum_{i=1}^{N}\\frac{N!\\left\\{i\\in\\pi(x,r)\\right\\}}{\\xi_{\\alpha}-1}\\right]}\\cdot\\sqrt{\\mathbb{E}_{(\\alpha,\\gamma)\\sim\\mathcal{D}}\\left[\\sum_{i=1}^{N}\\frac{\\xi_{\\alpha}-1}{N}\\left(f_{m,i}(x)-f_{i}^{\\ast}(x)\\right)^{2}\\right]}}\\\\ {{\\displaystyle\\leq\\sqrt{\\frac{N K}{\\xi_{\\alpha}-1}}\\cdot\\sqrt{\\mathbb{E}_{(\\alpha,\\gamma)\\sim\\mathcal{D}}\\left[\\sum_{i=1}^{N}\\frac{\\xi_{\\alpha}-1}{N}\\left(f_{m,i}(x)-f_{i}^{\\ast}(x)\\right)^{2}\\right]}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since $f_{m}$ is the output of $\\mathsf{A l g}_{\\mathsf{o f f}}$ with i.i.d tuples $\\{(x_{t},S_{t},i_{t})\\}_{t=\\tau_{m-1}+1}^{\\tau_{m}}$ , according to Lemma 3.3 and Event 1, we know that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{64\\mathrm{Errlog}(\\tau_{m}-\\tau_{m-1},1/T^{2},\\mathcal{F})}}\\\\ &{\\geq32\\mathbb{E}_{(x,r)\\sim\\mathcal{D},S\\sim q_{m-1}(x,r)}\\left[\\|\\mu(S,f_{m}(x))-\\mu(S,f^{\\star}(x))\\|_{2}^{2}\\right]}&{\\mathrm{(Lemma~3.3)}}\\\\ &{\\geq\\frac{32\\varepsilon_{m-1}}{N}\\sum_{i=1}^{N}\\mathbb{E}_{(x,r)\\sim\\mathcal{D}}\\left[\\|\\mu(\\{i\\},f_{m}(x))-\\mu(\\{i\\},f^{\\star}(x))\\|_{2}^{2}\\right]}&{\\mathrm{(according~to~Eq.~(4))}}\\\\ &{\\geq\\frac{\\varepsilon_{m-1}}{N}\\sum_{i=1}^{N}\\mathbb{E}_{(x,r)\\sim\\mathcal{D}}\\left[\\sum_{i=1}^{N}\\left(f_{m,i}(x)-f_{i}^{\\star}(x)\\right)^{2}\\right].}&{\\mathrm{(using~Lemma~B.3~with~}d=1)}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Plugging the above inequality back to Eq. (14) and noticing that $\\tau_{m}=2^{m-1}-1$ , we know that ", "page_idx": 15}, {"type": "equation", "text": "$$\n|R_{m}(\\pi)-R(\\pi)|\\leq8{\\sqrt{{\\frac{N K}{\\varepsilon_{m-1}}}\\cdot\\mathbf{Err}_{\\log}(2^{m-2},1/T^{2},{\\mathcal{F}})}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Now we are ready to prove Theorem 3.4 ", "page_idx": 15}, {"type": "text", "text": "Theorem 3.4 Under Assumption $^{\\,l}$ and Assumption 2, Algorithm $^{\\,l}$ with $q_{m}$ defined in Eq. (4) and the optimal choice of \u03b5m ensures RegMNL = \u2308mlo=g12 $\\begin{array}{r}{\\mathbf{Reg}_{\\mathsf{M N L}}=\\sum_{m=1}^{\\lceil\\log_{2}T\\rceil}\\mathcal{O}\\left(2^{m}(N K\\mathbf{Err}_{\\log}(2^{m-1},1/T^{2},\\mathcal{F}))^{\\frac{1}{3}}\\right)}\\end{array}$ . ", "page_idx": 15}, {"type": "text", "text": "Proof Consider the regret within epoch $m\\geq2$ . Under Event 1, we know that for any $\\pi\\in\\Psi$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Reg}(\\pi)=R(\\pi_{f^{\\star}})-R(\\pi)}\\\\ &{\\qquad\\quad=(R(\\pi_{f^{\\star}})-R_{m}(\\pi_{f_{m}}))-(R_{m}(\\pi)-R_{m}(\\pi_{f_{m}}))+(R_{m}(\\pi)-R(\\pi))}\\\\ &{\\qquad\\quad\\leq(R(\\pi_{f^{\\star}})-R_{m}(\\pi_{f^{\\star}}))+(R_{m}(\\pi_{f_{m}})-R_{m}(\\pi))+(R_{m}(\\pi)-R(\\pi))}\\\\ &{\\qquad\\quad\\leq(R_{m}(\\pi_{f_{m}})-R_{m}(\\pi))+16\\sqrt{\\frac{N K}{\\varepsilon_{m-1}}\\cdot\\mathbf{Err}_{\\log}(2^{m-2},1/T^{2},\\mathcal{F})},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the first inequality is because $R_{m}(\\pi_{f_{m}})~\\ge~R_{m}(\\pi_{f^{\\star}})$ by definition and the second inequality is due to Lemma B.6. Taking summation over all rounds within epoch $m$ and picking $\\bar{\\varepsilon_{m}}=\\bar{(N K)}^{\\frac{1}{3}}\\mathbf{Err}_{\\mathrm{log}}^{\\frac{1}{3}}(2^{m-2},1/T^{2},\\mathcal{F})$ , we know that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\underset{t=r_{m+1}}{\\overset{r_{m+1}}{\\sum}}\\left(\\frac{\\operatorname*{max}}{\\mathrm{St}\\delta}R(S,x_{t},f^{*}(x_{t}))-R(S_{t},x_{t},f^{*}(x_{t}))\\right)\\right]}\\\\ &{=(\\tau_{m+1}-\\tau_{m})\\mathbb{E}\\left[\\sum_{\\tau\\in\\mathcal{T}_{m}}(\\pi_{m})\\mathrm{Reg}(\\pi)\\right]}\\\\ &{\\overset{(i)}{\\le}2^{m-1}\\cdot\\mathbb{E}\\left[(1(-\\tau_{m})\\mathrm{Reg}(\\pi_{m})+\\varepsilon_{m})\\right]}\\\\ &{\\overset{(i i)}{\\le}\\frac{2^{m-1}}{T}+2^{m-1}\\mathbb{E}\\left[((1-\\varepsilon_{m})\\mathrm{Reg}(\\pi_{f_{m}})+\\varepsilon_{m})\\right]\\mathrm{Event~I~holds}\\right]}\\\\ &{\\overset{(i i i)}{\\le}\\frac{2^{m-1}}{T}+2^{m-1}\\left(\\varepsilon_{m}+16\\sqrt{\\frac{N K}{\\varepsilon_{m-1}}}\\cdot\\mathrm{Err}_{\\log(2^{m-2},1/T^{2},F)}\\right)}\\\\ &{\\overset{(i i i)}{=}\\frac{2^{m-1}}{T}+\\mathcal{O}\\left(2^{m-1}\\left(N K\\mathrm{Err}_{\\mathrm{Reg}}(2^{m-2},1/T^{2},F)\\right)^{\\frac{3}{2}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $(i)$ is due to $\\tau_{m}\\,=\\,2^{m-1}\\,-\\,1$ and the construction of $q_{m}(x,r)$ defined in Eq. (4); $(i i)$ is because Event 1 holds with probability at least $\\textstyle1\\,-\\,{\\frac{1}{T}}$ ; $(i i i)$ uses Eq. (15); and $(i v)$ is due to the choice of $\\varepsilon_{m}$ . Taking summation over all $m=2,3,\\ldots\\left\\lceil\\log_{2}T\\right\\rceil+1$ epochs, we can obtain that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{Reg}_{\\mathtt{M N L}}=\\sum_{m=1}^{\\lceil\\log_{2}T\\rceil}\\mathcal{O}\\left(2^{m}\\left(N K\\mathbf{Err}_{\\log}(2^{m-1},1/T^{2},\\mathcal{F})\\right)^{\\frac{1}{3}}\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "B.4 Omitted Details in Section 3.2 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "First, we restate and prove Lemma 3.6, which shows that $q_{m}$ defined in Eq. (5) enjoys a low-regrethigh-dispersion guarantee. ", "page_idx": 16}, {"type": "text", "text": "Lemma B.7 For any $x\\in\\mathscr{X}$ and $r\\in[0,1]^{N}$ , the distribution $q_{m}(x,r)$ defined in Eq. (5) satisfies: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{max}_{S^{\\star}\\in S}R(S^{\\star},f_{m}(x),r)-\\mathbb{E}_{S\\sim q_{m}(x,r)}\\left[R(S,f_{m}(x),r)\\right]\\le\\frac{N(K+1)^{4}}{\\gamma_{m}},}\\\\ {\\forall S\\in S,}&{\\displaystyle\\sum_{i\\in S}\\frac{1}{w_{i}(q_{m}(x,r))}\\le N+\\frac{\\gamma_{m}}{(K+1)^{4}}\\left(\\operatorname*{max}_{S^{\\star}\\in S}R(S^{\\star},f_{m}(x),r)-R(S,f_{m}(x),r)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof It is direct to see that solving Eq. (5) is equivalent to solving the following optimization problem: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{argmin}_{\\rho\\in\\Delta(S)}\\mathbb{E}_{S\\sim\\rho}\\left[\\operatorname*{max}_{S^{\\star}\\in S}R(S^{\\star},f_{m}(x),r)-R(S,f_{m}(x),r)\\right]+\\frac{(K+1)^{4}}{\\gamma_{m}}\\sum_{i=1}^{N}\\log\\frac{1}{w_{i}(\\rho)}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Moreover, relaxing the constraint $\\rho$ from $\\Delta(S)$ to $\\{\\rho\\in[0,1]^{S}:\\textstyle\\sum_{S\\in\\mathcal{S}}\\rho(S)\\leq1\\}$ in Eq. (16) does not change the solution, since for any $\\rho\\in[0,1]^{S}$ such that $\\begin{array}{r}{\\sum_{S\\in\\mathcal S}\\rho(S)<1}\\end{array}$ , putting the remaining $\\textstyle1-\\sum_{S\\in S}\\rho(S)$ probability mass on a $\\mathrm{rgmax}_{S^{\\star}\\in S}\\,R(S^{\\star},f_{m}(\\bar{x}),r)$ can only make the objective smaller. ", "page_idx": 16}, {"type": "text", "text": "Now, consider the Lagrangian form of Eq. (16) over this relaxed constraint and set the derivative with respect to $\\rho(S)$ to zero. We obtain ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{S^{\\star}\\in S}R(S^{\\star},f_{m}(x),r)-R(S,f_{m}(x),r)-\\frac{(K+1)^{4}}{\\gamma_{m}}\\sum_{i:i\\in S}\\frac{1}{w_{i}(\\rho)}-\\lambda(S)+\\lambda=0,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\lambda\\geq0$ and $\\lambda(S)\\ge0$ , $S\\in S$ are the Lagrangian multipliers. Let $\\rho^{\\star}\\in\\Delta(S)$ be the optimal solution of Eq. (16). Replacing $\\rho$ by $\\rho^{\\star}$ in Eq. (17), multiplying Eq. (17) by $\\rho^{\\star}(S)$ for each $S\\in S$ , and taking the summation over $S\\in S$ , we know that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{S\\in S}\\rho^{\\star}(S)\\left(\\operatorname*{max}_{S^{\\star}\\in S}R(S^{\\star},f_{m}(x),r)-R(S,f_{m}(x),r)\\right)}\\\\ &{\\quad\\quad-\\,\\frac{(K+1)^{4}}{\\gamma_{m}}\\displaystyle\\sum_{S\\in S}\\rho^{\\star}(S)\\sum_{i:i\\in S}\\frac{1}{w_{i}(\\rho^{\\star})}-\\sum_{S\\in S}\\rho^{\\star}(S)\\lambda(S)+\\lambda=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Rearranging the terms, we know that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{S\\in S}\\rho^{*}(S)\\left(\\operatorname*{max}_{S^{*}\\in S}R(S^{*},f_{m}(x),r)-R(S,f_{m}(x),r)\\right)}\\\\ &{\\displaystyle=\\frac{(K+1)^{4}}{\\gamma_{m}}\\sum_{S\\in\\mathcal{S}}\\rho^{*}(S)\\sum_{i:\\;i\\in S}\\frac{1}{w_{i}(\\rho^{*})}+\\sum_{S\\in\\mathcal{S}}\\rho^{*}(S)\\lambda(S)-\\lambda}\\\\ &{\\displaystyle=\\frac{(K+1)^{4}}{\\gamma_{m}}\\sum_{i=1}^{N}\\frac{1}{w_{i}(\\rho^{*})}\\sum_{S\\in S\\;;\\;i\\in S}\\rho^{*}(S)-\\lambda}\\\\ &{\\displaystyle=\\frac{N(K+1)^{4}}{\\gamma_{m}}-\\lambda\\le\\frac{N(K+1)^{4}}{\\gamma_{m}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "mplementary slackness) ", "page_idx": 17}, {"type": "text", "text": "proving Eq. (6). The above also implies that $\\begin{array}{r}{\\lambda\\leq\\frac{N(K+1)^{4}}{\\gamma_{m}}}\\end{array}$ since ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{S\\in S}\\rho^{\\star}(S)\\left(\\operatorname*{max}_{S^{\\star}\\in S}R(S^{\\star},f_{m}(x),r)-R(S,f_{m}(x),r)\\right)\\geq0.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, Eq. (17) implies that for any $S\\in S$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{i\\colon i\\in S}\\frac{1}{w_{i}(\\rho^{\\star})}=\\frac{\\gamma_{m}}{(K+1)^{4}}\\left(\\operatorname*{max}_{S^{\\star}\\in S}R(S^{\\star},f_{m}(x),r)-R(S,f_{m}(x),r)-\\lambda_{S}+\\lambda\\right)}}\\\\ &{}&{\\le\\frac{\\gamma_{m}}{(K+1)^{4}}\\left(\\operatorname*{max}_{S^{\\star}\\in S}R(S^{\\star},f_{m}(x),r)-R(S,f_{m}(x),r)\\right)+N,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the last inequality uses the fact that \u03bb \u2264N(K\u03b3+1) and $\\lambda_{S}\\geq0$ . This proves Eq. (7). \u25a1 Now, to prove Theorem 3.7, we first prove the following lemma, which shows that the regret with respect to the true value function $f^{\\star}$ and the one respect to the value predictor $f_{m}$ is within a factor of 2 plus an additional term of order $\\frac{N(K{+}1)^{4}}{\\gamma_{m}}$ ", "page_idx": 17}, {"type": "text", "text": "Lemma B.8 Suppose that Event $^{\\,l}$ holds. For all epochs $m\\geq2$ , all rounds $t$ in this epoch, and all policies $\\pi\\in\\Psi$ , with $\\begin{array}{r}{\\gamma_{m}=\\operatorname*{max}\\left\\lbrace1,\\sqrt{\\frac{N(K+1)^{4}}{\\mathbf{Err}_{\\mathrm{log}}(2^{m-2},1/T^{2},\\mathcal{F})}}\\right\\rbrace}\\end{array}$ and $\\lambda=33$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathrm{Reg}(\\pi)\\leq2\\cdot\\mathrm{Reg}_{m}(\\pi)+\\frac{\\lambda N(K+1)^{4}}{\\gamma_{m}},}\\\\ &{\\quad\\mathrm{Reg}_{m}(\\pi)\\leq2\\cdot\\mathrm{Reg}(\\pi)+\\frac{\\lambda N(K+1)^{4}}{\\gamma_{m}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof We prove this by induction. The base case holds trivially. Suppose that this holds for all epochs with index less than $m$ . Consider epoch $m$ . We first show that $\\mathrm{Reg}(\\pi)\\,\\leq\\,2\\mathrm{Reg}_{m}(\\pi)\\,+$ \u03bbN(\u03b3Km+1)4 for all deterministic policy \u03c0 \u2208 \u03a8. This holds trivially if Errlog(N2(mK\u2212+2,11)/4T 2,F) \u2264 1 since $\\mathrm{Reg}(\\pi)\\leq1$ . Consider the case in which $\\begin{array}{r}{\\gamma_{m}=\\sqrt{\\frac{N(K+1)^{4}}{\\mathbf{Err}_{\\mathrm{log}}(2^{m-2},1/T^{2},\\mathcal{F})}}}\\end{array}$ . Specifically, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Reg}(\\pi)-\\mathrm{Reg}_{m}(\\pi)}\\\\ &{\\ =(R(\\pi_{f^{\\star}})-R(\\pi))-(R_{m}(\\pi_{f_{m}})-R_{m}(\\pi))}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overset{(i)}{\\leq}\\left(R(\\pi_{f^{\\star}})-R(\\pi)\\right)-\\left(R_{m}(\\pi_{f^{\\star}})-R_{m}(\\pi)\\right)}\\\\ &{\\leq|R_{m}(\\pi_{f^{\\star}})-R(\\pi_{f^{\\star}})|+|R_{m}(\\pi)-R(\\pi)|}\\\\ &{\\overset{(i i)}{\\leq}\\sqrt{V(q_{m-1},\\pi_{f^{\\star}})\\cdot\\mathbb{E}_{(x,r)\\sim\\mathcal{D},\\;S\\sim q_{m-1}(x,r)}\\left[\\displaystyle\\sum_{i\\in S}\\left(f_{m,i}(x)-f_{i}^{\\star}(x)\\right)^{2}\\right]}}\\\\ &{\\qquad+\\,\\sqrt{V(q_{m-1},\\pi)\\cdot\\mathbb{E}_{(x,r)\\sim\\mathcal{D},\\;S\\sim q_{m-1}(x,r)}\\left[\\displaystyle\\sum_{i\\in S}\\left(f_{m,i}(x)-f_{i}^{\\star}(x)\\right)^{2}\\right]},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $(i)$ is because $R_{m}(\\pi_{f_{m}})\\geq R_{m}(\\pi_{f^{\\star}})$ by definition and $(i i)$ follows Lemma 3.2. Next, using Lemma 3.3 and Lemma B.3, since Event 1 holds, we know that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{4(K+1)^{4}\\mathbf{Err}_{\\log}(2^{m-2},1/T^{2},\\mathcal{F})}\\\\ &{~~\\ge2(K+1)^{4}\\mathbb{E}_{(x,r)\\sim\\mathcal{D},~S\\sim q_{m-1}(x,r)}\\left[\\|\\mu(S,f_{m}(x))-\\mu(S,f^{\\star}(x))\\|_{2}^{2}\\right]}\\\\ &{~~\\ge\\mathbb{E}_{(x,r)\\sim\\mathcal{D},~S\\sim q_{m-1}(x,r)}\\left[\\displaystyle\\sum_{i\\in S}(f_{m,i}(x)-f_{i}^{\\star}(x))^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Plugging the above back to Eq. (18), we obtain that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Reg}(\\pi)-\\mathrm{Reg}_{m}(\\pi)}\\\\ &{\\leq2(K+1)^{2}\\sqrt{V(q_{m-1},\\pi_{f^{\\star}})\\mathbf{Err}_{\\log}(2^{m-2},1/T^{2},\\mathcal{F})}}\\\\ &{\\phantom{=}+2(K+1)^{2}\\sqrt{V(q_{m-1},\\pi)\\mathbf{Err}_{\\log}(2^{m-2},1/T^{2},\\mathcal{F})}}\\\\ &{\\leq\\frac{(K+1)^{4}V(q_{m-1},\\pi_{f^{\\star}})}{8\\gamma_{m}}+\\frac{(K+1)^{4}V(q_{m-1},\\pi)}{8\\gamma_{m}}+16\\gamma_{m}\\mathbf{Err}_{\\log}(2^{m-2},1/T^{2},\\mathcal{F})}\\\\ &{\\phantom{=}-(K+1)^{4}V(q_{m-1},\\pi_{f^{\\star}})}\\\\ &{=\\frac{(K+1)^{4}V(q_{m-1},\\pi_{f^{\\star}})}{8\\gamma_{m}}+\\frac{(K+1)^{4}V(q_{m-1},\\pi)}{8\\gamma_{m}}+\\frac{16N(K+1)^{4}}{\\gamma_{m}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the last equality is because $\\begin{array}{r}{\\gamma_{m}=\\sqrt{\\frac{N(K+1)^{4}}{\\mathbf{Err}_{\\mathrm{log}}(2^{m-2},1/T^{2},\\mathcal{F})}}}\\end{array}$ . According to Lemma 3.6, we know that for all , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V(q_{m-1},\\pi)=\\mathbb{E}_{(x,r)\\sim\\mathcal{D}}\\left[\\displaystyle\\sum_{i\\in\\pi(x,r)}\\frac{1}{w_{i}(q_{m-1}|x,r)}\\right]}\\\\ &{\\qquad\\qquad\\le\\mathbb{E}_{(x,r)\\sim\\mathcal{D}}\\left[N+\\frac{\\gamma_{m-1}}{(K+1)^{4}}\\left(\\operatorname*{max}_{S^{\\star}\\in S}R(S^{\\star},r,f_{m-1}(x))-R(S,r,f_{m-1}(x))\\right)\\right]}\\\\ &{\\qquad\\qquad=N+\\frac{\\gamma_{m-1}}{(K+1)^{4}}\\mathrm{Reg}_{m-1}(\\pi).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Using Eq. (21), we bound the first and the second term in Eq. (20) as follows ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{(K+1)^{4}V(q_{m-1},\\pi)}{8\\gamma_{m}}\\leq\\frac{N(K+1)^{4}}{8\\gamma_{m}}+\\frac{\\gamma_{m-1}\\mathrm{Re}\\mathrm{g}_{m-1}\\left(\\pi\\right)}{8\\gamma_{m}}}\\\\ &{\\phantom{\\frac{(K+1)^{4}V(q_{m-1},\\pi)}{8\\gamma_{m}}}\\leq\\frac{N(K+1)^{4}}{8\\gamma_{m}}+\\frac{\\gamma_{m-1}\\left(2\\mathrm{Re}\\{\\pi\\}\\left(\\pi\\right)+\\frac{\\lambda N(K+1)^{4}}{\\gamma_{m}}\\right)}{8\\gamma_{m}}}\\\\ &{\\phantom{\\frac{(K+1)^{4}V(q_{m-1},\\pi)}{8\\gamma_{m}}}\\leq\\frac{1}{4}\\mathrm{Re}\\{\\pi\\}+\\frac{\\lambda+1}{8\\gamma_{m}}\\cdot N(K+1)^{4},\\qquad\\mathrm{(since~}\\gamma_{m-1}\\leq\\gamma_{m})}\\\\ &{\\frac{(K+1)^{4}V(q_{m-1},\\pi,f_{*})}{8\\gamma_{m}}\\leq\\frac{N(K+1)^{4}}{8\\gamma_{m}}+\\frac{\\gamma_{m-1}\\mathrm{Re}\\mathrm{g}_{m-1}\\left(\\pi_{f^{*}}\\right)}{8\\gamma_{m}}}\\\\ &{\\phantom{\\frac{(K+1)^{4}V(q_{m-1},\\pi)}{8\\gamma_{m}}}\\leq\\frac{N(K+1)^{4}}{8\\gamma_{m}}+\\frac{\\gamma_{m-1}\\left(2\\mathrm{Re}\\{\\pi_{f^{*}}\\}+\\frac{\\lambda N(K+1)^{4}}{\\gamma_{m-1}}\\right)}{8\\gamma_{m}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\leq\\frac{\\lambda+1}{8\\gamma_{m}}\\cdot N(K+1)^{4}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Plugging back to Eq. (20), we know that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{Reg}(\\pi)-\\mathrm{Reg}_{m}(\\pi)\\leq\\frac{1}{4}\\mathrm{Reg}(\\pi)+\\frac{16N(K+1)^{4}}{\\gamma_{m}}+\\frac{\\lambda+1}{4\\gamma_{m}}N(K+1)^{4}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Rearranging the terms, we know that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Reg}(\\pi)\\leq\\displaystyle\\frac{4}{3}\\mathrm{Reg}_{m}(\\pi)+\\frac{12N(K+1)^{4}}{\\gamma_{m}}+\\frac{\\lambda+1}{3\\gamma_{m}}N(K+1)^{4}}\\\\ &{\\qquad\\quad\\leq2\\mathrm{Reg}_{m}(\\pi)+\\frac{\\lambda N(K+1)^{4}}{\\gamma_{m}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the last inequality uses $\\lambda=33$ . ", "page_idx": 19}, {"type": "text", "text": "For the other direction, similar to Eq. (20), we know that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Reg}_{m}(\\pi)-\\mathrm{Reg}(\\pi)}\\\\ &{=(R_{m}(\\pi_{f_{n}})-R_{m}(\\pi))-(R(\\pi_{f^{*}})-R(\\pi))}\\\\ &{\\le(R(\\pi_{f_{n}})-R(\\pi))-(R(\\pi_{f_{n}})-R(\\pi))}\\\\ &{\\le|R_{m}(\\pi_{f_{n}})-R(\\pi_{f_{n}})|+|R_{m}(\\pi)-R(\\pi)|}\\\\ &{\\le2(K+1)^{2}\\sqrt{V(q_{m-1},\\pi_{f_{n}})\\mathrm{Err}_{\\log}(2^{m-2},1/T^{2},\\mathcal{F})}}\\\\ &{\\qquad+2(K+1)^{2}\\sqrt{V(q_{m-1},\\pi)\\mathrm{Err}_{\\log}(2^{m-2},1/T^{2},\\mathcal{F})}}\\\\ &{\\le\\frac{(K+1)^{4}V(q_{m-1},\\pi_{f_{n}})}{8\\gamma_{m}}+\\frac{(K+1)^{4}V(q_{m-1},\\pi)}{8\\gamma_{m}}+16\\gamma_{m}\\mathrm{Err}_{\\log}(2^{m-2},1/T^{2},\\mathcal{F})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad(K+1)^{4}}\\\\ &{\\stackrel{(i)}{\\le}(K+1)^{4}V(q_{m-1},\\pi_{f_{m}})\\underset{+}{=}(K+1)^{4}V(q_{m-1},\\pi)\\underset{+}{=}\\;\\frac{16N(K+1)^{4}}{4}}\\end{array}\n$$$$\n\\overset{(i)}{=}\\frac{(K+1)^{4}V(q_{m-1},\\pi_{f_{m}})}{8\\gamma_{m}}+\\frac{(K+1)^{4}V(q_{m-1},\\pi)}{8\\gamma_{m}}+\\frac{16N(K+1)^{4}}{\\gamma_{m}},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $(i)$ is again because $\\begin{array}{r}{\\gamma_{m}\\;=\\;\\sqrt{\\frac{N(K+1)^{4}}{\\mathbf{Err}_{\\mathrm{log}}(2^{m-2},1/T^{2},\\mathcal{F})}}}\\end{array}$ . Applying Eq. (21) to the first term in Eq. (23), we know that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{(K+1)^{4}V(q_{m-1},\\pi_{f_{m}})}{8\\gamma_{m}}}\\\\ &{\\le\\frac{N(K+1)^{4}}{8\\gamma_{m}}+\\frac{\\gamma_{m-1}\\mathrm{Re}g_{m-1}\\left(\\pi_{f_{m}}\\right)}{8\\gamma_{m}}}\\\\ &{\\le\\frac{N(K+1)^{4}}{8\\gamma_{m}}+\\frac{\\gamma_{m-1}\\left(2\\mathrm{Re}g(\\pi_{f_{m}})+\\frac{\\lambda N(K+1)^{4}}{\\gamma_{m-1}}\\right)}{8\\gamma_{m}}}\\\\ &{\\overset{(i)}{\\le}\\frac{\\lambda+1}{8\\gamma_{m}}\\cdot N(K+1)^{4}+\\frac{1}{4}\\left(2\\mathrm{Re}\\mathrm{g}_{m}(\\pi_{f_{m}})+\\frac{\\lambda N(K+1)^{4}}{\\gamma_{m}}\\right)}\\\\ &{\\overset{(i i)}{=}\\frac{1+3\\lambda}{8\\gamma_{m}}N(K+1)^{4},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $(i)$ is because $\\gamma_{m-1}\\leq\\,\\gamma_{m}$ and Eq. (22), and $(i i)$ is due to $\\mathrm{Reg}_{m}(\\pi_{f_{m}})=0$ . Plugging the above back to Eq. (23), we obtain that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\mathrm{Reg}_{m}(\\pi)\\leq\\mathrm{Reg}(\\pi)+\\frac{2+4\\lambda}{8\\gamma_{m}}N(K+1)^{4}+\\frac{1}{4}\\mathrm{Reg}(\\pi)+\\frac{16N(K+1)^{4}}{\\gamma_{m}}}}\\\\ {{\\displaystyle\\leq2\\mathrm{Reg}(\\pi)+\\frac{\\lambda N(K+1)^{4}}{\\gamma_{m}},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which finishes the proof. ", "page_idx": 19}, {"type": "text", "text": "Algorithm 2 Contextual MNL Algorithms via an Online Regression Oracle ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Input: an online regression oracle $\\mathsf{A l g}_{\\mathsf{o n}}$ satisfying Assumption 3.   \nfor $t=1,2,\\ldots,T$ do Obtain value predictor $f_{t}$ from oracle $\\mathsf{A l g}_{\\mathsf{o n}}$ . Receive context $x_{t}\\in\\mathscr{X}$ and reward vector $r_{t}\\in[0,1]^{N}$ . Calculate $q_{t}\\in\\Delta(S)$ based on $f(x_{t})$ and $r_{t}$ , via either Eq. (9) or Eq. (10). Sample $S_{t}\\sim q_{t}$ and receive purchase decision $i_{t}\\in S_{t}\\cup\\{0\\}$ drawn according Eq. (1). Feed the tuple $(x_{t},S_{t},i_{t})$ to the oracle $\\mathsf{A l g}_{\\mathsf{o n}}$ . ", "page_idx": 20}, {"type": "text", "text": "Theorem 3.7 Under Assumption $^{\\,l}$ and Assumption 2, Algorithm $^{\\,l}$ with $q_{m}$ defined in Eq. (5) and the optimal choice of $\\gamma_{m}$ ensures $\\begin{array}{r}{\\mathbf{Reg}_{\\mathrm{MNL}}=\\mathcal{O}\\left(\\sum_{m=1}^{\\lceil\\log_{2}T\\rceil}2^{m}K^{2}\\sqrt{N\\mathbf{Err}_{\\log}(2^{m-1},1/T^{2},\\mathcal{F})}\\right)}\\end{array}$ ", "page_idx": 20}, {"type": "text", "text": "Proof Choose $\\begin{array}{r}{\\gamma_{m}=\\operatorname*{max}\\left\\lbrace1,\\sqrt{\\frac{N(K+1)^{4}}{\\mathbf{Err}_{\\mathrm{log}}(2^{m-2},1/T^{2},\\mathcal{F})}}\\right\\rbrace}\\end{array}$ for all $m\\geq2$ . Consider the regret within epoch $m\\;\\geq\\;2$ . We first show that $\\begin{array}{r}{\\sum_{\\pi\\in\\Psi}Q_{m}(\\pi)\\mathrm{Reg}_{m}(\\pi)\\;\\le\\;\\frac{N(K+1)^{4}}{\\gamma_{m}}}\\end{array}$ . Concretely, according to Lemma 3.6 and Lemma B.5, we k now that $\\begin{array}{r l}&{\\displaystyle\\sum_{\\pi\\in\\Psi}Q_{m}(\\pi)\\mathrm{Reg}_{m}(\\pi)}\\\\ &{=\\mathbb{E}_{(x,r)\\sim\\mathcal{D}}\\left[\\displaystyle\\sum_{S\\in\\mathcal{S}}q_{m}(S|x,r)\\left(\\operatorname*{max}_{S^{\\star}\\in\\mathcal{S}}R(S^{\\star},f_{m}(x),r)-R(S,f_{m}(x),r)\\right)\\right]\\le\\frac{N(K+1)^{4}}{\\gamma_{m}}.}\\end{array}$ (24) ", "page_idx": 20}, {"type": "text", "text": "Now consider the regret within epoch $m$ . Since Event 1 holds with probability at least $\\textstyle1\\,-\\,{\\frac{1}{T}}$ , we know that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\underset{t=\\tau_{m+1}}{\\overset{\\tau_{m+1}}{\\sum}}\\left(\\underset{s\\in\\mathcal{S}}{\\operatorname*{max}}\\,R(S,x_{t},f^{*}(x_{t}))-R(S_{t},x_{t},f^{*}(x_{t}))\\right)\\right]}\\\\ &{=(\\tau_{m+1}-\\tau_{m})\\mathbb{E}\\left[\\underset{\\pi\\in\\Psi}{\\sum}Q_{m}(\\pi)\\mathrm{Reg}(\\pi)\\right]}\\\\ &{\\leq\\frac{\\tau_{m+1}-\\tau_{m}}{T}+(\\tau_{m+1}-\\tau_{m})\\mathbb{E}\\left[\\underset{\\pi\\in\\Psi}{\\sum}Q_{m}(\\pi)\\mathrm{Reg}(\\pi)\\,\\,\\Bigg|\\,\\,\\mathrm{Event~}1\\,\\,\\mathrm{holds}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\stackrel{(i)}{\\leq}\\frac{\\tau_{m+1}-\\tau_{m}}{T}+(\\tau_{m+1}-\\tau_{m})\\mathbb{E}\\left[\\underset{\\pi\\in\\Psi}{\\sum}Q_{m}(\\pi)\\left(2\\mathrm{Reg}_{m}(\\pi)+\\frac{33N(K+1)^{4}}{\\gamma_{m}}\\right)\\;\\Bigg|\\;\\mathrm{Event~}1\\mathrm{~holds}\\right]}&\\\\ &{\\leq\\frac{\\tau_{m+1}-\\tau_{m}}{T}+(\\tau_{m+1}-\\tau_{m})\\cdot\\frac{35N(K+1)^{4}}{\\gamma_{m}}}&{\\mathrm{(using~Eq.~}(24))}\\\\ &{=\\mathcal{O}\\left(\\frac{\\tau_{m+1}-\\tau_{m}}{T}+2^{m-1}K^{2}\\sqrt{N\\mathrm{Err}_{\\log}(2^{m-2},1/T^{2},\\mathcal{F})}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $(i)$ uses Lemma B.8. Taking summation over $m=2,3,\\ldots,\\lceil\\log_{2}T+1\\rceil$ , we conclude that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbf{Reg}_{\\mathsf{M N L}}=\\mathcal{O}\\left(\\sum_{m=1}^{\\lceil\\log_{2}T\\rceil}2^{m}K^{2}\\sqrt{N\\mathbf{Err}_{\\log}(2^{m-1},1/T^{2},\\mathcal{F})}\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "C Omitted Details in Section 4.1 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we show omitted details in Section 4.1. ", "page_idx": 20}, {"type": "text", "text": "C.1 Online Regression Oracle ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We first show that there exists efficient online regression oracle for the finite class and the linear class. ", "page_idx": 21}, {"type": "text", "text": "Lemma C.1 For the finite class and the linear class discussed in Lemma 3.1, the following concrete oracles satisfy Assumption 3: ", "page_idx": 21}, {"type": "text", "text": "\u2022 (Finite class) Hedge [16] with $\\begin{array}{r}{\\mathbf{Reg}_{\\mathrm{log}}(T,\\mathcal{F})=\\mathcal{O}(\\sqrt{T\\log\\vert\\mathcal{F}\\vert}\\log\\frac{K}{\\beta});}\\end{array}$ ; ", "page_idx": 21}, {"type": "text", "text": "\u2022 (Linear class) Online Gradient Descent [32] with $\\mathbf{Reg}_{\\mathrm{log}}(T,\\mathcal{F})=\\mathcal{O}(B\\sqrt{T}).$ ", "page_idx": 21}, {"type": "text", "text": "Proof We first consider the finite function class. Since for any $S\\in S$ , $i\\in S\\cup\\{0\\}$ , and $x\\in\\mathscr{X}$ , we have fi(x) \u2265\u03b2, we know that \u2113log(\u00b5(S, f(x)), i) \u2264log K\u03b2+ 1. Therefore, Hedge [16] guarantees that $\\begin{array}{r}{\\mathbf{Reg}_{\\mathrm{log}}(T,\\mathcal{F})=\\mathcal{O}\\left(\\log\\frac{K}{\\beta}\\sqrt{T\\log|\\mathcal{F}|}\\right)}\\end{array}$ ", "page_idx": 21}, {"type": "text", "text": "For the linear class, we first prove that given $S\\in S$ , $i\\in S\\cup\\{0\\}$ and $\\boldsymbol{x}\\in\\mathbb{R}^{d\\times N}$ , for any $f_{\\theta}\\in{\\mathcal{F}}$ , $\\ell_{\\mathrm{log}}(\\mu(S,f_{\\theta}(x)),i)$ is convex in $\\theta$ . Specifically, for $u\\in\\mathbb{R}^{d}$ , $\\begin{array}{r}{h(u)=\\log(\\sum_{i=1}^{d}e^{u_{i}})}\\end{array}$ is convex in $u$ since for any $\\alpha\\in\\mathbb{R}^{d}$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\alpha^{\\top}\\nabla_{u}^{2}h(u)\\alpha=\\alpha^{\\top}\\left(\\frac{1}{\\mathbf{1}^{\\top}u}\\mathrm{diag}(u)-\\frac{1}{(\\mathbf{1}^{\\top}u)^{2}}u u^{\\top}\\right)\\alpha}\\\\ &{\\qquad\\qquad=\\frac{(\\sum_{k=1}^{d}u_{k}\\alpha_{k}^{2})(\\sum_{k=1}^{d}u_{k})-(\\sum_{k=1}^{d}u_{k}\\alpha_{k})^{2}}{(\\mathbf{1}^{\\top}u)^{2}}\\geq0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the last inequality is due to Cauchy-Schwarz inequality. Define $\\begin{array}{r l r l}{x_{0}}&{{}=}&{\\mathbf{0}}&{{}\\in}\\end{array}$ $\\mathbb{R}^{d}$ to be the $d_{\\cdot}$ -dimensional all-zero vector. Then, we know that $\\begin{array}{r l}{\\ell_{\\mathrm{log}}(\\mu(S,f_{\\theta}(x),i))}&{{}=}\\end{array}$ $\\begin{array}{r}{\\log\\Big(e^{\\theta^{\\top}x_{0}}+\\sum_{j\\in S}e^{\\theta^{\\top}x_{j}-B}\\Big)\\,-\\,(\\theta^{\\top}x_{i}\\,-\\,B)\\cdot\\mathbb{1}\\{i\\,\\neq\\,0\\}}\\end{array}$ is convex in $\\theta$ . Moreover, direct calculation shows that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|\\nabla_{\\theta}\\ell_{\\log}(\\mu(S,f_{\\theta}(x)),i)\\|_{2}=\\left\\|\\frac{\\sum_{j\\in S}e^{\\theta^{\\top}x_{j}-B}\\cdot x_{j}}{1+\\sum_{j\\in S}e^{\\theta^{\\top}x_{j}-B}}-x_{i}\\cdot\\mathbb{1}\\{i\\neq0\\}\\right\\|_{2}\\le2.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Therefore, Online Gradient Descent [32] g uarantees that Reglog(T, F) = O(B \u221aT), since \u2225\u03b8\u22252 \u2264 ", "page_idx": 21}, {"type": "text", "text": "For completeness, we restate and prove Lemma 4.2, which is extended from the analysis in [14, 15]. ", "page_idx": 21}, {"type": "text", "text": "Lemma C.2 Under Assumption $^{\\,l}$ and Assumption 3, Algorithm 2 (with any $q_{t.}$ ) ensures ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbf{Reg}_{\\mathsf{M N L}}\\leq\\mathbb{E}\\left[\\sum_{t=1}^{T}\\mathsf{d e c}_{\\gamma}(q_{t};f_{t}(x_{t}),r_{t})\\right]+2\\gamma\\mathbf{Reg}_{\\log}(T,\\mathcal{F})\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "for any $\\gamma>0$ , where $\\mathsf{d e c}_{\\gamma}(q;v,r)$ is the Decision-Estimation Coefficient (DEC) defined as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{v^{\\star}\\in[0,1]^{N}}\\operatorname*{max}_{S^{\\star}\\in S}\\left\\{R(S^{\\star},v^{\\star},r)-\\mathbb{E}_{S\\sim q}\\left[R(S,v^{\\star},r)\\right]-\\gamma\\mathbb{E}_{S\\sim q}\\left[\\|\\mu(S,v)-\\mu(S,v^{\\star})\\|_{2}^{2}\\right]\\right\\}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof Following the regret decomposition in [14, 15], we decompose $\\mathbf{Reg}_{\\mathsf{M N L}}$ as follows: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{Reg}_{\\mathrm{MM}}}\\\\ &{=\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}\\operatorname*{max}_{s^{\\star}\\in\\mathcal{S}}R(S,f^{\\star}(x_{t}),r_{t})-\\displaystyle\\sum_{t=1}^{T}q_{t}(S)R(S,f^{\\star}(x_{t}),r_{t})\\right]}\\\\ &{=\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}\\operatorname*{max}_{s^{\\star}\\in\\mathcal{S}}R(S^{\\star},f^{\\star}(x_{t}),r_{t})-\\displaystyle\\sum_{t=1}^{T}q_{t}(S)R(S,f^{\\star}(x_{t}),r_{t})\\right.}\\\\ &{\\qquad\\left.-\\gamma\\displaystyle\\sum_{s\\in\\mathcal{S}}q_{t}(S)\\|\\mu(S,f_{t}(x_{t}))-\\mu(S,f^{\\star}(x_{t}))\\|_{2}^{2}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad+\\gamma\\mathbb{E}\\left[\\displaystyle\\sum_{S\\in\\mathcal{S}}q_{t}(S)\\big\\|\\mu(S,f_{t}(x_{t}))-\\mu(S,f^{*}(x_{t}))\\big\\|_{2}^{2}\\right]}\\\\ &{\\leq\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}s_{\\in S,\\tau^{*}\\in[0,1]^{\\vee}}\\left\\lbrace R(S^{*},v^{*},r_{t})-\\displaystyle\\sum_{t=1}^{T}q_{t}(S)R(S,v^{*},r_{t})-\\right.}\\\\ &{\\qquad\\gamma\\displaystyle\\sum_{S\\in\\mathcal{S}}q_{t}(S)\\big\\|\\mu(S,f_{t}(x_{t}))-\\mu(S,v^{*})\\big\\|_{2}^{2}\\right\\rbrace\\right]}\\\\ &{\\qquad+\\gamma\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}\\big\\|\\mu(S_{t},f_{t}(x_{t}))-\\mu(S,f^{*}(x_{t}))\\big\\|_{2}^{2}\\right]}\\\\ &{=\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}\\operatorname{dec}_{\\gamma}(q_{t};f_{t}(x_{t}),r_{t})\\right]+\\gamma\\cdot\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}\\big\\|\\mu(S_{t},f_{t}(x_{t}))-\\mu(S_{t},f^{*}(x_{t}))\\big\\|_{2}^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the last equality is by the definition of $\\mathsf{d e c}_{\\gamma}(q_{t};f_{t}(x_{t}),r_{t})$ . According to Lemma 3.3, we know that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}\\left[\\sum_{t=1}^{T}\\|\\mu(S_{t},f_{t}(x_{t}))-\\mu(S_{t},f^{\\star}(x_{t}))\\|_{2}^{2}\\right]}\\\\ {\\displaystyle\\le2\\mathbb{E}\\left[\\sum_{t=1}^{T}\\ell_{\\log}(\\mu(S_{t},f_{t}(x_{t})),i_{t})-\\sum_{t=1}^{T}\\ell_{\\log}(\\mu(S_{t},f^{\\star}(x_{t})),i_{t})\\right]\\le2\\mathbf{Reg}_{\\log}(T,\\mathcal{F}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Combining Eq. (25) and Eq. (26) finishes the proof. ", "page_idx": 22}, {"type": "text", "text": "C.2 Proof of Theorem 4.3 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Next, we prove Theorem 4.3, which shows that similar to the stochastic environment, a simple but efficient $\\varepsilon$ -greedy strategy achieves $\\mathcal{O}\\left(T^{2/3}(N K\\mathbf{Reg}_{\\mathrm{log}}(T,\\mathcal{F}))^{1/3}\\right)$ expected regret. ", "page_idx": 22}, {"type": "text", "text": "Theorem 4.3 The strategy defined in Eq. (9) guarantees $\\begin{array}{r}{\\mathsf{d e c}_{\\gamma}(q_{t};f_{t}(x_{t}),r_{t})=\\mathcal{O}(\\frac{N K}{\\gamma\\varepsilon}+\\varepsilon)}\\end{array}$ . Consequently, under Assumption $^{\\,l}$ and Assumption $^3$ , Algorithm 2 with $q_{t}$ calculated via Eq. (9) and the optimal choice of $\\varepsilon$ and $\\gamma$ ensures $\\mathbf{Reg}_{\\mathrm{MNL}}=\\mathcal{O}\\big((N K\\mathbf{Reg}_{\\mathrm{log}}(T,\\mathcal{F})\\big)^{\\frac{1}{3}}T^{\\frac{2}{3}}\\big)$ . ", "page_idx": 22}, {"type": "text", "text": "Proof We first prove that $q_{t}$ defined in Eq. (9) guarantees $\\begin{array}{r}{\\mathsf{d e c}_{\\gamma}(q_{t};f_{t}(x_{t}),r_{t})\\,\\le\\,\\mathcal{O}\\left(\\frac{N K}{\\gamma\\varepsilon}+\\varepsilon\\right)}\\end{array}$ Specifically, for any $S^{\\star}\\in S$ and $v^{\\star}\\in[0,1]^{N}$ , we know that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R(S^{*},\\nu^{*},\\tau_{t})-\\sum_{\\ell\\in\\mathcal{S}}\\eta_{\\ell}(S)R(S,\\nu^{*},\\tau_{t})-\\gamma\\sum_{\\ell\\in\\mathcal{S}}\\eta_{\\ell}(S)\\big\\|\\mu(S,f_{\\ell}(x_{t}))-\\mu(S,f^{*}(x_{t}))\\big\\|_{2}^{2}}\\\\ &{\\overset{(i i)}{\\leq}\\displaystyle\\sum_{i\\in\\mathcal{S}^{*}}\\|\\nu_{i}^{*}-f_{i,\\ell}(x_{i})\\|+\\sum_{\\ell\\in\\mathcal{S}}\\eta_{\\ell}(S)\\displaystyle\\sum_{i\\in\\mathcal{S}}\\mu_{i}(S,\\nu^{*})-\\mu_{i}(S,f_{\\ell}(x_{t}))\\big\\|}\\\\ &{\\qquad+R(S^{*},f_{i}(x_{t}),\\tau_{t})-\\sum_{\\ell\\in\\mathcal{S}}\\eta_{\\ell}(S)R(S,f_{\\ell}(x_{t}),\\tau_{t})-\\gamma\\sum_{S\\in\\mathcal{S}}\\eta_{\\ell}(S)\\big\\|\\mu(S,f_{\\ell}(x_{t}))-\\mu(S,\\nu^{*})\\big\\|_{2}^{2}}\\\\ &{\\overset{(i i i)}{\\leq}\\displaystyle\\sum_{i\\in\\mathcal{S}^{*}}|\\nu_{i}^{*}-f_{i,\\ell}(x_{i})|+\\frac{2K}{\\gamma}}\\\\ &{\\qquad+R(S^{*},f_{i}(x_{t}),\\tau_{t})-\\displaystyle\\sum_{\\ell\\in\\mathcal{S}}\\eta(S)R(S,f_{\\ell}(x_{t}),\\tau_{t})-\\frac{7}{2}\\sum_{S\\in\\mathcal{S}}\\eta_{\\ell}(S)\\big\\|\\mu(S,f_{\\ell}(x_{t}))-\\mu(S,\\nu^{*})\\big\\|_{2}^{2}}\\\\ &{\\overset{(i i i i)}{\\leq}\\displaystyle\\sum_{i\\in\\mathcal{S}}|\\nu_{i}^{*}-f_{i,\\ell}(x_{i})|+\\frac{2K}{\\gamma}+\\epsilon+R(S^{*},f_{i}(x_{t}),\\tau_{t})-\\mathtt{m a x}\\,R(S,f_{\\ell}(x_{t}),\\tau_{t})}\\\\ &{\\qquad+\\frac{7K}{2}\\displaystyle\\sum_{i=1}^{N}\\|\\mu(\\langle i,f_{i}(x_{t})\\rangle-\\mu(\\{i,i,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\stackrel{(i v)}{\\le}\\sum_{i\\in S^{\\star}}|v_{i}^{\\star}-f_{t,i}(x_{t})|+\\frac{2K}{\\gamma}+\\varepsilon+R(S^{\\star},f_{t}(x_{t}),r_{t})-\\operatorname*{max}_{S\\in S}R(S,f_{t}(x_{t}),r_{t})}}\\\\ &{}&{\\displaystyle-\\,\\frac{\\gamma\\varepsilon}{64N}\\sum_{i=1}^{N}(f_{t,i}(x_{t})-v_{i}^{\\star})^{2}}\\\\ &{\\stackrel{(v)}{\\le}\\displaystyle\\frac{16N K}{\\gamma\\varepsilon}+\\frac{2K}{\\gamma}+\\varepsilon}\\\\ &{\\le\\mathcal{O}\\left(\\frac{N K}{\\gamma\\varepsilon}+\\varepsilon\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $(i)$ uses Lemma B.1, $(i i)$ is due to AM-GM inequality and $|S|\\leq K$ , $(i i i)$ is according to the construction of $q_{t}$ and $R(S,v,r)\\in[0,1]$ , $(i v)$ uses Lemma B.3 with $d=1$ , and $\\overline{{(v)}}$ is uses AM-GM inequality and the fact that $|S^{\\star}|\\leq K$ . Taking maximum over all $S^{\\star}\\in{\\mathcal{S}}$ and $\\dot{v^{\\star}}\\in[0,1]^{N}$ proves that $\\begin{array}{r}{\\mathsf{d e c}_{\\gamma}(q_{t};f_{t}(x_{t}),r_{t})\\le\\mathcal{O}\\left(\\frac{N K}{\\gamma\\varepsilon}+\\varepsilon\\right)}\\end{array}$ . ", "page_idx": 23}, {"type": "text", "text": "Combining the above result with Lemma 4.2, we know that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbf{Reg}_{\\mathtt{M N L}}=\\mathcal{O}\\left(\\frac{N K T}{\\gamma\\varepsilon}+\\varepsilon T+\\gamma\\mathbf{Reg}_{\\mathrm{log}}(T,\\mathcal{F})\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Picking $\\gamma$ and $\\varepsilon$ optimally finishes the proof. ", "page_idx": 23}, {"type": "text", "text": "C.3 Proof of Theorem 4.5 and Theorem 4.6 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this section, we restate and prove Theorem 4.5, which proves that $q_{t}$ calculated via Eq. (10) guarantees that $\\begin{array}{r}{\\mathsf{d e c}_{\\gamma}(q_{t};f_{t}(x_{t}),r_{t})\\le\\mathcal{O}\\left(\\frac{N K^{4}}{\\gamma}\\right)}\\end{array}$ ", "page_idx": 23}, {"type": "text", "text": "Theorem 4.5 The following distribution satisfies $\\begin{array}{r}{\\mathsf{d e c}_{\\gamma}(q_{t},f_{t}(x_{t}),r_{t})\\le\\mathcal{O}\\left(\\frac{N K^{4}}{\\gamma}\\right):}\\end{array}$ ", "page_idx": 23}, {"type": "equation", "text": "$$\nq_{t}=\\operatorname*{argmax}_{q\\in\\Delta(S)}\\mathbb{E}_{S\\sim q}\\left[R\\big(S,f_{t}(x_{t}),r_{t}\\big)\\right]-\\frac{(K+1)^{4}}{\\gamma}\\sum_{i=1}^{N}\\log\\frac{1}{w_{i}(q)}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof Since the construction of $q_{t}$ is the same as Eq. (5) with $f_{m}$ replaced by $f_{t}$ and $\\gamma_{m}$ replaced by $\\gamma$ , according to Lemma 3.6, we know that $q_{t}$ satisfies that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{max}_{S^{\\star}\\in S}R(S^{\\star},f_{t}(x_{t}),r_{t})-\\displaystyle\\sum_{S\\in S}q_{t}(S)\\cdot R(S,f_{t}(x_{t}),r_{t})\\le\\frac{N(K+1)^{4}}{\\gamma},}\\\\ &{\\forall S\\in S,\\ \\displaystyle\\sum_{i\\in S}\\frac{1}{w_{i}(q)}\\le N+\\frac{\\gamma}{(K+1)^{4}}\\left(\\displaystyle\\operatorname*{max}_{S^{\\star}\\in S}R(S^{\\star},f_{t}(x_{t}),r_{t})-R(S,f_{t}(x_{t}),r_{t})\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Using Eq. (27) and Eq. (28), we know that for any $S^{\\star}\\in{\\mathcal{S}}$ and $v^{\\star}\\in[0,1]^{N}$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R(S^{\\star},v^{\\star},r_{t})-\\displaystyle\\sum_{S\\in\\mathcal{S}}q_{t}(S)R(S,v^{\\star},r_{t})-\\displaystyle\\gamma\\displaystyle\\sum_{S\\in\\mathcal{S}}q_{t}(S)\\|\\mu(S,f_{t}(x_{t}))-\\mu(S,f^{\\star}(x_{t}))\\|_{2}^{2}}\\\\ &{\\le\\displaystyle\\sum_{i\\in S^{\\star}}|v_{i}^{\\star}-f_{t,i}(x_{t})|+\\displaystyle\\sum_{S\\in\\mathcal{S}}q_{t}(S)\\displaystyle\\sum_{i\\in S}|v_{i}^{\\star}-f_{t,i}(x_{t})|}{\\mathrm{~(according~to~Lemma~B.l~}}}\\\\ &{\\qquad+R(S^{\\star},f_{t}(x_{t}),r_{t})-\\displaystyle\\sum_{S\\in\\mathcal{S}}q_{t}(S)R(S,f_{t}(x_{t}),r_{t})-\\displaystyle\\gamma\\displaystyle\\sum_{S\\in\\mathcal{S}}q_{t}(S)\\|\\mu(S,f_{t}(x_{t}))-\\mu(S,v^{\\star})\\|_{2}^{2}}\\\\ &{\\le\\displaystyle\\sum_{i\\in S^{\\star}}|v_{i}^{\\star}-f_{t,i}(x_{t})|+\\displaystyle\\sum_{i=1}^{N}w_{i}(q_{t})\\cdot|v_{i}^{\\star}-f_{t,i}(x_{t})|}\\\\ &{\\qquad+R(S^{\\star},f_{t}(x_{t}),r_{t})-\\displaystyle\\sum_{S\\in\\mathcal{S}}q_{t}(S)R(S,f_{t}(x_{t}),r_{t})-\\displaystyle\\gamma\\displaystyle\\sum_{S\\in\\mathcal{S}}q_{t}(S)\\|\\mu(S,f_{t}(x_{t}))-\\mu(S,v^{\\star})\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "$\\begin{array}{c l c r}{{\\displaystyle\\leq\\sum_{i\\in S^{\\star}}|v_{i}^{\\star}-f_{t,i}(x_{t})|+\\sum_{i=1}^{N}w_{i}(q_{t})\\cdot|v_{i}^{\\star}-f_{t,i}(x_{t})|}}\\\\ {{\\displaystyle}}&{{\\displaystyle\\quad+\\,R(S^{\\star},f_{t}(x_{t}),r_{t})-\\sum_{S\\in S}q_{t}(S)R(S,f_{t}(x_{t}),r_{t})-\\frac{\\gamma}{2(K+1)^{4}}\\sum_{S\\in S}q_{t}(S)\\sum_{i\\in S}(v_{i}^{\\star}-f_{t,i}(x_{t}))^{2}}}\\end{array}$ (according to Lemma 3.3)   \n$\\begin{array}{r l}&{=\\displaystyle\\sum_{i\\in S^{\\star}}|v_{i}^{\\star}-f_{t,i}(x_{t})|+\\displaystyle\\sum_{i=1}^{N}w_{i}(q_{t})\\cdot|v_{i}^{\\star}-f_{t,i}(x_{t})|}\\\\ &{\\phantom{=\\displaystyle\\sum_{i\\in S^{\\star}}}+R(S^{\\star},f_{t}(x_{t}),r_{t})-\\displaystyle\\sum_{S\\in\\mathcal{S}}q_{t}(S)R(S,f_{t}(x_{t}),r_{t})-\\displaystyle\\frac{\\gamma}{2(K+1)^{4}}\\sum_{i=1}^{N}w_{i}(q_{t})(v_{i}^{\\star}-f_{t,i}(x_{t}))^{2}}\\\\ &{\\leq\\displaystyle\\frac{N(K+1)^{4}}{\\gamma}+\\displaystyle\\sum_{i\\in S^{\\star}}\\frac{(K+1)^{4}}{\\gamma w_{i}(q_{t})}+R(S^{\\star},f_{t}(x_{t}),r_{t})-\\displaystyle\\sum_{S\\in\\mathcal{S}}q_{t}(S)R(S,f_{t}(x_{t}),r_{t})}\\end{array}$ (AM-GM inequality)   \n$\\begin{array}{r l}&{=\\displaystyle\\frac{N(K+1)^{4}}{\\gamma}+\\sum_{i\\in S^{*}}\\frac{(K+1)^{4}}{\\gamma w_{i}(q_{t})}-\\left(\\operatorname*{max}_{S_{0}\\in S}R(S_{0},f_{t}(x_{t}),r_{t})-R(S^{*},f_{t}(x_{t}),r_{t})\\right)}\\\\ &{\\qquad+\\displaystyle\\operatorname*{max}_{S_{0}\\in S}R(S_{0},f_{t}(x_{t}),r_{t})-\\sum_{S\\in S}q_{t}(S)R(S,f_{t}(x_{t}),r_{t})}\\\\ &{\\leq\\displaystyle\\frac{N(K+1)^{4}}{\\gamma}+\\frac{(K+1)^{4}}{\\gamma}\\left(N+\\frac{\\gamma}{(K+1)^{4}}\\left(\\operatorname*{max}_{S_{0}\\in S}R(S_{0},f_{t}(x_{t}),r_{t})-R(S^{*},f_{t}(x_{t}),r_{t})\\right)\\right)}\\\\ &{\\qquad-\\left(\\operatorname*{max}_{S_{0}\\in S}R(S_{0},f_{t}(x_{t}),r_{t})-R(S^{*},f_{t}(x_{t}),r_{t})\\right)+\\frac{N(K+1)^{4}}{\\gamma}}\\end{array}$ (according to Eq. (27) and Eq. (28))   \n$=\\frac{3N(K+1)^{4}}{\\gamma}.$ ", "page_idx": 24}, {"type": "text", "text": "Taking maximum over all $S^{\\star}\\in S$ and $v^{\\star}\\in[0,1]^{N}$ finishes the proof. ", "page_idx": 24}, {"type": "text", "text": "Combining Lemma 4.2 and Theorem 4.5, we are able to prove Theorem 4.6. ", "page_idx": 24}, {"type": "text", "text": "Theorem 4.6 Under Assumption $^{\\,l}$ and Assumption $^3$ , Algorithm 2 with $q_{t}$ calculated via Eq. (10) and the optimal choice of $\\gamma$ ensures $\\mathbf{Reg}_{\\mathrm{MNL}}=\\mathcal{O}\\Big(K^{2}\\sqrt{N T\\mathbf{Reg}_{\\mathrm{log}}(T,\\mathcal{F})}\\Big).$ ", "page_idx": 24}, {"type": "text", "text": "Proof Combining Lemma 4.2 and Theorem 4.5, we know that Algorithm 2 with $q_{t}$ calculated via Eq. (10) satisfies that $\\begin{array}{r}{\\mathbf{Reg}_{\\mathrm{MNL}}\\,=\\,\\mathcal{O}\\left(\\frac{N K^{4}}{\\gamma}+\\gamma\\mathbf{Reg}_{\\mathrm{log}}(T,\\mathcal{F})\\right)}\\end{array}$ . Picking $\\begin{array}{r}{\\gamma\\,=\\,K^{2}\\sqrt{\\frac{N T}{\\mathbf{Reg}_{\\mathrm{log}}(T,\\mathcal{F})}}}\\end{array}$ finishes the proof. \u25a1 ", "page_idx": 24}, {"type": "text", "text": "D Regression Oracle for More Function Classes ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this section, we provide examples on regression oracles for a broader Lipschitz function class satisfying Assumption 2 and Assumption 3. ", "page_idx": 24}, {"type": "text", "text": "Lemma D.1 Suppose that $\\mathcal{F}$ is a 1-Lipschitz function class defined as ${\\mathcal F}=\\{f_{\\theta,i}(x)\\in[\\beta,1]\\mid\\theta\\in{\\mathfrak{$ $[0,1]^{d}\\}$ where $\\beta\\,>\\,0$ and $\\|f_{\\theta_{1},i}\\,-\\,f_{\\theta_{2},i}\\|_{\\infty}\\,\\leq\\,\\|\\theta_{1}\\,-\\,\\theta_{2}\\|_{\\infty}$ for all $\\theta_{1},\\theta_{2}\\;\\in\\;[0,1]^{d}$ and $i~\\in~[N]$ . Then, ERM strategy $\\begin{array}{r}{\\hat{f}_{D}\\,=\\,\\underset{f\\in\\mathcal{F}}{\\operatorname{argmin}}_{f\\in\\mathcal{F}}\\sum_{(x,S,i)\\in D}\\ell_{\\log}(\\mu(S,f(x)),i)}\\end{array}$ satisfies Assumption 2 with $\\begin{array}{r}{\\mathbf{Err}_{\\mathrm{log}}(n,\\delta,\\mathcal{F})=\\mathcal{O}\\left(\\frac{d\\log\\frac{K}{\\beta}\\log\\frac{n}{\\beta}\\log\\frac{1}{\\delta}}{n}\\right)}\\end{array}$ . Moreover, there exists an algorithm satisfying Assumption 3 with $\\mathbf{Reg}_{\\mathrm{log}}(T,\\mathcal{F})=\\mathcal{O}\\Big(\\sqrt{d T\\log(T/\\beta)}\\log(K/\\beta)\\Big).$ ", "page_idx": 24}, {"type": "text", "text": "Proof We first consider the ERM strategy. For notational convenience, let $Z\\triangleq(x,S,i)$ and with an abuse of notation, we denote $\\ell_{\\log}(\\mu(S,f(x)),i)$ by $\\ell_{\\log}^{f}(Z)$ . According to Theorem 7.7 in [26], we know that for any $\\mathcal{F}=\\{f:\\mathcal{X}\\mapsto[\\beta,1]^{N}\\}$ such that the $\\varepsilon$ -covering number of $\\{\\ell_{\\mathrm{log}}^{f}:f\\in\\mathcal{F}\\}$ is $\\mathcal{N}(\\varepsilon)$ , ERM predictor $\\widehat{f}_{D}$ guarantees that with probability $1-\\delta$ : ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}_{Z\\sim\\mathcal{D}}\\left[\\ell_{\\mathrm{log}}^{\\widehat{f}_{D}}(Z)\\right]\\leq\\mathbb{E}_{Z\\sim\\mathcal{D}}\\left[\\ell_{\\mathrm{log}}^{f^{\\star}}(Z)\\right]+\\mathcal{O}\\left(\\frac{\\log\\frac{K}{\\beta}\\log(\\mathcal{N}(\\frac{1}{n^{2}}))\\log\\frac{1}{\\delta}}{n}\\right).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Now we show that for the 1-Lipschitz function class, $\\begin{array}{r}{\\mathcal{N}(\\varepsilon)\\leq\\left(1+\\frac{2}{\\beta\\varepsilon}\\right)^{d}}\\end{array}$ . Specifically, define the $\\frac{\\beta\\varepsilon}{2}$ -grid of $[0,1]^{d}$ as $\\begin{array}{r}{\\mathcal{C}(\\varepsilon)=\\{\\theta\\in[0,1]^{d}:\\theta_{i}\\in\\{0,\\frac{\\beta\\varepsilon}{2},\\beta\\varepsilon,\\dots,1\\},i\\in[N]\\}}\\end{array}$ . For any $\\theta_{1}\\,\\in\\,[0,1]^{d}$ , let $\\theta_{2}\\,=\\,\\mathrm{argmin}_{\\theta\\in{\\mathcal{C}}(\\varepsilon)}\\,\\|\\theta-\\theta_{1}\\|_{\\infty}$ . By definition, we know that $\\begin{array}{r}{\\|\\theta_{1}-\\theta_{2}\\|_{\\infty}\\leq\\frac{\\beta\\varepsilon}{2}}\\end{array}$ . Given any $\\boldsymbol{Z}=(\\boldsymbol{x},\\boldsymbol{S},i)$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|F_{\\mu_{0}}^{\\mu}(Z)-F_{\\mu_{0}}^{\\nu}(Z)|}\\\\ &{=|\\{b_{\\mu}(\\mu_{0}(B,f_{\\mu}(x),i)-\\ell_{\\nu\\mu}(\\mu)(S,f_{\\mu}(x),i)\\}|}\\\\ &{\\leq\\left|\\exp\\frac{1-\\sum_{j\\in\\mathcal{S}}f_{\\mu_{0},\\lambda}(Z)}{1+\\sum_{j\\in\\mathcal{S}}f_{\\mu_{0},\\lambda}(j)}\\right|+\\left|\\exp\\frac{\\int_{\\mathbb{R}_{0}(x)}(x)}{f_{\\mu_{0},\\lambda}(x)}\\cdot\\mathbb{I}(i\\neq0)\\right|}\\\\ &{=\\ln\\frac{1+\\operatorname*{max}\\sum_{j\\in\\mathcal{S}}f_{\\mu_{0},\\lambda}(Z)\\cdot\\sum_{j\\in\\mathcal{S}}f_{\\mu_{0},\\lambda}(Z)}{1+\\operatorname*{max}\\sum_{j\\in\\mathcal{S}}f_{\\mu_{0},\\lambda}(j)\\cdot\\sum_{j\\in\\mathcal{S}}f_{\\mu_{0},\\lambda}(Z)}+\\log\\frac{\\operatorname*{max}\\left\\{f_{\\mu_{0},\\lambda}(Z),f_{\\mu_{0},\\lambda}(z)\\right\\}}{\\operatorname*{min}\\{f_{\\mu_{0},\\lambda}(z),f_{\\mu_{0},\\lambda}(z)\\}}\\cdot\\mathbb{I}(i\\neq0)}\\\\ &{=\\log\\left(1+\\frac{\\ln(1-\\sum_{j\\in\\mathcal{S}}f_{\\mu_{0},\\lambda}(Z)-\\sum_{j\\in\\mathcal{S}}f_{\\mu_{0},\\lambda}(Z))}{\\sum_{j\\in\\mathcal{S}}f_{\\mu_{0},\\lambda}(j)\\cdot\\sum_{j\\in\\mathcal{S}}f_{\\mu_{0},\\lambda}(Z)}\\right)}\\\\ &{\\ \\ \\ \\ +\\log\\left(1+\\frac{\\int_{\\mathbb{R}_{0}(x)}(f_{\\mu_{0},i}(x)-f_{\\mu_{0},\\lambda}(z))}{\\operatorname*{min}\\{f_{\\mu_{0},\\lambda}(z),f_{\\mu_{0},\\lambda}(z)\\}}\\right)\\cdot\\mathbb{I}(i\\neq0)}\\\\ &{\\leq\\frac{\\sum_{j\\in\\mathcal{S}}{1\\}}{\\sum_{j\\in\\mathcal{S}}f_{\\mu_{0},\\lambda}(j)\\cdot\\sum_{j\\in\\mathcal{S}}f_{\\mu_{0},\\lambda}(z)}+\\log\\frac \n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the second inequality is by $\\log(1+x)\\leq x$ for $x\\geq0$ and the triangular inequality, and the third inequality is due to the Lipschitz property and the lower bound $\\beta$ on all values. Therefore, we know that $\\begin{array}{r}{\\mathcal{N}(\\varepsilon)\\leq|\\mathcal{C}(\\varepsilon)|\\leq\\left(1+\\frac{2}{\\beta\\varepsilon}\\right)^{d}}\\end{array}$ . Plugging in this to Eq. (29) proves the claim. ", "page_idx": 25}, {"type": "text", "text": "Next, we consider the adversarial environment. Consider applying Hedge [16] on the discretized set C(1/T). Since \u2113log(\u00b5(S, f(x)), i) \u2264log K\u03b2+1 for all context $x$ , $S\\,\\in\\,S$ , and $i\\in S\\cup\\{0\\}$ , Hedge guarantees that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}\\ell_{\\log}(\\mu(S_{t},f_{\\theta_{t}}(x_{t})),i_{t})-\\displaystyle\\sum_{t=1}^{T}\\ell_{\\log}(\\mu(S_{t},f_{\\hat{\\theta}}(x_{t})),i_{t})\\right]}\\\\ &{\\le\\mathcal{O}\\left(\\log\\displaystyle\\frac{K}{\\beta}\\sqrt{T\\log|\\mathcal{C}(1/T)|}\\right)}\\\\ &{=\\mathcal{O}\\left(\\log\\displaystyle\\frac{K}{\\beta}\\sqrt{d T\\log(T/\\beta)}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "for all ${\\widehat{\\theta\\,}}\\in{\\mathcal{C}}(1/T)$ . Picking $\\widehat{\\theta}=\\mathrm{argmin}_{\\theta\\in{\\mathcal{C}}(1/T)}\\|\\theta-\\theta^{\\star}\\|_{\\infty}$ where $f^{\\star}\\triangleq f_{\\theta^{\\star}}$ and applying Eq. (30) to $\\widehat{\\theta}$ an  d $\\theta^{\\star}$ finishes the proo f . \u25a1 ", "page_idx": 25}, {"type": "text", "text": "E Omitted Details in Section 4.2 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In this section, we show omitted details in Section 4.2. We start by describing the algorithm: it maintains a distribution $p_{t}$ over the value function class $\\mathcal{F}$ , and at each round $t$ , it samples $f_{t}$ from ", "page_idx": 25}, {"type": "text", "text": "Algorithm 3 Feel-Good Thompson Sampling for Contextual MNL bandits ", "page_idx": 26}, {"type": "text", "text": "Input: a learning rate $\\eta>0$ .   \nInitialize $p_{1}\\in\\Delta(\\mathcal{F})$ to be the uniform distribution over $\\mathcal{F}$ .   \nfor $t=1,2,\\ldots,T$ do Sample a value function $f_{t}$ from $p_{t}$ . Receive context $x_{t}$ and reward vector $r_{t}\\in[0,1]^{N}$ . Select $S_{t}=\\operatorname{argmax}_{S\\in S}R(S,f_{t}(x),r_{t})$ and receive feedback $i_{t}\\in S_{t}\\cup\\{0\\}$ . Define the loss estimator $\\widehat{\\ell}_{t,f}$ for each $f\\in\\mathcal F$ as $\\widehat{\\ell}_{t,f}=\\frac{1}{16\\eta}\\sum_{i\\in S_{t}}\\left(\\mu_{i}(S_{t},f(x_{t}))-\\mathbb{1}[i=i_{t}]\\right)^{2}-\\operatorname*{max}_{S\\in\\mathcal{S}}R(S,f(x_{t}),r_{t}).$ (31) Update $p_{t+1,f}\\propto p_{t,f}\\cdot\\exp(-\\eta\\widehat{\\ell}_{t,f})$ . ", "page_idx": 26}, {"type": "text", "text": "$p_{t}$ and selects the subset $S_{t}$ that maximizes the expected reward with respect to the value function $f_{t}$ and the reward vector $r_{t}$ . After receiving the purchase decision $i_{t}$ , the algorithm constructs a loss estimator $\\widehat{\\ell}_{t,f}$ for each $f\\in\\mathcal F$ as defined in Eq. (31), and updates the distribution $p_{t}$ using a standard multiplica t ive update with learning rate $\\eta$ . See Algorithm 3. ", "page_idx": 26}, {"type": "text", "text": "The idea of the loss estimator Eq. (31) is as follows. The first term measures how accurate $f$ is via the squared distance between the multinomial distribution induced by $f$ and the true outcome. The second term, which is the highest expected reward one could get if the value function was $f$ , is subtracted from the first term to serve as a form of optimism (the \u201cfeel-good\u201d part), encouraging exploration for those $f$ \u2019s that promise a high reward. ", "page_idx": 26}, {"type": "text", "text": "We extend the analysis of Zhang [30] and combine it with our technical lemmas (such as Lemma 3.3 and Lemma B.1) to prove the following regret guarantee, where the term $Z_{T}$ should be interpreted as a certain complexity measure for the class $\\mathcal{F}$ . ", "page_idx": 26}, {"type": "text", "text": "Theorem E.1 Under Assumption $^{\\,l}$ , Algorithm 3 with learning rate $\\eta\\:\\leq\\:1$ ensures $\\Rsh\\mathtt{g}_{\\mathsf{M N L}}\\leq$ $\\begin{array}{r}{32\\eta N(K+1)^{4}T+4\\eta T+\\frac{Z_{T}}{\\eta}}\\end{array}$ \u03b7 , where $\\begin{array}{r}{Z_{T}=-\\mathbb{E}[\\log\\mathbb{E}_{f\\sim p_{1}}[\\exp(-\\eta\\sum_{t=1}^{T}(\\widehat{\\ell}_{t,f}-\\widehat{\\ell}_{t,f^{\\star}}))]]}\\end{array}$ . ", "page_idx": 26}, {"type": "text", "text": "Proof First, we decompose the regret as follows: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Regan}=\\mathbb{E}\\left[\\underset{t=1}{\\overset{\\sum}{\\sum}}\\left(\\underset{s\\leq\\delta}{\\operatorname*{max}}R(s,f^{*}(x_{t}),r_{t})-R(S_{t},f^{*}(x_{t}),r_{t})\\right)\\right]}\\\\ &{=\\mathbb{E}\\left[\\underset{t=1}{\\overset{\\sum}{\\sum}}\\left(H(S_{t},f_{t}(x_{t}),r_{t})-R(S_{t},f^{*}(x_{t}),r_{t})\\right)\\right]}\\\\ &{\\qquad-\\mathbb{E}\\left[\\underset{t=1}{\\overset{\\sum}{\\sum}}\\left(R(S_{t},f_{t}(x_{t}),r_{t})-\\underset{0\\leq i\\leq j}{\\operatorname*{max}}R(S_{t},f^{*}(x_{t}),r_{t})\\right)\\right]}\\\\ &{\\overset{(i j)}{=}\\mathbb{E}\\left[\\underset{t=1}{\\overset{\\sum}{\\sum}}\\left(R(S_{t},f_{t}(x_{t}),r_{t})-R(S_{t},f^{*}(x_{t}),r_{t})\\right)\\right]}\\\\ &{\\qquad-\\mathbb{E}\\left[\\underset{t=1}{\\overset{\\sum}{\\sum}}\\left(\\underset{s\\leq\\delta}{\\operatorname*{max}}R(S_{t},f_{t}(x_{t}),r_{t})-\\underset{s\\leq\\delta}{\\operatorname*{max}}R(S_{t},f^{*}(x_{t}),r_{t})\\right)\\right]}\\\\ &{\\overset{(i i j)}{=}\\mathbb{E}\\left[\\underset{t=1}{\\overset{\\sum}{\\sum}}\\left\\lbrace f_{t=1}(x_{t})-f_{t}(x_{t})\\right\\rbrace-\\underset{t=1}{\\overset{\\sum}{\\operatorname*{max}}}\\mathbb{E}(S_{t},f^{*}(x_{t}),r_{t})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $(i)$ is because $S_{t}=\\operatorname{argmax}_{S\\in S}R(S,f_{t}(x_{t}),r_{t})$ according to Algorithm 3 and $(i i)$ is using Lemma B.1. Here, \u201cFeel-Good\u201d term $\\mathrm{FG}_{t}$ measures the difference between the expected reward of the best subset given the value predictor $f_{t}$ and that of the true value predictor $f^{\\star}$ . ", "page_idx": 26}, {"type": "text", "text": "Next, we analyze the first term $\\begin{array}{r}{\\sum_{t=1}^{T}\\sum_{i\\in S_{t}}|f_{t,i}(x_{t})-f_{i}^{\\star}(x_{t})|}\\end{array}$ . Given any context $x\\in\\mathscr{X}$ , reward vector $r_{t}\\,\\,\\in\\,[0,1]^{N}$ , and a val ue predictor $f\\ \\in\\ {\\mathcal{F}}$ , let $S(f(x),r)\\;=\\;\\mathrm{argmax}_{S\\in S}\\:R(S,f(x),r)$ . According to Algorithm 3, we have $S_{t}=S(\\theta_{t},x_{t})$ . With a slight abuse of notation, for distribution $p_{t}$ over $\\mathcal{F}$ , let $\\bar{w_{t,i}}=\\mathbb{E}_{f\\sim p_{t}}[\\mathbb{1}\\{i\\in S(f(x_{t}),r_{t})\\}]$ be the probability that item $i$ is included in the selected set at round $t$ . Let $q_{t}~\\in~\\Delta(S)$ be the distribution over $\\boldsymbol{S}$ induced by $p_{t}$ , meaning that $q_{t}(S)=\\mathbb{E}_{f\\sim p_{t}}[\\mathbb{1}\\{S(f(x_{t}),r_{t})=S\\}]$ . Then, for each $i\\in[N]$ , for any $\\mu>0$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{f\\sim p_{t}}\\left[\\vert f_{i}(x_{t})-f_{i}^{\\star}(x_{t})\\vert\\cdot\\mathbb{1}\\{i\\in S(f(x_{t}),r_{t})\\}\\right]}\\\\ &{\\le\\mathbb{E}_{f\\sim p_{t}}\\left[\\frac{\\mathbb{1}\\{i\\in S(f(x_{t}),r_{t})\\}}{4\\mu w_{t,i}}+w_{t,i}(f_{i}(x_{t})-f_{i}^{\\star}(x_{t}))^{2}\\right]}\\\\ &{=\\frac{1}{4\\mu}+\\mu w_{t,i}\\mathbb{E}_{f\\sim p_{t}}\\left[(f_{i}(x_{t})-f_{i}^{\\star}(x_{t}))^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Taking a summation over all $i\\in[N]$ , we know that for any $\\mu>0$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\displaystyle\\sum_{i\\in S_{t}}|f_{t,i}(x_{t})-f_{i}^{*}(x_{t})|\\right]}\\\\ &{=\\mathbb{E}\\left[\\displaystyle\\sum_{i=1}^{N}|f_{t,i}(x_{t})-f_{i}^{*}(x_{t})|\\cdot\\mathbf{1}\\{i\\in S(f_{t}(x_{t}),r_{t})\\}\\right]}\\\\ &{=\\mathbb{E}\\left[\\displaystyle\\sum_{i=1}^{N}\\mathbb{E}f_{\\sim\\rho_{t}}|[f_{i}(x_{t})-f_{i}^{*}(x_{t})|\\cdot\\mathbf{1}\\{i\\in S(f(x_{t}),r_{t})\\}]\\right]}\\\\ &{\\overset{(i)}{\\leq}\\frac{N}{4\\mu}+\\mu\\mathbb{E}\\left[w_{t,i}\\mathbb{E}f_{\\sim p_{t}}\\left[\\displaystyle\\sum_{i=1}^{N}(f_{t}(x_{t})-f_{i}^{*}(x_{t}))^{2}\\right]\\right]}\\\\ &{\\overset{(i i)}{=}\\frac{N}{4\\mu}+\\mu\\mathbb{E}_{S\\sim\\rho_{t}}\\mathbb{E}f_{\\sim\\rho_{t}}\\left[\\displaystyle\\sum_{i\\in S_{t}}(f_{i}(x_{t})-f_{i}^{*}(x_{t}))^{2}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $(i)$ uses Eq. (33) and $(i i)$ is by definition of $w_{t,i}$ and $q_{t}$ . ", "page_idx": 27}, {"type": "text", "text": "Let $\\begin{array}{r}{\\mathrm{LS}_{t}\\,=\\,\\sum_{i\\in S_{t}}(f_{i}(x_{t})\\,-\\,f_{i}^{\\star}(x_{t}))^{2}}\\end{array}$ (\u201cLeast Squares\u201d). Combining Eq. (32) with Eq. (34), we know that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbf{Reg}_{\\mathrm{MNL}}\\leq\\frac{N T}{4\\mu}+\\mu\\mathbb{E}\\left[\\sum_{t=1}^{T}\\mathbb{E}_{S_{t}\\sim q_{t}}\\mathbb{E}_{f\\sim p_{t}}[\\mathrm{LS}_{t}]\\right]-\\mathbb{E}\\left[\\sum_{t=1}^{T}\\mathrm{FG}_{t}\\right].\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "To bound the last two terms in Eq. (35), using Lemma B.3 and the fact that $i_{t}$ is a drawn from the distribution $\\mu(S_{t},f^{\\star}(x_{t}),r_{t})$ and, we show in Lemma E.2 that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\frac{1}{(28\\eta(K+1)^{4}}\\mathbb{E}_{f\\sim p_{t}}[\\mathrm{LS}_{t}]-\\mathbb{E}_{f_{t}\\sim q_{t}}[\\mathrm{FG}_{t}]\\leq-\\frac{1}{\\eta}\\log\\mathbb{E}_{i_{t}|x_{t},S_{t}}\\mathbb{E}_{f\\sim p_{t}}\\left[\\exp(-\\eta(\\widehat{\\ell}_{t,f}-\\widehat{\\ell}_{t,f^{*}}))\\right]+4\\eta.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Therefore, picking $\\begin{array}{r}{\\mu=\\frac{1}{128\\eta(K+1)^{4}}}\\end{array}$ and combining Eq. (35) and Eq. (36), we know that ", "page_idx": 27}, {"type": "text", "text": "RegMNL ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\leq32\\eta N(K+1)^{4}T+4\\eta T-\\frac{1}{\\eta}\\mathbb{E}\\left[\\sum_{t=1}^{T}\\log\\mathbb{E}_{i_{t}|x_{t},S_{t}}\\mathbb{E}_{f\\sim p_{t}}\\left[\\exp\\left(-\\eta\\left(\\widehat{\\ell}_{t,f}-\\widehat{\\ell}_{t,f^{\\star}}\\right)\\right)\\right]\\right]\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "To bound the last term in Eq. (37), we use the exponential weight update dynamic of $p_{t}$ . Following a classic analysis of exponential weight update, we show in Lemma E.3 that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{-\\mathbb{E}\\left[\\log\\mathbb{E}_{i_{t}|x_{t},S_{t}}\\,\\mathbb{E}_{f\\sim p_{t}}\\;\\left[\\exp(-\\eta(\\widehat{\\ell}_{t,f}-\\widehat{\\ell}_{t,f^{*}}))\\right]\\right]\\leq Z_{t}-Z_{t-1},\\;\\;\\;}\\\\ {Z_{t}\\triangleq-\\mathbb{E}\\left[\\log\\mathbb{E}_{f\\sim p_{1}}\\left[\\exp\\left(-\\eta\\sum_{\\tau=1}^{t}\\left(\\widehat{\\ell}_{t,f}-\\widehat{\\ell}_{t,f^{*}}\\right)\\right)\\right]\\right].\\;\\mathrm{Combining~Eq.~}(37)}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where and Eq. (38), we arrive at ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbf{Reg}_{\\mathsf{M N L}}\\leq32\\eta N(K+1)^{4}T+4\\eta T+\\frac{1}{\\eta}\\sum_{t=1}^{T}(Z_{t}-Z_{t-1})\n$$", "text_format": "latex", "page_idx": 27}, {"type": "equation", "text": "$$\n\\leq32\\eta N(K+1)^{4}T+4\\eta T+\\frac{Z_{T}}{\\eta},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where the last inequality uses the fact that $Z_{0}=0$ . ", "page_idx": 28}, {"type": "text", "text": "Lemma E.2 Suppose that $\\eta\\leq1$ . For any distribution $p_{t}$ over $\\mathcal{F}$ , we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\frac{1}{128\\eta(K+1)^{4}}\\mathbb{E}_{f\\sim p_{t}}[\\mathrm{LS}_{t}]-\\mathbb{E}_{f_{t}\\sim q_{t}}[\\mathrm{FG}_{t}]\\leq-\\frac{1}{\\eta}\\log\\mathbb{E}_{i_{t}|x_{t},S_{t}}\\mathbb{E}_{f\\sim p_{t}}\\left[\\exp(-\\eta(\\widehat{\\ell}_{t,f}-\\widehat{\\ell}_{t,f^{*}}))\\right]+4\\eta,\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\mathrm{LS}_{t}$ and $\\mathrm{FG}_{t}$ are defined in the proof of Theorem $E.I$ , and $\\widehat{\\ell}_{t,f}$ is defined in Eq. (31). ", "page_idx": 28}, {"type": "text", "text": "Proof For notational convenience, define $c_{t,i}\\,=\\,\\mathbb{1}\\{i\\,=\\,i_{t}\\}$ for all $i\\ \\in\\ [N]$ . Let $\\varepsilon_{t,i}~=~c_{t,i}~-$ $\\mu_{i}\\bigl(S_{t},f^{\\star}(x_{t})\\bigr)$ for all $i\\ \\in\\ S_{t}$ and $\\varepsilon_{t}~\\in~\\mathbb{R}^{N+1}$ is the corresponding vector. Consider the term $-\\eta\\left(\\widehat{\\ell}_{t,f}-\\widehat{\\ell}_{t,f^{\\star}}\\right)$ for an arbitrary $f\\in\\mathcal F$ . ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{-\\eta\\left(\\widehat{\\ell}_{t,j}-\\widehat{\\ell}_{t,t^{\\prime}}\\right)}}\\\\ &{=-\\frac{1}{16}\\sum_{i\\in\\mathcal{N}_{i}}(\\rho_{i}(S_{t},f(x_{t}))-c_{t,i})^{2}+\\frac{1}{8K}\\sum_{i\\in\\mathcal{N}_{i}}(\\mu_{i}(S_{t},f^{\\ast}(x_{t}))-c_{t,i})^{2}}\\\\ &{\\quad+\\eta\\cdot\\frac{1}{8\\pi\\delta}R(S,f(x_{t}),r_{t})-\\eta\\cdot\\operatorname*{max}_{i\\in\\mathcal{N}}R(S,f^{\\ast}(x_{t}),r_{t})}\\\\ &{=-\\frac{1}{16}\\sum_{i\\in\\mathcal{N}_{i}}(\\rho_{i}(S_{t},f(x_{t}))-\\mu_{i}(S_{t},f^{\\ast}(x_{t})))(2c_{t,i}-\\mu_{i}(S_{t},f(x_{t}))-\\mu_{i}(S_{t},f^{\\ast}(x_{t})))}\\\\ &{\\quad+\\eta\\cdot\\frac{1}{8\\pi\\delta}R(S,f(x_{t}),r_{t})-\\eta\\cdot\\operatorname*{max}_{i\\in\\mathcal{N}}R(S,f^{\\ast}(x_{t}),r_{t})}\\\\ &{=-\\frac{1}{16}\\sum_{i\\in\\mathcal{N}_{i}}(\\rho_{i}(S_{t},f(x_{t}))-\\mu_{i}(S_{t},f^{\\ast}(x_{t})))(\\mu_{i}(S_{t},f^{\\ast}(x_{t}))-\\mu_{i}(S_{t},f(x_{t}))+2\\varepsilon_{i,t})}\\\\ &{\\quad+\\eta\\cdot\\frac{1}{8\\pi}\\sum_{i\\in\\mathcal{N}_{i}}(\\rho_{i}(S_{t},f^{\\ast}(x_{t}))-\\mu_{i}(S_{t},f^{\\ast}(x_{t})))(\\mu_{i}(S_{t},f^{\\ast}(x_{t}))-\\mu_{i}(S_{t},f(x_{t}))+2\\varepsilon_{i,t})}\\\\ &{\\quad+\\eta\\cdot\\operatorname*{max}_{i\\in\\mathcal{N}_{i}}(\\rho_{i}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where we define the first term as $\\widehat{\\mathrm{LS}}_{t}$ , which we will show later how this term is related to $\\mathrm{LS}_{t}$ , and the second term $\\begin{array}{r}{\\mathrm{FG}_{t}(f)\\,=\\,\\mathrm{max}_{S\\in\\mathcal{S}}\\,R(S,f(x_{t}),r_{t})\\,-\\,\\mathrm{max}_{S\\in\\mathcal{S}}\\,R(S,f^{\\star}(x_{t}),r_{t})}\\end{array}$ (so $\\mathrm{FG}_{t}\\,=$ $\\mathrm{FG}_{t}(f_{t}))$ . Consider the log of the expectation of the exponent on both sides. ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\log\\mathbb E_{f\\sim p_{t}}\\mathbb E_{c_{t}\\mid x_{t},S_{t}}\\left[\\exp\\left(-\\eta\\left(\\widehat{\\ell}_{t,f}-\\widehat{\\ell}_{t,f^{\\star}}\\right)\\right)\\right]}\\\\ &{=\\log\\mathbb E_{f\\sim p_{t}}\\mathbb E_{c_{t}\\mid x_{t},S_{t}}\\left[\\exp\\left(-\\frac{1}{16}\\widehat{\\mathrm{LS}}_{t}+\\eta\\mathrm{FG}_{t}(f)\\right)\\right]}\\\\ &{\\leq\\frac{1}{2}\\log\\mathbb E_{f\\sim p_{t}}\\left(\\mathbb E_{c_{t}\\mid x_{t},S_{t}}\\left[\\exp\\left(-\\frac{1}{16}\\widehat{\\mathrm{LS}}_{t}\\right)\\right]^{2}\\right)+\\frac{1}{2}\\log\\mathbb E_{f\\sim p_{t}}\\left[\\exp\\left(2\\eta\\mathrm{FG}_{t}(f)\\right)\\right]}\\\\ &{\\leq\\frac{1}{2}\\log\\mathbb E_{f\\sim p_{t}}\\left(\\mathbb E_{c_{t}\\mid x_{t},S_{t}}\\left[\\exp\\left(-\\frac{1}{8}\\widehat{\\mathrm{LS}}_{t}\\right)\\right]\\right)+\\frac{1}{2}\\log\\mathbb E_{f\\sim p_{t}}\\left[\\exp\\left(2\\eta\\mathrm{FG}_{t}(f)\\right)\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where the first inequality is by Cauchy-Schwarz inequality and the second inequality is because $\\mathbb{E}[x]^{2}\\leq\\mathbb{E}[x^{2}]$ . Next, we consider bounding each of the two terms. For the first term, since ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left|\\frac{1}{4}\\sum_{i\\in S_{t}}(\\mu_{i}(S_{t},f(x_{t}))-\\mu_{i}(S_{t},f^{\\star}(x_{t})))\\varepsilon_{t,i})\\right|\\leq\\frac{\\|\\mu(S_{t},f(x_{t}))-\\mu(S_{t},f^{\\star}(x_{t}))\\|_{2}}{2\\sqrt{2}}\\leq\\frac{1}{2},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "we know that $-\\textstyle{\\frac{1}{4}}(\\mu(S_{t},f^{\\star}(x_{t})\\ -\\ \\mu(S_{t},f(x_{t})))^{\\top}\\varepsilon_{t}$ is a zero-mean, $\\textstyle{\\frac{1}{8}}\\|\\mu(S_{t},f(x_{t}))\\ -$ $\\mu(S_{t},f^{\\star}(x_{t}))\\|_{2}^{2}$ -sub-Gaussian random variable given $x_{t}$ and $S_{t}$ , meaning that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{\\bar{\\xi}_{c_{t}|x_{t},S_{t}}}\\left[\\exp\\left(-\\frac{1}{4}(\\mu(S_{t},f^{\\star}(x_{t})-\\mu(S_{t},f(x_{t})))^{\\top}\\varepsilon_{t}\\right)\\right]\\le\\exp\\left(\\frac{1}{16}\\|\\mu(S_{t},f(x_{t}))-\\mu(S_{t},f^{\\star}(x_{t}))\\|^{2}\\right)\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Therefore, we know that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Xi_{c_{t}|x_{t},S_{t}}\\left[\\exp\\left(-\\frac{1}{8}\\widehat{\\mathrm{LS}}_{t}\\right)\\right]}\\\\ &{=\\exp\\left(-\\frac{1}{8}\\|\\mu(S_{t},f(x_{t}))-\\mu(S_{t},f^{\\star}(x_{t}))\\|_{2}^{2}\\right)\\mathbb{E}_{c_{t}|x_{t},S_{t}}\\left[\\exp\\left(-\\frac{1}{4}(\\mu(S_{t},f^{\\star}(x_{t})-\\mu(S_{t},f(x_{t})))^{\\top}\\varepsilon\\right)\\right]}\\\\ &{\\leq\\exp\\left(-\\frac{1}{16}\\|\\mu(S_{t},f(x_{t}))-\\mu(S_{t},f^{\\star}(x_{t}))\\|_{2}^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Then, since $\\begin{array}{r}{\\frac{1}{16}\\|\\mu(S_{t},f(x_{t}))\\,-\\,\\mu(S_{t},f^{\\star}(x_{t}))\\|_{2}^{2}\\;\\le\\;\\frac{1}{8}}\\end{array}$ , using the fact that $\\exp(x)\\;\\leq\\;1+\\frac{x}{2}$ for $x\\in[-1,0]$ , we know that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{c_{t}\\mid x_{t},S_{t}}\\left[\\exp\\left(-\\frac{1}{8}\\widehat{\\mathrm{LS}}_{t}\\right)\\right]}\\\\ &{\\leqslant1-\\displaystyle\\frac{1}{32}\\|\\mu(S_{t},f(x_{t}))-\\mu(S_{t},f^{\\star}(x_{t}))\\|_{2}^{2}}\\\\ &{\\leqslant1-\\displaystyle\\frac{1}{64(K+1)^{4}}\\sum_{i\\in S_{t}}(f_{i}(x_{t})-f_{i}^{\\star}(x_{t}))^{2}}\\\\ &{=1-\\displaystyle\\frac{1}{64(K+1)^{4}}\\mathrm{LS}_{t},}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where the second inequality is because Lemma B.3. Further using the fact that $\\log(1+x)\\leq x$ for all $x\\geq-1$ , we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n{\\frac{1}{2}}\\log\\mathbb{E}_{f\\sim p_{t}}\\left(\\mathbb{E}_{c_{t}|x_{t},S_{t}}\\left[\\exp\\left(-{\\frac{1}{8}}\\widehat{\\mathrm{LS}}_{t}\\right)\\right]\\right)\\leq-{\\frac{1}{128(K+1)^{4}}}\\mathrm{LS}_{t}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Consider the second term in Eq. (39). Since $\\eta\\leq1$ and $|\\mathrm{FG}_{t}(f)|\\le1$ , using $e^{x}\\leq1+x+2x^{2}$ for $x\\leq1$ , we know that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\frac{1}{2}\\log\\mathbb{E}_{f\\sim q_{t}}\\left[\\exp(2\\eta\\mathrm{FG}_{t}(f))\\right]\\leq\\frac{1}{2}\\log\\left(1+2\\eta\\mathbb{E}_{f\\sim q_{t}}[\\mathrm{FG}_{t}(f)]+2(2\\eta)^{2}\\right)}&{}&\\\\ {\\leq\\eta\\mathbb{E}_{f\\sim q_{t}}[\\mathrm{FG}_{t}(f)]+4\\eta^{2}}&{(\\log(1+x)\\leq x)}\\\\ &{=\\eta\\mathbb{E}_{f\\sim q_{t}}[\\mathrm{FG}_{t}]+4\\eta^{2}.}&{(f_{t}\\mathrm{~is~drawn~from~}q_{t})}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Plugging the last bound and Eq. (40) into Eq. (39) and rearranging finishes the proof. ", "page_idx": 29}, {"type": "text", "text": "The next lemma follows the classic analysis of multiplicative weight update algorithm. ", "page_idx": 29}, {"type": "text", "text": "Lemma E.3 Algorithm 3 guarantees that for each $t\\in[T]$ , ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\mathbb{E}\\left[\\mathbb{E}_{S_{t}\\sim q_{t}}\\log\\mathbb{E}_{c_{t}\\mid x_{t},S_{t}}\\operatorname{\\mathbb{E}}_{f\\sim p_{t}}\\left[\\exp(-\\eta(\\widehat{\\ell}_{t,f}-\\widehat{\\ell}_{t,f^{*}}))\\right]\\right]\\leq Z_{t}-Z_{t-1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $\\begin{array}{r}{Z_{t}\\ =\\ -\\mathbb{E}\\left[\\log\\mathbb{E}_{f\\sim p_{1}}\\left[\\exp\\left(-\\eta\\sum_{\\tau=1}^{t}\\left(\\widehat{\\ell}_{t,f}-\\widehat{\\ell}_{t,f^{*}}\\right)\\right)\\right]\\right]}\\end{array}$ and $q_{t}\\ \\in\\ \\Delta(S)$ satisfies that $\\begin{array}{r}{\\chi_{t}(S)=\\mathbb{E}_{f\\sim p_{t}}[\\mathbb{1}\\{\\bar{S}=\\operatorname*{argmax}_{S^{\\prime}\\in S}\\ln(S^{\\prime},f(x_{t}),r_{t})\\}].\\qquad\\qquad\\mathrm{~}}\\end{array}$ ", "page_idx": 29}, {"type": "text", "text": "Proof Let $\\begin{array}{r l}{\\!\\!\\!\\!\\!G_{t,f}\\triangleq\\exp\\left(-\\eta\\sum_{\\tau=1}^{t}\\left(\\widehat{\\ell}_{t,f}-\\widehat{\\ell}_{t,f^{*}}\\right)\\right)}\\end{array}$ . According to Algorithm 3, we know that ", "page_idx": 29}, {"type": "equation", "text": "$$\np_{t,f}=\\frac{\\exp\\left(-\\eta\\sum_{\\tau=1}^{t-1}\\widehat{\\ell}_{\\tau,f}\\right)}{\\int_{f^{\\prime}\\in\\mathcal{F}}\\exp\\left(-\\eta\\sum_{\\tau=1}^{t-1}\\widehat{\\ell}_{\\tau,f^{\\prime}}\\right)d f^{\\prime}}=\\frac{G_{t-1,f}}{\\int_{f^{\\prime}\\in\\mathcal{F}}G_{t-1,f^{\\prime}}d f^{\\prime}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Then, according to the definition of $Z_{t}$ , we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Z_{t-1}-Z_{t}}\\\\ &{=\\mathbb{E}\\left[\\log\\frac{\\int_{f\\in\\mathcal{F}}G_{t,f}d f}{\\int_{f\\in\\mathcal{F}}G_{t-1,f}d f}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\mathbb{E}\\left[\\log\\frac{\\int_{f\\in\\mathcal{F}}G_{t-1,f}\\exp(-\\eta(\\widehat{\\ell}_{t,f}-\\widehat{\\ell}_{t,f^{*}}))d f}{\\int_{f\\in\\mathcal{F}}G_{t-1,f}d f}\\right]}\\\\ &{=\\mathbb{E}\\left[\\log\\mathbb{E}_{f\\sim p_{t}}\\left[\\exp(-\\eta(\\widehat{\\ell}_{t,f}-\\widehat{\\ell}_{t,f^{*}})\\right]\\right]}\\\\ &{\\leq\\mathbb{E}\\left[\\mathbb{E}_{S_{t}\\sim q_{t}}\\log\\mathbb{E}_{c_{t}|x_{t},S_{t}}\\mathbb{E}_{f\\sim p_{t}}\\left[\\exp(-\\eta(\\widehat{\\ell}_{t,f}-\\widehat{\\ell}_{t,f^{*}})\\right]\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where the last inequality is due to Jensen\u2019s inequality. Rearranging the terms finishes the proof. \u25a1 Next, we restate and prove Corollary 4.8. ", "page_idx": 30}, {"type": "text", "text": "Corollary E.4 Under Assumption $^{\\,l}$ , Algorithm 3 wensures ${\\bf R e g}_{\\mathrm{MNL}}=\\mathcal{O}\\big(K^{2}\\sqrt{N T\\log|\\mathcal{F}|}\\big)$ for the finite class and ${\\bf R e g}_{\\sf M N L}=\\mathcal{O}\\big(K^{2}\\sqrt{d N T\\log(B T K)}\\big)$ for the linear class. ", "page_idx": 30}, {"type": "text", "text": "Proof For a finite function class $\\mathcal{F}$ , since $q_{1}$ is uniform, we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Z_{T}=-\\mathbb{E}\\left[\\log\\displaystyle\\sum_{f\\in\\mathcal{F}}\\frac{1}{|\\mathcal{F}|}\\exp\\left(-\\eta\\displaystyle\\sum_{t=1}^{T}\\left(\\widehat{\\ell}_{t,f}-\\widehat{\\ell}_{t,f^{\\star}}\\right)\\right)\\right]}\\\\ &{\\leq-\\mathbb{E}\\left[\\log\\displaystyle\\frac{1}{|\\mathcal{F}|}\\exp\\left(-\\eta\\displaystyle\\sum_{t=1}^{T}\\left(\\widehat{\\ell}_{t,f^{\\star}}-\\widehat{\\ell}_{t,f^{\\star}}\\right)\\right)\\right]=\\log|\\mathcal{F}|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Combining with Theorem E.1 and picking $\\begin{array}{r}{\\eta=\\frac{1}{K^{2}}\\sqrt{\\frac{N\\log|\\mathcal{F}|}{T}}.}\\end{array}$ , we prove the first conclusion. ", "page_idx": 30}, {"type": "text", "text": "To prove our results for the linear class, we first show a more general results for parametrized Lipschitz function class. Suppose that $\\mathcal{F}$ is a $d$ -dimensional parametrized function class defined as: ", "page_idx": 30}, {"type": "text", "text": "Direct calculation shows that the linear function class we consider is an instance of Eq. (41) with $\\alpha\\,=\\,1$ . For function class satisfying Eq. (41), we aim to show that $Z_{T}=\\mathcal{O}(K\\eta+d\\bar{\\log}(\\alpha B T))$ . Specifically, we consider a small $\\ell_{2}$ -ball around the true parameter $\\theta^{\\star}$ : $\\Omega_{T}\\,=\\,\\{\\theta\\,:\\,\\|\\theta\\,-\\,\\theta^{*}\\|_{2}\\,\\leq$ $\\bar{\\frac{1}{\\alpha T K}}\\}$ . Since $\\mathcal{F}$ is $\\alpha$ -Lipschitz with respect to $\\|\\cdot\\|_{2}$ , we know that for any $x\\in\\mathscr{X}$ , and any $i\\in[N]$ , ", "page_idx": 30}, {"type": "equation", "text": "$$\n|f_{\\theta,i}(x)-f_{\\theta^{\\star},i}(x)|\\leq\\frac{1}{T K}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Therefore, for any $\\theta\\in\\Omega_{T}$ , ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\;\\eta(\\widehat{\\ell}_{t,f_{\\theta}}-\\widehat{\\ell}_{t,f_{\\theta^{\\star}}})}\\\\ &{=-\\displaystyle\\frac{1}{16}\\sum_{i\\in S_{t}}(f_{\\theta,i}(x_{t})-c_{t,i})^{2}+\\displaystyle\\frac{1}{16}\\sum_{i\\in S_{t}}(f_{\\theta,i}(x_{t})-c_{t,i})^{2}}\\\\ &{\\qquad+\\displaystyle\\eta\\cdot\\operatorname*{max}_{S\\in\\mathcal{S}}R(S,f_{\\theta}(x_{t}),r_{t})-\\eta\\cdot\\operatorname*{max}_{S\\in\\mathcal{S}}R(S,f_{\\theta^{\\star}}(x_{t}),r_{t})}\\\\ &{\\geq-\\displaystyle\\frac{1}{8}\\sum_{i\\in S_{t}}|f_{\\theta,i}(x_{t})-f_{\\theta^{\\star},i}(x_{t})|+\\eta\\cdot\\operatorname*{max}_{S\\in\\mathcal{S}}R(S,f_{\\theta}(x_{t}),r_{t})-\\eta\\cdot\\operatorname*{max}_{S\\in\\mathcal{S}}R(S,f_{\\theta^{\\star}}(x_{t}),r_{t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Let $S(f_{\\theta^{\\star}}(x_{t}),r_{t})=\\operatorname*{argmax}_{S\\in\\mathcal{S}}R(S,f_{\\theta^{\\star}}(x_{t}),r_{t})$ . Then, we can further lower bound Eq. (43) as follows: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle-\\,\\eta(\\widehat{\\ell}_{t,f_{\\theta}}-\\widehat{\\ell}_{t,f_{\\theta^{\\star}}})}\\\\ {\\displaystyle\\ge-\\frac{1}{8}\\sum_{i\\in S_{t}}\\vert f_{\\theta,i}(x_{t})-f_{\\theta^{\\star},i}(x_{t})\\vert+\\eta R\\big(S(f_{\\theta^{\\star}}(x_{t}),r_{t}),f_{\\theta}(x_{t}),r_{t}\\big)-\\eta\\cdot\\operatorname*{max}_{S\\in S}R(S,f_{\\theta^{\\star}}(x_{t}),r_{t})}\\\\ {\\displaystyle\\overset{(i)}{\\ge}-\\frac{1}{8}\\sum_{i\\in S_{t}}\\vert f_{\\theta,i}(x_{t})-f_{\\theta^{\\star},i}(x_{t})\\vert-\\eta\\sum_{i\\in S(f_{\\theta^{\\star}}(x_{t}),r_{t})}\\vert f_{\\theta,i}(x_{t})-f_{\\theta^{\\star},i}(x_{t})\\vert}\\\\ {\\displaystyle\\overset{(i i)}{\\ge}-\\frac{1}{8T}-\\frac{\\eta}{T},}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $(i)$ is because Lemma B.1 and $(i i)$ uses Eq. (42). This means that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Z_{T}=-\\mathbb{E}\\left[\\log\\mathbb{E}_{f\\sim q_{1}}\\exp\\left(-\\eta\\displaystyle\\sum_{t=1}^{T}\\left(\\widehat{\\ell}_{t,f_{\\theta}}-\\widehat{\\ell}_{t,f_{\\theta^{\\star}}}\\right)\\right)\\right]}\\\\ &{\\quad\\le-\\mathbb{E}\\left[\\log(\\alpha B T)^{-d}\\operatorname*{inf}_{\\theta\\in\\Omega_{T}}\\exp\\left(-\\eta\\displaystyle\\sum_{t=1}^{T}\\left(\\widehat{\\ell}_{t,f_{\\theta}}-\\widehat{\\ell}_{t,f_{\\theta^{\\star}}}\\right)\\right)\\right]}\\\\ &{\\quad\\le d\\log(\\alpha B T)+\\frac{1}{8}+\\eta=\\mathcal{O}(K\\eta+d\\log(\\alpha B K T)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "With the optimal choice of \u03b7 = K12 $\\begin{array}{r}{\\eta=\\frac{1}{K^{2}}\\sqrt{\\frac{N d\\log(\\alpha B T K)}{T}}}\\end{array}$ Nd log(T\u03b1BT K), Theorem E.1 shows that Algorithm 3 guarantees that for linear function class ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbf{Reg}_{\\mathrm{MNL}}=\\mathcal{O}\\left(K^{2}\\sqrt{d N T\\log(B T K)}\\right).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: See abstract and Section 1. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 32}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: See Section 1. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 32}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: See Assumption 1, Assumption 2, Assumption 3, and the appendix. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 33}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: This paper does not include experiments. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 33}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: This paper does not include experiments. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so No is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 34}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: This paper does not include experiments. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 34}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: This paper does not include experiments. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: This paper does not include experiments. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 35}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: The authors have reviewed the NeurIPS Code of Ethics. The research conducted in this paper conforms with it in every respect. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 35}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: This work is mostly theoretical, and we do not foresee any negative ethical or societal outcomes. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. \u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. ", "page_idx": 35}, {"type": "text", "text": "\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 36}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: This paper poses no such risks. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 36}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: This paper does not use existing assets. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/ datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 36}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 37}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 37}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 37}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 37}]