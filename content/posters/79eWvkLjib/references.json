{"references": [{"fullname_first_author": "Andr\u00e9 Barreto", "paper_title": "Successor features for transfer in reinforcement learning", "publication_date": "2017", "reason": "This paper introduces successor features, a core concept used in the zero-shot RL methods explored in the main paper."}, {"fullname_first_author": "Ahmed Touati", "paper_title": "Learning one representation to optimize all rewards", "publication_date": "2021", "reason": "This paper introduces forward-backward representations, the main zero-shot RL algorithm compared against in the main paper."}, {"fullname_first_author": "Aviral Kumar", "paper_title": "Conservative q-learning for offline reinforcement learning", "publication_date": "2020", "reason": "This paper introduces conservative Q-learning, a technique that addresses the out-of-distribution issue prevalent in offline RL and adapted for zero-shot RL in the main paper."}, {"fullname_first_author": "Justin Fu", "paper_title": "D4rl: Datasets for deep data-driven reinforcement learning", "publication_date": "2020", "reason": "This paper introduces the D4RL benchmark, one of the datasets used for experiments in the main paper."}, {"fullname_first_author": "Denis Yarats", "paper_title": "Don't change the algorithm, change the data: Exploratory data for offline reinforcement learning", "publication_date": "2022", "reason": "This paper introduces the ExORL benchmark, the primary dataset used in the main paper's experiments."}]}