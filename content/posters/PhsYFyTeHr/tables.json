[{"figure_path": "PhsYFyTeHr/tables/tables_3_1.jpg", "caption": "Table 1: Effectiveness of disentangled architecture.", "description": "This table presents the results of an ablation study comparing the effectiveness of a baseline NAT architecture against a disentangled architecture.  The baseline uses the same architecture as existing NAT models, processing visible and [MASK] tokens equally. The disentangled architecture processes them separately. The table shows that the disentangled architecture achieves a lower FID score (5.50) with a similar GFLOPS compared to the baseline (6.54). This demonstrates the effectiveness of disentangling visible and [MASK] token computations in NATs.", "section": "4.1 Spatial Level Interaction"}, {"figure_path": "PhsYFyTeHr/tables/tables_4_1.jpg", "caption": "Table 2: Effects of prioritizing visible tokens. NE, ND: encoder/decoder layers (for visible/[MASK] tokens). Network width is slightly adjusted to make GFLOPS approximately unchanged.", "description": "This ablation study investigates the impact of computation allocation between visible and masked tokens on the performance of ENAT.  By varying the number of encoder (NE) and decoder (ND) layers for visible and masked tokens respectively, while maintaining a roughly constant computational cost (GFLOPs), the impact on FID score is evaluated.  The results demonstrate that prioritizing computation for visible tokens significantly improves performance.", "section": "4.2 Temporal Level Interaction"}, {"figure_path": "PhsYFyTeHr/tables/tables_7_1.jpg", "caption": "Table 3: Results on ImageNet 256\u00d7256 . TFLOPs quantify the total computational cost for generating a single image. For DPM-Solver [40] augmented diffusion models (\u2020), we follow [40] to tune configurations and report the lowest FID. Diff: diffusion, AR: autoregressive.", "description": "This table compares the performance of ENAT with other generative models on the ImageNet 256x256 dataset.  It shows various metrics including the number of parameters, the number of steps in the generation process, the computational cost (in TFLOPs), the Fr\u00e9chet Inception Distance (FID), and the Inception Score (IS).  Lower FID values indicate better image quality, while higher IS values indicate greater diversity.  The table highlights that ENAT achieves superior performance with significantly lower computational cost compared to other methods.", "section": "5.1 Main Results"}, {"figure_path": "PhsYFyTeHr/tables/tables_7_2.jpg", "caption": "Table 4: Results on ImageNet 512\u00d7512. \u2020: DPM-Solver [40] augmented diffusion models.", "description": "This table presents the results of various image generation models on the ImageNet 512x512 dataset.  It compares different model types (Autoregressive, Diffusion, and Non-autoregressive Transformers), the number of parameters, the number of steps used in the generation process, the total computational cost in TeraFLOPS, the Frechet Inception Distance (FID) score, and the Inception Score (IS). Lower FID indicates better image quality, and higher IS indicates better diversity. The \u2020 symbol indicates that the DPM-Solver method was used for augmented diffusion models.  This table highlights that EfficientNAT (ENAT) achieves competitive results with lower computational costs compared to other state-of-the-art methods.", "section": "5.1 Main Results"}, {"figure_path": "PhsYFyTeHr/tables/tables_7_3.jpg", "caption": "Table 5: Results on MS-COCO; all models are trained and evaluated on MS-COCO. \u2020: DPM-Solver [40] augmented diffusion models.", "description": "This table compares the performance of different image generation models on the MS-COCO dataset for text-to-image generation.  It shows the number of parameters, number of generation steps, computational cost (TFLOPs), and FID score for each model.  The table highlights the superior performance of ENAT-B in terms of FID score while maintaining a very low computational cost.", "section": "5.1 Main Results"}, {"figure_path": "PhsYFyTeHr/tables/tables_9_1.jpg", "caption": "Table 6: Ablation studies on ImageNet 256\u00d7256. We use ENAT-S with 8 generation steps as our default setting, which is marked in gray. We report FID-50K following [3, 49] and total GFLOPS for each NAT model throughout the generation process.", "description": "This table presents ablation studies performed on the ImageNet 256x256 dataset using the ENAT-S model with 8 generation steps.  It demonstrates the impact of the disentangled architecture and computation reuse mechanisms on the model's performance. FID (Fr\u00e9chet Inception Distance) and GFLOPs (floating point operations) are used to measure the performance and computational cost respectively. The default setting is marked in gray, allowing for easy comparison between different configurations.", "section": "5.2 Ablation Studies"}, {"figure_path": "PhsYFyTeHr/tables/tables_9_2.jpg", "caption": "Table 6: Ablation studies on ImageNet 256x256. We use ENAT-S with 8 generation steps as our default setting, which is marked in gray. We report FID-50K following [3, 49] and total GFLOPS for each NAT model throughout the generation process.", "description": "This table presents ablation study results on ImageNet 256x256, using ENAT-S with 8 generation steps as the baseline.  It evaluates the impact of different design choices on the model's performance, measured by FID (Fr\u00e9chet Inception Distance) and GFLOPs (giga floating-point operations).  The modifications tested include the disentangled architecture, reuse mechanism, SC-Attention, and various choices about which token features to reuse and which layers' features to reuse. The results show how these choices affect the FID score and computational cost of the model.", "section": "5.2 Ablation Studies"}, {"figure_path": "PhsYFyTeHr/tables/tables_9_3.jpg", "caption": "Table 6: Ablation studies on ImageNet 256\u00d7256. We use ENAT-S with 8 generation steps as our default setting, which is marked in gray. We report FID-50K following [3, 49] and total GFLOPS for each NAT model throughout the generation process.", "description": "This table presents ablation studies performed on the ImageNet 256x256 dataset using the ENAT-S model with 8 generation steps.  It investigates the impact of different design choices within the ENAT architecture on model performance, measured by FID-50K and computational cost (GFLOPS). The default ENAT-S configuration is shown in gray, and the table shows variations in FID and GFLOPS when specific architectural components are removed or modified.  The experiments assess the impact of the disentangled architecture, the computation reuse mechanism, the choice of attention mechanism (SC-Attention vs. self and cross-attention), and the selection of layers to reuse features from. The results highlight the relative importance of different ENAT components for achieving efficient and effective image generation.", "section": "5.2 Ablation Studies"}, {"figure_path": "PhsYFyTeHr/tables/tables_16_1.jpg", "caption": "Table 7: Summary of model configurations. NE: encoder layers (for visible token encoding), ND: decoder layers (for [MASK] token decoding). *: In conventional NAT models, the layers for visible token encoding are shared with the layers for [MASK] token decoding.", "description": "This table presents the detailed configurations of all NAT models used in the paper.  It lists the number of encoder layers (NE), decoder layers (ND), the dimension of hidden states (embed dim.), and the number of attention heads (#attn. heads) for each model.  It also notes whether the model uses the computation reuse technique ('reuse?') and indicates that in conventional NAT models, the encoder layers are shared for both visible and [MASK] token decoding.", "section": "A.1 Detailed model configurations"}, {"figure_path": "PhsYFyTeHr/tables/tables_18_1.jpg", "caption": "Table 7: Summary of model configurations. NE: encoder layers (for visible token encoding), ND: decoder layers (for [MASK] token decoding). *: In conventional NAT models, the layers for visible token encoding are shared with the layers for [MASK] token decoding.", "description": "This table presents the detailed configurations of all NAT models used in the paper.  It lists the number of encoder and decoder layers, the dimensionality of the hidden states, and the number of attention heads for each model. The table highlights whether the encoder layers are shared (in conventional NATs) or separated (in ENAT).", "section": "A.1 Detailed model configurations"}]