{"references": [{"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper introduces the foundational large language model (LLM) architecture that NEST builds upon, demonstrating the potential of LLMs as multi-task solvers."}, {"fullname_first_author": "Urvashi Khandelwal", "paper_title": "Generalization through Memorization: Nearest Neighbor Language Models", "publication_date": "2020-00-00", "reason": "This paper introduces the kNN-LM approach, a semi-parametric method that NEST extends, enabling LLMs to leverage external knowledge for improved generation quality and attribution."}, {"fullname_first_author": "Sebastian Borgeaud", "paper_title": "Improving language models by retrieving from trillions of tokens", "publication_date": "2022-00-00", "reason": "This paper introduces RETRO, a retrieval-augmented LLM that directly addresses the hallucination problem by retrieving relevant text passages, inspiring NEST's retrieval mechanism."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Llama: Open and efficient foundation language models", "publication_date": "2023-02-13", "reason": "This paper introduces the Llama family of LLMs, which NEST uses as the base model for evaluation, demonstrating the effectiveness of NEST across different-sized models."}, {"fullname_first_author": "Yaniv Leviathan", "paper_title": "Fast inference from transformers via speculative decoding", "publication_date": "2023-07-23", "reason": "This paper introduces speculative decoding, a method for accelerating LLM inference by generating multiple tokens concurrently. NEST adapts this method, achieving a significant speedup."}]}