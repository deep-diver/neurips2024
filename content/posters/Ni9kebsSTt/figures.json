[{"figure_path": "Ni9kebsSTt/figures/figures_2_1.jpg", "caption": "Figure 1: The NEST approach first locates the tokens in the corpus using the LM hidden states. The retrieval distribution Pk-NN is dynamically interpolated with PLM based on the retriever's uncertainty \u03bbt. The token and its n-gram continuation are then selected from the mixture distribution P\u043c, while the final span length is determined by speculative decoding to remove undesired tokens. The spans incorporated in the final generation provide direct attribution and amortize the generation latency.", "description": "This figure illustrates the three main steps of the NEST approach: First, it uses the language model's hidden states to locate relevant tokens within a corpus.  Second, it dynamically interpolates the retrieved token distribution with the language model's distribution based on retrieval confidence. Third, it selects a token and its continuation (span) using a speculative decoding method, which helps to ensure fluency and avoid hallucinations. The final output incorporates spans from the corpus which provides source attribution for generated text, improving both accuracy and efficiency. The figure also shows the amortized generation latency, highlighting the efficiency gains offered by NEST.", "section": "3 Nearest Neighbor Speculative Decoding"}, {"figure_path": "Ni9kebsSTt/figures/figures_7_1.jpg", "caption": "Figure 2: Latency-accuracy trade-off and breakdown on Biography using Llama-2-Chat 70B+NEST. As the relaxation factor \u03b3 decreases, NEST tends to accept longer spans from the corpus. We choose \u03b3 = 5e-2 in our main experiments, which accelerates the generation and improves the FACTSCORE.", "description": "This figure presents a latency-accuracy trade-off analysis and a latency breakdown for the Llama-2-Chat 70B model enhanced with NEST on the Biography dataset.  The left chart (a) shows the breakdown of latency across different components of the NEST model (passage search, token index building, LM encoding, token search, rejection sampling) for various values of the relaxation factor (\u03b3). The right chart (b) shows the relationship between the relaxation factor, average span length selected from the corpus, and FACTSCORE.  Lower relaxation factors (\u03b3) correlate with longer span lengths and higher FACTSCORE. The main experiment used \u03b3 = 5e-2, balancing speed and accuracy improvements.", "section": "4.5 Latency Analysis"}, {"figure_path": "Ni9kebsSTt/figures/figures_7_2.jpg", "caption": "Figure 2: Latency-accuracy trade-off and breakdown on Biography using Llama-2-Chat 70B+NEST. As the relaxation factor \u03b3 decreases, NEST tends to accept longer spans from the corpus. We choose \u03b3 = 5e-2 in our main experiments, which accelerates the generation and improves the FACTSCORE.", "description": "This figure shows two sub-figures. (a) shows a bar chart representing the latency breakdown of the NEST model (a = 0.3, r = 0.1, \u03b4 = 0.5) for different relaxation factors on the Biography validation data. The latency experiment is done on 8xA100 GPUs (for model parallelization) and 32 CPU threads (for search). The batch size is set to 1.  The sub-figure shows the components: base LM latency, passage search, token index building, LM encoding, token search and rejection sampling. (b) is a line chart showing the FACTSCORE and average span length (number of tokens) as a function of the relaxation factor (\u03b3).  The FACTSCORE is shown in blue and the span length is shown in orange. This sub-figure visually illustrates the latency-accuracy trade-off by tuning the relaxation factor.  It shows that smaller \u03b3 values lead to longer spans but may not always result in higher FACTSCORE, indicating an optimal point for the relaxation factor that balances both latency and accuracy.", "section": "4.5 Latency Analysis"}, {"figure_path": "Ni9kebsSTt/figures/figures_20_1.jpg", "caption": "Figure 1: The NEST approach first locates the tokens in the corpus using the LM hidden states. The retrieval distribution Pk-NN is dynamically interpolated with PLM based on the retriever's uncertainty \u03bbt. The token and its n-gram continuation are then selected from the mixture distribution P\u043c, while the final span length is determined by speculative decoding to remove undesired tokens. The spans incorporated in the final generation provide direct attribution and amortize the generation latency.", "description": "This figure illustrates the Nearest Neighbor Speculative Decoding (NEST) approach.  It begins by using the language model's hidden states to identify relevant tokens within a corpus.  A retrieval distribution (Pk-NN) is then created, which is dynamically combined with the language model's probability distribution (PLM) based on the uncertainty of the retrieval process.  The algorithm selects the most likely token and its n-gram continuation from the resulting mixed distribution. Speculative decoding is then employed to refine the span length and eliminate unwanted tokens.  The final generated text incorporates these spans, thereby providing source attribution and improving generation speed.", "section": "3 Nearest Neighbor Speculative Decoding"}]