[{"figure_path": "ZJ2ONmSgCS/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration of DiffHammer. From left to right: (a) Set of purifications with unshared (So) / shared (S1) vulnerabilities. (b,c) Maximizing the loss function on S0 U S\u2081 may lead to less effective attacks than on S\u2081. (d,e) Selective attack targets on S\u2081 to avoid the gradient dilemma. We identify purifications from S\u2081 in the E-step and aggregate the gradients of these purifications in the M-step.", "description": "This figure illustrates the DiffHammer attack method.  Panel (a) shows two sets of purifications: those with shared vulnerabilities (S1) and those with non-shared vulnerabilities (S0). Panels (b) and (c) demonstrate the gradient dilemma.  Attacking purifications with both shared and non-shared vulnerabilities (S0 U S1) is less effective than focusing solely on the shared vulnerabilities (S1). Panels (d) and (e) illustrate the EM algorithm used in DiffHammer. The E-step identifies purifications with shared vulnerabilities (S1), and the M-step aggregates their gradients to improve attack effectiveness.", "section": "3 DiffHammer"}, {"figure_path": "ZJ2ONmSgCS/figures/figures_4_1.jpg", "caption": "Figure 1: Illustration of DiffHammer. From left to right: (a) Set of purifications with unshared (So) / shared (S1) vulnerabilities. (b,c) Maximizing the loss function on So U S1 may lead to less effective attacks than on S1. (d,e) Selective attack targets on S1 to avoid the gradient dilemma. We identify purifications from S1 in the E-step and aggregate the gradients of these purifications in the M-step.", "description": "This figure illustrates the DiffHammer algorithm, highlighting its selective attack strategy to overcome the gradient dilemma in diffusion-based purification. It shows how DiffHammer identifies purifications with shared vulnerabilities (S1) and avoids attacking purifications with unshared vulnerabilities (So) which leads to ineffective gradient updates. The E-step identifies S1 purifications, and the M-step aggregates their gradients for a more effective attack.", "section": "3 DiffHammer"}, {"figure_path": "ZJ2ONmSgCS/figures/figures_4_2.jpg", "caption": "Figure 2: Illustration of efficient gradient aggregation.", "description": "This figure illustrates the gradient aggregation method used in DiffHammer for efficiency.  It shows three purifications (\u03c6\u2081, \u03c6\u2082, \u03c6\u2083) applied to the input sample x, each followed by a classifier and loss calculation.  The gradients from these purifications (grad\u2081, grad\u2082, grad\u2083) are weighted and aggregated before backpropagation. The weights (\u00d70.2, \u00d70.8, \u00d70.5) show how the gradients are combined for a more efficient gradient estimate compared to computing gradients for all purifications individually.", "section": "3.1.2 Gradient grafting"}, {"figure_path": "ZJ2ONmSgCS/figures/figures_5_1.jpg", "caption": "Figure 3: Distribution of attack results (l\u221e : 8/255) for 1-evaluation (inner ring) and 10-evaluation (outer ring). 32.6%-46.3% of the samples have unshared vulnerabilities, imposing underestimated resubmit risk in 1-evaluation.", "description": "This figure shows the distribution of attack success rates for different diffusion-based purification methods under two evaluation scenarios: 1-evaluation and 10-evaluation.  The inner ring represents the results of a single attack attempt (1-evaluation), while the outer ring shows the results when an attack is allowed to be resubmitted up to 10 times (10-evaluation).  A significant finding is that 32.6% to 46.3% of samples exhibit unshared vulnerabilities, meaning that different purifications have different vulnerabilities.  This highlights the limitation of single-attempt (1-evaluation) as a measure of robustness, as it underestimates the risk when attackers can repeatedly try to find a weakness.", "section": "3.2 In-loop N-evaluation"}, {"figure_path": "ZJ2ONmSgCS/figures/figures_7_1.jpg", "caption": "Figure 4: Avg.Rob and Wor.Rob for the first 75 steps of different attacks with l\u221e: 8/255.", "description": "This figure displays the average robustness (Avg.Rob) and worst-case robustness (Wor.Rob) over the first 75 steps of different attack methods against three different diffusion-based purification defenses: DiffPure, GDMP, and LM.  The x-axis represents the number of attack iterations, while the y-axis shows the robustness percentage.  Each line represents a different attack algorithm (BPDA, DA, PGD, DH), and the colored lines distinguish between average and worst-case robustness for each method.  The figure helps to visualize the effectiveness and efficiency of the different attack methods against each defense, highlighting the relative performance of DiffHammer (DH) compared to other state-of-the-art attacks.", "section": "4.2 Effectiveness and efficiency of DiffHammer"}, {"figure_path": "ZJ2ONmSgCS/figures/figures_8_1.jpg", "caption": "Figure 5: Clustering effects and forgetting phenomenon in gradient dilemma (l\u221e: 8/255).", "description": "This figure visualizes the clustering effects and forgetting phenomenon observed in the gradient dilemma.  The left subplot (a) shows the distribution of silhouette coefficients (SC) for gradients categorized into sets S0 (gradients with unshared vulnerabilities) and S1 (gradients with shared vulnerabilities), using cosine similarity.  A clear separation between S0 and S1 suggests distinct gradient characteristics. The right subplot (b) illustrates the attack success rate (ASR) in the previous iteration (t-1) for different attacks. It highlights the consistency of DiffHammer's attacks compared to other methods.  The consistent ASR for DiffHammer across iterations indicates an effective strategy in mitigating the gradient dilemma.", "section": "4.3 Gradient dilemma and transfer-based attack"}, {"figure_path": "ZJ2ONmSgCS/figures/figures_17_1.jpg", "caption": "Figure 4: Avg.Rob and Wor.Rob for the first 75 steps of different attacks with l\u221e: 8/255.", "description": "This figure shows the average robustness (Avg.Rob) and worst-case robustness (Wor.Rob) for three different diffusion-based purification methods (DiffPure, GDMP, and LM) against various attack methods over the first 75 iterations.  The l\u221e norm is set to 8/255.  It illustrates the effectiveness and efficiency of different attacks against these defense mechanisms.", "section": "4.2 Effectiveness and efficiency of DiffHammer"}, {"figure_path": "ZJ2ONmSgCS/figures/figures_17_2.jpg", "caption": "Figure 4: Avg.Rob and Wor.Rob for the first 75 steps of different attacks with l\u221e: 8/255.", "description": "This figure shows the average robustness (Avg.Rob) and worst-case robustness (Wor.Rob) for three different diffusion-based purification methods (DiffPure, GDMP, and LM) against various attack algorithms (BPDA, DA/AA, PGD, and DiffHammer) over the first 75 steps.  The l\u221e norm is set to 8/255. The figure helps to visualize how the robustness of the purification methods changes over the iterations of the attacks and allows for comparison of different attack methods against the purification methods.", "section": "4.2 Effectiveness and efficiency of DiffHammer"}, {"figure_path": "ZJ2ONmSgCS/figures/figures_17_3.jpg", "caption": "Figure 4: Avg.Rob and Wor.Rob for the first 75 steps of different attacks with l\u221e: 8/255.", "description": "This figure shows the average robustness (Avg.Rob) and worst-case robustness (Wor.Rob) for three different diffusion-based purification methods (DiffPure, GDMP, LM) against various attacks (BPDA, DA/AA, PGD, DH) over 75 steps.  The l\u221e norm constraint is set to 8/255, representing a small perturbation. The plot helps to visualize how the robustness of each defense changes over the course of the attack and how DiffHammer compares to other attack methods.", "section": "4.2 Effectiveness and efficiency of DiffHammer"}, {"figure_path": "ZJ2ONmSgCS/figures/figures_17_4.jpg", "caption": "Figure 4: Avg.Rob and Wor.Rob for the first 75 steps of different attacks with l\u221e: 8/255.", "description": "The figure shows the average robustness (Avg.Rob) and worst-case robustness (Wor.Rob) for different attack methods (BPDA, DA, PGD, and DH) against three different diffusion-based purification defenses (DiffPure, GDMP, and LM) over the first 75 attack steps.  The l\u221e norm is set to 8/255.  It illustrates the effectiveness and efficiency of DiffHammer compared to other attack methods.", "section": "4.2 Effectiveness and efficiency of DiffHammer"}, {"figure_path": "ZJ2ONmSgCS/figures/figures_17_5.jpg", "caption": "Figure 4: Avg.Rob and Wor.Rob for the first 75 steps of different attacks with l\u221e: 8/255.", "description": "This figure shows the average robustness (Avg.Rob) and worst-case robustness (Wor.Rob) for three different diffusion-based purification methods (DiffPure, GDMP, and LM) against several attack algorithms (BPDA, DA, PGD, and DiffHammer) over the first 75 steps of the attack process.  The l\u221e norm is set to 8/255. The curves illustrate how the robustness of each defense method changes as the attack progresses.  It provides a visual comparison of the effectiveness and efficiency of different attack methods against these three purification techniques.", "section": "4.2 Effectiveness and efficiency of DiffHammer"}, {"figure_path": "ZJ2ONmSgCS/figures/figures_17_6.jpg", "caption": "Figure 4: Avg.Rob and Wor.Rob for the first 75 steps of different attacks with l\u221e: 8/255.", "description": "This figure shows the average robustness (Avg.Rob) and worst-case robustness (Wor.Rob) over the first 75 steps for different attack methods against three different diffusion-based purification methods (DiffPure, GDMP, and LM).  The x-axis represents the number of attack iterations, and the y-axis represents the robustness percentage.  The lines represent different attack algorithms (BPDA, AA, PGD, and DiffHammer).  The figure helps to illustrate the relative effectiveness and efficiency of each attack method against the different purification techniques.", "section": "4.2 Effectiveness and efficiency of DiffHammer"}, {"figure_path": "ZJ2ONmSgCS/figures/figures_17_7.jpg", "caption": "Figure 4: Avg.Rob and Wor.Rob for the first 75 steps of different attacks with l\u221e: 8/255.", "description": "This figure shows the average robustness (Avg.Rob) and worst-case robustness (Wor.Rob) for three different diffusion-based purification methods (DiffPure, GDMP, LM) against various attack algorithms (BPDA, DA, PGD, DH) over the first 75 attack steps.  The l\u221e norm is set to 8/255.  The graph illustrates how the robustness of each purification method decreases as the number of attack iterations increases, and how DiffHammer generally achieves lower robustness values compared to other attack methods.", "section": "4.2 Effectiveness and efficiency of DiffHammer"}, {"figure_path": "ZJ2ONmSgCS/figures/figures_17_8.jpg", "caption": "Figure 4: Avg.Rob and Wor.Rob for the first 75 steps of different attacks with l\u221e: 8/255.", "description": "This figure shows the average robustness (Avg.Rob) and worst-case robustness (Wor.Rob) for three different diffusion-based purification defenses (DiffPure, GDMP, LM) against various attack methods (BPDA, DA, PGD, DH) over the first 75 attack steps.  The l\u221e norm is set to 8/255.  The curves illustrate how the robustness of each defense changes as the attacks progress.", "section": "4.2 Effectiveness and efficiency of DiffHammer"}, {"figure_path": "ZJ2ONmSgCS/figures/figures_17_9.jpg", "caption": "Figure 4: Avg.Rob and Wor.Rob for the first 75 steps of different attacks with l\u221e: 8/255.", "description": "This figure shows the average robustness (Avg.Rob) and worst-case robustness (Wor.Rob) for different attacks (BPDA, PGD, AA, and DiffHammer) against three different diffusion-based purification defenses (DiffPure, GDMP, and LM) over the first 75 steps of the attack process. The l\u221e norm is set to 8/255.  The graph allows for comparison of the effectiveness and efficiency of the attacks against the different defenses, highlighting the performance of DiffHammer.", "section": "4.2 Effectiveness and efficiency of DiffHammer"}, {"figure_path": "ZJ2ONmSgCS/figures/figures_17_10.jpg", "caption": "Figure 4: Avg.Rob and Wor.Rob for the first 75 steps of different attacks with l\u221e: 8/255.", "description": "This figure presents the average robustness (Avg.Rob) and worst-case robustness (Wor.Rob) for three different diffusion-based purification methods (DiffPure, GDMP, LM) across various attack algorithms (BPDA, PGD, DA, DH) over 75 iterations.  The l\u221e norm is set to 8/255. It visually demonstrates the effectiveness and efficiency of the DiffHammer attack, especially in comparison to other methods. The curves illustrate how the robustness of each defense decreases with increasing attack iterations.", "section": "4.2 Effectiveness and efficiency of DiffHammer"}, {"figure_path": "ZJ2ONmSgCS/figures/figures_17_11.jpg", "caption": "Figure 4: Avg.Rob and Wor.Rob for the first 75 steps of different attacks with l\u221e: 8/255.", "description": "This figure compares the average robustness (Avg.Rob) and worst-case robustness (Wor.Rob) of three different diffusion-based purification methods (DiffPure, GDMP, and LM) against various attacks (BPDA, PGD, DA, and DiffHammer) over 75 iterations.  It shows how the robustness of each defense degrades as the number of attack iterations increases. The different line styles represent different attacks. The figure helps to visualize the effectiveness and efficiency of the proposed DiffHammer attack compared to existing attacks.", "section": "4.2 Effectiveness and efficiency of DiffHammer"}, {"figure_path": "ZJ2ONmSgCS/figures/figures_17_12.jpg", "caption": "Figure 4: Avg.Rob and Wor.Rob for the first 75 steps of different attacks with l\u221e: 8/255.", "description": "This figure shows the average robustness (Avg.Rob) and worst-case robustness (Wor.Rob) for three different diffusion-based purification methods (DiffPure, GDMP, LM) against various attack algorithms (BPDA, PGD, AA, DH) over the first 75 steps.  The l\u221e norm is set to 8/255.  The plot illustrates how the robustness of each purification method changes as the attack progresses.  It helps to visualize the relative effectiveness and efficiency of each attack method against different purification techniques.", "section": "4.2 Effectiveness and efficiency of DiffHammer"}, {"figure_path": "ZJ2ONmSgCS/figures/figures_17_13.jpg", "caption": "Figure 4: Avg.Rob and Wor.Rob for the first 75 steps of different attacks with l\u221e: 8/255.", "description": "This figure shows the average robustness (Avg.Rob) and worst-case robustness (Wor.Rob) for three different diffusion-based purification methods (DiffPure, GDMP, and LM) against various attack methods (BPDA, PGD, DA, and DiffHammer) over the first 75 steps of the attacks.  The l\u221e norm constraint is set to 8/255.  The graph helps to visualize the effectiveness and efficiency of different attacks against these defense mechanisms, showing how robustness changes over time.", "section": "4.2 Effectiveness and efficiency of DiffHammer"}, {"figure_path": "ZJ2ONmSgCS/figures/figures_17_14.jpg", "caption": "Figure 4: Avg.Rob and Wor.Rob for the first 75 steps of different attacks with l\u221e: 8/255.", "description": "This figure shows the average robustness (Avg.Rob) and worst-case robustness (Wor.Rob) for different attack methods (BPDA, AA, PGD, DH) against three different diffusion-based purification defenses (DiffPure, GDMP, LM) over the first 75 steps. The l\u221e norm is set to 8/255.  It helps visualize the effectiveness and efficiency of different attack methods in reducing the robustness of diffusion-based purification.", "section": "4.2 Effectiveness and efficiency of DiffHammer"}, {"figure_path": "ZJ2ONmSgCS/figures/figures_17_15.jpg", "caption": "Figure 4: Avg.Rob and Wor.Rob for the first 75 steps of different attacks with l\u221e: 8/255.", "description": "This figure shows the average robustness (Avg.Rob) and worst-case robustness (Wor.Rob) for different attack methods against diffusion-based purification.  The x-axis represents the number of iterations, and the y-axis represents the robustness percentage.  The lines show that DiffHammer achieves a higher attack success rate (lower robustness) faster than other methods.", "section": "4.2 Effectiveness and efficiency of DiffHammer"}, {"figure_path": "ZJ2ONmSgCS/figures/figures_18_1.jpg", "caption": "Figure 11: A toy model of Gaussian mixing. Diffusion-based purification can be attacked toward unclustered features, leading to inconsistent gradients.", "description": "This figure illustrates a toy example to demonstrate the gradient dilemma.  Panel (a) shows a simplified two-dimensional data distribution with four clusters representing different classes.  The purification process is represented by a vector field that pulls samples slightly towards the center before diffusing them back to the data distribution. The samples are colored in panel (b) based on the proportion they belong to the set S1 (those with shared vulnerabilities). The colorbar indicates a higher proportion of samples belonging to S1, showing where the attack is most effective and where there is a gradient dilemma.", "section": "3.1.3 Discussion"}, {"figure_path": "ZJ2ONmSgCS/figures/figures_19_1.jpg", "caption": "Figure 12: Time spent on gradient grafting and full gradient under different N.", "description": "This figure shows the time (in seconds) spent per step for different methods (Ours, N-Grad, DiffPure, GDMP, LM) while varying the number of samples (N) used in the evaluation.  The \"Ours\" method represents the proposed DiffHammer approach, which employs gradient grafting for efficiency.  \"N-Grad\" refers to the baseline method that uses full gradients for all N samples.  The figure illustrates that the proposed gradient grafting significantly reduces computational time, especially as N increases, showing a clear advantage of the DiffHammer approach over the baseline.", "section": "3.1.2 Gradient grafting"}, {"figure_path": "ZJ2ONmSgCS/figures/figures_19_2.jpg", "caption": "Figure 3: Distribution of attack results (l\u221e : 8/255) for 1-evaluation (inner ring) and 10-evaluation (outer ring). 32.6%-46.3% of the samples have unshared vulnerabilities, imposing underestimated resubmit risk in 1-evaluation.", "description": "This figure shows the distribution of attack success rates for different diffusion-based purification methods under two evaluation scenarios: 1-evaluation and 10-evaluation.  The inner ring represents the results of a single attack attempt (1-evaluation), while the outer ring shows the results after 10 consecutive attempts (10-evaluation).  A significant finding is that a substantial portion (32.6% - 46.3%) of the samples exhibit vulnerabilities that are not shared across multiple purification attempts (unshared vulnerabilities). This highlights a critical limitation of 1-evaluation, which underestimates the risk of an attacker repeatedly submitting adversarial samples to exploit these unshared vulnerabilities.", "section": "3.2 In-loop N-evaluation"}, {"figure_path": "ZJ2ONmSgCS/figures/figures_20_1.jpg", "caption": "Figure 14: Example of visualization of adversarial samples on CIFAR10 (l\u221e : 4/255). Original labels are shown in green and adversarial labels are shown in red.", "description": "This figure shows examples of adversarial samples generated by DiffHammer against DiffPure on the CIFAR10 dataset using an l\u221e norm with a perturbation budget of 4/255.  Each image shows an original correctly classified sample and the corresponding adversarially perturbed sample that is misclassified. The original label is shown in green and the adversarial (incorrect) label is shown in red.  This illustrates the effectiveness of DiffHammer in generating nearly imperceptible perturbations that cause misclassification.", "section": "C.9 Visualization"}, {"figure_path": "ZJ2ONmSgCS/figures/figures_20_2.jpg", "caption": "Figure 14: Example of visualization of adversarial samples on CIFAR10 (l\u221e: 4/255). Original labels are shown in green and adversarial labels are shown in red.", "description": "This figure shows examples of adversarial samples generated by DiffHammer on the CIFAR-10 dataset with an l\u221e perturbation of 4/255.  Each image pair shows the original image with its correct label in green and the adversarially perturbed image with its misclassified label in red, demonstrating the effectiveness of DiffHammer in generating imperceptible yet successful adversarial examples.", "section": "C.9 Visualization"}, {"figure_path": "ZJ2ONmSgCS/figures/figures_21_1.jpg", "caption": "Figure 14: Example of visualization of adversarial samples on CIFAR10 (l\u221e: 4/255). Original labels are shown in green and adversarial labels are shown in red.", "description": "This figure shows examples of adversarial samples generated by DiffHammer on CIFAR10 with an adversarial perturbation of 4/255. Each image pair shows an original image and the corresponding adversarially perturbed image. The original label is shown in green, while the adversarial label is shown in red.  This visualization demonstrates the effectiveness of DiffHammer in generating imperceptible perturbations that cause misclassification.", "section": "C.9 Visualization"}, {"figure_path": "ZJ2ONmSgCS/figures/figures_21_2.jpg", "caption": "Figure 5: Clustering effects and forgetting phenomenon in gradient dilemma (l\u221e: 8/255).", "description": "This figure shows the results of clustering the gradients into two sets (S0 and S1) using cosine similarity. The left panel shows the distribution of silhouette coefficients, indicating a significant difference between the gradients in the two sets. The right panel shows the attack success rate (ASR) in the previous iteration (t-1), demonstrating that the gradients in the two sets differ significantly. The gradient dilemma leads to \"attack forgetting,\" where the effects of previous attacks are forgotten due to inconsistent gradients. DiffHammer avoids this by targeting shared vulnerabilities.", "section": "4.3 Gradient dilemma and transfer-based attack"}, {"figure_path": "ZJ2ONmSgCS/figures/figures_21_3.jpg", "caption": "Figure 10: Avg.Rob and Wor.Rob for the first 75 steps in ImageNette with l\u221e : 4/255.", "description": "This figure shows the average robustness (Avg.Rob) and worst-case robustness (Wor.Rob) for different attack methods against three different diffusion-based purification models (DiffPure, GDMP, LM) on ImageNette dataset, using an l\u221e norm constraint of 4/255.  The x-axis represents the number of attack steps (iterations), and the y-axis shows the robustness percentage.  It illustrates how the robustness of each model decreases over the course of multiple attack iterations for the different attack methods (BPDA, DA/AA, PGD, DH).", "section": "4.2 Effectiveness and efficiency of DiffHammer"}]