[{"type": "text", "text": "Referring Human Pose and Mask Estimation in the Wild ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Bo Miao1 Mingtao Feng2 Zijie $\\mathbf{W}\\mathbf{u}^{3}$ Mohammed Bennamoun1 Yongsheng Gao4 Ajmal Mian1 ", "page_idx": 0}, {"type": "text", "text": "1University of Western Australia 2Xidian University 3Hunan University 4Griffith University https://github.com/bo-miao/RefHuman ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We introduce Referring Human Pose and Mask Estimation (R-HPM) in the wild, where either a text or positional prompt specifies the person of interest in an image. This new task holds significant potential for human-centric applications such as assistive robotics and sports analysis. In contrast to previous works, R-HPM (i) ensures high-quality, identity-aware results corresponding to the referred person, and (ii) simultaneously predicts human pose and mask for a comprehensive representation. To achieve this, we introduce a large-scale dataset named RefHuman, which substantially extends the MS COCO dataset with additional text and positional prompt annotations. RefHuman includes over 50,000 annotated instances in the wild, each equipped with keypoint, mask, and prompt annotations. To enable prompt-conditioned estimation, we propose the first end-to-end promptable approach named UniPHD for R-HPM. UniPHD extracts multimodal representations and employs a proposed pose-centric hierarchical decoder to process (text or positional) instance queries and keypoint queries, producing results specific to the referred person. Extensive experiments demonstrate that UniPHD produces quality results based on user-friendly prompts and achieves top-tier performance on RefHuman val and MS COCO val2017. ", "page_idx": 0}, {"type": "image", "img_path": "fXEi3LVflp/tmp/ac3d39dbabc510ddb85903dc9ddc1b2755bd9deecde877df1f1ba0268ffcb02a.jpg", "img_caption": ["(a) Multi-person Pose Estimation ", "(b) Referring Human Pose and Mask Estimation "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "Figure 1: Task illustration of (a) multi-person pose estimation predicts numerous outcomes and requires selection strategies during deployment, potentially leading to false negatives or suboptimal target results, and (b) our referring human pose and mask estimation requires a unified promptable model to simultaneously predict accurate pose and mask for the person of interest, providing comprehensive and identity-aware human representations to benefit human-AI interaction. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Human pose estimation in the wild is a fundamental yet challenging problem in the vision community, fueling advancements in various applications like human-AI interaction, activity analysis, video surveillance, assistive robotics, and sports analysis. This task aims to locate keypoints (joint locations) of humans within images in unconstrained environments. ", "page_idx": 0}, {"type": "text", "text": "Previous multi-person pose estimation techniques typically follow a two-stage paradigm, separating the problem into person detection and local keypoint regression. These techniques can be summarized as top-down and bottom-up approaches. Top-down approaches [9, 15, 63, 79, 84] use a detection model to identify human bounding boxes and a separate pose estimation model to predict keypoints on the cropped single-human image. The independent models in these methods lead to a non-endto-end pipeline with substantial computational costs. Bottom-up approaches [3, 10, 31, 62] usually predict keypoint heatmaps to obtain instance-agnostic keypoints and assign them to individuals using heuristic grouping algorithms. The intricate grouping algorithms in these approaches introduce hand-designed parameters and face challenges in handling complex scenarios such as occlusions. ", "page_idx": 1}, {"type": "text", "text": "With advancements in attention mechanisms [2, 56, 57, 59, 77] and transformer architectures [4, 95], many recent approaches regard human pose estimation as a direct set prediction problem and design end-to-end differentiable transformers, leading to notable improvements. They employ bipartite matching to establish one-to-one instance correspondence for the set of predictions, avoiding the need for post-processing in the training stage. Among them, PETR [70] proposes a transformer that progressively predicts keypoint positions through pose and keypoint decoders. QueryPose [85], ED-Pose [87], and GroupPose [44] further incorporate a human detection stage for instance feature extraction or query initialization to improve performance and expedite model convergence. They all use keypoint-level queries to capture local details for accurate pose estimation. ", "page_idx": 1}, {"type": "text", "text": "Despite demonstrating favorable performance, these methods still require designed strategies to select the best match for a target individual during deployment, which can result in suboptimal outcomes or false negatives, and lack exploration of human-AI interaction to directly predict expected results based on natural prompts. Additionally, they overlook joint human pose and mask estimation, which provides comprehensive human representations to facilitate applications like assistive robotics and sports analysis. For example, accurate joint human pose and mask estimation in unconstrained environments enables robots to locate, analyze, and interact with target individuals, enhancing user experience and assistive tasks. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we propose the new task of Referring Human Pose and Mask Estimation (R-HPM) in the wild. As illustrated in Figure 1, unlike multi-person pose estimation [3, 15, 24, 66], R-HPM is a multimodal reasoning task that requires a unified model to predict both pose and mask of a referred individual using user-friendly text or positional prompts, enabling comprehensive and identityaware human representations without post-processing. To achieve this, we introduce RefHuman, a large-scale dataset that substantially extends MS COCO [40] with additional text, scribble, and point prompt annotations. Our dataset accommodates diverse task settings to enhance human-AI interaction. Manually annotating text descriptions and scribbles is expensive. To reduce annotation costs, we design a human-in-the-loop text generation strategy using powerful large language models and employ bezier curves to automate scribble generation. ", "page_idx": 1}, {"type": "text", "text": "To benchmark R-HPM, we propose an end-to-end promptable approach called UniPHD. Our approach directly adopts prompts as instance queries and coordinates them with keypoint queries to jointly predict identity-aware keypoint positions and masks for the target. Unlike previous works [44, 70, 85, 87], UniPHD introduces a pose-centric hierarchical decoder (PHD) that employs deformable and graph attention to effectively model local details and global dependencies, ensuring target-awareness and instance coherence. Furthermore, our approach follows a general paradigm that enables seamless integration with the decoders from recent transformer-based methods such as GroupPose [44] and ED-Pose [87] for R-HPM. We conduct extensive experiments on the RefHuman dataset. Integrating our approach with GroupPose and ED-Pose yields promising results in an end-to-end fashion. Moreover, our UniPHD approach achieves top-tier performance on both RefHuman val and MS COCO val2017, demonstrating the effectiveness of our approach and the significance of our task. ", "page_idx": 1}, {"type": "text", "text": "Our main contributions are summarized below: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose Referring Human Pose and Mask Estimation (R-HPM) in the wild, a new task that simultaneously predicts the pose and mask of a specified individual using natural, user-friendly text or positional prompts. This task enhances models with identity-awareness and produces comprehensive human representations to benefit human-AI interaction. \u2022 We introduce RefHuman, a large-scale dataset that substantially extends COCO for R-HPM. RefHuman contains pose and mask annotations for individuals in diverse, unconstrained environments and is enriched with corresponding text and positional prompts. ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose an end-to-end promptable UniPHD approach for R-HPM. Our approach performs pose-centric hierarchical decoding, achieving top-ranked performance and establishing a solid benchmark for future advancements in this field. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 2D Human Pose Estimation in the Wild ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Human pose estimation aims to localize keypoints of individuals in unconstrained environments. For a long period, two-stage approaches have dominated this field, generally divided into top-down and bottom-up methods. Top-down methods [9, 15, 34, 63, 79, 84] first detect and crop each person using an object detector, then perform pose estimation on these cropped instance-level images using a separate model. Although effective, the redundancy from the extra detection step, region of interest operations, and separate training makes them suboptimal. Bottom-Up methods [3, 10, 31, 62] first detect abundant instance-agnostic keypoints and then group them into individual poses. While generally efficient, their intricate grouping algorithms pose challenges in handling complex scenarios, resulting in inferior performance. Furthermore, these two-stage approaches suffer from non-differentiable, hand-crafted post-processing steps that challenge optimization. Inspired by the one-stage object detectors [21, 76], pixel-wise regression methods [49, 53, 55, 64, 73, 75, 78, 81, 93] densely predict pose candidates in an end-to-end fashion and apply Non-maximum Suppression (NMS) to obtain poses for different individuals. However, these methods produce redundant results, challenging the removal of duplicates. ", "page_idx": 2}, {"type": "text", "text": "Recent human pose estimation methods [51, 52, 72] explore transformer-based architectures [2, 4, 77, 95] due to their sparse, end-to-end design and promising performance. These methods treat human pose estimation as a direct set prediction problem and use bipartite matching to establish one-to-one instance correspondence during training. Among these, [70] proposes a transformer with a pose decoder to predict keypoints and a joint decoder to refine them. [85] performs estimation on extracted object features to reduce noisy context. [85, 87] incorporate a human detection stage for query initialization to enhance performance. These methods achieve favorable performance but require complex strategies to identify the best match for a specified person during deployment due to the lack of exploration of prompt reasoning. In this work, we propose a multimodal reasoning task to directly and jointly predict the identity-aware pose and mask for a referred person, resulting in comprehensive human body representations and facilitating applications in human-AI interaction. ", "page_idx": 2}, {"type": "text", "text": "2.2 Referring Image Segmentation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Referring image segmentation (RIS) aims to segment target objects in images based on natural linguistic descriptions [18, 30, 91]. It related tasks include interactive image segmentation [7, 8, 25, 37, 41, 86], which segments targets based on user clicks. Early RIS methods [6, 17, 18, 35, 43, 54, 71, 90] employ convolution and recurrent neural networks for multimodal encoding and segmentation. Their intrinsic constraints in capturing long-range dependencies and handling free-form features often lead to suboptimal performance. To improve multimodal representation and alignment, attentionbased bidirectional [19, 20] and progressive [16, 22, 23, 88, 89] cross-modal interaction modules are proposed. Additionally, [80, 92] leverage the strong cross-modal alignment capabilities of pre-trained large language models [67, 69]. ", "page_idx": 2}, {"type": "text", "text": "With advancements in transformer architectures [4, 61, 77, 95], [11, 13, 29, 38, 42, 45, 58, 60, 83, 94] design end-to-end transformers for language-conditioned segmentation and achieve favorable performance. In this work, we investigate a unified promptable transformer that effectively processes both text and positional prompts, thereby broadening its generalizability and application scope. Moreover, our model coordinates prompts with keypoint queries, enabling the joint prediction of keypoint positions and segmentation masks for specified individuals. ", "page_idx": 2}, {"type": "text", "text": "3 RefHuman Dataset ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We substantially extend COCO [40] to construct the RefHuman dataset. It contains pose and mask annotations for humans along with text and positional prompts to facilitate the new task of R-HPM. ", "page_idx": 2}, {"type": "image", "img_path": "fXEi3LVflp/tmp/53c340831d0d0ebd0d53e074ad2b062dc9a6305b13b94f17daefce7c6183ff46.jpg", "img_caption": ["Figure 2: Human-in-the-loop text prompt generation. We use GPT to generate descriptions with complementary local details and global context, then manually review/correct the descriptions. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "3.1 Data Annotation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Pioneer works on 2D human pose estimation datasets in RGB images include [5, 12, 14, 26, 28]. Recent efforts focus on human pose estimation \u2018in the wild\u2019, establishing datasets such as MPII [1], MS COCO [40], CrowdPose [33], COCO-WholeBody [27], and AI Challenger [82]. Despite their prevalence, these datasets lack crucial text and scribble prompt annotations needed for human-AI interaction. The large-scale RefHuman dataset extends the widely-used MS COCO with additional text and positional (scribbles and points) prompt annotations, facilitating promptable models to enhance human-AI interaction. ", "page_idx": 3}, {"type": "text", "text": "Text Prompt Annotation. Manually annotating text descriptions is costly. To mitigate this, we design a human-in-the-loop text generation strategy using the large language model, ChatGPT [65]. As shown in Figure 2 (a), we first provide the entire image with a bounding box indication to GPT-4 to generate a text description [Opt1] with global context. However, large language models often misidentify targets in complex scenes. To handle this, we crop the image to focus on the target person and let GPT-4 generate a description [Opt2] with correct local details. Given that individuals may have similar appearances, descriptions generated based on instance-level images are often not distinguishable. Therefore, we combine the global and local descriptions, integrating them into a comprehensive description [Opt3] of the target person. Finally, we manually select and revise these automatically generated descriptions to create accurate text prompts. ", "page_idx": 3}, {"type": "text", "text": "Our strategy successfully generates acceptable descriptions for images with simple scenarios. These correspond to over $35\\%$ of the cases. In the remaining cases, unsatisfactory descriptions are generated. Common issues include misidentification of individuals, incorrect orientation (e.g., facing left or right), and indistinguishable context. Nevertheless, these descriptions often provide valuable context, expediting the annotation process. To further scale up the RefHuman training set, we integrate text annotations from $\\mathrm{RefCOCO}/+/\\mathrm{g}$ [30, 50], doubling the number of referred instances. ", "page_idx": 3}, {"type": "text", "text": "Positional Prompt Annotation. B\u00e9zier curves are parametric curves based on Bernstein Polynomials and widely used in computer graphics. In this work, we employ cubic B\u00e9zier curves to simulate scribbles and generate clicks. Given four control points $\\bf{p_{0}},\\,\\bf{p_{1}},\\,\\bf{p_{2}}$ , and $\\bf{p_{3}}$ , the cubic B\u00e9zier curve starts at $\\mathbf{p_{0}}$ moving toward $\\mathbf{p_{1}}$ and reaches $\\mathbf{p_{3}}$ from the direction of $\\mathbf{p_{2}}$ . The general equation for a B\u00e9zier curve $\\mathbf{c}(t,n)$ of degree $n$ is: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{c}(t,n)=\\sum_{i=0}^{n}b_{i,n}(t)\\mathbf{p}_{i},\\quad0\\leq t\\leq1\n$$", "text_format": "latex", "page_idx": 3}, {"type": "equation", "text": "$$\nb_{i,n}(t)={\\binom{n}{i}}(1-t)^{n-i}t^{i},\\quad i=0,...,n\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $b_{i,n}(t)$ represents the Bernstein basis polynomials and $\\binom{n}{i}$ is the binomial coefficient. To generate scribble prompts, we randomly sample four control points within the foreground mask to fit a cubic B\u00e9zier curve, represented as an ordered set of points $\\left\\{(x_{1},y_{1}),(x_{2},y_{2}),...,(x_{n},y_{n})\\right\\}$ . The curve is then discretized by uniformly sampling twelve points to form a scribble prompt $\\mathbf{s}\\!\\!=\\!\\!\\{(x_{\\left\\lfloor\\frac{k n}{12}\\right\\rfloor},y_{\\left\\lfloor\\frac{k n}{12}\\right\\rfloor})\\,\\mid\\,k\\,=\\,1,2,\\bar{\\,}...\\,,12\\}$ . For point prompts, we randomly select a single point $\\scriptstyle\\mathbf{p}=\\left(x,\\,y\\right)$ from s, where $x$ and $y$ are the horizontal and vertical coordinates. ", "page_idx": 3}, {"type": "text", "text": "Table 1: Data statistics of human-related images in RefCOCO [30], RefCOCO $^+$ [30], RefCOCOg [50], their combined dataset $\\mathrm{RefCOCO}/+/\\mathrm{g}$ , and our RefHuman. ", "page_idx": 4}, {"type": "table", "img_path": "fXEi3LVflp/tmp/526d6b427e91190dcdc2b689cb7222ea27fa6e3db4cea1bfaae3cd58f196cda5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "3.2 Data Statistics ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "As shown in Table 1, RefHuman includes 50,210 human instances across 21,634 images, larger than RefCOCO [30], $\\mathbf{RefCOCO+}$ [30], and $\\mathbf{RefCOCOg}$ [50], and with additional positional prompts. To construct RefHuman train set, we annotate prompts for all humans in MS COCO train2017 set with at least three surrounding people, a minimum of eight visible keypoints, and an area ratio of at least $2\\%$ . For the RefHuman val set, we annotate humans in MS COCO val2017 set, excluding those with non-visible keypoints or an area ratio below $1\\%$ , as instances below this threshold are often not visually clear and difficult to describe accurately. Note that each instance may have multiple text, scribble, and point annotations, with each image-prompt pair treated as a separate sample. ", "page_idx": 4}, {"type": "text", "text": "4 Method ", "text_level": 1, "page_idx": 4}, {"type": "image", "img_path": "fXEi3LVflp/tmp/00ed7fcdc3e220fccd34bdb086933469ce19839bac34cf7731ce334406eaeeaf.jpg", "img_caption": ["Figure 3: Detailed architecture of our UniPHD, which contains a multimodal encoder that imbues visual features with prompt awareness and a pose-centric hierarchical decoder that enables promptconditioned queries to effectively capture local details and global dependencies within targets. Our unified model is end-to-end and accepts text descriptions, scribbles, or points as prompts to predict the keypoint positions and segmentation mask of the target person. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Given an image I and a text prompt $\\mathbf{T}$ or positional prompt $\\mathbf{P}$ , our task aims to simultaneously predict the keypoint positions $\\mathbf{K}\\in\\bar{\\mathbb{R}}^{N\\times\\bar{2}}$ and binary segmentation mask $\\mathbf{M}\\in\\mathbb{R}^{H\\times W}$ of the referred person, where $N$ is the number of keypoints, and $H$ and $W$ are the spatial dimensions. To this end, we propose a fully end-to-end promptable UniPHD approach, as illustrated in Figure 3. UniPHD consists of four key components: Backbone, Multimodal Encoder, Pose-centric Hierarchical Decoder, and Task-specific Prediction Heads. A small set of prompt-conditioned queries is employed to identify the referred person and estimate results. During inference, we directly output expected predictions from the highest-scoring query group without any post-processing. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "4.1 Backbone ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Visual Encoder. We start by using a visual encoder to extract multi-scale visual features and apply a point-wise convolution to reduce the channel dimension of features to $D=\\!256$ for efficient multimodal interactions. Feature maps with downsampling rates of {8, 16, 32, 64} are then flattened and concatenated into tokenized visual representations ${\\bf{F}}^{v}$ . We adopt Swin Transformer [47] as the visual encoder in this work. ", "page_idx": 5}, {"type": "text", "text": "Prompt Encoder. For a linguistic description with $L$ words as a prompt, we use RoBERTa [46] to extract word-level features $\\mathbf{F}^{w}\\,\\in\\,\\mathbb{R}^{L^{\\times}D}$ and generate sentence-level feature as the instance query $\\mathbf{Q}^{I}\\in\\mathbb{R}^{1\\times D}$ by pooling ${\\bf{F}}^{w}$ . For a discretized scribble or a single point prompt, we retrieve embeddings directly from visual features at stride 16 based on their positions to obtain pixel-level prompt features ${\\bf F}^{w}$ , which are then pooled to form the instance query ${\\bf{Q}}^{I}$ . ", "page_idx": 5}, {"type": "text", "text": "4.2 Multimodal Encoder ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To generate target-related multimodal representations ${\\bf F}^{v l}$ , visual tokens ${\\bf{F}}^{v}$ are first enhanced through a modality-specific cross-attention, which integrate information from prompt features: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{F}^{v l}=\\mathbf{F}^{v}+{\\mathsf{A t t n}}_{i}(Q=\\mathbf{F}^{v},K=\\mathbf{F}^{w},V=\\mathbf{F}^{w})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $i=0$ for text prompt and $i=1$ for positional prompts, i.e., we adopt separate parameters for cross-modal fusion of different prompt types. After that, we follow previous works [44, 58, 87] to use a memory-efficient deformable transformer encoder to encode the multimodal features. The output of the transformer encoder ${\\bf F}^{v l}$ is then forwarded to the decoder to update queries. ", "page_idx": 5}, {"type": "text", "text": "4.3 Pose-centric Hierarchical Decoder ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We employ $n$ groups of queries for the referred human and generate $n$ groups of results. Each group consists of a prompt-conditioned instance query and $k$ learnable keypoint queries. The keypoint queries regard each keypoint as a target and aim to regress their respective positions, while each instance query, conditioned on the prompt, predicts a confidence score, dynamic segmentation fliters, and a bounding box to locate the referred human. The scores are supervised by the losses of each query group to indicate result quality. During deployment, we directly generate results for the target using the highest-scoring query group without any post-processing. ", "page_idx": 5}, {"type": "text", "text": "Prompt-conditioned Query Initialization. We first construct a query group template $\\mathbf{Q}_{I n i t}^{I P}\\in$ $\\mathbb{R}^{(k+1)\\times D}$ by concatenating the text/positional prompt embedding ${\\bf{Q}}^{I}$ with $k$ (e.g., 17) randomly initialized learnable embeddings ${\\bf{Q}}^{P}$ . To generate $n$ query groups, we apply linear layers to the prompt-aware multimodal features ${\\bf F}^{v l}$ to identify the top- ${\\cdot n}$ highest-scoring positions $\\mathbf{p}$ and estimate their corresponding keypoint positions and centers as reference points, denoted as $\\mathbf{c}\\in\\mathbb{R}^{n\\times(k+1)\\times2}$ . The template $\\mathbf{Q}_{I n i t}^{I P^{-}}$ is then repeated $n$ times and enhanced with reference points c, their associated positional embeddings, and the top- $n$ highest-scoring multimodal embeddings $\\mathbf{F}_{\\mathbf{p}}^{v l}$ , to form ${\\bf Q}^{I P}$ . Finally, the instance query in each copy incorporates the prompt embedding and a positional embedding derived from the keypoints center of each candidate for pose-centric decoding. During decoding process, we employ graph-attention to model global dependencies for each query group, enabling all queries to utilize the prompt as guidance to identify the referred person only. ", "page_idx": 5}, {"type": "text", "text": "Hierarchical Local Context and Global Dependency Modeling. We introduce a pose-centric hierarchical decoder to effectively model complementary local details and global dependencies for target-aware decoding. As illustrated in Figure 3, local details are first aggregated using the efficient deformable attention [95], similar to [44, 58, 87]. Each instance and keypoint query in $\\mathbf{\\breve{Q}}^{I P}$ independently predicts its sampling offsets and corresponding attention weights on the multimodal features ${\\bf F}^{v l}$ . We then aggregate the sampled features accordingly for each query to capture local details. However, after local detail aggregation, the keypoint queries struggle to perceive the prompt information and lacks interactions with each other, challenging the target-awareness and instance coherence. Additionally, the instance query lacks sufficient relevant global context to accurately locate the referred person at pixel-level. ", "page_idx": 5}, {"type": "text", "text": "To address these issues, we model each query group as a bipartite graph $\\mathbf{G}=\\{\\mathbf{V},\\mathbf{E},\\mathbf{A}\\}$ , with nodes $\\mathbf{V}$ representing different queries and edges $\\mathbf{E}$ denoting relations between nodes, constrained by a learnable soft adjacent matrix $\\mathbf{A}\\in\\mathbb{R}^{(k+1)\\times(k+1)}$ , which simulates inherent keypoint-to-keypoint, keypoint-to-instance, and instance-to-keypoint relations. The edge $\\mathbf{E}_{i j}$ from the $i$ -th node to the $j$ -th node can be formulated as: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbf{E}_{i j}=(W^{q}\\mathbf{V}_{i})(W^{k}\\mathbf{V}_{j})^{\\top}+\\mathbf{A}_{i j}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $W^{q}$ and $\\boldsymbol{W}^{k}$ are learnable projection matrices. Through our graph attention, both instance and keypoint queries capture global dependencies of the target and thus ensure instance coherence. Ultimately, the instance query generates dynamic filters for mask prediction, while the keypoint queries regress target keypoint positions. ", "page_idx": 6}, {"type": "text", "text": "4.4 Task-specific Prediction Heads ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "As illustrate in Figure 3, four lightweight heads are built on top of the decoded queries to predict bounding boxes, class scores, masks, and keypoint positions for the target. The box head predicts the bounding box location of the target to aid the learning process. The class head outputs confidence scores, supervised by losses from each query group, to indicate the prediction quality of each query group. The mask head produces dynamic filters for conditional segmentation on the multimodal features ${\\bf F}^{v l}$ . The keypoint head predicts keypoint positions and their visibility for the referred person in images. Detailed training loss functions for these outputs are supplied in the Appendix. ", "page_idx": 6}, {"type": "text", "text": "Conditional Segmentation. After extracting semantically-rich multimodal features ${\\bf F}^{v l}$ , we address pixel-level target localization using dynamic fliters generated by instance queries, which capture both local details and global dependencies. Similar to [58], we standardize the resolution of the multi-scale multimodal features ${\\bf F}^{v l}$ to $H/8\\times W/8$ and combine them into a single feature map $\\mathbf{F}^{m}$ through efficient element-wise addition. We then reshape the prompt-conditioned dynamic fliters to form two point-wise convolutions that apply to $\\mathbf{F}^{m}$ to obtain the segmentation mask for the referred person. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Metrics. We use standard metrics to evaluate our task. For pose estimation, we adopt OKS-based average precision (AP) and PCKh at a 0.5 threshold $(\\mathrm{PCKh}@0.5)$ [1]. For segmentation, we report mask AP and overall Intersection-over-Union (IoU). The AP metrics are evaluated across all query groups, consistent with prior works [44, 87], while PCKh $@0.5$ and oIoU are measured using only the highest-scoring query group to better reflect real-world deployment. ", "page_idx": 6}, {"type": "text", "text": "Implementation Details. We train our model on the RefHuman train, which contains approximately 46K person instances with 17 keypoints per instance, and evaluate on the RefHuman val. We also evaluate our model on MS COCO by generating positional prompts for all images in the dataset. Due to the page limit, we leave the further implementation details in the Appendix. ", "page_idx": 6}, {"type": "text", "text": "5.1 Main Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In Table 2, we evaluate our models and the recent advances [44, 87] on the RefHuman val set. For fairness, all models are evaluated with inputs resized to a maximum of 640 pixels on the longer side. ", "page_idx": 6}, {"type": "text", "text": "Effectiveness of Our Promptable Paradigm. Recent human pose estimation methods like GroupPose [87] and ED-Pose [44] predict poses for multiple humans but require hand engineered selection strategies to identify the best match for a specified person, which can result in suboptimal outcomes or false negatives. By integrating their decoders into our end-to-end promptable paradigm, we directly generate poses and masks in one go for the referred person, achieving up to 73.0 pose AP, 89.6 $\\operatorname{PCKh}(\\varpi0.5\\$ , and 85.6 oIoU with scribble prompts. This performance rivals that of previous models which use $3\\times$ more training data and hand engineered result selection strategies. Intersection-based result selection is chosen in Table 2 because distance- and IoU-based strategies lead to inferior performance, as discussed in the ablation study. Furthermore, our paradigm achieves advanced performance even with a single point prompt, while previous models struggle with such simple guidance, as the random positive points/clicks can be near poses of unintended humans in crowds. These results demonstrate the effectiveness of our end-to-end promptable paradigm and the significance of our proposed R-HPM task, i.e., accurate joint pose and mask estimation based on user-friendly prompts. ", "page_idx": 6}, {"type": "table", "img_path": "fXEi3LVflp/tmp/53de1275a183370fbf0ba66ef96a9fc9d732b1fa8da5e9e69a13a0d2f17fcc58.jpg", "table_caption": ["Table 2: Results on RefHuman val split. Uni-ED-Pose and Uni-GroupPose integrate ED-Pose [87] and GroupPose [44] into our end-to-end paradigm. Intersection-based result selection: selects results covering at least $30\\%$ of the ground truth box. \u2020: trains models using complete images in COCO train2017. FPS is measured on RTX 3090 with a batch size of 24. Uni-ED-Pose and Uni-GroupPose are trained with less data but rival the performance of vanilla models, which perform post-processing with ground truth boxes. Our UniPHD approach achieves top-tier performance. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "fXEi3LVflp/tmp/ed1ad77c5a0dbed90c440bbb69c6b209a7b16ca3e05d20a1d03d8f1704c74d93.jpg", "table_caption": ["Table 3: Comparison with state-of-the-art methods on MS COCO val2017. Our method achieves leading performance in pose estimation while also offering segmentation capabilities. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Effectiveness of Our UniPHD Approach. In Table 2, our UniPHD approach sets a new benchmark in overall performance for R-HPM through pose-centric hierarchical decoding, outperforming the nearest competitor, Uni-GroupPose w/ Scribble, by 1.7 Pose AP and 1.0 Mask AP, with comparable FPS. Intuitively, the interdependent influence of human keypoints makes graph networks suitable for modeling their dependencies. After applying deformable attention to capture local details, we employ graph attention with a learnable soft adjacent matrix to simulate keypoint-to-keypoint, keypointto-instance, and instance-to-keypoint relations. This matrix guides edge construction to effectively model global dependencies and ensure instance-awareness and coherence. ", "page_idx": 7}, {"type": "text", "text": "Results for Different Prompts. Table 2 shows that scribble prompts outperform point and text prompts in R-HPM. Scribble prompts offer more explicit positional guidance, enhancing instance query robustness. Point prompts, while slightly less accurate, offer greater user convenience in human-AI interaction due to their single-click simplicity. Text prompts face challenges in multimodal alignment, especially for crowded scenarios, but provide linguistic flexibility and enable non-physical interactions. Overall, our unified model effectively handles various prompts, showing great potential to facilitate human-AI interaction. ", "page_idx": 7}, {"type": "text", "text": "Evaluation on MS COCO val2017. We further evaluate UniPHD on MS COCO by aggregating results from all instances in each image. As shown in Table 3, all models are evaluated using inputs resized to a maximum of 640 pixels on the longest side for comparison. Trained solely with positional prompts, UniPHD achieves state-of-the-art performance on COCO val2017, even when compared to competitors using higher-resolution inputs, further demonstrating the efficacy of our approach and the significance of the proposed task. ", "page_idx": 7}, {"type": "image", "img_path": "fXEi3LVflp/tmp/96572b23e3a8642c36a420198ebfddf702348fbafc7f40078c236b14ae4a8538.jpg", "img_caption": ["Figure 4: Qualitative results of our UniPHD with text prompts in various challenging scenarios. "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "fXEi3LVflp/tmp/4a08a8b5d3cb6ba4e6b7196625229b92d147ff87033b88c37fddfeca49e4a54d.jpg", "table_caption": ["Table 4: Comparison with state-of-the-art textbased segmentation methods on RefHuman. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "fXEi3LVflp/tmp/c9c3ec42fb6ed3b58715c00412ceb4f1febaa3fa4383a9e003d209cdbaf5d8c2.jpg", "table_caption": ["Table 6: Ablation of multi-task learning. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "fXEi3LVflp/tmp/9995e5319129343814f192776bf0185b5a3537b03e7ab765c53bf07bbbed89fc.jpg", "table_caption": ["Table 5: Ablation of result selection strategies for GroupPose [44] with Swin-T. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "fXEi3LVflp/tmp/23db852871ac8ce2ee13ca15c0d55af8a4d2696794ca3382f5f14686227dc682.jpg", "table_caption": ["Table 7: Ablation of global dependency modeling. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Comparison with Text-based Segmentation Methods. To further validate the effectiveness of UniPHD in segmentation, we compare it with popular open-source text-based segmentation methods trained on RefHuman, as shown in Table 4. Using only text prompts for training, our model achieves superior performance compared to competitive methods like $\\mathrm{SgMg}$ [58] and ReLA [42]. ", "page_idx": 8}, {"type": "text", "text": "Qualitative Results. In Figure 4, we present qualitative results of UniPHD in various challenging scenarios such as crowded scenes, occlusions, similar appearances, dim lighting, and viewpoint changes. UniPHD effectively captures appearance, location, action, and context information to generate high-quality outputs. More visualizations are supplied in the Appendix. ", "page_idx": 8}, {"type": "text", "text": "5.2 Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Result Selection Strategies. Table 5 shows the performance of a recent human pose estimation model [44] using various result selection strategies on RefHuman val. Without carefully designed strategies, the model struggles to accurately predict target individuals. The distance-based strategy fails to effectively filter out high-scoring irrelevant results, while the intersection-based strategy delivers the best performance. ", "page_idx": 8}, {"type": "text", "text": "Multi-task Learning. In Table 6, we evaluate the impact of multi-task learning in our approach on RefHuman val using the AP metric. Removing either head adversely affects the performance of the other. This validates the effectiveness of our decoder, which enables bidirectional information flow between keypoint and instance queries to enhance both predictions. ", "page_idx": 8}, {"type": "table", "img_path": "fXEi3LVflp/tmp/982b15e4b6072ce33b3caf5652ab4722cb7eb4289b93cb3c5d8a7a83c32ab591.jpg", "table_caption": ["Table 8: Ablation of query initialization. "], "table_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "fXEi3LVflp/tmp/2326dcccffcdfc75a0cbc356fed648f8547cba76e43116a43a560d45aa96a4d3.jpg", "table_caption": ["Table 9: Training UniPHD with extra data. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Pose-centric Hierarchical Decoder. In Table 7, we analyze the impact of global dependency modeling in our decoder. Without global dependencies, keypoint queries struggle to perceive the prompts for predicting target-aware results. Our graph attention clearly outperforms self-attention because it not only models dynamic node relations but also simulates inherent keypoint-to-keypoint, keypoint-to-instance, and instance-to-keypoint relations via soft adjacent matrix. ", "page_idx": 9}, {"type": "text", "text": "Query Initialization. Similar to recent transformer-based pose estimation methods [44, 87], we use prompt-conditioned query initialization to enrich queries with dynamic spatial priors. Table 8 reveals that omitting these dynamic priors results in a performance decrease of $0.4{-}1.2\\%$ AP. Nonetheless, our model maintains robust performance without query initialization, demonstrating its efficacy. ", "page_idx": 9}, {"type": "text", "text": "Training with Extra Data. To unleash the capability of UniPHD for positional prompt-based prediction, we expand our training data beyond RefHuman to encompass the entire MS COCO train2017 images by generating additional point and scribble prompt annotations based on B\u00e9zier curves. As shown in Table 9, UniPHD, trained exclusively with positional prompts on the expanded dataset, achieves impressive performance on RefHuman val, with up to ${\\bf94.9\\,P C K h}@0.5$ and 89.4 oIoU, demonstrating its scalability. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We introduced Referring Human Pose and Mask Estimation in the wild, a new task aimed at simultaneously predicting the poses and masks of specified individuals using natural, user-friendly text or positional prompts. This task holds significant potential for enhancing human-AI interactions in fields such as assistive robotics and sports analysis. To achieve this, we introduced the RefHuman dataset that substantially extends MS COCO with additional text and positional prompt annotations. We also proposed the first end-to-end promptable approach named UniPHD, which employs posecentric hierarchical decoder to model local details and global dependencies for R-HPM. UniPHD achieves top-tier performance, establishing a solid benchmark for future advancements in this field. We hope our new task, dataset, and approach could foster advancements in human-AI interaction and related areas. ", "page_idx": 9}, {"type": "text", "text": "Broader Impacts. Malicious use of R-HPM models can bring potential negative societal impacts, such as unauthorized mass surveillance. We believe that the model itself is neutral with positive human-centric applications, including assistive robotics and sports analysis. ", "page_idx": 9}, {"type": "text", "text": "Limitations. The proposed UniPHD approach produces high-quality results with various prompts. However, text prompts perform worse than positional prompts due to misidentification. Future research could enhance vision-language alignment to reduce this performance disparity. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments. This work was supported by the Australian Research Council Industrial Transformation Research Hub IH180100002. Professor Ajmal Mian is the recipient of an Australian Research Council Future Fellowship Award (project number FT210100268) funded by the Australian Government. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Andriluka, M., Pishchulin, L., Gehler, P., Schiele, B.: 2d human pose estimation: New benchmark and state of the art analysis. In: CVPR. pp. 3686\u20133693 (2014)   \n[2] Bahdanau, D., Cho, K., Bengio, Y.: Neural machine translation by jointly learning to align and translate. In: ICLR (2015)   \n[3] Cao, Z., Simon, T., Wei, S.E., Sheikh, Y.: Realtime multi-person 2d pose estimation using part affinity fields. In: CVPR. pp. 7291\u20137299 (2017)   \n[4] Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., Zagoruyko, S.: End-to-end object detection with transformers. In: ECCV. pp. 213\u2013229. Springer (2020)   \n[5] Charles, J., Pfister, T., Magee, D., Hogg, D., Zisserman, A.: Personalizing human video pose estimation. In: CVPR. pp. 3063\u20133072 (2016)   \n[6] Chen, D.J., Jia, S., Lo, Y.C., Chen, H.T., Liu, T.L.: See-through-text grouping for referring image segmentation. In: ICCV. pp. 7454\u20137463 (2019)   \n[7] Chen, X., Cheung, Y.S.J., Lim, S.N., Zhao, H.: Scribbleseg: Scribble-based interactive image segmentation. arXiv preprint arXiv:2303.11320 (2023)   \n[8] Chen, X., Zhao, Z., Zhang, Y., Duan, M., Qi, D., Zhao, H.: Focalclick: Towards practical interactive image segmentation. In: CVPR. pp. 1300\u20131309 (2022)   \n[9] Chen, Y., Wang, Z., Peng, Y., Zhang, Z., Yu, G., Sun, J.: Cascaded pyramid network for multi-person pose estimation. In: CVPR. pp. 7103\u20137112 (2018)   \n[10] Cheng, B., Xiao, B., Wang, J., Shi, H., Huang, T.S., Zhang, L.: Higherhrnet: Scale-aware representation learning for bottom-up human pose estimation. In: CVPR. pp. 5386\u20135395 (2020)   \n[11] Chng, Y.X., Zheng, H., Han, Y., Qiu, X., Huang, G.: Mask grounding for referring image segmentation. arXiv preprint arXiv:2312.12198 (2023)   \n[12] Dantone, M., Gall, J., Leistner, C., Van Gool, L.: Human pose estimation using body parts dependent joint regressors. In: CVPR. pp. 3041\u20133048 (2013)   \n[13] Ding, H., Liu, C., Wang, S., Jiang, X.: Vlt: Vision-language transformer and query generation for referring segmentation. TPAMI (2022)   \n[14] Everingham, M., Van Gool, L., Williams, C.K., Winn, J., Zisserman, A.: The pascal visual object classes (voc) challenge. International journal of computer vision 88, 303\u2013338 (2010)   \n[15] Fang, H.S., Xie, S., Tai, Y.W., Lu, C.: Rmpe: Regional multi-person pose estimation. In: ICCV. pp. 2334\u20132343 (2017)   \n[16] Feng, G., Hu, Z., Zhang, L., Lu, H.: Encoder fusion network with co-attention embedding for referring image segmentation. In: CVPR. pp. 15506\u201315515 (2021)   \n[17] Hu, R., Rohrbach, M., Andreas, J., Darrell, T., Saenko, K.: Modeling relationships in referential expressions with compositional modular networks. In: CVPR. pp. 1115\u20131124 (2017)   \n[18] Hu, R., Rohrbach, M., Darrell, T.: Segmentation from natural language expressions. In: ECCV. pp. 108\u2013124. Springer (2016)   \n[19] Hu, Y., Wang, Q., Shao, W., Xie, E., Li, Z., Han, J., Luo, P.: Beyond one-to-one: Rethinking the referring image segmentation. In: ICCV. pp. 4067\u20134077 (2023)   \n[20] Hu, Z., Feng, G., Sun, J., Zhang, L., Lu, H.: Bi-directional relationship inferring network for referring image segmentation. In: CVPR. pp. 4424\u20134433 (2020)   \n[21] Huang, L., Yang, Y., Deng, Y., Yu, Y.: Densebox: Unifying landmark localization with end to end object detection. arXiv preprint arXiv:1509.04874 (2015)   \n[22] Huang, S., Hui, T., Liu, S., Li, G., Wei, Y., Han, J., Liu, L., Li, B.: Referring image segmentation via cross-modal progressive comprehension. In: CVPR. pp. 10488\u201310497 (2020)   \n[23] Hui, T., Liu, S., Huang, S., Li, G., Yu, S., Zhang, F., Han, J.: Linguistic structure guided context modeling for referring image segmentation. In: Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part X 16. pp. 59\u201375. Springer (2020)   \n[24] Insafutdinov, E., Pishchulin, L., Andres, B., Andriluka, M., Schiele, B.: Deepercut: A deeper, stronger, and faster multi-person pose estimation model. In: Computer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VI 14. pp. 34\u201350. Springer (2016)   \n[25] Jang, W.D., Kim, C.S.: Interactive image segmentation via backpropagating refinement scheme. In: CVPR. pp. 5297\u20135306 (2019)   \n[26] Jhuang, H., Gall, J., Zuff,i S., Schmid, C., Black, M.J.: Towards understanding action recognition. In: ICCV. pp. 3192\u20133199 (2013)   \n[27] Jin, S., Xu, L., Xu, J., Wang, C., Liu, W., Qian, C., Ouyang, W., Luo, P.: Whole-body human pose estimation in the wild. In: Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part IX 16. pp. 196\u2013214. Springer (2020)   \n[28] Johnson, S., Everingham, M.: Clustered pose and nonlinear appearance models for human pose estimation. In: bmvc. vol. 2, p. 5. Aberystwyth, UK (2010)   \n[29] Kamath, A., Singh, M., LeCun, Y., Synnaeve, G., Misra, I., Carion, N.: Mdetr-modulated detection for end-to-end multi-modal understanding. In: ICCV. pp. 1780\u20131790 (2021)   \n[30] Kazemzadeh, S., Ordonez, V., Matten, M., Berg, T.: Referitgame: Referring to objects in photographs of natural scenes. In: Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). pp. 787\u2013798 (2014)   \n[31] Kreiss, S., Bertoni, L., Alahi, A.: Pifpaf: Composite fields for human pose estimation. In: CVPR. pp. 11977\u201311986 (2019)   \n[32] Kuhn, H.W.: The hungarian method for the assignment problem. Naval research logistics quarterly 2(1-2), 83\u201397 (1955)   \n[33] Li, J., Wang, C., Zhu, H., Mao, Y., Fang, H.S., Lu, C.: Crowdpose: Efficient crowded scenes pose estimation and a new benchmark. In: CVPR. pp. 10863\u201310872 (2019)   \n[34] Li, K., Wang, S., Zhang, X., Xu, Y., Xu, W., Tu, Z.: Pose recognition with cascade transformers. In: CVPR. pp. 1944\u20131953 (2021)   \n[35] Li, R., Li, K., Kuo, Y.C., Shu, M., Qi, X., Shen, X., Jia, J.: Referring image segmentation via recurrent refinement networks. In: CVPR. pp. 5745\u20135753 (2018)   \n[36] Li, X., Sun, X., Meng, Y., Liang, J., Wu, F., Li, J.: Dice loss for data-imbalanced nlp tasks. In: ACL (2020)   \n[37] Li, Z., Chen, Q., Koltun, V.: Interactive image segmentation with latent diversity. In: CVPR. pp. 577\u2013585 (2018)   \n[38] Li, Z., Wang, M., Mei, J., Liu, Y.: Mail: A unified mask-image-language trimodal network for referring image segmentation. arXiv preprint arXiv:2111.10747 (2021)   \n[39] Lin, T.Y., Goyal, P., Girshick, R., He, K., Doll\u00e1r, P.: Focal loss for dense object detection. In: ICCV. pp. 2980\u20132988 (2017)   \n[40] Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll\u00e1r, P., Zitnick, C.L.: Microsoft coco: Common objects in context. In: Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13. pp. 740\u2013755. Springer (2014)   \n[41] Lin, Z., Zhang, Z., Chen, L.Z., Cheng, M.M., Lu, S.P.: Interactive image segmentation with first click attention. In: CVPR. pp. 13339\u201313348 (2020)   \n[42] Liu, C., Ding, H., Jiang, X.: Gres: Generalized referring expression segmentation. In: CVPR. pp. 23592\u201323601 (2023)   \n[43] Liu, C., Lin, Z., Shen, X., Yang, J., Lu, X., Yuille, A.: Recurrent multimodal interaction for referring image segmentation. In: ICCV. pp. 1271\u20131280 (2017)   \n[44] Liu, H., Chen, Q., Tan, Z., Liu, J.J., Wang, J., Su, X., Li, X., Yao, K., Han, J., Ding, E., et al.: Group pose: A simple baseline for end-to-end multi-person pose estimation. In: ICCV. pp. 15029\u201315038 (2023)   \n[45] Liu, J., Ding, H., Cai, Z., Zhang, Y., Satzoda, R.K., Mahadevan, V., Manmatha, R.: Polyformer: Referring image segmentation as sequential polygon generation. In: CVPR. pp. 18653\u201318663 (2023)   \n[46] Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., Stoyanov, V.: Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 (2019)   \n[47] Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: ICCV. pp. 10012\u201310022 (2021)   \n[48] Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101 (2017)   \n[49] Lu, P., Jiang, T., Li, Y., Li, X., Chen, K., Yang, W.: Rtmo: Towards high-performance one-stage real-time multi-person pose estimation. arXiv preprint arXiv:2312.07526 (2023)   \n[50] Mao, J., Huang, J., Toshev, A., Camburu, O., Yuille, A.L., Murphy, K.: Generation and comprehension of unambiguous object descriptions. In: CVPR. pp. 11\u201320 (2016)   \n[51] Mao, W., Ge, Y., Shen, C., Tian, Z., Wang, X., Wang, Z.: Tfpose: Direct human pose estimation with transformers. arXiv preprint arXiv:2103.15320 (2021)   \n[52] Mao, W., Ge, Y., Shen, C., Tian, Z., Wang, X., Wang, Z., den Hengel, A.v.: Poseur: Direct human pose regression with transformers. In: ECCV. pp. 72\u201388. Springer (2022)   \n[53] Mao, W., Tian, Z., Wang, X., Shen, C.: Fcpose: Fully convolutional multi-person pose estimation with dynamic instance-aware convolutions. In: CVPR. pp. 9034\u20139043 (2021)   \n[54] Margffoy-Tuay, E., P\u00e9rez, J.C., Botero, E., Arbel\u00e1ez, P.: Dynamic multimodal instance segmentation guided by natural language queries. In: ECCV. pp. 630\u2013645 (2018)   \n[55] McNally, W., Vats, K., Wong, A., McPhee, J.: Rethinking keypoint representations: Modeling keypoints and poses as objects for multi-person human pose estimation. In: ECCV. pp. 37\u201354. Springer (2022)   \n[56] Miao, B., Bennamoun, M., Gao, Y., Mian, A.: Regional video object segmentation by efficient motion-aware mask propagation. In: DICTA. pp. 1\u20136 (2022)   \n[57] Miao, B., Bennamoun, M., Gao, Y., Mian, A.: Self-supervised video object segmentation by motion-aware mask propagation. In: ICME (2022)   \n[58] Miao, B., Bennamoun, M., Gao, Y., Mian, A.: Spectrum-guided multi-granularity referring video object segmentation. In: ICCV. pp. 920\u2013930 (2023)   \n[59] Miao, B., Bennamoun, M., Gao, Y., Mian, A.: Region aware video object segmentation with deep motion modeling. IEEE Transactions on Image Processing (2024)   \n[60] Miao, B., Bennamoun, M., Gao, Y., Shah, M., Mian, A.: Temporally consistent referring video object segmentation with hybrid memory. IEEE Transactions on Circuits and Systems for Video Technology (2024)   \n[61] Miao, B., Zhou, L., Mian, A.S., Lam, T.L., Xu, Y.: Object-to-scene: Learning to transfer object knowledge to indoor scene recognition. In: IROS. pp. 2069\u20132075 (2021)   \n[62] Newell, A., Huang, Z., Deng, J.: Associative embedding: End-to-end learning for joint detection and grouping. Advances in neural information processing systems 30 (2017)   \n[63] Newell, A., Yang, K., Deng, J.: Stacked hourglass networks for human pose estimation. In: Computer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VIII 14. pp. 483\u2013499. Springer (2016)   \n[64] Nie, X., Feng, J., Zhang, J., Yan, S.: Single-stage multi-person pose machines. In: ICCV. pp. 6951\u20136960 (2019)   \n[65] OpenAI: Gpt-4 technical report (2023)   \n[66] Papandreou, G., Zhu, T., Kanazawa, N., Toshev, A., Tompson, J., Bregler, C., Murphy, K.: Towards accurate multi-person pose estimation in the wild. In: CVPR. pp. 4903\u20134911 (2017)   \n[67] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: ICML. pp. 8748\u20138763. PMLR (2021)   \n[68] Rezatofighi, H., Tsoi, N., Gwak, J., Sadeghian, A., Reid, I., Savarese, S.: Generalized intersection over union: A metric and a loss for bounding box regression. In: CVPR. pp. 658\u2013666 (2019)   \n[69] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image synthesis with latent diffusion models. In: CVPR. pp. 10684\u201310695 (2022)   \n[70] Shi, D., Wei, X., Li, L., Ren, Y., Tan, W.: End-to-end multi-person pose estimation with transformers. In: CVPR. pp. 11069\u201311078 (2022)   \n[71] Shi, H., Li, H., Meng, F., Wu, Q.: Key-word-aware network for referring expression image segmentation. In: ECCV. pp. 38\u201354 (2018)   \n[72] Stoff,l L., Vidal, M., Mathis, A.: End-to-end trainable multi-instance pose estimation with transformers. arXiv preprint arXiv:2103.12115 (2021)   \n[73] Sun, K., Xiao, B., Liu, D., Wang, J.: Deep high-resolution representation learning for human pose estimation. In: CVPR. pp. 5693\u20135703 (2019)   \n[74] Tang, J., Zheng, G., Shi, C., Yang, S.: Contrastive grouping with transformer for referring image segmentation. In: CVPR. pp. 23570\u201323580 (2023)   \n[75] Tian, Z., Chen, H., Shen, C.: Directpose: Direct end-to-end multi-person pose estimation. arXiv preprint arXiv:1911.07451 (2019)   \n[76] Tian, Z., Chu, X., Wang, X., Wei, X., Shen, C.: Fully convolutional one-stage 3d object detection on lidar range images. Advances in Neural Information Processing Systems 35, 34899\u201334911 (2022)   \n[77] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, \u0141., Polosukhin, I.: Attention is all you need. In: NeurIPS (2017)   \n[78] Wang, D., Zhang, S.: Contextual instance decoupling for robust multi-person pose estimation. In: CVPR. pp. 11060\u201311068 (2022)   \n[79] Wang, J., Sun, K., Cheng, T., Jiang, B., Deng, C., Zhao, Y., Liu, D., Mu, Y., Tan, M., Wang, X., et al.: Deep high-resolution representation learning for visual recognition. IEEE transactions on pattern analysis and machine intelligence 43(10), 3349\u20133364 (2020)   \n[80] Wang, Z., Lu, Y., Li, Q., Tao, X., Guo, Y., Gong, M., Liu, T.: Cris: Clip-driven referring image segmentation. In: CVPR. pp. 11686\u201311695 (2022)   \n[81] Wei, F., Sun, X., Li, H., Wang, J., Lin, S.: Point-set anchors for object detection, instance segmentation and pose estimation. In: Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part X 16. pp. 527\u2013544. Springer (2020)   \n[82] Wu, J., Zheng, H., Zhao, B., Li, Y., Yan, B., Liang, R., Wang, W., Zhou, S., Lin, G., Fu, Y., et al.: Ai challenger: A large-scale dataset for going deeper in image understanding. arXiv preprint arXiv:1711.06475 (2017)   \n[83] Wu, J., Jiang, Y., Sun, P., Yuan, Z., Luo, P.: Language as queries for referring video object segmentation. In: CVPR. pp. 4974\u20134984 (2022)   \n[84] Xiao, B., Wu, H., Wei, Y.: Simple baselines for human pose estimation and tracking. In: ECCV. pp. 466\u2013481 (2018)   \n[85] Xiao, Y., Su, K., Wang, X., Yu, D., Jin, L., He, M., Yuan, Z.: Querypose: Sparse multi-person pose regression via spatial-aware part-level query. Advances in Neural Information Processing Systems 35, 12464\u201312477 (2022)   \n[86] Xu, N., Price, B., Cohen, S., Yang, J., Huang, T.S.: Deep interactive object selection. In: CVPR. pp. 373\u2013381 (2016)   \n[87] Yang, J., Zeng, A., Liu, S., Li, F., Zhang, R., Zhang, L.: Explicit box detection unifies endto-end multi-person pose estimation. In: The Eleventh International Conference on Learning Representations (2022)   \n[88] Yang, Z., Wang, J., Tang, Y., Chen, K., Zhao, H., Torr, P.H.: Lavt: Language-aware vision transformer for referring image segmentation. In: CVPR. pp. 18155\u201318165 (2022)   \n[89] Ye, L., Rochan, M., Liu, Z., Wang, Y.: Cross-modal self-attention network for referring image segmentation. In: CVPR. pp. 10502\u201310511 (2019)   \n[90] Yu, L., Lin, Z., Shen, X., Yang, J., Lu, X., Bansal, M., Berg, T.L.: Mattnet: Modular attention network for referring expression comprehension. In: CVPR. pp. 1307\u20131315 (2018)   \n[91] Yu, L., Poirson, P., Yang, S., Berg, A.C., Berg, T.L.: Modeling context in referring expressions. In: Computer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14. pp. 69\u201385. Springer (2016)   \n[92] Zhao, W., Rao, Y., Liu, Z., Liu, B., Zhou, J., Lu, J.: Unleashing text-to-image diffusion models for visual perception. In: ICCV. pp. 5729\u20135739 (2023)   \n[93] Zhou, X., Wang, D., Kr\u00e4henb\u00fchl, P.: Objects as points. arXiv preprint arXiv:1904.07850 (2019)   \n[94] Zhu, C., Zhou, Y., Shen, Y., Luo, G., Pan, X., Lin, M., Chen, C., Cao, L., Sun, X., Ji, R.: Seqtr: A simple yet universal network for visual grounding. In: ECCV. pp. 598\u2013615. Springer (2022)   \n[95] Zhu, X., Su, W., Lu, L., Li, B., Wang, X., Dai, J.: Deformable detr: Deformable transformers for end-to-end object detection. In: ICLR (2021) ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.1 Training Loss Functions ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The overall loss function for UniPHD consists of four parts: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{L}_{t r a i n}=\\mathcal{L}_{b o x}+\\mathcal{L}_{c l a s s}+\\mathcal{L}_{p o s e}+\\mathcal{L}_{m a s k},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\mathcal{L}_{b o x}$ is for human box regression that contains L1 loss and GIOU [68] loss, $\\mathcal{L}_{c l a s s}$ is for classification includes focal loss [39] with $\\alpha=0.25$ and $\\gamma=2$ , $\\mathcal{L}_{p o s e}$ is for keypoint regression and visibility prediction that includes L1 loss, the constrained L1 loss-OKS loss [70], and cross entropy loss, $\\mathcal{L}_{m a s k}$ is for segmentation that includes Dice [36] loss and focal loss. The loss coefficients $\\lambda_{b o x}^{L1}$ , $\\lambda_{b o x}^{G I O U}$ , \uff0c $\\lambda_{c l a s s}^{f o c a l}$ , $\\lambda_{p o s e}^{L1}$ , $\\lambda_{p o s e}^{O K S}$ , $\\lambda_{p o s e}^{C E}$ , \u03bbDmiacsek, \u03bbfmoacsakl are 5, 2, 2, 10, 4, 4, 5, 2, following [44, 58, 87]. The Hungarian algorithm [32] is used to identify the optimal assignment (query group) with the highest similarity to the ground truth for training. The class label for the optimal instance query with the minimum loss from the ground truth is set to one, while all others are set to zero. ", "page_idx": 15}, {"type": "text", "text": "A.2 Implementation Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We follow the common optimization strategies in [44, 58, 83, 87] to train the models. During training, we augment input images through random filp, random crop, and random resize with the shorter sides within the range of 360 to 640 pixels and the longer sides up to 640 pixels. For each iteration, we randomly select either a text or positional prompt with equal probability. We use the AdamW [48] optimizer with a weight decay of $1\\!\\times\\!10^{-4}$ and train our models on 24GB RTX 3090 GPUs with batch size 16 for 20 epochs. The initial learning rates are set to $1\\!\\times\\!10^{-5}$ for the visual encoder and $1\\!\\times\\!10^{-4}$ for other components, with a rate decay at the 18th epoch by a factor of 10. Both the multimodal encoder and pose-centric hierarchical decoder consist of 6 layers, and we use 20 query groups for our models. During testing, we resize the input images with their longer sides up to 640 pixels. ", "page_idx": 15}, {"type": "text", "text": "A.3 Different Query Group Numbers ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In Table 10, our approach perform well across different numbers of query groups, measured by the AP metric. This hyperparameter primarily affects pose estimation, with 20 query groups identifying more candidates to achieve the best performance. ", "page_idx": 15}, {"type": "table", "img_path": "fXEi3LVflp/tmp/2e10362db6446d9f06ab57317e84aafcaf306fb6522e272e4a96d5a3a80cce20.jpg", "table_caption": ["Table 10: Ablation of different number of query groups. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "A.4 Increasing Model Capacity ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In Table 11, we enhance model capacity by adding multimodal encoder layers and increasing the feature dimensions from 256 to 384. The results indicate our current settings are already highly effective. ", "page_idx": 15}, {"type": "table", "img_path": "fXEi3LVflp/tmp/8f6b9882756da83c5b5a5daa10b6c00451f93a62a65d3be2ae390e362cc9aaca.jpg", "table_caption": ["Table 11: Ablation of increasing model capacity. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "A.5 Additional Visualizations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In Figure 5, we present additional qualitative results of UniPHD with text and positional prompts.   \nUniPHD effectively processes these prompts to produce high-quality results. ", "page_idx": 16}, {"type": "image", "img_path": "fXEi3LVflp/tmp/7e34be107efa0e504aacf9db4d8e484af602682ea3b726d13d7b6b4dabc5b294.jpg", "img_caption": ["Figure 5: Qualitative results of our UniPHD with different prompts in various challenging scenarios. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Licenses of MS COCO and RefCOCO/+/g. The annotations in MS COCO belong to the COCO Consortium and are licensed under CC BY 4.0. The use of the images complies with Flickr Terms of Use. See https://cocodataset.org/#termsofuse for more details. The RefCOCO/+ datasets are licensed under Apache License 2.0 and the RefCOCOg dataset is licensed under CC BY 4.0. ", "page_idx": 16}, {"type": "text", "text": "Terms of Use and License of RefHuman. RefHuman is licensed under CC BY 4.0. ", "page_idx": 16}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: The abstract and introduction clearly state the claims, which are supported by our experimental results. See Abstract and Sec. 1. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 17}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification: See Sec. 6. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 17}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 17}, {"type": "text", "text": "Justification: The paper does not contain any theoretical assumptions or proofs. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 18}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: We report the dataset, model, and training details in Sec. 3, Sec. 4, Sec. 5, and the Appendix. The introduced dataset and code are publicly available. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 18}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The introduced dataset and code are publicly available. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 19}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: See Sec. 5 and the Appendix. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 19}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [No] ", "page_idx": 19}, {"type": "text", "text": "Justification: We follow the usual format used in previous related works to report and compare the results. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: See the Appendix. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 20}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The research conducted in the paper conform with the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 20}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: See Sec. 6. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 21}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We cite the creators or original papers of all the related code, data, and models used in this paper. All the assets are free for research study and widely used in previous related works. We also provide the licenses of the used datasets in the Appendix. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We report the dataset, model, and training details in Sec. 3, Sec. 4, Sec. 5, and the Appendix. The introduced dataset and code are publicly available. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 22}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: We did not use crowdsourcing or conduct research with human subjects. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 22}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: We did not use crowdsourcing or conduct research with human subjects. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 22}]