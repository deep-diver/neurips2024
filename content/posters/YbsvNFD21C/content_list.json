[{"type": "text", "text": "Robust Sparse Regression with Non-Isotropic Designs ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Chih-Hung Liu Department of Electrical Engineering National Taiwan University chliu@ntu.edtw ", "page_idx": 0}, {"type": "text", "text": "GlebNovikov Lucerne School of Computer Science and Information Technology gleb.novikov@hslu.ch ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We develop a technique to design efficiently computable estimators for sparse linear regression in the simultaneous presence of two adversaries: oblivious and adaptive. Consider the model $y^{*}=X^{*}\\beta^{*}+\\eta$ where $X^{\\ast}$ is an $n\\times d$ random design matrix, $\\beta^{*}\\in\\mathbb{R}^{d}$ is a $k$ -sparse vector, and the noise $\\eta$ is independent of $X^{\\ast}$ and chosen by the oblivious adversary. Apart from the independence of $X^{\\ast}$ , we only require a small fraction entries of $\\eta$ to have magnitude at most 1. The adaptive adversary is allowed to arbitrarily corrupt an $\\varepsilon$ -fraction of the samples $(X_{1}^{*},y_{1}^{*}),\\ldots,(X_{n}^{*},y_{n}^{*})$ . Given the $\\varepsilon$ -corrupted samples $(X_{1},y_{1}),\\ldots,(X_{n},y_{n})$ , the goal is to estimate $\\beta^{*}$ .We assume that the rows of $X^{\\ast}$ are iid samples from some $d$ -dimensional distribution $\\mathcal{D}$ with zero mean and (unknown) covariance matrix $\\Sigma$ with bounded condition number. We design several robust algorithms that outperform the state of the art even in the special case of Gaussian noise $\\eta\\,\\sim\\,{\\cal N}(0,1)^{n}$ . In particular, we provide a polynomial-time algorithm that with high probability recovers $\\beta^{*}$ up to error $O({\\sqrt{\\varepsilon}})$ as long as $n\\,\\geqslant\\,\\tilde{O}(k^{2}/\\varepsilon)$ , only assuming some bounds on the third and the fourth moments of $\\mathcal{D}$ . In addition, prior to this work, even in the special case of Gaussian design $\\mathcal{D}\\,=\\,N(0,\\Sigma)$ and noise $\\eta\\,\\sim\\,N(0,1)$ , no polynomial time algorithm was known to achieve error $o({\\sqrt{\\varepsilon}})$ in the sparse setting $n<d^{2}$ .We show that under some assumptions on the fourth and the eighth moments of $\\mathcal{D}$ , there is a polynomial-time algorithm that achieves error $o({\\sqrt{\\varepsilon}})$ as long as $n\\geqslant{\\tilde{O}}(k^{4}/\\varepsilon^{3})$ For Gaussian distribution $\\mathcal{D}\\,=\\,N(0,\\Sigma)$ , this algorithm achieves error $O(\\varepsilon^{3/4})$ Moreover, our algorithm achieves error $o({\\sqrt{\\varepsilon}})$ for all log-concave distributions if $\\varepsilon\\leqslant1/\\mathrm{polylog(d)}$ ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "Our algorithms are based on the filtering of the covariates that uses sum-of-squares relaxations, and weighted Huber loss minimization with $\\ell_{1}$ regularizer. We provide a novel analysis of weighted penalized Huber loss that is suitable for heavy-tailed designs in the presence of two adversaries. Furthermore, we complement our algorithmic results with Statistical Query lower bounds, providing evidence that our estimators are likely to have nearly optimal sample complexity. ", "page_idx": 0}, {"type": "text", "text": "Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Linear regression is the fundamental task in statistics, with many applications in data science and machine learning. In ordinary (non-sparse linear regression, we are givenobservations $y_{1}^{*},\\ldots,y_{n}^{*}$ and $X_{1}^{*},\\ldots,X_{n}^{*}\\in\\mathbb{R}^{d}$ such that $y_{i}^{*}=\\left\\langle X_{i}^{*},\\beta^{*}\\right\\rangle\\!+\\eta_{i}$ for some $\\beta^{*}\\in\\mathbb{R}^{d}$ and some noise $\\eta\\in\\mathbb{R}^{n}$ , and the goal is to estimate $\\beta^{*}$ .If $\\eta$ is independent of $X^{\\ast}$ and has iid Gaussian entries $\\eta_{i}\\sim N(0,1)$ , the classical least squares estimator $\\hat{\\beta}$ with high probability achieves the prediction error $\\begin{array}{r}{\\frac{1}{\\sqrt{n}}\\|X^{*}(\\hat{\\beta}\\!-\\!\\beta^{*})\\|\\leqslant O\\Big(\\sqrt{d/n}\\Big)}\\end{array}$ Note that if $d/n\\rightarrow0$ , the error is vanishing. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Despite the huge dimensions of modern data, many practical applications only depend on a small part of the dimensions of data, thus motivating sparse regression, where only $k\\ll d$ explanatory variables are actually important (i.e., $\\beta^{*}$ is $k$ -sparse). In this case we want the error to be small even if we only have $n\\ll d$ samples. In this case, there exists an estimator that achieves prediction error $O\\left({\\sqrt{k\\log(d)/n}}\\right)$ (for $\\eta\\sim N(0,1)^{n})$ . However, this estimator requires exponential computation time. Moreover, under a standard assumption from computational complexity theory $(\\mathbf{NP\\mathcal{C}\\,P/p o l y})$ estimators that can be computed in polynomial time require an assumption on $X^{\\ast}$ called a restricted eigenvalue condition in order to achieve error $O\\left({\\sqrt{k\\log(d)/n}}\\right)$ (see [ZWJ14] for more details). One efficiently computable estimator that achieves error $O\\left({\\sqrt{k\\log(d)/n}}\\right)$ under the restricted eigenvalue condition is Lasso, that is, a minimizer of the quadratic loss with $\\ell_{1}$ regularizer. In particular, the restricted eigenvalue condition is satisfied for $X^{\\ast}$ with rows $X_{i}^{*}\\overset{\\mathrm{iid}}{\\sim}N(0,\\Sigma)$ ,where $\\Sigma$ has condition number $O(1)$ , as long as $n\\gtrsim k\\log d$ (with high probability). ", "page_idx": 1}, {"type": "text", "text": "Further we assume that the designs have id random rows, and the condition number of the covariance matrix is bounded by some constant. In addition, for random designs, we use the standard error $\\|\\Sigma^{1/2}(\\hat{\\beta}-\\beta^{*})\\|$ . Note that when the number of samples is large enough, this error is very close to $\\begin{array}{r}{\\frac{1}{\\sqrt{n}}\\|X^{*}(\\hat{\\beta}-\\beta^{*})\\|}\\end{array}$ ", "page_idx": 1}, {"type": "text", "text": "Recently, there was an extensive interest in the linear regression with the presence of adversarially chosen outliers. Under the assumption $X_{i}^{*}\\overset{\\mathrm{iid}}{\\sim}N(0,\\Sigma)$ , the line of works [TJSO14, BJKK17, SBRJ19, dNS21, $\\mathrm{dLN}^{+}21]$ studied the case when the noise $\\eta$ is unbounded and chosen by an oblivious adversary, i.e., when $\\eta$ is an arbitrary vector independent of $X^{\\ast}$ . As was shown in $[\\mathrm{dLN}^{+}21]$ , in this case, it is possible to achieve the same error (up to a constant factor) as for $\\eta\\sim N(0,1)^{n}$ if we only assume that $\\Omega(1)$ fraction of the entries of $\\eta$ have magnitude at most 1. They analyzed the Huber loss estimator with $\\ell_{1}$ regularizer. ", "page_idx": 1}, {"type": "text", "text": "Another line of works [BJK15, DT19, MNW22, Tho23] assumed that $\\eta$ has id random entries that satisfy some assumptions on the moments, but an adversarially chosen $\\varepsilon$ -fraction of $y_{1}^{*},\\ldots,y_{n}^{*}$ is replaced by arbitrary values by an adaptive adversary that can observe $X^{\\ast}$ $\\beta^{*}$ and $\\eta$ (so the corruptions can depend on them). [Tho23] showed that for $X^{\\ast}$ with id sub-Gaussian rows and $\\eta$ with id sub-Gaussian entries with unit variance, Huber loss estimator with $\\ell_{1}$ regularizer achieves an error of $O\\left(\\sqrt{k\\log(d)/n}+\\varepsilon\\log(1/\\varepsilon)\\right)$ with high probability. Note that the second term depends on $\\varepsilon$ , but not on $n$ ; hence, even if we take more samples, this term does not decrease (if $\\varepsilon$ remains the same). It is inherent: in the presence of the adaptive adversarial outliers, even for $X_{i}^{*}\\overset{\\mathrm{iid}}{\\sim}N(0,\\mathrm{Id})$ and $\\eta\\sim N(0,1)^{n}$ , the information theoretically optimal error is $\\Omega\\Bigl(\\sqrt{k\\log(d)/n}+\\varepsilon\\Bigr)$ , so independently of the number of samples, it is $\\Omega(\\varepsilon)$ . In the algorithmic high-dimensional robust statistics, we are interested in estimators that are computable in time poly $(d)$ . There is evidence that it is unlikely that $\\mathrm{poly}(d)$ -time computable estimators can achieve error $O(\\varepsilon)$ [DKS17]. Furthermore, for other design distributions the optimal error can be different. ", "page_idx": 1}, {"type": "text", "text": "Hence the natural questions to ask are : Given an error bound $f(\\varepsilon)$ , does there exist a $\\mathrm{poly}(d)$ -time computable estimator that achieves error at most $f(\\varepsilon)$ with high probability? If possible, what is the smallest number of samples $n$ that is enough to achieve error $f(\\varepsilon)$ in time $\\mathsf{p o l y}(d)?$ In the rest of this section, we write error bounds in terms of $\\varepsilon$ and mention the number of samples that is required to achieve this error. In addition, we focus on the results for the high dimensional regime, where $f(\\varepsilon)$ does not depend polynomially on $k$ or $d$ ", "page_idx": 1}, {"type": "text", "text": "Another line of works [BDLS17, LSLC20, PJL20, Sas22, SF23] considered the case when the adaptive adversary is allowed to corrupt $\\varepsilon$ -fraction of all observed data, i.e. not only $y_{1}^{*},\\ldots,y_{n}^{*}$ but also $X_{1}^{*},\\ldots,X_{n}^{*}$ , while the noise $\\eta$ is assumed to have id random entries that satisfy some concentration assumptions. For simplicity, to fix the scale of the noise, we formulate their results assuming that $\\eta\\,\\sim\\,N(0,1)^{n}$ . In non-sparse settings, [PJL20] showed that in the case of identity covariance sub-Gaussian designs, Huber loss minimization after a proper filtering of $X^{\\ast}$ achieves error $\\tilde{O}(\\varepsilon)$ with $n\\gtrsim d/\\varepsilon^{2}$ samples. Informally speaking, filtering removes the samples $X_{i}^{*}$ that look corrupted, and if the distribution of the design is nice enough, then after filtering we can work with $(X^{*},\\Bar{y}^{*})$ just like in the case when only $y^{*}$ is corrupted. For unknown covariance they showed a bound $O\\big(\\sqrt{\\varepsilon}\\big)$ for a large class of distributions of the design. If $X_{i}^{*}\\overset{\\mathrm{iid}}{\\sim}N(0,\\Sigma)$ for unknown $\\Sigma$ , one can use $n\\geqslant{\\tilde{O}}{\\left({{d^{2}}/{\\varepsilon^{2}}}\\right)}$ samples to robustly estimate the covariance, and achieve nearly optimal error $\\tilde{O}(\\varepsilon)$ in the case (see [DKS19] for more details). ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "In the sparse seting, there is likely an information-computation gap for the sample complexity of this problem, even in the case of the isotropic Gaussian design $X_{i}^{*}\\overset{\\mathrm{iid}}{\\sim}N(0,\\mathrm{Id})$ . While it is informationtheoretically possible to achieve optimal error $O(\\varepsilon)$ with $n\\,\\geqslant\\,{\\tilde{O}}(k/\\varepsilon^{2})$ samples, achieving any error $o(1)$ is likely to be not possible for $\\mathrm{poly}(d)$ -time computable estimators if $n\\,\\ll\\,k^{2}$ . Formal evidence for this conjecture include reductions from some version of the Planted Clique problem [BB20], as well as a Statistical Query lower bound (Proposition 1.10). For $n\\geqslant\\tilde{O}(k^{2}\\bar{/}\\varepsilon^{2})$ , several algorithmic results are known to achieve error $\\tilde{O}(\\varepsilon)$ , in particular, [BDLS17, LSLC20], and [SF23] for more general isotropic sub-Gaussian designs. Similarly to the approach of [PJL20], [SF23] used $\\ell_{1}$ -penalized) Huber minimization after filtering $X^{\\ast}$ ", "page_idx": 2}, {"type": "text", "text": "The non-isotropic case (when $\\Sigma\\neq{\\mathrm{Id}}$ is unknown) is more challenging. [SF23] showed that for sub-Gaussian designs it is possible to achieve error $O\\big(\\sqrt{\\varepsilon}\\big)$ with $n\\geqslant\\tilde{O}(k^{2})$ samples. [Sas22] showed that $O\\big(\\sqrt{\\varepsilon}\\big)$ error with $n\\geqslant\\tilde{O}(k^{2}+\\|\\beta^{*}\\|_{1}^{4}/k^{2})$ samples can be achieved under some assumptions on the fourth and the eighth moments of the design distribution. While this result works for a large class of designs, the clear disadvantage is that the sample complexity depends polynomially on the norm of $\\beta^{*}$ . For example, if all nonzero entries of $\\beta^{*}$ have the same magnitude and $\\|\\beta^{*}\\|=\\sqrt{d}$ , then the sample complexity is $n>d^{2}$ , which is not suitable in the sparse regime. ", "page_idx": 2}, {"type": "text", "text": "Prior to this work, no poly $(d)$ -time computable estimator that could achieve error $o\\mathopen{}\\mathclose\\bgroup\\left(\\sqrt{\\varepsilon}\\aftergroup\\egroup\\right)$ with unknownwas kown,even in the case of Gausiandesigns X id $X_{i}^{*}\\overset{\\mathrm{iid}}{\\sim}N(0,\\Sigma)$ and the Gaussian noise $\\eta\\sim N(0,1)^{n}$ (apart from the non-sparse setting, where such estimators require $n>d^{2}$ ", "page_idx": 2}, {"type": "text", "text": "1.1 Results ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We present two main results, both of them follow from a more general statement; see Theorem B.3.   \nBefore formally stating the results, we define the model as follows. ", "page_idx": 2}, {"type": "text", "text": "Definition 1.1 (Robust Sparse Regression with 2 Adversaries). Let $n,d,k\\in\\mathbb{N}$ such that $k\\leqslant d$ $\\sigma>0$ , and $\\ensuremath{\\varepsilon}\\in\\left(0,1\\right)$ is smaller than some sufficiently small absolute constant. Let $\\mathcal{D}$ be a probability distribution in $\\mathbb{R}^{d}$ with mean O and covariance $\\Sigma$ . Let $y^{*}=X^{*}\\beta^{*}+\\eta$ , where $X^{\\ast}$ is an $n\\times d$ random matrix with rows $X_{i}^{*}\\overset{\\mathrm{iid}}{\\sim}\\mathcal{D},\\beta^{*}\\in\\mathbb{R}^{d}$ .is $k$ -sparse, $\\eta\\in\\mathbb{R}^{n}$ is independent of $X^{\\ast}$ and has a least $0.01\\cdot n$ entries bounded by $\\sigma$ in absolute value1. We denote by $\\kappa(\\Sigma)$ the condition number of $\\Sigma$ ", "page_idx": 2}, {"type": "text", "text": "An instance of our model is a pair $(X,y)$ , where $X\\in\\mathbb{R}^{n\\times d}$ is a matrix and $y\\in\\mathbb{R}^{n}$ is a vector such that there exists a set $S_{\\mathrm{good}}\\subseteq[n]$ of size at least $(1-\\varepsilon)n$ such that for all $i\\in S_{\\mathrm{good}}$ \uff0c $X_{i}=X_{i}^{*}$ and $y_{i}=y_{i}^{*}$ ", "page_idx": 2}, {"type": "text", "text": "Note that random noise models studied in prior works are captured by our model in Definition 1.1. For example, if $\\eta$ has id entries that satisfy $\\mathbb{E}|\\eta_{i}|\\,\\leqslant\\,\\sigma/2$ , by Markov's inequality, $|\\eta_{i}|\\,\\leqslant\\,\\sigma$ with probability at least $1/2$ , and with overwhelming probability, at least $0.01\\cdot n$ entries of $\\eta$ are bounded by $\\sigma$ in absolute value. In addition, Cauchy noise (that does not have the first moment) with location parameter O and scale $\\sigma$ also satisfies these assumptions, as well as other heavy-tailed distributions studied in literature (with appropriate scale parameter $\\sigma$ . ", "page_idx": 2}, {"type": "text", "text": "We formulate our results assuming that the condition number of the covariance is bounded by some constant: $\\kappa(\\Sigma)\\leqslant O(1)$ . In the most general formulation (Theorem B.3), we show the dependence2 of the number of samples and the error on $\\kappa(\\Sigma)$ ", "page_idx": 3}, {"type": "text", "text": "1.1.1  Robust regression with heavy-tailed designs ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We use the following notion of boundness of the moments of $\\mathcal{D}$ ", "page_idx": 3}, {"type": "text", "text": "Definition 1.2. Let $M>0$ $t\\geqslant2$ and $d\\in\\mathbb{N}$ We say that a probability distribution $\\mathcal{D}$ in $\\mathbb{R}^{d}$ with zero mean and covariance $\\Sigma$ has $M$ -bounded $t$ -th moment, if for all $u\\in\\mathbb{R}^{d}$ ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\big(\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}|\\langle x,u\\rangle|^{t}\\big)^{1/t}\\leqslant M\\cdot\\sqrt{\\|\\Sigma\\|}\\cdot\\|u\\|\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Note that an arbitrary linear transformation of an isotropic distribution with $M$ -bounded $t$ -th moment also has $M$ -bounded $t$ -th moment. Also note that if $t^{\\prime}\\leqslant t$ and a distribution $\\mathcal{D}$ has $M$ -bounded $t$ -th moment, then the $t^{\\prime}$ -th moment of $\\mathcal{D}$ is also $M$ -bounded. In particular, $M$ cannot be smaller than 1, since the second moment cannot be $M$ -bounded for $M<1$ . In addition, we will need the following (weaker) notion of the boundness of moments: ", "page_idx": 3}, {"type": "text", "text": "Definition 1.3. Let $\\nu>0,t\\geqslant2$ and $d\\in\\mathbb{N}$ . We say that a probability distribution $\\mathcal{D}$ in $\\mathbb{R}^{d}$ with zero mean and covariance $\\Sigma$ has entrywise $\\nu$ -bounded $t$ -th moment,if ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{j\\in[d]}\\;\\mathbb{E}_{\\boldsymbol{x}\\sim\\mathcal{D}}|\\boldsymbol{x}_{j}|^{t}\\leqslant\\nu^{t}\\cdot\\|\\Sigma\\|^{t/2}\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "If a distribution has $M$ -bounded $t$ -th moment, then it also has entrywise $M$ -bounded $t$ -thmoment, but the converse might not be true for some distributions. Now we are ready to state our first result. ", "page_idx": 3}, {"type": "text", "text": "Theorem 1.4. Let $n,d,k,X,y,\\varepsilon,\\mathcal{D},\\Sigma,\\sigma,\\beta^{*}$ be as in Definition 1.1. Suppose that $\\kappa(\\Sigma)\\leqslant O(1)$ and that forsome $1\\leqslant M\\leqslant O{\\dot{(}}1)$ and $1\\leqslant\\nu\\leqslant O(1),$ $\\mathcal{D}$ has $M$ -bounded3-rdmomentandentrywise v-bounded 4-th moment. There exists an algorithm that, given $X,y,k,\\varepsilon,\\sigma$ in time $(n+d)^{O(1)}$ outputs $\\hat{\\beta}\\in\\mathbb{R}^{d}$ such that if $\\dot{\\mathbf{\\xi}}n\\gtrsim k^{2}\\log(d)/\\varepsilon$ , then with probability at least $1-d^{-10}$ \uff0c ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\|\\Sigma^{1/2}(\\hat{\\beta}-\\beta^{*})\\|\\leqslant O(\\sigma\\cdot\\sqrt{\\varepsilon})\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Let us compare Theorem 1.4 with the state of the art. For heavy-tailed designs, prior to this work, the best estimator was [Sas22]. That estimator also achieves error $O(\\sigma{\\sqrt{\\varepsilon}})$ , but its sample complexity depends polynomially on the norm of $\\beta^{*}$ , while our sample complexity does not depend on it. In addition, they require the distribution to have bounded 4-th moment (as opposed to our 3-rd moment assumption), and bounded entrywise 8-th moment (as opposed to our entrywise 4-th moment assumption). Finally, our noise assumption is weaker than theirs since they required the entries of $\\eta$ to be iid random variables such that $\\mathbb{E}|\\eta_{i}|\\leqslant\\sigma^{\\prime}$ for some $\\sigma^{\\prime}>0$ known to the algorithm designer; as we mentioned after Definition 1.1, it is a special case of the oblivious noise with $\\sigma=2\\sigma^{\\prime}$ ", "page_idx": 3}, {"type": "text", "text": "Let us also discuss our assumptions and possibilities of an improvement of our result. The third moment assumption can be relaxed, more precisely, it is enough to require the $t$ -th moment tobe bounded,where $t$ is an arbitrary constant greater than 2, and in this case the sample complexity is increased by a constant factor? ; see Theorem B.3 for more details. The entrywise fourth moment assumption is not improvable with our techniques, that is, we get worse dependence on $k$ if werelax it to, say, the third moment assumption. ", "page_idx": 3}, {"type": "text", "text": "The dependence of $n$ on $\\varepsilon$ is not improvable with our techniques4. The dependence of the error on $\\sigma$ is optimal. The dependence of $n$ on $k$ and the error on $\\sqrt{\\varepsilon}$ is likely to be (nearly) optimal: Statistical Query lower bounds (Proposition 1.10 and Proposition 1.11) provide evidence that for $\\sigma=\\Theta(1)$ , it is unlikely that polynomial-time algorithms can achieve error $o(1)$ if $n\\ll k^{2}$ , or error $o({\\sqrt{\\varepsilon}})$ if $n\\ll k^{4}$ Remark 1.5. Our results also imply bounds on other types of error studied in literature. In particular, observe that $\\lVert\\hat{\\beta}-\\beta^{*}\\rVert\\leqslant\\lVert\\Sigma^{1/2}(\\hat{\\beta}-\\beta^{*})\\rVert/\\sqrt{\\lambda_{\\operatorname*{min}}(\\Sigma)}$ where $\\lambda_{\\mathrm{min}}(\\Sigma)$ is the minimal eigenvalue of $\\Sigma$ ", "page_idx": 3}, {"type": "text", "text": "In addition, our estimator also satisfies $\\|\\hat{\\beta}-\\beta^{*}\\|_{1}\\leqslant O(\\|\\Sigma^{1/2}(\\hat{\\beta}-\\beta^{*})\\|\\cdot\\sqrt{k/\\lambda_{\\operatorname*{min}}(\\Sigma)})$ Thesame is also true for our estimator from Theorem 1.7 below. These relations between different types of errors are standard for sparse regression, and they are not improvable. ", "page_idx": 4}, {"type": "text", "text": "1.1.2Beyond $\\sqrt{\\varepsilon}$ error ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Prior to this work, no polynomial-time algorithm for (non-isotropic) robust sparse regression was known to achieve error $o(\\sigma\\sqrt{\\varepsilon})$ , even for Gaussian designs $X_{i}^{*}\\overset{\\mathrm{iid}}{\\sim}N(0,\\Sigma)$ and Gaussian $\\boldsymbol{\\eta}\\sim N(0,\\sigma)^{n}$ In this section we show that for a large class of designs, it is possible to achieve error $o(\\sigma{\\sqrt{\\varepsilon}})$ in polynomial time, even when $\\eta$ is chosen by an oblivious adversary. For our second result, we require not only some bounds on the moments of $\\mathcal{D}$ , but also their certifiability in the sum-of-squares proof system: ", "page_idx": 4}, {"type": "text", "text": "Definition 1.6. Let $M>0$ and let $\\ell\\geqslant4$ be an even number. We say that a probability distribution $\\mathcal{D}$ in $\\mathbb{R}^{d}$ with zero mean and covariance $\\Sigma$ has $\\ell$ -certifiably $M$ -bounded 4-th moment, if there exist polynomials $h_{1},\\ldots,h_{m}\\in\\mathbb{R}[u_{1},\\ldots,u_{d}]$ of degree at most $\\ell/2$ such that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\times\\sim\\mathcal{D}}\\left\\langle x,u\\right\\rangle^{4}+\\sum_{i=1}^{m}h_{i}^{2}(u)=M^{4}\\cdot\\left\\Vert\\Sigma\\right\\Vert^{2}\\cdot\\left\\Vert u\\right\\Vert^{4}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Definition 1.6 with arbitrary $\\ell$ implies Definition 1.2 (with the same $M$ ). Under standard complexitytheoretic assumptions, there exist distributions with bounded moments that are not $\\ell$ -certifiably bounded even for very large $\\ell$ [HL19]. Note that similarly to Definition 1.2, an arbitrary linear transformation of an isotropic distribution with $\\ell$ -certifiably $M$ -bounded 4-th moment also has $\\ell$ -certifiably $M$ -bounded 4-th moment. ", "page_idx": 4}, {"type": "text", "text": "Distributions with certifiably bounded moments are very important in algorithmic robust statistics. They were extensively studied in literature, e.g. [KS17a, KS17b, HL18, HL19, $\\mathrm{DKK}^{+}22]$ ", "page_idx": 4}, {"type": "text", "text": "Now we can state our second result. ", "page_idx": 4}, {"type": "text", "text": "Theorem 1.7. Let $n,d,k,X,y,\\varepsilon,\\mathcal{D},\\Sigma,\\sigma,\\beta^{*}$ be as in Definition 1.1. Suppose that $\\kappa(\\Sigma)\\leqslant O(1)$ and thatforsome $M\\geqslant1$ ,someevennumber $\\ell\\,\\geqslant\\,4$ and $1\\,\\leqslant\\,\\nu\\,\\leqslant\\,O(1)$ $\\mathcal{D}$ has $\\ell$ -certifiably $M$ -bounded 4-th moment and entrywise $\\nu$ -bounded 8-th moment. There exists an algorithm that, given $X,y,k,\\varepsilon,\\sigma,M,\\ell,$ in time $(n+d)^{O(\\ell)}$ outputs $\\hat{\\beta}\\in\\mathbb{R}^{d}$ such that if $n\\,\\gtrsim\\,M^{4}\\cdot k^{4}\\log(d)/\\varepsilon^{3}$ \uff0c thenwith probability at least $1-d^{-10}$ \uff0c ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\|\\Sigma^{1/2}(\\hat{\\beta}-\\beta^{*})\\|\\leqslant O(M\\cdot\\sigma\\cdot\\varepsilon^{3/4})\\,.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In particular, in the regime $M\\leqslant O(1)$ , as long as $n\\geqslant{\\tilde{O}}(k^{4}/\\varepsilon^{3})$ , the algorithm recovers $\\beta^{*}$ from $(X,y)$ up to error $O(\\sigma\\varepsilon^{3/4})$ (with high probability). If $\\ell\\leqslant O(1)$ , the algorithm runs in polynomial time. Note that in this theorem we do not assume that $M$ is constant as opposed to Theorem 1.4 since for some natural classes of distributions, only some bounds on $M$ that depend on $d$ are known. ", "page_idx": 4}, {"type": "text", "text": "The natural question is what distributions have certifiably bounded fourth moment with $\\ell\\leqslant O(1)$ First, these are products of one-dimensional distributions with $M$ -bounded fourth moment, and their linear transformations (with $\\ell=4$ ). Hence, linear transformations of products of one-dimensional distributions with $O(1)$ -bounded 8-th moment satisfy the assumptions of the theorem with $M\\leqslant O(1)$ and $\\ell=4$ . Note that such distributions might not even have a 9-th moment. This class also includes Gaussian distributions (since they are linear transformations of the $N(0,1)^{d}$ and $N(0,1)$ has $O(1)$ bounded 8-th moment). ", "page_idx": 4}, {"type": "text", "text": "Another important class is the distributions that satisfy Poincare inequality. Concretely, these distributions, for some $C_{P}\\geqslant1$ ,satisfy $\\mathrm{Var}_{x\\sim\\mathcal{D}}\\,g(x)\\leqslant C_{P}^{\\tilde{2}}\\cdot\\|\\Sigma\\|\\cdot\\mathbb{E}_{x\\sim\\mathcal{D}}\\|\\nabla g(\\tilde{x})\\|_{2}^{2}$ for all continuously differentiable functions $g\\,:\\,\\mathbb{R}^{d}\\,\\rightarrow\\,\\mathbb{R}$ [KS17a] showed that such distributions have 4-certifiably $O(C_{P})$ -bounded fourth moment. We will not further discuss Poincare inequality, and focus on the known results on the classes of distributions satisfy this inequality. ", "page_idx": 4}, {"type": "text", "text": "The Kannan-Lovasz-Simonovits (KLS) conjecture from convex geometry says that $C_{P}$ is bounded by some universal constant for all log-concave distributions. Recall that a distribution $\\mathcal{D}$ is called log-concave if for some convex function $V:\\ensuremath{\\mathbb{R}^{d}}\\to\\ensuremath{\\mathbb{R}}$ , the density of $\\mathcal{D}$ is proportional to $e^{-V(x)}$ ", "page_idx": 4}, {"type": "text", "text": "Apart from the Gaussian distribution, examples include uniform distributions over convex bodies, the Wishart distribution and the Dirichlet distribution $\\langle\\mathrm{Pre}71]$ , see also [KBJoo] for further examples). In recent years there has been a big progrees towards the proof of the KLS conjecture. [Che21] showed that $C_{P}\\leqslant d^{o(1)}$ , and since then, the upper bound has been further significantly improved. The best current bound is $C_{P}\\,\\leqslant\\,O({\\sqrt{\\log d}})$ obtained by [Kla23]. This bound implies that for all log-concave distributions whose covariance has bounded condition number, the error of our estimator is $O(\\sigma{\\sqrt{\\log d}}\\cdot\\varepsilon^{3/4})$ . Hence for $\\varepsilon\\leqslant o(1/\\log^{2}(d))$ and $\\sigma\\leqslant O(1)$ , the error is $o({\\sqrt{\\varepsilon}})$ . Note that if the KLS conjecture is true, the error of our estimator is $O(\\sigma\\varepsilon^{3/4})$ for all log-concave distributions with $\\kappa(\\Sigma)\\leqslant O(1)$ , without any restrictions on $\\varepsilon$ (except the standard $\\varepsilon\\lesssim1$ ", "page_idx": 5}, {"type": "text", "text": "Remark 1.8. Theorem 1.7 can be generalized as follows: If the $(2t)$ -th moment of $\\mathcal{D}$ is $M$ -bounded for a constant $t\\in\\mathbb{N}_{\\geqslant2}$ , if this bound can be certified by a constant degree sum-of-squares proof? , and if $\\mathcal{D}$ has entrywise (4t)-th $O(1)$ -bounded moment, then with high probability, there is a poly $(d)$ -time computable estimator that achieves error $O(M\\sigma\\varepsilon^{1-1/(2t)})$ as long as $n\\gtrsim M^{4}k^{2t}\\log(d)/\\varepsilon^{2t-1}$ .See Theorem B.3 for more details. ", "page_idx": 5}, {"type": "text", "text": "Remark 1.9. The dependence of $n$ on $\\varepsilon$ can be improved under the assumption that $\\mathcal{D}$ is a subexponential distribution. In particular, all log-concave distributions are sub-exponential. Under this additional assumption, in order to achieve the error $O(\\sigma{\\sqrt{\\varepsilon}})$ , it is enough to take $n\\gtrsim k^{2}\\operatorname{polylog}(d)+$ $k\\log(d)/\\varepsilon$ , and to achieve error $O(M\\sigma\\varepsilon^{3/4})$ , it is enough to take $n\\gtrsim k^{4}\\operatorname{polylog}(d)+k\\log(d)/\\varepsilon^{3/2}$ samples (assuming, as in Theorem 1.7, that the fourth moment is $M$ -certifiably bounded). ", "page_idx": 5}, {"type": "text", "text": "1.1.3 Lower bounds ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We provide Statistical Query (SQ) lower bounds by which our estimators likely have optimal sample complexities needed to achieve the errors $O({\\sqrt{\\varepsilon}})$ and $o({\\sqrt{\\varepsilon}})$ , even when the design and the noise are Gaussian. SQ lower bounds are usually interpreted as a tradeoff between the time complexity and sample complexity of estimators; see Appendix $\\mathrm{G}$ and [DKS17] for more details. Our proofs are very similar to prior works [DKS17, DKS19, $\\mathrm{DKK}^{+}22]$ since as was observed in [DKS19], lower bounds for mean estimation can be used to prove lower bounds for linear regression, and we use the lower bounds for sparse mean estimation from [DKS17, $\\mathrm{DKK}^{+}22^{-}$ ", "page_idx": 5}, {"type": "text", "text": "Let us fix the scale of the noise $\\sigma=1$ . The first proposition shows that already for $\\Sigma=\\operatorname{Id}$ $k^{2}$ samples are likely to be necessary to achieve error $o(1)$ ", "page_idx": 5}, {"type": "text", "text": "Proposition 1.10 (Informal, see Proposition G.9). Let $n,d,k,X,y,\\varepsilon,\\mathcal{D},\\Sigma,\\sigma,\\beta^{*}$ be as in Definition 1.1. Suppose that ${\\mathcal D}\\,=\\,N(0,\\mathrm{Id})$ and $\\eta\\sim\\,N(0,\\tilde{\\sigma}^{2})^{n}$ , where $0.99\\,\\leqslant\\,\\tilde{\\sigma}\\,\\leqslant\\,1$ Suppose that $d^{0.01}\\leqslant k\\leqslant{\\sqrt{d}}$ $\\varepsilon\\gtrsim{\\frac{1}{\\sqrt{\\log d}}}$ and $n\\,\\leqslant k^{1.99}$ ThenforeachSQalgorithm $A$ that finds $\\hat{\\beta}$ such that $\\|\\beta^{*}-\\hat{\\beta}\\|\\leqslant10^{-5}$ the simulation of $A$ withn sampleshas to simulate super-polynomial $(\\exp(d^{\\Omega(1)}),$ number of queries. ", "page_idx": 5}, {"type": "text", "text": "Note that under assumptions of Proposition 1.10, Theorem 1.4 implies that if we take $n~\\geqslant$ $k^{2}$ polylog $(d)$ samples, the estimator achieves error $O({\\sqrt{\\varepsilon}})$ that is $o(1)$ if $\\varepsilon\\rightarrow0$ as $d\\rightarrow\\infty$ ", "page_idx": 5}, {"type": "text", "text": "The second proposition shows that for ${\\textstyle\\frac{1}{2}}\\leq\\Sigma\\leq\\operatorname{Id},k^{4}$ samples are likely to be necessary to achieve error $o({\\sqrt{\\varepsilon}})$ ", "page_idx": 5}, {"type": "text", "text": "Proposition 1.11 (Informal, see Proposition G.10). L $e t\\,n,d,k,X,y,\\varepsilon,\\mathcal{D},\\Sigma,\\sigma,\\beta^{*}$ be as in Definition 1.1. Suppose that $\\mathcal{D}=N(0,\\Sigma)$ for some $\\Sigma$ such that $\\begin{array}{r}{\\frac{1}{2}\\,\\leq\\,\\Sigma\\,\\leq\\,\\mathrm{Id},}\\end{array}$ and $\\eta\\sim N(0,\\tilde{\\sigma}^{2})^{n}$ , where $0.99\\leqslant\\tilde{\\sigma}\\leqslant1$ Suppose that $\\begin{array}{r}{d^{0.01}\\leqslant k\\leqslant\\sqrt{d},\\,\\varepsilon\\gtrsim\\frac{1}{\\log d}}\\end{array}$ and $n\\leqslant k^{3.99}$ Then for each S algorithm A that finds $\\hat{\\beta}$ such that $\\|\\beta^{*}-\\hat{\\beta}\\|\\leqslant10^{-5}\\sqrt{\\varepsilon}$ the simulation of $A$ with n samples has to simulate super-polynomial $(\\exp(d^{\\Omega(1)}))$ number of queries. ", "page_idx": 5}, {"type": "text", "text": "Note that under assumptions of Proposition 1.11, Theorem 1.7 implies that if we take $n~\\geqslant$ $k^{4}\\,\\mathrm{polylog}(d)$ samples, the estimator achieves error $O(\\varepsilon^{3/4})$ that is $o({\\sqrt{\\varepsilon}})$ if $\\varepsilon\\rightarrow0$ as $d\\to\\infty$ ", "page_idx": 5}, {"type": "text", "text": "2 Techniques ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Since the problem has multiple aspects, we first illustrate our approach on the simplest example $X_{i}^{*}\\overset{\\mathrm{iid}}{\\sim}N(0,\\Sigma)$ under the assumption that $0.1\\cdot\\mathrm{Id}\\leq\\Sigma\\leq10\\cdot\\mathrm{Id}$ . Note that already i this case, even for $\\eta\\sim N(0,1)^{n}$ , our estimator from Theorem 1.7 outperforms the state of the art. In addition, we assume that $\\sigma=1$ ", "page_idx": 6}, {"type": "text", "text": "Our estimators are based on preprocessing $X$ , and then minimizing $\\ell_{1}$ -penalized Huber loss. In the Gaussian case, the preprocessing step consists only of fltering, while for heavy-tailed designs, an additional truncation step is required. The idea of using filtering before minimizing the Huber loss first appeared in [PJL20] for the dense settings, and was applied to sparse settings in [Sas22, SF23]. We will not discuss the filtering method in detail, and rather focus on its outcome: It is a set ${\\hat{S}}\\subseteq[n]$ of size at least $(1-O(\\varepsilon))n$ that satisfies some nice properties\u00b0. Further, we will see what properties we need from $\\hat{S}$ , and now let us define the Huber loss estimator. ", "page_idx": 6}, {"type": "text", "text": "Definition 2.1. For $S\\subseteq[n]$ , the Huber loss function restricted to $S$ is defined as ", "page_idx": 6}, {"type": "equation", "text": "$$\nH_{S}(\\beta)=\\frac{1}{n}\\sum_{i\\in S}h(\\langle X_{i},\\beta\\rangle-y_{i})\\ \\mathrm{where}\\ h(x_{i})=\\left\\{\\begin{array}{l l}{\\frac{1}{2}x_{i}^{2}}&{\\mathrm{if}\\ |x_{i}|\\leqslant2;}\\\\ {2|x_{i}|-2}&{\\mathrm{otherwise}.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "For a penalty parameter $\\lambda$ , the $\\ell_{1}$ -penalized Huber loss restricted to $S$ is defined as $L_{S}(\\beta)\\;:=\\;$ $H_{S}(\\beta)\\,\\bar{+}\\,\\lambda\\cdot\\|\\bar{\\beta}\\|_{1}$ . We use the notation $\\phi(x)$ for the derivative of $h(x)$ . Note that for all $x$ \uff0c $|\\phi(x)|\\leqslant2$ ", "page_idx": 6}, {"type": "text", "text": "Our estimator is the minimizer $\\hat{\\beta}_{\\hat{S}}$ of $L_{\\hat{S}}(\\beta)$ , where $\\hat{S}$ is the set returned by the filtering algorithm. To investigate the properties of this estimator, it is convenient to work with elastic balls. The $k$ -elastic ball of radius $r$ is the following set: $\\mathcal{E}_{k}(r):=\\{u\\in\\mathbb{R}^{d}\\ |\\ \\|u\\|\\leqslant r\\,,\\|u\\|_{1}\\leqslant\\sqrt{k}\\cdot r\\}$ . Note that this ball contains all $k$ -sparse vectors with Euclidean norm at most $r$ (as well as some other vectors). Elastic balls are very useful for sparse regression since if the following two properties hold, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{u\\in\\mathcal{E}_{k}(r),\\quad|\\langle\\nabla H_{\\hat{S}},u\\rangle|\\lesssim\\frac{r}{\\sqrt{k}}\\|u\\|_{1}+r\\|u\\|\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "2. Strong convexity on the boundary: For all $u\\in\\mathcal{E}_{k}(r)$ such that $\\|u\\|=r$ ", "page_idx": 6}, {"type": "equation", "text": "$$\nH_{\\hat{S}}(\\beta^{*}+u)-H_{\\hat{S}}(\\beta^{*})-\\langle\\nabla H_{\\hat{S}},u\\rangle\\geqslant\\Omega\\big(r^{2}\\big)\\;,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "then for an appropriate choice of the penalty parameter $\\lambda$ , then $\\left\\|\\beta^{*}-\\hat{\\beta}_{\\hat{S}}\\right\\|<r$ 7. ", "page_idx": 6}, {"type": "text", "text": "Hence it is enough to show these two properties. In the Gaussian case, the strong convexity property can be proved in exactly the same way as it is done in $[\\mathrm{dLN}^{+}21]$ for the case of the oblivious adversary, while for heavy-tailed designs it is significantly more challenging. Since we now discuss the Gaussian case, let us focus on the gradient bound. Denote $\\begin{array}{r}{H_{S}^{*}(\\beta)=\\frac{\\bar{1}}{n}\\sum_{i\\in S}{h\\left(\\langle X_{i}^{*},\\beta\\rangle-y_{i}^{*}\\right)}}\\end{array}$ By triangle inequality, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\langle\\nabla H_{\\hat{S}},u\\rangle|=|\\langle\\nabla H_{S_{\\mathrm{good}}\\cap\\hat{S}}^{*},u\\rangle+\\langle\\nabla H_{S_{\\mathrm{bad}}\\cap\\hat{S}},u\\rangle|}\\\\ &{\\qquad\\qquad\\leqslant|\\langle\\nabla H_{[n]}^{*},u\\rangle|+|\\langle\\nabla H_{[n]\\backslash\\left(S_{\\mathrm{good}}\\cap\\hat{S}\\right)}^{*},u\\rangle|+|\\langle\\nabla H_{S_{\\mathrm{bad}}\\cap\\hat{S}},u\\rangle|\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Since the first term can be bounded by $\\|\\nabla H_{[n]}^{*}\\|_{\\infty}\\!\\cdot\\|u\\|_{1}$ it is enough to show that $\\|\\nabla H_{[n]}^{*}\\|_{\\infty}\\lesssim r/\\sqrt{k},$ where $r$ is the error we aim to achieve. Note that Z\"=1 \u03a6(ni)(X\\*,u) does not depend on the outier creatd by the adaptive adversary Th sharp boud on $\\|\\nabla H_{[n]}^{*}\\|_{\\infty}$ can be derived in exactly the same way as in $[\\mathrm{dLN}^{+}21]$ (or other prior works): Since $\\eta$ and $X^{\\ast}$ are independent and $|\\phi(\\eta)|\\leqslant2,\\nabla H_{[n]}^{*}$ is a Gaussian vector whose entries have variance $\\dot{(1/n)}$ . By standard properties of Gaussian vectors, $\\|\\nabla H_{[n]}^{*}\\|_{\\infty}\\leqslant O({\\sqrt{\\log(d)/n}})$ with high probability. ", "page_idx": 6}, {"type": "text", "text": "To bound the second and the third term, we can use Cauchy-Schwarz inequality and get $O({\\sqrt{\\varepsilon}})$ dependence on the error (like it is done in prior works on robust sparse regression, for example, [Sas22] or [SF23]), or use Holder's inequality and get better dependence, but also more challenges since we have to work with higher (empirical) moments of $X^{\\ast}$ and $X$ . Let us use Holder'sinequality and illustrate how we work with higher moments. Note that both sets $[n]\\setminus(S_{\\mathrm{good}}\\cap{\\hat{S}})$ and $S_{\\mathrm{bad}}\\cap{\\bar{\\hat{S}}}$ have size at most $O(\\varepsilon n)$ . Hence the second term can be bounded by ", "page_idx": 7}, {"type": "equation", "text": "$$\nO(\\varepsilon^{3/4})\\cdot\\big(\\sum_{i\\in[n]\\backslash\\big(S_{\\mathrm{good}}\\cap\\hat{S}\\big)}\\frac{1}{n}\\langle X_{i}^{*},u\\rangle^{4}\\big)^{1/4}\\leqslant O(\\varepsilon^{3/4})\\cdot\\big(\\sum_{i\\in[n]}\\frac{1}{n}\\langle X_{i}^{*},u\\rangle^{4}\\big)^{1/4}\\,,\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "while the third term is bounded by ", "page_idx": 7}, {"type": "equation", "text": "$$\nO(\\varepsilon^{3/4})\\cdot\\big(\\sum_{i\\in S_{\\mathrm{bad}}\\cap\\hat{S}}\\frac{1}{n}\\langle X_{i},u\\rangle^{4}\\big)^{1/4}\\leqslant O(\\varepsilon^{3/4})\\cdot\\big(\\sum_{i\\in\\hat{S}}\\frac{1}{n}\\langle X_{i},u\\rangle^{4}\\big)^{1/4}\\,.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "A careful probabilistic analysis shows that with high probability, for all $r\\geqslant0$ and all $u\\,\\in\\mathcal{E}_{k}(r)$ \uff0c $\\begin{array}{r}{\\sum_{i\\in[n]}\\frac{1}{n}\\langle X_{i}^{*},u\\rangle^{4}\\leqslant O\\big(\\|u\\|^{4}\\big)}\\end{array}$ . Hence, our requirement on $\\hat{S}$ is that $\\begin{array}{r}{\\sum_{i\\in\\hat{S}}\\frac{1}{n}\\langle X_{i},u\\rangle^{4}\\leqslant O(1)}\\end{array}$ for all $u\\,\\in\\mathcal{E}_{k}(1)$ (by scaling argument, it is enough to consider $r\\,=\\,1$ ). If we find such a set $\\hat{S}$ , we get the desired bound. Indeed, if $n\\gtrsim k\\log(d)/\\varepsilon^{3/2}$ \uff0c $\\|\\nabla H_{[n]}^{*}\\|_{\\infty}\\leqslant O(\\varepsilon^{3/4}/\\sqrt{k})$ and the other terms are bounded by $O(\\varepsilon^{3/4})$ , implying that $\\left\\|{\\hat{\\beta}}-{\\hat{\\beta}}_{\\hat{S}}\\right\\|<r={\\dot{O(\\varepsilon^{3/4})}}$ ", "page_idx": 7}, {"type": "text", "text": "Note that such sets of size $(1-O(\\varepsilon))n$ exist since $S_{\\mathrm{good}}$ satisfies this property. It is clear how to find such a set inefficiently: we just need to check all candidate sets $S$ and maximize the quartic function $\\textstyle\\sum_{i\\in S}\\langle X_{i},u\\rangle^{4}$ over $u\\in\\mathcal{E}_{k}(1)$ . Furthermore, the by-now standard filtering method allows to avoid checking all the sets: If we can maximize $\\textstyle\\sum_{i\\in S}\\langle X_{i},u\\rangle^{4}$ over $u\\in\\mathcal{E}_{k}(1)$ efficiently, we can also find the desired set efficiently. ", "page_idx": 7}, {"type": "text", "text": "Before explaining how we maximize this function, let us see how prior works [BDLS17, SF23], optimized a simpler quadratic function $\\textstyle\\sum_{i\\in S}\\langle X_{i},u\\rangle^{2}$ over $u\\ \\in\\,{\\mathcal{E}}_{k}(1)$ . They use the basic $S D P$ relaxation for sparse PCA, that is, they optimize the linear function $\\dot{\\sum}_{i\\in S}\\langle\\dot{X_{i}}X_{i}^{\\top},U\\rangle$ over $\\mathcal{B}_{k}:=$ $\\{U\\in\\mathbb{R}^{d\\times d}\\mid U\\,\\geq\\,0\\,,\\mathrm{Tr}(U)\\,\\leqslant\\,1\\,,\\|U\\|_{1}\\,\\leqslant\\,k\\}$ . This set has been used in literature for numerous sparse problems since it is a nice (perhaps the best) convex relaxation of the set $S_{k}=\\{u u^{\\top}\\mid u\\in$ $\\mathbb{R}^{d}\\,,\\,\\|u\\|\\leqslant1\\,,\\,\\|u\\|_{0}\\leqslant k\\}$ . Moreover, crucially for sparse regression, it is easy to see that $\\mathcal{B}_{k}$ also contains all matrices $u\\,u^{\\top}$ such that $u\\in\\mathcal{E}_{k}(1)$ . Hence, one may try to optimize quartic functions by using relaxations of $S_{k}\\,=\\,\\{u^{\\otimes4}\\ |\\ u\\,\\in\\mathbb{R}^{d}$ $\\|u\\|\\leqslant1$ \uff0c $\\|u\\|_{0}\\leqslant k\\}$ . A natural relaxation is the sumof-squares with sparsity constraints. $[\\mathrm{DKK}^{+}22]$ used these relaxations for sparse mean estimation8. They showed that these relaxations provide nice guarantees for distributions with certifiably bounded 4-th moment, assuming that the distribution has sub-exponential tails. Since we now discuss the Gaussian case, the assumption on the tails is satisfied. However, there is no guarantee that these relaxations capture $u^{\\otimes4}$ for all $u\\in\\mathcal{E}_{k}(1)$ . So, for sparse regression, we need another relaxation. ", "page_idx": 7}, {"type": "text", "text": "We use the sum-of-squares relaxations with elastic constraints. These constraints ensure that the set of relaxations $\\mathcal{P}_{k}\\,\\subset\\,\\mathbb{R}^{d^{4}}$ is guaranteed to contain $u^{\\otimes4}$ for all $u~\\in\\,{\\mathcal{E}}_{k}(1)$ . We show that if $n\\gtrsim\\tilde{O}(k^{4})$ , there is a degree- $O(1)$ sum-of-squares proof from the elastic constraints of the fact that $\\begin{array}{r}{\\frac{1}{n}\\sum_{i\\in[n]}\\langle X_{i},u\\rangle^{4}\\leqslant O(1)}\\end{array}$ It imlies tat therelaxationis nie:f $\\begin{array}{r}{\\frac{1}{n}\\sum_{i\\in S}\\langle X_{i},u\\rangle^{4}\\leqslant O(1)}\\end{array}$ forall $u\\in\\mathcal{E}_{k}(1)$ , then $\\begin{array}{r}{\\frac{1}{n}\\sum_{i\\in S}\\langle X_{i}^{\\otimes4},U\\rangle\\leqslant O(1)}\\end{array}$ for all $U\\in\\mathcal{P}_{k}$ . Since we can effciently optimize over $\\mathcal{P}_{k}$ we get an efficiently computable estimator with error $O(\\varepsilon^{3/4})$ for Gaussian distributions. Furthermore, if we first use a proper thresholding (that we discuss below), our sum-of-squares proof also works for heavy-tailed distributions, that, apart from the certifiably bounded 4-th moment (that we cannot avoid with the sum-of-squares approach), are only required to have entrywise bounded 8-th moment. ", "page_idx": 7}, {"type": "text", "text": "Robust sparse regression with heavy-tailed designs is much more challenging. Again, for simplicity assume that $0.1\\cdot\\mathrm{Id}\\,\\leq\\,\\Sigma\\,\\leq\\,10\\cdot\\mathrm{Id}$ and $\\sigma=1$ . First, there is an issue even without the adversarial noise: $\\|\\nabla H_{[n]}^{*}\\|_{\\infty}$ canbe very lare.Evenudebouded fourthmment autin, it ca hav magnitude ${\\tilde{O}}(d^{1/4}/n)$ , which is too large in the sparse setting. Hence we have to perform an additional thresholding step and remove large entries of $X$ . Usually thresholding of the design matrix should be done very carefully since it breaks the relation between $X$ and $y$ [Sas22] required the thresholding parameter $\\tau$ to be large enough and depend polynomially on $\\|\\beta^{*}\\|$ so that this dependence does not break significantly. Since $\\|\\nabla H_{[n]}^{*}\\|_{\\infty}$ can be as large as $\\tilde{O}(\\tau/n)$ , the sample complexity of their estimator also depends polynomially on $\\|\\beta^{*}\\|$ ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Our idea of thresholding is very different, and it plays a significant role in our analysis, especially in the proof of strong convexity. Since we already have to work with outliers chosen by the adaptive adversary, we know that for an $\\varepsilon$ -fraction of samples, the dependence of $y$ on $X$ can already be broken. So, if we choose the thresholding parameter $\\tau$ to be large enough so that with high probability it only affects an $\\varepsilon$ -fraction of samples, we can simply treat the samples affected by such thresholding as additional adversarial outliers, and assume that the adaptive adversary corrupted $2\\varepsilon n$ samples. Note that since $\\mathcal{D}$ is heavy-tailed, each sample $X_{i}^{*}$ might have entries of magnitude $\\overbar{d}^{\\Omega(1)}$ .However, $y$ depends only on the inner products $\\langle X_{i}^{*},\\beta^{*}\\rangle$ , and this inner product depends only on the entries of $X^{\\ast}$ that correspond to the support of $\\beta^{*}$ . Even though we don't know the support, we can guarantee that for $\\tau\\geqslant20\\sqrt{k/\\varepsilon}$ , all entries of $X_{i}$ from the support of $\\beta^{*}$ are bounded by $\\tau$ with probability $1-\\varepsilon/2$ Indeed, since the variance of each entry is bounded by 10, Chebyshev's inequality implies that this entry is smaller than $\\tau$ with probability at least $1-\\dot{\\varepsilon}/(2k)$ , and by union bound, $\\langle\\bar{X}_{i}^{*},\\beta^{*}\\rangle$ is not affected by the thresholding with probability $1-\\varepsilon/2$ . Hence by Chernoff bound, with overwhelming probability, the number of samples affected by our thresholding is at most $\\varepsilon n$ ", "page_idx": 8}, {"type": "text", "text": "Let us denote the distribution of the rows of $X^{\\ast}$ after thresholding with parameter $\\tau$ by $\\mathcal{D}(\\tau)$ . After the thresholding step, we can asume that $X_{i}^{*}\\stackrel{\\mathrm{iid}}{\\sim}\\mathcal{D}(\\tau)$ Note that thresholding can shift the mean, i.e. $\\mathbb{E}\\,X_{i}^{*}$ can be nonzero. It is easy to see that $\\|\\dot{\\mathbb{E}}_{x\\sim\\mathcal{D}(\\tau)}\\,x\\|_{\\infty}\\leqslant O(1/\\tau)$ . Hence by Bernstein's inequality, $\\|\\nabla\\dot{H}_{[n]}^{*}\\|_{\\infty}\\leqslant\\tilde{O}\\Big(\\sqrt{1/n}+\\tau/n+1/\\tau\\Big)$ with high probability9. In particular, in order to get the error bounded by $O(\\varepsilon^{3/4})$ , we need to take $\\tau\\gtrsim\\sqrt{k}/\\varepsilon^{3/4}$ , and it affects sample complexity. Furthermore, our sum-of-squares proof requires that $\\begin{array}{r}{\\left\\lVert\\frac{1}{n}\\sum_{i=1}^{n}\\left(\\boldsymbol{X}_{i}^{*}\\right)^{\\otimes4}-\\mathbb{E}\\!\\left(\\boldsymbol{X}_{1}^{*}\\right)^{\\otimes4}\\right\\rVert_{\\infty}}\\end{array}$ is smaller that $1/k^{2}$ . It can be shown that this quantity is bounded by ${\\tilde{O}}\\left({\\sqrt{1/n}}+\\tau^{4}/n+1/\\tau^{4}\\right)$ with high probability10. In particular, we need $n\\,\\geqslant\\,{\\tilde{O}}\\bigl(\\tau^{4}k^{2}\\bigr)$ , so for $\\tau\\,\\gtrsim\\,\\sqrt{k}/\\varepsilon^{3/4}$ , we have to take $n\\,\\geqslant\\,{\\tilde{O}}\\bigl(k^{4}/\\varepsilon^{3}\\bigr)$ .As was discussed in Remark 1.9, if $\\mathcal{D}$ has sub-exponential tails, we do not have to do the threshoiding, and the bounds from $[\\mathrm{DKK}^{+}22]$ allow to avoid this dependence of $n$ on $\\varepsilon$ . Note that due to the SQ lower bound (Proposition 1.11), sample complexity $k^{4}$ is likely to be necessary, even for Gaussian designs. Finally, let us discuss the strong convexity property. Here, we do not assume any properties related to sum-of-squares, and focus on the weak assumptions of Theorem 1.4. First, assume that we need to show strong convexity only for sparse vectors, and not for all $u\\in\\mathcal{E}_{k}(r)$ . As was observed in prior works on regression with oblivious outliers, e.g. $\\mathrm{[dLN^{+}21]}$ $\\rho(u):=H_{\\hat{S}}(\\beta^{\\ast}\\!+\\!u)\\!-\\!H_{\\hat{S}}(\\beta^{\\ast})-\\langle\\nabla H_{\\hat{S}}^{\\phantom{\\dagger}},u\\rangle$ can be lower bounded by $\\textstyle\\frac{1}{2}\\sum_{i\\in\\hat{S}}\\langle X_{i},u\\rangle^{2}\\mathbf{1}_{[|\\langle X_{i},u\\rangle-y_{i}|\\leqslant1]}\\mathbf{1}_{[|\\langle X_{i},u\\rangle|\\leqslant1]}$ Let $C(u)=S_{\\mathrm{good}}\\cap\\hat{S}\\cap A\\cap B(u)$ where $A$ is the set of samples where $|\\eta_{i}|\\;\\leqslant\\;1$ and $B(u)\\ =\\ \\{i\\ \\in\\ [n]\\ \\vert\\ \\vert\\langle X_{i},u\\rangle\\vert\\ \\leqslant\\ 1\\}$ . Then, $\\rho(u)\\geqslant\\Omega(\\sum_{i\\in C(u)}\\langle X_{i}^{*},u\\rangle^{2})$ . It can be shown that for some suitable $r$ and for each $k$ -sparse $u$ of norm $r$ $C(u)$ is a large subset of the set $A$ (of size at least $0.99|A|\\rangle$ . Note that since $A$ is independent of $X^{\\ast}$ , the rows of $X^{\\ast}$ that correspond to indices from $A$ are just id samples from $\\mathcal{D}$ .If $X_{i}^{*}$ were Gaussian, we could have applied concentration bounds and prove strong convexity via union bound argument over subsets of size $0.99|A|$ . In the heavy-tailed case, we need a different argument. For a fixed set $C$ of size $0.99|A|$ , we can use Bernstein's inequalityll . We cannot use union bound argument over all subsets of size $0.99|A|$ (there are too many), but fortunately we do not need it since for each $k$ sparse $u$ of norm $r$ , it is enough to show that $\\begin{array}{r}{\\sum_{i\\in T(u)}\\langle X_{i}^{*},u\\rangle^{2}\\geqslant\\overline{\\Omega}(r^{2})}\\end{array}$ where $T(u)\\subset A$ is the set of the smallest (in absolute value) $0.99|A|$ entries of the vector $X_{A}^{*}u\\in\\mathbb{R}^{|A|}$ . Hence, we can use an epsilon-net argument for the set of $k$ -sparse vectors $u$ (of norm $r$ ). This set has very dense nets of (relatively) small size, and this is enough to show the lower bound $\\begin{array}{r}{\\sum_{i\\in C(u)}\\langle X_{i}^{*},u\\rangle^{2}\\geqslant\\Omega(r^{2})}\\end{array}$ for all $k$ -sparse $u$ of norm $r$ with high probability, as long as $n\\geqslant\\tilde{O}(k^{2})$ ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "In order to show the same bound for all $u\\in\\mathcal{E}_{k}(r)$ of norm $r$ , we observe thatl2 if a quadratic form is $\\Theta(r^{2})$ on $K$ -sparse vectors of norm $r$ for some $K\\gtrsim k$ , then it is also $\\Theta(r^{2})$ on all $\\bar{\\boldsymbol{u}}\\in\\mathcal{E}_{k}(\\boldsymbol{r})$ , and applying the argument from the previous paragraph to $K$ -sparse vectors, we get the desired bound. We remark that directly proving it for $u\\in\\mathcal{E}_{k}(r)$ is challenging, since we extensively used the properties of the set of sparse vectors that are not satisfied by $\\mathcal{E}_{k}(r)$ , e.g. the existence of very dense epsilon-nets of small size. ", "page_idx": 9}, {"type": "text", "text": "3 Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "There is an interesting open problem in robust sparse regression that is not captured by our techniques. For sparse mean estimation, in the Gaussian case, there exists a polynomial time algorithm with nearly optimal guarantees: It achieves error $O(\\tilde{\\varepsilon})$ with $k^{4}\\,\\mathrm{polylog}(\\dot{d})/\\dot{\\varepsilon}^{2}$ samples ( $[\\mathrm{DKK}^{+}22]$ ). This algorithm uses a sophisticated sum-of-squares program13. It is reasonable to apply the techniques of $[\\mathrm{DKK}^{+}22]$ to robust sparse regression in order to achieve nearly optimal error $\\bar{O}(\\tilde{\\varepsilon})$ with poly $(k)$ samples. However, simple approaches (e.g. our approach with replacing the sparse constraints by the elastic constraints) fail in this case. Here we provide a high-level explanation of the issue. In order to combine the filtering algorithm with their techniques, we need to check whether the values of a certain quartic form are small on all sparse vectors. The analysis in $[\\mathrm{DKK}^{+}22]$ shows that this form is indeed small for the uncorrupted sample with high probability (see their Lemma E.2.). Since we want the filtering algorithm to be efficient, we have to use a relaxation of sparse vectors. Hence we need to find a sum-of-squares (or some other nice relaxation) version of the proof from $[\\mathrm{DKK}^{+}22]$ . However, in their proof they use a covering argument, and it is not clear how to avoid it. This argument fails for reasonable relaxations that we have thought about. Both potential outcomes (either an algorithm or a computational lower bound) are interesting: An algorithm would likely require new sophisticated ideas, and a lower bound would show a significant difference between robust sparse regression and robust mean estimation, while, so far, the complexity pictures of these problems have seemed to be quite similar. ", "page_idx": 9}, {"type": "text", "text": "Another interesting direction is to get error $o({\\sqrt{\\varepsilon}})$ for distributions that do not necessarily have certifiably bounded moments. As was shown in [HL19], only moment assumptions (without certifiability) are not enough for efficient robust mean estimation, and the same should be true also for linear regression. However, other assumptions on distribution $\\mathcal{D}$ can make the problem solvable in polynomial time. For robust mean estimation, some symmetry assumptions are enough even for heavy-tailed distributions without the second momentl4 (see [NST23]). It is interesting to investigate what assumptions on the design distribution are sufficient for existence of efficiently computable estimators for robust sparse regression. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Chih-Hung Liu is supported by Ministry of Education, Taiwan under Yushan Fellow Program with the grant number MOE-111-YSFEE-0003-006-P1 and by National Science and Technology Council, Taiwan with the grant number 111-2222-E-002-017-MY2. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[BB20] Matthew S. Brennan and Guy Bresler, Reducibility and statistical-computational gaps from secret leakage,Proceedings of the 33rd Annual Conference on Learning Theory (COLT), vol. 125, 2020, pp. 648-847.   \n[BDLS17] Sivaraman Balakrishnan, Simon S. Du, Jerry Li, and Aarti Singh, Computationally efficient robust sparse estimation in high dimensions, Proceedings of the 3Oth Conference on Learning Theory COLT 2017, 2017, pp. 169-212.   \n[BJK15] Kush Bhatia, Prateek Jain, and Purushottam Kar, Robust regression via hard thresholding, NIPS, 2015, pp. 721-729.   \n[BJKK17]  Kush Bhatia, Pratek Jain, Parameswaran Kamalaruban, and Purushottam Kar, Consistent robust regression, Advances in Neural Information Processing Systems (I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, eds.), vol. 30, Curran Associates, Inc., 2017.   \n[Che21] Yuansi Chen,Analmost constant lower bound of the isoperimetric coeficient in the ls conjecture, 2021.   \n$[\\mathrm{DKK}^{+}22]$ llias Diakonikolas, Daniel M. Kane, Sushrut Karmalkar, Ankit Pensia, and Thanasis Pittas, Robust sparse mean estimationvia sum of squares, Proceedings of the 35th Annual Conference on Learning Theory (COLT22), Proceedings of Machine Learning Research, 2022, Pp. 4703-4763.   \n[DKLP22] Iias Diakonikolas, Daniel Kane, Jasper C. H. Lee, and Ankit Pensia, Outlier-robust sparse mean estimation for heavy-tailed distributions, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022 (Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, eds.), 2022.   \n[dKNS20]  Tommaso d'Orsi, Pravesh K Kothari, Gleb Novikov, and David Steurer, Sparse pca: alorithms, adversarial perturbations and certifcates, 2020IEEE 61st Annual Symposium on Foundations of Computer Science (FOCS), IEEE, 2020, pp. 553-564.   \n[DKS17]  lias Diakonikolas, Daniel M Kane, and Alistair Stewart, Statistical query lower bounds for robust estimation of high-dimensional gaussians and gaussian mixtures, Proceedings of the IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS17), 2017, pp. 73-84.   \n[DKS19]  Ilias Diakonikolas, Weihao Kong, and Alistair Stewart, Efficient algorithms and lower bounds for robust linear regression, Proceedings of the 30th Annual Symposium on Discrete Algorithms (SODA19), 2019, pp. 2745-2754.   \n$[\\mathrm{dLN}^{+}21]$ Tommaso d'Orsi, Chih-Hung Liu, Rajai Nasser, Gleb Novikov, David Steurer, and Stefan Tiegel, Consistent estimation for pca and sparse regression with oblivious outliers, Advances in Neural Information Processing Systems 34 (2021), 25427-25438.   \n[dNS21] Tommaso d'Orsi, Gleb Novikov, and David Steurer, Consistent regression when oblivious outliers overwhelm, Proceedings of the 38th International Conference on Machine Learning, (ICML 2021) (Marina Meila and Tong Zhang, eds.), 2021, pp. 2297-2306.   \n[DT19] Arnak S. Dalalyan and Philip Thompson, Outlier-robust estimation of a sparse linear model using $\\ell_{1}$ -penalized huber's M-estimator, Proceedings of the 32nd Annual Conference on Neural Information Processing Systems (NeurIPS19), 2019, pp. 13188-13198.   \n[HL18] Samuel .Hopkins and Jerry Li, Mixture models, robustes, and sum of squares profs, Proceedings of the 5Oth Annual ACM SIGACT Symposium on Theory of Computing, STOC 2018, Los Angeles, CA, USA, June 25-29, 2018, 2018, pp. 1021-1034.   \n[HL19] , How hard is robust mean estimation?, Proceedings of the 32nd Annual Conference on Learning Theory (COLT19), 2019, pp. 1649-1682.   \n[KBJ00] Samuel Kotz, N. Balakrishnan, and Norman L. Johnson, Continuous multivariate distributions: Models and applications, Wiley, 2000.   \n[Kla23] Bo'az Klartag, Logarithmic bounds for isoperimetry and slices of convex sets, 2023.   \n[KMZ22] Pravesh K. Kothari, Peter Manohar, and Brian Hu Zhang, Polynomial-time sum-ofsquares can robustly estimate mean and covariance of gaussians optimally, Proceedings of The 33rd Intermational Conference on Algorithmic Learning Theory Sanjoy Dasgupta and Nika Haghtalab, eds.), Proceedings of Machine Learning Research, vol. 167, PMLR, 29 Mar-01 Apr 2022, pp. 638-667.   \n[KS17a]  Pravesh K. Kothari and Jacob Steinhardt, Better agnostic clustering via relaxed tensor norms, CoRR abs/1711.07465 (2017).   \n[KS17b]   Pravesh K. Kothari and David Steurer, Outlier-robust moment-estimation via sum-ofsquares, CoRR abs/1711.11581 (2017). Jean B. Lasserre,New positive semidefinite relaxations for nonconvex quadratic programs, Advances in convex analysis and global optimization (Pythagorion, 200o0), Nonconvex Optim. Appl., vol. 54, Kluwer Acad. Publ., Dordrecht, 2001, pp. 319-331. MR 1846160   \n[LSLC20] Liu Liu, Yanyao Shen, Tianyang Li, and Constantine Caramanis, High dimensional robust sparse regression, Proceedings of the 23rd International Conference on Artificial Intelligence and stics AISTATS 2020, 2020, pp. 411-421.   \n[MNW22] Stanislav Minsker, Mohamed Ndaoud, and Lang Wang, Robust and tuning-free sparse linear regression via square-root slope, arXiv preprint arXiv:2210.16808 (2022).   \n[Nes00] YuriNesterov,quardfuctional systemsandoptmiatnpoblems,Higherfomance optimization, Appl. Optim., vol. 33, Kluwer Acad. Publ., Dordrecht, 2000, pp. 405-440. MR 1748764   \n[NST23]   Gleb Novikov, David Steurer, and Stefan Tiegel, Robust mean estimation without moments for symmetric distributions, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023 (Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, eds.), 2023. Pablo A Parrilo, Structured semidefinite programs and semialgebraic geometry methods in robustness and optimization, Ph.D. thesis, California Institute of Technology, 2000. Ankit Pensia, Varun S. Jog, and Po-Ling Loh, Robust regression with covariate filteing: Heavy tails and adversarial contamination, CoRR abs/2009.12976 (2020).   \n[Pr\u00e971] Andras Prekopa, Logarithmic concave measures with application to stochastic programming, Acta Scientiarum Mathematicarum (1971), 301-316.   \n[RH23] Philippe Rigollet and Jan-Christian Hutter, High-dimensional statistics, 2023.   \n[Sas22] Takeyuki Sasai, Robust and sparse estimation of linear regression coeffcients with heavy-tailed noises and covariates, CoRR abs/2206.07594 (2022).   \n[SBRJ19]  Arun Sai Suggala, Kush Bhatia, Pradeep Ravikumar, and Prateek Jain, Adaptive hard thresholding for near-optimal consistent robust regression, Conference on Learning Theory, COLT 2019, 25-28 June 2019, Phoenix, AZ, USA (Alina Beygelzimer and Daniel Hsu, eds.), Proceedings of Machine Learning Research, vol. 99, PMLR, 2019, pp.2892-2897.   \n[SF23] Takeyuki Sasai and Hironori Fujisawa, Outlier robust and sparse estimation of linear regression coefficients, CoRR abs/2208.11592 (2023).   \n[Sho87] N. Z. Shor, Quadratic optimization problems, Izv. Akad. Nauk SSSR Tekhn. Kibernet. (1987), no. 1, 128-139, 222. MR 939596   \n[Tho23] Philip Thompson, Outlier-robust sparselow-rank least-squares regression and robust matrix completion, 2023.   \n[TJSO14] Efthymios Tsakonas, Joakim Jald\u00e9n, Nicholas D. Sidiropoulos, and Bjorn Ottersten, Convergence of the huber regression m-estimate in the presence of dense outliers, IEEE Signal Processing Letters 21 (2014), no. 10, 1211-1214.   \n[Tro15] Joel A. Tropp, An introduction to matrix concentration inequalities, Foundations and Trends in Machine Learning 8 (2015), no. 1-2, 1-230.   \n[ZWJ14]  Yuchen Zhang, Martin J. Wainwright, and Michael 1. Jordan, Lower bounds on the performance of polynomial-time algorithms for sparse linear regression, Proceedings of the 27th Annual Conference on Learning Theory (COLT14), 2014, pp. 921-948. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A  Properties of the Huber loss minimizer ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Defntion A.1. For $w\\in\\mathbb{R}_{\\geqslant0}^{n}$ wegHons ", "page_idx": 12}, {"type": "equation", "text": "$$\nH_{w}(\\beta)=\\sum_{i\\in[n]}w_{i}h(\\langle X_{i},\\beta\\rangle-y_{i}){\\mathrm{~where~}}h(x_{i})={\\left\\{\\begin{array}{l l}{{\\frac{1}{2}}x_{i}^{2}}&{{\\mathrm{if~}}|x_{i}|}\\\\ {2|x_{i}|-2}&{{\\mathrm{other}}}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "For a penalty parameter $\\lambda$ , the $\\ell_{1}$ -penalized Huber loss restricted to $S$ is defined as $L_{w}(\\beta)\\;:=\\;$ $H_{w}(\\beta)+\\lambda\\cdot\\|\\beta\\|_{1}$ ", "page_idx": 12}, {"type": "text", "text": "Lemma A.2. Suppose that $w\\in\\mathbb{R}_{\\geqslant0}^{n},\\,u\\in\\mathbb{R}^{d},\\,\\gamma_{1},\\gamma_{2},\\lambda>0$ satisfy the following properties: ", "page_idx": 12}, {"type": "text", "text": "$\\begin{array}{r}{I.\\ \\left|\\sum_{i=1}^{n}w_{i}\\phi(\\eta_{i}+\\zeta_{i})\\big\\langle X_{i}^{\\prime\\prime}(\\tau),u\\big\\rangle\\right|\\leqslant\\gamma_{1}\\|u\\|_{1}+\\gamma_{2}\\big\\|\\Sigma^{1/2}u\\big\\|\\,,}\\end{array}$ 2. $\\lambda\\geqslant2\\gamma_{1}$ 3. $\\begin{array}{r}{H_{w}(\\beta^{*}+u)+\\lambda\\cdot\\|\\beta^{*}+u\\|_{1}\\leqslant H_{w}(\\beta^{*})+\\lambda\\cdot\\|\\beta^{*}\\|_{1}\\,.}\\end{array}$ ", "page_idx": 12}, {"type": "text", "text": "Then ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\|u\\|_{1}\\leqslant\\left(4\\sqrt{k/\\sigma_{\\operatorname*{min}}}+2\\gamma_{2}/\\lambda\\right)\\cdot\\left\\|\\Sigma^{1/2}u\\right\\|,\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $\\sigma_{\\mathrm{min}}$ is the minimal eigenvalue of $\\Sigma$ ", "page_idx": 12}, {"type": "text", "text": "Proof. Let $\\mathcal{K}=\\operatorname{supp}(\\beta^{*})$ . Note that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\left\\|\\beta^{*}+u\\right\\|_{1}=\\left\\|\\beta^{*}+u_{\\overline{{\\mathcal{K}}}}+u_{\\mathcal{K}}\\right\\|_{1}\\geqslant\\left\\|\\beta^{*}\\right\\|_{1}+\\left\\|u_{\\overline{{\\mathcal{K}}}}\\right\\|_{1}-\\left\\|u_{\\mathcal{K}}\\right\\|_{1}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "By the convexity of $H_{w}$ ", "page_idx": 12}, {"type": "equation", "text": "$$\nH_{w}(\\beta^{*}+u)-H_{w}(\\beta^{*})\\geqslant-\\left|\\sum_{i=1}^{n}w_{i}\\phi(\\eta_{i}+\\zeta_{i})\\big\\langle X_{i}^{\\prime\\prime}(\\tau),u\\big\\rangle\\right|\\geqslant-\\lambda\\|u\\|_{1}/2-\\gamma_{2}\\Big\\|\\Sigma^{1/2}u\\Big\\|.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Hence ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{0\\geqslant\\lambda\\cdot\\left(\\|\\beta^{*}+u\\|_{1}-\\|\\beta^{*}\\|_{1}\\right)+H_{w}(\\eta+\\zeta+X u)-H_{w}(\\eta+\\zeta)}\\\\ &{\\quad\\geqslant\\lambda\\cdot\\left(\\left\\|u_{\\overline{{\\mathcal{K}}}}\\right\\|_{1}-\\|u_{\\mathcal{K}}\\|_{1}\\right)-\\frac{1}{2}\\lambda\\cdot\\left\\|u_{\\mathcal{K}}\\right\\|_{1}-\\frac{1}{2}\\lambda\\cdot\\left\\|u_{\\overline{{\\mathcal{K}}}}\\right\\|_{1}-\\gamma_{2}}\\\\ &{\\quad\\geqslant\\frac{1}{2}\\lambda\\cdot\\left\\|u_{\\overline{{\\mathcal{K}}}}\\right\\|_{1}-\\frac{3}{2}\\lambda\\|u_{\\mathcal{K}}\\|_{1}-\\gamma_{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Therefore, ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! \n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Theorem A.3. Let $\\rho,\\gamma_{1},\\gamma_{2}>0$ and ", "page_idx": 12}, {"type": "equation", "text": "$$\nr=100\\cdot\\left(\\frac{\\lambda\\sqrt{k/\\sigma_{\\mathrm{min}}}}{\\rho}+\\frac{\\gamma_{2}}{\\rho}\\right),\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $\\sigma_{\\mathrm{min}}$ is the minimal eigenvalue of $\\Sigma$ Let $k^{\\prime}\\geqslant100k/\\sigma_{\\mathrm{min}}$ Consider the $k^{\\prime}$ -elastic ellipsoid of radius $r$ ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathcal{E}_{k^{\\prime}}(r)=\\left\\{u\\in\\mathbb{R}^{d}\\;\\Big|\\;\\|\\Sigma^{1/2}u\\|\\leqslant r\\,,\\|u\\|_{1}\\leqslant\\sqrt{k^{\\prime}}\\cdot r\\right\\}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Suppose that the weights $w\\in\\mathbb{R}^{n}$ are such that the following two properties hold: ", "page_idx": 12}, {"type": "text", "text": "1. Gradient bound: For all $u\\in\\mathcal{E}_{k^{\\prime}}(r)$ ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\left|\\sum_{i=1}^{n}w_{i}\\phi(\\eta_{i}+\\zeta_{i})\\big\\langle X_{i}^{\\prime\\prime}(\\tau),u\\big\\rangle\\right|\\leqslant\\gamma_{1}\\|u\\|_{1}+\\gamma_{2}\\Big\\|\\Sigma^{1/2}u\\Big\\|,,\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "2. Strong convexity on the boundary: For all $u\\in\\mathcal{E}_{k^{\\prime}}(r)$ such that $\\left\\|\\Sigma^{1/2}u\\right\\|=r$ ", "page_idx": 13}, {"type": "equation", "text": "$$\nH_{w}(\\beta^{*}+u)-H_{w}(\\beta^{*})\\geqslant-\\left|\\sum_{i=1}^{n}w_{i}\\phi(\\eta_{i}+\\zeta_{i})\\big\\langle X_{i}^{\\prime\\prime}(\\tau),u\\big\\rangle\\right|+\\rho\\cdot r^{2}\\,.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Let ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\lambda\\geqslant2\\gamma_{1}+\\gamma_{2}\\cdot\\sqrt{\\frac{\\sigma_{\\operatorname*{min}}}{k}}\\,.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Then the minimizer $\\hat{\\beta}$ of the weighted penalized Huber loss with penalty $\\lambda$ and weights w satisfies ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\Sigma^{1/2}\\left(\\hat{\\beta}-\\beta^{*}\\right)\\right\\|<r\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof. Let $\\hat{u}=\\hat{\\beta}-\\beta^{*}$ . If $\\left\\|\\Sigma^{1/2}\\hat{u}\\right\\|<r$ , we get the desired bound. Otherwise, let $u$ be the (unique) point in the intersection of $\\partial\\mathcal{E}_{k^{\\prime}}(r)$ and the segment $[0,\\hat{u}]\\subset\\mathbb{R}^{d}$ . By convexity of the penalized loss, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{H_{w}(\\eta+\\zeta+X u)+\\lambda\\cdot\\|\\beta^{*}+u\\|_{1}\\leqslant H_{w}(\\eta+\\zeta)+\\lambda\\cdot\\|\\beta^{*}\\|_{1}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Since $u\\in\\partial\\mathcal{E}_{k^{\\prime}}(\\boldsymbol{r})$ either $\\left\\|\\Sigma^{1/2}u\\right\\|=r$ or $\\|u\\|_{1}=\\sqrt{k^{\\prime}}\\cdot r$ Let us show that the latter is not possible. Since $\\lambda\\geqslant2\\gamma_{1}$ , we can apply Lemma A.2: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\sqrt{k^{\\prime}}\\cdot r=\\left(4\\sqrt{k/\\sigma_{\\mathrm{min}}}+2\\gamma_{2}/\\lambda\\right)\\cdot r\\,.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Cancelling r and using the bound \u5165 \u2265 2 \u00b7 \u221a $\\begin{array}{r}{\\lambda\\geqslant\\gamma_{2}\\cdot\\sqrt{\\frac{\\sigma_{\\operatorname*{min}}}{k}}}\\end{array}$ , we get a contradiction. Hence $\\left\\|\\Sigma^{1/2}u\\right\\|=r$ By the strong convexity and the gradient bound, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{H_{w}(\\beta^{*}+u)-H_{w}(\\beta^{*})\\geqslant-\\left|\\displaystyle\\sum_{i=1}^{n}w_{i}\\phi(\\eta_{i}+\\zeta_{i})\\big\\langle X_{i}^{\\prime\\prime}(\\tau),u\\big\\rangle\\right|+\\rho\\cdot\\left\\|\\Sigma^{1/2}u\\right\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\geqslant\\rho\\cdot r^{2}-\\frac{1}{2}\\lambda\\cdot\\|u\\|_{1}-\\gamma_{2}\\Big\\|\\Sigma^{1/2}u\\Big\\|}\\\\ &{\\qquad\\qquad=\\rho\\cdot r^{2}-\\frac{1}{2}\\lambda\\cdot\\|u\\|_{1}-\\gamma_{2}r\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Note that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{H_{w}(\\beta^{*}+u)-H_{w}(\\beta^{*})\\leqslant\\lambda\\cdot\\left(\\|\\beta^{*}\\|_{1}-\\|\\beta^{*}+u\\|_{1}\\right)\\leqslant\\lambda\\|u\\|_{1}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "By putting the above two inequality together and by Lemma A.2 , we have that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\rho\\cdot r^{2}\\leqslant\\frac{3}{2}\\lambda\\|u\\|_{1}+\\gamma_{2}r\\leqslant6\\lambda\\sqrt{k/\\sigma_{\\operatorname*{min}}}\\cdot r+5\\gamma_{2}r\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Dividing both sides by $\\rho\\cdot r$ , we get ", "page_idx": 13}, {"type": "equation", "text": "$$\nr<100\\cdot\\left(\\frac{\\lambda\\sqrt{k/\\sigma_{\\mathrm{min}}}}{\\rho}+\\frac{\\gamma_{2}}{\\rho}\\right),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "a contradiction. Therefore, $\\left\\|\\Sigma^{1/2}\\hat{u}\\right\\|<r$ ", "page_idx": 13}, {"type": "text", "text": "B Heavy-tailed Designs ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "First, we define a bit more general model than Definition 1.1 ", "page_idx": 13}, {"type": "text", "text": "Definition B.1 (Robust Sparse Regression with 2 Adversaries). Let $n,d,k\\in\\mathbb{N}$ such that $k\\leqslant d$ $\\sigma>0$ \uff0c $\\alpha\\in(0,1]$ and $\\varepsilon\\lesssim\\alpha$ . Let $\\mathcal{D}$ be a probability distribution in $\\mathbb{R}^{d}$ with mean O and covariance ", "page_idx": 13}, {"type": "text", "text": "$\\Sigma$ Let $y^{*}=X^{*}\\beta^{*}+\\eta$ where $X$ is an $n\\times d$ random matrix with rows $X_{i}^{*}\\overset{\\mathrm{iid}}{\\sim}\\mathcal{D},\\beta^{*}\\in\\mathbb{R}^{d}$ .s $k$ sparse, $\\eta\\in\\mathbb{R}^{n}$ is independent of $X^{\\ast}$ and has at least $\\alpha\\cdot n$ entries bounded by $\\sigma$ in absolute value15 . ", "page_idx": 14}, {"type": "text", "text": "An instance of our model is a pair $(X,y)$ , where $X\\in\\mathbb{R}^{n\\times d}$ is a matrix and $y\\in\\mathbb{R}^{n}$ is a vector such that there exists a set $S_{\\mathrm{good}}\\subseteq[n]$ of size at least $(1-\\varepsilon)n$ such that for all $i\\in S_{\\mathrm{good}}$ \uff0c $X_{i}=X_{i}^{*}$ and $y_{i}=y_{i}^{*}$ ", "page_idx": 14}, {"type": "text", "text": "Definition B.2. Let $M>0$ $t\\,\\in\\mathbb{N}$ , and let $\\ell\\geqslant2t$ be an even number. We say that a probability distribution $\\mathcal{D}$ in $\\mathbb{R}^{d}$ with zero mean and covariance $\\Sigma$ has $\\ell$ -certifiably $M$ -bounded $(2t)$ -th moment, if there exist polynomials $h_{1},\\ldots,h_{m}\\in\\mathbb{R}[u_{1},\\ldots,u_{d}]$ of degree at most $\\ell/2$ such that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\times\\!\\sim\\mathcal{D}}\\left\\langle x,u\\right\\rangle^{2t}+\\sum_{i=1}^{m}h_{i}^{2}(u)=M^{2t}\\cdot\\left\\Vert\\Sigma\\right\\Vert^{t}\\cdot\\left\\Vert u\\right\\Vert^{2t}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "In this section we prove the following theorem ", "page_idx": 14}, {"type": "text", "text": "Theorem B.3 (Heavy-tailed designs, general formulation). L $\\textit{o t n},d,k,X,y,\\varepsilon,\\mathcal{D},\\Sigma,\\sigma,\\alpha$ beas in Definition B.1, and let $\\delta\\in(0,1)$ ", "page_idx": 14}, {"type": "text", "text": "Suppose that for some $s>2,\\,t\\in\\mathbb{N},\\,M_{s},\\,M_{2t}\\geqslant1$ ,andevennumber $\\ell\\geqslant2t$ $\\mathcal{D}$ has $M_{s}$ -bounded s-th moment,and $\\ell$ -certifiably $M_{2t}$ -bounded $\\left(2t\\right)$ -th moment. In addition, $\\mathcal{D}$ has entrywise $\\nu$ -bounded (4t)-th moment. ", "page_idx": 14}, {"type": "text", "text": "There exists an algorithm that, given $X,y,k,\\varepsilon,\\sigma,M_{2t},\\ell,t,\\delta$ and $\\hat{\\sigma}_{\\mathrm{max}}$ such that $\\|\\Sigma\\|\\,\\leqslant\\,\\hat{\\sigma}_{\\operatorname*{max}}\\,\\leqslant$ $O(||\\Sigma||)$ intime $(n+d)^{O(\\ell)}$ outputs $X^{\\prime}\\in\\mathbb{R}^{n\\times d}$ and weights $\\boldsymbol{w}=(w_{1},\\dots,w_{n})$ such that $i f$ ", "page_idx": 14}, {"type": "equation", "text": "$$\nn\\gtrsim\\frac{10^{10t}\\Big(M_{2t}^{2t}\\cdot\\nu^{4t}+\\big(10^{5}M_{s}\\big)^{\\frac{2s}{s-2}}\\Big)\\cdot\\Big(\\kappa(\\Sigma)^{4+s/(s-2)}+\\kappa(\\Sigma)^{2t}\\Big)}{\\varepsilon^{2t-1}}\\cdot k^{2t}\\log(d/\\delta)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "then with probability at least $1-\\delta$ ,the weighted $\\ell_{1}$ -penalizedHuber loss estimator $\\hat{\\beta}_{w}=\\hat{\\beta}_{w}(X^{\\prime},y)$ with weights $w$ (as in Definition 2.1) and parameter $h$ satisfies ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left\\|\\Sigma^{1/2}\\Big(\\widehat{\\beta}_{w}-\\beta^{*}\\Big)\\right\\|\\leqslant O\\left(\\frac{M_{2t}\\sqrt{\\kappa(\\Sigma)}}{\\alpha}\\cdot\\sigma\\cdot\\varepsilon^{1-\\frac{1}{2t}}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Let us explain how this result implies Theorem 1.4 and Theorem 1.7. ", "page_idx": 14}, {"type": "text", "text": "Theorem 1.4 is a special case of Theorem B.3 with $t=1$ \uff0c $s=3$ $\\ell=2$ $M_{2t}=1$ \uff0c $M_{s}=M$ \uff0c $\\alpha=0.01$ Indeed, we only need to estimate $\\|\\Sigma\\|$ up to a constant factor. We can do it by estimating the variance of the first coordinate of $x\\sim\\mathcal{D}$ . Applying median-of-means algorithm16 to the first coordinate, we get an estimator $\\widetilde{\\sigma}^{2}$ that is $O(\\nu^{2}\\|\\Sigma\\|\\sqrt{\\varepsilon})$ -close to the variance of the first coordinate $\\sigma_{1}^{2}$ . Note that $\\|\\Sigma\\|/\\kappa(\\Sigma)\\leqslant\\sigma_{1}^{2}\\leqslant\\|\\Sigma\\|$ . Since in Theorem $1.4\\;\\kappa(\\Sigma)$ and $\\nu$ are constants, and $\\varepsilon$ is sufficiently small, we get that $\\begin{array}{r}{\\frac{1}{2\\kappa(\\Sigma)}\\|\\Sigma\\|\\leqslant\\tilde{\\sigma}_{\\mathrm{max}}^{2}\\leqslant2\\|\\Sigma\\|}\\end{array}$ Hence for a constant $C\\geqslant\\kappa(\\Sigma)$ $\\hat{\\sigma}_{\\mathrm{max}}=2C\\tilde{\\sigma}^{2}$ is the dsired estimator of $\\|\\Sigma\\|$ ", "page_idx": 14}, {"type": "text", "text": "Similarly, Theorem 1.7 is a special case of Theorem B.3 with $t\\,=\\,2$ $s\\;=\\;4$ $M_{2t}\\,=\\,M_{s}\\,=\\,M$ $\\alpha=0.01.\\;\\|\\Sigma\\|$ can be estimated using the procedure described above. ", "page_idx": 14}, {"type": "text", "text": "Before proving the theorem, note that we can without loss of generality assume that $\\sigma=1$ . Indeed, since $\\sigma$ is known, we can simply divide $X$ and $y$ by it before applying the algorithm. ", "page_idx": 14}, {"type": "text", "text": "B.1 Truncation ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We cannot work with $X^{\\ast}$ directly since it might have very large values, and Bernstein inequality that we use for random vectors concentration would give very bad bounds if we work with $X^{\\ast}$ .Fortunately, we can perform truncation. This technique was used in [DKLP22] for sparse mean estimation and in [Sas22] for sparse regression. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "For $\\tau\\,>\\,0$ $X_{i j}^{\\prime}(\\tau)\\,=\\,X_{i j}^{*}\\mathbf{1}_{\\left[|X_{i j}^{*}|\\leqslant\\tau\\right]}.$ Note tha ine $\\mathbb{P}\\bigg[|X_{i j}^{*}|>\\tau\\bigg]\\,\\leqslant\\,\\|\\Sigma\\|/\\tau^{2}$ $\\tau\\gtrsim\\sqrt{\\|\\Sigma\\|k/\\varepsilon},$ then the number of entries $i$ where $\\left\\langle X_{i}^{*},\\beta^{*}\\right\\rangle\\neq\\left\\langle X_{i}^{\\prime}(\\tau),\\beta^{*}\\right\\rangle$ is at most $\\varepsilon n$ with probability at least $1-2^{-\\varepsilon n/10}\\geqslant1-\\delta/10.$ Hence in the algorithm we assume that the input is $X^{\\prime}(\\tau)$ instead of $X^{\\ast}$ , and we treat the entries where $\\left\\langle X_{i}^{*},\\beta^{*}\\right\\rangle\\neq\\left\\langle\\bar{X}_{i}^{\\prime}(\\tau),\\beta^{*}\\right\\rangle$ as corrupted by an adversary. ", "page_idx": 15}, {"type": "text", "text": "Concretely further we asume that we are given $\\left\\{\\left(X_{i}^{\\prime\\prime}(\\tau),y_{i},w_{i}\\right)\\right\\}_{i=1}^{n}$ such that $y=X^{\\prime}(\\tau)\\beta^{*}+\\eta+\\zeta$ where $X^{\\prime\\prime}(\\tau)\\in\\mathbb{R}^{n\\times d}$ differs from $X^{\\prime}(\\tau)\\in\\mathbb{R}^{n\\times d}$ only in rows from the set $S_{\\mathrm{bad}}\\,\\subset\\,[n]$ of size at most ${\\widetilde{\\varepsilon}}n$ (where $\\varepsilon\\,\\leqslant\\,\\tilde{\\varepsilon}\\,\\leqslant O(\\varepsilon))$ \uff0c $\\zeta\\in\\mathbb{R}^{n}$ is an $\\tilde{\\varepsilon}n$ -sparse vector such that $\\operatorname{supp}(\\zeta)\\subseteq S_{\\mathrm{bad}},\\,\\beta^{*}\\in\\mathbb{R}^{d}$ a $k$ -sparse vector, and $\\eta\\in\\mathbb{R}^{n}$ is oblivious noise such that at least $\\alpha n$ entries do not exceed 1 in absolute value. ", "page_idx": 15}, {"type": "text", "text": "In addition, we define ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{W}_{\\tilde{\\varepsilon}}=\\left\\{w\\in\\mathbb{R}^{n}\\;\\bigg|\\;\\forall i\\in[n]\\;\\;0\\leqslant w_{i}\\leqslant1/n,\\;\\sum_{i=1}^{n}w_{i}\\geqslant(1-\\tilde{\\varepsilon})n\\right\\}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The weights for the Huber loss will be from $\\mathbf{\\mathcal{W}}_{\\tilde{\\varepsilon}}$ ", "page_idx": 15}, {"type": "text", "text": "Appendix $\\boldsymbol{\\mathrm E}$ will discuss more properties of the truncation. ", "page_idx": 15}, {"type": "text", "text": "B.2 Gradient Bound ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Lemma B.4. Let $b,\\gamma_{1}>0$ Suppose that $w\\in\\mathscr{W}_{\\tilde{\\varepsilon}}$ and $u\\in\\mathbb{R}^{d}$ satisfy ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{i\\in[n]}w_{i}\\big\\langle X_{i}^{\\prime\\prime}(\\tau),u\\big\\rangle^{2t}\\leqslant b^{2t}\\cdot\\left\\|\\Sigma^{1/2}u\\right\\|^{2t},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\,\\sum_{i\\in[n]}\\left\\langle X_{i}^{\\prime}(\\tau),u\\right\\rangle^{2t}\\leqslant b^{2t}\\cdot\\left\\|\\Sigma^{1/2}u\\right\\|^{2t},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left|\\frac{1}{n}\\sum_{i\\in[n]}\\phi(\\eta_{i})X_{i}^{\\prime}(\\tau)\\right|_{\\infty}\\leqslant\\gamma_{1}\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left|\\sum_{i=1}^{n}w_{i}\\phi(\\eta_{i}+\\zeta_{i})\\bigl\\langle X_{i}^{\\prime\\prime}(\\tau),u\\bigr\\rangle\\right|\\leqslant\\gamma_{1}\\cdot\\|u\\|_{1}+6\\cdot b\\Big\\|\\Sigma^{1/2}u\\Big\\|^{2t}\\cdot\\tilde{\\varepsilon}^{1-\\frac{1}{2t}}\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. Denote $\\begin{array}{r}{F(w)=\\sum_{i\\in[n]}(1/n-w_{i})\\phi(\\eta_{i})\\big\\langle X_{i}^{\\prime}(\\tau),u\\big\\rangle}\\end{array}$ It is a linear function of $\\boldsymbol{w}$ $|F(w)|$ is maximized in one of the vertices of the polytope $\\mathbf{\\mathcal{W}}_{\\tilde{\\varepsilon}}$ . This vertex corresponds to set $S_{w}$ of size at least $(1-\\tilde{\\varepsilon})n$ . That is, the weights of the entries from $S_{w}$ are $1/n$ , and outside of $S_{w}$ the weights are zero. It follows that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\left\\vert\\sum_{i=1}^{n}w_{i}\\phi(\\eta_{i}+\\zeta_{i})\\big\\langle X_{i}^{\\prime\\prime}(\\tau),u\\big\\rangle\\right\\vert}\\\\ &{\\leqslant\\displaystyle\\left\\vert\\sum_{i\\in[n]}w_{i}\\phi(\\eta_{i})\\big\\langle X_{i}^{\\prime}(\\tau),u\\big\\rangle\\right\\vert+\\displaystyle\\left\\vert\\sum_{i\\in S_{\\mathrm{bad}}}w_{i}\\phi(\\eta_{i})\\big\\langle X_{i}^{\\prime}(\\tau),u\\big\\rangle\\right\\vert+\\displaystyle\\left\\vert\\sum_{i\\in S_{\\mathrm{bad}}}w_{i}\\phi(\\eta_{i}+\\zeta_{i})\\big\\langle X_{i}^{\\prime\\prime}(\\tau),u\\big\\rangle\\right\\vert}\\\\ &{\\leqslant\\gamma_{1}\\cdot\\|u\\|_{1}+2\\displaystyle\\sum_{i\\in S_{w}}\\frac{1}{n}\\big\\vert\\big\\langle X_{i}^{\\prime}(\\tau),u\\big\\rangle\\big\\vert+2\\displaystyle\\sum_{i\\in S_{\\mathrm{bad}}}\\frac{1}{n}\\big\\vert\\big\\langle X_{i}^{\\prime}(\\tau),u\\big\\rangle\\big\\vert+2\\displaystyle\\sum_{i\\in S_{\\mathrm{bad}}}w_{i}\\big\\vert\\big\\langle X_{i}^{\\prime\\prime}(\\tau),u\\big\\rangle\\big\\vert}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "(Holder's inequality) ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leqslant\\gamma_{1}\\cdot\\|u\\|_{1}+4\\cdot\\tilde{\\varepsilon}^{1-\\frac{1}{2t}}\\cdot\\left(\\sum_{i\\in[n]}\\frac{1}{n}\\big\\langle X_{i}^{\\prime}(\\tau),u\\big\\rangle^{2t}\\right)^{\\frac{1}{2t}}+2\\tilde{\\varepsilon}^{1-\\frac{1}{2t}}\\cdot\\left(\\sum_{i\\in[n]}w_{i}\\big\\langle X_{i}^{\\prime\\prime}(\\tau),u\\big\\rangle^{2t}\\right)^{\\frac{1}{2t}}}\\\\ &{\\leqslant\\gamma_{1}\\cdot\\|u\\|_{1}+6\\cdot\\tilde{\\varepsilon}^{1-\\frac{1}{2t}}\\cdot b\\left\\|\\sum^{1/2}u\\right\\|^{2t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The following lemma provides a bound on $\\gamma_{1}$ ", "page_idx": 16}, {"type": "text", "text": "Lemma B.5. With probability at least $1-\\delta/10,$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left\\|\\frac{1}{n}\\sum_{i\\in[n]}\\phi(\\eta_{i})X_{i}^{\\prime}(\\tau)\\right\\|_{\\infty}\\leqslant10\\sqrt{\\|\\Sigma\\|n\\log(d/\\delta)}+10\\tau\\cdot\\log(d/\\delta)+2n\\cdot\\|\\Sigma\\|/\\tau\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. It follows from Bernstein's inequality Fact I.1 and the fact that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{n}\\displaystyle\\sum_{i\\in[n]}\\lvert\\phi(\\eta_{i})\\rvert\\cdot\\big\\rvert\\mathbb{E}\\,X_{i}^{\\prime}(\\tau)\\big\\rvert\\leqslant2\\,\\mathbb{E}\\,X_{1}^{\\prime}(\\tau)\\leqslant2n\\cdot\\|\\Sigma\\|/\\tau\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where we used Corollary E.4. ", "page_idx": 16}, {"type": "text", "text": "B.2.1 Strong Convexity ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Lemma B.6. Suppose that $\\alpha\\geqslant1000\\tilde{\\varepsilon},\\,\\tau\\gtrsim1000\\cdot\\nu^{2}\\cdot\\|\\Sigma\\|\\sqrt{k^{\\prime\\prime}}/\\big(r\\sqrt{\\sigma_{\\mathrm{min}}}\\big)$ and ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{n\\gtrsim\\big((k^{\\prime\\prime})^{2}\\log d+k^{\\prime\\prime}\\log(1/\\delta)\\big)10^{5s/(s-2)}M_{s}^{s/(s-2)}\\kappa(\\Sigma)^{2+s/(s-2)}/\\alpha\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $k^{\\prime\\prime}=10^{4}\\cdot k^{\\prime}\\cdot\\sqrt{\\|\\Sigma\\|}$ Then with probability $1-\\delta/10,$ for all $u\\in\\mathcal{E}_{k^{\\prime}}(r)$ such that $\\left\\|\\Sigma^{1/2}u\\right\\|=r$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{H(\\beta^{*}+u)-H(\\beta^{*})\\geqslant-\\left|\\displaystyle\\sum_{i=1}^{n}w_{i}\\phi(\\eta_{i}+\\zeta_{i})\\langle X_{i},u\\rangle\\right|+\\frac{1}{4}\\cdot r^{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. Denote $A_{\\mathrm{good}}=S_{\\mathrm{good}}\\cap\\mathcal{R}$ , where $\\mathcal{A}$ is a set of entries $i$ such that $|\\eta_{i}|\\leqslant1$ . Note that $\\mathcal{A}$ is independent of $X^{\\breve{*}}$ . It follows that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{H(\\beta^{*}+u)-H(\\beta^{*})-\\displaystyle\\sum_{i=1}^{n}w_{i}\\phi(\\eta_{i}+\\zeta_{i})\\langle X_{i},u\\rangle\\geqslant\\frac{1}{2}\\displaystyle\\sum_{i=1}^{n}w_{i}\\langle X_{i},u\\rangle^{2}\\mathbf{1}_{[|\\eta_{i}+\\zeta_{i}|\\leqslant1]}\\mathbf{1}_{[\\langle X_{i},u\\rangle|\\leqslant1]}}&{}\\\\ {\\geqslant\\frac{1}{2}\\displaystyle\\sum_{i\\in S_{\\mathrm{god}}}w_{i}\\big\\langle X_{i}^{\\prime}(\\tau),u\\big\\rangle^{2}\\mathbf{1}_{[|\\eta_{i}|\\leqslant1]}\\mathbf{1}_{[\\langle X_{i}^{\\prime}(\\tau),u\\rangle]\\leqslant1}\\big\\}}&{}\\\\ {\\geqslant\\frac{1}{2}\\displaystyle\\sum_{i\\in A_{\\mathrm{god}}}w_{i}\\big\\langle X_{i}^{\\prime}(\\tau),u\\big\\rangle^{2}\\mathbf{1}_{[\\langle X_{i}^{\\prime}(\\tau),u\\rangle|\\leqslant1]}}&{}\\\\ {\\geqslant\\frac{1}{2}\\displaystyle\\sum_{i\\in A_{\\mathrm{god}}}w_{i}\\big\\langle\\tilde{X}_{i},u\\big\\rangle^{2}\\mathbf{1}_{[\\langle\\tilde{X}_{i},u\\rangle|\\leqslant1]}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\tilde{X}_{i}=\\mathbf{1}_{\\left[\\left\\Vert X_{i}^{\\prime}(\\tau)\\right\\Vert\\leqslant10^{5s/(s-2)}\\cdot M_{s}^{s/(s-2)}\\sqrt{\\|\\Sigma\\|\\cdot k^{\\prime\\prime}}\\right]}X_{i}^{\\prime}(\\tau).$ ", "page_idx": 16}, {"type": "text", "text": "Denote $\\begin{array}{r}{F(w)=\\sum_{i\\in A_{\\mathrm{good}}}w_{i}\\big\\langle\\tilde{X}_{i},u\\big\\rangle^{2}\\mathbf{1}_{\\left[|\\left\\langle\\tilde{X}_{i},u\\right\\rangle|\\leqslant1\\right]}}\\end{array}$ It is a linear fntion of $w$ itis maximized in one of the vertices of the polytope $\\mathbf{\\mathcal{W}}_{\\tilde{\\varepsilon}}$ . This vertex corresponds to set $S_{w}$ of size at least $(1-\\tilde{\\varepsilon})n$ That is, the weights of the entries from $S_{w}$ are $1/n$ , and outside of $S_{w}$ the weights are zero. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{i\\in A_{\\mathrm{good}}}w_{i}\\big\\langle\\tilde{X}_{i},u\\big\\rangle^{2}\\mathbf{1}_{\\left[|\\left\\langle\\tilde{X}_{i},u\\right\\rangle|\\leqslant1\\right]}\\geqslant\\frac{1}{n}\\sum_{i\\in A_{\\mathrm{good}}\\cap S_{w}}\\big\\langle\\tilde{X}_{i},u\\big\\rangle^{2}\\mathbf{1}_{\\left[|\\left\\langle\\tilde{X}_{i},u\\right\\rangle|\\leqslant1\\right]}\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Hence we need a lower bound for $\\textstyle\\sum_{i\\in A(u)}\\bigl\\langle\\tilde{X}_{i},u\\bigr\\rangle^{2}$ , where ", "page_idx": 17}, {"type": "equation", "text": "$$\nA(u)=A_{\\mathrm{good}}\\cap S_{w}\\cap\\left\\{i\\in[n]\\;\\big|\\;|\\big\\langle\\tilde{X}_{i},u\\big\\rangle|\\leqslant1\\right\\}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "In order to bound $\\textstyle\\sum_{i\\in A(u)}\\bigl\\langle\\tilde{X}_{i},u\\bigr\\rangle^{2}$ for vectors $u$ from the elastic ball $\\mathcal{E}_{k^{\\prime}}(r)$ , we first show that it is bounded for $k^{\\prime\\prime}$ -sparse vectors $u^{\\prime}$ for some large enough $k^{\\prime\\prime}$ . First we need to show that $\\sum_{i\\in\\mathcal{I}}\\langle\\tilde{X}_{i},u^{\\prime}\\rangle^{2}$ is well-concentrated for a fixed set $\\boldsymbol{\\mathcal{I}}$ . Concretely, we need the following lemma: ", "page_idx": 17}, {"type": "text", "text": "Lemma B.7. Suppose that $\\tau\\geqslant1000\\cdot M_{2t}\\cdot\\nu^{2}\\cdot\\|\\Sigma\\|\\sqrt{k^{\\prime\\prime}}/\\big(r\\sqrt{\\sigma_{\\mathrm{min}}}\\big)$ for some $k^{\\prime\\prime}\\in\\mathbb{N}$ Then for $a$ fixed (independent of $\\tilde{X}$ ) set $\\boldsymbol{\\mathcal{I}}$ of size ", "page_idx": 17}, {"type": "equation", "text": "$$\n|\\bar{I}|\\gtrsim\\left((k^{\\prime\\prime})^{2}\\log d+k^{\\prime\\prime}\\log(1/\\delta)\\right)10^{10s/(s-2)}M_{s}^{2s/(s-2)}\\kappa(\\Sigma)^{2+2s/(s-2)}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and for all $k^{\\prime\\prime}$ -sparse vectors $u^{\\prime}\\in\\mathbb{R}^{d}$ such that $r\\leqslant\\|\\Sigma^{1/2}u^{\\prime}\\|\\leqslant2r$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n0.99\\cdot\\Vert\\Sigma^{1/2}u^{\\prime}\\Vert^{2}\\leqslant\\frac{1}{|\\mathcal{I}|}\\sum_{i\\in\\mathcal{I}}\\bigl\\langle\\tilde{X}_{i},u^{\\prime}\\bigr\\rangle^{2}\\leqslant1.01\\cdot\\Vert\\Sigma^{1/2}u^{\\prime}\\Vert^{2}\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "with probability at least $1-\\delta$ ", "page_idx": 17}, {"type": "text", "text": "Proof. First let us show that ", "page_idx": 17}, {"type": "equation", "text": "$$\n0.995\\cdot\\mathbb{E}\\!\\left\\langle X_{i}^{*},u^{\\prime}\\right\\rangle^{2}\\leqslant\\mathbb{E}\\!\\left\\langle\\tilde{X}_{i},u^{\\prime}\\right\\rangle^{2}\\leqslant1.005\\cdot\\mathbb{E}\\!\\left\\langle X_{i}^{*},u^{\\prime}\\right\\rangle^{2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Since for each set $\\mathcal{K}$ of size $\\begin{array}{r}{k^{\\prime\\prime},\\,\\mathbb{E}\\Big\\|\\big(X_{i}^{\\prime}(\\tau)\\big)_{\\mathcal{K}}\\Big\\|^{2}\\,=\\,\\sum_{j\\in\\mathcal{K}}\\mathbb{E}\\Big(X_{i j}^{\\prime}(\\tau)\\Big)^{2}\\,\\leqslant\\,2\\|\\Sigma\\|k^{\\prime\\prime}}\\end{array}$ , by Markov's inequality, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left[\\left\\lVert\\left(X_{i}^{\\prime}(\\tau)\\right)_{\\mathcal{K}}\\right\\rVert^{2}>10^{10s/(s-2)}\\cdot M_{s}^{2s/(s-2)}\\cdot\\kappa(\\Sigma)^{s/(s-2)}\\rVert\\Sigma\\rVert\\cdot k^{\\prime\\prime}\\right]\\leqslant\\frac{1}{10^{10s/(s-2)}\\cdot M_{s}^{2s/(s-2)}\\cdot\\kappa(\\Sigma)^{s/(s-2)}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Denote $B\\,=\\,10^{5s/(s-2)}\\cdot M_{s}^{s/(s-2)}\\cdot\\kappa(\\Sigma)^{s/(2s-4)}\\sqrt{\\|\\Sigma\\|\\cdot k^{\\prime\\prime}}$ ByHlers qltal $u^{\\prime}\\in\\mathbb{R}^{d}$ with support $\\mathcal{K}$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{B}\\big\\langle X_{i}^{\\prime}(\\tau),u^{\\prime}\\big\\rangle^{2}=\\mathbb{B}\\langle X_{i}(\\tau),u^{\\prime}\\rangle^{2}\\mathbf{1}[\\|(X_{i}^{\\prime}(\\tau))_{\\mathcal{K}}\\|\\leqslant B\\big\\langle X_{i}^{\\prime}(\\tau),u^{\\prime}\\big\\rangle^{2}\\mathbf{1}[\\|(X_{i}^{\\prime}(\\tau))_{\\mathcal{K}}\\|>\\!B\\big]}\\\\ &{\\qquad\\qquad\\leqslant\\mathbb{B}\\big\\langle\\tilde{X}_{i},u^{\\prime}\\big\\rangle^{2}+2\\,\\mathbb{B}\\big\\langle X_{i}^{*},u^{\\prime}\\big\\rangle^{2}\\mathbf{1}[\\|(X_{i}^{\\prime}(\\tau))_{\\mathcal{K}}\\|>\\!B\\big]+2\\,\\mathbb{B}\\big\\langle X_{i}^{\\prime}(\\tau)-X_{i}^{*},u^{\\prime}\\big\\rangle^{2}}\\\\ &{\\qquad\\qquad\\leqslant\\mathbb{B}\\big\\langle\\tilde{X}_{i},u^{\\prime}\\big\\rangle^{2}+2\\,\\Big(\\mathbb{B}\\mathbf{1}[\\|(X_{i}^{\\prime}(\\tau))_{\\mathcal{K}}\\|>\\!B\\big]\\Big)^{-\\frac{2}{\\delta}}\\cdot\\Big(\\mathbb{B}\\big\\langle X_{i}^{*},u^{\\prime}\\big\\rangle^{\\delta}\\Big)^{\\frac{2}{\\delta}}+\\frac{2\\,\\nu^{4}\\|\\Sigma\\|^{2}\\,k^{\\prime\\prime}\\|u^{\\prime}\\|^{2}}{\\tau^{2}}}\\\\ &{\\qquad\\qquad\\leqslant\\mathbb{B}\\big\\langle\\tilde{X}_{i},u^{\\prime}\\big\\rangle^{2}+2\\,\\frac{\\|\\Sigma\\|\\cdot\\|u\\|^{2}}{10^{10}\\cdot\\kappa(\\Sigma)}+2\\,r^{2}/10^{6}}\\\\ &{\\qquad\\qquad\\leqslant\\mathbb{B}\\big\\langle\\tilde{X}_{i},u^{\\prime}\\big\\rangle^{2}+2\\,r^{2}/10^{10}\\cdot\\kappa(\\Sigma)+2\\,r^{2}/10^{6}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where we used Lemma E.1 and the fact that $\\|u^{\\prime}u^{\\prime\\top}\\|_{1}\\leqslant k^{\\prime\\prime}\\|u^{\\prime}u^{\\prime\\top}\\|\\leqslant k^{\\prime\\prime}\\|u\\|^{2}$ . By Corollary E.5, $\\mathbb{E}\\big<X_{i}^{*},u^{\\prime}\\big>^{2}-2r^{2}/10^{6}\\leqslant\\cdot\\mathbb{E}\\big<X_{i}^{\\prime}(\\tau),u^{\\prime}\\big>^{2}\\leqslant\\mathbb{E}\\big<X_{i}^{*},u^{\\prime}\\big>^{2}+2r^{2}/10^{6}.$ Hence ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{1.995\\cdot\\mathbb{B}\\big<X_{i}^{*},u^{\\prime}\\big>^{2}\\leqslant0.999\\cdot\\mathbb{B}\\big<X_{i}^{\\prime}(\\tau),u^{\\prime}\\big>^{2}\\leqslant\\mathbb{B}\\big<\\tilde{X}_{i},u^{\\prime}\\big>^{2}\\leqslant1.001\\cdot\\mathbb{B}\\big<X_{i}^{\\prime}(\\tau),u^{\\prime}\\big>^{2}\\leqslant1.005\\cdot\\mathbb{B}\\big<X_{i}^{*},u^{\\prime}\\big>^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "For a fixed set $\\mathcal{K}$ of size $k^{\\prime\\prime}$ and for all unit vectors $u^{\\prime}\\in\\mathbb{R}^{d}$ with support $\\mathcal{K}$ , by Bernstein inequality for covariance Fact I.2, with probability $1-\\delta$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\bigg|\\frac{1}{|T|}\\sum_{i\\in{\\cal I}}\\bigl\\langle\\tilde{X}_{i},u^{\\prime}\\bigr\\rangle^{2}-\\mathbb{E}\\bigl\\langle\\tilde{X}_{i},u^{\\prime}\\bigr\\rangle^{2}\\bigg|\\leqslant1000\\cdot\\left(\\sqrt{\\frac{\\|\\Sigma\\|B^{2}\\log(d/\\delta)}{|{\\cal I}|}}+\\frac{B^{2}\\log(d/\\delta)}{|{\\cal I}|}\\right)\\cdot\\|u^{\\prime}\\|^{2}}\\\\ &{}&{\\leqslant4000\\cdot\\left(\\sqrt{\\frac{\\|\\Sigma\\|B^{2}\\log(d/\\delta)}{\\sigma_{\\operatorname*{min}}^{2}|{\\cal I}|}}+\\frac{B^{2}\\log(d/\\delta)}{\\sigma_{\\operatorname*{min}}|{\\cal I}|}\\right)\\cdot r^{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "In order to make this quantity smaller than $r^{2}/1000$ \uff0cit is sufficient to take $|{\\cal Z}|\\quad\\gtrsim$ $10^{10s/(s-2)}M_{s}^{2s/(s-2)}\\kappa(\\Sigma)^{2+s\\bar{/}(s-2)}\\cdot\\bar{k}^{\\prime\\prime}\\log(d/\\delta)$ ", "page_idx": 18}, {"type": "text", "text": "By union bound over all subsets $\\mathcal{K}$ of $[d]$ of size $k^{\\prime\\prime}$ , we get the desired bound. ", "page_idx": 18}, {"type": "text", "text": "Let us bound the size of $A(u).\\;A_{\\mathrm{good}}\\cap S_{w}$ has size at least $(\\alpha-3\\tilde{\\varepsilon})n\\geqslant0.997\\alpha n$ By Lemma B.7 and Lemma F.2, $\\lVert\\tilde{X}u\\rVert^{2}\\,\\leqslant\\,1.1\\cdot\\alpha n r^{2}$ , hence at most $3\\alpha n r^{2}/h\\,\\leqslant\\,0.001\\alpha n$ entries of $\\tilde{X}u$ can be greater than $h/2$ . Therefore, $|A(u)|\\geqslant0.99\\alpha n$ ", "page_idx": 18}, {"type": "text", "text": "Let $k^{\\prime\\prime}\\;=\\;\\operatorname*{min}\\Bigl\\{\\lceil10^{4}k^{\\prime}\\rceil|\\Sigma||\\rceil,d\\Bigr\\}$ . Recall that $\\mathcal{A}$ a set of entries $i$ such that $\\lvert\\eta_{i}\\rvert\\ \\leqslant\\ 1$ , and $\\mathcal{A}$ is independent of ${\\dot{X}}^{*}$ . By union bound, the result of Lemma B.7 also holds for all sets $\\boldsymbol{\\mathcal{I}}$ that correspond to the bottom 0.99-fraction of entries of vectors $\\left(\\tilde{X}u^{\\prime\\prime}\\right)_{\\mathcal{R}}$ , where $u^{\\prime\\prime}$ are from an $\\left(1/n^{10}\\right)$ -net $\\boldsymbol{\\cal N}$ in the set of all $k^{\\prime\\prime}$ -sparse vectors $u^{\\prime}$ such that $\\left\\Vert\\Sigma^{1/2}u^{\\prime}\\right\\Vert=1.01r$ Let $u^{\\prime}$ be an arbitrary $k^{\\prime\\prime}$ -sparse vector such that $\\left\\Vert\\Sigma^{1/2}u^{\\prime}\\right\\Vert=1.01r$ , and let $u^{\\prime\\prime}=u^{\\prime}+\\Delta u$ be the closest vector in the net $\\boldsymbol{\\cal N}$ 10 $u^{\\prime}$ It fllows that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{i\\in A(u)}\\left\\langle\\tilde{X}_{i},u^{\\prime}\\right\\rangle^{2}=\\displaystyle\\sum_{i\\in A(u)}\\left\\langle\\tilde{X}_{i},u^{\\prime\\prime}+\\Delta u\\right\\rangle^{2}}&{}\\\\ {\\displaystyle\\geqslant\\displaystyle\\sum_{i\\in A(u)}\\left\\langle\\tilde{X}_{i},u^{\\prime\\prime}\\right\\rangle^{2}-2n^{3}/n^{10}}&{}\\\\ {\\displaystyle}&{\\geqslant0.99\\alpha n\\cdot r^{2}-2n^{-7}}&{}\\\\ {\\displaystyle}&{\\geqslant0.9\\alpha n\\cdot r^{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "If $k^{\\prime\\prime}=d$ , we get the desired bound, since we can take $u^{\\prime}=u$ . Otherwise, by Lemma B.7, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{i\\in A(u)}\\left\\langle\\tilde{X}_{i},u^{\\prime}\\right\\rangle^{2}\\leqslant\\sum_{i\\in\\mathcal{A}}\\left\\langle\\tilde{X}_{i},u^{\\prime}\\right\\rangle^{2}\\leqslant1.1\\cdot\\alpha n\\cdot r^{2}\\,,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and we get the desired bound by Lemma F.2 ", "page_idx": 18}, {"type": "text", "text": "B.3  Putting everything together ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "First, we truncate the entries of $X$ and $X^{\\ast}$ and obtain $X^{\\prime\\prime}(\\tau)$ and $X^{\\prime}(\\tau)$ using some $\\tau$ such that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\tau\\gtrsim M_{2t}\\sqrt{\\lVert\\Sigma\\rVert}\\cdot\\nu^{2}\\cdot\\sqrt{k^{\\prime\\prime}}/\\varepsilon^{1-\\frac{1}{2t}}\\;,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $k^{\\prime\\prime}\\,=\\,10^{6}\\cdot k\\cdot\\kappa(\\Sigma)$ We discuss the choice of $\\tau$ further in this subsection. Let us denote $\\tau^{\\prime}=\\tau/\\sqrt{\\|\\Sigma\\|}$ ", "page_idx": 18}, {"type": "text", "text": "Then we find the weights $w_{1},\\dots w_{n}$ using Algorithm C.1. ", "page_idx": 18}, {"type": "text", "text": "We will show all the conditions of Theorem A.3 are satisfied if ", "page_idx": 18}, {"type": "equation", "text": "$$\nn\\geqslant C\\cdot\\frac{10^{10t}\\left(M_{2t}^{2t}\\cdot\\nu^{4t}+\\left(10^{5}M_{s}\\right)^{\\frac{2s}{s-2}}\\right)\\cdot\\left(\\kappa(\\Sigma)^{4+s/(s-2)}+\\kappa(\\Sigma)^{2t}\\right)}{\\varepsilon^{2t-1}}\\cdot k^{2t}\\log(d/\\delta)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for some large enough absolute constant $C$ and ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\lambda=1000\\cdot M_{2t}\\sqrt{\\hat{\\sigma}_{\\operatorname*{max}}}\\cdot\\varepsilon^{1-1/(2t)}/\\sqrt{k}\\geqslant1000\\cdot\\frac{M_{2t}\\sqrt{\\kappa(\\sqrt{\\Sigma})}\\cdot\\varepsilon^{1-1/(2t)}}{\\sqrt{k/\\sigma_{\\operatorname*{min}}}}\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "First let us show that the assumptions of Lemma B.4 are satisfied with $\\gamma_{1}\\;\\leqslant\\;100\\cdot M_{2t}{\\sqrt{\\|{\\boldsymbol{\\Sigma}}\\|}}$ $\\varepsilon^{1-1/(2t)}/\\sqrt{k}$ and $\\gamma_{2}\\leqslant10M_{2t}\\sqrt{\\kappa(\\Sigma)}$ ", "page_idx": 18}, {"type": "text", "text": "First we bound $\\gamma_{2}$ . Note that if $u\\in\\mathcal{E}_{k^{\\prime}}(\\boldsymbol{r})$ for $k^{\\prime}=100k/\\sigma_{\\mathrm{min}}$ , then $\\|u\\|_{1}\\leqslant k^{\\prime\\prime}\\|u\\|$ . Hence if ", "page_idx": 18}, {"type": "equation", "text": "$$\nn\\geqslant1000\\Bigl(\\nu^{4t}\\cdot(k^{\\prime\\prime})^{t}+(\\tau^{\\prime})^{2t}\\Bigr)\\cdot(k^{\\prime\\prime})^{t}\\cdot t\\log(d/\\delta)\\,,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "then Lemma D.2 implies that for all $u\\in\\mathcal{E}_{k^{\\prime}}(r)$ , with probability $1-\\delta/10$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{n}\\displaystyle\\sum_{i\\in[n]}\\left<X_{i}^{\\prime}(\\tau),u\\right>^{2t}\\leqslant\\left(2M_{2t}\\sqrt{\\|\\Sigma\\|}\\right)^{2t}\\cdot\\|u\\|^{2t}\\leqslant\\left(2M_{2t}\\sqrt{\\kappa(\\Sigma)}\\right)^{2t}\\cdot\\left\\|\\Sigma^{1/2}u\\right\\|^{2t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Lemma D.2 and Lemma C.2 imply that for all $u\\in\\mathcal{E}_{k^{\\prime}}(\\boldsymbol{r})$ , with probability $1-\\delta/10.$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{i\\in[n]}w_{i}\\big\\langle X_{i}^{\\prime\\prime}(\\tau)(\\tau),u\\big\\rangle^{2t}\\leqslant\\left(2M_{2t}\\sqrt{\\kappa(\\Sigma)}\\right)^{t}\\cdot\\left\\|\\Sigma^{1/2}u\\right\\|^{2t}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Let us bound $\\gamma_{1}$ . By Lemma B.5, if ", "page_idx": 19}, {"type": "equation", "text": "$$\nn\\geqslant1000\\Bigl(k\\log(d/\\delta)/\\varepsilon^{2-1/t}+\\tau\\log(d/\\delta)\\sqrt{k^{\\prime}}/\\varepsilon^{1-1/(2t)}\\Bigr)\\;,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "thenby with probabilty 1-/10, \u2264100.M ", "page_idx": 19}, {"type": "text", "text": "The strong convexity holds by Lemma B.6 with probability $1-\\delta/10$ as long as ", "page_idx": 19}, {"type": "equation", "text": "$$\nn\\gtrsim\\left(k^{2}\\log(d/\\delta)\\right)10^{5s/(s-2)}M_{s}^{s/(s-2)}\\kappa(\\Sigma)^{4+s/(s-2)}/\\varepsilon\\,,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where we used th fact that $\\varepsilon\\lesssim\\alpha$ and that $\\tau\\gtrsim\\sqrt{\\|\\Sigma\\|}\\cdot\\nu^{2}\\cdot\\sqrt{k^{\\prime\\prime}}/\\varepsilon^{1-\\frac{1}{2t}}$ satisfies the assumption of that lemma. ", "page_idx": 19}, {"type": "text", "text": "Therefore, all the conditions of Theorem A.3 are satisfied and we attain the desired bound of $\\begin{array}{r}{O\\left(\\frac{M_{2t}\\sqrt{\\kappa(\\Sigma)}}{\\alpha}\\cdot\\varepsilon^{1-\\frac{1}{2t}}\\right)}\\end{array}$ stated in Theorem B.3. ", "page_idx": 19}, {"type": "text", "text": "Now let us discuss the choice of $\\tau$ . First we can find an estimator $\\hat{\\kappa}$ of $\\kappa(\\Sigma)$ by plugging it into the formula ", "page_idx": 19}, {"type": "equation", "text": "$$\nn=C\\cdot\\frac{10^{10t}\\cdot\\left(\\hat{\\kappa}^{4+s/(s-2)}+\\hat{\\kappa}^{2t}\\right)}{\\varepsilon^{2t-1}}\\cdot k^{2t}\\log(d/\\delta).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then we can take $\\begin{array}{r}{\\tau^{\\prime}=0.01\\cdot\\left(\\frac{n}{\\hat{\\kappa}^{t}\\cdot k^{t}\\cdot t\\log(d/\\delta)}\\right)^{1/(2t)}}\\end{array}$ . Note that if we express $n$ in terms of $\\hat{\\kappa}$ and plug into the formula for $\\tau^{\\prime}$ , we get that $\\tau^{\\prime}$ is an increasing function of $\\hat{\\kappa}$ .Also note that $\\hat{\\kappa}\\geqslant\\kappa(\\Sigma)$ . Hence both conditions are satisfied: $\\tau:=\\sqrt{\\hat{\\sigma}_{\\operatorname*{max}}}\\cdot\\tau^{\\prime}$ is larger than the required lower bound for it, and $n$ is larger than $10000(\\tau^{\\prime})^{2t}\\cdot(k^{\\prime\\prime})^{t}\\log(d/\\delta)$ and $10000\\tau\\log(d/\\delta)\\sqrt{k^{\\prime}}/\\varepsilon^{1-1/(2t)}$ as required. ", "page_idx": 19}, {"type": "text", "text": "C Filtering ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We use the following system of elastic constraints with sparsity parameter $K\\geqslant1$ and variables $v_{1},\\dots,v_{d},s_{1},\\dots,s_{d}$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathscr{R}_{K}:\\left\\{\\begin{array}{c c}{\\displaystyle\\forall i\\in[d]}&{\\displaystyle s_{i}^{2}=1}\\\\ {\\displaystyle\\forall i\\in[d]}&{\\displaystyle s_{i}v_{i}\\geqslant v_{i}}\\\\ {\\displaystyle\\forall i\\in[d]}&{\\displaystyle s_{i}v_{i}\\geqslant-v_{i}}\\\\ &{\\displaystyle\\sum_{i=1}^{d}v_{i}^{2}\\leqslant1}\\\\ &{\\displaystyle\\sum_{i=1}^{d}s_{i}v_{i}\\leqslant\\sqrt{K}\\right\\}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Note that the vectors from the elastic ball $\\left\\{v\\in\\mathbb{R}^{d}\\;\\Big|\\;\\|v\\|\\leqslant1\\,,\\;\\|v\\|_{1}\\leqslant\\sqrt{K}\\right\\}$ satisfy these constraints with $s_{i}=\\mathrm{sign}(v_{i})$ . We will later discuss the corresponding sum-of-squares certificates in Appendix D Let $a>0$ be such that $\\begin{array}{r}{\\left\\langle\\frac{1}{n}\\sum_{i=1}^{n}\\left(X_{i}^{*}\\right)^{\\otimes2t},\\tilde{\\mathbb{E}}v^{\\otimes2t}\\right\\rangle\\leqslant a^{2t}.}\\end{array}$ ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "Algorithm C.1 (Filtering algorithm).   \n1. Assign weights $w_{1}=\\dots w_{n}=1/n$   \n2. Find a degree $2\\ell$ pseudo-expectation $\\tilde{\\mathbb{E}}$ that satisfies $\\mathcal{A}_{K}$ and maximizes $\\left\\langle\\sum_{i=1}^{n}w_{i}X_{i}^{\\otimes t},\\tilde{\\mathbb{E}}v^{\\otimes t}\\right\\rangle$   \n3. If $\\begin{array}{r}{\\left\\langle\\frac{1}{n}\\sum_{i=1}^{n}X_{i}^{\\otimes2t},\\tilde{\\mathbb{E}}v^{\\otimes2t}\\right\\rangle<10^{t}a^{2t}}\\end{array}$ , stop.   \n4. Compute $\\tau_{i}=\\left\\langle X_{i}^{\\otimes2t},\\tilde{\\mathbb{E}}v^{\\otimes2t}\\right\\rangle$ and reweight: $\\begin{array}{r}{w_{i}^{\\prime}=\\big(1-\\frac{\\tau_{i}}{\\|\\tau\\|_{\\infty}}\\big)\\cdot w_{i}}\\end{array}$   \n5. goto 2. ", "page_idx": 20}, {"type": "text", "text": "Lemma C.2. If at each step $\\begin{array}{r}{\\left\\langle\\frac{1}{n}\\sum_{i=1}^{n}\\left(X_{i}^{*}\\right)^{\\otimes2t},\\tilde{\\mathbb{E}}v^{\\otimes2t}\\right\\rangle\\,\\leqslant\\,a^{2t}}\\end{array}$ then the algorithm terminates in at most $\\lceil2\\varepsilon n\\rceil$ steps, and the resuling weights satisfy $\\begin{array}{r}{\\sum_{i=1}^{\\dot{n}}w_{i}\\geqslant1-2\\varepsilon}\\end{array}$ ", "page_idx": 20}, {"type": "text", "text": "To prove it, we will use the following lemma: ", "page_idx": 20}, {"type": "text", "text": "Lemma C.3. Assume that $\\begin{array}{r}{\\left\\langle\\frac{1}{n}\\sum_{i=1}^{n}\\left(X_{i}^{*}\\right)^{\\otimes2t},\\tilde{\\mathbb{E}}v^{\\otimes2t}\\right\\rangle\\leqslant a^{2t},\\,\\left\\langle\\frac{1}{n}\\sum_{i=1}^{n}X_{i}^{\\otimes2t},\\tilde{\\mathbb{E}}v^{\\otimes2t}\\right\\rangle\\geqslant10^{t}a^{2t}}\\end{array}$ and ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{i\\in S_{g}}\\left(\\frac{1}{n}-w_{i}\\right)\\leqslant\\sum_{i\\in S_{b}}\\left(\\frac{1}{n}-w_{i}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{i\\in S_{g}}\\left(\\frac{1}{n}-w_{i}^{\\prime}\\right)<\\sum_{i\\in S_{b}}\\left(\\frac{1}{n}-w_{i}^{\\prime}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof of Lemma C.3. Note, that it is enough to show that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{i\\in S_{g}}w_{i}-w_{i}^{\\prime}<\\sum_{i\\in S_{b}}w_{i}-w_{i}^{\\prime}\\,.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "$\\begin{array}{r}{w_{i}^{\\prime}=\\bigg(1-\\frac{\\tau_{i}}{\\tau_{\\mathrm{max}}}\\bigg)w_{i}}\\end{array}$ Sso foral $\\begin{array}{r}{i\\in[n],w_{i}-w_{i}^{\\prime}=\\frac{1}{\\tau_{\\mathrm{max}}}\\tau_{i}w_{i}}\\end{array}$ Honces noughto show that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{i\\in S_{g}}\\tau_{i}w_{i}<\\sum_{i\\in S_{b}}\\tau_{i}w_{i}\\;.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Since $S_{g}$ and $S_{b}$ partition $[n]$ and ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}w_{i}\\tau_{i}=\\left\\langle\\sum_{i=1}^{n}w_{i}X_{i}^{\\otimes2t},\\tilde{\\mathbb{E}}v^{\\otimes2t}\\right\\rangle.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "we can prove $\\textstyle\\sum_{i\\in S_{g}}\\tau_{i}w_{i}<\\sum_{i\\in S_{b}}\\tau_{i}w_{i}$ by showing that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{i\\in S_{g}}\\tau_{i}w_{i}\\leqslant a^{2t}<\\frac{\\left\\langle\\sum_{i=1}^{n}w_{i}X_{i}^{\\otimes t},\\tilde{\\mathbb{E}}v^{\\otimes2t}\\right\\rangle}{2}\\,.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Note that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{i\\in S_{g}}\\tau_{i}w_{i}=\\left\\langle\\sum_{i\\in S_{g}}w_{i}X_{i}^{\\otimes2t},\\lceil\\tilde{\\mathbb{E}}v^{\\otimes2t}\\right\\rangle\\leqslant\\left\\langle\\frac{1}{n}\\sum_{i=1}^{n}\\left(X_{i}^{*}\\right)^{\\otimes2t},\\lceil\\tilde{\\mathbb{E}}v^{\\otimes2t}\\right\\rangle\\leqslant a^{2t}\\,.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof of Lemma C.2. We will show that the algorithm terminates after at most $\\lceil2\\varepsilon n\\rceil$ iterations. Assume that it does not terminate after $T=\\lceil2\\varepsilon n\\rceil$ iterations. Note that the number of entries of $w$ that are equal to O increases by at least 1 in every iteration. Hence, after $T$ iterations we have set ", "page_idx": 20}, {"type": "text", "text": "at least $\\varepsilon n$ entries of $w$ to zero whose index lies in $S_{g}$ . By assumption that the algorithm did not terminate and Lemma C.3, it holds that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\varepsilon\\leqslant\\sum_{i\\in S_{g}}\\left(\\frac{1}{n}-w_{i}^{(T)}\\right)<\\sum_{i\\in S_{b}}\\left(\\frac{1}{n}-w_{i}^{(T)}\\right)\\leqslant\\frac{|S_{b}|}{n}\\leqslant\\varepsilon\\,,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "a contradiction. ", "page_idx": 21}, {"type": "text", "text": "Let $T$ be the index of the last iteration of the algorithm before termination. Then ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left\\|\\frac{1}{n}-w^{(T)}\\right\\|_{1}=\\sum_{i\\in S_{g}}\\frac{1}{n}-w_{i}^{(T)}+\\sum_{i\\in S_{b}}\\frac{1}{n}-w_{i}^{(T)}<2\\sum_{i\\in S_{b}}\\frac{1}{n}-w_{i}^{(T)}\\leqslant2\\varepsilon\\,.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "D  Sum-of-Squares Certificates ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We use the standard sum-of-squares machinery, used in numerous prior works, e.g. [KS17a, KS17b, HL18, HL19, dKNS20, $\\mathrm{DKK}^{+}22]$ ", "page_idx": 21}, {"type": "text", "text": "Let $f_{1},f_{2},\\ldots,f_{r}$ and $g$ be multivariate polynomials in $x$ A sum-of-squares proof that the constraints $\\{f_{1}\\;\\geqslant\\;0,\\ldots,f_{m}\\;\\geqslant\\;0\\}$ imply the constraint $\\{g~\\geqslant~0\\}$ consists of sum-of-squares polynomials $(p_{S})_{S\\subseteq[m]}$ such that ", "page_idx": 21}, {"type": "equation", "text": "$$\ng=\\sum_{S\\subseteq[m]}p_{S}\\cdot\\Pi_{i\\in S}f_{i}\\,.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We say that this proof has degree $\\ell$ if for every set $S\\subseteq[m]$ , the polynomial $p_{S}\\Pi_{i\\in S}f_{i}$ has degree at most $\\ell$ . If there is a degree $\\ell$ SoS proof that $\\{f_{i}\\geqslant0\\mid i\\leqslant r\\}$ implies $\\{g\\geqslant0\\}$ , we write: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left\\{f_{i}\\geqslant0\\;\\right|\\,i\\leqslant r\\right\\}\\left|_{\\ell}\\;\\left\\{g\\geqslant0\\right\\}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We provide degree $2\\ell$ sum-of-squares proofs from the system $\\mathcal{A}_{K}$ (see below) of $(n\\,+\\,d)^{O(1)}$ constraints. The sum-of-squares algorithm (that appeared it [Sho87, Par00, Nes00, Las0i]. See, e.g., Theorem 2.6. $[\\mathrm{DKK}^{+}22]$ for the precise formulation) returns a linear functional $\\tilde{\\mathbb{E}}:\\mathbb{R}[x]_{\\leqslant2\\ell}\\rightarrow\\mathbb{R},$ that is called a degree $2\\ell$ pseudo-expectation, that satisfies the constraints of $\\mathcal{A}_{K}$ in time $(n+d)^{O(\\ell)}$ In particular, it means that once we prove in sum-of-squares of degree $2\\ell$ that constraints $\\mathcal{A}_{K}$ imply that some polynomial $g(u)$ is non-negative, the value of the $\\tilde{\\mathbb{E}}$ returned by the algorithm on $g(u)$ is also non-negative. ", "page_idx": 21}, {"type": "text", "text": "Recall the system $\\mathcal{A}_{K}$ of elastic constraints in Equation (C.1) as follows: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathscr{R}_{K}:\\left\\{\\begin{array}{c c}{\\displaystyle\\forall i\\in[d]}&{\\displaystyle s_{i}^{2}=1}\\\\ {\\displaystyle\\forall i\\in[d]}&{\\displaystyle s_{i}v_{i}\\geqslant v_{i}}\\\\ {\\displaystyle\\forall i\\in[d]}&{\\displaystyle s_{i}v_{i}\\geqslant-v_{i}}\\\\ &{\\displaystyle\\sum_{i=1}^{d}v_{i}^{2}\\leqslant1}\\\\ &{\\displaystyle\\sum_{i=1}^{d}s_{i}v_{i}\\leqslant\\sqrt{K}\\right\\}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Also recall that the vectors from the elastic ball $\\left\\{v\\in\\mathbb{R}^{d}\\;\\Big|\\;\\|v\\|\\leqslant1\\,,\\;\\|v\\|_{1}\\leqslant\\sqrt{K}\\right\\}$ satisfy these constraints with $s_{i}=\\mathrm{sign}(v_{i})$ ", "page_idx": 21}, {"type": "text", "text": "The following lemma is similar to Lemma 3.4 from $[\\mathrm{DKK}^{+}22]$ , but we prove it in using the elastic constraints. The derivation from the elastic constraints requires a bit more work. ", "page_idx": 21}, {"type": "text", "text": "Lemma D.1. For arbitrary polynomial $\\begin{array}{r}{p(v)=\\sum_{1\\leqslant i_{1},\\dots,i_{t}\\leqslant d}p_{i_{1}\\dots i_{t}}\\cdot v_{i_{1}}\\cdot\\cdot\\cdot v_{i_{t}}}\\end{array}$ of degree at most t we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{A}_{K}\\left|\\frac{s,v}{4t}\\,\\left\\{(p(v))^{2}\\leqslant\\|p\\|_{\\infty}^{2}\\cdot K^{t}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. Observe that $\\mathcal{A}_{K}\\left|\\frac{s,v}{2}\\,s_{i}v_{i}\\geqslant0\\right.$ hence $\\begin{array}{r}{\\mathcal{A}_{K}\\left|\\frac{s,v}{4t}\\left(\\sum_{i=1}^{d}s_{i}v_{i}\\right)^{2t}\\right.\\leqslant K^{t}}\\end{array}$ . In addition, by note that, $v_{i_{1}}\\cdot\\cdot\\cdot v_{i_{t}}\\leqslant s_{i_{1}}v_{i_{1}}\\cdot\\cdot\\cdot s_{i_{t}}v_{i_{t}}.$ It follows that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{R}_{K}\\frac{1}{2!}\\mathcal{L}\\left\\{\\displaystyle\\sum_{u_{i}=b_{i-i},u_{i}=t}^{K,S}p_{i_{1}-\\hat{u}_{i}}\\cdot v_{i_{1}}\\cdot v_{i_{i}},\\ast\\sum_{1\\leqslant i=b_{i-i},u_{i}=t}^{i}\\left[\\underbrace{p_{i_{1-i},i}|v_{i_{1}}v_{i_{1}}\\cdot\\cdot\\cdot\\cdot s_{i_{1}}v_{i_{1}}}_{\\mathrm{LiG-~af~af~}}\\right]\\right.}\\\\ &{\\left.\\left.\\left[\\displaystyle\\sum_{u_{1}=b_{i-i},u_{i}=d}^{S,F}p_{i_{1-i},i}\\cdot v_{i_{1}}\\cdot\\cdot v_{i_{1}}\\cdot\\cdot\\sum_{u_{i}=b_{i-i},u_{i}=t}^{i}\\left[\\underbrace{p_{i_{1-i},i}|v_{i_{1}}v_{i_{1}}\\cdot\\cdot\\cdot\\cdot s_{i_{1}}v_{i_{1}}}_{\\mathrm{LiG-~af~af~}}\\right]\\right.\\right.}\\\\ &{\\left.\\left.\\left[\\displaystyle\\sum_{u_{1}=b_{i-i},u_{i}=d}^{S,F}p_{i_{1-i},i}\\cdot v_{i_{1}}\\cdot\\cdot v_{i_{1}}\\right]^{2}\\leqslant\\left(\\displaystyle\\sum_{u_{1}=b_{i-i},u_{i}=d}^{2}\\left[\\underbrace{p_{i_{1-i},i}|v_{i_{1}}\\cdot\\cdot\\cdot v_{i_{1}}v_{i_{1}}\\right]^{2}}_{\\mathrm{LiG-~af~af~}}\\right)\\right]\\right.}\\\\ &{\\left.\\left[\\displaystyle\\sum_{u_{1}=b_{i-i},u_{i}=d}^{S,F}p_{i_{1-i},i}\\cdot v_{i_{1}}\\cdot\\cdot v_{i_{1}}\\right]^{2}\\leqslant\\left\\lVert\\mathbb{P}\\right\\rVert_{L^{2}}\\left(\\displaystyle\\sum_{u_{1}=b_{i-i},u_{i}}^{2}\\left[\\underbrace{\\mathcal{R}}_{u_{1}}\\right]^{2}\\right)\\right.}\\\\ &{\\left.\\left[\\displaystyle\\sum_{u_{1}=b_{i-i},u_{i}=d}^{S,F}p_{i_{1-i},i}\\cdot v_{i_{1}}\\cdot\\cdot v_{i_{i}}\\right]^{2}\\\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The following lemma shows that we can certify an upper bound on the value of the empirical moments (as polylinear functions) of truncated distribution $Z_{i}(\\tau)$ on the vectors from the elastic ball. ", "page_idx": 22}, {"type": "text", "text": "Lemma D.2 (Certifiable bound on empirical moments). Suppose that for some t, $\\ell\\in\\mathbb N$ and $M_{2t}\\geqslant1$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{A}_{K}\\left|\\frac{s,v}{\\ell}\\left\\{\\mathbb{E}\\langle X_{1}^{*},v\\rangle^{2t}\\leqslant M_{2t}^{2t}\\cdot\\|\\Sigma\\|^{t}\\right\\},}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad}\\\\ &{\\quad\\quad\\quad\\quad\\operatorname*{max}_{j\\in[d]}|X_{1j}^{*}|^{4t}\\leqslant\\nu^{4t}\\cdot\\|\\Sigma\\|^{2t}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "andforsome $\\nu\\geqslant1$ ", "page_idx": 22}, {"type": "text", "text": "$I f\\tau\\gtrsim\\nu^{2}\\cdot\\sqrt{K}\\cdot\\sqrt{\\|\\Sigma\\|}$ and ", "page_idx": 22}, {"type": "equation", "text": "$$\nn\\geqslant1000\\left(\\nu^{4t}\\cdot K^{t}+\\left(\\frac{\\tau}{\\sqrt{\\|\\Sigma\\|}}\\right)^{2t}\\right)\\cdot K^{t}\\cdot t\\log(d/\\delta)\\,,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "then with probability at least $1-\\delta$ for each degree $2\\ell$ pseudo-expectation $\\tilde{\\mathbb{E}}$ that satisfies $\\mathcal{A}_{K}$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\tilde{\\mathbb{E}}\\Bigg[\\frac{1}{n}\\sum_{i=1}^{n}\\langle X_{i}^{\\prime}(\\tau),v\\rangle^{2t}\\Bigg]\\leqslant(2M_{2t})^{2t}\\cdot\\|\\Sigma\\|^{t}\\,.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. Consider the polynomial ", "page_idx": 22}, {"type": "equation", "text": "$$\np(v)=\\frac{1}{n}\\sum_{i=1}^{n}\\langle X_{i}^{\\prime}(\\tau),v\\rangle^{2t}-\\mathbb{E}\\langle X_{1}^{*},v\\rangle^{2t},.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "By Lemma E.6 and the assumptions on $n$ and $\\tau$ , its coefficients are bounded by ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\Delta=20\\sqrt{\\frac{\\nu^{4t}\\|\\boldsymbol{\\Sigma}\\|^{2t}\\cdot t\\log(d/\\delta)}{n}}+20\\frac{\\tau^{2t}\\cdot t\\log(d/\\delta)}{n}+\\frac{2t\\nu^{4t}\\cdot\\|\\boldsymbol{\\Sigma}\\|^{2t}}{\\tau^{2t}}\\leqslant\\frac{2^{t}M_{2t}^{2t}\\cdot\\|\\boldsymbol{\\Sigma}\\|^{t}}{K^{t}}\\,.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "It follows that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathcal{R}_{K}\\left|\\frac{s,v}{2\\ell}\\left\\{\\left(\\frac{1}{n}\\sum_{i=1}^{n}\\langle X_{i}^{\\prime}(\\tau),v\\rangle^{2t}\\right)^{2}\\leqslant\\left(\\frac{1}{n}\\sum_{i=1}^{n}\\langle X_{i}^{\\prime}(\\tau),v\\rangle^{2t}-\\mathbb{E}\\langle X_{1}^{*},v\\rangle^{2t}+\\mathbb{E}\\langle X_{1}^{*},v\\rangle^{2t}\\right)^{2}\\right\\}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{|s,v}{2\\ell}\\left\\{\\left(\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\langle X_{i}^{\\prime}(\\tau),v\\rangle^{2t}\\right)^{2}\\leqslant2\\left(\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\langle X_{i}^{\\prime}(\\tau),v\\rangle^{2t}-\\mathbb{E}\\langle X_{1}^{*},v\\rangle^{2t}\\right)^{2}+2\\left(\\mathbb{E}\\langle X_{1}^{*},v\\rangle^{2t}\\right)^{2}\\right\\}}\\\\ &{\\frac{|s,v}{2\\ell}\\left\\{\\left(\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\langle X_{i}^{\\prime}(\\tau),v\\rangle^{2t}\\right)^{2}\\leqslant2\\Delta^{2}K^{2t}+2M_{2t}^{4t}\\|\\Sigma\\|^{2t}\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Hence ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\tilde{\\mathbb{E}}\\Bigg(\\frac{1}{n}\\sum_{i=1}^{n}\\langle X_{i},v\\rangle^{2t}\\Bigg)^{2}\\leqslant(2M_{2t})^{4t}\\cdot\\|\\Sigma\\|^{2t}\\,.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "By Cauchy-Schwarz inequality for pseudo-expectations (see, for example, Fact A.2. from $[\\mathrm{DKK}^{+}22]$ we get the desired bound. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "E  Properties of the truncation ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "As before, let $X_{1}^{*},\\dots,X_{n}^{*}$ be id samples from $\\mathcal{D}$ ", "page_idx": 23}, {"type": "text", "text": "For $\\tau>0$ $X_{i j}^{\\prime}(\\tau)=X_{i j}^{*}\\mathbf{1}_{\\left[|X_{i j}^{*}|\\leqslant\\tau\\right]}$ $X_{i j}^{\\prime}(\\tau)$ that weuseo in the paper. We start with the following lemma. ", "page_idx": 23}, {"type": "text", "text": "Lemma E.1. Suppose that for some $\\nu\\geqslant1$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{j\\in[d]}\\mathbb{E}|X_{i j}^{*}|^{4}\\leqslant\\nu^{4}\\cdot\\|\\Sigma\\|^{2}\\,.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Then ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left\\|\\mathbb{E}\\big(X_{i}^{\\prime}(\\tau)-X_{i}^{*}\\big)\\big(X_{i}^{\\prime}(\\tau)-X_{i}^{*}\\big)^{\\top}\\right\\|_{\\infty}\\leqslant\\frac{\\nu^{4}\\cdot\\|\\Sigma\\|^{2}}{\\tau^{2}}\\,.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\Big(X_{i j}^{\\prime}(\\tau)-X_{i j}^{*}\\Big)\\Big(X_{i j^{\\prime}}^{\\prime}(\\tau)-X_{i j^{\\prime}}^{*}\\Big)\\Big|=\\Bigg|\\mathbb{E}\\,X_{i j}^{*}\\mathbf{1}_{\\left[\\vert X_{i j}^{*}\\vert>\\tau\\right]}X_{i j^{\\prime}}^{*}\\mathbf{1}_{\\left[\\vert X_{i j^{\\prime}}^{*}\\vert>\\tau\\right]}\\Bigg|}&{}\\\\ {\\leqslant\\sqrt{\\mathbb{E}\\,\\mathbf{1}_{\\left[\\vert X_{i j}^{*}\\vert>\\tau\\right]}\\left(X_{i j}^{*}\\right)^{2}}\\cdot\\sqrt{\\mathbb{E}\\,\\mathbf{1}_{\\left[\\vert X_{i j^{\\prime}}^{*}\\vert>\\tau\\right]}\\left(X_{i j^{\\prime}}^{*}\\right)^{2}}}\\\\ {\\leqslant\\Bigg(\\mathbb{E}\\,\\mathbf{1}_{\\left[\\vert X_{i j}^{*}\\vert>\\tau\\right]}\\cdot\\mathbb{E}\\Big(X_{i j}^{*}\\Big)^{4}\\Bigg)^{1/4}\\cdot\\left(\\mathbb{B}\\,\\mathbf{1}_{\\left[\\vert X_{i j^{\\prime}}^{*}\\vert>\\tau\\right]}\\cdot\\mathbb{E}\\Big(X_{i j^{\\prime}}^{*}\\Big)^{4}\\right)^{1/4}}\\\\ {\\leqslant\\left(\\mathbb{P}\\Big[\\left\\vert X_{i j}^{*}\\vert^{4}>\\tau^{4}\\right]\\cdot\\mathbb{P}\\Big[\\big\\vert X_{i j^{\\prime}}^{*}\\vert^{4}>\\tau^{4}\\Big]\\right)^{1/4}\\cdot\\nu^{2}\\|\\Sigma\\|}\\\\ {\\leqslant\\nu^{4}\\|\\Sigma\\|^{2}/\\tau^{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The following lemma shows that the moments of the truncated distribution are close to the moments Of $X_{i}^{*}$ in $\\ell_{\\infty}$ -norm. ", "page_idx": 23}, {"type": "text", "text": "Lemma E.2. Let $t\\in\\mathbb{N}$ and suppose that for some $B>0$ and $q>0$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{j\\in[d]}\\mathbb{E}|X_{i j}^{*}|^{t+q}\\leqslant B^{t+q}\\,.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Then ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left\\|\\mathbb{E}\\big(X_{i}^{\\prime}(\\tau)\\big)^{\\otimes t}-\\mathbb{E}\\big(X_{i}^{*}\\big)^{\\otimes t}\\right\\|_{\\infty}\\leqslant\\frac{t\\cdot B^{t+q}}{\\tau^{q}}\\,.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. Denote $a=X_{i}^{\\prime}(\\tau),b=X_{i}^{*}$ Note that by Holder's inequality, for all $s\\in[t]$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Xi|b_{j_{1}}\\cdots b_{j_{s-1}}|\\cdot|a_{j_{s}}-b_{j_{s}}|\\cdot|a_{j_{s+1}}\\cdots a_{j_{i}}|=\\Xi|b_{j_{1}}\\cdots b_{j_{s-1}}b_{j_{s}}a_{j_{s+1}}\\cdots a_{j_{i}}|\\cdot\\mathbf{1}_{\\left\\{a_{j_{s}}=0\\right\\}}}&{}\\\\ {\\leqslant\\left(\\mathbb{P}\\big[a_{j_{s}}=0\\big]\\right)^{\\frac{t_{q}}{t+q}}\\cdot\\left(\\mathbb{B}|b_{j_{1}}\\cdots b_{j_{s-1}}b_{j_{s}}a_{j_{s+1}}\\cdots a_{j_{t}}|^{1+q/t}\\right)^{\\frac{t}{t+q}}}&{}\\\\ {\\leqslant\\left(\\mathbb{P}\\Big[\\big(X_{i_{j_{s}}}^{*}\\big)^{t+q}>\\tau^{t+q}\\Big]\\right)^{\\frac{q}{t+q}}\\cdot\\left(\\mathbb{B}|b_{j_{1}}\\cdots b_{j_{t}}|^{1+q/t}\\right)^{\\frac{t}{t+q}}}&{}\\\\ {\\leqslant\\frac{B\\cdot q}{\\tau^{q}}\\cdot\\left(\\underset{j\\in\\mathbb{I}}{\\operatorname*{max}}\\mathbb{B}|b_{j}|^{t+q}\\right)^{\\frac{t}{t+q}}}&{}\\\\ {\\leqslant\\frac{B^{t+q}}{\\tau^{q}}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "It follows that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\mathbb{E}a_{j_{1}}a_{j_{2}}\\cdots a_{j_{t}}-\\mathbb{E}b_{j_{1}}b_{j_{2}}\\cdots b_{j_{t}}\\right|\\leqslant\\mathbb{E}\\big|a_{j_{1}}a_{j_{2}}\\cdots a_{j_{t}}-b_{j_{1}}b_{j_{2}}\\cdots b_{j_{t}}\\big|}\\\\ &{\\qquad\\qquad\\qquad\\leqslant\\mathbb{E}\\big|a_{j_{1}}a_{j_{2}}\\cdots a_{j_{t}}-b_{j_{1}}a_{j_{2}}\\cdots a_{j_{t}}+b_{j_{1}}a_{j_{2}}\\cdots a_{j_{t}}-b_{j_{1}}b_{j_{2}}\\cdots b_{j_{t}}\\big|}\\\\ &{\\qquad\\qquad\\leqslant\\mathbb{E}\\big|a_{j_{1}}-b_{j_{1}}\\big|\\cdot\\big|a_{j_{2}}\\cdots a_{j_{t}}\\big|+\\mathbb{E}\\big|b_{j_{1}}\\big|\\cdot\\big|a_{j_{2}}\\cdots a_{j_{t}}-b_{j_{2}}\\cdots b_{j_{t}}\\big|}\\\\ &{\\qquad\\qquad\\leqslant\\frac{t\\cdot B^{t+q}}{\\tau^{q}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The following statement is a straightforward corollary of Lemma E.2 with $q=t$ ", "page_idx": 24}, {"type": "text", "text": "Corollary E.3. Let $t\\in\\mathbb{N}$ and suppose that for some $B>0$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{j\\in[d]}\\mathbb{E}|X_{i j}^{*}|^{2t}\\leqslant B^{2t}\\,.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Then ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left\\|\\mathbb{E}\\big(X_{i}^{\\prime}(\\tau)\\big)^{\\otimes t}-\\mathbb{E}\\big(X_{i}^{*}\\big)^{\\otimes t}\\right\\|_{\\infty}\\leqslant\\frac{t\\cdot B^{2t}}{\\tau^{t}}\\,.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The following two statements are special cases of Corollary E.3 for $t=1$ and $t=2$ ", "page_idx": 24}, {"type": "text", "text": "Corollary E.4. ", "text_level": 1, "page_idx": 24}, {"type": "equation", "text": "$$\n\\left\\|\\mathbb{E}\\,X_{i}^{\\prime}(\\tau)\\right\\|_{\\infty}\\leqslant\\frac{\\|\\Sigma\\|}{\\tau}\\,.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Corollary E.5. Suppose that for some $\\nu\\geqslant1$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{j\\in[d]}\\mathbb{E}|X_{i j}^{*}|^{4}\\leqslant\\nu^{4}\\cdot\\|\\Sigma\\|^{2}\\,.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Then ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left\\|\\mathbb{E}\\big(X_{i}^{\\prime}(\\tau)\\big)\\big(X_{i}^{\\prime}(\\tau)\\big)^{\\top}-\\mathbb{E}\\big(X_{i}^{*}\\big)\\big(X_{i}^{*}\\big)^{\\top}\\right\\|_{\\infty}\\leqslant\\frac{2\\cdot\\nu^{4}\\cdot\\|\\Sigma\\|^{2}}{\\tau^{2}}\\,.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The following lemma shows that the empirical mean of $\\left(X_{i}^{\\prime}(\\tau)\\right)^{\\otimes t}$ is close to $\\mathbb{E}{\\left(X_{1}^{*}\\right)}^{\\otimes t}$ for an appropriate choice of $\\tau$ and large enough $n$ ", "page_idx": 24}, {"type": "text", "text": "Lemma E.6. Let $t\\in\\mathbb{N}$ be and suppose that for some $\\nu\\geqslant1$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{j\\in[d]}\\mathbb{E}|X_{i j}^{*}|^{2t}\\leqslant\\nu^{2t}\\cdot\\|\\Sigma\\|^{t}\\,.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Then with probability $1-\\delta$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\sum_{i=1}^{n}\\left(X_{i}^{\\prime}(\\tau)\\right)^{\\otimes t}-\\mathbb{E}\\big(X_{1}^{*}\\big)^{\\otimes t}\\Bigg\\Vert_{\\infty}\\leqslant10\\sqrt{\\frac{\\nu^{2t}\\cdot\\|\\Sigma\\|^{t}\\cdot t\\log(d/\\delta)}{n}}+10\\frac{\\tau^{t}\\cdot t\\log(d/\\delta)}{n}+\\frac{t\\cdot\\nu^{2t}\\cdot\\|\\Sigma\\|^{t}}{\\tau^{t}}\\,.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. It follows from Corollary E.3, Bernstein inequality Fact I.1, and a union bound over all $d^{t}$ entries of $\\mathbb{E}{\\left(X_{1}^{*}\\right)}^{\\otimes t}$ \u53e3 ", "page_idx": 24}, {"type": "text", "text": "F  Properties of sparse vectors ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Lemma F.1. Let $\\Sigma\\in\\mathbb{R}^{d\\times d}$ be a positive definite matrix, $k^{\\prime},k^{\\prime\\prime}\\in\\mathbb{N},\\,r,\\delta\\geqslant0,$ and ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}_{k^{\\prime}}(r)=\\left\\{u\\in\\mathbb{R}^{d}\\;\\Big|\\;\\|\\Sigma^{1/2}u\\|\\leqslant r\\,,\\|u\\|_{1}\\leqslant\\sqrt{k^{\\prime}}\\cdot r\\right\\},\\;\\;\\;\\;}\\\\ {S_{k^{\\prime\\prime}}(r)=\\left\\{u\\in\\mathbb{R}^{d}\\;\\Big|\\;\\|\\Sigma^{1/2}u\\|=(1+\\delta)\\cdot r\\,,u\\;i s\\;k^{\\prime\\prime}\\mathrm{.s}p a r s e\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "$I\\!f k^{\\prime\\prime}\\geqslant4k^{\\prime}\\|\\Sigma\\|/\\delta^{2},$ then ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathcal{E}_{k^{\\prime}}(\\boldsymbol{r})\\subseteq\\mathrm{conv}(S_{k^{\\prime\\prime}}(\\boldsymbol{r}))\\,.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. Let us take some $u\\,\\in\\mathcal{E}_{k^{\\prime}}(r)$ . Without loss of generality assume that $u_{1}\\,\\geqslant\\,u_{2}\\,\\geqslant\\,.\\,.\\,\\geqslant\\,u_{d}$ Let's split indices $\\{1,2,\\ldots,d\\}$ into blocks $B_{1}\\ldots,B_{\\lceil d/k^{\\prime\\prime}\\rceil}$ of size $k^{\\prime\\prime}$ (the last block might be of smaller size). Let for each block $B_{i}$ , let ", "page_idx": 25}, {"type": "equation", "text": "$$\np_{i}=\\frac{\\|\\Sigma^{1/2}u_{B_{i}}\\|}{\\sum_{j=1}^{\\lceil d/k^{\\prime\\prime}\\rceil}\\|\\Sigma^{1/2}u_{B_{j}}\\|}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "$\\begin{array}{r}{\\sum_{i}^{\\lceil d/k^{\\prime\\prime}\\rceil}p_{i}=1}\\end{array}$ and $\\begin{array}{r}{u=\\sum_{i}^{\\lceil d/k^{\\prime\\prime}\\rceil}p_{i}u_{B_{i}}/p_{i}}\\end{array}$ ,itis suficient to show that for all $i$ $\\|\\Sigma^{1/2}u_{B_{i}}\\|/p_{i}\\leqslant$ $(1+\\delta)r$ ", "page_idx": 25}, {"type": "text", "text": "Note that for all $j\\geqslant2$ since $\\|u_{B_{j}}\\|\\leqslant{\\sqrt{k^{\\prime\\prime}}}\\|u_{B_{j}}\\|_{\\infty}$ and $\\begin{array}{r}{\\|u_{B_{j}}\\|_{\\infty}\\leqslant\\frac{1}{k^{\\prime\\prime}}\\|u_{B_{j-1}}\\|_{1},}\\end{array}$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\|\\Sigma^{1/2}u_{B_{j}}\\|\\leqslant\\sqrt{\\|\\Sigma\\|}\\cdot\\|u_{B_{j}}\\|\\leqslant\\sqrt{k^{\\prime\\prime}\\|\\Sigma\\|}\\cdot\\|u_{B_{j}}\\|_{\\infty}\\leqslant\\sqrt{\\frac{\\|\\Sigma\\|}{k^{\\prime\\prime}}}\\cdot\\|u_{B_{j-1}}\\|_{1}\\,.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "By the triangle inequality, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\|{\\boldsymbol\\Sigma}^{1/2}u_{B_{1}}\\|\\leqslant\\|{\\boldsymbol\\Sigma}^{1/2}u\\|+\\sum_{j=2}^{\\lceil d/k^{\\prime\\prime}\\rceil}\\|{\\boldsymbol\\Sigma}^{1/2}u_{B_{j}}\\|\\,.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Hence ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\frac{\\|{\\bf{E}}^{[2]}u_{\\delta}\\|}{p_{i}}=\\displaystyle\\sum_{j=1}^{[2]^{n}}\\|{\\bf{E}}^{[2]^{n}}u_{\\delta}\\|}&{}\\\\ {\\displaystyle}&{\\leqslant\\|{\\bf{E}}^{[1/2]}u\\|+2\\displaystyle\\sum_{j=2}^{[4]^{n}}\\|{\\bf{E}}^{[1/2]}u_{\\delta}\\|}\\\\ &{\\leqslant r+2\\sqrt{\\displaystyle\\|\\frac{\\mathrm{E}[1/\\delta]^{n-1}}{k^{r}}}\\displaystyle\\sum_{j=1}^{[4/k-1]}\\|u_{\\delta,j-1}\\|}\\\\ &{\\leqslant r+2\\sqrt{\\displaystyle\\|\\frac{\\mathrm{E}[1]}{k^{r}}}\\|u\\|}\\\\ &{\\leqslant\\left(1+2\\sqrt{\\displaystyle\\frac{k^{\\prime}\\|\\mathrm{E}\\|}{k^{r}}}\\right)\\cdot r}\\\\ &{\\leqslant\\left(1+\\delta\\right)\\cdot r.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Lemma F.2. Let $\\Sigma\\in\\mathbb{R}^{d\\times d}$ be a positive defnitematix, andlt $X\\in\\mathbb{R}^{m\\times d}$ be a matrix such that for some $r>0$ and $\\delta\\in(0,1)$ , for all $k^{\\prime\\prime}$ -sparsevectors $u^{\\prime}$ such that $r\\leqslant\\|\\Sigma^{1/2}u^{\\prime}\\|\\leqslant2r$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(1-\\delta)\\cdot\\|\\Sigma^{1/2}u^{\\prime}\\|\\leqslant\\frac{1}{\\sqrt{m}}\\|X u^{\\prime}\\|\\leqslant(1+\\delta)\\cdot\\|\\Sigma^{1/2}u^{\\prime}\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "$I\\!f k^{\\prime\\prime}\\geqslant4k^{\\prime}\\|\\Sigma\\|/\\delta^{2},$ then for all u such that $\\|\\Sigma^{1/2}u\\|=r$ and $\\|u\\|_{1}\\leqslant r\\sqrt{k^{\\prime}}$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(1-4\\delta)\\cdot r\\leqslant\\frac{1}{\\sqrt{m}}\\|X u\\|\\leqslant(1+4\\delta)\\cdot r\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. The inequlity $\\begin{array}{r}{\\frac{1}{\\sqrt{m}}\\|X u\\|\\leqslant\\left(1+\\delta\\right)^{2}\\cdot r\\leqslant\\left(1+4\\delta\\right)\\cdot r}\\end{array}$ follows from Jesen's inequality and Lemma F.1. ", "page_idx": 26}, {"type": "text", "text": "Let us show that $\\begin{array}{r}{(1-4\\delta)\\cdot r\\,\\leqslant\\frac{1}{\\sqrt{m}}\\|X u\\|}\\end{array}$ Let $B_{1},\\dots,B_{\\lceil d/k^{\\prime\\prime}\\rceil}$ be blocks of indices as in the proof of Lemma F.1. It follows that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{\\sqrt{m}}\\|X u\\|\\geqslant\\frac{1}{\\sqrt{m}}\\|X u_{B_{1}}\\|-\\displaystyle\\sum_{j=2}^{d/k^{\\prime\\prime}}\\frac{1}{\\sqrt{m}}\\|X u_{B_{j}}\\|}\\\\ &{\\qquad\\qquad\\geqslant(1-\\delta)\\cdot\\|\\Sigma^{1/2}u_{B_{1}}\\|-(1+\\delta)\\displaystyle\\sum_{j=2}^{[d/k^{\\prime\\prime}]}\\|\\Sigma^{1/2}u_{B_{j}}\\|}\\\\ &{\\qquad\\qquad\\geqslant(1-\\delta)\\cdot\\|\\Sigma^{1/2}u\\|-2\\cdot r\\sqrt{\\frac{k^{\\prime}\\|\\Sigma\\|}{k^{\\prime\\prime}}}}\\\\ &{\\qquad\\qquad\\geqslant(1-\\delta)^{2}\\cdot r-\\delta r}\\\\ &{\\qquad\\qquad\\geqslant(1-4\\delta)\\cdot r\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "G Lower bounds ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In this section we prove Statistical Query lower bounds. SQ lower bounds is a standard tool of showing computational lower bounds for statistical estimation and decision problems. SQ algorithms do not use samples, but have access to an oracle that can return the expectation of any bounded function (up to a desired additive error, called tolerance). The SQ lower bounds formally show the trateoff between the number of queries to the oracle and the tolerance. The standard interpretation of SQ lower bounds relies on the fact that simulating a query with small tolerance using iid samples requires large number of samples. Hence these lower bounds are interpreted as a tradeoff between the time complexity (number of queries) and sample complexity (tolerance) of estimators. See [DKS17] for more details. ", "page_idx": 26}, {"type": "text", "text": "First we give necessary definitions. These definitions are standard and can be found in [DKS17]. ", "page_idx": 26}, {"type": "text", "text": "Definition G.1 (STAT Oracle). Let $\\mathcal{D}$ be a distribution over $\\mathbb{R}^{d}$ . A statistical query is a function $f:\\mathbb{R}^{d}\\rightarrow[-1,1]$ For $\\tau>0$ the $\\operatorname{STAT}(\\tau)$ oracle responds to the query $f$ with a value $v$ such that $\\begin{array}{r}{\\left|v-\\mathbb{E}_{X\\sim\\mathcal{D}}\\,f(X)\\right|\\leqslant\\tau}\\end{array}$ . Parameter $\\tau$ is called the tolerance of the statistical query. ", "page_idx": 26}, {"type": "text", "text": "Simulating a query $\\operatorname{STAT}(\\tau)$ normallyrequires $\\Omega(1/\\tau^{2})$ iid samples from $\\mathcal{D}$ ,henceSQlower bounds provide a trade-off between the running time (number of queries) and the sample complexity $(\\Omega(1/\\tau^{\\hat{2}}))$ ", "page_idx": 26}, {"type": "text", "text": "Definition G.2 (Pairwise Correlation). Let $\\mathcal{D}_{1},\\mathcal{D}_{2},\\mathcal{D}$ be absolutely continuous distributions over $\\mathbb{R}^{d}$ : and suppose that $\\operatorname{supp}(\\mathcal{D})=\\mathbb{R}^{d}$ The pairwise correlation of $\\mathcal{D}_{1}$ and $\\mathcal{D}_{2}$ with respect to $\\mathcal{D}$ .s defined as ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\chi_{D}(\\mathcal{D}_{1},\\mathcal{D}_{2})=\\int_{\\mathbb{R}^{d}}\\frac{p_{\\mathcal{D}_{1}}(x)p_{\\mathcal{D}_{2}}(x)}{p_{\\mathcal{D}}(x)}d x-1\\,,\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $p_{\\mathcal{D}_{1}}(x),p_{\\mathcal{D}_{2}}(x),p_{\\mathcal{D}}(x)$ are densities of $\\mathcal{D}_{1},\\mathcal{D}_{2},\\mathcal{D}$ respectively. ", "page_idx": 26}, {"type": "text", "text": "Definition G.3 (Chi-Squared Divergence). Let $\\mathcal{D}^{\\prime},\\mathcal{D}$ be absolutely continuous distributions over $\\mathbb{R}^{d}$ , and suppose that $\\operatorname{supp}(\\mathcal{D})=\\mathbb{R}^{d}$ The chi-squared divergence from $\\mathcal{D}$ to $\\mathcal{D}$ is ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\chi^{2}(\\mathcal{D}^{\\prime},\\mathcal{D})=\\chi_{D}(\\mathcal{D}^{\\prime},\\mathcal{D}^{\\prime})\\,.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Definition G.4 $((\\gamma,\\rho)$ -correlation). Let $\\rho,\\gamma>0$ , and let $\\mathcal{D}$ be a distribution over $\\mathbb{R}^{d}$ . We say that a family of distributions $\\mathcal{F}$ over $\\mathbb{R}^{d}$ .s $(\\gamma,\\rho)$ -correlated relative to $\\mathcal{D}$ , if for all distinct $\\mathcal{D}^{\\prime},\\mathcal{D}^{\\prime\\prime}\\in\\mathcal{F}$ \uff0c $|\\chi_{D}(\\mathcal{D}^{\\prime},\\mathcal{D}^{\\prime\\prime})|\\leqslant\\gamma$ and $|\\chi_{D}(\\mathcal{D}^{\\prime},\\mathcal{D}^{\\prime})|\\leqslant\\rho$ ", "page_idx": 26}, {"type": "text", "text": "Fact G.5. Let $\\mathcal{D}$ beadistributionover $\\mathbb{R}^{d}$ and $\\mathcal{F}$ bea family of distributions over $\\mathbb{R}^{d}$ that does not contain $\\mathcal{D}$ . and consider a hypothesis testing problem of determining whether a given distribution $\\mathcal{D}^{\\prime}=\\mathcal{D}$ or $\\mathcal{D}^{\\prime}\\in\\mathcal{F}$ ", "page_idx": 26}, {"type": "text", "text": "Let y $,\\rho>0,s\\in\\mathbb{N},$ and suppose that there exists a subfamily of $\\mathcal{F}$ of size s that is $(\\gamma,\\rho)$ -correlated relative to $\\mathcal{D}$ .Then for all $\\gamma^{\\prime}>0$ every $S Q$ algorithmforthe hypothesis testing problem requires queries of tolerance $\\sqrt{\\gamma+\\gamma^{\\prime}}$ or makes at least $s\\gamma^{\\prime}/(\\rho-\\gamma)$ queries. ", "page_idx": 27}, {"type": "text", "text": "We will also need the following facts: ", "page_idx": 27}, {"type": "text", "text": "Fact G.6 ([DKS17, Lemma 6.7]). Let $c\\in(0,1)$ and $k,d\\in\\mathbb{N}$ be such that $k\\leqslant{\\sqrt{d}}$ There exists a set $\\mathcal{V}\\subset\\mathbb{R}^{d}$ of $k$ -sparse unit vectors of size $d^{c k^{c}/8}$ such that for all distinct u, $v\\in\\mathcal{V}$ $\\langle v,u\\rangle\\leqslant2k^{c-1}$ ", "page_idx": 27}, {"type": "text", "text": "Fact G.7 ([DKS17, Lemma 3.4]). Let $m\\in\\mathbb{N}$ and suppose that a distribution $\\mathcal{M}$ over $\\mathbb{R}$ matches first m moments of $\\ 'N(0,1)$ . For a unit vector $\\boldsymbol{v}\\in\\mathbb{R}^{d}$ let $\\mathcal{P}_{v}$ be a distribution such that its projections onto the direction of $v$ hasdistribution $\\mathcal{M}$ the projection onto the orthogonal complement is $N(0,{\\mathrm{Id}}_{d-1})$ \uff0c and these projections are independent. Then for all $u,v\\in\\mathbb{R}^{d}$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\chi_{N(0,\\mathrm{Id}_{d})}(\\mathcal{P}_{v},\\mathcal{P}_{u})\\right|\\leqslant|\\langle u,v\\rangle|^{m+1}\\chi^{2}(\\mathcal{M},N(0,1))\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The following fact is a slight reformulation of Lemma E.4 from [DKS19] ", "page_idx": 27}, {"type": "text", "text": "Fact G.8 ([DKS19, Lemma E.4). Let $y\\sim N(0,1),$ $\\mu_{0}>0$ $m\\in\\mathbb{N}$ and $g:\\mathbb{R}\\to\\mathbb{R}.$ Let $M_{\\mu}$ be $a$ family of distributions over $\\mathbb{R}$ satisfies the following properties: ", "page_idx": 27}, {"type": "text", "text": "1. $\\mathcal{M}=(1-\\varepsilon_{\\mu})N(\\mu,\\Theta(1))+\\varepsilon_{\\mu}\\mathcal{B}_{\\mu}$ for some $\\varepsilon_{\\mu}$ and ${\\mathcal{B}}_{\\mu}$ such that $M_{\\mu}$ has thesame firstm moments as $N(0,1)$   \n2.1 $f|\\mu|\\geqslant10\\mu_{0},$ then $\\varepsilon_{\\mu}/(1-\\varepsilon_{\\mu})\\leqslant O\\big(\\mu^{2}\\big)$ and $\\chi^{2}(\\mathcal{M},N(0,1))\\leqslant e^{O\\left(\\operatorname*{max}\\{1/\\mu^{2},\\mu^{2}\\}\\right)}.$   \n3. 1 $f|\\mu|\\leqslant10\\mu_{0},$ then $\\varepsilon_{\\mu}=\\varepsilon$ and $\\chi^{2}(\\mathcal{M},N(0,1))\\leqslant g(\\varepsilon)$ ", "page_idx": 27}, {"type": "text", "text": "For unit $\\boldsymbol{v}\\in\\mathbb{R}^{d}$ let $\\mathcal{P}_{v,\\mu}$ be the same as $\\mathcal{P}_{v}$ in Fact $G.7$ whoseprojectiononto $v$ .s $M_{\\mu}$ . Let $Q_{v}^{\\prime}$ be a distrbtion over $\\mathbb{R}^{d+1}$ such that $(X,y)\\sim Q_{v}^{\\prime}$ satisythefolowingproperies? $y\\sim N(0,1)$ and $X|y\\sim\\mathcal{P}_{v,\\mu_{0}\\cdot y}$ . Then for all unit $u,v\\in\\mathbb{R}^{d}$ \uff0c ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\chi_{\\mathcal{D}}(Q_{v}^{\\prime},Q_{u}^{\\prime})\\leqslant(g(\\varepsilon)+O(1))\\cdot|\\langle v,u\\rangle|^{m+1}\\,,\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $\\mathcal{D}=N(0,\\mathrm{Id}_{d+1})$ ", "page_idx": 27}, {"type": "text", "text": "Proposition G.9 (Formal version of Proposition 1.10). Let k, $d\\in\\mathbb{N}$ $k\\leqslant{\\sqrt{d}}$ $\\varepsilon\\in(0,1/2)$ \uff0c\uff0c $c\\in(0,1)$ For a vector $\\beta^{*}\\in\\mathbb{R}^{d}$ and a number $\\sigma>0$ consider the distribution $\\mathcal{G}(\\beta^{*},\\sigma)$ over $\\mathbb{R}^{d+1}$ such that $(X,y)\\sim\\mathcal{G}(\\beta^{\\ast},\\sigma)$ satisfy $X\\sim N(0,\\operatorname{Id})$ and $y=\\langle X,\\beta^{*}\\rangle+\\eta,$ where $\\eta\\sim N(0,\\sigma^{2})$ is independent of $X$ ", "page_idx": 27}, {"type": "text", "text": "There exist a set $\\mathcal{B}\\subset\\mathbb{R}^{d}$ of $k$ -sparse vectors, $0.99\\leqslant\\sigma\\leqslant1$ and a distribution $\\boldsymbol{Q}$ over $\\mathbb{R}^{d+1}$ ,such that if an SQ algorithm $\\mathcal{A}$ given access to a mixture $(1-\\varepsilon)\\mathscr{G}(\\beta^{\\ast},\\Sigma,\\sigma)+\\varepsilon\\mathscr{Q}$ for $\\beta^{*}\\in\\mathcal{B}$ outputs $\\hat{\\beta^{*}}$ suchthat $\\|\\beta^{*}-\\hat{\\beta}\\|\\leqslant10^{-5}$ then $\\mathcal{A}$ either ", "page_idx": 27}, {"type": "text", "text": "\u00b7makes $d^{c k^{c}/8}\\cdot k^{-2+2c}$ queries, \u00b7 or makes at least one query with tolerance smaller than $k^{-1+c}e^{O(1/\\varepsilon^{2})}$ ", "page_idx": 27}, {"type": "text", "text": "ProofNote that $(X,y)\\,\\sim\\,\\mathcal{G}(\\beta^{\\ast},\\sigma)$ satisfy $y\\,\\sim\\,N(0,\\sigma_{y}^{2})$ ,Wwhere $\\sigma_{y}^{2}\\,=\\,\\|\\beta^{*}\\|^{2}\\,+\\,\\sigma^{2}$ and $X|y\\,\\sim$ $\\begin{array}{r}{N\\Big(\\frac{y}{\\sigma_{y}^{2}}\\beta^{*},\\mathrm{Id}-\\frac{1}{\\sigma_{y}^{2}}\\beta^{*}\\beta^{*\\top}\\Big)}\\end{array}$ ", "page_idx": 27}, {"type": "text", "text": "We will use vectors $\\beta^{*}$ of $\\mathrm{norm~}10^{-5}$ . Denote $v\\;=\\;\\beta^{*}/\\lVert\\beta^{*}\\rVert$ and let $\\sigma^{2}\\,=\\,1\\,-\\,\\|\\beta^{*}\\|^{2}$ . Consider a distribution $\\begin{array}{r}{\\mathcal{M}_{\\mu}\\;=\\;(1\\,-\\,\\varepsilon)N\\big(\\mu,1-\\|\\beta^{*}\\|^{2}\\big)\\,+\\,\\varepsilon N\\big({-}\\frac{1-\\varepsilon}{\\varepsilon}\\mu,1\\big)}\\end{array}$ . Note that $\\chi^{2}\\bigl(\\mathcal{M}_{\\mu},N(0,1)\\bigr)\\ \\leqslant$ e(max(1/p2,\u00b22), and e/(1 - e)\u2264 O(\u03bc\u00b2) for \u03bc \u2265 10-4. Hence by Fact G.8, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\chi_{\\mathcal{D}}(Q_{v}^{\\prime},Q_{u}^{\\prime})\\leqslant e^{O(1/\\varepsilon^{2})}\\langle v,u\\rangle^{2}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "for all unit $\\boldsymbol{v},\\boldsymbol{u}\\in\\mathbb{R}^{d}$ ", "page_idx": 27}, {"type": "text", "text": "Using Fact G.6, we can apply Fact G.5 with $\\gamma=k^{2c-2}e^{O(1/\\varepsilon^{2})}$ \uff0c $\\rho=e^{O(1/\\varepsilon^{2})}$ \uff0c $\\gamma^{\\prime}=(\\rho-\\gamma)\\cdot k^{-2+2c}$ \uff0c we get that $\\mathcal{A}$ requires at least $d^{c k^{c}/8}k^{-2+2c}$ queries with tolerance greater than $k^{-1+c}e^{O(1/\\varepsilon^{2})}$ \u53e3 ", "page_idx": 27}, {"type": "text", "text": "Proposition G.10 (Formal version of Proposition 1.11). Let $k,d\\,\\in\\,\\mathbb{N},$ $k\\ \\leqslant\\ {\\sqrt{d}},$ $\\varepsilon\\;\\in\\;(0,1/2)$ $c\\,\\,\\in\\,\\,(0,\\,1)$ . For a vector $\\beta^{*}\\,\\in\\,\\mathbb{R}^{d}$ apositivedefinitematrix $\\Sigma$ and a number $\\sigma\\~>~0$ consider the distribution $\\mathcal{G}(\\beta^{*},\\Sigma,\\sigma)$ over $\\mathbb{R}^{d+1}$ such that $(X,y)\\,\\sim\\,{\\mathcal{G}}(\\beta^{\\ast},\\Sigma,\\sigma)$ satisfy $\\displaystyle X\\,\\sim\\,N(0,\\Sigma)$ and $y=\\left\\langle{X,\\beta^{*}}\\right\\rangle+\\eta,$ where $\\eta\\sim N(0,\\sigma^{2})$ isindependentof $X$ ", "page_idx": 28}, {"type": "text", "text": "There exist a set $\\mathcal{B}\\subset\\mathbb{R}^{d}$ of $k$ -sparse vectors, $\\begin{array}{r}{\\frac{1}{2}\\mathrm{Id}\\le\\Sigma\\le\\mathrm{Id},\\,0.99\\leqslant\\sigma\\leqslant1}\\end{array}$ and a distribution $\\boldsymbol{\\mathcal{Q}}$ over $\\mathbb{R}^{d+1}$ such thatif anSQ algorithm $\\mathcal{A}$ given access toa mixture $(1-\\varepsilon)\\mathscr{G}(\\beta^{\\ast},\\Sigma,\\sigma)+\\varepsilon\\mathscr{Q}$ for $\\beta^{*}\\in\\mathcal{B}$ outputs $\\hat{\\beta}$ such that $\\|\\beta^{*}-\\hat{\\beta}\\|\\leqslant10^{-5}\\sqrt{\\varepsilon}$ then $\\mathcal{A}$ either ", "page_idx": 28}, {"type": "text", "text": "makes dcke /8 . k-4+4c c queries, \u00b7 or makes at least one query with tolerance at least $k^{-2+2c}e^{O(1/\\varepsilon)}$ ", "page_idx": 28}, {"type": "text", "text": "Proof. Note that $(X,y)\\sim{\\mathcal{G}}(\\beta^{*},\\Sigma,\\sigma)$ satisfy $y\\sim N(0,\\sigma_{y}^{2})$ , where $\\sigma_{y}^{2}=\\beta^{*\\top}\\Sigma\\beta^{*}+\\sigma^{2}$ and $X|y\\sim$ $\\begin{array}{r}{N\\Big(\\frac{y}{\\sigma_{y}^{2}}\\Sigma\\beta^{*},\\Sigma-\\frac{1}{\\sigma_{y}^{2}}(\\Sigma\\beta^{*})(\\Sigma\\beta^{*})^{\\top}\\Big)}\\end{array}$ ", "page_idx": 28}, {"type": "text", "text": "We will use vectors $\\beta^{*}$ of norm $10^{-5}\\sqrt\\varepsilon$ .Denote $v=\\beta^{*}/\\lVert\\beta^{*}\\rVert$ and let $\\sigma^{2}=1\\!-\\!\\beta^{*\\top}\\Sigma\\beta^{*},\\Sigma=\\mathrm{Id}\\!-\\!c^{\\prime}v v^{\\top}$ \uff0c where $c^{\\prime}$ is a constant such that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\Sigma-\\frac{1}{\\sigma_{y}^{2}}(\\Sigma\\beta^{*})(\\Sigma\\beta^{*})^{\\top}=\\mathrm{Id}-c^{\\prime}v v^{\\top}-\\big(10^{-5}(1-c^{\\prime})^{2}\\varepsilon\\big)v v^{\\top}=\\mathrm{Id}-v v^{\\top}/3\\,.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "By [DKS19, Lemmas E.2], there exists a distribution $\\mathcal{M}$ that satisfies the assumption of Fact G.8 with $m=3$ and $g(\\varepsilon)=e^{O(1/\\varepsilon)}$ . Hence by Fact G.8, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\chi_{\\mathcal{D}}(Q_{v}^{\\prime},Q_{u}^{\\prime})\\leqslant e^{O(1/\\varepsilon)}\\langle v,u\\rangle^{4}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "for all unit $\\boldsymbol{v},\\boldsymbol{u}\\in\\mathbb{R}^{d}$ ", "page_idx": 28}, {"type": "text", "text": "Using Fact G.6, we can apply Fact G.5 with $\\gamma=k^{4c-4}e^{O(1/\\varepsilon)}$ \uff0c $\\rho=e^{O(1/\\varepsilon)}$ \uff0c $\\gamma^{\\prime}=(\\rho-\\gamma)\\cdot k^{-4+4c}$ we get that $\\mathcal{A}$ requires at least $d^{c k^{c}/8}k^{-4+4c}$ queries with tolerance smaller than $k^{-2+2c}e^{O(1/\\varepsilon)}$ \u53e3 ", "page_idx": 28}, {"type": "text", "text": "H  Sub-exponential designs ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Recall that a distribution $\\mathcal{D}$ in $\\mathbb{R}^{d}$ is called $L$ -sub-exponential, if it has $(L t)$ -bounded $t$ -th moment for each $t\\in\\mathbb{N}$ In particular, all log-concave distributions are $L$ -sub-exponential for some $L\\leqslant O(1)$ ", "page_idx": 28}, {"type": "text", "text": "In this section we discuss how we can improve the dependence of the sample complexity on $\\varepsilon$ if (in addition to the assumptions of Theorem B.3) we assume that $\\mathcal{D}$ is $L$ -sub-exponential. For these designs we do not need a truncation. ", "page_idx": 28}, {"type": "text", "text": "First, let us show how the gradient bound Lemma B.5 modifies in this case. It can be obtained directly from Bernstein's inequality for sub-exponential distributions ([RH23, Theorem 1.13]) ", "page_idx": 28}, {"type": "text", "text": "Lemma H.1. With probability at least $1-\\delta/10_{\\mathrm{{z}}}$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left\\|\\frac{1}{n}\\sum_{i\\in[n]}\\phi(\\eta_{i})X_{i}^{*}\\right\\|_{\\infty}\\leqslant10\\sqrt{\\frac{\\|\\Sigma\\|\\log(d/\\delta)}{n}}+10\\frac{\\sqrt{\\|\\Sigma\\|}\\cdot L\\cdot\\log(d/\\delta)}{n}\\,.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "The proof of strong convexity bound (Lemma B.6) is exactly the same, with $X^{\\prime}(\\tau)=X^{*}$ ", "page_idx": 28}, {"type": "text", "text": "Finally, we need to bound $\\begin{array}{r}{\\left\\|\\frac{1}{n}\\sum_{i=1}^{n}\\left(\\boldsymbol{X}_{i}^{*}\\right)^{\\otimes2t}-\\mathbb{E}\\!\\left(\\boldsymbol{X}_{1}^{*}\\right)^{\\otimes2t}\\right\\|_{\\infty}}\\end{array}$ , since we need to prove Lemma D.2 for sub-exponential distributions. By Lemma C.1. from $[\\mathrm{DKK}^{+}22]$ , for all $L$ -sub-exponential distributions, with probability $1-\\delta$ \uff0c ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left\\lVert\\frac{1}{n}\\sum_{i=1}^{n}\\left(X_{i}^{*}\\right)^{\\otimes2t}-\\mathbb{E}\\bigl(X_{1}^{*}\\bigr)^{\\otimes2t}\\right\\rVert_{\\infty}\\leqslant O\\left(\\sqrt{\\frac{t\\log(d/\\delta)}{n}}\\cdot\\left(10L\\sqrt{\\|\\Sigma\\|}\\cdot t^{2}\\log(d/\\delta)\\right)^{2t}\\right)\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Hence with $\\begin{array}{r}{\\colon n\\gtrsim K^{2t}\\cdot\\big(10t^{2}\\log(d/\\delta)\\big)^{4t+1},\\,\\mathrm{we~get}\\left\\|\\frac{1}{n}\\sum_{i=1}^{n}\\big(X_{i}^{\\ast}\\big)^{\\otimes2t}-\\mathbb{E}\\big(X_{1}^{\\ast}\\big)^{\\otimes2t}\\right\\|_{\\infty}\\leqslant L^{2t}\\|\\Sigma\\|^{t}/K^{t},}\\end{array}$ so we get the conclusion of Lemma D.2. ", "page_idx": 29}, {"type": "text", "text": "Putting everything together, the sample complexity is ", "page_idx": 29}, {"type": "equation", "text": "$$\ni\\gtrsim\\frac{k\\log(d/\\delta)}{\\varepsilon^{2-1/t}}+\\frac{\\left(k^{2}\\log(d/\\delta)\\right)10^{5s/(s-2)}M_{s}^{s/(s-2)}\\kappa(\\Sigma)^{4+s/(s-2)}}{\\alpha}+k^{2t}\\cdot\\kappa(\\Sigma)^{2t}\\cdot\\left(10^{6}\\cdot t^{2}\\log(d/\\delta)\\right)^{4t+1}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Note that we can use $s=4$ and $M_{s}=4L$ ", "page_idx": 29}, {"type": "text", "text": "Consider the case when $\\mathcal{D}$ is log-concave, so $L\\leqslant O(1)$ . For $\\kappa(\\Sigma)\\leqslant O(1),\\,\\alpha\\geqslant\\Omega(1),\\,t=1,$ with high probability we get error $O(\\sigma{\\sqrt{\\varepsilon}})$ as long as ", "page_idx": 29}, {"type": "equation", "text": "$$\nn\\gtrsim\\frac{k\\log d}{\\varepsilon}+k^{2}\\cdot(\\log d)^{5}\\,.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Similarly, for $\\kappa(\\Sigma)\\leqslant O(1),\\,\\alpha\\geqslant\\Omega(1),\\,t=2,$ with high probability we get error $O(M\\sigma\\varepsilon^{3/4})$ (where $M\\leqslant O({\\sqrt{\\log d}})$ is the same as in Theorem 1.7) as long as ", "page_idx": 29}, {"type": "equation", "text": "$$\nn\\gtrsim\\frac{k\\log d}{\\varepsilon^{3/2}}+k^{4}\\cdot\\left(\\log d\\right)^{9}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Note that for sub-Gaussian distributions one can use better tail bounds and the polylog $(d)$ factors should be better in this case. ", "page_idx": 29}, {"type": "text", "text": "1 Concentration Bounds ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Throughout the paper we use the following versions of versions of Bernstein's inequality. The proofs can be found in [Tro15]. ", "page_idx": 29}, {"type": "text", "text": "Fact I.1 (Bernstein inequality). Let $L>0$ and let $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ be a zero-mean random variable. Let $x_{1},\\ldots,x_{n}$ be i.d. copies of $x$ Suppose that $|x|\\leqslant L$ Then the estimator $\\textstyle{\\bar{x}}={\\frac{1}{n}}\\sum_{i=1}^{n}x_{i}$ satisfies for all $t>0$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{P}(|\\bar{x}|\\geqslant t)\\leqslant2\\cdot\\exp\\!\\left(-\\frac{t^{2}n}{2\\,\\mathbb{E}\\,x^{2}+L t}\\right).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Fact 1.2 (Bernstein inequality for covariance). Let $L\\ >\\ 0$ and let $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ be a $d$ -dimensional random vector. Let $x_{1},\\ldots,x_{n}$ be i.i.d. copies of $x$ .Suppose that $\\|x\\|^{2}\\leqslant L$ Then the estimator $\\begin{array}{r}{\\bar{\\Sigma}=\\frac{1}{n}\\sum_{i=1}^{n}x_{i}x_{i}^{\\top}}\\end{array}$ satisfies for ll $t>0$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{P}\\big(\\big\\|\\bar{\\Sigma}-\\mathbb{E}\\,x x^{\\top}\\big\\|\\geqslant t\\big)\\leqslant2d\\cdot\\mathrm{exp}\\bigg({-\\frac{t^{2}n}{2L\\|\\Sigma\\|+L t}}\\bigg)\\,.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Abstract, Section 1, Section 1.1 and Section 2. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 30}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: Section 1.1 and Section 2. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should refect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational effciency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 30}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Sections 1.1 and 2 and Appendices A to I ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 31}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: This is a theory paper, and does not include experiments. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b)If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 31}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: This is a theory paper, and does not include experiments. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : / /nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 32}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: This is a theory paper, and does not include experiments. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 32}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: This is a theory paper. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative errorrates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: This is a theory paper, and does not include experiments. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 33}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: The research conforms, in every respect, with the NeurIPs Code of Ethics. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 33}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: There is no societal impact of the work performed. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to ", "page_idx": 33}, {"type": "text", "text": "generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 34}, {"type": "text", "text": "\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 34}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 34}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: The paper does not use existing assets. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 34}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 35}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 35}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 35}]