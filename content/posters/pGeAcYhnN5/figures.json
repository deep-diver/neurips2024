[{"figure_path": "pGeAcYhnN5/figures/figures_3_1.jpg", "caption": "Figure 1: Illustration of CTC-drafter model training and inference strategy.", "description": "This figure illustrates the architecture and workflow of the CTC-drafter model, which is a novel approach for accelerating large language model inference. The figure shows two main parts: the training phase and the inference phase. In the training phase, the model learns to predict sequences of tokens using a connectionist temporal classification (CTC) loss function.  In the inference phase, the CTC-drafter model generates candidate sequences of tokens, and these are then verified by a base language model. The overall aim of the architecture is to improve inference speed by generating higher-quality candidate sequences during the draft phase and achieving a higher acceptance rate.", "section": "3 CTC-drafter Model"}, {"figure_path": "pGeAcYhnN5/figures/figures_7_1.jpg", "caption": "Figure 2: Average number of tokens accepted per decoding step in different question categories on MT-bench, with Vicuna-7B as base model. The performance on Vicuna-13B and Vicuna-33B is consistent with this result. The blue color represents CTC-drafter method, orange color represents Medusa method and green color represents baseline. All evaluation experiments are conducted on the same device.", "description": "This figure shows the average number of tokens accepted per decoding step for different question categories in the MT-bench dataset.  Three methods are compared: CTC-drafter (blue), Medusa (orange), and Vanilla (green). The results are shown for Vicuna-7B, but the trends are consistent across different Vicuna model sizes (7B, 13B, 33B).  The chart illustrates the relative performance of each method across various question types, indicating how effectively each approach predicts multiple tokens simultaneously.", "section": "4.3 Ablation experiments"}, {"figure_path": "pGeAcYhnN5/figures/figures_8_1.jpg", "caption": "Figure 1: Illustration of CTC-drafter model training and inference strategy.", "description": "This figure illustrates the architecture and training/inference procedures of the proposed CTC-drafter model.  The training process shows the base model's output feeding into an attention draft module that generates draft tokens using the CTC algorithm.  These tokens are combined, and the resulting sequences are used to calculate the CTC loss. The inference process similarly shows how the draft tokens are generated from the base model using the attention draft module, these tokens undergo CTC transformation, and candidate sequences are produced to improve speed and accuracy.", "section": "3 CTC-drafter Model"}, {"figure_path": "pGeAcYhnN5/figures/figures_11_1.jpg", "caption": "Figure 1: Illustration of CTC-drafter model training and inference strategy.", "description": "This figure illustrates the architecture and training/inference processes of the CTC-drafter model.  The training process shows how the base model's hidden states are fed into an attention draft module, which uses a CTC loss function to predict probability distributions of draft tokens.  These draft tokens are then combined to create candidate sequences that are verified by the base model. The inference process is similar, but instead of training, the draft module generates candidates that are verified and accepted or rejected by the base model. This allows for faster inference compared to traditional methods.", "section": "3 CTC-drafter Model"}]