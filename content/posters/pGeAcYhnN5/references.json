{"references": [{"fullname_first_author": "Josh Achiam", "paper_title": "GPT-4 Technical Report", "publication_date": "2023-03-08", "reason": "This paper provides technical details of GPT-4, a large language model relevant to the current research on LLM inference acceleration."}, {"fullname_first_author": "Zachary Ankner", "paper_title": "Hydra: Sequentially-dependent draft heads for medusa decoding", "publication_date": "2024-02-05", "reason": "This paper introduces Hydra, a method that improves upon Medusa decoding by incorporating sequential dependencies between draft tokens, directly relevant to the proposed CTC-based approach."}, {"fullname_first_author": "Tianle Cai", "paper_title": "Medusa: Simple framework for accelerating LLM generation with multiple decoding heads", "publication_date": "2023-00-00", "reason": "This paper proposes Medusa, a significant prior work in speculative decoding that uses multiple decoding heads for parallel generation, providing a strong baseline for comparison and context."}, {"fullname_first_author": "Charlie Chen", "paper_title": "Accelerating large language model decoding with speculative sampling", "publication_date": "2023-02-01", "reason": "This paper introduces speculative sampling, a key method in LLM inference acceleration that the current work builds upon and improves, making it highly relevant to the core methodology."}, {"fullname_first_author": "Alex Graves", "paper_title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks", "publication_date": "2006-00-00", "reason": "This foundational paper introduces Connectionist Temporal Classification (CTC), the core algorithm used in the proposed CTC-based draft model, providing the theoretical basis for the novel approach."}]}