{"references": [{"fullname_first_author": "Ting Chen", "paper_title": "A simple framework for contrastive learning of visual representations", "publication_date": "2020-00-00", "reason": "This paper is foundational for contrastive learning, a key self-supervised learning approach that DiGGR builds upon."}, {"fullname_first_author": "Yuning You", "paper_title": "Graph contrastive learning with augmentations", "publication_date": "2020-00-00", "reason": "This paper is highly relevant due to its focus on graph contrastive learning, a method DiGGR improves on by leveraging generative models and disentanglement."}, {"fullname_first_author": "Jacob Devlin", "paper_title": "Bert: Pre-training of deep bidirectional transformers for language understanding", "publication_date": "2018-00-00", "reason": "BERT is a highly influential generative pre-training model that inspired the generative aspect of DiGGR."}, {"fullname_first_author": "Kaiming He", "paper_title": "Masked autoencoders are scalable vision learners", "publication_date": "2022-00-00", "reason": "MAE, introduced in this paper, directly inspired the masked autoencoder architecture used in DiGGR, particularly its application to graph data."}, {"fullname_first_author": "Yoshua Bengio", "paper_title": "Representation learning: A review and new perspectives", "publication_date": "2013-00-00", "reason": "This paper provides a comprehensive overview of representation learning, a foundational concept for the disentanglement techniques in DiGGR."}]}