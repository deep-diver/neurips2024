[{"type": "text", "text": "Clustering with Non-adaptive Subset Queries ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Hadley Black Euiwoong Lee Arya Mazumdar Barna Saha UC San Diego University of Michigan UC San Diego UC San Diego ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recovering the underlying clustering of a set $U$ of $n$ points by asking pair-wise same-cluster queries has garnered significant interest in the last decade. Given a query $S\\subset U$ , $|S|=2$ , the oracle returns yes if the points are in the same cluster and $^{n o}$ otherwise. We study a natural generalization of this problem to subset queries for $|S|>2$ , where the oracle returns the number of clusters intersecting $S$ . Our aim is to determine the minimum number of queries needed for exactly recovering an arbitrary $k$ -clustering. We focus on non-adaptive schemes, where all the queries are asked in one round, thus allowing for the querying process to be parallelized, which is a highly desirable property. ", "page_idx": 0}, {"type": "text", "text": "For adaptive algorithms with pair-wise queries, the complexity is known to be $\\Theta(n k)$ , where $k$ is the number of clusters. In contrast, non-adaptive pair-wise query algorithms are extremely limited: even for $k=3$ , such algorithms require $\\bar{\\Omega}(n^{2})$ queries, which matches the trivial $O(n^{2})$ upper bound attained by querying every pair of points. Allowing for subset queries of unbounded size, $O(n)$ queries is possible with an adaptive scheme. However, the realm of non-adaptive algorithms remains completely unknown. Is it possible to attain algorithms that are nonadaptive while still making a near-linear number of queries? ", "page_idx": 0}, {"type": "text", "text": "In this paper, we give the first non-adaptive algorithms for clustering with subset queries. We provide, (i) a non-adaptive algorithm making $O(n\\log^{2}n\\operatorname*{iog}k)$ queries which improves to $O(n\\log k)$ when the cluster sizes are within any constant factor of each other, (ii) for constant $k$ , a non-adaptive algorithm making $O(n\\log\\log n)$ queries. In addition to non-adaptivity, we take into account other practical considerations, such as enforcing a bound on query size. For constant $k$ , we give an algorithm making $\\widetilde{\\cal O}(n^{2}/s^{2})$ queries on subsets of size at most $s\\leq\\sqrt{n}$ , which is optimal among all non-adaptive algorithms within a $\\log n$ -factor. For arbitrary $k$ , the dependence varies as $\\tilde{O}(n^{2}/s)$ . ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Clustering is one of the most fundamental problems in unsupervised machine learning, and permeates beyond the boundaries of statistics and computer science to social sciences, economics and so on. The goal of clustering is to partition items so that similar items are in the same group. The applications of clustering are manifold. However, finding the underlying clusters is sometimes hard for an automated process due to data being noisy, incomplete, but easily discernible by humans. Motivated by this scenario, in order to improve the quality of clustering, early works have studied the so-called clustering under \u201climited supervision\u201d (e.g.,[1, 2]). Balcan and Blum initiated the study of clustering under active feedback [3] where given the current clustering solution, the users can provide feedback whether a cluster needs to be merged or split. Perhaps a simpler query model would be where users only need to answer the number of clusters, and that too only on a subset of points without requiring to analyze the entire clustering. This scenario is common in unsupervised learning problems, where a centralized algorithm aims to compute a clustering by crowdsourcing. The crowd-workers play the role of an oracle here, and are able to answer simple queries that involve a small subset of the universe. ", "page_idx": 0}, {"type": "text", "text": "Mazumdar and Saha [4, 5, 6], and in independent works Mitzenmacher and Tsourakis [7], as well as Asthani, Kushagra and Ben-David [8] initiated a theoretical study of clustering with pair-wise aka same-cluster queries. Given any pair of points $u,v$ , the oracle returns whether $u$ and $v$ belong to the same cluster or not. Such queries are easy to answer and lend itself to simple implementations [9]. This has been subsequently extremely well-studied in the literature, e.g. [10, 11, 4, 12, 13]. In fact, triangle-queries have also been studied, e.g. [14]. Moreover, clustering with pair-wise queries is intimately related to several well-studied problems such as correlation clustering [15, 16, 17, 10, 18], edge-sign prediction problem [19, 7], stochastic block model [20, 21] etc. ", "page_idx": 1}, {"type": "text", "text": "Depending on whether there is an interaction between the learner/algorithm and the oracle, the querying algorithms can be classified as adaptive and non-adaptive [5]. In adaptive querying, the learner can decide the next query based on the answers to the previous queries. An algorithm is called non-adaptive if all of its queries can be specified in one-round. Non-adaptive algorithms can parallelize the querying process as they decide the entire set of queries apriori. This may greatly speed up the algorithm in practice, significantly reducing the time to acquire answers [22]. Thus, in a crowdsourcing setting being non-adaptive is a highly desirable property. On the flip side, this makes non-adaptive algorithms significantly harder to design. In fact, when adaptivity is allowed, nk pair-wise queries are both necessary and sufficient to recover the entire clustering, where $n$ is the number of points in the ground set to be clustered and $k$ (unknown) is the number of clusters. However as shown in [5] and our Theorem C.1, even for $k\\,=\\,3$ , even randomized non-adaptive algorithms can do no better than the trivial $O(n^{2})$ upper bound attained by querying all pairs. ", "page_idx": 1}, {"type": "text", "text": "We study a generalization of pair-wise queries to subset queries, where given any subset of points, the oracle returns the number of clusters in it. We consider the problem of recovering an unknown $k$ -clustering (a partition) on a universe $U$ of $n$ points via black-box access to a subset query oracle. More precisely, we assume that there exists a groundtruth partitioning of $\\textstyle U=\\bigsqcup_{i=1}^{k}{\\bar{C}}_{i}$ , and upon querying with a subset $S\\subseteq U$ , the oracle returns $q(S)=|\\{\\bar{i}:C_{i}\\cap S\\neq\\emptyset\\}|$ , the number of clusters intersecting $S$ . Considering the limitations of pair-wise queries for non-adaptive schemes, we ask the question if it is possible to use subset queries to design significantly better non-adaptive algorithms. ", "page_idx": 1}, {"type": "text", "text": "In addition to being a natural model for interactive clustering, this problem also falls into the growing body of work known as combinatorial search [23, 24] where the goal is to reconstruct a hidden object by viewing it through the lens of some indirect query model (such as group testing [25, 26, 24, 27, 28]). The problem is also intimately connected to coin weighing where given a hidden vector $x\\in\\{0,1\\}^{n}$ , the goal is to reconstruct $x$ using queries of the form $\\textstyle{\\bar{q}}(S):=\\sum_{i\\in S}x_{i}$ for $S\\subseteq[n]$ . It is known that $\\Theta(n/\\log n)$ is the optimal number of queries [29, 30, 31], which can be obtained by a non-adaptive algorithm. There are improvements for the case when $\\|x\\|_{1}=d$ for $d\\ll n$ [32, 33, 34]. Moreover, there has been significant work on graph reconstruction where the task is to reconstruct a hidden graph $G=(V,E)$ from queries of the form $q(S,T):=|\\{(u,v)\\in E:u\\in S,v\\in T\\}|$ for subsets $S,T\\subseteq V$ . [35, 36, 37, 38]. There are also algorithms that perform certain tasks more efficiently than learning the whole graph (sometimes using different types of queries) [39, 40, 41, 42, 43, 44, 45, 46], and quantum algorithms that use fewer queries than classical algorithms [47]. ", "page_idx": 1}, {"type": "text", "text": "It is not too difficult to show that an algorithm making $O(n\\log k)$ queries (Appendix H) is possible for $k$ -clustering, while $\\Omega(n)$ queries is an obvious information theoretic lower bound since each query returns $\\log k$ bits of information and the number of possible $k$ -clusterings is $k^{n}=2^{n\\log k}$ . In fact, it is possible to have an algorithm with $O(n)$ query complexity (personal communication, Chakrabarty and Liao). However, both of these algorithms are adaptive, ruling them out for the non-adaptive setting. So far, the non-adaptive setting of this problem remained unexplored. ", "page_idx": 1}, {"type": "text", "text": "1.1 Results ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Our main results showcase the significant strength of using subset queries in the non-adaptive setting. We give randomized algorithms that recover the exact clustering with probability $1-\\delta$ , for any arbitrary constant $\\delta>0$ using only near-linear number of subset queries. ", "page_idx": 1}, {"type": "text", "text": "Theorem 1.1. (Theorem 2.5, simplified) There is a randomized, non-adaptive $k$ -clustering algorithm making $O(n\\log^{2}n\\log k)$ subset queries. ", "page_idx": 1}, {"type": "text", "text": "For constant $k$ , this dependency can be further improved. ", "page_idx": 1}, {"type": "text", "text": "Theorem 1.2. (Theorem 2.2, simplified) There is a randomized, non-adaptive $k$ -clustering algorithm making $O(n\\log\\log n)$ subset queries when $k$ is any constant. ", "page_idx": 2}, {"type": "text", "text": "Note that the algorithm of Theorem 1.2 works for any value of $k$ , but its dependence on this parameter is inferior to that of Theorem 1.1 (see the formal version Theorem 2.2 for the exact dependence on $k$ ). Thus, we state the theorem above for constant $k$ to emphasize the much improved dependence on $n$ . ", "page_idx": 2}, {"type": "text", "text": "Our algorithms also run in polynomial time, and generalizes to work with queries of bounded size. ", "page_idx": 2}, {"type": "text", "text": "Bounding query size: Another practical consideration is query size, $s$ . Depending on the scenarios, and capabilities of the oracle, it may be easier to handle queries on small subsets. An extreme case is pair-wise queries ( $\\mathbf{\\chi}_{s}=2\\mathbf{\\chi}$ ), where $O(n k)$ pair queries are enough with adaptivity but any non-adaptive algorithm has to use $\\Omega(n^{2})\\,$ queries even for $k=3$ . Since a subset query on $S$ can be simulated by $\\binom{|\\protect\\nabla|}{2}$ pair queries, we immediately get the following theorem. ", "page_idx": 2}, {"type": "text", "text": "Theorem 1.3. (Corollary C.2, restated) Any non-adaptive $k$ -clustering algorithm that is only allowed to query subsets of size at most s must make at least $\\begin{array}{r}{\\Omega(\\operatorname*{min}(\\frac{n^{2}}{s^{2}},n))}\\end{array}$ ) queries. ", "page_idx": 2}, {"type": "text", "text": "Theorems 1.1 and 1.2 above show that this can be bypassed by allowing larger subset queries. However, some of these queries are of size $\\Omega(n)$ , and this raises the que\u221astion, is there a near-linear non-adaptive algorithm which only queries subsets of size at most ${\\bar{O}}({\\sqrt{n}})$ ? We answer this in the affirmative, implying that our lower bound is tight in terms of $s$ . ", "page_idx": 2}, {"type": "text", "text": "Theorem 1.4 (Theorem A.1, informal). There is a \u221anon-adaptive $k$ -clustering algorithm making $O(n\\log n\\log\\log n)$ subset queries of size at most $O({\\sqrt{n}})$ when $k$ is any constant. For all sufficiently small $s=o({\\sqrt{n}})$ , the algorithm makes $O({\\frac{n^{2}}{s^{2}}}\\log{n})$ subset queries of size at most s. ", "page_idx": 2}, {"type": "text", "text": "The result also extends to arbitrary $k$ with slightly worse dependency on $s$ (Theorem 2.5). Our algorithm for bounded queries from Theorem 1.4 has the additional desirable property of being sample-based meaning that each of its queries is a set formed by independent, uniform samples. I.e. the algorithm specifies a query size $t\\leq s$ , and then receives $(S,q(S))$ where $S$ is formed by $t$ i.i.d. uniform samples from $U$ . Being sample-based enables the algorithm to leave the task of curating each query up to the individual answering the query. The algorithm needs only to specify the query sizes, and then recover the clustering once the queries have been curated and answered. ", "page_idx": 2}, {"type": "text", "text": "The \"roughly balanced\" case: Next, we consider the natural special case of recovering a $k$ -clustering when the cluster sizes are within a constant factor of one another. Informally, let us call such a clustering \"roughly balanced\". ", "page_idx": 2}, {"type": "text", "text": "Theorem 1.5 (Theorems B.1 and E.1, informal). There are non-adaptive algorithms for recovering a roughly bala nced $k$ -clustering which make (a) $O(n\\log k)$ subset queries when $\\begin{array}{r}{k\\le C(\\frac{n}{\\log^{3}n})}\\end{array}$ , and (b) $O(n\\log^{2}k)$ subset queries for any $k\\leq n$ . ", "page_idx": 2}, {"type": "text", "text": "Allowing two rounds of adaptivity Finally, we show if we allow an extra round of adaptivity, then that helps to improve the dependency on the logarithmic factors further. Specifically, we prove the following theorems. ", "page_idx": 2}, {"type": "text", "text": "Theorem 1.6 (Theorems F.1 and F.3, informal). There is a 2-round deterministic $k$ -clustering algorithm making $O(n\\log k)$ subset queries. There is a randomized 2-round algorithm for recovering a roughly-balanced $k$ -clustering making $O(n\\log\\log k)$ subset queries. ", "page_idx": 2}, {"type": "text", "text": "Organization: The remainder of the paper is organized as follows. In Section 2, we give our main results developing non-adaptive algorithms with near-linear query complexity Theorems 1.1 and 1.2. Our results for sample-based, bounded query algorithms are given in Appendix A. Finally, we prove our results for the balanced setting in Appendix B, our lower bounds in Appendix C, and our results for two-round algorithms in Appendix F. ", "page_idx": 2}, {"type": "text", "text": "2 Algorithms with Nearly Linear Query Complexity ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section we describe the algorithms behind our main results, Theorems 1.1 and 1.2, and give formal proofs of their correctness. In Section 2.1 we describe an algorithm making $O(n\\log\\log n)$ subset queries when the number of clusters $k$ is assumed to be a constant. In general, the dependence on the number of clusters is $O(k\\log k)$ . In Section 2.2, we give an alternative algorithm with ${\\widetilde{O}}(n)$ query complexity for any $k\\leq n$ . ", "page_idx": 2}, {"type": "text", "text": "Warm Up. When there are only 2 clusters, there is a trivial non-adaptive algorithm making $O(n)$ pair queries: Choose an arbitrary $x\\in U$ and query $\\{x,y\\}$ for every $y\\in U$ . The set of points $y$ where $q(\\{x,y\\})=1$ form one cluster, and the second cluster is the complement. If we allow one more round of adaptivity, then for 3-clustering we could repeat this one more time and again get an $O(n)$ query algorithm. However, for non-adaptive 3-clustering it is impossible to do better than the trivial $\\bar{O}(\\bar{n}^{2})$ algorithm (see Theorem C.1). Essentially, this is because in order to distinguish the clusterings $(\\{x\\},\\{y\\},U\\setminus\\{x,y\\})$ and $(\\{x,y\\},\\emptyset,U\\setminus\\{x,y\\})$ the algorithm must query $\\{x,y\\}$ and their are $\\binom{n}{2}$ ways to hide this pair. Overcoming this barrier using subset queries require significant new ideas. ", "page_idx": 3}, {"type": "text", "text": "Our main ideas are best communicated by focusing on the case of 3-clustering. It suffices to correctly reconstruct the two largest clusters, since the third cluster is just the complement of their union. Let $A,B$ denote the largest, and second largest clusters, respectively. Since $|\\mathbf{\\bar{{A}}}|\\ge n/3$ , it is easy to find: sample a random $x\\in U$ and query $\\{\\bar{x_{}},y\\}$ for every $y\\in U$ . The cluster containing $x$ is precisely $\\{y\\,\\in\\,U\\colon q(\\{x,y\\})\\,=\\,1\\}$ . With probability at least $1/3$ , we have $x\\,\\in\\,A$ and so repeating this a constant number of times will always recover $A$ . On the other hand, $B$ may be arbitrarily small and in this case the procedure clearly fails to recover it. The first observation is that once we know $A$ , we can exploit larger subset queries to explore $U\\setminus A$ since $q(S\\setminus A)=q(S)-\\mathbf{1}(S\\cap A\\neq\\emptyset)$ Importantly, the algorithm is non-adaptive and so the choice of $S$ cannot depend on $A$ , but we are still able to exploit this trick with the following two strategies. Let $\\delta n=|\\boldsymbol{B}|$ denote the size of $B$ and note that this implies $|A|\\geq(1-2\\delta)n$ since the third cluster is of size at most $B$ . ", "page_idx": 3}, {"type": "text", "text": "Strategy $^{\\,l}$ : Suppose a query $S$ contains exactly one point outside of $A$ , i.e. $S\\setminus A=\\{x\\}$ . Then, for $\\overline{{y\\notin A,\\,q(S\\cup\\{y\\})}}=q(S)$ iff $x,y$ belong to the same cluster. Thus, we can query $S\\cup\\{y\\}$ for every $y\\in U$ to learn the cluster containing $x$ . If $S$ is a random set of size $t\\approx1/\\delta$ , then the probability that $\\vert S\\setminus A\\vert=1$ is at least $t\\cdot\\delta\\cdot(1-2\\delta)^{t-1}=\\Omega(1)$ . Of course, we do not know $\\delta$ , but we can try $t=2^{p}$ for every $p\\leq\\log n$ and one of these choices will be within a factor of 2 from $1/\\delta$ . This gives us an $O(n\\log n)$ query algorithm since we make $n$ queries per iteration. ", "page_idx": 3}, {"type": "text", "text": "Strategy 2: Suppose $S$ intersects $A$ and contains exactly two points outside of $A$ , i.e. $S\\setminus A=\\{x,y\\}$ Then, ${\\overline{{q(\\{x,y\\})}}}=q(S)-1$ which tells us whether or not $x,y$ belong to the same cluster. If $x,y$ belong to same cluster, add it to a set $E$ , and let $G(U\\setminus A,E)$ denote a graph on the remaining points with this set of edges. By transitivity, a connected component in this graph corresponds to a subset of one of the remaining two clusters. In particular, if the induced subgraph, $G[B]$ , is connected, then we recover $B$ . Moreover, if $S$ is a random set of size $t\\approx1/\\delta$ , then the probability that two points land in $B$ and the rest land in $A$ is at least $\\binom{t}{2}\\cdot\\delta^{2}\\cdot(1-2\\delta)^{t-2}=\\Omega(1)$ . A basic fact from random graph theory says that after $\\approx|B|\\ln|B|\\leq\\delta\\bar{n}\\ln n$ occurrences of this, $G[B]$ becomes connected with high probability and so querying $\\Omega(\\delta n\\ln n)$ random $S$ of size $\\approx1/\\delta$ will suffice. Again, we try $t=2^{p}$ for every $p\\leq\\log n$ , resulting in a total of $\\begin{array}{r}{\\approx n\\ln n\\sum_{p}2^{-p}={\\stackrel{.}{O}}(n\\log n)}\\end{array}$ queries. ", "page_idx": 3}, {"type": "text", "text": "Finally, we can combine strategies (1) and (2) as follows to obtain our $O(n\\log\\log n)$ query algorithm. The main observation is that the query complexity of strategy (2) improves greatly if we assume that $|B|$ riys  csommalpll eexniotyu gbhe.c Iof mwees $\\delta\\leq{\\frac{\\cdot}{\\log n}}$ .n eOend  tthoe  tortyh $t=2^{p}\\geq\\log n$ sasnudm seo  tthhaet $\\begin{array}{r}{\\approx n\\ln n\\sum_{p\\geq\\log\\log n}2^{-p}=O(n)}\\end{array}$   \n$\\begin{array}{r}{\\bar{\\delta}>\\frac{1}{\\log n}}\\end{array}$ , then in strategy (1) we o nly need to try $p\\leq\\log\\log n$ yielding a total of $O(n\\log\\log n)$ queries. Combining these yields the final algorithm. ", "page_idx": 3}, {"type": "text", "text": "Remark 2.1 (On approximate clustering). We point out that these ideas can be used to obtain more efficient algorithms for the easier task of correctly clustering a $(1-\\alpha)$ -fraction of points. In this setting we can ignore the case of $\\delta<\\alpha/2$ (recall the definition of \u03b4 above) as this will only result in an incorrect classification of an $\\alpha$ -fraction of points. Thus, for example, one can employ \"strategy $I^{\\prime\\prime}$ above, but only iterate over $p\\leq\\log(2/\\alpha)$ , leading to an $\\begin{array}{r}{\\dot{O}(n\\log{\\frac{1}{\\alpha}})}\\end{array}$ query algorithm. However, in this paper we focus on the more challenging task of recovering the clustering exactly, and leave the possibility of more efficient approximate algorithms as a possible direction of future work. ", "page_idx": 3}, {"type": "text", "text": "Algorithm. A full description of the algorithm is given in pseudocode Alg. 1, which is split into two phases: a \"query selection phase\", which describes how queries are chosen by the algorithm, and a \"reconstruction phase\" which describes how the algorithm uses the query responses to determine the clustering. Both phases contain a for-loop iterating over all $p\\in\\{0,\\bar{1},\\bar{\\ldots},\\bar{\\log n}\\}$ where the goal of the algorithm during the $p$ \u2019th iteration is to learn all remaining clusters of size at least $\\frac{n}{2k\\!\\cdot\\!2^{p}}$ . This is accomplished by two different strategies depending on whether $p$ is small or large. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "When $p\\leq\\log\\log n$ , the algorithm samples $O(k\\log k)$ random sets $T$ formed by $2^{p}$ samples from $U$ and makes a query on $T$ and $T\\cup\\{x\\}$ for every $x\\in U$ (see lines 5-9 of Alg. 1). Let $\\mathcal{R}_{p}$ be the union of all clusters reconstructed before phase $p$ (i.e., clusters of size at least $\\!\\,{\\frac{n}{2k\\cdot2^{p-1}}}^{\\,\\!\\!{\\big/}}$ ). If such a $T$ contains exactly one point $z\\in T\\setminus\\mathcal{R}_{p}$ belonging to an unrecovered cluster, then we can use these queries to learn the cluster containing $z$ (see lines 24-28 of Alg. 1), since for $x\\in U\\setminus\\mathcal{R}_{p},q(T)=q(\\bar{T}\\cup\\{x\\})$ if and only if $x,z$ belong to the same cluster. Moreover, we show that this occurs with probability $\\Omega(1)$ and repeat this $O(k\\log k)$ times to ensure that every cluster $C$ where $|C|\\in\\left[{\\frac{n}{2k\\cdot2^{p}}},{\\frac{n}{2k\\cdot2^{p-1}}}\\right)$ is learned with high probability. The total number of queries made during iterations $p\\leq\\log\\log n$ is $O(n\\log\\log n\\cdot\\bar{k}\\log k)$ . ", "page_idx": 4}, {"type": "text", "text": "Whe $p>\\log\\log n$ , the algorithm queries $O(n k\\cdot{\\frac{\\log n}{2p}})$ rand\u2212opm  sets $T$ 1again formed by $2^{p}$ samples from $U$ (see lines 11-14 of Alg. 1). Note that $\\begin{array}{r}{\\sum_{p>\\log\\log n}2^{-p}=O\\big(\\frac{1}{\\log n}\\big)}\\end{array}$ and so the total number of queries made during these iterations is $O(n k)$ . ", "page_idx": 4}, {"type": "text", "text": "We now describe the reconstruction phase (see lines 32-37 of Alg. 1). If $T$ contains exactly two points $x,y\\in T\\setminus\\mathcal{R}_{p}$ belonging to unrecovered clusters, then we can use the fact that we already know the clustering on $\\mathcal{R}_{p}$ to tell whether or not $x,y$ belong to the same cluster or not, i.e. we can compute $q(\\{x,y\\})\\in\\{1,\\dot{2}\\}$ from $q(T)$ . We then consider the set of all such pairs where $q(\\{x,y\\})=1$ (this is $Q_{p}^{\\prime\\prime}$ defined in line 34) and consider the graph $G$ with this edge set, and vertex set $U\\setminus\\mathcal{R}_{p}$ , the set of points whose cluster hasn\u2019t yet been determined. If two points belong to the same connected component in this graph, then they belong to the same cluster. Thus, the analysis for this iteration boils down to showing that with high probability, the induced subgraph $G[C]$ will be connected for every C where |C| \u2208[2kn\u00b72p ,2k\u00b72n . This is accomplished by applying a basic fact from the theory of random graphs, namely Fact 2.4. ", "page_idx": 4}, {"type": "text", "text": "Analysis We restate the main theorem for this section. ", "page_idx": 4}, {"type": "text", "text": "Theorem 2.2. There is a non-adaptive algorithm for $k$ -clustering that uses $O(n\\log\\log n\\cdot k\\log k)$ subset queries and succeeds with probability at least $1-\\delta$ for any constant $\\delta>0^{1}$ . ", "page_idx": 4}, {"type": "text", "text": "The following Lemma 2.3 establishes that after the first $p$ iterations of the algorithm\u2019s query selection and reconstruction phases, all clusters of size at least $\\frac{n}{2k\\!\\cdot\\!2^{p}}$ have been learned with high probability. This is the main technical component of the proof. After stating the lemma we show it easily implies that Alg. 1 succeeds with probability at least $99/100$ by an appropriate union bound. The choice of $99/100\\$ is arbitrary, and can be made $1-\\delta$ for any constant $\\delta$ . ", "page_idx": 4}, {"type": "text", "text": "Lemma 2.3. For each $p=0,1,\\ldots,\\log n_{!}$ , let $\\mathcal{E}_{p}$ denote the event that all clusters of size at least $\\frac{n}{2k\\!\\cdot\\!2^{p}}$ have been successfully recovered immediately following iteration $p$ of Alg. 1. Then, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}[\\lnot\\mathcal{E}_{0}]\\le\\frac{1}{100k}\\;\\;a n d\\;\\;\\operatorname*{Pr}[\\lnot\\mathcal{E}_{p}\\mid\\mathcal{E}_{p-1}]\\le\\frac{1}{100k}\\;\\,f o r\\,a l l\\,p\\in\\{1,2\\ldots,\\log n\\}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Proof of Theorem 2.2: Before proving Lemma 2.3, we first observe that it immediately implies the correctness of Alg. 1 and thus proves Theorem 2.2. Let $\\begin{array}{r}{I_{0}=\\left(\\frac{n}{2k},n\\right]}\\end{array}$ and for $1\\leq p\\leq\\log n$ , let $\\begin{array}{r}{I_{p}=\\left[\\frac{n}{2k\\cdot2^{p}},\\frac{n}{2k\\cdot2^{p-1}}\\right)}\\end{array}$ . If there are no clusters $C$ for which $|C|\\in I_{p}$ , then trivially $\\operatorname*{Pr}[\\lnot\\mathcal{E}_{p}\\mid\\mathcal{E}_{p-1}]=0$ and otherwise Pr[\u00acEp | Ep\u22121] \u22641010k by the lemma. Since there are $k$ clusters, clearly there are at most $k$ values of $p$ for which there exists a cluster with size in the interval $I_{p}$ . Using this observation and a union bound, we have ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}[\\lnot\\mathcal{E}_{\\log n}]\\leq\\operatorname*{Pr}[\\lnot\\mathcal{E}_{0}]+\\sum_{p=1}^{\\log n}\\operatorname*{Pr}[\\lnot\\mathcal{E}_{p}\\mid\\mathcal{E}_{p-1}]\\leq\\frac{1}{100}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "which completes the proof of correctness since the algorithm succeeds iff $\\mathcal{E}_{\\log n}$ occurs. ", "page_idx": 4}, {"type": "text", "text": "Query complexity: During iterations $p<\\log\\log n$ the algorithm makes at most $O(n\\log\\log n\\;.$ $k\\log k)$ queries. During iterations $p>\\log\\log n$ , it makes at most $\\begin{array}{r l}{O(n k\\log n)\\sum_{p>\\log\\log n}2^{-p}=}\\end{array}$ $O(n k)$ queries since $k\\leq n$ . ", "page_idx": 4}, {"type": "text", "text": "Time complexity: We assume that obtaining a uniform random sample from a set of size $n$ can be done in $O(1)$ time. Thus, since the algorithm makes $O(n\\log\\log n\\cdot k\\log k)$ queries and each is on a set of size at most $n$ , the total runtime of the query selection phase (lines 3-15) is bounded by $O(n^{2}\\log\\log n\\cdot k\\log k)$ . We now account for the runtime in the reconstruction phase. Lines (25-28) clearly can be performed in $O(n)$ time and so the time spent in lines (24-28) is $O(|Q_{p}|\\cdot n)$ . Now, for $T\\in Q_{p}$ , checking if $|T\\setminus\\dot{\\mathcal{R}_{p}}|=2$ can clearly be done in $O(n)$ time and so lines (33-34) run in time $O(|Q_{p}|\\cdot n)$ . Line (36) amounts to finding every connected component in $G_{p}$ which can be done in time $\\dot{O}(|\\dot{Q}_{p}^{\\prime\\prime}|+n)=O(|Q_{p}|+n)$ by iteratively running a BFS (costing time linear in the number of edges plus the number of vertices). Thus, the runtime of the $p$ \u2019th iteration of the for-loop is always dominated by $O(|Q_{p}|\\cdot n)$ . Since the total number of queries is $O(n\\log\\log n\\cdot k\\log k)$ , the total runtime of the reconstruction phase is $O(n^{2}\\log\\log n\\cdot k\\log k)$ . ", "page_idx": 5}, {"type": "text", "text": "We now prove the main Lemma 2.3. ", "page_idx": 5}, {"type": "text", "text": "Proof. of Lemma 2.3. Let $\\mathcal{C}_{p}$ denote the set of clusters recovered before phase $p$ and let $\\mathcal{R}_{p}\\,=$ $\\textstyle\\bigcup_{C\\in{\\mathcal{C}}_{p}}C$ . When $p=0$ , both of these sets are empty. We will consider three cases depending on the value of $p$ . ", "page_idx": 5}, {"type": "text", "text": "Case 1: $p=0$ . Let $C$ denote some cluster of size $\\textstyle|C|\\geq{\\frac{n}{2k}}$ . Note that in this iteration the sets $T$ sampled by the algorithm in line (7) are singletons. We need to argue that one of these singletons will land in $C$ , and thus $C$ is recovered in line (28), with probability at least $1-{\\frac{1}{100k^{2}}}$ . Since there are at most clusters, applying a union bound completes the proof in this case. ", "page_idx": 5}, {"type": "text", "text": "A uniform random element lands in $C$ with probability at least $\\scriptstyle{\\frac{1}{2k}}$ and so this fails to occur for all $|Q_{0}|\\geq4k\\ln10k$ samples with probability at most $\\begin{array}{r}{(1-\\frac{1}{2k})^{4k\\ln\\bar{1}\\bar{0}k}\\le\\exp(-2\\ln10k)=\\frac{1}{100k^{2}}}\\end{array}$ 100k2 , as claimed. ", "page_idx": 5}, {"type": "text", "text": "Case 2: $1\\leq p\\leq\\log\\log n$ . Let $C$ denote some cluster with sizen $\\begin{array}{r}{\\left|C\\right|\\in\\left[\\frac{n}{2k\\cdot2^{p}},\\frac{n}{2k\\cdot2^{p-1}}\\right)}\\end{array}$ . Note that we are conditioning on the event that every cluster of ${\\mathrm{size}}\\geq{\\frac{n}{2k\\cdot2^{p-1}}}$ has already been successfully recovered after iteration $p-1$ . Thus, the number of elements belonging to unrecovered clusters is $\\begin{array}{r}{|U\\setminus\\mathcal{R}_{p}|\\le k\\cdot\\frac{n}{2k\\cdot2^{p-1}}=\\frac{n}{2^{p}}}\\end{array}$ . We need to argue that the set $Q_{p}$ will contain some $T$ sampled in line (7) such that $T\\,\\backslash\\,\\\"\\tilde{\\mathcal{R}}_{p}=\\{z\\}$ where $z\\in C$ , and thus $C$ is successfully recovered in line (28), with probability at least $1-\\frac{1}{100k^{2}}$ . Once this is established, the lemma again follows by a union bound. We have ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{\\colon,\\ |T|=2^{p}}[|T\\backslash\\mathcal{R}_{p}|=1\\ \\mathrm{and}\\ T\\backslash\\mathcal{R}_{p}\\subseteq C]=|T|\\cdot\\frac{|C|}{n}\\cdot\\left(\\frac{|\\mathcal{R}_{p}|}{n}\\right)^{|T|-1}\\geq\\frac{2^{p}}{k\\cdot2^{p+1}}\\left(1-\\frac{1}{2^{p}}\\right)^{2^{p}}\\geq\\frac{1}{2e k}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "and so the probability that this occurs for some $T\\in Q_{p}$ is at least $\\begin{array}{r}{1-\\bigl(1-\\frac{1}{2e k}\\bigr)^{4e k\\ln10k}\\geq1-\\frac{1}{100k^{2}},}\\end{array}$ as claimed. ", "page_idx": 5}, {"type": "text", "text": "Case 3: $p>\\log\\log n$ . Let $C$ denote some cluster with size $|C|\\;\\in\\;\\left[{\\frac{n}{2k\\cdot2^{p}}},\\,{\\frac{n}{2k\\cdot2^{p-1}}}\\right)$ . Note that $\\begin{array}{r}{|U\\setminus\\mathcal{R}_{p}|\\le k\\cdot\\frac{n}{2k\\cdot2^{p-1}}=\\frac{n}{2^{p}}}\\end{array}$ . Recall from lines (34-35) the definition of $Q_{p}^{\\prime\\prime}$ and recall that $G_{p}$ is the graph with vertex set $U\\setminus\\mathcal{R}_{p}$ and edge set $Q_{p}^{\\prime\\prime}$ . We need to argue that the induced subgraph $G_{p}[C]$ is connected, and thus $C$ is successfully recovered in lines (36-37), with probability at least $1-{\\frac{1}{100k^{2}}}$ . Once this is established, the lemma again follows by a union bound. We rely on the following standard fact from the theory of random graphs. For completeness, we give a proof in Appendix D.2. ", "page_idx": 5}, {"type": "text", "text": "Fact 2.4. Let $G(N,p)$ denote an Erd\u00f6s-R\u00e9nyi random graph. That is, the graph contains $N$ vertices and there is an edge between each pair of vertices with probability $p$ . If $\\bar{p}\\geq1-(\\delta/3N)^{2/N}$ , then $G(N,p)$ is connected with probability at least $1-\\delta$ . ", "page_idx": 5}, {"type": "text", "text": "Consider any $x,y\\in C$ and observe that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{T:\\ |T|=2p}[T\\ \\backslash\\ \\mathcal{R}_{p}=\\{x,y\\}]=\\binom{2p}{2}\\cdot\\frac{1}{n^{2}}\\cdot\\left(\\frac{|\\mathcal{R}_{p}|}{n}\\right)^{2^{p}-2}\\geq\\frac{2^{2p}}{3n^{2}}\\left(1-\\frac{1}{2^{p}}\\right)^{2^{p}}\\geq\\frac{2^{2p}}{10n^{2}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Algorithm 1: Non-adaptive Algorithm for Constant $k$ ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "1 Input: Subset query access to a hidden partition $C_{1}\\sqcup\\cdot\\cdot\\cdot\\sqcup C_{k}=U$ of $|U|=n$ points;   \n2 (Query Selection Phase)   \n3 for $p=0,1,\\ldots,\\log n$ do   \n4 Initialize $Q_{p}\\leftarrow\\emptyset$ ;   \n5 if $p\\leq\\log\\log n$ then   \n6 Repeat 4ek $;\\ln(10k)$ times;   \n7 \u2192Sample $T\\subseteq U$ formed by $2^{p}$ independent uniform samples from $U$ ;   \n8 \u2192Query $T$ and $T\\cup\\{x\\}$ for all $x\\in U$ ;   \n9 \u2212\u2192Add $T$ to $Q_{p}$ ;   \n10 end   \n11 if $p>\\log\\log n$ then   \n12 Repeat $\\frac{40n k\\ln(300n k^{2})}{2^{p}}$ times;   \n13 \u2192Sample $T\\subseteq U$ formed by $2^{p}$ independent uniform samples from $U$ ;   \n14 \u2192Query $T$ and add it to $Q_{p}$ ;   \n15 end   \n16 end   \n17 (Reconstruction Phase)   \n18 Initialize learned cluster set $\\mathcal{C}_{0}\\gets\\emptyset$ ;   \n19 for $p=0,1,\\ldots,\\log n$ do   \n20 Let $\\mathcal{C}_{p}$ denote the collection of clusters reconstructed before iteration $p$ ;   \n21 Let $\\begin{array}{r}{\\dot{\\mathcal{R}}_{p}=\\bigcup_{C\\in\\mathcal{C}_{p}}C}\\end{array}$ denote the points belonging to these clusters;   \n22 Initialize $\\mathcal{C}_{p+1}\\leftarrow\\mathcal{C}_{p}$ ;   \n23 if $p\\leq\\log\\log n$ then   \n24 for $T\\in Q_{p}$ do   \n25 if $|T\\setminus\\bar{\\mathcal{R}}_{p}|=1$ then   \n26 Let $z$ denote the unique point in $T\\,\\backslash\\,\\mathcal{R}_{p}$ ;   \n27 If $x\\in U\\setminus\\mathcal{R}_{p}$ , then $\\bar{q}(\\bar{T)}=q(T\\cup\\dot{\\{x\\}})$ iff $x,z$ are in the same cluster;   \n28 Thus, we add $\\{x\\in U\\setminus\\mathcal{R}_{p}\\colon q(T)=q(T\\cup\\{x\\})\\}$ to $\\mathcal{C}_{p+1}$ ;   \n29 end   \n30 end   \n31 end   \n32 if $p>\\log\\log n$ then   \n33 Let $\\bar{Q}_{p}^{\\prime}\\stackrel{-}{=}\\{T\\setminus\\mathcal{R}_{p}\\colon T\\in Q_{p}$ and $|T\\setminus\\mathcal{R}_{p}|=2\\}$ . Since each $T\\in Q_{p}$ is a uniform   \nrandom set, the elements of $Q_{p}^{\\prime}$ are uniform random pairs in $U\\setminus\\mathcal{R}_{p}$ ;   \n34 Let $Q_{p}^{\\prime\\prime}=\\{\\{x,y\\}\\in Q_{p}^{\\prime}\\colon q(\\{x,\\stackrel{\\cdot}{y}\\}=1)\\}$ denote the set of pairs in $Q_{p}^{\\prime}$ where both points   \nlie in the same cluster. This set can be computed since $q(T\\setminus\\mathcal{R}_{p})=\\dot{q}(T)-q(T\\cap\\mathcal{R}_{p})$   \nand $q(T\\cap\\mathcal{R}_{p})$ is known since at this point we have reconstructed the clustering on $\\mathcal{R}_{p}$ ;   \n35 Let $G_{p}$ denote the graph with vertex set $U\\setminus\\mathcal{R}_{p}$ and edge set $Q_{p}^{\\prime\\prime}$ ;   \n36 Let $C_{1},\\ldots,C_{\\ell}$ denote the connected components of $G_{p}$ with size at least $\\frac{n}{2k\\!\\cdot\\!2^{p}}$ ;   \n37 Add $C_{1},\\ldots,C_{\\ell}$ to $\\mathcal{C}_{p+1}$ ;   \n38 end   \n39 end ", "page_idx": 6}, {"type": "text", "text": "40 Output clustering $\\mathcal{C}_{\\log n+1}$ ", "page_idx": 6}, {"type": "text", "text": "Recall that the algorithm queries $\\begin{array}{r}{|Q_{p}|={\\frac{40\\cdot n k\\ln(300n k^{2})}{2^{p}}}}\\end{array}$ random sets $T$ of size $2^{p}$ . Thus, ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{Pr}_{Q_{p}}\\left[(x,y)\\in E(G_{p}[C])\\right]=\\operatorname*{Pr}_{Q_{p}}\\left[\\{x,y\\}\\in Q_{p}^{\\prime\\prime}\\right]=\\operatorname*{Pr}_{Q_{p}}\\left[\\exists T\\in Q_{p}\\colon T\\setminus\\mathcal{R}_{p}=\\{x,y\\}\\right]}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\ge1-\\left(1-\\frac{2^{2p}}{10n^{2}}\\right)^{40\\frac{n}{2p}\\cdot k\\ln\\left(300n k^{2}\\right)}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\ge1-\\exp\\left(-\\frac{2^{p}}{n}\\cdot4k\\ln(300n k^{2})\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "and using $\\begin{array}{r}{|C|\\geq\\frac{n}{2k\\cdot2^{p}}}\\end{array}$ and $|C|\\leq n$ , we obtain ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle{\\operatorname*{Pr}_{Q_{p}}}\\left[(x,y)\\in E(G_{p}[C])\\right]\\geq1-\\exp\\left(-\\frac{2\\ln(300n k^{2})}{|C|}\\right)}\\\\ {\\displaystyle{\\geq1-\\exp\\left(-\\frac{2\\ln(300k^{2}|C|)}{|C|}\\right)=1-\\left(\\frac{1}{300k^{2}|C|}\\right)^{\\frac{2}{|C|}}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Thus, $(x,y)$ is an edge in $G_{p}[C]$ with probability at least $\\begin{array}{r}{{1-\\left({\\frac{1}{{300k^{2}}\\left|C\\right|}}\\right)^{\\frac{2}{\\left|C\\right|}}}}\\end{array}$ and so by Fact $2.4\\:G_{p}[C]$ is connected with probability at least $1-\\frac{1}{100k^{2}}$ 1001k2 , as claimed. \u53e3 ", "page_idx": 7}, {"type": "text", "text": "Bounded Query Size We can restrict the query size to $s\\leq\\sqrt{n}$ , and still achieve a near-linear query complexity. We sketch the main ideas here for the case of $k=3$ similar to the \"warm-up\" in Section 2.1. Details are provided in Appendix A. Our Theorem 1.4 gives an $O(n\\log n\\log\\log n)$ query non-adaptive sample-based algorithm using subset queries of size at most $O({\\sqrt{n}})$ . The main idea is to employ \"Strategy $2\"$ described in the warm-up section of Section 2.1 with a slight alteration. Let $A,B$ denote the largest, and second largest clusters, respectively, where $|B|=\\delta n$ and so $|A|\\,\\geq\\,(1-2\\delta)n$ . Observe that if we take a random set $S$ of size $t\\,\\approx\\,\\sqrt{1/\\delta}$ , then the probability that two points land in $B$ and the rest land in $A$ is at least $\\binom{t}{2}\\cdot\\delta^{2}\\cdot(1-\\dot{2}\\delta)^{t-2}=\\Omega(\\delta)$ . Recalling the definition of the graph $G$ and the discussion in Section 2.1, after querying $\\Omega(n\\ln n)$ such $S$ , the induced subgraph $G[B]$ becomes connected with high probability, thus recovering the clustering. Similar ideas let us generaize to any $s$ , and achieve an optimal dependency on $s$ as stated in Corollary C.2 for constant $k$ . ", "page_idx": 7}, {"type": "text", "text": "2.2 An $O(n\\log^{2}n\\log k)$ Algorithm for General $k$ ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We now consider the situation with general $k$ , for which our algorithm and analysis follow a completely different approach by using techniques from combinatorial group testing. ", "page_idx": 7}, {"type": "text", "text": "Warm up. The main subroutine in our algorithm is a procedure for recovering the support of a Boolean vector via OR queries. Given a vector $v\\in\\{0,\\bar{1}\\}^{n}$ , an OR query on a set $S\\subseteq[n]$ returns $\\mathsf{O R}_{S}(v)=\\mathsf{V}_{i\\in S}\\,v_{i}$ , i.e. it returns 1 iff $v$ has a 1-valued coordinate in $S$ . The problem of recovering the support of $v$ , $\\mathsf{s u p p}(v)=\\{i\\colon v_{i}=1\\}$ via OR queries is a basic problem from the group testing and coin-weighing literature. The relevance of this problem for $k$ -clustering with subset queries is as follows. Consider a hidden clustering $C_{1}\\sqcup\\cdots\\sqcup C_{k}=U$ . Given $x\\in U$ , let $C(x)$ denote the cluster containing $U=\\{x_{1},\\ldots,x_{n}\\}$ (an arbitrary ordering of $U$ ), and let $v^{(x)}\\in\\{0,1\\}^{n}$ denote the Boolean vector where $v_{i}^{(x)}=\\mathbf{1}(x_{i}\\in C(x))$ . An OR query on set $S$ to $\\boldsymbol{v}^{(x)}$ can be simulated by a subset query to the clustering on sets $S$ and $S\\cup\\{x\\}$ since ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathsf{O R}_{S}(v^{(x)})=\\bigvee_{i\\in S}v_{i}^{(x)}=\\mathbf{1}(C(x)\\cap S\\neq\\emptyset)=\\mathbf{1}(q(S\\cup\\{x\\})=q(S)).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Thus, the problem or reconstructing $C(x)$ via subset queries is equivalent to the problem of recovering $\\boldsymbol{v}^{(x)}$ via OR queries, up to a factor of 2 in the query complexity. ", "page_idx": 7}, {"type": "text", "text": "Then, to learn a cluster $C$ with size $\\begin{array}{r}{\\frac{n}{2^{p}}\\,\\leq\\,|C|\\,\\leq\\,\\frac{n}{2^{p-1}}}\\end{array}$ it suffices to sample $O(2^{p})$ random $x$ (one of which lands in $C$ with high probability) and then recover $C(x)$ using $\\begin{array}{r}{O\\big(\\frac{n}{2^{p}}\\log\\frac{n}{\\delta}\\big)}\\end{array}$ OR queries. Iterating over every $p\\leq\\log n$ and boosting the number of samples to guarantee a high probability of success for all $k$ clusters yields our algorithm. ", "page_idx": 7}, {"type": "text", "text": "This algorithm can also be restricted to only make subset queries of size at most $s$ , and the query complexity scales with $\\frac{1}{s}$ . ", "page_idx": 8}, {"type": "text", "text": "Theorem 2.5. For every $s\\ \\in\\ [2,n]$ , there is a non-adaptive $k$ -clustering algorithm making $O(n\\log n\\log k\\cdot(\\textstyle{\\frac{n}{s}}+\\log s))$ subset queries of size at most s. In particular, for unbounded query size the algorithm makes $O(n\\log^{2}n\\log k)$ queries. ", "page_idx": 8}, {"type": "text", "text": "Proof of Theorem 2.5 We will use the following lemma for recovering $\\mathsf{s u p p}(v)=\\{i\\colon v_{i}=1\\}$ via OR queries. We prove and discuss this lemma in Appendix D.1 (see Lemma D.5). ", "page_idx": 8}, {"type": "text", "text": "Lemma 2.6. Let $v\\in\\{0,1\\}^{n}$ and $s,t\\geq1$ be positive integers where $\\begin{array}{r}{s\\leq\\frac{n}{t}}\\end{array}$ . There is a non-adaptive algorithm that makes $\\textstyle O\\big(\\frac{\\bar{n}}{s}\\log{\\frac{n}{\\delta}}\\big)$ OR queries on subsets of size s, and if $|\\mathsf{s u p p}(v)|\\,\\leq\\,t,$ , returns supp $(v)$ with probability $\\bar{1}-\\delta$ , and otherwise certifies that $|\\mathsf{s u p p}(v)|>t$ . The algorithm runs in time $O(n\\log{\\frac{n}{\\delta}})$ . ", "page_idx": 8}, {"type": "text", "text": "Recall that ${\\mathsf{O R}}_{S}(v^{(x)})\\,=\\,{\\bf1}(q(S\\cup\\{x\\})\\,=\\,q(S))$ , i.e. an OR query on $S$ is simulated by subset queries on sets $S$ and $S\\cup\\{x\\}$ . Thus, we immediately get the following corollary. ", "page_idx": 8}, {"type": "text", "text": "Corollary 2.7. Let $x\\in U$ and $r\\,\\geq\\,2,t\\,\\geq\\,1$ be positive integers where $r\\leq\\textstyle{\\frac{n}{t}}$ . There is a nonadaptive algorithm that makes $O(\\frac{n}{r}\\log{\\frac{n}{\\delta}})$ subset-queries on sets of size at most $r$ , and $i f|C(x)|\\leq t,$ , returns $C(x)$ with probability $1-\\delta$ , and otherwise certifies that $|C(x)|>t$ . The algorithm runs in time $O(n\\log{\\frac{n}{\\delta}})$ . ", "page_idx": 8}, {"type": "text", "text": "Algorithm The pseudocode for the algorithm is given in Alg. 2. The idea is to draw random points $x\\in U$ (line 5) and then use the procedure from Corollary 2.7 as a subroutine to try to learn $C(x)$ (line 6). By the corollary, this will succeed with high probability in recovering $C(x)$ as long as $t$ is set to something larger than $|C(x)|$ . Note that the query complexity of this subroutine depends2 on $t$ . If a cluster $C$ is small, then $\\operatorname*{Pr}[x\\in C]$ is small, but we can call the subroutine with small $t$ , while if $C(x)$ is large, then $\\operatorname*{Pr}[x\\in C]$ is reasonably large, though we will need to call the subroutine with larger $t$ Concretely, the algorithm iterates over every $p\\in\\{1,\\ldots,\\log n\\}$ (line 3), and in iteration $p$ the goal is to learn every cluster $C$ with $\\textstyle|C|\\in\\left[{\\frac{n}{2^{p}}},{\\frac{\\bar{n}}{2^{p-1}}}\\right]$ . To accomplish this, we sample $\\Theta(2^{p}\\log k)$ random points $x\\in U$ (line 4-5) and for each one, call the subroutine with $\\textstyle t={\\frac{n}{2^{p-1}}}$ (line 6), which is an upper bound on the sizes of the clusters we are trying to learn.Note that we always invoke the corollary with query size $r=\\mathrm{min}(s,2^{p-1})\\leq s$ , enforcing the query size bounded stated in Theorem 2.5. ", "page_idx": 8}, {"type": "text", "text": "Algorithm 2: Non-adaptive Algorithm for General $k$ ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "1 Input: Subset query access to a hidden partition $C_{1}\\sqcup\\cdot\\cdot\\cdot\\sqcup C_{k}=U$ of $|U|=n$ points;   \n2 Initialize hypothesis clustering $\\mathcal{C}\\gets\\emptyset$ ;   \n3 for $p=1,\\ldots,\\log n$ do   \n4 Repeat $2^{p}\\ln(200k)$ times:   \n5 \u2212\u2192Sample $x\\in U$ uniformly at random;   \n6 $\\longrightarrow\\mathrm{Run}$ the procedure from Corollary 2.7 on $x$ with $\\textstyle t={\\frac{n}{2^{p-1}}}$ , query-size $r=\\mathrm{min}(s,2^{p-1})$ ,   \nand error probability $\\begin{array}{r}{\\delta=\\frac{1}{200k}}\\end{array}$ . This outputs $C(x)$ , the cluster containing $x$ , with   \nprobability at least $1-\\delta$ if $|C(x)|\\leq t$ ;   \n7 $\\longrightarrow$ If the procedure returns a set $C$ , then set ${\\mathcal{C}}\\gets{\\mathcal{C}}\\cup\\{C\\}$ . Otherwise, continue;   \n8 end   \n9 Output the clustering $\\mathcal{C}$ . ", "page_idx": 8}, {"type": "text", "text": "Query complexity: Note that the number of queries made in line (6) during the $p$ \u2019th iteration is $O({\\frac{n}{s}}\\log{n})$ when $\\bar{2}^{p-1}\\geq s$ , and $\\textstyle O\\!\\left({\\frac{n}{2^{p}}}\\log n\\right)$ when $2^{p-1}<s$ . Therefore, the total number of queries made is at most ", "page_idx": 8}, {"type": "equation", "text": "$$\nO(\\log k)\\left(\\sum_{p\\,:\\;1\\leq2^{p-1}<s}O(2^{p}\\cdot\\frac{n}{2^{p}}\\log n)+\\sum_{p\\,:\\;s\\leq2^{p-1}\\leq n}O(2^{p}\\cdot\\frac{n}{s}\\log n)\\right).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "The first sum is bounded by $O(n\\log n\\log s)$ and the second sum is bounded by $O({\\frac{n^{2}}{s}}\\log{n})$ . The time-complexity is clearly identical by Corollary 2.7. ", "page_idx": 8}, {"type": "text", "text": "Time complexity: We assume that attaining a uniform sample from a set of size $n$ can be performed in $O(1)$ time. The procedure in line (6) has runtime at most ${\\bar{O}}(n\\log n)$ since we set $\\delta=\\Theta\\!\\left({\\frac{1}{k}}\\right)$ . Thus, the total runtime of the algorithm is $\\begin{array}{r}{O(n\\log n\\log k)\\cdot\\sum_{p\\leq\\log n}2^{p}=O(n^{2}\\log n\\log k)}\\end{array}$ . ", "page_idx": 9}, {"type": "text", "text": "Correctness: Consider any cluster $C$ and let $p\\in\\{1,\\ldots,\\log n\\}$ be such that $\\begin{array}{r}{\\frac{n}{2^{p}}\\,\\leq\\,|C|\\,\\leq\\,\\frac{n}{2^{p-1}}}\\end{array}$ . Let $\\mathcal{E}_{C}$ denote the event that some element $x\\,\\in\\,C$ is sampled in line (5) during iteration $p$ . Let $\\mathcal{R}_{C}$ denote the event that $C\\in\\mathcal{C}$ when the algorithm terminates. Observe that by Corollary 2.7, $\\begin{array}{r}{\\operatorname*{Pr}[\\mathcal{R}_{C}\\mid\\mathcal{E}_{C}]\\geq1-\\delta=1-\\frac{1}{200k}}\\end{array}$ . Moreover, using our lower bound on $C$ we have ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}[\\neg\\mathcal{E}_{C}]\\leq\\left(1-\\frac{|C|}{n}\\right)^{2^{p}\\ln200k}\\leq\\left(1-\\frac{1}{2^{p}}\\right)^{2^{p}\\ln200k}\\leq\\frac{1}{200k}.\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "Thus, $\\begin{array}{r}{\\operatorname*{Pr}[\\lnot\\mathcal{R}_{C}]\\le\\operatorname*{Pr}[\\lnot\\mathcal{E}_{C}]+\\operatorname*{Pr}[\\lnot\\mathcal{R}_{C}\\mid\\mathcal{E}_{C}]\\le\\frac{1}{100k}}\\end{array}$ and taking another union bound over all $k$ clusters completes the proof. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements. Hadley Black, Arya Mazumdar, and Barna Saha were supported by NSF TRIPODS Institute grant 2217058 (EnCORE) and NSF 2133484. Euiwoong Lee was also supported in part by NSF grant 2236669 and Google. The collaboration is the result of an EnCORE Institute Workshop. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] David Cohn, Rich Caruana, and Andrew McCallum. Semi-supervised clustering with user feedback. Constrained clustering: advances in algorithms, theory, and applications, 4(1):17\u201332, 2003. 1 [2] Eric Bair. Semi-supervised clustering methods. Wiley Interdisciplinary Reviews: Computational Statistics, 5(5):349\u2013361, 2013. 1   \n[3] Maria-Florina Balcan and Avrim Blum. Clustering with interactive feedback. In International Conference on Algorithmic Learning Theory, pages 316\u2013328. Springer, 2008. 1 [4] Arya Mazumdar and Barna Saha. A theoretical analysis of first heuristics of crowdsourced entity resolution. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 31, 2017. 2   \n[5] Arya Mazumdar and Barna Saha. Clustering with noisy queries. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 5788\u20135799, 2017. 2 [6] Arya Mazumdar and Barna Saha. Query complexity of clustering with side information. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, 2017. 2   \n[7] Michael Mitzenmacher and Charalampos E Tsourakakis. Predicting signed edges with $o(n^{1+o(1)}\\log n)$ queries. arXiv preprint arXiv:1609.00750, 2016. 2 [8] Hassan Ashtiani, Shrinu Kushagra, and Shai Ben-David. Clustering with same-cluster queries. Advances in neural information processing systems, 29, 2016. 2 [9] Arya Mazumdar and Soumyabrata Pal. Semisupervised clustering, and-queries and locally encodable source coding. Advances in Neural Information Processing Systems, 30, 2017. 2   \n[10] Barna Saha and Sanjay Subramanian. Correlation clustering with same-cluster queries bounded by optimal cost. In 27th Annual European Symposium on Algorithms (ESA 2019). SchlossDagstuhl-Leibniz Zentrum f\u00fcr Informatik, 2019. 2   \n[11] Alberto Del Pia, Mingchen Ma, and Christos Tzamos. Clustering with queries under semirandom noise. In Conference on Learning Theory, pages 5278\u20135313. PMLR, 2022. 2   \n[12] Marco Bressan, Nicol\u00f2 Cesa-Bianchi, Silvio Lattanzi, and Andrea Paudice. Exact recovery of mangled clusters with same-cluster queries. Advances in Neural Information Processing Systems, 33:9324\u20139334, 2020. 2   \n[13] Wasim Huleihel, Arya Mazumdar, Muriel M\u00e9dard, and Soumyabrata Pal. Same-cluster querying for overlapping clusters. Advances in Neural Information Processing Systems, 32, 2019. 2   \n[14] Ramya Korlakai Vinayak and Babak Hassibi. Crowdsourced clustering: Querying edges vs triangles. Advances in Neural Information Processing Systems, 29, 2016. 2   \n[15] Nikhil Bansal, Avrim Blum, and Shuchi Chawla. Correlation clustering. Machine learning, 56(1):89\u2013113, 2004. 2   \n[16] Nir Ailon, Moses Charikar, and Alantha Newman. Aggregating inconsistent information: Ranking and clustering. Journal of the ACM, 55(5):1\u201327, 2008. 2   \n[17] Shuchi Chawla, Konstantin Makarychev, Tselil Schramm, and Grigory Yaroslavtsev. Near optimal LP rounding algorithm for correlation clustering on complete and complete $k$ -partite graphs. In Proceedings of the 47th Annual ACM Symposium on Theory of Computing (STOC), pages 219\u2013228, 2015. 2 [18] Nairen Cao, Vincent Cohen-Addad, Euiwoong Lee, Shi Li, Alantha Newman, and Lukas Vogl. Understanding the cluster lp for correlation clustering. In Proceedings of the 56th Annual ACM Symposium on Theory of Computing (STOC), 2024. 2 [19] Jure Leskovec, Daniel Huttenlocher, and Jon Kleinberg. Predicting positive and negative links in online social networks. In Proceedings of the 19th international conference on World wide web, pages 641\u2013650, 2010. 2 [20] Emmanuel Abbe. Community detection and stochastic block models: recent developments. Journal of Machine Learning Research, 18(177):1\u201386, 2018. 2 [21] Chandra Sekhar Mukherjee, Pan Peng, and Jiapeng Zhang. Recovering unbalanced communities in the stochastic block model with application to clustering with a faulty oracle. Advances in Neural Information Processing Systems, 36, 2024. 2 [22] Quanquan Gu and Jiawei Han. Towards active learning on graphs: An error bound minimization approach. In 2012 IEEE 12th International Conference on Data Mining, pages 882\u2013887. IEEE,   \n2012. 2 [23] Martin Aigner. Combinatorial search. John Wiley & Sons, Inc., 1988. 2 [24] Dingzhu Du and Frank K Hwang. Combinatorial group testing and its applications. World Scientific, 12, 2000. 2 [25] Dingzhu Du, Frank K Hwang, and Frank Hwang. Combinatorial group testing and its applications, volume 12. World Scientific, 2000. 2 [26] F. Hwang and V. S\u00f3s. Non-adaptive hypergeometric group testing. Studia Sci. Math. Hungar,   \n1987. 2, 19 [27] Arya Mazumdar. Nonadaptive group testing with random set of defectives. IEEE Transactions on Information Theory, 62(12):7522\u20137531, 2016. 2 [28] Ely Porat and Amir Rothschild. Explicit non-adaptive combinatorial group testing schemes. In Automata, Languages and Programming, 35th International Colloquium, ICALP 2008, Lecture Notes in Computer Science, 2008. 2, 19 [29] Bernt Lindstr\u00f6m. On a combinatory detection problem i. A Magyar Tudom\u00e1nyos Akad\u00e9mia Matematikai Kutat\u00f3 Int\u00e9zet\u00e9nek K\u00f6zlem\u00e9nyei, 9(1-2):195\u2013207, 1964. 2 [30] Bernt Lindstr\u00f6m. On a combinatory detection problem. ii. Studia Sci. Math. Hungar, 1:353\u2013361,   \n1966. 2 [31] David G Cantor and WH Mills. Determination of a subset from certain combinatorial properties. Canadian Journal of Mathematics, 18:42\u201348, 1966. 2 [32] Nader H Bshouty. Optimal algorithms for the coin weighing problem with a spring scale. In COLT, volume 2009, page 82, 2009. 2 [33] Nader H. Bshouty and Hanna Mazzawi. On parity check $(0,1)$ -matrix over $_{z^{\\mathrm{p}}}$ . In Proceedings, ACM-SIAM Symposium on Discrete Algorithms (SODA), 2011. 2 [34] Nader H. Bshouty and Hanna Mazzawi. Algorithms for the coin weighing problems with the presence of noise. Electron. Colloquium Comput. Complex., TR11-124, 2011. 2 [35] Dana Angluin and Jiang Chen. Learning a hidden graph using o(log n) queries per edge. 2004.   \n2 [36] Sung-Soon Choi and Jeong Han Kim. Optimal query complexity bounds for finding graphs. In Proceedings of the fortieth annual ACM symposium on Theory of computing, pages 749\u2013758,   \n2008. 2 [37] Hanna Mazzawi. Optimally reconstructing weighted graphs using queries. In Proceedings of the Twenty-First Annual ACM-SIAM Symposium on Discrete Algorithms, pages 608\u2013615. SIAM, 2010. 2   \n[38] Sung-Soon Choi. Polynomial time optimal query algorithms for finding graphs with arbitrary real weights. In Conference on Learning Theory, pages 797\u2013818. PMLR, 2013. 2   \n[39] Aviad Rubinstein, Tselil Schramm, and S Matthew Weinberg. Computing exact minimum cuts without knowing the graph. arXiv preprint arXiv:1711.03165, 2017. 2   \n[40] Sagnik Mukhopadhyay and Danupon Nanongkai. Weighted min-cut: sequential, cut-query, and streaming algorithms. In Proceedings, ACM Symposium on Theory of Computing (STOC), 2020. 2   \n[41] Sepehr Assadi, Deeparnab Chakrabarty, and Sanjeev Khanna. Graph connectivity and single element recovery via linear and or queries. In 29th Annual European Symposium on Algorithms (ESA 2021). Schloss-Dagstuhl-Leibniz Zentrum f\u00fcr Informatik, 2021. 2   \n[42] Simon Apers, Yuval Efron, Pawel Gawrychowski, Troy Lee, Sagnik Mukhopadhyay, and Danupon Nanongkai. Cut query algorithms with star contraction. In Proceedings, IEEE Symposium on Foundations of Computer Science (FOCS), 2022. 2   \n[43] Hang Liao and Deeparnab Chakrabarty. Learning spanning forests optimally in weighted undirected graphs with cut queries. In International Conference on Algorithmic Learning Theory, pages 785\u2013807. PMLR, 2024. 2   \n[44] Paul Beame, Sariel Har-Peled, Sivaramakrishnan Natarajan Ramamoorthy, Cyrus Rashtchian, and Makrand Sinha. Edge estimation with independent set oracles. ACM Trans. Algorithms, 16(4):52:1\u201352:27, 2020. 2   \n[45] Xi Chen, Amit Levi, and Erik Waingarten. Nearly optimal edge estimation with independent set queries. In Shuchi Chawla, editor, Proceedings, ACM-SIAM Symposium on Discrete Algorithms (SODA), 2020. 2   \n[46] Raghavendra Addanki, Andrew McGregor, and Cameron Musco. Non-adaptive edge counting and sampling via bipartite independent set queries. In Shiri Chechik, Gonzalo Navarro, Eva Rotenberg, and Grzegorz Herman, editors, 30th Annual European Symposium on Algorithms, ESA 2022, September 5-9, 2022, Berlin/Potsdam, Germany, volume 244 of LIPIcs, pages 2:1\u20132:16. Schloss Dagstuhl - Leibniz-Zentrum f\u00fcr Informatik, 2022. 2   \n[47] Troy Lee, Miklos Santha, and Shengyu Zhang. Quantum algorithms for graph problems with cut queries. In Proceedings of the 2021 ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 939\u2013958. SIAM, 2021. 2   \n[48] Sepehr Assadi, Deeparnab Chakrabarty, and Sanjeev Khanna. Graph connectivity and single element recovery via linear and OR queries. In Petra Mutzel, Rasmus Pagh, and Grzegorz Herman, editors, 29th Annual European Symposium on Algorithms, ESA 2021, September 6-8, 2021, Lisbon, Portugal (Virtual Conference), volume 204 of LIPIcs, pages 7:1\u20137:19. Schloss Dagstuhl - Leibniz-Zentrum f\u00fcr Informatik, 2021. 19   \n[49] Vladimir Grebinski and Gregory Kucherov. Optimal reconstruction of graphs under the additive model. Algorithmica, 28(1):104\u2013124, 2000. 24 ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Bounded Query Size and Sample-Based Algorithms ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section we present an algorithm using subset queries with size bounded by $s$ , which matches the lower bound of Theorem C.1, up to a $\\log n$ -factor. Our algorithm has the additional desirable property of being sample-based, meaning that the subsets it queries are formed by taking uniform independent samples. In addition to Theorem A.1, we also obtain a non-adaptive sample-based algorithm using $O(n k\\log n)$ unbounded queries in Theorem G.1, using a similar approach. We also show a lower bound of $\\Omega(n\\log n)$ for any $k\\geq2$ in Appendix C.2 for sample-based algorithms, showing that the dependence on $n$ is optimal for this special class of algorithms. ", "page_idx": 13}, {"type": "text", "text": "Theorem A.1. There are non-adaptive, sample-ba\u221ased $k$ -clustering algorithms making (a) $O(n k\\log n\\log\\log n)$ subset queries of size at most $O({\\sqrt{n}})$ , and (b) $O({\\frac{\\breve{n^{2}}}{s^{2}}}k\\log n)$ subset queries of size at most $s=n^{1/2-\\delta}$ for any constant $\\delta\\in(0,1/2)$ . Each algorithm is correct with probability at least $99/100$ . ", "page_idx": 13}, {"type": "text", "text": "For convenience, we will parameterize the query-size bound by $s=n^{1/r}$ where $r$ is any positive real number in the range $2\\leq r\\leq\\log n$ . Before proving the theorem formally, we informally describe the algorithm and its analysis. A full description of the algorithm is given in pseudocode in Alg. 3, which is split into two phases: a \"query selection phase\", describing how queries are chosen by the algorithm, and a \"reconstruction phase\", describing how the algorithm uses the query responses to determine the clustering. Both phases contain a for-loop iterating over all $p\\,\\in\\,\\{0,1,\\dots,\\log_{r}\\log n\\,-\\,1\\}$ where the goal of the algorithm during the $p$ \u2019th iteration is to learn all remaining clusters of size at least $\\frac{n}{k}\\cdot2^{\\frac{\\omega}{-r^{p+1}}}$ . We prove that this occurs with high probability in Lemma 2.3, which gives the main analysis. If each iteration is successful in doing so than the entire clustering has been learned successfully after iteration p = logr log n \u22121 (since 2\u2212rlogr log n $\\begin{array}{r}{\\frac{\\smile}{2^{-r^{\\log_{r}\\log_{n}}}}=2^{-\\log n}=\\frac{1}{n})}\\end{array}$ , and we justify this formally just after the statement of Lemma A.2. ", "page_idx": 13}, {"type": "text", "text": "We describe the algorit\u221ahm and it\u2019s analysis informally for the case of $r=2$ , i.e. when the query sizes are bounded by $s={\\sqrt{n}}$ . We also refer the reader to Section 2 for discussion of the ideas for the simple case of $k=3$ . Consider some iteration $p\\in\\{0,1,\\ldots,\\log\\log n-1\\}$ and suppose that prior to this iteration, all clusters of size at least ${\\frac{n}{k}}\\cdot2^{-2^{p}}$ have been successfully recovered. Let $\\mathcal{C}_{p}$ denote the collection of all such clusters and let $\\begin{array}{r}{\\mathcal{R}_{p}^{'}=\\bigcup_{C\\in\\mathcal{C}_{v}}C}\\end{array}$ be the set of points they contain. The goal in iteration $p$ is to learn every cluster $C$ w ith $|\\overline{{C}}|\\in[\\frac{n}{k}\\cdot2^{-2^{p+1}},\\frac{n}{k}\\cdot2^{-2^{p}})$ . The algorithm queries $O(n k\\log n)$ random sets $T$ formed by $2^{2^{p}}$ samples3 from $U$ (see lines 5-7 of Alg. 3). Similar to the proof of Theorem 2.2, if $T$ contains exactly two points $x,y\\in T\\setminus\\mathcal{R}_{p}$ belonging to unrecovered clusters, then we can use the fact that we already know the clustering on $\\mathcal{R}_{p}$ to tell whether or not $x,y$ belong to the same cluster or not, i.e. we can compute $q(\\{x,y\\})^{-}\\in\\{1,\\dot{2}\\}$ from $q(T)$ . We then consider the set of all such pairs where $q(\\{x,y\\})=1$ (this is $Q_{p}^{\\prime\\prime}$ defined in line 16) and consider the graph $G$ with this edge set, and vertex set $U\\setminus\\mathcal{R}_{p}$ , the set of points whose cluster hasn\u2019t yet been determined. If two points belong to the same connected component in this graph, then they belong to the same cluster. Thus, the analysis boils down to showing that with high probability, the induced subgraph $G[C]$ will be connected for every $C$ where $|C|\\in[\\frac{n}{k}\\cdot2^{-2^{p+1}},\\frac{n}{k}\\cdot2^{p})$ . This is accomplished by applying a basic fact from the theory of random graphs, namely Fact 2.4. ", "page_idx": 13}, {"type": "text", "text": "Proof of Theorem A.1: The following Lemma A.2 establishes that after the first $p$ iterations of the algorithm\u2019s query selection and reconstruction phases, all clusters of size at least ${\\frac{n}{k}}\\cdot2^{-r^{p+1}}$ have been learned with high probability. This is the main effort of the proof. After stating the lemma we show it easily implies that Alg. 3 succeeds with probability at least $99/100$ by an appropriate union bound. ", "page_idx": 13}, {"type": "text", "text": "Lemma A.2. For each $p=0,1,\\ldots,\\log_{r}\\log n-1$ , let $\\mathcal{E}_{p}$ denote the event that all clusters of size at least kn \u00b7 2\u2212rp+1 have been successfully recovered immediately following iteration p of Alg. 3. Then, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}[\\neg\\mathcal{E}_{0}]\\le\\frac{1}{100k}\\;\\;a n d\\;\\;\\operatorname*{Pr}[\\neg\\mathcal{E}_{p}\\;|\\;\\mathcal{E}_{p-1}]\\le\\frac{1}{100k}\\;f o r\\;a l l\\;p\\in\\{1,2,\\ldots,\\log_{r}\\log n-1\\}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Before proving Lemma A.2, we observe that it immediately implies Theorem A.1 as follows. Let I0 = [ kn \u00b7 2\u2212r, n] and for 1 \u2264p < logr log n, let Ip = [ kn \u00b7 2\u2212rp+1, . If there are no clusters 3Note that $p\\leq\\log\\log n-1$ and so $2^{2^{p}}\\leq2^{\\frac{1}{2}\\log n}={\\sqrt{n}}$ . ", "page_idx": 13}, {"type": "text", "text": "1 Input: Subset query access to a hidden partition $C_{1}\\sqcup\\cdot\\cdot\\cdot\\sqcup C_{k}=U$ of $|U|=n$ points;   \n2 (Query Selection Phase)   \n3 for $p=0,1,\\ldots,\\log_{r}\\log n-1$ do   \n4 Initialize query set $Q_{p}\\leftarrow\\emptyset$ ;   \n5 Repeat $20\\cdot n k\\ln(300n k^{2})\\cdot2^{r^{p+1}(1-{\\frac{2}{r}})}$ times;   \n6 \u2212 $\\rightarrow$ Sample $T\\subseteq U$ formed by $2^{r^{p}}$ independent uniform samples from $U$ ;   \n7 $\\longrightarrow$ Query $T$ and add it to $Q_{p}$ ;   \n8 end   \n9 (Reconstruction Phase)   \n10 Initialize learned cluster set $\\mathcal{C}_{0}\\gets\\emptyset$ ;   \n11 for $p=0,1,\\ldots,\\log_{r}\\log n-1$ do   \n12 Let $\\mathcal{C}_{p}$ denote the collection of clusters reconstructed before iteration $p$ ;   \n13 Let $\\begin{array}{r}{\\dot{\\mathcal{R}}_{p}=\\bigcup_{C\\in\\mathcal{C}_{p}}C}\\end{array}$ denote the points belonging to these clusters;   \n14 Initialize $\\mathcal{C}_{p+1}\\leftarrow\\mathcal{C}_{p}$ ;   \n15 Let $Q_{p}^{\\prime}=\\bar{\\{T\\,}}\\backslash\\mathcal{R}_{p}\\colon T\\in Q_{p}$ and $|T\\setminus\\mathcal{R}_{p}|=2\\}$ . Since each $T\\in Q_{p}$ is a uniform random   \nset, the elements of $Q_{p}^{\\prime}$ are uniform random pairs in $U\\setminus\\mathcal{R}_{p}$ ;   \n16 Let $Q_{p}^{\\prime\\prime}=\\{\\{x,y\\}\\in Q_{p}^{\\dot{\\prime}}\\colon q(\\{x,y\\}=1)\\}$ denote the set of pairs in $Q_{p}^{\\prime}$ where both points lie   \nin the same cluster. This set can be computed since $q(T\\setminus\\mathcal{R}_{p})=q(\\dot{T})-q(T\\cap\\mathcal{R}_{p})$ and   \n$q(T\\cap\\mathcal{R}_{p})$ is known since at this point we have reconstructed the clustering on $\\mathcal{R}_{p}$ ;   \n17 Let $G_{p}$ denote the graph with vertex set $U\\setminus\\mathcal{R}_{p}$ and edge set $Q_{p}^{\\prime\\prime}$ ;   \n18 Let $C_{1},\\ldots,C_{\\ell}$ denote the connected components of $G_{p}$ with size at least $\\scriptstyle{\\frac{n}{k}}\\cdot2^{-r^{p+1}}$ ;   \n19 Add $C_{1},\\ldots,C_{\\ell}$ to $\\mathcal{C}_{p+1}$ ;   \n20 end   \n21 Output clustering $\\mathcal{C}_{\\log_{r}\\log{n}}$ ", "page_idx": 14}, {"type": "text", "text": "$C$ for which $|C|\\in I_{p}$ , then trivially $\\operatorname*{Pr}[\\lnot\\mathcal{E}_{p}\\mid\\mathcal{E}_{p-1}]=0$ , and otherwise $\\begin{array}{r}{\\operatorname*{Pr}[\\neg\\mathcal{E}_{p}\\mid\\mathcal{E}_{p-1}]\\le\\frac{1}{100k}}\\end{array}$ by the lemma. Since there are $k$ clusters, clearly there are at most $k$ values of $p$ for which there exists a cluster with size in the interval $I_{p}$ . Using this observation and a union bound, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}[\\neg\\mathcal{E}_{\\log_{r}\\log{n}-1}]\\leq\\operatorname*{Pr}[\\neg\\mathcal{E}_{0}]+\\sum_{p=1}^{\\log_{r}\\log{n}}\\operatorname*{Pr}[\\neg\\mathcal{E}_{p}\\mid\\mathcal{E}_{p-1}]\\leq{\\frac{1}{100}}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which completes the proof of correctness since the algorithm succeeds iff $\\mathcal{E}_{\\log_{r}\\log n-1}$ occurs. ", "page_idx": 14}, {"type": "text", "text": "Query complexity: Note that the total number of queries made is $O(n k\\log n)\\!\\cdot\\!\\sum_{p=1}^{\\log_{r}\\log n}2^{r^{p}(1-\\frac{2}{r})}$ When , the summation evaluates to $\\log\\log n$ which establishes the query complexity in item (a) of Theorem A.1. ", "page_idx": 14}, {"type": "text", "text": "Otherwise, let $r=2+C$ for some constant $C>0$ . We will argue that $\\begin{array}{r}{2^{r^{p}(1-\\frac{2}{r})}\\leq\\frac{1}{2}2^{r^{p+1}(1-\\frac{2}{r})}}\\end{array}$ for any $p\\leq\\log_{r}\\log n-1$ greater than some constant and thus the summation is bounded as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\sum_{p=1}^{\\log_{r}\\log n}2^{r^{p}(1-\\frac{2}{r})}=O(2^{r^{\\log_{r}\\log n}(1-\\frac{2}{r})})=O(n^{1-\\frac{2}{r}})=O(n/s^{2})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "establishing the query complexity in item (b) of Theorem A.1. Observe that $\\begin{array}{r}{2^{r^{p}(1-\\frac{2}{r})}\\leq\\frac{1}{2}2^{r^{p+1}(1-\\frac{2}{r})}}\\end{array}$ is equivalent to $\\begin{array}{r}{r^{p}(1-\\frac{2}{r})\\leq r^{p+1}(1-\\frac{2}{r})-1}\\end{array}$ , or equivalently ", "page_idx": 14}, {"type": "equation", "text": "$$\nr^{p-1}\\geq\\frac{1}{(r-1)(r-2)}=\\frac{1}{C(1+C)}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which clearly holds as long as $p-1>\\log{\\frac{1}{C}}$ since $r>2$ . ", "page_idx": 14}, {"type": "text", "text": "Time complexity: We assume that sampling a uniform random element from a set of size $n$ can be done in $O(1)$ time. Thus any set that is sampled during the course of the algorithm can be constructed in $O(s)$ time. No matter the value of $s$ , the number of queries made by the algorithm is dominated by $O_{\\circ}({\\frac{{\\dot{n}}^{2}}{\\mathrm{e}^{2}}}k\\log n\\log\\log n)$ . Thus, the runtime of the query selection phase (lines 3-7) is bounded by $O(\\textstyle{\\frac{n^{\\ast}}{s}}k\\log n\\log\\log n)$ . Now for the reconstruction phase. In line (15), $|T\\backslash\\mathcal{R}_{p}|$ can be computed in $O(\\stackrel{\\cdot}{n})$ time and so lines (15-16) take time $O(|Q_{p}|\\cdot n)$ . Line (18) amounts to finding every connected component in $G_{p}$ which can be done in time $\\dot{O}(|Q_{p}^{\\prime\\prime}|+n)=O(|Q_{p}|+n)$ by iteratively running a BFS (costing time linear in the number of edges plus the number of vertices). Thus, the runtime of the $p^{i}$ \u2019th iteration of the for-loop is always dominated by $O(|Q_{p}|\\cdot n)$ . Since the total number of queries is dominated by $\\begin{array}{r}{O(\\frac{n^{2}}{s^{2}}k\\log n\\log\\log n)}\\end{array}$ , the total runtime of the reconstruction phase (lines 11-19) is $O({\\frac{n^{s}}{s^{2}}}k\\log n\\,\\dot{\\log\\log n})$ , which dominates the runtime of the query selection phase. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "We now prove the main Lemma A.2. ", "page_idx": 15}, {"type": "text", "text": "Proof. of Lemma A.2. Let $\\mathcal{R}_{p}$ denote the set of points belonging to a cluster which has been recovered before iteration $p$ . ", "page_idx": 15}, {"type": "text", "text": "Case 1: $p=0$ . In this iteration, the algorithm queries $|Q_{0}|\\geq8\\cdot n k\\ln(300n k^{2})\\cdot2^{r-2}$ random pairs and we need to show that it successfully recovers all clusters with size at least $\\frac{n}{k\\!\\cdot\\!2^{r}}$ with probability at least 1 \u22121010k . Let $C$ denote any such cluster and recall from lines (16-17) the definition of the graph $G_{0}$ with vertex set $U$ and edge set $Q_{0}^{\\prime\\prime}$ . We will show that the induced subgraph $G_{0}[C]$ is connected, and thus $C$ is correctly recovered in lines (18-19), with probability at least $\\begin{array}{r}{1-\\frac{1}{100k^{2}}}\\end{array}$ 1001k2 . Since there are at most $k$ clusters, the lemma holds by a union bound. ", "page_idx": 15}, {"type": "text", "text": "Consider any two vertices x, y \u2208C and note that |Q0| \u22652n2 ln|(C30|0nk2)s ince $\\textstyle|C|\\geq{\\frac{n}{k\\cdot2^{r}}}$ . We lower bound the probability that $(x,y)$ is an edge in $G_{0}[C]$ as follows. Note that this occurs iff $\\{x,y\\}\\in Q_{0}$ . Thus, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\operatorname*{Pr}[(\\boldsymbol{x},\\boldsymbol{y})\\in E(G_{0}[C])]=\\operatorname*{Pr}_{Q_{0}}[\\{\\boldsymbol{x},\\boldsymbol{y}\\}\\in Q_{0}]=1-\\left(1-\\frac{1}{n^{2}}\\right)^{|Q_{0}|}}\\\\ {\\displaystyle\\geq1-\\exp\\left(-\\frac{2\\ln(300n k^{2})}{|C|}\\right)}\\\\ {\\displaystyle\\geq1-\\exp\\left(-\\frac{2\\ln(300k^{2}|C|)}{|C|}\\right)=1-\\left(\\frac{1}{300k^{2}|C|}\\right)^{\\frac{2}{|C|}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and so by Fact 2.4, $G_{0}[C]$ is connected with probability at least $1-\\frac{1}{100k^{2}}$ as claimed. ", "page_idx": 15}, {"type": "text", "text": "Case 2: $1\\leq p<\\log_{r}\\log n$ . Recall from lines (12-13) that $\\mathcal{C}_{p}$ denotes the set of clusters recovered prior to iteration $p$ and $\\textstyle{\\bar{\\mathcal{R}_{p}}}=\\bigcup_{C\\in\\mathcal{C}_{p}}C$ is the set of points belonging to these clusters. Note that we are conditioning on the event that every cluster of size at least ${\\frac{n}{k}}\\cdot2^{-r^{p}}$ has been recovered prior to iteration $p$ . Let $C$ denote some cluster with size ", "page_idx": 15}, {"type": "equation", "text": "$$\n|C|\\in\\left[{\\frac{n}{k}}\\cdot2^{-r^{p+1}},{\\frac{n}{k}}\\cdot2^{-r^{p}}\\right){\\mathrm{~and~note~that~}}|U\\setminus\\mathcal{R}_{p}|\\leq k\\cdot{\\frac{n}{k}}\\cdot2^{-r^{p}}=n\\cdot2^{-r^{p}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Recall from lines (16-17) the definition of $Q_{p}^{\\prime\\prime}$ and that $G_{p}$ is the graph with vertex set $U\\setminus\\mathcal{R}_{p}$ and edge set $Q_{p}^{\\prime\\prime}$ . We need to argue that the induced subgraph $G_{p}[C]$ is connected, and thus $C$ is correctly recovered in lines (18-19), with probability at least $1-{\\frac{1}{100k^{2}}}$ 1001k2 . Since there are at most k clusters, a union bound completes the proof of the lemma. ", "page_idx": 15}, {"type": "text", "text": "Consider any two vertices $x,y\\in C$ . We lower bound the probability that $(x,y)$ is an edge in $G_{p}[C]$ , which occurs iff there is some $T\\in Q_{p}$ where $T\\setminus\\mathcal{R}_{p}=\\{x,y\\}$ . We have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{T:\\ |T|=2^{r^{p}}}[T\\setminus\\mathcal{R}_{p}=\\{x,y\\}]=\\binom{2^{r^{p}}}{2}\\cdot\\frac{1}{n^{2}}\\cdot\\left(\\frac{|\\mathcal{R}_{p}|}{n}\\right)^{t-2}\\geq\\frac{2^{2r^{p}}}{3n^{2}}\\left(1-2^{-r^{p}}\\right)^{t}\\geq\\frac{2^{2r^{p}}}{10n^{2}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and since $|Q_{p}|=20n k\\ln(300n k^{2})\\cdot2^{r^{p+1}(1-\\frac{2}{r})}$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{Pr}_{Q_{p}}\\left[(x,y)\\in E(G_{p}[C])\\right]=\\operatorname*{Pr}_{Q_{p}}\\left[\\{x,y\\}\\in Q_{p}^{\\prime\\prime}\\right]=\\operatorname*{Pr}_{Q_{p}}\\left[\\exists T\\in Q_{p}\\colon T\\setminus\\mathcal{R}_{p}=\\{x,y\\}\\right]}\\\\ &{\\hphantom{\\sum_{Q_{p}}\\sum}\\ge1-\\left(1-\\frac{2^{2r^{p}}}{10n^{2}}\\right)^{20\\cdot n k\\cdot2^{r^{p+1}-2r^{p}}\\ln(300n k^{2})}}\\\\ &{\\hphantom{\\sum_{Q_{p}}\\sum}\\ge1-\\exp\\left(-\\frac{2\\cdot2^{r^{p+1}}k\\ln(300n k^{2})}{n}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and plugging in $\\begin{array}{r}{|C|\\geq\\frac{n}{k}\\cdot2^{-r^{p+1}}}\\end{array}$ and $|C|\\leq n$ into the RHS yields ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle{\\operatorname*{Pr}}\\left[(x,y)\\in E(G_{p}[C])\\right]\\geq1-\\exp\\left(-\\frac{2\\ln(300n k^{2})}{|C|}\\right)}\\\\ {\\displaystyle{\\geq1-\\exp\\left(-\\frac{2\\ln(300k^{2}|C|)}{|C|}\\right)=1-\\left(\\frac{1}{300k^{2}|C|}\\right)^{\\frac{2}{|C|}}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, $(x,y)$ is an edge in $G_{p}[C]$ with probability at least $\\begin{array}{r}{{1-\\left({\\frac{1}{{300k^{2}}\\left|C\\right|}}\\right)^{\\frac{2}{\\left|C\\right|}}}}\\end{array}$ , which by Fact 2.4 implies that $G_{p}[C]$ is connected with probability at least $1-{\\frac{1}{100k^{2}}}$ 1001k2 as claimed. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "B The Special Case of Balanced Clusters ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Given $B\\geq1$ , we say that a $k$ -partition $C_{1},\\ldots,C_{k}$ is $B$ -balanced if $\\begin{array}{r}{\\frac{n}{B k}\\leq|C_{i}|\\leq\\frac{B n}{k}}\\end{array}$ for all $i\\in[k]$ In this section we prove the following theorem, which gives a non-adaptive algorithm for recovering a roughly balanced $k$ -clustering making $O(n\\log k)$ subset queries when $\\textstyle k=O({\\frac{n}{\\log^{3}n}})$ . We give an alternative algorithm making $O(n\\log^{2}k)$ queries for arbitrary $k$ in Appendix E. We also described a two-round algorithm for this setting making $O(n\\log\\log k)$ queries in Appendix F.2. ", "page_idx": 16}, {"type": "text", "text": "Theorem B.1. There is a non-adaptive algorithm that recovers a $B$ -balanced $k$ -clustering using $O(B^{2}n\\log k)\\!+\\!O(B k\\log^{4}k)$ subset queries of size $O(k\\log k)$ and succeeds with probability $49/50$ . ", "page_idx": 16}, {"type": "text", "text": "Pseudocode for the algorithm is given in Alg. 4. In line (3) we draw $s=\\Theta(B^{2}\\log k)$ sets $T_{1},\\mathbf{\\Pi}\\cdot\\mathbf{\\Pi}\\cdot,T_{s}$ each formed by $k/B$ samples from $U$ and in line (5) learn the clustering over their union using Theorem 2.5. I.e., for $T=T_{1}\\cup\\cdot\\cdot\\cdot\\cup T_{s}$ , we find $R_{j}=T\\cap C_{j}$ . Then, we query $T_{i}$ and $T_{i}\\cup\\{x\\}$ for every $x\\in U$ and every $i\\in[s]$ in line (5). Now, consider some point $x\\in U$ and let $j^{*}$ be it\u2019s cluster\u2019s index. Note that $q(T_{i}\\cup\\{x\\})=q(T_{i})$ iff $T_{i}$ intersects $C_{j^{*}}$ . Thus, if $T_{i}$ does not intersect $C_{j^{*}}$ , then every cluster $j$ that $T_{i}$ intersects can be ruled out as a candidate for being the cluster containing $x$ . The set $J_{x}$ computed in line (8) is the set of all $j$ which can be ruled out in this way. If for every $j\\neq j^{*}$ , there is some $T_{i}$ containing $j$ , but not $j^{*}$ , then $J_{x}=\\{j^{*}\\}$ and we determine $j^{*}$ in line (9). This occurs for every $x\\in U$ if the following holds: for every pair $(j,j^{\\prime})\\,\\in\\,\\binom{U}{2}$ , there exists $T_{i}$ intersecting $C_{j}$ , but not $C_{j^{\\prime}}$ . We show in Claim B.2 that this happens with high probability. ", "page_idx": 16}, {"type": "text", "text": "Proof of Theorem B.1 There are $O(B^{2}n\\log k)$ queries made in line (5) and $O(B\\!\\cdot\\!k\\log^{4}k)$ queries in line (4), since $|\\cup_{i\\in[s]}T_{i}|=O(B k\\log k)$ . ", "page_idx": 16}, {"type": "text", "text": "Time complexity: We assume the attaining a uniform sample from any set can be done $O(1)$ time. Thus, constructing sets $T_{1},...,T_{s}$ in line (3) costs $O(B k\\ln k)$ time and by Theorem 2.5 line (4) costs $O(k^{2}B^{2}\\ln^{4}k)$ . Line (5) costs $O(n\\cdot s)=O(B^{2}n\\ln k)$ time. Constructing $J_{x}$ in line (8) amounts to checking if $q(T_{i}\\cup\\{x\\})\\neq q(T_{i})$ and if $T_{i}\\cap R_{j}\\neq\\emptyset$ for each $i\\in[s]$ and $j\\in[k]$ . This can be done in time $O(|T_{i}|\\cdot|R_{j}|)=O(k^{2}\\ln k)$ simply using $|R_{j}|\\leq|R|=O(B k\\ln k)$ and $\\left|T_{i}\\right|=k/B$ . Thus, the total runtime of lines (7-14) is dominated by $O(n k^{2}\\ln k)$ . ", "page_idx": 16}, {"type": "text", "text": "Correctness: We now prove correctness, which is due to the following claim. ", "page_idx": 16}, {"type": "text", "text": "Claim B.2. For $i\\in[s],j\\in[k]$ , let $\\mathcal{E}_{i,j}$ denote the event that $T_{i}\\cap C_{j}\\neq\\emptyset$ . Then, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{T_{1},\\ldots,T_{s}}\\left[\\forall(j,j^{\\prime})\\in\\binom{[k]}{2},\\exists i\\in[s]\\colon\\mathcal{E}_{i,j}\\land\\lnot\\mathcal{E}_{i,j^{\\prime}}\\right]\\geq\\frac{99}{100}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "1 Input: Subset query access to a $B$ -balanced partition $C_{1}\\sqcup\\cdots\\sqcup C_{k}=U$ of $|U|=n$ points;   \n2 (Query Selection Phase)   \n3 Choose $s=2e B^{2}\\ln(100k^{2})$ sets $T_{1},...,T_{s}$ each formed by $\\frac{k}{B}$ uniform samples from $U$ ;   \n4 Run the algorithm from Theorem 2.5 to learn the clustering restricted on $\\textstyle R=\\bigcup_{i=1}^{s}T_{i}$ . Let $R_{1},\\ldots,R_{k}$ be the output of the algorithm. I.e., if the algorithm is successful , then $R_{j}=R\\cap C_{j}$ for all $\\bar{j}\\in[k]$ ;   \n5 Query $T_{i}$ and $T_{i}\\cup\\{x\\}$ for all $i\\in[s]$ and all $x\\in U$ ;   \n6 (Reconstruction Phase)   \n7 for $x\\in U$ do   \n8 Let $\\begin{array}{r}{J_{x}=\\bigcup_{i\\in[s]:~q(T_{i}\\cup\\{x\\})\\neq q(T_{i})}\\{j\\in[k]\\colon T_{i}\\cap R_{j}\\neq\\emptyset\\}}\\end{array}$ . Note that $T_{i}\\cap R_{j}\\neq\\emptyset$ iff $T_{i}\\cap C_{j}\\neq\\emptyset$ . Note that $q(T_{i}\\cup\\{x\\})\\neq q(T_{i})$ iff $x$ does not belong to any cluster that is hit by $T_{i}$ . Thus, $J_{x}$ is the collection of all $j$ such that some set $T_{i}$ has revealed that $x\\notin C_{j}$ ;   \n9 if $|J_{x}|=k-1$ then   \n10 Add $x$ to $R_{j^{*}}$ where $j^{*}$ is the unique element of $[k]\\setminus J_{x}$ ;   \n11 else   \n12 Output fail;   \n13 end   \n14 end ", "page_idx": 17}, {"type": "text", "text": "15 Output clustering $(R_{1},\\ldots,R_{k})$ ; ", "page_idx": 17}, {"type": "text", "text": "Proof. Firstly, for fixed $i\\;\\in\\;[s]$ and $j\\neq j^{\\prime}$ , since each cluster\u2019s size is bounded in the interval $\\left[{\\frac{n}{B k}},\\,{\\frac{B n}{k}}\\right]$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{Pr}[\\mathcal{E}_{i,j}\\wedge\\neg\\mathcal{E}_{i,j^{\\prime}}]=\\operatorname*{Pr}[\\mathcal{E}_{i,j}]\\cdot\\operatorname*{Pr}[\\neg\\mathcal{E}_{i,j^{\\prime}}\\mid\\mathcal{E}_{i,j}]}\\\\ &{\\qquad\\qquad\\qquad=\\left(1-\\left(1-\\displaystyle\\frac{|C_{j}|}{n}\\right)^{|T_{i}|}\\right)\\cdot\\left(1-\\displaystyle\\frac{|C_{j^{\\prime}}|}{n}\\right)^{|T_{i}|-1}}\\\\ &{\\qquad\\qquad\\geq\\left(1-\\left(1-\\displaystyle\\frac{1}{B k}\\right)^{k/B}\\right)\\cdot\\left(1-\\displaystyle\\frac{B}{k}\\right)^{k/B}\\geq\\left(1-\\exp\\left(B^{-2}\\right)\\right)\\cdot\\displaystyle\\frac1e\\geq\\frac{1}{2e B^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and so for a fixed $(j,j^{\\prime})\\in\\binom{[k]}{2}$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{T_{1},\\ldots,T_{s}}\\left[\\forall i\\in[s]\\colon\\neg\\left(\\mathcal{E}_{i,j}\\wedge\\neg\\mathcal{E}_{i,j^{\\prime}}\\right)\\right]\\le\\left(1-\\frac{1}{2e B^{2}}\\right)^{2e B^{2}\\ln(100k^{2})}\\le\\frac{1}{100k^{2}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and the claim follows by a union bound over all $(j,j^{\\prime})\\in\\binom{[k]}{2}$ . ", "page_idx": 17}, {"type": "text", "text": "By Claim B.2, with probability at least $99/100$ , for every $j\\neq j^{\\prime}\\in[k]$ we have some $T_{i}$ such that $\\bar{T_{i}}\\cap C_{j}\\neq\\emptyset$ and $T_{i}\\cap C_{j^{\\prime}}=\\emptyset$ . In particular, for $x\\in U$ , let $C_{j^{*}}$ be the cluster containing $x$ . For every $j\\neq j^{*}$ we have some $T_{i}$ such that $T_{i}\\cap C_{j}\\neq\\emptyset$ and $T_{i}\\cap\\check{C}_{j^{*}}=\\emptyset$ which means that in line (9) of the algorithm, we have $J_{x}=[k]\\setminus\\{j^{*}\\}$ and so we successfully identify the cluster containing $x$ Moreover, this occurs for all $x$ . Finally, line (4) succeeds with probability $\\dot{9}9/100$ and thus the entire algorithm succeeds with probability at least $49/50$ by a union bound. ", "page_idx": 17}, {"type": "text", "text": "C Lower Bounds ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "C.1 An $\\Omega\\big(\\frac{n^{2}}{s^{2}}\\big)$ Lower Bound for Non-adaptive 3-Partition Recovery ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Theorem C.1. Non-adaptive 3-clustering requires $\\Omega(n^{2})\\,$ pair queries. ", "page_idx": 17}, {"type": "text", "text": "Proof. For every $(x,y)\\in\\left({U\\atop2}\\right)$ consider the following pair of partitions: ", "page_idx": 17}, {"type": "text", "text": "Observe that the oracle returns the same value for $P_{x,y}^{1}$ and $P_{x,y}^{2}$ on every possible query except on the set $\\{x,y\\}$ . Thus, if query set $Q\\subseteq U\\times U$ distinguishes these two clusterings, then $Q\\ni\\{x,y\\}$ . Therefore, the number of pairs $\\{x,y\\}$ such that $Q$ distinguishes $P_{x,y}^{1}$ and $P_{x,y}^{2}$ is at most $|Q|$ . Now, let $A$ be any non-adaptive pair-query algorithm which successfully recovers an arbitrary 3-clustering with probability $\\geq2/3$ . The algorithm $A$ queries a random set $Q\\subseteq U\\times U$ according to some distribution, $\\mathcal{D}_{A}$ . In particular, for every $\\{x,y\\}\\,\\in\\,{\\binom{U}{2}}$ , $Q$ distinguishes $P_{x,y}^{1}$ and $P_{x,y}^{2}$ with probability $\\geq2/3$ . Thus, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{2}{3}\\binom{n}{2}\\leq\\underset{\\{x,y\\}\\in\\binom{U}{2}}{\\sum}\\underset{Q\\sim\\mathcal{D}_{A}}{\\operatorname*{Pr}}[Q\\mathrm{~distinguishes}\\;P_{x,y}^{1}\\mathrm{~and~}P_{x,y}^{2}]}\\\\ &{\\qquad=\\mathbf{E}_{Q\\sim\\mathcal{D}_{A}}\\left[\\left|\\left\\{\\{x,y\\}\\in\\binom{U}{2}:Q\\mathrm{~distinguishes}\\;P_{x,y}^{1}\\mathrm{~and~}P_{x,y}^{2}\\right\\}\\right|\\right]\\leq|Q|}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "using linearity of expectation, and this completes the proof. ", "page_idx": 18}, {"type": "text", "text": "Corollary C.2. Non-adaptive 3-clustering requires $\\Omega(n^{2}/s^{2})$ subset queries of size at most s. ", "page_idx": 18}, {"type": "text", "text": "Proof. This follows from Theorem C.1 since one $s$ -sized query can be simulated by $\\binom{s}{2}$ pairqueries. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "Thus, in order to achieve a near-linear non-adaptive upper bound for 3-clustering, we require an algorithm which makes queries of size $\\widetilde{\\Omega}(\\sqrt{n})$ . ", "page_idx": 18}, {"type": "text", "text": "C.2 An $\\Omega(n\\log n)$ Lower Bound for Sample-Based 2-Partition Recovery ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Theorem C.3. Sample-based 2-clustering requires $\\Omega(n\\log n)$ subset queries. ", "page_idx": 18}, {"type": "text", "text": "Proof. Let $|U|=n$ be even and let $A,B\\subseteq U$ be two disjoint sets of size $|A|=|B|=n/2$ . Let $P\\,=\\,(A,B)$ and for any $x\\in U$ let $P_{x}$ denote the partition obtained by switching the set that $x$ belongs to. We show that it requires $\\Omega(n\\log n)$ sample-based subset queries to distinguish $P$ from $P_{x}$ for all $x$ . For $x\\in U$ and $T\\subseteq U$ , let $\\mathcal{E}_{x,T}$ denote the event that querying $T$ distinguishes $P$ from $P_{x}$ . Note that ${\\mathcal E}_{x}$ occurs iff $x\\in T$ and $T\\setminus x\\subseteq A$ or $T\\setminus x\\subseteq B$ . Thus, for a random set $T$ of size $s\\geq2$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{T:\\;|T|=s}\\left[\\mathcal{E}_{x,T}\\right]=s\\cdot\\frac{1}{n}\\cdot2\\cdot\\left(\\frac{n/2}{n}\\right)^{s-1}=\\frac{s}{n}\\cdot\\left(\\frac{1}{2}\\right)^{s-2}\\leq\\frac{2}{n}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "since the second-to-last quantity is clearly maximized when $s=2$ . Now, let $Q$ be a collection of sets, each of which consists of some of number of independent uniform samples. Note that the cardinality of these sets can differ from one another. Note that $Q$ distinguishes $P$ from every $P_{x}$ iff $\\mathcal{E}_{x,T}$ occurs for every $x$ and some $T$ . By eq. (3) and a standard coupon-collector argument, if $|Q|=o(n\\log n)$ , then with high probability there will be some $x$ for which $\\neg\\,\\lor_{T\\in Q}\\,\\mathcal{E}_{x,T}$ occurs. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "D Useful Lemmas ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "D.1 Vector Support Recovery from OR Queries ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Given $x\\in\\{0,1\\}^{n}$ , let $\\mathsf{s u p p}(x)=\\{i\\colon x_{i}=1\\}$ denote the support of $x$ . An OR-query on set $S\\subseteq[n]$ returns ", "page_idx": 18}, {"type": "equation", "text": "$$\n{\\mathsf{O R}}_{S}(x)=\\bigvee_{i\\in S}x_{i}={\\bf1}\\left({\\mathsf{s u p p}}(x)\\cap S\\neq\\emptyset\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This section discusses the problem of recovering the support of a vector via OR queries. In particular, we are interested in non-adaptive algorithms for this problem. The results in this section are standard in the combinatorial group testing and coin-weighing literature. See e.g. [26, 28] and also [48], who applied these results to obtain query algorithms for graph connectivity. ", "page_idx": 18}, {"type": "text", "text": "Lemma D.1. Let $x\\,\\in\\,\\{0,1\\}^{n}$ such that $|\\mathsf{s u p p}(x)|\\,=\\,1$ . There is a deterministic, non-adaptive algorithm that makes $\\lceil\\log n\\rceil$ OR queries and returns supp $(x)$ . The runtime is also $O(\\log n)$ . ", "page_idx": 18}, {"type": "text", "text": "Proof. Since $|\\mathsf{s u p p}(x)|\\,=\\,1$ , an OR query on set $S$ is equivalent to taking $\\langle x,v\\rangle$ where $v_{i}\\,=\\,1$ iff $i\\in S$ . Let $M$ be the $\\lceil\\log n\\rceil\\times n$ matrix whose $i^{'}$ \u2019th column is simply $b^{i}\\,\\in\\,\\{0,1\\}^{\\lceil\\log n\\rceil}$ , the binary representation of $i$ . The rows of $M$ correspond to OR queries. Then, $\\begin{array}{r}{M x=\\sum_{i=1}^{n}x_{i}b^{i}=}\\end{array}$ $\\textstyle\\sum_{i\\colon x_{i}=1}b^{i}=b_{j}$ where $j$ is the unique coordinate where $x_{j}=1$ . \u53e3 ", "page_idx": 19}, {"type": "text", "text": "Lemma D.2. Let $x\\in\\{0,1\\}^{n}$ . There is a deterministic, non-adaptive algorithm SER1bit that makes $2\\lceil\\log n\\rceil$ OR queries and certifies whether $|\\mathsf{s u p p}(x)|\\,=\\,0,|\\mathsf{s u p p}(x)|\\,=\\,1$ , or $|\\mathsf{s u p p}(x)|\\,>\\,1$ . If $|\\mathsf{s u p p}(x)|=1$ , then it outputs supp $(x)$ . The runtime is also $O(\\log n)$ . ", "page_idx": 19}, {"type": "text", "text": "Proof. Let $M$ be the $\\lceil\\log n\\rceil\\times n$ matrix described in the proof of Lemma D.1. Let $\\mathbf{1}=\\mathbf{1}^{\\lceil\\log n\\rceil\\times n}$ denote the all 1\u2019s matrix with the same dimensions. We query $M\\cdot x$ and $({\\bf1}-M)\\cdot x$ where here (\u00b7) denotes the $\\because O R$ product\". I.e. the $\\ddot{\\iota}$ \u2019th coordinate of $M\\cdot x$ is $\\mathbf{1}((M x)_{i}>0)$ ). Note that $\\mathbf{1}-M$ is obtained by flipping every bit in $M$ . Note that if $|\\mathsf{s u p p}(x)|=1$ , then $M\\cdot x$ is guaranteed to return the unique coordinate where $x$ has a one, as in the proof of Lemma D.1. Thus, it suffices to show that we can use these queries to determine whether $|\\mathsf{s u p p}(x)|$ is $0,1$ , or strictly greater than 1. ", "page_idx": 19}, {"type": "text", "text": "First, $|\\mathsf{s u p p}(x)|=0$ iff $(M\\cdot x)_{1}=0$ and $((\\mathbf{1}-M)\\cdot x)_{1}=0$ since the sets of 1-coordinates in the first row of $M$ and ${\\bf1}-M$ partition $[n]$ . ", "page_idx": 19}, {"type": "text", "text": "Next, we claim that $|\\mathsf{s u p p}(x)|>1$ iff there exists some $i\\in\\left[\\lceil\\log n\\rceil\\right]$ such that $(M\\cdot x)_{i}=1$ and $((\\mathbf{1}-M)\\cdot x)_{i}=1$ . Note that for every row $i$ , the 1-coordinates in the $i^{'}$ th row of $M$ and $\\mathbf{1}-M$ partition $[n]$ . Thus, clearly if $(M\\!\\cdot\\!x)_{i}=\\overline{{1}}$ and $((\\mathbf{1}-M)\\cdot x)_{i}=1$ , then there are at least 2 coordinates where $x$ has a one. Now we prove the converse. Suppose there exists $i\\neq j\\in[n]$ where $x_{i}=x_{j}=1$ . Let $b^{i},b^{j}\\,\\in\\,\\{0,1\\}^{\\lceil\\log n\\rceil}$ denote the binary representations of $i,j$ respectively. Since $i\\neq j$ , there exists some bit $k$ where $b_{k}^{i}\\neq b_{k}^{j}$ . Without loss of generality let $b_{k}^{i}=1$ and $b_{k}^{j}=0$ . Then, ", "page_idx": 19}, {"type": "equation", "text": "$$\n(M\\cdot x)_{k}=\\mathbf{1}\\left(\\left(\\sum_{\\ell=1}^{n}x_{\\ell}b^{\\ell}\\right)_{k}>0\\right)=\\mathbf{1}\\left(\\sum_{\\ell:\\;x_{\\ell}=1}^{n}b_{k}^{\\ell}>0\\right)=1,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n((1-M)\\cdot x)_{k}=\\mathbf{1}\\left(\\left(\\sum_{\\ell=1}^{n}x_{\\ell}(\\vec{1}-b^{\\ell})\\right)_{k}>0\\right)=\\mathbf{1}\\left(\\sum_{\\ell:\\,x_{\\ell}=1}^{n}(1-b_{k}^{\\ell})>0\\right)=1\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and this completes the proof. ", "page_idx": 19}, {"type": "text", "text": "Next, we describe a randomized non-adaptive algorithm for recovering the entire support of $x$ . ", "page_idx": 19}, {"type": "text", "text": "Lemma D.3. Let $x\\in\\{0,1\\}^{n}$ . There is a non-adaptive algorithm that makes $\\begin{array}{r}{O(t\\log{\\frac{n}{\\delta}})}\\end{array}$ OR queries on subsets of size $\\left\\lceil{\\frac{n}{t}}\\right\\rceil$ , and if $|\\mathsf{s u p p}(x)|\\leq t$ , returns supp $(x)$ with probability $1-\\delta$ , and otherwise certifies that $|\\mathsf{s u p p}(\\bar{x})|>t$ . The algorithm\u2019s runtime is $\\begin{array}{r}{O(n\\log{\\frac{n}{\\delta}})}\\end{array}$ . ", "page_idx": 19}, {"type": "text", "text": "Proof. For brevity, we assume that $t$ divides $n$ . Let $\\begin{array}{r}{m=e\\cdot t\\ln{\\frac{n}{\\delta}}}\\end{array}$ . We make OR queries on sets $S_{1},\\ldots,S_{m}$ , each formed by taking $n/t$ i.i.d. uniform samples from $[n]$ and define ", "page_idx": 19}, {"type": "equation", "text": "$$\nX=[n]\\setminus\\bigcup_{\\ell\\in[m]:\\ {\\sf O R}_{S_{\\ell}}(x)=0}S_{\\ell}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "If $|X|>t$ , we certify $|\\mathsf{s u p p}(x)|>t$ and if $\\vert X\\vert\\leq t$ , then we output $X$ ", "page_idx": 19}, {"type": "text", "text": "Assuming a uniform sample from $[n]$ can be obtained in $O(1)$ time, the runtime of the algorithm is $\\begin{array}{r}{O(m\\cdot\\frac{n}{t})=O(n\\ln\\frac{n}{\\delta})}\\end{array}$ . ", "page_idx": 19}, {"type": "text", "text": "Suppose that $|\\mathsf{s u p p}(x)|>t$ . Observe that $\\mathsf{s u p p}(x)\\subseteq X$ and so $|X|>t$ with probability 1. Thus, the algorithm is always correct in this case. ", "page_idx": 19}, {"type": "text", "text": "Now suppose $|\\mathsf{s u p p}(x)|\\leq t$ . We argue that $X={\\mathsf{s u p p}}(x)$ with probability at least $1-\\delta$ . Consider some $i\\not\\in{\\mathsf{s u p p}}(x)$ . Note that $i\\not\\in X$ iff there is some query $S_{\\ell}\\ni i$ for which $S_{\\ell}\\cap{\\mathsf{s u p p}}(x)=\\emptyset$ . Let $\\mathscr{E}_{i,\\ell}$ denote the event that $i\\in S_{\\ell}$ and $S_{\\ell}\\cap{\\mathsf{s u p p}}(x)=\\emptyset$ . Then, since $|\\mathsf{s u p p}(x)|\\leq t$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}[\\mathcal{E}_{i,\\ell}]=\\frac{n}{t}\\cdot\\frac{1}{n}\\cdot\\left(1-\\frac{\\left|\\mathsf{s u p p}(x)\\right|}{n}\\right)^{\\frac{n}{t}-1}\\geq\\frac{1}{t}\\left(1-\\frac{t}{n}\\right)^{\\frac{n}{t}}\\geq\\frac{1}{e t}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and so ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}[i\\in X]=\\operatorname*{Pr}\\left[\\neg\\mathcal{E}_{i,\\ell}{\\mathrm{~for~all~}}\\ell\\in[m]\\right]\\leq\\left(1-\\frac{1}{e t}\\right)^{m}\\leq\\frac{\\delta}{N}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "since $\\begin{array}{r}{m=e\\cdot t\\ln{\\frac{N}{\\delta}}}\\end{array}$ . Thus, by a union bound, we have $\\operatorname*{Pr}[X\\neq\\mathsf{s u p p}(x)]\\leq\\delta$ . ", "page_idx": 20}, {"type": "text", "text": "Finally, we make the following simple observation regarding algorithms that are restricted to making OR queries on subsets of bounded size. ", "page_idx": 20}, {"type": "text", "text": "Observation D.4. A single OR query on a set $S$ can be simulated by $\\frac{|S|}{s}$ queries of size at most s. ", "page_idx": 20}, {"type": "text", "text": "Combining this observation with Lemma D.3 gives the following lemma. ", "page_idx": 20}, {"type": "text", "text": "Lemma D.5. Let $x\\in\\{0,1\\}^{n}$ and $s,t\\geq1$ be positive integers where $\\begin{array}{r}{s\\leq\\frac{n}{t}}\\end{array}$ . There is a non-adaptive algorithm that makes $\\textstyle O\\big(\\frac{n}{s}\\log\\frac{n}{\\delta}\\big)$ OR queries on subsets of size s, and i $r\\,|\\mathsf{s u p p}(x)|\\,\\leq\\,t$ , returns supp $(x)$ with probability $1-\\delta$ , and otherwise certifies that $|\\mathsf{s u p p}(x)|>t$ . The algorithm runs in time $O(n\\log{\\frac{n}{\\delta}})$ . ", "page_idx": 20}, {"type": "text", "text": "D.2 Connectivity of Erd\u00f6s-R\u00e9nyi Random Graphs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Our proofs in Section 2.1, Appendix A, and Appendix G make use of the following bound on the probability of a random graph being connected. For intuition, note that for sufficiently large $n$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n1-(\\delta/3n)^{2/n}\\approx1-\\exp(-\\frac{2\\ln(3n/\\delta)}{n})\\approx\\frac{\\ln(3n/\\delta)}{n}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Thus, Fact D.6 asserts that for sufficiently large $n$ a random graph containing $\\gg n\\ln n$ edges is connected with high probability, which may be a more familiar statement to the reader. However, we need such a bound to be true even for very small $n$ and so we give the following more broadly applicable version. ", "page_idx": 20}, {"type": "text", "text": "Fact D.6. Let $G(n,p)$ denote an Erd\u00f6s-R\u00e9nyi random graph. If $p\\ge1-(\\delta/3n)^{2/n}$ , then $G(n,p)$ is connected with probability at least $1-\\delta$ . ", "page_idx": 20}, {"type": "text", "text": "Proof. A graph $G=(V,E)$ is connected if and only if for every cut $S\\subset V$ , there exists an edge $(u,v)\\in E\\cap(S\\times{\\overline{{S}}})$ . When $G$ is drawn from $G(n,p)$ , this does not occur for a cut $S$ of size $|S|=t$ with probability exactly $(1-p)^{t(n-t)}$ . There are exactly $\\binom{n}{t}$ such cuts. Thus, taking a union bound over all cuts and using our lower bound on $p$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{G\\sim G(n,p)}{\\operatorname*{Pr}}[G\\;\\mathrm{not\\;connected}]\\leq\\underset{t=1}{\\overset{n-1}{\\sum}}\\left(\\underset{t}{n}\\right)\\left(\\frac{\\delta}{3n}\\right)^{\\frac{2}{n}\\cdot t(n-t)}}&{\\quad}\\\\ {\\leq2\\underset{t=1}{\\overset{{\\lfloor n/2\\rfloor}}{\\sum}}\\left(\\underset{t}{n}\\right)\\left(\\frac{\\delta}{3n}\\right)^{\\frac{2}{n}\\cdot t(n-t)}}&{\\quad}\\\\ &{\\leq2\\underset{t=1}{\\overset{{\\lfloor n/2\\rfloor}}{\\sum}}\\left(\\underset{t}{n}\\right)\\left(\\frac{\\delta}{3n}\\right)^{\\frac{2}{n}\\cdot\\frac{t n}{2}}\\leq2\\underset{t=1}{\\overset{{\\lfloor n/2\\rfloor}}{\\sum}}n^{t}\\left(\\frac{\\delta}{3n}\\right)^{t}=2\\underset{t=1}{\\overset{{\\lfloor n/2\\rfloor}}{\\sum}}\\left(\\delta/3\\right)^{t}\\leq\\delta}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and this completes the proof. ", "page_idx": 20}, {"type": "text", "text": "E An $O(n\\log^{2}k)$ Algorithm for the Balanced Case ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In Appendix B, we gave an algorithm for $k$ -clustering making $O(n\\log k+k\\log^{4}k)$ subset queries when the cluster sizes are balanced within any constant factor. This query complexity simplifies to $O(n\\log k)$ as long as $\\textstyle k=O({\\frac{n}{\\log^{3}n}})$ . In this section we give an alternative algorithm which is more efficient when k \u226blogn3 n. ", "page_idx": 20}, {"type": "text", "text": "Theorem E.1. There is a non-adaptive algorithm for recovering a $B$ -balanced $k$ -clustering using $O(B^{2}n\\log^{2}k)$ subset queries of size $O(k)$ which succeeds with probability 99/100. ", "page_idx": 20}, {"type": "text", "text": "Proof. Recall that for a vector $v\\in\\{0,1\\}^{n}$ , an OR query on a set $S\\subseteq[n]$ returns $\\mathsf{O R}_{S}(v)=\\mathsf{V}_{i\\in S}\\,v_{i}$ . We will use the following lemma for recovering $\\mathsf{s u p p}(v)=\\{i\\colon v_{i}=1\\}$ via OR queries. We prove and discuss this lemma in Appendix D.1 (see Lemma D.2). ", "page_idx": 21}, {"type": "text", "text": "Lemma E.2. There is a deterministic, non-adaptive algorithm that takes an arbitrary $v\\in\\{0,1\\}^{n}$ , makes $2\\lceil\\log n\\rceil$ OR queries, and certifies whether $|{\\mathsf{s u p p}}(v)|=0$ , $|\\mathsf{s u p p}(v)|=1$ , or $|\\mathsf{s u p p}(v)|>1$ . $I\\!f\\,|\\mathsf{s u p p}(v)|=1$ , then it outputs supp $(v)$ . The runtime is ${\\cal O}(\\log n)$ . ", "page_idx": 21}, {"type": "text", "text": "Given $x\\;\\in\\;U\\;=\\;\\{x_{1},\\ldots,x_{n}\\}$ , let $C(x)$ denote the cluster containing it. Let $v^{(x)}\\,\\in\\,\\{0,1\\}^{n}$ denote the Boolean vector with $v_{i}^{(x)}\\,=\\,{\\bf1}(x_{i}\\,\\in\\,C(x))$ . As in Section 2.2, we have $\\mathsf{O R}_{S}(x)\\,=$ $\\mathbf{1}(q(S\\cup\\{x\\})=q(S))$ . I.e. OR queries to $\\boldsymbol{v}^{(x)}$ are simluted by two subset queries to the clustering. This implies the following corollary. ", "page_idx": 21}, {"type": "text", "text": "Corollary E.3. Given a $k$ -clustering on $U$ of size n and an element $x\\in U$ , let $C(x)$ denote the cluster containing $x$ . There is a deterministic non-adaptive algorithm which takes as input $x$ and $a$ set $R\\subseteq U$ , makes $O(\\log|R|)$ subset queries, and if $|\\bar{R_{\\mathit{\\Pi}}}\\cap C(x)|=1$ , then the algorithm returns the unique $z\\in R\\cap C(x)$ , and otherwise certifies that $|R\\cap C(x)|\\neq1$ . The runtime is $O(\\log|R|)$ . ", "page_idx": 21}, {"type": "text", "text": "The pseudocode for the algorithm is given in Alg. 5. In words, Corollary E.3 says that if we have a set $R$ containing exactly one representative from $C(x)$ , then with $O(\\log|R|)$ subset queries we can identify that representative. Thus, suppose we have a collection of sets $R_{1},\\ldots,R_{s}$ such that for every cluster $j\\in[k]$ , there is some $R_{i}$ containing a unique representative from $C_{j}$ . Consider the bipartite graph where on the left we have $U$ and on the right we have $R_{1}\\cup\\cdot\\cdot\\cdot\\cup R_{s}$ . Then, for every $x\\in U$ and every $R_{i}$ we can run the procedure from Corollary E.3, and if it returns a representative $y\\,\\in\\,R_{i}\\cap C(x)$ , then we add the edge $(x,y)$ to this graph. By the property of $R_{1},\\ldots,R_{s}$ , two vertices $x,y\\in U$ belong to the same cluster iff they are connected by a path of length 2 in this graph. We show that setting $\\dot{s^{\\ast}}=\\Theta(B^{2}\\log k)$ and letting each $R_{i}$ be a random sample of $k/B$ elements from $U$ results in a collection of sets with this good property with high probability. This leads to a query complexity of $n\\cdot s\\cdot O(\\log k)=O(n\\log^{2}k)$ . ", "page_idx": 21}, {"type": "text", "text": "Algorithm 5: Second Algorithm for the $B$ -Balanced Case ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "1 Input: Subset query access to a $B$ -balanced partition $C_{1}\\sqcup\\cdots\\sqcup C_{k}=U$ of $|U|=n$ points;   \n2 Choose $s=e B^{2}\\ln(100k)$ sets $R_{1},\\ldots,R_{s}$ each formed by $\\frac{k}{B}$ uniform samples from $U$ ;   \n3 Construct a bipartite graph $G(U,\\cup_{j=1}^{s}R_{j},E)$ as follows;   \n4 for $x\\in U$ and $i\\in[s]$ do   \n5 Run the algorithm from Corollary E.3 on input $x$ and $R_{i}$ ;   \n6 if the algorithm certifies there is a unique $y\\in R_{i}$ such that $x,y$ are in the same cluster then   \n7 Add the edge $(x,y)$ to $E(G)$ ;   \n8 end   \n9 end   \n10 Let $C_{1},\\ldots,C_{\\ell}$ denote the connected components of $G$ ;   \n11 Output the clustering $\\left(C_{1},\\ldots,C_{\\ell}\\right)$ ; ", "page_idx": 21}, {"type": "text", "text": "Query complexity and time complexity: The algorithm makes $\\begin{array}{r}{n\\cdot s\\cdot O(\\log\\frac{k}{B})=O(B^{2}n\\log^{2}k)}\\end{array}$ queries. We assume that a uniform random sample can be obtained in $O(1)$ time. Thus, line (2) runs in $O(B k\\ln k)$ time. By Corollary E.3, line (5) runs in time $O(|R_{i}|)=\\dot{O}(\\log k)$ . Thus, the entire for-loop (lines 4-9) runs in time $O(n s\\log k)=O(B^{2}n\\log^{2}k)$ . The bipartite graph $G$ has at most $O(n+B k\\log k)$ vertices and at most $O(n s)=O(B^{2}n\\log k)$ edges. Thus, line (10) can be executed in time $O(B^{2}n\\log k)$ time. The total runtime is thus dominated by $O(B^{2}n\\log^{2}k)$ . ", "page_idx": 21}, {"type": "text", "text": "The correctness of the algorithm now follows immediately from the following claim. ", "page_idx": 21}, {"type": "text", "text": "Claim E.4. With probability at least 99/100, for every $j~\\in~[k]$ , there exists $i~\\in~[s]$ such that $|R_{i}\\cap C_{j}|=1$ . ", "page_idx": 21}, {"type": "text", "text": "Proof. Fix $j\\in[k]$ and $i\\in[s]$ . We have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}[|R_{i}\\cap C_{j}|=1]=|R_{i}|\\cdot{\\frac{|C_{j}|}{n}}\\cdot\\left(1-{\\frac{|C_{j}|}{n}}\\right)^{|R_{i}|-1}\\geq{\\frac{k}{B}}\\cdot{\\frac{1}{B k}}\\cdot\\left(1-{\\frac{B}{k}}\\right)^{k/B}\\geq{\\frac{1}{e B^{2}}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and so for a fixed $j\\in[k]$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}[\\forall i\\in[s]\\colon|R_{i}\\cap C_{j}|\\neq1]\\leq\\left(1-\\frac{1}{e B^{2}}\\right)^{e B^{2}\\ln(100k)}\\leq\\frac{1}{100k}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and so by a union bound ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}[\\exists j\\in[k],\\forall i\\in[s]\\colon|R_{i}\\cap C_{j}|\\neq1]\\leq{\\frac{1}{100}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and this completes the proof. ", "page_idx": 22}, {"type": "text", "text": "F Two-Round Algorithms ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this section we describe two algorithms that use two rounds of adaptivity. That is, these algorithms are allowed to specify a round of queries, receive the responses, perform some computation, then specify a second round of queries and receive the responses, before finally recovering the clustering. We give a simple deterministic algorithm making $O(\\bar{n}\\log k)$ queries in Appendix F.1 and a randomized algorithm for recovering a balanced clustering with $O(n\\log\\log k)$ queries in Appendix F.2. Both algorithms exploit the additional round of queries to first compute a set containing exactly one representative from every cluster. ", "page_idx": 22}, {"type": "text", "text": "F.1 A Two Round $O(n\\log k)$ Deterministic Algorithm using Single Element Recovery ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Theorem F.1. There is a two-round, non-adaptive, deterministic algorithm for $k$ -clustering using $O(n\\log k)$ subset queries. ", "page_idx": 22}, {"type": "text", "text": "Algorithm 6: Deterministic 2-Round Algorithm ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1 Input: Subset query access to a hidden partition $C_{1},\\ldots,C_{k}$ of $U=\\{x_{1},\\ldots,x_{n}\\}$ ;   \n2 Round $^{\\,I}$ :   \n3 Query $P_{t}=\\{x_{i}\\colon i\\leq t\\}$ for every $t\\in[n]$ ;   \n4 Define $R=\\{\\stackrel{.}{x}_{t}:q(P_{t})\\stackrel{.}{-}q(P_{t-1})=1\\}$ containing exactly one point from every cluster;   \n5 For each $y\\in R$ , define cluster $R_{y}=\\{y\\}$ ;   \n6 Round 2:   \n7 for $x\\in U$ do   \n8 Use the $O(\\log k)$ deterministic non-adaptive algorithm of Corollary F.2 to find the unique   \n$y\\in R$ for which $x,y$ lie in the same cluster;   \n9 Place $x$ into $R_{y}$ ;   \n10 end   \n11 Output clustering $(R_{y}\\colon y\\in R)$ ; ", "page_idx": 22}, {"type": "text", "text": "Proof. Pseudocode for the algorithm is given in Alg. 6. The runtime is clearly dominated by the for-loop (lines 7-9) which run in time $O\\bar{(}n\\log k)$ by Corollary E.3. Fix an arbitrary ordering $U=$ $\\{x_{1},\\ldots,x_{n}\\}$ . The first round of queries (lines 3-5) is used to compute a set $R\\subseteq U$ containing exactly one representative from every cluster. This is done by querying every prefix $P_{t}\\,=\\,\\{x_{1},\\bar{\\cdot}\\,.\\,.\\,,x_{t}\\}$ and observing that $q(P_{t})-q(P_{t-1})=1$ iff $x_{t}$ is the only representative for its cluster in $P_{t}$ . Thus, the set $R$ computed in line (4) contains, for each cluster $C$ , the first member of $C$ in the ordering $x_{1},\\ldots,x_{n}$ . In particular, it contains exactly one representative from every cluster. The second round of queries is used to determine, for every $x\\in U$ , the unique representative of $C(x)$ in $R$ (see line 8). To accomplish this we recall Corollary E.3 from Appendix $\\mathrm{E}$ which we restate below. This completes the proof. \u53e3 ", "page_idx": 22}, {"type": "text", "text": "Corollary F.2. Given a $k$ -clustering on $U$ of size n and an element $x\\in U$ , let $C(x)$ denote the cluster containing $x$ . There is a deterministic non-adaptive algorithm which takes as input $x$ and $a$ set $R\\subseteq U$ , makes $O(\\log|R|)$ subset queries, and if $|R\\cap C(x)|=1,$ , then the algorithm returns the unique $z\\in R\\cap C(x)$ , and otherwise certifies that $|R\\cap C(x)|\\neq1$ . ", "page_idx": 22}, {"type": "text", "text": "F.2 A Two Round $O(n\\log\\log k)$ Algorithm for Balanced Clusters ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Recall that a clustering $C_{1}\\sqcup\\cdots\\sqcup C_{k}=U$ is $B$ -balanced if $\\begin{array}{r}{\\frac{n}{B k}\\leq|C_{j}|\\leq\\frac{B n}{k}}\\end{array}$ . ", "page_idx": 23}, {"type": "text", "text": "Theorem F.3. Ther\u221ae is a two round, non-adaptive algorithm which recovers a $B$ -balanced $k$ - clustering using $O({\\sqrt{B}}\\cdot n\\log\\log k)$ subset queries. ", "page_idx": 23}, {"type": "text", "text": "Proof. We will use the following result of [49] on query-based reconstruction of bipartite graphs as a black-box. Given a bipartite graph $G(V,W,E)$ , an edge-count query on $(S,T)$ where $S\\subseteq V$ , $T\\subseteq W$ returns $|E\\cap S\\times{\\bar{T}}|$ , the number of edges between $S$ and $T$ . ", "page_idx": 23}, {"type": "text", "text": "Lemma F.4 ([49], see Section 4.3). There is a non-adaptive algorithm which reconstructs any bipartite graph $G(V,W,E)$ where (a) $\\vert V\\vert=n$ , ( $b)\\left|W\\right|=m$ , and (c) every vertex in $V$ has degree at most 1, using $O(n\\cdot{\\frac{\\log n}{\\log m}})$ edge-count queries. ", "page_idx": 23}, {"type": "text", "text": "We will say a set $A\\subseteq U$ is an independent set if each element of $A$ belongs to a distinct cluster. Given two independent sets $A,B$ let $M(A,B)$ be the matching where there is an edge from $x\\in A$ to $y\\in B$ if $x,y$ belong to the same cluster. We observe that edge-count queries in $M(A,B)$ can be simulated by subset queries, leading to the following corollary. ", "page_idx": 23}, {"type": "text", "text": "Corollary F.5. Suppose that $A,B\\subseteq U$ are independent sets. There is a deterministic, non-adaptive algorithm which reconstructs $M(A,B)$ using ${\\bar{O(|A|\\cdot{\\frac{\\log|A|}{\\log|B|}})}}$ subset queries. ", "page_idx": 23}, {"type": "text", "text": "Proof. We need to show that an edge-count query $(S,T)$ where $S\\subseteq A,B\\subseteq T$ can be simulated by a constant number of subset queries. Let $m(S,T)$ denote the number of edges in $M(A,B)$ between $S$ and $T$ . Since $A,B$ are independent sets, $S,T$ are also independent sets, and so we have ", "page_idx": 23}, {"type": "equation", "text": "$$\nm(S,T)=q(S)+q(T)-q(S\\cup T)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "since $m(S,T)$ is the number of clusters intersected by both $S$ and $T$ . Thus, one edge-count query to $M(A,B)$ can be simulated by three subset queries and this completes the proof. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "Pseudocode for the algorithm is given in Alg. 7. The algorithm is parameterized in terms of a value $\\tau>1$ which we will choose later in the proof so as to minimize the query complexity. The first round is used to accomplish the following. In lines (4-5) we construct a set $R$ containing exactly one representative from every cluster and use this to define an initial clustering. In line (6) we sample random sets $I_{1},\\ldots,I_{s}$ and in line (8) make a query to each to check whether or not it is an independent set. Line (10) defines $V$ which is the union of all the $I_{i}$ \u2019s which are independent sets. We now describe the second round. In line (14) we run the procedure of Corollary F.5 to construct the matching $M(I_{i},R)$ whenever $I_{i}$ is an independent set. Finally, we determine for every $x\\in U$ , the unique $y\\in R$ for which $x,y$ belong to the same cluster. If $x\\in V$ this is done in lines (18-20) by taking $x$ \u2019s neighbor in $M(I_{i},R)$ for some independent set $I_{i}$ . If $x\\notin V$ , this is done in lines (23-24) by running the procedure of Corollary F.2. ", "page_idx": 23}, {"type": "text", "text": "The algorithm always either outputs fail in line (11), or correctly reconstructs the clustering by Corollary F.5 and Corollary F.2. Thus we only need to argue that $\\begin{array}{r}{|U\\setminus V|\\le\\frac{n}{\\tau}}\\end{array}$ occurs with probability at least $99/100$ allowing it to pass the check in line (11), and that conditioned on this, the algorithm makes $O({n\\ln\\ln k})$ queries when we set $\\tau$ appropriately. Let us first count the nu\u221amber of queries conditioned on this event. Line (8) performs $s$ queries. \u221aSince each $I_{i}$ i\u221as of size $\\sqrt{k}$ and $|R|=k$ , by Corollary F.5, lines (13-14) perform a total of $O(s\\cdot\\sqrt{k}\\ln\\tau)=O(\\sqrt{B}\\cdot n\\ln\\tau)$ queries. Lines (22-23) use $|U\\setminus V|O(\\log k)=O(\\frac{n}{\\tau}\\log k)$ queries. Setting $\\tau=\\Theta(\\ln k)$ yields a query complexity of $O({\\sqrt{B}}n\\log\\log k)$ . We now prove in Claim F.6 that the required bound on $|U\\setminus V|$ holds with high probability, and this completes the proof. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "Claim F.6. With probability at least 99/100, we have $\\begin{array}{r}{|U\\setminus V|\\le\\frac{n}{\\tau}}\\end{array}$ . ", "page_idx": 23}, {"type": "text", "text": "Proof. We prove an appropriate bound on $\\mathbf{E}[|U\\setminus V|]$ and then apply Markov\u2019s inequality. Fix $x\\in U$ . For $i\\in[s]$ , let $\\mathcal{E}_{x,i}$ denote the event that $x\\in I_{i}$ and $I_{i}$ is an independent set. Observe that $x\\in U\\setminus V$ iff $\\mathcal{E}_{x,i}$ does not occur for every $i\\in[s]$ . We first lower bound the probability of $\\mathcal{E}_{x,i}$ . Observe that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{r}[\\mathcal{E}_{x,i}]=\\operatorname*{Pr}[x\\in I_{i}]\\operatorname*{Pr}[I_{i}{\\mathrm{~an~independent~set~}}|{\\mathrm{~}}x\\in I_{i}]\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "1 Input: Subset query access to a hidden partition $C_{1}\\sqcup\\cdot\\cdot\\cdot\\sqcup C_{k}=U$ of $|U|=n$ points;   \n2 Round $^{\\,I}$ :   \n3 Query $P_{t}=\\{x_{i}\\colon i\\leq t\\}$ for every $t\\in[n]$ ;   \n4 Define $R=\\{x_{t}\\colon q(P_{t})-q(P_{t-1})=1\\}$ containing exactly one point from every cluster;   \n5 For each $y\\in R$ , define cluster $R_{y}=\\{y\\}$ ;   \n6 Sample $\\begin{array}{r}{s=10\\sqrt{\\frac{B}{k}}\\cdot n\\ln(100\\tau)}\\end{array}$ sets $I_{1},\\ldots,I_{s}\\subset U$ each formed by $\\sqrt{\\frac{k}{10B}}$ samples from $U$ ;   \n7 for $i\\in[s]\\ \\mathbf{do}$   \n8 Query $I_{i}$ . (This is to check if $q(I_{i})=|I_{i}|$ , i.e. whether $I_{i}$ is an independent set.);   \n9 end   \n10 Let $\\begin{array}{r}{V=\\bigcup_{i\\in[s]:~q(I_{i})=|I_{i}|}I_{i}}\\end{array}$ be the points in $U$ lying in an independent set among $I_{1},\\ldots,I_{s}$ ;   \n11 If $\\begin{array}{r}{|V|<n(1-\\frac{1}{\\tau})}\\end{array}$ , then output fail. Otherwise, continue;   \n12 Round 2:   \n13 for $i\\in s\\colon q(I_{i})=|I_{i}|$ do   \n14 Run the algorithm from Corollary F.5 on sets $I_{i},R$ and let $M_{i}\\subset I_{i}\\times R$ be the output;   \n15 end   \n16 for $x\\in U$ do   \n17 if $x\\in V$ then   \n18 Choose $I_{i}$ such that $x\\in I_{i}$ and $I_{i}$ is an independent set;   \n19 Let $y\\in R$ denote the neighbor of $x$ in the matching $M_{i}\\subset I_{i}\\times R$ ;   \n20 Place x into Ry;   \n21 end   \n22 if $x\\in U\\setminus V$ then   \n23 Use the $O(\\log k)$ deterministic non-adaptive algorithm of Corollary F.2 to find the unique   \n$y\\in R$ for which $x,y$ lie in the same cluster;   \n24 Place $x$ into $R_{y}$ ;   \n25 end   \n26 end   \n27 Output clustering $(R_{y}\\colon y\\in R)$ ; ", "page_idx": 24}, {"type": "text", "text": "and ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{I_{i}}[x\\in I_{i}]=1-\\left(1-\\frac{1}{n}\\right)^{|I_{i}|}\\geq1-\\exp\\left(-\\frac{|I_{i}|}{n}\\right)\\geq\\frac{|I_{i}|}{2n}\\geq\\sqrt{\\frac{k}{B}}\\cdot\\frac{1}{8n}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where we have used the inequality $\\exp(-z)\\leq1-\\frac{z}{2}$ for $z\\in[0,1]$ . Next, by a simple union bound over all pairs in $I_{i}$ and the fact that every cluster is bounded as $|C_{j}|\\leq{\\frac{B n}{k}}$ , we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}[I_{i}{\\mathrm{~not~an~independent~set~}}\\mid x\\in I_{i}]\\leq|I_{i}|^{2}{\\frac{B}{k}}\\leq{\\frac{1}{10}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Plugging these bounds back into Equation (5) yields $\\operatorname*{Pr}_{I_{i}}[\\mathcal{E}_{x,i}]\\ge\\sqrt{\\frac{k}{B}}\\cdot\\frac{1}{10n}$ and noting that these events are independent due to the $I_{i}$ \u2019s being independent yields ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}[x\\not\\in V]=\\operatorname*{Pr}[-\\mathcal{E}_{x,i},\\ \\forall i\\in[s]]\\leq\\left(1-\\sqrt{\\frac{k}{B}}\\cdot\\frac{1}{10n}\\right)^{s}=\\exp(-\\ln(100\\tau))=\\frac{1}{100\\tau}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where we have used the definition of $s=10\\sqrt{B/k}\\cdot n\\ln(100\\tau)$ . Finally, this implies $\\mathbf{E}[|U\\setminus V|]\\leq$ $\\frac{n}{100\\tau}$ and so by Markov\u2019s inequality $\\begin{array}{r}{\\operatorname*{Pr}[|U\\setminus V|>\\frac{n}{\\tau}]<\\frac{1}{100}}\\end{array}$ . This completes the proof. \u53e3 ", "page_idx": 24}, {"type": "text", "text": "G Sample-Based Algorithm using Unbounded Queries ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Theorem G.1. There is a non-adaptive, sample-based $k$ -clustering algorithm making $O(n k\\log n)$ subset queries which is correct with probability at least 99/100. ", "page_idx": 24}, {"type": "text", "text": "Proof. The algorithm is defined in Alg. 8. The proof techniques are quite similar to that of Theorems 2.2 and A.1 detailed in Section 2.1 and appendix A. We also refer the reader to Section 2 for a discussion on the main ideas. ", "page_idx": 25}, {"type": "image", "img_path": "lgtsXxk4dF/tmp/64aa9c9427c2e68a1ba443bfde69a28a78597ee8eb93b3dbd614de225e7741e5.jpg", "img_caption": [], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Since $\\textstyle\\sum_{p=0}^{\\log n}{\\frac{1}{2^{p}}}=O(1)$ , the number of queries made by the algorithm is $O(n k\\log n)$ . To prove correctness it suffices to prove the following lemma. ", "page_idx": 25}, {"type": "text", "text": "Lemma G.2. For each $p=0,1,\\ldots,\\log n_{!}$ , let $\\mathcal{E}_{p}$ denote the event that all clusters of size at least $\\frac{n}{2k\\!\\cdot\\!2^{p}}$ have been successfully recovered immediately following iteration $p$ of Alg. 8. Then, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}[\\lnot\\mathcal{E}_{0}]\\leq\\frac{1}{100k}\\;\\;a n d\\;\\;\\operatorname*{Pr}[\\lnot\\mathcal{E}_{p}\\mid\\mathcal{E}_{p-1}]\\leq\\frac{1}{100k}\\;\\,f o r\\,a l l\\,p\\in\\{1,2\\ldots,\\log n\\}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The proof that Lemma G.2 implies Theorem G.1 is identical to the proof that Lemma 2.3 implies Theorem 2.2 given just after the statement of Lemma 2.3. Thus, we move on to proving Lemma G.2. \u53e3 ", "page_idx": 25}, {"type": "text", "text": "Proof. of Lemma G.2. First consider the case of $p\\,=\\,0$ . In this iteration, the algorithm queries $|Q_{0}|\\geq40\\!\\cdot\\!n k\\ln(300n k^{2})$ random pairs and we need to show that it successfully recovers all clusters with size at least $\\frac{n}{2k}$ with probability at least \u22121010k. Let C denote any such cluster and recall from lines (16-17) the definition of the graph $G_{0}$ with vertex set $U$ and edge set $Q_{\\mathrm{0}}^{\\prime\\prime}$ . We will show that the induced subgraph $G_{0}[C]$ is connected, and thus $C$ is correctly recovered in lines (18-19), with probability at least $1-\\frac{1}{100k^{2}}$ 1001k2 . Since there are at most k clusters, the lemma holds by a union bound. ", "page_idx": 25}, {"type": "text", "text": "Consider any two vertices $x,y\\in C$ and note that $\\begin{array}{r}{|Q_{0}|\\geq\\frac{20n^{2}\\ln(300n k^{2})}{|C|}}\\end{array}$ since $\\textstyle|C|\\geq{\\frac{n}{2k}}$ . We lower bound the probability that $(x,y)$ is an edge in $G_{0}[C]$ as follows. Note that this occurs iff $\\{x,y\\}\\in Q_{0}$ . Using an identical calculation to that of eq. (1), this probability is at least $\\begin{array}{r}{1\\!-\\!\\big(\\frac{1}{300k^{2}|C|}\\big)^{2/|\\bar{C}|}}\\end{array}$ , implying that $G_{0}[C]$ is connected with probability at least $1-{\\frac{1}{100k^{2}}}$ 1001k2 by Fact 2.4. ", "page_idx": 25}, {"type": "text", "text": "The argument for the case of $p>0$ is identical to the argument given in \"Case $3\"$ of in the proof of Lemma 2.3 in Section 2. \u53e3 ", "page_idx": 25}, {"type": "text", "text": "H An $O(n\\log k)$ Adaptive algorithm ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Here we sketch a simple adaptive algorithm using $O(n\\log k)$ queries. Suppose, we have identified one element from $i$ clusters (initially $\\textit{i}=\\mathrm{~0~}$ , and we have $i\\ \\leq\\ k$ always). Suppose they are $X=\\{x_{1},x_{2},...,x_{i}\\}$ . We now want to find the cluster to which a new point $y$ belongs to. We first query $\\{X,y\\}$ . If the answer is $i+1$ , then $y$ is part of a new cluster and $i$ grows to $i+1$ . Otherwise, $y$ is part of the $i$ clusters, and we detect the cluster to which $y$ belongs to using a binary search. We consider the two sets $X_{1}=\\{x_{1},x_{2},..,x_{\\lceil i/2\\rceil}\\}$ , and $X_{2}=\\{x_{\\lceil i/2\\rceil+1},..,x_{i}\\}$ . We then query $\\{X_{1},y\\}$ . If the answer is $\\lceil i/2\\rceil+1$ , then we search recursively in $X_{2}$ , else if the query answer is $\\lceil i/2\\rceil$ , then we search recursively in $X_{1}$ . Clearly, the query complexity is $O(\\log k)$ per item, and it requires $O(\\log k)$ rounds of adaptivity even to place one element. ", "page_idx": 26}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: All of our claims made in the abstract and theorems stated in the introduction are proved formally in the paper. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 27}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: All of our results are theoretical and are expressed in the form of theorem statements, in which any assumptions are explicitly stated. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 27}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: All of our results are theoretical and the paper contains a complete formal proof of every result. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 28}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: Our paper does not include any experiments. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 28}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: Our paper does not include any experiments. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 29}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: Our paper does not include any experiments. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 29}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: Our paper does not include any experiments. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: Our paper does not include any experiments. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 30}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Our results are all purely theoretical and did not require the use of any data-sets or human subjects and don\u2019t pose any potential violation of the code of ethics. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 30}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: Since all of our results are theoretical and pertain to a specific model for the very broadly applicable problem of clustering, it is difficult to meaningfully discuss the specific societal impact of our work. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 30}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 31}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 31}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper does not use existing assets. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: The paper does not release new assets ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 32}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 32}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 32}]