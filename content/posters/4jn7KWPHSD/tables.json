[{"figure_path": "4jn7KWPHSD/tables/tables_6_1.jpg", "caption": "Table 1: Quantitative results of vanilla training, adversarial training, DUP Defense and our proposed Frequency Adversarial Training (FAT) on the ModelNet-C test set. Our proposed FAT outperforms all other methods in terms of mean corruption error (mCE), which demonstrates the effectiveness of FAT for improving corruption robustness.", "description": "This table presents a quantitative comparison of four different methods (Vanilla Training, Adversarial Training, DUP Defense, and FAT) for improving the robustness of 3D point cloud recognition models against common corruptions.  The evaluation is performed on the ModelNet-C test set, and the results are presented in terms of Overall Accuracy (OA), Mean Corruption Error (mCE), and corruption error (CE) for several corruption types.", "section": "4.2 Main results"}, {"figure_path": "4jn7KWPHSD/tables/tables_8_1.jpg", "caption": "Table 2: Quantitative results of combining FAT with different data augmentation strategies on the ModelNet-C test set. Compared with a single data augmentation strategy, the combination of FAT and different data augmentation strategies achieves a better mCE. Training GDANet with the combination of our proposed FAT with WOLFMix achieves the new state-of-the-art performance, with an impressive 0.537 mCE.", "description": "This table presents the results of experiments combining Frequency Adversarial Training (FAT) with different data augmentation methods on the ModelNet-C dataset.  The table shows that integrating FAT with various augmentation techniques (RSMix, PointWOLF, WOLFMix) consistently improves the mean corruption error (mCE), indicating enhanced robustness. Notably, the combination of FAT and WOLFMix on the GDANet model achieves the best performance, surpassing previous state-of-the-art results.", "section": "4.3 Data augmentation"}, {"figure_path": "4jn7KWPHSD/tables/tables_8_2.jpg", "caption": "Table 1: Quantitative results of vanilla training, adversarial training, DUP Defense and our proposed Frequency Adversarial Training (FAT) on the ModelNet-C test set. Our proposed FAT outperforms all other methods in terms of mean corruption error (mCE), which demonstrates the effectiveness of FAT for improving corruption robustness.", "description": "This table presents a comparison of four different training methods (vanilla training, adversarial training, DUP defense, and Frequency Adversarial Training) on the ModelNet-C dataset. The methods are evaluated based on their overall accuracy (OA) and mean corruption error (mCE).  The table demonstrates that the proposed FAT method achieves the best results in terms of mCE, indicating its effectiveness in improving the robustness of 3D point cloud recognition models against common corruptions.", "section": "4.2 Main results"}, {"figure_path": "4jn7KWPHSD/tables/tables_18_1.jpg", "caption": "Table 1: Quantitative results of vanilla training, adversarial training, DUP Defense and our proposed Frequency Adversarial Training (FAT) on the ModelNet-C test set. Our proposed FAT outperforms all other methods in terms of mean corruption error (mCE), which demonstrates the effectiveness of FAT for improving corruption robustness.", "description": "This table presents a quantitative comparison of four different methods for improving the robustness of 3D point cloud recognition models against common corruptions.  The methods compared are vanilla training, adversarial training, DUP Defense, and the proposed Frequency Adversarial Training (FAT). The table shows the overall accuracy (OA), mean corruption error (mCE), and corruption error (CE) for various corruption types (Rotate, Jitter, Scale, Drop-G, Drop-L, Add-G, Add-L) for each method. The results demonstrate that FAT significantly outperforms other methods in terms of reducing the mean corruption error.", "section": "4.2 Main results"}, {"figure_path": "4jn7KWPHSD/tables/tables_19_1.jpg", "caption": "Table 1: Quantitative results of vanilla training, adversarial training, DUP Defense and our proposed Frequency Adversarial Training (FAT) on the ModelNet-C test set. Our proposed FAT outperforms all other methods in terms of mean corruption error (mCE), which demonstrates the effectiveness of FAT for improving corruption robustness.", "description": "This table presents a comparison of four different methods for improving the robustness of 3D point cloud recognition models against common corruptions.  The methods compared are vanilla training, adversarial training, DUP Defense, and the authors' proposed Frequency Adversarial Training (FAT). The table shows the overall accuracy (OA) and mean corruption error (mCE) for each method, along with the corruption error (CE) for specific corruption types (Rotate, Jitter, Scale, Drop-G, Drop-L, Add-G, Add-L).  The results demonstrate that FAT significantly outperforms the other methods in reducing the mean corruption error.", "section": "4.2 Main results"}, {"figure_path": "4jn7KWPHSD/tables/tables_19_2.jpg", "caption": "Table 1: Quantitative results of vanilla training, adversarial training, DUP Defense and our proposed Frequency Adversarial Training (FAT) on the ModelNet-C test set. Our proposed FAT outperforms all other methods in terms of mean corruption error (mCE), which demonstrates the effectiveness of FAT for improving corruption robustness.", "description": "This table presents a comparison of four different methods for improving the robustness of 3D point cloud recognition models against common corruptions.  The methods compared are vanilla training, adversarial training, DUP Defense, and the proposed Frequency Adversarial Training (FAT). The table shows the overall accuracy (OA), mean corruption error (mCE), and corruption error (CE) for each corruption type (Rotate, Jitter, Scale, Drop-Global, Drop-Local, Add-Global, Add-Local) for each model architecture (DGCNN, PointNet, PCT, GDANet).  The results demonstrate that FAT consistently achieves the lowest mean corruption error, indicating its effectiveness in improving corruption robustness.", "section": "4.2 Main results"}, {"figure_path": "4jn7KWPHSD/tables/tables_19_3.jpg", "caption": "Table B.4: Quantitative results of the performance of FAT when integrated with data augmentation strategy in terms of mCE and ERcor on the ModelNet40-C test set. In previous studies [51], PCT with CutMix-R achieves the best robustness with the 0.635 mCE and 0.163 ERcor. However, training GDANet with the combination of our proposed FAT with WOLFMix achieves the new state-of-the-art performance, with the impressive 0.555 mCE and 0.147 ERcor.", "description": "This table presents a comparison of different methods' performance on ModelNet40-C, a benchmark dataset designed to evaluate the robustness of 3D point cloud recognition models against various corruptions. The table highlights the improvement in model robustness achieved by combining Frequency Adversarial Training (FAT) with different data augmentation techniques. The results demonstrate that the combination of FAT and WOLFMix achieves state-of-the-art performance on GDANet, significantly reducing the mean corruption error (mCE) and relative error (ERcor) compared to other methods.", "section": "B.3 The performance on the ModelNet40-C"}, {"figure_path": "4jn7KWPHSD/tables/tables_20_1.jpg", "caption": "Table 1: Quantitative results of vanilla training, adversarial training, DUP Defense and our proposed Frequency Adversarial Training (FAT) on the ModelNet-C test set. Our proposed FAT outperforms all other methods in terms of mean corruption error (mCE), which demonstrates the effectiveness of FAT for improving corruption robustness.", "description": "This table presents a quantitative comparison of four different methods for improving the robustness of 3D point cloud recognition models against common corruptions.  The methods compared are vanilla training, adversarial training, DUP Defense, and the authors' proposed Frequency Adversarial Training (FAT).  The table shows the overall accuracy (OA) and mean corruption error (mCE) for each method, as well as the corruption error (CE) for individual corruption types (Rotate, Jitter, Scale, Drop-G, Drop-L, Add-G, Add-L). The results demonstrate that FAT achieves the lowest mCE, indicating its superior performance in improving robustness against corruptions.", "section": "4.2 Main results"}, {"figure_path": "4jn7KWPHSD/tables/tables_20_2.jpg", "caption": "Table 1: Quantitative results of vanilla training, adversarial training, DUP Defense and our proposed Frequency Adversarial Training (FAT) on the ModelNet-C test set. Our proposed FAT outperforms all other methods in terms of mean corruption error (mCE), which demonstrates the effectiveness of FAT for improving corruption robustness.", "description": "This table presents a quantitative comparison of four different methods for improving the robustness of 3D point cloud recognition models against common corruptions. The methods compared are vanilla training, adversarial training, DUP Defense, and the proposed Frequency Adversarial Training (FAT).  The table shows the overall accuracy (OA) and mean corruption error (mCE) for each method, as well as the corruption error (CE) for specific corruption types (Rotate, Jitter, Scale, Drop-G, Drop-L, Add-G, Add-L).  The results demonstrate that FAT outperforms the other methods in terms of mCE, indicating its effectiveness in improving robustness against corruptions.", "section": "4.2 Main results"}, {"figure_path": "4jn7KWPHSD/tables/tables_21_1.jpg", "caption": "Table 1: Quantitative results of vanilla training, adversarial training, DUP Defense and our proposed Frequency Adversarial Training (FAT) on the ModelNet-C test set. Our proposed FAT outperforms all other methods in terms of mean corruption error (mCE), which demonstrates the effectiveness of FAT for improving corruption robustness.", "description": "This table presents a comparison of four different methods for improving the robustness of 3D point cloud recognition models against common corruptions.  The methods compared are vanilla training, adversarial training, DUP Defense, and the authors' proposed Frequency Adversarial Training (FAT).  The table shows the overall accuracy (OA) and mean corruption error (mCE) for each method, along with the corruption error (CE) broken down by corruption type (Rotate, Jitter, Scale, Drop-G, Drop-L, Add-G, Add-L).  The results demonstrate that FAT significantly outperforms the other methods in reducing the mean corruption error, highlighting its effectiveness in improving model robustness.", "section": "4.2 Main results"}]