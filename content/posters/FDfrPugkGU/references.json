{"references": [{"fullname_first_author": "Josh Achiam", "paper_title": "GPT-4 technical report", "publication_date": "2023-03-08", "reason": "This paper provides the technical details of GPT-4, a large language model crucial to the functioning of DoFIT, making it a foundational reference for understanding the base model used in the research."}, {"fullname_first_author": "Tianyu Han", "paper_title": "MedAlpaca-an open-source collection of medical conversational AI models and training data", "publication_date": "2023-04-08", "reason": "MedAlpaca is one of the datasets used to train and evaluate DoFIT, providing essential data for the research and impacting its results."}, {"fullname_first_author": "Zeyu Han", "paper_title": "Parameter-efficient fine-tuning for large models: A comprehensive survey", "publication_date": "2024-03-14", "reason": "This survey paper provides a comprehensive overview of parameter-efficient fine-tuning methods, which is the core methodology used in DoFIT."}, {"fullname_first_author": "Edward J Hu", "paper_title": "LoRA: Low-rank adaptation of large language models", "publication_date": "2021-00-00", "reason": "LoRA is a parameter-efficient fine-tuning technique central to DoFIT; this paper introduces the technique and is critical to understanding its application in the research."}, {"fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-00-00", "reason": "This paper details instruction tuning, a key concept in DoFIT, and its use of human feedback, which is relevant to the research's goals."}]}