[{"figure_path": "niG3Yyb6oA/tables/tables_9_1.jpg", "caption": "Table 2: Detailed statistics on ImageNet when top-1 testing accuracy achieves 75.9%.", "description": "This table presents a quantitative comparison of different optimization algorithms on the ImageNet dataset.  The metrics reported include the number of epochs required to reach a top-1 testing accuracy of 75.9%, the total training time, the time per epoch, the acceleration achieved relative to SGD (Stochastic Gradient Descent), and the best test accuracy reached.", "section": "4.2 ImageNet Training"}, {"figure_path": "niG3Yyb6oA/tables/tables_9_2.jpg", "caption": "Table 3: Detailed statistics on WMT when Bleu achieves 32%.", "description": "This table shows the detailed statistics of different optimizers on the WMT English-German machine translation corpus when the Bleu score reaches 32%. It compares the number of steps, total time, time per 1K steps, acceleration, and best test Bleu score achieved by SGD, Adam, KFAC, and LNGD.  The results highlight LNGD's efficiency in achieving high Bleu scores with fewer steps and less time compared to other methods.", "section": "4.3 Transformer Training"}, {"figure_path": "niG3Yyb6oA/tables/tables_13_1.jpg", "caption": "Table 4: Summary of some NGD optimizers", "description": "This table summarizes how different natural gradient descent (NGD) optimizers approximate the Fisher information matrix (F_i) for each layer (i).  KFAC uses a Kronecker product of two matrices (A and B). EKFAC refines this by incorporating eigenvalue decomposition and rescaling. TKFAC uses a Kronecker product scaled by a coefficient (\u03b4). LNGD, the proposed method, uses a Kronecker product of matrices (\u03a6 and \u03a8), with \u03a8 being a diagonal matrix.", "section": "B Comparisons and Explanations"}, {"figure_path": "niG3Yyb6oA/tables/tables_22_1.jpg", "caption": "Table 5: Detailed statistics of ablation study when top-1 testing accuracy achieves 75.9%.", "description": "This table presents the results of ablation studies conducted to evaluate the individual contributions of the adaptive layer-wise learning rate and the layer-wise sampling technique within the LNGD optimizer.  The results are presented for three versions of the LNGD algorithm:\n\n1. **LNGD-lr**: Uses the adaptive learning rate but not the layer-wise sampling.\n2. **LNGD-sample**: Uses the layer-wise sampling but not the adaptive learning rate.\n3. **LNGD**: Uses both the adaptive learning rate and the layer-wise sampling.\n\nThe table shows the number of epochs required to reach a top-1 testing accuracy of 75.9%, the total training time, the training time per epoch, and the relative acceleration compared to the standard LNGD algorithm.  It demonstrates the synergistic effect of combining both techniques for faster and more efficient model training.", "section": "E.3 Ablation Analysis"}, {"figure_path": "niG3Yyb6oA/tables/tables_22_2.jpg", "caption": "Table 6: Detailed statistics on CIFAR-10 when top-1 testing accuracy achieves 91%.", "description": "This table presents a comparison of different optimization methods on the CIFAR-10 dataset for image classification.  The comparison focuses on the number of epochs required to reach a top-1 testing accuracy of 91%, the total training time, the time per epoch, and the speedup achieved compared to SGD. The results show that LNGD achieves this accuracy using the fewest epochs and shortest total training time.", "section": "E.4 Results of More Comparisons"}]