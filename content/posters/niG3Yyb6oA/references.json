{"references": [{"fullname_first_author": "Herbert Robbins", "paper_title": "A stochastic approximation method", "publication_date": "1951-00-00", "reason": "This paper introduces a fundamental stochastic approximation method, which is a foundational concept in many optimization algorithms for machine learning, including those discussed in this paper."}, {"fullname_first_author": "Yu E Nesterov", "paper_title": "A method for solving the convex programming problem with convergence rate O(1/k\")", "publication_date": "1983-00-00", "reason": "This paper presents a significant advancement in convex optimization, providing a faster convergence rate that serves as a basis for various accelerated gradient methods relevant to the methods presented in the study."}, {"fullname_first_author": "Shun-Ichi Amari", "paper_title": "Natural gradient works efficiently in learning", "publication_date": "1998-00-00", "reason": "This work introduces the concept of the natural gradient, a crucial element in the proposed layer-wise natural gradient descent (LNGD) optimization method, which is central to this paper's contributions."}, {"fullname_first_author": "James Martens", "paper_title": "Optimizing neural networks with Kronecker-factored approximate curvature", "publication_date": "2015-00-00", "reason": "This study presents the Kronecker-factored approximate curvature (KFAC) method, a key precursor to the LNGD method proposed in this paper, influencing the development and comparison of the novel approach."}, {"fullname_first_author": "Diederik P Kingma", "paper_title": "Adam: A method for stochastic optimization", "publication_date": "2014-00-00", "reason": "This work introduces the Adam optimizer, a widely used first-order optimization method that serves as a significant baseline for comparison against the second-order LNGD optimizer proposed in the paper."}]}