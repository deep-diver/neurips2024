[{"figure_path": "n60xBFZWrk/tables/tables_3_1.jpg", "caption": "Table 1: Left pane: embedding in Poincar\u00e9 disk of the MDTs corresponding to the DTs learned at the 1st (top row) and 10th (bottom row) boosting iteration, on four UCI domains. Stark differences emerge between these domains from the plots alone. Right pane: comparison between Poincar\u00e9 disk embedding and its t = 0 t-self for an MDT learned on UCI online_shoppers_intention (top, boosting coefficients information not shown) and UCI hardware (bottom). The p\u2208 {0.001,0.999} isoline (rectangle) is barely distinguishable from OB but is clearly distinct from JB(0). Note that in B(0), the center looks similar to a scaling (zoom) of B; while near the border, the high nonlinearity of B(0) allows us to spot nodes that have high confidence / training accuracy (in orange) but can hardly be distinguished from the bulk of \u201cjust good\u201d nodes in B. Note also the set of red nodes in the Poincar\u00e9 disk for j = 70 (rectangle) that mistakenly look aligned, but not in the t-self.", "description": "This table presents a comparison of different embedding methods for decision trees (DTs) and their monotonic counterparts (MDTs) on four UCI datasets.  The left pane shows the Poincar\u00e9 disk embeddings of MDTs at the 1st and 10th boosting iterations, highlighting visual differences between datasets. The right pane compares standard Poincar\u00e9 disk embeddings with its t-self (t=0) variant for MDTs trained on the 'online_shoppers_intention' and 'hardware' datasets, illustrating the improved visualization near the disk's border offered by the t-self.", "section": "Experiments"}, {"figure_path": "n60xBFZWrk/tables/tables_9_1.jpg", "caption": "Table II: Results of the experiments checking whether \u201creducing\u201d the interpretation of a DT to that of its MDT (using its hyperbolic embedding) can give accurate information about the tree as well. Numerical columns, from left to right, give the average\u00b1std dev error for DTs and MDTs and provide the p-value of a Student paired t-test with H0 being the identity of the average errors. Entries in bold faces correspond to keeping H0 for a 0.05 first-order risk. See text for details.", "description": "This table presents the results of experiments comparing the classification accuracy of decision trees (DTs) and their corresponding monotonic decision trees (MDTs).  It shows the average test error and standard deviation for both types of trees across various datasets.  The p-value indicates the statistical significance of the difference in error rates between DTs and MDTs. Bold p-values highlight cases where the null hypothesis (no significant difference) is not rejected at the 0.05 significance level. This table helps to assess whether using MDTs instead of DTs for visualization and interpretation leads to significant loss of accuracy.", "section": "Experiments"}, {"figure_path": "n60xBFZWrk/tables/tables_25_1.jpg", "caption": "Table I: UCI, OpenML (Analcatdata_supreme) and Kaggle (Give_me_some_credit) domains considered in our experiments (m = total number of examples, d = number of features), ordered in increasing m \u00d7 n. Datset licenses listed in last column.", "description": "This table lists the datasets used in the experiments of the paper.  It provides the name of each dataset, the number of examples (m), the number of features (d), and the license under which the data is available. The datasets are from UCI, OpenML, and Kaggle.", "section": "Experiments"}, {"figure_path": "n60xBFZWrk/tables/tables_25_2.jpg", "caption": "Table II: Results of the experiments checking whether \u201creducing\u201d the interpretation of a DT to that of its MDT (using its hyperbolic embedding) can give accurate information about the tree as well. Numerical columns, from left to right, give the average\u00b1std dev error for DTs and MDTs and provide the p-value of a Student paired t-test with H0 being the identity of the average errors. Entries in bold faces correspond to keeping H0 for a 0.05 first-order risk. See text for details.", "description": "This table presents the results of experiments comparing the performance of decision trees (DTs) and their corresponding monotonic decision trees (MDTs) in terms of test error.  The p-values indicate the statistical significance of the difference in test errors between DTs and MDTs for each dataset.  Bold p-values indicate that the null hypothesis (no difference between DT and MDT performance) cannot be rejected at the 0.05 significance level.", "section": "6 Experiments"}, {"figure_path": "n60xBFZWrk/tables/tables_26_1.jpg", "caption": "Table 1: Left pane: embedding in Poincar\u00e9 disk of the MDTs corresponding to the DTs learned at the 1st (top row) and 10th (bottom row) boosting iteration, on four UCI domains. Stark differences emerge between these domains from the plots alone. Right pane: comparison between Poincar\u00e9 disk embedding and its t = 0 t-self for an MDT learned on UCI online_shoppers_intention (top, boosting coefficients information not shown) and UCI hardware (bottom). The p \u2208 {0.001,0.999} isoline (rectangle) is barely distinguishable from OB but is clearly distinct from JB(0). Note that in B(0), the center looks similar to a scaling (zoom) of B; while near the border, the high nonlinearity of B(0) allows us to spot nodes that have high confidence / training accuracy (in orange) but can hardly be distinguished from the bulk of \u201cjust good\u201d nodes in B. Note also the set of red nodes in the Poincar\u00e9 disk for j = 70 (rectangle) that mistakenly look aligned, but not in the t-self.", "description": "This table presents a comparison of different embedding methods for decision trees in the Poincar\u00e9 disk.  The left side shows embeddings of MDTs (Monotonic Decision Trees) at different boosting iterations across four UCI datasets, highlighting the visual differences in embedding patterns across datasets. The right side compares standard Poincar\u00e9 disk embeddings with its t-self (a modified version) for a specific MDT on two datasets, illustrating improvements in visualization and distinguishing high-confidence nodes near the boundary.", "section": "Experiments"}]