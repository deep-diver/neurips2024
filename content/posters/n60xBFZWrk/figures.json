[{"figure_path": "n60xBFZWrk/figures/figures_2_1.jpg", "caption": "Figure 1: Left pane: subtree of a decision tree (DT) (colors red, green denote the majority class in each node, grey = random posterior, tests at arcs not shown, n\u2081 is the root, no leaves shown) and its embedding following the simple recipe in (3): it is impossible to tell n2 from n5 and the tree depicted in B is not a faithful representation of the DT nor of any of its subtrees. Right pane: a small DT learned on UCI abalone (left) and its corresponding monotonic decision tree (MDT, right) learned using GETMDT. In each node, the real-valued prediction (VLOG (P)) is indicated, also in color. Observe that, indeed, H does not grant path-monotonic classification but H' does (Definition 4.1). In H', some nodes have outdegree 1; also, internal node #6 in the DT, whose prediction is worse than its parent, disappears in H'. One arc in H' is represented with double width as its Boolean test aggregates both tests it takes to go from #3 to #10 in H. Observations that would be classified by leaf #11 (resp. #5, resp. #9) in H are now classified by internal node #3 (resp. #2, resp. #4) in H', but predicted labels are the same for H and H' and so the accuracy of H and H' are the same.", "description": "The figure shows a comparison between a standard decision tree and its monotonic version. The left panel illustrates that a simple embedding approach fails to accurately represent the structure of the decision tree. The right panel introduces monotonic decision trees (MDTs) as a solution, which provides a more accurate and unambiguous embedding by only considering the monotonically increasing path of node confidences.", "section": "4 Posterior embedding in Poincar\u00e9 disk and clean embeddings for DTs"}, {"figure_path": "n60xBFZWrk/figures/figures_5_1.jpg", "caption": "Figure 1: Left pane: subtree of a decision tree (DT) (colors red, green denote the majority class in each node, grey = random posterior, tests at arcs not shown, n\u2081 is the root, no leaves shown) and its embedding following the simple recipe in (3): it is impossible to tell n2 from n5 and the tree depicted in B is not a faithful representation of the DT nor of any of its subtrees. Right pane: a small DT learned on UCI abalone (left) and its corresponding monotonic decision tree (MDT, right) learned using GETMDT. In each node, the real-valued prediction (VLOG (P)) is indicated, also in color. Observe that, indeed, H does not grant path-monotonic classification but H' does (Definition 4.1). In H', some nodes have outdegree 1; also, internal node #6 in the DT, whose prediction is worse than its parent, disappears in H'. One arc in H' is represented with double width as its Boolean test aggregates both tests it takes to go from #3 to #10 in H. Observations that would be classified by leaf #11 (resp. #5, resp. #9) in H are now classified by internal node #3 (resp. #2, resp. #4) in H', but predicted labels are the same for H and H' and so the accuracy of H and H' are the same.", "description": "This figure shows an example of a decision tree (DT) and its embedding in the Poincar\u00e9 disk.  The left panel illustrates the limitations of a naive embedding approach, highlighting how distinct nodes can be mapped to the same location. The right panel introduces monotonic decision trees (MDTs), a modified tree structure that allows for a clean and unambiguous embedding. The MDT is derived from the original DT by selecting only the monotonically increasing confidence paths and is shown to be a good representation of the original DT, while enabling a more accurate and interpretable embedding.", "section": "4 Posterior embedding in Poincar\u00e9 disk and clean embeddings for DTs"}, {"figure_path": "n60xBFZWrk/figures/figures_7_1.jpg", "caption": "Figure 1: Left pane: subtree of a decision tree (DT) (colors red, green denote the majority class in each node, grey = random posterior, tests at arcs not shown, n\u2081 is the root, no leaves shown) and its embedding following the simple recipe in (3): it is impossible to tell n2 from n5 and the tree depicted in B is not a faithful representation of the DT nor of any of its subtrees. Right pane: a small DT learned on UCI abalone (left) and its corresponding monotonic decision tree (MDT, right) learned using GETMDT. In each node, the real-valued prediction (VLOG (P)) is indicated, also in color. Observe that, indeed, H does not grant path-monotonic classification but H' does (Definition 4.1). In H', some nodes have outdegree 1; also, internal node #6 in the DT, whose prediction is worse than its parent, disappears in H'. One arc in H' is represented with double width as its Boolean test aggregates both tests it takes to go from #3 to #10 in H. Observations that would be classified by leaf #11 (resp. #5, resp. #9) in H are now classified by internal node #3 (resp. #2, resp. #4) in H', but predicted labels are the same for H and H' and so the accuracy of H and H' are the same.", "description": "The figure shows two examples of decision trees. The left pane shows a subtree of a decision tree and its embedding in a Poincar\u00e9 disk.  The right pane presents a small decision tree learned from data and its corresponding monotonic decision tree.  The embedding of the decision tree is problematic, while the monotonic version offers a cleaner embedding suitable for visualization and interpretation.", "section": "4 Posterior embedding in Poincar\u00e9 disk and clean embeddings for DTs"}, {"figure_path": "n60xBFZWrk/figures/figures_7_2.jpg", "caption": "Figure 2: Two example MDTs learned on UCI domain online_shoppers_intention, showing embedding error \u03c1 (5). Left: key parts of the embedding. Right: annotations used in Experiments, Section 6.", "description": "This figure shows two examples of MDTs (Monotonic Decision Trees) learned from the UCI online_shoppers_intention dataset. The left panel illustrates key aspects of the MDT embedding in the Poincar\u00e9 disk, highlighting the embedding error (\u03c1) and posterior isolines.  The right panel provides annotations that are used and explained in section 6 of the paper, illustrating how the visualizations are interpreted and the information they convey about the models and their performance.", "section": "Experiments"}, {"figure_path": "n60xBFZWrk/figures/figures_22_1.jpg", "caption": "Figure 1: Left pane: subtree of a decision tree (DT) (colors red, green denote the majority class in each node, grey = random posterior, tests at arcs not shown, n\u2081 is the root, no leaves shown) and its embedding following the simple recipe in (3): it is impossible to tell n2 from n5 and the tree depicted in B is not a faithful representation of the DT nor of any of its subtrees. Right pane: a small DT learned on UCI abalone (left) and its corresponding monotonic decision tree (MDT, right) learned using GETMDT. In each node, the real-valued prediction (VLOG (P)) is indicated, also in color. Observe that, indeed, H does not grant path-monotonic classification but H' does (Definition 4.1). In H', some nodes have outdegree 1; also, internal node #6 in the DT, whose prediction is worse than its parent, disappears in H'. One arc in H' is represented with double width as its Boolean test aggregates both tests it takes to go from #3 to #10 in H. Observations that would be classified by leaf #11 (resp. #5, resp. #9) in H are now classified by internal node #3 (resp. #2, resp. #4) in H', but predicted labels are the same for H and H' and so the accuracy of H and H' are the same.", "description": "This figure shows a comparison between a regular decision tree and its monotonic version. The left panel shows that a naive embedding of a decision tree in the Poincar\u00e9 disk can lead to ambiguous results, where different nodes or edges in the tree might be mapped to the same points in the Poincar\u00e9 disk. The right panel illustrates how creating a monotonic decision tree by only keeping the monotonically increasing confidence predictions resolves this issue. A monotonic decision tree is a more faithful representation of the original decision tree in hyperbolic space.", "section": "4 Posterior embedding in Poincar\u00e9 disk and clean embeddings for DTs"}, {"figure_path": "n60xBFZWrk/figures/figures_23_1.jpg", "caption": "Figure 1: Left pane: subtree of a decision tree (DT) (colors red, green denote the majority class in each node, grey = random posterior, tests at arcs not shown, n\u2081 is the root, no leaves shown) and its embedding following the simple recipe in (3): it is impossible to tell n2 from n5 and the tree depicted in B is not a faithful representation of the DT nor of any of its subtrees. Right pane: a small DT learned on UCI abalone (left) and its corresponding monotonic decision tree (MDT, right) learned using GETMDT. In each node, the real-valued prediction (VLOG (P)) is indicated, also in color. Observe that, indeed, H does not grant path-monotonic classification but H' does (Definition 4.1). In H', some nodes have outdegree 1; also, internal node #6 in the DT, whose prediction is worse than its parent, disappears in H'. One arc in H' is represented with double width as its Boolean test aggregates both tests it takes to go from #3 to #10 in H. Observations that would be classified by leaf #11 (resp. #5, resp. #9) in H are now classified by internal node #3 (resp. #2, resp. #4) in H', but predicted labels are the same for H and H' and so the accuracy of H and H' are the same.", "description": "The figure shows a comparison between a decision tree (DT) and its corresponding monotonic decision tree (MDT).  The left panel illustrates how a naive embedding of a DT can lead to ambiguity, while the right panel demonstrates the unambiguous embedding of an MDT, highlighting the advantages of using MDTs for representing supervised models in hyperbolic space.  Key differences between the DT and MDT representations are explained.", "section": "4 Posterior embedding in Poincar\u00e9 disk and clean embeddings for DTs"}, {"figure_path": "n60xBFZWrk/figures/figures_24_1.jpg", "caption": "Figure 1: Left pane: subtree of a decision tree (DT) (colors red, green denote the majority class in each node, grey = random posterior, tests at arcs not shown, n\u2081 is the root, no leaves shown) and its embedding following the simple recipe in (3): it is impossible to tell n2 from n5 and the tree depicted in B is not a faithful representation of the DT nor of any of its subtrees. Right pane: a small DT learned on UCI abalone (left) and its corresponding monotonic decision tree (MDT, right) learned using GETMDT. In each node, the real-valued prediction (VLOG (P)) is indicated, also in color. Observe that, indeed, H does not grant path-monotonic classification but H' does (Definition 4.1). In H', some nodes have outdegree 1; also, internal node #6 in the DT, whose prediction is worse than its parent, disappears in H'. One arc in H' is represented with double width as its Boolean test aggregates both tests it takes to go from #3 to #10 in H. Observations that would be classified by leaf #11 (resp. #5, resp. #9) in H are now classified by internal node #3 (resp. #2, resp. #4) in H', but predicted labels are the same for H and H' and so the accuracy of H and H' are the same.", "description": "The figure shows a comparison between a regular decision tree and its monotonic version.  The left panel illustrates how a standard decision tree embedding can be ambiguous, failing to faithfully represent the tree structure. The right panel demonstrates the improved, unambiguous embedding achieved using a monotonic decision tree, highlighting the benefits of this approach for visualization and interpretation.", "section": "4 Posterior embedding in Poincar\u00e9 disk and clean embeddings for DTs"}, {"figure_path": "n60xBFZWrk/figures/figures_26_1.jpg", "caption": "Figure 1: Left pane: subtree of a decision tree (DT) (colors red, green denote the majority class in each node, grey = random posterior, tests at arcs not shown, n\u2081 is the root, no leaves shown) and its embedding following the simple recipe in (3): it is impossible to tell n2 from n5 and the tree depicted in B is not a faithful representation of the DT nor of any of its subtrees. Right pane: a small DT learned on UCI abalone (left) and its corresponding monotonic decision tree (MDT, right) learned using GETMDT. In each node, the real-valued prediction (VLOG (P)) is indicated, also in color. Observe that, indeed, H does not grant path-monotonic classification but H' does (Definition 4.1). In H', some nodes have outdegree 1; also, internal node #6 in the DT, whose prediction is worse than its parent, disappears in H'. One arc in H' is represented with double width as its Boolean test aggregates both tests it takes to go from #3 to #10 in H. Observations that would be classified by leaf #11 (resp. #5, resp. #9) in H are now classified by internal node #3 (resp. #2, resp. #4) in H', but predicted labels are the same for H and H' and so the accuracy of H and H' are the same.", "description": "The figure shows a comparison between a standard decision tree (DT) and its corresponding monotonic decision tree (MDT). The left pane illustrates a subtree of a DT and its embedding in the Poincar\u00e9 disk, highlighting the inability to distinguish between certain nodes based on their embedding alone. The right pane presents a small DT learned on the UCI Abalone dataset and its MDT generated using the GETMDT algorithm. The MDT is designed to provide a clear and unambiguous embedding in the Poincar\u00e9 disk, eliminating the ambiguities present in the original DT embedding.", "section": "4 Posterior embedding in Poincar\u00e9 disk and clean embeddings for DTs"}, {"figure_path": "n60xBFZWrk/figures/figures_27_1.jpg", "caption": "Figure 2: Two example MDTs learned on UCI domain online_shoppers_intention, showing embedding error \u03c1 (5). Left: key parts of the embedding. Right: annotations used in Experiments, Section 6.", "description": "This figure shows two examples of Monotonic Decision Trees (MDTs) and their embeddings in the Poincar\u00e9 disk.  The left panel highlights key aspects of the embedding, such as the mapping between the MDT structure and its representation in the Poincar\u00e9 disk.  The right panel provides annotations clarifying the visualization elements, illustrating how the embedding relates to features like posterior probabilities and the confidence of predictions. The embedding error (\u03c1) is also indicated, representing the difference between the model's confidence and its hyperbolic distance from the origin. These visualizations are discussed further in Section 6 of the paper.", "section": "Experiments"}, {"figure_path": "n60xBFZWrk/figures/figures_27_2.jpg", "caption": "Figure 1: Left pane: subtree of a decision tree (DT) (colors red, green denote the majority class in each node, grey = random posterior, tests at arcs not shown, n\u2081 is the root, no leaves shown) and its embedding following the simple recipe in (3): it is impossible to tell n2 from n5 and the tree depicted in B is not a faithful representation of the DT nor of any of its subtrees. Right pane: a small DT learned on UCI abalone (left) and its corresponding monotonic decision tree (MDT, right) learned using GETMDT. In each node, the real-valued prediction (VLOG (P)) is indicated, also in color. Observe that, indeed, H does not grant path-monotonic classification but H' does (Definition 4.1). In H', some nodes have outdegree 1; also, internal node #6 in the DT, whose prediction is worse than its parent, disappears in H'. One arc in H' is represented with double width as its Boolean test aggregates both tests it takes to go from #3 to #10 in H. Observations that would be classified by leaf #11 (resp. #5, resp. #9) in H are now classified by internal node #3 (resp. #2, resp. #4) in H', but predicted labels are the same for H and H' and so the accuracy of H and H' are the same.", "description": "The figure shows a comparison between a standard decision tree (DT) and its corresponding monotonic decision tree (MDT).  The left panel illustrates the problem of embedding a DT directly into the Poincar\u00e9 disk, highlighting the ambiguity and loss of structural information. The right panel demonstrates the solution proposed by the authors: using MDTs to achieve a clean and unambiguous embedding. MDTs are a modified version of DTs that guarantee monotonically increasing confidence along any path from root to leaf, enabling clear visualization in hyperbolic space.", "section": "4 Posterior embedding in Poincar\u00e9 disk and clean embeddings for DTs"}, {"figure_path": "n60xBFZWrk/figures/figures_27_3.jpg", "caption": "Figure 1: Left pane: subtree of a decision tree (DT) (colors red, green denote the majority class in each node, grey = random posterior, tests at arcs not shown, n\u2081 is the root, no leaves shown) and its embedding following the simple recipe in (3): it is impossible to tell n2 from n5 and the tree depicted in B is not a faithful representation of the DT nor of any of its subtrees. Right pane: a small DT learned on UCI abalone (left) and its corresponding monotonic decision tree (MDT, right) learned using GETMDT. In each node, the real-valued prediction (VLOG (P)) is indicated, also in color. Observe that, indeed, H does not grant path-monotonic classification but H' does (Definition 4.1). In H', some nodes have outdegree 1; also, internal node #6 in the DT, whose prediction is worse than its parent, disappears in H'. One arc in H' is represented with double width as its Boolean test aggregates both tests it takes to go from #3 to #10 in H. Observations that would be classified by leaf #11 (resp. #5, resp. #9) in H are now classified by internal node #3 (resp. #2, resp. #4) in H', but predicted labels are the same for H and H' and so the accuracy of H and H' are the same.", "description": "This figure shows a comparison between a decision tree (DT) and its corresponding monotonic decision tree (MDT). The left panel illustrates a subtree of a DT and its flawed embedding in the Poincar\u00e9 disk, highlighting the ambiguity in representing nodes and arcs. The right panel demonstrates a small DT and its MDT generated using the GETMDT algorithm. The MDT provides a path-monotonic classification, resolving the ambiguity issues of the DT embedding while maintaining predictive accuracy.", "section": "4 Posterior embedding in Poincar\u00e9 disk and clean embeddings for DTs"}, {"figure_path": "n60xBFZWrk/figures/figures_28_1.jpg", "caption": "Figure 2: Two example MDTs learned on UCI domain online_shoppers_intention, showing embedding error \u03c1 (5). Left: key parts of the embedding. Right: annotations used in Experiments, Section 6.", "description": "This figure shows two examples of MDTs (Monotonic Decision Trees) learned from the UCI online_shoppers_intention dataset.  The left panel displays key elements of the hyperbolic embedding in the Poincar\u00e9 disk, illustrating the relationship between confidence (posterior probability) and distance from the origin. The right panel provides annotations to aid interpretation of the visualizations used in the experiments described in Section 6 of the paper.", "section": "Experiments"}, {"figure_path": "n60xBFZWrk/figures/figures_28_2.jpg", "caption": "Figure 2: Two example MDTs learned on UCI domain online_shoppers_intention, showing embedding error \u03c1 (5). Left: key parts of the embedding. Right: annotations used in Experiments, Section 6.", "description": "This figure shows two examples of Monotonic Decision Trees (MDTs) and their embeddings in the Poincar\u00e9 disk.  The left panel shows a detailed view of the embedding, highlighting key features like the embedding error, posterior isolines, and leveraging coefficients. The right panel provides annotations to aid interpretation, referring to elements discussed in Section 6 of the paper, such as the relationship between node placement and prediction confidence, the use of color to represent class majority, and how arc thickness conveys the complexity of decision paths within the original decision tree.", "section": "Experiments"}, {"figure_path": "n60xBFZWrk/figures/figures_28_3.jpg", "caption": "Figure 1: Left pane: subtree of a decision tree (DT) (colors red, green denote the majority class in each node, grey = random posterior, tests at arcs not shown, n\u2081 is the root, no leaves shown) and its embedding following the simple recipe in (3): it is impossible to tell n2 from n5 and the tree depicted in B is not a faithful representation of the DT nor of any of its subtrees. Right pane: a small DT learned on UCI abalone (left) and its corresponding monotonic decision tree (MDT, right) learned using GETMDT. In each node, the real-valued prediction (VLOG (P)) is indicated, also in color. Observe that, indeed, H does not grant path-monotonic classification but H' does (Definition 4.1). In H', some nodes have outdegree 1; also, internal node #6 in the DT, whose prediction is worse than its parent, disappears in H'. One arc in H' is represented with double width as its Boolean test aggregates both tests it takes to go from #3 to #10 in H. Observations that would be classified by leaf #11 (resp. #5, resp. #9) in H are now classified by internal node #3 (resp. #2, resp. #4) in H', but predicted labels are the same for H and H' and so the accuracy of H and H' are the same.", "description": "This figure demonstrates the problem of embedding a decision tree (DT) directly into hyperbolic space. The left panel shows a DT subtree and its embedding, highlighting the ambiguity introduced by the direct mapping. The right panel showcases a monotonic decision tree (MDT), a modified version of the DT that provides a clearer and unambiguous embedding.", "section": "4 Posterior embedding in Poincar\u00e9 disk and clean embeddings for DTs"}, {"figure_path": "n60xBFZWrk/figures/figures_29_1.jpg", "caption": "Figure 1: Left pane: subtree of a decision tree (DT) (colors red, green denote the majority class in each node, grey = random posterior, tests at arcs not shown, n\u2081 is the root, no leaves shown) and its embedding following the simple recipe in (3): it is impossible to tell n2 from n5 and the tree depicted in B is not a faithful representation of the DT nor of any of its subtrees. Right pane: a small DT learned on UCI abalone (left) and its corresponding monotonic decision tree (MDT, right) learned using GETMDT. In each node, the real-valued prediction (VLOG (P)) is indicated, also in color. Observe that, indeed, H does not grant path-monotonic classification but H' does (Definition 4.1). In H', some nodes have outdegree 1; also, internal node #6 in the DT, whose prediction is worse than its parent, disappears in H'. One arc in H' is represented with double width as its Boolean test aggregates both tests it takes to go from #3 to #10 in H. Observations that would be classified by leaf #11 (resp. #5, resp. #9) in H are now classified by internal node #3 (resp. #2, resp. #4) in H', but predicted labels are the same for H and H' and so the accuracy of H and H' are the same.", "description": "The figure shows a comparison between a regular decision tree and its monotonic version.  The left panel illustrates the problem of ambiguous embedding of a decision tree in the Poincar\u00e9 disk, where different nodes are mapped to the same location. The right panel introduces monotonic decision trees (MDTs) as a solution to this problem, showcasing how MDTs provide a clean and unambiguous embedding.", "section": "4 Posterior embedding in Poincar\u00e9 disk and clean embeddings for DTs"}, {"figure_path": "n60xBFZWrk/figures/figures_29_2.jpg", "caption": "Figure 2: Two example MDTs learned on UCI domain online_shoppers_intention, showing embedding error \u03c1 (5). Left: key parts of the embedding. Right: annotations used in Experiments, Section 6.", "description": "This figure shows two examples of MDTs (Monotonic Decision Trees) and their embeddings in the Poincar\u00e9 disk.  The left panel displays key aspects of the embedding, highlighting the relationship between model confidence and distance from the origin. The right panel provides annotations explaining various aspects of the visualization used in the experiments detailed in Section 6 of the paper.", "section": "Experiments"}, {"figure_path": "n60xBFZWrk/figures/figures_30_1.jpg", "caption": "Figure 1: Left pane: subtree of a decision tree (DT) (colors red, green denote the majority class in each node, grey = random posterior, tests at arcs not shown, n\u2081 is the root, no leaves shown) and its embedding following the simple recipe in (3): it is impossible to tell n2 from n5 and the tree depicted in B is not a faithful representation of the DT nor of any of its subtrees. Right pane: a small DT learned on UCI abalone (left) and its corresponding monotonic decision tree (MDT, right) learned using GETMDT. In each node, the real-valued prediction (VLOG (P)) is indicated, also in color. Observe that, indeed, H does not grant path-monotonic classification but H' does (Definition 4.1). In H', some nodes have outdegree 1; also, internal node #6 in the DT, whose prediction is worse than its parent, disappears in H'. One arc in H' is represented with double width as its Boolean test aggregates both tests it takes to go from #3 to #10 in H. Observations that would be classified by leaf #11 (resp. #5, resp. #9) in H are now classified by internal node #3 (resp. #2, resp. #4) in H', but predicted labels are the same for H and H' and so the accuracy of H and H' are the same.", "description": "The figure shows a comparison between a decision tree (DT) and its corresponding monotonic decision tree (MDT).  The left panel illustrates a subtree of a DT and its flawed embedding in the Poincar\u00e9 disk, highlighting the ambiguity of the representation. The right panel demonstrates a small DT and its MDT, illustrating how the MDT provides a path-monotonic classification and a cleaner, more interpretable embedding. The MDT achieves this by only including nodes with strictly increasing confidence along any path from root to leaf.", "section": "4 Posterior embedding in Poincar\u00e9 disk and clean embeddings for DTs"}, {"figure_path": "n60xBFZWrk/figures/figures_30_2.jpg", "caption": "Figure 1: Left pane: subtree of a decision tree (DT) (colors red, green denote the majority class in each node, grey = random posterior, tests at arcs not shown, n\u2081 is the root, no leaves shown) and its embedding following the simple recipe in (3): it is impossible to tell n2 from n5 and the tree depicted in B is not a faithful representation of the DT nor of any of its subtrees. Right pane: a small DT learned on UCI abalone (left) and its corresponding monotonic decision tree (MDT, right) learned using GETMDT. In each node, the real-valued prediction (VLOG (P)) is indicated, also in color. Observe that, indeed, H does not grant path-monotonic classification but H' does (Definition 4.1). In H', some nodes have outdegree 1; also, internal node #6 in the DT, whose prediction is worse than its parent, disappears in H'. One arc in H' is represented with double width as its Boolean test aggregates both tests it takes to go from #3 to #10 in H. Observations that would be classified by leaf #11 (resp. #5, resp. #9) in H are now classified by internal node #3 (resp. #2, resp. #4) in H', but predicted labels are the same for H and H' and so the accuracy of H and H' are the same.", "description": "This figure shows a comparison between a regular decision tree (DT) and a monotonic decision tree (MDT). The left panel shows how a naive embedding of a DT in the Poincar\u00e9 disk can result in indistinguishable nodes.  The right panel demonstrates that the MDT, a modified version of the DT which ensures monotonicity of the path predictions, provides a clear and unambiguous representation in the Poincar\u00e9 disk.  The MDT simplifies the tree structure and removes nodes with non-monotonic predictions, making it suitable for embedding.", "section": "4 Posterior embedding in Poincar\u00e9 disk and clean embeddings for DTs"}]