[{"type": "text", "text": "F-OAL: Forward-only Online Analytic Learning with Fast Training and Low Memory Footprint in Class Incremental Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Huiping Zhuang1,\u2217 Yuchen Liu2,\u2217 Run He1, Kai Tong1, Ziqian Zeng1, Cen Chen1,4,\u2020 Yi Wang3, Lap-Pui Chau3 ", "page_idx": 0}, {"type": "text", "text": "1South China University of Technology, China 2The University of Hong Kong, Hong Kong SAR 3The Hong Kong Polytechnic University, Hong Kong SAR 4Pazhou Lab, Guangzhou, China {hpzhuang,runhe,kaitong,zqzeng,cenchen}@scut.edu.cn, liuyuchen@connect.hku.hk, {yi-eie.wang, lap-pui.chau}@polyu.edu.hk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Online Class Incremental Learning (OCIL) aims to train models incrementally, where data arrive in mini-batches, and previous data are not accessible. A major challenge in OCIL is Catastrophic Forgetting, i.e., the loss of previously learned knowledge. Among existing baselines, replay-based methods show competitive results but requires extra memory for storing exemplars, while exemplar-free (i.e., data need not be stored for replay in production) methods are resourcefriendly but often lack accuracy. In this paper, we propose an exemplar-free approach\u2014Forward-only Online Analytic Learning (F-OAL). Unlike traditional methods, F-OAL does not rely on back-propagation and is forward-only, significantly reducing memory usage and computational time. Cooperating with a pre-trained frozen encoder with Feature Fusion, F-OAL only needs to update a linear classifier by recursive least square. This approach simultaneously achieves high accuracy and low resource consumption. Extensive experiments on benchmark datasets demonstrate F-OAL\u2019s robust performance in OCIL scenarios. Code is available at: https://github.com/liuyuchen-cz/F-OAL ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Class Incremental Learning (CIL) updates the model incrementally in a task-by-task manner with new classes in the new task. Traditional CIL most plans for static offilne datasets which historical data are accessible. However, with the rapid increase of social media and mobile devices, massive amount of image data have been produced online in a streaming fashion, and render training models on static data less applicable. To address this, Online Class Incremental Learning (OCIL) is developed taking an online constraint in addition to the existing CIL setting. OCIL is a more challenging CIL setting in which data come in mini-batches, and the model is trained only in one epoch (i.e., learning from one pass data stream) [14]. The model is required to achieve high accuracy, fast training time, and low resource consumption [25]. ", "page_idx": 0}, {"type": "text", "text": "However, CIL techniques (including OCIL) suffer from Catastrophic Forgetting (CF) [27], also known as the erosion of previous knowledge when new data are introduced. The problem becomes more severe in online scenarios since the model can only see data once. Two major factors contribute to CF: (1) Using the loss function to update the whole network leads to uncompleted feature capturing and diminished global representation [11]. (2) Using back-propagation to adjust linear classifier results in recency bias, which is a significantly imbalanced weight distribution, showing preference only on current learning data [15]. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To address CF in an online setting, replay-based methods [9, 21] are the mainstream solution by preserving old exemplars and revisiting them in new tasks. This strategy has strong performance but is resource consuming, while exemplar-free methods [17, 20] have lower resource consumption but show less competitive results. ", "page_idx": 1}, {"type": "text", "text": "Recently, Analytic Continual Learning (ACL) [49] methods emerged as an exemplar-free branch, delivering encouraging outcomes. ACL methods pinpoint the iterative back-propagation as the main factor behind catastrophic forgetting and seek to address it through linear recursive strategies. Remarkably, for the first time, these methods achieve outcomes comparable to those utilizing replaybased techniques. ", "page_idx": 1}, {"type": "text", "text": "There are two limitations in existing ACL methods: (1) Multiple iterations of base training are needed when the model is applied. Subsequently, the acquired knowledge is encoded into a regularized feature autocorrelation matrix by analytic re-alignment. The incremental learning phase then unfolds, utilizing the recursive least squares method for updates. This pattern is repeated when the dataset is switched, significantly elevating the temporal cost in an online scenario. (2) Classic ACL methods demand data aggregation from a single task, facilitating analytic learning in one fell swoop. This process increases GPU memory footprint and is unsuitable for online contexts where data for each task is presented as mini-batches. ", "page_idx": 1}, {"type": "text", "text": "To address those limitations, we propose Forward-only Online Analytic Learning (F-OAL) that learns online batch-wise data streams. The F-OAL consists of a frozen pre-trained encoder and an Analytic Classifier (AC). The frozen encoder is capable of countering the uncompleted feature representation caused by using the loss function to update and replace the time-consuming base training. With Feature Fusion and Smooth Projection, the encoder provide informative representation for analytic learning. The AC is updated by recursive least square rather than back-propagation to solve recency bias and decrease calculation. Therefore, F-OAL is an exemplar-free countermeasure to CF and reduces resource consumption since the encoder is frozen and only the AC is updated. ", "page_idx": 1}, {"type": "text", "text": "Our main contributions can be concluded as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We present the F-OAL, an exemplar-free technique that achieves high accuracy and low resource consumption together for the OCIL.   \n\u2022 F-OAL redefines the OCIL problem into a recursive least square manner and is updated in a mini-batch manner.   \n\u2022 F-OAL introduces a framework of frozen pre-trained encoder with Feature Fusion to generate representative features and Smooth Projection for recursively updating AC to counter CF.   \n\u2022 We conduct massive experiments on benchmark datasets with other OCIL baselines. The results demonstrate that F-OAL achieves competitive results with fasting training speed and low GPU footprint. ", "page_idx": 1}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Online Class Incremental Learning focuses on extracting knowledge from one pass data stream with new classes in the new task. Time and memory consumption requirements are particularly critical in OCIL, given the fast and large nature of online data streams [13]. ", "page_idx": 1}, {"type": "text", "text": "Replay-based methods [33, 25, 21, 9, 31, 12, 2, 11, 15, 41, 37, 36, 1, 38, 19, 6, 39, 35, 9] are mainstream solutions for OCIL problems by preserving historical exemplars and using them in new tasks. The accuracy is better than exemplar-free methods\u2019, but the training time and memory consumption are higher. ", "page_idx": 1}, {"type": "text", "text": "Analytic Learning (AL), also referred to as pseudo-inverse learning [10], emerges as a solution to the pitfalls of back-propagation by recursive least square. Analytic Learning\u2019s computational intensity is demanding since the entire dataset is processed. The obstacle is solved by the block-wise recursive ", "page_idx": 1}, {"type": "image", "img_path": "rGEDFS3emy/tmp/c84fc5cf2c1049e783b56e720d8372259c2018adaf68a8036e83d0ba4471bdd7.jpg", "img_caption": [], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 1: This diagram illustrates the learning agenda of F-OAL. In the encoder , features from each block of the ViT are extracted, summed, and averaged to form a composite feature. This feature is then randomly projected into a higher-dimensional space and normalized using the sigmoid function, serving as the activation for updating the classifier. All parameters in the encoder remain frozen. In the analytic classifier section, we introduce $\\pmb R$ to retain historical information and update the linear classifier using recursive least squares. This process is forward-only with no gradients. ", "page_idx": 2}, {"type": "text", "text": "Moore-Penrose algorithm [47], which achieves equivalent precision with joint-learning (i.e., training with all data). AL has already been used in online reinforcement learning [22], which shows the potential of AL in OCIL. ", "page_idx": 2}, {"type": "text", "text": "Exemplar-free methods can be categorized into regularization-based, prototype-based, and recently emerged ACL methods. Regularization-based methods [20, 17, 42, 8, 44] apply constraints on the loss function or change the gradients to preserve knowledge. These solutions reduce resource consumption but commonly can not outperform replay-based methods. Prototype-based methods [43, 45, 29, 40, 32, 7, 28] mitigate CF by augmenting the prototypes of features to classify old classes or generating psudo-features of old classes from new representations. ACL methods represent a new branch of the CIL community and show great potential in the OCIL scenario. Analytic Class Incremental Learning (ACIL) [49] is the first approach that applies AL to the CIL problem. The ACIL achieves a competitive performance in the offilne CIL scenario. Gaussian Kernel Embedded Analytic Learning (GKEAL) [48], following the ACIL, focuses on solving CF in the few-shot scenario by adopting the kernel method. DS-AL [46] overcomes the under-fitting problem of AL. ", "page_idx": 2}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Proposed Framework ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Our F-OAL framework consists of two modules: ", "page_idx": 2}, {"type": "text", "text": "Encoder. Encoder has two parts: backbone with Feature Fusion and Smooth Projection. The performance of analytic learning is highly dependent on the quality of the extracted features. Therefore, we employ Vision Transformer (ViT) [5] as the backbone instead of CNN, because ViT generates more comprehensive feature representation [30]. To further enhance the representativeness of the features, we utilize Feature Fusion. Specifically, we extract the outputs from each block of the ViT, sum them, and take their average to form the image feature. ", "page_idx": 2}, {"type": "text", "text": "Subsequently, we expand this feature into a higher-dimensional space using random projection and apply the sigmoid function for smoothing (i.e., Smooth Projection). This results in the final activation used to update the classifier. Therefore, the encoder $\\phi(\\cdot)$ is defined as: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\phi(x)=\\left(L P\\left({\\frac{\\overbrace{B_{1}(x)+B_{1}(B_{2}(x))+\\cdot\\cdot\\cdot+B_{n}(B_{n-1}(\\cdot\\cdot\\cdot B_{1}(x)))}^{\\mathrm{Feaure\\;Fusion}}}{n}}\\right)\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $B_{1}(\\cdot)\\cdot\\cdot\\cdot B_{n}(\\cdot)$ are blocks in ViT, $L P(\\cdot)$ is linear projection and $\\sigma(\\cdot)$ is sigmoid activation function. ", "page_idx": 3}, {"type": "text", "text": "Backbone is pre-trained and frozen and weight matrix of projection is sampled from normal distribution and is frozen. Freezing the encoder avoids selective learning caused by loss function, which prioritizes features that are easiest to learn rather than the most representative [11] and significantly reduces the number of trainable parameters. ", "page_idx": 3}, {"type": "text", "text": "Analytic Classifier. Unlike back-propagation, we employ the least squares method to obtain the analytic solution for the linear mapping from the activation to the one-hot label. We then recursively update the weight matrix of the linear mapping. This approach is forward-only and does not require the use of gradients, resulting in low memory usage and fast computation speed. ", "page_idx": 3}, {"type": "table", "img_path": "rGEDFS3emy/tmp/05655ef9f7e27ca8f23c67209f01f1c9322f83c98a5bd028b6d928c7a5026daa.jpg", "table_caption": ["3.2 Analytic Solution "], "table_footnote": ["Table 1: Description of notations and their dimensions. Here, $\\overline{V}$ is the number of all images, $\\overline{{D}}$ is the encoder output dimension, $M$ is the number of all classes, $S$ is the batch size, $C_{s}$ is the number of classes seen so far, and $V_{s}$ is the number of images seen so far. "], "page_idx": 3}, {"type": "text", "text": "The overview is shown in Figure 1. To derive our solution, the notations needed are listed in Table 1. Unlike other methods that treat model training as a back-propagation process, our method formulates the problem using linear regression, which allows for direct computation of the optimal parameters in a closed-form solution: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\pmb{Y}=\\phi(\\pmb{X})\\pmb{W},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\phi(\\cdot)$ is the pre-trained and forzen encoder and $W$ is the learnable parameter of linear classifier. The learning problem can be rewritten into an optimization form: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{argmin}_{\\pmb{W}}\\ \\|\\pmb{Y}\\ -\\phi(\\pmb{X})\\pmb{W}\\|_{\\mathrm{F}}^{2}+\\gamma\\,\\|\\pmb{W}\\|_{\\mathrm{F}}^{2}\\,,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\|\\cdot\\|_{\\mathrm{F}}$ represents the Frobenius norm and $\\gamma$ is the regularization term. The optimal solution is defined as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{\\pmb{W}}=\\left(\\phi(\\pmb{X})^{\\mathrm{T}}\\phi(\\pmb{X})+\\gamma\\pmb{I}\\right)^{-1}\\phi(\\pmb{X})^{\\mathrm{T}}\\pmb{Y}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "3.3 Learning Agenda ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Given dataset $\\{X_{k}^{\\mathrm{train}},Y_{k}^{\\mathrm{train}}\\}_{1}^{m}$ btrea itnhe trtariainning set of task $k\\,(k=1,2,\\ldots,m)$ . Every task is divided into mini-batches. We denote $\\{X_{k,i}^{\\mathrm{train}},\\,Y_{k,i}^{\\mathrm{train}}\\}$ , as the mini-batch $(i{=}1,\\,2,\\,...,n)$ of training set of task At task 1 batch $k$ , the model aims to optimize ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{argmin}_{W^{(1,i)}}\\;\\left\\|Y_{1,1:i}^{\\mathrm{train}}-(X_{1,1:i}^{(a)})W^{(1,i)}\\right\\|_{\\mathrm{F}}^{2}+\\gamma\\left\\|W^{(1,i)}\\right\\|_{\\mathrm{F}}^{2},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where ", "page_idx": 4}, {"type": "equation", "text": "$$\nX_{1,1:i}^{(a)}=\\phi(X_{1,1:i}^{\\mathrm{train}}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The optimal solution to parameter $\\pmb{W}^{(1,i)}$ is given as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\pmb{W}}^{(1,i)}=\\left((\\pmb{X}_{1,1:i}^{(a)})^{\\mathrm{T}}\\pmb{X}_{1,1:i}^{(a)}+\\gamma\\pmb{I}\\right)^{-1}(\\pmb{X}_{1,1:i}^{(a)})^{\\mathrm{T}}\\pmb{Y}_{1,1:i}^{\\mathrm{train}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Regarding $\\pmb{X}_{1,1:i}^{(a)}$ and $Y_{1,1:i}^{\\mathrm{train}}$ as stacks of row activation vectors and one-hot label vectors respectively, vweec toobrss.e rNvoe ttichea tt $(X_{1,1:i}^{(a)})^{\\mathrm{T}}(X_{1,1:i}^{(a)})$ oanntadi $(X_{1,1:i}^{(a)})^{\\mathrm{T}}Y_{1,1:i}^{\\mathrm{train}}$ ealartei obno.t hT hsuusm sw eo fc aonu tfeurr tphreor dduecrtisv ew .Er.qt ufaetiatounr e7 into ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\boldsymbol{W}}^{1,i}=\\left(\\left[(\\boldsymbol{X}_{1,1:i-1}^{(a)})^{\\mathrm{T}}\\boldsymbol{\\mathrm{\\Lambda}}\\right.\\!\\!(\\boldsymbol{X}_{1,i}^{(a)})^{\\mathrm{T}}\\right]\\left[\\boldsymbol{X}_{1,1:i}^{(a)}\\!\\!\\!+\\!\\!1\\right]+\\gamma{\\boldsymbol{I}}\\right)^{-1}\\left[(\\boldsymbol{X}_{1,1:i-1}^{(a)})^{\\mathrm{T}}\\boldsymbol{\\mathrm{\\Lambda}}\\right.\\!\\!(\\boldsymbol{X}_{1,i}^{(a)})^{\\mathrm{T}}\\right]\\left[\\boldsymbol{\\mathrm{I}}_{1,1:i-1}^{\\mathrm{ruin}}\\!\\!\\!(\\boldsymbol{\\mathrm{I}}_{1,i}^{\\mathrm{0}})\\!\\!\\!}\\\\ &{\\qquad=\\left((\\boldsymbol{X}_{1,1:i-1}^{(a)})^{\\mathrm{T}}(\\boldsymbol{X}_{1,1:i-1}^{(a)})+(\\boldsymbol{X}_{1,i}^{(a)})^{\\mathrm{T}}(\\boldsymbol{X}_{1,i}^{(a)})+\\gamma{\\boldsymbol{I}}\\right)^{-1}\\left[(\\boldsymbol{X}_{1,1:i-1}^{(a)})^{\\mathrm{T}}\\boldsymbol{\\mathrm{I}}_{1,1:i-1}^{\\mathrm{ruin}}+(\\boldsymbol{X}_{1,i}^{(a)})^{\\mathrm{T}}\\boldsymbol{\\mathrm{I}}_{1,i}^{\\mathrm{ruin}}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Let ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\pmb{R}_{1,i-1}=\\left[(\\pmb{X}_{1,1:i-1}^{(a)})^{\\operatorname{T}}(\\pmb{X}_{1,1:i-1}^{(a)})+\\gamma\\pmb{I}\\right]^{-1}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "be the regularized feature autocorrelation matrix at batch $i{-}1$ of task 1, where both historical and current information is encoded in. ", "page_idx": 4}, {"type": "text", "text": "Therefore, we can calculate both $\\pmb R$ and $W$ in a recursive manner by the following theorems when the serial numbers of tasks and batches are given: ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.1 For the batch 1 of task k, Let W\u02c6(0) be the null matrix. Let \u02c6W(k\u22121,n) $\\hat{\\boldsymbol{W}}^{(k-1,n)^{\\prime}}=\\left[\\hat{\\boldsymbol{W}}^{(k-1,n)}~~~\\mathbf{0}\\right]$ and W\u02c6(k,1) can be calculated via: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{\\hat{\\boldsymbol W}}^{(k,1)}={\\hat{\\boldsymbol W}}^{(k-1,n)^{\\prime}}+R_{k,1}X_{k,1}^{\\mathrm{(a)T}}\\left(Y_{k,1}^{\\mathrm{train}}-X_{k,1}^{\\mathrm{(a)}}{\\hat{\\boldsymbol W}}^{(k-1,n)^{\\prime}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\pmb{R}_{k,1}=\\pmb{R}_{k-1,n}-\\pmb{R}_{k-1,n}\\pmb{X}_{k,1}^{\\mathrm{(a)T}}\\left(\\pmb{I}+\\pmb{X}_{k,1}^{\\mathrm{(a)}}\\pmb{R}_{k-1,n}\\pmb{X}_{k,1}^{\\mathrm{(a)T}}\\right)^{-1}\\pmb{X}_{k,1}^{\\mathrm{(a)}}\\pmb{R}_{k-1,n}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Theorem 3.2 For the batch i ( i > 1 ) of task k, W\u02c6(k,i) can be calculated via: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\boldsymbol W}^{(k,i)}=\\hat{\\boldsymbol W}^{(k,i-1)}+R_{k,i}\\boldsymbol X_{k,i}^{\\mathrm{(a)T}}\\left(\\boldsymbol Y_{k,i}^{\\mathrm{train}}-\\boldsymbol X_{k,i}^{(a)}\\hat{\\boldsymbol W}^{(k,i-1)}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\pmb{R}_{k,i}=\\pmb{R}_{k,i-1}-\\pmb{R}_{k,i-1}\\pmb{X}_{k,i}^{\\mathrm{(a)T}}\\left(\\pmb{I}+\\pmb{X}_{k,i}^{\\mathrm{(a)}}\\pmb{R}_{k,i-1}\\pmb{X}_{k,i}^{\\mathrm{(a)T}}\\right)^{-1}\\pmb{X}_{k,i}^{\\mathrm{(a)}}\\pmb{R}_{k,i-1}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Proof. See Appendix A. ", "page_idx": 4}, {"type": "text", "text": "Thus, we achieve absolute memorization in an exemplar-free way with all data used only once. The learning agenda of F-OAL is summarised in Algorithm 1. ", "page_idx": 4}, {"type": "text", "text": "Input: Mini-batches $\\{X_{k,i}^{\\mathrm{train}},Y_{k,i}^{\\mathrm{train}}\\}_{1,1}^{m,n}$ .   \nInitialization: Identity matrix $\\pmb{R}_{0}$ ; Null matrix $\\pmb{W}^{(0)}$ .   \nfor $k=1$ to $m$ do for $i=1$ to $n$ do if $i=1$ then # Theorem 3.1: Load $\\hat{\\boldsymbol{W}}^{(k-1,n)}$ and $\\pmb{R}_{k-1,n}$ ; Expand W\u02c6(k\u22121,n) to $\\hat{\\pmb{W}}^{(k-1,n)^{\\prime}}$ Obtain activation $X_{k,1}^{\\mathrm{(a)}}$ based on Xtkr,ai1n ; Update $\\pmb{R}_{k,1}$ by $X_{k,1}^{\\mathrm{(a)}}$ and $\\pmb{R}_{k-1,n}$ ; Update $\\boldsymbol{W}^{(k,1)}$ by $X_{k,1}^{\\mathrm{(a)}};Y_{k,1}^{\\mathrm{train}}$ , $W^{(k-1,n)^{\\prime}}$ and $\\pmb{R}_{k}$ ; else # Theorem 3.2: Load $W^{(k,i-1)}$ and $R_{k,i-1}$ ; OUbptdaaitne $X_{k,i}^{\\mathrm{(a)}}$ $X_{k,i}^{\\mathrm{train}}$ $\\pmb{R}_{k,i}$ $X_{k,i}^{\\mathrm{(a)}}$ $R_{k,i-1}$ Update $\\pmb{W}^{(k,i)}$ by $X_{k,i}^{\\mathrm{(a)}}$ ; $Y_{k,i}^{\\mathrm{train}}$ , $\\boldsymbol{W}^{(k,i-1)}$ and $R_{k,i-1}$ end if end for   \nend for ", "page_idx": 5}, {"type": "text", "text": "3.4 Complexity Analysis ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In terms of space complexity, our trainable parameters are only $\\pmb R$ and $W.$ . The $\\pmb R$ matrix is of size $D\\times D$ , where $D$ is the output dimension of the encoder. In our paper, the encoder output dimension is 1000. Therefore, according to Equation 9, the size of the $\\pmb R$ matrix is $1,000\\times1,000$ . The $W$ matrix has dimensions of $C\\times D$ , where $C$ is the number of classes in the target dataset. For example, with CIFAR-100, its size is $100\\times1,000$ . The total number of trainable parameters is relatively small and does not require gradients. This results in our method using less than 2GB of GPU memory. ", "page_idx": 5}, {"type": "text", "text": "In terms of computational complexity, we denote the batch size as $S$ , encoder\u2019s output size as $D$ , and class number as $C$ . Therefore, the dimensions of $X$ , Y, $\\pmb R$ and $W$ are $S\\times D$ , $S\\times C$ , $D\\times D$ , and $C\\times D$ , respectively. Thus, the calculation is shown below: ", "page_idx": 5}, {"type": "text", "text": "The computational complexity for updating $R$ is dominated by the matrix multiplications, thus: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\{\\mathcal{O}(S D C),\\mathcal{O}(S C),\\mathcal{O}(S D C)\\}\\approx\\operatorname*{max}\\{\\mathcal{O}(S D C),\\mathcal{O}(D^{2}C)\\}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The computational complexity for updating $W$ is dominated by the matrix multiplications and the matrix inversion: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\{\\mathcal{O}(S D),\\mathcal{O}(S D^{2}),\\mathcal{O}(S^{2}),\\mathcal{O}(S^{3}),\\mathcal{O}(D S^{2}),\\mathcal{O}(D^{2}S),\\mathcal{O}(D^{2})\\}\\approx\\operatorname*{max}\\{\\mathcal{O}(S^{3}),\\mathcal{O}(D^{2}S)\\}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In the OCIL setting, the batch size is relatively smaller. Therefore, the overall computational complexity is primarily controlled by $D$ . ", "page_idx": 5}, {"type": "text", "text": "3.5 Overhead Analysis ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In terms of space overhead, compared to the conventional backbone $^+$ classifier structure, F-OAL introduces an additional linear projection to control the output dimension $D$ of the encoder, and a matrix $\\pmb R$ , where only $\\pmb R$ is trainable. According to Equation 9, the dimension of $\\pmb R$ remains a fixed size of $D\\times D$ . Other methods require more extra space. For instance, LwF [20] employs knowledge distillation, necessitating the storage of additional models, while replay-based methods require extra storage to retain historical samples. In contrast, the overhead introduced by F-OAL, consisting of an additional matrix and a linear layer, is smaller. ", "page_idx": 5}, {"type": "table", "img_path": "rGEDFS3emy/tmp/25ff23694038f7058ad38dd5feada382c3c53773d046063df5b0e621257c12a0.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Table 2: This table shows the comparison results of our method with other baselines on six datasets. We select three metrics: average accuracy $(A_{a v g})$ , last task accuracy $(A_{l a s t})$ , and forgetting rate $(F)$ . Higher values for $A_{a v g}$ and $A_{l a s t}$ indicate better performance, while lower values for $F$ indicate better performance. In Replay? column, replay-based methods are marked with $\\checkmark$ . Conversely, exemplar-free methods are marked with $\\pmb{x}$ . Data in Bold are the best within exemplar-free methods, and data underlined are the best considering both categories. ", "page_idx": 6}, {"type": "text", "text": "In terms of time overhead, our method primarily consists of a forward pass and matrix multiplication, which is determined by the output dimension of the encoder. By changing the output dimension of the encoder, we can balance the accuracy and time. According to [16], the backward pass in back-propagation (forward pass $^+$ backward pass) accounts for $70\\%$ of the time. Therefore, our method\u2019s time overhead is also relatively small. ", "page_idx": 6}, {"type": "text", "text": "4 Experiment ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To show the effectiveness of F-OAL, we conduct extensive experiments to compare our approach with baseline methods. We build our code and reproduce relevant results based on [24, 34]. All baselines are with pre-trained ViT as the backbone. ", "page_idx": 6}, {"type": "text", "text": "4.1 Datasets ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We focus on coarse-grained data scenarios, such as CIFAR-100, Tiny-ImageNet, and Core50, as well as fine-grained data scenarios, including DTD, FGVCAircraft, and Country211. All of these are open-source benchmark datasets. However, there are other data scenarios, such as long-tail distributions, where unbalanced data distribution will make it harder to achieve good performance by only training the classifier. We will study this case in future work. ", "page_idx": 6}, {"type": "text", "text": "\u2022 CIFAR-100 [18] is constructed into 10 tasks with disjoint classes, and each task has 10 classes.   \nEach task has 5,000 images for training and 1,000 for testing. ", "page_idx": 6}, {"type": "text", "text": "\u2022 CORe50 [23] is a benchmark designed for class incremental learning with 9 tasks and 50 classes: 10 classes in the first task and 5 classes in the subsequent 8 tasks. Each class has 2,398 training images and 900 testing images on average. ", "page_idx": 7}, {"type": "text", "text": "\u2022 FGVCAircraft [26] contains 102 different classes of aircraft models. 100 classes are selected and divided into 10 tasks. Each class has 33 training images and 33 testing images on average. ", "page_idx": 7}, {"type": "text", "text": "\u2022 DTD [3] is a texture database, organized into 47 classes. 40 classes are selected and divided into 10 tasks. Each class has 40 training images and 40 testing images. ", "page_idx": 7}, {"type": "text", "text": "\u2022 Tiny-ImageNet is a subset of ImageNet with 200 classes for training. Each class has 500 images.   \nThe test set contains 10,000 images. The dataset is evenly divided into 10 tasks. ", "page_idx": 7}, {"type": "text", "text": "\u2022 Country211 contains 211 classes of country images, with 150 train and test images per class. The dataset is evenly divided into 10 tasks with 210 classes chosen. ", "page_idx": 7}, {"type": "text", "text": "4.2 Evaluation Metrics ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We define $a_{i,j}$ as the accuracy evaluated on the test set of task $j$ after training the network from task 1 through to $i$ , and the average accuracy is defined as ", "page_idx": 7}, {"type": "equation", "text": "$$\nA_{i}=\\frac{1}{i}\\sum_{j=1}^{i}a_{i,j}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "When $i\\,=\\,m$ (i.e., the total number of tasks), $A_{m}$ represents the average accuracy by the end of training. ", "page_idx": 7}, {"type": "text", "text": "Forgetting rate at task $i$ is defined as Equation 17. $f_{i,j}$ represents how much the model has forgot about task $j$ after being trained on task $i$ . Specifically, $\\operatorname*{max}_{l\\in\\{1,...,k-1\\}}(a_{l,j})$ denotes the best test accuracy the model has ever achieved on task $j$ before learning task $k$ , and $a_{k,j}$ is the test accuracy on task $j$ after learning task $k$ . ", "page_idx": 7}, {"type": "equation", "text": "$$\nF_{i}=\\frac{1}{i-1}\\sum_{j=1}^{i-1}f_{i,j},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where ", "page_idx": 7}, {"type": "equation", "text": "$$\nf_{k,j}=\\operatorname*{max}_{l\\in\\{1,\\ldots,k-1\\}}(a_{l,j})-a_{k,j},\\quad\\forall j<k.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "4.3 Implementation Details ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "ViT-B [5], pre-trained on ImageNet-1K [4], serves as the backbone network for all methods. The data stream is kept identical across all experiments to ensure a fair comparison. The learning rate and batch size are set to 0.001 and 10, respectively. The optimizer is SGD. We assign 5,000 memory buffer sizes for replay-based methods. In F-OAL, the expansion size is 1,000 (i.e., the output activation size to update AC is 1,000). The regularization term is set to be 1. All experiments are conducted on a single RTX 4070ti GPU 12GB, and an average of 3 runs is reported. ", "page_idx": 7}, {"type": "text", "text": "4.4 Result Comparison ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We tabulate the average accuracy $(A_{a v g})$ , last task accuracy $(A_{l a s t})$ , and the forgetting rate $(F)$ from the compared methods in Table 2. ", "page_idx": 7}, {"type": "text", "text": "For fine-grained datasets such as FGCVAircraft, DTD, and Country211, F-OAL achieves the best performance, with average accuracies of $66.2\\%$ , $82.8\\%$ , and $24.4\\%$ , respectively. The second-best results are $55.6\\%$ , $76.0\\%$ , and $17.8\\%$ , respectively. Similarly, F-OAL also outperforms in last task accuracy. This demonstrates the excellent transferability of our method in the OCIL setting, effectively leveraging the feature extraction capabilities of the pre-trained encoder. ", "page_idx": 7}, {"type": "text", "text": "On coarse-grained datasets such as CIFAR-100, CORe50, and Tiny-ImageNet, F-OAL still demonstrates excellent performance, achieving the highest accuracy among all exemplar-free methods, except on Tiny-ImageNet where it is $0.8\\%$ behind EASE in average accuracy. Compared to the best replay-based methods, it only lags by $1.3\\%$ , $0.8\\%$ , and $0.3\\%$ , respectively. ", "page_idx": 7}, {"type": "table", "img_path": "rGEDFS3emy/tmp/a8acd448247acc51ce95326ded02b78f760f4b61c6ae0b53a605009b7a1d2acd.jpg", "table_caption": [], "table_footnote": ["Table 3: Training time including feature extraction is reported in seconds where replay-based methods are with 5,000 buffer size. Data in Bold show the fastest time. "], "page_idx": 8}, {"type": "text", "text": "Typically, a lower forgetting rate is better, but forgetting is based on accuracy. Therefore, when comparing forgetting rates, it is important to consider models with similar accuracy levels. When comparing F-OAL to the well-performing DVC, F-OAL exhibits a lower forgetting rate. On CIFAR100, F-OAL\u2019s forgetting rate is $5.5\\%$ , while DVC\u2019s is $8.2\\%$ . This indicates that F-OAL not only maintains a high level of accuracy but also effectively manages forgetting. ", "page_idx": 8}, {"type": "image", "img_path": "rGEDFS3emy/tmp/099bfe562276917a2749390368b02f87287f9ecc9b4d564bdacd87e26c9be2c6.jpg", "img_caption": ["Figure 2: Peak GPU memory footprint in GB with 10 batch size on CIFAR-100. Replay-based methods are with 5,000 buffer size. F-OAL has low GPU footprint since it does not require gradients. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "rGEDFS3emy/tmp/53fd0486a801ea532ae223b33373659481b6e2d33ae8cba045b67a58d60d4928.jpg", "img_caption": ["Figure 3: Visualization of the weights of a linear classifier. The result comes from F-OAL on DTD. Based on [15], when recency bias happens, L2 norm of current task is significantly larger. In F-OAL, the L2 norm of current task is in a average level. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.5 Resource Consumption ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "GPU. Peak GPU memory footprint on CIFAR-100 is shown in Figure 2. As we state in Section 3.5, without using gradient and extra space for exemplars, F-OAL requires a low memory footprint while having good performance (i.e., higher accuracy than LAE and EASE on most datasets). ", "page_idx": 8}, {"type": "text", "text": "Training Time. Table 3 illustrates training time including feature extraction. F-OAL is fast with competitive accuracy (i.e., higher accuracy than LAE). Only the classifier and regularized feature autocorrelation matrix are updated, leading to fewer of trainable parameters and fast training speed. ", "page_idx": 8}, {"type": "text", "text": "4.6 Countering Recency Bias ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "As demonstrated in Figure 3, the linear classifier of AC obtained through F-OAL training does not exhibit recency bias. Notably, the weights corresponding to the classes in the most recent tasks do not significantly surpass those of the earlier classes. The frozen encoder and the recursive least square updating manner ensure equal treatment for all samples. ", "page_idx": 8}, {"type": "text", "text": "4.7 Ablation Study ", "text_level": 1, "page_idx": 9}, {"type": "table", "img_path": "rGEDFS3emy/tmp/02741c77c85e7277bf6d19e4489e39be324ec643adb4418c254f5e2068246b53.jpg", "table_caption": ["We design the ablation study in Table 4 to verify the effectiveness of the Feature Fusion and Smooth Projection modules. Without these two components, the accuracy of F-OAL drops, especially on fine-grained datasets. "], "table_footnote": ["Table 4: Average accuracy comparison with Feature Fusion (FF) and Smooth Projection (SP) modules across various datasets. $\\checkmark$ means with and $\\pmb{x}$ means without. "], "page_idx": 9}, {"type": "text", "text": "As Table 5 (See appendix B) shows, we prove analytic classifier is key to high accuracy. Herein, we define the Fully Connected Classifier (FCC) as having the identical structure to the AC, but it is updated through back-propagation rather than utilizing the $\\pmb R$ and recursive least square. Without AC, the accuracy drops from $91.1\\%$ to $32.4\\%$ on CIFAR-100. ", "page_idx": 9}, {"type": "text", "text": "Regularization Term. Table 6 (See appendix B) shows the effects of varying $\\gamma$ . For large volume dataset, F-OAL gives a robust performance in wide range of $\\gamma$ value (e.g., $10^{\\bar{2}}-10^{-3})$ , while small datasets need larger value (e.g., $10^{2}-1\\AA$ ). The comprehensive results show that $\\gamma{=}1$ is the suitable choice. ", "page_idx": 9}, {"type": "text", "text": "Projection Size. Figure 4 (See appendix B) demonstrates the influence of different random projection sizes. The results suggest that the setting of a 1,000-dimensional projection is appropriate. ", "page_idx": 9}, {"type": "text", "text": "4.8 Potential Positive and Negative Societal Impacts ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The key advantage of our approach is its ability to achieve exemplar-free OCIL with fast training and low memory footprint, offering an environmentally friendly and efficient solution for this research track. However, the main limitation of our method lies in its reliance on a powerful pre-trained encoder. As a result, it is crucial for us to leverage open-source pre-trained backbone networks from the deep learning community, rather than training our own, which will otherwise lead to higher GPU usage and increased resource consumption. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we propose Forward-only Online Analytic Learning (F-OAL), an exemplar-free method for addressing several limitations of the Online Class Incremental Learning scenario. We use Analytic Learning to acquire the optimal solution of the classifier directly instead of training for dozens of epochs by back-propagation. Leveraging the frozen pre-trained encoder with Feature Fusion and Smooth Projection and the Analytic Classifier updated by recursive least square, our approach achieves the identical solution to joint-learning on the whole dataset without preserving any historical exemplars, achieving high accuracy and reducing the resource consumption. Our experiments show the competitive performance of F-OAL. ", "page_idx": 9}, {"type": "text", "text": "6 Acknowledgment ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This research was supported by the National Natural Science Foundation of China (62306117), the Guangzhou Basic and Applied Basic Research Foundation (2024A04J3681, 2023A04J1687), the South China University of Technology-TCL Technology Innovation Fund, the Fundamental Research Funds for the Central Universities (2023ZYGXZR023, 2024ZYGXZR074), the Guangdong Basic and Applied Basic Research Foundation (2024A1515010220), and the CAAI- MindSpore Open Fund developed on Openl Community. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Rahaf Aljundi, Eugene Belilovsky, Tinne Tuytelaars, Laurent Charlin, Massimo Caccia, Min Lin, and Lucas Page-Caccia. Online continual learning with maximal interfered retrieval. In Advances in Neural Information Processing Systems, pages 11849\u201311860, 2019.   \n[2] Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Bengio. Gradient based sample selection for online continual learning. In Advances in Neural Information Processing Systems, pages 11816\u201311825, 2019. [3] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3606\u20133613, 2014.   \n[4] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pages 248\u2013255, 2009. [5] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2020.   \n[6] Arthur Douillard, Matthieu Cord, Charles Ollion, Thomas Robert, and Eduardo Valle. Podnet: Pooled outputs distillation for small-tasks incremental learning. In Computer vision\u2013ECCV 2020: 16th European conference, Glasgow, UK, August 23\u201328, 2020, proceedings, part XX 16, pages 86\u2013102, 2020.   \n[7] Qiankun Gao, Chen Zhao, Bernard Ghanem, and Jian Zhang. R-dfcil: Relation-guided representation learning for data-free class incremental learning. In European Conference on Computer Vision, pages 423\u2013439, 2022.   \n[8] Qiankun Gao, Chen Zhao, Yifan Sun, Teng Xi, Gang Zhang, Bernard Ghanem, and Jian Zhang. A unified continual learning framework with general parameter-efficient tuning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11483\u201311493, 2023.   \n[9] Yanan Gu, Xu Yang, Kun Wei, and Cheng Deng. Not just selection, but exploration: Online classincremental continual learning via dual view consistency. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7442\u20137451, 2022.   \n[10] Ping Guo, Michael R Lyu, and NE Mastorakis. Pseudoinverse learning algorithm for feedforward neural networks. In Advances in Neural Networks and Applications, pages 321\u2013326, 2001.   \n[11] Yiduo Guo, Bing Liu, and Dongyan Zhao. Online continual learning through mutual information maximization. In International Conference on Machine Learning, pages 8109\u20138126. PMLR, 2022.   \n[12] Tyler L Hayes, Nathan D Cahill, and Christopher Kanan. Memory efficient experience replay for streaming learning. In 2019 International Conference on Robotics and Automation, pages 9769\u20139776, 2019.   \n[13] Jiangpeng He, Runyu Mao, Zeman Shao, and Fengqing Zhu. Incremental learning in online scenario. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13926\u2013 13935, 2020.   \n[14] Xu He and Herbert Jaeger. Overcoming catastrophic interference using conceptor-aided backpropagation. 2018.   \n[15] Saihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang, and Dahua Lin. Learning a unified classifier incrementally via rebalancing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 831\u2013839, 2019.   \n[16] Zhouyuan Huo, Bin Gu, Heng Huang, et al. Decoupled parallel backpropagation with convergence guarantee. In International Conference on Machine Learning, pages 2098\u20132106. PMLR, 2018.   \n[17] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. In Proceedings of the National Academy of Sciences, pages 3521\u20133526, 2017.   \n[18] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \n[19] Byung Hyun Lee, Okchul Jung, Jonghyun Choi, and Se Young Chun. Online continual learning on hierarchical label expansion. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11761\u201311770, 2023.   \n[20] Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE Transactions on Pattern Analysis and Machine Intelligence, pages 2935\u20132947, 2017.   \n[21] Huiwei Lin, Baoquan Zhang, Shanshan Feng, Xutao Li, and Yunming Ye. Pcr: Proxy-based contrastive replay for online class-incremental continual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 24246\u201324255, 2023.   \n[22] Zichen Liu, Chao Du, Wee Sun Lee, and Min Lin. Locality sensitive sparse encoding for learning world models online. In International Conference on Learning Representations, 2024.   \n[23] Vincenzo Lomonaco and Davide Maltoni. Core50: a new dataset and benchmark for continuous object recognition. In Conference on Robot Learning, pages 17\u201326, 2017.   \n[24] Zheda Mai, Ruiwen Li, Jihwan Jeong, David Quispe, Hyunwoo Kim, and Scott Sanner. Online continual learning in image classification: An empirical survey. Neurocomputing, pages 28\u201351, 2022.   \n[25] Zheda Mai, Ruiwen Li, Hyunwoo Kim, and Scott Sanner. Supervised contrastive replay: Revisiting the nearest class mean classifier in online class-incremental continual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3589\u20133599, 2021.   \n[26] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained visual classification of aircraft. arXiv preprint arXiv:1306.5151, 2013.   \n[27] Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The sequential learning problem. In Psychology of learning and motivation, volume 24, pages 109\u2013165. 1989.   \n[28] Zichong Meng, Jie Zhang, Changdi Yang, Zheng Zhan, Pu Zhao, and Yanzhi WAng. Diffclass: Diffusionbased class incremental learning. European Conference on Computer Vision, 2024.   \n[29] Gr\u00e9goire Petit, Adrian Popescu, Hugo Schindler, David Picard, and Bertrand Delezoide. Fetril: Feature translation for exemplar-free class-incremental learning. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 3911\u20133920, 2023.   \n[30] Maithra Raghu, Thomas Unterthiner, Simon Kornblith, Chiyuan Zhang, and Alexey Dosovitskiy. Do vision transformers see like convolutional neural networks? In Advances in Neural Information Processing Systems, pages 12116\u201312128, 2021.   \n[31] Sylvestre-Alvise Rebuff,i Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. icarl: Incremental classifier and representation learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2001\u20132010, 2017.   \n[32] Grzegorz Rype\u00b4s\u00b4c, Sebastian Cygert, Valeriya Khan, Tomasz Trzcinski, Bartosz Micha\u0142 Zieli\u00b4nski, and Bart\u0142omiej Twardowski. Divide and not forget: Ensemble of selectively trained experts in continual learning. In International Conference on Learning Representations, 2024.   \n[33] Dongsub Shim, Zheda Mai, Jihwan Jeong, Scott Sanner, Hyunwoo Kim, and Jongseong Jang. Online class-incremental continual learning with adversarial shapley value. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 9630\u20139638, 2021.   \n[34] Hai-Long Sun, Da-Wei Zhou, Han-Jia Ye, and De-Chuan Zhan. Pilot: A pre-trained model-based continual learning toolbox. arXiv preprint arXiv:2309.07117, 2023.   \n[35] Fu-Yun Wang, Da-Wei Zhou, Han-Jia Ye, and De-Chuan Zhan. Foster: Feature boosting and compression for class-incremental learning. In European conference on computer vision, pages 398\u2013414, 2022.   \n[36] Quanziang Wang, Renzhen Wang, Yichen Wu, Xixi Jia, and Deyu Meng. Cba: Improving online continual learning via continual bias adaptor. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 19082\u201319092, 2023.   \n[37] Zhen Wang, Liu Liu, Yajing Kong, Jiaxian Guo, and Dacheng Tao. Online continual learning with contrastive vision transformer. In European Conference on Computer Vision, pages 631\u2013650, 2022.   \n[38] Yujie Wei, Jiaxin Ye, Zhizhong Huang, Junping Zhang, and Hongming Shan. Online prototype learning for online continual learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 18764\u201318774, 2023.   \n[39] Shipeng Yan, Jiangwei Xie, and Xuming He. Der: Dynamically expandable representation for class incremental learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 3014\u20133023, 2021.   \n[40] Gengwei Zhang, Liyuan Wang, Guoliang Kang, Ling Chen, and Yunchao Wei. Slca: Slow learner with classifier alignment for continual learning on a pre-trained model. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 19148\u201319158, 2023.   \n[41] Yaqian Zhang, Bernhard Pfahringer, Eibe Frank, Albert Bifet, Nick Jin Sean Lim, and Yunzhe Jia. A simple but strong baseline for online continual learning: Repeated augmented rehearsal. In Advances in Neural Information Processing Systems, pages 14771\u201314783, 2022.   \n[42] Da-Wei Zhou, Hai-Long Sun, Han-Jia Ye, and De-Chuan Zhan. Expandable subspace ensemble for pre-trained model-based class-incremental learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 23554\u201323564, 2024.   \n[43] Fei Zhu, Xu-Yao Zhang, Chuang Wang, Fei Yin, and Cheng-Lin Liu. Prototype augmentation and selfsupervision for incremental learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5871\u20135880, 2021.   \n[44] Kai Zhu, Wei Zhai, Yang Cao, Jiebo Luo, and Zheng-Jun Zha. Self-sustaining representation expansion for non-exemplar class-incremental learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9296\u20139305, 2022.   \n[45] Kai Zhu, Wei Zhai, Yang Cao, Jiebo Luo, and Zheng-Jun Zha. Self-sustaining representation expansion for non-exemplar class-incremental learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9296\u20139305, 2022.   \n[46] Huiping Zhuang, Run He, Kai Tong, Ziqian Zeng, Cen Chen, and Zhiping Lin. Ds-al: A dual-stream analytic learning for exemplar-free class-incremental learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 17237\u201317244, 2024.   \n[47] Huiping Zhuang, Zhiping Lin, and Kar-Ann Toh. Blockwise recursive moore\u2013penrose inverse for network learning. IEEE Transactions on Systems, Man, and Cybernetics: Systems, 52(5):3237\u20133250, 2021.   \n[48] Huiping Zhuang, Zhenyu Weng, Run He, Zhiping Lin, and Ziqian Zeng. Gkeal: Gaussian kernel embedded analytic learning for few-shot class incremental task. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7746\u20137755, 2023.   \n[49] Huiping Zhuang, Zhenyu Weng, Hongxin Wei, Renchunzi Xie, Kar-Ann Toh, and Zhiping Lin. Acil: Analytic class-incremental learning with absolute memorization and privacy protection. In Advances in Neural Information Processing Systems, pages 11602\u201311614, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Proof of Theorem 3.1 and 3.2 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "For Theorem 3.1, we start with proving the case in batch 1 of task $k$ . ", "page_idx": 13}, {"type": "text", "text": "According to Equation 8, of task $k-1$ , we can expand the stacked activation and label matrixes: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\hat{\\boldsymbol{W}}^{(k-1,n)}=\\left(\\sum_{m=1}^{k-1}\\sum_{i=1}^{n}\\boldsymbol{X}_{m,i}^{\\mathrm{(a)T}}\\boldsymbol{X}_{m,i}^{\\mathrm{(a)}}+\\gamma I\\right)^{-1}\\left[\\begin{array}{c}{\\boldsymbol{X}_{1,1}^{\\mathrm{(a)T}}\\boldsymbol{Y}_{1,1}^{\\mathrm{train}}}\\\\ {\\vdots}\\\\ {\\boldsymbol{X}_{k-1,n}^{\\mathrm{(a)T}}\\boldsymbol{Y}_{k-1,n}^{\\mathrm{train}}}\\end{array}\\right].\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Hence, at batch 1 of task $k$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\hat{\\boldsymbol W}^{(k,1)}=\\left(\\sum_{m=1}^{k-1}\\sum_{i=1}^{n}\\boldsymbol X_{m,i}^{(\\mathrm{a})\\mathrm{T}}\\boldsymbol X_{m,i}^{(\\mathrm{a})}+\\gamma\\boldsymbol I+\\boldsymbol X_{k,1}^{(\\mathrm{a})\\mathrm{T}}\\boldsymbol X_{k,1}^{(\\mathrm{a})}\\right)^{-1}\\left[\\begin{array}{c}{\\boldsymbol X_{1,1}^{(\\mathrm{a})\\mathrm{T}}\\boldsymbol Y_{1,1}^{\\mathrm{train}}}\\\\ {\\vdots}\\\\ {\\boldsymbol X_{k-1,n}^{(\\mathrm{a})\\mathrm{T}}\\boldsymbol Y_{k-1,n}^{\\mathrm{train}}}\\\\ {\\boldsymbol X_{k,1}^{(\\mathrm{a})\\mathrm{T}}\\boldsymbol Y_{k,1}^{\\mathrm{train}}}\\end{array}\\right].\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We have defined regularized feature autocorrelation matrix $\\scriptstyle R_{k-1,n}$ via ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\pmb{R}_{k-1,n}=\\left(\\sum_{m=1}^{k-1}\\sum_{i=1}^{n}X_{m,i}^{\\mathrm{(a)T}}X_{m,i}^{\\mathrm{(a)}}+\\gamma\\pmb{I}\\right)^{-1}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "To facilitate subsequent calculations, here we also define a cross-correlation matrix $\\boldsymbol{Q}_{k-1,n}$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\pmb{Q}_{k-1,n}=\\left[\\pmb{X}_{1,1}^{\\mathrm{(a)T}}\\pmb{Y}_{1,1}^{\\mathrm{train}}\\quad\\mathrm{~.~.~}\\quad\\pmb{X}_{k-1,n}^{\\mathrm{(a)T}}\\pmb{Y}_{k-1,n}^{\\mathrm{train}}\\right].\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Thus, we can rewrite Equation 20 as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\hat{\\boldsymbol W}^{(k-1,n)}=R_{k-1,n}\\boldsymbol Q_{k-1,n}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Therefore, at batch 1 of task $k$ we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\hat{\\boldsymbol{W}}^{(k,1)}=R_{k,1}\\boldsymbol{Q}_{k,1}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "From Equation 21 we can recursively calculate $R_{k,1}$ from $R_{k-1,n}$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\pmb{R}_{k,1}=\\left(\\pmb{R}_{k-1,n}^{-1}+\\pmb{X}_{k,1}^{\\mathrm{(a)T}}\\pmb{X}_{k,1}\\right)^{-1}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "According to the Woodbury matrix identity, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\left(\\boldsymbol{A}+\\boldsymbol{U}\\boldsymbol{C}\\boldsymbol{V}\\right)^{-1}=\\boldsymbol{A}^{-1}-\\boldsymbol{A}^{-1}\\boldsymbol{U}\\left(\\boldsymbol{C}^{-1}+\\boldsymbol{V}\\boldsymbol{A}^{-1}\\boldsymbol{U}\\right)^{-1}\\boldsymbol{V}\\boldsymbol{A}^{-1}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Let $A=R_{k-1,n}^{-1},U=X_{k,1}^{\\mathrm{(a)T}}$ , $C=I$ , $V=X_{k,1}^{(\\mathrm{a})}$ in Equation 26, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{R_{k,1}=R_{k-1,n}-R_{k-1,n}X_{k,1}^{\\mathrm{(a)T}}\\left(I+X_{k,1}^{\\mathrm{(a)}}R_{k-1,n}X_{k,1}^{\\mathrm{(a)T}}\\right)^{-1}X_{k,1}^{\\mathrm{(a)}}R_{k-1,n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Hence, $\\scriptstyle R_{k}$ can be recursively updated using its last task counterpart $\\scriptstyle R_{k-1,n}$ and current data (i.e., $\\displaystyle X_{k,1},$ . This proves the recursive calculation in Equation 11. ", "page_idx": 13}, {"type": "text", "text": "Next, we derive the recursive formulation of $\\hat{W}^{(k,1)}$ . To this end, we also recuse the cross-correlation matrix $Q_{k,1}$ at the batch 1 of task $k$ : ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{Q_{k,1}=\\left[X_{1,1}^{\\mathrm{(a)T}}Y_{1,1}^{\\mathrm{train}}\\quad\\dots\\quad X_{k-1,n}^{\\mathrm{(a)T}}Y_{k-1,n}^{\\mathrm{train}}\\quad X_{k,1}^{\\mathrm{(a)T}}Y_{k,1}^{\\mathrm{train}}\\right]=Q^{\\prime}{}_{k-1,n}+X_{k,1}^{\\mathrm{(a)T}}Y_{k,1}^{\\mathrm{train}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\pmb{Q}_{k-1,n}^{\\prime}=\\left[\\pmb{Q}_{k-1,n}\\quad\\pmb{0}_{d_{(\\mathrm{a})}\\times d_{y k}}\\right]\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $d_{\\mathrm{(a)}}$ is the dimension of activation and $d_{y k}$ is dimension of total classes of task $k$ . Note that the concatenation in Equation 29 is due to fact that $\\boldsymbol{Y}_{k,1}$ of task $k$ contains more data classes (hence more columns) than Y k\u22121,n ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{R_{k-1,n}X_{k,1}^{\\mathrm{(a)T}}(I+X_{k,1}^{\\mathrm{(a)}}R_{k-1,n}X_{k,1}^{\\mathrm{(a)T}})^{-1}=R_{k,1}X_{k,1}^{\\mathrm{(a)T}}K_{k,1}}&{}\\\\ {=R_{k,1}X_{k}^{\\mathrm{(a)T}}(I-K_{k,1}X_{k,1}^{\\mathrm{(a)}}R_{k-1,n}X_{k,1}^{\\mathrm{(a)T}})}\\\\ &{=(R_{k-1,n}-R_{k-1,n}X_{k,1}^{\\mathrm{(a)T}}K_{k,1}X_{k,1}^{\\mathrm{(a)}}R_{k-1,n})X_{k}^{\\mathrm{(a)T}}}\\\\ &{=R_{k,1}X_{k,1}^{\\mathrm{(a)T}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "As $\\hat{\\boldsymbol W}^{(k-1,n)^{\\prime}}=[\\hat{\\boldsymbol W}^{(k-1,n)}~~~\\mathbf{0}]$ has expanded its dimension similar to what $\\boldsymbol{Q}^{\\prime}{}_{k-1,n}$ does, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\hat{\\boldsymbol{W}}^{(k-1,n)^{\\prime}}=\\boldsymbol{R}_{k-1,n}\\,\\boldsymbol{Q}^{\\prime}{}_{k-1,n}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Hence, W\u02c6(k,1) can be rewritten as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\boldsymbol W}^{(k,1)}=R_{k,1}\\boldsymbol{Q}_{k,1}=R_{k,1}(\\boldsymbol{Q}^{\\prime}{}_{k-1,n}+X_{k,1}^{\\mathrm{(a)T}}Y_{k,1}^{\\mathrm{train}})=R_{k,1}\\boldsymbol{Q}^{\\prime}{}_{k-1,n}+R_{k,1}X_{k,1}^{\\mathrm{(a)T}}Y_{k,1}^{\\mathrm{train}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "By substituting Equation 27 into $\\pmb{R}_{k,1}\\pmb{Q}_{k-1,n}^{\\prime}$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R_{k,1}Q_{k-1,n}^{\\prime}=R_{k-1,n}Q_{k-1,n}^{\\prime}-R_{k-1,n}X_{k,1}^{\\mathrm{(a)T}}(I+X_{k,1}^{\\mathrm{(a)}}R_{k-1,n}X_{k,1}^{\\mathrm{(a)T}})^{-1}X_{k,1}^{\\mathrm{(a)}}R_{k-1,n}Q_{k-1,n}^{\\prime}}\\\\ &{\\qquad\\qquad=W^{(k-1,n)^{\\prime}}-R_{k-1,n}X_{k,1}^{\\mathrm{(a)T}}(I+X_{k,1}R_{k-1,n}X_{k,1}^{\\mathrm{(a)T}})^{-1}X_{k,1}^{\\mathrm{(a)}}W^{(k-1,n)^{\\prime}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "According to Equation 30, Equation 33 can be rewritten as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\pmb{R}_{k,1}\\pmb{Q}_{\\textnormal{\\tiny{k-1}},n}^{\\prime}=\\hat{\\pmb{W}}^{(k-1,n)^{\\prime}}-\\pmb{R}_{k,1}\\pmb{X}_{k,1}^{\\mathrm{(a)T}}\\pmb{X}_{k,1}^{\\mathrm{(a)}}\\hat{\\pmb{W}}^{(k-1,n)^{\\prime}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "By inserting Equation 33 into Equation 32, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\pmb{W}}^{(k,1)}=\\hat{\\pmb{W}}^{(k-1,n)^{\\prime}}-\\pmb{R}_{k,1}\\pmb{X}_{k,1}^{\\mathrm{(a)T}}\\pmb{X}_{k,1}^{\\mathrm{(a)}}\\hat{\\pmb{W}}^{(k-1,n)^{\\prime}}+\\pmb{R}_{k,1}\\pmb{X}_{k,1}^{\\mathrm{(a)T}}\\pmb{Y}_{k,1}^{\\mathrm{train}}}\\\\ &{\\qquad=\\hat{\\pmb{W}}^{(k-1,n)^{\\prime}}+\\pmb{R}_{k,1}\\pmb{X}_{k,1}^{\\mathrm{(a)T}}(\\pmb{Y}_{k,1}^{\\mathrm{tain}}-\\pmb{X}_{k,1}^{\\mathrm{(a)}}\\hat{\\pmb{W}}^{(k-1,n)^{\\prime}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which proves the case in the batch 1 of task $k$ . ", "page_idx": 14}, {"type": "text", "text": "For Theorem 3.2, we consider the case at rest batches of task $k$ . In the rest of batches, the number of classes maintain unchanged compared with batch 1, which means no column expansion is required. According to Equation 35, we substitute W\u02c6(k\u22121,n) \u2032by W\u02c6(k,i\u22121). S imilar substitution is applied to $\\scriptstyle R_{k-1,n}$ with $R_{k,i-1}$ according to Equation 27 since the shape of regularized feature autocorrelation matrix is remained through whole learning agenda. Then we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\boldsymbol W}^{(k,i)}=\\hat{\\boldsymbol W}^{(k,i-1)}+R_{k,i}\\boldsymbol X_{k,i}^{\\mathrm{(a)T}}\\left(\\boldsymbol Y_{k,i}^{\\mathrm{train}}-\\boldsymbol X_{k,i}^{(a)}\\hat{\\boldsymbol W}^{(k,i-1)}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{R_{k,i}=R_{k,i-1}-R_{k,i-1}X_{k,i}^{\\mathrm{(a)T}}\\left(I+X_{k,i}^{\\mathrm{(a)}}R_{k,i-1}X_{k,i}^{\\mathrm{(a)T}}\\right)^{-1}X_{k,i}^{\\mathrm{(a)}}R_{k,i-1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which complete the proof. ", "page_idx": 14}, {"type": "image", "img_path": "rGEDFS3emy/tmp/02bd96a92205e99d32b79af5d02356d577f26d76cd4856ad30e9f37bb220b632.jpg", "img_caption": ["Figure 4: Average accuracy measured in different projection sizes. Due to the time complexity in computing Equation 11, higher projection sizes escalate training time while increase little performance. On small datasets, higher projection sizes may even lead to reduction. ", "Table 5: Average accuracy of CIFAR100 with updating manner in classifier. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "B Appendix ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Figure 4 demonstrates the influence of different random projection sizes. These experiments reveal that higherdimensional projections may not always improve accuracy, suggesting that projection size must fti the dataset\u2019s volume. If the volume of the dataset is small, a high-dimensional projection results in over-ftiting to the training set. The maximum accuracy on DTD is at 1,000 projection size, while the accuracy slightly increases after 1,000. When the projection size is 2,000, the results of FGVCAircrft and CORe50 are a little higher than the results on 1,000, but time consumption is twice higher due to the cubic time complexity in the matrix inverse. Thus, we limit the projection size measurement to 2,000 for CORe50 and Tiny-ImageNet due to the significant increase in time consumption. ", "page_idx": 15}, {"type": "text", "text": "Table 5 justify the contribution of AC. The reduction from $91.1\\%$ to $32.4\\%$ without AL suggesting that one epoch of simple back-propagation is unable to handle the OCIL. ", "page_idx": 15}, {"type": "text", "text": "Table 6 varifies the choice of 1 as regularization term is suitable for OCIL. Fine-grained datasets suffer a lot from the wrong choice of $\\gamma$ . The average accuracy on FGVCAircraft drops from $61.1\\%$ to $22.4\\%$ when $\\gamma$ is tuned from 1 to $10^{-3}$ . ", "page_idx": 15}, {"type": "table", "img_path": "rGEDFS3emy/tmp/9089dcc7613437527a6b1eeb1fb092c0bbc69d60a2b45fe7afde9e31a5e9d989.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "rGEDFS3emy/tmp/407e3bd4df5c338f5f5c0fc5553f3e21288dcef2381e8c9965937760c9f6cec1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Table 6: Average accuracy measured with different regularization terms. $\\gamma{=}1$ shows robust results across all datasets. ", "page_idx": 15}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "1. Claims ", "page_idx": 16}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: The most important idea of this paper is claimed in substract(i.e., forward-only learning). Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 16}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: Main limitation is F-OAL needs a powerful pre-trained backbone ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 16}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: We have detailed proof in Apendix. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification: The results can be reproduced in released code. Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 17}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification: We release the code. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: We have detailed information in Experiment section. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.   \n\u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 18}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 18}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification:We do not include statistical significance. ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 18}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: We have this in Experiment section. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. ", "page_idx": 18}, {"type": "text", "text": "\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 19}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We follow the rules. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 19}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: We have this content in section 4.8. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 19}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: We algorithm will not generate harmful content directly. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 19}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We cite the source code, which is allowed by code author. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 20}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: We do not release new assets. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 20}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 20}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 21}]