[{"type": "text", "text": "Remix-DiT: Mixing Diffusion Transformers for Multi-Expert Denoising ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Gongfan Fang Xinyin Ma Xinchao Wang\u2217 National University of Singapore {gongfan,maxinyin}@u.nus.edu, xinchao@nus.edu.sg ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Transformer-based diffusion models have achieved significant advancements across a variety of generative tasks. However, producing high-quality outputs typically necessitates large transformer models, which result in substantial training and inference overhead. In this work, we investigate an alternative approach involving multiple experts for denoising, and introduce Remix-DiT, a novel method designed to enhance output quality at a low cost. The goal of Remix-DiT is to craft $N$ diffusion experts for different denoising timesteps, yet without the need for expensive training of $N$ independent models. To achieve this, Remix-DiT employs $K$ basis models (where $K<N)$ ) and utilizes learnable mixing coefficients to adaptively craft expert models. This design offers two significant advantages: first, although the total model size is increased, the model produced by the mixing operation shares the same architecture as a plain model, making the overall model as efficient as a standard diffusion transformer. Second, the learnable mixing adaptively allocates model capacity across timesteps, thereby effectively improving generation quality. Experiments conducted on the ImageNet dataset demonstrate that RemixDiT achieves promising results compared to standard diffusion transformers and other multiple-expert methods. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Transformer-based Diffusion models [39, 8, 29, 2] have shown significant potential in generating high-quality images and videos [24, 20, 3]. However, achieving such quality often requires large transformer architectures [35], which incur considerable training and inference costs. To alleviate these computational burdens, multi-expert denoising has emerged as a promising approach [1, 18, 28, 16, 27], which employs multiple specialized diffusion models, each designed for distinct time intervals within the denoising process. ", "page_idx": 0}, {"type": "text", "text": "The goal of multi-expert denoising is to increase the overall capacity of diffusion models while keeping an acceptable overhead. The denoising process of diffusion models involves multiple different timesteps [13, 32, 18], requiring the network to make predictions at various noise levels. Previous work has shown that the tasks of diffusion models vary across different timesteps [13, 37, 27]. For instance, at higher noise levels, the model focuses more on low-frequency features, while at lower noise levels, the model emphasizes generating high-frequency details [37]. This variability inherently leads to a multi-task problem [16, 18]. However, due to the limited capacity of a single diffusion model, it is challenging to craft a comprehensive and balanced model that performs well across the entire denoising process. To alleviate this issue, multiple-expert denoising deploys specialized expert models at different timesteps. To this end, each model only needs to learn the denoising task for a specific subset of timesteps. Although this design introduces multiple models, only one model is activated at each timestep during the inference process, and therefore the overall computational complexity does not increase significantly. ", "page_idx": 0}, {"type": "image", "img_path": "vo5LONGAdo/tmp/6fa46a033dd2305ba2c7c284b2944abef5ca548860771439dac4373f2f1b4055.jpg", "img_caption": ["Figure 1: (a) Re-training a larger DiT incurs significant training and inference costs. (b) Multiexpert denoising trains multiple expert models to improve generation quality while maintaining low inference overhead. However, training multiple experts still results in significant training costs. (c) This work introduces RemixDiT, a learnable method to craft any number of experts by mixing basis models. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In a multi-step denoising process, such as a 1000-step denoising chain, determining the optimal number of intervals and their partitioning remains an open problem. Existing approaches typically employ an even-split strategy, which distributes the denoising process equally among a predefined number of expert models. This uniform allocation often leads to suboptimal utilization of model capacity. In addition, the training of multiple expert models introduces substantial computational costs. Due to the inherent task similarity between adjacent intervals in the denoising process, training separate experts without accounting for this similarity can result in redundant and inefficient training. Although some recent methodologies, such as tree-structured training [1], have proposed hierarchical strategies to mitigate these issues, the process of training and capacity allocation remains largely manual and heuristic. To address these challenges, this paper introduces a novel, learnable strategy to craft multiple experts adaptively for denoising. ", "page_idx": 1}, {"type": "text", "text": "Consider a denoising process of $T$ steps. The objective of this paper is to construct $N$ expert models, each handling an interval of length $\\textstyle{\\frac{T}{N}}$ as shown in Figure 1 (b). A straightforward approach to improve performance is to increase the number of experts, as more experts provide greater total capacity, leading to enhanced performance. However, this also results in a linear increase in training costs. The core idea of the proposed method lies in training only $K(K<N)$ basis models, which can be used to craft $N$ expert models efficiently for inference. To be exact, we construct $K$ transformer-based diffusion models and introduce learnable mixing coefficients to fuse their parameters, which can be easily implemented as an embedding layer. During training, the coefficients will learn to allocate the model capacity across different timesteps. During inference, we precompute those mixed models to do a chained multi-expert denoising. ", "page_idx": 1}, {"type": "text", "text": "We validated the effectiveness of our method on ImageNet-256x256 [6]. Owning to the learnable nature of Remix-DiT, our approach adaptively allocates model capacity across timesteps, thereby achieving superior generation results compared to independently trained multiple expert models. Our analysis of the learned mixing coefficients revealed several insightful findings: Firstly, the coefficients exhibit a natural similarity between adjacent timesteps, indicating that tasks at neighboring steps are relatively similar. Conversely, the coefficients for timesteps that are further apart show distinct differences, supporting the multi-expert denoising hypothesis that the learning target at different steps can be quite diverged. Additionally, our method\u2019s learnable coefficients tend to allocate more capacity to early timesteps, suggesting that the algorithm identifies predictions at lower noise levels as more challenging and thus requires greater capacity to generate finer details. In contrast, at higher noise levels (i.e., timesteps approaching $T$ ), the algorithm integrates multiple basis models for higher utilization of model capacity. Furthermore, we observed that experts created through hybridization exhibit specialization, achieving lower loss at specific timesteps while incurring higher loss at others. Through collaborative denoising, such specialization reduces overall prediction errors, highlighting the effectiveness of our method. ", "page_idx": 1}, {"type": "text", "text": "The contribution of this work lies in a novel method for multi-expert denoising, which creates N experts by combining K $(\\mathsf{K}{<}\\mathsf{N})$ basis models. This approach effectively reduces training costs and improves performance. ", "page_idx": 2}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Multiple Experts in Diffusion Models The multi-step denoising process inherent to the diffusion model [14, 33, 34] can be viewed as a multitasking problem [1, 18, 28, 16]. At each step, the model receives inputs with varying levels of noise and makes predictions accordingly. Given the disparate learning objectives for each denoising step, the utilization of a singular model across all stages is, to a certain extent, inefficient and challenging. Several prior works suggested employing multiple models, each dedicated to handling partial timestep intervals. For example, E-Diff [1] introduces a simple binary strategy to learn various experts and during inference, take the ensemble of experts for prediction. OMS-DPM [18] proposes a model scheduler, dynamically taking denoiser of different sizes from a model zoo for inference. MEME [16] deploys a similar paradigm, yet training lightweight models for different steps. The ensemble of these lightweight denoisers can achieve superior performance compared to their larger counterparts while maintaining efficiency during inference. Another exploration to enhance efficiency using multiple experts is T-Stich [27], which stitches the denoising trajectories of different pre-trained models, enabling dynamic inference cost without additional training. However, it\u2019s noteworthy that multiple diffusion models may incur additional storage and context switch costs. Thus, DTR [28] introduces a novel multi-tasking strategy to learn a single model with scheduled task masks, activating specific sub-networks for different timesteps. ", "page_idx": 2}, {"type": "text", "text": "Transformer-based Diffusion Models Transformer-based diffusion models have achieved impressive results on image generation [2, 29, 4, 10, 20, 17]. DiT [29] explores the scalability of transformers for image generation, achieving competitive performance compared to CNN-based diffusion models. UViT [2] independently explores transformer design for generation, incorporating the U-Net [31, 22, 30] architecture for improved training efficiency and quality. Furthermore, recent works have expanded transformer-based diffusion models to video generation [20, 21, 24, 5, 3], audio [19, 15] and 3D [25] which shows the power of transformers in modeling complicated data. However, training diffusion models remains inefficient, typically requiring millions of steps to produce satisfactory results. A series of works in the literature focus on the efficiency of diffusion-based transformers [40, 40, 23, 9]. MaskDiT [40] and MDTv2 [10] utilize masked transformers and an additional masked autoencoder task [12] to streamline training. UViT [2] also demonstrates the beneftis of skip connections for accelerating convergence. In this work, we introduce a new strategy to enhance the training efficiency of plain diffusion transformers ", "page_idx": 2}, {"type": "text", "text": "3 Preliminary ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Diffusion Probabilistic Models Diffusion Probabilistic Models (DPMs) train a denoiser network to revert a process of adding gradually increased noises [13]. Given an input $x_{0}$ , a $T$ -step forward process is deployed to\u221a transform $x_{0}$ into latent $x_{1},\\ldots.x_{T}$ , where the transformation is defined as $\\dot{q}(\\pmb{x}_{t}|\\pmb{x}_{t-1})\\doteq\\dot{\\mathcal{N}}(\\pmb{x}_{t};\\sqrt{1-\\beta_{t}}\\pmb{x}_{t-1},\\beta_{t}I)$ under a increased variance schedule $\\beta_{1:T}$ . To revert this process, a network $q(\\pmb{x}_{t-1}|\\pmb{x}_{t})$ is trained to predict $x_{t-1}$ given $x_{t}$ . By defining $\\alpha_{t}=1-\\beta_{t}$ and $\\begin{array}{r}{\\bar{\\alpha}_{t}=\\prod_{s=1}^{t}\\alpha_{s}}\\end{array}$ , the training objective is formalized as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\theta)=\\mathbb{E}_{t,x_{0},\\epsilon\\sim\\mathcal{N}(0,1)}\\left[\\lVert\\epsilon-\\epsilon_{\\theta}(\\sqrt{\\bar{\\alpha}_{t}}x_{0}+\\sqrt{1-\\bar{\\alpha}_{t}}\\epsilon,t)\\rVert^{2}\\right],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\epsilon_{\\theta}$ is a trainable neural network and $\\epsilon$ refer to the noises at timestep $t$ . A common practice in the literature is to train a single neural network for all timesteps $t$ . However, it has been observed that the denoising behavior usually varies across timesteps [13, 1, 37, 27]. Due to the limited capacity, a single model may struggle to fit all steps. Therefore, a more suitable yet natural approach is to use multiple models specifically learned for different timestep intervals [1].\" ", "page_idx": 2}, {"type": "text", "text": "Multi-Expert Denoising To facilitate denoising with $N$ expert models, the timesteps are typically divided into $N$ intervals. Each model, denoted as $\\epsilon_{\\theta_{i}}$ , contains independent parameters $\\theta_{i}$ . For each model $\\theta_{i}$ , we minimize the training objective as formalized in Equation 1 on the corresponding ", "page_idx": 2}, {"type": "image", "img_path": "vo5LONGAdo/tmp/5bc6cc567cc6111038ffb430922444c5b860fd62a073b00b316b0d5acfedeac7.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 2: An example of mixing 4 linear layers basis into 6 expert layers. Each expert linear layer is a weighted averaging of the basis layers. At each denoising interval, only one expert is activated for inference or training. To increase the number of experts, we increase the number of coefficients $_{\\alpha}$ , which is more efficient than independently training new experts. ", "page_idx": 3}, {"type": "text", "text": "timestep interval $\\begin{array}{r}{t\\in[i\\times\\lfloor\\frac{T}{N}\\rfloor,(i+1)\\times\\lfloor\\frac{T}{N}\\rfloor]}\\end{array}$ as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}(\\theta_{i}):=\\mathbb{E}_{t\\in\\left[i\\times\\lfloor\\frac{T}{N}\\rfloor,(i+1)\\times\\lfloor\\frac{T}{N}\\rfloor\\right],x_{0},\\epsilon\\sim\\mathcal{N}(0,1)}\\left[\\|\\epsilon-\\epsilon_{\\theta_{i}}(\\sqrt{\\bar{\\alpha}_{t}}x_{0}+\\sqrt{1-\\bar{\\alpha}_{t}}\\epsilon,t)\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Although the simple form of multi-expert denoising has been verified effective in previous works [1, 16], the above objective still presents two major challenges: 1) The optimal number of experts and the best interval partition is unknown; 2) Training multiple independent experts can be expensive. In this work, we introduce a new approach, Remix-DiT to address the above issues. ", "page_idx": 3}, {"type": "text", "text": "4 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Uniform Partition of Denoising Process Let\u2019s consider a multi-expert denoising problem with $N$ experts for $T$ timesteps. For simplicity, we assume that each expert\u2019s parameters $\\theta_{i}$ is a column vector with $P$ trainable elements and the parameter matrix of all experts can be formalized as $\\pmb{\\Theta}_{N\\times P}=[\\pmb{\\theta}_{1},\\pmb{\\theta}_{2},\\cdot\\cdot\\cdot\\mathrm{~,~}\\pmb{\\theta}_{N}]^{\\top}$ . This is natural since the parameters in a model can be flattened and concatenated into a vector. We consider a simple partition of timesteps, which equally divides the denoising process into $N$ intervals, i.e., $[0,T{\\bar{/}}N]^{*}...\\,[(N-1)\\cdot T/{\\bar{N}},N\\cdot T/N]$ . The motivation behind the oracle partition is that adjacent steps share similar noise levels and training objectives. Following Equation 2, this leads to the learning objective to optimize each expert on their associated time intervals. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\Theta^{*}=\\underset{\\Theta}{\\arg\\operatorname*{min}}\\sum_{\\theta_{i}\\in\\Theta}\\mathcal{L}(\\pmb{\\theta}_{i}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The above process involves $N$ independent optimization problem, which presents some issues. First, the optimal number of experts is unknown. It\u2019s difficult to increase the number of experts since this will introduce huge training costs. Besides, the optimal partition is also unclear, making the uniform partition inefficient. In this work, we investigate such a problem: Is it possible to learn any number of experts, while keeping an affordable overhead? ", "page_idx": 3}, {"type": "text", "text": "Crafting Experts by Mixing To address the problem of uniform partition, we leverage a set of basis models to avoid straightforward training on experts. The core idea lies in that, it is possible to fuse the parameters of two diffusion models to achieve better performance [38]. Formally, instead of training $N$ independent models directly, the core idea of Remix-DiT is to learn $K$ $\\langle K\\,<\\,N\\rangle$ basis models with the same architecture as experts, parameterized by $\\beta_{K\\times P}=[\\beta_{1},\\beta_{2},\\dots\\beta_{K}]^{\\top}$ . And the expert models can be crafted by mixing the basis parameters with certain coefficients. For each expert $\\theta_{i}$ , we associate it with a coefficients $\\pmb{\\alpha}_{i}=[\\bar{\\alpha_{i1}},\\bar{\\alpha_{i2}},...\\,\\alpha_{i K}]^{\\top}$ and compute the mixed expert parameter through a weighted averaging $\\begin{array}{r}{\\pmb{\\theta}_{i}=\\sum_{k}\\alpha_{i k}\\beta_{k}}\\end{array}$ . This leads to the coefficient matrix denoted as $\\pmb{\\alpha}_{N\\times K}=[\\pmb{\\alpha}_{1},\\pmb{\\alpha}_{2},\\dots,\\pmb{\\alpha}_{N}]^{\\top}$ . Then, the expert\u2019s parameter matrix can be easily obtained through a simple matrix multiplication: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\Theta_{N\\times P}=\\pmb{\\alpha}_{N\\times K}\\beta_{K\\times P}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "With Equation 4, we can freely craft different experts using different mixing coefficients. Now the problem lies in that how to learn a good coefficients $_{\\alpha}$ and basis models $\\beta$ . ", "page_idx": 4}, {"type": "text", "text": "Architecture of Remix-DiT Let\u2019s further delve into the details of $K$ basis models, denoted as $\\beta$ , alongside a coefficients matrix $_{\\alpha}$ . In this study, we adopt the DiT architecture proposed in [29], which primarily consists of linear and normalization layers. Rather than constructing $K$ distinct models, we propose a simple trick to construct a singular and extended DiT model to encapsulate all $K$ basis models [36]. This is achieved by increasing the width of the linear layers by a factor of $K$ , with a modified forward process to support the mixing of basis. An example of the extended linear ", "page_idx": 4}, {"type": "table", "img_path": "vo5LONGAdo/tmp/416c5331ee528375e39a8aa7bd93a7f56b470eee629cd0a76840ac19b28022ed.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "layer is depicted in Algorithm 1. A notable distinction between the standard DiT and the proposed Remix-DiT is the integration of a weighted averaging step before the true computation. Despite the expansion in width by a factor of $K$ , the effective weights engaged during forward propagation are equivalent to those of a normal DiT of the original width. The additional computational cost is attributed solely to the element-wise operation during mixing. Furthermore, during the backward propagation phase, the computational cost of the mixed network almost mirrors those of a standard DiT, except for the additional overhead caused by the mixing operation. ", "page_idx": 4}, {"type": "text", "text": "Network Training A notable challenge presented by Remix-DiT is the restriction of activating only one mixed expert during each forwarding and backwarding phase. To address this limitation, we employ a hierarchical sampling strategy whereby the expert $\\theta_{i}$ is selected first, followed by a random selection of timestep $t$ within the interval $\\hat{\\mathbf{\\eta}}[i\\times\\lfloor\\frac{T}{N}\\rfloor,(i+1)\\times\\lfloor\\frac{T}{N}\\rfloor]$ . Consequently, only one expert is activated per training step. Despite this constraint, a critical advantage of Remix-DiT is that regardless of which expert is activated, all basis models remain updatable. This feature distinctly sets our approach apart from methodologies that train experts independently. The training and inference processes of Remix-DiT are delineated in Algorithm 2. The training protocol for Remix-DiT closely mirrors that of a standard DiT, with the exception that the sampled timesteps $t$ must originate from a singular time interval during each step. During inference, the compact nature of the basis models allows for reduced GPU memory usage compared to maintaining $N$ separate experts, providing a significant advantage. Additionally, it is still feasible to pre-compute the $N$ experts for more efficient inference without runtime mixing. ", "page_idx": 4}, {"type": "text", "text": "Transform Pre-trained DiT to Remix-DiT with Prior Coefficients Training generative models from scratch is usually inefficient. Note that the proposed method Builds a DiT model with $K$ -times width. This allows it to inherit the weights of pre-trained DiTs, by replicating the pre-trained weight $K$ times. This leads to $K$ basis models with the same parameters. However, this will be problematic if we inspect the gradient to each mixing coefficient, denoted as $\\nabla_{\\alpha_{i,k}}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\nabla_{\\alpha_{i,k}}=\\sum_{k}\\nabla\\pmb{\\theta}_{i}\\odot\\beta_{k},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\odot$ is an element-wise product of two matrices. First, the update of basis parameter $\\widehat{\\pmb{\\theta}}_{i}$ is usually slow, this makes all mixed experts similar to each other at the beginning of training and also makes the gradient w.r.t the mixed expert $\\widehat{\\pmb{\\theta}}_{i}$ almost unchanged. In this case, the gradient to the coefficients will be constant. To address this issue, we introduce a prior regularization to force each basis $\\widehat{\\pmb{\\theta}}_{i}$ to learn different objectives. Inspired by the oracle partition in multiple expert denoising, we craft prior coefficients $\\alpha_{*}$ with one-hot vectors. For example, the hand-coded prior coefficient $[0,1,0,0]$ directly assigns the second basis as the expert, this encourages a polarization for different bases. And the regularization can be easily implemented as a cross-entropy between the prior coefficients and the learnable coefficients. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{R}=-\\gamma\\sum_{i}\\alpha_{i,k}^{*}\\log(\\alpha_{i,k}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "1: // Training   \n2: Initialize $K$ basis models $\\beta=\\left[\\beta_{1},\\beta_{2},...\\,\\beta_{K}\\right]^{\\intercal}$ from random or pretrained models   \n3: Randomly initialize mixing logits $\\pi=[\\pi_{1},\\pi_{2},...\\,\\pi_{N}]^{\\top}$   \n4: while Training not terminated do   \n5: Randomly sample a expert index $i\\in[0,N)$   \n6: Compute the mixing coefficients $\\pmb{\\alpha}_{i}=\\mathrm{Softmax}(\\pmb{\\pi}_{i})$   \n7: Obtain the $i$ -th mixed expert $\\pmb{\\theta}_{i}=\\pmb{\\alpha}_{i}\\beta$   \n8: Randomly sample the timestep $\\begin{array}{r}{t\\in[i^{'}\\times\\lfloor\\frac{T}{N}\\rfloor,(i+1)\\times\\lfloor\\frac{T}{N}\\rfloor]}\\end{array}$   \n9: Compute the loss $\\mathcal{L}_{t,i}=\\|\\epsilon-\\epsilon_{\\theta_{i}}(\\sqrt{\\bar{\\alpha}_{t}}x_{0}+\\sqrt{1-\\bar{\\alpha}_{t}}\\epsilon,t)\\|^{2}$   \n10: Update $\\pi$ and $\\Theta$ by back-propagation   \n11: end while   \n12: // Inference   \n13: Compute the mixing coefficients $\\alpha=[\\mathrm{Softmax}(\\pi_{1}),\\mathrm{Softmax}(\\pi_{2}),...\\,,\\mathrm{Softmax}(\\pi_{N})]$   \n14: Pre-compute experts with $\\Theta=\\alpha^{T}\\beta$   \n15: Inference with $\\Theta$ ", "page_idx": 5}, {"type": "text", "text": "where $\\gamma$ is a hyperparameter to control the strength of regularization. In practice, we adopt an annealing strategy to linearly remove the regularization with $\\gamma\\to0$ . ", "page_idx": 5}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "5.1 Experimental Settings ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Network Architecture. This paper utilizes the DiT [29] models as the fundamental architecture. By reloading the computation process of the linear layers, additional mixing coefficients is introduced to create RemixDiT. For a fair comparison, we ensure that the dimensions of all mixed experts are completely consistent with the original DiT. For instance, Remix-DiT-S is constructed by quickly combining multiple DiT-S models to form an expert model. ", "page_idx": 5}, {"type": "text", "text": "Training Details. Since our mixed model is entirely consistent with the standard DiT, this method can be applied to pre-trained models. We first trained a standard DiT model on ImageNet and then initialized the basis model with the same pre-trained weights. We introduce prior as discussed in Equation 6 to the mixing coefficients to accelerate the learning of different basis models. In our experiments, we conducted $100\\,\\mathrm{K}$ fine-tuning on DiT-S/B/L models [29], pre-trained for 2M/1M/1M steps correspondingly. ", "page_idx": 5}, {"type": "text", "text": "5.2 Transform Pretrained DiT to Remix-DiT ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In Table 1, we present the results of fine-tuning standard DiT models using the Remix-DiT approach. Given the architectural consistency between the mixed model and the standard DiT, it is feasible to initialize $K$ basis models within the standard DiT framework. This enables the construction of RemixDiT with minimal additional training steps since the pre-trained DiT has been comprehensively trained at all temporal steps. For instance, starting with a pre-trained DiT-B model, our method successfully crafts a Remix-B with only 100K training steps, achieving superior performance compared to both the original models and baselines such as continual training and multiple experts [1]. ", "page_idx": 5}, {"type": "text", "text": "Moreover, within the same computational budget, our approach outperforms Multi-expert baselines, where experts are trained independently. It is noteworthy that for Multi-expert baselines with eight experts, the total training budget is uniformly allocated to each expert. As the number of experts increases, the allocated training steps for each expert become more limited. In contrast, RemixDiT trains shared basis models throughout the entire training process, allowing each model to be sufficiently updated. ", "page_idx": 5}, {"type": "text", "text": "As discussed earlier, RemixDiT allocates gradients from different time steps by employing mixing coefficients. Therefore, for $K$ basis models initialized similarly, greater diversity in the initial mixing coefficients facilitates the rapid learning of different models. To achieve this, we sequentially distribute the entire denoising process among the basis models. During subsequent fine-tuning, the variation in coefficients will enable a more refined allocation of model capacity. In Table 3, we compare the fine-tuned Remix-B to other diffusion models. ", "page_idx": 5}, {"type": "table", "img_path": "vo5LONGAdo/tmp/1ebaa9577c3033fafc1294f421ca19c2cca9aac5d8e68db6302ea250ef215309.jpg", "table_caption": [""], "table_footnote": ["Table 1: Finetuning pre-trained DiT for $100\\mathbf{k}$ steps on ImageNet- $.256\\!\\times\\!256$ . During inference, all expert share the same architecture as a standard DiT. "], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "5.3 Visualization of Learned Experts ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To further investigate the behavior of the experts learned by the algorithm, we visualized the mixing coefficients. As shown in Figure 4a, we conducted experiments on DiT-S. During the training of 20 expert models, we observed that the algorithm assigned more one-hot coefficients to timesteps close to 0. At these steps, the denoising model focuses more on high-frequency detail features. In contrast, at late timesteps resembling random noise, the algorithm enabled and mixed multiple models to generate content rather than using only one expert. As will be illustrated in Figure 5, mixing multiple models can improve the shape quality. Moreover, it can be observed that adjacent timesteps exhibit similar mixing coefficients, whereas more distant timesteps show greater differences. This finding supports the core hypothesis of multi-expert reasoning. To validate this observation, we further trained a RemixDiT with 8 experts. Figure 4b also shows the distribution of mixing coefficients, revealing results similar to the previous experiment. Additionally, Figure $4c$ visualizes the training loss functions of the 8 expert models throughout the denoising process. Each mixed expert has a lower loss function within its respective timestep intervals. The farther from an expert model\u2019s current interval, the higher its loss. Notably, at step 0, we found that the ensemble-constructed model effectively reduces prediction loss. ", "page_idx": 6}, {"type": "table", "img_path": "vo5LONGAdo/tmp/83d48ab17e2037a3d7d658f97e4d01a91f00af373e0ed670c1db6783e94e603d.jpg", "table_caption": [], "table_footnote": ["Figure 3: Comparision to existing methods "], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "5.4 Analytical Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we validate some key design aspects of RemixDiT. This includes the model\u2019s mixing method, the number of expert models and basis models, and whether the coefficients are independent or shared across layers. ", "page_idx": 6}, {"type": "text", "text": "Model Mixers. The core of the proposed method lies in the mixing of multiple models, and we explored three mixing methods: 1) Oracle Mixing: Basis models are manually assigned to different intervals as expert models, equivalent to training independent expert models, which makes the algorithm revert to the standard multi-expert training strategy regardless of N. 2) Raw Mixer: Each expert is assigned a real-valued coefficient without any constraints. 3) Softmax Mixer: This builds on the raw mixer by applying a softmax operation, ensuring that the mixed model is always a weighted average of the basis models. Table 2 shows the effectiveness of each mixer. We found that while oracle mixing improves model performance, it often fails to achieve optimal results due to manually ", "page_idx": 6}, {"type": "image", "img_path": "vo5LONGAdo/tmp/de4626d621e36b51ac743a65678abbf117ff80a7a78068462cc4c7cc2bc0ff83.jpg", "img_caption": ["(a) Learned mixing coefficients for RemixDiT-S-4-20, which crafts 20 experts by mixing 4 basis models. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "vo5LONGAdo/tmp/84ff571582b5a2ee703c9c525975e140cf282c3ff43be49887bd98175355ea2b.jpg", "img_caption": ["(b) Learned mixing coefficients for Remix-DiT-S-4-8 "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "vo5LONGAdo/tmp/0ba677934a6db517d248926b510630d2948872f7f976c7eb30d1602960941bfe.jpg", "img_caption": ["(c) Training losses of 8 experts over timesteps "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 4: (a) Learned Coefficients for a Remix-DiT-S-4-8, which mixes 4 basis DiT-S to obtain 8 models, each associated with a 125-step interval. The $\\mathbf{X}$ -axis shows the corresponding timestep intervals associated with each mixed model. The value in each grid refers to the coefficients that will be used to weight the corresponding basis models. At early timesteps $T\\rightarrow999$ , Remix-DiT tend to use an ensembled model for inference, which is averaged over all basis models. And at the late timesteps $T\\rightarrow0$ , more specialized models, such as the first basis model are picked for fine-grained prediction. ", "page_idx": 7}, {"type": "text", "text": "designed partitions. The raw mixer, with its learnable coefficients, achieved better performance but included some impractical solutions, such as negative coefficients, which increased learning difficulty. The softmax mixer yielded the best results. ", "page_idx": 7}, {"type": "text", "text": "Global or Local Mixer. A DiT model typically comprises multiple layers. When mixing models, we can use globally shared mixing coefficients, referred to as the global mixer, or employ a layer-wise independent mixing strategy, referred to as the local mixer. The first strategy is simpler to train, as it requires only a small $K\\times N$ coefficient matrix regardless of the network\u2019s depth. The second strategy, however, offers a larger model space by introducing trainable parameters that grow linearly with the number of layers. Despite this, the additional parameter size for both methods is very small compared to the original DiT model, which usually has tens to hundreds of millions of parameters. We validated both strategies in Table 2. However, we found that a simple global mixer yielded slightly better performance compared to the local mixer. For simplicity, we use a global mixer in our experiments. ", "page_idx": 7}, {"type": "text", "text": "The Number of Experts and Basis In this part, we also explored the impact of different numbers of basis and expert models on the algorithm\u2019s performance. It is evident that indefinitely increasing the number of expert models does not continuously enhance performance. There are three reasons for this: 1) First, the total capacity of the basis models is limited. With 4 basis models, it is difficult to craft 1,000 varied experts for denoising; 2) As illustrated in Figure 4a, some denoising steps share similar mixing coefficients, which limits the maximal number of effective experts in our method; 3) Due to the fact that only one expert is activated in forward and backward passes, the gradient w.r.t the mixing coefficients will be sparse. With a large coefficient table, the sparsity level of gradients will be higher, which introduces difficulty in optimization. However, compared to training the same $K$ expert models independently, our algorithm\u2019s advantage lies in its ability to learnably allocate the model\u2019s capacity through mixing coefficients. This allocation is done through a soft partition based on weighted loss, rather than a hard partition on individual models. This allows us to roughly choose an appropriate $N$ which is slightly larger than the optimal $N^{*}$ , and the learnable coefficients can \u201cmerge\u201d some experts if necessary by generating the same coefficients. For example, as shown in Figure 4a, we can find highly similar coefficients between the timestep intervals [900, 950] and [950, 1000], which means we don\u2019t need to allocate different experts in these timesteps. In our experiments, we craft 20 experts with 4 basis models. ", "page_idx": 7}, {"type": "table", "img_path": "vo5LONGAdo/tmp/faa6859e0aeb1f5c1318c5d62c983e7b752ab287cfb9bf7329275c40fd46cc19.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "vo5LONGAdo/tmp/e2c099a8f51864203b8f7b4f6aec67c57ca9a6e94486c8e2075458e9d01b8db9.jpg", "table_caption": ["Table 2: Ablation with 10K fintuning over a pre-trained DiT-S/2. For efficiency, we compute the FID-10K with 100 sampling steps. "], "table_footnote": ["Table 3: Training and inference efficiency of Remix-DiT. For inference, we can craft expert models by runtime mixing or pre-computing all experts, which is usually more efficient. "], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Inference and training Efficiency. In Table 3, we present the training and inference efficiency of the proposed Remix-DiT. We found that the training speed slightly decreased from 2.93 to 2.29 due to the mixing of four basis models. Additionally, our method requires extra GPU memory to store the basis models. During inference, we estimated the latency of compact models that craft experts at runtime and a standard multi-expert denoising approach, where all experts are precomputed. Post-training, our method can achieve efficiency comparable to a standard DiT. ", "page_idx": 8}, {"type": "text", "text": "Visualization of Generated Images. Table 5 compares the generated images of the proposed RemixDiT-B and DiT-B. It can be observed that the shape of objects can be improved using the proposed methods. This is as expected since we allocate more model capacity to the early and intermediate stages of denoising, as illustrated in Figure 4, which mainly contributes to the image contents rather than details. ", "page_idx": 8}, {"type": "text", "text": "6 Limitations and Broader Impact ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this work, we introduce a learnable coefficient, implemented as a simple embedding layer for mixing. However, due to the limitations of the deep learning framework, we can only create one expert per forward and backward pass. This leads to sparse gradients in the embedding layers, where coefficients without gradients can only be updated with momentum rather than accurate gradients. One solution to alleviate this issue is distributed training, where processes craft different expert models. Despite this, the challenge remains significant when training a Remix-DiT with a large number of experts, such as 1000. However, according to the visualization of learned coefficients, we find that 1000 experts may not be necessary since many adjacent timesteps share a similar model. In addition, this work will not introduce negative societal impact. ", "page_idx": 8}, {"type": "image", "img_path": "vo5LONGAdo/tmp/eb4c53538960e7dc7c2e20358f2dbdc310e726926fb0f06c4a22746769f92156.jpg", "img_caption": ["Figure 5: Visualization of generated samples from DiT-B and Remix-DiT-B. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we introduce a multi-expert method to enhance the quality of transformer-based diffusion models while maintaining an acceptable inference overhead. The core contribution lies in the ability to craft a large number of experts from a few basis models, thereby significantly reducing the training effort. Besides, with our method, we don\u2019t have to accurately estimate the optimal number of required experts, since the learnable coefficients will adaptive merge experts if necessary, which brings huge flexibility to the practice. ", "page_idx": 9}, {"type": "text", "text": "8 Acknowledgment ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This project is supported by the Ministry of Education, Singapore, under its Academic Research Fund Tier 2 (Award Number: MOE-T2EP20122-0006). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Qinsheng Zhang, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, et al. ediff-i: Text-to-image diffusion models with an ensemble of expert denoisers. arXiv preprint arXiv:2211.01324, 2022.   \n[2] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: A vit backbone for diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22669\u201322679, 2023.   \n[3] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024.   \n[4] He Cao, Jianan Wang, Tianhe Ren, Xianbiao Qi, Yihao Chen, Yuan Yao, and Lei Zhang. Exploring vision transformers as diffusion learners. arXiv preprint arXiv:2212.13771, 2022.   \n[5] Shoufa Chen, Mengmeng Xu, Jiawei Ren, Yuren Cong, Sen He, Yanping Xie, Animesh Sinha, Ping Luo, Tao Xiang, and Juan-Manuel Perez-Rua. Gentron: Delving deep into diffusion transformers for image and video generation. arXiv preprint arXiv:2312.04557, 2023.   \n[6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009.   \n[7] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:8780\u20138794, 2021.   \n[8] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.   \n[9] Gongfan Fang, Xinyin Ma, and Xinchao Wang. Structural pruning for diffusion models. In Advances in Neural Information Processing Systems, 2023.   \n[10] Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. Masked diffusion transformer is a strong image synthesizer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 23164\u201323173, 2023.   \n[11] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vector quantized diffusion model for text-to-image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10696\u201310706, 2022.   \n[12] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16000\u201316009, 2022.   \n[13] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851, 2020.   \n[14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851, 2020.   \n[15] Xin Jing, Yi Chang, Zijiang Yang, Jiangjian Xie, Andreas Triantafyllopoulos, and Bjoern W Schuller. U-dit tts: U-diffusion vision transformer for text-to-speech. In Speech Communication; 15th ITG Conference, pages 56\u201360. VDE, 2023.   \n[16] Yunsung Lee, JinYoung Kim, Hyojun Go, Myeongho Jeong, Shinhyeok Oh, and Seungtaek Choi. Multiarchitecture multi-expert diffusion models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 13427\u201313436, 2024.   \n[17] Elad Levi, Eli Brosh, Mykola Mykhailych, and Meir Perez. Dlt: Conditioned layout generation with joint discrete-continuous diffusion layout transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2106\u20132115, 2023.   \n[18] Enshu Liu, Xuefei Ning, Zinan Lin, Huazhong Yang, and Yu Wang. Oms-dpm: Optimizing the model schedule for diffusion probabilistic models. arXiv preprint arXiv:2306.08860, 2023.   \n[19] Huadai Liu, Rongjie Huang, Xuan Lin, Wenqiang Xu, Maozong Zheng, Hong Chen, Jinzheng He, and Zhou Zhao. Vit-tts: visual text-to-speech with scalable diffusion transformer. arXiv preprint arXiv:2305.12708, 2023.   \n[20] Haoyu Lu, Guoxing Yang, Nanyi Fei, Yuqi Huo, Zhiwu Lu, Ping Luo, and Mingyu Ding. Vdt: Generalpurpose video diffusion transformers via mask modeling. In The Twelfth International Conference on Learning Representations, 2023.   \n[21] Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte: Latent diffusion transformer for video generation. arXiv preprint arXiv:2401.03048, 2024.   \n[22] Xinyin Ma, Gongfan Fang, and Xinchao Wang. Deepcache: Accelerating diffusion models for free. arXiv preprint arXiv:2312.00858, 2023.   \n[23] Xinyin Ma, Gongfan Fang, and Xinchao Wang. Deepcache: Accelerating diffusion models for free. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15762\u2013 15772, 2024.   \n[24] Willi Menapace, Aliaksandr Siarohin, Ivan Skorokhodov, Ekaterina Deyneka, Tsai-Shien Chen, Anil Kag, Yuwei Fang, Aleksei Stoliar, Elisa Ricci, Jian Ren, et al. Snap video: Scaled spatiotemporal transformers for text-to-video synthesis. arXiv preprint arXiv:2402.14797, 2024.   \n[25] Shentong Mo, Enze Xie, Ruihang Chu, Lanqing Hong, Matthias Niessner, and Zhenguo Li. Dit-3d: Exploring plain diffusion transformers for 3d shape generation. Advances in Neural Information Processing Systems, 36, 2024.   \n[26] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In International conference on machine learning, pages 8162\u20138171. PMLR, 2021.   \n[27] Zizheng Pan, Bohan Zhuang, De-An Huang, Weili Nie, Zhiding Yu, Chaowei Xiao, Jianfei Cai, and Anima Anandkumar. T-stitch: Accelerating sampling in pre-trained diffusion models with trajectory stitching. arXiv preprint arXiv:2402.14167, 2024.   \n[28] Byeongjun Park, Sangmin Woo, Hyojun Go, Jin-Young Kim, and Changick Kim. Denoising task routing for diffusion models. arXiv preprint arXiv:2310.07138, 2023.   \n[29] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4195\u20134205, 2023.   \n[30] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684\u201310695, 2022.   \n[31] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted intervention\u2013MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pages 234\u2013241. Springer, 2015.   \n[32] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020.   \n[33] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019.   \n[34] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020.   \n[35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \n[36] Xingyi Yang, Jingwen Ye, and Xinchao Wang. Factorizing knowledge in neural networks. In European Conference on Computer Vision, pages 73\u201391. Springer, 2022.   \n[37] Xingyi Yang, Daquan Zhou, Jiashi Feng, and Xinchao Wang. Diffusion probabilistic model made slim. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22552\u2013 22562, 2023.   \n[38] Xingyi Yang, Daquan Zhou, Songhua Liu, Jingwen Ye, and Xinchao Wang. Deep model reassembly. Advances in neural information processing systems, 35:25739\u201325753, 2022.   \n[39] Xiulong Yang, Sheng-Min Shih, Yinlin Fu, Xiaoting Zhao, and Shihao Ji. Your vit is secretly a hybrid discriminative-generative diffusion model. arXiv preprint arXiv:2208.07791, 2022.   \n[40] Hongkai Zheng, Weili Nie, Arash Vahdat, and Anima Anandkumar. Fast training of diffusion models with masked transformers. arXiv preprint arXiv:2306.09305, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 13}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 13}, {"type": "text", "text": "Justification: This submission introduced a new method for multi-expert denoising. ", "page_idx": 13}, {"type": "text", "text": "Guidelines: ", "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 13}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 13}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Justification: A limitation section is included. ", "page_idx": 13}, {"type": "text", "text": "Guidelines: ", "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 13}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 13}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 13}, {"type": "text", "text": "Justification: This work does not involve theoretical results. Guidelines: ", "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 14}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Justification: We provide hyper-parameters in the appendix ", "page_idx": 14}, {"type": "text", "text": "Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 14}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 14}, {"type": "text", "text": "Justification: Code is available in the supplemental material. ", "page_idx": 14}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 15}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: Training details is summarized in the paper and appendix Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 15}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 15}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Justification: This submission does not include error bars. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified. ", "page_idx": 15}, {"type": "text", "text": "\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). \u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 16}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: The number of GPUs, training efficiency and memory are reported in the paper. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 16}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: The research was conducted with the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 16}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Justification: We discuss the societal impacts in the main paper. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 16}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 17}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 17}, {"type": "text", "text": "Justification: the paper poses no such risks. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 17}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: All papers and assets properly credited. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 17}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 17}, {"type": "text", "text": "Justification: the paper does not release new assets. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 18}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: the paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 18}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: the paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 18}]