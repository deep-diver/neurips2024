[{"figure_path": "vo5LONGAdo/tables/tables_4_1.jpg", "caption": "Table 1: Finetuning pre-trained DiT for 100k steps on ImageNet-256x256. During inference, all expert share the same architecture as a standard DiT.", "description": "This table presents the results of fine-tuning standard DiT models (DiT-L, DiT-B, DiT-S) using the Remix-DiT approach with 100k training steps.  It compares the performance of the standard DiT models against those fine-tuned using continual training and multi-expert methods.  The Remix-DiT models demonstrate improvements in terms of Inception Score (IS), Fr\u00e9chet Inception Distance (FID), Precision, and Recall metrics, showcasing its effectiveness in enhancing model performance even with a limited training budget.", "section": "5.2 Transform Pretrained DiT to Remix-DiT"}, {"figure_path": "vo5LONGAdo/tables/tables_6_1.jpg", "caption": "Table 1: Finetuning pre-trained DiT for 100k steps on ImageNet-256x256. During inference, all expert share the same architecture as a standard DiT.", "description": "This table presents the results of fine-tuning standard DiT models using the Remix-DiT approach. It compares the performance of different models on ImageNet-256x256 after 100k fine-tuning steps.  The models compared include standard DiTs, DiTs with continued training, multi-expert DiTs, and Remix-DiTs. The metrics used for comparison include Inception Score (IS), Fr\u00e9chet Inception Distance (FID), Precision, and Recall.  The table highlights that Remix-DiT achieves superior performance compared to other methods while maintaining a similar architecture and computational efficiency.", "section": "5.2 Transform Pretrained DiT to Remix-DiT"}, {"figure_path": "vo5LONGAdo/tables/tables_6_2.jpg", "caption": "Table 3: Comparision to existing methods", "description": "This table compares the performance of Remix-DiT with other state-of-the-art diffusion models on the ImageNet dataset. The comparison is based on the FID (Fr\u00e9chet Inception Distance) score, a metric that measures the quality of generated images.  The table shows that Remix-DiT achieves a FID score of 9.02, which is lower than the FID scores of other methods, indicating that Remix-DiT generates higher quality images.", "section": "5.3 Visualization of Learned Experts"}, {"figure_path": "vo5LONGAdo/tables/tables_8_1.jpg", "caption": "Table 1: Finetuning pre-trained DiT for 100k steps on ImageNet-256x256. During inference, all expert share the same architecture as a standard DiT.", "description": "This table shows the results of fine-tuning pre-trained Diffusion Transformers (DiT) models for 100,000 steps on the ImageNet-256x256 dataset.  It compares the performance of several models: the standard DiT models, continual training on the standard DiT, multi-expert models, and the proposed Remix-DiT models.  The evaluation metrics used are Inception Score (IS), Fr\u00e9chet Inception Distance (FID), Precision, and Recall.  The key takeaway is that Remix-DiT achieves competitive or better results with fewer parameters than the other methods.  Note that during inference, all expert models in Remix-DiT share the same architecture as a standard DiT, making the inference process efficient. ", "section": "5.2 Transform Pretrained DiT to Remix-DiT"}, {"figure_path": "vo5LONGAdo/tables/tables_8_2.jpg", "caption": "Table 3: Training and inference efficiency of Remix-DiT. For inference, we can craft expert models by runtime mixing or pre-computing all experts, which is usually more efficient.", "description": "This table presents a comparison of the training and inference efficiency between the standard DiT model and the proposed Remix-DiT model.  The metrics shown are steps per second, GPU memory usage (in MiB), latency for runtime mixing, and latency when experts are pre-computed.  The results show a trade-off between training speed and memory usage, with pre-computed experts offering faster inference.", "section": "5.2 Transform Pretrained DiT to Remix-DiT"}]