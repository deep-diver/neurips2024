[{"type": "text", "text": "SyntheOcc: Synthesize Geometric-Controlled Street View Images through 3D Semantic MPIs ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 The advancement of autonomous driving is increasingly reliant on high-quality   \n2 annotated datasets, especially in the task of 3D occupancy prediction, where the   \n3 occupancy labels require dense 3D annotation with significant human effort. In   \n4 this paper, we propose SytheOcc, which denotes a diffusion model that Synthesize   \n5 photorealistic and geometric-controlled images by conditioning Occupancy labels   \n6 in driving scenarios. This yields an unlimited amount of diverse, annotated, and   \n7 controllable datasets for applications like training perception models and simu  \n8 lation. SyntheOcc addresses the critical challenge of how to efficiently encode   \n9 3D geometric information as conditional input to a 2D diffusion model. Our ap  \n10 proach innovatively incorporates 3D semantic multi-plane images (MPIs) to pro  \n11 vide comprehensive and spatially aligned 3D scene descriptions for conditioning.   \n2 As a result, SyntheOcc can generate photorealistic multi-view images and videos   \n13 that faithfully align with the given geometric labels (semantics in 3D voxel space).   \n14 Extensive qualitative and quantitative evaluations of SyntheOcc on the nuScenes   \n15 dataset prove its effectiveness in generating controllable occupancy datasets that   \n16 serve as an effective data augmentation to perception models. ", "page_idx": 0}, {"type": "text", "text": "17 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "18 With the rapid development of generative models, they have shown realistic image synthesis and   \n19 diverse controllability. This progress has opened up new avenues for dataset generation in autonomous   \n20 driving [5, 12, 23, 30]. The task of dataset generation is usually modeled as controllable image   \n21 generation, where the ground truth (e.g. 3D Box) is employed to control the generation of new datasets   \n22 in downstream tasks (e.g. 3D detection). This approach helps to mitigate the data collection and   \n23 annotation effort as it can generate labeled data for free. However, a novel task of vital importance,   \n24 occupancy prediction [24, 27], poses new challenges for dataset generation compared with 3D   \n25 detection. It requires finer and more nuanced geometry controllability, which refers to use the   \n26 occupancy state and semantics of voxels in the whole 3D space to control the image generation.   \n27 We argue that solving this problem not only allows us to synthesize occupancy datasets, but also   \n28 empowers valuable applications such as editing geometry to generate rare data for corner case   \n29 evaluation, as shown in Fig. 1. In the following, we first illustrate why prior work struggles to achieve   \n30 the above objective, and then demonstrate how we address these challenges.   \n31 In the area of diffusion models, several representative works have displayed high-quality image   \n32 synthesis; however, they are constrained by limited 3D controllability: they are incapable of editing 3D   \n33 voxels for precise control. For example, BEVGen [23] generates street view images by conditioning   \n34 BEV layouts using diffusion models. MagicDrive [5] extend BEVGen and additionally converts the   \n35 3D box parameters into text embedding through Fourier mapping that is similar to NeRF [19], and   \n36 uses cross-attention to learn conditional generation. Although these methods achieve satisfactory   \n37 results in image generation, their 3D controllability is inherently limited. These approaches are   \n38 restricted to manipulating the scene in types of 3D boxes and BEV layouts, and hardly adapt to finer   \n39 geometry control such as editing the shape of objects and scenes. Meanwhile, they usually convert   \n40 conditional input into 1D embedding that aligns with prompt embedding, which is less effective in   \n41 3D-aware generation due to lack of spatial alignment with the generated images. This limitation   \n42 hinders their utility in downstream applications, such as occupancy prediction and editing scene   \n43 geometry to create long-tailed scenes, where granular volumetric control is paramount in both tasks.   \n44 ControlNet [41] and GLIGEN [14] is another type of prominent method in the field of controllable   \n45 image generation. These approaches exhibit several desirable attributes in terms of controllability.   \n46 They leverage conditional images such as semantic masks for control, thereby offering a unified   \n47 framework to manipulate both foreground and background. However, despite its precise spatial   \n48 control, ControlNet does not align with our specific requirements. Their conditions of pixel-level   \n49 images differ fundamentally from what we require in 3D contexts. Our experimental results also find   \n50 that ControlNet struggles to handle overlapping objects with varying depths (see Fig. 6 (a)), as it only   \n51 utilizes an ambiguous 2D semantic map as conditional input. As a result, it is non-trivial to extend   \n52 the ControlNet framework and convey their desirable attributes for 3D conditioning.   \n53 To address the above challenges, we propose an innovative representation, 3D semantic multi-plane   \n54 images (MPIs), which contribute to image generation with finer geometric control. In detail, we   \n55 employ multi-plane images [43] to represent the occupancy, where each plane represents a slice of   \n56 semantic label at a specific depth. Our 3D semantic MPIs not only preserve accurate and authentic 3D   \n57 information, but also keep pixel-wise alignment with the generated images. We additionally introduce   \n58 the MPI encoder to encode features, and the reweighing methods to ease the training with long-tailed   \n59 cases. As a collection, our framework enables 3D geometry and semantic control for image generation   \n60 and further facilitates corner case evaluation as depicted in Fig. 1. Finally, experimental results   \n61 demonstrate that our synthetic data achieve better recognizability, and are effective in improving the   \n62 perception model on occupancy prediction. In summary, our contributions include:   \n63 \u2022 We present SytheOcc, a novel image generation framework to attain finer and precise 3D   \n64 geometric control, thereby unlocking a spectrum of applications such as 3D editing, dataset   \n65 generation, and long-tailed scene generation.   \n66 \u2022 Incorporating the proposed 3D semantic MPI, MPI encoder, and reweighing strategy, we   \n67 deliver a substantial advancement in image quality and recognizability over prior works.   \n68 \u2022 Our extensive experimental results demonstrate that our synthetic data yields an effective   \n69 data augmentation in the realm of 3D occupancy prediction. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "image", "img_path": "NhP8MTJzT5/tmp/4f8976a568082b3743152dc5b1f24c2dffa7a562a880a39bc635a545ce9d7c5a.jpg", "img_caption": ["Figure 1: A showcase of application of SytheOcc. We enable geometric-controlled generation that conveys the user editing in 3D voxel space to generate realistic street view images. In this case, we create a rare scene that traffic cones block the way. This advancement facilitates the evaluation of autonomous systems, such as the end-to-end planner VAD [9], in simulated corner case scenes. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "70 2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "71 2.1 3D Occupancy Prediction ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "72 The task of 3D occupancy prediction aims to predict the occupancy status of each voxel in 3D space,   \n73 as well as its semantic label if occupied. Compared with previous perception methods like 3D object   \n74 detection, occupancy prediction offers a more detailed and nuanced understanding of the environment,   \n75 as it provides finer geometric details, is capable of handling general, out-of-vocabulary objects, and   \n76 finally, enriches the planning stack with comprehensive 3D information. Early methods exploited   \n77 LiDAR as inputs to complete the 3D occupancy of the entire 3D scene [18, 33]. Recent methods   \n78 began to explore the more challenging vision-based 3D occupancy prediction [24,25,27,29]. By   \n79 predicting the geometric and semantic properties of both dynamic and static elements, 3D occupancy   \n80 prediction offers a more comprehensive understanding of the surrounding environment. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "81 2.2 Diffusion-based Image Generation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "82 Recent advancements in diffusion models (DMs) have achieved remarkable progress in image   \n83 generation. In particular, Stable Diffusion (SD) [21] employs DMs within the latent space of   \n84 autoencoders, striking a balance between computational efficiency and high image quality. Beyond   \n85 text control, there is also the introduction of additional control signals. A noteworthy work is   \n86 ControlNet [41], which incorporates a trainable copy of the SD encoder to extract the feature of   \n87 conditional images and adds it to the UNet feature. It significantly enhances the controllability and   \n88 unlocking pathways for advanced applications. We refer readers to recent survey [35] for more details. ", "page_idx": 2}, {"type": "text", "text": "89 2.3 Image Generation in Autonomous Driving ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "90 As training neural networks relies heavily on labeled data, numerous studies are delving into dataset   \n91 generation to boost training. Lift3D [12] designs generative NeRF to synthesize labeled datasets   \n92 for 3D detection for the first time. Several other works employ BEV layouts to synthesize image   \n93 data, proving beneficial for perception models. For example, BEVGen [23] conditions BEV layouts   \n94 to generate multi-view street images, while BEVControl [34] separately generates foregrounds and   \n95 backgrounds from BEV layouts. MagicDrive [5] generates images with 3D geometry controls by   \n96 independently encoding objects and maps through a text encoder or map encoder. Compared with   \n97 MagicDrive, our geometry control is characterized by a more detailed and lossless representation of   \n98 3D scenes for control, which poses significant challenges than projected layout or box embedding.   \n99 Recently, DriveDreamer [26], DrivingDiffusion [13], Drive-WM [28] and Panacea [30] use a Con  \n100 trolNet framework, which involves projecting bounding boxes and road maps onto 2D FoV images as   \n101 a conditioning input. This approach has proven to be effective for geometric control. However, it is   \n102 limited in that it only achieves alignment at the 2D-pixel level. Consequently, this method falls short   \n103 in capturing the depth hierarchy and fails to account for the occlusion relationships present in the 3D   \n104 real world. Besides, adding a depth channel like Panacea [30] may address the limitations of depth   \n105 order, but it discards the occluded part and only contains partial observation. UrbanGiraffe [37] train   \n106 a generative NeRF to perform image generation. WoVoGen [17] creates a 4D world volume feature   \n107 using occupancy to guide the generation, but seems to rely on object mask guidance.   \n108 As described above, most of the prior work is restricted by only modeling a projected primitive of 3D   \n109 boxes and road maps as conditions. They suffer from ill-posed un-projection ambiguity. In contrast,   \n110 we model 3D occupancy labels as conditions, as they provide finer geometric details and semantic   \n111 information. However, designing an input representation of 3D occupancy labels into a 2D diffusion   \n112 model is challenging. In this paper, we propose a novel representation: 3D semantic Multi-Plane   \n113 Images (MPIs) as conditional inputs, which not only provide spatial alignment that improves visual   \n114 consistency, but also encode comprehensive 3D geometric information including occluded parts. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "115 3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "116 Overview The overview of our method is depicted in Fig. 2. Built upon the SD pipeline, we   \n117 aim to perform geometry-controlled image generation by conditioning on 3D geometry labels with   \n118 semantics (occupancy labels). One requirement is that the images should faithfully align with the   \n119 given label. This task is more challenging than conditioned on 3D box due to the sparse and irregular   \n120 nature of occupancy. We first discuss how to efficiently represent occupancy in Sec. 3.2, followed   \n121 by our designed MPI encoder to enhance generation quality in Sec. 3.3, and reweighing strategy to   \n122 handle the long-tailed depth and category in Sec. 3.5. ", "page_idx": 2}, {"type": "text", "text": "123 3.1 Representation of Condition: Local Control Aligns Better than Global Control ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "124 One of the key challenges is how to represent our conditional occupancy input. A straightforward   \n125 method [3,5] is to convert the 3D occupancy voxel to 1D global embedding that is similar to text   \n126 embedding, and then use cross-attention to learn controllable generation. However, these global   \n127 methods can be less effective when dealing with dense or irregular data due to the following reasons:   \n128 (i) They perform controllable generation through hard encoding the spatial relationship between 1D   \n129 global embedding and 2D UNet features. (ii) Ignore the underlying geometry alignment between the   \n130 conditional input and the generated image. In contrast, local methods like ControlNet, directly add   \n131 spatial features to the UNet features, providing 2D local control with pixel-level spatial alignment.   \n132 They are better than the global method (see Tab. 1), but suffer from 3D ambiguity (see Fig. 6 (a)).   \n133 Consequently, this comparison motivates us to seek a more compact and efficient manner to encode   \n134 and condition our 3D occupancy labels. ", "page_idx": 2}, {"type": "image", "img_path": "NhP8MTJzT5/tmp/d2ad566dfe395310fb733062d3de34f3fa2cd83973e06cafdf6d425e0c745bd9.jpg", "img_caption": ["Figure 2: The overall architecture of SytheOcc. We achieve 3D geometric control in image generation by utilizing our proposed 3D semantic multiplane images to encode scene occupancy. In our framework, we can edit the occupied state and semantics of every voxel in 3D space to control the image generation, thereby opening up a wide spectrum of applications as shown in the top right. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "135 3.2 Represent Occupancy as 3D Semantic Multiplane Images ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "136 It is non-trivial to design a 3D representation for conditioning. To efficiently store both the semantic   \n137 and geometric information of the irregular occupancy input, we propose to use multiplane images   \n138 (MPIs) [43] as representation. An MPI is composed of a series of fronto-parallel RGBA layers within   \n139 the frustum of the source camera with a specific viewpoint. These planes are arranged at varying   \n140 depths, from $d_{m i n}$ to $d_{m a x}$ , starting from the nearest to the farthest. Each layer of these images   \n141 contains both an RGB image and an alpha map, which collectively capture the visual and geometric   \n142 details of the scene at the respective depth. In our work, instead of storing RGB value and alpha map   \n143 in the original MPI, we store our 3D semantic labels. Each layer of MPI represents the semantic   \n144 index at the corresponding depth. We display the colored MPI in the top row of Fig. 2 for visual   \n145 clarity, but we actually use the integer index for learning. We obtain our 3D semantic MPI by: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{P_{l}=(u\\times d_{l},~v\\times d_{l},~d_{l})^{T},~d_{l}=d_{m i n}+(d_{m a x}-d_{m i n})\\times l/D,}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathtt{M P I}_{n,l}=\\mathtt{I n t e r p o l a t e}(\\mathtt{0c c u p a n c y},\\;\\mathbf{T_{n}}\\cdot\\mathbf{K_{n}^{-1}}\\cdot P_{l}),}\\\\ &{\\mathtt{M P I}=\\mathtt{C o n c a t e n a t e}(\\mathtt{M P I}_{i,j}),\\;i\\in(0,N),\\;j\\in(0,D),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "146 where $(u,v)$ is a pixel coordinate in image space, $d_{l}$ is depth value of the $l^{t h}$ layer, $n$ denotes the $n^{t h}$   \n147 camera view. This equation implies we first back project points $P$ in camera frustum space $(u,v,d)$   \n148 to Euclid space $\\left(x,y,z\\right)$ by multiplying inverse intrinsic $\\dot{\\bf K}^{-1}$ . Then we use transformation matrix $\\mathbf{T}$   \n149 to map points from camera coordinates to occupancy coordinates. We then use the point coordinates   \n150 to interpolate the nearest semantic index from the dense occupancy voxel to form a slice of MPI.   \n151 Finally, we concatenate all slices to form $\\mathtt{M P I}\\in\\mathbb{R}^{N\\times D\\times H\\times W}$ , where $D$ is the number of layers that   \n152 is set at 256, $N$ is the number of camera views in the case of batch $\\mathrm{size}=1$ .   \n153 By representing occupancy as 3D semantic MPI, every pixel in MPI contains geometry and semantic   \n154 information with implicit depth, seamlessly integrating occluded elements, and ensuring a precise   \n155 spatial alignment with the generated images. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "156 3.3 3D Semantic MPI Encoder ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "157 To enable local control with spatially aligned conditions, we develop a simple but effective MPI   \n158 encoder that aligns the 3D multi-plane feature to the latent space of the diffusion model. The   \n159 purpose of the MPI encoder is to obtain features from multi-plane images to perform 3D-aware   \n160 image synthesis. Unlike the original ControlNet which downsampling conditional input through $3{\\times}3$   \n161 convolutions with padding, we design a $1\\!\\times\\!1$ convolutional encoder without downsampling to encode   \n162 features. In detail, the 3D multiplane features which have the sample resolution with latent features,   \n163 are transformed by a $1\\!\\times\\!1$ convolution layer and ReLU activation [1] in the MPI encoder.   \n164 After obtaining the multi-scale feature after the MPI encoder, we add the feature to the decoder of   \n165 diffusion UNet to provide spatial features. Experimental results in Tab. 3 will show that our $1\\!\\times\\!1$ conv   \n166 in MPI encoder is more effective than $3{\\times}3$ conv, as the $1\\!\\times\\!1$ conv with receptive field $=1$ provides a   \n167 spatial align feature to the latent feature in the diffusion UNet. In contrast, $3{\\times}3$ conv is conducted   \n168 in a camera frustum space rather than Euclid space, making an imprecise correspondence between   \n169 3D multiplane features and 2D image features. Moreover, using $3{\\times}3$ conv to process 3D semantic   \n170 MPI will introduce a large computational burden as the channel number increases from 3 channels of   \n171 RGB to 256 planes. We display our 3D geometry and semantic control property in Fig. 3.   \n172 In summary, we chose MPIs as the representation because they (i) Incorporate lossless 3D information,   \n173 including scene geometry rather than 2.5D depth. (ii) Provide spatially aligned conditional features   \n174 that naturally extend the ControlNet framework from image level to 3D level. (iii) Capable of   \n175 representing geometry and semantics including occluded elements. ", "page_idx": 3}, {"type": "image", "img_path": "NhP8MTJzT5/tmp/fc4b361b8737aaa2c4b240784c16fbbe712a939323495ccbf171d57485995d4a.jpg", "img_caption": ["Figure 3: Visualizations of geometric controlled generation. Top row: Fusion of 3D semantic MPI. Bottom row: our generation concatenated from neighboring views. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "176 3.4 Cross-View and Cross-Frame Attention ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "177 The sensor arrangement in a self-driving car usually requires a full surround view of cameras to capture   \n178 the entire 360-degree environment. To effectively simulate the multi-view and subsequent multi-frame   \n179 generation, zero-initialized [41] cross-view and cross-frame attention are integrated into the diffusion   \n180 model to maintain consistency between views and frames. Following prior work [5, 28, 30, 31],   \n181 each cross-view attention allows the target view to access information from its neighboring left and   \n182 right views, thus training cross-view attention using multi-view consistent images will enforce it to   \n183 generate the same instance in the overlapping region of multi-view cameras. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathsf{A t t e n t\\,i o n}(Q,K,V)=\\mathsf{s o f t m a x}(\\frac{Q K^{T}}{\\sqrt{d}})\\cdot V,}\\\\ {h_{o u t}=h_{i n}+\\sum_{i\\in\\{l,r\\}}\\!\\mathsf{A t t e n t\\,i o n}(Q_{i n},K_{i},V_{i}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "184 where $l$ , and $r$ is the camera view of left and right. $Q_{i n}$ and $h_{i n}$ denotes the query and the hidden   \n185 state of input view. Similarly, we add cross-frame attention that attend previous frame and future   \n186 frame to enable video generation. In this case, we use the same formulation while $i\\in\\{f,h\\}$ , where   \n187 $f$ and $h$ is the camera view of future and history frames. ", "page_idx": 4}, {"type": "text", "text": "188 3.5 Importance Reweighing ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "189 To deal with the extreme imbalance problem between   \n190 foreground, background, and object categories, and   \n191 also to ease the training, we propose three types of   \n192 reweighting methods to improve the generation quality   \n193 of foreground objects.   \n194 Progressive Foreground Enhancement To miti  \n195 gate the complexity of the learning task, we propose a   \n196 progressive reweighting method that incrementally en  \n197 hances the loss associated with the foreground regions   \n198 (based on semantic class) as the training progresses.   \n199 The detailed formulation is: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "image", "img_path": "NhP8MTJzT5/tmp/6815618a43324eaab6eb8bfead541b8e21ef4f3021d57d67286e05d3c96ad33c.jpg", "img_caption": ["Figure 4: Visualizations of the reweighing function in Eq. 6. "], "img_footnote": [], "page_idx": 4}, {"type": "equation", "text": "$$\nw(x,m,n)=\\frac{(m-1)}{2}\\cdot(1+\\cos(\\frac{x}{n}\\cdot\\pi+\\pi))+1,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "image", "img_path": "NhP8MTJzT5/tmp/21fcd71f5203d366e8588836b9385d9764123e646d2adbf576dfb85edb2c2a99.jpg", "img_caption": ["Figure 5: Visualizations of generated multi-view images. The generation conditions (occupancy labels) are from nuScenes validation set. We highlight that (i) Geometry alignment of trees in red rectangle in (b). (ii) Use text prompt to control high-level appearance in (c,d). ", "(d) Generation3, Style control: Minecraft style (top) and Diablo style (bottom) "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "200 where $x$ is the current training step, $m$ is the maximum value of weights that set at 2, and $n$ is the   \n201 total training steps. This approach is engineered to facilitate a learning trajectory that progresses   \n202 from simplicity to complexity, thereby aiding in the convergence of the model. This curve can be   \n203 interpreted as a cosine annealing but inverted to amplify the importance of the foreground region.   \n204 Depth-aware Foreground Reweighing In the meantime, we acknowledge the learning difficulty   \n205 in different depth places in 3D scenes. Following GeoDiffusion [3], we perform depth reweighing to   \n206 foreground objects by adaptively assigning higher weights to farther foreground areas. This enables   \n207 the model to focus more thoroughly on hard examples with depth-aware importance reweighting.   \n208 Instead of using their exponential function to increase weights, we use our designed cosine function   \n209 Eq. 6 for stability. Here $x$ is the input depth value, and $n$ is the maximum depth that set at 50.   \n210 CBGS Sampling To deal with the class imbalance problem in driving scenarios, where cer  \n211 tain object categories appear infrequently, we employ the Class-Balanced Grouping and Sampling   \n212 (CBGS) [44] to better handle the long-tailed classes. CBGS addresses the challenge of class imbal  \n213 ance by grouping and re-sampling training data to ensure each group has a balanced distribution of   \n214 sample frequency across different object categories. This method reduces the bias towards more   \n215 frequent classes and enables better generalization to rare scenarios. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "216 3.6 Model Training ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "217 To ease the training of the MPI encoder and added attention module, we use a two stage training   \n218 pipeline. We first train MPI encoder and cross-view attention in a multi-view image generation setting.   \n219 Then we train cross-frame attention and freeze other components in a video generation setting.   \n220 Objective Function Our final objective function can be formulated as a standard denoising   \n221 objective with reweighing: ", "page_idx": 5}, {"type": "table", "img_path": "NhP8MTJzT5/tmp/49ccd44c940edf8e07fe05a18a9eabf151a6fdf60d0244e1948497eb40feac13.jpg", "table_caption": [], "table_footnote": ["Table 1: Downstream evaluation on the nuScenes-Occupancy validation set. Based on the used train and val data, two types of settings are reported. The first is to use generated training set to augment the real training set, and evaluate on the real validation set, denoted as Aug. The second is to use pretrained models trained on the real training datasets to test on the generated validation set, denoted as Gen. "], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\mathbb{E}_{\\mathcal{E}(x),\\epsilon,t}\\|\\epsilon-\\epsilon_{\\theta}\\bigl(z_{t},t,\\tau_{\\theta}(y)\\bigr)\\|^{2}\\odot w,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "222 where $w$ is the multiplication of progressive reweighing and depth-aware reweighing. ", "page_idx": 6}, {"type": "text", "text": "223 4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "224 4.1 Dataset and Setups ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "225 We conduct our experiments on the nuScenes dataset [2], which is collected using 6 surrounded-view   \n226 cameras that cover the full $360^{\\circ}$ field of view around the ego-vehicle. It contains 700 scenes for   \n227 training and 150 scenes for validation. We resize the original image from $1600\\times900$ to $800\\times448$ for   \n228 training. In our work, we use the occupancy label with a resolution of $0.2m$ from OpenOccupancy [27]   \n229 as condition input, while the benchmark of occupancy prediction uses a resolution of $0.4m$ from   \n230 Occ3D [24] dataset for its popularity.   \n231 Networks We use Stable Diffusion [21] v2.1 checkpoint as initialization and only train occupancy   \n232 encoder, cross-view attention. We additionally add cross-frame attention if in video experiments. We   \n233 adopt FB-Occ [15] as the target model for occupancy prediction for its SOTA performance in this task.   \n234 The pretrained checkpoint of the network is obtained from their official repository. Since FB-Occ   \n235 predicts occupancy using only single frame images, we thus train SyntheOcc without cross-frame   \n236 attention in related experiments. For video generation, we provide experimental results in appendix.   \n237 Metrics We use Frechet Inception Distance (FID) [6] to measure the perceptual quality of generated   \n238 images, and use mIoU to measure the precision of occupancy prediction.   \n239 Hyperparameters We set $D=256$ , $d_{m i n}=0$ and $d_{m a x}=50$ . The depth resolution of MPI is   \n240 thus higher than occupancy voxel. We train our model in 6 epochs with batch size $=8$ . The learning   \n241 rate is set at $2e^{-5}$ . The training phase takes around 1 day using 8 NVIDIA A100 80G GPUs. We use   \n242 UniPC scheduler [42] with the classifier-free guidance (CFG) [7] that is set as 7.0. During inference,   \n243 we use 20 denoising steps for dataset generation.   \n244 Baselines We compare our method with prior methods in Tab. 1. ControlNet denotes we train   \n245 a ControlNet using an RGB semantic mask as the condition. ControlNet+depth denotes we add a   \n246 depth channel after the semantic mask to provide 2.5D depth information. The depth map rendered   \n247 by occupancy is normalized to [0-255] to accommodate the RGB value. The ControlNet+depth can   \n248 be regarded as a degradation of SytheOcc which is reduced to a single plane. Then we evaluate   \n249 MagicDrive since it is the only open-sourced method in this area. MagicDrive separately encodes   \n250 foreground and background using prompt and BEV layout. Furthermore, we evaluate the image   \n251 quality (FID [6]) of our method in Tab. 2. Compared with prior methods, we use a unified 3D   \n252 representation that seamlessly handles foreground and background, surpassing them by a large margin. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "253 4.2 Qualitative Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "254 High-level Control using Prompt In Fig. 5 (c,d) and Fig. 6 (c), we demonstrate the capability   \n255 to employ user-defined prompts to generate images with specific weather conditions and high-level   \n256 style. Although the nuScenes dataset doesn\u2019t contain rare weather images like snow and sandstorms,   \n257 our method successfully conveys prior knowledge pretrained from stable diffusion to our scenes.   \n258 Compared with visualization results in prior work like Fig. 8 of MagicDrive, our method shows better   \n259 alignment with the text prompt, demonstrating the cross-domain generalization ability of our method.   \n260 3D Geometric Control Our flexible framework enables us to create novel scenes by manipulating   \n261 voxels as displayed in Fig. 1 and Fig. 3. Basically, we can edit the occupied state and semantics of   \n262 every voxel in our scenes for generation. We highlight that we can create a hinged-articulated truck   \n263 and an excavator as shown in Fig. 6 (d,e). The generated excavator image exhibits a remarkable   \n264 alignment with the input occupancy that is delineated by a black outline.   \n265 Long-tailed Scene Generation The flexibility of 3D semantic MPI has conferred significant   \n266 advantages upon our approach. In the following, we create long-tail scenes that rarely occur in   \n267 our real world for evaluation. In Fig. 1, we show that we manually add parallel traffic cones in   \n268 front of the ego vehicle. This scene has never happened in the training dataset, but our geometric   \n269 controllability provides us the capability to create such data. We then use the created scene to test   \n270 autonomous driving systems such as end-to-end planner VAD [9] to validate its effectiveness. In   \n271 this case, VAD successfully predicts correct waypoints with the high-level command \u2018turn left\u2019.   \n272 Moreover, in appendix Sec. B, we generate long-tailed scenes with extreme weather such as snow   \n273 and sandstorms, and evaluate perception model on it to examine its generalizability of rare weather.   \n274 Comparison with Baselines In Fig. 6 (a), we visualize a comparison with ControlNet. We find   \n275 that ControlNet struggles to distinguish the overlapping instances in 2D-pixel space. This leads to the   \n276 two parked cars being merged into a single car with incorrect pose. In contrast, our 3D semantic MPIs   \n277 contain more than 2D semantic mask, but also account for complete scene geometry with occluded   \n278 parts. Together with our proposed MPI encoder and reweighing strategy, our framework yields a   \n279 realistic image generation with high-quality label alignment. More comparison is provided in Sec. D. ", "page_idx": 6}, {"type": "image", "img_path": "NhP8MTJzT5/tmp/69af6adc9e6b730b6c742190ba1b0470f8e22a94bffa05268b584ca41688ab37.jpg", "img_caption": ["Figure 6: Top row: Comparison with ControlNet. We achieve a precise alignment between conditional labels and synthesized images, while ControlNet generates objects with incorrect pose due to ambiguous 2D condition. Mid and Bottom row: Visualizations of geometry-controlled image generation. We can faithfully generate objects with the desired topology in a specific 3D position. ", "Table 2: Comparison of FID with previous methods on the nuScenes dataset. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "table", "img_path": "NhP8MTJzT5/tmp/8976c1c5976da2b44a67f5112cfb298b32f9d7f9a6cd3d920d6e2ffaf6839b83.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "NhP8MTJzT5/tmp/b37c0d04ef6494c5b1c17867e4769f0e562cef2f6e822c42c2eb021585af4fca.jpg", "table_caption": [], "table_footnote": ["Table 3: Ablation of different designs of the MPI encoder and reweighing methods. "], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "280 4.3 Quantitative Results ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "281 Recognizability, Realism and Controllability Evaluation To evaluate whether our generated   \n282 images aligned with given annotations, we provide Gen experiment in Tab. 1. Using the annotation of   \n283 val set, we synthesize a copy of val set\u2019s images, then use perception model trained on real training set   \n284 to perform evaluation. The performance will be more effective as it is close to the oracle performance.   \n285 We find that local method (ControlNet) perform better than global method (MagicDrive). Furthermore,   \n286 SytheOcc generalizes the locality for 3D conditioning to yield better performance.   \n287 Data Augmentation for 3D Occupancy Prediction Notably, we conduct experiments using our   \n288 synthesized dataset to enhance the real training set in Tab. 1. We first use the occupancy labels from   \n289 training set to create a synthetic training set. Then we modify the loading pipeline in perception model   \n290 to randomly sample images from real dataset or synthetic dataset and train network from scratch.   \n291 Therefore, our approach preserves the inherent training dynamics of the neural network by solely   \n292 modifying the training images, without any alteration to the number of training iterations or epochs.   \n293 As MagicDrive-Aug exhibits numerical overflow when training FB-Occ, which may attributed to   \n294 unsatisfactory recognizability, we have to omit it and only provide MagicDrive-Gen experiments.   \n295 As shown in Tab. 1, where SytheOcc-Aug denotes the augmentation experiments using our generated   \n296 dataset, shows a satisfactory improvement over the prior state of the art. We emphasize that surpassing   \n297 the performance of the original dataset is not the primary objective of our work; rather, it is an   \n298 ancillary benefit that emerges from our framework for geometry-controlled generation.   \n299 Ablations In Tab. 3, we present ablation studies across several design spaces of our model, analo  \n300 gous to the Gen experiment in Tab. 1. We find that our designed MPI encoder of $1\\!\\times\\!1$ conv have sig  \n301 nificant improvement when compared to the conventional $3{\\times}3$ conv approach. Besides, our proposed   \n302 three types of reweighing methods demonstrate a consistent improvement over the baseline. As a   \n303 result, the improved image quality and label alignment enable higher precision in downstream tasks. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "304 5 Limitation and Broader Impacts ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "305 Layout Genereation Our method is restricted in a conditional generation framework that should   \n306 have a conditional input at first. Our condition signal is from the original dataset annotation. Thus   \n307 most of the augmented data is generated using the same occupancy layout, or with minimal human   \n308 editing. Future research can incorporate the recent research [10,17,32,40] that generates occupancy   \n309 descriptions of the scenes to synthesize images with novel occupancy layouts.   \n310 Closed-loop Simulation Given the underlying diverse and controllable image generation of our   \n311 method, it would be advantageous and valuable to extend our work to a broader domain such as closed  \n312 loop simulation [16,38], to enable high-fidelity autonomous systems testing. This line of work can   \n313 be conducted by utilizing motion conditions to generate future frames as in world model [17,28,36],   \n314 or by explicitly modeling scene graph as in the case of UniSim [20,38] and NeuroNCAP [16].   \n315 Long-tailed Scene Generation In this paper, we only investigate a limited number of long-tailed   \n316 scene generation and corner case evaluations such as rare layout in Fig. 1 and extreme weather in   \n317 Sec. B. Future work can extend our framework to (i) Synthesize more samples for tail classes to boost   \n318 performance. (ii) Generate or replicate large-scale databases of corner cases [11] for robust perception. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "319 6 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "320 In this paper, we propose SytheOcc, an innovative image generation framework that is empowered   \n321 with geometry-controlled capabilities using occupancy. We introduce a novel 3D representation,   \n322 3D semantic MPIs, to address the critical challenge of how to efficiently encode occupancy. This   \n323 representation not only preserves the authentic and complete 3D geometry details with semantics, but   \n324 also provides a spatial-align feature representation for 2D diffusion models. With this property, our   \n325 method enjoys photorealistic appearances and fine-grained 3D controllability, serves as a generative   \n326 data engine to enable a broad range of applications. Extensive experiments demonstrate that our   \n327 synthetic data facilitate the training for perception models on occupancy prediction, and provide   \n328 valuable corner case evaluation in a simulated world. ", "page_idx": 8}, {"type": "text", "text": "329 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "330 [1] Abien Fred Agarap. Deep learning using rectified linear units (relu). arXiv preprint arXiv:1803.08375,   \n331 2018. 5   \n332 [2] Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan,   \n333 Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal dataset for autonomous driving.   \n334 In CVPR, 2020. 7   \n335 [3] Kai Chen, Enze Xie, Zhe Chen, Lanqing Hong, Zhenguo Li, and Dit-Yan Yeung. Integrating geometric   \n336 control into text-to-image diffusion models for high-quality detection data generation via text prompt.   \n337 arXiv preprint arXiv:2306.04607, 2023. 3, 6   \n338 [4] Jaeyoung Chung, Suyoung Lee, Hyeongjin Nam, Jaerin Lee, and Kyoung Mu Lee. Luciddreamer: Domain  \n339 free generation of 3d gaussian splatting scenes. arXiv preprint arXiv:2311.13384, 2023. 12   \n340 [5] Ruiyuan Gao, Kai Chen, Enze Xie, Lanqing Hong, Zhenguo Li, Dit-Yan Yeung, and Qiang Xu. Magicdrive:   \n341 Street view generation with diverse 3d geometry control. In ICLR, 2024. 1, 3, 5, 8, 14   \n342 [6] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans   \n343 trained by a two time-scale update rule converge to a local nash equilibrium. NeurIPS, 2017. 7   \n344 [7] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint:2207.12598, 2022. 7   \n345 [8] Lukas H\u00f6llein, Ang Cao, Andrew Owens, Justin Johnson, and Matthias Nie\u00dfner. Text2room: Extracting   \n346 textured 3d meshes from 2d text-to-image models. In ICCV, 2023. 12   \n347 [9] Bo Jiang, Shaoyu Chen, Qing Xu, Bencheng Liao, Jiajie Chen, Helong Zhou, Qian Zhang, Wenyu Liu,   \n348 Chang Huang, and Xinggang Wang. Vad: Vectorized scene representation for efficient autonomous driving.   \n349 In ICCV, 2023. 2, 8, 12, 13   \n350 [10] Jumin Lee, Sebin Lee, Changho Jo, Woobin Im, Juhyeong Seon, and Sung-Eui Yoon. Semcity: Semantic   \n351 scene generation with triplane diffusion. arXiv preprint arXiv:2403.07773, 2024. 9   \n352 [11] Kaican Li, Kai Chen, Haoyu Wang, Lanqing Hong, Chaoqiang Ye, Jianhua Han, Yukuai Chen, Wei Zhang,   \n353 Chunjing Xu, Dit-Yan Yeung, et al. Coda: A real-world road corner case dataset for object detection in   \n354 autonomous driving. In ECCV, 2022. 9   \n355 [12] Leheng Li, Qing Lian, Luozhou Wang, Ningning Ma, and Ying-Cong Chen. Lift3d: Synthesize 3d training   \n356 data by lifting 2d gan to 3d generative radiance field. In CVPR, 2023. 1, 3   \n357 [13] Xiaofan Li, Yifu Zhang, and Xiaoqing Ye. Drivingdiffusion: Layout-guided multi-view driving scene   \n358 video generation with latent diffusion model. arXiv preprint arXiv:2310.07771, 2023. 3   \n359 [14] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and   \n360 Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. In CVPR, 2023. 2   \n361 [15] Zhiqi Li, Zhiding Yu, David Austin, Mingsheng Fang, Shiyi Lan, Jan Kautz, and Jose M Alvarez.   \n362 Fb-occ: 3d occupancy prediction based on forward-backward view transformation. arXiv preprint   \n363 arXiv:2307.01492, 2023. 7   \n364 [16] William Ljungbergh, Adam Tonderski, Joakim Johnander, Holger Caesar, Kalle \u00c5str\u00f6m, Michael Felsberg,   \n365 and Christoffer Petersson. Neuroncap: Photorealistic closed-loop safety testing for autonomous driving.   \n366 arXiv preprint arXiv:2404.07762, 2024. 9   \n367 [17] Jiachen Lu, Ze Huang, Jiahui Zhang, Zeyu Yang, and Li Zhang. Wovogen: World volume-aware diffusion   \n368 for controllable multi-camera driving scene generation. arXiv preprint arXiv:2312.02934, 2023. 3, 9   \n369 [18] Jianbiao Mei, Yu Yang, Mengmeng Wang, Tianxin Huang, Xuemeng Yang, and Yong Liu. Ssc-rs: Elevate   \n370 lidar semantic scene completion with representation separation and bev fusion. In IROS, 2023. 3   \n371 [19] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren   \n372 Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In ECCV, 2020. 1   \n373 [20] Julian Ost, Fahim Mannan, Nils Thuerey, Julian Knodt, and Felix Heide. Neural scene graphs for dynamic   \n374 scenes. In CVPR, 2021. 9   \n375 [21] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution   \n376 image synthesis with latent diffusion models. In CVPR, 2022. 3, 7   \n377 [22] Liangchen Song, Liangliang Cao, Hongyu Xu, Kai Kang, Feng Tang, Junsong Yuan, and Yang Zhao.   \n378 Roomdreamer: Text-driven 3d indoor scene synthesis with coherent geometry and texture. arXiv preprint   \n379 arXiv:2305.11337, 2023. 12   \n380 [23] Alexander Swerdlow, Runsheng Xu, and Bolei Zhou. Street-view image generation from a bird\u2019s-eye view   \n381 layout. IEEE RAL, 2024. 1, 3, 8 ", "page_idx": 9}, {"type": "text", "text": "382 [24] Xiaoyu Tian, Tao Jiang, Longfei Yun, Yucheng Mao, Huitong Yang, Yue Wang, Yilun Wang, and Hang   \n383 Zhao. Occ3d: A large-scale 3d occupancy prediction benchmark for autonomous driving. NeurIPS, 2024.   \n384 1, 3, 7   \n385 [25] Wenwen Tong, Chonghao Sima, Tai Wang, Li Chen, Silei Wu, Hanming Deng, Yi Gu, Lewei Lu, Ping   \n386 Luo, Dahua Lin, et al. Scene as occupancy. In ICCV, 2023. 3 [26] Xiaofeng Wang, Zheng Zhu, Guan Huang, Xinze Chen, and Jiwen Lu. Drivedreamer: Towards real-world  \n388 driven world models for autonomous driving. arXiv preprint arXiv:2309.09777, 2023. 3, 8 [27] Xiaofeng Wang, Zheng Zhu, Wenbo Xu, Yunpeng Zhang, Yi Wei, Xu Chi, Yun Ye, Dalong Du, Jiwen   \n390 Lu, and Xingang Wang. Openoccupancy: A large scale benchmark for surrounding semantic occupancy   \n391 perception. In ICCV, 2023. 1, 3, 7 [28] Yuqi Wang, Jiawei He, Lue Fan, Hongxin Li, Yuntao Chen, and Zhaoxiang Zhang. Driving into the future:   \n393 Multiview visual forecasting and planning with world model for autonomous driving. arXiv preprint   \n394 arXiv:2311.17918, 2023. 3, 5, 9   \n395 [29] Yi Wei, Linqing Zhao, Wenzhao Zheng, Zheng Zhu, Jie Zhou, and Jiwen Lu. Surroundocc: Multi-camera   \n396 3d occupancy prediction for autonomous driving. In ICCV, 2023. 3 [30] Yuqing Wen, Yucheng Zhao, Yingfei Liu, Fan Jia, Yanhui Wang, Chong Luo, Chi Zhang, Tiancai Wang,   \n398 Xiaoyan Sun, and Xiangyu Zhang. Panacea: Panoramic and controllable video generation for autonomous   \n399 driving. arXiv preprint arXiv:2311.16813, 2023. 1, 3, 5, 8   \n400 [31] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying   \n401 Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for   \n402 text-to-video generation. In ICCV, 2023. 5, 14 [32] Zhennan Wu, Yang Li, Han Yan, Taizhang Shang, Weixuan Sun, Senbo Wang, Ruikai Cui, Weizhe Liu,   \n404 Hiroyuki Sato, Hongdong Li, et al. Blockfusion: Expandable 3d scene generation using latent tri-plane   \n405 extrapolation. arXiv preprint arXiv:2401.17053, 2024. 9 [33] Xu Yan, Jiantao Gao, Jie Li, Ruimao Zhang, Zhen Li, Rui Huang, and Shuguang Cui. Sparse single sweep   \n407 lidar point cloud segmentation via learning contextual shape priors from scene completion. In AAAI, 2021.   \n408 3 [34] Kairui Yang, Enhui Ma, Jibin Peng, Qing Guo, Di Lin, and Kaicheng Yu. Bevcontrol: Accurately   \n410 controlling street-view elements with multi-perspective consistency via bev sketch layout. arXiv preprint   \n411 arXiv:2308.01661, 2023. 3, 8 [35] Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Wentao Zhang, Bin Cui,   \n413 and Ming-Hsuan Yang. Diffusion models: A comprehensive survey of methods and applications. ACM   \n414 Computing Surveys, 2023. 3 [36] Mengjiao Yang, Yilun Du, Kamyar Ghasemipour, Jonathan Tompson, Dale Schuurmans, and Pieter Abbeel.   \n416 Learning interactive real-world simulators. arXiv preprint arXiv:2310.06114, 2023. 9 [37] Yuanbo Yang, Yifei Yang, Hanlei Guo, Rong Xiong, Yue Wang, and Yiyi Liao. Urbangiraffe: Representing   \n418 urban scenes as compositional generative neural feature fields. In ICCV, 2023. 3 [38] Ze Yang, Yun Chen, Jingkang Wang, Sivabalan Manivasagam, Wei-Chiu Ma, Anqi Joyce Yang, and Raquel   \n420 Urtasun. Unisim: A neural closed-loop sensor simulator. In CVPR, 2023. 9 [39] Hong-Xing Yu, Haoyi Duan, Junhwa Hur, Kyle Sargent, Michael Rubinstein, William T Freeman, Forrester   \n422 Cole, Deqing Sun, Noah Snavely, Jiajun Wu, et al. Wonderjourney: Going from anywhere to everywhere.   \n423 arXiv preprint arXiv:2312.03884, 2023. 12   \n424 [40] Junge Zhang, Qihang Zhang, Li Zhang, Ramana Rao Kompella, Gaowen Liu, and Bolei Zhou. Urban   \n425 scene diffusion through semantic occupancy map. arXiv preprint arXiv:2403.11697, 2024. 9 [41] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion   \n427 models. In ICCV, 2023. 2, 3, 5   \n428 [42] Wenliang Zhao, Lujia Bai, Yongming Rao, Jie Zhou, and Jiwen Lu. Unipc: A unified predictor-corrector   \n429 framework for fast sampling of diffusion models. NeurIPS, 2023. 7   \n430 [43] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. Stereo magnification:   \n431 Learning view synthesis using multiplane images. arXiv preprint arXiv:1805.09817, 2018. 2, 4   \n432 [44] Benjin Zhu, Zhengkai Jiang, Xiangxin Zhou, Zeming Li, and Gang Yu. Class-balanced grouping and   \n433 sampling for point cloud 3d object detection. arXiv preprint arXiv:1908.09492, 2019. 6 ", "page_idx": 10}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 11}, {"type": "table", "img_path": "NhP8MTJzT5/tmp/a5bc6f7101493449a5d56db89017b7eb7030bcf497d3786105b87f94551c3da2.jpg", "table_caption": ["435 In the appendix, we provide the following content: "], "table_footnote": [], "page_idx": 11}, {"type": "text", "text": "436 A Statement of Geometric Control ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "437 In our paper, we refer the geometric controllable generation as using a voxel grid in 3D space to   \n438 control the image generation. Although the voxel is a quantized representation of the 3D world,   \n439 when the resolution goes larger, it can already faithfully represent the geometry detail of scenes.   \n440 Currently, we are limited by the precision of ground truth labels. The $0.2m$ occupancy grid is a tensor   \n441 of $500{\\times}500{\\times}40$ that cover a space in $\\mathbf{X}$ -axis spanning $[-50m,50m]$ , y-axis spanning $\\left[-50m,50m\\right]$ ,   \n442 z-axis spanning $[-5m,3m]$ . In the future, we plan to explore a higher resolution of geometric control   \n443 to refine our generation.   \n444 Except for occupancy, several other 3D representations can be expressed by 3D semantic MPI,   \n445 such as mesh, dense point clouds, and even 3D boxes or HD maps. The underlying mechanism is   \n446 to cast several slices of multi-plane images at different depths to retrieve geometric information.   \n447 Thus, our 3D semantic MPI can be regarded as a general 3D conditioning representation to benefit   \n448 a wide spectrum of practical systems. These encompass but are not limited to 3D generation such   \n449 as text2room [8], RoomDreamer [22], WonderJourney [39], and LucidDreamer [4], each of which   \n450 stands to benefit from the rich geometric context provided by our approach. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "451 B Long-Tailed Scene Evaluation ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "452 In this section, we explore to use SytheOcc to create long-tailed scenes for downstream evaluation.   \n453 This also stands for evaluating our model using several corner cases. Similar to the SytheOcc-Gen   \n454 experiment in Tab. 1, we generate a synthetic validation set but use prompts control to manipulate   \n455 weather patterns or the intensity of illumination.   \n456 As depicted in Fig. 7. We create a variety of weather conditions including sandstorms, snow, foggy,   \n457 rainy, day night, and day time. The motivation behind the creation of these scenes lies in their extreme   \n458 rarity compared to the ordinary scenes we have captured. The generation of such data is of significant   \n459 value, as it aids in addressing the long-tailed distribution of scenes, thereby enriching the diversity of   \n460 our dataset. More visualization is provided in Fig. 13 to Fig. 14.   \n461 In Tab. 4, we observe that all kinds of extreme weather lead to a degradation in performance. This   \n462 observation underscores the limitations of the perception model in terms of its generalizability to   \n463 infrequent weather scenarios. Among them, we find that foggy, rainy, and day night exert the most   \n464 severe impact, as they contribute to a large reduction in visibility as shown in Fig. 7. To improve the   \n465 generalizability to handle various weather conditions, future work can leverage our generated data to   \n466 cover the long-tailed scenes, or use adversarial search to find severe scenes based on our framework.   \n467 Furthermore, we perform long-tailed scene evaluation in Fig. 8. We display the failure of the   \n468 downstream model VAD [9] in our synthetic long-tailed scene. In this case, we simulate a foggy   \n469 environment that the dense fog obscures the majority of the ego view. Our experiment reveals that   \n470 due to the lack of training images of foggy scenes, VAD erroneously predicts waypoints that would   \n471 result in a collision with the bus. This experiment elucidates the boundaries and failure cases of the   \n472 VAD model [9]. It exposes the limitations of the system under certain conditions, thereby providing   \n473 insights into scenarios where the model\u2019s performance may be compromised. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "table", "img_path": "NhP8MTJzT5/tmp/c2f7e2fdccf1e1b464fe6b2e6d34e86f6888351ee6c4bffd584a6107b5138075.jpg", "table_caption": ["Table 4: Experiments of downstream evaluation on long-tailed scenes with extreme weather. "], "table_footnote": [], "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "image", "img_path": "NhP8MTJzT5/tmp/abed4c69c7049988772f1623142bb64918ee66f84664dfb9d84355af651a3bc6.jpg", "img_caption": ["Figure 7: From top to bottom, we display images of fusion of 3D semantic MPI, synthesized images of sandstorm, snow, foggy, rainy, day night, day time, and ground truth. "], "img_footnote": [], "page_idx": 12}, {"type": "image", "img_path": "NhP8MTJzT5/tmp/cd249b21d2d1481496b81a1ac1e9443318cae707faedbaa0ebed5417ad8956b0.jpg", "img_caption": ["Figure 8: Use SytheOcc to create long-tailed scenes for testing. Top: In the ordinary scene of a bus placed in front of the ego vehicle, the end-to-end planner VAD [9] predicts future waypoints without movement, thus not plotted in the image. Bottom: By harnessing the prompt-level control in our framework, we simulate a scene with the same layout but filled with fog. VAD predicts wrong waypoints that will collide with the bus. "], "img_footnote": [], "page_idx": 12}, {"type": "text", "text": "474 C Ablation of plane number of MPIs ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "475 In our proposed 3D semantic MPIs, the number of planes is a hyperparameter that affects the precision   \n476 of 3D representation. The plane number can be regarded as the 3D resolution in depth axis. The   \n477 larger the plane number, the MPI will contain more details. We find that an increase in the number of   \n478 planes is associated with improved accuracy in downstream tasks. This finding denotes that more   \n479 condition information leads to better downstream task performance. ", "page_idx": 12}, {"type": "image", "img_path": "NhP8MTJzT5/tmp/893f6a0e219ac449d6dee57c6e79b8c59e3b0174ae2819b09926bde9db174e8a.jpg", "img_caption": ["Figure 9: Comparison with baselines. ", "Table 5: Ablation of the number of multi-plane images. "], "img_footnote": [], "page_idx": 13}, {"type": "table", "img_path": "NhP8MTJzT5/tmp/27c30cac7a5c138a7ad772193636a355f45c969cebd5385c0e2a8bd41bd63611.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "480 D Qualitative Comparison with Baselines and SOTA ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "481 In Fig. 9, we conduct a qualitative comparison of our method against MagicDrive, ControlNet, and   \n482 ControlNet+depth. We find that all the methods display a satisfactory image quality, as they build upon   \n483 the foundation of the stable diffusion model. The generation of MagicDrive fails to synthesize barriers   \n484 as shown in the bottom row. ControlNet struggles to generate objects with the correct pose solely   \n485 from only 2D conditions as shown in the second row. ControlNet+depth, a degradation of our method,   \n486 an enhancement over ControlNet in terms of alignment, nevertheless suffers from a loss of finer detail   \n487 in scenes with heavy occlusion, as shown in the human of the third row. Our method, in contrast, aims   \n488 to address these challenges and provide a more nuanced and accurate generation of complex scenes. ", "page_idx": 13}, {"type": "text", "text": "489 E Extend to Video Generation ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "490 As described in the main paper Sec. 3.4, we further extend the cross-view attention to cross-frame   \n491 attention to perform video generation. Our generation results are Fig. 11, Fig. 12 and Fig. 16.   \n492 Our implementation is adopted from MagicDrive [5] which is similar to Tune-a-video [31]. The   \n493 formulation of cross-frame attention is: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathtt{A t t e n t i o n}(Q,K,V)=\\mathtt{s o f t m a x}(\\frac{Q K^{T}}{\\sqrt{d}})\\cdot V,}\\\\ {h_{o u t}=h_{i n}+\\sum_{i\\in\\{f,h\\}}\\mathtt{A t t e n t i o n}(Q_{i n},K_{i},V_{i}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "494 where $f$ , and $h$ are the camera view of future and history frames. $Q_{i n}$ and $h_{i n}$ denotes the query and   \n495 the hidden state of input view. We train our model in a two-stage pipeline. We first train the MPI   \n496 encoder and cross-view attention in a multi-view image generation setting. Then we train cross-frame   \n497 attention and freeze other components in a video generation setting.   \n498 In practice, we use the keyframe annotation of the nuScenes dataset to train our video model. We start   \n499 with our pretrained MPI encoder and cross-view attention and only train our cross-frame attention   \n500 while keeping others frozen. We employ a sequence of 7 frames as a batch, resulting in a batch size   \n501 of 42 images for the training process.   \n502 Given that our primary contribution does not lie in video generation, this experiment serves as a   \n503 proof of concept, demonstrating the potential of our framework. Future research may extend our   \n504 methodology to facilitate the generation of longer video sequences, thereby expanding the scope and   \n505 applicability of our framework. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "image", "img_path": "NhP8MTJzT5/tmp/92909c03e91ce6559d5c667ab40f4910e964b1bc1c78c409f3fb8821c55f485e.jpg", "img_caption": ["Figure 10: We demonstrate the generalizability of SytheOcc to new camera intrinsic. We multiply factors to the focal length while keeping the resolution the same. In (b,c), focal length $\\times0.8$ denotes a camera with a larger field of view similar to zoom out, focal length $\\times1.2$ denotes a camera with a smaller field of view similar to zoom in. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "506 F Generalize to New Cameras ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "507 In this section, we investigate the adaptability of our method to a new set of cameras with different   \n508 intrinsic. Given that our training set has a fixed camera intrinsic and extrinsic, generalizing to novel   \n509 cameras indicates that our approach possesses robust generalization capabilities. As shown in Fig. 10,   \n510 benefiting from our local type of condition, SytheOcc generates images that faithfully align with   \n511 the new intrinsic, proving that SytheOcc do not over-fit certain parameters. Regarding extrinsic   \n512 parameters, we can cast our MPI at the desirable locations to retrieve geometric information, thus   \n513 inherently ensuring generalizability without doubt. ", "page_idx": 14}, {"type": "text", "text": "514 G The Influence of the Amount of Augmented Data ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "515 As SytheOcc is capable of generating an infinite number of synthetic data, we investigate the influence   \n516 of the amount of augmented data on downstream tasks in Tab. 6. We find that when our augmented   \n517 data is expanded from one-fold to two-fold of the training dataset, the performance of perception   \n518 model slightly decreases. This may indicate the generated data has an optimal ratio for downstream   \n519 tasks. Due to limited computational resources, we only experiment with a limited amount of ratio.   \n520 Future work can conduct more thorough experiments to find a universal theorem. ", "page_idx": 14}, {"type": "table", "img_path": "NhP8MTJzT5/tmp/605770039e16a76c6ce681cbc2c636c1f64ba1da7f6182f1cc2ff0e8c76da25c.jpg", "table_caption": ["Table 6: Ablation of the amount of augmented data. "], "table_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "NhP8MTJzT5/tmp/288454876f77475d23f495412d9968720e01570bbee7338e3409c7cbe73bc1f9.jpg", "img_caption": ["Figure 11: Video generation results. In the temporal progression, the distant buildings maintain a high degree of consistency, and objects retain their identical shapes and textures across different views and frames. "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "NhP8MTJzT5/tmp/2c13763bf8888fe7992668b41c10889d5f5afb68777ab9c699106eb85003f77b.jpg", "img_caption": ["Figure 12: Video generation results of large dynamics scenes. The white car comes across different views and frames depicting consistent shapes with only a slight appearance change. "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "NhP8MTJzT5/tmp/b07079c09c874634b5f3ecdb92aa730bfb0d3b7efd4953ea67f02337dfad800b.jpg", "img_caption": ["Figure 13: From top to bottom, we display images of fusion of 3D semantic MPI, synthesized images of sandstorm, snow, foggy, rainy, day night, day time, and ground truth. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "NhP8MTJzT5/tmp/08b15f1555ddde46a3ccf87fbe58106f8f6a8dcdd0737368b0a21b48ee52acec.jpg", "img_caption": ["Figure 14: Weather variation. Same structure with Fig. 13. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "NhP8MTJzT5/tmp/6192ea0f948302eccac600dc8ebbfb9d55120a22ff490db95627823cc072af87.jpg", "img_caption": ["Figure 15: Out of distribution generation. We use prompts to control the high-level appearance of images with specific styles. From top to bottom, we display (1) fusion of 3D semantic MPI. (2) Sunny day. (3) Science fiction style. (4) 8-bit pixel art style. (5) Snowfall. (6) Minecraft style. (7) Pok\u00e9mon style. (8) Diablo style. (9) Ghibli style. (10) Metropolis style. (11) Gotham style. (12) Ground truth. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "521 H Failure Cases ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "522 We display several failure cases of our method. In Fig. 16, we show a crowd scenes. In this scenario,   \n523 the excessive number of pedestrians presents a challenge to the cross-view attention and cross-frame   \n524 attention modules. We find our method incapable of discerning individual entities with clarity. Future   \n525 research can improve the model capacity or enrich high-quality data to mitigate this problem.   \n527 The checklist is designed to encourage best practices for responsible machine learning research,   \n528 addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove   \n529 the checklist: The papers not including the checklist will be desk rejected. The checklist should   \n530 follow the references and follow the (optional) supplemental material. The checklist does NOT count   \n531 towards the page limit.   \n532 Please read the checklist guidelines carefully for information on how to answer these questions. For   \n533 each question in the checklist:   \n534 \u2022 You should answer [Yes] , [No] , or [NA] .   \n535 \u2022 [NA] means either that the question is Not Applicable for that particular paper or the   \n536 relevant information is Not Available.   \n537 \u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA).   \n538 The checklist answers are an integral part of your paper submission. They are visible to the   \n539 reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it   \n540 (after eventual revisions) with the final version of your paper, and its final version will be published   \n541 with the paper.   \n542 The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation.   \n543 While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a   \n544 proper justification is given (e.g., \"error bars are not reported because it would be too computationally   \n545 expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering   \n546 \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we   \n547 acknowledge that the true answer is often more nuanced, so please just use your best judgment and   \n548 write a justification to elaborate. All supporting evidence can appear either in the main paper or the   \n549 supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification   \n550 please point to the section(s) where related material for the question can be found. ", "page_idx": 18}, {"type": "image", "img_path": "NhP8MTJzT5/tmp/c3f81e406f9468c23585e7db45e5bf625bf7b4ee87d05807051adc43760f18f7.jpg", "img_caption": ["Figure 16: Failure case of video generation results. Our cross-frame attention module is challenging to distinguish a crowd of people across different views and frames. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "551 IMPORTANT, please: ", "page_idx": 19}, {"type": "text", "text": "552 \u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\",   \n553 \u2022 Keep the checklist subsection headings, questions/answers and guidelines below.   \n554 \u2022 Do not modify the questions and only use the provided macros for your answers.   \n555 1. Claims   \n556 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n557 paper\u2019s contributions and scope?   \n558 Answer: [Yes]   \n559 Justification: Please find this part in Sec. 3.   \n560 Guidelines:   \n561 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n562 made in the paper.   \n563 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n564 contributions made in the paper and important assumptions and limitations. A No or   \n565 NA answer to this question will not be perceived well by the reviewers.   \n566 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n567 much the results can be expected to generalize to other settings.   \n568 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n569 are not attained by the paper.   \n570 2. Limitations   \n571 Question: Does the paper discuss the limitations of the work performed by the authors?   \n572 Answer: [Yes]   \n573 Justification: Please find this part in Sec. 5.   \n574 Guidelines:   \n575 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n576 the paper has limitations, but those are not discussed in the paper.   \n577 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n578 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n579 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n580 model well-specification, asymptotic approximations only holding locally). The authors   \n581 should reflect on how these assumptions might be violated in practice and what the   \n582 implications would be.   \n583 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n584 only tested on a few datasets or with a few runs. In general, empirical results often   \n585 depend on implicit assumptions, which should be articulated.   \n586 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n587 For example, a facial recognition algorithm may perform poorly when image resolution   \n588 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n589 used reliably to provide closed captions for online lectures because it fails to handle   \n590 technical jargon.   \n591 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n592 and how they scale with dataset size.   \n593 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n594 address problems of privacy and fairness.   \n595 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n596 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n597 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n598 judgment and recognize that individual actions in favor of transparency play an impor  \n599 tant role in developing norms that preserve the integrity of the community. Reviewers   \n600 will be specifically instructed to not penalize honesty concerning limitations.   \n601 3. Theory Assumptions and Proofs   \n602 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n603 a complete (and correct) proof?   \n604 Answer: [NA]   \n605 Justification: The paper does not include theoretical results.   \n606 Guidelines: Do not have theoretical results.   \n607 \u2022 The answer NA means that the paper does not include theoretical results.   \n608 \u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross  \n609 referenced.   \n610 \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n611 \u2022 The proofs can either appear in the main paper or the supplemental material, but if   \n612 they appear in the supplemental material, the authors are encouraged to provide a short   \n613 proof sketch to provide intuition.   \n614 \u2022 Inversely, any informal proof provided in the core of the paper should be complemented   \n615 by formal proofs provided in appendix or supplemental material.   \n616 \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced.   \n617 4. Experimental Result Reproducibility   \n618 Question: Does the paper fully disclose all the information needed to reproduce the main   \n619 experimental results of the paper to the extent that it affects the main claims and/or conclu  \n620 sions of the paper (regardless of whether the code and data are provided or not)?   \n621 Answer: [Yes]   \n622 Justification: Please find this part in Sec. 4.   \n623 Guidelines:   \n624 \u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. \u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. \u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. \u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. 5. Open access to data and code Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Please find this part in Sec. 4. Guidelines: \u2022 The answer NA means that paper does not include experiments requiring code. \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. \u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). \u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. \u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. \u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. \u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 21}, {"type": "text", "text": "679 \u2022 Providing as much information as possible in supplemental material (appended to the   \n680 paper) is recommended, but including URLs to data and code is permitted.   \n681 6. Experimental Setting/Details   \n682 Question: Does the paper specify all the training and test details (e.g., data splits, hyperpa  \n683 rameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?   \n684 Answer: [Yes]   \n685 Justification: Please find this part in Sec. 4.   \n686 Guidelines:   \n687 \u2022 The answer NA means that the paper does not include experiments.   \n688 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n689 that is necessary to appreciate the results and make sense of them.   \n690 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n691 material.   \n692 7. Experiment Statistical Significance   \n693 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n694 information about the statistical significance of the experiments?   \n695 Answer: [Yes]   \n696 Justification: Please find this part in Sec. 4.   \n697 Guidelines:   \n698 \u2022 The answer NA means that the paper does not include experiments.   \n699 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n700 dence intervals, or statistical significance tests, at least for the experiments that support   \n701 the main claims of the paper.   \n702 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n703 example, train/test split, initialization, random drawing of some parameter, or overall   \n704 run with given experimental conditions).   \n705 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n706 call to a library function, bootstrap, etc.)   \n707 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n708 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n709 of the mean.   \n710 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n711 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n712 of Normality of errors is not verified.   \n713 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n714 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n715 error rates).   \n716 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n717 they were calculated and reference the corresponding figures or tables in the text.   \n718 8. Experiments Compute Resources   \n719 Question: For each experiment, does the paper provide sufficient information on the com  \n720 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n721 the experiments?   \n722 Answer: [Yes]   \n723 Justification: Please find this part in Sec. 4.   \n724 Guidelines:   \n725 \u2022 The answer NA means that the paper does not include experiments.   \n726 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n727 or cloud provider, including relevant memory and storage.   \n728 \u2022 The paper should provide the amount of compute required for each of the individual   \n729 experimental runs as well as estimate the total compute.   \n730 \u2022 The paper should disclose whether the full research project required more compute   \n731 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n732 didn\u2019t make it into the paper).   \n733 9. Code Of Ethics   \n734 Question: Does the research conducted in the paper conform, in every respect, with the   \n735 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n736 Answer: [Yes]   \n737 Justification: It should be fine.   \n738 Guidelines:   \n739 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n740 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n741 deviation from the Code of Ethics.   \n742 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n743 eration due to laws or regulations in their jurisdiction).   \n744 10. Broader Impacts   \n745 Question: Does the paper discuss both potential positive societal impacts and negative   \n746 societal impacts of the work performed?   \n747 Answer: [Yes]   \n748 Justification: Please find this part in Sec. 5.   \n749 Guidelines:   \n750 \u2022 The answer NA means that there is no societal impact of the work performed.   \n751 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n752 impact or why the paper does not address societal impact.   \n753 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n754 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n755 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n756 groups), privacy considerations, and security considerations.   \n757 \u2022 The conference expects that many papers will be foundational research and not tied   \n758 to particular applications, let alone deployments. However, if there is a direct path to   \n759 any negative applications, the authors should point it out. For example, it is legitimate   \n760 to point out that an improvement in the quality of generative models could be used to   \n761 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n762 that a generic algorithm for optimizing neural networks could enable people to train   \n763 models that generate Deepfakes faster.   \n764 \u2022 The authors should consider possible harms that could arise when the technology is   \n765 being used as intended and functioning correctly, harms that could arise when the   \n766 technology is being used as intended but gives incorrect results, and harms following   \n767 from (intentional or unintentional) misuse of the technology.   \n768 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n769 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n770 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n771 feedback over time, improving the efficiency and accessibility of ML).   \n772 11. Safeguards   \n773 Question: Does the paper describe safeguards that have been put in place for responsible   \n774 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n775 image generators, or scraped datasets)?   \n776 Answer: [NA]   \n777 Justification: Our paper poses no such risks.   \n778 Guidelines:   \n779 \u2022 The answer NA means that the paper poses no such risks.   \n80 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n81 necessary safeguards to allow for controlled use of the model, for example by requiring   \n82 that users adhere to usage guidelines or restrictions to access the model or implementing   \n83 safety filters.   \n84 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n85 should describe how they avoided releasing unsafe images.   \n86 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n87 not require this, but we encourage authors to take this into account and make a best   \n88 faith effort. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "789 12. Licenses for existing assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "790 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n791 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n792 properly respected?   \n793 Answer: [Yes]   \n794 Justification: Please find this part in Sec. 4.   \n795 Guidelines: ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 24}, {"type": "text", "text": "811 13. New Assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "812 Question: Are new assets introduced in the paper well documented and is the documentation   \n813 provided alongside the assets? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 24}, {"type": "text", "text": "825 14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "826 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n827 include the full text of instructions given to participants and screenshots, if applicable, as   \n828 well as details about compensation (if any)?   \n829 Answer: [NA]   \n830 Justification: Our paper does not involve crowdsourcing nor research with human subjects.   \n831   \n832   \n833   \n834   \n835   \n836   \n837   \n838   \n839   \n840   \n841   \n842   \n843   \n844   \n845   \n846   \n847   \n848   \n849   \n850   \n851   \n852   \n853   \n854   \n855   \n856   \n857   \n858 ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. \u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. \u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. \u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 25}]