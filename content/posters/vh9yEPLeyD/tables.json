[{"figure_path": "vh9yEPLeyD/tables/tables_5_1.jpg", "caption": "Table 1: Cross-dataset evaluations (AUC) from FF++ [37] (in-dataset) to CDFv1 [29], CDFv2 [29], DFDCP [17] and DFDC [17] (cross-dataset). C-Avg. denotes the average value of cross-dataset results. The best results are highlighted in bold. Cross-dataset improvements compared with the previous best one are written in pink. See Appendix for comparisons with video-based methods.", "description": "This table presents the Area Under the Curve (AUC) scores for various deepfake detection methods evaluated on multiple datasets.  It compares the performance of different methods on in-dataset (FaceForensics++) and cross-dataset (Celeb-DF-v1, Celeb-DF-v2, DeepFake Detection Challenge Preview, and DeepFake Detection Challenge) evaluations.  The table highlights the best-performing method for each dataset and shows improvements over the previous state-of-the-art.", "section": "4.1 Experimental Setting"}, {"figure_path": "vh9yEPLeyD/tables/tables_6_1.jpg", "caption": "Table 1: Cross-dataset evaluations (AUC) from FF++ [37] (in-dataset) to CDFv1 [29], CDFv2 [29], DFDCP [17] and DFDC [17] (cross-dataset). C-Avg. denotes the average value of cross-dataset results. The best results are highlighted in bold. Cross-dataset improvements compared with the previous best one are written in pink. See Appendix for comparisons with video-based methods.", "description": "This table presents the Area Under the Curve (AUC) scores for various deepfake detection methods on five different datasets.  The methods were trained on the FaceForensics++ (FF++) dataset and tested on four other datasets (Celeb-DF-v1, Celeb-DF-v2, DeepFake Detection Challenge Preview, and DeepFake Detection Challenge). The table highlights the best performing method for each dataset and shows the improvement in cross-dataset performance compared to the previous state-of-the-art.  The average AUC across all cross-dataset evaluations is also included.", "section": "4 Experiments"}, {"figure_path": "vh9yEPLeyD/tables/tables_7_1.jpg", "caption": "Table 2: Ablations for each network component (AUC\u2191 and EER\u2193). All variants are trained on FF++ (in-dataset) and evaluated on other datasets (cross-dataset). BF-only represents using only blendfake data as the negative samples. M-C, M-L, and TB denotes Multi-Class, Multi-Label, and Triplet Binary strategies, respectively.", "description": "This table presents the ablation study results, showing the impact of different components of the proposed method (ProDet) on deepfake detection performance.  It compares the Area Under the Curve (AUC) and Equal Error Rate (EER) metrics across four different datasets (cross-dataset evaluation). The variants include using only blendfake data (BF-only), vanilla hybrid training (VHT), and removing different components of ProDet (w/o Lo, w/o FB, w/o Lt).  It also shows a comparison of three different multi-attribute classification strategies: Multi-Class (M-C), Multi-Label (M-L), and Triplet Binary (TB), highlighting the performance of the proposed TB strategy.", "section": "4.3 Ablation Study"}, {"figure_path": "vh9yEPLeyD/tables/tables_7_2.jpg", "caption": "Table 2: Ablations for each network component (AUC\u2191 and EER\u2193). All variants are trained on FF++ (in-dataset) and evaluated on other datasets (cross-dataset). BF-only represents using only blendfake data as the negative samples. M-C, M-L, and TB denotes Multi-Class, Multi-Label, and Triplet Binary strategies, respectively.", "description": "This table presents the ablation study results. It shows the impact of different components of the proposed method (ProDet) on the deepfake detection performance.  The AUC (Area Under the Curve) and EER (Equal Error Rate) metrics are reported for both in-dataset (FF++) and cross-dataset evaluations.  The table compares the baseline of using only blendfake data (BF-only) and vanilla hybrid training (VHT) with different combinations of the proposed components: Oriented Progressive Regularizer (OPR) with various classification strategies (multi-class, multi-label, triplet binary), feature bridging, and transition loss.  It helps assess the contribution of each component to the overall performance.", "section": "4.3 Ablation Study"}, {"figure_path": "vh9yEPLeyD/tables/tables_9_1.jpg", "caption": "Table 4: The mPD\u2193 and AUC\u2191 for robustness against unseen perturbations on FF++ test set.", "description": "This table presents the results of an ablation study evaluating the robustness of two deepfake detection models (VHT and the proposed model, 'Ours') against three types of unseen perturbations: Block-wise masking, Gaussian noise, and Shifting.  The table shows the mean perturbed distance (mPD) and Area Under the Curve (AUC) for each perturbation type and for both models. Lower mPD values indicate better robustness, while higher AUC values indicate better detection performance. The average mPD and AUC across all perturbation types are also provided.", "section": "4.4 Robustness against Unseen Perturbations"}, {"figure_path": "vh9yEPLeyD/tables/tables_9_2.jpg", "caption": "Table 5: Evaluation on different latent space organizations (AUC). All variants are trained on FF++ (in-dataset) and evaluated on other datasets (cross-dataset).", "description": "This table presents the AUC scores achieved by different latent space organizations on four different datasets.  The methods compared include the unorganized vanilla hybrid training (VHT), R2D2B (Real to Deepfake to Blendfake), Surround (Blendfake and Deepfake surrounding Real), and the proposed R2B2D (Real to Blendfake to Deepfake). The table showcases the performance of each method across various datasets, highlighting the effectiveness of the proposed R2B2D organization in improving the generalization ability of deepfake detectors.", "section": "4.6 Evaluation on Alternative Organized Distribution"}, {"figure_path": "vh9yEPLeyD/tables/tables_14_1.jpg", "caption": "Table 1: Cross-dataset evaluations (AUC) from FF++ [37] (in-dataset) to CDFv1 [29], CDFv2 [29], DFDCP [17] and DFDC [17] (cross-dataset). C-Avg. denotes the average value of cross-dataset results. The best results are highlighted in bold. Cross-dataset improvements compared with the previous best one are written in pink. See Appendix for comparisons with video-based methods.", "description": "This table presents the Area Under the Curve (AUC) scores achieved by various deepfake detection methods on four different datasets: Celeb-DF-v1, Celeb-DF-v2, DeepFake Detection Challenge Preview (DFDCP), and DeepFake Detection Challenge (DFDC).  The methods were initially trained on the FaceForensics++ (FF++) dataset. The table highlights the best performing method for each dataset and indicates improvements compared to the previous state-of-the-art (SOTA).  It also notes that a more detailed comparison with video-based methods can be found in the appendix.", "section": "4.1 Experimental Setting"}, {"figure_path": "vh9yEPLeyD/tables/tables_15_1.jpg", "caption": "Table 7: Generalization evaluations on comprehensive datasets.", "description": "This table presents the cross-dataset generalization performance of various deepfake detection methods, including DF-only, BF-only, VHT, and the proposed ProDet method.  The results are reported as AUC scores for multiple deepfake datasets: DFD, DF1.0, FAVC, WDF, DiffSwap, UniFace, E4S, BlendFace, and MobileSwap. Each entry shows the performance (AUC) on in-dataset and cross-dataset evaluations.  The proposed method consistently outperforms other approaches across all datasets, demonstrating improved generalization capability.", "section": "4.4 Robustness against Unseen Perturbations"}]