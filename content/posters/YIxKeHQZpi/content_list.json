[{"type": "text", "text": "UNIT: Unifying Image and Text Recognition in One Vision Encoder ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yi Zhu1, Yanpeng Zhou1, Chunwei Wang1, Yang $\\mathbf{Cao^{2}}$ , Jianhua $\\mathbf{Han}^{1}$ , Lu Hou1, Hang $\\mathbf{X}\\mathbf{u}^{1}$ ", "page_idx": 0}, {"type": "text", "text": "1Huawei Noah\u2019s Ark Lab, 2Hong Kong University of Science and Technology https://github.com/yeezhu/UNIT. ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Currently, vision encoder models like Vision Transformers (ViTs) typically excel at image recognition tasks but cannot simultaneously support text recognition like human visual recognition. To address this limitation, we propose UNIT, a novel training framework aimed at UNifying Image and Text recognition within a single model. Starting with a vision encoder pre-trained with image recognition tasks, UNIT introduces a lightweight language decoder for predicting text outputs and a lightweight vision decoder to prevent catastrophic forgetting of the original image encoding capabilities. The training process comprises two stages: intra-scale pretraining and inter-scale finetuning. During intra-scale pretraining, UNIT learns unified representations from multi-scale inputs, where images and documents are at their commonly used resolution, to enable fundamental recognition capability. In the inter-scale finetuning stage, the model introduces scale-exchanged data, featuring images and documents at resolutions different from the most commonly used ones, to enhance its scale robustness. Notably, UNIT retains the original vision encoder architecture, making it cost-free in terms of inference and deployment. Experiments across multiple benchmarks confirm that our method significantly outperforms existing methods on document-related tasks (e.g., OCR and DocQA) while maintaining the performances on natural images, demonstrating its ability to substantially enhance text recognition without compromising its core image recognition capabilities. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Vision encoder models [6, 43, 44, 53, 41, 24] are crucial for extracting high-level visual features, thereby enhancing performance across a range of downstream tasks [9, 38, 8, 14, 57]. They play a vital role in integrating visual information into intelligent applications, driving advancements in both computer vision and artificial intelligence. In recent years, Vision Transformers (ViTs)based encoder models [16, 66, 27, 58, 71] have revolutionized the field of computer vision. ViT models pretrained on image classification tasks are extensively used as backbones for a wide array of image recognition tasks and achieves state-of-the-art performances. Another line of remarkable ViT models are trained via image-text cross-modal contrastive learning [43, 64, 52, 67]. These models are often used as plug-in vision encoders in Large-scale Vision-Language Models (LVLMs), which have emerged as versatile tools with the potential to revolutionize various domains, including healthcare diagnostics, autonomous driving, digital assistants, and advanced content analysis in media and entertainment. Despite these advancements, existing vision encoder models, exhibit a significant limitation: they have not yet demonstrated the capability to simultaneously support both image and text recognition like humans do. Such ability is essential for document analysis applications, for instance, a model must accurately recognize and interpret textual information embedded within complex layouts, such as tables, graphs, and mixed media. ", "page_idx": 0}, {"type": "text", "text": "Text recognition [50, 49, 12], particularly in the context of dense documents and complex visual environments, presents unique challenges that are distinct from those encountered in pure image recognition. While image recognition typically focuses on global feature extraction and classification, text recognition requires precise local feature extraction and sequence prediction. ViTs, though adept at handling global image features, often struggle with the detailed local features needed for accurate text recognition, particularly in high-resolution document images where fine details are crucial. A straightforward solution is to finetune pre-trained ViTs using high-resolution documents. However, this approach requires interpolating the pre-trained positional embeddings to handle longer sequences. Such interpolation with a large magnification will disrupt the alignment between the original positional embeddings and their corresponding spatial positions, thereby impacting the model\u2019s performance. ", "page_idx": 1}, {"type": "text", "text": "Existing methods [39, 4, 5] build document-specific models by retraining ViTs exclusively on the Optical Character Recognition (OCR) task, thereby discarding the original image encoding capability. In downstream applications, these models are often ensembled with other expert models, requiring users to specify the data type in advance, which is inadequate for scenarios requiring dynamic and simultaneous recognition of both image and text without prior knowledge of the input content. Other ensemble-based methods [53, 54] use similar model architectures trained independently on image and text recognition tasks. Inputs are fed into both models, and the output features are concatenated to form a unified representation. However, this approach will result in a significant increase in computational cost. Additionally, some LVLM methods [62, 63] enhance document analysis tasks (e.g., DocQA) in an OCR-free manner by finetuning with document-instruction data. Nevertheless, their capabilities are limited to handling images with prominent text and fail with dense documents, struggling to generalize across different font sizes, typefaces, and backgrounds. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we propose UNIT, a novel training framework for UNifying Image and Text recognition abilities within a single model. UNIT upgrades an existing Vision Transformer (ViT) model to effectively integrate text recognition. First, a lightweight language decoder (e.g., OPT-125M) is introduced to decode the learned visual features into text sequences in an auto-regressive manner, enabling the encoder model to capture fine-detailed shape and sequential information necessary for text recognition. Then, a tiny vision decoder (e.g., two MLP layers) is introduced to reconstruct the visual features of the original vision encoder from the newly learned features, preventing catastrophic forgetting of the model\u2019s fundamental encoding ability for natural images. The training pipeline involves two stages. The first is the intra-scale pretraining stage, where the model takes images and documents at their commonly used resolutions as inputs, specifically low-resolution images and high-resolution documents $\\times4$ times the low-resolution). During this stage, the model is optimized with three objectives: OCR for document inputs, feature reconstruction of the original ViT encoder\u2019s features and image captioning for natural image inputs. The second stage is inter-scale finetuning stage, where the model is trained under a scale-exchanged setting with high-resolution images and low-resolution documents, enhancing its scale robustness for both image and text recognition. It is important to note that UNIT retains the original vision encoder architecture, ensuring that there is no increase in inference cost. ", "page_idx": 1}, {"type": "text", "text": "Extensive experiments show that UNIT significantly outperforms document-specific models on OCR tasks while maintaining core image recognition capabilities. When integrated into LVLMs, it also benefits downstream document analysis tasks without degrading image understanding. This demonstrates that UNIT effectively unifies image and text recognition abilities. ", "page_idx": 1}, {"type": "text", "text": "The main contributions of this paper are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose UNIT, a novel framework that unifies image and text recognition in a single vision encoder through joint training of multi-tasks. The model retains the original vision encoder architecture, ensuring cost-free deployment and no increase in inference cost. \u2022 The training paradigm comprises two key stages: an intra-scale pretraining stage, where images and documents are trained at their commonly used resolutions, and an inter-scale finetuning stage, where their resolutions are exchanged to enhance scale robustness. \u2022 Extensive experiments demonstrate that UNIT significantly enhances performance on vision tasks requiring text recognition compared to their counterparts, while preserving the fundamental encoding ability on natural images. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Vision Transformer for Text Recognition. In recent work, the application of Vision Transformers (ViT) to document reading has gained traction due to their success in image recognition [30, 10, 24, 4, 15]. Nougat [5]leverages a ViT model for Optical Character Recognition (OCR) tasks, focusing on converting human-readable scientific documents into a machine-readable markup language. Similarly, KOSMOS-2.5 [19] employs a ViT-based vision encoder coupled with a Transformer-based language decoder, aiming to serve as a versatile tool for diverse text-intensive image understanding tasks through supervised fine-tuning. The Donut model [23], while introducing an OCR-free transformer for visual document understanding, may encounter challenges in generalizing to unfamiliar document types. Despite their demonstrated efficacy in specialized OCR scenarios, these models may not fully harness the original image encoding capabilities present in pre-trained ViT architectures. This limitation can restrict their applicability to text-only images. ", "page_idx": 2}, {"type": "text", "text": "LVLMs for Document Analysis. Several recent studies have explored OCR-free visual-situated language understanding using Large Vision-Language Models (LVLMs) [47, 55, 59, 1, 2, 70, 40, 28], where document-instruction data is used to fine-tune LLMs without adapting the vision encoder [62, 60, 61]. These methods often rely on a pretrained Vision Transformer (ViT) as a fixed vision encoder, which may present several challenges: 1) The frozen ViT may limit text recognition capabilities, especially when documents feature varied fonts, styles, or low image quality not seen during its pretraining. 2) The model\u2019s reliance on a constant image resolution can lead to inaccuracies in text recognition for high-resolution documents or those with noise and distortion. Additionally, several recent works [53, 54] have successfully enhanced LVLMs with OCR-like abilities, by retraining a document-specific Vision Transformer (ViT) and then concatenating its output visual features with those from a pretrained ViT model. However, this method comes with trade-offs, notably an increase in computational expense and a significant underutilization of the model\u2019s capacity, leading to inefficiencies. In contrast, our UNIT integrates both capabilities within a single model, offering greater efficiency, cost-free deployment, and no increase in inference time. ", "page_idx": 2}, {"type": "text", "text": "Multi-scale Vision Transformer. Vision Transformer models are commonly pre-trained to process images at a fixed resolution, such as 224 or 336 pixels, which is commonly used for many image understanding tasks [17, 20, 33, 26, 25]. However, these resolutions might be insufficient for precisely discerning tightly packed text in higher-resolution documents. Consequently, a multi-scale training approach for Vision Transformers is crucial for effectively managing both image and text recognition challenges. The progression of Vision Transformer architectures has embraced a notable trend toward multi-scale modeling. This paradigm enables the models to capture and process visual information across different scales, enhancing their capability to understand complex scenes and texts. Some methods [29, 51, 56] integrate a hierarchical pyramid structure into the original Vision Transformer architectures, which continue to operate at a constant resolution. In contrast to these methods, our proposed UNIT model breaks the limitations of fixed-resolution inputs. UNIT is designed to handle multi-scale training for multi-tasks, yielding a unified representation with scale robustness for both images and documents. ", "page_idx": 2}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Backbone. We employ the Vision Transformer (ViT) [16] architecture as our backbone. Let $f_{\\theta}(\\cdot)$ be the vision encoder with parameters $\\theta$ , for an input image $\\dot{\\mathbf{I}^{\\prime}}\\in\\mathbb{R}^{H\\times W\\times3}$ , where $H$ and $W$ represent the image height and width, respectively. ViT model first partitions each image into fixed-size of patches in $p\\times p$ pixels, and then encode these patches into hidden features of dimension $d$ . Subsequently, the features are added with their corresponding position embeddings and fed into multiple layers of stacked Transformer blocks, interacting with each other via attention mechanisms. Finally the model outputs visual tokens $\\mathbf{X}\\in\\mathbb{R}^{N\\times d}$ , as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{X}=f_{\\theta}(\\mathbf{I})=\\{\\mathbf{x}_{i}\\}_{i=1}^{N},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\begin{array}{r}{N_{s}=\\frac{H}{p}\\,\\times\\,\\frac{W}{p}}\\end{array}$ $N\\,=\\,N_{s}\\,+\\,N_{c}$ asnd thec number of [CLS] tokens denotes the total number of visual tokens, comprising the spatial tokens $N_{c}$ . ", "page_idx": 2}, {"type": "image", "img_path": "YIxKeHQZpi/tmp/615a77f330ace387c0b1398918fe1f0762347fa286c8843db7ea94b49f231aa8.jpg", "img_caption": ["Figure 1: Overview of UNIT Architecture. The model processes high-resolution documents and lowresolution images, generating a set of visual tokens. These tokens pass through an input embedding layer, with document tokens fed into the language decoder to predict text sequences, enhancing the model\u2019s text recognition capability. Simultaneously, to preserve the model\u2019s original image encoding ability, the visual tokens from natural images are reconstructed via a lightweight vision decoder, mimicking the output of the teacher model. Additionally, an image captioning task is included alongside the OCR task to further enhance image understanding. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Architecture. UNIT aims to unify image and text recognition capabilities within a single ViT model, as illustrated in Fig. 1. This is achieved through the introduction of a lightweight language decoder (e.g., OPT-125M) for document-level OCR, enabling the ViT model to recognize text. Additionally, to preserve the original image recognition capabilities of the vision encoder, UNIT incorporates a vision decoder responsible for token-wise feature reconstruction from the original ViT model. To further enhance image understanding, an image captioning task is integrated alongside the OCR task. UNIT is trained in a multi-scale setting, where images and texts are processed at their commonly used resolutions during pretraining and then finetuned at other resolutions. ", "page_idx": 3}, {"type": "text", "text": "3.2 Text Recognition Ability Enhancement ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Multi-Scale Inputs. In contrast to image recognition that focuses on global feature extraction and classification, text recognition requires fine local feature extraction and sequence prediction. ViTs are excellent at global feature extraction but may not perform well in text recognition. The reason is that ViTs use a learned position embedding for each input patch in an image, which in turn forces that the model always operate at a constant resolution, typically chosen as 224, 256 or 336. At these resolutions, the generated visual features may lose critical information, leading to difficulties in recognizing characters from dense texts such as PDF documents, websites, or digital books. Naively cropping high-resolution document images into low-resolution inputs would break the sequential order of characters and may impair the recognition of letters at the cutting boundaries. To prevent these issues, we introduce the Cropped Position Embedding (CPE) [22] augmentation, which interpolates the original position embeddings to match the number of positions of the maximum input size. For low-resolution images, the position embeddings are then randomly cropped and interpolated to match the number of input patches for the original model. ", "page_idx": 3}, {"type": "text", "text": "Language Decoder. Conventional methods often employ CLIP-style contrastive learning to align images with their language annotations. However, this approach presents two notable drawbacks in our context. Firstly, contrastive learning relies on a CLIP pretrained text encoder to generate language embeddings. The maximum sequence length of 77 tokens is insufficient for text recognition annotations, especially in dense documents. Secondly, such training paradigm is inadequate for the vision encoder to accurately capture each word, leading to undesirable information loss during the encoding process. Taking the above factors into account, we introduce a lightweight Transformer decoder, e.g., OPT-125M [69], to efficiently predict the language sequences presented in the input documents. Similar to LVLMs, we employ an input embedding layer to project the visual features from the vision encoder into the embedding space of the language decoder. Here we initialize the input embedding layer with a two-layer Q-Former [25] with $K$ learnable query tokens $\\mathbf{Q}=$ $\\{{\\bf q}_{k}\\}_{k=1}^{\\tilde{K}}$ , $\\mathbf{q}_{k}\\in\\mathbb{R}^{d}$ . The visual tokens $\\mathbf{X}=\\{\\mathbf{x}_{i}\\}_{i=1}^{N}$ generated from the vision encoder are first fed into the input embedding layer and then the language decoder. We denote this process as $f_{\\phi}(\\cdot)$ and $\\phi$ denotes the parameters of the input embedding layer and the language decoder: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\{\\mathbf{z}_{1},...,\\mathbf{z}_{N}\\}=f_{\\phi}(\\{\\mathbf{x}_{1},...,\\mathbf{x}_{N}\\},\\{\\mathbf{q}_{1},...,\\mathbf{q}_{K}\\}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The decoder uses the last hidden state $\\mathbf{z}_{t}^{L}\\in\\mathbb{R}^{d}$ at time step $t$ to predict language sequence in the form of text tokens $\\hat{\\mathbf{y}}=\\{\\hat{y}_{1},\\hat{y}_{2},...,\\hat{y}_{T}\\}$ via the language decoder in an auto-regressive manner. The cross-entropy loss for training the auto-regressive Transformer decoder is defined as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{lan}}(\\mathbf{y},\\hat{\\mathbf{y}})=-\\sum_{t=1}^{T}\\log P(\\hat{y}_{t}=y_{t}|\\mathbf{y}_{1:t-1},\\mathbf{z}_{t}^{L}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbf{y}=\\{y_{1},y_{2},...,y_{T}\\}$ represents the target sequence, and the probability $P(\\hat{y}_{t}=y_{t}|\\mathbf{y}_{1:t-1},\\mathbf{z}_{t}^{L})$ is obtained from the softmax output of the final layer of the decoder. ", "page_idx": 4}, {"type": "text", "text": "3.3 Image Recognition Ability Preservation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Vision Decoder. Finetuning the vision encoder exclusively on document datasets would result in severe catastrophic forgetting of its original image encoding capabilities. To mitigate this, we introduce an token-wise feature reconstruction task on natural image datasets, ensuring that the newly learned vision encoder retains its ability to encode visual concepts. We incorporate a visual decoder $f_{\\pi}(\\cdot)$ consisting of two fully connected layers with a GeLU activation function in the intermediate layer. The decoder processes each visual token independently, preserving the positional information inherent in each token more effectively. We utilize the original pretrained ViT model as a teacher model to provide original features for each natural image, providing supervision signals for visual feature reconstruction. Denoting the teacher tokens as $\\hat{\\mathbf{X}}\\,=\\,\\{\\hat{\\mathbf{x}}_{i}\\}_{i=1}^{N},\\hat{\\mathbf{x}}_{i}\\,\\in\\,\\mathbb{R}^{d}$ , we enforce the alignment of the new learned student tokens with the original ones using a weighted sum of the cosine distance $L_{c o s}$ and smooth L1 loss $L_{l1}$ . This approach ensures consistency in both vector direction and magnitude between the new and original features, as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{vis}}(\\mathbf{X},\\hat{\\mathbf{X}})=\\sum_{i\\in\\mathcal{C}}L_{\\mathrm{cos}}(f_{\\pi}(\\mathbf{x}_{i}),\\hat{\\mathbf{x}}_{i})+\\mu L_{11}(f_{\\pi}(\\mathbf{x}_{i}),\\hat{\\mathbf{x}}_{i}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathcal{C}$ represents a subset of features randomly sampled from the $\\hat{\\textbf{X}}$ , to mitigate overfitting and enhance robustness during training. $\\mu$ denotes the loss weight scalar. ", "page_idx": 4}, {"type": "text", "text": "Language Decoder. Furthermore, we incorporate an image captioning task alongside the text recognition task to ensure that the learned visual tokens can be effectively projected into the language space, thereby enhancing image understanding ability. The forward process and loss formulation for image inputs are the same as those for document inputs. ", "page_idx": 4}, {"type": "text", "text": "3.4 Intra-Scale Pretraining ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "As described in Sec. 3.2 and Sec. 3.3, UNIT tackles images and documents at different input scales with different optimization objectives. In this section, we introduce the intra-scale pretraining stage (see Fig. 2a) where image and text recognition are trained at a fixed scale, namely, low resolution for images and high resolution for documents, without considering variations in scale. ", "page_idx": 4}, {"type": "text", "text": "Dataset Preparation. Let $\\mathcal{D}$ be a curated dataset with 5M samples, where $\\mathcal{D}=\\mathcal{D}_{\\times1}^{I}\\cup\\mathcal{D}_{\\times4}^{T}$ . Here, $\\mathcal{D}_{\\times1}^{I}$ represents a dataset of natural images annotated with coarse captions (less than 30 words). The images are resized to $\\times1$ times the original scale $r$ , primarily sourced from the Conceptual Caption dataset [46], comprising 3M samples. Meanwhile, $\\dot{\\mathcal{D}}_{\\times4}^{T}$ represents a dataset of documents annotated with dense OCR data (more than 500 words). The document images are resized $\\times4$ times the original scale $r$ , sourced from our synthetic dataset of English PDF documents, comprising 2M samples. ", "page_idx": 4}, {"type": "text", "text": "Instruction Prompts. The instruction prompts follow the format used in popular LVLMs [70, 25, 1, 3, 2], where image tokens are prefixed with text tokens. Specifically, we use two special tokens \u201c<img>\u201d and $^{\\leftarrow}{<}/\\mathrm{img}{>}^{\\circ}$ , to indicate the start and the end position of the image tokens, followed by instructions that indicate task requirements. For natural images, the prompt is set as \u201cGive a caption of this image:\u201d and the LLM outputs a language sequence summarizing the content of the image. For document images, the prompt is set as \u201cRead the text in this image:\u201d and the LLM outputs sentences row by row as they appear in the document in the order of reading. ", "page_idx": 4}, {"type": "image", "img_path": "YIxKeHQZpi/tmp/f4c3259e5f753a25f33dfe2c5d8b20d8e3a310daccfeac91f636b4d1d78686bf.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 2: Illustration of the UNIT training paradigm. The (a) intra-scale pretraining stage processes images and documents at their commonly used resolutions to integrate basic text recognition with existing image recognition capabilities. The (b) inter-scale finetuning stage processes scale-exchanged data and tasks to enhance scale robustness, benefiting downstream document analysis tasks when integrated into (c) LVLMs applications. ", "page_idx": 5}, {"type": "text", "text": "Joint Optimization. We train all parameters, including the ViT backbone, vision decoder and language decoder, using images and documents annotated with language sentences. During training, the model is optimized by the cross-entropy loss $\\mathcal{L}_{\\mathrm{lan}}$ between language outputs and annotations, and also the feature reconstruction loss ${\\mathcal L}_{\\mathrm{vis}}$ to force the newly learned visual features to be similar with the original ones. The UNIT model is updated to minimize the total loss: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\theta=\\arg\\operatorname*{min}_{\\theta}\\mathcal{L}_{\\mathrm{lan}}\\big(\\{\\mathcal{D}_{\\times1}^{I}\\cup\\mathcal{D}_{\\times4}^{T}\\}\\big)+\\lambda\\mathcal{L}_{\\mathrm{vis}}\\big(\\{\\mathcal{D}_{\\times1}^{I}\\}\\big).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "3.5 Inter-Scale Finetuning ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "After the intra-scale pretraining stage, UNIT is capable of processing both image and text recognition at their commonly used resolutions, namely, low-resolution images and high-resolution documents. However, we observed that this model lacks scale robustness when handling texts with larger fonts and images with larger dimensions, significantly limiting its generalization ability in various vision tasks. Naively add the scale-exchanged datasets in the intra-scale pretraining process may slow down the convergence of the model and making it prone to local optima. To address this, we further conduct inter-scale finetuning (see Fig. 2b), using high-resolution images and low-resolution documents. ", "page_idx": 5}, {"type": "text", "text": "Dataset Preparation. Here we build another scale exchanged dataset $D^{*}\\,=\\,\\mathcal{D}_{\\times4}^{I}\\cup\\mathcal{D}_{\\times1}^{T}$ with 1M samples for the inter-scale finetuning. We conduct a dataset $\\mathcal{D}_{\\times4}^{I}$ comprising natural images annotated with detailed captions (each containing over 100 words). These images are resized to $\\times4$ times the original scale $r$ , primarily drawn from the ShareGPT4V dataset [11], totaling 1M samples. Additionally, we create a dataset $\\dot{\\mathcal{D}}_{\\times1}^{T}$ comprising 1M documents with an average font size $\\times4$ times larger than that of $\\mathcal{D}_{\\times4}^{T}$ . The document images are resized $\\times r$ times, sourced from our synthetic dataset which includes cropped pages from digital books, advertisements, websites, and other sources containing short yet large font texts. We adopt the instruction prompt setting same to the intra-scale pretraining stage. ", "page_idx": 5}, {"type": "text", "text": "Random Feature Sampling. When dealing with natural images at higher resolutions, the resulting increase in visual tokens compared to lower resolutions can lead to gradient overflow during the training process, primarily due to the token-wise feature reconstruction loss. To address this challenge, we propose to randomly sample a set of visual tokens equal to the number in the low-resolution inputs and discarding other tokens. ", "page_idx": 5}, {"type": "text", "text": "Joint Optimization. This stage aims to enhance the scale-robust recognition ability for both image and text. We retain half of the data at the previous stage and then introduce the scale-exchanged data. The model is updated to minimize the total loss: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\theta=\\arg\\operatorname*{min}_{\\theta}\\mathcal{L}_{\\mathrm{lan}}\\big(\\{\\mathcal{D}_{\\times4}^{I}\\cup\\mathcal{D}_{\\times1}^{T}\\cup\\mathcal{\\frac{1}{2}}\\mathcal{D}_{\\times1}^{I}\\cup\\mathcal{\\frac{1}{2}}\\mathcal{D}_{\\times4}^{T}\\big\\}\\big)+\\lambda\\mathcal{L}_{\\mathrm{vis}}\\big(\\{\\frac{1}{2}\\mathcal{D}_{\\times1}^{I}\\cup\\mathcal{D}_{\\times4}^{I}\\}\\big).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Implementation Details. We select OpenCLIP ViT-H [43] with 32 layers and a hidden size of 1280 as our vision encoder. We choose OPT-125M [68] model with a hidden size of 768 as the language ", "page_idx": 5}, {"type": "table", "img_path": "YIxKeHQZpi/tmp/89773baa261b47def8bdd0520e48972c48415773f5fe9cfe521f3f545b60c900.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 3: Visualization examples of text recognition. UNIT predicts accurate OCR results even across diverse scenarios, e.g., handwritten texts, receipts, and interleaved image-text documents. Please see clearly by zooming in. More promising examples are shown in the supplementary material. ", "page_idx": 6}, {"type": "table", "img_path": "YIxKeHQZpi/tmp/791da6478b94d71b8a49281f6d8c82a3153d985e1d8fd2620c0fa558432f555d.jpg", "table_caption": ["Table 1: Comparison with ViT-based models for text recognition ability. The presented numbers are F1 values. An asterisk $(^{*})$ indicates reimplemented results on our document datasets. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "decoder in the two training stages. Our optimization strategy involves AdamW [32] with a weight decay 0.01. The initial learning rate to 5e-5, and changes with a cosine learning rate decay scheduler. The warmup ratio is set to 0.03, and the global batch size is 256. We set loss weights $\\lambda=2$ and $\\mu\\,=\\,0.2$ . These settings are shared for both two training stages. Our model can be trained with any resolution below the maximum limit. We chose two primary resolutions for training to ensure consistent tensor sizes, simplifying batch processing and optimizing resource use. UNIT is trained on these two resolutions and tested on different ones. To ensure efficient data loading and processing during training, we preprocessed the document images by padding them to square shapes. During inference, we maintain the original aspect ratio of the input images. ", "page_idx": 6}, {"type": "text", "text": "4.1 Evaluation Benchmarks ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Text Recognition: 1) Document-level OCR: We evaluate our model on three public OCR datasets: FUNSD (50 images), SROIE (347 images), and CORD (100 images), reporting the F1 score. Given the lack of large blocks of dense text in these datasets, we create two additional OCR evaluation datasets using English PDF files from arXiv. The first, SYN-L-val (200 images), consists of PDFs with small font sizes at approximately 1k resolution. The second, SYN-S-val (200 images), includes images cropped from these PDFs, resized to a lower resolution (e.g., 224), resulting in larger font sizes. 2) Markdown conversion: The task requires structured text output from images of documents, capturing the stylistic and structural elements of the text in a markdown format. We add 1M markdown data (collected following Nougat [5]) into the inter-scale finetuning stage, endowing the ability of markdown conversion of our model. Due to the lack of cleaned markdown conversion validation set, MD-val, we conduct a dataset containing 82 images with markdown format annotations. ", "page_idx": 6}, {"type": "text", "text": "Image Recognition: 1) Zero-shot classification: We calculate the feature similarity between the [CLS] token of the visual encoder and the text features extracted from the corresponding CLIP text encoder. The evaluation is performed on the ImageNet-1K dataset, and top-1 accuracy is reported. 2) $\\mathbf{k}$ -NN classification: We first compute the [CLS] token of the visual encoder for the training images of ImageNet-1K, and then for each validation image, we utilize a weighted sum of the $k$ nearest training vectors to select a label. 3) Semantic Segmentation: we append a linear head onto the vision encoder and train it with the encoder frozen. The AdamW optimizer is used with a learning rate of $10^{-3}$ . The segmentation mIoU $(\\%)$ is computed as in the MMSeg framework [13] on the ADE20k dataset. For both training and evaluation, we use an input size of $512\\times512$ . ", "page_idx": 6}, {"type": "table", "img_path": "YIxKeHQZpi/tmp/8c49917bfd364376c2515fde2de285b90193ceff8a060446c40e97fa291fe982.jpg", "table_caption": [], "table_footnote": ["Table 2: Comparison of image recognition capabilitiesloss weight $\\lambda$ with both the image captioning with existing vision encoders. task during the intra-scale pretraining stage. "], "page_idx": 7}, {"type": "table", "img_path": "YIxKeHQZpi/tmp/0a57ed5f6521f241899291411f360b00bf77f818ad91356ce5938c950a0f7534.jpg", "table_caption": [], "table_footnote": ["Table 4: Ablation of the inter-scale finetuning stage and the random feature sampling strategy. "], "page_idx": 7}, {"type": "text", "text": "Downstream Vision Tasks: We replace the vision encoder in the LLava-1.5 [28] setting with our own encoder. We first pretrain the input embedding layers with the vision encoder and the LLM frozen, then conduct instruction tuning to finetune a Vicuna-7B model [42]. The learning rate of stage 3 is set to 5e-4 and a batch size of 512. We evaluate the trained models on general visual question answering (VQA) datasets including VQAv2 [17], GQA [20], and OKVQA [33], and also document comprehension datasets including ChartQA [34], DocQA [36], and InfoVQA [35]. ", "page_idx": 7}, {"type": "text", "text": "4.2 Comparison with Existing Approaches ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Text Recognition.We compare UNIT with two ViT-based OCR expert model, including Donut [23] and Nougat [5], and two open-source vision encoders that support high-resolution inputs, including Vary [53] and RADIO [44]. We use the open-source weights of Donut-Base and Nougat for evaluation. As for Vary and RADIO, we retrain them on our dataset for a fair comparison. The models are evaluated on public OCR dataset like FUNSD [21], SROIE [19], CORD [19] and our synthetic datasets SYN-L-val and MD-val. As shown in Tab. 1, UNIT consistently outperforms these methods on OCR and markdown conversion tasks, except for Nougat, exhaustively trained on markdown data. This shows UNIT\u2019s superior ability to encode fine-detailed sequential and spatial information in documents, resulting in highly accurate OCR outcomes, see Fig. 3. Based on the fundamental text recognition ability, UNIT can be further finetuned with a small amount of data to adapt to highly specialized tasks, such as text recognition with grounding (in both natural images and pure texts), markdown conversion and Chinese OCR. Please refer to the supplementary for more details. ", "page_idx": 7}, {"type": "text", "text": "Image Recognition. To validate the image encoding ability of UNIT, we compare it with existing efficient vision encoders on zero-shot and kNN classification on ImageNet-1k, as well as a linear prob semantic segmentation benchmark. In Tab. 2, for fair comparision, all models share the same teacher knowledge, derived from DINOv2 and OpenCLIP-H. The compared models directly use these two models for training feature reconstruction, and UNIT employs one of the best-performing students, RADIO-H, as the teacher encoder. From the results shown in Tab. 2, we observe that UNIT achieves the best performance among all these models, demonstrating its ability to effectively preserve the fundamental image encoding capabilities. ", "page_idx": 7}, {"type": "text", "text": "4.3 Ablation Studies ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "About intra-scale pretraining. As shown in Table 3, removing the additional image captioning task described in Sec. 3.3 leads to a significant drop in performance on zero-shot classification (from $78.54\\%$ to $76.24\\%$ ). This demonstrates that the image captioning task can further enhance image recognition capabilities. Additionally, we ablate the feature reconstruction loss weight $\\lambda$ , finding the best performance when $\\lambda\\,=\\,2$ . When the feature reconstruction loss $\\lambda\\,=\\,0$ , the zero-shot classification failed. This is reasonable because zero-shot classification evaluates the alignment between the vision and text encoders. If $\\lambda=0$ , the vision encoder cannot ensure proper alignment with the text encoder of OpenCLIP-H, leading to the failure of zero-shot classification. ", "page_idx": 7}, {"type": "image", "img_path": "YIxKeHQZpi/tmp/d4fee2d906ecf8f33e1bb373748ab68810b5795506e5e50642d59cd1b4999391.jpg", "img_caption": ["Figure 4: Visualization examples of downstream document analysis tasks. UNIT accurately recognizes tiny words and digits, providing correct answers for document-related questions from users. ", "Table 5: Ablation studies on different input resolutions during inference. "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "YIxKeHQZpi/tmp/b52bfb38d5f5c0dc0ec79bf104ca6c44c48345e90c5de8d82a83c429fe9b233c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "About inter-scale finetuning. As shown in Table 4, \u201cw/o inter-scale\u201d represents the model trained without inter-scale finetuning. Commonly-used Resolution indicates that the input sizes are at the common resolution of the evaluated tasks, e.g., $224\\!\\times\\!224$ for zero-shot classification and $896\\!\\times\\!896$ for document OCR (SYN-L-val). Exchanged Resolution indicates $896\\times896$ for zero-shot classification and $224\\times224$ for document OCR (SYN-S-val). We can see that after inter-scale finetuning, UNIT demonstrates enhanced scale robustness on the exchanged resolution for both image and documents. \u201cw/o feat. sample\u201d represents the model trained without random feature sampling mentioned in Sec. 3.5. This setting causes a dramatic performance drop on zero-shot classification on at exchanged resolution, showing the effectiveness of random feature sampling. ", "page_idx": 8}, {"type": "text", "text": "Different resolutions. Our model supports arbitrary input sizes within the maximum limit. As shown in Table 5, our model, trained on two primary resolutions, generalizes well and maintains consistent performance in both zero-shot classification and OCR tasks. ", "page_idx": 8}, {"type": "text", "text": "4.4 Downstream Task Performance ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Naive resolution. Vision encoders act as a key component in Large Vision-Language Models (LVLMs) to provide rich and detailed visual representations that enable effective cross-modal understanding and reasoning. Following LLava-1.5 [28], we build LVLMs using UNIT or other vision encoders based on the Vicuna-7B LLM. The vision encoders are fixed during LVLM training. Images are padded and resized to one of two primary resolutions based on the task requirements. A lower resolution (e.g., 224x224) is used for global understanding tasks (e.g., VQA), while a higher resolution (e.g., 896x896) is used for tasks requiring fine-detail perception (e.g., DocQA). Each image is represented as 256 visual tokens through the input embedding layer. The LLM is finetuned with LoRA [18]. The multi-modal pretraining process uses 4M image-caption data (randomly extracted from Conceptual Caption [46] and the LAION-COCO [45]) and 200k document OCR data (randomly sampled from the training set). In the Supervised Finetuning (SFT) stage, we use the LLaVA-80k or LLaVA-CC665k along with the train set of DocVQA [36] and ChartQA [34] as the fine-tuning dataset. In Tab. 6, UNIT significantly outperforms the teacher model RADIO and other compared models on the document analysis tasks, including ChartQA, DocQA and InfoVQA, while maintains comparable performance on image understanding tasks. From Fig. 4 we can see that UNIT can extract the texts in documents, charts or websites, showing its potential in real-world document analysis applications. ", "page_idx": 8}, {"type": "table", "img_path": "YIxKeHQZpi/tmp/72a1c9c8915ac03d16b2ffa9fa8a3098c409404d9c1fffdc490afc88631df929.jpg", "table_caption": [], "table_footnote": ["Table 6: Comparison of the performance of various vision encoders with naive resolution. "], "page_idx": 9}, {"type": "table", "img_path": "YIxKeHQZpi/tmp/e1e88751fedb33ae72175b5aca4a95b112ccfaaa1f53dbdfd8ca1a70a172352c.jpg", "table_caption": [], "table_footnote": ["Table 7: Comparison of commonly used vision encoders in LVLMs with high-resolution grid slicing. "], "page_idx": 9}, {"type": "text", "text": "High resolution. In the naive resolution setting, each image is represented using 256 tokens. However, this may be insufficient for capturing the details of images with extensive content, such as densely organized documents containing thousands of words. To address this, we evaluate UNIT in LlavaNext with a grid slicing strategy and compare it with two commonly used vision encoders in LVLMs. This method divides each image into several grids based on their aspect ratio. For instance, with a maximum of 4 grids, possible configurations include $\\{2\\times2,1\\times2,1\\times3,2\\times1,3\\times1\\}$ , with each grid sized at $M\\times M$ . Here, $M$ corresponds to the resolution used during model pretraining\u2014336 for CLIP-L and 384 for SigLIP. And we set 448 for UNIT. After the vision encoder, we use CAbstractor [37] as the input embedding layer. It generates 256 visual tokens for each grid, which are then concatenated to create the final image representation. During the multi-modal pretraining stage, the input embedding layer is initially trained on Llava $665\\mathrm{k}$ . Following this, the LLM and the last few layers of the vision encoder are trained using 1.5M image captioning data. In the SFT stage, all parameters of the LVLM are fine-tuned using 1.2M SFT data. As shown in Tab. 7, our method significantly outperforms the compared models on document-oriented QA tasks and demonstrates comparable performance on other QA tasks. CLIP-L and SigLIP are initially pretrained only on natural images and subsequently fine-tuned with document images for downstream tasks. In contrast, UNIT is pretrained on both natural images and documents using fundamental recognition tasks such as image captioning and OCR. Our approach offers two key advantages: First, pretraining with fundamental recognition tasks enhances UNIT\u2019s versatility and suitability for downstream document tasks. Second, the early integration of text recognition capabilities in UNIT provides a robust initialization for LVLM training, leading to more stable training and faster convergence. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We propose UNIT, a simple yet effective framework that unifies image and text recognition in a single vision encoder, without increasing deployment or inference cost. UNIT incorporates a lightweight language decoder for text recognition and a lightweight vision decoder to preserve image recognition capabilities. Trained with multi-scale inputs, UNIT processes images and documents at their conventional resolutions during intra-scale retraining for basic recognition, and at exchanged resolutions during inter-scale finetuning to enhance scale robustness. Extensive experiments demonstrate that UNIT significantly outperforms document-specific models, maintains core image encoding ability, and benefits downstream document analysis tasks when integrated into LVLMs, showing great potential in processing interleaved text and image information in real-world scenarios. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] J. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, R. Ring, E. Rutherford, S. Cabi, T. Han, Z. Gong, S. Samangooei, M. Monteiro, J. L. Menick, S. Borgeaud, A. Brock, A. Nematzadeh, S. Sharifzadeh, M. Binkowski, R. Barreira, O. Vinyals, A. Zisserman, and K. Simonyan. Flamingo: a visual language model for few-shot learning. In Advances in Neural Information Processing Systems (NeurIPS), 2022. [2] Alibaba. Introducing qwen-7b: Open foundation and human-aligned models (of the state-of-the-arts). https://github.com/QwenLM/Qwen-7B, 2023.   \n[3] J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han, F. Huang, B. Hui, L. Ji, M. Li, J. Lin, R. Lin, D. Liu, G. Liu, C. Lu, K. Lu, J. Ma, R. Men, X. Ren, X. Ren, C. Tan, S. Tan, J. Tu, P. Wang, S. Wang, W. Wang, S. Wu, B. Xu, J. Xu, A. Yang, H. Yang, J. Yang, S. Yang, Y. Yao, B. Yu, H. Yuan, Z. Yuan, J. Zhang, X. Zhang, Y. Zhang, Z. Zhang, C. Zhou, J. Zhou, X. Zhou, and T. Zhu. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. [4] D. Bautista and R. Atienza. Scene text recognition with permuted autoregressive sequence models. arXiv preprint arXiv:2207.06966, 2022. cs.   \n[5] L. Blecher, G. Cucurull, T. Scialom, and R. Stojnic. Nougat: Neural optical understanding for academic documents. arXiv preprint arXiv:2308.13418, 2023.   \n[6] A. Brock, S. De, S. L. Smith, and K. Simonyan. High-performance large-scale image recognition without normalization. In International Conference on Machine Learning (ICML), pages 1059\u20131071. PMLR, 2021. [7] H. Cai, J. Li, M. Hu, C. Gan, and S. Han. Efficientvit: Multi-scale linear attention for high-resolution dense prediction, 2023. [8] Y. Cao, Z. Yihan, H. Xu, and D. Xu. Coda: Collaborative novel box discovery and cross-modal alignment for open-vocabulary 3d object detection. Advances in Neural Information Processing Systems (NeurIPS), 36, 2024. [9] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko. End-to-end object detection with transformers. In European Conference on Computer Vision (ECCV), pages 213\u2013229. Springer, 2020.   \n[10] K. Chen, J. Wang, J. Pang, Y. Cao, Y. Xiong, X. Li, S. Sun, W. Feng, Z. Liu, J. Xu, Z. Zhang, D. Cheng, C. Zhu, T. Cheng, Q. Zhao, B. Li, X. Lu, R. Zhu, Y. Wu, J. Dai, J. Wang, J. Shi, W. Ouyang, C. C. Loy, and D. Lin. MMDetection: Open mmlab detection toolbox and benchmark. arXiv preprint arXiv:1906.07155, 2019.   \n[11] L. Chen, J. Li, X. Dong, P. Zhang, C. He, J. Wang, F. Zhao, and D. Lin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793, 2023.   \n[12] X. Chen, L. Jin, Y. Zhu, C. Luo, and T. Wang. Text recognition in the wild: A survey. ACM Computing Surveys (CSUR), 54(2):1\u201335, 2021.   \n[13] M. Contributors. MMSegmentation: Openmmlab semantic segmentation toolbox and benchmark. https: //github.com/open-mmlab/mmsegmentation, 2020.   \n[14] X. Dai, Y. Chen, J. Yang, P. Zhang, L. Yuan, and L. Zhang. Dynamic detr: End-to-end object detection with dynamic attention. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 2988\u20132997, 2021.   \n[15] D. H. Diaz, S. Qin, R. Ingle, Y. Fujii, and A. Bissacco. Rethinking text line recognition models. arXiv preprint arXiv:2104.07787, 2021. cs.   \n[16] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale, 2021.   \n[17] Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, and D. Parikh. Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2017.   \n[18] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. Lora: Low-rank adaptation of large language models. In International Conference on Machine Learning (ICLR), 2022.   \n[19] S. Huang, L. Dong, W. Wang, Y. Hao, S. Singhal, S. Ma, T. Lv, L. Cui, O. K. Mohammed, B. Patra, et al. Language is not all you need: Aligning perception with language models. Advances in Neural Information Processing Systems (NeurIPS), 36:72096\u201372109, 2023.   \n[20] D. A. Hudson and C. D. Manning. GQA: a new dataset for compositional question answering over real-world images. CoRR, abs/1902.09506, 2019.   \n[21] G. Jaume, H. K. Ekenel, and J.-P. Thiran. Funsd: A dataset for form understanding in noisy scanned documents. In International Conference on Document Analysis and Recognition Workshops (ICDARW), volume 2, pages 1\u20136. IEEE, 2019.   \n[22] D. Kim, A. Angelova, and W. Kuo. Region-aware pretraining for open-vocabulary object detection with vision transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 11144\u201311154, 2023.   \n[23] G. Kim, T. Hong, M. Yim, J. Nam, J. Park, J. Yim, W. Hwang, S. Yun, D. Han, and S. Park. Ocrfree document understanding transformer. In European Conference on Computer Vision (ECCV), pages 498\u2013517. Springer, 2022.   \n[24] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo, et al. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 4015\u20134026, 2023.   \n[25] J. Li, D. Li, S. Savarese, and S. Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International Conference on Machine Learning (ICML), pages 19730\u201319742. PMLR, 2023.   \n[26] J. Li, D. Li, C. Xiong, and S. C. H. Hoi. BLIP: bootstrapping language-image pre-training for unified vision-language understanding and generation. In International conference on machine learning (ICML), volume 162 of Proceedings of Machine Learning Research, pages 12888\u201312900. PMLR, 2022.   \n[27] Y. Li, C.-Y. Wu, H. Fan, K. Mangalam, B. Xiong, J. Malik, and C. Feichtenhofer. Mvitv2: Improved multiscale vision transformers for classification and detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 4804\u20134814, 2022.   \n[28] H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning. Advances in Neural Information Processing Systems (NeurIPS), 36, 2024.   \n[29] Z. Liu, H. Hu, Y. Lin, Z. Yao, Z. Xie, Y. Wei, J. Ning, Y. Cao, Z. Zhang, L. Dong, et al. Swin transformer v2: Scaling up capacity and resolution. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 12009\u201312019, 2022.   \n[30] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 10012\u201310022, 2021.   \n[31] Z. Liu, H. Mao, C.-Y. Wu, C. Feichtenhofer, T. Darrell, and S. Xie. A convnet for the 2020s, 2022.   \n[32] I. Loshchilov and F. Hutter. Decoupled weight decay regularization. In International Conference on Machine Learning (ICLR), 2019.   \n[33] K. Marino, M. Rastegari, A. Farhadi, and R. Mottaghi. Ok-vqa: A visual question answering benchmark requiring external knowledge. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 3195\u20133204, 2019.   \n[34] A. Masry, D. X. Long, J. Q. Tan, S. Joty, and E. Hoque. Chartqa: A benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022.   \n[35] M. Mathew, V. Bagal, R. Tito, D. Karatzas, E. Valveny, and C. Jawahar. Infographicvqa. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 1697\u20131706, 2022.   \n[36] M. Mathew, D. Karatzas, and C. Jawahar. Docvqa: A dataset for vqa on document images. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 2200\u20132209, 2021.   \n[37] B. McKinzie, Z. Gan, J.-P. Fauconnier, S. Dodge, B. Zhang, P. Dufter, D. Shah, X. Du, F. Peng, F. Weers, et al. Mm1: Methods, analysis & insights from multimodal llm pre-training. arXiv preprint arXiv:2403.09611, 2024. Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 2906\u20132917, 2021.   \n[39] B. Moysset, C. Kermorvant, and C. Wolf. Full-page text recognition: Learning where to start and when to stop. In 2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR), volume 1, pages 871\u2013876. IEEE, 2017.   \n[40] OpenAI. Chatgpt. https://openai.com/blog/chatgpt/, 2023.   \n[41] M. Oquab, T. Darcet, T. Moutakanni, H. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D. Haziza, F. Massa, A. El-Nouby, M. Assran, N. Ballas, W. Galuba, R. Howes, P.-Y. Huang, S.-W. Li, I. Misra, M. Rabbat, V. Sharma, G. Synnaeve, H. Xu, H. Jegou, J. Mairal, P. Labatut, A. Joulin, and P. Bojanowski. Dinov2: Learning robust visual features without supervision, 2023.   \n[42] B. Peng, C. Li, P. He, M. Galley, and J. Gao. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277, 2023.   \n[43] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning (ICML), pages 8748\u20138763. PMLR, 2021.   \n[44] M. Ranzinger, G. Heinrich, J. Kautz, and P. Molchanov. Am-radio: Agglomerative vision foundation model reduce all domains into one. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 12490\u201312500, June 2024.   \n[45] C. Schuhmann, R. Vencu, R. Beaumont, R. Kaczmarczyk, C. Mullis, A. Katta, T. Coombes, J. Jitsev, and A. Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021.   \n[46] P. Sharma, N. Ding, S. Goodman, and R. Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Annual Meeting of the Association for Computational Linguistics (ACL), pages 2556\u20132565, 2018.   \n[47] Y. Shen, K. Song, X. Tan, D. Li, W. Lu, and Y. Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. arXiv preprint arXiv:2303.17580, 2023.   \n[48] Z. Tu, H. Talebi, H. Zhang, F. Yang, P. Milanfar, A. Bovik, and Y. Li. Maxvit: Multi-axis vision transformer, 2022.   \n[49] K. Wang, B. Babenko, and S. Belongie. End-to-end scene text recognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 1457\u20131464. IEEE, 2011.   \n[50] T. Wang, D. J. Wu, A. Coates, and A. Y. Ng. End-to-end text recognition with convolutional neural networks. In Proceedings of International Conference on Pattern Recognition (ICPR), pages 3304\u20133308. IEEE, 2012.   \n[51] W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu, P. Luo, and L. Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 568\u2013578, 2021.   \n[52] S. T. Wasim, M. Naseer, S. Khan, F. S. Khan, and M. Shah. Vita-clip: Video and text adaptive clip via multimodal prompting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 23034\u201323044, 2023.   \n[53] H. Wei, L. Kong, J. Chen, L. Zhao, Z. Ge, J. Yang, J. Sun, C. Han, and X. Zhang. Vary: Scaling up the vision vocabulary for large vision-language models. arXiv preprint arXiv:2312.06109, 2023.   \n[54] H. Wei, L. Kong, J. Chen, L. Zhao, Z. Ge, E. Yu, J. Sun, C. Han, and X. Zhang. Small language model meets with reinforced vision vocabulary. arXiv preprint arXiv:2401.12503, 2024.   \n[55] C. Wu, S. Yin, W. Qi, X. Wang, Z. Tang, and N. Duan. Visual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671, 2023.   \n[56] H. Wu, B. Xiao, N. Codella, M. Liu, X. Dai, L. Yuan, and L. Zhang. Cvt: Introducing convolutions to vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 22\u201331, 2021.   \n[57] E. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez, and P. Luo. Segformer: Simple and efficient design for semantic segmentation with transformers. Advances in Neural Information Processing Systems (NeurIPS), 34:12077\u201312090, 2021.   \n[58] J. Xu, S. De Mello, S. Liu, W. Byeon, T. Breuel, J. Kautz, and X. Wang. Groupvit: Semantic segmentation emerges from text supervision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 18134\u201318144, 2022.   \n[59] R. Yang, L. Song, Y. Li, S. Zhao, Y. Ge, X. Li, and Y. Shan. Gpt4tools: Teaching large language model to use tools via self-instruction. arXiv preprint arXiv:2305.18752, 2023.   \n[60] J. Ye, A. Hu, H. Xu, Q. Ye, M. Yan, Y. Dan, C. Zhao, G. Xu, C. Li, J. Tian, et al. mplug-docowl: Modularized multimodal large language model for document understanding. arXiv preprint arXiv:2307.02499, 2023.   \n[61] J. Ye, A. Hu, H. Xu, Q. Ye, M. Yan, G. Xu, C. Li, J. Tian, Q. Qian, J. Zhang, et al. Ureader: Universal ocr-free visually-situated language understanding with multimodal large language model. arXiv preprint arXiv:2310.05126, 2023.   \n[62] Q. Ye, H. Xu, G. Xu, J. Ye, M. Yan, Y. Zhou, J. Wang, A. Hu, P. Shi, Y. Shi, et al. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023.   \n[63] Q. Ye, H. Xu, J. Ye, M. Yan, A. Hu, H. Liu, Q. Qian, J. Zhang, and F. Huang. mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 13040\u201313051, 2024.   \n[64] Y. Zeng, C. Jiang, J. Mao, J. Han, C. Ye, Q. Huang, D.-Y. Yeung, Z. Yang, X. Liang, and H. Xu. Clip2: Contrastive language-image-point pretraining from real-world point cloud data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 15244\u201315253, 2023.   \n[65] X. Zhai, B. Mustafa, A. Kolesnikov, and L. Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 11975\u201311986, 2023.   \n[66] Q. Zhang, J. Zhang, Y. Xu, and D. Tao. Vision transformer with quadrangle attention. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2024.   \n[67] R. Zhang, Z. Guo, W. Zhang, K. Li, X. Miao, B. Cui, Y. Qiao, P. Gao, and H. Li. Pointclip: Point cloud understanding by clip. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 8552\u20138562, 2022.   \n[68] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V. Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.   \n[69] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V. Lin, et al. Opt: Open pre-trained transformer language models. URL https://arxiv. org/abs/2205.01068, 3:19\u20130, 2023.   \n[70] D. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023.   \n[71] L. Zhu, X. Wang, Z. Ke, W. Zhang, and R. W. Lau. Biformer: Vision transformer with bi-level routing attention. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 10323\u201310333, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Appendix / supplemental material ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Computer Resources ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We implement our method using PyTorch 2.1.0 with CUDA 11.7. The training process requires 128 Ascend 910B GPUs (each with 64 GB of memory). Each training stage runs for one epoch, totaling 150 hours of training time. ", "page_idx": 14}, {"type": "text", "text": "A.2 Limitations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Integrating text and image recognition in a single model may entail trade-offs, potentially yielding slightly inferior performance in highly specialized tasks compared to models exclusively dedicated to text or image recognition. For example, as illustrated in the Table 1 of our paper, UNIT\u2019s performance in the markdown conversion task (a highly specialized OCR task) falls short of Nougat, a model exhaustively trained on markdown data and specifically optimized for markdown conversion. ", "page_idx": 14}, {"type": "text", "text": "A.3 Broader Impacts ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Positive Societal Impacts: 1) Enhanced Accessibility: By unifying image and text recognition within a single model, UNIT could potentially improve accessibility for individuals with visual impairments. The model\u2019s ability to accurately recognize and interpret both images and text could facilitate the development of assistive technologies such as screen readers and optical character recognition (OCR) systems. 2) Efficiency in Document Analysis: UNIT\u2019s capability to seamlessly handle both image and text recognition tasks could streamline document analysis processes in various domains, including healthcare, education, and legal sectors. This could lead to increased efficiency and productivity in tasks such as document digitization, content extraction, and information retrieval. 3) Technological Advancement: The development of novel training frameworks like UNIT contributes to the advancement of computer vision and natural language processing technologies. Such advancements have the potential to drive innovation in diverse fields, including artificial intelligence, robotics, and human-computer interaction. ", "page_idx": 14}, {"type": "text", "text": "Possible Negative Societal Impacts: 1) Privacy Concerns: The improved accuracy and efficiency of text recognition enabled by models like UNIT may raise concerns regarding privacy and data security. Increased capability to extract and analyze text from images could potentially infringe upon individuals\u2019 privacy rights if not properly regulated or safeguarded. 2) Job Displacement: The automation of document analysis tasks facilitated by UNIT and similar models could lead to job displacement in industries that rely heavily on manual data entry and document processing. While this could result in increased efficiency and cost savings for businesses, it may also contribute to unemployment and economic inequality. ", "page_idx": 14}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: We believe that the main claims reflect the areas we are targeting: vision fundation models and large-scale vision-language models. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 15}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Justification: Please see the Appendix. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 15}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 15}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 15}, {"type": "text", "text": "Justification: Our paper does not include theoretical results. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 16}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: See the Section 3 Methodology and Section 4 Experiments. Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 16}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 16}, {"type": "text", "text": "Answer: [No] ", "page_idx": 17}, {"type": "text", "text": "Justification: According to the code and data authorization rule of our institution, we are not allowed to directly attach the code and data in the anonymous submission. We will apply for a open-source license once our paper is accepted. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 17}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: See the Section 4 Experiments for implementation details. Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 17}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 17}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification: Our experiments are quite stable with multiple runs. All results are observed via a fixed seed. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: See Appendix for more details. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 18}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: The research is conducted with the NeurIPS code of Ethics in every respect. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 18}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: See Appendix. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: Our paper poses no such risks. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 19}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: We have cited the creators of all existing assets. All the used existing assets are open-source. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 19}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 20}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 20}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 20}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 20}]