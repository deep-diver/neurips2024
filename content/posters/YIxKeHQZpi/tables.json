[{"figure_path": "YIxKeHQZpi/tables/tables_6_1.jpg", "caption": "Table 1: Comparison with ViT-based models for text recognition ability. The presented numbers are F1 values. An asterisk (*) indicates reimplemented results on our document datasets.", "description": "This table compares the performance of UNIT with other ViT-based models on text recognition tasks, specifically focusing on F1 scores across various datasets.  The asterisk indicates that some results were re-implemented using the authors' document datasets for better comparison.  The results show that UNIT outperforms other models on the document recognition tasks.", "section": "4.1 Evaluation Benchmarks"}, {"figure_path": "YIxKeHQZpi/tables/tables_6_2.jpg", "caption": "Table 1: Comparison with ViT-based models for text recognition ability. The presented numbers are F1 values. An asterisk (*) indicates reimplemented results on our document datasets.", "description": "This table compares the performance of UNIT against other ViT-based models on text recognition tasks, specifically focusing on F1 scores across several benchmark datasets.  The datasets include FUNSD, SROIE, CORD, SYN-L-val, and MD-val, each representing different challenges in document image analysis. The asterisk indicates that some results were re-implemented by the authors on their own document datasets to ensure fair comparison.", "section": "4.1 Evaluation Benchmarks"}, {"figure_path": "YIxKeHQZpi/tables/tables_7_1.jpg", "caption": "Table 1: Comparison with ViT-based models for text recognition ability. The presented numbers are F1 values. An asterisk (*) indicates reimplemented results on our document datasets.", "description": "This table compares the performance of UNIT with other ViT-based models on text recognition tasks using F1 scores.  The models are evaluated on several document-level OCR datasets.  The asterisk (*) indicates that some results were re-implemented by the authors for a fair comparison since the original results were reported on different datasets.", "section": "4.1 Evaluation Benchmarks"}, {"figure_path": "YIxKeHQZpi/tables/tables_7_2.jpg", "caption": "Table 1: Comparison with ViT-based models for text recognition ability. The presented numbers are F1 values. An asterisk (*) indicates reimplemented results on our document datasets.", "description": "This table compares the performance of UNIT with other ViT-based models on text recognition tasks, specifically focusing on F1 scores.  It includes results across multiple datasets, highlighting UNIT's superior performance compared to existing methods.", "section": "4.1 Evaluation Benchmarks"}, {"figure_path": "YIxKeHQZpi/tables/tables_8_1.jpg", "caption": "Table 1: Comparison with ViT-based models for text recognition ability. The presented numbers are F1 values. An asterisk (*) indicates reimplemented results on our document datasets.", "description": "This table compares the performance of UNIT with other ViT-based models on text recognition tasks, specifically focusing on F1 scores.  It includes results on several datasets (FUNSD, SROIE, CORD, SYN-L-val, and MD-val) and highlights that UNIT significantly outperforms existing methods. The asterisk denotes that certain results were re-implemented by the authors on their document datasets for a fair comparison.", "section": "4.1 Evaluation Benchmarks"}, {"figure_path": "YIxKeHQZpi/tables/tables_9_1.jpg", "caption": "Table 6: Comparison of the performance of various vision encoders with naive resolution.", "description": "This table compares the performance of several vision encoders, including the proposed UNIT model, on various downstream tasks categorized into document analysis and image understanding.  The naive resolution refers to the commonly used resolutions for the respective tasks.  The results show UNIT's performance in relation to other state-of-the-art vision encoders for document analysis tasks like DocQA, ChartQA, and InfoVQA, while maintaining comparable performance on image understanding tasks such as VQAv2, GQA, and OKVQA.", "section": "4.2 Comparison with Existing Approaches"}, {"figure_path": "YIxKeHQZpi/tables/tables_9_2.jpg", "caption": "Table 7: Comparison of commonly used vision encoders in LVLMs with high-resolution grid slicing.", "description": "This table compares the performance of various vision encoders (CLIP-L, SigLIP, and UNIT) when integrated into Large Vision-Language Models (LVLMs) using a high-resolution grid slicing technique for image processing.  The performance is evaluated across several downstream tasks, including ChartQA, DocVQA, InfoVQA, OCRBench, GQA, OKVQA, MME, and MathVista.  The results highlight UNIT's superior performance across these tasks compared to the other vision encoders.", "section": "4.2 Comparison with Existing Approaches"}]