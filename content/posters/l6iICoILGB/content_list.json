[{"type": "text", "text": "Practical 0.385-Approximation for Submodular Maximization Subject to a Cardinality Constraint ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Murad Tukan Loay Mualem DataHeroes Israel Department of Computer Science murad@dataheroes.ai University of Haifa Haifa Israel loaymual@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Moran Feldman Department of Computer Science University of Haifa Haifa Israel moranfe@cs.haifa.ac.il ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Non-monotone constrained submodular maximization plays a crucial role in various machine learning applications. However, existing algorithms often struggle with a trade-off between approximation guarantees and practical efficiency. The current state-of-the-art is a recent 0.401-approximation algorithm, but its computational complexity makes it highly impractical. The best practical algorithms for the problem only guarantee $1/e$ -approximation. In this work, we present a novel algorithm for submodular maximization subject to a cardinality constraint that combines a guarantee of 0.385-approximation with a low and practical query complexity of $O(n{+}k^{2})$ , where $n$ is the size of the ground set and $k$ is the maximum size of a feasible solution. Furthermore, we evaluate the empirical performance of our algorithm in experiments based on the machine learning applications of Movie Recommendation, Image Summarization, and Revenue Maximization. These experiments demonstrate the efficacy of our approach. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In the last few years, the ability to effectively summarize data has gained importance due to the advent of massive datasets in many fields. Such summarization often consists of selecting a small representative subset from a large corpus of images, text, movies, etc. Without a specific structure, this task can be as challenging as finding a global minimum of a non-convex function. Fortunately, many practical machine learning problems exhibit some structure, making them suitable for optimization techniques (either exact or approximate). ", "page_idx": 0}, {"type": "text", "text": "A key structure present in many such problems is submodularity, also known as the principle of diminishing returns. This principle suggests that the incremental value of an element decreases as the set it is added to grows. Submodularity enables the creation of algorithms that can provide near-optimal solutions, making it fundamental in machine learning. It has been successfully applied to various tasks, such as social graph analysis [39], adversarial attacks [26, 36], dictionary learning [13], data summarization [32, 34, 35], interpreting neural networks [14], robotics [45, 41], and many more. ", "page_idx": 0}, {"type": "text", "text": "To exemplify the notion of submodularity, consider the following task. Given a large dataset, our goal is to identify a subset that effectively summarizes (or covers) the data, with a good representative set being one that covers the majority of the data. Note that adding an element $s$ to a set $B$ is less beneficial to this goal than adding it to a subset $A\\subset B$ due to the higher likelihood of overlapping coverage. Formally, if $\\mathcal{N}$ is the set of elements in the dataset, and we define a function $f\\colon2^{\\mathcal{N}^{\\ast}}\\bar{\\vec{\\mathbb{R}}}$ mapping every set of elements to its coverage, then, the above discussion implies that, for every two sets $A\\subseteq B\\subseteq\\mathcal{N}$ and element $\\mathit{s}_{\\mathrm{~}}\\in\\bar{\\mathcal{N}}\\setminus B$ , it must hold that $f(s\\mid A)\\;\\stackrel{}{\\geq}\\;f(s\\mid B)$ , where $f(s\\mid A)\\triangleq f(\\{s\\}\\cup A)-f(A)$ denotes the marginal gain of the element $s$ with respect to the set $A$ . We say that a set function is submodular if it obeys this property. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Unfortunately, maximizing submodular functions is NP-hard even without a constraint [16], and therefore, works on maximization of such functions aim for approximations. Many of these works make the extra assumption that the submodular function $f\\colon2^{\\mathcal{N}^{\\ast}}\\mathcal{{\\rightarrow}\\mathbb{R}}$ is monotone, i.e., that for every two sets $A\\subseteq B\\subseteq{\\mathcal{N}}$ , it holds that $f(B)\\geq f(A)$ . Two of the first works of this kind, by Nemhauser and Wolsey [37] and Nemhauser et al. [38], showed that a greedy algorithm achieves a tight $1-{^1}/{e}$ approximation for the problem of maximizing a non-negative monotone submodular function subject to a cardinality constraint using $O(n k)$ function evaluations, where $n$ is the size of the ground set $\\mathcal{N}$ and $k$ is the maximum cardinality allowed for the output set. An important line of work aimed to improve the time complexity of the last algorithm, culminating with deterministic and randomized algorithms that have managed to reduce the time complexity to linear at the cost of an approximation guarantee that is worse only by a factor of $1-\\varepsilon$ [6, 31, 27, 22, 20]. ", "page_idx": 1}, {"type": "text", "text": "Unfortunately, the submodular functions that arise in machine learning applications are often nonmonotone, either because they are naturally non-monotone, or because a diversity-promoting nonmonotone regularizer is added to them. Maximizing a non-monotone submodular function is challenging. The only tight approximation known for such functions is for the case of unconstrained maximization, which enjoys a tight approximation ratio of $^1\\!/\\!2$ [16, 7]. A slightly more involved case is the problem of maximizing a non-negative (not necessarily monotone) submodular function subject to a cardinality constraint. This problem has been studied extensively. First, Lee et al. [25] suggested an algorithm guaranteeing $\\left(1/4-\\varepsilon\\right)$ -approximation for it. This approximation ratio was improved in a long series of works [5, 11, 15, 43], leading to a very recent 0.401-approximation algorithm due to Buchbinder and Feldman [4], which improved over a previous 0.385-approximation algorithm due to Buchbinder and Feldman [3]. On the inapproximability side, it has been shown that no algorithm can guarantee a better approximation ratio than 0.478 in polynomial time [40]. ", "page_idx": 1}, {"type": "text", "text": "Most of the results in the above-mentioned line of work are only of theoretical interest due to a very high time complexity. The two exceptions are the Random Greedy algorithm of Buchbinder et al. [5] that guarantees $^1\\!/e$ -approximation using $O(n k)$ queries to the objective function, and the Sample Greedy algorithm of Buchbinder et al. [6] that reduces the query complexity to $O_{\\varepsilon}(n)$ at the cost of a slightly worse approximation ratio of $1/e-\\varepsilon$ . ", "page_idx": 1}, {"type": "text", "text": "1.1 Our contribution ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this work, we introduce a novel combinatorial algorithm for maximizing a non-negative submodular function subject to a cardinality constraint. Our suggested method combines a practical query complexity of $O(n+k^{2})$ with an approximation guarantee of 0.385, which improves over the $^1\\!/e$ -approximation of the state-of-the-art practical algorithm. To emphasize the effectiveness of our suggested method, we empirically evaluate it on 3 applications: (i) Movie Recommendation, (ii) Image Summarization, and (iii) Revenue Maximization. Our experiments on these applications demonstrate that our algorithm (Algorithm 3) outperforms the current practical state-of-the-art algorithms. ", "page_idx": 1}, {"type": "text", "text": "Remark. An independent work that recently appeared on arXiv [12] suggests another 0.385- approximation algorithm for our problem using $O(n k)$ oracle queries. Interestingly, their algorithm is very similar to a basic version of our algorithm presented in Appendix A. In this work, our main goal is to find ways to speed up this basic algorithm, which leads to our main result. In contrast, the main goal of [12] is to derandomize the basic algorithm and extend it to other constraints. ", "page_idx": 1}, {"type": "text", "text": "1.2 Additional notation ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Let us define some additional notation used throughout the paper. Given an element $u\\in\\mathcal N$ and a set $S\\subseteq N$ , we use $S+u$ and $S-u$ as shorthands for $S\\cup\\{u\\}$ and $S\\setminus\\{u\\}$ , respectively. Given also a set function $f\\colon2^{\\mathcal{N}}\\to\\mathbb{R}$ , we recall that $f(u\\mid S)$ is used to denote the marginal contribution of $u$ to ", "page_idx": 1}, {"type": "text", "text": "$S$ . Similarly, given an additional set $T\\subseteq N$ , we define $f(T\\mid S)\\triangleq f(S\\cup T)-f(S)$ . Finally, we denote by $\\mathbb{O}\\mathbb{P}\\mathbb{T}$ an arbitrary optimal solution for the problem we consider. ", "page_idx": 2}, {"type": "text", "text": "2 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we present our algorithm for non-monotone submodular maximization under cardinality constraints, which is the algorithm used to prove the main theoretical result of our work (Theorem 2.3). We begin with a brief overview of our algorithm. Motivated by the ideas underlying the impractical 0.385-approximation algorithm of [3], our algorithm comprises three steps: ", "page_idx": 2}, {"type": "text", "text": ". Initial Solution: We start by searching for a good initial solution that guarantees a constant approximation to the optimal set. This is accomplished by running the recent deterministic $^1\\!/\\!4$ -approximation algorithm of Balkanski et al. [2].1 ", "page_idx": 2}, {"type": "text", "text": "2. Accelerated Local Search (Algorithm 1): Next, the algorithm aims to find an (approximate) local optimum set $Z$ using a local search method. This can be done using a classical local search algorithm at the cost of $O_{\\varepsilon}(n k^{2})$ queries (see Appendix A for more detail). As an alternative, we introduce, in Subsection 2.1, our accelerated local search algorithm FAST-LOCAL-SEARCH (Algorithm 1), which reduces the query complexity to $O_{\\varepsilon}(\\overline{{n}}+k^{2})$ . ", "page_idx": 2}, {"type": "text", "text": "3. Accelerated Stochastic Greedy Improvement (Algorithm 2): It can be shown that when the set $Z$ does not have a good value, it contains only little of the value of the optimal solution, and at the same time, it contains many of the elements that negatively affects this optimal solution. Thus, it makes sense to try to avoid this set. Accordingly, after obtaining the set $Z$ , our algorithm constructs a second possible solution using a stochastic greedy algorithm that picks only elements of ${\\mathcal{N}}\\setminus Z$ in its first iterations. One can use for this purpose a version of the Random Greedy algorithm suggested by Buchbinder et al. [5] that uses $O(n k)$ queries (see Appendix A for details). To get the same result using fewer queries, we employ Algorithm 2 (described in Subsection 2.2), which is accelerated using ideas borrowed from the Sample Greedy algorithm of [6]. ", "page_idx": 2}, {"type": "text", "text": "Our final algorithm (given as Algorithm 3 in Subsections 2.3) returns the better among the two sets produced in the last two steps (i.e., the output sets of Algorithm 1, and Algorithm 2). Intuitively, this algorithm guarantees our target approximation ratio of 0.385 because when $f(Z)$ is smaller than this value, the set $Z$ is bad enough that avoiding it (in the first iterations) allows Algorithm 2 to get a good enough solution. ", "page_idx": 2}, {"type": "text", "text": "2.1 Fast local search ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we present our accelerated local search algorithm, which is the algorithm used to implement the first two steps of our main algorithm. The properties of this algorithm are formally given by Theorem 2.1. Let $\\mathbb{O}\\mathbb{P}\\mathbb{T}$ be an optimal solution. ", "page_idx": 2}, {"type": "text", "text": "Theorem 2.1. There exists an algorithm that given a positive integer $k$ , a value $\\varepsilon\\in(0,1)$ , and $a$ non-negative submodular function $f\\colon2^{\\mathcal{N}}\\to{\\check{\\mathbb{R}}}_{\\geq0}$ , outputs a set $S\\subseteq N$ of size at max $k$ that, with probability at least $1-\\varepsilon_{i}$ , obeys ", "page_idx": 2}, {"type": "equation", "text": "$$\nf(S)\\geq\\frac{f(S\\cap\\mathbb{O P T})+f(S\\cup\\mathbb{O P T})}{2+\\varepsilon}\\quad a n d\\quad f(S)\\geq\\frac{f(S\\cap\\mathbb{O P T})}{1+\\varepsilon}\\mathrm{~.~}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Furthermore, the query complexity of the above algorithm is $O_{\\varepsilon}(n+k^{2})$ . ", "page_idx": 2}, {"type": "text", "text": "Note that the guarantee of Theorem 2.1 is similar to the guarantee of a classical local search algorithm (see Appendix A for details). However, such a classical local search algorithm uses $O_{\\varepsilon}(n k^{2})$ queries, which is higher than the number of queries required for the algorithm from Theorem 2.1. ", "page_idx": 2}, {"type": "text", "text": "We defer the formal proof of Theorem 2.1 to Appendix B. However, we note that this proof is based on Algorithm 1. Algorithm 1 implicitly assumes that the ground set $\\mathcal{N}$ includes at least $k+1$ dummy ", "page_idx": 2}, {"type": "text", "text": "elements that always have a zero marginal contribution to $f$ . Such elements can always be added to the ground set (before executing the algorithm) without affecting the properties of $f$ , and removing them from the output set of the algorithm does not affect the guarantee of Theorem 2.1. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{Algorithm\\1!:FAST-LOCAL-SEARCH}(k,f,\\varepsilon,L)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "input :A positive integer $k\\geq1$ , a submodular function $f$ , an approximation factor $\\varepsilon\\in(0,1)$ , and a number $L$ of iterations. output :A subset of $\\mathcal{N}$ of cardinality at most $k$ . 1 Initialize $S_{0}$ to be a feasible solution that with probability at least $1-\\varepsilon$ provides $c$ -approximation for the problem for some constant $c\\in(0,1]$ . 2 Fill $S_{0}$ with dummy elements to ensure $|\\dot{S}_{0}|=k$ . 3 for $j=1$ to $\\lceil\\log_{2}\\frac{1}{\\varepsilon}\\rceil$ do 4 Let $S_{0}^{j}\\leftarrow S_{0}$ . 5 for $i=1$ to $L$ do 6 $Z_{i}^{j}\\gets$ Sample $\\textstyle{\\frac{n}{k}}$ items from $\\mathcal{N}$ uniformly at random. 7 $u_{i}^{j}\\leftarrow\\arg\\operatorname*{max}_{u^{\\prime}\\in Z_{i}^{j}}f(u_{i}^{j}\\mid S_{i-1}^{j})$ . 8 if $f(u_{i}^{j}\\mid S_{i-1}^{j})\\le0$ then $u_{i}^{j}\\leftarrow\\mathrm{dummy}$ element that does not belong to $S_{i-1}^{j}$ . 9 $v_{i}^{j}\\leftarrow\\arg\\operatorname*{min}_{v^{\\prime}\\in S_{i-1}^{j}}f(v^{\\prime}\\mid S_{i-1}^{j}-v^{\\prime}).$ . 10 if $f(S_{i-1}^{j})<f(S_{i-1}^{j}-v_{i}^{j}+u_{i}^{j})$ then $S_{i}^{j}\\leftarrow S_{i-1}^{j}-v_{i}^{j}+u_{i}^{j}$ . 11 else $S_{i}^{j}\\gets S_{i-1}^{j}$ . 12 Pick a uniformly random integer $0\\leq i^{*}<L$ . 13 if for every integer $0\\leq t\\leq k$ it holds that $\\operatorname*{max}_{S\\subseteq{N\\backslash S_{i^{*}}^{j}},|S|=t}\\sum_{u\\in S}f(u\\mid S_{i^{*}}^{j})\\leq\\operatorname*{min}_{S\\subseteq S_{i^{*}}^{j},|S|=t}\\sum_{v\\in S}f(v\\mid S_{i^{*}}^{j}-v)+\\varepsilon f(S_{i^{*}}^{j})$ en return $S_{i^{*}}^{j}$ . 14 return FAILURE. ", "page_idx": 3}, {"type": "text", "text": "Algorithm 1 starts by finding an initial solution $S_{0}$ guaranteeing constant approximation (we implement this step using the deterministic $^1\\!/\\!4$ -approximation algorithm of Balkanski et al. [2]). If the size of the initial solution is less than $k$ (i.e., $|S_{0}|<k)$ , the algorithm adds to it $k-|S_{0}|$ dummy elements. Then, Algorithm 1 makes roughly $\\log_{2}\\varepsilon^{-1}$ attempts to find a good output. Each attempt trys to improve the (same) initial solution using $L$ iterations. Each iteration consisting of three steps: In Step (i), the algorithm samples $\\frac{n}{k}$ items, and picks the element $u$ from the sample with the largest marginal contribution to the current solution $S_{i-1}$ . If there are no elements in the sample with a positive marginal contribution, the algorithm picks a dummy element outside $S_{i-1}$ as $u$ . In Step (ii), the algorithm picks the element $v\\in S_{i-1}$ that has the lowest marginal value, i.e., the element whose removal from $S_{i-1}$ would lead to the smallest drop in value. In Step (iii), the algorithm swaps the elements $u$ and $v$ if such a swap increases the value of the current solution. Once $L$ iterations are over, the algorithm picks a uniformly random solution among all the solutions seen during this attempt (recall that the algorithm makes roughly $\\log_{2}\\varepsilon^{-1}$ attempts to find a good solution). If the random solution found obeys the technical condition given on Line 13, then the algorithm returns it. Otherwise, the algorithm continues to the next attempt. If none of the attempts returns a set, the algorithm admits failure. ", "page_idx": 3}, {"type": "text", "text": "2.2 Guided stochastic greedy ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we prove Theorem 2.2, which provides the last step of our main algorithm. ", "page_idx": 3}, {"type": "text", "text": "Theorem 2.2. There exists an algorithm that given a positive integer $k$ , a value $\\varepsilon\\in(0,1)$ , a value $t_{s}\\,\\in\\,[0,1]$ , a non-negative submodular function $f\\colon\\dot{2}^{{\\mathcal{N}}}\\rightarrow\\mathbb{R}_{\\geq0}$ , and a set $Z\\subseteq{\\mathcal{N}}$ obeying the inequalities stated in Theorem 2.1, outputs a solution $S_{k}$ , obeying ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[f(S_{k})]\\geq\\Big(\\frac{k-\\lceil t_{s}\\cdot k\\rceil}{k}\\alpha^{k-\\lceil t_{s}\\cdot k\\rceil-1}+\\alpha^{k-\\lceil t_{s}\\cdot k\\rceil}-\\alpha^{k}\\Big)f(\\mathbb{O}\\mathbb{P}\\mathbb{T})+}\\\\ &{\\qquad\\qquad+\\left(\\alpha^{k}+\\alpha^{k-1}-\\frac{2k-\\lceil t_{s}\\cdot k\\rceil}{k}\\alpha^{k-\\lceil t_{s}\\cdot k\\rceil-1}\\right)f(\\mathbb{O}\\mathbb{P}\\mathbb{T}\\cup Z)}\\\\ &{\\qquad\\qquad+\\left(\\alpha^{k}-\\alpha^{k-\\lceil t_{s}\\cdot k\\rceil}\\right)f(\\mathbb{O}\\mathbb{P}\\mathbb{T}\\cap Z)-2\\varepsilon f(\\mathbb{O}\\mathbb{P}\\mathbb{T})\\ ,}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The algorithm used to prove Theorem 2.2 is Algorithm 2. This algorithm starts with an empty set and adds elements to it in iterations (at most one element per iteration) until its final solution is ready after $k$ iterations. In its first $\\lceil k\\cdot t_{s}\\rceil$ iterations, the algorithm ignores the elements of $Z$ , and in the other iterations, it considers all elements. However, except for this difference, the behavior of the algorithm in all iterations is very similar. Specifically, in each iteration $i$ the algorithm does the following two steps. In Step (i), the algorithm samples a subset $M_{i}$ containing $O_{\\varepsilon}(\\bar{n}/k)$ elements from the data. In Step (ii), the algorithm considers a subset of $M_{i}$ (of size either $s_{1}\\lceil p(n-\\rceil Z\\backslash)\\rceil$ or $s_{2}[p n]$ ) containing the elements of $M_{i}$ with the largest marginal contributions with respect to the current solution $S_{i-1}$ , and adds a uniformly random element out of this subset to the solution (if this element has a positive marginal contribution). ", "page_idx": 4}, {"type": "text", "text": "Algorithm 2: Guided Stochastic Greedy ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "input :A set $Z\\subseteq N$ , a positive integer $k\\geq1$ , values $\\varepsilon\\in(0,1)$ and $t_{s}\\in[0,1]$ , and a non-negative   \nsubmodular function $f$   \noutput :A set $S_{k}\\subseteq{\\mathcal{N}}$   \n1 Initialize $S_{0}\\gets\\emptyset$ .   \n2 Define $p\\gets\\operatorname*{min}\\{1,8k^{-1}\\varepsilon^{-2}\\ln(2\\varepsilon^{-1})\\}$ .   \n3 Define $s_{1}\\leftarrow k/(\\dot{n}-|Z|)$ and $s_{2}\\leftarrow k/n$ .   \n4 for $i=1$ to $\\lceil k\\cdot t_{s}\\rceil$ do   \n5 Let $M_{i}\\subseteq\\mathcal{N}\\setminus Z$ be a uniformly random set containing $\\lceil p\\cdot(n-\\lvert Z\\rvert)\\rceil$ elements.   \n6 Let $d_{i}$ be uniformly random scalar from the range $(0,s_{1}\\lceil p\\cdot(n-\\lvert Z\\rvert)\\rceil]$ .   \n7 Let $u_{i}$ be an element of $M_{i}$ associated with the $\\lceil d_{i}\\rceil$ -th largest marginal contribution to $S_{i-1}$ (if   \n$\\lceil d_{i}\\rceil>\\lvert M_{i}\\rvert$ , we set $u_{i}$ to be a dummy element having 0 marginal contribution to $f$ ).   \n8 if $f(i_{i}\\mid S_{i-1})\\ge0$ then   \n9 $S_{i}\\leftarrow S_{i-1}\\cup\\{u_{i}\\}$ .   \n10 else   \n11 \u2014 $S_{i}\\gets S_{i-1}$ .   \n12 for $i=\\lceil\\boldsymbol k\\cdot\\boldsymbol t_{s}\\rceil+1$ to $k$ do   \n13 Let $M_{i}\\subseteq\\mathcal{N}$ be a uniformly random set containing $\\lceil p\\cdot n\\rceil$ elements.   \n14 Let $d_{i}$ be uniformly random scalar from the range $(0,s_{2}[p\\cdot n]]$ .   \n15 Let $u_{i}$ be an element of $M_{i}$ associated with the $\\lceil d_{i}\\rceil$ -th largest marginal contribution to $S_{i-1}$ .   \n16 if $f(u_{i}\\mid S_{i-1})\\ge0$ then $S_{i}\\leftarrow S_{i-1}\\cup\\{u_{i}\\}$ .   \n17 else $S_{i}\\gets S_{i-1}$ .   \n18 return $S_{k}$ . ", "page_idx": 4}, {"type": "text", "text": "2.3 0.385-Approximation guarantee ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, our objective is to prove the following theorem. ", "page_idx": 4}, {"type": "text", "text": "Theorem 2.3. Given an integer $k\\geq1$ and a non-negative submodular function $f\\colon2^{{\\mathcal{N}}}\\to\\mathbb{R}_{\\geq0}$ , there exists an 0.385-approximation algorithm for the problem of finding a set $S\\subseteq N$ of size at most $k$ maximizing $f$ . This algorithm uses $O(n+k^{2})$ queries to the objective function. ", "page_idx": 4}, {"type": "text", "text": "The algorithm used to prove Theorem 2.3 is Algorithm 3. Our technical guarantee for Algorithm 3 is given as Lemma 2.4. When $k$ is large enough, this lemma immediately implies Theorem 2.3 by choosing $\\varepsilon$ to be a small enough positive constant. If $k$ is small, getting Theorem 2.3 from Lemma 2.4 requires a three steps process. First, we choose an integer constant $\\rho$ such that $\\rho k$ is large enough, and we create a new ground set $\\mathcal{N}_{\\rho}=\\{u_{i}\\ |\\ u\\in\\mathcal{N},i\\in\\bar{[\\rho]}\\}$ and a new objective function $g\\colon2^{\\ensuremath{\\mathcal{N}}_{\\rho}}\\rightarrow\\ensuremath{\\mathbb{R}}$ defined as $g(S)=\\mathbb{E}[f(R(S))]$ , where $R(S)$ is a random subset of $\\mathcal{N}$ that includes every element $u\\in N$ with probability $|S\\cap\\!\\left(\\{u\\}\\times[\\rho]\\right)\\rvert/\\rho$ . Then, we use Lemma 2.4 to get a set $\\hat{S}$ that provides 0.385-approximation for the problem $\\operatorname*{max}\\{g(S)~|~|S|\\leq\\rho k\\}$ . Finally, the Pipage Rounding technique of [8] can be used to get from $\\hat{S}$ a 0.385-approximation for our original problem. Notice that since the size of $\\hat{S}$ is constant (as we consider the case of a small $k$ ), this rounding can be done using a constant number of queries to the objective. ", "page_idx": 4}, {"type": "text", "text": "Lemma 2.4. Algorithm 3 makes $O_{\\varepsilon}(n+k^{2})$ queries to the objective function, and returns a set whose expected value is at least $(c-\\dot{O}(\\varepsilon+\\dot{k}^{-\\bar{1}}))f(\\mathbb{O}\\mathbb{P T})$ for some constant $c>0.385$ . ", "page_idx": 4}, {"type": "text", "text": "Algorithm 3: A 0.385-approximation algorithm for submodular maximization ", "page_idx": 5}, {"type": "text", "text": "input :A positive integer $k\\geq1$ , a non-negative submodular function $f$ , error parameter $\\overline{{\\varepsilon\\in(0,1)}}$ and a flip point 0 \u2264ts \u22641 output :A set $S_{L}\\subseteq{\\mathcal{N}}$ 1 $Z\\gets$ FAST-LOCAL- $\\mathrm{SEARCH}(k,f,\\varepsilon,L:=\\lceil2k/(\\varepsilon(1-1/e))\\rceil.$ . 2 if the last algorithm did not fail then 3 $A\\leftarrow$ GUIDED-STOCHASTIC-GREEDY $(Z,k,t_{s},\\varepsilon)$ . 4 return the set maximizing $f$ among $Z$ and $A$ . 5 else return $\\varnothing$ . ", "page_idx": 5}, {"type": "text", "text": "Proof. According to the proof of Theorem 2.1, our choice of the parameter $L$ in Algorithm 1 guarantees that with probability at least $1-\\varepsilon$ the set $Z$ obeys the inequalities ", "page_idx": 5}, {"type": "equation", "text": "$$\nf(Z)\\geq\\frac{f(Z\\cup\\mathbb{O}\\mathbb{P}\\mathbb{T})+f(Z\\cap\\mathbb{O}\\mathbb{P}\\mathbb{T})}{2+\\varepsilon}\\qquad\\mathrm{and}\\qquad f(Z)\\geq\\frac{f(Z\\cap\\mathbb{O}\\mathbb{P}\\mathbb{T})}{1+\\varepsilon}\\enspace.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Let us denote by $\\mathcal{E}$ the event that these inequalities hold. By Theorem 2.2, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[f(A)\\mid\\mathcal{E}]\\ge\\Big(\\frac{k-\\lceil t_{s}\\cdot k\\rceil}{k}\\alpha^{k-\\lceil t_{s}\\cdot k\\rceil-1}+\\alpha^{k-\\lceil t_{s}\\cdot k\\rceil}-\\alpha^{k}\\Big)f(\\mathbb{O}\\mathbb{P}\\mathbb{T})+}\\\\ &{\\qquad\\qquad\\qquad+\\left(\\alpha^{k}+\\alpha^{k-1}-\\frac{2k-\\lceil t_{s}\\cdot k\\rceil}{k}\\alpha^{k-\\lceil t_{s}\\cdot k\\rceil-1}\\right)\\mathbb{E}[f(\\mathbb{O}\\mathbb{P}\\mathbb{T}\\cup Z)\\mid\\mathcal{E}]}\\\\ &{\\qquad\\qquad\\quad+\\left(\\alpha^{k}-\\alpha^{k-\\lceil t_{s}\\cdot k\\rceil}\\right)\\mathbb{E}[f(\\mathbb{O}\\mathbb{P}\\mathbb{T}\\cap Z)\\mid\\mathcal{E}]-2\\varepsilon f(\\mathbb{O}\\mathbb{P}\\mathbb{T})\\ \\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Since the output of Algorithm 2 is the better set among $A$ and $Z$ , we can lower bound its value by any convex combination of lower bounds on the values of $A$ and $Z$ . More formally, if we denote by $p_{1},p_{2}$ and $p_{3}$ any three non-negative values that add up to 1, then we get ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\operatorname*{max}\\{f(A),f(Z)\\}\\mid\\mathcal{E}]\\ge p_{3}\\Big(\\frac{k-\\lceil t_{s}\\cdot k\\rceil}{k}\\alpha^{k-\\lceil t_{s}\\cdot k\\rceil-1}+\\alpha^{k-\\lceil t_{s}\\cdot k\\rceil}-\\alpha^{k}\\Big)f(\\mathbb{O}\\mathbb{P}\\mathbb{T})}\\\\ &{\\qquad+\\left(\\frac{p_{1}}{2+\\varepsilon}+p_{3}\\Big(\\alpha^{k}+\\alpha^{k-1}-\\frac{2k-\\lceil t_{s}\\cdot k\\rceil}{k}\\alpha^{k-\\lceil t_{s}\\cdot k\\rceil-1}\\Big)\\right)\\mathbb{E}[f(\\mathbb{O}\\mathbb{P}\\mathbb{T}\\cup Z)\\mid\\mathcal{E}]}\\\\ &{\\qquad+\\left(\\frac{p_{2}}{1+\\varepsilon}+\\frac{p_{1}}{2+\\varepsilon}-p_{3}\\Big(\\alpha^{k-\\lceil t_{s}\\cdot k\\rceil}-\\alpha^{k}\\Big)\\right)\\mathbb{E}[f(\\mathbb{O}\\mathbb{P}\\mathbb{T}\\cap Z)\\mid\\mathcal{E}]-2\\varepsilon p_{3}f(\\mathbb{O}\\mathbb{P}\\mathbb{T})\\enspace.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "To simplify the above inequality, we need to bound some of the terms in it. First, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{k-\\lceil t_{s}\\cdot k\\rceil}{k}\\alpha^{k-\\lceil t_{s}\\cdot k\\rceil-1}+\\alpha^{k-\\lceil t_{s}\\cdot k\\rceil}-\\alpha^{k}\\geq\\bigg(2-t_{s}-\\frac{1}{k}\\bigg)\\alpha^{k(1-t_{s})}-\\alpha^{k}}\\\\ &{\\qquad\\qquad\\geq\\bigg(2-t_{s}-\\frac{1}{k}\\bigg)e^{t_{s}-1}\\bigg(1-\\displaystyle\\frac{1}{k}\\bigg)^{1-t_{s}}-e^{-1}}\\\\ &{\\qquad\\qquad\\geq\\bigg(2-t_{s}-\\frac{3}{k}\\bigg)e^{t_{s}-1}-e^{-1}\\geq\\big(2-t_{s}-e^{-t_{s}}\\big)e^{t_{s}-1}-\\displaystyle\\frac{3}{k}\\enspace,}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the first inequality holds since $\\lceil t_{s}\\cdot k\\rceil\\leq t_{s}\\cdot k+1$ , $\\alpha\\leq1$ , the second inequality follows since $\\begin{array}{r}{\\alpha^{k(1-t_{s})}\\geq e^{t_{s}-1}\\big(1-\\frac{1}{k}\\big)^{1-t_{s}}}\\end{array}$ , and the last inequality holds since $e^{t_{s}-1}\\leq1$ . Second, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\alpha^{k}+\\alpha^{k-1}-\\cfrac{2k-\\lceil t_{s}\\cdot k\\rceil}{k}\\alpha^{k-\\lceil t_{s}\\cdot k\\rceil-1}\\geq2e^{-1}-\\cfrac{2k-t_{s}\\cdot k}{k}\\alpha^{k-t_{s}\\cdot k-2}-\\cfrac{2e^{-1}}{k}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\geq2e^{-1}-(2-t_{s})e^{t_{s}-1}-\\cfrac{8+2e^{-1}}{k}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=-e^{t_{s}-1}\\big(2-t_{s}-2e^{-t_{s}}\\big)-\\cfrac{8+2e^{-1}}{k}\\ ,}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the first inequality holds since $\\begin{array}{r}{\\alpha^{k-1}\\geq\\alpha^{k}\\geq e^{-1}\\big(1-\\frac{1}{k}\\big)}\\end{array}$ , and the second inequality holds since $\\alpha^{k-t_{s}\\cdot k-2}\\leq e^{t_{s}-1}{+}4/k$ . Finally, it holds that $\\begin{array}{r}{\\alpha^{k}\\!-\\!\\alpha^{k-\\lceil t_{s}\\cdot\\dot{k}\\rceil}\\geq e^{-1}(1\\!-\\!\\frac1k)\\!-\\!e^{t_{s}-1}\\big/(1\\!-\\!\\frac1k)\\geq}\\end{array}$ $\\begin{array}{r}{e^{-1}(1-\\frac{1}{k})-e^{t_{s}-1}(1+\\frac{1}{k})\\geq-e^{t_{s}-1}(1-e^{-t_{s}})-\\frac{2}{k}}\\end{array}$ . ", "page_idx": 5}, {"type": "text", "text": "Plugging all the above lower bounds into Inequality (1) yields the promised simplified guarantee that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\operatorname*{max}\\{f(A),f(Z)\\}\\mid\\mathcal{E}]\\ge p_{3}\\big(2-t_{s}-e^{-t_{s}}\\big)e^{t_{s}-1}f(\\mathbb{O}\\mathbb{P}\\mathbb{T})-O\\big(\\varepsilon+k^{-1}\\big)f(\\mathbb{O}\\mathbb{P}\\mathbb{T})}\\\\ &{\\qquad\\qquad\\qquad+\\left(\\frac{p_{1}}{2+\\varepsilon}-p_{3}e^{t_{s}-1}\\big(2-t_{s}-2e^{-t_{s}}\\big)-O(k^{-1})\\right)\\mathbb{E}[f(\\mathbb{O}\\mathbb{P}\\mathbb{T}\\cup Z)\\mid\\mathcal{E}]}\\\\ &{\\qquad\\qquad\\qquad+\\left(\\frac{p_{2}}{1+\\varepsilon}+\\frac{p_{1}}{2+\\varepsilon}-p_{3}e^{t_{s}-1}\\big(1-e^{-t_{s}}\\big)-O(k^{-1})\\right)\\mathbb{E}[f(\\mathbb{O}\\mathbb{P}\\mathbb{T}\\cap Z)\\mid\\mathcal{E}]\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "By [3], for an appropriate choice of values for $p_{1},p_{2},p_{3}$ and $t_{s}$ the last inequality implies ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\operatorname*{max}\\{f(A),f(Z)\\}\\mid\\mathcal{E}]\\ge(c-O(\\varepsilon+k^{-1}))f(\\mathbb{O P T})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad-O(k^{-1})\\cdot\\mathbb{E}[f(\\mathbb{O P T}\\cup Z)+f(\\mathbb{O P T}\\cap Z)]}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "for some constant $c>0.385$ . To get from the last inequality the bound on $\\mathbb{E}[\\operatorname*{max}\\{f(A),f(Z)\\}\\mid\\mathcal{E}]$ stated in the lemma, we need to show that the conditioning on $\\mathcal{E}$ and the last term of the inequality can both be dropped. To see why the conditioning can be dropped, note that the event $\\mathcal{E}$ happens with probability at least $1-\\varepsilon$ , and when it does not happen the set returned by the algorithm still has a nonnegative value. These observations show together that removing the conditioning on $\\mathcal{E}$ in Inequality (2) only affects the constant inside the big $O$ notation. Notice now that since $\\mathbb{O P T}\\cap Z\\subseteq\\mathbb{O P T}$ is always a feasible solution, it deterministically holds that $f(\\mathbb{O P T}\\cap Z)\\le f(\\mathbb{O P T})$ . Similarly, since $Z$ is a feasible solution, the submodularity of $f$ guarantees that $f(\\mathbb{O P T}\\cup Z)+f(\\mathbb{O P T}\\cap Z)\\leq$ $f(\\mathbb{O P T})+f(Z)\\leq2f(\\mathbb{O P T})$ . These two bounds allow us to drop the last term of Inequality (2) at the cost of increasing (again) the constant inside the big $O$ notation. ", "page_idx": 6}, {"type": "text", "text": "To complete the proof of the lemma, note that Line 1 of Algorithm 3 requires $O_{\\varepsilon}\\left(n+k^{2}\\right)$ queries to the objective function as shown in the proof of Theorem 2.1, while Line 3 of Algorithm 3 requires $O_{\\varepsilon}(n)$ queries to the objective function as dictated by Theorem 2.2. \u53e3 ", "page_idx": 6}, {"type": "text", "text": "3 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To emphasize the effectiveness of our suggested method from Section 2, in this section, we empirically compare Algorithm 3 with two benchmark algorithms on three machine-learning applications: movie recommendation, image summarization, and revenue maximization. Each one of these applications necessitates maximization of a non-monotone submodular function. The benchmark algorithms we consider are the Random Greedy algorithm of Buchbinder et al. [5], and the Random Sampling algorithm of [6]. These algorithms are the current state-of-the-art practical algorithms for maximizing non-monotone submodular functions. ", "page_idx": 6}, {"type": "text", "text": "As stated, Algorithm 2 requires $O\\big(\\frac{n\\!\\cdot\\!\\ln\\varepsilon^{-1}}{\\varepsilon^{2}}\\big)$ queries to the objective function, where the dependence on $\\varepsilon$ comes from the choice of value for the parameter $p$ of the algorithm. However, we have found out that in practice a more modest choice of value for $p$ suffices. Specifically, in our experiments, we have replaced Line 2 of Algorithm 2 with $\\begin{array}{r}{p\\leftarrow\\operatorname*{min}\\{\\bar{1},\\frac{8}{k\\cdot\\varepsilon}\\}}\\end{array}$ . Throughout the experiments, we have set $\\varepsilon=0.1$ ; and all the reported results are averaged across 8 executions. We use shades in our plots to depict the standard deviations of the individual results obtained in these 8 executions. ", "page_idx": 6}, {"type": "text", "text": "Software/Hardware. Our algorithms were implemented in Python 3.11 using mainly \u201cNumpy\u201d [19], and Numba [23]. The implementations\u2019 code can be found at https://github.com/muradtuk/ 385ApproximationSubMax. The experiments were performed on a 2.2GHz i9-13980HX (24 cores total) machine with 64GB RAM. ", "page_idx": 6}, {"type": "text", "text": "3.1 Personalized movie recommendation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Consider a movie recommendation system in which each user specifies what genres they are interested in, and the system has to provide a representative subset of movies from these genres. Assume that each movie is represented by a vector consisting of users\u2019 ratings for the corresponding movie. One challenge here is that each user does not necessarily rate all the movies. Hence, the vectors representing the movies do not necessarily have similar sizes. To overcome this challenge, low-rank matrix completion techniques [9] can be performed on the matrix with missing values to obtain a complete rating matrix. Formally, given a few ratings from $k$ users to $n$ movies we obtain in this way a rating matrix $\\mathbf{M}$ of size $k\\times n$ . Following [33, 30], to score the quality of a selected subset of movies, we use the function $\\begin{array}{r}{f(S)=\\sum_{u\\in\\mathcal{N}}\\sum_{v\\in S}s_{u,v}-\\lambda\\sum_{u\\in S}\\sum_{v\\in S}s_{u,v}}\\end{array}$ . Here, $\\mathcal{N}$ is the set of $n$ movies, $\\lambda\\in[0,1]$ is a parameter and $s_{u,v}$ denotes the similarity between movies $u$ and $v$ (the similarity $s_{u,v}$ can be calculated based on the matrix $\\mathbf{M}$ in multiple ways: cosine similarity, inner product, etc). Note that the first term in $f$ \u2019s definition captures the coverage, while the second term captures diversity. Thus, the parameter $\\lambda$ controls the importance of diversity in the returned subset. For any $\\lambda\\leq0.5$ , $f(S)$ is monotone [34], however, it can be non-monotone for larger values of $\\lambda$ . ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "We followed the experimental setup of the prior works [33, 30] and used a subset of movies from the MovieLens data set [18] which includes 10,437 movies. Each movie in this data set is represented by a 25 dimensional feature vector calculated using user ratings, and we used the inner product similarity to obtain the similarity values $s_{u,v}$ based on these vectors. When experimenting with this application, we fixed $\\lambda$ to be either 0.55 or 0.75, and varied $k$ . ", "page_idx": 7}, {"type": "image", "img_path": "l6iICoILGB/tmp/2d79f9745ca48a1beb5c371befae386155278a016069a622dcecba188eac177e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 1: Experimental results for Personalized Movie Recommendation. Plots (a) and (b) compare the output of our algorithm with the benchmark algorithms mentioned at the beginning of Section 3 for a particular value of the parameter $\\lambda$ and a varying number $k$ of movies. Plots (c) and (d) compare the number of queries used by the various algorithms. ", "page_idx": 7}, {"type": "text", "text": "The results of these experiments are depicted in Figure 1. One can observe that our proposed method, Algorithm 3, demonstrates superior performance compared to the other methods. Moreover, this performance is stable, and presents a much smaller variance compared to the variance in the performance of the two benchmark algorithms. The number of queries used by our algorithm is only slightly larger than the number of queries used by the Random Sampling algorithm of [6], and is typically smaller than the number of queries used by the Random Greedy algorithm of Buchbinder et al. [5], sometimes by as much as a factor of 2. ", "page_idx": 7}, {"type": "text", "text": "3.2 Personalized image summarization ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Consider a setting in which we get as input a collection $\\mathcal{N}$ of images from $\\ell$ disjoint categories (e.g., birds, dogs, cats) and the user specifies $r\\in[\\ell]$ categories, and then demands a subset of the images in these categories that summarizes all the images of the categories. Following [30] again, to evaluate a given subset of images, we use the function $\\begin{array}{r}{\\bar{f}(S)=\\sum_{u\\in\\mathcal{N}}\\operatorname*{max}_{v\\in S}s_{u,v}-\\bar{\\frac{1}{|\\mathcal{N}|}}\\sum_{u\\in S}\\sum_{v\\in S}s_{u,v}.}\\end{array}$ , where $s_{u,v}$ is a non-negative similarity between images $u$ and $v$ . ", "page_idx": 7}, {"type": "text", "text": "To obtain the similarity between pair of images $u,v$ , we utilized the DINO-VITB16 model [10] from HuggingFace [44] as the feature encoder for vision datasets. Specifically, the final layer CLS token embedding output was used as the feature representation. The similarity between pairs of images was then computed as the cosine similarity of the corresponding embedding vectors. To experiment in this setting, we used three datasets: (i) CIFAR10 [21] \u2013 A dataset of 50,000 images belonging to 10 different classes (categories). (ii) CIFAR100 [21] \u2013 A dataset of 50,000 images belonging to 100 different classes. (iii) Tiny ImageNet [24] \u2013 A dataset of 100,000 images belonging to 200 different classes. In each one of our experiments, the task was to summarize a set of 10,000 images sampled uniformly from one of these datasets. The upper bound $k$ on the number images allowed in the summary varied between experiments. The results of our experiments are depicted in Figure 2. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Similarly to Section 3.1, we observe that our algorithm (Algorithm 3) produces higher values compared to the state-of-the-art practical algorithms, and enjoys a lower variance in the quality of its output. However, due to the small values used for $k$ , our algorithm requires significantly more queries to the objective function compared to the two other algorithms. ", "page_idx": 8}, {"type": "image", "img_path": "l6iICoILGB/tmp/69f1a6cc93fdde3fa6c65020c22093a3e946c4a076feb33eb1662d39ac7fcfd8.jpg", "img_caption": ["(a) Function values for the CIFAR10 dataset. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "l6iICoILGB/tmp/33a32350016d94ff249839231f7a19ca04e406e2c2c72fc79ae2c51b9623ff92.jpg", "img_caption": ["(b) Function values for the CIFAR100 dataset. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "l6iICoILGB/tmp/43c51ea142e40823606516abeb391072ca5992026764e828641bcf111a07b95b.jpg", "img_caption": ["(c) Function values for the Tiny ImageNet dataset. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "l6iICoILGB/tmp/a8233db639d1265ce634ac73c1fd5947d180512081e88313bf68b43ed148e8f9.jpg", "img_caption": ["(d) Number of queries for the CIFAR10 dataset. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "l6iICoILGB/tmp/2535129527f4e5101234c11fea2c090cb6f605e97706cf0c9fd07f1487868dbd.jpg", "img_caption": ["(e) Number of queries for the CIFAR100 dataset. (f) Number of queries for the Tiny ImageNet dataset. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "l6iICoILGB/tmp/54b4258244d83ae1760bfb02bffa3e9550f35bad81601ee4046ed0e15d14a07e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 2: Experimental results for Personalized Image Summarization. Plots (a)\u2013(c) compare the output of our algorithm with the benchmark algorithms mentioned at the beginning of Section 3 for a varying number $k$ of images. Each plot corresponds to a different dataset. Plots (d)\u2013(f) compare the number of queries used by the various algorithms. ", "page_idx": 8}, {"type": "text", "text": "3.3 Revenue maximization ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Consider a company whose objective is to promote a product to users to boost revenue through the \u201cword-of-mouth\u201d effect. More specifically, given a social network, we need to choose a subset of up to $k$ users to receive a product for free in exchange for advertising it to their network neighbors, and the goal is to choose users in a manner that maximizes revenue. The problem of optimizing this objective can be formalized as follows. The input is a weighted undirected graph $G=(V,E)$ representing a social network, where $w_{i j}$ represents the weight of the edge between vertex $i$ and vertex $j$ (with $w_{i j}=0$ if the edge $(i,j)$ is absent from the graph). Given a set $S\\subseteq V$ of users who have become advocates for the product, the expected revenue generated is proportional to the total influence of $S$ \u2019s users on non-advocate users, formally expressed as $\\textstyle f(x)\\stackrel{}{=}\\sum_{i\\in S}\\sum_{j\\in V\\backslash S}w_{i j}$ . It has been demonstrated that $f$ is non-monotone and submodular [1]. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "image", "img_path": "l6iICoILGB/tmp/fc11771c437b573259cd203090a736bfd7547f4495e735961a6be9485e0fe4a4.jpg", "img_caption": ["(a) Function values for Advogato network dataset. "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "l6iICoILGB/tmp/74eec4107eae4ff0d99a5cfee8a31f73d1b51e83641d88a6d1b984820d4cea68.jpg", "img_caption": ["(b) Function values for Facebook network dataset. "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "l6iICoILGB/tmp/51c231f1b2c48e5fee335dcd9d62ec76174d2e6e442cb3b67026941be5bf23f3.jpg", "img_caption": ["(c) Number of queries for Advogato network dataset.(d) Number of queries for Facebook network dataset. "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "l6iICoILGB/tmp/26ebde44c0f0f7507ca668f6b04477057bb0ef0da43acf0fa1ec86b2c8ab729a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Figure 3: Experimental results for Revenue Maximization. Plots (a) and (b) compare the output of our algorithm with the benchmark algorithms mentioned at the beginning of Section 3 for a varying number $k$ of images on the Advogato and Facebook network datasets. Plots (c) and (d) compare the number of queries used by the various algorithms. ", "page_idx": 9}, {"type": "text", "text": "In our experiment, we compared the performance of Algorithm 3 and the two benchmark algorithms on the Facebook network [42] and the Advogato network [29]. The results of this experiment are depicted in Figure 3. Once again, our algorithm enjoys both better output values and lower standard deviations compared to the benchmark algorithms. Our algorithm uses more queries compared to the Random Sampling algorithm of [6], but the ratio between the number of queries used by the two algorithms tends to decrease as $k$ increases. The behavior of the Random Greedy algorithm of [5] greatly depends on $k$ . For smaller values of $k$ this algorithm requires roughly as many queries as Random Sampling, but for larger value of $k$ it requires significantly more queries than our algorithm. ", "page_idx": 9}, {"type": "text", "text": "4 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we have presented a novel algorithm for submodular maximization subject to cardinality constraint that combines a practical query complexity of $O(n+k^{2})$ with an approximation guarantee of 0.385, which improves over the $^1\\!/e$ -approximation of the state-of-the-art practical algorithms. In addition to giving a theoretical analysis of our algorithm, we have demonstrated its empirical superiority (compared to practical state-of-the-art methods) in various machine learning applications. We hope future work will be able to improve the query complexity of our algorithm to be cleanly linear without sacrificing either the approximation guarantee or the practicality of the algorithm. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgment ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The work of Loay Mualem and Moran Feldman was supported in part by Israel Science Foundation (ISF) grant number 459/20. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Eric Balkanski, Adam Breuer, and Yaron Singer. Non-monotone submodular maximization in exponentially fewer iterations. Advances in Neural Information Processing Systems, 31, 2018. [2] Eric Balkanski, Steven DiSilvio, and Alan Kuhnle. Submodular maximization in exactly $n$ queries. CoRR, abs/2406.00148, 2024.   \n[3] Niv Buchbinder and Moran Feldman. Constrained submodular maximization via a nonsymmetric technique. Mathematics of Operations Research, 44(3):988\u20131005, 2019. [4] Niv Buchbinder and Moran Feldman. Constrained submodular maximization via new bounds for dr-submodular functions. arXiv preprint arXiv:2311.01129, 2023.   \n[5] Niv Buchbinder, Moran Feldman, Joseph Naor, and Roy Schwartz. Submodular maximization with cardinality constraints. In Proceedings of the twenty-fifth annual ACM-SIAM symposium on Discrete algorithms, pages 1433\u20131452. SIAM, 2014.   \n[6] Niv Buchbinder, Moran Feldman, and Roy Schwartz. Comparing apples and oranges: Query trade-off in submodular maximization. Mathematics of Operations Research, 42(2):308\u2013329, 2017.   \n[7] Niv Buchbinder, Moran Feldman, Joseph Seff,i and Roy Schwartz. A tight linear time (1/2)- approximation for unconstrained submodular maximization. SIAM Journal on Computing, 44(5):1384\u20131402, 2015.   \n[8] Gruia C\u02d8alinescu, Chandra Chekuri, Martin P\u00e1l, and Jan Vondr\u00e1k. Maximizing a monotone submodular function subject to a matroid constraint. SIAM J. Comput., 40(6):1740\u20131766, 2011.   \n[9] Emmanuel J Cand\u00e8s and Benjamin Recht. Exact matrix completion via convex optimization. Foundations of Computational mathematics, 9(6):717\u2013772, 2009.   \n[10] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 9650\u20139660, 2021.   \n[11] Chandra Chekuri, Jan Vondr\u00e1k, and Rico Zenklusen. Submodular function maximization via the multilinear relaxation and contention resolution schemes. SIAM J. Comput., 43(6):1831\u20131879, 2014.   \n[12] Yixin Chen, Ankur Nath, Chunli Peng, and Alan Kuhnle. Guided combinatorial algorithms for submodular maximization. arXiv preprint arXiv:2405.05202, 2024.   \n[13] Abhimanyu Das and David Kempe. Submodular meets spectral: greedy algorithms for subset selection, sparse approximation and dictionary selection. In ICML, pages 1057\u20131064, 2011.   \n[14] Ethan R. Elenberg, Alexandros G. Dimakis, Moran Feldman, and Amin Karbasi. Streaming weak submodularity: interpreting neural networks on the fly. In NeurIPS, pages 4047\u20134057, 2017.   \n[15] Alina Ene and Huy L Nguyen. Constrained submodular maximization: Beyond 1/e. In 2016 IEEE 57th Annual Symposium on Foundations of Computer Science (FOCS), pages 248\u2013257. IEEE, 2016.   \n[16] Uriel Feige, Vahab S. Mirrokni, and Jan Vondr\u00e1k. Maximizing non-monotone submodular functions. SIAM J. Comput., 40(4):1133\u20131153, 2011.   \n[17] Kai Han, Shuang Cui, Benwei Wu, et al. Deterministic approximation for submodular maximization over a matroid in nearly linear time. Advances in Neural Information Processing Systems, 33:430\u2013441, 2020.   \n[18] F. Maxwell Harper and Joseph A. Konstan. The movielens datasets: History and context. Acm transactions on interactive intelligent systems (TiiS), 5(4):1\u201319, 2015.   \n[19] Charles R. Harris, K. Jarrod Millman, St\u00e9fan J van der Walt, Ralf Gommers, Pauli Virtanen, David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert Kern, Matti Picus, Stephan Hoyer, Marten H. van Kerkwijk, Matthew Brett, Allan Haldane, Jaime Fern\u00e1ndez del R\u00edo, Mark Wiebe, Pearu Peterson, Pierre G\u00e9rard-Marchant, Kevin Sheppard, Tyler Reddy, Warren Weckesser, Hameer Abbasi, Christoph Gohlke, and Travis E. Oliphant. Array programming with NumPy. Nature, 585:357\u2013362, 2020.   \n[20] Chien-Chung Huang and Naonori Kakimura. Multi-pass streaming algorithms for monotone submodular function maximization. Theory of Computing Systems, 66(1):354\u2013394, 2022.   \n[21] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. Technical report, University of Toronto, Toronto, Ontario, Canada, 2009.   \n[22] Alan Kuhnle. Quick streaming algorithms for maximization of monotone submodular functions in linear time. In International Conference on Artificial Intelligence and Statistics, pages 1360\u20131368. PMLR, 2021.   \n[23] Siu Kwan Lam, Antoine Pitrou, and Stanley Seibert. Numba: A llvm-based python jit compiler. In Proceedings of the Second Workshop on the LLVM Compiler Infrastructure in HPC, pages 1\u20136, 2015.   \n[24] Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N, 7(7):3, 2015.   \n[25] Jon Lee, Vahab S. Mirrokni, Viswanath Nagarajan, and Maxim Sviridenko. Non-monotone submodular maximization under matroid and knapsack constraints. In Michael Mitzenmacher, editor, Proceedings of the 41st Annual ACM Symposium on Theory of Computing (STOC), pages 323\u2013332. ACM, 2009.   \n[26] Qi Lei, Lingfei Wu, Pin-Yu Chen, Alex Dimakis, Inderjit S. Dhillon, and Michael J. Witbrock. Discrete adversarial attacks and submodular optimization with applications to text classification. In Ameet Talwalkar, Virginia Smith, and Matei Zaharia, editors, MLSys. mlsys.org, 2019.   \n[27] Wenxin Li, Moran Feldman, Ehsan Kazemi, and Amin Karbasi. Submodular maximization in clean linear time. Advances in Neural Information Processing Systems, 35:17473\u201317487, 2022.   \n[28] L\u00e1szl\u00f3 Lov\u00e1sz. Submodular functions and convexity. In A. Bachem, M. Gr\u00f6tschel, and B. Korte, editors, Mathematical Programming: the State of the Art, pages 235\u2013257. Springer, 1983.   \n[29] Paolo Massa, Martino Salvetti, and Danilo Tomasoni. Bowling alone and trust decline in social network sites. In IEEE International Conference on Dependable, Autonomic and Secure Computing (DASC), pages 658\u2013663. IEEE Computer Society, 2009.   \n[30] Baharan Mirzasoleiman, Ashwinkumar Badanidiyuru, and Amin Karbasi. Fast constrained submodular maximization: Personalized data summarization. In ICML, pages 1358\u20131367. PMLR, 2016.   \n[31] Baharan Mirzasoleiman, Ashwinkumar Badanidiyuru, Amin Karbasi, Jan Vondr\u00e1k, and Andreas Krause. Lazier than lazy greedy. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 1812\u20131818. AAAI Press, 2015.   \n[32] Marko Mitrovic, Ehsan Kazemi, Morteza Zadimoghaddam, and Amin Karbasi. Data summarization at scale: A two-stage submodular approach. In ICML, pages 3596\u20133605. PMLR, 2018.   \n[33] Loay Mualem and Moran Feldman. Using partial monotonicity in submodular maximization. Advances in Neural Information Processing Systems, 35:2723\u20132736, 2022.   \n[34] Loay Mualem and Moran Feldman. Resolving the approximability of offline and online nonmonotone dr-submodular maximization over general convex sets. In International Conference on Artificial Intelligence and Statistics, pages 2542\u20132564. PMLR, 2023.   \n[35] Loay Mualem, Murad Tukan, and Moran Fledman. Bridging the gap between general and down-closed convex sets in submodular maximization. arXiv preprint arXiv:2401.09251, 2024.   \n[36] Loay Raed Mualem, Ethan R Elenberg, Moran Feldman, and Amin Karbasi. Submodular minimax optimization: Finding effective sets. In International Conference on Artificial Intelligence and Statistics, pages 1081\u20131089. PMLR, 2024.   \n[37] George L Nemhauser and Laurence A Wolsey. Best algorithms for approximating the maximum of a submodular set function. Mathematics of operations research, 3(3):177\u2013188, 1978.   \n[38] George L Nemhauser, Laurence A Wolsey, and Marshall L Fisher. An analysis of approximations for maximizing submodular set functions\u2014i. Mathematical programming, 14:265\u2013294, 1978.   \n[39] Ashkan Norouzi-Fard, Jakub Tarnawski, Slobodan Mitrovic, Amir Zandieh, Aidasadat Mousavifar, and Ola Svensson. Beyond 1/2-approximation for submodular maximization on massive data streams. In ICML, pages 3829\u20133838. PMLR, 2018.   \n[40] Benjamin Qi. On maximizing sums of non-monotone submodular and linear functions. In Sang Won Bae and Heejin Park, editors, International Symposium on Algorithms and Computation (ISAAC), volume 248 of LIPIcs, pages 41:1\u201341:16. Schloss Dagstuhl - Leibniz-Zentrum f\u00fcr Informatik, 2022.   \n[41] Murad Tukan, Fares Fares, Yotam Grufinkle, Ido Talmor, Loay Mualem, Vladimir Braverman, and Dan Feldman. Orbslam3-enhanced autonomous toy drones: Pioneering indoor exploration. arXiv preprint arXiv:2312.13385, 2023.   \n[42] Bimal Viswanath, Alan Mislove, Meeyoung Cha, and Krishna P Gummadi. On the evolution of user interaction in facebook. In Proceedings of the 2nd ACM workshop on Online social networks, pages 37\u201342, 2009.   \n[43] Jan Vondr\u00e1k. Symmetry and approximability of submodular maximization problems. SIAM J. Comput., 42(1):265\u2013304, 2013.   \n[44] T Wolf. Huggingface\u2019s transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019.   \n[45] Lifeng Zhou and Pratap Tokekar. Risk-aware submodular optimization for multirobot coordination. IEEE Transactions on Robotics, 38(5):3064\u20133084, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A A warmup version of our algorithm ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we present and analyze a simpler version of our algorithm with the same general structure, but excluding the speedup techniques used to obtain our main result. Inspired by Buchbinder et al. [3], and similar to Algorithm 3, this simpler version (given as Algorithm 6) comprises three steps: (i) Searching for a good initial solution that guarantees a constant approximation to the optimal set. This is accomplished by running the Twin Greedy algorithm of [17], (ii) Finding an (approximate) local search optimum set $Z$ using a local search method. In this simple version, we use the classical local search algorithm for this purpose, which requires $O(n k^{2})$ queries to the objective function. (iii) Lastly, we construct another solution using a version of the Random Greedy algorithm suggested by Buchbinder et al. [5] that avoids elements of the set $Z$ in its first iterations. The algorithm terminates by outputting the better of the two solutions generated in the last two steps. ", "page_idx": 13}, {"type": "text", "text": "A.1 Local search ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we present (as Algorithm 4) a simple local search algorithm, which is the algorithm used to implement the first two steps of Algorithm 6. Algorithm 4 begins by finding an initial solution $S$ using the Twin Greedy algorithm of [17]. The algorithm then proceeds as follows: (i) If $|S|<k$ , it checks for an element $u$ such that adding $u$ to $S$ increases the function value by at least $\\textstyle(1+{\\frac{\\varepsilon}{2k}})f(S)$ . If such an element is found, it is added to $S$ . (ii) If $|S|=k$ , it looks for two elements $u\\in\\mathcal{N}\\setminus S$ and $v\\in S$ such that swapping $u$ and $v$ (i.e., removing $v$ from $S$ and adding $u$ to $S$ ) increases the function value by at least $\\textstyle(1+{\\frac{\\varepsilon}{2k}})f(S)$ . If such elements exist, the algorithm performs the swap. (iii) If no elements satisfy the previous two conditions, the algorithm checks for an element $v$ such that removing $v$ from $S$ increases the function value by $\\textstyle(1+{\\frac{\\varepsilon}{2k}})f(S)$ . If such an element is found, it is removed from $S$ . Algorithm 4 continues to search for elements satisfying any of the above three conditions until no such elements exist any longer. When this happens, the algorithm terminates and returns the set $S$ as its output. ", "page_idx": 13}, {"type": "text", "text": "$\\mathbf{Algorithm}\\ 4\\colon\\mathrm{LOCAL-SEARCH}(k,f)$ ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "input :A positive integer $k\\geq1$ , a non-negative submodular function $f$ , and an error parameter $\\varepsilon\\in(0,1)$ . output :A set $S\\subseteq N$ 1 Initialize $S$ to be a feasible solution guaranteeing $c$ -approximation for the problem for some constant $c\\in(0,1]$ . 2 while true do 3 if $\\exists u\\in\\mathcal{N}\\setminus S$ such that $\\begin{array}{r}{f(S+u)\\ge\\left(1+\\frac{\\varepsilon}{2k}\\right)f(S)}\\end{array}$ and $|S|<k$ then 4 \u2014 $S\\gets S+u$ . 5 else if $\\exists u\\in\\mathcal{N}\\setminus S,v\\in S$ such that $\\begin{array}{r}{f(S+u-v)\\ge\\big(1+\\frac{\\varepsilon}{2k}\\big)f(S)}\\end{array}$ and $|S|=k$ then 6 \u2014 $S\\gets S-v+u$ . 7 else if $\\exists v\\in S$ such that $\\begin{array}{r}{f(S-v)\\ge\\left(1+\\frac{\\varepsilon}{2k}\\right)f(S)}\\end{array}$ then 8 $S\\gets S-v$ . 9 else 0 return S ", "page_idx": 13}, {"type": "text", "text": "The properties of Algorithm 4 are formally established by Theorem A.1 ", "page_idx": 13}, {"type": "text", "text": "Theorem A.1. Given a positive integer $k$ , a non-negative submodular function $f$ and an error parameter $\\varepsilon\\in(0,1)$ , Algorithm 4 returns a set $S\\subseteq N$ of size at most $k$ such that ", "page_idx": 13}, {"type": "equation", "text": "$$\nf(S)\\geq\\frac{f(S\\cup\\mathbb{O P T})+f(S\\cap\\mathbb{O P T})}{2+\\varepsilon}\\qquad a n d\\qquad\\frac{f(S\\cap\\mathbb{O P T})}{1+\\varepsilon}\\;\\;,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "while requiring $O_{\\varepsilon}(n k^{2})$ queries to the objective function. ", "page_idx": 13}, {"type": "text", "text": "Proof. Let $S$ denote the output of Algorithm 4, and let $E_{-}$ , $E_{\\pm}$ , and $E_{+}$ be defined as follows. ", "page_idx": 13}, {"type": "text", "text": "\u2022 $E_{-}$ is the event that there exists $v\\in S$ such that $\\begin{array}{r}{f(S-v)\\ge\\big(1+\\frac{\\varepsilon}{2k}\\big)f(S)}\\end{array}$ . ", "page_idx": 13}, {"type": "text", "text": "\u2022 $E_{\\pm}$ is the event that there exists $u\\in\\mathcal{N}\\backslash S,v\\in S$ such that $\\begin{array}{r}{f(S+u-v)\\ge\\big(1+\\frac{\\varepsilon}{2k}\\big)f(S)}\\end{array}$ and $|S|=k$ . ", "page_idx": 14}, {"type": "text", "text": "Since the algorithm terminated with the set $S$ , none of the events $E_{\\mathrm{+}},E_{\\mathrm{-}}$ or $E_{\\pm}$ occurs. To lower bound the value of the set $S$ , we inspect the implications arising from each of these events not occurring. ", "page_idx": 14}, {"type": "text", "text": "Implications of $E_{-}$ not occurring. Since $E_{-}$ does not occur, for every $v\\in S$ , $\\begin{array}{r}{\\left(1+\\frac{\\varepsilon}{2k}\\right)f(S)>}\\end{array}$ $f(S-v)$ . Summing this inequality across every element in $v\\in S\\setminus\\mathbb{O P T}$ yields ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\left(1+\\displaystyle\\frac{\\varepsilon}{2k}\\right)f(S)\\geq f(S)+\\displaystyle\\frac{1}{|S\\setminus\\mathbb{O P T}|}\\displaystyle\\sum_{v\\in S\\setminus\\mathbb{O P T}}[f(S-v)-f(S)]}\\\\ &{}&{\\geq f(S)+\\displaystyle\\frac{1}{|S\\setminus\\mathbb{O P T}|}(f(S\\cap\\mathbb{O P T})-f(S))~~,}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the second inequality holds by submodularity of $f$ . By rearrangement, we obtain that ", "page_idx": 14}, {"type": "equation", "text": "$$\nf(S)\\geq{\\frac{1}{{\\frac{\\varepsilon}{2k}}|S\\setminus\\mathbb{O P T}|+1}}f(S\\cap\\mathbb{O P T})\\geq{\\frac{1}{1+\\varepsilon/2}}f(S\\cap\\mathbb{O P T})\\ ,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the last inequality holds since $|S\\setminus\\mathbb{O P T}|\\leq|S|\\leq k$ . ", "page_idx": 14}, {"type": "text", "text": "The above proves the second inequality guaranteed by the theorem. Below we show that the first inequality guaranteed by the theorem is implied either by $E_{+}$ not occurring, or by $E_{\\pm}$ not occuring, depending on the size of $S$ . ", "page_idx": 14}, {"type": "text", "text": "Implications of $E_{+}$ not occurring when $|S|<k$ . Since $E_{+}$ does not occur and $|S|<k$ , for every $u\\in\\mathcal{N}\\setminus S$ , it holds that $\\begin{array}{r}{\\left(1+\\frac{\\varepsilon}{2k}\\right)f(S)>\\dot{f}(S+u)}\\end{array}$ . Summing the above inequality across every element in $u\\in\\mathbb{O P T}\\setminus S$ yields ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\left(1+\\frac{\\varepsilon}{2k}\\right)f(S)\\ge f(S)+\\frac{1}{\\left|\\mathbb{O P T}\\setminus S\\right|}\\sum_{u\\in\\mathbb{O P T}\\setminus S}[f(S+u)-f(S)]}}\\\\ &{}&{\\ge f(S)+\\frac{1}{\\left|\\mathbb{O P T}\\setminus S\\right|}(f(S\\cup\\mathbb{O P T})-f(S))\\;\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the second inequality follows from the submodularity of $f$ . By rearrangement, we now obtain ", "page_idx": 14}, {"type": "equation", "text": "$$\nf(S)\\geq{\\frac{1}{{\\frac{\\varepsilon}{2k}}|\\mathbb{O}\\mathbb{P}\\mathbb{T}\\setminus S|+1}}f(S\\cup\\mathbb{O}\\mathbb{P}^{\\mathbb{T}})\\geq{\\frac{1}{1+\\varepsilon/2}}f(S\\cup\\mathbb{O}\\mathbb{P}\\mathbb{T})\\,~,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the last inequality holds since $|\\mathbb{O P T}\\setminus S|\\le|\\mathbb{O P T}|\\le k$ . Averaging Inequalities (3) and (4) gives ", "page_idx": 14}, {"type": "equation", "text": "$$\nf(S)\\geq\\frac{f(S\\cup\\mathbb{O P T})+f(S\\cap\\mathbb{O P T})}{2+2\\varepsilon}\\;\\;,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which proves the first inequality guaranteed by the theorem in the case of $|S|<k$ . ", "page_idx": 14}, {"type": "text", "text": "Implications of $E_{\\pm}$ not occurring when $|S|=k$ . Since $E_{\\pm}$ does not occur and $|S|=k$ , for every pair $u,v$ where $u\\in\\mathcal{N}\\setminus S$ and $v\\in S$ , $\\begin{array}{r}{\\left(1+\\frac{\\varepsilon}{2k}\\right)f(S)\\ge f(S+u-v)}\\end{array}$ . Summing the above inequality for every $v\\in S\\setminus\\mathbb{O P T}$ and $u\\in\\mathbb{O P T}\\setminus S$ , yields that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\Big(1+\\frac{\\varepsilon}{2k}\\Big)f(S)\\geq\\frac{1}{|S\\setminus\\mathbb{O P T}|\\cdot|\\mathbb{O P T}\\setminus S|}\\sum_{u\\in\\mathbb{O P T}\\setminus S}\\sum_{v\\in S\\setminus\\mathbb{O P T}}f(S+u-v)\\;\\;.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Observe that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\overline{{|S\\setminus\\mathbb{O}\\mathbb{P}\\mathbb{T}|\\cdot|\\mathbb{O}\\mathbb{P}\\mathbb{T}\\setminus S|}}\\sum_{u\\in\\mathbb{O}\\mathbb{P}\\mathbb{T}\\setminus S}\\sum_{v\\in S\\setminus\\mathbb{O}\\mathbb{P}\\mathbb{T}}f(S+u-v)-f(S)}\\\\ &{\\displaystyle\\overbrace{=\\frac{\\sum_{u\\in\\mathbb{O}\\mathbb{P}\\mathbb{T}\\setminus S}\\sum_{v\\in S\\setminus\\mathbb{O}\\mathbb{P}\\mathbb{T}}[f(S+u-v)-f(S-v)]}^{\\mathbb{A}}}^{\\mathbb{B}}\\overbrace{\\sum_{|S\\setminus\\mathbb{O}\\mathbb{P}\\mathbb{T}\\setminus S\\,v\\in S\\setminus\\mathbb{O}\\mathbb{P}\\mathbb{T}}[f(S-v)-f(S)]}^{\\mathbb{B}}}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad|S\\setminus\\mathbb{O}\\mathbb{P}\\mathbb{T}|\\cdot|\\mathbb{O}\\mathbb{P}\\mathbb{T}\\setminus S|}\\end{array}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "To bound $\\mathbb{A}$ , note that, by the submodularity of $f$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{u\\in\\mathbb{O P T}\\setminus S}\\left[f(S+u-v)-f(S-v)\\right]\\ge\\displaystyle\\sum_{u\\in\\mathbb{O P T}\\setminus S}\\left[f(S+u)-f(S)\\right]}&{}\\\\ {\\ge f(S\\cup\\mathbb{O P T})-f(S)\\enspace,}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and hence, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{\\displaystyle\\sum_{u\\in\\mathbb{O P T}\\setminus S}\\sum_{v\\in S\\setminus\\mathbb{O P T}}[f(S+u-v)-f(S-v)]}{|S\\setminus\\mathbb{O P T}|\\cdot|\\mathbb{O P T}\\setminus S|}\\ge\\frac{f(S\\cup\\mathbb{O P T})-f(S)}{|\\mathbb{O P T}\\setminus S|}\\ .\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "To bound $\\mathbb{B}$ , we note that the submodularity of $f$ implies that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{\\displaystyle\\sum_{u\\in\\mathbb{O}\\mathbb{P}\\mathbb{T}\\backslash S}\\sum_{v\\in S\\setminus\\mathrm{OFT}}[f(S-v)-f(S)]}{\\left|S\\setminus\\mathbb{O}\\mathbb{P}\\mathbb{T}\\right|\\cdot\\left|\\mathbb{O}\\mathbb{P}\\mathbb{T}\\setminus S\\right|}=\\frac{\\displaystyle\\sum_{v\\in S\\setminus\\mathrm{OFT}}[f(S-v)-f(S)]}{\\left|S\\setminus\\mathbb{O}\\mathbb{P T}\\right|}\\ge\\frac{\\displaystyle f(S\\cap\\mathbb{O}\\mathbb{P T})-f(S)}{\\left|S\\setminus\\mathbb{O}\\mathbb{P T}\\right|}\\ .\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Combining all of the above yields that ", "page_idx": 15}, {"type": "equation", "text": "$$\n{\\frac{\\varepsilon}{2k}}f(S)\\geq{\\frac{f(S\\cap\\mathbb{O P T})-f(S)}{|S\\setminus\\mathbb{O P T}|}}+{\\frac{f(S\\cup\\mathbb{O P T})-f(S)}{|\\mathbb{O P T}\\setminus S|}}\\enspace,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which by rearrangement implies ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(S)\\geq\\frac{f(S\\cap\\mathbb{D}^{\\mathbb{P}}\\mathbb{T})}{1+\\frac{\\varepsilon}{2k}|S\\setminus\\mathbb{O}\\mathbb{P T}|+\\frac{|S\\setminus\\mathbb{O}\\mathbb{P T}|}{|\\mathbb{O}\\mathbb{P T}\\setminus S|}}+\\frac{f(S\\cup\\mathbb{O}\\mathbb{P T})}{1+\\frac{\\varepsilon}{2k}|\\mathbb{O}\\mathbb{P T}\\setminus S|+\\frac{|\\mathbb{O}\\mathbb{P T}\\setminus S|}{|S\\setminus\\mathbb{O}\\mathbb{P T}|}}}\\\\ &{\\qquad\\geq\\frac{f(S\\cup\\mathbb{O}\\mathbb{P T})+f(S\\cap\\mathbb{O}\\mathbb{P T})}{2+\\varepsilon},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the last inequality holds since $|\\mathbb{O P T}\\setminus S|=|S\\setminus\\mathbb{O P T}|\\le k$ . This completes the proof of the first inequality guaranteed by the theorem in the case of $|S|=k$ . ", "page_idx": 15}, {"type": "text", "text": "To complete the proof of the theorem, it remains to analyze the query complexity of Algorithm 1. First, we recall that $S$ is initialized on Line 1 of Algorithm 4 by the Twin Greedy algorithm of [17], which gives an approximation ratio $c=1/5$ using $O(n\\log k)$ queries to the objective function. Each iteration of the loop of Algorithm 4 can be implemented using $O(n k)$ queries since there are only $O(n k)$ ways to choose $u\\in\\bar{\\mathcal{N}}\\backslash S$ and $v\\in S$ . Let us bound the number $L$ of such iterations. Observe that the function value of the set $S$ in Algorithm 4 increases by a multiplicative factor of $\\begin{array}{r}{1+\\frac{\\varepsilon}{2k}}\\end{array}$ following each iteration of the while loop (except for the last one). Since $S$ is initialized with a solution of value at least $c f(\\mathbb{O}\\mathbb{P}\\mathbb{T})$ , and its value is never larger than $f(O P T)$ (because it remains feasible), we get ", "page_idx": 15}, {"type": "equation", "text": "$$\nf(O P T)\\geq\\Big(1+\\frac{\\varepsilon}{k}\\Big)^{L-1}c f(\\mathbb{O P T})\\;\\;,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and rearranging gives us ", "page_idx": 15}, {"type": "equation", "text": "$$\nL\\leq1+{\\frac{\\ln{\\frac{1}{c}}}{\\ln(1+{\\frac{\\varepsilon}{2k}})}}=1+{\\frac{\\ln5}{\\ln(1+{\\frac{\\varepsilon}{2k}})}}=O(k/\\varepsilon)\\,\\,\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Combining the above results, we get that the query complexity of Algorithm 4 is upper bounded by $O(n\\log k)+O(n k)\\cdot L=O(n\\log k)+O(n k)\\cdot O(k/\\varepsilon)=O(n k^{2}\\varepsilon^{-1}).$ . \u53e3 ", "page_idx": 15}, {"type": "text", "text": "A.2 Guided Random Greedy ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we present the Guided Random Greedy algorithm (Algorithm 5), which is the variant of the Random Greedy algorithm of [5] used to implement the last step of Algorithm 6. Algorithm 5 starts with an empty set, and adds to it one element in each iteration until returning the final solution after $k$ iterations. In its first $\\lceil k\\cdot t_{s}\\rceil$ iterations, the algorithm ignores the elements of $Z$ , and in the rest of the iterations, it considers all elements. However, except for this difference, the behavior of the algorithm in all iterations is very similar. Specifically, in each iteration $i$ the algorithm does the following two steps. In Step (i) the algorithm finds a subset $M_{i}$ of size $k$ maximizing the sum of marginal gains of the elements $u\\in M_{i}$ with respect to the current solution $S_{i-1}$ . In step $(i i)$ , the algorithm chooses a random element from $M_{i}$ and adds it to the solution. This algorithm implicitly assumes that $\\vert\\mathcal{N}\\vert\\ge3k$ . If this is not the case, one can fix that by adding to the ground set $2k$ dummy elements of value 0 before executing the algorithm (and then removing any dummy elements that appear in the solution of the algorithm). ", "page_idx": 16}, {"type": "text", "text": "Algorithm 5: Guided Random Greedy ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "input :A set $\\overline{{Z\\subseteq{\\mathcal{N}}}}$ , a positive integer $k\\geq1$ , a non-negative submodular function $f$ , and a flip point $t_{s}\\in[0,1]$ output :A set $S\\subseteq N$ 1 Initialize $S_{0}\\gets\\emptyset$ . 2 for $i=1$ to $\\lceil k\\cdot t_{s}\\rceil$ do 3 Let $M_{i}\\subseteq\\mathcal{N}\\setminus(S_{i-1}\\cup Z)$ be a subset of size $k$ maximizing $\\textstyle\\sum_{u\\in M_{i}}f(u\\mid S_{i-1}+u)$ . 4 Let $u_{i}$ be a uniformly random element from $M_{i}$ . 5 $S_{i}\\leftarrow S_{i-1}+u_{i}$ . 6 for $i=\\lceil\\boldsymbol k\\cdot\\boldsymbol t_{s}\\rceil+1$ to $k$ do Let $M_{i}\\subseteq\\mathcal{N}\\setminus S_{i-1}$ be a subset of size $k$ maximizing $\\textstyle\\sum_{u\\in M_{i}}f(u\\mid S_{i-1}+u)$ . 8 Let $u_{i}$ be a uniformly random element from $M_{i}$ . 9 $S_{i}\\leftarrow S_{i-1}+u_{i}$ . 10 return $S_{k}$ . ", "page_idx": 16}, {"type": "text", "text": "The properties of Algorithm 5 are given by Theorem A.2. ", "page_idx": 16}, {"type": "text", "text": "Theorem A.2. There exists an algorithm that given a positive integer $k$ , a value $t_{s}\\;\\in\\;[0,1]$ , a non-negative submodular function $\\tilde{f}\\colon2^{\\mathcal{N}}\\to\\mathbb{R}_{\\geq0}$ , and a set $Z\\subseteq N$ obeying the inequalities given in Theorem A.1, outputs a solution $S_{k}$ , obeying ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[f(S_{k})]\\geq\\left(\\frac{k-\\lceil t_{s}\\cdot k\\rceil}{k}\\alpha^{k-\\lceil t_{s}\\cdot k\\rceil-1}+\\alpha^{k-\\lceil t_{s}\\cdot k\\rceil}-\\alpha^{k}\\right)f(\\mathbb{O}\\mathbb{P}\\mathbb{T})+}\\\\ &{\\quad\\quad\\quad+\\left(\\alpha^{k}+\\alpha^{k-1}-\\frac{2k-\\lceil t_{s}\\cdot k\\rceil}{k}\\alpha^{k-\\lceil t_{s}\\cdot k\\rceil-1}\\right)f(\\mathbb{O}\\mathbb{P}\\mathbb{T}\\cup Z)}\\\\ &{\\quad\\quad\\quad+\\left(\\alpha^{k}-\\alpha^{k-\\lceil t_{s}\\cdot k\\rceil}\\right)f(\\mathbb{O}\\mathbb{P}\\mathbb{T}\\cap Z)\\,\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\alpha\\,=\\,1\\,-\\,1/k$ . Furthermore, this algorithm requires only $O(n k)$ queries to the objective function. ", "page_idx": 16}, {"type": "text", "text": "To prove Theorem A.2, we first need to present some preliminaries. The Lov\u00e1sz extension of $f$ is a function $\\hat{f}:[0,1]^{\\mathcal{N}}\\rightarrow\\mathbb{R}$ defined as follows. For every vector $x\\in[0,1]^{\\mathcal{N}}$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\hat{f}(x)=\\int_{0}^{1}f(T_{\\lambda}(x))d\\lambda\\ \\,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $T_{\\lambda}(x)\\triangleq\\{u\\in\\mathcal{N}\\mid x_{u}\\geq\\lambda\\}$ . The Lov\u00e1sz extension of a submodular function is known to be convex. More important for us is the following known lemma regarding this extension. This lemma stems from an equality, proved by Lov\u00e1sz [28], between the Lov\u00e1sz extension of a submodular function and another extension known as the convex closure. ", "page_idx": 16}, {"type": "text", "text": "Lemma A.3. Let $f\\colon2^{\\mathcal{N}}\\to\\mathbb{R}$ be a submodular function, and let $\\hat{f}$ be its Lov\u00e1sz extension. For every $\\mathbf{x}\\in[0,1]^{\\mathcal{N}}$ and random set $D_{x}\\subseteq{\\mathcal{N}}$ obeying $\\operatorname*{Pr}[u\\in D_{x}]=x_{u}$ for every $u\\in N$ (i.e., the marginals of $D_{x}$ agree with $x$ ), $\\hat{f}(x)\\leq\\mathbb{E}[f(D_{x})]$ . ", "page_idx": 16}, {"type": "text", "text": "Using the last lemma, we now prove a lower bound on the expected value of the union of any set $A$ with the solution $S_{i}$ of Algorithm 5 after $i$ iterations. ", "page_idx": 17}, {"type": "text", "text": "Lemma A.4. For every integer $0\\leq i\\leq k$ and set $A\\subseteq N$ , it holds that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}[f(S_{i}\\cup A)]\\ge\\left(1-\\frac{1}{k}\\right)^{\\beta_{i}}\\cdot f(A)-\\left[\\left(1-\\frac{1}{k}\\right)^{\\beta_{i}}-\\left(1-\\frac{1}{k}\\right)^{i-1}\\right]\\cdot f(A\\cup Z)\\ ,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\beta_{i}=\\operatorname*{max}\\{0,i-\\lceil t_{s}\\cdot k\\rceil\\}$ . ", "page_idx": 17}, {"type": "text", "text": "Proof. Let $x^{(i)}\\in[0,1]^{\\mathcal{N}}$ be the vector of the marginal probabilities of elements to belong to $S_{i}$ . In other words, for every element $u\\in N$ , $x_{u}^{(i)}=\\mathrm{Pr}[u\\in S_{i}]$ . Since each iteration of Algorithm 5 adds each element to the solution with probability at most $1/k$ , the coordinates of $\\boldsymbol{x}^{(i)}$ are all upper bounded by $1-(1-1/k)^{i}$ . For elements of $Z$ we also know that they are not added by Algorithm 5 to the solution in the first $\\lceil k\\cdot t_{s}\\rceil$ iterations, and therefore, their coordinates in $\\boldsymbol{x}^{(i)}$ are upper bounded by ", "page_idx": 17}, {"type": "equation", "text": "$$\n1-(1-1/k)^{\\operatorname*{max}\\{0,i-\\lceil k\\cdot t_{s}\\rceil\\}}=1-(1-1/k)^{\\beta_{i}}\\ .\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Let ${\\mathbf{1}}_{A}$ denote a vector in $\\{0,1\\}^{\\mathcal{N}}$ containing 1s at the entries that correspond to elements present in $A$ and 0 in the remaining coordinates. We also denote by $x^{(i)}\\vee\\mathbf{1}_{A}$ the coordinate-wise maximum of $\\boldsymbol{x}^{(i)}$ and ${\\mathbf{1}}_{A}$ . By Lemma A.3, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[f(S_{i}\\cup A)]\\geq\\hat{f}(\\mathbf{x}_{s_{i}}\\vee\\mathbf{1}_{A})=\\displaystyle\\int_{0}^{1}f(T_{\\lambda}(\\mathbf{x}_{s_{i}}\\vee\\mathbf{1}_{A}))d\\lambda}\\\\ &{\\qquad\\qquad\\qquad\\geq\\displaystyle\\int_{1-(1-\\frac{1}{k})^{s-1}}^{1-(1-\\frac{1}{k})^{s-1}}f(T_{\\lambda}(\\mathbf{x}_{s_{i}}\\vee\\mathbf{1}_{A}))d\\lambda+\\displaystyle\\int_{1-(1-\\frac{1}{k})^{s-1}}^{1}f(T_{\\lambda}(\\mathbf{x}_{s_{i}}\\vee\\mathbf{1}_{A}))d\\lambda}\\\\ &{\\qquad\\qquad\\qquad\\overset{1}{1-(1-\\frac{1}{k})^{s_{i}}}}\\\\ &{\\qquad=\\displaystyle\\int\\int_{1-(1-\\frac{1}{k})^{s_{i}}}f(T_{\\lambda}(\\mathbf{x}_{s_{i}}\\vee\\mathbf{1}_{A}))d\\lambda+\\Big(1-\\frac{1}{k}\\Big)^{i-1}f(A)}\\\\ &{\\qquad\\qquad\\geq\\Big[\\Big(1-\\frac{1}{k}\\Big)^{s_{i}}-\\Big(1-\\frac{1}{k}\\Big)^{i-1}\\Big]\\cdot[f(A)-f(A\\cup Z)]+\\Big(1-\\frac{1}{k}\\Big)^{i-1}f(A)\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the second inequality holds by the non-negativity of $f$ , and the last inequality follows since $T_{\\lambda}(\\mathbf{x}_{S_{i}}\\vee\\mathbf{1}_{A})=B_{\\lambda}\\,\\bar{\\cup}\\,A$ for some set $B_{\\lambda}\\subset\\mathcal{N}\\backslash\\overline{{Z}}$ , and the submodularity and non-negativity of $f$ imply together that ", "page_idx": 17}, {"type": "equation", "text": "$$\nf(B_{\\lambda}\\cup A)\\geq f(A)+f(A\\cup B_{\\lambda}\\cup Z)-f(A\\cup Z)\\geq f(A)-f(A\\cup Z)\\enspace.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "With the above result, we are now ready to bound the expected value of $f(S_{i})$ . ", "page_idx": 17}, {"type": "text", "text": "Lemma A.5. Let $\\begin{array}{r}{\\alpha=1-\\frac{1}{k}}\\end{array}$ . Then, for every integer $0\\leq i\\leq\\lceil t_{s}\\cdot k\\rceil$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}[f(S_{i})]\\geq\\left(1-\\alpha^{i}\\right)f(\\mathbb{O}\\mathbb{P}\\mathbb{T}\\setminus Z)-\\left(1-\\alpha^{i}-i(1-\\alpha)\\alpha^{i-1}\\right)f(\\mathbb{O}\\mathbb{P}\\mathbb{T}\\cup Z)\\enspace,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and for every integer $\\lceil t_{s}\\cdot k\\rceil\\leq i\\leq k,$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}[f(S_{i})]\\geq\\frac{i-\\lceil t_{s}\\cdot k\\rceil}{k}\\alpha^{i-\\lceil t_{s}\\cdot k\\rceil-1}f(\\mathbb{O}\\mathbb{P}\\mathbb{T})-\\frac{i-\\lceil t_{s}\\cdot k\\rceil}{k}\\Big(\\alpha^{i-\\lceil t_{s}\\cdot k\\rceil-1}-\\alpha^{i-1}\\Big)f(\\mathbb{O}\\mathbb{P}\\mathbb{T}\\cup Z)}\\\\ {+\\alpha^{i-\\lceil t_{s}\\cdot k\\rceil}\\cdot f(S_{\\lceil t_{s}\\cdot k\\rceil})\\ .\\ \\ }\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. Let $E_{i}$ be an event fixing all the random decisions in Algorithm 5 up to iteration $i\\mathrm{~-~}1$ (including), and let $A_{i}=\\mathbb{O P T}\\setminus Z$ for $i\\leq\\lceil t_{s}\\cdot k\\rceil$ and $A_{i}=\\mathbb{O}\\mathbb{P}\\mathbb{T}$ for $i>\\lceil t_{s}\\cdot k\\rceil$ . Since all the elements of $A_{i}$ can be chosen to be in $M_{i}$ , and so can the dummy elements, we get that, conditioned on $E_{i}$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[f(u_{i}\\mid S_{i-1})]\\ge\\displaystyle\\operatorname*{max}_{u\\in M_{i}}f(u\\mid S_{i-1})\\ge k^{-1}\\displaystyle\\sum_{u\\in M_{i}}f(u\\mid S_{i-1})}\\\\ &{\\qquad\\qquad\\ge k^{-1}\\displaystyle\\sum_{u\\in A_{i}}f(u\\mid S_{i-1})\\ge k^{-1}[f(S_{i-1}\\cup A_{i})-f(S_{i-1})]~,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the last inequality holds by the submodularity of $f$ . Since $S_{i}=S_{i-1}+u_{i}$ , rearranging the last inequality gives $\\mathbb{E}[{\\overline{{f}}}(S_{i})]\\geq{\\overline{{k}}}^{-1}f(S_{i-1}\\cup A_{i})+\\alpha{\\bar{f}}(S_{i-1})$ . Taking expectation now over all possible events $E_{i}$ we get that, without conditioning on anything, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[f(S_{i})]\\ge k^{-1}\\mathbb{E}[f(S_{i-1}\\cup A_{i})]+\\alpha\\mathbb{E}[f(S_{i-1})]\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proving the first inequality of the lemma. We prove the first inequality of the lemma (for $0\\leq i\\leq\\lceil t_{s}\\cdot k\\rceil)$ by induction on $i$ . For $i=0$ the inequality holds by the non-negativity of $f$ . Assume now that $1\\leq i\\leq\\lceil t_{s}\\cdot k\\rceil$ and the inequality holds for $i-1$ , and let us prove the inequality for $i$ . ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[f(S_{j})]\\geq k^{-1}\\mathbb{E}[f(S_{i-1}\\cup A_{i})]+\\alpha\\mathbb{E}[f(S_{i-1})]}\\\\ &{\\qquad\\qquad\\geq k^{-1}\\Big[f(\\mathbb{O}\\mathbb{P}\\mathbb{T}\\cup Z)-\\Big(1-\\Big(1-\\frac{1}{k}\\Big)^{i-1}\\Big)\\cdot f(\\mathbb{O}\\mathbb{P}\\mathbb{T}\\cup Z)\\Big]}\\\\ &{\\qquad\\qquad+\\alpha(1-\\alpha^{i-1})f(\\mathbb{O}\\mathbb{P}\\mathbb{T}\\setminus Z)-\\alpha\\Big(1-\\alpha^{i-1}-(1-\\alpha)(i-1)\\alpha^{i-2}\\Big)f(\\mathbb{O}\\mathbb{P}\\mathbb{T}\\cup Z)}\\\\ &{\\qquad=(1-\\alpha)\\Big[f(\\mathbb{O}\\mathbb{P}\\mathbb{T}\\setminus Z)-(1-\\alpha^{i-1})\\cdot f(\\mathbb{O}\\mathbb{P}\\mathbb{T}\\cup Z)\\Big]}\\\\ &{\\qquad\\qquad+\\alpha(1-\\alpha^{i-1})f(\\mathbb{O}\\mathbb{P}\\mathbb{T}\\setminus Z)-\\alpha\\Big(1-\\alpha^{i-1}-(1-\\alpha)(i-1)\\alpha^{i-2}\\Big)f(\\mathbb{O}\\mathbb{P T}\\cup Z)}\\\\ &{\\qquad=\\alpha(1-\\alpha^{i})f(\\mathbb{O}\\mathbb{P T}\\backslash Z)-\\Big(1-\\alpha^{i}-(1-\\alpha)i\\alpha^{i-1}\\Big)f(\\mathbb{O}\\mathbb{P T}\\cup Z)~,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the second inequality holds by Lemma A.5 and the induction hypothesis since $A_{i}=\\mathbb{O P T}\\setminus Z$ , and the first equality holds by definition of $\\alpha$ . ", "page_idx": 18}, {"type": "text", "text": "Proving the second inequality of the lemma. We prove the second inequality of the lemma (for $\\lceil t_{s}\\cdot k\\rceil\\leq i\\leq k)$ by induction on $i$ . One can verify that for $i=\\lceil t_{s}\\cdot k\\rceil$ the inequality trivially holds. Assume now that $\\dot{\\lceil t_{s}\\cdot k\\rceil}<i\\leq k$ and the inequality holds for $i-1$ , and let us prove the inequality for $i$ . ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}[f(S_{j})]\\geq k^{-1}\\mathbb{E}[f(S_{i-1}\\lfloor A\\rfloor)+\\alpha\\mathbb{E}[f(S_{i-1})]}&{}\\\\ {\\geq k^{-1}\\Big[\\Big(1-\\frac1{k}\\Big)^{i-\\lceil t+k\\rceil-1}\\cdot f((0\\mathbb{P}\\mathbb{T})-\\Big(\\Big(1-\\frac1{k}\\Big)^{i-\\lceil t_{k}\\rceil-1}}\\\\ &{\\quad-\\Big(1-\\frac1{k}\\Big)^{i-1}\\cdot f(0\\mathbb{P}\\mathbb{T})[\\Omega]\\Big)+\\frac{i-\\lceil t_{k}\\rceil-1}{k}a^{-\\lceil t_{k}\\rceil-1-1}f(0\\mathbb{P}\\mathbb{T})}\\\\ &{\\quad-\\frac{i-\\lceil t_{k}\\rceil-1}{k}\\cdot\\Big[\\alpha^{-\\lceil t_{k}\\rceil,k-1}-\\alpha^{-1}\\Big)f(0\\mathbb{P}\\mathbb{T}\\cup Z)+\\alpha^{-\\lceil t_{k}\\rceil-1}\\cdot f(S_{\\lfloor t_{k}\\rfloor-k})}\\\\ &{=k^{-1}[a_{i}^{-\\lceil t_{k}\\rceil-1}\\cdot f(0\\mathbb{P}\\mathbb{T})-(a^{-\\lceil t_{k}\\rceil-1}-a^{-1})\\cdot f(0\\mathbb{P}\\mathbb{T}\\cup Z)]}\\\\ &{\\quad+\\frac{i-\\lceil t_{k}\\rceil-1}{k}\\!-\\!1\\!\\alpha^{-\\lceil t_{k}\\rceil-1}\\!-\\!I(0\\mathbb{P}\\mathbb{T})}\\\\ &{\\quad-\\frac{i-\\lceil t_{k}\\rceil-1}{k}\\!-\\!\\Big(\\!\\alpha^{-\\lceil t_{k}\\rceil,k-1}\\!-\\!\\alpha^{i-1}\\!\\Big)f(0\\mathbb{P T}\\cup Z)+\\alpha^{-\\lceil t_{k}\\rceil}\\cdot f(S_{\\lfloor t_{k}\\rfloor})}\\\\ &{=\\frac{i-\\lceil t_{k}\\rceil}{k}a^{-\\lceil t_{k}\\rceil}a^{-\\lceil t_{k}\\rceil-1}f(0\\mathbb{P T})-\\frac{i-\\lceil t_{k}\\rceil}{k}\\!\\Big(a^{-\\slash t_{k}\\rceil-1}\\!-\\!\\alpha^{i-1}\\!\\Big)f(0\\mathbb{P T}\\cup Z)}\\\\ &{\\quad+\\alpha^{-\\lceil t_{k}\\rceil-1}\\cdot f(S_{\\lfloor t_{k}\\rfloor})\\ ,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the second inequality holds by Lemma A.5 and the induction hypothesis since $A_{i}=\\mathbb{O}\\mathbb{P}\\mathbb{T}$ , and the first equality holds by definition of $\\alpha$ . \u53e3 ", "page_idx": 18}, {"type": "text", "text": "We are now ready to prove Theorem A.2. ", "page_idx": 18}, {"type": "text", "text": "Proof of Theorem A.2. Note that by submodularity and non-negativity of $f$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\nf(\\mathbb{O P T}\\setminus Z)\\geq f(\\varnothing)+f(\\mathbb{O P T})-f(\\mathbb{O P T}\\cap Z)\\geq f(\\mathbb{O P T})-f(\\mathbb{O P T}\\cap Z)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The lower bound in the theorem follows by combining the above inequality with the following two inequalities: the inequality arising by plugging $i=\\lceil t_{s}\\cdot k\\rceil$ into the first case of Lemma A.5, and the inequality arising by plugging $i=k$ into the second case of Lemma A.5. ", "page_idx": 18}, {"type": "text", "text": "To conclude the proof, observe that each one of the $k$ iterations of Algorithm 5 requires us to compute the marginal gain of at most $n$ elements with respect to the set $S_{i-1}$ , which can be done using $O(n)$ queries to the objective function per iteration, and $O(n k)$ queries in total. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "A.3 0.385-Approximation Guarantee ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We are now ready to present Algorithm 6 (the simpler version of our main algorithm). Recall that this algorithm returns the better among the two sets produced in the last two steps described in the beginning of this appendix. Formally, these sets are the output sets of LOCAL-SEARCH and Algorithm 5. ", "page_idx": 19}, {"type": "table", "img_path": "l6iICoILGB/tmp/297a454eeb15023e0388fc04b581a4b88ebc20051c4dd02ee4cf7b59b662953e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "The following theorem is proved by setting $\\varepsilon$ in Algorithm 6 to be a small enough positive constant. ", "page_idx": 19}, {"type": "text", "text": "Theorem A.6 (Approximation guarantee). Given an integer $k\\geq1$ and a non-negative submodular function $f\\colon2^{{\\mathcal{N}}}\\ {\\overset{\\ldots}{\\to}}\\ \\mathbb{R}_{\\geq0}$ , there exists a 0.385-approximation algorithm for the problem of finding a set $S\\subseteq N$ of size at most $k$ maximizing $f$ . This algorithm uses $O(n k^{2})$ queries to the objective function. ", "page_idx": 19}, {"type": "text", "text": "Proof. The proof of the approximation guarantee is very similar to the corresponding part in the proof Theorem 2.3, and is thus, omitted. The query complexity stated in the theorem follows directly from Theorems A.1 and A.2 and the fact that $\\varepsilon$ is set to a positive constant value. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "B Omitted Proofs of Section 2 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we prove the theorems whose proofs have been omitted from Section 2, namely, Theorems 2.1, and 2.2. ", "page_idx": 19}, {"type": "text", "text": "B.1 Proof of Theorem 2.1 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we prove Theorem 2.1. We begin with the following lemma. We assume without loss of generality that $\\mathbb{O}\\mathbb{P}\\mathbb{T}$ is of size $k$ (otherwise, we add to $\\mathbb{O}\\mathbb{P}\\mathbb{T}$ dummy elements). ", "page_idx": 19}, {"type": "text", "text": "Lemma B.1. If the set $S_{0}$ provides $c$ -approximation, then each iteration of the loop starting on Line 3 in Algorithm $^{\\,l}$ returns a set with probability at least $k/(c\\varepsilon(1-{^1}/{e})L)$ . Moreover, when this happens, the output set $S$ returned obeys ", "page_idx": 19}, {"type": "equation", "text": "$$\nf(S)\\geq\\frac{f(S\\cap\\mathbb{O P T})+f(S\\cup\\mathbb{O P T})}{2+\\varepsilon}\\quad a n d\\quad f(S)\\geq\\frac{f(S\\cap\\mathbb{O P T})}{1+\\varepsilon}\\ \\ .\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. For every two integers $0\\leq i<L$ and $1\\leq j\\leq\\lceil\\log\\frac{1}{\\varepsilon}\\rceil$ , we denote by $A_{i}^{j}$ the event that the set $S_{i}^{j}$ obeys the condition on Line 13 of Algorithm 1, i.e., the event that for every integer $0\\le t\\le k$ it holds that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{S\\subseteq{N\\setminus S_{i}^{j}},|S|=t}\\sum_{u\\in S}f(u\\mid S_{i}^{j})\\le\\operatorname*{min}_{S\\subseteq S_{i}^{j},|S|=t}\\sum_{v\\in S}f(v\\mid S_{i}^{j}-v)+\\varepsilon f(S_{i}^{j})\\;\\;.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "To better understand the implication of an event $A_{i}^{j}$ , assume that such an event occurs, and observe that for $t=|\\mathbb{O P T}\\setminus S_{i}^{j}|$ , it holds that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(S_{i}^{j})-f(S_{i}^{j}\\cap\\mathbb{D}\\mathbb{P}\\mathbb{T})+\\varepsilon f(S_{i}^{j})=\\displaystyle\\sum_{\\ell=1}^{|K_{i}^{j}|}f(u_{\\ell}\\mid J_{i}^{j}\\cup\\{u_{1},\\ldots,u_{\\ell-1}\\})+\\varepsilon f(S_{i}^{j})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\geq\\displaystyle\\sum_{\\ell=1}^{|K_{i}^{j}|}f(u_{\\ell}\\mid J_{L}^{j}\\cup K_{L}^{j}-u_{\\ell})+\\varepsilon f(S_{i}^{j})}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\displaystyle\\sum_{v\\in S_{i}^{j}\\setminus\\{0,P\\}}f(v\\mid S_{i}^{j}-v)+\\varepsilon f(S_{i}^{j})}\\\\ &{\\ge\\displaystyle\\operatorname*{min}_{s\\ge S_{i}^{j},\\mid S\\mid=t}\\sum_{v\\in S}f(v\\mid S-v)+\\varepsilon f(S_{i}^{j})}\\\\ &{\\ge\\displaystyle\\operatorname*{max}_{s\\ge N\\setminus S_{i}^{j},\\mid S\\mid=t}\\sum_{u\\in S}f(u\\mid S_{i}^{j})}\\\\ &{\\ge\\displaystyle\\sum_{u\\in\\mathrm{OPT}\\setminus S_{i}^{j}}f(u\\mid S_{i}^{j})\\ge f(S_{i}^{j}\\cup\\mathbb{O P T})-f(S_{i}^{j})\\enspace,}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the first equality holds by setting $J_{i}^{j}\\triangleq S_{i}^{j}\\cap\\mathbb{O}\\mathbb{P}\\mathbb{T}$ , $K_{i}^{j}\\triangleq S_{i}^{j}\\backslash\\mathbb{O}\\mathbb{P}\\mathbb{T}$ and using $u_{1},u_{2},\\ldots,u_{|K_{i}^{j}|}$ to denote the elements of $K_{i}^{j}$ in some arbitrary order. The first and last inequalities follow from submodularity of $f$ , the second inequality holds since the fact that $S_{i}^{j}$ and $\\mathbb{O}\\mathbb{P}\\mathbb{T}$ are both of size $k$ implies that $t=|\\mathbb{O P T}\\setminus S_{i}^{j}|=|S_{i}^{j}\\setminus\\mathbb{O P T}|$ , and the third inequality holds under the assumption that event $A_{i}^{j}$ occurs. Rearranging the last inequality, we get ", "page_idx": 20}, {"type": "equation", "text": "$$\nf(S_{i}^{j})\\geq\\frac{f(S_{i}^{j}\\cup\\mathbb{O P T})+f(S_{i}^{j}\\cap\\mathbb{O P T})}{2+\\varepsilon}\\;\\;.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "In addition, the existence of the dummy elements implies that $\\operatorname*{max}_{S\\subseteq\\mathcal{N}\\setminus S_{i}^{j},|S|=t\\,u\\in S}f(u\\mid S_{i}^{j})\\ge0$ , and plugging this inequality into Inequality (6) yields that the event $A_{i}^{j}$ also implies ", "page_idx": 20}, {"type": "equation", "text": "$$\nf(S_{i}^{j})-f(S_{i}^{j}\\cap\\mathbb{P}^{\\mathbb{P}}\\mathbb{T})+\\varepsilon f(S_{i}^{j})\\geq\\operatorname*{min}_{S\\subseteq S_{i}^{j},|S|=t}\\sum_{v\\in S}f(v\\mid S_{i}^{j}-v)+\\varepsilon f(S_{i}^{j})\\geq0\\;\\;,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and rearranging this inequality gives $\\begin{array}{r}{f(S_{i}^{j})\\ge\\frac{f(S_{i}^{j}\\cap\\mathbb{O P T})}{1+\\varepsilon}}\\end{array}$ . ", "page_idx": 20}, {"type": "text", "text": "The above shows that to prove the lemma it suffices to show that the probability that $A_{i^{*}}^{j}$ holds is at least $k/(c\\varepsilon(1-{^1}/{e})L)$ . Towards this goal, let us study the implications of the complementary event $\\bar{A}_{i}^{j}$ . Specifically, we would like to lower bound $\\mathbb{E}\\left[f(S_{i+1}^{j})-\\bar{f}(S_{i}^{j})\\mid\\bar{A}_{i}^{j}\\right]$ . ", "page_idx": 20}, {"type": "text", "text": "Fix a particular set $S_{i}^{j}$ that causes the event $\\bar{A}_{i}^{j}$ to occur (notice that the occurrence of this event depends only on the set $S_{i}^{j}$ ). Then, there must exist sets $T_{+}\\subseteq\\mathcal{N}\\setminus S_{i}^{j}$ and $T_{-}\\subseteq S_{i}^{j}$ of size $t\\leq k$ such that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{u\\in T_{+}}f(u\\mid S_{i}^{j})>\\sum_{v\\in T_{-}}f(v\\mid S_{i}^{j}-v)+\\varepsilon f(S_{i}^{j})\\;\\;.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We can now define the event $B_{i}^{j}$ as the event that $Z_{i+1}^{j}\\cap T_{+}\\neq\\emptyset$ (notice that the event $B_{i}^{j}$ is defined only for this particular set $S_{j}^{i})$ . The probability of the event $B_{i}^{j}$ is ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathrm{Pr}(B_{i}^{j}\\mid S_{i}^{j})\\ge1-\\left(\\frac{n-n/k}{n}\\right)^{|T_{+}|}\\ge1-e^{-\\frac{|T_{+}|}{k}}\\ge(1-1/e)\\frac{|T_{+}|}{k}~~,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the first inequality holds since $(1-{\\frac{1}{k}})^{x}\\,\\leq\\,e^{-\\,{\\frac{x}{k}}}$ for any $x\\geq0$ , and the second inequality holds for any $x\\in[0,1]$ by the concavity of $1-e^{-x}$ . By the law of total expectation and the fact that $f(S_{i+1}^{j})$ is always at least $f(S_{i}^{j})$ , we now get ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\Big[f(S_{i+1}^{j})-f(S_{i}^{j})\\mid S_{i}^{j}\\Big]\\geq\\operatorname*{Pr}(B_{i}^{j}\\mid S_{i}^{j})\\cdot\\mathbb{E}\\Big[f(S_{i+1}^{j})-f(S_{i}^{j})\\mid S_{i}^{j},B_{i}^{j}\\Big]}\\\\ &{\\qquad\\qquad\\qquad\\geq(1-1/e)\\frac{\\left|T_{+}\\right|}{k}\\cdot\\mathbb{E}\\Big[f(S_{i+1}^{j})-f(S_{i}^{j})\\mid S_{i}^{j},B_{i}^{j}\\Big]}\\\\ &{\\qquad\\qquad\\qquad\\geq\\frac{1-1/e}{k}\\varepsilon c f(\\mathbb{O P T})\\;\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the last inequality holds since ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\Big[f(S_{i+1}^{j})-f(S_{i}^{j})\\ |\\ S_{i}^{j},B_{i}^{j}\\Big]\\geq\\mathbb{E}\\Big[f(S_{i}^{j}-v_{i+1}^{j}+u_{i+1}^{j})-f(S_{i}^{j})\\ |\\ S_{i}^{j},B_{i}^{j}\\Big]}\\\\ &{\\qquad\\qquad=\\mathbb{E}\\Big[f(S_{i}^{j}-v_{i+1}^{j}+u_{i+1}^{j})-f(S_{i}^{j}-v_{i+1}^{j})+f(S_{i}^{j}-v_{i+1}^{j})-f(S_{i}^{j})\\ |\\ S_{i}^{j},B_{i}^{j}\\Big]}\\\\ &{\\qquad\\qquad\\geq\\mathbb{E}\\Big[f(S_{i}^{j}+u_{i+1}^{j})-f(S_{i}^{j})+f(S_{i}^{j}-v_{i+1}^{j})-f(S_{i}^{j})\\ |\\ S_{i}^{j},B_{i}^{j}\\Big]}\\\\ &{\\qquad\\qquad\\geq\\mathbb{E}\\Big[f(S_{i}^{j}+u^{\\prime})-f(S_{i}^{j})\\ |\\ S_{i}^{j},B_{i}^{j}\\Big]+\\mathbb{E}\\Big[f(S_{i}^{j}-v^{\\prime})-f(S_{i}^{j})\\ |\\ S_{i}^{j},B_{i}^{j}\\Big]}\\\\ &{\\qquad\\qquad\\underset{=}{\\sum}f(u\\ |\\ S_{i}^{j})\\quad\\underset{=}{-\\frac{v\\in T_{-}}{2}}f(v\\ |\\ S_{i}^{j}-v)}\\geq\\frac{\\varepsilon f(S_{i}^{j})}{|T_{+}|}\\geq\\frac{c\\varepsilon f(\\mathbb{O}\\mathbb{P}\\mathbb{T})}{|T_{+}|}\\ ,}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the second inequality holds by submodularity of $f(\\cdot)$ , the third inequality holds by the way Algorithm 1 chooses $u_{i}^{j}$ and $v_{i}^{j}$ if we let $u^{\\prime}$ be a uniformly random element of $T_{+}\\cap Z_{i+1}^{j}$ and $v^{\\prime}$ be a uniformly random element of $T_{-}$ , the penultimate inequality holds by our assumption that $S_{i}^{j}$ implies the event $\\bar{A}_{i}^{j}$ , and finally, the last inequality holds since it is guaranteed that $f(S_{i}^{j})\\,\\geq\\,f(S_{0}^{j})\\,=$ $f(S_{0})\\geq c\\cdot{\\overset{\\cdot}{f}}(\\mathbb{O P T})$ . ", "page_idx": 21}, {"type": "text", "text": "Since the above bound on the expectation holds conditioned on every set $S_{i}^{j}$ that implies the event $\\bar{A}_{i}^{j}$ , it holds (by the law of total expectation) also conditioned on the event $\\bar{A}_{i}^{j}$ itself. Adding this lower bound for all $i$ values, and using the non-negativity of $f$ , we get ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\Big[f(S_{L}^{j})\\Big]\\geq\\displaystyle\\sum_{\\ell=0}^{L-1}\\mathbb{E}\\Big[f(S_{\\ell+1}^{j})-f(S_{\\ell}^{j})\\Big]\\geq\\displaystyle\\sum_{\\ell=0}^{L-1}\\operatorname*{Pr}(\\bar{A}_{\\ell}^{j})\\cdot\\mathbb{E}\\Big[f(S_{\\ell+1}^{j})-f(S_{\\ell}^{j})\\ |\\ \\bar{A}_{\\ell}^{j}\\Big]}\\\\ &{\\qquad\\qquad\\geq\\displaystyle\\frac{1-1/e}{k}c\\varepsilon f(\\mathbb{O P T})\\cdot\\displaystyle\\sum_{\\ell=1}^{L}\\operatorname*{Pr}(\\bar{A}_{\\ell}^{j})\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Combining the last inequality with the fact that $f(S_{L}^{j})$ is deterministically at most $f(\\mathbb{O}\\mathbb{P}\\mathbb{T})$ , it must hold that ", "page_idx": 21}, {"type": "equation", "text": "$$\n1\\geq\\frac{1-1/e}{k}c\\varepsilon\\cdot\\sum_{\\ell=1}^{L}\\operatorname*{Pr}\\!\\left(\\bar{A}_{\\ell}^{j}\\right)\\quad\\Rightarrow\\quad\\sum_{\\ell=1}^{L}\\operatorname*{Pr}\\!\\left(\\bar{A}_{\\ell}^{j}\\right)\\leq\\frac{k}{c\\varepsilon(1-1/e)}\\enspace.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Hence, the probability that the event $A_{i^{*}}^{j}$ does not hold for a uniformly random $i^{*}\\in[L]$ is ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{\\sum_{\\ell=1}^{L}\\mathrm{Pr}\\!\\left(\\bar{A}_{\\ell}^{j}\\right)}{L}\\le\\frac{k}{c\\varepsilon(1-1/e)L}\\;\\;.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Theorem 2.1. There exists an algorithm that given a positive integer $k$ , a value $\\varepsilon\\in(0,1)$ , and $a$ non-negative submodular function $f\\colon2^{\\mathcal{N}}\\to{\\check{\\mathbb{R}}}_{\\geq0}$ , outputs a set $S\\subseteq N$ of size at max $k$ that, with probability at least $1-\\varepsilon$ , obeys ", "page_idx": 21}, {"type": "equation", "text": "$$\nf(S)\\geq\\frac{f(S\\cap\\mathbb{O P T})+f(S\\cup\\mathbb{O P T})}{2+\\varepsilon}\\quad a n d\\quad f(S)\\geq\\frac{f(S\\cap\\mathbb{O P T})}{1+\\varepsilon}\\mathrm{~.~}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Furthermore, the query complexity of the above algorithm is $O_{\\varepsilon}(n+k^{2})$ . ", "page_idx": 21}, {"type": "text", "text": "Proof. As mentioned above, we initialize the set $S_{0}$ using the deterministic $^{1}\\!/\\!4$ -approximation algorithm of Balkanski et al. [2], which uses only $O(n)$ queries to the objective function. Thus, $c=1/4$ in our implementation of Algorithm 1. Let us now set $\\begin{array}{r}{L=\\left\\lceil\\frac{2k}{c\\varepsilon(1-^{1}/e)}\\right\\rceil}\\end{array}$ in Algorithm 1. Then, Lemma B.1 guarantees that every iteration of the outer loop of the algorithm returns a set (obeying the requirement of the theorem) with probability at least $1/2$ . Hence, by repeating this loop $\\lceil\\bar{\\log_{2}\\varepsilon}^{-1}\\rceil$ times, we are guaranteed that Algorithm 1 outputs a set with probability at least $1-\\varepsilon$ . To complete the proof of the theorem, it only remains to bound the number of queries to the objective function that are necessary for implementing it. Each iteration of Algorithm 1 can be implemented using $O(n/k+k)$ queries, and for the above choices of $L$ , Algorithm 1 has only $O_{\\varepsilon}(k)$ iterations. Thus, all the iterations of the algorithm can be implemented using $O_{\\varepsilon}(n+k^{2})$ queries in total. It should also be mentioned that evaluating the condition on Line 13 of the algorithm requires $O(n+k)$ queries to the objective, and since this condition is evaluated $\\lceil\\log_{2}\\varepsilon^{-1}\\rceil^{\\bar{\\}}=O_{\\varepsilon}(1)$ times, all its evaluations require in total only $O_{\\varepsilon}(n+k)$ queries. \u53e3 ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "B.2 Proof of Theorem 2.2 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this section, we prove Theorem 2.2. We begin by observing that, like in Algorithm 5, each iteration of Algorithm 2 adds each element $u\\in N$ into the solution with probability at most $1/k$ , and furthermore, the first $\\lceil t_{s}\\cdot k\\rceil$ iterations of the algorithm do not pick elements of $Z$ at all (see the analysis of Sample Greedy in [6] for a proof of a similar observation that is given in more detail). Given this observation, the proof of Lemma A.4 applies also to Algorithm 2. Thus, this lemma can be used in the proof of the following result. ", "page_idx": 22}, {"type": "text", "text": "Lemma B.2. Let $\\begin{array}{r}{\\alpha=1-\\frac{1}{k}}\\end{array}$ . Then, for every integer $0\\leq i\\leq\\lceil t_{s}\\cdot k\\rceil$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}[f(S_{i})]\\geq\\left(1-\\alpha^{i}\\right)f(\\mathbb{O}\\mathbb{P}\\mathbb{T}\\setminus Z)-\\left(1-\\alpha^{i}-i(1-\\alpha)\\alpha^{i-1}\\right)f(\\mathbb{O}\\mathbb{P}\\mathbb{T}\\cup Z)-\\frac{2\\varepsilon i}{k}\\enspace,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and for every integer $[t_{s}\\cdot k]\\leq i\\leq k_{}$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}[f(S_{i})]\\geq\\frac{i-\\lceil t_{s}\\cdot k\\rceil}{k}\\alpha^{i-\\lceil t_{s}\\cdot k\\rceil-1}f(\\mathbb{O}\\mathbb{P}\\mathbb{T})-\\frac{i-\\lceil t_{s}\\cdot k\\rceil}{k}\\Big(\\alpha^{i-\\lceil t_{s}\\cdot k\\rceil-1}-\\alpha^{i-1}\\Big)f(\\mathbb{O}\\mathbb{P}\\mathbb{T}\\cup Z)}\\\\ {+\\alpha^{i-\\lceil t_{s}\\cdot k\\rceil}\\cdot f(S_{\\lceil t_{s}\\cdot k\\rceil})-\\frac{2\\varepsilon(i-\\lceil t_{s}\\cdot k\\rceil)}{k}~.~~~~~~~~~~~~~}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. Let $E_{i}$ be an event fixing all the random decisions in Algorithm 2 up to iteration $i\\mathrm{~-~}1$ (including), and let $A_{i}=\\mathbb{O P T}\\setminus Z$ for $i\\leq\\lceil t_{s}\\cdot k\\rceil$ and $A_{i}=\\mathbb{O}\\mathbb{P}\\mathbb{T}$ for $i>\\lceil t_{s}\\cdot k\\rceil$ . Since all the elements of $A_{i}$ can be sampled in iteration $i$ , by following the proof of Lemma 13 in the analysis of the Sample Greedy algorithm by [6], one can obtain that, conditioned on $E_{i}$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\operatorname*{max}\\{0,f(u_{i}\\mid S_{i-1})\\}]\\ge\\frac{1-\\varepsilon}{k}\\big[f(A_{i}\\cup S_{i-1})-f(S_{i-1})\\big]\\enspace.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "To be more specific, Lemma 11 of [6] shows that with probability at least $1-\\varepsilon$ the element chosen as $u_{i}$ in iteration $i$ of Algorithm 2 belongs to the $k$ elements with the largest marginal values among the elements that can be sampled in this iteration (if less than $k$ elements can be sampled, dummy elements should added for the purpose of this argument). Let $B_{i}$ denote the set of these $k$ elements. Since the probability of each element of $B_{i}$ to be selected as $u_{i}$ is non-decreasing in $f(u_{i}\\mid S_{i-1})$ , by Chebyshev\u2019s sum inequality, we get ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\operatorname*{max}\\{0,f(u_{i}\\mid S_{i-1})\\}]\\ge(1-\\varepsilon)\\frac{\\sum_{u\\in B_{i}}\\operatorname*{max}\\{0,f(u\\mid S_{i-1})\\}}{k}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\ge(1-\\varepsilon)\\frac{\\displaystyle\\sum_{u\\in A_{i}}f(u\\mid S_{i-1})}{k}\\ge\\frac{1-\\varepsilon}{k}[f(A_{i}\\cup S_{i-1})-f(S_{i-1})]~,}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the second inequality holds since $B_{i}$ contains the $k$ elements with the largest marginals among the elements that can be samples, and $A_{i}$ is a set of up to $k$ such elements; and the last inequality follows from the submodularity of $f$ . ", "page_idx": 22}, {"type": "text", "text": "Since $S_{i}=S_{i-1}+u_{i}$ when $f(u_{i}\\mid S_{i-1})\\ge0$ and $S_{i}=S_{i-1}$ otherwise, we get $f(S_{i})-f(S_{i-1})=$ $\\operatorname*{max}\\{0,f(u_{i}\\mid S_{i-1})\\}$ . Plugging this observation into the previous inequality, and rearranging gives ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[f(S_{i})]\\ge\\frac{1-\\varepsilon}{k}f(S_{i-1}\\cup A_{i})+\\Big(1-\\frac{1-\\varepsilon}{k}\\Big)f(S_{i-1})}\\\\ &{\\qquad\\qquad\\ge\\frac{1}{k}f(S_{i-1}\\cup A_{i})+\\alpha f(S_{i-1})-\\frac{\\varepsilon}{k}f(S_{i-1}\\cup A_{i})}\\\\ &{\\qquad\\qquad\\ge\\frac{1}{k}f(S_{i-1}\\cup A_{i})+\\alpha f(S_{i-1})-\\frac{2\\varepsilon}{k}f(\\mathbb{O}\\mathbb{P}\\mathbb{T})\\;\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the second inequality uses the non-negativity of $f$ , and the last inequality holds since $f(S_{i-1}\\cup$ $A_{i})\\leq f(S_{i-1})+f(A_{i})-f(S_{i-1}\\cap A_{i})\\leq f(S_{i-1})+f(A_{i})\\leq2f(\\mathbb{O}\\mathbb{P}\\mathbb{T})$ because both $S_{i-1}$ and ", "page_idx": 22}, {"type": "text", "text": "$A_{i}$ are feasible solutions, and thus, cannot have a value larger than $f(\\mathbb{O}\\mathbb{P}\\mathbb{T})$ . Taking expectation now over all possible events $E_{i}$ we get that, without conditioning on anything, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}[f(S_{i})]\\ge k^{-1}\\mathbb{E}[f(S_{i-1}\\cup A_{i})]+\\alpha\\mathbb{E}[f(S_{i-1})]-\\frac{2\\varepsilon}{k}f(\\mathbb{O}\\mathbb{P}\\mathbb{T})\\;\\;.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The remaining part of this proof is omitted since it is very similar to the corresponding part in the proof of Lemma A.5, except that the last inequality should be used instead of Inequality (5). \u53e3 ", "page_idx": 23}, {"type": "text", "text": "We are now ready to prove Theorem 2.2. ", "page_idx": 23}, {"type": "text", "text": "Theorem 2.2. There exists an algorithm that given a positive integer $k$ , a value $\\varepsilon\\in(0,1).$ , a value $t_{s}\\,\\in\\,[0,1]$ , a non-negative submodular function $f\\colon\\dot{2}^{{\\mathcal{N}}}\\rightarrow\\mathbb{R}_{\\geq0}$ , and $a$ set $Z\\subseteq{\\mathcal{N}}$ obeying the inequalities stated in Theorem 2.1, outputs a solution $S_{k}$ , obeying ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[f(S_{k})]\\geq\\Big(\\frac{k-\\lceil t_{s}\\cdot k\\rceil}{k}\\alpha^{k-\\lceil t_{s}\\cdot k\\rceil-1}+\\alpha^{k-\\lceil t_{s}\\cdot k\\rceil}-\\alpha^{k}\\Big)f(\\mathbb{O}\\mathbb{P}\\mathbb{T})+}\\\\ &{\\qquad\\qquad+\\left(\\alpha^{k}+\\alpha^{k-1}-\\frac{2k-\\lceil t_{s}\\cdot k\\rceil}{k}\\alpha^{k-\\lceil t_{s}\\cdot k\\rceil-1}\\right)f(\\mathbb{O}\\mathbb{P}\\mathbb{T}\\cup Z)}\\\\ &{\\qquad\\qquad+\\left(\\alpha^{k}-\\alpha^{k-\\lceil t_{s}\\cdot k\\rceil}\\right)f(\\mathbb{O}\\mathbb{P}\\mathbb{T}\\cap Z)-2\\varepsilon f(\\mathbb{O}\\mathbb{P}\\mathbb{T})\\ ,}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\alpha\\triangleq1-1/k$ . Moreover, this algorithm requires only $O_{\\varepsilon}(n)$ queries to the objective function. ", "page_idx": 23}, {"type": "text", "text": "Proof of Theorem 2.2. The lower bound in Theorem 2.2 follows from the same arguments used in the proof of Theorem A.2, except that Lemma B.2 is used instead of Lemma A.5, which results in the additional error term $2\\varepsilon f(\\mathbb{O}\\mathbb{P}\\mathbb{T})$ in the lower bound of Theorem 2.2. ", "page_idx": 23}, {"type": "text", "text": "To bound the number of queries to the objective function necessary for implementing Algorithm 2, observe that each iteration of Algorithm 2 samples $O_{\\varepsilon}(n/k)$ elements, and the marginal gain (with respect to $S_{i-1}$ ) has to be computed only for the sampled elements. Thus, each iteration of Algorithm 2 requires only $O_{\\varepsilon}(n/k)$ queries to the objective function. Since the algorithm has only $k$ iterations, its total query complexity is $k\\cdot O_{\\varepsilon}(n/k)=O_{\\varepsilon}(n)$ . \u53e3 ", "page_idx": 23}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Our theoretical contribution can be found in Section 2, while the empirical evaluation of our method can be found in Section 3. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 24}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: Our paper has no limitations. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 24}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The proofs of the theoretical results can be found in Section 2, Appendix A, and Appendix B. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 25}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: All the information needed to reproduce the main experimental results of the paper can be found in Section 3. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 25}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The code can be accessed at the URL https://github.com/muradtuk/ 385ApproximationSubMax. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 26}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: All the details can be found in Section 3. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 26}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We present the error bars as the shaded regions in all of the figures. In Section 3, we clearly explain that the sizes of the error bars are determined by the standard deviations. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [No] ", "page_idx": 27}, {"type": "text", "text": "Justification: In Section 3, we wrote the computer type and memory needed to reproduce the experiments. Due to its high dependence on implementation details, we do not provide the time of execution. Instead, we provide in Section 3 the number of queries needed to obtain the results. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 27}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 27}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: Our paper studies a well-known submodular optimization problem without societal impact. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 27}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 28}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper poses no such risks. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 28}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The creators or original owners of assets (e.g., code, data, models) used in the paper are properly credited in Section 3. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 29}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper involves neither crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 29}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper involves neither crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}]