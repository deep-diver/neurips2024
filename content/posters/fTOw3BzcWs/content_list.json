[{"type": "text", "text": "ExID: Offline RL with Intuitive Expert Insights in Limited-Data Settings ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 With the ability to learn from static datasets, Offilne Reinforcement Learning (RL)   \n2 emerges as a compelling avenue for real-world applications. However, state-of-the  \n3 art offilne RL algorithms perform sub-optimally when confronted with limited data   \n4 confined to specific regions within the state space. The performance degradation   \n5 is attributed to the inability of offline RL algorithms to learn appropriate actions   \n6 for rare or unseen observations. This paper proposes a novel domain knowledge  \n7 based regularization technique and adaptively refines the initial domain knowledge   \n8 to considerably boost performance in limited data with partially omitted states.   \n9 The key insight is that the regularization term mitigates erroneous actions for   \n0 sparse samples and unobserved states covered by domain knowledge. Empirical   \n11 evaluations on standard discrete environment datasets demonstrate a substantial   \n2 average performance increase compared to ensemble of domain knowledge and   \n13 existing offline RL algorithms operating on limited data. ", "page_idx": 0}, {"type": "text", "text": "14 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "15 Offline RL [9, 1], also referred to as batch RL, is a learning approach that focuses on extracting   \n16 knowledge solely from static datasets. This class of algorithms has a wider range of applications being   \n17 particularly appealing to real-world data sets from business [46], healthcare [25], and robotics [35].   \n18 However, offline RL poses unique challenges, including over-fitting and the need for generalization   \n19 to data not present in the dataset. To surpass the behavior policy, offline RL algorithms need to   \n20 query Q values of actions not in the dataset, causing extrapolation errors [21]. Most offline RL   \n21 algorithms address this problem by enforcing constraints that ensure that the learned policy does not   \n22 deviate too far away from the data set\u2019s state action distribution [13, 11] or is conservative towards   \n23 Out-of-Distribution (OOD) actions [21, 20]. However, such approaches are designed on coherent   \n24 batches [13], which do not account for OOD states.   \n25 In many domains, such as business and healthcare, available data is scarce and often confined to expert   \n26 behaviors within a limited state space. For example, a sales recommendation system, where historic   \n27 data may not contain details about many active users and operator gives coupon of higher value to   \n28 attract sales. Learning on such limited data sets can curtail the generalization capabilities of state-of  \n29 the-art (SOTA) offline RL algorithms, resulting in sub-optimal performance [23]. We illustrate this   \n30 limitation via Fig 1. In Fig 1a) the state action space of a simple Mountain Car environment [27] is   \n31 plotted for an expert dataset [32] and a partial dataset with first $10\\%$ samples from the entire dataset.   \n32 Fig 1b) shows the average reward obtained over these data sets and the average difference between   \n33 the Q value of action taken by the under-performing Conservative Q Learning (CQL) [21] agent and   \n34 the action in the full expert dataset for unseen states. It can be observed that the performance of the   \n35 offline RL agent considerably drops. This is attributed to the critic overestimating the Q value of   \n36 non-optimal actions for states that do not occur in the dataset while training. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "image", "img_path": "fTOw3BzcWs/tmp/25449154728efe4cdd13ec741111d3e523f403eef3e2a360e672727ee67899fd.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: a) Full expert, Mountain Car dataset, and reduced dataset with first $10\\%$ samples showing distribution of state (position, velocity) and action b) CQL agent converging to a sub-optimal policy for reduced dataset exhibiting high Q values for actions different from actions in the expert dataset for unseen states. ", "page_idx": 1}, {"type": "text", "text": "37 In numerous real-world applications, expert insights regarding the general behavior of a policy are   \n38 often accessible [33]. For example, sales operators often distribute lower discount coupons to active   \n39 users to maximize profti. While these insights may not be optimal, they serve as valuable guidelines   \n40 for understanding the overall behavior of the policy. A rich literature in knowledge distillation [18]   \n41 has shown that teacher networks trained on domain knowledge can transfer knowledge to another   \n42 network unaware of it. This work aims to leverage a teacher network mimicking simple decision   \n43 tree-based domain knowledge to help offline RL generalize in limited data settings. ", "page_idx": 1}, {"type": "text", "text": "44 The paper makes the following novel contributions: ", "page_idx": 1}, {"type": "text", "text": "45 \u2022 We introduce an algorithm dubbed ExID, leveraging intuitive human obtainable expert   \n46 insights. The domain expertise is incorporated into a teacher policy, which improves offline   \n47 RL in limited-data settings through regularization.   \n48 \u2022 The teacher based on expected performance improvement of the offline policy during   \n49 training, improving the teacher network beyond initial heuristics.   \n50 \u2022 We demonstrate the effectiveness of our methodology on real sales promotion dataset,   \n51 several discrete OpenAI gym and Minigrid environments with standard offilne RL data sets   \n52 and show that ExID significantly exceeds the performance when faced with limited data. ", "page_idx": 1}, {"type": "text", "text": "53 2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "54 This work improves offline RL learning on batches sampled from static datasets using domain   \n55 expertise. One of the major concerns in offline RL is the erroneous extrapolation of OOD actions   \n56 [13]. Two techniques have been studied in the literature to prevent such errors. 1) Constraining the   \n57 policy to be close to the behavior policy 2) Penalizing overly optimistic Q values [24]. We discuss a   \n58 few relevant algorithms following these principles. In Batch-Constrained deep Q-learning (BCQ)   \n59 [13] candidate actions sampled from an adversarial generative model are considered, aiming to   \n60 balance proximity to the batch while enhancing action diversity. Algorithms like Random Ensemble   \n61 Mixture Model (REM) [2], Ensemble-Diversified Actor-Critic (EDAC) [3] and Uncertainty Weighted   \n62 Actor-Critic (UWAC) [42] penalize the Q value according to uncertainty by either using Q ensemble   \n63 networks or directly weighting the loss with uncertainty. CQL [21] enforces regularization on Q  \n64 functions by incorporating a term that reduces Q-values for OOD actions while increasing Q-values   \n65 for actions within the expected distribution. However, these algorithms do not handle OOD actions   \n66 for states not in the static dataset and can have errors induced by changes in transition probability.   \n67 Integration of domain knowledge in offline RL, though an important avenue, has not yet been   \n68 extensively explored. Domain knowledge incorporation has improved online RL with tight regret   \n69 bounds [33, 4]. In offline RL, bootstrapping via blending heuristics computed using Monte-Carlo   \n70 returns with rewards has shown to outperform SOTA algorithms by $9\\%$ [15]. Recent works improve   \n71 offline RL by incorporating a safety expert [40] and preference query [44], contrary to our work   \n72 which improves imperfect domain knowledge. The closest to our work is Domain Knowledge guided   \n73 Q learning (DKQ) [46] where domain knowledge is represented in terms of action importance and   \n74 the Q value is weighted according to importance. However, obtaining action importance in practical   \n75 scenarios is nontrivial. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "76 3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "77 A DRL setting is represented by a Markov Decision Process (MDP) formalized as $(S,A,T,r,\\rho_{0},\\gamma)$ .   \n78 Here, $S$ denotes the state space, $A$ signifies the action space, $T(s^{\\prime}|s,a)$ represents the transition prob  \n79 ability distribution, $r:S\\times A\\to\\mathbb{R}$ is the reward function, $\\rho_{0}$ represents the initial state distribution,   \n80 and $\\gamma\\in(0,1]$ is the discount factor. The primary objective of any DRL algorithm is to identify an   \n81 optimal policy $\\pi(a|s)$ that maximizes $\\begin{array}{r}{\\mathbb{E}_{s_{t},a_{t}}\\bigl[\\sum_{t=0}^{\\infty}\\gamma^{t}r\\bigl(s_{t},a_{t}\\bigr)\\bigr]}\\end{array}$ where, $s_{0}\\sim d_{0}(.),a_{t}\\sim\\pi(.|s_{t})$ , and   \n82 $s^{\\prime}\\sim T(.|s_{t},a_{t})$ . Deep Q networks (DQNs) [26] learn this objective by minimizing the Bellman resid  \n83 ual $(Q_{\\theta}(s,a)\\stackrel{.}{-}B^{\\pi_{\\theta}}\\bar{Q}_{\\theta}(s,a))^{2}$ where $B^{\\pi_{\\theta}}Q_{\\theta}(s,a)=\\vec{\\mathbb{E}_{s^{\\prime}\\sim T}}[r(s,\\dot{a})+\\gamma\\mathbb{E}_{a^{\\prime}\\sim\\pi_{\\theta}(.|s^{\\prime})}[Q_{\\theta^{\\prime}}(s^{\\prime},a^{\\prime})]]$ .   \n84 The policy $\\pi_{\\theta}$ chooses actions that maximize the $\\mathrm{\\DeltaQ}$ value $\\operatorname*{max}_{a^{\\prime}\\in A}Q_{\\theta}(s^{\\prime},a^{\\prime})$ . However, in offline   \n85 RL where transitions are sampled from a pre-collected dataset $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ , the chosen action $a^{\\prime}$ may exhibit a   \n86 bias towards OOD actions with inaccurately high Q-values. To handle the erroneous propagation   \n87 from OOD actions, CQL [22] learns conservative $\\mathrm{^Q}$ values by penalizing OOD actions. The CQL   \n88 loss for discrete action space is given by ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{L}_{c q l}(\\theta)=\\operatorname*{min}_{Q}\\,\\alpha\\,\\mathbb{E}_{s\\sim B}[l o g\\sum_{a}e x p(Q_{\\theta}(s,a))-\n$$", "text_format": "latex", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbb{E}_{a\\sim\\mathcal{B}|s}[Q_{\\theta}(s,a)]]+\\frac{1}{2}\\mathbb{E}_{s,a,s^{\\prime}\\sim\\mathcal{B}}[Q_{\\theta}-Q_{\\theta^{\\prime}}]^{2}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "89 Eq. 1 encourages the policy to be close to the actions seen in the dataset. However, CQL works on the   \n90 assumption of coherent batches, i.e., if $(s,a,s^{\\prime})\\in B$ , then $s^{\\prime}\\in\\mathcal{B}$ . There is no provision for handling   \n91 OOD actions for $s\\notin\\mathcal{B}$ , which can lead to policy failure when data is limited. In the next sections, we   \n92 present ExID, a domain knowledge-based approach to improve performance in data-scarce scenarios. ", "page_idx": 2}, {"type": "text", "text": "93 4 Problem Setting and Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "94 In our problem setting, the RL agent learns the policy on a limited dataset with rare and unseen   \n95 demonstrations. We define the characteristics of this dataset as follows:   \n96 Definition 4.1. A reduced buffer $\\scriptstyle B_{r}$ is a proper subset of the full dataset $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ i.e., $B_{r}\\subset B$ satisfying   \n97 the following conditions: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "98 ", "page_idx": 2}, {"type": "text", "text": "99   \n100 ", "page_idx": 2}, {"type": "text", "text": "\u2022 The number of samples $N(s,a,s^{\\prime})$ for some transitions in $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ are less in $\\boldsymbol{{B_{r}}}$ i.e, $\\exists(s,a,s^{\\prime})\\in$ $\\mathcal{B}:N(s,a,s^{\\prime}){_{\\mathcal{B}_{r}}}<N(s,\\dot{a},s^{\\prime}){_{\\mathcal{B}}}$ ", "page_idx": 2}, {"type": "text", "text": "101 We observe, performing $Q-L$ earning by sampling from a limited buffer $\\boldsymbol{{\\beta}}_{r}$ may not converge   \n102 to an optimal policy for the MDP $M_{B}$ representing the full buffer. This can be shown as a special   \n103 case of (Theorem 1,[13]) as $p_{B}(s^{\\prime}|s,a)\\neq p_{B_{r}}(s^{\\prime}|s,a)$ and no $\\mathrm{\\DeltaQ}$ updates for $(s,a)\\notin B_{r}$ leading to   \n104 sub-optimal policy. Please refer to the App. B for analysis and example.   \n105 We also assume a set of common sense rules in the form of domain knowledge is available. Domain   \n106 knowledge $\\mathcal{D}$ is defined as hierarchical decision nodes capturing $S\\rightarrow A$ as represented by Eq. 2.   \n107 Each decision node $T_{\\eta_{i}}$ is represented by a constraint $\\phi_{\\eta_{i}}$ and Boolean indicator $\\mu_{\\eta_{i}}$ function selects   \n108 the branch to be traversed based on $\\phi_{\\eta_{i}}$ . ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A c t i o n=\\left\\{a_{\\eta_{i}}\\begin{array}{l l l}{\\quad}&&{\\mathrm{if}\\ \\ l e a f}\\\\ {\\quad}&{\\quad}\\\\ {\\mu_{\\eta_{i}}T_{\\eta_{i}\\swarrow}(s)+(1-\\mu_{\\eta_{i}})T_{\\eta_{i}\\searrow}(s)}&&{\\mathrm{o/w}}\\end{array}\\right.}\\\\ &{\\mu_{\\eta_{i}}(s)=\\left\\{\\begin{array}{l l}{1\\ \\mathrm{~if~}s\\ensuremath{\\left|~\\right|}=\\phi_{\\eta_{i}}}&\\\\ {0\\ \\ }&{\\mathrm{o/w}}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "image", "img_path": "fTOw3BzcWs/tmp/6443e7ec89d3faea5613952db46ec0de943735a0ffb71b8533f23ce6f986e9c4.jpg", "img_caption": ["Figure 2: Overview of the proposed methodology (a) Training a teacher policy network with domain knowledge and synthetic data (b) Updating the offline RL critic network with teacher network "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "109 We assume that $\\mathcal{D}$ gives heuristically reasonable actions for $s\\vDash D$ and $S_{D}\\cap S_{B_{r}}\\neq\\emptyset$ where $S_{D},S_{B_{r}}$   \n110 are the state coverage of $\\mathcal{D}$ and $\\scriptstyle B_{r}$ .   \n111 Training Teacher: An overview of our methodology is depicted in Fig 2. We first construct a   \n112 trainable actor network $\\pi_{t}^{\\omega}$ parameterized by $\\omega$ from $\\mathcal{D}$ , Fig 2 step 1. For training $\\pi_{t}^{\\omega}$ synthetic   \n113 data $\\hat{S}$ is generated by sampling states from a uniform random distribution over state boundaries   \n114 $B(s)$ , $\\hat{S}=\\mathcal{U}(B(S))$ . Note that this does not represent the true state distribution and may have state   \n115 combinations that will never occur. We train $\\pi_{t}^{\\omega}$ using behavior cloning where state $\\hat{s}\\sim\\hat{S}$ is checked   \n116 with root decision node in Eq. 2. A random action is chosen if $\\hat{s}$ does not satisfy decision node $T_{\\eta_{0}}$   \n117 or leaf action is absent. If $\\hat{s}$ satisfies a $T_{\\eta_{i}}$ , $T_{\\eta_{i}}$ is traversed and action $a_{\\eta_{i}}$ is returned from the leaf   \n118 node. This is illustrated in Fig 2 (a). We term the pre-trained actor network $\\pi_{t}^{\\omega}$ as the teacher policy.   \n119 Regularizing Critic: We now introduce Algo 1 (App C) to train an offilne RL agent on $\\scriptstyle B_{r}$ . Algo 1   \n120 takes $\\boldsymbol{{\\beta}}_{r}$ and pretrained $\\pi_{t}^{\\omega}$ as input. The algorithm uses two hyper-parameters, warm start parameter   \n121 $k$ and mixing parameter $\\lambda$ . A critic network $Q_{s}^{\\theta}$ with Monte-Carlo (MC) dropout and target network   \n122 $Q_{s}^{\\theta^{\\prime}}$ are initialized. ExID is divided into two phases. In the first phase, we aim to warm start the critic   \n123 network $Q_{s}^{\\theta}$ with actions from $\\pi_{t}^{\\omega}$ as shown in Fig 2b( i). However, this must be done selectively   \n124 as the teacher\u2019s policy is random around the states that do not satisfy domain knowledge. In each   \n125 iteration, we first check the states sampled from a mini-batch of $\\boldsymbol{{\\beta}}_{r}$ with $\\mathcal{D}$ . For the states which   \n126 satisfy $\\mathcal{D}$ we compute the teacher action $\\pi_{t}^{\\omega}(s)$ and critic\u2019s action $\\operatorname{argmax}_{a}(Q_{s}^{\\theta}(s,a))$ and collect it   \n127 in lists $\\boldsymbol{a}_{t},\\boldsymbol{a}_{s}$ , Algo 1 lines 4-10. Our main objective is to keep actions chosen by the critic network   \n128 for $s\\vDash\\mathcal{D}$ close to the teacher\u2019s policy. To achieve this, we introduce a regularization term: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{r}(\\theta)=\\underbrace{\\mathbb{E}_{s\\sim\\mathcal{B}_{r}\\wedge s}|{=}\\mathcal{D}}_{\\mathrm{states\\;matching\\;domain}}\\underbrace{[Q_{s}^{\\theta}(s,a_{s})-Q_{s}^{\\theta}(s,a_{t})]^{2}}_{\\mathrm{Uegularizer}}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "129 Eq 3 incentivizes the critic to increase Q values for actions from $\\pi_{t}^{\\omega}$ and decreases Q values for other   \n130 actions when argmax ${}_{\\iota}(Q_{s}^{\\theta}(s,a))\\neq\\pi_{t}^{\\omega}(s)$ for states that satisfy domain knowledge. Note that Eq 3   \n131 will only be 0 when $\\mathrm{argmax}_{a}(Q_{s}^{\\theta}(s,a))=\\pi_{t}^{\\omega}(s)$ for $s\\vDash\\mathcal{D}$ . It is also set to 0 for $s\\not\\in{\\mathcal{D}}$ . However,   \n132 since $\\pi_{t}^{\\omega}$ mimicking heuristic rules is sub-optimal, it is also important to incorporate learning from   \n133 the data. The final loss is a combination of Eq. 1 and Eq. 3 with a mixing parameter $\\lambda\\in[0,\\bar{1}]$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\theta)=\\mathcal{L}_{c q l}(\\theta)+\\lambda\\mathbb{E}_{s\\sim\\mathcal{B}_{r}\\wedge s}|=\\mathcal{D}\\big[Q_{s}^{\\theta}(s,a_{s})-Q_{s}^{\\theta}(s,a_{t})\\big]^{2}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "134 The choice of $\\lambda$ and the warm start parameter $k$ depends on the quality of $\\mathcal{D}$ . In the case of perfect   \n135 domain knowledge, $\\lambda$ would be set to 1, and setting $\\lambda$ to 0 would lead to the vanilla CQL loss. Mixing   \n136 both the losses allows the critic to learn both from the data in $B_{r}$ and knowledge in $\\mathcal{D}$ .   \n137 Updating Teacher: Given a reasonable warm start, the critic is expected to give higher $\\mathrm{\\DeltaQ}$ values   \n138 for optimal actions for $s\\,\\in\\,\\mathcal{D}\\cap\\mathcal{B}_{r}$ as it learns from data. We aim to leverage this knowledge   \n139 to enhance the initial teacher policy $\\pi_{t}^{\\omega}$ trained on heuristic domain knowledge. For $s\\,\\sim\\,B$ and   \n140 $s\\vDash\\mathcal{D}$ , we calculate the average $\\mathrm{\\DeltaQ}$ values over critic actions and teacher actions and check which   \n141 one is higher in Algo 1 line 11 which refers to Cond. 6. For brevity $\\mathbb{E}_{s\\sim\\mathcal{B}_{r}\\wedge s\\left\\vert=\\mathcal{D}\\right.}$ is written as $\\mathbb{E}$ .   \n142 If $\\mathbb{E}(Q_{s}^{\\theta}(s,a_{s}))>\\mathbb{E}(Q_{s}^{\\theta}(s,a_{t}))$ it denotes the critic expects a better return on an average over its   \n143 own policy than the teacher\u2019s policy. Hence, we can use the critic\u2019s policy to update $\\pi_{t}^{\\omega}$ , making   \n144 it better over $\\mathcal{D}$ . However, only checking the critic\u2019s value can be erroneous as the critic can have   \n145 high values for OOD actions. We check the average uncertainty of the predicted Q values to prevent   \n146 the teacher from getting updated by OOD actions. Uncertainty has been shown to be a good metric   \n147 for OOD action detection by [42, 3]. A well-established methodology to capture uncertainty is   \n148 predictive variance, which takes inspiration from Bayesian formulation for the critic function and   \n149 aims to maximize $p(\\theta|X,Y)=p(Y|\\bar{X},\\theta)p(\\theta)/p(Y|\\dot{X})$ where $X=(s,a)$ and $Y$ represents the true   \n150 Q value of the states. However, $p(Y|X)$ is generally intractable and is approximated using Monte   \n151 Carlo (MC) dropout, which involves including dropout before every layer of the critic network and   \n152 using it during inference [14]. Following [42], we measure the uncertainty of prediction using $\\operatorname{Eq}5$ . ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\nV a r^{T}[Q(s,a)]\\approx\\frac{1}{T}\\sum_{t=1}^{T}[Q(s,a)-\\bar{Q}(s,a)]^{2}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "153 Eq 5 estimates the variance of $\\mathrm{^Q}$ value $Q(s,a)$ for an action $a$ using $T$ forward passes on the $Q_{s}^{\\theta}(s,a)$   \n154 with dropout where $\\bar{Q}(s,a)$ represents the predictive mean. We check the average uncertainty of   \n155 the Q value for action chosen by the critic and teacher policy over the states that match domain   \n156 knowledge in a batch. The teacher network is updated using the critic\u2019s action only when the policy   \n157 expects a higher average $\\mathrm{^Q}$ return on its action and the average uncertainty of taking this action is   \n158 lower than the teacher action. $\\mathbb{E}(V a r^{T}Q_{s}^{\\theta}(s_{r},a_{s}))<\\mathbb{E}(V a r^{T}Q_{s}^{\\theta}(s_{r},a_{t}))$ indicates the actions were   \n159 learned from the expert data in the buffer and are not OOD samples. The condition is summarized in   \n160 cond. 6: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}(Q_{s}^{\\theta}(s_{r},a_{s}))>\\mathbb{E}(Q_{s}^{\\theta}(s_{r},a_{t}))\\wedge}\\\\ {\\mathbb{E}(V a r^{T}Q_{s}^{\\theta}(s_{r},a_{s}))<\\mathbb{E}(V a r^{T}Q_{s}^{\\theta}(s_{r},a_{t}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "161 We update the teacher with cross-entropy described in Eq 7: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\omega)=-\\sum_{s\\downarrow=D}(\\pi_{t}^{\\omega}(s)l o g(\\pi_{s}(s)))\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "162 where, $\\begin{array}{r}{\\pi_{s}(s,a)=\\frac{e^{Q(s,a)}}{\\sum_{a^{\\prime}}Q(s,a^{\\prime})}}\\end{array}$ . When the critic\u2019s policy is better than the teacher\u2019s policy, ${\\mathcal{L}}_{r}(\\theta)$ is   \n163 set to 0 Algo 1 Lines 11 to 13. Finally, the critic network is updated using calculated loss ${\\mathcal{L}}(\\theta)$ Algo   \n164 1 Lines 17-18. We theoretically analyse the implications of using ExID in propositions 4.2 and 4.3.   \n165 Proposition 4.2. Denote $\\hat{\\pi}$ as the policy learned by $E x I D$ , $\\pi_{u}$ as any offilne RL policy learned on $\\boldsymbol{{\\beta}}_{r}$   \n166 and optimal $Q$ function as $Q^{*}$ and $V_{.}$ function as $V^{*}$ . Then it holds that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\eta(\\hat{\\pi})-\\eta(\\pi_{u})\\geq\\mathbb{E}_{s\\sim{\\cal O}|\\pi_{u}}[V^{*}(s)-Q^{*}(s,\\pi_{u}(s))]-\\bar{\\rho}_{\\hat{\\pi}}\\alpha\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "167 Where $\\begin{array}{r}{\\alpha=\\mathbb{E}_{s\\sim O}[V^{*}(s)-Q^{*}(s,\\hat{\\pi}(s))],\\bar{\\rho}_{\\pi}(s)=\\left[\\frac{1}{|S_{\\hat{\\pi}}|(1-\\gamma)},\\frac{1}{1-\\gamma}\\right](\\big\\lvert\\,S_{\\hat{\\pi}}\\,\\big\\rvert\\,}\\end{array}$ is the number of different   \n168 states observed by $\\hat{\\pi}$ ) and $O\\notin B_{r}$ . Here $\\alpha$ denotes the quality of regularized action for $s\\notin\\ensuremath{\\mathcal{B}}_{r}$ . Hence,   \n169 updating $\\pi_{t}^{\\omega}$ is important as high divergence of action from the optimal can lead to performance   \n170 degradation. In offilne RL, the extrapolation error for non optimal action is usually high for states not   \n171 observed in dataset (as illustrated in 1b), regularization can lead performance improvement when $\\pi_{t}^{\\omega}$   \n172 is reasonable. Furthermore, in ExID coarse actions from $\\pi_{t}^{\\omega}$ are updated driving them closer to the   \n173 optimal actions, improving the performance lower bound. Additionally $\\pi_{t}^{\\omega}$ increases $\\mid S_{\\hat{\\pi}}\\mid$ making   \n174 $\\bar{\\rho}_{\\pi}\\ll1$ in practice further improving the performance lower bound. Proof is deferred to App. $A$ . ", "page_idx": 4}, {"type": "text", "text": "177 5 Empirical Evaluations ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "178 We investigate the following through our empirical evaluations: 1. Does ExID perform better than   \n179 combining $\\mathcal{D}$ and offline RL algos on different environments with datasets exhibiting rare and OOD   \n180 states Sec 5.2? 2. Does ExID generalize to OOD states covered by $\\mathcal{D}$ Sec 5.4? 3. What is the effect of   \n181 varying $k$ , \u03bb and updating $\\pi_{t}^{\\omega}$ Sec 5.5? 4. How does performance vary with the quality of D Sec 5.6? ", "page_idx": 5}, {"type": "text", "text": "182 5.1 Experimental Setting ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "183 We evaluate our methodology on open-AI gym [5], MiniGrid [6] and real sales promotion (SP) [30]   \n184 offline data sets. All our data sets are generated using standard methodologies defined in [32, 31]   \n185 except $S P$ which is generated by human operators. All experiments have been conducted on a   \n186 Ubuntu $22.04.2\\,\\mathrm{LTS}$ system with 1 NVIDIA K80 GPU, 4 CPUs, and 61GiB RAM. App. F notes the   \n187 hyperparameter values and network architectures.   \n188 Dataset: We experiment on three types of data sets. Expert Data-set [10, 16, 22] generated using   \n189 an optimal policy without any exploration with high trajectory quality but low state action coverage.   \n190 Replay Data-set [2, 13] generated from a policy while training it online, exhibiting a mixture of   \n191 multiple behavioral policies with high trajectory quality and state action coverage. Noisy Data-set   \n192 [12, 13, 22, 16] generated using an optimal policy that also selects random actions with $\\epsilon$ greedy   \n193 strategy where $\\epsilon=0.2$ having low trajectory quality and high state action coverage. Additionally we   \n194 also experiment on human generated dataset for sales promotion task.   \n195 Baselines: We do comparative studies on 10 baselines for OpenAI gym datasets. The first baseline   \n196 simply checks the conditions of $\\mathcal{D}$ and applies corresponding actions in execution. The performance   \n197 of this baseline shows that $\\mathcal{D}$ is imperfect and does not achieve the optimal reward. CQL SE is   \n198 from [40] where the expert is replaced by $\\mathcal{D}$ . The other baselines are an ensemble of $\\mathcal{D}$ and eight   \n199 algorithms popular in the Offline RL literature for discrete environments. These algorithms include   \n200 Behavior Cloning (BC) [29], Behaviour Value Estimation (BVE) [16], Quantile Regression DQN   \n201 (QRDQN) [7], REM, MCE, BCQ, CQL and Critic Regularized Regression Q-Learning (CRR) [41].   \n202 For a fair comparison, we use actions from domain knowledge for states not in the buffer and actions   \n203 from the trained policy for other states to obtain the final reward. Hence, each algorithm is renamed   \n204 with the suffix D in Table 5.1.   \n205 Limiting Data: To create limited-data settings for benchmark datasets, we first extract a small   \n206 percentage of samples from the full dataset and remove some of the samples based on state conditions.   \n207 This is done to ensure the reduced buffer satisfies the conditions defined in Def 4.1. We describe   \n208 the specific conditions of removal in the next section. Further insights and the state visualizations   \n209 for selected reduced datasets are in App H. Note : no data reduction has been performed on SP   \n210 dataset to demonstrate a real dataset exhibits characteristics of reduced buffer. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "211 5.2 Performance across Different Datasets ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "212 Our results for OpenAI gym environments are summarised in Table 5.1 and Minigrid in Table 3 (App   \n213 D). We observe the performance of offilne RL algorithms degrades substantially when part of the data   \n214 is not seen and trajectory ratios change. For these cases with only $10\\%$ partial data, ExID surpasses   \n215 the performance by at least $27\\%$ in the presence of reasonable domain knowledge. The proposed   \n216 method performs strongest on the replay dataset where the contribution of $L_{r}(\\theta)$ is significant due   \n217 to state coverage, and the teacher learns from high-quality trajectories. Environment details are   \n218 described in the App. D. All domain knowledge trees are shown in the App. D Fig 10. We describe   \n219 limiting data conditions and domain knowledge specific to the environment as follows:   \n220 Mountain Car Environment: [27] We use simple, intuitive domain knowledge in this environment   \n221 shown in the App. D Fig 10 (c), which represents taking a left action when the car is at the bottom of   \n222 the valley with low velocity to gain momentum; otherwise, taking the right action to drive the car up.   \n223 Fig 6 (c) shows the state action pairs this rule generates on states sampled from a uniform random   \n224 distribution over the state boundaries. It can be observed that the states of $\\mathcal{D}$ cover part of the missing   \n225 data in Fig 1 (a). For limiting datasets, we remove states with position $>-0.8$ . The performance of   \n226 CQLD and ExID are shown in Fig 3 (a),(b) where ExID surpasses CQLD for all three datasets.   \n227 Cart-pole Environment: For this environment, we use domain knowledge from [33], which aims to   \n228 move in the direction opposite to the lean of the pole, keeping the cart close enough to the center. If   \n229 the cart is close to an edge, the domain knowledge attempts to account for the cart\u2019s velocity and   \n230 recenter the cart. The full tree is given in the App. D Fig 10 (a). We remove states with cart velocity   \n231 $>-1.5$ to create the reduced buffer.   \n232 Lunar-Lander Environment: We borrow the decision nodes from [34] and get actions from a   \n233 sub-optimal policy trained online with an average reward of 52.48. The full set of decision nodes is   \n234 shown in the App. D Fig 10 (b). $\\mathcal{D}$ focuses on keeping the lander balanced when the lander is above   \n235 ground. When the lander is near the surface, $\\mathcal{D}$ focuses on keeping the y velocity lower. To create the   \n236 reduced datasets, we remove data of lander angle $<-0.04$ .   \n237 Mini-Grid Environments: For our experiments, we choose two environments: Random Dynamic   \n238 Obstacles 6X6 and LavaGapS 7X7. We use intuitive domain knowledge which avoids crashing into   \n239 obstacles in front, left, or right of agent ref. App. D Fig 10 (d), (e). We remove states with obstacles   \n240 on the right for creating limited data settings. Due to limitation of space we report the results of the   \n241 best-performing algorithms on the replay dataset in Table 3 (App D). ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "table", "img_path": "fTOw3BzcWs/tmp/ce5b30b9b91c0a37b965cc13d70e62cf93c469b5c484283fbeb52e38866e7959.jpg", "table_caption": ["Table 1: Average reward [\u2191] obtained during online evaluation over 3 seeds on openAI gym envs "], "table_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "fTOw3BzcWs/tmp/fe0079042b292053ecda0b721d0c4605222196866a203ae3f628ed74d099246f.jpg", "img_caption": ["Figure 3: Performance of (a) CQL and (b) EXID on all datasets for Mountain Car during online evaluation (c) Evaluation curves for the sales promotion dataset "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "242 5.3 Case study on real human generated Sales Promotion (SP) dataset ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "243 SP dataset and environment [30] simulates a real-world sales promotion platform. The number of   \n244 coupons and the discount the user received will affect his behavior. A higher discount will promote   \n245 the sales, but the cost will also increase. The goal for the platform operator is to maximize the   \n246 total profit. The horizon of the dataset is 50 days for the training and 30 days for the test. Domain   \n247 knowledge ([30], App A] : Active users can be given more coupons with lower discount to maximize   \n248 profti. We model this as $o r d e r_{n u m b e r}>60\\land A v g_{f e e}>0.8\\implies[5,0.95]$ where action 1 is number   \n249 of coupons range [0,5] and action 2 is coupon value (discount value $=$ (1-coupon value)) range   \n250 [0.6,0.95]. The dataset exhibits the properties in Def 4.1 as first 50 days of sales does not contain   \n251 many active users as reported in the coverage column of Tab 2 depicting scarcity. The domain rule is   \n252 imperfect as coupon value and number depend on multiple factors such as user purchase history and   \n253 behavior. As illustrated in the table 2 and Fig 3 (c) the intuitive domain rule enhances performance   \n254 by $10.49\\%$ in the real dataset. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "table", "img_path": "fTOw3BzcWs/tmp/944609198e5358af36c74012b0fb72018e883f8fba7ce340da1f214c1905246c.jpg", "table_caption": ["Table 2: Results on human generated Sales Promotion dataset "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "255 5.4 Generalization to OOD states and contribution of ${\\mathcal{L}}_{r}(\\theta)$ ", "text_level": 1, "page_idx": 7}, {"type": "image", "img_path": "fTOw3BzcWs/tmp/26c58b2a94ccb24ddaa64e51fd3f6a98a4ffdc11bbe5c31fc2acc3d31f3c795a.jpg", "img_caption": ["Figure 4: Q value difference between CQL and EXID for expert and policy action on states not present in the buffer for a) expert b) noisy in log scale c) contribution of $\\bar{\\mathcal{L}}_{r}(\\dot{\\theta})$ "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "256 In Fig 4 (a), (b), we plot $Q_{s}^{\\theta}(s,a_{e x p e r t})-Q_{s}^{\\theta}(s,a_{\\theta})$ for CQL and EXID policies for different datasets   \n257 of Mountain-Car environments. Action $a_{e x p e r t}$ is obtained from the full expert dataset where position   \n258 $>-0.8$ . We observe that the $\\mathrm{\\DeltaQ}$ value for actions of CQL policy diverges from the expert policy   \n259 actions with high values for the states not in the reduced buffer, whereas ExID stays close to the   \n260 expert actions for the unseen states. This empirically shows generalization to OOD states not in the   \n261 dataset but covered by domain knowledge. In Fig 4 (d), we plot the contribution by $\\mathcal{L}_{r}(\\theta)$ during the   \n262 training and observe the contribution is higher for replay data sets with more state coverage. ", "page_idx": 7}, {"type": "text", "text": "263 5.5 Performance on varying $\\lambda,k$ , and ablation of $\\pi_{t}^{\\omega}$ ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "264 We study the effect of varying $\\lambda$ on the algorithm for the given domain knowledge. We empirically   \n265 observe setting a high or a low $\\lambda$ can yield sub-optimal performance, and $\\lambda=0.5$ generally gives   \n266 good performance. In Fig 5 (a), we show this effect for LunarLander. Plots for other environments   \n267 are in the App. G Fig 11. For $k$ we observe setting the warm start parameter to 0 yields a sub-optimal   \n268 policy, as the critic may update $\\pi_{t}^{\\omega}$ without completely learning from it. The starting performance   \n269 increases with an increase in $k$ as shown in Fig 5 (b) for LunarLander. $k=30$ works best according   \n270 to empirical evaluations. Plots for other environments are in the App. G Fig 12. We show two   \n271 ablations for Cart-pole in Fig 5 (c) with no teacher update after the warm start and no inclusion of   \n272 $\\mathcal{L}_{r}(\\theta)$ after the warm start. The warm start in this environment is set to 30 episodes. Fig 5 c) shows   \n273 without teacher updated, the sub-optimal teacher drags down the performance of the policy beyond   \n274 the warm start, exhibiting the necessity of $\\pi_{t}^{\\omega}$ update. Also, the student converges to a sub-optimal   \n275 policy if no ${\\mathcal{L}}_{r}(\\theta)$ is included beyond the warm start. ", "page_idx": 7}, {"type": "image", "img_path": "fTOw3BzcWs/tmp/96dbfa51d60838deefe59e98d891d18e1fa5dcb7db711ef6e52250376f82491c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 5: (a) Effect of different $\\lambda$ on the performance of ExID on Lunar Lander (b) Effect of different $k$ on the performance of EXID on Lunar Lander (c) Performance of EXID with teacher update, no teacher update, and just warm start on Cart-pole. ", "page_idx": 8}, {"type": "image", "img_path": "fTOw3BzcWs/tmp/75ad873be1e82d54dcc35cec49a829aa9b745341f581216f6ec42f1fb2cfc2e8.jpg", "img_caption": ["Figure 6: (a) $\\mathcal{D}$ with different average rewards (b) Performance effect on Lunar-lander (c) State distribution generated for training the teacher network for mountain-car "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "276 5.6 Effect of varying $\\mathcal{D}$ quality ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "277 We show the effect of choosing policies as $\\mathcal{D}$ with different average rewards for Lunar-Lander expert   \n278 data in Fig 6 (a) and (b). Rule 1 is optimal and has almost the same effect as Rule 3, which is the $\\mathcal{D}$   \n279 used in our experiments exhibiting that updating a sub-optimal $\\mathcal{D}$ can lead to equivalent performance   \n280 as optimal $\\mathcal{D}$ . Using a rule with high uncertainty, as Rule 2, induces high uncertainty in the learned   \n281 policy but performs slightly better than the baseline. Rule 4, which has a lower average reward, also   \n282 causes gains on average performance with slower convergence. Finally, Rule 5, with very bad actions,   \n283 affects policy performance adversely and leads to a performance lower than baseline CQL. ", "page_idx": 8}, {"type": "text", "text": "284 6 Conclusion and Limitation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "285 In this paper, we study the effect of limited and partial data on offline RL and observe that the   \n286 performance of SOTA offline RL algorithms is sub-optimal in such settings. The paper proposes a   \n287 methodology to handle offilne RL\u2019s performance degradation using domain insights. We incorporate   \n288 a regularization loss in the CQL training using a teacher policy and refine the initial teacher policy   \n289 while training. We show that incorporating reasonable domain knowledge in offline RL enhances   \n290 performance, achieving a performance close to full data. However, this method is limited by the   \n291 quality of the domain knowledge and the overlap between domain knowledge states and reduced   \n292 buffer data. The study is also limited to discrete domains. In the future, the authors would like to   \n293 improve on capturing domain knowledge into the policy network without dependence on data and   \n294 extending the methodology to algorithms that handle continuous action space. ", "page_idx": 8}, {"type": "text", "text": "295 7 Broader Impact ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "296 During the trial-and-error training phase, RL agents may exhibit irrational behavior, which can be   \n297 risky and costly in real-world scenarios. As a more practical alternative to online RL, offline RL   \n298 utilizes pre-existing collected data to eliminate the need for real-time interactions during training.   \n299 However, a drawback of offline RL is its dependence on the quality and quantity of historical data,   \n300 which, when sub-optimal, could adversely affect overall performance. Therefore, through this work,   \n301 we use domain knowledge to suppress erroneous actions when available data is limited. However, this   \n302 inclusion may facilitate harmful behavior in the presence of biased domain knowledge. Therefore,   \n303 we advocate the use of well-regulated domain knowledge obtained from experts. Beyond this, we do   \n304 not foresee any ethical impact on our work. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "305 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "306 [1] A survey on offilne reinforcement learning: Taxonomy, review, and open problems. IEEE Transactions on   \n307 Neural Networks and Learning Systems, 2023. ISSN 21622388. doi: 10.1109/TNNLS.2023.3250269.   \n308 [2] Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. An optimistic perspective on offline   \n309 reinforcement learning. In International Conference on Machine Learning, pages 104\u2013114. PMLR, 2020.   \n310 [3] Gaon An, Seungyong Moon, Jang-Hyun Kim, and Hyun Oh Song. Uncertainty-based offilne reinforcement   \n311 learning with diversified q-ensemble. Advances in neural information processing systems, 34:7436\u20137447,   \n312 2021.   \n313 [4] Peter L. Bartlett and Ambuj Tewari. Regal: A regularization based algorithm for reinforcement learning in   \n314 weakly communicating mdps. In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial   \n315 Intelligence, UAI \u201909, page 35\u201342, Arlington, Virginia, USA, 2009. AUAI Press. ISBN 9780974903958.   \n316 [5] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and   \n317 Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.   \n318 [6] Maxime Chevalier-Boisvert, Bolun Dai, Mark Towers, Rodrigo de Lazcano, Lucas Willems, Salem Lahlou,   \n319 Suman Pal, Pablo Samuel Castro, and Jordan Terry. Minigrid & miniworld: Modular & customizable   \n320 reinforcement learning environments for goal-oriented tasks. CoRR, abs/2306.13831, 2023.   \n321 [7] Will Dabney, Mark Rowland, Marc Bellemare, and R\u00e9mi Munos. Distributional reinforcement learning   \n322 with quantile regression. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32,   \n323 2018.   \n324 [8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirec  \n325 tional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.   \n326 [9] Damien Ernst, Pierre Geurts, and Louis Wehenkel. Tree-based batch mode reinforcement learning. Journal   \n327 of Machine Learning Research, 6, 2005.   \n328 [10] Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep   \n329 data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020.   \n330 [11] Scott Fujimoto and Shixiang Shane Gu. A minimalist approach to offilne reinforcement learning. Advances   \n331 in neural information processing systems, 34:20132\u201320145, 2021.   \n332 [12] Scott Fujimoto, Edoardo Conti, Mohammad Ghavamzadeh, and Joelle Pineau. Benchmarking batch deep   \n333 reinforcement learning algorithms. arXiv preprint arXiv:1910.01708, 2019.   \n334 [13] Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without explo  \n335 ration. In International conference on machine learning, pages 2052\u20132062. PMLR, 2019.   \n336 [14] Yarin Gal and Zoubin Ghahramani. A theoretically grounded application of dropout in recurrent neural   \n337 networks. Advances in neural information processing systems, 29, 2016.   \n338 [15] Sinong Geng, Aldo Pacchiano, Andrey Kolobov, and Ching-An Cheng. Improving offline rl by blending   \n339 heuristics. arXiv preprint arXiv:2306.00321, 2023.   \n340 [16] Caglar Gulcehre, Sergio G\u00f3mez Colmenarejo, Ziyu Wang, Jakub Sygnowski, Thomas Paine, Konrad   \n341 Zolna, Yutian Chen, Matthew Hoffman, Razvan Pascanu, and Nando de Freitas. Regularized behavior   \n342 value estimation. arXiv preprint arXiv:2103.09575, 2021.   \n343 [17] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv   \n344 preprint arXiv:1503.02531, 2015.   \n345 [18] Zhiting Hu, Xuezhe Ma, Zhengzhong Liu, Eduard Hovy, and Eric Xing. Harnessing deep neural networks   \n346 with logic rules. arXiv preprint arXiv:1603.06318, 2016.   \n347 [19] Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In   \n348 Proceedings of the Nineteenth International Conference on Machine Learning, pages 267\u2013274, 2002.   \n349 [20] Ilya Kostrikov, Rob Fergus, Jonathan Tompson, and Ofir Nachum. Offline reinforcement learning with   \n350 fisher divergence critic regularization. In International Conference on Machine Learning, pages 5774\u20135783.   \n351 PMLR, 2021.   \n352 [21] Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy $\\mathbf{q}$ -learning   \n353 via bootstrapping error reduction. Advances in Neural Information Processing Systems, 32, 2019.   \n354 [22] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative $\\mathbf{q}$ -learning for offline   \n355 reinforcement learning. Advances in Neural Information Processing Systems, 33:1179\u20131191, 2020.   \n356 [23] Sergey Levine, Aviral Kumar, G. Tucker, and Justin Fu. Offline reinforcement learning: Tutorial,   \n357 review, and perspectives on open problems. ArXiv, abs/2005.01643, 2020. URL https://api.   \n358 semanticscholar.org/CorpusID:218486979.   \n359 [24] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial,   \n360 review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.   \n361 [25] Siqi Liu, Kay Choong See, Kee Yuan Ngiam, Leo Anthony Celi, Xingzhi Sun, and Mengling Feng.   \n362 Reinforcement learning for clinical decision support in critical care: comprehensive review. Journal of   \n363 medical Internet research, 22(7):e18477, 2020.   \n364 [26] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,   \n365 Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through   \n366 deep reinforcement learning. nature, 518(7540):529\u2013533, 2015.   \n367 [27] Andrew William Moore. Efficient memory-based learning for robot control. Technical report, University   \n368 of Cambridge, Computer Laboratory, 1990.   \n369 [28] Susan A Murphy. A generalization error for q-learning. 2005.   \n370 [29] Dean A Pomerleau. Efficient training of artificial neural networks for autonomous navigation. Neural   \n371 computation, 3(1):88\u201397, 1991.   \n372 [30] Rong-Jun Qin, Xingyuan Zhang, Songyi Gao, Xiong-Hui Chen, Zewen Li, Weinan Zhang, and Yang Yu.   \n373 Neorl: A near real-world benchmark for offline reinforcement learning. Advances in Neural Information   \n374 Processing Systems, 35:24753\u201324765, 2022.   \n375 [31] Kajetan Schweighofer, Markus Hofmarcher, Marius-Constantin Dinu, Philipp Renz, Angela Bitto-Nemling,   \n376 Vihang Prakash Patil, and Sepp Hochreiter. Understanding the effects of dataset characteristics on offilne   \n377 reinforcement learning. In Deep RL Workshop NeurIPS 2021, 2021. URL https://openreview.net/   \n378 forum?id=A4EWtf-TO3Y.   \n379 [32] Kajetan Schweighofer, Marius-constantin Dinu, Andreas Radler, Markus Hofmarcher, Vihang Prakash   \n380 Patil, Angela Bitto-Nemling, Hamid Eghbal-zadeh, and Sepp Hochreiter. A dataset perspective on offilne   \n381 reinforcement learning. In Conference on Lifelong Learning Agents, pages 470\u2013517. PMLR, 2022.   \n382 [33] Andrew Silva and Matthew Gombolay. Encoding human domain knowledge to warm start reinforcement   \n383 learning. In Proceedings of the AAAI conference on artificial intelligence, volume 35, pages 5042\u20135050,   \n384 2021.   \n385 [34] Andrew Silva, Matthew Gombolay, Taylor Killian, Ivan Jimenez, and Sung-Hyun Son. Optimization   \n386 methods for interpretable differentiable decision trees applied to reinforcement learning. In International   \n387 conference on artificial intelligence and statistics, pages 1855\u20131865. PMLR, 2020.   \n388 [35] Samarth Sinha, Ajay Mandlekar, and Animesh Garg. S4rl: Surprisingly simple self-supervision for offilne   \n389 reinforcement learning in robotics. In Aleksandra Faust, David Hsu, and Gerhard Neumann, editors,   \n390 Proceedings of the 5th Conference on Robot Learning, volume 164 of Proceedings of Machine Learning   \n391 Research, pages 907\u2013917. PMLR, 08\u201311 Nov 2022.   \n392 [36] Kihyuk Sohn, Zizhao Zhang, Chun-Liang Li, Han Zhang, Chen-Yu Lee, and Tomas Pfister. A simple   \n393 semi-supervised learning framework for object detection. arXiv preprint arXiv:2005.04757, 2020.   \n394 [37] Jiaxi Tang and Ke Wang. Ranking distillation: Learning compact ranking models with high performance   \n395 for recommender system. In Proceedings of the 24th ACM SIGKDD international conference on knowledge   \n396 discovery & data mining, pages 2289\u20132298, 2018.   \n397 [38] Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, and Jimmy Lin. Distilling task-specific   \n398 knowledge from bert into simple neural networks. arXiv preprint arXiv:1903.12136, 2019.   \n399 [39] Wei-Cheng Tseng, Tsun-Hsuan Johnson Wang, Yen-Chen Lin, and Phillip Isola. Offline multi-agent   \n400 reinforcement learning with knowledge distillation. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave,   \n401 K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages   \n402 226\u2013237. Curran Associates, Inc., 2022.   \n403 [40] Richa Verma, Durgesh Kalwar, Harshad Khadilkar, and Balaraman Ravindran. Guiding offline reinforce  \n404 ment learning using a safety expert. In Proceedings of the 7th Joint International Conference on Data   \n405 Science & Management of Data (11th ACM IKDD CODS and 29th COMAD), pages 82\u201390, 2024.   \n406 [41] Ziyu Wang, Alexander Novikov, Konrad Zolna, Josh S Merel, Jost Tobias Springenberg, Scott E Reed,   \n407 Bobak Shahriari, Noah Siegel, Caglar Gulcehre, Nicolas Heess, et al. Critic regularized regression.   \n408 Advances in Neural Information Processing Systems, 33:7768\u20137778, 2020.   \n409 [42] Yue Wu, Shuangfei Zhai, Nitish Srivastava, Joshua M. Susskind, Jian Zhang, Ruslan Salakhutdinov, and   \n410 Hanlin Goh. Uncertainty weighted actor-critic for offline reinforcement learning. In International Confer  \n411 ence on Machine Learning, 2021. URL https://api.semanticscholar.org/CorpusID:234763307.   \n412 [43] Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V Le. Self-training with noisy student improves   \n413 imagenet classification. In Proceedings of the IEEE/CVF conference on computer vision and pattern   \n414 recognition, pages 10687\u201310698, 2020.   \n415 [44] Qisen Yang, Shenzhi Wang, Matthieu Gaetan Lin, Shiji Song, and Gao Huang. Boosting offilne reinforce  \n416 ment learning with action preference query. In International Conference on Machine Learning, pages   \n417 39509\u201339523. PMLR, 2023.   \n418 [45] Li Yuan, Francis EH Tay, Guilin Li, Tao Wang, and Jiashi Feng. Revisiting knowledge distillation via label   \n419 smoothing regularization. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition   \n420 (CVPR), pages 3902\u20133910, 2020. doi: 10.1109/CVPR42600.2020.00396.   \n421 [46] Xiaoxuan Zhang and S Zhang Y Yu. Domain knowledge guided offline q learning. In Second Offline   \n422 Reinforcement Learning Workshop at Neurips, volume 2021, 2021.   \n423 [47] Ying Zheng, Haoyu Chen, Qingyang Duan, Lixiang Lin, Yiyang Shao, Wei Wang, Xin Wang, and   \n424 Yuedong Xu. Leveraging domain knowledge for robust deep reinforcement learning in networking.   \n425 In IEEE INFOCOM 2021 - IEEE Conference on Computer Communications, pages 1\u201310, 2021. doi:   \n426 10.1109/INFOCOM42981.2021.9488863. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "427 A Theoretical Analysis ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "428 Notations ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "429 For any deterministic policy $\\pi$ the performance return is formulated as $\\begin{array}{r}{\\eta(\\pi)=\\mathbb{E}_{\\tau\\sim\\pi}[\\sum_{t=0}^{\\infty}\\gamma^{t}r(s_{t},a_{t})]}\\end{array}$   \n430 For any policy $\\pi$ , $\\rho_{\\pi}$ is the (unormalized) discounted visitation frequency given by $\\begin{array}{r}{\\rho_{\\pi}(s)=\\sum_{t=0}^{\\infty}\\gamma^{t}P(s_{t}=s)}\\end{array}$   \n431 where $s_{0}\\;\\sim\\;\\rho^{0}(s_{0})$ and the trajectory $\\left(s_{0},s_{1},\\ldots\\right)$ is sampled from the policy $\\pi$ and $\\textstyle\\rho_{\\pi}(s)\\ \\in\\ [0,{\\frac{1}{1-\\gamma}}]$ .   \n432 $\\begin{array}{r}{\\bar{\\rho}_{\\pi}(s)=s u p\\{\\rho_{\\pi}(s),s\\in S\\}\\in[\\frac{1}{|S_{\\pi}|(1-\\gamma)},\\frac{1}{(1-\\gamma)}]}\\end{array}$ ", "page_idx": 12}, {"type": "text", "text": "433 We denote the regularized policy learned by $\\mathrm{ExID}$ on $\\scriptstyle{\\mathcal{B}}_{r}$ as $\\hat{\\pi}$ and the unregularized policy as $\\pi_{u}$ . ", "page_idx": 12}, {"type": "text", "text": "434 Lemmas ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "435 We introduce the following Lemma required for our theoretical analysis. ", "page_idx": 12}, {"type": "text", "text": "436 Lemma A.1. ([44]) Given two policies $\\pi_{1}$ and $\\pi_{2}$ ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\eta(\\pi_{1})-\\eta(\\pi_{2})=\\int_{s\\in S}\\rho_{\\pi_{1}}(s)(Q^{*}(s,\\pi_{1}(s)-V^{*}(s))d s-\\int_{s\\in S}\\rho_{\\pi_{2}}(s)(Q^{*}(s,\\pi_{2}(s)-V^{*}(s))d s))d s.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "437 Proof. Please refer to Lemma A.1 Eq 17 in [44] ", "page_idx": 12}, {"type": "text", "text": "438 Proposition A.2. (4.2) Denote $\\hat{\\pi}$ as the policy learned by $E x I D$ , $\\pi_{u}$ as any offilne RL policy learned on $\\scriptstyle{{\\mathcal{B}}_{r}}$ and   \n439 optimal $Q$ function as $Q^{*}$ and $V_{.}$ function as $V^{*}$ . Then it holds that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\eta(\\hat{\\pi})-\\eta(\\pi_{u})\\geq\\mathbb{E}_{s\\sim{\\cal O}|\\pi_{u}}[V^{*}(s)-Q^{*}(s,\\pi_{u}(s))]-\\bar{\\rho}_{\\hat{\\pi}}\\alpha\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "440 Proof. According to [19] performance improvement between two policies if given by ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\eta(\\pi_{1})=\\eta(\\pi_{2})+\\mathbb{E}_{\\tau\\sim\\pi_{1}}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}Q_{\\pi_{2}}(s_{t},a_{t})-V_{\\pi_{2}}(s_{t})\\right]\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "441 Replacing $\\pi_{1}$ by $\\hat{\\pi}$ and $\\pi_{2}$ by $\\pi_{u}$ and by following Lemma A.1 ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\eta(\\widehat{\\pi})-\\eta(\\pi_{u})=\\displaystyle\\int_{s\\in S}\\rho_{\\widehat{\\pi}}(s)(Q^{*}(s,\\widehat{\\pi}(s))-V^{*}(s))d s-\\displaystyle\\int_{s\\in S}\\rho_{\\pi_{u}}(s)(Q^{*}(s,\\pi_{u}(s))-V^{*}(s))d s}\\\\ &{\\qquad=\\displaystyle\\int_{s\\in S}\\rho_{\\pi_{u}}(s)(V^{*}(s)-Q^{*}(s,\\pi_{u}(s)))d s-\\displaystyle\\int_{s\\in S}\\rho_{\\widehat{\\pi}}(s)(V^{*}(s)-Q^{*}(s,\\widehat{\\pi}(s)))d s}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "442 Dividing the state space into in dataset domain states (I) and OOD states (O). The ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underbrace{\\left[\\displaystyle\\int_{s\\in I}\\rho_{\\pi_{u}}(s)(V^{*}(s)-Q^{*}(s,\\pi_{u}(s)))d s-\\displaystyle\\int_{s\\in I}\\rho_{\\widehat{\\pi}}(s)(V^{*}(s)-Q^{*}(s,\\widehat{\\pi}(s)))d s\\right]}_{a}+\\underbrace{\\left[\\displaystyle\\int_{s\\in O}\\rho_{\\pi_{u}}(s)(V^{*}(s)-Q^{*}(s,\\widehat{\\pi}(s)))d s\\right]}_{b}+}\\\\ &{\\underbrace{\\left[\\displaystyle\\int_{s\\in O}\\rho_{\\pi_{u}}(s)(V^{*}(s)-Q^{*}(s,\\pi_{u}(s)))d s-\\displaystyle\\int_{s\\in O}\\rho_{\\widehat{\\pi}}(s)(V^{*}(s)-Q^{*}(s,\\widehat{\\pi}(s)))d s\\right]}_{b}}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Since the regularization loss facilitates visitation to OOD states via knowledge distillation we assume 443 $\\rho_{\\hat{\\pi}}=\\rho_{\\pi_{u}}-\\Delta_{i}$ for $s\\in i$ and $\\rho_{\\hat{\\pi}}=\\rho_{\\pi_{u}}+\\Delta_{o}$ for $s\\in o$ where $\\Delta_{i}\\in[0,\\rho_{\\pi_{u}(s)}]$ and $\\begin{array}{r}{\\Delta_{o}\\in\\left[0,\\frac{1}{1-\\gamma}-\\rho_{\\pi_{u}(s)}\\right]}\\end{array}$ ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle a=\\int_{s\\in I}\\rho_{\\pi_{u}}(s)(V^{*}(s)-Q^{*}(s,\\pi_{u}(s)))d s-\\int_{s\\in I}(\\rho_{\\pi_{u}}-\\Delta_{i})(s)(V^{*}(s)-Q^{*}(s,\\hat{\\pi}(s)))d s}\\\\ {\\displaystyle-\\int_{s\\in I}\\rho_{\\pi_{u}}(s)(Q^{*}(s,\\hat{\\pi}(s))-Q^{*}(s,\\pi_{u}(s)))d s+\\int_{s\\in I}\\Delta_{i}(s)(V^{*}(s)-Q^{*}(s,\\hat{\\pi}(s)))d s}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Under assumption in distribution action can be learned from the dataset due to conservatism of offline RL 444 $\\left(Q^{*}(s,{\\hat{\\pi}}(s))^{-}{\\cal Q}^{*}(s,\\pi_{u}(s))\\right)\\approx0$ , $a\\geq0$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{b=\\int_{s\\in\\mathcal{O}}\\rho_{\\pi u}(s)(V^{*}(s)-Q^{*}(s,\\pi_{u}(s)))d s-\\int_{s\\in\\mathcal{O}}(\\rho_{\\pi u}+\\Delta_{o})(s)(V^{*}(s)-Q^{*}(s,\\hat{\\pi}(s)))d s}}\\\\ &{\\qquad\\ge\\int_{s\\in\\mathcal{O}}\\rho_{\\pi_{u}}(s)(V^{*}(s)-Q^{*}(s,\\pi_{u}(s)))d s-\\int_{s\\in\\mathcal{O}}\\rho_{\\hat{\\pi}}(s)(V^{*}(s)-Q^{*}(s,\\hat{\\pi}(s)))d s}\\\\ &{\\qquad\\qquad\\ge\\mathbb{E}_{s\\sim O|\\pi_{u}}[V^{*}(s)-Q^{*}(s,\\pi_{u}(s))]-\\mathbb{E}_{s\\sim O|\\hat{\\pi}}[V^{*}(s)-Q^{*}(s,\\hat{\\pi}(s))]}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "445 Further loosening the lower bound ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\mathbb{E}_{s\\sim O\\mid\\pi_{u}}[V^{*}(s)-Q^{*}(s,\\pi_{u}(s))]-\\bar{\\rho}_{\\hat{\\pi}}\\int_{s\\in O}\\frac{\\rho_{\\hat{\\pi}}}{\\bar{\\rho}_{\\hat{\\pi}}}(V^{*}(s)-Q^{*}(s,\\hat{\\pi}(s)))d s}\\\\ &{\\ \\ \\geq\\mathbb{E}_{s\\sim O\\mid\\pi_{u}}[V^{*}(s)-Q^{*}(s,\\pi_{u}(s))]-\\bar{\\rho}_{\\hat{\\pi}}\\int_{s\\in O}(V^{*}(s)-Q^{*}(s,\\hat{\\pi}(s)))d s}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "446 Combining Eq 14, 17 and 19, and denoting $\\alpha=\\mathbb{E}_{s\\sim O}[V^{*}(s)-Q^{*}(s,\\hat{\\pi}(s))]$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\eta(\\hat{\\pi})-\\eta(\\pi_{u})\\geq\\mathbb{E}_{s\\sim{\\cal O}|\\pi_{u}}[V^{*}(s)-Q^{*}(s,\\pi_{u}(s))]-\\bar{\\rho}_{\\hat{\\pi}}\\alpha\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "447 Hence, Proposition 4.2 follows Q.E.D ", "page_idx": 13}, {"type": "text", "text": "448 ", "page_idx": 13}, {"type": "text", "text": "449 Proposition A.3. (4.3) Algo 1 reduces generalization error if $Q^{*}(s,\\pi_{t}^{\\omega}(s))>Q^{*}(s,\\pi(s))\\,f o r$ $s\\in\\mathcal{D}\\cap B_{r}$ ,   \n450 where $\\pi$ is vanilla offline RL policy learnt on $\\scriptstyle{{\\mathcal{B}}_{r}}$ . ", "page_idx": 13}, {"type": "text", "text": "451 Proof. Generalization error for any policy $\\pi$ as defined by [28] can be written as: ", "page_idx": 13}, {"type": "equation", "text": "$$\nG_{\\pi}=V^{*}(s_{0})-V_{\\pi}(s_{0})=-\\mathbb{E}_{\\tau\\sim\\pi}[\\sum_{t=0}^{T}\\gamma^{t}Q^{*}(s_{t},\\pi(s_{t}))-V^{*}(s_{t})]\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Here, $\\mathbb{E}_{\\tau\\sim\\pi}$ represents sampling trajectories with policy $\\pi$ . Since the state space is continuous, we can represent 452 the expectation as an integral over the state space ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle=-\\sum_{t=0}^{T}\\gamma^{t}\\int_{s\\in S}P(s_{t}=s|\\pi)(Q^{*}(s_{t},\\pi(s_{t}))-V^{*}(s_{t}))d s}\\\\ {\\displaystyle=-\\int_{s\\in S}\\sum_{t=0}^{T}\\gamma^{t}P(s_{t}=s|\\pi)(Q^{*}(s_{t},\\pi(s_{t}))-V^{*}(s_{t}))d s}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "453 Analysing with respect to $s\\in\\mathcal{D}\\cap B_{r}$ we can break the integration into two parts ", "page_idx": 13}, {"type": "equation", "text": "$$\n=-\\left[\\int_{s\\in S/D}\\sum_{t=0}^{T}\\gamma^{t}P(s_{t}=s|\\pi)(Q^{*}(s_{t},\\pi(s_{t}))-V^{*}(s_{t}))d s+\\int_{s\\in D}\\sum_{t=0}^{T}\\gamma^{t}P(s_{t}=s|\\pi)(Q^{*}(s_{t},\\pi(s_{t}))-V^{*}(s_{t}))d s\\right].\n$$", "text_format": "latex", "page_idx": 13}, {"type": "equation", "text": "$$\n=-\\left[f(s|\\pi)+\\int_{s\\in D}\\sum_{t=0}^{T}\\gamma^{t}P(s_{t}=s|\\pi)(Q^{*}(s_{t},\\pi(s_{t}))-V^{*}(s_{t}))\\right]\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "For a policy $\\hat{\\pi}$ learnt in Algo 1 the action for $s_{t}=s\\in\\mathcal{D}$ is regularized to be close to $\\pi_{t}^{\\omega}$ which either follows domain knowledge or expert demonstrations. Hence, it is reasonable to assume $Q^{*}(s_{t},\\pi_{t}^{\\omega}(s_{t}))>Q^{*}(s_{t},\\pi(s_{t}))$ . 454 It follows ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\int_{s\\in D}\\sum_{t=0}^{T}\\gamma^{t}P(s_{t}=s|\\hat{\\pi})(Q^{*}(s_{t},\\hat{\\pi}(s_{t}))-V^{*}(s_{t}))<\\int_{s\\in D}\\sum_{t=0}^{T}\\gamma^{t}P(s_{t}=s|\\pi)(Q^{*}(s_{t},\\pi(s_{t}))-V^{*}(s_{t}))\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Note for $s\\not\\in{\\mathcal{D}}$ , $f(s|\\hat{\\pi})\\approx f(s|\\pi)$ . This is because the regularization term assigns max Q value to a different 455 action for $s\\in\\mathcal{D}$ but $m a x_{a}(Q(s,a))$ remains same ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\therefore-\\Bigg[f(s|\\hat{\\pi})+\\int_{s\\in D}\\sum_{t=0}^{T}\\gamma^{t}P(s_{t}=s|\\hat{\\pi})(Q^{*}(s_{t},\\hat{\\pi}(s_{t}))-V^{*}(s_{t}))\\Bigg]}\\\\ {\\displaystyle<-\\Bigg[f(s|\\pi)+\\int_{s\\in D}\\sum_{t=0}^{T}\\gamma^{t}P(s_{t}=s|\\pi)(Q^{*}(s_{t},\\pi(s_{t}))-V^{*}(s_{t}))\\Bigg]}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "456 Hence, $G_{\\hat{\\pi}}<G_{\\pi}$ Proposition 2 follows Q.E.D ", "page_idx": 14}, {"type": "text", "text": "458 B Missing Examples ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "459 Performing $Q-L$ earning by sampling from a reduced batch $\\scriptstyle{{\\mathcal{B}}_{r}}$ may not converge to an optimal policy for the   \n460 MDP MB representing the full buffer.   \n461 Example (Theorem 1,[13]) defines MDP $M_{B}$ of $\\boldsymbol{\\mathbf{\\rho}}_{\\perp}$ from same state action space of the original MDP $M$ with   \n462 transition probabilities pB(s\u2032|s, a) =  Ns\u02dc (Ns,(as,,sa,)s\u02dc) where $N(s,a,s^{\\prime})$ is the number of times $(s,a,s^{\\prime})$ occurs in $\\boldsymbol{\\mathbf{\\rho}}_{\\perp}$   \n463 and an terminal state $s_{i n i t}$ . It states $p s(s_{i n i t}|s,a)=1$ when $\\textstyle\\sum_{\\tilde{s}}N(s,a,\\tilde{s})=0$ . This happens when transitions   \n464 of some $s^{\\prime}$ of $(s,a,s^{\\prime})$ are missing from the buffer, which  may occur in $\\scriptstyle{\\mathcal{B}}_{r}$ when $B_{r}\\,\\,\\bar{\\subset}\\,\\,B$ . $r(s_{i n i t},s,a)$ is   \n465 initialized to $Q(s,a)$ . We assume that a policy learned on reduced dataset $\\scriptstyle{\\mathcal{B}}_{r}$ converges to optimal value function   \n466 and disprove it using the following counterexample:   \n482 Figure 8: We hypothesize the suboptimal perfor  \n483 mance of offline RL for limited data can be ad  \n484 dressed via domain knowledge via action regular  \n485 ization and knowledge distillation.   \n486   \n487   \n488   \n489 A visualization is shown in Fig 8. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "image", "img_path": "fTOw3BzcWs/tmp/0b8103fcf5065bbaf49cb95012782b4b299b10926807f3580504ab12a8fcf654.jpg", "img_caption": ["Figure 7: Example MDP, sampled buffer MDP and reduced buffer with Q tables "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "fTOw3BzcWs/tmp/ae8a01b3ffe512291933eecb1b086cd50d1c86a099f09ffc4a4fb1b80f636b8c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "We take a simple MDP illustrated in Fig 7 with 3 states and 2 actions (0,1). The reward of each action is marked along the transition. The sampled MDP is constructed the following samples (1,0,2)- 2,(1,1,2)-3, (2,0,3)-3, and (2,1,3)-2 and the reduced buffer MDP with samples (1,0,2)-2 and (1,1,2)-1. The probabilities are marked along the transition. It is easy to see that the policy learned under the reduced MDP converges to a nonoptimal policy after one step of the Q table update with $\\bar{Q}(s,a)\\,=$ $r(s,a)+\\acute{p}(s^{\\prime}|s,a)*m a x_{a^{\\prime}}\\acute{(}Q(s^{\\prime},a^{\\prime}))$ . This happens because of transition probability shift on reducing samples $p s(s^{\\prime}|s,a)\\;\\;\\hat{\\neq}\\;\\;p s_{r}(s^{\\prime}|s,a)$ and no $\\mathrm{\\DeltaQ}$ updates for $(s,a)\\notin B_{r}$ . ", "page_idx": 15}, {"type": "text", "text": "Our methodology addresses these issues as follows: ", "page_idx": 15}, {"type": "text", "text": "\u2022 For $s\\in D\\cap B_{r}$ better actions are enforced through regularization using $\\pi_{t}^{\\omega}$ even when the transition probabilities are low for optimal transitions. ", "page_idx": 15}, {"type": "text", "text": "\u2022 Incorporating regularization distills the teacher\u2019s knowledge in the critic-enhancing generalization. ", "page_idx": 15}, {"type": "text", "text": "490 C Algorithm ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "491 The pseudo code of the algorithm is described in Algo 1. ", "page_idx": 15}, {"type": "text", "text": "1: Input: Reduced buffer $\\boldsymbol{B}_{r}$ , Initial teacher network $\\pi_{t}^{\\omega}$ , Training steps $N$ , Warm-up steps $k$ , Soft   \nupdate $\\tau$ , hyperparameters: $\\lambda,\\alpha$   \n2: Initialize Critic with MC dropout and Target Critic $Q_{s}^{\\theta},Q_{s}^{\\theta^{\\prime}}$   \n3: for $n\\leftarrow1$ to $N$ do   \n4: Sample mini-batch $b$ of transitions $(s,a,r,s^{\\prime})\\sim\\mathcal{B}_{r}\\;a_{t}=[],a_{s}=[],s_{r}=[]$   \n5: for $s\\in b$ do   \n6: if $s\\vDash\\mathcal{D}$ and $\\pi_{t}^{\\omega}(s)\\neq a r g m a x_{a}(Q_{s}^{\\theta}(s,a))$ then   \n7: $a_{t}.a p p e n d(\\pi_{t}^{\\omega}(s))$   \n8: $\\begin{array}{r l}&{a_{s}.a p p e n d(\\mathrm{argmax}_{a}(Q_{s}^{\\theta}(s,a)))}\\\\ &{s_{r}.a p p e n d(s)}\\end{array}$   \n9:   \n10: end if   \n11: end for   \n12: if $n>k/\\setminus$ Cond. 6 then   \n13: Update $\\pi_{t}^{\\omega}(s)$ using Eq 7   \n14: $\\bar{\\mathcal{L}_{r}}(\\theta)=\\dot{0}$   \n15: else   \n16: Calculate ${\\mathcal{L}}_{r}(\\theta)$ using Eq 3   \n17: end if   \n18: Calculate ${\\mathcal{L}}(\\theta)$ using Eq 4   \n19: Update $Q_{s}^{\\theta}$ with ${\\mathcal{L}}(\\theta)$ and softy update $Q_{s}^{\\theta^{\\prime}}$ and $\\tau$   \n20: end for ", "page_idx": 16}, {"type": "text", "text": "492 D Environments and Domain Knowledge Trees ", "text_level": 1, "page_idx": 16}, {"type": "image", "img_path": "fTOw3BzcWs/tmp/27737678247b1154db43d3d25b0c6ef87f2bad536c088b6a38b3c958528bb3da.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure 9: Graphical visualizations of environments used in the experiments. These environments are a) MountainCar-v0 b) CartPole-v1 c) LunarLander-v2 d) MiniGrid-LavaGapS7-v0 e) MiniGridDynamic-Obstacles-Random-6x6-v0 ", "page_idx": 16}, {"type": "text", "text": "493 The graphical visualization of each environment is depicted in Fig 9. The choice of environment in this paper   \n494 depended on two factors: a) Pre-existing standard methods of generating offline RL datasets. b) Possibility of   \n495 creating intuitive decision tree-based domain knowledge. All datasets have been created via [31]. We explain the   \n496 environments in detail as follows:   \n497 Mountain-car Environment: This environment Fig 9 a) has two state variables, position and velocity, and three   \n498 discrete actions: left push, right push, and no action [27]. The goal is to drive a car up a valley to reach the flag.   \n499 This environment is challenging for offilne RL because of sparse rewards, which are only obtained on reaching   \n500 the flag.   \n501 Cart-pole Environment The environment Fig 9 b) has 4 states and 2 actions representing left force and right   \n502 force. The objective is to balance a pole on a moving cart.   \n503 Lunar-Lander Environment: The task is to land a lunar rover between two flags $\\mathrm{Fig}\\;9\\;\\mathrm{c}$ ) by observing 8 states   \n504 and applying one of 4 actions.   \n505 Minigrid Environments: Mini-grid [6] is an environment suite containing 2D grid-worlds with goal oriented   \n506 tasks. As explained in the main text, we experiment using MiniGrid-LavaGapS7-v0 and MiniGrid-Dynamic  \n507 Obstacles-Random-6x6-v0 from this environment suite is shown in Fig 9 d) and e). In MiniGrid-LavaGapS7-v0,   \n508 the agent has to avoid Lava and pass through the gap to reach the goal. Dynamic obstacles are similar; however,   \n509 the agent can start at a random position and has to avoid dynamically moving balls to reach the goal. The   \n510 environment has image observation with 3 channels (OBJECT_ID, COLOR_ID, STATE). Following [31]   \n511 experiments, we flatten the image to an array of 98 observations and restrict action space to three actions: Turn   \n512 left, Turn Right, and Move forward. The results of minigrid environment are reported in Table 3. Since this   \n513 environment uses a semantic map from image observation, we collect states from a fixed policy with random   \n514 actions to generate the teacher\u2019s state distribution. CQL on the full dataset achieves the average reward of   \n515 $0.92\\pm0.1$ for DynamicObstacles and $0.53\\pm0.01$ for LavaGapS.   \n516 The domain knowledge trees for all the environments are shown in Fig 10. The cart pole domain knowledge   \n517 tree Fig 10 a) is taken from [33] (Fig 7). The Lunar Lander decision nodes Fig 10 b) have been taken from [34]   \n518 (Fig4). For the mini-grid environments, we construct intuitive decision trees shown in $\\mathrm{Fig}\\ 10\\,\\mathrm{d})$ and Fig 10 e).   \n519 Positions 52, 40, and 68 represent positions front, right, and left of the agent. Value 0.2 represents a wall, 0.9   \n520 represents Lava, and 0.6 represents a ball. We check positions 52, 40, and 68 for these obstacles and choose the   \n521 recommended actions as domain knowledge. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "image", "img_path": "fTOw3BzcWs/tmp/7bcdf9dd0ec73dc4461cea81558c0171f487ff5cfb1a09807c5b2bdfcf3fd5f7.jpg", "img_caption": ["Figure 10: Domain knowledge trees for a) CartPole-v1 b) LunarLander-v2 c) MountainCar-v0 d) MiniGrid-LavaGapS7-v0 e) MiniGrid-Dynamic-Obstacles-Random-6x6-v0 environments "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "522 E Related Work: Knowledge Distillation ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "523 Knowledge distillation is a well-embraced technique of incorporating additional information in neural networks   \n524 and has been applied to various fields like computer vision [43, 36], natural language processing [8, 38], and   \n525 recommendation systems [37]. [17] introduced the concept of distilling knowledge from a complex, pre-trained   \n526 model (teacher) into a smaller model (student). In recent years, researchers have explored the integration   \n527 of rule-based regularization techniques within the context of knowledge distillation. Rule regularization   \n528 introduces additional constraints based on predefined rules, guiding the learning process of the student model   \n529 [18, 45]. These techniques have shown to reduce overfitting and enhance generalization [38]. Knowledge   \n530 distillation is also prevalent in the field of RL [47] and offline RL [39]. Contrary to prevalent teacher-student   \n531 knowledge distillation techniques, our work does not enforce parameter sharing among the networks. Through   \n532 experiments, we demonstrate that a simple regularization loss and expected performance-based updates can   \n533 improve generalization to unobserved states covered by domain knowledge. There are also no constraints on   \n534 keeping the same network structure for the teacher, paving ways for capturing the domain knowledge into more   \n535 structured networks such as Differentiable Decision Trees (DDTs). ", "page_idx": 17}, {"type": "table", "img_path": "fTOw3BzcWs/tmp/adc6ef7bec126a995cda7b643d84ab70285d28e9d9d9414f85d767ff7740150c.jpg", "table_caption": ["Table 3: Average reward [\u2191] obtained during online evaluation over 3 seeds on Minigrid environments "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "536 F Network Architecture and Hyper-parameters ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "537 We follow the network architecture and hyper-parameters proposed by [31] for all our networks, including the   \n538 baseline networks. The teacher BC network $\\bar{\\pi_{\\omega}^{t}}$ and Critic network $\\dot{Q}_{s}^{\\theta}(s,a)$ consists of 3 linear layers, each   \n539 having a hidden size of 256 neurons. The number of input and output neurons depends on the environment\u2019s state   \n540 and action size. All layers except the last are SELU activation functions; the final layer uses linear activation.   \n541 $\\pi_{\\omega}^{t}$ uses a softmax activation function in the last layer for producing action probabilities. A learning rate of   \n542 0.0001 with batch size 32 and $\\alpha\\,=\\,0.1$ is used for all environments. MC dropout probability of 0.5 and   \n543 number of stochastic passes ${\\mathrm{T}}{=}10$ have been used for the critic network. The uncertainty check is performed   \n544 every 15 episodes after the warm start to avoid computational overhead. The hyper-parameters specific to our   \n545 algorithm for OpenAI gym are reported in Table F. The hyper-parameters specific to our algorithm for Minigrid   \n546 environments are reported in Table 5. ", "page_idx": 18}, {"type": "table", "img_path": "fTOw3BzcWs/tmp/f4402c8d1439900e30f765fa29c811728abd0027b3af030ee055baaae50bf267.jpg", "table_caption": ["Table 4: Hyperparameters for openAI gym environments "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "fTOw3BzcWs/tmp/e2c2bcb8b13dd6b6535bb1e29e5d81dda1f6bad0993a81007a7631b55c037a22.jpg", "table_caption": ["Table 5: Hyper-parameters for Mini-grid environments for replay dataset "], "table_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "fTOw3BzcWs/tmp/1562eca7f660c592fc5c6f777739fd3fd851fdac65be2248d4dabaeb7a6bc25e.jpg", "img_caption": ["Figure 11: Effect of $\\lambda$ on the performance of ExID for different environments expert datasets. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "547 G Effect of $k$ and $\\lambda$ and Evaluation Plots ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "548 We empirically evaluate the effect of $\\lambda$ In Fig 11 and $k$ in Fig 12. We believe these parameters depend on the   \n549 quality of $\\mathcal{D}$ . For the given $\\mathcal{D}$ in the environments we empirically observe, $\\lambda=0.5$ generally performs well,   \n550 except for Minigrid environments where $\\lambda=0.1$ works better. Increasing the warm start parameter $k$ generally   \n551 increases the initial performance of the policy, allowing it to learn from the teacher. Meanwhile, no warm start   \n552 adversely affects policy performance as the critic may erroneously update the teacher. From empirical evaluation,   \n553 we observe that $k=30$ gives a reasonable start to the policy. All the evaluation plots are shown in Fig 13, where   \n554 it can be observed that ExID performs better than baseline CQL. ", "page_idx": 19}, {"type": "image", "img_path": "fTOw3BzcWs/tmp/1e13f41834adca7367d73775ebdf88022894740b00bbafbdbe94fb9251cfdc13.jpg", "img_caption": ["Figure 12: Effect of $k$ on the performance of ExID for different environments expert datasets. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "555 H Data reduction design and data distribution visualization of reduced 556 dataset ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "557 In this section, we discuss the intuition behind our data-limiting choices. We also visually represent selected   \n558 reduced datasets for the OpenAI gym environments.   \n559 Reducing transitions from the dataset: For all datasets, $10\\%$ of the data samples were extracted from the full   \n560 dataset. This experimental design choice is based on the observation shown in Fig 14 (a). Performance degrades   \n561 on reducing samples to $0.1\\%$ of the dataset and reduces further on reducing samples to $0.05\\%$ of the dataset.   \n562 However, this drop is not substantial. The performance also reduces on removing part of the dataset from the   \n563 full dataset with states $>-0.8$ . However, the worst performance is observed when both samples are reduced   \n564 and data is omitted, attributing to accumulated errors from probability ratio shift contributing to an increase in   \n565 generalization error. Our methodology aims to address this gap in performance.   \n566 Removing part of the state space: Due to the simplicity of the Mountain-Car environment, we analyze the   \n567 Mountain-Car expert dataset to show the effect of removing data matching state conditions of the different nodes   \n568 in the decision tree in $\\mathrm{Fig}~10$ (c). The performance for each condition is summarised in Table 6. The most   \n569 informative node in the tree is position $>-0.5$ ; removing states matching this condition causes a performance   \n570 drop in the algorithm as the domain knowledge regularization does not contribute significant information to the   \n571 policy. Similarly, removing data with velocity $<0.01$ causes a performance drop. However, both performances   \n572 are higher than the baseline CQL trained on reduced data. Based on this observation, we choose state removal   \n573 conditions that preserve states matching part of the information in the tree such that the regularization term   \n574 contributes substantially to the policy. Fig 15 shows the data distribution plot of $10\\%$ samples extracted from   \n575 mountain car replay and noisy data with states $>-0.8$ removed. Fig 16 shows visualizations for $10\\%$ samples   \n576 extracted from expert data with velocity $>-1.5$ removed. Fig 17 shows visualizations for $10\\%$ samples   \n577 extracted from expert data with lander angle $<-0.04$ removed. ", "page_idx": 19}, {"type": "image", "img_path": "fTOw3BzcWs/tmp/7ae239380290479badf3edce7b87ddcd95c9bec1e805ca3018530034db38b0f6.jpg", "img_caption": ["Figure 13: Evaluation plots of CQL and EXID algorithms for Cartpole, Lunar-Lander, and Minigrid environments using different data types and seeds reported in the main paper Table 5.1. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "fTOw3BzcWs/tmp/22f2bce6f77d3ecb42d2a1e8f531911cbd6e496a5f50a0a7cfd691d183af227d.jpg", "img_caption": ["Figure 14: (a) The effect of data reduction and removal on baseline CQL visualized on Mountain Car Environment (b) Performance of ExID on removing different parts of the data based on nodes of Fig 10 (c) from Mountain Car expert dataset "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "Table 6: Performance of ExID on removing different parts of the data based on nodes of Fig 10 (c) from Mountain Car expert dataset ", "page_idx": 21}, {"type": "table", "img_path": "fTOw3BzcWs/tmp/6830b9671abf9476f0f38d07a99a40b8cac2472b516524350b9c57dc8fed8314.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "fTOw3BzcWs/tmp/8f362c50ce6193dd06a3c92dd3b81bf4aa255523b9ca4be4719b0769442f8951.jpg", "img_caption": ["Figure 15: Data distribution of reduced dataset compared to the full dataset for mountain replay and noisy data "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "fTOw3BzcWs/tmp/4d707e3eb80361cc3dc0d39f97b67a3d4f4934be13534f5dfcf0cd860f4b4a71.jpg", "img_caption": ["Figure 16: Data distribution of reduced cart pole expert dataset compared to the full dataset "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "fTOw3BzcWs/tmp/98bd24f968e6ba6336412a3e2b27458532c7af8589fefbe224763bcbdf195b90.jpg", "img_caption": ["Figure 17: Data distribution of reduced LunarLander expert dataset compared to the full dataset "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "578 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "579 1. Claims   \n580 Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s   \n581 contributions and scope?   \n582 Answer: [Yes]   \n583 Justification: The claims made in the paper have been experimented on different settings for validity   \n584 and generalization. Please refer to sec 5.2.   \n585 Guidelines:   \n586 \u2022 The answer NA means that the abstract and introduction do not include the claims made in the   \n587 paper.   \n588 \u2022 The abstract and/or introduction should clearly state the claims made, including the contributions   \n589 made in the paper and important assumptions and limitations. A No or NA answer to this   \n590 question will not be perceived well by the reviewers.   \n591 \u2022 The claims made should match theoretical and experimental results, and reflect how much the   \n592 results can be expected to generalize to other settings.   \n593 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not   \n594 attained by the paper.   \n595 2. Limitations   \n596 Question: Does the paper discuss the limitations of the work performed by the authors?   \n597 Answer: [Yes]   \n598 Justification: The paper acknowledges the dependency on reasonable domain knowledge and coverage   \n599 please refer to sec 6   \n600 Guidelines:   \n601 \u2022 The answer NA means that the paper has no limitation while the answer No means that the paper   \n602 has limitations, but those are not discussed in the paper.   \n603 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n604 \u2022 The paper should point out any strong assumptions and how robust the results are to violations of   \n660056 these assumptions (e.g., independence assumptions, noiseless settings, model well-specification,   \nasymptotic approximations only holding locally). The authors should reflect on how these   \n607 assumptions might be violated in practice and what the implications would be.   \n608 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested   \n609 on a few datasets or with a few runs. In general, empirical results often depend on implicit   \n610 assumptions, which should be articulated.   \n611 \u2022 The authors should reflect on the factors that influence the performance of the approach. For   \n612 example, a facial recognition algorithm may perform poorly when image resolution is low or   \n613 images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide   \n614 closed captions for online lectures because it fails to handle technical jargon.   \n615 \u2022 The authors should discuss the computational efficiency of the proposed algorithms and how   \n616 they scale with dataset size.   \n617 \u2022 If applicable, the authors should discuss possible limitations of their approach to address problems   \n618 of privacy and fairness.   \n619 \u2022 While the authors might fear that complete honesty about limitations might be used by reviewers   \n620 as grounds for rejection, a worse outcome might be that reviewers discover limitations that   \n621 aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize   \n622 that individual actions in favor of transparency play an important role in developing norms that   \n623 preserve the integrity of the community. Reviewers will be specifically instructed to not penalize   \n624 honesty concerning limitations.   \n625 3. Theory Assumptions and Proofs   \n626 Question: For each theoretical result, does the paper provide the full set of assumptions and a complete   \n627 (and correct) proof?   \n628 Answer: [Yes]   \n629 Justification: Please refer to App A in the supplement material for theoretical analysis and proofs.   \n630 Guidelines:   \n631 \u2022 The answer NA means that the paper does not include theoretical results.   \n632 \u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. ", "page_idx": 23}, {"type": "text", "text": "\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 24}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Yes all hyper-parameters and experimental setting have been clearly listed in the paper. Please refer to App F and sec 5.1.   \nGuidelines: ", "page_idx": 24}, {"type": "text", "text": "633   \n634   \n635   \n636   \n637   \n638   \n639   \n640   \n641   \n642   \n643   \n644   \n645   \n646   \n647   \n648   \n649   \n650   \n651   \n652   \n653   \n654   \n655   \n656   \n657   \n658   \n659   \n660   \n661   \n662   \n663   \n664   \n665   \n666   \n667   \n668   \n669   \n670   \n671   \n672   \n673   \n674   \n675   \n676   \n677   \n678   \n679   \n680   \n681   \n682   \n683   \n684   \n685   \n686   \n687   \n688   \n689   \n690   \n691 ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 24}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: The code is provided with the submission in a zip file with Readme for instructions. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "701 6. Experimental Setting/Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "02 Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters,   \n03 how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper uses open source code to create the dataset and lists the modifications in details in the main paper and supplement material. Please refer to App F and sec 5.1. ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.   \n\u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 25}, {"type": "text", "text": "712 7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: All experiments have been run on 3 random seeds and the error bounds have been reported in Table 5.1, Table 2 and Table 3. ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "738 8. Experiments Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "39 Question: For each experiment, does the paper provide sufficient information on the computer   \n40 resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?   \n41 Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Please refer to the Experimental setup section in the main paper sec 5.1. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 26}, {"type": "text", "text": "52 9. Code Of Ethics ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "53 Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code   \n54 of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 26}, {"type": "text", "text": "56 Justification: The authors have reviewed the code of ethics and the paper adheres to it. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "757   \n758 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n759 \u2022 If the authors answer No, they should explain the special circumstances that require a deviation   \n760 from the Code of Ethics.   \n761 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due   \n762 to laws or regulations in their jurisdiction).   \n763 10. Broader Impacts   \n764 Question: Does the paper discuss both potential positive societal impacts and negative societal impacts   \n765 of the work performed?   \n766 Answer: [Yes]   \n767 Justification: Please refer to the section broader impacts 7.   \n768 Guidelines:   \n769 \u2022 The answer NA means that there is no societal impact of the work performed.   \n770 \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or   \n771 why the paper does not address societal impact.   \n772 \u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g.,   \n773 disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deploy  \n774 ment of technologies that could make decisions that unfairly impact specific groups), privacy   \n775 considerations, and security considerations.   \n776 \u2022 The conference expects that many papers will be foundational research and not tied to particular   \n777 applications, let alone deployments. However, if there is a direct path to any negative applications,   \n778 the authors should point it out. For example, it is legitimate to point out that an improvement in   \n779 the quality of generative models could be used to generate deepfakes for disinformation. On the   \n780 other hand, it is not needed to point out that a generic algorithm for optimizing neural networks   \n781 could enable people to train models that generate Deepfakes faster.   \n782 \u2022 The authors should consider possible harms that could arise when the technology is being used   \n783 as intended and functioning correctly, harms that could arise when the technology is being used   \n784 as intended but gives incorrect results, and harms following from (intentional or unintentional)   \n785 misuse of the technology.   \n786 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies   \n787 (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitor  \n788 ing misuse, mechanisms to monitor how a system learns from feedback over time, improving the   \n789 efficiency and accessibility of ML). ", "page_idx": 26}, {"type": "text", "text": "790 11. Safeguards ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "91 Question: Does the paper describe safeguards that have been put in place for responsible release of   \n92 data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or   \n93 scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "Justification: The algorithm proposed in this paper does not not pose any such risk of misuse. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. ", "page_idx": 26}, {"type": "text", "text": "803 \u2022 We recognize that providing effective safeguards is challenging, and many papers do not require   \n804 this, but we encourage authors to take this into account and make a best faith effort.   \n805 12. Licenses for existing assets   \n806 Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper,   \n807 properly credited and are the license and terms of use explicitly mentioned and properly respected?   \n808 Answer: [Yes]   \n809 Justification: All codes and datasets used in this paper are under MIT licence and the original owners   \n810 have been cited.   \n811 Guidelines:   \n812 \u2022 The answer NA means that the paper does not use existing assets.   \n813 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n814 \u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n815 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n816 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of   \n817 that source should be provided.   \n818 \u2022 If assets are released, the license, copyright information, and terms of use in the package should   \n819 be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for   \n820 some datasets. Their licensing guide can help determine the license of a dataset.   \n821 \u2022 For existing datasets that are re-packaged, both the original license and the license of the derived   \n822 asset (if it has changed) should be provided.   \n823 \u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s   \n24 creators.   \n825 13. New Assets   \n826 Question: Are new assets introduced in the paper well documented and is the documentation provided   \n827 alongside the assets?   \n828 Answer: [NA]   \n829 Justification: No new assets have been introduced in this paper.   \n830 Guidelines:   \n831 \u2022 The answer NA means that the paper does not release new assets.   \n832 \u2022 Researchers should communicate the details of the dataset/code/model as part of their sub  \n833 missions via structured templates. This includes details about training, license, limitations,   \n834 etc.   \n835 \u2022 The paper should discuss whether and how consent was obtained from people whose asset is   \n836 used.   \n837 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an   \n838 anonymized URL or include an anonymized zip file.   \n839 14. Crowdsourcing and Research with Human Subjects   \n884401 Question: For crowdsourcing experiments and research with human subjects, does the paper include   \nthe full text of instructions given to participants and screenshots, if applicable, as well as details about   \n842 compensation (if any)?   \n843 Answer: [NA]   \n844 Justification: The paper did not require any crowdsourcing or human subject for experimentation.   \n845 Guidelines:   \n846 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human   \n847 subjects.   \n848 \u2022 Including this information in the supplemental material is fine, but if the main contribution of the   \n849 paper involves human subjects, then as much detail as possible should be included in the main   \n850 paper.   \n851 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other   \n852 labor should be paid at least the minimum wage in the country of the data collector.   \n853 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects   \n854 Question: Does the paper describe potential risks incurred by study participants, whether such   \n855 risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an ", "page_idx": 27}, {"type": "text", "text": "equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 27}, {"type": "text", "text": "57 Answer: [NA]   \n58 Justification: The paper did not require any crowdsourcing or human subject for experimentation.   \n59 Guidelines:   \n60 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human   \n61 subjects.   \n62 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be   \n63 required for any human subjects research. If you obtained IRB approval, you should clearly state   \n64 this in the paper.   \n65 \u2022 We recognize that the procedures for this may vary significantly between institutions and   \n66 locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for   \n67 their institution.   \n68 \u2022 For initial submissions, do not include any information that would break anonymity (if applica  \n69 ble), such as the institution conducting the review. ", "page_idx": 28}]