[{"figure_path": "tLXgzQ5WZl/figures/figures_1_1.jpg", "caption": "Figure 1: SCube. Given sparse input images with little or no overlap, our model reconstructs a high-resolution and large-scale scene in 3D represented with VoxSplats, ready to be used for novel view synthesis or LiDAR simulation.", "description": "This figure shows the overall pipeline of SCube. Given a set of sparse input images (with little to no overlap), SCube reconstructs a high-resolution and large-scale 3D scene.  This 3D scene is represented using VoxSplats, a novel representation combining sparse voxel grids with Gaussian splats for efficient rendering. The reconstructed scene can then be used for various downstream tasks, such as novel view synthesis (generating images from unseen viewpoints) and LiDAR simulation (creating realistic LiDAR point clouds).", "section": "1 Introduction"}, {"figure_path": "tLXgzQ5WZl/figures/figures_2_1.jpg", "caption": "Figure 2: Framework. SCube consists of two stages: (1) We reconstruct a sparse voxel grid with semantic logit conditioned on the input images using a conditional latent diffusion model based on XCube [40]. (2) We predict the appearance of the scene represented as VoxSplats and a sky panorama using a feedforward network. Our method allows us to synthesize novel views in a fast and accurate manner, along with many other applications.", "description": "This figure illustrates the two-stage framework of SCube. The first stage reconstructs a sparse voxel grid using a conditional latent diffusion model, conditioned on input images. This model progressively generates high-resolution grids in a coarse-to-fine manner, leveraging a hierarchical voxel latent diffusion model and incorporating semantic information.  The second stage predicts the appearance of the scene, represented as VoxSplats (a novel representation combining Gaussian splats and a sparse voxel hierarchy), and a sky panorama using a feedforward network.  This combination allows for fast and accurate novel view synthesis, as well as other applications. The figure visually depicts the process and different components at each stage.", "section": "3 Method"}, {"figure_path": "tLXgzQ5WZl/figures/figures_5_1.jpg", "caption": "Figure 3: Data Processing Pipeline. We add COLMAP [44] dense reconstruction points to the accumulated LiDAR points and compensate for dynamic objects using their bounding boxes. This provides us with a more complete geometry for training.", "description": "This figure illustrates the data processing pipeline used to create a more complete and accurate 3D geometry dataset for training the model.  It shows three stages: 1. Accumulating LiDAR points, removing points within bounding boxes of dynamic objects. 2. Adding dense 3D points from COLMAP multi-view stereo (MVS) reconstruction, and obtaining semantic labels. 3. Adding point samples for dynamic objects (according to bounding boxes). This results in static and accumulated ground truths for training, with each sample being a 102.4m x 102.4m cropped chunk centered around a random ego-vehicle pose, with more space allocated for the forward direction.", "section": "3.3 Postprocessing and Applications"}, {"figure_path": "tLXgzQ5WZl/figures/figures_6_1.jpg", "caption": "Figure 4: Novel View Synthesis. We show the synthesized novel views of SCube+ compared to baselines approaches. The inset of each subfigure shows a top-down visualization (an extreme novel view) of the reconstructed scene geometry.", "description": "This figure compares the novel view synthesis capabilities of the SCube+ method with several baseline methods.  It presents synthesized views from different perspectives (front-left, front, front-right) at timestamps T+5 and T+10, showing how well each method reconstructs the scene and generates novel views. The insets provide a top-down perspective for each image, highlighting the differences in 3D scene reconstruction accuracy and completeness.", "section": "4.2 Large-scale Scene Reconstruction"}, {"figure_path": "tLXgzQ5WZl/figures/figures_6_2.jpg", "caption": "Figure 4: Novel View Synthesis. We show the synthesized novel views of SCube+ compared to baselines approaches. The inset of each subfigure shows a top-down visualization (an extreme novel view) of the reconstructed scene geometry.", "description": "This figure compares the novel view synthesis capabilities of the proposed SCube+ method against several baseline methods. It shows the rendered novel views from three different perspectives (front-left, front, and front-right) for both the proposed method and the baselines, along with a top-down view of the reconstructed 3D scene to illustrate the model's ability to generate extreme novel viewpoints.", "section": "4.2 Large-scale Scene Reconstruction"}, {"figure_path": "tLXgzQ5WZl/figures/figures_7_1.jpg", "caption": "Figure 5: Geometry Reconstruction from Sparse Views. We show the comparison between our method and Metric3Dv2 [18]. The semantics of Metric3Dv2 are obtained from Segformer [64].", "description": "This figure compares the 3D scene reconstruction results of the proposed SCube method against the Metric3Dv2 method, using sparse input views.  The top row shows input images from two different scenes. The bottom row displays the 3D point clouds generated by each method for the respective scenes. The color-coding represents semantic information obtained from Segformer [64]. The visualization highlights the differences in geometry reconstruction accuracy and completeness between the two methods. SCube demonstrates superior performance in reconstructing the scene geometry accurately and completely, especially in regions where views are sparse.", "section": "4.2 Large-scale Scene Reconstruction"}, {"figure_path": "tLXgzQ5WZl/figures/figures_8_1.jpg", "caption": "Figure 6: LiDAR Simulation. We demonstrate qualitative results of image-to-consistent-LiDAR transfer. The LiDAR sequences are simulated by moving the camera forward by 60 meters.", "description": "This figure shows the results of LiDAR simulation using the SCube model.  Two examples are presented, each consisting of a single input front view image and a sequence of simulated LiDAR point clouds generated by moving the camera 60 meters forward. The LiDAR point clouds accurately reflect the underlying 3D geometry of the scene, demonstrating the ability of SCube to produce consistent and temporally coherent LiDAR sequences. This capability is crucial for the training and verification of autonomous driving systems.", "section": "4.4 Other Applications"}, {"figure_path": "tLXgzQ5WZl/figures/figures_9_1.jpg", "caption": "Figure 1: SCube. Given sparse input images with little or no overlap, our model reconstructs a high-resolution and large-scale scene in 3D represented with VoxSplats, ready to be used for novel view synthesis or LiDAR simulation.", "description": "This figure shows the overall pipeline of SCube, which reconstructs a high-resolution large-scale 3D scene from sparse input images with minimal overlap.  The left side shows the input images, and the right side shows the 3D reconstruction rendered from novel viewpoints. This demonstrates the ability of SCube to generate novel views and simulate LiDAR data from limited input.", "section": "1 Introduction"}, {"figure_path": "tLXgzQ5WZl/figures/figures_9_2.jpg", "caption": "Figure 4: Novel View Synthesis. We show the synthesized novel views of SCube+ compared to baselines approaches. The inset of each subfigure shows a top-down visualization (an extreme novel view) of the reconstructed scene geometry.", "description": "This figure compares novel view synthesis results from SCube+ with several baseline methods.  It shows renderings from three standard viewpoints (front-left, front, front-right) as well as an extreme viewpoint (top-down view) for a better understanding of the 3D scene reconstruction. This comparison highlights the quality and completeness of SCube+'s scene representation and its ability to synthesize novel views.", "section": "4.2 Large-scale Scene Reconstruction"}, {"figure_path": "tLXgzQ5WZl/figures/figures_16_1.jpg", "caption": "Figure 9: SCube+*. Results from the postprocessing network without per-scene optimization. White-balance inconsistencies from different views (marked in red box) can be fixed.", "description": "This figure shows the results of applying an optional GAN-based post-processing step to the novel view synthesis results.  The leftmost column shows results from the SCube model without post-processing. The rightmost column shows the results after applying the post-processing step. This post-processing step addresses some artifacts and inconsistencies in the rendered images that may result from the main reconstruction method. The red boxes highlight regions where the post-processing step successfully corrects for white-balance inconsistencies present in the original render.", "section": "3.3 Postprocessing and Applications"}, {"figure_path": "tLXgzQ5WZl/figures/figures_17_1.jpg", "caption": "Figure 10: Result on the data sample with the worst voxel Chamfer distance. We show geometry reconstruction and the image renderings.", "description": "This figure displays the results for the data sample with the highest voxel Chamfer distance, which measures the geometric difference between the predicted and ground truth voxels. It shows the ground truth voxels, predicted voxels, the reconstructed 3D Gaussian scene, and finally the rendered images from different viewpoints.  The visualization helps illustrate the accuracy of the method despite challenging conditions.", "section": "4.2 Large-scale Scene Reconstruction"}, {"figure_path": "tLXgzQ5WZl/figures/figures_17_2.jpg", "caption": "Figure 4: Novel View Synthesis. We show the synthesized novel views of SCube+ compared to baselines approaches. The inset of each subfigure shows a top-down visualization (an extreme novel view) of the reconstructed scene geometry.", "description": "This figure compares the novel view synthesis capabilities of SCube+ with several baseline methods.  It shows the rendered images from three different viewpoints (front-left, front, front-right) for both the input views and newly synthesized views at two future time steps.  The insets provide a top-down view of the reconstructed 3D scene, demonstrating the model's ability to generate views from unusual perspectives.", "section": "4.2 Large-scale Scene Reconstruction"}, {"figure_path": "tLXgzQ5WZl/figures/figures_18_1.jpg", "caption": "Figure 1: SCube. Given sparse input images with little or no overlap, our model reconstructs a high-resolution and large-scale scene in 3D represented with VoxSplats, ready to be used for novel view synthesis or LiDAR simulation.", "description": "This figure shows the overall pipeline of SCube. Given a set of sparse input images with minimal overlap, the SCube model reconstructs a high-resolution, large-scale 3D scene. The scene is represented using VoxSplats, a novel representation based on 3D Gaussians. The reconstructed scene can be used for various applications, including novel view synthesis (generating images from unseen viewpoints) and LiDAR simulation (creating simulated LiDAR point clouds).", "section": "1 Introduction"}, {"figure_path": "tLXgzQ5WZl/figures/figures_18_2.jpg", "caption": "Figure 1: SCube. Given sparse input images with little or no overlap, our model reconstructs a high-resolution and large-scale scene in 3D represented with VoxSplats, ready to be used for novel view synthesis or LiDAR simulation.", "description": "This figure showcases the SCube model's ability to reconstruct a high-resolution, large-scale 3D scene from only a few sparse input images, even with minimal overlap. The reconstructed scene is represented using VoxSplats, a novel representation that makes it efficient to generate novel views or simulate LiDAR data.", "section": "1 Introduction"}, {"figure_path": "tLXgzQ5WZl/figures/figures_19_1.jpg", "caption": "Figure 1: SCube. Given sparse input images with little or no overlap, our model reconstructs a high-resolution and large-scale scene in 3D represented with VoxSplats, ready to be used for novel view synthesis or LiDAR simulation.", "description": "This figure shows the overall pipeline of SCube.  Given a small number of input images (even with little to no overlap), the model reconstructs a large-scale, high-resolution 3D scene. This 3D scene is encoded using a novel representation called VoxSplats, a combination of Gaussian splats on a sparse voxel grid. The reconstructed scene can be used to generate novel views or simulate LiDAR data.", "section": "1 Introduction"}, {"figure_path": "tLXgzQ5WZl/figures/figures_19_2.jpg", "caption": "Figure 1: SCube. Given sparse input images with little or no overlap, our model reconstructs a high-resolution and large-scale scene in 3D represented with VoxSplats, ready to be used for novel view synthesis or LiDAR simulation.", "description": "This figure shows the overall pipeline of SCube. Given a set of sparse input images (with little or no overlap), SCube reconstructs a high-resolution and large-scale 3D scene. The reconstructed scene is represented using a novel representation called VoxSplats.  VoxSplats are ready to be used for various downstream tasks such as novel view synthesis (generating images from viewpoints not present in the input) and LiDAR simulation (simulating LiDAR scans of the scene from new locations). The figure visually demonstrates the input images, the reconstruction process, and the resulting novel views.", "section": "1 Introduction"}, {"figure_path": "tLXgzQ5WZl/figures/figures_20_1.jpg", "caption": "Figure 1: SCube. Given sparse input images with little or no overlap, our model reconstructs a high-resolution and large-scale scene in 3D represented with VoxSplats, ready to be used for novel view synthesis or LiDAR simulation.", "description": "This figure demonstrates the overall pipeline of SCube. Given a set of sparse input images with minimal overlap, the model reconstructs a high-resolution, large-scale 3D scene. This 3D representation, called VoxSplat, is a combination of Gaussian splats on a sparse voxel hierarchy. The reconstructed scene is shown to be useful for novel view synthesis and LiDAR simulation.", "section": "1 Introduction"}, {"figure_path": "tLXgzQ5WZl/figures/figures_21_1.jpg", "caption": "Figure 15: More LiDAR Simulation results.", "description": "This figure shows additional results of LiDAR simulation using the proposed SCube method.  It demonstrates the generation of consistent LiDAR point cloud sequences from input front views. The sequences show the simulated LiDAR scan as the virtual camera moves forward.", "section": "4.4 Other Applications"}, {"figure_path": "tLXgzQ5WZl/figures/figures_21_2.jpg", "caption": "Figure 1: SCube. Given sparse input images with little or no overlap, our model reconstructs a high-resolution and large-scale scene in 3D represented with VoxSplats, ready to be used for novel view synthesis or LiDAR simulation.", "description": "This figure shows the overall pipeline of SCube. Given a sparse set of input images, SCube reconstructs a high-resolution, large-scale 3D scene using a novel representation called VoxSplats.  The reconstructed scene can then be used for various applications such as novel view synthesis and LiDAR simulation. The figure visually demonstrates the input images, the reconstruction process, and the resulting novel views.", "section": "1 Introduction"}]