{"references": [{"fullname_first_author": "Scott M. Lundberg", "paper_title": "A unified approach to interpreting model predictions", "publication_date": "2017-12-01", "reason": "This paper introduces SHAP (SHapley Additive exPlanations), a game theoretic method for explaining predictions that is widely used and highly influential in the field of explainable AI."}, {"fullname_first_author": "Amirata Ghorbani", "paper_title": "Data Shapley: Equitable valuation of data for machine learning", "publication_date": "2019-06-09", "reason": "This paper introduces Data Shapley, a game-theoretic approach to data valuation which is highly relevant to the paper's focus on accelerating data valuation methods."}, {"fullname_first_author": "Marco Tulio Ribeiro", "paper_title": "\"Why should I trust you?\" Explaining the predictions of any classifier", "publication_date": "2016-08-13", "reason": "This paper introduces LIME (Local Interpretable Model-agnostic Explanations), a widely used method for explaining individual predictions that is closely related to the paper's work on feature attribution."}, {"fullname_first_author": "Brandon Amos", "paper_title": "Tutorial on amortized optimization for learning to optimize over continuous domains", "publication_date": "2022-02-01", "reason": "This tutorial provides a comprehensive overview of amortized optimization, which is the core methodological approach of the paper."}, {"fullname_first_author": "Ian Covert", "paper_title": "Learning to estimate Shapley values with vision transformers", "publication_date": "2022-06-01", "reason": "This paper is another work by the authors of the current paper that leverages amortization to improve feature attribution methods, providing relevant methodological context and supporting evidence."}]}