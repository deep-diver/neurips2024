{"references": [{"fullname_first_author": "Albert Gu", "paper_title": "Mamba: Linear-time sequence modeling with selective state spaces", "publication_date": "2023-12-00", "reason": "This paper introduces the Mamba model, a foundational architecture for the current work, achieving superior performance in sequence modeling with linear complexity."}, {"fullname_first_author": "Alexey Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2020-00-00", "reason": "This is a highly influential paper that introduces Vision Transformers, providing a strong baseline and architectural context for the current vision-based approach."}, {"fullname_first_author": "Ze Liu", "paper_title": "Swin Transformer: Hierarchical vision transformer using shifted windows", "publication_date": "2021-00-00", "reason": "This work introduces the Swin Transformer, a key architectural component integrated into the proposed QuadMamba model, improving efficiency and performance in visual tasks."}, {"fullname_first_author": "Kaiming He", "paper_title": "Deep residual learning for image recognition", "publication_date": "2016-00-00", "reason": "This highly influential paper introduces ResNet, a fundamental architecture in computer vision, serving as a comparison and baseline for the presented work."}, {"fullname_first_author": "Tao Huang", "paper_title": "LocalMamba: Visual state space model with windowed selective scan", "publication_date": "2024-00-00", "reason": "This paper proposes a related approach, LocalMamba, addressing some of the same challenges in adapting SSMs to vision tasks, thus providing a key comparative method."}]}