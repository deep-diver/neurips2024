[{"type": "text", "text": "DI-MaskDINO: A Joint Object Detection and Instance Segmentation Model ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Zhixiong Nan1, Xianghong $\\mathrm{Li}^{1}$ , Tao Xiang \u22171, and Jifeng Dai2 ", "page_idx": 0}, {"type": "text", "text": "1College of Computer Science, Chongqing University, Chongqing, China. 2Department of Electronic Engineering, Tsinghua University, Beijing, China. nanzx@cqu.edu.cn, lixianghong@stu.cqu.edu.cn, txiang@cqu.edu.cn, daijifeng@tsinghua.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "This paper is motivated by an interesting phenomenon: the performance of object detection lags behind that of instance segmentation (i.e., performance imbalance) when investigating the intermediate results from the beginning transformer decoder layer of MaskDINO (i.e., the SOTA model for joint detection and segmentation). This phenomenon inspires us to think about a question: will the performance imbalance at the beginning layer of transformer decoder constrain the upper bound of the final performance? With this question in mind, we further conduct qualitative and quantitative pre-experiments, which validate the negative impact of detectionsegmentation imbalance issue on the model performance. To address this issue, this paper proposes DI-MaskDINO model, the core idea of which is to improve the final performance by alleviating the detection-segmentation imbalance. DIMaskDINO is implemented by configuring our proposed De-Imbalance $(D I)$ module and Balance-Aware Tokens Optimization (BATO) module to MaskDINO. ${\\cal D}I$ is responsible for generating balance-aware query, and BATO uses the balanceaware query to guide the optimization of the initial feature tokens. The balanceaware query and optimized feature tokens are respectively taken as the Query and Key&Value of transformer decoder to perform joint object detection and instance segmentation. DI-MaskDINO outperforms existing joint object detection and instance segmentation models on COCO and BDD100K benchmarks, achieving $+{\\bf1.2}\\,A P^{b o x}$ and $\\mathbf{+0.9}$ $\\bullet A P^{m a s k}$ improvements compared to SOTA joint detection and segmentation model MaskDINO. In addition, DI-MaskDINO also obtains $+{\\bf1.0\\nabla}A\\bar{P}^{b o x}$ improvement compared to SOTA object detection model DINO and $\\pm{3.0\\ A P^{m a s k}}$ improvement compared to SOTA segmentation model Mask2Former. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Object detection and instance segmentation are two fundamental tasks in the computer vision community. Intuitively, the two tasks are closely-related and mutually-beneficial. However, in the current time, specialized detection or segmentation gains more focuses, and the amount of works studying the specialized task is significantly larger than that for joint tasks. One typical explanatory is that the multi-task training even hurts the performance of the individual task. ", "page_idx": 0}, {"type": "text", "text": "Confronting current research situations, we think about whether there are some essential cruxes that are ignored in previous works and these cruxes hinder the cooperation of object detection and instance segmentation tasks, which further constrains the breakthrough of the performance upper bound. This paper reveals one of cruxes is the imbalance of object detection and instance segmentation. As shown in Fig. 1, when investigating the results at the first layer of transformer decoder of MaskDINO model [25], an interesting phenomenon is found that there exists the performance imbalance between object detection and instance segmentation, as qualitatively illustrated in Fig. 1a and quantitatively illustrated in the first bar of Fig. 1c. After considering the imbalance issue, the performance gap at the first layer is narrowed as illustrated in the first bar of Fig. 1d, and the final performance upper bounds are improved (i.e., 28.1 to 29.5 for detection and 25.3 to 25.7 for segmentation) as illustrated in the second bar of Fig. 1d. The qualitative results are also optimized, the detection bounding boxes closely surround segmentation masks, as illustrated in Fig. 1b. ", "page_idx": 0}, {"type": "image", "img_path": "srQxkSPJLW/tmp/721ba74c8e38ff8534e017b57de0188d3ba2bf60e897e4676eeaf9dc1d35bf04.jpg", "img_caption": ["Figure 1: Qualitatively, (a) shows that the detection bounding boxes predicted by the query/feature at the first decoder layer of MaskDINO do not fit well with segmentation masks, and (b) exhibits that the corresponding results of DI-MaskDINO are optimized and the detection bounding boxes closely surround segmentation masks. Quantitatively, (c) displays that there exists a significant performance gap between detection and segmentation at the first decoder layer of MaskDINO, and (d) demonstrates DI-MaskDINO not only alleviates the performance imbalance at the first layer but also improves the performance upper bound. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "According to the above analysis, we can find the detection-segmentation imbalance at the beginning layer is one of essential cruxes that hinders the cooperation of object detection and instance segmentation. Therefore, we reviewed the previous works to investigate whether there are works that have been aware of this issue. The idea of many classical and excellent methods [16, 2, 5, 11] is combining two tasks by adding a segmentation branch to an object detector. These detect-then-segment methods make the performance of segmentation task to be limited by the performance of the object detector. Thanks to the thriving of transformer [44] and DETR [3], recent research attention has been geared towards transformer-based methods, which make giant contributions to the community. For example, [10, 50, 25] use the unified query representation for object detection and instance segmentation tasks based on transformer architecture. ", "page_idx": 1}, {"type": "text", "text": "However, to our best knowledge, there is no existing work to solve the above mentioned detectionsegmentation imbalance issue. Factually, the imbalance issue naturally exists, which is determined by the individual characteristics of detection and segmentation tasks and also derived from the supervision manners for the two tasks. Firstly, segmentation is a pixel-level grouping and classification task [16, 46], thus local detailed information is important for this task. In contrast, detection is a region-level task to locate and regress the object bounding box [13, 38], which requires global information focusing on the complete object. The query at the beginning decoder layer conveys relatively local features, which is more beneficial for the segmentation task, thus the detection task tends to achieve lower performance at the beginning layer. Secondly, supervision manners for detection and segmentation are distinctly different. The segmentation is densely supervised by all pixels of the GT mask, while detection is sparsely supervised by a 4D vector (i.e., x, y, w, and h) of GT bounding box. The dense supervision provides richer and stronger information than the sparse supervision during the optimization procedure. Therefore, the optimization speeds of the two tasks are not synchronous, which will lead to the imbalance issue. ", "page_idx": 1}, {"type": "text", "text": "Based on two above-analyzed reasons for the detection-segmentation imbalance issue, it is straightforward that the performance of detection task will lag behind at the beginning layer. Considering existing methods share a unified query for detection and segmentation tasks, the performance of a task will be negatively affected by another poorly-performed task, leading to that the multi-task joint training even hurts the performance of the individual task. Therefore, addressing the detection-segmentation imbalance issue is significant for designing a joint object detection and instance segmentation model. To address the detection-segmentation imbalance issue, we propose DI-MaskDINO model, which is implemented by configuring our proposed $\\pmb{D}e$ -Imbalance $(D I)$ module and Balance-Aware Tokens Optimization (BATO) module to MaskDINO. $D I$ module is responsible for generating balance-aware query. Specifically, $D I$ module strengthens the detection at the beginning decoder layer to balance the performance of the two tasks, and the core of ${\\cal D}I$ module is our proposed residual double-selection mechanism. In this mechanism, the token interaction based double-selection structure learns the global geometric, contextual, and semantic patch-to-patch relations to update initial feature tokens, and high-confidence tokens are selected to benefit the detection task since the selected tokens have learned global semantics during the token interaction procedure. In addition, this mechanism makes use of initial feature tokens by the residual structure, which is the necessary compensation for the information loss occurring during double-selection. Apart from ${\\cal D}I$ module, we also design BATO module, which uses the balance-aware query to guide the optimization of initial feature tokens. The balance-aware query and optimized feature tokens are respectively taken as the Query and Key&Value of transformer decoder to perform joint object detection and instance segmentation. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "The contributions of this paper are as follows: i) to our best knowledge, this paper for the first time focuses on the detection-segmentation imbalance issue and proposes ${\\cal D}I$ module with the residual double-selection mechanism to alleviate the imbalance; ii) DI-MaskDINO outperforms existing SOTA joint object detection and instance segmentation model MaskDINO $(+{\\bf1}.\\dot{2}\\:A P^{b o x}$ and $\\mathbf{+0.9}$ $A P^{m a\\bar{s}k}$ on COCO, using ResNet50 backbone with 12 training epochs), SOTA object detection model DINO $\\mathbf{\\Psi}(\\mathbf{+1.0\\A}P^{b o x}$ on COCO), and SOTA segmentation model Mask2Former $+3.\\mathbf{0}\\ A P^{m a s k}$ on COCO). ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Object Detection. Classical object detection methods [13, 38, 27, 37, 5, 43, 42] have achieved significant success. In recent years, transformer-based methods such as DETR [3] make a giant contribution to object detection by introducing the concept of object queries and the one-to-one matching mechanism. The success of DETR has sparked a boom in query-based end-to-end detectors, and numerous excellent variants are proposed [56, 33, 52, 28, 23, 6, 47, 51, 24, 32, 21, 54, 18]. Specifically, to enhance the convergence speed of DETR, Deformable DETR [56] proposes deformable attention mechanism that learns sparse feature sampling and aggregates multi-scale features accelerating model convergence and improving performance. From the interpretability of object queries, DAB-DETR [28] formulates the queries as 4D anchor boxes and dynamically updates them in each decoder layer. ", "page_idx": 2}, {"type": "text", "text": "Instance Segmentation. CNN-based instance segmentation methods are categorized into top-down and bottom-up paradigms. The top-down paradigm [16, 2, 11, 19, 7, 4, 1] firstly generates bounding boxes by object detectors, and then segments the masks. The bottom-up paradigm [35, 9, 29, 12] treats instance segmentation as a labeling-clustering problem, where pixels are firstly labeled as a class or embedded into a feature space and then clustered into each object. Recently, many transformer-based instance segmentation methods [8, 14, 20, 53, 17, 10, 50, 25] are proposed. The transformer-based methods treat the instance segmentation task as a mask classification problem that associates the instance categories with a set of predicted binary masks. ", "page_idx": 2}, {"type": "text", "text": "Joint Object Detection and Instance Segmentation. The goal of joint object detection and instance segmentation is to carry out the two tasks simultaneously [45, 34, 41, 36]. The traditional joint object detection and instance segmentation methods [16, 2, 5, 11] are usually implemented by adding a mask branch to a strong object detector. For example, the classical model Mask RCNN [16] achieves joint object detection and instance segmentation by adding a mask branch to Faster RCNN [39]. Recently, the proposal of the transformer-based framework has promoted the development of joint object detection and instance segmentation. Following DETR [3], SOLQ [10] proposes a unified query representation for simultaneous detection and segmentation tasks. The recent MaskDINO [25] achieves optimal performance with the unified query representation on both detection and segmentation tasks. ", "page_idx": 2}, {"type": "image", "img_path": "srQxkSPJLW/tmp/d67d5020a05e0f5f8ec5bf37662f298f0b2c2c744023d0ae7bc90e47366a27af.jpg", "img_caption": ["Figure 2: The overview of DI-MaskDINO model based on MaskDINO (grey shaded), with the extensions (green shaded) of De-Imbalance and Balance-Aware Tokens Optimization. For simplicity, content token and position token are merged in $\\pmb{D}e$ -Imbalance (i.e., $\\mathbf{\\boldsymbol{T}}_{i}$ , $\\mathbf{\\mathit{T}}_{s1}$ , $\\mathbf{\\nabla}T_{s2}$ , and $Q_{b a l}$ contain both content and position token) in presentation. GTG is short for guiding token generation. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "3 Proposed Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In response to the naturally-existing but commonly-ignored imbalance issue between object detection and instance segmentation, we propose DI-MaskDINO model, which is based on MaskDINO [25]. To better understand our proposed DI-MaskDINO, we firstly review MaskDINO $(\\S\\,3.1)$ , and then introduce DI-MaskDINO (\u00a7 3.2). ", "page_idx": 3}, {"type": "text", "text": "3.1 Preliminaries: MaskDINO ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "MaskDINO is a unified object detection and segmentation framework, which adds a mask prediction branch on the structure of DINO [52]. MaskDINO (grey shaded part in Fig. 2) is composed of a backbone, a transformer encoder, a transformer decoder, and detection and segmentation heads (i.e., \u201cPrediction\" in Fig. 2). Position embeddings and the flattened multi-scale features (extracted by backbone) are inputted to the transformer encoder to generate the initial feature tokens $(T_{i})$ . Note that in MaskDINO, the top-ranked feature tokens selected from $\\mathbf{\\boldsymbol{T}}_{i}$ directly serve as the Query of transformer decoder, while we design $\\pmb{D}e$ -Imbalance module with a residual double-selection mechanism to firstly alleviate the detection-segmentation imbalance and then obtain the balance-aware query $Q_{b a l}$ to serve as the Query of transformer decoder. In addition, we design Balance-Aware Tokens Optimization module to optimize $\\mathbf{\\boldsymbol{T}}_{i}$ and generate the balance-aware feature tokens ${\\mathbfit{T}}_{b a l}$ to serve as the Key&Value of transformer decoder. Token and query are specialized terms, and their explanations are provided in Appendix A. ", "page_idx": 3}, {"type": "text", "text": "3.2 Our Method: DI-MaskDINO ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Fig. 2 illustrates the overview of DI-MaskDINO, which consists of four modules, including Feature Tokens Extractor $(F T E)$ , $\\pmb{D}e$ -Imbalance $(D I)$ , Balance-Aware Tokens Optimization $(B A T O)$ , and Transformer Decoder $(T D)$ . FTE extracts the initial feature tokens $\\mathbf{\\boldsymbol{T}}_{i}$ from the input image using backbone and MaskDINO encoder. The goal of $\\pmb{D}\\pmb{I}$ is to generate the balance-aware query $Q_{b a l}$ , which is implemented by applying our proposed residual double-selection mechanism on the initial feature tokens $\\mathbf{\\boldsymbol{T}}_{i}$ . $\\mathbf{\\nabla}B A T O$ utilizes $Q_{b a l}$ to optimize $\\mathbf{\\nabla}T_{i}$ , generating the balance-aware feature tokens $\\mathbf{\\deltaT}_{b a l}$ . $\\mathbf{\\nabla}T D$ takes ${\\mathbfit{T}}_{b a l}$ as the Key&Value and $Q_{b a l}$ as the Query to perform joint object detection and instance segmentation. ", "page_idx": 3}, {"type": "text", "text": "3.2.1 Feature Tokens Extractor ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Given an image $\\pmb{I}\\in\\mathbb{R}^{H\\times W\\times3}$ , the backbone (e.g., ResNet [15]) firstly extracts multi-scale features, which are then flattened and concatenated to serve as the input of transformer encoder comprising six multi-scale deformable attention layers [56], obtaining the initial feature tokens $\\mathbf{\\boldsymbol{T}}_{i}$ that is composed of $\\textstyle\\sum_{i=3}^{6}\\left({\\frac{H}{2^{i}}}\\,\\times\\,{\\frac{W}{2^{i}}}\\right)$ tokens, where each token expresses the feature of the corresponding patch in $\\boldsymbol{\\mathit{I}}$ . ", "page_idx": 4}, {"type": "text", "text": "3.2.2 De-Imbalance ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "There exists the detection-segmentation imbalance at the beginning layer of transformer decoder, which might constrain the upper bound of model performance. To handle this issue, we design ${\\cal D}I$ module to alleviate the imbalance, instead of directly providing $\\mathbf{\\boldsymbol{T}}_{i}$ to the transformer decoder as MaskDINO does. Specifically, detection-segmentation imbalance means that the performance of object detection lags behind that of instance segmentation at the beginning layer of transformer decoder. Therefore, we propose the residual double-selection mechanism to strengthen the performance of object detection. ", "page_idx": 4}, {"type": "text", "text": "The double-selection consists of the first selection and the second selection. In the first selection, we select top- $k_{1}$ ranked feature tokens in $\\mathbf{\\boldsymbol{T}}_{i}$ , based on their category classification scores: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{T_{s1}=S(T_{i},k_{1}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbf{\\mathit{T}}_{s1}$ represents the firstly-selected feature tokens, $\\boldsymbol{S}$ denotes the selection operator. The first selection mainly fliters out most of the tokens conveying background information, making $\\mathbf{\\mathit{T}}_{s1}$ focus on the objects. ", "page_idx": 4}, {"type": "text", "text": "Before the second selection, a token interaction network comprising two self-attention layers is applied on $\\mathbf{\\mathit{T}}_{s1}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\nT_{s1}^{s a}=\\mathrm{MHSA}(T_{s1}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where MHSA is Multi-Head Self-Attention and $\\mathbfcal{T}_{s1}^{s a}$ indicates the feature tokens processed by MHSA. ", "page_idx": 4}, {"type": "text", "text": "The token interaction is the key point to make sure that the secondly-selected tokens are beneficial for detection, we explain its rationality as follows. As we know, each token actually corresponds to a patch (remarkably smaller than an object in most cases) in the image [55]. The bounding box of an object is regressed by integrating the multiple patches (belonging to the same object) that have global patch-to-patch spatial relations, thus it is really needed for the detection task to learn the interaction relation between patches. In contrast, the dense all-pixel supervision for the segmentation task mainly focuses on local pixel-level similarity with GT mask [25], hence the segmentation task is not particularly depend on the patch-to-patch relation as the detection task. By self-attention layers, different tokens representing the patches (belonging to the same object) can interact with each other to learn the global geometric, contextual, and semantic patch-to-patch relations, beneftiing the perception of object bounding boxes. Therefore, executing token interaction before the second selection makes $\\pmb{D}\\pmb{I}$ module to be more beneficial for detection. In addition, verified by some studies (e.g., [32]), the tokens with higher category scores correspond to higher IOU scores of object bounding boxes. Therefore, the second selection further strengthens the object detection and alleviates the detection-segmentation imbalance. ", "page_idx": 4}, {"type": "text", "text": "In the second selection, we select the top- $k_{2}$ ranked feature tokens in $T_{s1}^{s a}$ to obtain the secondlyselected feature tokens $\\mathbf{\\nabla}T_{s2}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{T_{s2}=S(T_{s1}^{s a},k_{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The residual double-selection is inspired by the residual idea in [15], and the residual is the necessary compensation for double-selection since the information loss occurs in the selection procedures. The formulation of this mechanism is combining $\\mathbf{\\boldsymbol{T}}_{i}$ with $\\mathbf{\\nabla}T_{s2}$ by the Multi-Head Cross-Attention network (MHCA, a self-attention layer and a FFN layer are omitted here), generating $Q_{b a l}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\nQ_{b a l}=\\mathbf{M}\\mathbf{H}\\mathbf{C}\\mathbf{A}(T_{s2},T_{i}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "$Q_{b a l}$ conveys the balance-aware information, thus it is named as balance-aware query. Subsequently, $Q_{b a l}$ is fed to $\\mathbf{\\nabla}B A T O$ to guide the optimization of initial feature tokens $\\mathbf{\\boldsymbol{T}}_{i}$ . It is noted that the tokens in $Q_{b a l}$ have become significantly different from the tokens in $\\mathbf{\\boldsymbol{T}}_{i}$ . Through Eq. 1-4, the tokens in $Q_{b a l}$ have obtained larger receptive field and considered the interaction with other tokens, thus they are better understood as object/instance candidates. Correspondingly, the denotation has been changed from $_T$ to $Q$ . ", "page_idx": 4}, {"type": "text", "text": "3.2.3 Balance-Aware Tokens Optimization ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In MaskDINO, initial feature tokens $\\mathbf{\\nabla}T_{i}$ directly serve as the Key&Value of $\\mathbf{\\nabla}T D$ . Instead, we design $\\mathbf{\\nabla}B A T O$ module that makes use of both balance-aware query $Q_{b a l}$ and $\\mathbf{\\boldsymbol{T}}_{i}$ to generate the Key&Value of $\\mathbf{\\nabla}T D$ . $\\mathbf{\\boldsymbol{T}}_{i}$ contains a large number $(\\approx20\\mathrm{k})$ of tokens conveying detailed local information for both background and objects/instances, while $Q_{b a l}$ consists of a small number $(=\\!300)$ of high-confidence tokens mainly focusing on objects/instances. In addition, benefiting from the token interaction (i.e., Eq. 2), $Q_{b a l}$ has learned rich semantic and contextual interaction relations. Therefore, $Q_{b a l}$ is used to guide the optimization of $\\mathbf{\\boldsymbol{T}}_{i}$ . The optimized feature tokens (denoted as $\\mathbf{\\delta}T_{b a l}$ ) is taken as the Key&Value of $\\mathbf{\\nabla}T D$ . ", "page_idx": 5}, {"type": "text", "text": "Firstly, to provide guidance for both detection and segmentation, the mask network and box network are separately applied on $Q_{b a l}$ to generate mask guiding token $\\pmb{T}_{g}^{m a s k}$ and box guiding token $T_{g}^{b o x}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\nT_{g}^{m a s k}=\\mathcal{N}_{m a s k}(Q_{b a l}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{T_{g}^{b o x}=\\mathcal{N}_{b o x}(Q_{b a l}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathcal{N}_{m a s k}$ and $\\mathcal{N}_{b o x}$ indicate the mask network and box network, respectively. Both $\\mathcal{N}_{m a s k}$ and $\\mathcal{N}_{b o x}$ consist of a $m l p$ network. ", "page_idx": 5}, {"type": "text", "text": "Then, the overall guiding token ${\\cal T}_{g}$ is obtained by fusing $\\pmb{T}_{g}^{m a s k}$ and T box: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{T_{g}=T_{g}^{m a s k}+T_{g}^{b o x}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Finally, ${\\cal T}_{g}$ guides the optimization of the initial feature tokens $\\mathbf{\\boldsymbol{T}}_{i}$ through a Multi-Head CrossAttention. The motivation is straightforward. Same with $Q_{b a l}$ , each token in $\\pmb{T_{g}}$ corresponds to an object/instance candidate. When $\\mathbf{\\boldsymbol{T}}_{i}$ interacts with $\\scriptstyle{T_{g}}$ , the tokens (in $\\mathbf{\\boldsymbol{T}}_{i}$ ) that belong to the same object/instance will be aggregated, enhancing the foreground information. For a better comprehension, a token in ${\\cal T}_{g}$ could be assumed as the center of a \u201ccluster\", and the tokens (in $\\mathbf{\\boldsymbol{T}}_{i}$ ) belonging to the same object/instance could be assumed as the points in the \u201ccluster\". The points move towards the \u201ccluster\" center, realizing the optimization of $\\mathbf{\\nabla}T_{i}$ . This procedure is formulated as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{T_{b a l}=\\mathbf{M}\\mathbf{H}\\mathbf{C}\\mathbf{A}(T_{i},T_{g}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "generating balance-aware feature tokens ${\\mathbfit{T}}_{b a l}$ (also called optimized feature tokens), which serve as the Key&Value of $\\mathbf{\\nabla}T D$ . ", "page_idx": 5}, {"type": "text", "text": "3.2.4 Transformer Decoder ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "$\\mathbf{\\nabla}T D$ is responsible for the predictions of instance mask, object box, and class. $\\mathbf{\\nabla}T D$ consists of decoder layers and each layer contains a self-attention, a cross-attention, and a FFN. The inputs of $\\mathbf{\\nabla}T D$ are $\\mathbf{\\deltaT}_{b a l}$ (in Eq. 8) and $Q_{b a l}$ (in Eq. 4). $Q_{b a l}$ interacts with $\\mathbf{\\deltaT}_{b a l}$ in the decoder layers, continuously refining the query: ", "page_idx": 5}, {"type": "equation", "text": "$$\nQ_{r e f}=\\mathcal{N}_{d e}(Q_{b a l},T_{b a l}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $Q_{r e f}$ is the refined query, and $\\mathcal{N}_{d e}$ denotes the transformer decoder network. ", "page_idx": 5}, {"type": "text", "text": "Subsequently, we follow the detection head and segmentation head structures of MaskDINO to perform object detection and instance segmentation. For object detection, $Q_{r e f}$ is used to predict the categories $^c$ and bounding boxes $^{b}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\{c,b\\}=\\mathcal{N}_{d e t}(Q_{r e f}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathcal{N}_{d e t}$ denotes the detection head network. For instance segmentation, $Q_{r e f},T_{i}$ , and the $1/4$ resolution CNN feature $F_{c n n}$ are used to predict the instance masks $\\mathbf{\\nabla}m$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\nm=\\mathcal{N}_{s e g}(Q_{r e f},T_{i},F_{c n n}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathcal{N}_{s e g}$ denotes the segmentation head network. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Settings ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We conduct extensive experiments on COCO [26] and BDD100K [49] datasets using ResNet50 [15] backbone pretrained on ImageNet-1k [40] as well as SwinL [30] backbone pretrained on ImageNet22k. NVIDIA RTX3090 GPUs are used when the backbone is ResNet50. Due to the large memory ", "page_idx": 5}, {"type": "text", "text": "requirement of SwinL, NVIDIA RTX A6000 GPUs are used when the backbone is SwinL. More implementation details are in Appendix B. ", "page_idx": 6}, {"type": "table", "img_path": "srQxkSPJLW/tmp/b09c4cf79c5d514a44a23c150485b7b1df76ed6080d295aa2eb510fd2028ecc8.jpg", "table_caption": ["Table 1: Comparison with other methods on the COCO validation set. "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "srQxkSPJLW/tmp/53446aea8675c77a7bbec6dd39841e4dec5aad30f48d5f199566d35a3025a923.jpg", "table_caption": ["Table 2: Comparison with other methods on the BDD100K validation set. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "4.2 Comparison Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "MaskDINO is the SOTA model for joint object detection and instance segmentation, thus we mainly compare our model with MaskDINO under different backbones (ResNet50 and SwinL). Additionally, our model is compared with some classical (i.e., Mask RCNN [16]) and recently-proposed (i.e., HTC [5] and SOLQ [10]) joint object detection and instance segmentation models. Furthermore, our model is compared with SOTA model that is specifically designed for object detection (i.e., DINO [52]) and instance segmentation (i.e., Mask2Former [8]). The comparison results on COCO dataset are summarized in Tab. 1. It is noted that the experiments with the Swin-L backbone are conducted on the A6000 GPUs with the batch size of 4 (the maximum bacth size that 4 A6000 GPUs supports). The batch size is smaller than that in MaskDINO paper (i.e., batch size $=16$ ) and the 4 A6000 GPUs present weaker computation power than 4 A100 GPUs, thus the results we reproduced are lower than those in the original MaskDINO paper. We can observe that 1) our model surpasses MaskDINO under different training conditions (epoch $=12$ , 24, and 50). Notably, our model presents more significant advantage with the training condition of epoch $=12$ , which potentially reveals that our model reaches the convergence with a faster speed; 2) under the Swin-L backbone, DI-MaskDINO exhibits significant superiority over MaskDINO, further confirming the effectiveness of our model; 3) the performance of our model on individual detection and segmentation tasks is simultaneously higher than that of SOTA models specifically designed for detection (i.e., DINO) and segmentation (i.e., Mask2Former) when they are fully trained $(\\mathrm{epoch}=36$ or 50), which is really hard-won since the single-task model usually designs the specific module for the specific task (e.g., DINO uses the tailored query formulation to improve the detection performance and Mask2Former proposes tailored masked attention module to improve the segmentation performance). ", "page_idx": 6}, {"type": "text", "text": "Existing joint detection and segmentation models like [5, 10, 50] only conduct the experiments on COCO dataset. In this paper, to further verify the robustness and generalization of our model, additional experiments are conducted on more complex traffic scene dataset BDD100K [49] using ResNet50 and Swin-L backbones, and the results we reproduce are shown in Tab. 2. Due to the complexity of traffic scenes, the overall performance is lower than the performance on COCO dataset, and the model asks for more training epochs $(\\mathrm{epoch}=68)$ to reach the convergence. It can be observed that our model still exhibits superiority over MaskDINO, DINO, and Mask2Former, which presents the robustness and generalization of our model. It should be noted that the performance of MaskDINO on detection task is lower than that of the specialized object detection model DINO, indicating that DINO still exhibits the advantage in complex traffic scene datasets. In contrast, our model improves DINO by $0.6\\ A P^{b o x}$ , further demonstrating the effectiveness of our model. ", "page_idx": 7}, {"type": "text", "text": "4.3 Diagnostic Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "4.3.1 Imbalance Tolerance Test ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "There exists the natural imbalance between object detection and instance segmentation, and we are interested in how will a model perform if the imbalance condition is worsened. Therefore, we conduct the imbalance tolerance test by designing two severe imbalance conditions: 1) loss weight constraint, which is implemented by constraining the weight of detection loss to 1/10 of the default value while the weight of segmentation loss remains unchanged; 2) position token constraint, position token conveys important cues of object locations, thus constraining position token will generate disturbing location information to confuse detection task. The position token constraint is implemented by randomly initializing position token of $Q_{b a l}$ (composed of position token and content token) in Eq. 4. DI module is mainly responsible for alleviating imbalance issue, thus the imbalance tolerance test on DI-MaskDINO only enables ${\\cal D}I$ module. The experiments are conducted on more complex BDD100K dataset, because the results on the more complex dataset can better reflect the performance of imbalance tolerance. Considering the imbalance issue is severe at the beginning decoder layer, thus the experiments utilize models configured with 3 decoder layers. ", "page_idx": 7}, {"type": "table", "img_path": "srQxkSPJLW/tmp/3e7dbfae1556640a80678cc23cf9d6a91f32cb071d7927abd7ab098faa0c57f3.jpg", "table_caption": ["Table 3: Imbalance tolerance test comparison of MaskDINO and DI-MaskDINO. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "The results of imbalance tolerance test are summarized in Tab. 3, and the percentage of performance drops (compared with standard condition) is highlighted in colors. We can observe that 1) the imbalance between detection and segmentation has remarkable affect on the upper bound of model performance, potentially indicating the significance of our work; 2) the effects of imbalance conditions on detection task are larger than that on segmentation task, because the two imbalance conditions are implemented to mainly constrain the detection task to simulate the natural detection-segmentation imbalance; 3) even the performance of SOTA model MaskDINO is largely affected by the imbalance conditions (e.g., $-21.8\\bar{\\varphi}_{\\!0\\ d}\\,_{\\!\\!A P}{}^{\\!\\!b o x}$ degradation on the condition of position token constraint), which potentially reflects that $\\pmb{D}e$ -Imbalance deserves the research focus; 4) compared with MaskDINO, the performance degradation of our model is slighter (i.e., $-10.2\\%$ v.s. ${\\bf-2.9\\,\\%}$ and $-21.8\\%$ v.s. $-14.7\\%$ on the $A P^{b o x}$ metric, $-3.0\\,\\%$ v.s. $+1.2\\%$ and $-7.6\\%$ v.s. ${\\bf-4.8\\%}$ on the $A P^{m a s k}$ metric), which demonstrates the effectiveness of our model; 5) from a comprehensive perspective, we think the standard condition is still a detection-segmentation imbalance condition (which is commonly treated as a regular condition in previous works), and we claim the imbalance is one of the cruxes that limit the upper bound of model performance, hence it should be further studied. ", "page_idx": 7}, {"type": "text", "text": "4.3.2 Diagnostic Experiments on Main Modules ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To test the effects of main modules in our model (i.e., $\\pmb{D}\\pmb{I}$ and $\\mathbf{\\nabla}B A T O$ ), we test the performance of our model under four configurations: #1 exclusion of both ${\\cal D}I$ and BATO; #2 exclusion of BATO; #3 exclusion of ${\\cal D}I$ ; #4 inclusion of both ${\\cal D}I$ and BATO. To make the results solid, the experiments are conducted on both BDD100K and COCO datasets, and the results are reported in Tab. 4. ", "page_idx": 7}, {"type": "text", "text": "Table 4: The results of diagnostic experiments on main modules. The experiments are conducted on the BDD100K dataset with 68 training epochs and on COCO dataset with 12 training epochs. The results in Tab. 5 and Tab. 6 are also obtained under the same experiment settings. ", "page_idx": 8}, {"type": "table", "img_path": "srQxkSPJLW/tmp/7ba3cf4af3b8908e73f2b4faa8874eb2567af5a5f8b7c10f125fcea0cf8877b5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "In comparison with #1, the model under the configuration of #2 or #3 yields higher performance on both datasets, and the optimum results are achieved when both $\\pmb{D}\\pmb{I}$ and $\\mathbf{\\delta}B A T O$ are enabled (#4). These results demonstrate the effectiveness of $\\pmb{D}\\pmb{I}$ and $\\mathbf{\\delta}B A T O$ . The results are explainable. ${\\cal D}I$ module alleviates the imbalance between detection and segmentation, generating balance-aware query $Q_{b a l}$ , which is then fed to $\\mathbf{\\nabla}B A T O$ to further make use of balance-aware information, contributing to performance improvement. ", "page_idx": 8}, {"type": "text", "text": "4.3.3 Diagnostic Experiments on DI Module ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The core of our model is ${\\cal D}I$ , which improves the model performance by mitigating the imbalance between detection and segmentation. $\\pmb{D}\\pmb{I}$ is realized by applying the residual double-selection mechanism on $\\mathbf{\\boldsymbol{T}}_{i}$ , generating $\\mathbf{\\mathit{T}}_{s1}$ (firstly-selected tokens), $\\mathbf{\\nabla}T_{s2}$ (secondly-selected tokens), and $Q_{b a l}$ (balance-aware query). To analyze $\\pmb{D}\\pmb{I}$ module, we design the fine-grained ablation experiments by respectively using $\\mathbf{\\nabla}T_{i}$ , $\\mathbf{\\mathit{T}}_{s1}$ , ${\\cal T}_{s2}$ , and $Q_{b a l}$ as the guidance for $\\mathbf{\\delta}B A T O$ (i.e., $g u i.=T_{i}$ , gui. $=T_{s1}$ , $g u i.=T_{s2}$ , and $g u i.=Q_{b a l},$ and examine the corresponding performance. ", "page_idx": 8}, {"type": "text", "text": "The experiment results on BDD100K and COCO datasets are reported in Tab. 5. gui. $=\\;T_{i}$ actually represents the situation when ${\\pmb D}{\\pmb I}$ module is disabled, which serves as the baseline for other situations. Firstly, we can observe $\\mathcal{P}(g u i.=T_{s2})>\\mathcal{P}(g u i.=T_{s1})>$ $\\mathcal{P}(g u i.=T_{i})$ where $\\mathcal{P}(*)$ denotes the performance of the model under the configuration $^*$ , demonstrating our double-selection mechanism is effective. The reason is intuitive, by applying double-selection mechanism, the tokens with high confidence are selected, and highconfidence tokens indicate the location of objects more clearly than other tokens, thus beneftiing the object detection task (i.e., mitigating the imbalance between detection and segmentation). Secondly, the highest performance is achieved when $g u i.=Q_{b a l}$ , validating the effectiveness of our residual double-selection mechanism. In ${\\cal D}I$ module, apart from the secondly-selected tokens $\\mathbf{\\nabla}T_{s2}$ , the initial feature tokens $\\mathbf{\\boldsymbol{T}}_{i}$ is also used to compute $Q_{b a l}$ , which could be coarsely formulated as $\\pmb{Q}_{b a l}=\\pmb{T}_{i}+\\pmb{S}(\\pmb{T}_{i})$ . This residual structure enables the model to make use of the information in both the initial feature tokens and the selected feature tokens, hence reaching the optimal performance. ", "page_idx": 8}, {"type": "table", "img_path": "srQxkSPJLW/tmp/35c58fdeae858f5df6f17b334d85af35d438552ddb23627532c8e9f36f9b81fc.jpg", "table_caption": ["Table 5: The results of diagnostic experiments on ${\\pmb D}{\\pmb I}$ module. gui. denotes the guidance in Fig. 2. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "4.3.4 Diagnostic Experiments on BATO Module ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "BATO targets to use the balance-aware query $Q_{b a l}$ to guide the optimization of the initial feature tokens $\\mathbf{\\boldsymbol{T}}_{i}$ The effectiveness of $\\mathbf{\\delta}B A T O$ has been validated in Tab. 4. We further conduct experiments to validate the effect of the proposed guiding token generation (GTG). The GTG is designed to provide guidance for both detection and segmentation, generating mask guiding token and box guiding token that are closely related to mask instances and object boxes through the mask network and box network, respectively. These guiding tokens can provide more global and semantic guiding information for the optimization of the initial feature tokens $\\mathbf{\\boldsymbol{T}}_{i}$ . As shown in Tab. 6, the model with GTG performs better, which demonstrates the effect of GTG. ", "page_idx": 8}, {"type": "table", "img_path": "srQxkSPJLW/tmp/a55508d231daedc422d4a6c1118ed38b915b2a699d4d71b41e9ab3a9b4a87e50.jpg", "table_caption": ["Table 6: The results of diagnostic experiments on $\\mathbf{\\nabla}B A T O$ module. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we investigate the naturally-existing but commonly-ignore detection-segmentation imbalance issue. The imbalance means that the performance of object detection lags behind that of instance segmentation at the beginning layer of transformer decoder, which is one of cruxes that hurt the cooperation of object detection and instance segmentation tasks and might constrain the breakthrough of the performance upper bound. To address the issue, we propose DI-MaskDINO model with the residual double-selection mechanism to alleviate the imbalance, achieving significant performance improvements compared with SOTA joint object detection and instance segmentation model MaskDINO, SOTA object detection model DINO, and SOTA segmentation model Mask2Former. ", "page_idx": 9}, {"type": "text", "text": "Limitations. This paper focuses on the task of joint object detection and instance segmentation, thus the model is not applicable for other segmentation tasks such as semantic segmentation and panoptic segmentation. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Daniel Bolya, Chong Zhou, Fanyi Xiao, and Yong Jae Lee. Yolact: Real-time instance segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9157\u20139166, 2019.   \n[2] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: High quality object detection and instance segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43(5):1483\u2013 1498, 2019. [3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In Proceedings of the European Conference on Computer Vision, pages 213\u2013229, 2020. [4] Hao Chen, Kunyang Sun, Zhi Tian, Chunhua Shen, Yongming Huang, and Youliang Yan. Blendmask: Top-down meets bottom-up for instance segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8573\u20138581, 2020.   \n[5] Kai Chen, Jiangmiao Pang, Jiaqi Wang, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jianping Shi, Wanli Ouyang, et al. Hybrid task cascade for instance segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4974\u20134983, 2019.   \n[6] Qiang Chen, Xiaokang Chen, Gang Zeng, and Jingdong Wang. Group detr: Fast training convergence with decoupled one-to-many label assignment. arXiv preprint arXiv:2207.13085, 2022.   \n[7] Xinlei Chen, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Tensormask: A foundation for dense object segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2061\u20132069, 2019.   \n[8] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-attention mask transformer for universal image segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1290\u20131299, 2022.   \n[9] Bert De Brabandere, Davy Neven, and Luc Van Gool. Semantic instance segmentation with a discriminative loss function. arXiv preprint arXiv:1708.02551, 2017.   \n[10] Bin Dong, Fangao Zeng, Tiancai Wang, Xiangyu Zhang, and Yichen Wei. Solq: Segmenting objects by learning queries. In Proceedings of the Neural Information Processing Systems, volume 34, pages 21898\u201321909, 2021.   \n[11] Yuxin Fang, Shusheng Yang, Xinggang Wang, Yu Li, Chen Fang, Ying Shan, Bin Feng, and Wenyu Liu. Instances as queries. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6910\u20136919, 2021.   \n[12] Naiyu Gao, Yanhu Shan, Yupei Wang, Xin Zhao, Yinan Yu, Ming Yang, and Kaiqi Huang. Ssap: Single-shot instance segmentation with affinity pyramid. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 642\u2013651, 2019.   \n[13] Ross Girshick. Fast r-cnn. In Proceedings of the IEEE International Conference on Computer Vision, pages 1440\u20131448, 2015.   \n[14] Junjie He, Pengyu Li, Yifeng Geng, and Xuansong Xie. Fastinst: A simple query-based model for real-time instance segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 23663\u201323672, 2023.   \n[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770\u2013778, 2016.   \n[16] Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE International Conference on Computer Vision, pages 2961\u20132969, 2017.   \n[17] Jie Hu, Liujuan Cao, Yao Lu, ShengChuan Zhang, Yan Wang, Ke Li, Feiyue Huang, Ling Shao, and Rongrong Ji. Istr: End-to-end instance segmentation with transformers. arXiv preprint arXiv:2105.00637, 2021.   \n[18] Zhengdong Hu, Yifan Sun, Jingdong Wang, and Yi Yang. Dac-detr: Divide the attention layers and conquer. Proceedings of the Neural Information Processing Systems, 36, 2024.   \n[19] Zhaojin Huang, Lichao Huang, Yongchao Gong, Chang Huang, and Xinggang Wang. Mask scoring r-cnn. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6409\u20136418, 2019.   \n[20] Jitesh Jain, Jiachen Li, Mang Tik Chiu, Ali Hassani, Nikita Orlov, and Humphrey Shi. Oneformer: One transformer to rule universal image segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2989\u20132998, 2023.   \n[21] Ding Jia, Yuhui Yuan, Haodi He, Xiaopei Wu, Haojun Yu, Weihong Lin, Lei Sun, Chao Zhang, and Han Hu. Detrs with hybrid matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19702\u201319712, 2023.   \n[22] Lei Ke, Martin Danelljan, Xia Li, Yu-Wing Tai, Chi-Keung Tang, and Fisher Yu. Mask transfiner for high-quality instance segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4412\u20134421, 2022.   \n[23] Feng Li, Hao Zhang, Shilong Liu, Jian Guo, Lionel M Ni, and Lei Zhang. Dn-detr: Accelerate detr training by introducing query denoising. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13619\u201313627, 2022.   \n[24] Feng Li, Ailing Zeng, Shilong Liu, Hao Zhang, Hongyang Li, Lei Zhang, and Lionel M Ni. Lite detr: An interleaved multi-scale encoder for efficient detr. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18558\u201318567, 2023.   \n[25] Feng Li, Hao Zhang, Huaizhe Xu, Shilong Liu, Lei Zhang, Lionel M Ni, and Heung-Yeung Shum. Mask dino: Towards a unified transformer-based framework for object detection and segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3041\u20133050, 2023.   \n[26] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Proceedings of the European Conference on Computer Vision, pages 740\u2013755, 2014.   \n[27] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection. In Proceedings of the IEEE International Conference on Computer Vision, pages 2980\u20132988, 2017.   \n[28] Shilong Liu, Feng Li, Hao Zhang, Xiao Yang, Xianbiao Qi, Hang Su, Jun Zhu, and Lei Zhang. Dab-detr: Dynamic anchor boxes are better queries for detr. In Proceedings of the International Conference on Learning Representations, 2022.   \n[29] Shu Liu, Jiaya Jia, Sanja Fidler, and Raquel Urtasun. Sgn: Sequential grouping networks for instance segmentation. In Proceedings of the IEEE International Conference on Computer Vision, pages 3496\u20133504, 2017.   \n[30] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10012\u201310022, 2021.   \n[31] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In Proceedings of the International Conference on Learning Representations, 2019.   \n[32] Wenyu Lv, Shangliang Xu, Yian Zhao, Guanzhong Wang, Jinman Wei, Cheng Cui, Yuning Du, Qingqing Dang, and Yi Liu. Detrs beat yolos on real-time object detection. arXiv preprint arXiv:2304.08069, 2023.   \n[33] Depu Meng, Xiaokang Chen, Zejia Fan, Gang Zeng, Houqiang Li, Yuhui Yuan, Lei Sun, and Jingdong Wang. Conditional detr for fast training convergence. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3651\u20133660, 2021.   \n[34] Zhixiong Nan, Jizhi Peng, Jingjing Jiang, Hui Chen, Ben Yang, Jingmin Xin, and Nanning Zheng. A joint object detection and semantic segmentation model with cross-attention and inner-attention mechanisms. Neurocomputing, 463:212\u2013225, 2021.   \n[35] Alejandro Newell, Zhiao Huang, and Jia Deng. Associative embedding: End-to-end learning for joint detection and grouping. In Proceedings of the Neural Information Processing Systems, volume 30, 2017.   \n[36] Jizhi Peng, Zhixiong Nan, Linhai Xu, Jingmin Xin, and Nanning Zheng. A deep model for joint object detection and semantic segmentation in traffic scenes. In Proceedings of the International Joint Conference on Neural Networks, pages 1\u20138, 2020.   \n[37] Joseph Redmon and Ali Farhadi. Yolov3: An incremental improvement. arXiv preprint arXiv:1804.02767, 2018.   \n[38] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 779\u2013788, 2016.   \n[39] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In Proceedings of the Neural Information Processing Systems, volume 28, 2015.   \n[40] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115:211\u2013252, 2015.   \n[41] Ganesh Sistu, Isabelle Leang, and Senthil Yogamani. Real-time joint object detection and semantic segmentation network for automated driving. arXiv preprint arXiv:1901.03912, 2019.   \n[42] Peize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chenfeng Xu, Wei Zhan, Masayoshi Tomizuka, Lei Li, Zehuan Yuan, Changhu Wang, et al. Sparse r-cnn: End-to-end object detection with learnable proposals. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14454\u201314463, 2021.   \n[43] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. Fcos: Fully convolutional one-stage object detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9627\u20139636, 2019.   \n[44] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proceedings of the Neural Information Processing Systems, volume 30, 2017.   \n[45] Shaoru Wang, Yongchao Gong, Junliang Xing, Lichao Huang, Chang Huang, and Weiming Hu. Rdsnet: A new deep architecture forreciprocal object detection and instance segmentation. In Proceedings of the Association for the Advancement of Artificial Intelligence, volume 34, pages 12208\u201312215, 2020.   \n[46] Xinlong Wang, Tao Kong, Chunhua Shen, Yuning Jiang, and Lei Li. Solo: Segmenting objects by locations. In Proceedings of the IEEE/CVF European Conference on Computer Vision, pages 649\u2013665, 2020.   \n[47] Yingming Wang, Xiangyu Zhang, Tong Yang, and Jian Sun. Anchor detr: Query design for transformer-based detector. In Proceedings of the Association for the Advancement of Artificial Intelligence, volume 36, pages 2567\u20132575, 2022.   \n[48] Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick. Detectron2. https://github.com/facebookresearch/detectron2, 2019.   \n[49] Fisher Yu, Haofeng Chen, Xin Wang, Wenqi Xian, Yingying Chen, Fangchen Liu, Vashisht Madhavan, and Trevor Darrell. Bdd100k: A diverse driving dataset for heterogeneous multitask learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2636\u20132645, 2020.   \n[50] Xiaodong Yu, Dahu Shi, Xing Wei, Ye Ren, Tingqun Ye, and Wenming Tan. Soit: Segmenting objects with instance-aware transformers. In Proceedings of the Association for the Advancement of Artificial Intelligence, volume 36, pages 3188\u20133196, 2022.   \n[51] Gongjie Zhang, Zhipeng Luo, Yingchen Yu, Kaiwen Cui, and Shijian Lu. Accelerating detr convergence via semantic-aligned matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 949\u2013958, 2022.   \n[52] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel M Ni, and HeungYeung Shum. Dino: Detr with improved denoising anchor boxes for end-to-end object detection. In Proceedings of the International Conference on Learning Representations, 2023.   \n[53] Hao Zhang, Feng Li, Huaizhe Xu, Shijia Huang, Shilong Liu, Lionel M Ni, and Lei Zhang. Mp-former: Mask-piloted transformer for image segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18074\u201318083, 2023.   \n[54] Dehua Zheng, Wenhui Dong, Hailin Hu, Xinghao Chen, and Yunhe Wang. Less is more: Focus attention for efficient detr. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6674\u20136683, 2023.   \n[55] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6881\u20136890, 2021.   \n[56] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable transformers for end-to-end object detection. arXiv preprint arXiv:2010.04159, 2020. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Due to the space limitation of the main text, we provide more results and discussions in the appendix, which are organized as follows: ", "page_idx": 13}, {"type": "text", "text": "\u2022 Section A: Prior Knowledge.   \n\u2022 Section B: Experiment Settings. \u2013 Section B.1: Datasets and Metrics. \u2013 Section B.2: Implementation Details.   \n\u2022 Section C: Additional Diagnostic Experiments. \u2013 Section C.1: Diagnostic Experiments on Token Selection. \u2013 Section C.2: Diagnostic Experiments on the Number of Decoder Layer.   \n\u2022 Section D: Visualization Analysis. ", "page_idx": 13}, {"type": "text", "text": "A Prior Knowledge ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Encoder and decoder of DETR-like models. MaskDINO is a type of DETR-like models [24, 54, 23, 47, 56, 51, 18]. A DETR-like model usually contains a backbone, an encoder, a decoder, and multiple prediction heads. The encoder is composed of multiple transformer encoder layers, and each encoder layer contains a multi-head self-attention and a FFN. The decoder consists of multiple transformer decoder layers, and each decoder layer has an extra multi-head cross-attention compared to the encoder layer. ", "page_idx": 13}, {"type": "text", "text": "Token and query. Both token and query are used to represent features. The concept of query comes from the original DETR [3]. Commonly, it denotes the feature of the object/instance in the decoder of DETR-like models. The token is a concept in the field of NLP (Natural Language Processing). In the computer vision domain, a token corresponds to a patch in an image. Feature tokens in this paper represent the features of image patches. ", "page_idx": 13}, {"type": "text", "text": "How to obtain the intermediate results from the beginning transformer decoder layer. The transformer decoder in MaskDINO is composed of multiple decoder layers, and MaskDINO attaches prediction heads after each decoder layer. Therefore, we can obtain prediction results from any decoder layer, which are called intermediate results. The intermediate results from the beginning transformer decoder layer are obtained by applying prediction heads on the 0-th decoder layer. ", "page_idx": 13}, {"type": "text", "text": "B Experiment Settings ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "B.1 Datasets and Metrics ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "COCO [26] is the most widely used dataset for the object detection and instance segmentation tasks, and many well-known models such as [16, 5, 3, 52, 8, 25] are evaluated on the COCO dataset. Following the common practice, we use the COCO train2017 split (118k images) for training and the val2017 split (5k images) for validation. In addition, considering autonomous driving is a typical and practical application of object detection and instance segmentation, the experiments are also conducted on BDD100K [49] dataset, which is composed of 10k high-quality instance masks and bounding boxes annotations for 8 classes. The training set and validation set are divided following the standard in [49]. Consistent with previous researches [5, 10, 11, 50, 45, 25], we report the metrics of $A P^{b o x}$ and $\\bar{A}P^{\\bar{m}a s k}$ for performance evaluation. ", "page_idx": 13}, {"type": "text", "text": "B.2 Implementation Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We implement DI-MaskDINO based on Detectron2 [48], using AdamW [31] optimizer with a step learning rate schedule. The initial learning rate is set as 0.0001. Following MaskDINO, DIMaskDINO is trained for 50 epochs on COCO with the batch size of 16, decaying the initial learning rate at fractions 0.9 and 0.95 of the total training iterations by a factor of 0.1. For BDD100K, following the setting in [22], we train our model for 68 epochs with the batch size of 8 and the learning rate drops at the 50-th epoch. The number of transformer encoder and decoder layers is 6. ", "page_idx": 13}, {"type": "text", "text": "The token numbers of $\\mathbf{\\mathit{T}}_{s1}$ and $\\mathbf{\\nabla}T_{s2}$ are 600 and 300, respectively. Unless otherwise specified, the feature channels in both encoder and decoder are set to 256, and the hidden dimension of FFN is set to 2048. The mask network and box network in BATO are both three-layer mlp networks. We use the same loss function as MaskDINO (i.e., L1 loss and GIOU loss for box loss, focal loss for classification loss, and cross-entropy loss and dice loss for mask loss). Under ResNet50 pretrained on ImageNet [40], our model is trained on NVIDIA RTX3090 GPUs. For Swin-L backbone, NVIDIA RTX A6000 GPUs are used for training and validating. ", "page_idx": 14}, {"type": "text", "text": "C Additional Diagnostic Experiments ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "C.1 Diagnostic Experiments on Token Selection ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Token selection is a crucial step in our proposed residual double-selection mechanism. The number of token selection and the amount of selected tokens may affect the performance. We conduct experiments to verify their impacts. The experimental results are summarized in Tab. 7 and Tab. 8. ", "page_idx": 14}, {"type": "text", "text": "The number of token selection. Single-selection actually represents the situation when disabling $\\pmb{D}\\pmb{I}$ module. Double-selection corresponds to our proposed method. Additionally, we add a token selection on $\\mathbf{\\nabla}T_{s2}$ for triple-selection. For fair comparison, we set the amount of lastly-selected tokens to the same value (i.e., 300) for single-, double-, and triple-selection. From Tab. 7, we draw two observations: 1) the results of single-selection are significantly lower than those in other situations, indicating the crucial role of $D I$ module; 2) our method achieves the optimal performance with doubleselection. The results are explainable. There exists information loss in each selection procedure. Therefore, triple-selection introduces more information loss, leading to a lower performance than double-selection. ", "page_idx": 14}, {"type": "text", "text": "The amount of selected tokens. Three $k_{1}$ and $k_{2}$ settings in the double-selection mechanism are tested, and their maximum performance gap of $A P^{b o x}$ on BDD100K dataset is 0.4 (i.e., 29.5-29.1). Similar results are exhibited on the metric of $A P^{m a s k}$ . These results demonstrate that our method is not sensitive to the hyper-parameters $k_{1}$ and $k_{2}$ . Furthermore, the experiments on COCO dataset also exhibit the similar results, indicating that our method is robust. At last, we explain that the settings of $k_{1}$ and $k_{2}$ are infinite since they can be set as any value from 1 to $20\\mathrm{k}$ . SOTA models such as [18, 25, 24, 14] take 300 as the query number of transformer decoder. In the experiments, $k_{1}$ and $k_{2}$ are set as 300 or the multiple of 300 to align with the settings of the SOTA models. ", "page_idx": 14}, {"type": "text", "text": "Table 7: The results of diagnostic experiments on the number of token selection. Single-, double-, and triple-selection are represented as sing., doub., and trip., respectively. $k_{i},i\\in[1,2,3]$ denotes the amount of selected tokens. ", "page_idx": 14}, {"type": "table", "img_path": "srQxkSPJLW/tmp/e1ced4facc2fbfe47b16eea51e115b729d0fcf9c5b5da8a6973d3e2c41f4c25a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "srQxkSPJLW/tmp/3e094ee5632b52ddc46b2faf0da98a16f05dd24a29b3f63ea6d177019fba4a34.jpg", "table_caption": ["Table 8: The results of diagnostic experiments on the amount of selected tokens with our proposed double-selection mechanism. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "C.2 Diagnostic Experiments on the Number of Decoder Layer ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We study the effect of different decoder layer numbers for the model performance, and the results are summarized in Tab. 9. We can observe the following: 1) increasing the number of decoder layers from 6 to 9 on BDD100K dataset results in the performance degradation, which can be explained by the inconsistency between the complexity of the model and the dataset. The BDD100K dataset only contains $7\\mathbf{k}$ training sets and 1k validation sets. The size of $\\mathrm{BDD100K}$ is small and the model with 9 decoder layers is relatively more complex, leading to the overftiting of the training set; 2) increasing the number of decoder layers will contribute to both detection and segmentation on COCO. However, the model configured with 9 decoder layers only achieves a slight improvement and introduces more computation cost. Therefore, we use 6 decoder layers in our model; 3) our model has achieved comparable performance in the configuration with 3 decoder layers compared to MaskDINO with 9 decoder layers (e.g., 45.8 v.s. 45.7 on the $A P^{b o x}$ metric and 41.3 v.s. 41.4 on the $A P^{m a s k}$ metric on COCO), demonstrating that our model greatly improves the efficiency of the decoder. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "table", "img_path": "srQxkSPJLW/tmp/c1fe2242012f37addcb2d576b9cd9a0a6b85c2dbdeeed4ea6c7b37707058528a.jpg", "table_caption": ["Table 9: The results of diagnostic experiments on the number of decoder layer. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "D Visualization Analysis ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We visualize the predictions of MaskDINO and DI-MaskDINO to show qualitative comparison on BDD100K dataset. As shown in Fig. 3, MaskDINO produces boxes that do not tightly encompass the objects (i.e., Fig. 3a) or do not fully surround the objects (i.e., Fig. 3b). Compared to MaskDINO, our model produces perfectly-fitting boxes, demonstrating the effectiveness of DI-MaskDINO. In addition, our model focuses attention on the foreground objects with high category scores through the residual double-selection mechanism that avoids mispredicting the background as a foreground object. Fig. 3c suggests that our proposed residual double-selection mechanism is effective. ", "page_idx": 15}, {"type": "image", "img_path": "srQxkSPJLW/tmp/97fabb05df2712a1ea35156a66f80e59b41143e2a2830304e63fa314bebf28b2.jpg", "img_caption": ["Figure 3: Qualitative comparison between MaskDINO and DI-MaskDINO on BDD100K dataset. Suggest zooming in to view this figure for a clearer view of details. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: The paper\u2019s contributions and scope are accurately made in the abstract and introduction. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 16}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Justification: This paper discusses the limitations of the work in $\\S\\ S$ ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 16}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 16}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 16}, {"type": "text", "text": "Justification: This paper does not include theoretical results. Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 17}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: This paper provides a detailed description of our proposed DI- MaskDINO model in $\\S\\ 3$ and comprehensive implementation details in $\\S\\ 4.1$ and Appendix B. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 17}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: This paper presents a detailed description of our proposed DI-MaskDINO model in $\\S\\ 3$ , along with comprehensive implementation details provided in Appendix B. We will release the source code of our model at the following URL: https://github. com/CQU-ADHRI-Lab/DI-MaskDINO. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 18}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: This paper specifies all the training and test details in $\\S\\ 4$ and Appendix B. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 18}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 18}, {"type": "text", "text": "Answer: [No] ", "page_idx": 18}, {"type": "text", "text": "Justification: Error bars are not reported because it would be too computationally expensive and the previous researches in the field of object detection and instance segmentation did not report error bars. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The paper provides sufficient information on the computer resources needed to reproduce the experiments in $\\S\\ 4$ and Appendix B. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 19}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The research conducted in the paper conforms with the NeurIPS Code of Ethics in every respect. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 19}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: Our research focuses only on improving the overall performance of joint object detection and instance segmentation models. As such, it does not have direct societal impacts beyond the scope of enhancing these technical capabilities. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 19}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 20}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 20}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The paper cites the original paper that produced the code package or dataset. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We will release the source code of our model at the following URL: https: //anonymous.4open.science/r/DI-MaskDINO-12E4. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 21}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 21}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}]