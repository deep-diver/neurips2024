[{"figure_path": "gVTkMsaaGI/tables/tables_1_1.jpg", "caption": "Table 1: Sources of diffusion priors and constraints.", "description": "This table summarizes four different applications of diffusion models where downstream tasks require sampling from product distributions that involve a pretrained diffusion model as a prior and an auxiliary constraint.  It outlines the specific diffusion model prior used in each domain (vision, language, and reinforcement learning), the type of constraint applied, and the resulting posterior distribution that is the target of the sampling. This table helps to illustrate the wide applicability of the proposed method and how it addresses the intractable posterior inference problem across diverse tasks.", "section": "1 Introduction"}, {"figure_path": "gVTkMsaaGI/tables/tables_8_1.jpg", "caption": "Table 3: Results on the story infilling task with autoregressive and discrete diffusion language models. Metrics are computed with respect to reference infills from the dataset. All metrics are mean\u00b1std over 5 samples for each of the 100 test examples. RTB with discrete diffusion prior performs better than best baseline with autoregressive prior.", "description": "This table presents the results of a text infilling task using different language models.  The task involves predicting a missing sentence in a short story, given the preceding and following sentences. The models compared include an autoregressive language model, a discrete diffusion language model, and a discrete diffusion language model fine-tuned using the Relative Trajectory Balance (RTB) method. The evaluation metrics used are BLEU-4, GLEU-4, and BERTScore, all of which measure the similarity between the generated text and the reference text. The results show that the RTB-fine-tuned model outperforms the other models, indicating the effectiveness of RTB for improving text infilling.", "section": "3.3 Text infilling with discrete diffusion language models"}, {"figure_path": "gVTkMsaaGI/tables/tables_9_1.jpg", "caption": "Table 4: Average rewards of trained policies on D4RL locomotion tasks (mean\u00b1std over 5 random seeds). Following past work, numbers within 5% of maximum in every row are highlighted.", "description": "This table presents the average rewards achieved by different offline reinforcement learning algorithms on three continuous control tasks from the D4RL benchmark.  The tasks involve locomotion with a half-cheetah, hopper, and walker2d robot.  Three different datasets are used for each task, representing varying levels of data quality (medium-expert, medium, medium-replay). The algorithms compared include several baselines (BC, CQL, IQL) and state-of-the-art diffusion-based offline RL methods (Diffuser, Decision Diffuser, D-QL, IDQL, QGPO), along with the proposed RTB method.  The table highlights the top-performing algorithms for each task and dataset by bolding the values within 5% of the maximum reward for that row.", "section": "3.4 KL-constrained policy search in offline reinforcement learning"}, {"figure_path": "gVTkMsaaGI/tables/tables_20_1.jpg", "caption": "Table 2: Classifier-guided posterior sampling with pretrained unconditional diffusion priors. We report the mean\u00b1std of each metric computed across all relevant classes for each experiment set, and highlight \u00b15% from highest/lower experimental value. The FID is computed between learned posterior samples and the true samples from the class in question. DP and LGD-MC fail to appropriately model the posterior distribution (high average logr(x)) while DDPO mode-collapses. RTB achieves comparable or superior performance to all other baselines, optimally balancing high reward and diversity as measured by FID. See Table E.1 for conditional variants.", "description": "This table presents the results of classifier-guided posterior sampling experiments using pretrained unconditional diffusion priors.  It compares the performance of several methods, including Relative Trajectory Balance (RTB),  on MNIST and CIFAR-10 datasets. Metrics such as expected log-likelihood of the classifier, FID (Fr\u00e9chet Inception Distance), and diversity are reported to assess the quality of posterior samples generated by each method.  The table highlights that RTB achieves comparable or better performance compared to other methods in balancing reward and diversity, successfully addressing issues like mode collapse observed in other approaches.", "section": "3.1 Class-conditional posterior sampling from unconditional diffusion priors"}, {"figure_path": "gVTkMsaaGI/tables/tables_21_1.jpg", "caption": "Table 2: Classifier-guided posterior sampling with pretrained unconditional diffusion priors. We report the mean\u00b1std of each metric computed across all relevant classes for each experiment set, and highlight \u00b15% from highest/lower experimental value. The FID is computed between learned posterior samples and the true samples from the class in question. DP and LGD-MC fail to appropriately model the posterior distribution (high average logr(x)) while DDPO mode-collapses. RTB achieves comparable or superior performance to all other baselines, optimally balancing high reward and diversity as measured by FID. See Table E.1 for conditional variants.", "description": "This table presents a quantitative comparison of different posterior sampling methods for classifier-guided image generation.  It shows the performance of several methods (including the proposed RTB method) across multiple metrics such as FID (Fr\u00e9chet Inception Distance), diversity, and the average log-likelihood of the constraint.  The results demonstrate that the Relative Trajectory Balance (RTB) method outperforms existing methods in terms of balancing high reward and diversity.", "section": "3.1 Class-conditional posterior sampling from unconditional diffusion priors"}, {"figure_path": "gVTkMsaaGI/tables/tables_22_1.jpg", "caption": "Table 2: Classifier-guided posterior sampling with pretrained unconditional diffusion priors. We report the mean\u00b1std of each metric computed across all relevant classes for each experiment set, and highlight \u00b15% from highest/lower experimental value. The FID is computed between learned posterior samples and the true samples from the class in question. DP and LGD-MC fail to appropriately model the posterior distribution (high average logr(x)) while DDPO mode-collapses. RTB achieves comparable or superior performance to all other baselines, optimally balancing high reward and diversity as measured by FID. See Table E.1 for conditional variants.", "description": "This table presents a quantitative comparison of different methods for classifier-guided posterior sampling using pretrained unconditional diffusion models.  It shows the performance of several methods, including Relative Trajectory Balance (RTB), on MNIST and CIFAR-10 datasets, evaluating metrics such as expected log-likelihood of the constraint (E[logr(x)]), Fr\u00e9chet Inception Distance (FID), and diversity. The results demonstrate that RTB achieves a better balance between high reward and diversity, and outperforms existing approaches.  Conditional variants are also noted.", "section": "3.1 Class-conditional posterior sampling from unconditional diffusion priors"}, {"figure_path": "gVTkMsaaGI/tables/tables_22_2.jpg", "caption": "Table 2: Classifier-guided posterior sampling with pretrained unconditional diffusion priors. We report the mean\u00b1std of each metric computed across all relevant classes for each experiment set, and highlight \u00b15% from highest/lower experimental value. The FID is computed between learned posterior samples and the true samples from the class in question. DP and LGD-MC fail to appropriately model the posterior distribution (high average logr(x)) while DDPO mode-collapses. RTB achieves comparable or superior performance to all other baselines, optimally balancing high reward and diversity as measured by FID. See Table E.1 for conditional variants.", "description": "This table presents a comparison of different methods for classifier-guided posterior sampling using pretrained unconditional diffusion priors.  The methods are evaluated on their ability to balance high reward and diversity in generated samples.  Metrics include the average log-likelihood of the constraint (logr(x)), Fr\u00e9chet Inception Distance (FID), and diversity.  The table highlights the superior performance of Relative Trajectory Balance (RTB) in achieving both high reward and diversity compared to other baselines (DPS, LGD-MC, DDPO, DPOK).", "section": "3.1 Class-conditional posterior sampling from unconditional diffusion priors"}, {"figure_path": "gVTkMsaaGI/tables/tables_23_1.jpg", "caption": "Table 2: Classifier-guided posterior sampling with pretrained unconditional diffusion priors. We report the mean\u00b1std of each metric computed across all relevant classes for each experiment set, and highlight \u00b15% from highest/lower experimental value. The FID is computed between learned posterior samples and the true samples from the class in question. DP and LGD-MC fail to appropriately model the posterior distribution (high average logr(x)) while DDPO mode-collapses. RTB achieves comparable or superior performance to all other baselines, optimally balancing high reward and diversity as measured by FID. See Table E.1 for conditional variants.", "description": "This table presents a quantitative comparison of different methods for class-conditional posterior sampling using pretrained unconditional diffusion priors.  The methods compared include Density Peak (DPS), Langevin Gradient Descent with Monte Carlo (LGD-MC), Diffusion with DDPM and KL penalty (DDPO), Diffusion with DPOK, and Relative Trajectory Balance (RTB). The table shows the mean and standard deviation of several metrics for each method across all relevant classes, including the expected log-likelihood of the constraint (logr(x)), Fr\u00e9chet Inception Distance (FID), and diversity.  The results demonstrate that RTB outperforms other baselines by balancing high reward and diversity.", "section": "3.1 Class-conditional posterior sampling from unconditional diffusion priors"}, {"figure_path": "gVTkMsaaGI/tables/tables_23_2.jpg", "caption": "Table 2: Classifier-guided posterior sampling with pretrained unconditional diffusion priors. We report the mean\u00b1std of each metric computed across all relevant classes for each experiment set, and highlight \u00b15% from highest/lower experimental value. The FID is computed between learned posterior samples and the true samples from the class in question. DP and LGD-MC fail to appropriately model the posterior distribution (high average logr(x)) while DDPO mode-collapses. RTB achieves comparable or superior performance to all other baselines, optimally balancing high reward and diversity as measured by FID. See Table E.1 for conditional variants.", "description": "This table presents the results of classifier-guided posterior sampling experiments using pretrained unconditional diffusion priors.  It compares the performance of the proposed Relative Trajectory Balance (RTB) method against several baselines (DPS, LGD-MC, DDPO, DPOK) across different metrics: expected log-likelihood of the constraint (E[logr(x)]), Frechet Inception Distance (FID), and diversity.  The results show that RTB outperforms or matches the baselines in balancing high reward and diversity, demonstrating its effectiveness in accurately modeling posterior distributions.", "section": "3.1 Class-conditional posterior sampling from unconditional diffusion priors"}, {"figure_path": "gVTkMsaaGI/tables/tables_24_1.jpg", "caption": "Table G.1: Mixed vs. online training on D4RL Tasks. We report mean\u00b1std over 5 random seeds.", "description": "This table compares the performance of the Relative Trajectory Balance (RTB) method when using two different training approaches on three different tasks from the D4RL benchmark. The \"Online\" approach uses only online samples from the current policy to update the model parameters. The \"Mixed\" approach additionally uses offline samples.  The table shows the average reward achieved by the trained policy on each task, with standard deviation across five random trials.  The results suggest that the mixed training approach can lead to slightly improved performance compared to online training on some tasks, although the differences are not very large for some tasks.", "section": "G Offline RL"}, {"figure_path": "gVTkMsaaGI/tables/tables_24_2.jpg", "caption": "Table G.2: Temperature \u03b1 = } for D4RL tasks", "description": "This table shows the hyperparameter values of temperature (\u03b1) used in the Relative Trajectory Balance (RTB) method for different D4RL tasks. The temperature parameter influences the balance between exploration and exploitation during training.", "section": "G Offline RL"}, {"figure_path": "gVTkMsaaGI/tables/tables_25_1.jpg", "caption": "Table 2: Classifier-guided posterior sampling with pretrained unconditional diffusion priors. We report the mean\u00b1std of each metric computed across all relevant classes for each experiment set, and highlight \u00b15% from highest/lower experimental value. The FID is computed between learned posterior samples and the true samples from the class in question. DP and LGD-MC fail to appropriately model the posterior distribution (high average logr(x)) while DDPO mode-collapses. RTB achieves comparable or superior performance to all other baselines, optimally balancing high reward and diversity as measured by FID. See Table E.1 for conditional variants.", "description": "This table presents a comparison of different methods for classifier-guided posterior sampling using pretrained unconditional diffusion priors.  The methods compared are DPS, LGD-MC, DDPO, DPOK, and the proposed RTB method.  The table shows the mean and standard deviation of three metrics: the expected log-likelihood of the constraint (E[log r(x)]), the Frechet Inception Distance (FID) which measures the quality of generated samples, and diversity (measured using cosine similarity).  RTB outperforms other methods in balancing reward and diversity.", "section": "3.1 Class-conditional posterior sampling from unconditional diffusion priors"}]