[{"figure_path": "7W0f7lifDk/figures/figures_0_1.jpg", "caption": "Figure 1: Given a single image of a person (top), our method Human-3Diffusion creates 3D Gaussian Splats of realistic avatars with cloth and interacting objects with high-fidelity geometry and texture.", "description": "This figure demonstrates the capability of the Human-3Diffusion model to generate realistic 3D avatars from a single input image.  The top row shows a single input image of a person in various poses and clothing. The bottom rows display multiple views of the generated 3D avatar, highlighting the model's ability to accurately reconstruct fine details like clothing and interacting objects, while maintaining high-fidelity geometry and texture. The 3D avatars are represented as 3D Gaussian Splats.", "section": "Abstract"}, {"figure_path": "7W0f7lifDk/figures/figures_3_1.jpg", "caption": "Figure 2: Method Overview. Given a single RGB image (A), we sample a realistic 3D avatar represented as 3D Gaussian Splats (D). At each reverse step, our 3D generation model g\u00f8 leverages 2D multi-view diffusion prior from ee which provides a strong shape prior but is not 3D consistent (B, cf. Sec. 4.1). We then refine the 2D reverse sampling trajectory with generated 3D renderings that are guaranteed to be 3D consistent (C, cf. Sec. 4.2). Our tight coupling ensures 3D consistency at each sampling step and obtains a high-quality 3D avatar (D).", "description": "This figure illustrates the method overview of Human-3Diffusion. Starting with a single RGB image as input, the model generates a realistic 3D avatar.  The process involves leveraging 2D multi-view diffusion models for shape priors, while simultaneously refining the 2D reverse sampling trajectory using generated 3D renderings to ensure 3D consistency.  This tight coupling between 2D and 3D models results in a high-quality 3D avatar representation.", "section": "4 3Diffusion"}, {"figure_path": "7W0f7lifDk/figures/figures_5_1.jpg", "caption": "Figure 2: Method Overview. Given a single RGB image (A), we sample a realistic 3D avatar represented as 3D Gaussian Splats (D). At each reverse step, our 3D generation model g\u00f8 leverages 2D multi-view diffusion prior from ee which provides a strong shape prior but is not 3D consistent (B, cf. Sec. 4.1). We then refine the 2D reverse sampling trajectory with generated 3D renderings that are guaranteed to be 3D consistent (C, cf. Sec. 4.2). Our tight coupling ensures 3D consistency at each sampling step and obtains a high-quality 3D avatar (D).", "description": "This figure provides a visual overview of the Human-3Diffusion method. It shows how a single RGB image is used as input to generate a realistic 3D avatar represented as 3D Gaussian Splats. The process involves leveraging 2D multi-view diffusion priors for shape information, refining the 2D reverse sampling trajectory with 3D-consistent renderings, and ensuring 3D consistency at each sampling step.", "section": "4 3Diffusion"}, {"figure_path": "7W0f7lifDk/figures/figures_6_1.jpg", "caption": "Figure 3: Qualitative comparison with baselines. Recent avatar reconstruction works ICON [87], ECON [88], SiTH [23] and SIFU [108]) cannot reconstruct loose clothing coherently. Additionally, SITH and SIFU generate blurry texture in unseen regions due to their deterministic formulation of regressing 3D avatar directly from single RGB imagse. In contract, our method is able to reconstruct avatars with realistic textures and plausible 3D geometry in both seen and unseen region.", "description": "This figure compares the results of the proposed Human-3Diffusion method to several state-of-the-art avatar reconstruction methods (ICON, ECON, SiTH, and SIFU). The comparison highlights the ability of Human-3Diffusion to generate realistic avatars with coherent clothing and textures, even in areas occluded in the input image, unlike the other methods that struggle with loose clothing or produce blurry textures.", "section": "5.2 Realistic Avatar from Image"}, {"figure_path": "7W0f7lifDk/figures/figures_7_1.jpg", "caption": "Figure 4: 3D reconstruction conditioned on different multi-view priors. Without our 3D-consistent sampling, the 2D diffusion model cannot generate 3D consistent multi-views (MVD, MVDft), leading to artifacts like floating 3D Gaussians splats.", "description": "This figure compares 3D reconstruction results obtained using different methods.  The input is a single RGB image of a person. The \"Ours\" column shows the results produced by the proposed Human-3Diffusion method, which uses an explicit 3D representation to ensure 3D consistency across multiple views. In contrast, the \"MVD\" and \"MVDft\" columns show the results of using only a 2D multi-view diffusion model without the explicit 3D representation. The comparison highlights the superior performance of the proposed Human-3Diffusion in generating 3D consistent multi-views and avoiding artifacts.", "section": "5.3 Ablation Studies"}, {"figure_path": "7W0f7lifDk/figures/figures_8_1.jpg", "caption": "Figure 5: 2D multi-view priors x enhances generalization to general objects in GSO [17] dataset.", "description": "This figure shows an ablation study demonstrating the benefit of using 2D multi-view priors in the 3D generation process.  The leftmost image is the input image of a stuffed rabbit. The middle section shows the results of the model when using the proposed method (with 2D multi-view priors), showcasing a relatively complete and consistent 3D reconstruction from various viewpoints. The rightmost section shows the results obtained using a \"pure 3D generative model without 2D priors\", highlighting the significant improvement achieved through the integration of 2D information in improving the quality and consistency of 3D generation for unseen objects.", "section": "5.3 Ablation Studies"}, {"figure_path": "7W0f7lifDk/figures/figures_9_1.jpg", "caption": "Figure 2: Method Overview. Given a single RGB image (A), we sample a realistic 3D avatar represented as 3D Gaussian Splats (D). At each reverse step, our 3D generation model g\u00f8 leverages 2D multi-view diffusion prior from ee which provides a strong shape prior but is not 3D consistent (B, cf. Sec. 4.1). We then refine the 2D reverse sampling trajectory with generated 3D renderings that are guaranteed to be 3D consistent (C, cf. Sec. 4.2). Our tight coupling ensures 3D consistency at each sampling step and obtains a high-quality 3D avatar (D).", "description": "This figure provides a visual overview of the Human-3Diffusion method. It shows how a single RGB image is used as input to generate a realistic 3D avatar.  The process involves leveraging 2D multi-view diffusion models for shape priors, refining the 2D reverse sampling trajectory with 3D-consistent renderings, and ultimately producing a high-quality 3D avatar represented as 3D Gaussian Splats.", "section": "4 3Diffusion"}, {"figure_path": "7W0f7lifDk/figures/figures_21_1.jpg", "caption": "Figure 7: Visualization of intermediate sampling steps from a Gaussian Noise (t = 1000) to the last denoising step (t = 0). From top to bottom: current state xt, estimated clear view by 2D diffusion models xt, and corrected clear view by generated 3D Gaussian Splatting. Our 2D diffusion model e\u03b8(0) already provides strong multi-view prior at an early stage with large t. Our 3D reconstruction model g\u03b8(0) can correct the inconsistency in xtgt illustrated in red circle.", "description": "This figure visualizes the intermediate sampling steps during the reverse diffusion process.  It shows how the model refines its estimates at each step, transitioning from noisy input to a clear, consistent output.  The comparison highlights the benefits of using the 3D reconstruction model to correct inconsistencies introduced by the 2D diffusion model alone, showing improved clarity and consistency in the final output.", "section": "4 3Diffusion"}, {"figure_path": "7W0f7lifDk/figures/figures_23_1.jpg", "caption": "Figure 8: Qualitative comparison on Sizer [72] and IIIT [32].", "description": "This figure presents a qualitative comparison of the proposed Human-3Diffusion method with several state-of-the-art avatar reconstruction methods on the Sizer and IIIT datasets.  The results show that Human-3Diffusion outperforms competing methods, especially in terms of reconstructing clothing and accessories accurately, with higher fidelity in geometry and appearance.  The comparison highlights the superiority of the Human-3diffusion model in handling loose and complex clothing.", "section": "B Comparison"}, {"figure_path": "7W0f7lifDk/figures/figures_24_1.jpg", "caption": "Figure 8: Qualitative comparison on Sizer [72] and IIIT [32].", "description": "This figure presents a qualitative comparison of different methods for human avatar reconstruction on the Sizer and IIIT datasets.  The \"Input\" column shows the original images used. The remaining columns show the results generated by various techniques, including the authors' proposed method, labeled as \"Ours.\"  The comparison highlights the differences in the quality of the generated avatars with respect to geometry, clothing details, and overall realism.", "section": "B Comparison"}, {"figure_path": "7W0f7lifDk/figures/figures_25_1.jpg", "caption": "Figure 10: Qualitative results on unseen data during training. Input image is in left column. Our method successfully reconstructs different degree of loose clothing.", "description": "This figure shows several examples of avatar reconstruction results from single images. The left column shows the input images, and the right column displays the reconstructed avatars produced by the Human-3Diffusion model.  Each row demonstrates the reconstruction of a person wearing clothing that exhibits varying degrees of looseness. The figure highlights the ability of the model to handle the challenges of loose clothing in 3D avatar reconstruction.", "section": "C.1 In-the-wild Data"}, {"figure_path": "7W0f7lifDk/figures/figures_26_1.jpg", "caption": "Figure 3: Qualitative comparison with baselines. Recent avatar reconstruction works ICON [87], ECON [88], SiTH [23] and SIFU [108]) cannot reconstruct loose clothing coherently. Additionally, SITH and SIFU generate blurry texture in unseen regions due to their deterministic formulation of regressing 3D avatar directly from single RGB imagse. In contract, our method is able to reconstruct avatars with realistic textures and plausible 3D geometry in both seen and unseen region.", "description": "This figure compares the qualitative results of the proposed method with several state-of-the-art avatar reconstruction methods.  It highlights the superior ability of the proposed method to reconstruct avatars with loose clothing and realistic textures, compared to the other methods which struggle with coherent reconstruction of loose clothing and generate blurry textures in unseen regions due to their deterministic nature.", "section": "5.2 Realistic Avatar from Image"}, {"figure_path": "7W0f7lifDk/figures/figures_27_1.jpg", "caption": "Figure 1: Given a single image of a person (top), our method Human-3Diffusion creates 3D Gaussian Splats of realistic avatars with cloth and interacting objects with high-fidelity geometry and texture.", "description": "This figure shows the results of the Human-3Diffusion method. Given a single image of a person as input (top row), the method generates a 3D model of the person represented as 3D Gaussian Splats. The generated models accurately capture details like clothing, interacting objects, and the overall geometry and texture of the person.", "section": "Abstract"}, {"figure_path": "7W0f7lifDk/figures/figures_28_1.jpg", "caption": "Figure 3: Qualitative comparison with baselines. Recent avatar reconstruction works ICON [87], ECON [88], SiTH [23] and SIFU [108]) cannot reconstruct loose clothing coherently. Additionally, SITH and SIFU generate blurry texture in unseen regions due to their deterministic formulation of regressing 3D avatar directly from single RGB imagse. In contract, our method is able to reconstruct avatars with realistic textures and plausible 3D geometry in both seen and unseen region.", "description": "This figure compares the results of the proposed method with several state-of-the-art avatar reconstruction methods. It highlights the superior performance of the proposed method in reconstructing avatars with realistic textures and geometry, especially for loose clothing and occluded regions.", "section": "5.2 Realistic Avatar from Image"}, {"figure_path": "7W0f7lifDk/figures/figures_29_1.jpg", "caption": "Figure 14: Qualitative results on UBC fashion [101] dataset. Results demonstrate that our model generalizes well to real world images in both geometry and appearance.", "description": "This figure shows qualitative results of the proposed Human-3Diffusion model on the UBC fashion dataset [101]. The input images are the first frame extracted from each video in this dataset. The results demonstrate that the model generalizes well to real-world images, achieving high fidelity in both geometry and appearance.", "section": "C.2 UBC Fashion Dataset"}, {"figure_path": "7W0f7lifDk/figures/figures_30_1.jpg", "caption": "Figure 2: Method Overview. Given a single RGB image (A), we sample a realistic 3D avatar represented as 3D Gaussian Splats (D). At each reverse step, our 3D generation model g\u00f8 leverages 2D multi-view diffusion prior from ee which provides a strong shape prior but is not 3D consistent (B, cf. Sec. 4.1). We then refine the 2D reverse sampling trajectory with generated 3D renderings that are guaranteed to be 3D consistent (C, cf. Sec. 4.2). Our tight coupling ensures 3D consistency at each sampling step and obtains a high-quality 3D avatar (D).", "description": "This figure shows the overall pipeline of Human-3Diffusion. Starting from a single RGB image, the model generates a realistic 3D avatar represented by 3D Gaussian Splats.  The process involves leveraging 2D multi-view diffusion priors for shape information while simultaneously refining the 2D reverse sampling trajectory using the generated 3D renderings to ensure 3D consistency.  The tight coupling between 2D and 3D models is crucial for achieving high-fidelity results.", "section": "4 3Diffusion"}, {"figure_path": "7W0f7lifDk/figures/figures_31_1.jpg", "caption": "Figure 3: Qualitative comparison with baselines. Recent avatar reconstruction works ICON [87], ECON [88], SiTH [23] and SIFU [108]) cannot reconstruct loose clothing coherently. Additionally, SITH and SIFU generate blurry texture in unseen regions due to their deterministic formulation of regressing 3D avatar directly from single RGB imagse. In contract, our method is able to reconstruct avatars with realistic textures and plausible 3D geometry in both seen and unseen region.", "description": "This figure compares the qualitative results of the proposed Human-3Diffusion model against several state-of-the-art avatar reconstruction methods (ICON, ECON, SiTH, SIFU). The comparison highlights the superior ability of Human-3Diffusion to reconstruct avatars with realistic textures and 3D geometry, especially in handling loose clothing and occluded regions where other methods produce blurry textures or incomplete geometry.", "section": "5.2 Realistic Avatar from Image"}, {"figure_path": "7W0f7lifDk/figures/figures_31_2.jpg", "caption": "Figure 3: Qualitative comparison with baselines. Recent avatar reconstruction works ICON [87], ECON [88], SiTH [23] and SIFU [108]) cannot reconstruct loose clothing coherently. Additionally, SITH and SIFU generate blurry texture in unseen regions due to their deterministic formulation of regressing 3D avatar directly from single RGB imagse. In contract, our method is able to reconstruct avatars with realistic textures and plausible 3D geometry in both seen and unseen region.", "description": "This figure compares the qualitative results of the proposed method against other state-of-the-art avatar reconstruction methods. The comparison highlights the superior performance of the proposed method in handling loose clothing and generating realistic textures, even in unseen regions.  The figure visually demonstrates that existing methods struggle to reconstruct loose clothing coherently and produce blurry textures, unlike the proposed method.", "section": "5.2 Realistic Avatar from Image"}, {"figure_path": "7W0f7lifDk/figures/figures_32_1.jpg", "caption": "Figure 18: Our model learns 3D distribution. By different sampling from the learned distribution, we obtain diverse yet plausible 3D representations. The generative power is a key to generate clear self-occluded region, which is impossible in non-generative reconstruction approaches [56, 57, 74, 108].", "description": "This figure shows the generative power of the proposed model. By sampling from the learned 3D distribution with different random seeds, the model generates diverse yet plausible 3D representations, particularly in the self-occluded regions (the back of the subject).  The differences are noticeable in the hair style, texture, and cloth wrinkles.  This is a key advantage over non-generative methods which tend to produce blurry or less detailed results in self-occluded areas.", "section": "C.5 Generative Power in Reconstruction"}, {"figure_path": "7W0f7lifDk/figures/figures_35_1.jpg", "caption": "Figure 19: Example scans in training datasets [1\u20134, 21, 27, 65, 98].", "description": "This figure shows example scans from the training datasets used in the Human-3Diffusion model.  The datasets include AXYZ, Custom Human, THuman 2.0, THuman 3.0, and Commercial datasets, each contributing a variety of body types, clothing styles, poses, and accessories. This diversity is crucial for training a robust and generalizable model that can handle a wide range of human appearances and situations.", "section": "D Dataset Overview"}, {"figure_path": "7W0f7lifDk/figures/figures_36_1.jpg", "caption": "Figure 1: Given a single image of a person (top), our method Human-3Diffusion creates 3D Gaussian Splats of realistic avatars with cloth and interacting objects with high-fidelity geometry and texture.", "description": "This figure shows the results of the Human-3Diffusion method. Given a single image of a person as input, the method generates a realistic 3D model of the person, including detailed clothing and any interacting objects.  The 3D model is represented using Gaussian Splats, a technique that allows for efficient rendering and manipulation of complex 3D shapes.  The figure highlights the high fidelity of the generated model in terms of both geometry and texture.", "section": "Abstract"}, {"figure_path": "7W0f7lifDk/figures/figures_36_2.jpg", "caption": "Figure 1: Given a single image of a person (top), our method Human-3Diffusion creates 3D Gaussian Splats of realistic avatars with cloth and interacting objects with high-fidelity geometry and texture.", "description": "This figure shows the result of the Human-3Diffusion method.  Given a single image of a person as input (shown at the top), the method generates a realistic 3D model of the person, represented as a collection of 3D Gaussian Splats.  The generated models accurately capture details such as clothing, and interactions with objects, showcasing high-fidelity geometry and texture. The multiple views presented demonstrate the 3D consistency achieved by the method.", "section": "Abstract"}, {"figure_path": "7W0f7lifDk/figures/figures_36_3.jpg", "caption": "Figure 3: Qualitative comparison with baselines. Recent avatar reconstruction works ICON [87], ECON [88], SiTH [23] and SIFU [108]) cannot reconstruct loose clothing coherently. Additionally, SITH and SIFU generate blurry texture in unseen regions due to their deterministic formulation of regressing 3D avatar directly from single RGB imagse. In contract, our method is able to reconstruct avatars with realistic textures and plausible 3D geometry in both seen and unseen region.", "description": "This figure compares the results of the proposed Human-3Diffusion method with other state-of-the-art avatar reconstruction methods (ICON, ECON, SiTH, and SIFU). It highlights that the proposed method is superior in reconstructing loose clothing and producing realistic textures, particularly in areas occluded in the input image. The comparison demonstrates the advantages of using an explicit 3D consistent diffusion model.", "section": "5.2 Realistic Avatar from Image"}, {"figure_path": "7W0f7lifDk/figures/figures_37_1.jpg", "caption": "Figure 23: Failure Case: our model cannot reconstruct the numbers on the cloth.", "description": "This figure shows a failure case of the Human-3Diffusion model.  The model is unable to reconstruct the numbers on the running shirt of the person in the image, indicating a limitation in reconstructing fine details, especially text, potentially due to the resolution limitations of the multi-view diffusion model used.", "section": "E Failure Cases"}, {"figure_path": "7W0f7lifDk/figures/figures_37_2.jpg", "caption": "Figure 24: Failure Case: our model fails in infer appearance of human with challenging pose.", "description": "This figure shows a failure case of the Human-3Diffusion model. The input image shows a person performing a challenging pose (a handstand). The model's reconstruction of the person's pose is inaccurate and blurry, especially in the head and upper body regions. The highlighted area in the reconstructed images indicates the region of notable inaccuracy.", "section": "E Failure Cases"}]