[{"type": "text", "text": "LLM Processes: Numerical Predictive Distributions Conditioned on Natural Language ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "James Requeima\u2217 John Bronskill\u2217 Dami Choi University of Toronto University of Cambridge University of Toronto requeima@cs.toronto.edu jfb54@cam.ac.uk choidami@cs.toronto.edu ", "page_idx": 0}, {"type": "text", "text": "Richard E. Turner ", "page_idx": 0}, {"type": "text", "text": "David Duvenaud ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "University of Cambridge The Alan Turing Institute ret26@cam.ac.uk ", "page_idx": 0}, {"type": "text", "text": "University of Toronto Vector Institute duvenaud@cs.toronto.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Machine learning practitioners often face significant challenges in formally integrating their prior knowledge and beliefs into predictive models, limiting the potential for nuanced and context-aware analyses. Moreover, the expertise needed to integrate this prior knowledge into probabilistic modeling typically limits the application of these models to specialists. Our goal is to build a regression model that can process numerical data and make probabilistic predictions at arbitrary locations, guided by natural language text which describes a user\u2019s prior knowledge. Large Language Models (LLMs) provide a useful starting point for designing such a tool since they 1) provide an interface where users can incorporate expert insights in natural language and 2) provide an opportunity for leveraging latent problem-relevant knowledge encoded in LLMs that users may not have themselves. We start by exploring strategies for eliciting explicit, coherent numerical predictive distributions from LLMs. We examine these joint predictive distributions, which we call LLM Processes, over arbitrarily-many quantities in settings such as forecasting, multi-dimensional regression, black-box optimization, and image modeling. We investigate the practical details of prompting to elicit coherent predictive distributions, and demonstrate their effectiveness at regression. Finally, we demonstrate the ability to usefully incorporate text into numerical predictions, improving predictive performance and giving quantitative structure that reflects qualitative descriptions. This lets us begin to explore the rich, grounded hypothesis space that LLMs implicitly encode. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Incorporating prior knowledge into predictive models is highly challenging which can restrict the scope for detailed, context-sensitive analysis. In addition, the skill required to incorporate this prior knowledge into probabilistic modelling can restrict the use of these models to experts. In this work, our objective is to develop a probabilistic prediction model that facilitates user interaction through straightforward, natural language. For this purpose, we explore strategies for eliciting explicit, coherent numerical predictive distributions from LLMs. ", "page_idx": 0}, {"type": "text", "text": "Why go to so much effort to elicit predictions from a slow, expensive, and sometimes inconsistent model like an LLM? We expect their hypothesis class to be both rich, and grounded in exactly the kinds of high-level side information that we currently struggle to communicate to our numerical models. For instance, knowing that prices rarely go below zero, that certain kinds of sensors can saturate at particular values, or that trends almost always eventually level off, are easy to express in natural language, but not straightforward to incorporate into a model without getting lost in difficult-to-specify details about aspects of the domain that aren\u2019t well understood. To summarize, we want to develop such a model because it would allow users to 1) provide prior, potentially expert, information to the model about the problem setting in plain-language rather than attempting to capture this information in closed form priors (e.g. Gaussian Process kernels) and 2) it would allow users to access problem-relevant latent knowledge encoded in LLMs that users may not have themselves. ", "page_idx": 0}, {"type": "image", "img_path": "HShs7q1Njh/tmp/8285c7b33dd623d616f39674f593471ad07dbb6909e8531f4703e536d1ea2936.jpg", "img_caption": ["Figure 1: Predictive distributions from an LLMP conditioned on both data and text information. The tenth-percentiles from 50 samples are visualized in faded blue and the median is presented in dark blue with five random samples shown in various colours. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "LLMs have recently been shown to be able to condition on the particular task being solved, leveraging contextual information to make better predictions or decisions [1]. They have also been shown to competitively predict time series based only on a text tokenization of numerical data [2]. In this work, we further push in both these directions; 1) using LLMs for numerical prediction tasks going beyond one-dimensional time series forecasting to multi-dimensional regression and density estimation and 2) exploring the ability of these models to condition on both numerical data and rich, unstructured text to improve these predictions. In this paper we make the following contributions: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We define LLM Processes (LLMPs) using methods we develop for eliciting numerical predictive distributions from LLMs.2 LLMPs go beyond one-dimensional time series forecasting to multi-dimensional regression and density estimation. We propose two approaches for defining this joint predictive distribution over a collection of query points and evaluate their compatibility in principle with the consistency axioms necessary to specify a valid statistical process. ", "page_idx": 1}, {"type": "text", "text": "\u2022 We develop effective prompting practices for eliciting joint numerical predictions. We investigate various methods for conditioning LLMs on numerical data, including prompt formatting, ordering, and scaling. We characterize which schemes perform best on a set of synthetic tasks. ", "page_idx": 1}, {"type": "text", "text": "\u2022 We show that LLMPs are competitive and flexible regressors even on messy data. Through an extensive set of synthetic and real world experiments, including image reconstruction and black-box function optimization, we evaluate the zero-shot regression and forecasting performance of LLMPs. We demonstrate that LLMPs have well-calibrated uncertainty and are competitive with Gaussian Processes (GPs), LLMTime [2], and Optuna [3]. We show that LLMPs use in-context learning to automatically leverage information from related datasets, can easily handle missing datapoints, perform image reconstruction, and output multimodal predictive distributions. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Lastly, we demonstrate the ability to usefully incorporate problem-relevant information provided through unstructured text into numerical predictions, visualized in Figure 1, resulting in quantitative structure that reflects qualitative descriptions. Other additions such as labelling features using text and specifying units allow LLMPs to make use of usually-ignored side information. ", "page_idx": 1}, {"type": "image", "img_path": "HShs7q1Njh/tmp/b9b0a3c4434d75b50e49f4ee4a8c1d84fa119079936aff57e9043ac8f4bbc8b4.jpg", "img_caption": ["Figure 2: Sampling from an LLM using either independent marginal or autoregressive sampling. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "2 LLM Processes: Defining a Stochastic Process That Can Condition on Text ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Our goal for this section is to use an LLM to elicit joint predictive distributions over arbitrary sized target sets that we can guide and modify using natural language. Formally, given a set of input and output observations $D_{\\mathrm{train}}\\;=\\;\\{(x_{i},\\dot{y}_{i})\\}_{i=1}^{M^{\\bullet}}$ and some text, $T$ , we would like to elicit the predictive distribution defined by an LLM at a collection of targets $\\{(x_{j}^{*},y_{j}^{*})\\}_{j=1}^{N}$ denoted $p_{\\mathrm{LLM}}(y_{1}^{*},\\ldots,y_{N}^{*}\\mid x_{1}^{*},\\ldots,x_{N}^{*},D_{\\mathrm{train}},T).$ . ", "page_idx": 2}, {"type": "text", "text": "Rejection sampling from an LLM allows us to access what we may interpret as the LLM\u2019s predictive distribution and gain insights into the model\u2019s inductive biases; sampling from the LLM\u2019s categorical distribution over text tokens while ignoring non-numerical tokens yields numerical samples from the LLM. The process of sampling from an LLM is depicted in Figure 2 and Algorithm 1. Sample prompts are in Appendix C. Since an accurate sampling-based empirical distribution incurs a high computational cost, next we define an approach to elicit continuous likelihoods from an LLM. ", "page_idx": 2}, {"type": "text", "text": "Continuous Marginal Likelihoods From an LLM. We approximate a continuous density over our target values by discretizing the space using bins with arbitrarily fine precision, similar to the method used in Gruver et al. [2]. Crucially, this hierarchical approach allows us to compute the probability of a bin with width $10^{-n}$ . For example, if $n\\,=\\,1$ then $\\operatorname*{Pr}\\{y\\;\\in\\;[1.0,1.1)\\}\\,=\\,p(1)p(.|1)p(0|1.)$ because $\\cdot_{1.0},$ is a prefix for all $y\\in[1.0,1.1)$ . We can convert probability mass to probability density by assuming a uniform distribution within each bin, and dividing the mass by the bin width. A visualization of this construction is in Figures G.2 to G.4. ", "page_idx": 2}, {"type": "text", "text": "Unlike [2], we do not rescale the values to remove decimal places. We hypothesize that such scaling removes prior information communicated to the LLM via the scale of the problem. We examine the effect of scaling values in Section 3. We also differ from [2] by including a terminal token after every value in our prompt \u2013 for example, given a terminal token $\\langle t\\rangle$ , we represent 12 as $12\\langle t\\rangle$ . Including a terminal token prevents numbers of varying orders of magnitude to share the same prefix \u2013 i.e. $p(1)p(2|1)p(\\langle t\\rangle|\\bar{12})$ no longer includes the probability of numbers in [120, 130), [1200, 1300), etc. ", "page_idx": 2}, {"type": "text", "text": "Note that this approach does not guarantee that $P(12\\langle t\\rangle)$ yields the mass assigned by the LLM to values in the bin [12, 13) but we empirically observed that our predictive distribution closely matches the sampling distribution to our satisfaction. See Section G.1 for more details and comparison. ", "page_idx": 2}, {"type": "text", "text": "Defining an LLM Process. Thus far we have established a procedure defining the predictive distribution at a single target location, $p_{\\mathrm{LLM}}(y_{n}^{*}\\mid x_{n}^{*},D_{\\mathrm{train}},T)$ . We now outline two methods which we call independent marginal (I-LLMP) and autoregressive (A-LLMP) predictions, for defining the joint predictive distribution over a collection of target points: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle p_{\\mathrm{I.LLMP}}(y_{1}^{*},...,y_{N}^{*}\\mid x_{1}^{*},...,x_{N}^{*},D_{\\mathrm{train}},T)=\\prod_{n=1}^{N}{p_{\\mathrm{LLM}}(y_{n}^{*},\\mid x_{n}^{*},D_{\\mathrm{train}},T)}}\\\\ {\\displaystyle p_{\\mathrm{A.LLMP}}(y_{1}^{*},...,y_{N}^{*}\\mid x_{1}^{*},...,x_{N}^{*},D_{\\mathrm{train}},T)=\\prod_{n=1}^{N}{p_{\\mathrm{LLM}}(y_{n}^{*}\\mid y_{1}^{*},...,y_{n-1}^{*},x_{1}^{*},...,x_{n}^{*},D_{\\mathrm{train}},T)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "We note that Equation (1) satisfies the Kolmogorov Extension Theorem [4] therefore defining valid stochastic process (see Appendix A.3). However, it assumes conditional independence given the training set and model weights and the stochastistity represented by the model is via independent marginals. Equation (2) takes inspiration from the autoregressive structure of the LLMs predictive distribution and should yield much richer predictive distributions as we are now able to model dependencies between output variables. However, this definition is no longer guaranteed to give us a valid stochastic process as the predictive distribution is now target order dependent and will likely fail the Kolmogorov exchangability condition. We investigate both of these questions in Section 3. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Connection to Neural processes Neural Processes (NPs) [5] are a class of meta-learning models parametrized by neural networks and trained to learn a map from training (context) sets to predictive distributions, $p_{\\theta}(y_{1}^{*},\\_...\\_,y_{N}^{*}\\mid x_{1}^{*},\\_...\\_,x_{N}^{*},D_{\\mathrm{train}})$ . The definitions in Equations 1 and 2 take inspiration from the joint distributions defined by Conditional NPs [5] as independent marginals conditioned on the training/context set and Autoregressive NPs [6] utilizing the chain rule of probability, respectively. Through this lens, LLMPs can be viewed as examples of NPs. However, NPs are directly trained to output this predictive distribution where as LLMPs are repurposing pretrained LLMs. ", "page_idx": 3}, {"type": "text", "text": "Multi-dimensional Density Estimation and Handling Missing Data. We highlight that, through the flexibility of the LLM prompt, we do not have to draw a distinction between which variables, or variable dimensions are to be modelled or conditioned and can easily handle missing values. Suppose we have a collection of variables $\\{x_{1},\\ldots,x_{n}\\}$ and $\\{y_{1},\\dots,y_{m}\\}$ (or more), some subset of which we would like to regress on (including $x$ and $y$ -values) and the remainder we wish to condition on. To do so using an LLMP, we simply construct the training prompt such that the variables we would like to regress on occur at the end of the prompt and are blank (generated) when sampling from the LLMP. If any values are missing they can simply be removed from the prompt. ", "page_idx": 3}, {"type": "text", "text": "3 LLMP Configuration ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Experiment Details. In all of the experiments in Sections 3 to 5, we use six different open source LLMs: Mixtral ${\\bf8}\\!\\times\\!7{\\bf B}$ , Mixtral- $\\mathbf{\\nabla}\\cdot8\\times7\\mathbf{B}$ -Instruct [7], Llama-2 7B, Llama-2 70B [8], Llama-3 8B, and Llama-3 70B [9]. Note that we never modify the LLM parameters via training or fine-tuning, we use only prompting. Our primary metrics are negative log probabilities (NLL) of the model evaluated at the true function values $f(x^{*})$ averaged over the target locations and Mean Absolute Error (MAE) between the predictive median and the true function value. Unless otherwise stated, we use 50 samples from the LLM at each target location $x^{*}$ and compute the median and the $95\\%$ confidence interval of the sample distribution. Details of the datasets are given in Appendix D. Since the LLMs used in our experiments have undisclosed training sets, we address the steps taken to mitigate the issue of data-leakage in Appendix E. Additional implementation details and processing times are in Appendix F. ", "page_idx": 3}, {"type": "image", "img_path": "HShs7q1Njh/tmp/2f6c53833d7c407aa46c52249ee789235fa8e477c1c691788cf8100f5a660340.jpg", "img_caption": ["Figure 3: NLL and MAE for various prompt formats ordered from the most to least token efficient (left), training data orderings (middle), and prompt $y$ -scaling (right) using the Mixtral- $\\mathbf{\\nabla}\\cdot8\\times7\\mathbf{B}$ LLM. The height of each bar is the mean of 10 random seeds that determine the training point locations. The vertical black lines indicate the standard error. In the Prompt Formatting legend (left), the two \u2018_\u2019 characters indicate the positions of the $x$ and $y$ values and $\\mathfrak{m}$ represents a new line terminal token. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Prompt Engineering. We perform a set of experiments for determining the best LLMP prompt configuration. We use the Sigmoid, Quadratic, and Linear+Cosine functions with 10, 20 and 75 training points, respectively (see Appendix D.1) with I-LLMP using the Mixtral- $\\mathbf{\\nabla}8\\!\\times\\!7\\mathbf{B}$ LLM. ", "page_idx": 3}, {"type": "text", "text": "\u2022 Prompt Formatting Two separators are required to achieve the best performance. One to separate the $x$ and $y$ values within a pair and another to separate the $x,y$ pairs. Figure 3 (left) demonstrates that $\\_\\mathfrak{m}$ is the best option in terms of performance and token efficiency. \u2022 Prompt Ordering Figure 3 (middle) shows that ordering the training points by distance to the current target point is best, outperforming both random and sequential ordering. We posit that ordering ", "page_idx": 3}, {"type": "image", "img_path": "HShs7q1Njh/tmp/b287916bc4b6bfdc78e142adca63ad91cabdd78426ffd02e70434b86857cd9e1.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 4: Autoregressive Experiments. Left: NLL and MAE for A-LLMP and I-LLMP using different prompt orderings using the Mixtral-8x7B LLM. The height of each bar is the mean of 3 random seeds that determine the training point locations. The black lines indicate the standard error. Center: Log-likelihood results of using various test set orderings with Llama-2-7B, Llama-2-70B and Mixtral-8x7B A-LLMP. The orange X indicates I-LLMP, the purple circles used distance ordered test points, and the blue whiskers are the mean and standard error of 10 randomly sampled test orderings. The red dashed line shows the log-likelihood of the test set under the generative process. Right: Heatmap visualization of the Llama-3-70B A-LLMP predictive distribution conditioned on data from a bimodal generative process. Black dots are training points. ", "page_idx": 4}, {"type": "text", "text": "by distance provides a hint to the LLM to weigh the contribution of closer training points to the current target point to a greater degree.   \n\u2022 Prompt $y$ -Scaling Figure 3 (right) shows that performance degrades as the range of the $y$ components of the training points increases and when incorporating negative values. This is due to the fact that when the range is wider, the LLM must accurately generate more numerical digits and potentially a negative sign when predicting $f(x^{*})$ .   \n\u2022 top-p and Temperature Figure G.9 shows that performance is surprisingly insensitive to varying the LLM nucleus sampling parameter top- $p$ [10] and LLM softmax temperature. ", "page_idx": 4}, {"type": "text", "text": "Autoregressive vs Independent Marginal Predictions. Here we examine two questions: first, does the autoregressive defininiton of the joint predictive likelihood (A-LLMP) in Equation (2) improve performance versus the independent marginal definition of Equation (1) (I-LLMP). Second, \u201chow close\u201d is A-LLMP to a stochastic process in terms of performance variability across query orderings. ", "page_idx": 4}, {"type": "text", "text": "We first look at log-likelihoods and MAE for A-LLMP and I-LLMP using the random and distance training point orderings discussed earlier. Results can be seen in Figure 4 (left). Similar to our findings earlier, ordering the training values according to distance to target has a large effect, improving performance for both I-LLMP and A-LLMP. Unsurprisingly, the richer joint distribution given by A-LLMP gives us better predictive performance. ", "page_idx": 4}, {"type": "text", "text": "We next examine the variability in performance of A-LLMP when different autoregressive target orderings are used to get a sense of how far our method is from a stochastic process (which would be permutation invariant in the target points). The results of using ten sets of randomly ordered target points compared to I-LLMP and the ground truth log-likelihood of the test sample under the generative distribution are presented in Figure 4 (center). Note that the training data is distance sorted in all cases. We also present the result when ordering target points according to distance to the closest training point, from smallest to largest. We make three key observations: first, log-likelihood performance of all A-LLMP orderings is better than I-LLMP. Second, the variance of random orderings is small on the scale of the log-likelihood of the generative model. And third, distance ordering the targets gives better or at least competitive performance with a random ordering. These results present practitioners a choice: do you care more about using a valid statistical process or obtaining good predictive performance? If it is the latter, you would be better served using A-LLMP. ", "page_idx": 4}, {"type": "text", "text": "4 Evaluating LLMP Performance on Numerical Data ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we evaluate the performance of LLMPs on purely numerical data in a wide variety of settings. Additional details and results for experiments in this section can be found in Appendix H. ", "page_idx": 4}, {"type": "text", "text": "1D Synthetic Data Experiments. To show that LLMPs are a viable regression model with wellcalibrated uncertainties, we benchmark in Table 1 our A-LLMP method against a GP on the Function Dataset (Appendix D.1). The GP uses an RBF kernel with optimized length scale and noise. The Mixtral- $\\mathbf{\\nabla}8\\!\\times\\!7\\mathbf{B}$ A-LLMP achieves the lowest negative log-likelihoods averaged over 7 function sizes and 3 seeds on 10 out of 12 of the functions and equal or better MAE on 8 of the functions. Visualizations of the predictive distributions and plots of MAE and A-LLMP are shown in Appendix H.1. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "table", "img_path": "HShs7q1Njh/tmp/ccdd960042bddd31808feda5ee9d91ccb6c21f12146403bf41ef64f2fc5927f3.jpg", "table_caption": ["Table 1: Mean and standard error of MAE and NLL averaged over over the seven training set sizes and 3 seeds of each function for Mixtral- $\\mathbf{\\nabla}8\\!\\times\\!7\\mathbf{B}$ A-LLMP and a GP with an RBF kernel. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "To verify that LLMPs are able to produce non-Gaussian, multimodal predictive distributions we sampled training data from synthetic, multimodal generative distribution (experimental details in Appendix H.2). The Llama-3-70B LLMP predictive distribution is visualized in Figure 4 (right). ", "page_idx": 5}, {"type": "text", "text": "Comparison to LLMTime. Figure 5 demonstrates that A-LLMP yields superior results in terms of MAE and NLL when compared to LLMTime using Llama-2-7B on a forecasting task using the weather dataset (described in Appendix D.2). Additional plots with missing training data are in Appendix H.3. We posit that A-LLMP betters LLMTime due to the fact that 1) A-LLMP naturally handles irregularly spaced $x$ and $y$ data whereas LLMTime uses only regularly spaced $y$ information requiring imputation with NaN values where data is missing; and 2) A-LLMP performs no scaling on $y$ values in contrast to LLMTime that scales data to eliminate the use of decimals and normalize the range of the data and as a result removes information that the LLM can potentially leverage. ", "page_idx": 5}, {"type": "image", "img_path": "HShs7q1Njh/tmp/87e92976273362441291ec130f4696cbfad36a1c4e952cae4e06bf8eadc6680d.jpg", "img_caption": ["Figure 5: Comparison of A-LLMP and LLMTime on the weather dataset. Left: Plot using all 50 training points. Right: Plot of MAE and NLL versus the amount of training data removed. A-LLMP has lower MAE and NLL and the margin over LLMTime increases as more training data is removed. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Comparison to From Words to Numbers. We compare our I-LLMP method to the approach in [11] on their Original #1 dataset. The experimental set-up is as follows: There are 100 trials with each trial consisting of 50 training points and a single target point. The training and target points for each trial are randomly generated using the function described in [11]. We use the code from their paper to generate the data and evaluate their approach and compare it to ours using identical numerical data. We use the Llama-2-7B LLM for both methods to ensure a fair comparison. I-LLMP achieved lower MAE on 78 of the 100 trials when compared to their method. When the errors are averaged over the 100 trials, the I-LLMP average error was 0.836 and theirs was 3.137. These results indicate that our LLMP approach is clearly superior. This is due to the facts that (i) we sort the training points according to distance to the current target point when creating the prompt whereas they do not, and (ii) we form a distributional estimate for the predicted point and then take the median sample value as the best estimate, whereas they generate a single point estimate. ", "page_idx": 5}, {"type": "text", "text": "In the next three experiments we showcase the ability of LLMPs to handle multi-dimensional data. ", "page_idx": 5}, {"type": "text", "text": "Image Reconstruction As a 2-dimensional input experiment, Figure 6 shows reconstruction results from images drawn from the Fashion-MNIST dataset [12]. We convert pixel data into prompt data points by forming a series of (row, column, pixel value) tuples. Additional results and details are in Appendix H.4. Using $20\\%$ train pixels, the basic form is captured and at $50\\%$ , the reconstruction is accurate despite the sharp pixel intensity transitions. ", "page_idx": 5}, {"type": "image", "img_path": "HShs7q1Njh/tmp/1e127d86cd678b3255b3f87bd59113105180419b553eaae08444d3a8bb0ca082.jpg", "img_caption": ["Figure 6: Fashion-MNIST Mixtral image reconstruction results. The blue pixels indicate unobserved. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Black-Box Function Optimization Black-box optimization involves minimizing or maximizing a function where there is only access to the output of a function for a specified input. We benchmark the ability of LLMPs to perform maximization on six commonly used multi-dimensional functions. We compare our results using Llama-2-7B to Optuna [3], a commercial hyperparameter optimization framework. Results and implementation details are in Appendix H.5. In all cases, LLMPs obtain as good or better approximation to the true maximum value in a fewer number of trials. ", "page_idx": 6}, {"type": "text", "text": "Simultaneous Temperature, Rainfall, and Wind Speed Regression To examine how well an LLMP can model multi-dimensional outputs, we compare LLMP regression to a multi-output GP on the weather dataset described in Appendix D.2. Figure 7 shows the results for the Llama-3-8B LLM (top) and a 3 output RBF kernel GP with trained hyperparameters (bottom). The LLM is similar to and in most cases better than the GP in terms of MAE and NLL. ", "page_idx": 6}, {"type": "image", "img_path": "HShs7q1Njh/tmp/a2c7c2cca3b2eaa662e786bf13f77bf28ad8abf685f5f145076f5cfd946cf47d.jpg", "img_caption": ["Figure 7: Results for simultaneously predicting temperature, precipitation, and wind speed using the Llama-3-7B LLM (top) and a 3 output RBF kernel GP with trained hyperparameters (bottom). "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "In-context Learning Using Related Data Examples. In this experiment, we investigate LLMPs\u2019 ability to learn from similar examples in-context to predict average monthly precipitation across 13 Canadian locations [13], one from each province and territory. For each location, we use the Mixtral- $\\mathbf{8}{\\times}7\\mathbf{B}$ A-LLMP to forecast 32 months of average precipitation values given the previous four month observations taken from a random historical three-year period between 1913-2017 (conditional on data availability). It is then provided with 1-12 examples of random three year periods of historical values from the same location in-context. Results shown in Figure 8 and experimental details in Appendix H.6. Conditioning the LLMP on historical examples improves performance saturating after 4 years, and degrading slightly thereafter. Generally, the LLMP is able to use the examples to pick up on seasonal trends from history. We note that some locations do not have obvious or strong seasonal patterns but examples still help performance in these cases (see Appendix H.6). ", "page_idx": 6}, {"type": "text", "text": "5 Conditioning LLMPs on Textual Information ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "One of the most exciting directions of LLMPs is the potential to incorporate prior information about problems via text. Now that we can examine functional predictive distributions of LLMs, we can begin to explore their rich prior over functions by conditioning on both text and numerical data. In this section we present two experiments with details and additional experiments presented in Appendix I. ", "page_idx": 6}, {"type": "text", "text": "Scenario-conditional Predictions. In this experiment, we examine the influence of text providing information about various synthetic problem settings on the predictive distribution of an LLMPs. In ", "page_idx": 6}, {"type": "image", "img_path": "HShs7q1Njh/tmp/6a07780d22beb4c04bec22801fff9c19059a39798cf87f5de3058605c38fd61a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 8: (Left three plots) Visualizations of the predictions given by the Mixtral- $\\mathbf{\\nabla}\\cdot8\\times7\\mathbf{B}$ LLMP for Ranfurly, Alberta. Blue and black circles are training and test points, respectively. Red circles are median predictions and shaded areas indicate tenth-percentiles over 30 samples. (Right) NLL vs number of examples. Error bars show standard error over 13 locations. ", "page_idx": 6}, {"type": "image", "img_path": "HShs7q1Njh/tmp/7a7489cb98cf8650e43ab70271b9a3588df89f6230561222a3ede064860d48ba.jpg", "img_caption": ["(d) Monthly precip. in Singapore (e) Monthly precip. in San Diego (f) Actual monthly averages "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 9: a)-e) predictive distributions from an A-LLMP using Llama-3-70B under various scenario prompts. Black points are two training points given to the LLM process, the same values for each scenario. The tenth-percentiles from 50 samples are visualized in faded blue and the median is presented in dark blue with five random samples shown in various colours. Figure f) shows the actual average monthly rainfall for Singapore from 1991-2020 [14] and San Diego from 2000-2024 [15]. ", "page_idx": 7}, {"type": "text", "text": "all of the following examples, we provide the same two synthetic training points to the LLMP but change the prompting text that comes before the training data. We then use A-LLMP with Llama-3- 70B to forecast trajectories 50 steps ahead. We begin by examining the predictive distribution with no prompt (Figure 9a). We prompt the LLMP to generate daily temperature measurements in degrees Celsius from Montreal in January (Figure 9b) and May (Figure 9c), and monthly precipitation values from San Diego, CA (Figure 9d) and Singapore (Figure 9e). Figure 1 Shows the results of prompting the LLMP to generate (left) a stock price financial time series (centre) for a company that eventually goes out of business and (right) for a company whose price goes to zero on day 30. ", "page_idx": 7}, {"type": "text", "text": "Indeed, the LLMP modifies the predictive distribution accordingly relative to the no prompt predictions. We highlight the following observations: first, for prompts b) and c), the model moves about half of its predictive mass below zero for temperatures beginning in January and above zero for the May temperatures. Second, the LLMP is able to recall actual historical trends for average monthly precipitation for Singapore and San Diego to condition on prompts d) and e). Despite getting the trend correct, we note that the median prediction in d) seems to be biased toward the training values and not reflective of the actual monthly median. ", "page_idx": 7}, {"type": "text", "text": "Last, for stock price simulations, the model places all of its density on positive numbers since it is modelling prices. It is able to produce realistic trajectories and decreases them in expectation when prompted that the company goes out of business. The model is able to condition on the fact that the price goes to zero on day 30 which correctly interprets the meaning of the $x$ -values as days starting from 0, that the $y$ -axis is the price and the phrase \u201cprice goes to zero\u201d corresponds to a $y$ -value of 0. ", "page_idx": 7}, {"type": "text", "text": "Labelling Features Using Text. In the following example, we examine the performance of a Mixtral- $\\mathbf{\\nabla}{8\\mathbf{x}7\\mathbf{B}}$ Instruct I-LLMP on predicting American housing prices. The dataset [16] contains 39980 housing prices and various variables around housing and demographics for the top 50 American cities by population. Note that this dataset was generated on 12/09/2023, however it contains data from the 2020 US Census and the 2022 American Community Survey (ACS) so we cannot guarantee that models did not see data within this dataset during training. ", "page_idx": 7}, {"type": "text", "text": "For each prediction task, we show the I-LLMP 10 randomly selected training examples from the dataset and predict on 20 randomly selected test examples. In the prompt, before the numerical value (price) we provide a string which encodes the datapoint index/features that the model can use. For our first experiment we examine the behaviour of the LLMP when more features are added to the prompt. We experiment with five ways of indexing the training and test points; For case (1), we provide latitude and longitude of the house as numerical values (eg. 32.74831, -97.21828) converted to strings similar to our method in previous experiments. For the remaining 4 cases, we provide additional labeled features, adding more features for each case with the prompt for case (5) containing all labelled features, illustrated with the following example: (2) Location: Fort Worth, Texas, Latitude: 32.74831, Longitude: -97.21828, (3) Zip Code: 76112, Median Household Income: 71452.0, (4) Zip Code Population: 42404 people, Zip Code Density: 1445.0 people per square mile, (5) Living Space: 1620 square feet, Number of Bedrooms: 3, Number of Bathrooms: 2. ", "page_idx": 7}, {"type": "image", "img_path": "HShs7q1Njh/tmp/d9374446e71370e80ea528ac55556ed565f526daa1a7037f6263e7005457f234.jpg", "img_caption": ["Figure 10: Results of a Mixtral- $\\mathrm{8x7B}$ Instruct I-LLMP predicting US housing prices. Left: Predictions for 10 randomly selected houses using index style 1) and 5). Xs are mean predictions using 30 samples from the LLMP and error bars indicate 2 standard deviations. Centre and right: Average MAE and NLL performance of the LLMP over 10 experiments with error bars representing the standard error for experiments from Section 5. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "This procedure is repeated 10 times to compute statistics. Results are presented in Figure 10 (left, centre). Note that the LLMP is able to take advantage of the additional features provided to improve predictive performance. To see examine the effect of adding text labels to the features, we ran another set of experiments on 10 new random datasets providing the LLMP with either labeled or unlabelled numerical features. The following are example feature strings: (i) \u201c30.45738, -97.75516\u201d (ii) \u201cLocation: Austin, Texas, Latitude: 30.45738, Longitude: -97.75516\u201d (iii) \u201c30.45738, -97.75516, 78729, 107830.0, 30907, 1216.1, 1349, 3\u201d (iv) \u201cLocation: Austin, Texas, Latitude: 30.45738, Longitude: -97.75516, Zip Code: 78729, Median Household Income: 107830.0, Zip Code Population: 30907 people, Zip Code Density: 1216.1 people per square mile, Living Space: 1349 square feet, Number of Bedrooms: 3, Number of Bathrooms: $2^{\\bullet}$ . Results of this experiment are presented in Figure 10 (right). Note that the LLMP is not able to use the raw feature values to improve performance from only 10 training examples, but is able to do so with labelled features suggesting that LLM is able to utilize the latent relationship between the feature and the price once the feature is identified. We found that the Mixtral- $\\mathbf{\\nabla}\\cdot8\\times7\\mathbf{B}$ Instruct model had the best performance on this task and was able to utilize text information better (results for other models in Appendix I.2). ", "page_idx": 8}, {"type": "text", "text": "6 Related Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we discuss work related to eliciting distributions from LLMs including forecasting, regression, in-context learning, and nearal processes among others. ", "page_idx": 8}, {"type": "text", "text": "LLM Forecasting The most closely related work to ours is LLMTime [2]. LLMTime is capable of zero-shot extrapolation of one-dimensional time series data at a level comparable to trained purposebuilt approaches. In addition, they develop a method for eliciting marginal probability distribution functions from LLM posteriors over functions, which we build on. They also begin to investigate the effect of conditioning on text. In contrast, we focus on (i) interpolation with multi-dimensional inputs and outputs; (ii) eliciting joint distributions over functions, not just marginals; and (iii) exploring the ability of models to condition simultaneously on both numerical data and text. More recently, TimesFM [17], a foundation model for one-dimensional zero-shot times series forecasting was introduced. However, TimesFM does not support interpolation or higher dimensional data and does not consider distributions. PromptCast [18] performs zero-shot time series forecasting by combining numerical data and text in a question answer format. Our approach for combining problem specific text along with numerical data differs in that it handles both interpolation and extrapolation and does not rely on a question-answer format. Hegselmann et al. [19] utilize LLMs to do zero-shot and few-shot classification on tabular data that compares favorably to standard ML approaches. ", "page_idx": 8}, {"type": "text", "text": "LLM Regression Pesut [20] do some initial investigations into the use of LLMs as regressors on 1D synthetic functions. Our work greatly expands on these early investigations. Vacareanu et al. [11] is concurrent work that shows that LLMs are capable linear and non-linear regressors. However, their work does not condition on any textual information, compute log probabilities, compare to Gaussian Processes, investigate the effect of prompt formatting, or employ auto-regressive sampling. ", "page_idx": 8}, {"type": "text", "text": "In-context learning (ICL) in LLMs Xie et al. [21] point out that ICL can be seen as being equivalent to Bayesian inference in a latent variable model. More recently, [22] explain in-context learning in LLMs as kernel regression. Garg et al. [23] train transformers to do in-context learning on various function classes including linear (up to 50 dimensions), decision trees, and two-layer ReLU networks. Coda-Forno et al. [24] demonstrate that LLMs are capable of meta-in-context learning and that performance on 1-D linear regression and two-armed bandit tasks improves with multiple examples. TabPFN [25] is a trained transformer that is able to do tabular classification given in-context examples. ", "page_idx": 9}, {"type": "text", "text": "LLM Hyperparameter Optimization Zhang et al. [26] and Liu et al. [27] use LLMs to perform hyperparameter optimization, showing that LLMs can condition on a mixture of textual data as numerical observations to effectively optimize hyperparameters in machine learning models. ", "page_idx": 9}, {"type": "text", "text": "Eliciting priors from LLMs Binz and Schulz [28] fine-tune LLMs on data from psychological experiments to achieve accurate representations of human behavior. Choi et al. [1] show how using an LLM to assess the importance of features or the causal relationship between variables that can improve performance on tasks. Lipkin et al. [29] find that LLMs can derive human-like distributions over the interpretations of complex pragmatic utterances. ", "page_idx": 9}, {"type": "text", "text": "Eliciting distributions from humans Schulz et al. [30] look at compositional inductive biases in function learning, showing humans have compositional structure in their priors on functions. [31] catalogue standard strategies for eliciting distributions from expert humans. ", "page_idx": 9}, {"type": "text", "text": "Neural processes Neural Processes are a class of meta-learning models trained to learn a map from training (context) sets to predictive distributions, $p_{\\theta}(y_{1}^{*},\\l,\\l.\\l.\\l,y_{N}^{*}\\mid x_{1}^{*},\\l.\\l.\\l,x_{N}^{*},D_{\\mathrm{train}})$ . These models are parameterized using a neural network and there have been various proposals for different architectures using attention [32], transformers [33], Gaussian Process output layers [34], and diffusion models [35]. The definitions of the joint distributions in equations 1 and 2 take inspiration from the joint distributions defined by Conditional Neural Processes [5] as independent marginals conditioned on the training/context set and Autoregressive Neural Processes [6] utilizing the chain rule of probability, respectively. Through this lens, LLMPs can be viewed as examples of Neural Processes. LLMPs differ from standard NPs in two main ways: (i) Training objective: Neural Processes are meta-trained using maximum likelihood to optimize $p(y^{*}|x^{*},D_{\\mathrm{train}})$ directly. LLMPs have a very indirect training procedure \u2013 they are trained to be language models i.e. autoregressive token predictors. One of the contributions of this paper is the demonstration that, despite this, they can perform zero-shot probabilistic regression. (ii) Architecture: NPs have an output layer that parametrizes the predictive distribution over targets directly. Since LLMPs are repurposing language models for regression, we need to define the mapping from distributions over language tokens to distributions over target variables. We note that LLMs themselves can be viewed as AR-CNPS [6] with a fixed, predefined target ordering. ", "page_idx": 9}, {"type": "text", "text": "7 Discussion, Limitations, and Societal Impact ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Below we discuss our findings, the limitations and societal impact of the work presented. Further discussion on these issues can be found in Appendix J. ", "page_idx": 9}, {"type": "text", "text": "Discussion We defined LLMPs for eliciting numerical predictive distributions from LLMs and when used as a zero-shot muti-dimensional regression model are competitive with GPs. Excitingly, we demonstrated the ability to condition on text to improve predictions and probe the LLMs\u2019 hypothesis space. An interesting extension would be to condition on other modalities in addition to text. ", "page_idx": 9}, {"type": "text", "text": "Limitations Along with the flexibility of LLMs, LLMPs inherit their drawbacks. Maximum context sizes limit the size of tasks we can apply this method to and the amount of textual information we can condition on. LLMPs are also significantly more computationally expensive compared to Gaussian Processes and standard regression methods. All of experiments were performed on readily available open source LLMs that are smaller and generally less capable compared to proprietary LLMs. ", "page_idx": 9}, {"type": "text", "text": "Societal Impact Our work has demonstrated a new and useful zero-shot approach for generating probabilistic predictions using plain language to augment numerical data. It has the potential to allow practitioners from fields such as medical research and climate modelling to more easily access probabilistic modelling and machine learning. Like all machine learning technology, there is potential for abuse, and possible consequences from incorrect predictions made with LLMPs. Also, we do not know the biases in the underlying LLMs used and what effect they may have on LLMPs output. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "James Requeima and David Duvenaud acknowledge funding from the Data Sciences Institute at the University of Toronto and the Vector Institute. Dami Choi was supported by the Open Phil AI Fellowship. Richard E. Turner is supported by Google, Amazon, ARM, Improbable, and the EPSRC Probabilistic AI Hub (ProbAI, EP/Y028783/1). ", "page_idx": 10}, {"type": "text", "text": "We thank Anna Vaughan for help with the weather datasets and discussions. We also thank Will Tebbutt, Matthew Ashman, Stratis Markou, and Aristeidis Panos for helpful comments and suggestions. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Kristy Choi, Chris Cundy, Sanjari Srivastava, and Stefano Ermon. Lmpriors: Pre-trained language models as task-specific priors. arXiv preprint arXiv:2210.12530, 2022.   \n[2] Nate Gruver, Marc Finzi, Shikai Qiu, and Andrew Gordon Wilson. Large language models are zero-shot time series forecasters. arXiv preprint arXiv:2310.07820, 2023. [3] Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. Optuna: A next-generation hyperparameter optimization framework. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, pages 2623\u20132631, 2019.   \n[4] Bernt Oksendal. Stochastic differential equations: an introduction with applications. Springer Science & Business Media, 2013.   \n[5] Marta Garnelo, Dan Rosenbaum, Christopher Maddison, Tiago Ramalho, David Saxton, Murray Shanahan, Yee Whye Teh, Danilo Rezende, and SM Ali Eslami. Conditional neural processes. In International conference on machine learning, pages 1704\u20131713. PMLR, 2018.   \n[6] Wessel P Bruinsma, Stratis Markou, James Requiema, Andrew YK Foong, Tom R Andersson, Anna Vaughan, Anthony Buonomo, J Scott Hosking, and Richard E Turner. Autoregressive conditional neural processes. arXiv preprint arXiv:2303.14468, 2023.   \n[7] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024. [8] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \n[9] AI@Meta. Llama 3 model card. 2024. URL https://github.com/meta-llama/llama3/ blob/main/MODEL_CARD.md.   \n[10] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. In International Conference on Learning Representations, 2020.   \n[11] Robert Vacareanu, Vlad-Andrei Negru, Vasile Suciu, and Mihai Surdeanu. From words to numbers: Your large language model is secretly a capable regressor when given in-context examples. arXiv preprint arXiv:2404.07544, 2024.   \n[12] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms, 2017.   \n[13] Environment and Climate Change Canada. Monthly total of daily adjusted total precipitation. Online, 2024. URL https: //www.canada.ca/en/environment-climate-change/services/ climate-change/science-research-data/climate-trends-variability/ adjusted-homogenized-canadian-data/precipitation-access.html. Accessed: April 2024, Last updated: 2017-08-09.   \n[14] University of East Anglia Climatic Research Unit. Observed Historical Climate Data for Singapore. World Bank Climate Knowledge Portal, 2024. URL https://climateknowledgeportal.worldbank.org/country/singapore/ climate-data-historical. Accessed: 2024-05-06.   \n[15] Climate Data. National Weather Service, 2024. URL https://www.weather.gov/wrh/ Climate?wf $\\scriptstyle0=\\mathtt{s g x}$ . Accessed: 2024-05-06.   \n[16] Jeremy Larcher. American house prices, 2023. URL https://www.kaggle.com/dsv/ 7162651.   \n[17] Abhimanyu Das, Weihao Kong, Rajat Sen, and Yichen Zhou. A decoder-only foundation model for time-series forecasting. arXiv preprint arXiv:2310.10688, 2023.   \n[18] Hao Xue and Flora D Salim. Promptcast: A new prompt-based learning paradigm for time series forecasting. IEEE Transactions on Knowledge and Data Engineering, 2023.   \n[19] Stefan Hegselmann, Alejandro Buendia, Hunter Lang, Monica Agrawal, Xiaoyi Jiang, and David Sontag. Tabllm: Few-shot classification of tabular data with large language models. In International Conference on Artificial Intelligence and Statistics, pages 5549\u20135581. PMLR, 2023.   \n[20] Lovre Pesut. Who models the models that model models? an exploration of gpt-3\u2019s in-context model fitting ability. URL https://www. alignmentforum. org/posts/c2RzFadrxkzyRAFXa/whomodels-the-models-that-model-models-an-exploration-of, 2022.   \n[21] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. arXiv preprint arXiv:2111.02080, 2021.   \n[22] Chi Han, Ziqi Wang, Han Zhao, and Heng Ji. Explaining emergent in-context learning as kernel regression. arXiv preprint arXiv:2305.12766, 2023.   \n[23] Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers learn in-context? a case study of simple function classes. Advances in Neural Information Processing Systems, 35:30583\u201330598, 2022.   \n[24] Julian Coda-Forno, Marcel Binz, Zeynep Akata, Matthew Botvinick, Jane X Wang, and Eric Schulz. Meta-in-context learning in large language models. arXiv preprint arXiv:2305.12907, 2023.   \n[25] Noah Hollmann, Samuel M\u00fcller, Katharina Eggensperger, and Frank Hutter. TabPFN: A transformer that solves small tabular classification problems in a second. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview. net/forum?id $=$ cp5PvcI6w8_.   \n[26] Michael Zhang, Nishkrit Desai, Juhan Bae, Jonathan Lorraine, and Jimmy Ba. Using large language models for hyperparameter optimization. In NeurIPS 2023 Foundation Models for Decision Making Workshop, 2023.   \n[27] Tennison Liu, Nicol\u00e1s Astorga, Nabeel Seedat, and Mihaela van der Schaar. Large language models to enhance bayesian optimization. arXiv preprint arXiv:2402.03921, 2024.   \n[28] Marcel Binz and Eric Schulz. Turning large language models into cognitive models. arXiv preprint arXiv:2306.03917, 2023.   \n[29] Benjamin Lipkin, Lionel Wong, Gabriel Grand, and Joshua B Tenenbaum. Evaluating statistical language models as pragmatic reasoners. arXiv preprint arXiv:2305.01020, 2023.   \n[30] Eric Schulz, Joshua B Tenenbaum, David Duvenaud, Maarten Speekenbrink, and Samuel J Gershman. Compositional inductive biases in function learning. Cognitive psychology, 99: 44\u201379, 2017.   \n[31] Bogdan Grigore, Jaime Peters, Christopher Hyde, and Ken Stein. Methods to elicit probability distributions from experts: a systematic review of reported practice in health technology assessment. Pharmacoeconomics, 31:991\u20131003, 2013.   \n[32] Hyunjik Kim, Andriy Mnih, Jonathan Schwarz, Marta Garnelo, Ali Eslami, Dan Rosenbaum, Oriol Vinyals, and Yee Whye Teh. Attentive neural processes. arXiv preprint arXiv:1901.05761, 2019.   \n[33] Tung Nguyen and Aditya Grover. Transformer neural processes: Uncertainty-aware meta learning via sequence modeling. arXiv preprint arXiv:2207.04179, 2022.   \n[34] Stratis Markou, James Requeima, Wessel Bruinsma, and Richard Turner. Efficient gaussian neural processes for regression. arXiv preprint arXiv:2108.09676, 2021.   \n[35] Vincent Dutordoir, Alan Saul, Zoubin Ghahramani, and Fergus Simpson. Neural diffusion processes. In International Conference on Machine Learning, pages 8990\u20139012. PMLR, 2023.   \n[36] OpenWeather. Weather API, 2024. URL https://openweathermap.org/api. Accessed: 2024-03-07.   \n[37] Jacob R Gardner, Geoff Pleiss, David Bindel, Kilian Q Weinberger, and Andrew Gordon Wilson. Gpytorch: Blackbox matrix-matrix gaussian process inference with gpu acceleration. In Advances in Neural Information Processing Systems, 2018.   \n[38] William R Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25(3-4):285\u2013294, 1933.   \n[39] William R Thompson. On the theory of apportionment. American Journal of Mathematics, 57 (2):450\u2013456, 1935.   \n[40] Matthew W. Hoffman and Bobak Shahriari. benchfunk. https://github.com/mwhoffman/ benchfunk, 2015.   \n[41] Robert B Gramacy and Herbert KH Lee. Cases for the nugget in modeling computer experiments. Statistics and Computing, 22:713\u2013722, 2012. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "In this section we elaborate on the explanations and definitions in Section 2. Our goal is to use an LLM to elicit joint predictive distribution over arbitrary sized target sets that we can guide and modify using plain language. Formally, given a set of observations $\\bar{D_{\\mathrm{train}}}=\\{(x_{i},y_{i})\\}_{i=1}^{M}$ and some text, $T$ , we would like to elicit the predictive distribution defined by an LLM at a collection of tcaarng ectas $\\{(x_{j}^{*},y_{j}^{*})\\}_{j=1}^{N}$ d  dtewnoo tiendt $p_{\\mathrm{LLM}}(y_{1}^{*},\\ldots,y_{N}^{*}\\ |\\ x_{1}^{*},\\ldots,x_{N}^{*},D_{\\mathrm{train}},T)$ i.c tTivoe a dcihsiterivbe utthioe ng odaelf, inwede by an LLM. First, we can interpret the LLM as maintaining having a predictive distribution over numerical values, which we can probe by sampling from the LLM. This interpretation is beneficial if we believe that the LLM has learned useful prior information that we would like to access via its beliefs about these numerical values and for our goal of guiding the predictive distribution using text. The other interpretation is more empirical: we simply use the LLM as a tool to define a valid predictive distribution and evaluate how well this definition performs on test cases. Our approach is a combination of the two philosophies \u2013 we will propose a method defining a predictive distribution that is valid and performs well on test cases, but closely matches what we think of as the LLM\u2019s underlying distribution. ", "page_idx": 13}, {"type": "text", "text": "A.1 Continuous Marginal Likelihoods From an LLM ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "As discussed in Section 2, we use a method similar to the one proposed by Gruver et al. [2]; we approximate the continuous density by discretizing the space using bins with arbitrarily fine precision. Let\u2019s assume a fixed number of decimal places $n$ , and that LLMs generate one digit at a time3. The key idea is that each new digit can be viewed as being generated from a categorical distribution with the probabilities $p$ given by a softmax over numerical tokens. Crucially, this hierarchical approach allows us to compute the probability of a bin with width $10^{-n}$ . For example, if $n\\,=\\,1$ then $\\operatorname*{Pr}\\{y\\,\\in\\,[1.0,1.1)\\}\\,=\\,\\stackrel{\\cdot}{p}(1)p(.|\\bar{1})p(0|1.)$ because $\"1.0\"$ is a prefix for all $y\\,\\in\\,[1.0,1.1)$ . We can convert probability mass to probability density by assuming a uniform distribution within each bin, and dividing the mass by the bin width. A visualization of this construction can be viewed in Appendix G.1. ", "page_idx": 13}, {"type": "text", "text": "The method in [2] has two main shortcomings for our purposes: first, the authors propose to scale all $y\\in D_{\\mathrm{train}}$ to eliminate decimals from their numerical representation. For example, for a precision of 2 decimal places, the numbers 0.123, 1.23, 12.3, and 123.0 will be transformed to 12, 123, 1230, and 12300 respectively. Scaling removes prior information communicated to the LLM via the scale of the problem. For example, it is likely that the LLM has encountered financial data with decimal places. Potentially, it also makes it more difficult to communicate prior information about the problem to the LLM via text. ", "page_idx": 13}, {"type": "text", "text": "Second, probabilities of all sequences of integers given by an LLM contain the mass of all values that also start with that sequence. We can think of this as the problem of not knowing when the LLM intends to terminate a value. For example, if $y=12$ , $\\mathrm{Pr}\\bar{\\{}y\\in[12,13)\\}\\neq p(1)\\bar{p(}2|1)$ since $p(1)p(2|1)$ includes the probability of all numbers with \u201812\u2019 as a prefix \u2013 this includes [12, 13) but also [120, 130), [1200, 1300) and so on. ", "page_idx": 13}, {"type": "text", "text": "A.2 The LLM Process Method ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We follow Gruver et al. [2] and discretize the continuous space with bins of width $10^{-n}$ , computing the probabilities for each bin using the hierarchical softmax approach. However, different from their approach we 1) keep values at their original scale, and 2) include a terminal token after every value \u2013 for example, given a terminal token $\\langle t\\rangle$ , we represent 12 as $12\\langle t\\rangle$ and 120 as $120\\langle t\\rangle$ . Including a terminal token prevents numbers of varying orders of magnitude from sharing the same prefix \u2013 i.e. $p(1)p(2|1)p(\\langle t\\rangle|12)$ no longer includes the probability of numbers in [120, 130), [1200, 1300), and so on. After we compute the mass of a bin via hierarchical softmax, we divide the mass by the bin width $10^{-n}$ to get an estimate of the density value. This procedure defines a valid predictive distribution over $\\mathrm{y}$ -values, and we call this elicitation method \u2018logit-based\u2019 since we derive probabilities from the logits directly instead of sampling. Pseudocode can be found in Algorithm 2. ", "page_idx": 13}, {"type": "text", "text": "It must be noted that this approach does not guarantee that $P(12\\langle t\\rangle)$ yields the mass assigned by the LLM to values in the bin [12, 13). However, we note that our method defines a valid predictive distribution and we empirically observed that our predictive distribution closely matches the sampling distribution to our satisfaction (see Appendix G.1). ", "page_idx": 14}, {"type": "text", "text": "A.3 Defining an LLM Process ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "So far we have established a procedure for defining the predictive distribution at a single target location, $p_{\\mathrm{LLM}}(y_{n}^{*}\\mid x_{n}^{*},D_{\\mathrm{train}},\\bar{T})$ . We now discuss how to define the joint predictive distribution over a collection target points. In particular, we would like to define a stochastic process via its finitedimensional marginal distributions $\\rho_{x_{1},\\ldots,x_{N}}$ defined over locations $x_{1},\\ldots,x_{N}$ . The Kolmogorov Extension Theorem [4] states that such a collection defines a stochastic process if it satisfies ", "page_idx": 14}, {"type": "text", "text": "1. Exchangeability: Given any permutation $\\pi$ of the integers $\\{1,\\ldots,N\\}$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\rho_{x_{1},\\ldots,x_{N}}(y_{1},y_{N})=\\rho_{x_{\\pi(1)},\\ldots,x_{\\pi(N)}}(y_{\\pi(1)},y_{\\pi(N)})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "2. Consistency: if $1\\leq M\\leq N$ then ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\rho_{x_{1},\\ldots,x_{M}}(y_{1},\\ldots,y_{M})=\\int\\rho_{x_{\\pi(1)},\\ldots,x_{\\pi(N)}}(y_{\\pi(1)},y_{\\pi(N)})\\,\\mathrm{d}y_{M+1}\\ldots\\mathrm{d}y_{N}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "In Equation (1) we define a collection of joint distributions by defining a factorized distribution over target locations x1\u2217, . . . , x\u2217N: ", "page_idx": 14}, {"type": "equation", "text": "$$\np_{\\mathrm{I.LLMP}}(y_{1}^{*},\\ldots,y_{N}^{*}\\mid x_{1}^{*},\\ldots,x_{N}^{*},D_{\\mathrm{train}},T)=\\prod_{n=1}^{N}p_{\\mathrm{LLM}}(y_{n}^{*},\\mid x_{n}^{*},D_{\\mathrm{train}},T)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $p_{\\mathrm{LLM}}(y_{n}^{*},\\mid x_{n}^{*},D_{\\mathrm{train}},T)$ is defined above. ", "page_idx": 14}, {"type": "text", "text": "This definition satisfies the Kolmogorov Extension Theorem and so it defines a valid stochastic process. However, it assumes conditional independence given the training set and model weights and, conditional on these variables, the stochastistity represented by the model is via independent marginals. Taking inspiration from the autoregressive structure of the LLMs predictive distribution, we can write the joint distribution according to the product rule: ", "page_idx": 14}, {"type": "equation", "text": "$$\np_{\\mathrm{A}\\cdot\\mathrm{LLMP}}(y_{1}^{*},\\,\\cdot\\,\\cdot\\,,y_{N}^{*}\\ |\\ x_{1}^{*},\\,\\cdot\\,.\\,,x_{N}^{*},D_{\\mathrm{train}},T)=\\prod_{n=1}^{N}p_{\\mathrm{LLM}}(y_{n}^{*}\\ |\\ y_{1}^{*},\\,\\cdot\\,.\\,,y_{n-1}^{*},x_{1}^{*},\\,\\cdot\\,.\\,,x_{n}^{*},D_{\\mathrm{train}},T)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Where, the previous target location is autoregressively added to the conditioning data via the LLM prompt. This should yield much richer predictive distributions as we are now able to model dependencies between output variables. However, this definition is no longer guaranteed to give us a valid stochastic process as the predictive distribution is now target order dependent and most likely will fail the Kolmogorov exchangability condition. We investigate these questions in Section 3. ", "page_idx": 14}, {"type": "text", "text": "B LLM Processes Pseudocode ", "text_level": 1, "page_idx": 15}, {"type": "table", "img_path": "HShs7q1Njh/tmp/b286c0f172ec50a2403139393f2fa49ae41e88ff195db3afc830a94679e87c1f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Algorithm 2 Pseudocode for computing the log pdf of y ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "$n\\leftarrow$ number of digits after decimal point   \nnonnum_idxs $\\leftarrow$ tokens $\\nless$ tokenize([\u20180\u2019, \u20181\u2019, ..., \u20189\u2019, \u2018-\u2019, \u2018.\u2019, \u2018\u27e8t\u27e9\u2019])   \nfull_tex $\\cdot\\leftarrow\\mathrm{prompt}+\\mathrm{str}(y)$   \ny_idxs $\\leftarrow$ indices of the tokens that correspond to y in full_text   \nlogits $\\leftarrow$ model(full_text)   \ny_logits $\\leftarrow$ logits[y_idxs]   \n$\\mathrm{y\\_logits[nonnum\\_idxs]}\\leftarrow-100$   \ny_logpmf $\\leftarrow$ CrossEntropy(logits $=\\mathbf{y}$ _logits[:-1], targets $=$ str(y)[1:]).sum( ) \u25b7Mass of bin that   \nincludes $y$   \n$\\gamma\\mathrm{\\underline{{{\\logpdf}}}}\\gets\\mathrm{y\\underline{{{\\logpmf}}}}+n\\log10$ \u25b7Convert mass to continuous likelihood ", "page_idx": 15}, {"type": "text", "text": "C Sample Prompts ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Figure C.1 depicts three observed training points and four target locations. Below are sample prompts for various configurations discussed in the paper. $T$ refers to problem related text. ", "page_idx": 16}, {"type": "image", "img_path": "HShs7q1Njh/tmp/d4ca465226f9fbc34b214b4f7d4d3195450170465f07b9ca06a22f836c8c6069.jpg", "img_caption": ["Figure C.1: Three observed training points and four target locations which serve as the basis for the example prompts. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Independent Marginal Prompts ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Sequential: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{^{\\ast}\\!{\\mathcal{T}}\\langle t\\rangle A_{x},A_{y}\\langle t\\rangle B_{x},B_{y}\\langle t\\rangle C_{x},C_{y}\\langle t\\rangle D_{x}^{\\ast,\\ast}}\\\\ {^{\\ast}\\!{\\mathcal{T}}\\langle t\\rangle A_{x},A_{y}\\langle t\\rangle B_{x},B_{y}\\langle t\\rangle C_{x},C_{y}\\langle t\\rangle E_{x}^{\\ast,\\ast}}\\\\ {^{\\ast}\\!{\\mathcal{T}}\\langle t\\rangle A_{x},A_{y}\\langle t\\rangle B_{x},B_{y}\\langle t\\rangle C_{x},C_{y}\\langle t\\rangle F_{x}^{\\ast,\\ast}}\\\\ {^{\\ast}\\!{\\mathcal{T}}\\langle t\\rangle A_{x},A_{y}\\langle t\\rangle B_{x},B_{y}\\langle t\\rangle C_{x},C_{y}\\langle t\\rangle G_{x}^{\\ast,\\ast}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Random: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{^{\\langle\\mathcal{T}\\langle t\\rangle C_{x},C_{y}\\langle t\\rangle A_{x},A_{y}\\langle t\\rangle B_{x},B_{y}\\langle t\\rangle D_{x}^{**}}\\\\ {^{\\langle\\mathcal{T}\\langle t\\rangle C_{x},C_{y}\\langle t\\rangle A_{x},A_{y}\\langle t\\rangle B_{x},B_{y}\\langle t\\rangle E_{x}^{**}}\\\\ {^{\\langle\\mathcal{T}\\langle t\\rangle C_{x},C_{y}\\langle t\\rangle A_{x},A_{y}\\langle t\\rangle B_{x},B_{y}\\langle t\\rangle F_{x}^{**}}\\\\ {^{\\langle\\mathcal{T}\\langle t\\rangle C_{x},C_{y}\\langle t\\rangle A_{x},A_{y}\\langle t\\rangle B_{x},B_{y}\\langle t\\rangle G_{x}^{**}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Distance: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{^{\\ast}\\!{\\mathcal{T}}\\langle t\\rangle C_{x},C_{y}\\langle t\\rangle B_{x},B_{y}\\langle t\\rangle A_{x},A_{y}\\langle t\\rangle D_{x}^{\\ast,\\ast}}\\\\ {^{\\ast}\\!{\\mathcal{T}}\\langle t\\rangle C_{x},C_{y}\\langle t\\rangle A_{x},A_{y}\\langle t\\rangle B_{x},B_{y}\\langle t\\rangle E_{x}^{\\ast,\\ast}}\\\\ {^{\\ast}\\!{\\mathcal{T}}\\langle t\\rangle A_{x},A_{y}\\langle t\\rangle C_{x},C_{y}\\langle t\\rangle B_{x},B_{y}\\langle t\\rangle F_{x}^{\\ast,\\ast}}\\\\ {^{\\ast}\\!{\\mathcal{T}}\\langle t\\rangle A_{x},A_{y}\\langle t\\rangle B_{x},B_{y}\\langle t\\rangle C_{x},C_{y}\\langle t\\rangle G_{x}^{\\ast,\\ast}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Autoregressive Prompts ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Sequential: ", "text_level": 1, "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{^{\\ast}T\\langle t\\rangle A_{x},A_{y}\\langle t\\rangle B_{x},B_{y}\\langle t\\rangle C_{x},C_{y}\\langle t\\rangle D_{x}^{\\ast\\ast}}\\\\ &{^{\\ast}T\\langle t\\rangle A_{x},A_{y}\\langle t\\rangle B_{x},B_{y}\\langle t\\rangle C_{x},C_{y}\\langle t\\rangle D_{x}^{\\ast},D_{y}^{\\ast}\\langle t\\rangle E_{x}^{\\ast\\ast}}\\\\ &{^{\\ast}T\\langle t\\rangle A_{x},A_{y}\\langle t\\rangle B_{x},B_{y}\\langle t\\rangle C_{x},C_{y}\\langle t\\rangle D_{x}^{\\ast},D_{y}^{\\ast}\\langle t\\rangle E_{x}^{\\ast},E_{y}^{\\ast}\\langle t\\rangle F_{x}^{\\ast\\ast}}\\\\ &{^{\\ast}T\\langle t\\rangle A_{x},A_{y}\\langle t\\rangle B_{x},B_{y}\\langle t\\rangle C_{x},C_{y}\\langle t\\rangle D_{x}^{\\ast},D_{y}^{\\ast}\\langle t\\rangle E_{x}^{\\ast},E_{y}^{\\ast}\\langle t\\rangle F_{x}^{\\ast},F_{y}^{\\ast}\\langle t\\rangle G_{x}^{\\ast\\ast}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Random: ", "text_level": 1, "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{^{\\ast}T\\langle t\\rangle C_{x},C_{y}\\langle t\\rangle A_{x},A_{y}\\langle t\\rangle B_{x},B_{y}\\langle t\\rangle D_{x}^{\\ast\\ast}}\\\\ &{^{\\ast}T\\langle t\\rangle C_{x},C_{y}\\langle t\\rangle A_{x},A_{y}\\langle t\\rangle B_{x},B_{y}\\langle t\\rangle D_{x}^{\\ast},D_{y}^{\\ast}\\langle t\\rangle E_{x}^{\\ast\\ast}}\\\\ &{^{\\ast}T\\langle t\\rangle C_{x},C_{y}\\langle t\\rangle A_{x},A_{y}\\langle t\\rangle B_{x},B_{y}\\langle t\\rangle D_{x}^{\\ast},D_{y}^{\\ast}\\langle t\\rangle E_{x}^{\\ast},E_{y}^{\\ast}\\langle t\\rangle F_{x}^{\\ast\\ast}}\\\\ &{^{\\ast}T\\langle t\\rangle C_{x},C_{y}\\langle t\\rangle A_{x},A_{y}\\langle t\\rangle B_{x},B_{y}\\langle t\\rangle D_{x}^{\\ast},D_{y}^{\\ast}\\langle t\\rangle E_{x}^{\\ast},E_{y}^{\\ast}\\langle t\\rangle F_{x}^{\\ast},F_{y}^{\\ast}\\langle t\\rangle G_{x}^{\\ast\\ast}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Distance: ", "text_level": 1, "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{^{\\ast}T\\langle t\\rangle C_{x},C_{y}\\langle t\\rangle B_{x},B_{y}\\langle t\\rangle A_{x},A_{y}\\langle t\\rangle D_{x}^{\\ast\\ast}}\\\\ &{^{\\ast}T\\langle t\\rangle C_{x},C_{y}\\langle t\\rangle D_{x}^{\\ast},D_{y}^{\\ast}\\langle t\\rangle A_{x},A_{y}\\langle t\\rangle B_{x},B_{y}\\langle t\\rangle E_{x}^{\\ast\\ast}}\\\\ &{^{\\ast}T\\langle t\\rangle D_{x}^{\\ast},D_{y}^{\\ast}\\langle t\\rangle A_{x},A_{y}\\langle t\\rangle E_{x}^{\\ast},E_{y}^{\\ast}\\langle t\\rangle C_{x},C_{y}\\langle t\\rangle B_{x},B_{y}\\langle t\\rangle F_{x}^{\\ast\\ast}}\\\\ &{^{\\ast}T\\langle t\\rangle D_{x}^{\\ast},D_{y}^{\\ast}\\langle t\\rangle A_{x},A_{y}\\langle t\\rangle E_{x}^{\\ast},E_{y}^{\\ast}\\langle t\\rangle B_{x},B_{y}\\langle t\\rangle F_{x}^{\\ast},F_{y}^{\\ast}\\langle t\\rangle C_{x},C_{y}\\langle t\\rangle G_{x}^{\\ast\\ast}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "D Dataset Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "This section provides details on the various datasets used in the experiments ", "page_idx": 18}, {"type": "text", "text": "D.1 Function Dataset ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We use the 12 synthetic function datasets (Linear, Exponential, Sigmoid, Log, Sine, Beat Inference, Linear $^+$ Cosine, Linear $\\times$ Sine, Gaussian Wave. Sinc, Quadratic, $\\mathrm{X}\\times\\mathrm{Sine})$ from Gruver et al. [2] each of which consists of 200 discrete points. We construct 7 datasets each with 10 random seeds for each function with a subset of 5, 10, 15, 20, 25, 50, and 75 randomly training points sampled from the original 200 points. We add Gaussian noise with $\\mu=0$ and $\\sigma=0.05$ to the training points and then round the values to 2 decimal places. Unless otherwise stated, we use 40 equally spaced target points to sample at. ", "page_idx": 18}, {"type": "text", "text": "D.2 Weather Dataset ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The dataset was queried from OpenWeather [36] and consists of daily high temperature, precipitation, and wind speed readings for 86 consecutive days from London, UK commencing on December 12, 2023. The data was recorded after the release dates of the Llama-2 and Mixtral- $\\mathbf{\\cdot8x7B}$ LLM release dates to avoid any data leakage into the LLM datasets. ", "page_idx": 18}, {"type": "text", "text": "For the \"Comparison to LLMTime\" experiment, We used the first 50 readings of the temperature data for training data and ask LLMTime and LLMPs to predict/forecast the final 36 values. The authors of LLMTime suggest the method can handle missing values by inputting NaN values in their place. Since LLMPs can work with irregularly spaced and missing data, we also compare the methods with a reduced number of randomly spaced training points. ", "page_idx": 18}, {"type": "text", "text": "For the \"Simultaneous Temperature, Rainfall, and Wind Speed Regression\" experiment we used 30 randomly chosen training points within the first 76 points, leaving the last 10 for extrapolation. ", "page_idx": 18}, {"type": "text", "text": "E Data Leakage ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "It is likely that LLMs used in our experiments have been exposed during training to some of the real-world data that we use in our experiments which would give it an advantage against other models. However, we feel confident that the LLMs tested were not simply recalling memorized data \u2013 note that in all cases the LLMPs produces a full distribution and not just a deterministic value \u2013 and we have taken steps in our experiments to mitigate this issue. When synthetic functions or Fashion MNIST data [12] is used, we have altered the original data via subsampling, rescaling and in some cases adding noise to the datapoints. Any data used from the internet was altered from its original form when given to the model. Some datasets (in particular the Weather Dataset described in Appendix D.2), were explicitly chosen to be recorded after the release dates of the LLMs that they were evaluated on. ", "page_idx": 18}, {"type": "text", "text": "F Additional Implementation Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "PyTorch is used as the basis for all of the experiments, with the exception of the Gaussian Processes baselines that are implemented using the GPyTorch package [37]. ", "page_idx": 19}, {"type": "text", "text": "The experiments using the Mixtral ${\\bf8}\\!\\times\\!7{\\bf B}$ , Mixtral- ${\\bf8}\\!\\times\\!7{\\bf B}$ -Instruct [7], Llama-2 70B [8], and Llama-3 70B [9] LLMs were run on two NVidia A100 GPUs with 80 GB of memory. The experiments using the Llama-2 7B [8] and Llama-3 8B [9] LLMs were run on one NVidia 3090 GPU with 24 GB of memory. The total compute used in the paper exceeded 600 GPU hours. ", "page_idx": 19}, {"type": "text", "text": "No training was done in our LLM experiments, we simply input the prompt to the LLM and ran it forward to get a prediction for a particular target point. ", "page_idx": 19}, {"type": "text", "text": "F.1 Processing Times ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Processing times vary as a function of: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The GPU used.   \n\u2022 The length of the prompt.   \n\u2022 The number of target points queried.   \n\u2022 The number of tokens required to be generated for a particular target point.   \n\u2022 The number of samples taken at each target point.   \n\u2022 Whether independent or autoregressive sampling is used. ", "page_idx": 19}, {"type": "text", "text": "Example experiment processing times: ", "page_idx": 19}, {"type": "text", "text": "Basic Scenario: Table F.1 indicates that the longer the prompt, the longer the computation time for each target point. For independent sampling (I-LLMP), the prompt length is constant and is only a function of the number of training points as each target point is processed independently. For autoregressive sampling (A-LLMP), the prompt length is a function of both the number of training points and the number of target points since each target point is appended to the prompt as it is sampled. ", "page_idx": 19}, {"type": "text", "text": "Table F.1: Times to load the LLM into GPU memory, for the LLM to generate all samples at all target points, and to compute the probability distribution over the true target points. All runs used the Llama-2-7B LLM and were executed on an NVIDIA 3090 GPU with 24GB of memory with a batch size of 10. All times are in seconds. ", "page_idx": 19}, {"type": "table", "img_path": "HShs7q1Njh/tmp/3a2bf3c989c07a2f1d141132e1e4441186add42728d5c373bcc3a7b7d2e45632.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "1D Synthetic Data Experiments: ", "page_idx": 19}, {"type": "text", "text": "\u2022 LLM: Mixtral- $\\mathbf{\\mathcal{B}}{\\times}{-}7\\mathbf{B}$   \n\u2022 GPU: $2~\\times$ Nvidia A100, 80 GB   \n\u2022 Parameters: A-LLMP, 40 target points, 50 samples, log probabilities   \n\u2022 Tasks: 12 functions x 3 seeds $\\mathtt{X4}$ sizes   \n\u2022 Approximate Time: 19.6 hours ", "page_idx": 19}, {"type": "text", "text": "Black Box Optimization: ", "page_idx": 20}, {"type": "text", "text": "\u2022 LLM: Llama-2 7B   \n\u2022 GPU: $1~\\times$ Nvidia A100, 80 GB   \n\u2022 Parameters: I-LLMP, 500 target points, 1 sample   \n\u2022 Tasks: 6 functions, 100 trials   \n\u2022 Approximate Time: 20 hours ", "page_idx": 20}, {"type": "text", "text": "Fashion MNIST Image Reconstruction: ", "page_idx": 20}, {"type": "text", "text": "\u2022 LLM: Mixtral- $\\mathbf{\\nabla}\\cdot8\\times-7\\mathbf{B}$   \n\u2022 GPU: $2~\\times$ Nvidia A100, 80 GB   \n\u2022 Parameters: I-LLMP, 400 target points, 50 samples   \n\u2022 Tasks: 6 images $\\textbf{x}2$ sizes   \n\u2022 Approximate Time: 15 hours ", "page_idx": 20}, {"type": "text", "text": "Simultaneous Temperature, Rainfall, and Wind Speed Regression ", "page_idx": 20}, {"type": "text", "text": "\u2022 LLM: Llama-3 8B   \n\u2022 GPU: $1~\\times$ Nvidia 3090, 24 GB   \n\u2022 Parameters: A-LLMP, 40 target points, 50 samples   \n\u2022 Tasks: 6 functions, 100 trials   \n\u2022 Approximate Time: 31 minutes ", "page_idx": 20}, {"type": "text", "text": "G Additional Configuration Results ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "G.1 Comparing Sampling and Logit Based Distributions ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We first investigate whether our logit-based method of eliciting distributions (Appendix A.2) match the sampling distribution of the LLM. In order to estimate the true distribution, we obtain 1000 samples from the LLM at each target location, and fit a histogram using the same bins as our logitbased method. Figures G.2 to G.4 show that our method yields a distribution that is visually similar to the one obtained by sampling. ", "page_idx": 21}, {"type": "image", "img_path": "HShs7q1Njh/tmp/e6d2f025d9df1c2fc2305873f7ad3ee57805f36eea1a26aee219f14f2d0424b0.jpg", "img_caption": ["Figure G.2: Visualization of the predictive densities estimated via sampling (middle) and model logits (bottom) for the Sigmoid function with 10 training points (shown in white). Cross section histograms $(t o p)$ are presented at $x=50$ , 100 and 150. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "HShs7q1Njh/tmp/b70898fef325778babeaaa4c8417e3041e71bbbb73b251d1287f130e19450707.jpg", "img_caption": ["Figure G.3: Visualization of the predictive densities estimated via sampling (middle) and model logits (bottom) for the Quadratic function with 20 training points (shown in white). Cross section histograms $(t o p)$ are presented at $x=50$ , 100 and 150. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "HShs7q1Njh/tmp/b385a81dbe0d4a551a3848cd0236e84096d5dedf0cb4b3e0c72121f7a99aae96.jpg", "img_caption": ["Figure G.4: Visualization of the predictive densities estimated via sampling (middle) and model logits (bottom) for the Linear $^+$ Cosine function with 75 training points (shown in white). Cross section histograms (top) are presented at $x=50$ , 100 and 150. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "G.2 Additional Prompt Format Results ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Figure G.5 shows NLL and MAE for various prompt formats and 3 LLMs. Tables G.2 and G.3 show the tabular versions of prompt formatting results. ", "page_idx": 23}, {"type": "text", "text": "Overall, LLMPs tested are robust to the prompt format. The results indicate that two separators are required to achieve the best performance. One to separate the $x$ and $y$ values within a pair and another to separate the $x,y$ pairs. The $-\\Im-$ format uses a comma to separate within a pair and nothing to separate the pairs and it has the worst results. The $\\mathrm{x\\_y}_{-}$ format uses letter prefixes to separate values and pairs with improved metrics. Trading off token efficiency and performance, $\\_{\\mathfrak{m}}$ is the best option as it uses only one comma to delimit $x$ and $y$ and $\\mathfrak{w}$ to delimit $x,y$ pairs. However, given that some regions use a comma as a decimal place, we use _, $\\_\\mathsf{m}$ prompt format in our experiments as it comparable performance and only uses one additional space per pair. The $\\left(\\ldots\\right)$ and $\\mathbf{X}{=}_{-}$ , ${\\mathrm{y}}{=}_{-}{\\mathrm{ln}}$ formats are more human readable, but the extra tokens do not improve performance. ", "page_idx": 23}, {"type": "image", "img_path": "HShs7q1Njh/tmp/fe2916045126e44a3e00760a23459627329f70311726569f38ed8c4bdc99a1be.jpg", "img_caption": ["Figure G.5: NLL and MAE for various prompt formats and each LLM. The height of each bar is the mean of 10 random seeds that determine the locations of the observed points. The small black lines at the top of each bar indicates the standard error. The two \u2019_\u2019 characters in the legend indicate the positions the $x$ and $y$ values. $\\mathfrak{m}$ indicates the newline character. From left to right, the prompts are ordered from the most to least token efficient. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Table G.2: NLL for various prompt formats and each LLM. Each entry is the mean and standard error of 10 random seeds that determine the locations of the observed points. From left to right, the prompts are ordered from the most to least token efficient. The number below each function indicates the number of observed points. ", "page_idx": 23}, {"type": "table", "img_path": "HShs7q1Njh/tmp/7568b0d5a4617b6593293b4e9b2ce11767c768308823a874770b76310951ee8a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "Table G.3: Mean Average Error (MAE) for various prompt formats and each LLM. Each entry is the mean and standard error of 10 random seeds that determine the locations of the observed points. From left to right, the prompts are ordered from the most to least token efficient. The number below each function indicates the number of observed points. ", "page_idx": 24}, {"type": "table", "img_path": "HShs7q1Njh/tmp/e036e7f811ebdab8aec4e0922184d7142fc05f81085800aa8b2ead1fe2839dc1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "G.3 Additional Prompt Ordering Results ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We consider the effect of three different orderings of the training data $D_{\\mathrm{train}}$ in the prompt: ", "page_idx": 25}, {"type": "text", "text": "\u2022 Sequential: $(x_{i},y_{i}),\\in D_{\\mathrm{train}}$ are ordered sequentially from smallest to largest $x_{i}$ , regardless of the location of the target point.   \n\u2022 Random: $(x_{i},y_{i}),\\in D_{\\mathrm{train}}$ are randomly ordered.   \n\u2022 Distance: For the prediction at target point $x^{*}$ , the training points $(x_{i},y_{i}),\\in\\,D_{\\mathrm{train}}$ are ordered from largest to smallest distance to the query point $x^{*}$ i.e. $|x_{n}^{*}-x_{i}|_{2}$ such that the training points closer to $x^{*}$ appear later in the prompt. ", "page_idx": 25}, {"type": "text", "text": "Figure G.6 shows NLL and MAE for various prompt orderings and each LLM. Table G.4 shows the tabular version of the results. ", "page_idx": 25}, {"type": "text", "text": "Distance ordering consistently yields the best results overall. We posit that distance ordering is effective as it provides a hint to the LLM to weigh the contribution of closer points to the current target point to a greater degree. Unless otherwise noted, we use distance ordering for our experiments. ", "page_idx": 25}, {"type": "image", "img_path": "HShs7q1Njh/tmp/c97322cbe7640ae64eeb2a73e59864a5e9ab5f6bf6d7fe9d53bda27d1c01b3da.jpg", "img_caption": ["Figure G.6: NLL and MAE for various prompt orderings and each LLM. The height of each bar is the mean of 10 random seeds that determine the locations of the observed points. The small black lines at the top of each bar indicates the standard error. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Table G.4: Mean Average Error (MAE) and NLL for various prompt orderings and each LLM. Each entry is the mean and standard error of 10 random seeds that determine the locations of the observed points. The number below each function indicates the number of observed points. ", "page_idx": 25}, {"type": "table", "img_path": "HShs7q1Njh/tmp/b38878df5450580f6479e7e4e5537f0c4273c85d527556453fc8562098f49a68.jpg", "table_caption": [], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "G.4 Additional Prompt $y$ -Scaling Results ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In this experiment, we examine the effect of the magnitude and sign of the $y$ -values of the task given to the LLM when no other contextual information is provided. We take the same three synthetic examples but scale the $y$ -values to be in the ranges $[0,1]$ , $[-1,1]$ , [0, 10] and $[-1000,1000]$ . ", "page_idx": 26}, {"type": "text", "text": "Figure G.7 shows NLL and MAE for various prompt $y$ -scaling and each LLM. Table G.5 shows the tabular results. The raw values given to the LLM are scaled meaning the observation noise is scaled accordingly. We have scaled the likelihoods and MAE values to compensate for the difference in range. According to the evaluation metrics we observe that performance degrades with increased range and incorporating negative values also hurts MAE. This is due to the fact that when the range is wider, the LLM must accurately generate more numerical digits and potentially a negative sign when predicting $f(x^{*})$ . ", "page_idx": 26}, {"type": "image", "img_path": "HShs7q1Njh/tmp/ccc0e579d229f343fe11f479fd33f5132a842d6d5c83d8510cd065fae65ba6c9.jpg", "img_caption": ["Figure G.7: NLL and MAE for various prompt y-scalings and each LLM. The height of each bar is the mean of 10 random seeds that determine the locations of the observed points. The small black lines at the top of each bar indicates the standard error. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Table G.5: MAE and NLL for various $y$ -scaling ranges and three LLMs. Each entry is the mean and standard error of 10 random seeds that determine the locations of the observed points. The number below each function indicates the number of observed points. ", "page_idx": 26}, {"type": "table", "img_path": "HShs7q1Njh/tmp/fc2f1dfcb95c157d67cb8244867c0e77acba66679110592e033728def1619738.jpg", "table_caption": [], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "However, observing the plots in Figure G.8 of the predictive distribution on each scale, the model gives reasonable predictions regardless of scale. If no scenario context is provided via text to the LLM, rescaling task values to be approximately between 0 and 1 improves performance in our experiments. However, in general we use unscaled data so that we can examine the prior beliefs learned by the LLM about tasks communicated through the raw values. ", "page_idx": 26}, {"type": "image", "img_path": "HShs7q1Njh/tmp/02d0a433d955988026df88e9faa82622d29aedcae46e2479ab85cb6a4e51eec8.jpg", "img_caption": [], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Figure G.8: Predictive distributions given by the Mixtral- $\\mathbf{\\nabla}\\cdot8\\times7\\mathbf{B}$ LLM on scaled Linear $+\\,\\mathrm{Cos}$ with 75 observations. This example exhibited one of the largest variation in metrics as a result of scaling. Despite this, all predictive distributions look reasonable. ", "page_idx": 27}, {"type": "text", "text": "G.5 top- $p$ and temperature results ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Figure G.9 shows how MAE varies with LLM top- $\\cdot p$ and temperature. Table G.6 shows the tabular version of the results. ", "page_idx": 28}, {"type": "text", "text": "Surprisingly, all LLM\u2019s are insensitive to temperature and top- $\\boldsymbol{p}$ with respect to MAE. ", "page_idx": 28}, {"type": "text", "text": "Though not evident from these MAE results, we sometimes observed that using a top- $\\cdot p$ of 1.0 can result in some extreme values in samples. However, we consider temperature $=1.0$ , and top- $p=1.0$ closest to the default distribution given by the LLM. Since it had competitive performance with the other options, we use these settings to compute log-likelihoods in our experiments which allows us to examine the default characteristics of the LLM\u2019s predictive distribution. ", "page_idx": 28}, {"type": "image", "img_path": "HShs7q1Njh/tmp/a5d750c689c013b25cdc5c4aebd69168e7500b022d2cc0167c1a4730ed1b39ae.jpg", "img_caption": ["Figure G.9: MAE (lower is better) for various temperature and top- $\\cdot p$ settings and each LLM. All LLM\u2019s are relatively insensitive to temperature and top p with respect to MAE. "], "img_footnote": [], "page_idx": 28}, {"type": "table", "img_path": "HShs7q1Njh/tmp/e9635dc1dad57b3e144561e6f9f52073b0b2f8c33333b43b631318f535a066ce.jpg", "table_caption": ["Table G.6: MAE (lower is better) for various top- $\\cdot p$ and temperature settings and all LLMs. "], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "G.6 Additional Autoregressive Sampling Results ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Figure G.10 shows NLL and MAE of random and distance training point orderings for A-LLMP and I-LLMP and each LLM. Table G.7 shows the tabular results. ", "page_idx": 29}, {"type": "image", "img_path": "HShs7q1Njh/tmp/6e5e2e72d1fc16d63dc778d2870a367f1f4ed68213fd573ee0f8ce0141e12493.jpg", "img_caption": ["Figure G.10: NLL and MAE for various prompt y-scalings and each LLM. The height of each bar is the mean of 3 random seeds that determine the locations of the observed points. The small black lines at the top of each bar indicates the standard error. "], "img_footnote": [], "page_idx": 29}, {"type": "table", "img_path": "HShs7q1Njh/tmp/cc2f9658a76ce8b8c99940c2f7bc8ea48f9bf6e533da924709090de269e3f98b.jpg", "table_caption": ["Table G.7: Mean Average Error (MAE) and Negative Log Likelihood (NLL) for autoregressive and marginal sampling with two different prompt orderings and three LLMs. "], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "G.7 Additional Autoregressive Process Results ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Figure G.11 shows the MAE results for the autoregressive process experiments. Figures G.12 and G.13 show the Avg log $p(y)$ and MAE for 10 different orderings of the query points. ", "page_idx": 30}, {"type": "image", "img_path": "HShs7q1Njh/tmp/7bcf267b6e915174e5d4373cdc5f32a5c36ca6ef0b1dacbd2802e38df3614bda.jpg", "img_caption": ["Figure G.11: Autoregressive process MAE results. "], "img_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "HShs7q1Njh/tmp/bff04782f47145f094c6cb66ed0a546007f12476f87e557c5e91a6b9c094b167.jpg", "img_caption": ["Figure G.12: Avg log $p(y)$ for the 10 seeds for each LLM for the autoregressive process experiment. "], "img_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "HShs7q1Njh/tmp/960546509cf527539d0fb20d970073bf66ef22a3d675eb135c9510521cf0135f.jpg", "img_caption": ["Figure G.13: MAE for the 10 seeds for each LLM for the autoregressive process experiment. "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "H Additional LLMP Performance Details and Results ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "H.1 Additional Comparison to Gaussian Processes (GP) Results ", "page_idx": 31}, {"type": "text", "text": "Figures H.14 to H.25 shows regression results from the Mixtral- $\\mathbf{\\nabla}8\\!\\times\\!7\\mathbf{B}$ LLM and an RBF kernel GP for the 12 different synthetic functions. ", "page_idx": 31}, {"type": "image", "img_path": "HShs7q1Njh/tmp/d0d90c118b9b93596ece2563594be32c02e060dafc8075e00289349ad10b032e.jpg", "img_caption": ["Figure H.14: MAE (lower is better) and NLL (lower is better) for the Mixtral- $\\mathbf{8}\\!\\times\\!7\\mathbf{B}$ LLM versus a GP as a function of the number of observed points for the Beat function. The GP uses an RBF kernel with optimized length scale and noise. "], "img_footnote": [], "page_idx": 31}, {"type": "image", "img_path": "HShs7q1Njh/tmp/0cc0a0ca2ec7fbca65365e737fcb9164172a19eb6d40ad2d638a21d044edc54d.jpg", "img_caption": ["Figure H.15: MAE (lower is better) and NLL (lower is better) for the Mixtral- $\\mathbf{8}\\!\\times\\!7\\mathbf{B}$ LLM versus a GP as a function of the number of observed points for the Exponential function. The GP uses an RBF kernel with optimized length scale and noise. "], "img_footnote": [], "page_idx": 32}, {"type": "image", "img_path": "HShs7q1Njh/tmp/fc1e506031eb96c05a7c18b67bec9f72c1c4f93e84d6df2cda40e5678c4f7489.jpg", "img_caption": ["Figure H.16: MAE (lower is better) and NLL (lower is better) for the Mixtral- $\\mathbf{\\nabla}\\cdot8\\times7\\mathbf{B}$ LLM versus a GP as a function of the number of observed points for the Gaussian Wave function. The GP uses an RBF kernel with optimized length scale and noise. "], "img_footnote": [], "page_idx": 33}, {"type": "image", "img_path": "HShs7q1Njh/tmp/955a667b2255d0322f97d08c39a13b5a88e209e461ad4b10dd0ed4768aa007df.jpg", "img_caption": ["Figure H.17: MAE (lower is better) and NLL (lower is better) for the Mixtral- $\\mathbf{8}\\!\\times\\!7\\mathbf{B}$ LLM versus a GP as a function of the number of observed points for the Linear function. The GP uses an RBF kernel with optimized length scale and noise. "], "img_footnote": [], "page_idx": 34}, {"type": "image", "img_path": "HShs7q1Njh/tmp/a1dfef1452a50b2cfdf9df87f00e2a8b369ff67d23f0fd84996498455009ad9d.jpg", "img_caption": ["Figure H.18: MAE (lower is better) and NLL (lower is better) for the Mixtral- $\\mathbf{\\nabla}\\cdot8\\times7\\mathbf{B}$ LLM versus a GP as a function of the number of observed points for the Linear $^+$ Cosine function. The GP uses an RBF kernel with optimized length scale and noise. "], "img_footnote": [], "page_idx": 35}, {"type": "image", "img_path": "HShs7q1Njh/tmp/2a8092e0a9c6a439adfbf519a9603a85c379b2ca9ce6cde467df83f3298b465d.jpg", "img_caption": ["Figure H.19: MAE (lower is better) and NLL (lower is better) for the Mixtral- $\\mathbf{\\nabla}\\cdot8\\times7\\mathbf{B}$ LLM versus a GP as a function of the number of observed points for the Log function. The GP uses an RBF kernel with optimized length scale and noise. "], "img_footnote": [], "page_idx": 36}, {"type": "image", "img_path": "HShs7q1Njh/tmp/d24103792b2816a6de903aa83c624666766adbab35b6cb003ac3fa3c2c86ec74.jpg", "img_caption": ["Figure H.20: MAE (lower is better) and NLL (lower is better) for the Mixtral- $\\mathbf{\\nabla}\\cdot8\\times7\\mathbf{B}$ LLM versus a GP as a function of the number of observed points for the Quadratic function. The GP uses an RBF kernel with optimized length scale and noise. "], "img_footnote": [], "page_idx": 37}, {"type": "image", "img_path": "HShs7q1Njh/tmp/f6d6124513c5a0448455dfe0ea8b38636f1d740bb0e7b6072c30c18f463569e9.jpg", "img_caption": ["Figure H.21: MAE (lower is better) and NLL (lower is better) for the Mixtral- $\\mathbf{\\nabla}\\cdot8\\times7\\mathbf{B}$ LLM versus a GP as a function of the number of observed points for the Sigmoid function. The GP uses an RBF kernel with optimized length scale and noise. "], "img_footnote": [], "page_idx": 38}, {"type": "image", "img_path": "HShs7q1Njh/tmp/fbc646eae9b986443e9fcfa81ef5e6f65bc7ce248eefaa4e59955db882e0c383.jpg", "img_caption": ["Figure H.22: MAE (lower is better) and NLL (lower is better) for the Mixtral- $\\mathbf{\\nabla}\\cdot8\\times7\\mathbf{B}$ LLM versus a GP as a function of the number of observed points for the Sinc function. The GP uses an RBF kernel with optimized length scale and noise. "], "img_footnote": [], "page_idx": 39}, {"type": "image", "img_path": "HShs7q1Njh/tmp/e48a7efcaff797b9a4d5f2ee369df54147c2e215410148e7a7157356f2a25b94.jpg", "img_caption": ["Figure H.23: MAE (lower is better) and NLL (lower is better) for the Mixtral- $\\mathbf{\\nabla}\\cdot8\\times7\\mathbf{B}$ LLM versus a GP as a function of the number of observed points for the Sine function. The GP uses an RBF kernel with optimized length scale and noise. "], "img_footnote": [], "page_idx": 40}, {"type": "image", "img_path": "HShs7q1Njh/tmp/70af9bbb0d5b483db2fa043c204445f7df7b2cdc47b25e5418c9655bec7110b0.jpg", "img_caption": ["Figure H.24: MAE (lower is better) and NLL (lower is better) for the Mixtral- $\\mathbf{\\nabla}\\cdot8\\times7\\mathbf{B}$ LLM versus a GP as a function of the number of observed points for the $X\\times5$ Sine function. The GP uses an RBF kernel with optimized length scale and noise. "], "img_footnote": [], "page_idx": 41}, {"type": "image", "img_path": "HShs7q1Njh/tmp/7d2287219ff4a9abc1db99e581096c67320b1b7ca13e4ce144c50219afd01bb7.jpg", "img_caption": ["Figure H.25: MAE (lower is better) and NLL (lower is better) for the Mixtral- $\\mathbf{\\nabla}\\cdot8\\times7\\mathbf{B}$ LLM versus a GP as a function of the number of observed points for the Linear $\\times$ Sine function. The GP uses an RBF kernel with optimized length scale and noise. "], "img_footnote": [], "page_idx": 42}, {"type": "text", "text": "Figure H.26 shows plot of NLL and MAE for the Mixtral- $\\mathbf{\\nabla}8\\!\\times\\!7\\mathbf{B}$ LLM and the RBF kernel GP for 12 for the 12 different synthetic functions. ", "page_idx": 43}, {"type": "image", "img_path": "HShs7q1Njh/tmp/fabb8cfdb93342b9f128bbd350f67317a63afecb3abd2caaea696ef30dd38a59.jpg", "img_caption": ["Figure H.26: MAE (lower is better) and NLL (lower is better) for the Mixtral- $\\mathbf{8}\\!\\times\\!7\\mathbf{B}$ LLM versus a GP as a function of the number of observed points for 12 different synthetic functions. Results are averaged over three sets of random samples for the observed points. The GP uses an RBF kernel with optimized length scale and noise. "], "img_footnote": [], "page_idx": 43}, {"type": "text", "text": "H.2 Multimodal Predictive Experiment Details ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "To verify that LLMPs are able to produce non-Gaussian, multimodal predictive distributions we sampled training data from the following synthetic, bimodal generative distribution: ", "page_idx": 44}, {"type": "equation", "text": "$$\ny={\\frac{.05}{1+\\exp-x}}+0.02x+\\epsilon_{1}(0.02x+0.08)+0.03\\epsilon_{2}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Where $\\epsilon_{\\mathrm{1}}\\sim\\mathrm{Bernoulli}(p=0.5)$ and $\\epsilon_{2}\\sim N(0,1)$ . The Llama-3-70B A-LLMP predictive distribution using 100 training points is visualized in Figure 4 (right) and using 40 training points is visualized in Figure H.27. ", "page_idx": 44}, {"type": "image", "img_path": "HShs7q1Njh/tmp/3c467d5c2b5f8b8aeed809ce53b52bb38956d713d07b379b611648e28695d154.jpg", "img_caption": [], "img_footnote": [], "page_idx": 44}, {"type": "text", "text": "Figure H.27: Heatmap visualization of the Llama-3-70B A-LLMP predictive distribution conditioned on data from a bimodal generative process. Black dots are the 40 training points. ", "page_idx": 44}, {"type": "text", "text": "H.3 Comparison to LLMTime ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Figure H.28 compares A-LLMP in a temperature forecasting scenario to LLMTime. The dataset consists of 86 daily high temperature readings, obtained after the training cut-off for the Llama-2 LLM to avoid data-leakage. We use the first 50 readings for training data and ask the two methods to predict/forecast the final 36 values. The authors of LLMTime suggest the method can handle missing values by inputting NaN values in their place. Since LLMPs can work with irregularly spaced and missing data, we also compare the methods with a reduced number of irregularly spaced training points. A-LLMP wins out over LLMTime, as the log probabilities for A-LLMP are significantly better. ", "page_idx": 45}, {"type": "image", "img_path": "HShs7q1Njh/tmp/3afe05403764d315cc79b79ac05a79177637f5dcb0ff9ec7d5f9b91238c9925e.jpg", "img_caption": ["Figure H.28: MAE $\\downarrow$ and NLL $\\downarrow$ for A-LLMP versus a LLMTime on a dataset of daily temperatures in London, UK recorded after the release date of the LLM with a varying number of training points. The LLM is Llama-2-7B in both cases. "], "img_footnote": [], "page_idx": 45}, {"type": "text", "text": "H.4 Additional Image Reconstruction Results and Details ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Figure H.29 depicts six image reconstruction results, all drawn from the Fashion-MNIST dataset [12]. The $28\\times28$ pixel images were first scaled to $20\\times20$ , due to the context size limitations of the open-source LLMs we used in our experiments. The pixel data was then converted into prompt data points by forming a series of (row, column, pixel value) integer tuples. We then sampled 80 pixel locations $(20\\%)$ and 200 pixel locations $(50\\%)$ as observed points for the reconstruction. Each pixel location (400 in all) was used as a target point location for independent marginal sampling with the Mixtral- $\\mathbf{\\nabla}\\cdot8\\times7\\mathbf{B}$ LLM. ", "page_idx": 46}, {"type": "image", "img_path": "HShs7q1Njh/tmp/5b4de60b3f9cb72ae0bc0d3a6854bbc81264e64a1a2314de02fa8e89e999ce89.jpg", "img_caption": ["Figure H.29: Image reconstruction results for six images drawn from the Fashion-MNIST dataset [12]. 1st column: True images.The 2nd and 4th columns are the observed pixels for the regression task and are sampled at $20\\%$ and $50\\%$ from the true image pixels. The blue pixels indicate unobserved. The 3rd and 5th columns show the reconstructions using the Mixtral- $\\mathbf{\\nabla}\\cdot8\\times7\\mathbf{B}$ LLM. "], "img_footnote": [], "page_idx": 46}, {"type": "text", "text": "H.5 Black-box Optimization Results and Implementation Details ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Black box optimization involves minimizing or maximizing a function where there is only access to the output of a function for a specified input. It is often used to optimize functions that are expensive to evaluate and the goal is to find the minimum or maximum value with the fewest number of calls to the function (often referred to as trials). To acquire the location of the next point to observe, we sample the LLM using Thompson sampling [38, 39]. Details are in Algorithm 3. We benchmark the ability of an LLM to perform black box maximization on six commonly used functions implemented in [40], including Gramacy [41], Branin, Bohachevsky, Goldstein, and Hartmann3. We compare our results using Llama-2-7B to Optuna [3], a commercial hyperparameter optimization framework. We run both methods for 100 trials and record the trial at which the the best approximation to the maximum occurs. The results are shown in Table H.8. In all cases, we obtain as good or better approximation to the true maximum value in a fewer number of trials. Note that Optuna will perform 100 trials in a few seconds while the LLM approach can take up to 2 Nvidia A100 GPU hours. However, the results show that the log likelihood of LLMPs is capable of accurately portraying regression uncertainty. ", "page_idx": 47}, {"type": "table", "img_path": "HShs7q1Njh/tmp/32421e0549658c8bef48050cf1f636e60cf34769fcd7a02169ed48c319231186.jpg", "table_caption": ["Table H.8: Black box optimization results. The number in the Function column indicates the number of $x$ dimensions. The Trial column indicates the trial at which the Best estimate of the maximum for each method occurred. "], "table_footnote": [], "page_idx": 47}, {"type": "text", "text": "Algorithm 3 Pseudocode for LLM black-box function optimization ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Require: $f(\\mathbf{x})$ : Function to be maximized   \nRequire: ${\\bf x}_{m i n}$ : Minimum bound on x   \nRequire: $\\mathbf{x}_{m a x}$ : Maximum bound on $\\mathbf{x}$   \nRequire: $T$ : Number of trials (default 100)   \nRequire: $M$ : Number of target points (default 500)   \nRequire: $C$ : Number of cold start points (default 7) observe $\\mathsf{d}_{x}\\gets[\\,]$ \u25b7List of observed x values observe $\\mathbb{1}_{y}\\gets[\\,]$ $\\triangleright$ List of observed $y$ points for trial $\\leftarrow1$ to $C$ do $\\mathbf{x}\\gets\\mathcal{U}(\\mathbf{x}_{m i n},\\mathbf{x}_{m a x})$ observed $_x$ .append ${\\bf\\Psi}({\\bf x})$ observed $_y$ .append $(f(\\mathbf{x}))$ end for for tria $\\leftarrow C+1$ to $T$ do targets $\\leftarrow[\\,\\,]$ \u25b7List of target x points samples $\\leftarrow[\\,\\,]$ $\\triangleright$ List of samples at target points for $\\mathrm{i}\\gets1$ to $M$ do $\\begin{array}{r}{\\mathrm{target}_{x}\\longleftarrow\\sim\\mathcal{U}(\\mathbf{x}_{m i n},\\mathbf{x}_{m a x})}\\end{array}$ targets.append $\\left(\\mathrm{target}_{x}\\right)$ ) prompt $\\leftarrow$ construct_prompt(observedx, observed $_y$ , $\\mathrm{target}_{x}$ ) $\\triangleright$ construct a text prompt samples $\\leftarrow$ Algorithm 1 $N=1)$ ) $\\triangleright$ Use Algorithm 1 to obtain a single sample at the target point end for new_observed $_x\\gets$ targets[argmax(samples)] \u25b7Thompson sampling observed $_x$ .append(new_observedx) observed $_y$ .append(f(new_observedx)) end for $\\mathrm{max}_{y}\\gets\\mathrm{max}(\\mathrm{observed}_{y})$ $\\triangleright$ Best estimate of maximum value of $f$ $\\operatorname*{max}_{x}\\leftarrow$ observedx[argmax(observedy)] $\\triangleright$ value of x where best estimate of maximum of $f$ occurs ", "page_idx": 48}, {"type": "text", "text": "H.6 In-context Experiment Details and Additional Plots ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "For the in-context learning experiment in Section 4 we investigate LLMPs\u2019 ability to learn from similar examples in-context to predict average monthly precipitation across 13 Canadian locations [13], one from each province and territory: Alert, NU, Charlottetown, PE, Comox, BC, Goose, NL, Greenwood, NS, Keylake, SK, Montreal, QC, Ottawa, ON. Ranfurly, AB, Saint John, NB, Thompson, MB, Whitehorse, YK, and Yellowknife, NT. For each location, we use the Mixtral- $\\mathbf{\\nabla}\\cdot8\\times7\\mathbf{B}$ A-LLMP to forecast 32 months of average precipitation values given the previous four month observations taken from a random historical three-year period between 1913-2017 (conditional on data availability). It is then provided with 1-12 examples of random three year periods of historical values from the same location in-context. An example prompts for 0, 1 (1976-1978) and 2 (1976-1978, 1949-1951) examples are: ", "page_idx": 49}, {"type": "text", "text": "1. \u201cMonthly total of daily adjusted rainfall, mm. \\n1976-1978:\\n\u201d, ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "2. \u201cMonthly total of daily adjusted rainfall, mm. $\\mathrm{\\backslashn1967\u20131969?in0,0.3\\backslashn1,0.6\\backslashn2,1.3\\backslashn}$ 3,0.6\\n4,31.7\\n5,59.9\\n6,135.4\\n7,107.7\\n8,78.3\\n9,40.7 \\n10,37.3\\n11,5.4\\n12,1.0 \\n 13,41.4\\n14,0.3\\n15,29.2\\n16,41.3\\n17,67.8\\n18,137.8\\n19,139.9\\n20,91.4\\n21,143.1\\n22,18.8 \\n23,0.9\\n24,0.6\\n25,14.0\\n26,4.0\\n27,6.2\\n28,45.1\\n29,98.3\\n30,97.0\\n31,160.4\\n32,116.3\\n 33,22.4\\n34,51.8\\n35,38.1\\n1976-1978:\\n\u201d,   \n3. \u201cMonthly total of daily adjusted rainfall, mm. \\n1967-1969:\\n0,0.3\\n1,0.6\\n2,1.3\\n $3.0.6\\backslash\\mathfrak{d}.31.7\\backslash\\mathfrak{I}_{\\mathfrak{I}},59.9\\backslash\\mathfrak{n}6,135.4\\backslash\\mathfrak{n}7,107.7\\backslash\\mathfrak{n}8,78.3\\backslash\\mathfrak{n}9.40.7\\backslash\\mathfrak{n}10.37.3\\backslash\\mathfrak{n}11.5\\.4\\backslash\\mathfrak{n}12,1.0\\backslash\\mathfrak{n}11.5\\times10^{3}$ 13,41.4\\n14,0.3\\n15,29.2\\n16,41.3\\n17,67.8\\n18,137.8\\n19,139.9\\n20,91.4\\n21,143.1\\n22,18.8\\n 23,0.9\\n24,0.6\\n25,14.0\\n26,4.0\\n27,6.2\\n28,45.1\\n29,98.3\\n30,97.0\\n31,160.4\\n32,116.3\\n 33,22.4\\n34,51.8\\n35,38.1\\n 1949-1951:\\n0,1.6\\n1,0.0\\n2,2.5\\n3,2.1\\n4,22.0\\n5,51.7\\n6,83.4\\n7,113.3\\n8,75.5\\n9,34.7\\n10,4.7\\n 11,1.4\\n12,1.1\\n13,0.0\\n14,0.8\\n15,9.5\\n16,33.3\\n17,92.6\\n18,118.5\\n19,70.3\\n20,34.6\\n21,58.2\\n 22,62.4\\n23,8.5\\n24,0.3\\n25,7.4\\n26,8.0\\n27,30.6\\n28,49.3\\n29,40.0\\n30,82.5\\n31,97.1\\n32,71.5\\n 33,17.1\\n34,32.1\\n35,10.1\\n1976-1978:\\n\u201d. ", "page_idx": 49}, {"type": "text", "text": "Results are presented in Figure 8, Figure H.30 and Figure H.31. ", "page_idx": 49}, {"type": "image", "img_path": "HShs7q1Njh/tmp/401aa1afca3b71fc2432dbde7f9fbeb2767f33e2ecdbd4240cbc416f33bde1e1.jpg", "img_caption": ["Figure H.30: Visualizations of the predictions given by the Mixtral- $\\mathbf{\\nabla}8\\!\\times\\!7\\mathbf{B}$ LLMP for seven locations locations accross Canada. Blue and black circles are training and test points, respectively. Red circles are median predictions and shaded areas indicate tenth-percentiles over 30 samples. "], "img_footnote": [], "page_idx": 50}, {"type": "image", "img_path": "HShs7q1Njh/tmp/cb889aaef2cf06b3e26e0690cdbdba3688146d41c21e29ae412c3e4de6b5bbda.jpg", "img_caption": ["Figure H.31: Visualizations of the predictions given by the Mixtral- $\\mathbf{8}\\!\\times\\!7\\mathbf{B}$ LLMP for six locations locations accross Canada. Blue and black circles are training and test points, respectively. Red circles are median predictions and shaded areas indicate tenth-percentiles over 30 samples. "], "img_footnote": [], "page_idx": 51}, {"type": "text", "text": "I Conditioning on Text Details and Additional Experiments ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "I.1 Scenario-conditional Predictions Details and Additional Experiments ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "For the scenario-conditional predictions experiment in Section 5, we examine the influence of text providing information about various synthetic problem settings on the predictive distribution of an Llama-3-70B LLMP. In all of the following examples, we provide the same two synthetic training points, $(1,2.53)$ and (2, 2.21) to the LLM Process but change the prompting text that comes before the training data. We then use A-LLMP to forecast trajectories integer 50 steps ahead. Prompts were prepended to the standard data formatting scheme used for LLMPs (see Appendix C). ", "page_idx": 52}, {"type": "text", "text": "The prompts provided to the LLMP visualized in Figure 9 are: ", "page_idx": 52}, {"type": "text", "text": "1. \u201c\u201d (i.e. no text);   \n2. \u2018The following are daily temperature measurements from Montreal in January in degrees Celsius\u201d   \n3. \u201cThe following are daily temperature measurements from Montreal in May in degrees Celsius\u201d   \n4. \u201cIn the following series, the first number is the number of Months from January and the second is the Monthly precipitation measurements in inches from San Diego, CA\u201d   \n5. \u201cIn the following series, the first number is the number of Months from February and the second is the Monthly precipitation measurements in inches from Singapore\u201d ", "page_idx": 52}, {"type": "text", "text": "The prompts visualized in Figure 1 are: ", "page_idx": 52}, {"type": "text", "text": "1. \u201cThe following are daily stock prices from a financial time series\u201d   \n2. \u201cThe following are daily stock prices from a financial time series for a company that eventually goes out of business\u201d   \n3. \u201cThe following are daily average stock prices from a financial time series for a company whose stock price goes to zero on day $30^{\\circ}$ ", "page_idx": 52}, {"type": "text", "text": "Lynx Hare Population Forecasting: Similar to the previous experiment, this experiment examines to what extent the predictive posterior of an LLM Process is influenced by textual information about the problem provided in the prompt. We preface the prompt with three different strings: ", "page_idx": 52}, {"type": "text", "text": "1. \u201c\u201d (i.e. no text);   \n2. \u201cThe following are samples from lynx-hare populations\u201d   \n3. \u2018\u2019The following are samples from the famous Canadian Hudson Bay Lynx-Hare population dataset. When hare increases, lynx increases. The first number of two is the year. The second number is the lynx population. It follows the pattern when lynx population increases, hare decreases\u201d ", "page_idx": 52}, {"type": "text", "text": "Figure I.32 shows the predictive distribution of the LLM with 10 and 50 observed points. As the specificity of the text increases from L to R, the posterior entropy decreases, and structure of the samples changes dramatically. ", "page_idx": 52}, {"type": "image", "img_path": "HShs7q1Njh/tmp/2c4006fc7e31dae011fe6fd135e925a0b5c4a2caf96cb9d8df80dc7abef11a6e.jpg", "img_caption": ["Figure I.32: Results of condition on both text and numerical data simultaneously, on the Mixtral model. Observed points are in purple. Colored lines show sampled trajectories. The blue shading is a visualization of percentiles based on 50 samples. Top: Conditioning on 10 observed points. Bottom: Conditioning on 50 observed points. The predictive distribution changes as more information about the problem is added to the prompt. "], "img_footnote": [], "page_idx": 52}, {"type": "text", "text": "I.2 Labelling Features Using Text Details and Additional Plots ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "In the experiments in section Section 5 we examine the performance of a Mixtral-8x7B Instruct I-LLMP on predicting American housing prices. The dataset [16] contains 39980 housing prices and various variables around housing and demographics for the top 50 American cities by population. This dataset was generated on 12/09/2023, however it contains data from the 2020 US Census and the 2022 American Community Survey (ACS). It is possible that data within this dataset was used to train Mixtral-8x7B but it is very unlikely that it was trained on the exact strings presented in this experiment. ", "page_idx": 53}, {"type": "text", "text": "For each prediction task, we show the I-LLMP 10 randomly selected training examples from the dataset and predict on 20 randomly selected test examples. In the prompt, before the numerical value (price) we provide a string which encodes the datapoint index/features that the model can use. For our first experiment we examine the behaviour of the LLMP when more features are added to the prompt. We experiment with five ways of indexing the training and test points illustrated by the following training examples; ", "page_idx": 53}, {"type": "text", "text": "1. \u201c32.74831, -97.21828, Price: 224900.00\u201d   \n2. \u201cLocation: Fort Worth, Texas, Latitude: 32.74831, Longitude: -97.21828, Price: 224900.00\u201d   \n3. \u201cLocation: Fort Worth, Texas, Latitude: 32.74831, Longitude: -97.21828, Zip Code: 76112, Median Household Income: 71452.0, Price: 224900.00\u201d   \n4. \u201cLocation: Fort Worth, Texas, Latitude: 32.74831, Longitude: -97.21828, Zip Code: 76112, Median Household Income: 71452.0, Zip Code Population: 42404 people, Zip Code Density: 1445.0 people per square mile, Price: 224900.00\u201d   \n5. \u201cLocation: Fort Worth, Texas, Latitude: 32.74831, Longitude: -97.21828, Zip Code: 76112, Median Household Income: 71452.0, Zip Code Population: 42404 people, Zip Code Density: 1445.0 people per square mile, Living Space: 1620 square feet, Number of Bedrooms: 3, Number of Bathrooms: 2, Price: 224900.00\u201d ", "page_idx": 53}, {"type": "text", "text": "This procedure is repeated 10 times to compute statistics. Results from this experiment are presented in Figure 10 (left, centre) and in I.34. We also ran this experiment using Mixtral- $8\\mathrm{x}7\\mathrm{B}$ and found that the performance, shown in Figure I.33, was not as good as with the instruction tuned version of Mixtral- $\\mathbf{\\nabla}\\cdot8\\times7\\mathbf{B}$ . ", "page_idx": 53}, {"type": "image", "img_path": "HShs7q1Njh/tmp/03fb4f38806b52c77f001a03756349f96e76400aa03d4d9eb6860388d6e9aceb.jpg", "img_caption": ["Figure I.33: Average MAE and NLL performance of the Mixtral-8x7BLLMP over 10 experiments with error bars representing the standard error. "], "img_footnote": [], "page_idx": 53}, {"type": "text", "text": "An additional experiment is presented in Section 5 to see examine the effect of adding text labels to the features. This experiment was run on 10 new random datasets providing the LLMP with either labeled or unlabelled numerical features. Due to the results of the previous experiment, a Mixtral- $\\mathbf{\\nabla}{8\\mathbf{x}7\\mathbf{B}}$ Instruct LLMP was used for this experiment. The following are example training strings for the four cases examined: ", "page_idx": 53}, {"type": "text", "text": "a. \u201c30.45738, -97.75516, Price: 385000.00\u201d ", "page_idx": 53}, {"type": "image", "img_path": "HShs7q1Njh/tmp/3fe174ecb290b445eac2dbbde018148a6eb5048619ab61f900c16a12222c0037.jpg", "img_caption": ["Figure I.34: Results of 10 runs using Mixtral- $\\boldsymbol{\\cdot}8\\boldsymbol{\\mathrm{x}}7\\boldsymbol{\\mathrm{B}}$ Instruct I-LLMP predicting US housing prices for 20 random houses from [16]. Predictions are visualized using index style 1) and 5). Xs are mean predictions using 30 samples from the LLMP and error bars indicate 2 standard deviations. ", "Run 10 "], "img_footnote": [], "page_idx": 54}, {"type": "text", "text": "b. \u201cLocation: Austin, Texas, Latitude: 30.45738, Longitude: -97.75516, Price: 385000.00\u201d c. \u201c30.45738, -97.75516, 78729, 107830.0, 30907, 1216.1, 1349, 3, 2, Price: 385000.00\u201d d. \u201cLocation: Austin, Texas, Latitude: 30.45738, Longitude: -97.75516, Zip Code: 78729, Median Household Income: 107830.0, Zip Code Population: 30907 people, Zip Code Density: 1216.1 people per square mile, Living Space: 1349 square feet, Number of Bedrooms: 3, Number of Bathrooms: 2, Price: 385000.00\u201d. ", "page_idx": 54}, {"type": "text", "text": "Results of this experiment are presented in Figure 10 (right). ", "page_idx": 54}, {"type": "text", "text": "J Additional Comments on Limitations and Societal Impact ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Limitations As mentioned in the main text along with the flexibility of LLMs, LLMPs inherit their drawbacks. An additional drawback of using LLMs for probabilistic regression is that results from LLMPs are inherently less interpretable than from methods like Gaussian processes where we explicitly encode priors. As with other black-box methods, we must, at the moment, rely on demonstrating empirically that it makes well-calibrated predictions. ", "page_idx": 55}, {"type": "text", "text": "Societal Impact Our work has demonstrated a new and useful zero-shot approach for generating probabilistic predictions using plain language to augment numerical data. It has the potential to allow practitioners from fields such as medical research and climate modelling to more easily access probabilistic modelling and machine learning. We hope that such an impact would help researchers improve the lives of all humans by tackling the problems that humanity faces today. ", "page_idx": 55}, {"type": "text", "text": "Like all machine learning technology, there is potential for abuse, and possible consequences from incorrect predictions made with LLMPs. Due to the black-box nature of the method, we do not know the biases in the underlying LLMs used and what effect they may have on LLMPs output. However, LLM researchers are striving to make LLMs more fair and equitable. An open area of research is whether LLM biases propagate to LLMP predictions and whether de-biasing LLMs helps to fix such an issue. ", "page_idx": 55}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 56}, {"type": "text", "text": "Answer:[Yes] ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Justification: We claim four contributions in our paper in Section 1 and we devote an entire section to each one to back up our claims. ", "page_idx": 56}, {"type": "text", "text": "\u2022 The definition of LLMPs (Section 2);   \n\u2022 Best practices for LLMP configuration (Section 3);   \n\u2022 LLMPs are competitive regressors (Section 4);   \n\u2022 Conditioning LLMPs on problem relevant text (Section 5). ", "page_idx": 56}, {"type": "text", "text": "Guidelines: ", "page_idx": 56}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 56}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 56}, {"type": "text", "text": "Justification: Refer to Section 7 where we discuss several limitations of our work. ", "page_idx": 56}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 56}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 57}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 57}, {"type": "text", "text": "Justification: Our paper does not include any theoretical results. ", "page_idx": 57}, {"type": "text", "text": "Guidelines: ", "page_idx": 57}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 57}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 57}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 57}, {"type": "text", "text": "Justification: Throughout the paper, we provide complete details for reproducing our experimental results. We do this by: ", "page_idx": 57}, {"type": "text", "text": "\u2022 Detailing the algorithms used, see Algorithms 1 to 3.   \n\u2022 Providing a sampling diagram and sample prompts, see Figure 2 and Appendix C.   \n\u2022 Complete source code at https://github.com/requeima/llm_processes.   \n\u2022 Extensive experiment sections in addition to lengthy appendices. ", "page_idx": 57}, {"type": "text", "text": "Guidelines: ", "page_idx": 57}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 57}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 58}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 58}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 58}, {"type": "text", "text": "Justification: We published source code to reproduce the experiments at https://github. com/requeima/llm_processes. Along with the code, we provide a README file that details installation, configuration, and options in order to execute the experiments. ", "page_idx": 58}, {"type": "text", "text": "Guidelines: ", "page_idx": 58}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 58}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 58}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Justification: We supply all experimental setting/details required to reproduce the experiments. We do this by supplying full source code as well as thorough information in the experiment sections and the extensive appendix. ", "page_idx": 58}, {"type": "text", "text": "Guidelines: ", "page_idx": 58}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 58}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 58}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 58}, {"type": "text", "text": "Justification: Where the experiments have multiple runs, we show error bars in tables and charts. When we plot regression results, we report $95\\%$ confidence intervals. ", "page_idx": 59}, {"type": "text", "text": "Guidelines: ", "page_idx": 59}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 59}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 59}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 59}, {"type": "text", "text": "Justification: Appendix F details compute resources used and processing times needed to reproduce the experiments. The full research project required more compute than the experiments reported in the paper due to early and failed experiments that didn\u2019t make it into the paper. ", "page_idx": 59}, {"type": "text", "text": "Guidelines: ", "page_idx": 59}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 59}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 59}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 59}, {"type": "text", "text": "Justification: The authors have reviewed the NeurIPS Code of Ethics and will comply with them. ", "page_idx": 59}, {"type": "text", "text": "Guidelines: ", "page_idx": 59}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. \u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. ", "page_idx": 59}, {"type": "text", "text": "\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 60}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 60}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 60}, {"type": "text", "text": "Justification: Refer to Section 7 for a discussion of positive and negative societal impact of our work. ", "page_idx": 60}, {"type": "text", "text": "Guidelines: ", "page_idx": 60}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 60}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 60}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 60}, {"type": "text", "text": "Justification: We are not releasing any models and the datasets that we will release are simple numerical functions and pose pose no such risks. ", "page_idx": 60}, {"type": "text", "text": "Guidelines: ", "page_idx": 60}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 60}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 60}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 61}, {"type": "text", "text": "Justification: In our paper we use the following assets: ", "page_idx": 61}, {"type": "text", "text": "\u2022 LLMs: All of the LLMs we use are open source and we properly reference them in the paper and list versions and URLs where the weights can be obtained in the README file included with our source code.   \n\u2022 We use a dataset included with the LLMTime source code and this is referenced in the paper and acknowledged in out source code.   \n\u2022 We use datasets obtained from the internet (e.g. Weather, Housing) and properly acknowledge the source and abide by usage licences.   \n\u2022 We repurposed code for the black-box optimization functions which is properly referenced in the paper and acknowledged in the README file included with our source code. ", "page_idx": 61}, {"type": "text", "text": "Guidelines: ", "page_idx": 61}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 61}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 61}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 61}, {"type": "text", "text": "Justification: We create several new datasets for the paper and these are well documented either in the main body of the paper or the appendix (e.g. Appendices D.1 and D.2. We also include these assets in the source code included in the supplementary material. ", "page_idx": 61}, {"type": "text", "text": "Guidelines: ", "page_idx": 61}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 61}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 61}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 61}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 61}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 62}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 62}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 62}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 62}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 62}]