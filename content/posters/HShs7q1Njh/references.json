{"references": [{"fullname_first_author": "Nate Gruver", "paper_title": "Large language models are zero-shot time series forecasters", "publication_date": "2023-10-27", "reason": "This paper introduces a method for zero-shot time series forecasting using LLMs, which is directly relevant to the current paper's exploration of using LLMs for numerical prediction tasks."}, {"fullname_first_author": "Marta Garnelo", "paper_title": "Conditional neural processes", "publication_date": "2018-07-01", "reason": "This paper introduces Conditional Neural Processes (CNPs), a type of meta-learning model that learns a mapping from training sets to predictive distributions; it serves as a theoretical foundation for the current paper's LLM Processes."}, {"fullname_first_author": "Kristy Choi", "paper_title": "Lmpriors: Pre-trained language models as task-specific priors", "publication_date": "2022-10-27", "reason": "This paper explores using pre-trained language models as priors for various tasks, providing insights into leveraging LLMs for incorporating prior knowledge into predictions, a key aspect of the current paper."}, {"fullname_first_author": "Wessel P Bruinsma", "paper_title": "Autoregressive conditional neural processes", "publication_date": "2023-03-14", "reason": "This paper introduces Autoregressive Conditional Neural Processes (ACNPs), which are closely related to the autoregressive LLMPs explored in this paper, building upon the theoretical framework for joint predictive distributions."}, {"fullname_first_author": "Robert Vacareanu", "paper_title": "From words to numbers: Your large language model is secretly a capable regressor when given in-context examples", "publication_date": "2024-04-07", "reason": "This concurrent work demonstrates LLMs' ability to perform regression tasks, directly supporting the current paper's exploration of LLMs for numerical prediction and highlighting the importance of prompt engineering and conditioning on textual information."}]}