[{"type": "text", "text": "Explicit Flow Matching: On The Theory of Flow Matching Algorithms with Applications ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 This paper proposes a novel method, Explicit Flow Matching (ExFM), for training   \n2 and analyzing flow-based generative models. ExFM leverages a theoretically   \n3 grounded loss function, ExFM loss (a tractable form of Flow Matching (FM) loss),   \n4 to demonstrably reduce variance during training, leading to faster convergence and   \n5 more stable learning. Based on theoretical analysis of these formulas, we derived   \n6 exact expressions for the vector field (and score in stochastic cases) for model   \n7 examples (in particular, for separating multiple exponents), and in some simple   \n8 cases, exact solutions for trajectories. In addition, we also investigated simple cases   \n9 of diffusion generative models by adding a stochastic term and obtained an explicit   \n0 form of the expression for score. While the paper emphasizes the theoretical   \n11 underpinnings of ExFM, it also showcases its effectiveness through numerical   \n12 experiments on various datasets, including high-dimensional ones. Compared to   \n3 traditional FM methods, ExFM achieves superior performance in terms of both   \n4 learning speed and final outcomes. ", "page_idx": 0}, {"type": "text", "text": "15 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "16 In recent years, there has been a remarkable surge in Deep Learning, wherein the advancements   \n17 have transitioned from purely neural networks to tackling differential equations. Notably, Diffusion   \n18 Models [16] have emerged as key players in this field. This models transform a simple initial   \n19 distribution, usually a standard Gaussian distribution, into a target distribution via a solution of   \n20 Stochastic Differentiable Equation (SDE) [1] or Ordinary Differentiable Equation (ODE)[2] with   \n21 right-hand side representing a trained neural network. The Conditional Flow Matching (CFM) [9]   \n22 technique, which we focus on in our research, is a promising approach for constructing probability   \n23 distributions using conditional probability paths, which is notably a robust and stable alternative for   \n24 training Diffusion Models. The development of the CFM-based approach includes various techniques   \n25 and heuristics [4, 7, 13] aimed at improving convergence or quality of learning or inference. For   \n26 example, in the works [19, 20, 10] it was proposed to straighten the trajectories between points by   \n27 different methods, which led to serious modifications of the learning process. We refer the reader   \n28 for, example, to the paper [20] where different FM-based approaches are summarised, and to the   \n29 paper [9] for the connection between Diffusion Models and CFM.   \n30 In our work, we introduced an approach which we called Explicit Flow Matching (ExFM), to consider   \n31 the Flow Matching framework theoretically by modifying the loss and writing the explicit value of   \n32 the vector field. Strictly speaking, the presented loss is a tractable form of the FM loss, see Eq. (5)   \n33 of [9]. Base on this methods we can improve the convergence of the method in practical examples   \n34 reducing the variance of the loss, but the main focus of our paper is on theoretical derivations.   \n35 Our method allows us to write an expression for the vector field in closed form for quite simple   \n36 cases (Gaussian distributions), however, we note that Diffusion Models framework in the case of   \n37 a Gaussian Mixture of two Gaussian as a target distribution is still under investigation, see recent   \n38 publications [15, 8]. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "39 Our main contributions are: ", "page_idx": 1}, {"type": "text", "text": "40 1. A tractable form of the FM loss is presented, which reaches a minimum on the same function   \n41 as the loss used in Conditional Flow Matching, but has a smaller variance;   \n42 2. The explicit expression in integral form for the vector field delivering the minimum to this   \n43 loss (therefore for Flow Matching loss) is presented.   \n44 3. As a consequence, we derive expressions for the flow matching vector field and score in   \n45 several particular cases (when linear conditional mapping is used, normal distribution, etc.);   \n46 4. Analytical analysis of SGD convergence showed that our formula have better training   \n47 variance on several cases;   \n48 5. Numerical experiments show that we can achieve better learning results in fewer steps. ", "page_idx": 1}, {"type": "text", "text": "49 1.1 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "50 Flow matching is well known method for finding a flow to connect samples from two distribution   \n51 with densities $\\rho_{0}$ and $\\rho_{1}$ . It is done by solving continuity equation with respect to the time dependent   \n52 vector field ${\\overline{{v}}}(x,t)$ and time-dependent density $\\rho({\\boldsymbol{x}},t)$ with boundary conditions: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\displaystyle\\frac{\\partial\\rho({\\boldsymbol{x}},t)}{\\partial t}=-\\operatorname{div}(\\rho({\\boldsymbol{x}},t)\\overline{{{\\boldsymbol{v}}}}({\\boldsymbol{x}},t)),}\\\\ {\\rho({\\boldsymbol{x}},0)=\\rho_{0}({\\boldsymbol{x}}),\\quad\\rho({\\boldsymbol{x}},1)=\\rho_{1}({\\boldsymbol{x}}).}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "53 Function $\\rho({\\boldsymbol{x}},t)$ is called probability density path. Typically, the distribution $\\rho_{0}$ is known and it is   \n54 chosen for convenience reasons, for example, as standard normal distribution $\\rho(x)=\\mathcal{N}(x\\mid0,I)$ .   \n55 The distribution $\\rho_{1}$ is unknown and we only know the set of samples from it, so the problem is to   \n56 approximate the vector field $v(x,t)\\approx\\overline{{v}}(x,t)$ using these samples. To make problem (1) well defined,   \n57 one usually imposes additional regularity conditions on the densities, such as smoothness. The   \n58 rigorous justification of the obtained results we put in the Appendix, leaving the general formulations   \n59 of theorems and ideas in the main text.   \n60 From a given vector field, we can construct a flow $\\phi_{t}$ , i. e., a time-dependent map, satisfying the   \n61 ODE \u2202\u03d5t(x) $\\begin{array}{r}{\\frac{\\partial\\bar{\\phi_{t}}(x)}{\\partial t}=v(\\phi_{t}(x),t)}\\end{array}$ with initial condition $\\phi_{0}(x)=x$ . Thus, one can sample a point $x_{0}$ from   \n62 the distribution $\\rho_{0}$ and then using this ODE obtain a point $x_{1}=\\phi_{1}(x_{0})$ which have a distribution   \n63 approximately equal to $\\rho_{1}$ . For given boundary $\\rho_{0}$ and $\\rho_{1}$ , the vector field or path solutions are not   \n64 the only solutions, but if we have found any solution, it will already allow us to sample from the   \n65 unknown density $r h o_{1}$ . However, if the problem is more narrowly defined, $e.\\,g.$ , one needs to have a   \n66 map that is close to the Optimal Transport (OT) map, we have to impose additional constraints.   \n67 The problem of finding any vector field $v$ is solved in conditional manner in the paper [9], where   \n68 so-called Conditional Flow Matching (CFM) is present. Namely, the following loss function was   \n69 introduced for the training a model $v_{\\theta}$ which depends on parameters $\\theta$ ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "equation", "text": "$$\nL_{\\mathrm{CFM}}(\\theta)=\\mathbb{E}_{t}\\mathbb{E}_{x_{1},x_{0}}\\big\\|v_{\\theta}(\\phi_{t,x_{1}}(x_{0}),\\,t)-\\phi_{t,x_{1}}^{\\prime}(x_{0})\\big\\|^{2},\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "70 where $\\phi_{t,x_{1}}(x_{0})$ is some flow, conditioned on $x_{1}$ (one can take $\\phi_{t,x_{1}}(x_{0})=(1-t)x_{0}+t x_{1}+\\sigma_{s}t x_{0}$   \n71 in the simplest case, where $\\sigma_{s}~>~0$ is a small parameter need for this map to be invertable at   \n72 any $0\\leq t\\leq1)$ ). Hereinafter the dash indicates the time derivative. Time variable $t$ is uniformly   \n73 distributed: $t\\sim\\mathcal{U}[0,1]$ and random variables $x_{0}$ and $x_{1}$ are distributed according to the initial and   \n74 final distributions, respectively: $x_{0}\\sim\\rho_{0}$ , $x_{1}\\sim\\rho_{1}$ . Below we omit specifying of the symbol $\\mathbb{E}$ the   \n75 distribution by which the expectation is taken where it does not lead to ambiguity. ", "page_idx": 1}, {"type": "text", "text": "76 1.2 Why new method? ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "77 Model training using loss (2) have the following disadvantage: during training, due to the randomness   \n78 of $x_{0}$ and $x_{1}$ , significantly different values can be presented for model as output value at close model   \n79 argument values $(x_{t},t)$ . Indeed, a fixed point $x_{t}=\\phi_{t,x_{1}}(x_{0})$ can be obtained by an infinite set of $x_{0}$   \n80 and $x_{1}$ pairs, some of which are directly opposite, and at least for small times $t$ the probability of these   \n81 different directions may not be significantly different. At the same time, data $\\bar{\\phi}_{t,x_{1}}^{\\prime}(x_{0})$ on which the   \n82 model learns significantly different for such different positions of pairs $x_{0}$ and $x_{1}$ . Thus, the model is   \n83 forced to do two functions during training: generalize and take the mathematical expectation (clean   \n84 the data from noise).   \n85 In our approach, see Fig. 1(a), we feed the model input with cleaned data with small variance. Thus,   \n86 the model only needs to generalize the data, which happens much faster (in fewer training steps).   \n87 Moreover, in the process of constructing the modified loss, we have developed the exact formula for   \n88 the vector field, see Eq. (11), (34). The existence of an explicit formula for the vector field is of great   \n89 importance not only from a theoretical but also from a practical point of view. ", "page_idx": 1}, {"type": "image", "img_path": "XYDMAckWMa/tmp/48dcbee746a77a839b4d15fb70540ea218053c6ac14f550b907034b67012277f.jpg", "img_caption": ["Figure 1: (Left) The key novelty of our approach is that in classical CFM, highly divergent directions can appear in a small spatial area at similar times (left part). In our approach (right part) we average over these vectors, training the model on a smoothed unnoised vector field. (Right) The comparison evaluated dispersion norm over time parameter $t$ for CFM and ExFM in matching standard Gaussian $\\rho_{0}=\\mathcal{N}(0,I)$ to general Gaussian $\\bar{\\rho_{1}}=\\mathcal{N}(\\mu,\\sigma^{2}I)$ distributions. The y-axis represents the sum of dispersion vector components, denoted as $|\\mathbb{D}_{x,x_{1}}\\Delta\\dot{v}(x,t)|$ . The left panel illustrates samples drawn from the $\\rho_{0}$ and $\\rho_{1}$ distributions, as well as the corresponding flows. The right panel depicts the dispersion trend over time for both CFM (black line) and ExFM (red line) objectives. The dotted lines correspond to the dispersion levels (in top-down order $|\\mathbb{D}x_{1}|,|\\mathbb{D}x_{0}|,|\\mathbb{D}x_{1}\\bar{|}/N.$ . "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "90 2 Main idea ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "91 2.1 Modified objective ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "92 Lets expand the last two mathematical expectations in the loss (2) and substitute variables using   \n93 map $\\phi_{t,x_{1}}$ , passing from the point $x_{0}$ to its position $x_{t}=\\phi_{t,x_{1}}(x_{0})$ at time $t$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{x_{1},x_{0}}\\big\\|v_{\\theta}(\\phi_{t,x_{1}}(x_{0}),t)-\\phi_{t,x_{1}}^{\\prime}(x_{0})\\big\\|^{2}\\!=\\!\\!\\displaystyle\\iint\\big\\|v_{\\theta}(\\phi_{t,x_{1}}(x_{0}),t)-\\phi_{t,x_{1}}^{\\prime}(x_{0})\\big\\|^{2}\\rho_{0}(x_{0})\\rho_{1}(x_{1})\\mathrm{d}x_{0}\\mathrm{d}x_{1}}\\\\ &{=\\!\\!\\displaystyle\\iint\\big\\|v_{\\theta}(x_{t},t)-\\phi_{t,x_{1}}^{\\prime}\\big(\\phi_{t,x_{1}}^{-1}(x_{t})\\big)\\big\\|^{2}\\underbrace{\\mathrm{det}\\Bigg[\\partial\\phi_{t,x_{1}}^{-1}(x)/\\partial x\\;\\bigg\\|_{x=x_{t}}\\bigg]\\rho_{0}\\big(\\phi_{t,x_{1}}^{-1}(x_{t})\\big)}_{\\rho_{x_{1}}(x_{t},t)}\\rho_{1}(x_{1})\\,\\mathrm{d}x_{t}\\,\\mathrm{d}x_{1}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\mathbb{E}_{x_{1},x_{t}\\sim\\rho_{x_{1}}(\\cdot,t)}\\big\\|v_{\\theta}(x_{t},t)-\\phi_{t,x_{1}}^{\\prime}\\big(\\phi_{t,x_{1}}^{-1}(x_{t})\\big)\\big\\|^{2}.\\quad(3)}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "94 We assume, that the map $\\phi_{t,x_{1}}$ is invertible at each $0\\,<\\,t\\,<\\,1,\\,i.\\,e$ . that $\\phi_{t,x_{1}}^{-1}(x_{t})$ exits on this   \n95 time interval and for all $x_{t}\\,=\\,\\{\\phi_{t}(x_{0})\\,\\mid\\,\\forall x_{0}\\,:\\,\\rho(x_{0})\\,>\\,0\\}$ . Eq. (3) can be seen as a transition   \n96 from expectation on the variable $x_{0}\\,\\sim\\,\\rho_{0}$ to expectation on the variable $x_{t}\\,\\sim\\,\\rho_{x_{1}}(\\cdot,t)$ , where   \n97 $\\rho_{x_{1}}(x,t)=[\\phi_{t,x_{1}}]_{*}\\rho_{0}(x):=\\rho_{0}\\left(\\phi_{t,x_{1}}^{-1}(x)\\right)\\operatorname*{det}\\left[\\partial\\phi_{t,x_{1}}^{-1}(x)/\\partial x\\right]$ . See paper [5] for details about the   \n98 push-forward operator $\\ldots,$ . Our representation (3) is very similar to expression (9) of the cited   \n99 paper [9], only we write it in terms of the conditional flow rather than the conditional vector field.   \n100 To obtain the modified loss, we return to end of the standard CFM loss representation in (3). It is   \n101 written as the expectation over two random variables $x_{1}$ and $x_{t}$ having a common distribution density ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\{x_{1},x_{t}\\}\\sim\\rho_{j}(x_{1},x_{t},t)=\\rho_{x_{1}}(x_{t},t)\\rho_{1}(x_{1}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "102 which, generally speaking, is not factorizable. Let us rewrite this expectations in terms of two inde  \n103 pendent random variables, each of which have its marginal distribution. The marginal distribution $\\rho_{m}$   \n104 of $x_{t}$ can be obtained via integration: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\rho_{m}(x_{t},t)=\\int\\rho_{j}(x_{1},x_{t},t)\\,\\mathrm{d}x_{1}=\\int\\rho_{x_{1}}(x_{t},t)\\rho_{1}(x_{1})\\,\\mathrm{d}x_{1}\\,,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "105 while the marginal distribution of $x_{1}$ is just (unknown) function $\\rho_{1}$ . Let for convenience $w(t,x_{1},x)=$   \n106 $\\phi_{t,x_{1}}^{\\prime}\\bigl(\\phi_{t,x_{1}}^{-1}(x)\\bigr)^{1}$ . We have ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\cal L}_{\\mathrm{CFM}}(\\theta)=\\mathbb{E}_{t,x_{1},x_{t}\\sim\\rho_{x_{1}}(\\cdot,t)}\\|v_{\\theta}(x_{t},\\ t)-w(t,x_{1},x_{t})\\|^{2}=}}\\\\ {\\displaystyle\\qquad\\int_{0}^{1}\\iint\\|v_{\\theta}(x_{t},\\ t)-w(t,x_{1},x_{t})\\|^{2}\\rho_{x_{1}}(x,t)\\rho_{1}(x_{1})\\,\\mathrm{d}x_{t}\\,\\mathrm{d}x_{1}\\mathrm{d}t=}\\\\ {\\displaystyle\\int_{0}^{1}\\iint\\|v_{\\theta}(x_{t},\\ t)-w(t,x_{1},x_{t})\\|^{2}\\,\\left(\\rho_{x_{1}}(x_{t},t)/\\rho_{m}(x_{t},t)\\right)\\rho_{m}(x_{t},t)\\rho_{1}(x_{1})\\,\\mathrm{d}x_{t}\\,\\mathrm{d}x_{1}\\mathrm{d}t=}\\\\ {\\displaystyle\\mathbb{E}_{t,x_{1},x\\sim\\rho_{m}(\\cdot,t)}\\|v_{\\theta}(x,t)-w(t,x_{1},x)\\|^{2}\\,\\rho_{c}(x|x_{1},t)/\\rho_{1}(x_{1}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "107 where we introduce a conditional distribution ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\rho_{c}(x|x_{1},t):=\\rho_{x_{1}}(x,t)\\rho_{1}(x_{1})/\\rho_{m}(x,t):=\\rho_{x_{1}}(x,t)\\rho_{1}(x_{1})\\Biggl/\\int\\rho_{x_{1}}(x,t)\\rho_{1}(x_{1})\\,\\mathrm{d}x_{1}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "108 The key feature of the representation (6) is that the integration variables $x_{1}$ and $x$ are independent.   \n109 Thus, we can evaluate them using Monte Carlo-like schemes in different ways. However, we go   \n110 further and make a modification to this loss to reduce the variance of Monte Carlo methods. ", "page_idx": 3}, {"type": "text", "text": "111 2.2 New loss and exact expression for vector field ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "112 Note that so far the expression for $L_{\\mathrm{CFM}}$ have not changed, it has just been rewritten in different forms.   \n113 Now we change this expression so that its numerical value, generally speaking, may be different, but   \n114 the derivative of the model parameters will be the same. We introduce the following loss ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L_{\\mathtt{E x F M}}(\\theta)=\\mathbb{E}_{t}\\mathbb{E}_{x\\sim\\rho_{m}}\\Big\\|v_{\\theta}(x,\\,t)-\\mathbb{E}_{x_{1}\\sim\\rho_{1}}w(t,x_{1},x)\\rho_{c}(x|x_{1},t)/\\rho_{1}(x_{1})\\Big\\|^{2}=}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\int_{0}^{1}\\int\\Big\\|v_{\\theta}(x,\\,t)-\\int w(t,x_{1},x)\\times\\rho_{c}(x|x_{1},t)\\,\\mathrm{d}x_{1}\\Big\\|^{2}\\rho_{m}(x,t)\\,\\mathrm{d}x\\,\\mathrm{d}t.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "115 Theorem 2.1. Losses $L_{C F M}$ in Eq. (2) and $L_{E x F M}$ in Eq. (8) have the same derivative with respect to   \n116 model parameters: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{d}L_{C F M}(\\theta)/\\mathrm{d}\\theta=\\mathrm{d}L_{E x F M}(\\theta)/\\mathrm{d}\\theta\\ .\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "117 Proof is in the Appendix A.1. ", "page_idx": 3}, {"type": "text", "text": "118 In the presented loss $L_{\\mathrm{ExFM}}$ , the integration (outside the norm operator) proceeds on those variables   \n119 on which the model depends, while inside this operator there are no other free variables. Thus, using   \n120 this kind of loss, it is possible to find an exact analytical expression for the vector field for which the   \n121 minimum of this loss is zero (unlike the loss $L_{\\mathrm{CFM}}$ ). Namely, we have ", "page_idx": 3}, {"type": "equation", "text": "$$\nv(x,t)=\\int w(t,x_{1},x)\\rho_{c}(x|x_{1},t)\\,\\mathrm{d}x_{1}\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "122 We can obtain the exact form of this vector field given the particular map $\\phi_{t,x_{1}}$ . For example, the   \n123 following statement holds:   \n124 Corollary 2.2. Consider the linear conditioned flow $\\phi_{t,x_{1}}(x_{0})=(1-t)x_{0}+t x_{1}$ which is inevitable   \n125 as $0\\,\\leq\\,t\\,<\\,1$ . Then $\\begin{array}{r}{w(t,x_{1},x)\\,=\\,\\frac{x_{1}-x}{1-t}}\\end{array}$ , $\\begin{array}{r}{\\rho_{x_{1}}(x,t)\\,=\\,\\rho_{0}\\left(\\frac{x-x_{1}t}{1-t}\\right)\\frac{1}{(1-t)^{d}}}\\end{array}$ and the loss $L_{E x F M}$ in   \n126 Eq. (8) reaches zero value when the model of the vector field have the following analytical form ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\nv(x,t)=\\int(x_{1}-x)\\rho_{0}\\left(\\frac{x-x_{1}t}{1-t}\\right)\\rho_{1}(x_{1})\\,\\mathrm{d}x_{1}\\,\\bigg/\\left((1-t)\\int\\rho_{0}\\left(\\frac{x-x_{1}t}{1-t}\\right)\\rho_{1}(x_{1})\\,\\mathrm{d}x_{1}\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "127 This is the exact value of the vector field whose flow translates the given distribution $\\rho_{0}\\ t o\\ \\rho_{1}$ . ", "page_idx": 4}, {"type": "text", "text": "128 Complete proofs are in the Appendix A.3.1. Note that the result (11) is not totally new, for example,   \n129 a similar result (though in the form of a general expression rather than an explicit formula), was given   \n130 in [19], Eq. (9). However, our contribution consists of both the general form (10) and practical and   \n131 theoretical conclusions from it (see below). ", "page_idx": 4}, {"type": "text", "text": "132 Remark 2.3. In the case of the initial and final times $t=0$ , 1, Eq. (11) is noticeably simpler ", "page_idx": 4}, {"type": "equation", "text": "$$\nv(x,0)=\\mathbb{E}_{x_{1}}x_{1}-x=\\int x_{1}\\rho_{1}(x_{1})\\,\\mathrm{d}x_{1}-x.\\quad v(x,1)=x-\\int x_{0}\\rho_{0}(x_{0})\\,\\mathrm{d}x_{0}\\,.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "133 This expression for the initial velocity means that each point first tends to the center of mass of the   \n134 unknown distribution $\\rho_{1}$ regardless of its initial position.   \n135 Extensions to SDE Now let the conditional map be stochastic: $\\phi_{t,x_{1}}=(1-t)x_{0}+t x_{1}+\\sigma_{e}(t)\\epsilon_{i}$ ,   \n136 where $\\mathbf{\\boldsymbol{\\epsilon}}\\sim\\mathcal{N}(\\mathbf{\\boldsymbol{0}},\\mathbf{\\boldsymbol{1}})$ . Typically, $\\sigma_{e}(0)=\\sigma_{e}(1)=0$ , for example, $\\sigma_{e}(t)=t(1-t)\\sigma_{e}$ .   \n137 Note that this formulation covers (with appropriate selection of the $\\sigma_{e}(t)$ parameter) the case of   \n138 diffusion models [20].   \n139 Then, we can write the exact solution for a so-called score and flow matching objective (see [20] for   \n140 details) ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{[\\mathrm{SF}]^{2}\\mathrm{M}}(\\theta)=\\mathbb{E}\\big[\\underbrace{\\|v_{\\theta}(x,t)-u_{t}^{\\circ}(x)\\|^{2}}_{\\mathrm{flow~matching~loss}}+\\lambda(t)^{2}\\underbrace{\\|s_{\\theta}(x,t)-\\nabla\\log p_{t}(x)\\|^{2}}_{\\mathrm{score~matching~loss}}\\big].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "141 that corresponds to this map. In the last expression, the following explicit conditional expressions are   \n142 considered in the cited paper for the case $\\sigma_{e}(t)=\\sqrt{t(1-t)}\\sigma_{e}$ ", "page_idx": 4}, {"type": "equation", "text": "$$\nu_{t}^{\\circ}(x)=\\frac{1-2t}{t(1-t)}(x-(t x_{1}+(1-t)x_{0}))+(x_{1}-x_{0}),\\ \\ \\nabla\\log p_{t}(x)=\\frac{t x_{1}+(1-t)x_{0}-x_{1}t(1-t)}{\\sigma_{e}^{2}t(1-t)}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "143 The exact solution (our result, explicit analog of the Eq. (10) from [20]) under consideration has   \n144 the form (44) and (46) and, for example for the for the Gaussian $\\rho_{0}$ this expressions reduced to the   \n145 Eq. (49) and (50), correspondingly. See Appendix E for the details on this case.   \n146 Simple examples Consider the case of Standard Normal Distribution as $\\rho_{0}$ and Gaussian Mixture   \n147 of two Gaussians as $\\rho_{1}$ . Vector field have a closed form (37) in this case, and we can fast numerically   \n148 solve ODE for trajectories. Random generated trajectories and plot of the vector field are shown   \n149 on Fig. 2 (a)\u2013(b). Detailed explanation of this case is in the Sec. D.2. Another example is related   \n150 to the case of a stochastic map in the form of Brownian Bridge, which briefly described in the last   \n151 paragraph and considered in Sec. E.3.2 in details, see Fig. 2 (c)\u2013(f). Note that at some $\\sigma_{e}$ values the   \n152 trajectories are a little bit straightened in this case compared to the usual linear map, if we compare   \n153 cases on the Fig. 6. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "154 2.3 Training scheme based on the modified loss ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "155 Let us consider the difference between our new scheme based on loss $L_{\\mathrm{ExFM}}$ and the classical CFM   \n156 learning scheme. As a basis for the implementation of the learning scheme, we take the open-source   \n157 code2 from the works [20, 19].   \n158 Consider a general framework of numerical schemes in classical CFM. We first sample $m$ random time   \n159 variables $t\\sim\\mathcal{U}[0,1]$ . Then we sample several values of $x$ . To do this, we sample a certain number $n$   \n160 samples $\\{x_{0}^{i}\\}_{i=1}^{n}$ from the \u201cnoisy\u201d distribution $\\rho_{0}$ , and the same number $n$ of samples $\\{x_{1}^{i}\\}_{i=1}^{n}$ from   \n161 the unknown distribution $\\rho_{1}$ . Then we pair them (according to some scheme), and get $n$ samples as   \n162 $x^{j,i}=\\phi_{t^{j},x_{1}^{i}}(x_{0}^{i})$ (e. g. a linear combination in the simple case of linear map: $x^{j,i}=(\\stackrel{\\bullet}{1}-t^{j})x_{0}^{i}+t^{j}x_{1}^{i})$ ,   \n163 $\\forall i=1,2,\\therefore,n;\\forall j=1,2,\\ldots,m$ . Note, than one of the variable $n$ or $m$ (or both) can be equal to 1. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "image", "img_path": "XYDMAckWMa/tmp/07e24193eb6f66c4d5499411f24b93ab8d27d3a47baade3cb847f48a9fdb97f3.jpg", "img_caption": ["Figure 2: Trajectories and vector field obtained in simple cases: (a) $N=80$ random trajectories from $\\breve{\\mathscr{N}}(\\cdot|\\,0,1^{2})$ to GM; (b) 2D plot of the vector field in this case (c)\u2013(f) $N=40$ random trajectories from $\\mathcal{N}\\left(\\cdot|\\,0,1^{2}\\right)$ to $\\mathcal{N}\\left(\\cdot|\\,2,3^{2}\\right)$ and 2D plot of the vector fieldfor different $\\sigma_{e}$ for the Brownian Bridge map "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "164 At the step 2, the following discrete loss is constructed from the obtained samples ", "page_idx": 5}, {"type": "equation", "text": "$$\nL_{\\mathrm{CFM}}^{d}(\\theta)=\\sum_{j=1}^{m}\\sum_{i=1}^{n}\\Bigl\\|v_{\\theta}(x^{j,i},\\,t^{j})-\\phi_{t^{j},x_{1}^{i}}^{\\prime}(x_{0}^{i})\\Bigr\\|^{2}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "165 Finally, we do a standard gradient descent step to update model parameters $\\theta$ using this loss. ", "page_idx": 5}, {"type": "text", "text": "166 The first and last step in our algorithm is the same as in the standard algorithm, but the second step is   \n167 significantly different. Namely, we additionally generate a sufficiently large number $N\\gg n\\cdot m$ of   \n168 samples ${\\overline{{x}}}_{1}$ from the unknown distribution $\\rho_{1}$ , sampling $(N-n)$ new samples and adding to it the   \n169 samples $\\{x_{1}^{i}\\}_{1}^{n}$ that are already obtained on the previous step.   \n170 Then we form the following discrete loss which replaces the integral on $x_{1}$ in $L_{\\mathrm{ExFM}}$ by its evalua  \n171 tion $v^{d}$ by self-normalized importance sampling or rejection sampling (see Appendix $\\mathbf{B}$ for details) ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\nL_{\\mathtt{E x F M}}^{d}(\\theta)=\\sum_{j=1}^{m}\\sum_{i=1}^{n}\\Bigl\\|v_{\\theta}(x^{j,i},\\,t^{j})-v^{d}(x^{j,i},\\,t^{j})\\Bigr\\|^{2}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "172 For example, if we use self-normalized importance sampling and assume that the Jacobian   \n173 $\\operatorname*{det}\\left[{\\partial\\phi_{t,x_{1}}^{-1}(x)}/{\\partial x}\\right]$ do not depend on $x_{1}$ , we can write ", "page_idx": 5}, {"type": "equation", "text": "$$\nv^{d}(x,\\,t)=\\left(\\sum_{k=1}^{N}w(t,\\overline{{x}}_{1}^{k},x)\\rho_{0}\\big(\\phi_{t,\\overline{{x}}_{1}^{k}}^{-1}(x)\\big)\\right)\\Biggl/\\sum_{k=1}^{N}\\rho_{0}\\big(\\phi_{t,\\overline{{x}}_{1}^{k}}^{-1}(x)\\big)\\ .\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "174 Theorem 2.4. Under mild conditions, the error variance of the integral gradient (9) using the Monte   \n175 Carlo method (14) is lower than using formula (13) with the same number $n\\cdot m$ of samples for $\\{x\\}$ .   \n176 Sketch of the proof is in the Appendix A.2. The steps of our scheme are formally summarized in   \n177 Algorithm 1.   \n178 Particular case of linear map and Gaussian noise Let $\\phi_{t,x_{1}}$ be the linear flow: $\\phi_{t,x_{1}}(x_{0})\\;=\\;$   \n179 $(1-t)x_{0}+t x_{1}$ . and consider the case of standard normal distribution for the initial density $\\rho_{0}$ :   \n180 $\\rho_{0}(x)\\sim\\mathcal{N}(x\\mid0,I)$ . Then in the case of using self-normalized importance sampling, we have ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\nv^{d}(x,\\ t)=\\sum_{k=1}^{N}{\\frac{{\\overline{{x}}}_{1}^{k}-x}{1-t}}\\bigl(\\mathrm{SoftMax}(Y^{1},\\ldots,Y^{N})\\bigr)_{k},\\quad\\mathrm{where}\\quad Y^{k}=-{\\frac{1}{2}}{\\frac{\\left\\|x-t\\cdot{\\overline{{x}}}_{1}^{k}\\right\\|_{\\mathbb{R}^{d}}^{2}}{1-t}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "181 Here, the lower index $k$ in SoftMax stands for the $k$ -th component, and the SoftMax operation itself   \n182 came about due to exponents in the Gaussian density as a more stable substitute for computing than   \n183 directly through exponents.   \n184 Extension of other maps and initial densities $\\rho_{0}$ Common expression (10) can be reduced to   \n185 closed form for the particular choices of density $\\rho_{0}$ and map $\\phi$ (consequently, expression for $w$ ). We   \n186 summarise several known approaches for which FM-based techniques can be applied in Table $1^{3}$ .   \n187 See Appendix C and D for derivations of formulas and for more extensions.   \n188 Complexity We assume that the main running time of the algorithm is spent on training the model,   \n189 especially if it is quite complex. Thus, the running time of one training step depends crucially on the   \n190 number $n\\cdot m$ of samples $\\{x\\}$ and it is approximately the same for both algorithms: the addition of   \n191 points $\\overline{{x}}_{1}$ entails only an additional calculation using formula (16), which can be done quickly and,   \n192 moreover, can be simple parallelized. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "table", "img_path": "XYDMAckWMa/tmp/6946d2f29f78f11f5ae4d83128841bdc2f8ff117a7986fff9f0e8c9c7543eb7d.jpg", "table_caption": ["Table 1: Correspondence between some methods which can reduced to FM framework and our theoretical descriptions of them. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "193 2.4 Irreducible dispersion of gradient for CFM optimization ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "194 Ensuring the stability of optimization is vital. Let $\\Delta\\theta$ be changes in parameters, obtained by SGD   \n195 with step size $\\gamma/2$ applied to the functional from Eq. (13): ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\Delta v(x^{j,i},t^{j})=-\\gamma\\cdot\\big(v(x^{j,i},t^{j})-v^{d}(x^{j,i},\\,t^{j})\\big).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "196 For simplification, we consider a function, $v_{\\theta}(x,t)$ , capable of perfectly ftiting the CFM problem and   \n197 providing an optimal solution for any point $x$ and time $t$ . For a linear conditional flow at a specific   \n198 point $x^{j,\\bar{i}}\\sim\\bar{\\rho_{x_{1}^{i}}}(\\cdot,t^{j})$ at time $t^{j}\\sim U(0,1)$ , the update $\\Delta v(x^{j,i},t^{j})$ can be represented as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\Delta v(x^{j,i},t^{j})=\\gamma\\left(x_{1}^{i}-\\hat{x}_{0}^{i}-v(x^{j,i},t^{j})\\right),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "199 where $\\begin{array}{r}{\\hat{x}_{0}^{i}=\\frac{x^{j,i}-t^{j}x_{1}^{i}}{1-t^{j}}}\\end{array}$ . We define the dispersion $\\mathbb{D}_{x,x_{1}}f(x,x_{1})$ for $x\\sim\\rho_{x_{1}}(\\cdot,t)$ and $x_{1}\\sim\\rho_{1}$ ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{D}_{x,x_{1}}f(x,x_{1})=\\mathbb{E}_{x,x_{1}}f^{2}(x,x_{1})-(\\mathbb{E}_{x,x_{1}}f(x,x_{1}))^{2}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "200 Proposition 2.5. At the time $t=0$ , the dispersion of update in the form (18) have the following   \n201 element-wise lower bound: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{D}_{x^{j,i},x_{1}^{i}}\\Delta v(x^{j,i},0)=\\gamma^{2}\\mathbb{D}_{x_{1}^{i}}x_{1}^{i}+\\gamma^{2}\\mathbb{D}_{x^{j,i},x_{1}^{i}}(x^{j,i}+v(x^{j,i},0))\\geq\\gamma^{2}\\mathbb{D}_{x_{1}^{i}}x_{1}^{i}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "202 Equality is reached when the model $v(x^{j,i},0)$ has exact values equal to (12). ", "page_idx": 6}, {"type": "text", "text": "203 Given that the dispersion cannot be reduced with an increase in batch size, the only available option   \n204 is to decrease the step size of the optimization method, $i,e.$ , reduce the learning rate slowing down the   \n205 convergence. The situation is much better for the proposed loss in (14). We can express the update   \n206 $\\Delta v(x^{j,\\bar{i}},t^{j})$ in the case of ExFM objective as: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\Delta v(x^{j},t^{j})=\\gamma^{2}\\Big(\\sum_{k=1}^{N}x_{1}^{k}\\tilde{\\rho}\\left(x^{j,i}|x_{1}^{k},t^{j}\\right)-x^{j,i}-v(x^{j,i},t^{j})\\Big),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "207 where $x^{j,i}\\sim\\rho_{x_{1}^{i}}(\\cdot,t^{j}),x_{1}^{k}\\sim\\rho_{1}$ and $\\begin{array}{r}{\\tilde{\\rho}\\left(x^{j,i}|x_{1}^{k},t^{j}\\right)=\\rho_{0}\\left(\\frac{x^{j,i}-t^{j}x_{1}^{k}}{1-t^{j}}\\right)/\\sum_{k=1}^{N}\\rho_{0}\\left(\\frac{x^{j,i}-t^{j}x_{1}^{k}}{1-t^{j}}\\right)}\\end{array}$ . Similar   \n208 to the derivations in the previous part, we can found simplified form for the dispersion of update at   \n209 $t=0$ .   \n210 Proposition 2.6. At the time $t=0$ , the dispersion of update from (20) have the following element-wise   \n211 lower bound: ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{D}_{x^{j,i},x_{1}^{k}}\\Delta v(x^{j,i},0)=\\frac{\\gamma^{2}}{N}\\mathbb{D}_{x_{1}^{k}}x_{1}^{k}+\\gamma^{2}\\mathbb{D}_{x^{j,i},x_{1}^{k}}(x^{j,i}+v(x^{j,i},0))\\geq\\frac{\\gamma^{2}}{N}\\mathbb{D}_{x_{1}^{k}}x_{1}^{k}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "212 Equality is reached when the model $v(x^{j,i},0)$ has exact values equal to (12). ", "page_idx": 6}, {"type": "text", "text": "213 In comparison to CFM, the dispersion of the update is $N$ times smaller than the dispersion of the   \n214 target distribution and could be controlled without impeding convergence by adjusting the number of   \n215 samples $N$ . In Figure 1(b), we visually compare the dispersions of CFM and ExFM. The illustration   \n216 aligns a standard normal distribution ${\\mathcal{N}}(0,I)$ with a shifted and scaled variant $\\mathcal{N}(\\mu,I\\sigma^{2})$ . ExFM   \n217 yields lower dispersion throughout the range $t\\in[0,1]$ . Detailed analytical calculations of the optimal   \n218 velocity $\\boldsymbol{v}(\\boldsymbol{x},t)$ and dispersion are provided in the Appendix G. ", "page_idx": 7}, {"type": "image", "img_path": "XYDMAckWMa/tmp/6f6312c87ac08152e79369b474689821b263307c54a78a7220d4a8de4bf90891.jpg", "img_caption": ["Figure 3: Visual comparison of methods on toy 2D data. First row are original samples, second row sampled by ExFM, third row sampled by CFM. "], "img_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "XYDMAckWMa/tmp/b0bbfab81bdb022c8af9d7983e796abbef0771368df03e1fe426fe2cee950cc1.jpg", "table_caption": ["Table 2: ExFM and CFM metrics comparison table on toy 2D data. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "XYDMAckWMa/tmp/a2ec9d90f8ab493acd3866c41ead3b6f04f34fbbe6d7568c3b5c0ff1a31cfbf4.jpg", "table_caption": ["Table 3: NLL comparison for ExFM, CFM and OT-CFM methods over 10 000 learning steps, mean and std taken from 10 sampling iterations. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "219 3 Numerical Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "220 Toy 2D data We conducted unconditional density estimation among eight distributions. Additional   \n221 details of the experiments see in the Appendix H. We commence the exposition of our findings   \n222 by showcasing a series of classical 2-dimensional examples, as depicted in Fig. 3 and Table 2.   \n223 Our observations indicate that ExFM adeptly handles complex distribution shapes is particularly   \n224 noteworthy, especially considering its ability to do so within a small number of epochs. Additionally,   \n225 the visual comparison underscores the evident superiority of ExFM over the CFM approach.   \n226 Tabular data We conducted unconditional density estimation on five tabular datasets, namely   \n227 power, gas, hepmass, minibone, and BSDS300. Additional details of the experiments see in the   \n228 Appendix H. The empirical findings obtained from the numerical experiments from Table 3 indicate   \n229 a statistically significant improvement in the performance of our proposed method. Notably, ExFM   \n230 demonstrates a notable acceleration in convergence rate.   \n231 High-dimensional data and additional experiments We conducted experiments on high  \n232 dimensional data, among them experiments on CIFAR10 and MNIST dataset. FID results on   \n233 CIFAR10 shows slightly better score among sampled images. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "234 Additional details of the experiments and sampled images see in the Appendix H. ", "page_idx": 8}, {"type": "text", "text": "235 Stochastic ExFM (ExFM-S) on toy 2D data We evaluated the performance of the stochastic   \n236 version of ExFM (ExFM-S) with use of expressions given in Sec. E.3.2 on four standard toy datasets.   \n237 The primary experimental setup follows that used in [19]. Additional details on the hyperparameters   \n238 used are available in Appendix H. Based on the findings presented in Table 4, we determine that   \n239 ExFM-S surpasses I-CFM on all four datasets in terms of generative performance $(\\mathcal{W}_{2})$ and also   \n240 outperforms in terms of OT optimality (NPE) on two of them, exhibiting similar results on the   \n241 remaining datasets. It also demonstrates performance similar to OT-CFM. While ExFM-S is not as   \n242 robust as the basic ExFM, it enables the matching of one dataset to another (moons $\\rightarrow8$ gaussians) as   \n243 it does not necessitate the presence of an explicit formula for $\\rho_{0}$ . Among other things, this experiment   \ndemonstrates the feasibility of our methods when both distributions $\\rho_{0}$ and $\\rho_{1}$ are unknown. ", "page_idx": 8}, {"type": "text", "text": "Table 4: ExFM-S evaluation on four toy datasets ( $\\mu\\pm\\sigma$ over three seeds). For comparison we take I-CFM, OT-CFM, and ExFM (no values for moons $\\rightarrow8$ gaussians due to the absence of explicit formula for $\\rho_{0}$ ). Performance in generative modeling $(\\mathcal{W}_{2})$ and dynamic OT optimality (NPE) is assessed. The best result for each metric is highlighted in bold. Instances where we outperform CFM are underscored. ", "page_idx": 8}, {"type": "table", "img_path": "XYDMAckWMa/tmp/a7dbf5515928e3fc0ad0888c3d7e2d397e4a81e8db7b156091f9ac907d83775d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "244 ", "page_idx": 8}, {"type": "text", "text": "245 4 Conclusions ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "246 The presented method introduces a new loss function in tracrable form (in terms of integrals) that   \n247 improves upon the existing Conditional Flow Matching approach. New loss as a function of the model   \n248 parameters, reaches zero at its minimum. Thanks to this, we can: a) write an explicit expression for   \n249 the vector field on which the loss minimum is achieved; b) get a smaller variance when training on   \n250 the discrete version of the loss, therefore, we can learn the model faster and more accurately.   \n251 Numerical experiments conducted on toy 2D data show reliable outcomes under uniform conditions   \n252 and parameters. Comparison of the absolute values of loss for the proposed method and for CFM for   \n53 the same distributions show that the absolute values of loss for these models differ strikingly, by a   \n254 factor of $10^{2}\u201310^{3}$ . Experiments on high-dimensional datasets also confirm the theoretical deductions   \n55 about the variance reduction of our method. However, we emphasize that we do not expect to use the   \n56 proposed method in its pure form. On the contrary, we expect that the theoretical implications of our   \n257 formulas will contribute to the construction of better learning or inference algorithms in conjunction   \n258 with other heuristics or methods.   \n259 Algebraic analysis of variance for some cases (in particular, for the case $t=0$ or for the case of   \n260 two Gaussians as initial and final distributions) show an improvement in variance when using the   \n261 new loss. However, it is rather difficult to analyze in the general case, for all times $t$ and general   \n262 distributions $\\rho_{0}$ and $\\rho_{1}$ .   \n263 Having the expression for the vector field and score in the form of integrals, we can explicitly write   \n264 out their expressions for some simple cases; in the case of Gaussian distributions we can also write   \n265 out the exact solution for the trajectories. Thus, our approach allows one to advance the theoretical   \n266 study of FM-based and Diffusion Model-based frameworks. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "267 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "268 [1] Michael S. Albergo, Nicholas M. Boff,i and Eric Vanden-Eijnden. \u201cStochastic Interpolants: A   \n269 Unifying Framework for Flows and Diffusions\u201d. In: arXiv preprint 2303.08797 (2023).   \n270 [2] Michael S. Albergo and Eric Vanden-Eijnden. \u201cBuilding Normalizing Flows with Stochastic   \n271 Interpolants\u201d. In: International Conference on Learning Representations (ICLR) (2023).   \n272 [3] Gabriel Cardoso et al. \u201cBR-SNIS: Bias Reduced Self-Normalized Importance Sampling\u201d.   \n273 In: Advances in Neural Information Processing Systems. Ed. by S. Koyejo et al. Vol. 35.   \n274 Curran Associates, Inc., 2022, pp. 716\u2013729. URL: https://proceedings.neurips.cc/   \n275 paper _ files / paper / 2022 / file / 04bd683d5428d91c5fbb5a7d2c27064d - Paper -   \n276 Conference.pdf.   \n277 [4] Ricky T. Q. Chen and Yaron Lipman. \u201cRiemannian Flow Matching on General Geometries\u201d.   \n278 In: arXiv:2302.03660 (2023).   \n279 [5] Ricky T. Q. Chen et al. \u201cNeural Ordinary Differential Equations\u201d. In: Advances in Neural   \n280 Information Processing Systems. Ed. by S. Bengio et al. Vol. 31. Curran Associates, Inc.,   \n281 2018. URL: https://proceedings.neurips.cc/paper_files/paper/2018/file/   \n282 69386f6bb1dfed68692a24c8686939b9-Paper.pdf.   \n283 [6] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising Diffusion Probabilistic Models. 2020.   \n284 arXiv: 2006.11239 [cs.LG].   \n285 [7] Alexia Jolicoeur-Martineau, Kilian Fatras, and Tal Kachman. \u201cGenerating and Imputing   \n286 Tabular Data via Diffusion and Flow-based Gradient-Boosted Trees\u201d. In: arXiv:2309.09968   \n287 (2023). arXiv: 2309.09968 [cs.LG].   \n288 [8] Puheng Li et al. \u201cOn the Generalization Properties of Diffusion Models\u201d. In: Advances in   \n289 Neural Information Processing Systems. Ed. by A. Oh et al. Vol. 36. Curran Associates, Inc.,   \n290 2023, pp. 2097\u20132127. URL: https://proceedings.neurips.cc/paper_files/paper/   \n291 2023/file/06abed94583030dd50abe6767bd643b1-Paper-Conference.pdf.   \n292 [9] Yaron Lipman et al. \u201cFlow Matching for Generative Modeling\u201d. In: The Eleventh International   \n293 Conference on Learning Representations. 2023. URL: https://openreview.net/forum?   \n294 id=PqvMRDCJT9t.   \n295 [10] Xingchao Liu, Chengyue Gong, and qiang liu. \u201cFlow Straight and Fast: Learning to Generate   \n296 and Transfer Data with Rectified Flow\u201d. In: The Eleventh International Conference on Learning   \n297 Representations. 2023. URL: https://openreview.net/forum?id=XVjTT1nw5z.   \n298 [11] D. Martin et al. \u201cA database of human segmented natural images and its application to   \n299 evaluating segmentation algorithms and measuring ecological statistics\u201d. In: Proceedings   \n300 Eighth IEEE International Conference on Computer Vision. ICCV 2001. Vol. 2. 2001, 416\u2013423   \n301 vol.2. DOI: 10.1109/ICCV.2001.937655.   \n302 [12] Gaurav Parmar, Richard Zhang, and Jun-Yan Zhu. \u201cOn Aliased Resizing and Surprising   \n303 Subtleties in GAN Evaluation\u201d. In: CVPR. 2022.   \n304 [13] Aram-Alexandre Pooladian et al. \u201cMultisample Flow Matching: Straightening Flows with   \n305 Minibatch Couplings\u201d. In: Proceedings of the 40th International Conference on Machine   \n306 Learning. Ed. by Andreas Krause et al. Vol. 202. Proceedings of Machine Learning Research.   \n307 PMLR, July 2023, pp. 28100\u201328127. URL: https://proceedings.mlr.press/v202/   \n308 pooladian23a.html.   \n309 [14] Aaditya Ramdas, Nicol\u00e1s Garc\u00eda Trillos, and Marco Cuturi. \u201cOn wasserstein two-sample   \n310 testing and related families of nonparametric tests\u201d. In: Entropy 19.2 (2017), p. 47.   \n311 [15] Kulin Shah, Sitan Chen, and Adam Klivans. \u201cLearning Mixtures of Gaussians Using the   \n312 DDPM Objective\u201d. In: Thirty-seventh Conference on Neural Information Processing Systems.   \n313 2023. URL: https://openreview.net/forum?id $=$ aig7sgdRfI.   \n314 [16] Jascha Sohl-Dickstein et al. \u201cDeep Unsupervised Learning using Nonequilibrium Thermody  \n315 namics\u201d. In: Proceedings of the 32nd International Conference on Machine Learning. Ed. by   \n316 Francis Bach and David Blei. Vol. 37. Proceedings of Machine Learning Research. Lille,   \n317 France: PMLR, July 2015, pp. 2256\u20132265. URL: https://proceedings.mlr.press/v37/   \n318 sohl-dickstein15.html.   \n319 [17] Yang Song and Stefano Ermon. \u201cGenerative Modeling by Estimating Gradients of the Data   \n320 Distribution\u201d. In: Neural Information Processing Systems (NeurIPS) (2019).   \n321 [18] G\u00e1bor J Sz\u00e9kely. \u201cE-statistics: The energy of statistical samples\u201d. In: Bowling Green State   \n322 University, Department of Mathematics and Statistics Technical Report 3.05 (2003), pp. 1\u201318.   \n323 [19] Alexander Tong et al. \u201cImproving and generalizing flow-based generative models with mini  \n324 batch optimal transport\u201d. In: Transactions on Machine Learning Research (2024). Expert   \n325 Certification. ISSN: 2835-8856. URL: https://openreview.net/forum?id $\\equiv$ CD9Snc73AW.   \n326 [20] Alexander Tong et al. \u201cSimulation-Free Schr\u00f6dinger Bridges via Score and Flow Matching\u201d.   \n327 In: The 27th International Conference on Artificial Intelligence and Statistics. 2024. URL:   \n328 https://virtual.aistats.org/virtual/2024/poster/6691. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "329 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "30 1. Claims   \n331 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n332 paper\u2019s contributions and scope?   \n33 Answer: [Yes]   \n34 Justification: Theoretical things are proved in theorems in the main text and in the Appendix,   \n35 and numerical experiments have been performed   \n36 Guidelines:   \n37 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n38 made in the paper.   \n39 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n40 contributions made in the paper and important assumptions and limitations. A No or   \n341 NA answer to this question will not be perceived well by the reviewers.   \n42 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n43 much the results can be expected to generalize to other settings.   \n44 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n45 are not attained by the paper.   \n46 2. Limitations   \n47 Question: Does the paper discuss the limitations of the work performed by the authors?   \n48 Answer: [Yes]   \n49 Justification:   \n50 Guidelines: Limitation is discussed both in the theoretical part (in particular, in the formu  \n351 lation of theorems) and in the practical part, where we can see at which cases our method   \n52 works better or worse   \n53 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n54 the paper has limitations, but those are not discussed in the paper.   \n55 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n56 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n57 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n58 model well-specification, asymptotic approximations only holding locally). The authors   \n59 should reflect on how these assumptions might be violated in practice and what the   \n60 implications would be.   \n361 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n62 only tested on a few datasets or with a few runs. In general, empirical results often   \n63 depend on implicit assumptions, which should be articulated.   \n64 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n65 For example, a facial recognition algorithm may perform poorly when image resolution   \n66 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n67 used reliably to provide closed captions for online lectures because it fails to handle   \n68 technical jargon.   \n69 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n70 and how they scale with dataset size.   \n71 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n72 address problems of privacy and fairness.   \n73 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n74 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n75 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n76 judgment and recognize that individual actions in favor of transparency play an impor  \n77 tant role in developing norms that preserve the integrity of the community. Reviewers   \n78 will be specifically instructed to not penalize honesty concerning limitations.   \n79 3. Theory Assumptions and Proofs   \n380 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n381 a complete (and correct) proof? ", "page_idx": 11}, {"type": "text", "text": "Justification: All theorems are formulated according to strict mathematical rules. There are proofs or proof sketches in the text or Appendix. ", "page_idx": 12}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 12}, {"type": "text", "text": "396 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "397 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n398 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n399 of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 12}, {"type": "text", "text": "Justification: Yes, all parameters and hyperparameters required for reproducibility are described in the Appendix ", "page_idx": 12}, {"type": "text", "text": "Guidelines: ", "page_idx": 12}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 12}, {"type": "text", "text": "435 5. Open access to data and code ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "436 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n437 tions to faithfully reproduce the main experimental results, as described in supplemental   \n438 material?   \n439 Answer: [NA]   \n440 Justification: We do not provide a code, paper is mostly theoretical. We use open datasets.   \n441 Guidelines:   \n442 \u2022 The answer NA means that paper does not include experiments requiring code.   \n443 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n444 public/guides/CodeSubmissionPolicy) for more details.   \n445 \u2022 While we encourage the release of code and data, we understand that this might not be   \n446 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n447 including code, unless this is central to the contribution (e.g., for a new open-source   \n448 benchmark).   \n449 \u2022 The instructions should contain the exact command and environment needed to run to   \n450 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n451 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n452 \u2022 The authors should provide instructions on data access and preparation, including how   \n453 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n454 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n455 proposed method and baselines. If only a subset of experiments are reproducible, they   \n456 should state which ones are omitted from the script and why.   \n457 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n458 versions (if applicable).   \n459 \u2022 Providing as much information as possible in supplemental material (appended to the   \n460 paper) is recommended, but including URLs to data and code is permitted.   \n461 6. Experimental Setting/Details   \n462 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n463 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n464 results?   \n465 Answer: [Yes]   \n466 Justification: Yes, all parameters and hyperparameters required for reproducibility are   \n467 described in the Appendix   \n468 Guidelines:   \n469 \u2022 The answer NA means that the paper does not include experiments.   \n470 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n471 that is necessary to appreciate the results and make sense of them.   \n472 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n473 material.   \n474 7. Experiment Statistical Significance   \n475 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n476 information about the statistical significance of the experiments?   \n477 Answer: [Yes]   \n478 Justification: For several experiments variances are given.   \n479 Guidelines:   \n480 \u2022 The answer NA means that the paper does not include experiments.   \n481 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n482 dence intervals, or statistical significance tests, at least for the experiments that support   \n483 the main claims of the paper.   \n484 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n485 example, train/test split, initialization, random drawing of some parameter, or overall   \n486 run with given experimental conditions).   \n487 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n488 call to a library function, bootstrap, etc.)   \n489 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n490 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n491 of the mean.   \n492 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n493 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n494 of Normality of errors is not verified.   \n495 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n496 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n497 error rates).   \n498 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n499 they were calculated and reference the corresponding figures or tables in the text.   \n500 8. Experiments Compute Resources   \n501 Question: For each experiment, does the paper provide sufficient information on the com  \n502 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n503 the experiments?   \n504 Answer: [Yes]   \n505 Justification: The experiments are simple enough to be reproduced on publicly available   \n506 resources   \n507 Guidelines:   \n508 \u2022 The answer NA means that the paper does not include experiments.   \n509 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n510 or cloud provider, including relevant memory and storage.   \n511 \u2022 The paper should provide the amount of compute required for each of the individual   \n512 experimental runs as well as estimate the total compute.   \n513 \u2022 The paper should disclose whether the full research project required more compute   \n514 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n515 didn\u2019t make it into the paper).   \n516 9. Code Of Ethics   \n517 Question: Does the research conducted in the paper conform, in every respect, with the   \n518 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n519 Answer: [Yes]   \n520 Justification: The article and experiments meet all the requirements of the code of ethics, all   \n521 citations for materials used are given.   \n522 Guidelines: The research conducted in the article and its text meet the ethics codex.   \n523 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n524 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n525 deviation from the Code of Ethics.   \n526 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n527 eration due to laws or regulations in their jurisdiction).   \n528 10. Broader Impacts   \n529 Question: Does the paper discuss both potential positive societal impacts and negative   \n530 societal impacts of the work performed?   \n531 Answer: [NA]   \n532 Justification: Paper is mostly theoretical, there\u2019s little chance there could be a negative   \n533 impact.   \n534 Guidelines:   \n535 \u2022 The answer NA means that there is no societal impact of the work performed.   \n536 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n537 impact or why the paper does not address societal impact.   \n538 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n539 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n540 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n541 groups), privacy considerations, and security considerations.   \n542 \u2022 The conference expects that many papers will be foundational research and not tied   \n543 to particular applications, let alone deployments. However, if there is a direct path to   \n544 any negative applications, the authors should point it out. For example, it is legitimate   \n545 to point out that an improvement in the quality of generative models could be used to   \n546 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n547 that a generic algorithm for optimizing neural networks could enable people to train   \n548 models that generate Deepfakes faster.   \n549 \u2022 The authors should consider possible harms that could arise when the technology is   \n550 being used as intended and functioning correctly, harms that could arise when the   \n551 technology is being used as intended but gives incorrect results, and harms following   \n552 from (intentional or unintentional) misuse of the technology.   \n553 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n554 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n555 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n556 feedback over time, improving the efficiency and accessibility of ML).   \n557 11. Safeguards   \n558 Question: Does the paper describe safeguards that have been put in place for responsible   \n559 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n560 image generators, or scraped datasets)?   \n561 Answer: [NA]   \n562 Justification: Our work has a very low risk of misuse.   \n563 Guidelines:   \n564 \u2022 The answer NA means that the paper poses no such risks.   \n565 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n566 necessary safeguards to allow for controlled use of the model, for example by requiring   \n567 that users adhere to usage guidelines or restrictions to access the model or implementing   \n568 safety filters.   \n569 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n570 should describe how they avoided releasing unsafe images.   \n571 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n572 not require this, but we encourage authors to take this into account and make a best   \n573 faith effort.   \n574 12. Licenses for existing assets   \n575 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n576 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n577 properly respected?   \n578 Answer: [Yes]   \n579 Justification: Links are provided to articles with open repositories whose code was used in   \n580 the work   \n581 Guidelines:   \n582 \u2022 The answer NA means that the paper does not use existing assets.   \n583 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n584 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n585 URL.   \n586 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n587 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n588 service of that source should be provided.   \n589 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n590 package should be provided. For popular datasets, paperswithcode.com/datasets   \n591 has curated licenses for some datasets. Their licensing guide can help determine the   \n592 license of a dataset.   \n593 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n594 the derived asset (if it has changed) should be provided.   \n595 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n596 the asset\u2019s creators.   \n597 13. New Assets   \n598 Question: Are new assets introduced in the paper well documented and is the documentation   \n599 provided alongside the assets?   \n600 Answer: [NA]   \n601 Justification: We do not provide a code. All Theorems and Satetements are well formulated.   \n602 Guidelines:   \n603 \u2022 The answer NA means that the paper does not release new assets.   \n604 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n605 submissions via structured templates. This includes details about training, license,   \n606 limitations, etc.   \n607 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n608 asset is used.   \n609 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n610 create an anonymized URL or include an anonymized zip file.   \n611 14. Crowdsourcing and Research with Human Subjects   \n612 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n613 include the full text of instructions given to participants and screenshots, if applicable, as   \n614 well as details about compensation (if any)?   \n615 Answer: [NA]   \n616 Justification: we have no crowdsourcing   \n617 Guidelines:   \n618 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n619 human subjects.   \n620 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n621 tion of the paper involves human subjects, then as much detail as possible should be   \n622 included in the main paper.   \n623 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n624 or other labor should be paid at least the minimum wage in the country of the data   \n625 collector.   \n626 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n627 Subjects   \n628 Question: Does the paper describe potential risks incurred by study participants, whether   \n629 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n630 approvals (or an equivalent approval/review based on the requirements of your country or   \n631 institution) were obtained?   \n632 Answer: [NA]   \n633 Justification: There are no experiments with subjects in the paper, only numerical experi  \n634 ments.   \n635 Guidelines:   \n636 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n637 human subjects.   \n638 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n639 may be required for any human subjects research. If you obtained IRB approval, you   \n640 should clearly state this in the paper. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 17}, {"type": "text", "text": "646 A Proof of the theorems ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "647 A.1 Proof of the Theorem 2.1 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "648 Proof. We need to proof, that $\\begin{array}{r}{\\frac{\\mathrm{d}L_{\\mathrm{CFM}}(\\theta)}{\\mathrm{d}\\theta}=\\frac{\\mathrm{d}L_{\\mathrm{ExFM}}(\\theta)}{\\mathrm{d}\\theta}}\\end{array}$ ", "page_idx": 18}, {"type": "text", "text": "649 To establish the equivalence of $L_{\\mathrm{CFM}}$ and $L_{\\mathrm{ExFM}}$ up to a constant term, we begin by expressing $L_{\\mathrm{CFM}}$   \n650 in the format specified by equation (6): ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{L_{\\mathrm{CFM}}=\\mathbb{E}_{t,x_{1},x\\sim\\rho_{m}(\\cdot,t)}\\|v_{\\theta}(x,t)-w(t,x_{1},x)\\|^{2}\\times\\rho_{c}(x|x_{1},t)/\\rho_{1}(x_{1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "651 Utilizing the bilinearity of the 2-norm, we can rewrite $L_{\\mathrm{CFM}}~\\mathrm{as}$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L_{\\mathrm{CFM}}=\\mathbb{E}_{t,x_{1},x\\sim\\rho_{m}(\\cdot,t)}\\frac{\\left\\Vert v_{\\theta}(x,\\,t)\\right\\Vert^{2}\\rho_{c}(x|x_{1},\\,t)}{\\rho_{1}(x_{1})}-}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad2\\mathbb{E}_{t,x_{1},x\\sim\\rho_{m}(\\cdot,t)}\\frac{v_{\\theta}(x,\\,t)^{T}\\cdot w(t,x_{1},\\,x)\\rho_{c}(x|x_{1},\\,t)}{\\rho_{1}(x_{1})}+C.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "652 Here, $T$ denotes transposed vector, dot denotes scalar product, $C$ represents a constant independent   \n653 of $\\theta$ . ", "page_idx": 18}, {"type": "text", "text": "654 Noting that $\\mathbb{E}_{x_{1}}\\rho_{c}(x|x_{1},t)/\\rho_{1}(x_{1})=1$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}_{x_{1}}\\frac{\\rho_{c}(x|x_{1},t)}{\\rho_{1}(x_{1})}=\\int\\frac{\\rho_{x_{1}}(x,t)\\rho_{1}(x_{1})\\,\\mathrm{d}x_{1}}{\\int\\rho_{x_{1}}(x,t)\\rho_{1}(x_{1})\\,\\mathrm{d}x_{1}}=1,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "655 we can simplify the first term in the expansion (21): ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{t,x_{1},x\\sim\\rho_{m}(\\cdot,t)}\\frac{\\Vert v_{\\theta}(x,\\,t)\\Vert^{2}\\rho_{c}(x|x_{1},\\,t)}{\\rho_{1}(x_{1})}=}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad E_{t,x\\sim\\rho_{m}(\\cdot,t)}\\Vert v_{\\theta}(x,\\,t)\\Vert^{2}\\,\\mathbb{E}_{x_{1}}\\frac{\\rho_{c}(x|x_{1},\\,t)}{\\rho_{1}(x_{1})}=E_{t,x\\sim\\rho_{m}(\\cdot,t)}\\Vert v_{\\theta}(x,\\,t)\\Vert^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "656 For our loss $L_{\\mathrm{ExFM}}$ in the form (8) we also use the bilinearity of the norm: ", "page_idx": 18}, {"type": "equation", "text": "$$\nL_{\\mathrm{ExFM}}=\\mathbb{E}_{t,x\\sim\\rho_{m}(\\cdot,t)}\\Vert v_{\\theta}(x,t)\\Vert^{2}-2\\mathbb{E}_{t,x\\sim\\rho_{m}(\\cdot,t)}\\mathbb{E}_{x_{1}}\\frac{v_{\\theta}(x,t)^{T}\\cdot w(t,x_{1},x)\\rho_{c}(x|x_{1},t)}{\\rho_{1}(x_{1})}+C.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "657 Comparing the last expression and the Eq. (21) with the modification (22) and also taking into account   \n658 the independence of random variables $x$ and $x_{1}$ , we come to the conclusion that $L_{\\mathrm{ExFM}}$ is equal to   \n659 $L_{\\mathrm{CFM}}$ up to some constant independent of the model parameters. ", "page_idx": 18}, {"type": "text", "text": "660 ", "page_idx": 18}, {"type": "text", "text": "661 A.2 Sketch of the proof of the Theorem 2.4 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "662 Proof. We need to prove that $\\begin{array}{r}{\\mathbb{D}\\frac{\\mathrm{d}L_{\\mathrm{ExFM}}^{d}(\\theta)}{\\mathrm{d}\\theta}\\leq\\mathbb{D}\\frac{\\mathrm{d}L_{\\mathrm{CFM}}^{d}(\\theta)}{\\mathrm{d}\\theta}}\\end{array}$ , where $L_{\\mathrm{ExFM}}^{d}(\\theta)$ and $L_{\\mathrm{CFM}}^{d}(\\theta)$ discrete loss   \n663 functions presented in (14) and (13). Firstly, let us rewrite the derivative of loss functions using the   \n664 bilinearity: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}L_{\\mathrm{ExFM}}^{d}(\\theta)}{\\mathrm{d}\\theta}=2\\sum_{i,j}\\left(\\frac{\\mathrm{d}v_{\\theta}(x^{j,i},\\,t^{j})}{\\mathrm{d}\\theta}\\right)^{T}\\cdot\\left(v_{\\theta}(x^{j,i},\\,t^{j})-v^{d}(x^{j,i},\\,t^{j})\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "665 Note that in this expression, values $x^{j,i}$ as well as $t^{j}$ , which are included in the argument of the   \n666 function $v$ , are fixed (our goal to calculate the variance with fixed model arguments). Thus, we need   \n667 to consider the variance of the remaining expression arising from the randomness of ${\\overline{{x}}_{1}^{k}}$ . ", "page_idx": 18}, {"type": "text", "text": "Recall (below we will omit the indices at variables $x$ and $t$ ), ", "page_idx": 18}, {"type": "equation", "text": "$$\nv^{d}(x,\\,t)=\\frac{\\sum_{k=1}^{N}w(t,\\overline{{x}}_{1}^{k},x)\\cdot\\rho_{0}\\big(\\phi_{t,\\overline{{x}}_{1}^{k}}^{-1}(x)\\big)}{\\sum_{k=1}^{N}\\rho_{0}\\big(\\phi_{t,\\overline{{x}}_{1}^{k}}^{-1}(x)\\big)}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "668 Note, that if $N=1$ , $(i.\\,e$ . we do not sample any additional points other than the ones we have   \n669 already sampled) this expression is exactly the same as the derivative of the common discretized   \n670 CFM loss $\\frac{\\mathrm{d}L_{\\mathrm{CFM}}^{d}(\\theta)}{\\mathrm{d}\\theta}$ .   \n671 Moreover, recall that one of the points (without loss of generality, we can assume that its index is 1)   \n672 ${\\overline{{x}}}_{1}^{1}$ is added from the set from which point $x$ was derived: $x=\\phi_{t,\\overline{{x}}_{1}^{1}}(x_{0})$ . (Here $x_{0}$ is the paired point   \n673 to ${\\overline{{x}}}_{1\\!}^{1}$ ) ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "674 Thus, we can rewrite expression for $v^{d}$ : ", "page_idx": 19}, {"type": "equation", "text": "$$\nv^{d}(x,\\,t)=\\frac{w(t,\\overline{{x}}_{1}^{1},x)\\rho_{0}(x_{0})+\\sum_{k=2}^{N}w(t,\\overline{{x}}_{1}^{k},x)\\cdot\\rho_{0}\\left(\\phi_{t,\\overline{{x}}_{1}^{k}}^{-1}(x)\\right)}{\\rho_{0}(x_{0})+\\sum_{k=2}^{N}\\rho_{0}\\left(\\phi_{t,\\overline{{x}}_{1}^{k}}^{-1}(x)\\right)}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "675 Thus, our task was reduced to evaluating how well the additional terms (for $k$ starting from 2) improve   \n676 approximate of the original integrals that are in loss (8). ", "page_idx": 19}, {"type": "text", "text": "So, we need to estimate the following dispersion ratio, where in the numerator is the variance of discrete loss CFM, and in the denominator \u2014 the variance of loss ExFM: ", "page_idx": 19}, {"type": "equation", "text": "$$\nk_{D}=\\frac{\\mathbb{P}\\big(v_{\\theta}(x,t)-w(t,\\overline{{x}}_{1}^{1},x)\\big)}{\\mathbb{D}\\left(v_{\\theta}(x,t)-\\frac{\\sum_{k=1}^{N}w(t,\\overline{{x}}_{1}^{k},x)\\cdot\\rho_{0}\\big(\\phi_{t,\\overline{{x}}_{1}^{k}}^{-1}(x)\\big)}{\\sum_{k=1}^{N}\\rho_{0}\\big(\\phi_{t,\\overline{{x}}_{1}^{k}}^{-1}(x)\\big)}\\right)}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "677 The smaller coefficient $k_{D}$ is, the better the proposed loss ExFM works. ", "page_idx": 19}, {"type": "text", "text": "Formally, we can write our problem as an importance sampling problem for the following integral: ", "page_idx": 19}, {"type": "equation", "text": "$$\nI=\\int f(x)p(x)\\,\\mathrm{d}x\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "This integral we estimate by sample mean of the following expectation over some random variable with density function $q(x)$ : ", "page_idx": 19}, {"type": "text", "text": "with ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{c}{I=\\mathbb{E}_{x\\sim q}\\big(w(x)f(x)\\big)}\\\\ {w(x)=\\displaystyle\\frac{p(x)}{q(x)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We replace the exact value of $I$ with the value ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\overline{{I}}=\\frac{\\sum_{k=1}^{N}w(\\overline{{x}}_{1}^{i})f(\\overline{{x}}_{1}^{k})}{\\sum_{i=k}^{N}w(\\overline{{x}}_{1}^{k})}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "678 It follows from the strong law of large numbers that in the limit $N\\rightarrow\\infty$ , $I\\rightarrow\\overline{{I}}$ almost surely. From   \n679 the central limit theorem we can find the asymptotic variance: ", "page_idx": 19}, {"type": "equation", "text": "$$\n{\\mathbb D}\\overline{{I}}=\\frac{1}{N}{\\mathbb E}_{x\\sim q}\\big(w^{2}(x)(f(x)-I)^{2}\\big).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "680 In our case (loss $L_{\\mathrm{ExFM.}}$ ), we have $q(x_{1})=\\rho_{1}(x_{1}),\\,f(x_{1})=w(t,x_{1},x)$ and $w(x_{1})=\\rho_{0}\\left(\\phi_{t,x_{1}}^{-1}(x)\\right)$ . ", "page_idx": 19}, {"type": "text", "text": "681 Despite the fact that the equation (25) for the variance contains $N$ in the denominator, it is rather   \n682 difficult to give an estimate of its behavior in general. The point is that this formula is well suited for   \n683 the case when $w$ in it is of approximately the same order. In the considered case, this is achieved at   \n684 times $t$ noticeably less than 1. ", "page_idx": 19}, {"type": "text", "text": "But in the case, when $t$ is closed to 1 we have, for example, for the linear map, that ", "page_idx": 19}, {"type": "equation", "text": "$$\nw(x_{1})=\\rho_{0}\\left(\\phi_{t,x_{1}}^{-1}(x)\\right)=\\rho_{0}\\left({\\frac{x-x_{1}t}{1-t}}\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "685 and this function has a sharp peak near the point $x/t$ if it is considered as a function of $x_{1}$ . Thus, at   \n686 such values of $t$ , only a small number of summands will give a sufficient contribution to the sum   \n687 compared to the first term.   \n688 Finally, inequality $k_{D}<1$ is formally fulfliled, but how much $k_{D}$ is less than one depends on many   \n689 factors. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "691 A.3 Expressions for the regularized map ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "692 To justify the expression (11), we use a invertable transformation and then strictly take the limit $\\sigma_{s}\\rightarrow$   \n693 0.   \n694 Expression Eq. (11), (16) are obtained for the simple map $\\phi_{t,x_{1}}(x_{0})\\,=\\,(1\\,-\\,t)x_{0}\\,+\\,t x_{1}$ which   \n695 is not invertable at $t=1$ . For the map with small regaluraziting parameter $\\sigma_{s}\\,>\\,0\\;\\phi_{t,x_{1}}(x_{0})\\,=$   \n696 $(1-t)x_{0}+t x_{1}+\\sigma_{s}x_{0}$ , which is invertable at all time values $0\\,\\leq\\,t\\,\\leq\\,1$ , Eq. (11), (16) needs   \n697 modifications. Namely, for this map the following exact formulas holds true ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "equation", "text": "$$\nv(x,t)=\\!\\!\\int\\!\\!w(t,x_{1},x)\\rho_{c}(x|x_{1},t)\\rho_{1}(x_{1})\\,\\mathrm{d}x_{1}\\!=\\!\\frac{\\int\\!\\left(x_{1}-x(1-\\sigma_{s})\\right)\\rho_{0}\\left(\\frac{x-x_{1}t}{1+\\sigma_{s}t-t}\\right)\\rho_{1}(x_{1})\\,\\mathrm{d}x_{1}}{(1+\\sigma_{s}t-t)\\int\\rho_{0}\\left(\\frac{x-x_{1}t}{1+\\sigma_{s}t-t}\\right)\\rho_{1}(x_{1})\\,\\mathrm{d}x_{1}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "698 By direct substitution we make sure that for this vector field ", "page_idx": 20}, {"type": "equation", "text": "$$\nv(x,\\,0)=\\int x_{1}\\rho_{1}(x_{1})\\,\\mathrm{d}x_{1}-x(1-\\sigma_{s})\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "699 and ", "page_idx": 20}, {"type": "equation", "text": "$$\nv(x,\\,1)=\\frac{\\int(x-y)\\rho_{0}(y)\\rho_{1}(x-y\\sigma_{s})\\,\\mathrm{d}y}{\\int\\rho_{0}(y)\\rho_{1}(x-y\\sigma_{s})\\,\\mathrm{d}y},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "700 where we perform change of the variables $\\begin{array}{r}{y\\leftarrow\\frac{x_{1}-x}{\\sigma_{s}t}}\\end{array}$ . ", "page_idx": 20}, {"type": "text", "text": "701 A.3.1 Prof of the explicit formula (11) for the vector field ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "702 Assumption A.1. Density $\\rho_{1}$ is continuous at any point $x\\in(-\\infty,\\infty)$ . ", "page_idx": 20}, {"type": "text", "text": "703 Theorem A.2. In equations (26), (27) and (28) we can take the limit $\\sigma_{s}\\rightarrow0$ under integrals to get   \n704 Eq. (11) and (12). ", "page_idx": 20}, {"type": "text", "text": "Proof. Assuming that the distribution $\\rho_{1}$ has a finite first moment: $|\\int\\xi\\rho_{1}(\\xi)\\,\\mathrm{d}\\xi\\,|<C_{1}$ and that the density of $\\rho_{0}$ is bounded: $\\rho_{0}(x)<C_{2}$ , $\\forall x\\in(-\\infty,\\infty)$ , we obtain that the integrand functions in the numerator and denominator in the Eq. (26) can be bounded by the following integrable functions independent of $\\sigma_{s}$ and $t$ : ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\rho_{0}\\left(\\frac{x-x_{1}t}{1+\\sigma_{s}-t}\\right)\\rho_{1}(x_{1})<C_{1}\\rho_{1}(x_{1})\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and ", "page_idx": 20}, {"type": "equation", "text": "$$\n0\\leq x_{1}\\rho_{0}\\left({\\frac{x-x_{1}t}{1+\\sigma_{s}t-t}}\\right)\\rho_{1}(x_{1})<x_{1}C_{1}\\rho_{1}(x_{1}),\\;\\;\\;x\\geq0,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\n0>x_{1}\\rho_{0}\\left(\\frac{x-x_{1}t}{1+\\sigma_{s}t-t}\\right)\\rho_{1}(x_{1})>x_{1}C_{1}\\rho_{1}(x_{1}),\\;\\;\\;x<0.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "705 It follows that both integrals in expression (26) converge absolutely and uniformly. So, we can swap   \n706 the operations of taking the limit and integration, and we can take the limit $\\sigma_{s}\\rightarrow0$ in the integrand   \n707 for any time $t\\in[0,\\,t_{0}]$ for arbitrary $t_{0}<1$ . ", "page_idx": 20}, {"type": "text", "text": "Now, let us consider the case $t=1$ . From Assumption A.1 the boundedness of the density $\\rho_{1}$ follows: $\\rho_{1}(x)<C_{2}$ , $\\forall x\\in(-\\infty,\\infty)$ . Thus, integrand functions in the numerator and denominator in the Eq. (28) can be bounded by the following integrable functions independent of $\\sigma_{s}$ : ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\rho_{0}(y)\\rho_{1}(x-y\\sigma_{s})<\\rho_{0}(y)C_{2}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{0\\leq y\\rho_{0}(y)\\rho_{1}(x-y\\sigma_{s})<y C_{2}\\rho_{0}(y),\\quad y\\geq0,}\\\\ &{0>y\\rho_{0}(y)\\rho_{1}(x-y\\sigma_{s})>y C_{2}\\rho_{0}(y),\\quad y<0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The existence of the limit ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\sigma_{s}\\rightarrow0}\\rho_{1}(x-y\\sigma_{s})=\\rho_{1}(x),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "708 follows from Assumption A.1. ", "page_idx": 20}, {"type": "text", "text": "709 Finally, we conclude that formula (11), regarded as the limit $\\sigma_{s}\\rightarrow0$ of the (26) at any $t\\in[0,\\,1]$ , is   \n710 true. \u53e3 ", "page_idx": 20}, {"type": "text", "text": "Theorem A.3. The vector field in Eq. (11) delivers minimum to the Flow Matching objective (see the work [9]), ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}_{t}\\mathbb{E}_{x\\sim\\rho(x,t)}\\|\\overline{{v}}(x,t)-v(x,t)\\|,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "711 where $\\rho({\\boldsymbol{x}},t)$ and ${\\overline{{v}}}(x,t)$ satisfy the equation (1) with the given densities $\\rho_{0}$ and $\\rho_{1}$ . ", "page_idx": 21}, {"type": "text", "text": "712 Proof. The proof is based on the previous statements and on a Theorem 1 from [9] (that the marginal   \n713 vector field based on conditional vector fields generates the marginal probability path based on   \n714 conditional probability paths.   \n715 To complete the proof, we must justify that, with $\\sigma_{s}$ tending to zero, the marginal path at $t=1$   \n716 coincides with a given probability $\\rho_{1}$ . ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "717 Consider the marginal probability path $p_{t}(\\boldsymbol{x},t)$ ", "page_idx": 21}, {"type": "equation", "text": "$$\np_{t}(x,t)=\\int p_{t}(x|x_{1},\\sigma_{s})\\rho_{1}(x_{1})\\mathrm{d}x_{1}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "718 where $p_{t}(x|x_{1},\\sigma_{s})$ is conditional probability paths obtained by regularized linear conditional map.   \n719 Distribution $p_{t}$ in the time $t=0$ is equal to standard normal distribution $p_{0}(x|x_{1},\\sigma_{s})=\\mathcal{N}(x\\mid0,1)$   \n720 and at the time $t=1$ it is a stretched Gaussian centered at $x_{1}\\colon p_{1}(x|x_{1},\\sigma_{s})={\\mathcal{N}}(x\\mid x_{1},\\sigma_{s}I)$ . ", "page_idx": 21}, {"type": "text", "text": "Substituting $p_{1}$ into the Eq. (29) and considering that there exists a limit $\\sigma_{s}\\rightarrow0$ due to Assumption A.1, we obtain ", "page_idx": 21}, {"type": "equation", "text": "$$\np_{1}(x)=\\operatorname*{lim}_{\\sigma_{s}\\rightarrow0}\\int p_{t}(x|x_{1},\\sigma_{s})\\rho_{1}(x_{1})\\mathrm{d}x_{1}=\\rho_{1}(x_{1}).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "721 This finish the proof. ", "page_idx": 21}, {"type": "text", "text": "722 A.3.2 Learning procedure for $\\sigma_{s}>0$ ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "723 Using standard normal distribution as initial density $\\rho_{0}$ , and the regularized map $\\phi_{t,x_{1}}(x_{0})\\;=\\;$   \n724 $(1-t)x_{0}+t x_{1}+\\sigma_{s}t x_{0}$ we obtain the following approximation formula ", "page_idx": 21}, {"type": "equation", "text": "$$\nv^{d}(x,\\ t)=\\frac{\\sum_{k=1}^{N}\\frac{\\overline{{x}}_{1}^{k}-x(1-\\sigma_{s})}{1-t(1-\\sigma_{s})}\\exp\\bigl(Y^{k}\\bigr)}{\\sum_{k=1}^{N}\\exp\\bigl(Y^{k}\\bigr)},\\quad\\mathrm{where}\\quad Y^{k}=-\\frac{1}{2}\\frac{\\left\\|x-t\\cdot\\overline{{x}}_{1}^{k}\\right\\|_{\\mathbb{R}^{d}}^{2}}{1-t(1-\\sigma_{s})}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "725 In practical applications, the exponent calculation is replaced by the SoftMax function calculation,   \n726 which is more stable. ", "page_idx": 21}, {"type": "text", "text": "727 B Estimation of integrals ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "728 In general, we need to estimate the following expression ", "page_idx": 21}, {"type": "equation", "text": "$$\nI(\\eta)=\\frac{\\int w(x_{1},\\,\\eta)f(x_{1},\\,\\eta)\\rho_{1}(x_{1})\\,\\mathrm{d}x_{1}}{\\int f(x_{1},\\,\\eta)\\rho_{1}(x_{1})\\,\\mathrm{d}x_{1}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "729 In particular, substituting $\\eta\\rightarrow\\{x,t\\}$ , $w(x,\\eta)\\,\\to\\,(x_{1}\\,-\\,x)/(1\\,-\\,t)$ we obtain formula (11) and   \n730 similar ones with similar substitutions.   \n731 If we can sample from the $\\rho_{1}$ distribution, we can estimate this integral in two ways: self-normalized   \n732 importance sampling and rejection sampling. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "733 Let $\\boldsymbol{\\mathcal{X}}=\\{x_{1}^{k}\\}_{k=1}^{N}$ be $N$ samples from the distribution $\\rho_{1}$ ", "page_idx": 21}, {"type": "text", "text": "734 Self-normalized Importance Sampling In this case ", "text_level": 1, "page_idx": 21}, {"type": "equation", "text": "$$\nI(\\eta)\\approx\\frac{\\displaystyle\\sum_{k=1}^{N}w(x_{1}^{k},\\eta)f(x_{1}^{k},\\eta)\\rho_{1}(x_{1}^{k})\\,\\mathrm{d}x_{1}}{\\displaystyle\\sum_{k=1}^{N}f(x_{1}^{k},\\,\\eta)\\rho_{1}(x_{1}^{k})\\,\\mathrm{d}x_{1}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "735 This estimate is biased in theory, but there several methods to reduce this bias and improve this   \n736 estimate, see, for example, [3]. Our numerical experiments generally show that the estimation (30) in   \n737 the form is already sufficient for stable results; we don not observe any bias. ", "page_idx": 21}, {"type": "text", "text": "Rejection sampling Let $\\mathcal{Y}=\\{y^{k}\\}_{k=1}^{M}\\subset\\mathcal{X}$ be a subset of the the initially given set of samples, which is formed according to the following rule. Let $C=\\operatorname*{sup}_{x}\\rho_{1}(x)$ . For a given sample $\\boldsymbol{x}_{1}^{j}$ we generate a random uniformly distributed variable $\\xi_{j}\\sim\\mathcal{U}(0,1)$ and if ", "page_idx": 22}, {"type": "equation", "text": "$$\nf(x_{1}^{j})\\geq C\\xi_{j},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "738 then we put the point $\\boldsymbol{x}_{k}^{j}$ to the set $\\boldsymbol{\\wp}$ ; otherwise we reject it.   \n739 Having formed the set $\\boldsymbol{\\wp}$ , we evaluate the integral as ", "page_idx": 22}, {"type": "equation", "text": "$$\nI(\\eta)\\approx\\frac{1}{M}\\sum_{k=1}^{M}w(y^{k},\\eta).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "740 To justify the last estimation, we note, that the points from the set $\\boldsymbol{\\wp}$ are distributed according   \n741 to (non-normalized) density $\\rho(x)f(x,\\eta)\\rho_{1}(x)$ . One can show it using the proof of the rejection   \n742 sampling method. This is the same density as in Eq. (7) and thus we estimate the expression (10)   \n743 using Important Sampling without any additional denominator.   \n744 Comparison When we apply these techniques to evaluating the expression for the vector field, we   \n745 know that when the time parameter $t$ is close to 1, the function $f(x_{1},\\eta)$ (which is a scaled $\\rho_{0}$ ) has a   \n746 peak at the point $x=x_{1}$ . This means that only a small number of points from the original set will   \n747 end up in the set $\\boldsymbol{\\wp}$ . Moreover, in the case when the time $t$ is very close to one and the data are well   \n748 separated, only one point $x_{1}$ will end up in $\\boldsymbol{\\wp}$ . This explains why we initially put this point in the   \n749 set $\\mathcal{X}$ , because otherwise it would be possible that the set $\\boldsymbol{\\wp}$ is empty and $M=0$ .   \n750 As a future work, we indicate a theoretical finding of the probability of hitting a particular point $x_{1}$ in   \n751 the set $\\boldsymbol{\\wp}$ and, thus, a modification of our algorithm, when the sample $x_{1}$ will not always go to the   \n752 set $\\mathcal{X}$ , but with some probability \u2014 the greater the $t$ the closer this probability to 1.   \n753 C The main Algorithm and extensions and generalization of the exact   \n754 expression ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "Algorithm 1 Vector field model training algorithm   \nRequire: Sampler from distribution $\\rho_{1}$ (or a set of samples); parameters $n$ and $m$ (number of spatial and time points, correspondingly); parameter $N$ (number of averaging point); model $v_{\\theta}(x,t)$ ; algorithm with parameters for SGD   \nEnsure: quasi-optimal parameters $\\theta$ for the trained model   \n1: Initialize $\\theta$ (maybe random)   \n2: while exit condition is not met do 3: Sample $m$ points $\\{t^{j}\\}$ from $\\mathcal{U}[0,1]$   \n4: Sample $n$ points pairs $\\{x_{0}^{i},x_{1}^{i}\\}_{i=1}^{n}$ from joint distribution $\\pi$ $\\mathbf{\\nabla}\\cdot(\\pi(x_{0},x_{1})\\,=\\,\\rho_{0}(x_{0})\\rho_{1}(x_{1})$ if variables are independent)   \n5: Sample $N-n$ points $\\{\\hat{x}_{1}^{l}\\}$ from $\\rho_{1}$ and form $\\{\\overline{{x}}_{1}^{k}\\}=\\{x_{1}^{i}\\}\\cup\\{\\hat{x}_{1}^{l}\\}$ // We can take all available samples as $\\{\\overline{{x}}_{1}^{k}\\}$ if we don\u2019t have access to a sampler, but only ready-made samples. 6: For all $i$ and $j$ calculate the sum at the right side of (14) (using (16) if $\\rho_{0}$ is standard Gaussian or (24) in general)   \n7: Calculate the sum on $i$ and $j$ in discrete loss (14), and take backward derivative, obtaining approximate grad $G\\approx\\nabla_{\\theta}L_{\\mathrm{ExFM}}$ of loss $L_{\\mathrm{ExFM}}$ on model parameters $\\theta$ . 8: Update model parameters $\\theta\\leftarrow S G D(\\theta,G)$   \n9: end while ", "page_idx": 22}, {"type": "text", "text": "755 General form of the proposed Algorithm is given in $\\mathrm{Alg~l~}$ . ", "page_idx": 22}, {"type": "text", "text": "756 When using other maps, formula (11) is modified accordingly. For example, if we use the regularized   \n757 map $\\phi_{t,x_{1}}(x_{0})\\,=\\,(1\\,-\\,t)x_{0}+t x_{1}\\,+\\,\\sigma_{s}t x_{0}$ , we get the formula (26).Note, that in this case the   \n758 final density $\\rho(x,\\,1)$ , obtained from the continuity equation is not equal to $\\rho_{1}$ , but is its smoothed   \n759 modification.   \n760 When using a different initial density $\\rho_{0}$ (not the normal distribution), an obvious modification will   \n761 be made to formula (16). ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "Diffusion-like models We can treat so-called Variance Preserving [6] model as CFM with the map ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\phi_{t,x_{1}}(x)=\\alpha_{1-t}x+\\sqrt{1-\\alpha_{1-t}^{2}}x_{1}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "762 and $\\rho_{0}$ as standard normal distribution: $\\rho_{0}=\\mathcal{N}\\left(\\cdot\\vert\\,0,1^{2}\\right)$ In this case, the common expression (10)   \n763 for vector filed transforms to ", "page_idx": 23}, {"type": "equation", "text": "$$\nv(x,t)=\\frac{\\int(x\\alpha_{1-t}-x_{1})\\alpha_{1-t}^{\\prime}\\,\\rho_{0}\\left(\\frac{x-x_{1}\\alpha_{1-t}}{\\sqrt{1-\\alpha_{1-t}^{2}}}\\right)\\rho_{1}(x_{1})\\,\\mathrm{d}x_{1}}{(1-\\alpha_{1-t}^{2})\\int\\rho_{0}\\left(\\frac{x-x_{1}\\alpha_{1-t}}{\\sqrt{1-\\alpha_{1-t}^{2}}}\\right)\\rho_{1}(x_{1})\\,\\mathrm{d}x_{1}},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "764 where $\\begin{array}{r}{\\alpha_{s}^{\\prime}=\\frac{\\mathrm{d}\\alpha_{s}}{\\mathrm{d}s}}\\end{array}$ . ", "page_idx": 23}, {"type": "text", "text": "Similarity we can treat so-called Variance Exploding [17] model as CFM with the map ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\phi_{t,x_{1}}(x)=\\sigma_{1-t}x+x_{1}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "765 and $\\rho_{0}$ also as standard normal distribution: $\\rho_{0}\\,=\\mathcal{N}\\left(\\cdot\\vert\\,0,1^{2}\\right)$ In this case, the common expres  \n766 sion (10) for vector filed transforms to ", "page_idx": 23}, {"type": "equation", "text": "$$\nv(x,t)=\\frac{\\int(x_{1}-x)\\sigma_{1-t}^{\\prime}\\,\\rho_{0}\\left(\\frac{x-x_{1}}{\\sigma_{1-t}}\\right)\\rho_{1}(x_{1})\\,\\mathrm{d}x_{1}}{\\sigma_{1-t}\\int\\rho_{0}\\left(\\frac{x-x_{1}}{\\sigma_{1-t}}\\right)\\rho_{1}(x_{1})\\,\\mathrm{d}x_{1}},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "767 where $\\begin{array}{r}{\\sigma_{s}^{\\prime}=\\frac{\\mathrm{d}\\sigma_{s}}{\\mathrm{d}s}}\\end{array}$ . ", "page_idx": 23}, {"type": "text", "text": "768 Joint Distribution Moreover, in addition to the independent densities $x_{0}\\sim\\rho_{0}$ and $x_{1}\\sim\\rho_{1}$ , we   \n769 can use the joint density $\\{x_{0},\\,x_{1}\\}\\,\\sim\\,\\pi(x_{0},\\,x_{1})$ . In the papers [20, 19], optimal transport $(\\mathrm{OT})$   \n770 and Schr\u00f6dinger\u2019s bridge are taken as $\\pi$ . In this case the expression for the vector field changes   \n771 insignificantly: the conditional probability $\\rho_{c}$ from Eq. (7) is subject to change: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\rho_{c}(x|x_{1},t)=\\frac{\\pi\\left(\\phi_{t,x_{1}}^{-1}(x),x_{1}\\right)\\operatorname*{det}\\left[\\frac{\\partial\\phi_{t,x_{1}}^{-1}(x)}{\\partial x}\\right]}{\\int\\pi\\left(\\phi_{t,x_{1}}^{-1}(x),x_{1}\\right)\\operatorname*{det}\\left[\\frac{\\partial\\phi_{t,x_{1}}^{-1}(x)}{\\partial x}\\right]\\operatorname{d}x_{1}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "772 Then, Eq. (10) remains the same in general case. In the case of linear $\\phi$ , the extension of Eq. (11)   \n773 reads ", "page_idx": 23}, {"type": "equation", "text": "$$\nv(x,t)=\\frac{\\int(x_{1}-x)\\,\\pi\\big(\\phi_{t,x_{1}}^{-1}(x),x_{1}\\big)\\operatorname*{det}\\biggl[\\frac{\\partial\\phi_{t,x_{1}}^{-1}(x)}{\\partial x}\\biggr]\\,\\mathrm{d}x_{1}}{(1-t)\\int\\pi\\big(\\phi_{t,x_{1}}^{-1}(x),x_{1}\\big)\\operatorname*{det}\\biggl[\\frac{\\partial\\phi_{t,x_{1}}^{-1}(x)}{\\partial x}\\biggr]\\,\\mathrm{d}x_{1}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "774 In all of the above cases, the essence of Algorithm 1 does not change (except that in the case of   \n775 dependent $x_{0}$ and $x_{1}$ we should be able either to calculate the value of $\\bar{\\pi}\\big(\\phi_{t,x_{1}}^{-1}(\\bar{x}),x_{1}\\big)\\,/\\,\\rho_{1}(x_{1})$ or to   \n776 estimate it). ", "page_idx": 23}, {"type": "text", "text": "777 D Several analytical results, following from the explicit formula ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "778 In this section, we present several analytical results that directly follow from our exact formulas for   \n779 the vector field, which, to the best of our knowledge, have not been published before. ", "page_idx": 23}, {"type": "text", "text": "780 D.1 Exact path from one Gaussian to another Gaussian ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "781 Consider the flow from a one-dimensional Gaussian distribution $\\rho_{0}\\sim\\mathcal{N}\\left(\\cdot\\vert\\,\\mu_{0},\\sigma_{0}^{2}\\right)$ into another (with   \n782 other parameters) Gaussian distribution $\\rho_{1}\\sim\\mathcal{N}\\left(\\cdot\\vert\\,\\mu_{1},\\sigma_{1}^{2}\\right)$ . Note that in this case the generalization   \n783 to the multivariate case is done directly, so the spatial variables are separated. ", "page_idx": 23}, {"type": "text", "text": "784 From the general formula (11) we have: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{v(x,t)=\\displaystyle\\frac{\\int(x_{1}-x)\\mathcal{N}\\left(\\frac{x-t x_{1}}{1-t}\\left|\\mu_{0},\\sigma_{0}^{2}\\right)\\mathcal{N}\\left(x_{1}\\right|\\mu_{1},\\sigma_{1}^{2}\\right)\\mathrm{d}x_{1}}{\\left(1-t\\right)\\int\\mathcal{N}\\left(\\frac{x-t x_{1}}{1-t}\\left|\\mu_{0},\\sigma_{0}^{2}\\right)\\mathcal{N}\\left(x_{1}\\right|\\mu_{1},\\sigma_{1}^{2}\\right)\\mathrm{d}x_{1}}}\\,,}}\\\\ {{=\\displaystyle\\frac{\\int(x_{1}-x)\\exp\\left(-\\left(\\frac{x-t x_{1}}{1-t}-\\mu_{0}\\right)^{2}/(2\\sigma_{0}^{2})-(x_{1}-\\mu_{1})^{2}/(2\\sigma_{1}^{2})\\right)\\mathrm{d}x_{1}}{(1-t)\\int\\exp\\left(-\\left(\\frac{x-t x_{1}}{1-t}-\\mu_{0}\\right)^{2}/(2\\sigma_{0}^{2})-(x_{1}-\\mu_{1})^{2}/(2\\sigma_{1}^{2})\\right)\\mathrm{d}x_{1}}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "785 Both integrals in the last expression are taken explicitly: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\int\\!\\!N\\left(\\frac{x-t x_{1}}{1-t}\\bigg|\\,\\mu_{0},\\sigma_{0}^{2}\\right)N\\left(x_{1}\\big|\\,\\mu_{1},\\sigma_{1}^{2}\\right)\\mathrm{d}x_{1}=}}\\\\ &{}&{=\\frac{\\exp\\left(-\\frac{(x-\\mu_{0}(1-t)-\\mu_{1}t)^{2}}{2\\left(\\sigma_{1}^{2}t^{2}+\\sigma_{0}^{2}(1-t)^{2}\\right)}\\right)}{\\sqrt{2\\pi}\\sqrt{\\sigma_{0}^{2}+\\frac{\\sigma_{1}^{2}t^{2}}{(t-1)^{2}}}}=N\\left(\\frac{x}{1-t}\\bigg|\\,\\frac{\\mu_{0}(1-t)+\\mu_{1}t}{1-t},\\sigma_{0}^{2}+\\frac{\\sigma_{1}^{2}t^{2}}{\\left(t-1\\right)^{2}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "786 Note that the last relation can be obtained as a distribution of two Gaussian random variables with   \n787 corresponding parameters. ", "page_idx": 24}, {"type": "text", "text": "788 The second integral: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\int\\frac{x_{1}-x}{1-t}N\\left(\\frac{x-t x_{1}}{1-t}\\bigg|\\mu_{0},\\sigma_{0}^{2}\\right)\\mathcal{N}\\left(x_{1}\\big|\\,\\mu_{1},\\sigma_{1}^{2}\\right)\\mathrm{d}x_{1}=}}\\\\ &{}&{=\\frac{\\exp\\bigg(-\\frac{(x-\\mu_{0}(1-t)-\\mu_{1}t)^{2}}{2\\left(\\sigma_{1}^{2}t^{2}+\\sigma_{0}^{2}(1-t)^{2}\\right)}\\bigg)}{\\sqrt{2\\pi}}\\frac{(1-t)\\,\\left(\\sigma_{1}^{2}t(x-\\mu_{0})+\\sigma_{0}^{2}(t-1)(x-\\mu_{1})\\right)}{\\left(\\sigma_{1}^{2}t^{2}+\\sigma_{0}^{2}(1-t)^{2}\\right)^{3/2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "789 Thus, in the considered case we can explicitly write the expression for the vector field $v$ : ", "page_idx": 24}, {"type": "equation", "text": "$$\nv(x,t)=\\frac{\\sigma_{1}^{2}t(x-\\mu_{0})-\\sigma_{0}^{2}(1-t)(x-\\mu_{1})}{\\sigma_{1}^{2}t^{2}+\\sigma_{0}^{2}(1-t)^{2}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "790 For this vector field we can explicitly solve the equation for the path $x(t)$ starting from the arbitrary   \n791 point $x_{0}$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\displaystyle\\frac{\\partial\\boldsymbol{x}(t)}{\\partial t}=\\boldsymbol{v}(\\boldsymbol{x}(t),\\,t),}\\\\ {\\boldsymbol{x}(0)=\\boldsymbol{x}_{0}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "792 The solution is: ", "page_idx": 24}, {"type": "equation", "text": "$$\nx(t)=(1-t)\\mu_{0}+t\\mu_{1}+(x_{0}-\\mu_{0})\\sqrt{(\\sigma_{1}/\\sigma_{0})^{2}t^{2}+(1-t)^{2}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "793 Note that although this solution does not correspond to the Optimal Transport joint distribution, since   \n794 the obtained path is not a straight line in general, (i. e. we do not have a solution to the Kantorovich\u2019s   \n795 formulation of the OT problem) the endpoint $x(1)=\\mu_{1}+(x_{0}-\\mu_{0})\\frac{\\sigma_{1}}{\\sigma_{0}}$ falls exactly in the one that   \n796 is optimal if we solve the OT problem in the Monge formultation. Thus, the map $x(0)\\to x(1)$ is the   \n797 OT map for the case of 2 Gaussian. ", "page_idx": 24}, {"type": "text", "text": "798 See the Fig. 4 for the examples of the paths for the obtained solution. ", "page_idx": 24}, {"type": "text", "text": "799 D.2 From one Gaussian to Gaussian Mixture ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "800 Let initial distribution be standard Gaussian $\\rho_{0}=\\mathcal{N}\\left(\\cdot\\vert\\,0,1^{2}\\right)$ , and the target distribution be Gaussian   \n801 Mixture (GM) of two symmetric Gaussians: $\\rho_{1}(x)=\\mathrm{{1}}/2(\\dot{{\\mathcal N}}\\left(x\\big|\\,\\mu,\\sigma^{2}\\right))+{\\mathcal N}\\left(x\\big|-\\mu,\\sigma^{2}\\right))$ , In this ", "page_idx": 24}, {"type": "image", "img_path": "XYDMAckWMa/tmp/7a4ddd652f499b2d4969aa58d30cde1e85705fadea089a316183047589e2e378.jpg", "img_caption": ["Figure 4: a) $N=40$ random trajectories from from $\\mathcal{N}\\left(\\cdot|\\,0,1^{2}\\right)$ to $\\mathcal{N}\\left(\\cdot|\\,2,3^{2}\\right)$ ; (b) 2D plot of the vector field in this case "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "802 case, we can obtain exact form for $v$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{v(x,t)=\\frac{\\exp\\Big(-\\frac{\\mu^{2}}{2\\sigma^{2}}+\\frac{\\mu^{2}t^{2}+x^{2}}{\\sigma^{2}t^{2}+(t-1)^{2}}-\\frac{x^{2}}{2(t-1)^{2}}\\Big)}{\\left(\\sigma^{2}t^{2}+(t-1)^{2}\\right)\\left(e^{\\frac{(x-\\mu)^{2}}{2(\\sigma^{2}t^{2}+(t-1)^{2})}}+e^{\\frac{(\\mu t+x)^{2}}{2(\\sigma^{2}t^{2}+(t-1)^{2})}}\\right)}\\times}\\\\ &{\\Bigg[\\mu(t-1)\\left(\\exp\\left(\\frac{\\left(\\mu(t-1)^{2}-\\sigma^{2}t x\\right)^{2}}{2\\sigma^{2}(t-1)^{2}\\left(\\sigma^{2}t^{2}+(t-1)^{2}\\right)}\\right)-\\exp\\left(\\frac{\\left(\\mu(t-1)^{2}+\\sigma^{2}t x\\right)^{2}}{2\\sigma^{2}(t-1)^{2}\\left(\\sigma^{2}t^{2}+(t-1)^{2}\\right)}\\right)\\right)+}\\\\ &{\\cdot x\\left(\\sigma^{2}t+t-1\\right)\\left(\\exp\\left(\\frac{\\left(\\mu(t-1)^{2}-\\sigma^{2}t x\\right)^{2}}{2\\sigma^{2}(t-1)^{2}\\left(\\sigma^{2}t^{2}+(t-1)^{2}\\right)}\\right)+\\exp\\left(\\frac{\\left(\\mu(t-1)^{2}+\\sigma^{2}t x\\right)^{2}}{2\\sigma^{2}(t-1)^{2}\\left(\\sigma^{2}t^{2}+(t-1)^{2}\\right)}\\right)\\Bigg]}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "803 but the expression for the path $x(t)$ is unknown. ", "page_idx": 25}, {"type": "image", "img_path": "XYDMAckWMa/tmp/e2346ecfce1eef66fa90e86f837e6a22dbbbf823ef0140aad1d37728690591c8.jpg", "img_caption": [], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Figure 5: a) $N~=~80$ random trajectories from $\\mathcal{N}\\left(\\cdot|\\,0,1^{2}\\right)$ to GM of $\\mathcal{N}\\left(\\cdot|-2,1/2^{2}\\right)$ and $\\mathcal{N}\\left(\\cdot|\\,2,1/2^{2}\\right)$ ; (b) 2D plot of the vector field in this case ", "page_idx": 25}, {"type": "text", "text": "804 Numerically solution of the differential equation with the obtained vector field give the trajectories   \n805 shown in Fig. 5. ", "page_idx": 25}, {"type": "text", "text": "806 D.3 From Gaussian to Gaussian with stochastic ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "807 Using Eq. (44)-(46) we can explicitly calculate vector field $v$ and score $s$ with the setup as in Sec. D.1   \n808 but with additional noise, $i.\\,e.$ . in the stochastic case. ", "page_idx": 26}, {"type": "text", "text": "809 D.3.1 Gaussian to Gaussian with noise ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "810 Consider like in the Sec. D.1 the flow from a one-dimensional standard Gaussian distribution   \n811 $\\rho_{0}\\sim\\mathcal{N}\\left(\\cdot\\vert\\,0,0^{2}\\right)$ into another (with other parameters) Gaussian distribution $\\rho_{1}\\sim\\mathcal{N}\\left(\\cdot\\vert\\,\\mu_{1},\\sigma_{1}^{2}\\right)$ but   \n812 with additional noise as described above. ", "page_idx": 26}, {"type": "text", "text": "813 In this case we have for the field. ", "page_idx": 26}, {"type": "equation", "text": "$$\nv(x,\\,t)=\\frac{x\\bigl(t\\sigma_{1}^{2}+(1-t)\\sigma_{e}^{2}/2\\bigr)-(x-\\mu_{1})\\bigl((1-t)+t\\sigma_{e}^{2}/2\\bigr)}{t(1-t)\\sigma_{e}^{2}+\\sigma_{1}^{2}t^{2}+(1-t)^{2}}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "814 We can solve ODE with this field and get the expression for the trajectories, starting from the given   \n815 point $x_{0}$ : ", "page_idx": 26}, {"type": "equation", "text": "$$\nx(t)=\\mu_{1}t+x_{0}\\sqrt{t(1-t)\\sigma_{e}^{2}+\\sigma_{1}^{2}t^{2}+(1-t)^{2}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "These trajectories, for different $x_{0}$ are depicted in Fig. 6. ", "page_idx": 26}, {"type": "image", "img_path": "XYDMAckWMa/tmp/acbcee6b12eb11cd78dff3950c4a2e4835bc77c924eb9b7a95f32ac05e965ebd.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Figure 6: a) $N=40$ random trajectories from $\\mathcal{N}\\left(\\cdot|\\,0,1^{2}\\right)$ to $\\mathcal{N}\\left(\\cdot|\\,2,3^{2}\\right)$ and 2D plot of the vector field in this case for different $\\sigma_{e}$ ", "page_idx": 26}, {"type": "text", "text": "816 ", "page_idx": 26}, {"type": "text", "text": "817 At the limit $\\sigma_{e}\\rightarrow0$ expressions (38) and (39) turn into expressions (35) and (36) as expected. ", "page_idx": 26}, {"type": "text", "text": "818 For the score $s$ in the considered case we have ", "page_idx": 26}, {"type": "equation", "text": "$$\ns(x,\\,t)=\\frac{t\\mu_{1}-x}{(1-t)^{2}+t(1-t)\\sigma_{e}^{2}+t^{2}\\sigma_{1}^{2}}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "819 Thus, we can explicitly write expressions for the stochastic process for the evolution from the initial   \n820 distribution $r h o_{0}$ (standard Gaussian) to the final distribution $\\rho_{1}$ : ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{d}x(x)=\\Bigg[\\frac{x\\left(t\\sigma_{1}^{2}+(1-t)\\sigma_{e}^{2}/2\\right)-\\left(x-\\mu_{1}\\right)\\left((1-t)+t\\sigma_{e}^{2}/2\\right)}{t(1-t)\\sigma_{e}^{2}+\\sigma_{1}^{2}t^{2}+(1-t)^{2}}\\,+}\\\\ {+\\,\\frac{g^{2}(t)}{2}\\frac{t\\mu_{1}-x}{(1-t)^{2}+t(1-t)\\sigma_{e}^{2}+t^{2}\\sigma_{1}^{2}}\\Bigg]\\,\\mathrm{d}t+g(t)\\,\\mathrm{d}W(t)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "821 Here $g(t)$ is arbitrary smooth function. In the case of Shr\u00f6dinger Bridge we take $g(t)=\\sigma_{e}\\sqrt{t(1-t)}$ . ", "page_idx": 26}, {"type": "text", "text": "822 E Detail on the SDE case ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "823 E.1 Optimal vector field and score for stochastic map ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "824 Following [20] we consider a so-called Brownian bridge $B(t)$ from $x_{0}$ to $x_{1}$ with constant diffusion   \n825 rate $\\sigma_{e}$ . This stochastic process can be expressed through a multidimensional standard Winner   \n826 process $W(t)$ as ", "page_idx": 27}, {"type": "equation", "text": "$$\nB(t\\mid x_{0},x_{1})=(1-t)x_{0}+t x_{1}+\\sigma_{e}(1-t)W\\left(\\frac{t}{1-t}\\right).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "827 Thus, the conditional distribution $p(t,x\\mid x_{0},\\,x_{1})$ conditioned on the starting $x_{0}$ and end point $x_{1}$ is   \n828 Gaussian: ", "page_idx": 27}, {"type": "equation", "text": "$$\np(x,t\\mid x_{0},\\,x_{1})=\\mathcal{N}\\left(x\\right|(1-t)x_{0}+t x_{1},\\sigma_{e}^{2}t(1-t)\\right).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "829 We can not directly use the results Theorem 3 from [9] (or similar Theorem 2.1 from [19] ) for   \n830 the Gaussian paths, as in this case $\\sigma(0)=0$ . To circumvent this obstacle and to be able to write   \n831 an expression for the conditional velocity, we assume that we have a Gaussian distribution with a   \n832 very narrow peak at the initial $(t=0_{\\mathrm{}}$ ) and final $\\mathit{\\Theta}(t=1)$ ) points. In other words, we will consider   \n833 conditional probabilities of the form ", "page_idx": 27}, {"type": "equation", "text": "$$\np(x,t\\mid x_{0},\\,x_{1})=\\mathcal{N}\\left(x\\right|(1-t)x_{0}+t x_{1},\\sigma_{e}^{2}(t+\\eta)(1-t+\\eta)\\right),\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "834 where parameter $\\eta$ is small enough. Then we can use the above Theorems and immediately write ", "page_idx": 27}, {"type": "equation", "text": "$$\nv_{x_{0},x_{1}}(x,t)=\\frac{\\sigma^{\\prime}(t)}{\\sigma(t)}\\big(x-\\mu(t)\\big)+\\mu^{\\prime}(t)=\\frac{1-2t}{2(t+\\eta)(1-t+\\eta)}\\big(x-(1-t)x_{0}-t x_{1}\\big)+x_{1}-x_{0}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "835 After integrating over $x_{0}$ and $x_{1}$ , we can take the limit $\\eta\\rightarrow0$ . Thus, now for fixed $x_{0}$ and $x_{1}$ we do   \n836 not have a fixed value of $x_{t}$ in which to train the model, but a random one. In general case, we end up   \n837 to the loss: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathcal{L}_{v}=\\mathbb{E}_{t\\sim\\mathcal{U}(0,1),\\;\\{x_{1},x_{0}\\}\\sim\\pi,\\;x\\sim p(\\cdot,t|x_{0},x_{1})}\\|v_{\\theta}(x,\\,t)-v_{x_{0},x_{1}}(x,t)\\|^{2},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "838 where $\\pi(x_{1},\\,x_{0})$ is the density of the joint distributions with the marginal equal to the two given   \n839 probabilities: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\int\\pi(x_{1},\\,x_{0})\\,\\mathrm{d}x_{1}=\\rho_{0}(x_{0}),\\quad\\int\\pi(x_{1},\\,x_{0})\\,\\mathrm{d}x_{0}=\\rho_{1}(x_{1}).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "840 In the simple case, $\\pi(x_{1},\\,x_{0})\\,=\\,\\rho_{0}(x_{0})\\rho_{1}(x_{1})$ . Vector field in Eq. (43) if taken in the form of   \n841 Eq. (42).   \n842 Now, we can obtain an explicit form for the vector field $v$ at which the written loss is reached its   \n843 minimum by performing the same calculations as in the derivation of formula (10): ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 27}, {"type": "equation", "text": "$$\nv(x,t)=\\frac{\\iint v_{x_{0},x_{1}}(x,t)\\,p(x,t\\mid x_{0},\\,x_{1})\\,\\pi(x_{0},x_{1})\\,\\mathrm{d}x_{0}\\,\\mathrm{d}x_{1}}{\\iint p(x,t\\mid x_{0},\\,x_{1})\\,\\pi(x_{0},x_{1})\\,\\mathrm{d}x_{0}\\,\\mathrm{d}x_{1}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "844 As in the work [20] we can also train score network. Namely, as marginals for Brownian bridge are   \n845 Gaussian, we can write explicit conditional score for conditional probabilistic path ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\nabla\\log p(x,t\\mid x_{0},\\,x_{1})=\\frac{\\mu(t)-x}{\\sigma_{e}^{2}(t)}=\\frac{x_{0}(1-t)+x_{1}t-x}{\\sigma_{e}^{2}t(1-t)}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "846 In the work [20] the following loss is introduced to train a model for this score ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathcal{L}_{s}=\\mathbb{E}_{t\\sim\\mathcal{U}(0,1),\\;\\{x_{1},x_{0}\\}\\sim\\pi,\\,x\\sim p(\\cdot,t|x_{0},x_{1})}\\|s_{\\theta}(x,t)-\\nabla\\log p(x,t\\mid x_{0},\\,x_{1})\\|^{2}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "847 Similar to (44), for the optimal score $s$ we have: ", "page_idx": 27}, {"type": "equation", "text": "$$\ns(x,t)=\\frac{\\iint\\nabla\\log p(x,t\\mid x_{0},\\,x_{1})\\,p(x,t\\mid x_{0},\\,x_{1})\\,\\pi(x_{0},x_{1})\\,\\mathrm{d}x_{0}\\,\\mathrm{d}x_{1}}{\\iint p(x,t\\mid x_{0},\\,x_{1})\\,\\pi(x_{0},x_{1})\\,\\mathrm{d}x_{0}\\,\\mathrm{d}x_{1}},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "848 where $p$ is given in (41). ", "page_idx": 27}, {"type": "text", "text": "849 E.2 Use stochastic ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "850 Note that the obtained vector field gives marginal distributions $p({\\boldsymbol{x}},t)$ , which (in the limit $\\eta\\rightarrow0$ ) at   \n851 $t=1$ leads to the distribution we need: $p(x,\\bar{t}=1)=\\rho_{1}(x)$ . However, the addition of the stochastic   \n852 term allows us to extend the scope of application of the explicit formula for the vector field. In   \n853 particular, it can be applied to the situation when we have two sets of samples and both distributions   \n854 are unknown, as well as the possibility of constructing SDE and solving it using, for example, the   \n855 Euler\u2013Maruyama method (see examples below). ", "page_idx": 28}, {"type": "text", "text": "856 As consequence of Theorem 3.1 from [20] we have that, if $v$ is given by Eq. (44) then ODE ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\frac{\\partial\\rho(x,t)}{\\partial t}=-\\operatorname{div}\\!\\big(\\rho(x,t)v(x,t)\\big)\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "857 recovers the marginal $\\rho(x,t)$ (with the given initial conditions) of the stochastic process $P(t)$ which   \n858 is obtained by marginalization conditional Brownian bridge (40) over initial and target distribution ", "page_idx": 28}, {"type": "equation", "text": "$$\nP(t)=\\int B(t\\mid x_{0},x_{1})\\pi(x_{0},x_{1})\\,\\mathrm{d}x_{0}\\,\\mathrm{d}x_{1}\\,.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "859 As the second consequence of this Theorem, the SDE ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathrm{d}x(t)=\\Big(v\\big(x(t),t\\big)+\\frac{g^{2}(t)}{2}s\\big(x(t),t\\big)\\Big)\\,\\mathrm{d}t+g(t)\\,\\mathrm{d}W(t)\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "860 generates so-called Markovization of the process $P(t)$ . Indeed, we can rewrite PDE Eq. (47) in the   \n861 form ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\frac{\\partial\\rho(x,t)}{\\partial t}=-\\,\\mathrm{div}\\Big(\\rho(x,t)v(x,t)+\\frac{g^{2}(t)}{2}\\nabla\\rho(x,t)\\Big)+\\frac{g^{2}(t)}{2}\\Delta\\rho(x,t),\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "862 where nabla operator is defined as $\\Delta=\\operatorname{div}\\nabla$ . Thus, we get the Fokker\u2013Planck equation for the   \n863 density of the stochastic process (48). ", "page_idx": 28}, {"type": "text", "text": "864 E.3 Particular cases ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "865 In particular case of Brownian bridge when $\\sigma_{e}(t)\\;\\;=\\;\\;\\sigma_{\\epsilon}\\sqrt{t(1-t)}$ , then $\\begin{array}{r l r}{\\sigma_{e}^{\\prime}(t)}&{{}=}&{\\sigma_{\\epsilon}(1\\mathrm{~-~}}\\end{array}$   \n866 $2t)/\\big(2\\sqrt{t(1-t)}\\big)$ . In this section we consider simple case of separable variables $\\pi(x_{0},x_{1})\\;=\\;$   \n867 $\\rho_{0}(x_{0})\\rho_{1}(x_{1})$ . ", "page_idx": 28}, {"type": "text", "text": "868 E.3.1 Gaussian initial distribution ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "869 In the case, when $\\rho_{0}$ is standard Gaussian distribution: $\\rho_{0}\\,=\\,\\mathcal{N}\\left(\\cdot\\vert\\,0,1^{2}\\right)$ , we can take integral   \n870 on $x_{0}$ and then take the limit $\\eta\\rightarrow0$ in the expressions for $v$ and $s$ . First, consider the expression   \n871 for $v$ : where we use explicit expression (41) for conditional density path and Eq. (42) for conditional   \n872 velocity: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{v(x,t)=\\frac{\\int w(x,t\\mid x_{1})N\\left(x\\mid x_{1}t,\\sigma_{e}^{2}t(1-t)+(1-t)^{2}\\right)\\rho_{1}(x_{1})\\mathrm{d}x_{1}}{\\int N\\left(x\\mid x_{1}t,\\sigma_{e}^{2}t(1-t)+(1-t)^{2}\\right)\\rho_{1}(x_{1})\\mathrm{d}x_{1}}=}}\\\\ &{}&{\\,=\\frac{\\int w(x,t\\mid x_{1})\\rho_{0}\\left(\\frac{x-x_{1}t}{\\sqrt{\\sigma_{e}^{2}t(1-t)+(1-t)^{2}}}\\right)\\rho_{1}(x_{1})\\mathrm{d}x_{1}}{\\int\\rho_{0}\\left(\\frac{x-x_{1}t}{\\sqrt{\\sigma_{e}^{2}t(1-t)+(1-t)^{2}}}\\right)\\rho_{1}(x_{1})\\mathrm{d}x_{1}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "873 where $w(x,t\\mid x_{1})$ is the conditional velocity, generated by the conditional map $\\phi_{t,x_{1}}(x)\\;=\\;$   \n874 $\\sqrt{\\sigma_{e}^{2}t(1-t)+(1-t)^{2}}+t x_{1}$ : ", "page_idx": 28}, {"type": "equation", "text": "$$\nw(x,t\\mid x_{1})=\\frac{x_{1}-x}{1-t+t\\sigma_{e}^{2}}+\\sigma_{e}^{2}\\frac{(1-2t)x+t x_{1}}{2\\big((1-t)^{2}+(1-t)t\\sigma_{e}^{2}\\big)}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "875 Thus, note that in the case of Gaussian distributions, all the difference between this expression and   \n876 the expression without the stochastic part is the appearance of additional (time-dependent, in general)   \n877 variance. Marginal distributions are still Gaussian\u2019s. ", "page_idx": 28}, {"type": "text", "text": "878 Similar, using Eq. (46) we have for the score $s$ : ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{s(x,t)=}&{\\frac{\\int(t x_{1}-x)\\mathcal{N}\\left(x\\big|\\,x_{1}t,\\sigma_{e}^{2}t(1-t)+(1-t)^{2}\\right)\\,\\rho_{1}(x_{1})\\mathrm{d}x_{1}}{\\big((1-t)^{2}+(1-t)t\\sigma_{e}^{2}\\big)\\int\\mathcal{N}\\left(x\\big|\\,x_{1}t,\\sigma_{e}^{2}t(1-t)+(1-t)^{2}\\right)\\,\\rho_{1}(x_{1})\\mathrm{d}x_{1}}=}&\\\\ &{}&{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\int(t x_{1}-x)\\rho_{0}\\Big(\\frac{x-x_{1}t}{\\sqrt{\\sigma_{e}^{2}t(1-t)+(1-t)^{2}}}\\Big)\\rho_{1}(x_{1})\\mathrm{d}x_{1}}&\\\\ &{}&{\\qquad\\qquad\\qquad=\\frac{\\int(1x-t)\\rho_{e}^{2}\\big(\\frac{x-x_{1}t}{\\sqrt{\\sigma_{e}^{2}t(1-t)+(1-t)^{2}}}\\big)\\rho_{1}(x_{1})\\mathrm{d}x_{1}}{\\big((1-t)^{2}+(1-t)t\\sigma_{e}^{2}\\big)\\int\\rho_{0}\\Big(\\frac{x-x_{1}t}{\\sqrt{\\sigma_{e}^{2}t(1-t)+(1-t)^{2}}}\\Big)\\rho_{1}(x_{1})\\mathrm{d}x_{1}}.}&\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "879 E.3.2 Samples instead of distributions ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "888801 dCiostnrsiibduetiro tnhs,e saen d e, rbe utw deo  onnolty  khnaovwe  tahceicre sesx ptloi ctith ee xsparemspslieosn $\\{x_{0}^{i}\\}_{i=1}^{N_{0}}$ caansde, $\\{x_{1}^{i}\\}_{i=1}^{N_{1}}$ e sftriommat eb tohthe $\\rho_{0}$ $\\rho_{1}$ 882 vector field using by a method similar to the one we used to estimate the vector field in (15): ", "page_idx": 29}, {"type": "equation", "text": "$$\nv(x,t)\\approx\\frac{\\sum_{i=1}^{N_{0}}\\sum_{j=1}^{N_{1}}v_{x_{0}^{i},x_{1}^{j}}(x,t)\\,p(x,t\\mid x_{0}^{i},\\,x_{1}^{j})}{\\sum_{i=1}^{N_{0}}\\sum_{j=1}^{N_{1}}p(x,t\\mid x_{0}^{i},\\,x_{1}^{j})}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "883 Similar for the score ", "page_idx": 29}, {"type": "equation", "text": "$$\ns(x,t)\\approx\\frac{\\sum_{i=1}^{N_{0}}\\sum_{j=1}^{N_{1}}\\pmb{\\nabla}p(x,t\\mid x_{0}^{i},x_{1}^{j})\\,p(x,t\\mid x_{0}^{i},\\,x_{1}^{j})}{\\sum_{i=1}^{N_{0}}\\sum_{j=1}^{N_{1}}p(x,t\\mid x_{0}^{i},\\,x_{1}^{j})}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "884 In addition, we can also use the importance sampling method in this case. Namely we can use   \n885 both approaches: self-normalized importance sampling and rejection sampling, similar to what is   \n886 described in Sec. B ", "page_idx": 29}, {"type": "text", "text": "887 F Consistency of Eq. (24) in the case of optimal transport ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Let us analyze what happens if in formula (24) the joint density $\\pi$ represents the following Dirac delta-function4: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\pi(x_{0},x_{1})=\\delta{\\bigl(}x_{0}-F(x_{1}){\\bigr)},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "888 i. e. we have a deterministic mapping $F$ from $x_{1}$ to $x_{0}$ . Then, the Eq. (34) come to ", "page_idx": 29}, {"type": "equation", "text": "$$\nv(x,t)=\\frac{\\int(x_{1}-x)\\,\\delta\\bigl(\\phi_{t,x_{1}}^{-1}(x)-F(x_{1})\\bigr)\\,\\mathrm{d}x_{1}}{(1-t)\\int\\delta\\bigl(\\phi_{t,x_{1}}^{-1}(x)-F(x_{1})\\bigr)\\,\\mathrm{d}x_{1}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "889 Let $y(x,t)$ be the unique solution of the equation ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\phi_{t,y}^{-1}(x)=F(y),\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "considered as an equation on $y$ . Then ", "page_idx": 29}, {"type": "equation", "text": "$$\nv(x,t)={\\frac{x-y(x,t)}{1-t}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Now, let us use linear mapping $\\phi_{t,x_{1}}(x)\\,=\\,x_{1}t+x(1-t)$ , with inverse $\\begin{array}{r}{\\phi_{t,x_{1}}^{-1}(x)\\,=\\,\\frac{x-t x_{1}}{1-t}}\\end{array}$ , and consider the simplest case when the original distribution is a $d$ -dimensional standard Gaussian and $\\rho_{1}$ is a $d$ -dimensional Gaussian with mean $\\mu$ and diagonal variance $\\Sigma=\\deg(\\sigma)$ . We know the OT correspondence between Gaussians, namely ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\left(F(x_{1})\\right)_{i}={\\frac{(x_{1}-\\mu)_{i}}{\\Sigma_{i i}}},\\quad\\forall1\\geq i\\geq d.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Here and further by index $i$ we denote $i$ th component of the corresponding vector. Then, the Eq. (53) reads as ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\frac{(x-y t)_{i}}{1-t}=\\frac{(y-\\mu)_{i}}{\\Sigma_{i i}},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "with the solution ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\big(y(x,t)\\big)_{i}=\\frac{\\mu_{i}(1-t)+x_{i}\\Sigma_{i i}}{1+(\\Sigma_{i i}-1)t}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Then the expression for the vector field is ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\left(v(x,t)\\right)_{i}=\\frac{\\mu_{i}+x_{i}(\\Sigma_{i i}-1)}{1+(\\Sigma_{i i}-1)t}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Now, knowing the expression for velocity, we can write the equations for the trajectories $x(t)$ : ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{c}{\\displaystyle\\left(x^{\\prime}(t)\\right)_{i}=\\frac{\\mu_{i}+(x(t))_{i}(\\Sigma_{i i}-1)}{1+(\\Sigma_{i i}-1)t},}\\\\ {x(0)_{i}=(x_{0})_{i}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "This equation have closed-form solution: ", "page_idx": 30}, {"type": "equation", "text": "$$\nx(t)=\\mu t+x_{0}-\\left(1-\\sigma\\right)t x_{0}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Analyzing the obtained solution, we conclude that, first, the trajectories obey the given mapping $F$ : ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\left(F(x(1))\\right)_{i}=(x_{0})_{i}=\\frac{(x(1)-\\mu)_{i}}{\\Sigma_{i i}},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "890 And, second, the trajectories are straight lines (in space), as they should be when the flow carries   \n891 points along the optimal transport.   \n892 As a final conclusion, note that, of course, if we are mapping optimal transport $F$ , then it is meaning  \n893 less to use numerical formula (16). However, usually the exact value of the mapping $F$ is not known,   \n894 and our theoretical formula (34) can help to rigorously establish the error that is committed when an   \n895 approximate mapping is used instead of the optimal one. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "896 G Analytical derivations for example in Fig. 1(b) ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "897 G.1 CFM dispersion ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "898 To derive the analytical expression for the optimal flow velocity in the case of two normal distributions   \n899 $\\rho_{0}\\sim N(0,I)$ and $\\rho_{1}\\sim\\dot{N}(\\mu,\\sigma^{2}I)$ , we start by substituting $\\mu_{0}=0$ , $\\sigma_{0}=1$ , $\\mu_{1}=\\mu$ , $\\sigma_{1}=\\sigma$ , to the   \n900 exact expression (35) to get ", "page_idx": 30}, {"type": "equation", "text": "$$\nv(x,t)=\\frac{t\\sigma^{2}+t-1}{(1-t)^{2}+t^{2}\\sigma^{2}}x+\\frac{1}{(1-t)^{2}+t^{2}\\sigma^{2}}(\\mu-t\\mu)=w(t)x+C,\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where ", "page_idx": 30}, {"type": "equation", "text": "$$\nw(t)=\\frac{t\\sigma^{2}+t-1}{(1-t)^{2}+t^{2}\\sigma^{2}},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "901 and $C$ is constant independent of $x$ . We then redefine the dispersion based on Eq. (19) using   \n902 $x=(1-t)x_{0}+t x_{1}$ with $x_{0}\\sim\\rho_{0}$ and $x_{1}\\sim\\rho_{1}$ : ", "page_idx": 30}, {"type": "equation", "text": "$$\n{\\mathbb D}_{x,x_{1}}f(x,\\,x_{1})={\\mathbb D}_{x_{0},x_{1}}f\\big((1-t)x_{0}+t x_{1},\\,x_{1}\\big)\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "903 This leads us to the final expression: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathbb{D}_{x,x_{1}}\\Delta v(x,t)=\\mathbb{D}_{x_{0},x_{1}}\\!\\left((1-w(t))x_{1}-(1+w(t)(1-t))x_{0}\\right)=}\\\\ &{}&{=(1+w(t)(1-t))^{2}\\mathbb{D}_{x_{0}}x_{0}+(1-w(t))^{2}\\mathbb{D}_{x_{1}}x_{1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "904 This provides a comprehensive representation of the updated dispersion for the CFM objective at any   \n905 given time $t$ . ", "page_idx": 30}, {"type": "text", "text": "Require: Density function for initial distribution $\\rho_{0}$ ; sampler for target distribution $\\rho_{1}$ ; parameter $M$ (number of samples for evaluation); parameter $N$ (number of samples from $\\rho_{1}$ for certain samples $x\\sim\\rho_{m}(x,t))$ ; optimal model $\\boldsymbol{v}(\\boldsymbol{x},t)$ ; time for evaluation $t$ .   \nEnsure: numerical evaluation of dispersion update for ExFM objective   \n1: Sample $(M\\cdot N)$ samples $\\boldsymbol{x}_{1}^{i,j}$ from $\\rho_{1}$ , where $i\\in[1,M]$ and $j\\in[1,N]$   \n2: Sample $(M)$ samples $\\mathbf{\\bar{\\rho}}_{x_{0}^{i}}$ from $\\rho_{0}$ , where $i\\in[1,M]$   \n34::  CCoommppuuttee $x^{i}$ $(1-t)x_{0}^{i}+t x_{1}^{i,0}$ , where   \n5: Compute $\\begin{array}{r l}&{v^{d}(x^{i},t)=\\displaystyle\\sum_{j=1}^{N}\\tilde{\\rho}^{i,j}(t)\\frac{x_{1}^{i,j}-x^{i}}{1-t},\\mathrm{where~}\\tilde{\\rho}^{i,j}(t)=\\rho_{0}\\left(\\frac{x^{i}-t x_{1}^{i,j}}{1-t}\\right)/\\displaystyle\\sum_{j=1}^{N}\\rho_{0}\\left(\\frac{x^{i}-t x_{1}^{i,j}}{1-t}\\right)}\\\\ &{\\mathrm{and~return~dispersion~}\\mathbb{D}_{i}(v(x^{i},t)-v^{d}(x^{i},t))}\\end{array}$ ", "page_idx": 31}, {"type": "text", "text": "906 G.2 ExFM dispersion ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "907 The analytical derivation of the updated dispersion for the ExFM objective proves to be complex in   \n908 practice. Therefore, for the example at hand, a numerical scheme was employed for evaluation. The   \n909 procedure outlined in Alg. 2 was utilized for this task. The experiment\u2019s parameters for the algorithm   \n910 were as follows: $M\\,=\\,200k$ , $N=128$ , $\\rho_{0}\\,=\\,N(0,I)$ , $\\rho_{1}\\,\\dot{^2}=\\,N(\\mu,\\sigma^{\\dot{2}}I)$ , and the optimal model   \n911 $\\boldsymbol{v}(\\boldsymbol{x},t)$ was derived from equation (54). ", "page_idx": 31}, {"type": "text", "text": "912 H Additional Experiments ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "913 H.1 2D toy examples ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "914 To ensure the reliability and impartiality of the outcomes, we carried out the experiment under   \n915 uniform conditions and parameters. Initially, we generated a training set of batch size $N=10\\small{,}000$   \n916 points. The employed model was a simple Multilayer Perceptron with ReLu activations and 2 hidden   \n917 layers of 512 neurons, Adam optimizer with a learning rate of $10^{-3}$ , and no learning rate scheduler.   \n918 We determined the number of iteration steps equal to 10000. Subsequently, we configured the mini   \n919 batch size $n=256$ during the training procedure, with the primary objective of minimizing the Mean   \n920 Squared Error (MSE) loss. The full training algorithm and notations can be seen in Algorithm 1. To   \n921 perform sampling, we employed the function odeint with dopri5 method from the python package   \n922 torchdiffeq import odeint with atol and rtol equal $1e-5$ . ", "page_idx": 31}, {"type": "text", "text": "923 H.2 Tabular ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "924 The power dataset (dimension $=6$ , train $\\mathrm{size}=1659917$ , test ${\\mathrm{size}}=204928)$ ) consisted of electric   \n925 power consumption data from households over a period of 47 months. The gas dataset (dimension   \n926 $=8$ , train s $\\mathrm{ize}=852174$ , test ${\\mathrm{size}}=105206)$ ) recorded readings from 16 chemical sensors exposed   \n927 to gas mixtures. The hepmass dataset (dimension $=21$ , train $\\mathrm{size}=315123$ , test si $\\mathrm{{ze}=174987}$ )   \n928 described Monte Carlo simulations for high energy physics experiments. The minibone (dimension   \n929 $=43$ , train ${\\mathrm{size}}=29556$ , test $\\mathrm{size}=3648)$ ) dataset contained examples of electron neutrino and muon   \n930 neutrino. Furthermore, we utilized the BSDS300 dataset (dimension $=63$ , train $\\mathrm{size}=1000000$ , test   \n931 $\\mathrm{size}=250000)$ , which involved extracting random $8\\mathrm{~x~}8$ monochrome patches from the BSDS300   \n932 datasets of natural images [11].   \n933 These diverse multivariate datasets are selected to provide a comprehensive evaluation of performance   \n934 across various domains. To maintain consistency, we followed the code available at the given GitHub   \n935 $\\operatorname{link}^{5}$ to ensure that the same instances and covariates were used for all the datasets.   \n936 To ensure the correctness of the experiments we conduct them with the same parameters. To train   \n937 the model we use the same MultiLayer Perceptron $(1024\\mathrm{~x~}3)$ model with ReLu activations, Adam   \n938 as optimizer with learning rate of $1\\dot{0}^{-3}$ and no learning rate scheduler. As in the pretrained step,   \n939 we use separately training and testing sets for training the model and calculating metrics. We train   \n940 the models on the full dataset (of size train_set_size) with batch size $N=5000$ (batch_size)   \n941 (except miniboone dataset, here we used 2000 since the smaller size of the dataset) and mini   \n942 batches $n=256$ elements (mini_batch_size), the number of epochs and steps for each dataset   \n943 is adaptive num_epochs $=$ train_set_size // batch_size and num_steps $=$ batch_size   \n944 // mini_batch_size.   \n945 For both 2D-toy an tabular data: we take $m=n$ time variable, individual value of variable $t$   \n946 corresponds to its pair $(x_{0},x_{1})$ . The notations $N$ , $n$ and $m$ corresponds to those in Algorithm 1. To   \n947 perform sampling, we employed the function odeint with dopri5 method from the python package   \n948 torchdiffeq import odeint with atol and rtol equal $1e-5$ . ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 31}, {"type": "image", "img_path": "XYDMAckWMa/tmp/d056258a6e67b17b5592efc5b609ea6082f6cbf8fb92061ff6a69f1d2942a002.jpg", "img_caption": ["Figure 7: Training loss comparison for ExFM, CFM and OT-CFM methods over 10 000 learning steps. "], "img_footnote": [], "page_idx": 32}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 32}, {"type": "table", "img_path": "XYDMAckWMa/tmp/a77d13731113c0f9b7e99b5993b472738610061b3eae4a6071d6f07b98d1fa33.jpg", "table_caption": ["Table 5: Learning parameters for Tabular datasets. "], "table_footnote": [], "page_idx": 32}, {"type": "table", "img_path": "XYDMAckWMa/tmp/86a6b887555d158cfa518b6c7dda1b32235ec1abf70f9ebb101dfe421b0c0f7f.jpg", "table_caption": ["Table 6: NLL comparison for ExFM, CFM and OT-CFM methods over 10 000 learning steps, mean and std taken from 10 sampling iterations. "], "table_footnote": [], "page_idx": 32}, {"type": "text", "text": "949 H.3 ExFM-S evaluation ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "950 The models were assessed using four toy datasets of two dimensions each. A three-layer MLP   \n951 network was utilized, featuring SeLU activations and a hidden dimension of 64. Optimization was   \n952 carried out using the AdamW optimizer with a learning rate of $10^{-3}$ and a weight decay of $10^{-5}$ .   \n953 The model was trained over 2,000 iterations with a batch size of 128. Inference was conducted using   \n954 the Euler solver for Ordinary Differential Equations (ODE) with 100 steps. To validate the models,   \n955 the POT library was employed to compute the Wasserstein distance based on 4,000 samples. The   \n956 experiments were performed on a single Nvidia H100 GPU with 80gb memory. ", "page_idx": 32}, {"type": "image", "img_path": "XYDMAckWMa/tmp/e760e0ca33857d663ee845729ad3a4ff7e9e4044d9aa168189f78951ea1e62fd.jpg", "img_caption": ["Figure 8: NLL comparison for ExFM, CFM and OT-CFM methods over 10 000 learning steps, mean and std for range taken from 10 sampling iterations. "], "img_footnote": [], "page_idx": 33}, {"type": "image", "img_path": "XYDMAckWMa/tmp/90c7e96dee5d35887f0bd88a0e98f4b2dd3eb5217384fdca00a486c01c7fda60.jpg", "img_caption": ["Figure 9: Training loss comparison for ExFM, CFM and OT-CFM methods, CIFAR-10 dataset. "], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "957 H.4 CIFAR 10 and MNIST ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "958 We conducted experiments related to high dimensional data, the parameters for training were taken   \n959 from the open-source code6 from the works [20, 19]. We saved the leverage of additional heuris  \n960 tics(EMA, lr scheduler). ", "page_idx": 33}, {"type": "text", "text": "Table 7: FID comparison for 4 sampling iterations, 400 000 learning steps. ", "page_idx": 33}, {"type": "table", "img_path": "XYDMAckWMa/tmp/9263db1d0a3ba90f0ef885521fd42ba3f23db4b5251c869bc2ab5f3575b77d0d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 33}, {"type": "table", "img_path": "XYDMAckWMa/tmp/11ae0fbd8e5eaee285bee1a256c38c41a857674d6b168511596cab6d6a988fa2.jpg", "table_caption": ["Table 8: FID comparison for ExFM, CFM and OT-CFM methods over 400 000 learning steps, mean and std taken from 4 sampling iterations. "], "table_footnote": [], "page_idx": 34}, {"type": "image", "img_path": "XYDMAckWMa/tmp/0117f623d683ffedfd0d031ce2c76b0c350d7cea92d0bc01dcc1a5628357e289.jpg", "img_caption": ["Figure 10: Training loss comparison for ExFM, CFM and OT-CFM methods, MNIST dataset. "], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "961 H.5 Metrics ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "962 For evaluating 2D toy data we use Energy Distance and W2 metricis, for Tabular datasets we use   \n963 Negative Log Likelihood, for CIFAR10 we took Fr\u00e9chet inception distance (FID) metrics. This   \n964 choice is connected with an instability and poor evaluation quality of Energy Distance metrics and   \n965 W2 among high-dimensional data . ", "page_idx": 34}, {"type": "text", "text": "966 H.5.1 Energy Distance ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "967 We use the generalized Energy Distance [18] (or E-metrics) to the metric space. ", "page_idx": 34}, {"type": "text", "text": "968 Consider the null hypothesis that two random variables, $X$ and $Y$ , have the same probability distribu  \n969 tions: $\\mu=\\nu$ . ", "page_idx": 34}, {"type": "text", "text": "For statistical samples from $X$ and $Y$ : ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\left\\{x_{1},\\ldots,x_{n}\\right\\}\\quad{\\mathrm{~and~}}\\quad\\left\\{y_{1},\\ldots,y_{m}\\right\\},\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "970 the following arithmetic averages of distances are computed between the $X$ and the $Y$ samples: ", "page_idx": 34}, {"type": "equation", "text": "$$\nA=\\frac{1}{n m}\\sum_{i=1}^{n}\\sum_{j=1}^{m}\\|x_{i}-y_{j}\\|,\\quad B=\\frac{1}{n^{2}}\\sum_{i=1}^{n}\\sum_{j=1}^{n}\\|x_{i}-x_{j}\\|,\\quad C=\\frac{1}{m^{2}}\\sum_{i=1}^{m}\\sum_{j=1}^{m}\\|y_{i}-y_{j}\\|.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "image", "img_path": "XYDMAckWMa/tmp/a93fc9c69f40b237eb48a9fd7017ecdf41b6bed001fac58905d5d1f4cf5bc4b5.jpg", "img_caption": ["Figure 11: FID comparison for ExFM, CFM and OT-CFM methods, CIFAR-10 dataset. "], "img_footnote": [], "page_idx": 35}, {"type": "image", "img_path": "XYDMAckWMa/tmp/7a9ce96e4dcea2579ba87e3e4ec467dbd5b5ab0f94f4a621cdea3adba7c10d7b.jpg", "img_caption": ["Figure 12: Sampled images from ExFM method, CIFAR-10 dataset. "], "img_footnote": [], "page_idx": 35}, {"type": "image", "img_path": "XYDMAckWMa/tmp/3efa278044275a8f9e39ae18bb59a3789c4d91aad96513624b204160c231de39.jpg", "img_caption": ["Figure 13: Sampled images from ExFM method, MNIST dataset. "], "img_footnote": [], "page_idx": 36}, {"type": "text", "text": "971 The E-statistic of the underlying null hypothesis is defined as follows: ", "page_idx": 36}, {"type": "equation", "text": "$$\nE_{n,m}(X,Y):=2A-B-C\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "972 H.5.2 2-Wasserstein distance (W2) ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "973 The 2-Wasserstein distance [14], also called the Earth mover\u2019s distance or the optimal transport   \n974 distance $W$ is a metric to describe the distance between two distributions, representing two different   \n975 subsets $A$ and $B$ . For continuous distributions, it is: ", "page_idx": 36}, {"type": "equation", "text": "$$\nW:=W(F_{A},F_{B})=\\left(\\int_{0}^{1}\\left|F_{A}^{-1}(u)-F_{B}^{-1}(u)\\right|^{2}d u\\right)^{\\frac{1}{2}},\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "976 where $F_{A}$ and $F_{B}$ are the corresponding cumulative distribution functions and $F_{A}^{-1}$ and $F_{B}^{-1}$ the   \n977 respective quantile functions. ", "page_idx": 36}, {"type": "text", "text": "978 H.5.3 Negative Log Likelihood (NLL) ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "979 To compute the NLL, we first sampled $N=5000$ samples $\\{x_{i}^{s}\\}_{i=1}^{N}$ from the target distribution. Then   \n980 we solved the following inverse flow ODE: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\displaystyle\\frac{\\partial x(t)}{\\partial t}=v_{\\theta}(x(t),t),}\\\\ {x(1)=x_{s}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "981 for $t$ from 1 to 0. For simplicity, changing time variable $\\tau=1-t$ we solve the following ODE: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\displaystyle{\\frac{\\partial x(\\tau)}{\\partial\\tau}=-v_{\\theta}(x(\\tau),1-\\tau),}}\\\\ {\\displaystyle{\\quad x(0)=x_{s}}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "faocrc $\\tau$ dfirnogm t o0 t thoe  s1t.a nTdhaursd  nwoer mobatl adiinsetrdi $N$ isoonl $\\{x_{i}^{0}\\}_{i=1}^{N}$ wwhe iccahl caurlea teex pNeLctLe ads to be distributed $\\mathcal{N}(\\boldsymbol{x}\\mid\\boldsymbol{0},\\dot{\\boldsymbol{I}})$ ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathrm{NLL}=-\\frac{1}{N}\\sum_{i=1}^{N}\\ln{\\mathcal{N}(x_{i}^{0}\\mid0,I)}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "982 H.5.4 Fr\u00e9chet inception distance (FID) ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "983 For images evaluation we take Fr\u00e9chet inception distance (FID) metrics, in particular the implementa  \n984 tion from [12]. The main idea of FID metrics is to measure the gap between two data distributions,   \n985 such as between a training set and samples from a trained model. After resizing the images, and   \n986 feature extraction, the mean $(\\mu,\\hat{\\mu})$ and covariance matrix $(\\Sigma,{\\hat{\\Sigma}})$ of the corresponding features are   \n987 used to compute FID:   \n988 $\\mathrm{FID}=||\\mu-\\hat{\\mu}||_{2}^{2}+\\mathrm{Tr}(\\Sigma+\\widehat{\\Sigma}-2(\\Sigma\\hat{\\Sigma})^{1/2}),$   \n989 where $T r$ is the trace of the matrix. ", "page_idx": 37}, {"type": "text", "text": "", "page_idx": 37}]