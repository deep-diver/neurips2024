[{"type": "text", "text": "Bandits with Ranking Feedback ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Davide Maran\u2217", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Francesco Bacchiocchi\u2217 ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Politecnico di Milano davide.maran@polimi.it ", "page_idx": 0}, {"type": "text", "text": "Politecnico di Milano francesco.bacchiocchi@polimi.it ", "page_idx": 0}, {"type": "text", "text": "Francesco Emanuele Stradi\u2217 Politecnico di Milano francescoemanuele.stradi@polimi.it ", "page_idx": 0}, {"type": "text", "text": "Matteo Castiglioni Politecnico di Milano matteo.castiglioni@polimi.it ", "page_idx": 0}, {"type": "text", "text": "Nicola Gatti Politecnico di Milano nicola.gatti@polimi.it ", "page_idx": 0}, {"type": "text", "text": "Marcello Restelli Politecnico di Milano marcello.restelli@polimi.it ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In this paper, we introduce a novel variation of multi-armed bandits called bandits with ranking feedback. Unlike traditional bandits, this variation provides feedback to the learner that allows them to rank the arms based on previous pulls, without quantifying numerically the difference in performance. This type of feedback is well-suited for scenarios where the arms\u2019 values cannot be precisely measured using metrics such as monetary scores, probabilities, or occurrences. Common examples include human preferences in matchmaking problems. Furthermore, its investigation answers the theoretical question on how numerical rewards are crucial in bandit settings. In particular, we study the problem of designing no-regret algorithms with ranking feedback both in the stochastic and adversarial settings. We show that, with stochastic rewards, differently from what happens with nonranking feedback, no algorithm can suffer a logarithmic regret in the time horizon $T$ in the instance-dependent case. Furthermore, we provide two algorithms. The first, namely DREE, guarantees a superlogarithmic regret in $T$ in the instance-dependent case thus matching our lower bound, while the second, namely R-LPE, guarantees a regret of $\\tilde{\\mathcal{O}}(\\sqrt{T})$ in the instance-independent case. Remarkably, we show that no algorithm can have an optimal regret bound in both instance-dependent and instance-independent cases. Finally, we prove that no algorithm can achieve a sublinear regret when the rewards are adversarial. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Multi-armed bandits are well-known sequential decision-making problems where a learner is given a number of arms whose reward is unknown [Lattimore and Szepesvari, 2017]. At every round, the learner can pull an arm and observe a realization of the reward associated with that arm, which can be generated stochastically [Auer et al., 2002] or adversarially [Auer et al., 1995]. The central question in multi-armed bandits concerns how to address the exploration/exploitation tradeoff to minimize the regret between the reward provided by the learning policy and the optimal clairvoyant algorithm. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this paper, we introduce a novel variation of multi-armed bandits that, to the best of our knowledge, is unexplored so far. We name the model as bandits with ranking feedback. This feedback provides the learner with a partial observation over the rewards given by the arms. More precisely, the learner can rank the arms based on the previous pulls they experienced, but they cannot quantify numerically the difference in performance. Thus, the learner is not allowed to asses how much an arm is better or worse than another. This type of feedback is well-suited for scenarios where the arms\u2019 values cannot be precisely measured using metrics such as monetary scores, probabilities, or occurrences, and naturally applies to various settings, e.g., when dealing with human preferences such as in matchmaking settings among humans and when the scores cannot be revealed for privacy or security reasons. This latter case can be found, e.g., in online advertising platforms offering automatic bidding services as they have no information on the actual revenue of the advertising campaigns since the advertisers prefer not to reveal these values being sensible data for the companies. Notice that a platform can observe the number of clicks received by an advertising campaign, but it cannot observe the revenue associated with that campaign. Remarkably, our model poses the interesting theoretical question whether the lack of numerical scores precludes the design of sublinear regret algorithms or worsens the regret bounds that are achievable when numerical scores are available. ", "page_idx": 1}, {"type": "text", "text": "Original contributions. We study the problem of designing no-regret algorithms for the bandits with ranking feedback problem, both in stochastic and adversarial settings. In the case of adversarial rewards, we prove that no algorithm can achieve sublinear regret. In contrast, with stochastic rewards, we show that the ranking feedback does not preclude such a possibility. In particular, in the instance-dependent case, we show that no algorithm can achieve logarithmic regret in the time horizon, and we provide an algorithm, namely DREE (Dynamical Ranking Exploration-Exploitation), guaranteeing a regret bound that matches this lower bound. In the instance-independent case, a crucial question is whether there exists an algorithm providing a better regret bound compared to the one achieved by the well-known Explore-then-Commit algorithm, which trivially guarantees an ${\\mathcal{O}}(T^{2/3})$ regret upper bound. We positively answer this question by designing an algorithm, namely R-LPE (Ranking Logarithmic Phased Elimination), which guarantees a regret of $\\widetilde{\\mathcal{O}}(\\sqrt{T})$ in the instance-independent case if the rewards are Gaussian. To achieve this result, we derive several non-standard results that allow us to discretize Brownian motions, which are of independent interest. These two different approaches leave open the problem of whether there exists an algorithm achieving optimal performance in both the instance-dependent and instance-independent cases. We negatively answer this question by showing that no algorithm can achieve an optimal regret bound in both cases, confirming the need to design two distinct algorithms for the two cases. Finally, we numerically evaluate our DREE and R-LPE algorithms in a testbed, and we compare their performance with some baselines from the literature in different settings. We show that our algorithms dramatically outperform the baselines in terms of empirical regret. ", "page_idx": 1}, {"type": "text", "text": "Related works. The field most related to bandits with ranking is preference learning, which aims at learning the preferences of one or more agents from some observations [F\u00fcrnkranz and H\u00fcllermeier, 2010]. Let us remark that preference learning has recently gained a lot of attention from the scientific community, as it enables the design of AI artifacts capable of interacting with human-in-the-loop (HTL) environments. Indeed, human feedback may be quite misleading when it is asked to report numerical values, while humans are far more effective at reporting ranking preferences. The preference learning literature mainly focuses on two kinds of preference observations: pairwise preferences and ranking. In the first case, the data observed by the learner involves preferences between two objects, i.e., a partial preference is given to the learner. In the latter, a complete ranking of the available data is given as feedback. Our work belongs to the latter branch. Preference learning has been widely investigated by the online learning community, see, e.g., [Bengs et al., 2018]. ", "page_idx": 1}, {"type": "text", "text": "Precisely, our work presents several similarities with the dueling bandits settings [Yue et al., 2012, Saha and Gaillard, 2022, Lekang and Lamperski, 2019], where, in each round, the learner pulls two arms and observes a ranking over them. Nevertheless, although dueling bandits share similarities to our setting, they present substantial differences. Specifically, in our model, the learner observes a ranking depending on the arms they have pulled so far. In dueling bandits, the learner observes an instantaneous comparison between the arms they have just pulled; thus, the outcome of such a comparison does not depend on the arms previously selected, as is the case of bandits with ranking feedback. As a consequence, while in bandits with ranking feedback the goal of the learner is to exploit the arm with the highest mean, in dueling bandits the goal of the learner is to select the arm winning with the highest probability. Furthermore, while we adopt the classical notion of regret used in the bandit literature to assess the theoretical properties of our algorithms, in dueling bandits, the algorithms are often evaluated with a suitable notion of regret, which differs from the classical one. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Dueling bandits have their reinforcement learning (RL) counterpart in the preference-based reinforcement learning (PbRL), see, e.g., [Novoseller et al., 2019] and [Wirth et al., 2017]. Interestingly, PbRL techniques differ from the standard RL approaches in that they allow an algorithm to learn from non-numerical rewards; this is particularly useful when the environment encompasses human-like entities [Chen et al., 2022]. Furthermore, preference-based reinforcement learning provides a bundle of results, ranging from theory [Xu et al., 2020] to practice [Christiano et al., 2017, Lee et al., 2021]. In PbRL, preferences may concern both states and actions; contrariwise, our framework is stateless since the rewards gained depend only on the action taken during the learning dynamic. Moreover, the differences outlined between dueling bandits and bandits with ranking feedback still hold for preference-based reinforcement learning, as preferences are considered between observations instead of the empirical mean of the accumulated rewards. ", "page_idx": 2}, {"type": "text", "text": "Paper structure. The paper is structured as follows. In Section 2, we report the problem formulation, the setting, and the necessary notation. In Section 3, we study the stochastic setting, that is, when the rewards are sampled from a fixed distribution. In Section 3.1, we present an instance-dependent regret lower bound that characterizes our problem. Thus, in Section 3.2, we propose our algorithm and we show that it achieves a tight instance-dependent regret bound. In Section 3.3, we show a trade-off between the regret upper bound achievable by any algorithm in the instance-dependent and instance-independent cases. Finally, in Section 3.4, we present an algorithm achieving an optimal instance-independent regret bound when the rewards are Gaussian. In Section 4, we study the adversarial setting, that is, when no statistical assumptions are made about the rewards. In this case, we present our impossibility result, showing that no algorithm can achieve sublinear regret. In Appendix A and Appendix B, we report the omitted proofs for the stochastic setting. In Appendix C, we report the omitted proof for the adversarial setting. Finally, in Appendix D, we report the empirical evaluation of our algorithms. ", "page_idx": 2}, {"type": "text", "text": "2 Problem formulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we formally state the model of bandits with ranking feedback and discuss the learnerenvironment interaction. Subsequently, we define policies and the notion of regret both in the stochastic and in the adversarial setting. ", "page_idx": 2}, {"type": "text", "text": "Setting and interaction. Differently from the classical version of the multi-armed bandit problem\u2014 see, e.g., [Lattimore and Szepesvari, 2017]\u2014in which the learner observes the reward associated with the pulled arm, in bandits with ranking feedback the learner can only observe a ranking over the arms based on the previous pulls. Formally, we assume the learner-environment interaction to unfold as follows.2 ", "page_idx": 2}, {"type": "text", "text": "(i) At every round $t\\in[T]$ , where $T$ is the time horizon, the learner chooses an arm $i_{t}\\in\\mathcal A:=$ $[n]$ , where $\\boldsymbol{\\mathcal{A}}$ is the set of available arms and $n=|\\mathcal{A}|<+\\infty$ .   \n(ii) We study both stochastic and adversarial settings. In the stochastic setting, the environment draws the reward $r_{t}(i_{t})$ associated with arm $i_{t}$ from a probability distribution $\\nu_{i_{t}}$ , i.e., $r_{t}(i_{t})\\;\\sim\\;\\nu_{i_{t}}$ , whereas, in the adversarial setting, $r_{t}(i_{t})$ is chosen adversarially by an opponent from a bounded set of reward functions.   \n(iii) There is a bandit feedback on the reward of the arm $i_{t}\\in\\mathcal{A}$ pulled at round $t$ leading to the ", "page_idx": 2}, {"type": "text", "text": "estimate of the empirical mean of $i_{t}$ as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\hat{r}_{t}(i):=\\frac{\\sum_{j\\in\\mathcal{W}_{t}(i)}r_{j}(i)}{Z_{i}(t)},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathcal{W}_{t}(i):=\\{\\tau\\in[t]\\ |\\ i_{\\tau}=i\\}$ and $Z_{i}(t):=|\\mathcal{W}_{t}(i)|$ .3 The learner observes the ranking over the empirical means $\\{\\hat{r}_{t}(i)\\}_{i\\in\\mathcal{A}}$ . Formally, we assume that the ranking $\\mathcal{R}_{t}\\,\\in\\,S_{A}$ observed by the learner at round $t$ is such that: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{r}_{t}(\\mathcal{R}_{t,i})\\geq\\hat{r}_{t}(\\mathcal{R}_{t,j})~\\forall t\\in[T]~\\forall i,j\\in[n]~\\mathrm{s.t.}~i\\geq j,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathscr{R}_{t,i}\\in\\mathscr{A}$ denotes the $i$ -th element in the ranking $\\mathcal{R}_{t}$ at round $t\\in[T]$ . Ties are broken in favor of the lower index. ", "page_idx": 3}, {"type": "text", "text": "For the sake of clarity, we provide an example to illustrate our setting and the corresponding learnerenvironment interaction. ", "page_idx": 3}, {"type": "text", "text": "Example. We consider an instance with two arms, i.e., $\\mathcal{A}=\\{1,2\\}$ , where the learner plays the first arm at rounds $t=1$ and $t=3$ , and the second arm at round $t=2$ , such that $\\mathcal{W}_{3}(1)\\bar{=}\\,\\mathrm{\\dot{\\{}}1,3\\}$ and $\\mathcal{W}_{3}(2)=\\{2\\}$ . Let $r_{1}(1)=1$ and $r_{3}(1)=5$ be the rewards obtained from playing the first arm at rounds $t=1$ and $t=3$ , respectively, and let $r_{2}(2)=5$ be the reward for playing the second arm at round $t=2$ . The empirical means of the two arms and the resulting rankings at each round $t\\in$ [3] are given by: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\hat{r}_{t}(1)=1,\\;\\hat{r}_{t}(2)=0\\;\\,\\mathcal{R}_{t}=\\langle1,2\\rangle\\quad t=1}\\\\ {\\hat{r}_{t}(1)=1,\\;\\hat{r}_{t}(2)=5\\;\\,\\mathcal{R}_{t}=\\langle2,1\\rangle\\quad t=2}\\\\ {\\hat{r}_{t}(1)=3,\\;\\hat{r}_{t}(2)=5\\;\\,\\mathcal{R}_{t}=\\langle2,1\\rangle\\quad t=3.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Policies and regret. At every round $t$ , the arm played by the learner is prescribed by a policy $\\pi$ . In both the stochastic and adversarial settings, we let the policy $\\pi$ be a randomized map from the history of the interaction $H_{t-1}=(\\mathcal{R}_{1},i_{1},\\mathcal{R}_{2},i_{2},\\ldots,\\mathcal{R}_{t-1},\\dot{i}_{t-1})$ to the set of all probability distributions with support $\\boldsymbol{\\mathcal{A}}$ . Formally, we let $\\pi:H_{t-1}\\to\\mathcal{P}\\left(A\\right)$ , for $t\\,\\in\\,[T]$ , such that $i_{t}\\sim\\pi(H_{t-1})$ . As is customary in stochastic bandits, the learner\u2019s goal is to design a policy $\\pi$ that minimizes the cumulative expected regret, which is formally defined as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\nR_{T}(\\pi)=\\mathbb{E}\\left[\\sum_{t=1}^{T}r_{t}(i^{*})-r_{t}(i_{t})\\right],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the expectation in the definition of regret is taken over the randomness of both the policy and the environment, and we define $i^{*}\\in\\arg\\operatorname*{max}_{i\\in A}\\mu_{i}$ with $\\mu_{i}:=\\mathbb{E}\\left[\\nu_{i}\\right]$ for each $i\\in[n]$ . In contrast, in the adversarial setting, the regret is defined as $\\begin{array}{r}{R_{T}(\\pi)\\,=\\,\\sum_{t=1}^{T}r_{t}(i^{*})\\,-\\,r_{t}(i_{t})}\\end{array}$ and we define $\\begin{array}{r}{i^{*}\\in\\arg\\operatorname*{max}_{i\\in\\mathcal{A}}\\sum_{t=1}^{T}r_{t}(i)}\\end{array}$ . Furthermore, from here on, we omit the dependence on the policy selected by the learner $\\pi$ in the regret formulation, referring to $R_{T}(\\pi)$ as $R_{T}$ whenever it is clear from the context. In the stochastic setting, we introduce the following additional notation. Given an arm $i\\in[n]$ , we let $\\Delta_{i}:=\\mu_{i^{*}}-\\mu_{i}$ represent the suboptimality gap of that arm. Furthermore, when $n=2$ , we simply refer to the suboptimality gap as $\\Delta$ . ", "page_idx": 3}, {"type": "text", "text": "As we will further discuss, the impossibility of observing the reward realizations raises several technical challenges when designing no-regret algorithms, as the approaches adopted for standard (non-ranking) bandits may be challenging to apply within our framework. In the following sections, we discuss how the lack of this information degrades the performance of the algorithms when the feedback is provided as a ranking. ", "page_idx": 3}, {"type": "text", "text": "3 Analysis in the stochastic setting ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "As a preliminary observation, we note that optimistic approaches, such as the UCB1 algorithm, are challenging to apply within our framework. This is because the learner lacks the information to estimate the expected rewards of the different arms, making it difficult to infer confidence bounds. Therefore, the most popular algorithm one can employ in bandits with ranking feedback is the explore-then-commit (EC) algorithm (see, e.g., Auer et al. [2002]), in which the learner explores the arms uniformly during the initial rounds and then commits to the one with the highest empirical mean. However, as we will also discuss in the following, such algorithm achieve suboptimal regret guarantees. Thus, in the rest of this section, we present two algorithms specifically designed to achieve optimal instance-dependent and instance-independent regret bounds. ", "page_idx": 3}, {"type": "text", "text": "3.1 Instance-dependent lower bound ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In the classical multi-armed bandit problem, it is well known that it is possible to achieve an instancedependent regret bound that is logarithmic in the time horizon $T$ . However, in this section, we show that such a result does not hold when the feedback is provided as a ranking. Our impossibility result leverages a connection between random walks and the cumulative rewards of the arms. Formally, we define an (asymmetric) random walk as follows. ", "page_idx": 4}, {"type": "text", "text": "Definition 1. A random walk is a stochastic process $\\{G_{t}\\}_{t\\in\\mathbb{N}}$ such that: ", "page_idx": 4}, {"type": "equation", "text": "$$\nG_{t}=\\left\\{{0\\atop G_{t-1}+\\epsilon_{t}}\\ \\ t=0\\atop{t\\geq1}}\\right.,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\{\\epsilon_{t}\\}_{t\\in\\mathbb{N}}$ is an i.i.d. sequence of integrable random variables, and $\\mathbb{E}[\\epsilon_{t}]=p$ is the drift of the random walk. ", "page_idx": 4}, {"type": "text", "text": "Before introducing our negative result, we introduce a lemma that characterizes the average performance of two random walks with different drifts. ", "page_idx": 4}, {"type": "text", "text": "Lemma 1 (Separation lemma). Let $\\{G_{t}\\}_{t\\in\\mathbb{N}}$ , $\\{G_{t}^{\\prime}\\}_{t\\in\\mathbb{N}}$ be two independent random walks defined as: ", "page_idx": 4}, {"type": "equation", "text": "$$\nG_{t+1}=G_{t}+\\epsilon_{t}\\qquad a n d\\qquad G_{t+1}^{\\prime}=G_{t}^{\\prime}+\\epsilon_{t}^{\\prime},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $G_{0}=G_{0}^{\\prime}=0$ and the drifts satisfy $\\mathbb{E}[\\epsilon_{t}]=p>q=\\mathbb{E}[\\epsilon_{t}^{\\prime}],$ , for each $t\\in\\mathbb{N}$ . Then, we have: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Big(\\forall t,t^{\\prime}\\in\\mathbb{N}^{*}\\;\\;G_{t}/t\\geq G_{t^{\\prime}}^{\\prime}/t^{\\prime}\\Big)\\geq c(p,q)>0.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We remark that the exact value of the constant $c(p,q)$ in Lemma 1 depends only on the two drifts $p$ and $q$ , as well as the probability distribution defining these drifts. In the simpler case of Bernoulli distributions, the constant $c(p,q)$ can be derived in closed form, as shown in Lemma 3 in the appendix. The rationale behind Lemma 1 is that, when considering two random walks with different drifts, there exists a separating line between them with strictly positive probability. Thus, with non-negligible probability, the empirical mean of the process with the higher drift always upper bounds the empirical mean of the process with the lower drift. ", "page_idx": 4}, {"type": "text", "text": "We also observe that the cumulative reward collected by a specific arm during the learning process can be represented as a random walk, whose drift corresponds to the expected reward associated with that arm. In the classical version of the multi-armed bandit problem, the learner can fully observe the evolution of this random walk, while in our setting, the learner can only observe a ranking of the different arms, making it impossible to quantify their performance numerically. Therefore, in bandits with ranking feedback, the only way for the learner to assess how close two arms are in terms of expected reward is by observing subsequent switches in their positions within the ranking. However, Lemma 1 implies that even if two arms have very close expected rewards, the learner may never observe a switch in the ranking, as the average mean of the arm with the higher expected reward may upper bound the second throughout the entire learning process. As a result, the following negative result holds. ", "page_idx": 4}, {"type": "text", "text": "Theorem 1 (Instance-dependent lower bound). Let $\\pi$ be any policy for the bandits with ranking feedback problem, then, for any $C:\\,[0,+\\infty)\\to[0,+\\infty)$ , there exists a $\\Delta>0$ and a time horizon $T>0$ such that $R_{T}>C(\\Delta)\\log(T)$ . ", "page_idx": 4}, {"type": "text", "text": "3.2 Instance-dependent upper bound ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We introduce the Dynamical Ranking Exploration-Exploitation algorithm (DREE). The pseudo-code is provided in Algorithm 1. As usual in bandit algorithms, in the first $n$ rounds, a pull for each arm is performed (Lines 2\u20133). At every subsequent round $t>n$ , the exploitation/exploration trade-off is addressed by playing the best arm according to the received feedback unless there is at least one arm whose number of pulls at $t$ is smaller than a superlogarithmic function $f(t):(0,\\infty)\\rightarrow\\mathbb{R}_{+}$ .4 More precisely, the algorithm plays an arm $i$ at round $t$ if it has been pulled less than $f(t)$ times (Lines 4\u20135), where ties due to multiple arms pulled less than $f(t)$ times are broken arbitrarily. Instead, if all arms have been pulled at least $f(t)$ times, the arm in the highest position of the last ranking feedback is pulled (Lines 6\u20138). Each round terminates with the learner receiving the updated ranking over the arms as feedback (Line 9). Let us observe that the exploration strategy of Algorithm 1 is deterministic, and the only source of randomness concerns the realization of the arms\u2019 rewards. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "We state the following result, providing the regret upper bound of Algorithm 1 as a function of $f$ . ", "page_idx": 5}, {"type": "text", "text": "Theorem 2 (Instance-dependent upper bound). Assume that the reward distribution of every arm is 1-subgaussian. Let $f:(0,\\infty)\\rightarrow\\mathbb{R}_{+}$ be a superlogarithmic nondecreasing function in $t$ . Then there is a term $C(f,\\Delta_{i})$ for each sub-optimal arm $i\\in[n]$ which does not depend on $T$ , such that Algorithm 1 satisfies $\\begin{array}{r}{R_{T}\\overset{}{\\leq}(1+f(T))\\sum_{i=1}^{n}\\Delta_{i}+\\log(\\dot{T})\\sum_{i=1}^{n}C(f,\\Delta_{i})}\\end{array}$ . ", "page_idx": 5}, {"type": "text", "text": "To minimize the asymptotic dependence on $T$ of the cumulative regret suffered by the algorithm, we can choose as an example $\\bar{f}(t)=\\log(t)^{1+\\delta}$ , where the parameter $\\delta>0$ is as small as possible. However, if $\\Delta_{i}<1$ , the minimization of $\\delta$ comes at the cost of increasing the term $C(f,\\Delta_{i})$ as it grows exponentially as $\\delta$ goes to zero. In particular, the terms $C(f,\\Delta_{i})$ are formally defined in the following corollary. ", "page_idx": 5}, {"type": "text", "text": "Corollary 1. Let $\\delta>0$ and $f(t)=\\log(t)^{1+\\delta}$ be the sperlogarithmic function used in Algorithm $^{\\,l}$ , then we have: ", "page_idx": 5}, {"type": "equation", "text": "$$\nC(f,\\Delta_{i})=\\frac{2\\Delta_{i}\\,\\left(e^{\\left(\\left(2/\\Delta_{i}^{2}\\right)^{1/\\delta}\\right)}+1\\right)}{1-e^{-\\Delta_{i}^{2}/2}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We remark that the term $C(f,\\Delta_{i})$ depends exponentially on $\\Delta_{i}$ , suggesting that $C(f,\\Delta_{i})$ may be large even when adopting values of $\\delta$ that are not arbitrarily close to zero. At this point, a natural question arises. Is such an exponential dependence with respect to the suboptimality gaps unavoidable, or is it a consequence of the kind of feedback the learner receives? In the next section, we answer this question. ", "page_idx": 5}, {"type": "text", "text": "Furthermore, let us observe that Algorithm 1 satisfies important properties. ", "page_idx": 5}, {"type": "table", "img_path": "aCaspFfAhG/tmp/a32548f62b5683045451e61d541fc6dfc7a788765855584c76aa7fba0a82adfb.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "More precisely, (i) it matches the instance-dependent regret lower-bound, since $f(\\cdot)$ can be chosen arbitrarily close to $\\log(t)$ , (ii) it works without requiring the knowledge of the time horizon $T$ , thus being an any-time algorithm. ", "page_idx": 5}, {"type": "text", "text": "3.3 Instance dependent/independent trade-off ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we provide a negative result, showing that no algorithm can achieve good performance in terms of both instance-dependent and instance-independent regret bounds, suggesting that the two cases need to be studied separately. Formally, the following theorem holds. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3 (Instance Dependent/Independent Trade-off). Let \u03c0 be any policy for the bandits with ranking feedback problem. If $\\pi$ satisfies the following properties: ", "page_idx": 5}, {"type": "text", "text": "\u2022 (instance-dependent regret upper bound) $\\begin{array}{r}{R_{T}\\leq\\sum_{i=1}^{n}C(\\Delta_{i})T^{\\alpha}}\\end{array}$ \u2022 (instance-independent regret upper bound) $R_{T}\\leq n C T^{\\beta}$ ", "page_idx": 5}, {"type": "text", "text": "then, $2\\alpha+\\beta\\geq1$ , where $\\alpha,\\beta\\ge0$ . ", "page_idx": 5}, {"type": "text", "text": "Proof. (Sketch) Let $p_{1}=0.5$ , $p_{2}=0.5-\\Delta$ , $p_{2}^{*}=0.5+\\Delta$ , for some $\\Delta>0$ specified in the proof. We consider three instances: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{P}:\\;\\left\\{\\begin{array}{l l}{\\nu_{1}=B e(p_{1})}&{\\qquad\\quad\\mathcal{P}^{*}:\\;\\left\\{\\begin{array}{l l}{\\nu_{1}=B e(p_{1})}&{\\qquad\\quad\\mathcal{P}^{**}:\\;\\left\\{\\begin{array}{l l}{\\nu_{1}=B e(1)}&{\\quad}\\\\ {\\nu_{2}=B e(p_{2}^{*})}&{\\quad}\\end{array}\\right.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Clearly, the first arm is optimal in instances $\\mathcal{P},\\mathcal{P}^{**}$ , while the second arm is optimal in $\\mathcal{P}^{*}$ . We then define the event: ", "page_idx": 6}, {"type": "equation", "text": "$$\nE_{t}=\\bigcap_{\\tau=1}^{t}\\{\\mathcal{R}_{t}=\\langle1,2\\rangle\\}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "In the first and in the third instances, the event $E_{t}$ corresponds to the optimal arm being ranked in the first position for the first $t$ time steps, while the opposite holds for the second instance. We observe the following two key points. (1) Since the event $E_{t}$ contains the realization of all the rankings up to time $t$ , no policy can distinguish between the three instances until this event holds. Therefore, we have: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathcal{P}}[Z_{2}(t)|E_{t}]=\\mathbb{E}_{\\mathcal{P}^{*}}[Z_{2}(t)|E_{t}]=\\mathbb{E}_{\\mathcal{P}^{**}}[Z_{2}(t)|E_{t}].\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "(2) In instance $\\mathcal{P}^{**}$ , the event $E_{t}$ happens almost surely for every $t$ . Therefore, to ensure that the policy $\\pi$ achieves an instance-dependent regret bounded by $C(1)T^{\\alpha}$ in this instance, we need $\\mathbb{E}[\\bar{Z_{2}}(T)\\bar{|}E_{T}]\\leq C T^{\\alpha}$ in all three instances. ", "page_idx": 6}, {"type": "text", "text": "The main question at this point reduces to: are $C T^{\\alpha}$ pulls of the last-ranking arm enough to distinguish between the two instances $\\mathcal{P}$ and $\\mathcal P^{*}\\mathcal{?}$ With a change of measure argument restricted to the first $C T^{\\alpha}$ pulls of the last-ranking arm, we are able to show that, for a sufficiently small value of $\\Delta>0$ , distinguishing between $\\mathcal{P},\\mathcal{P}^{*}$ is impossible with strictly positive probability. Then, we can prove that if the previous consideration holds, in the instance $\\mathcal{P}^{*}$ , we have: ", "page_idx": 6}, {"type": "equation", "text": "$$\nR_{T}\\geq\\Omega\\left(T^{1-2\\rho\\alpha}\\right),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "for a constant $\\rho>0$ close to one. This lower bound on the instance-independent regret entails that $\\beta\\geq1-2\\alpha$ , or, equivalently $\\beta+2\\alpha\\geq1$ . \u53e3 ", "page_idx": 6}, {"type": "text", "text": "From Theorem 3, we can easily infer the following impossibility result. ", "page_idx": 6}, {"type": "text", "text": "Corollary 2. There is no policy $\\pi$ for the bandits with ranking feedback problem achieving both subpolynomial regret in the instance-dependent case, i.e., for every $\\alpha>0$ , there exists a function $C(\\cdot)$ such that $\\begin{array}{r}{R_{T}^{}\\leq\\sum_{i=1}^{n}C(\\Delta_{i})T^{\\alpha}}\\end{array}$ , and sublinear regret in the instance-independent case. ", "page_idx": 6}, {"type": "text", "text": "To ease the interpretation of Corollary 2, in the following result, we discuss the instance-independent regret bound achieved by Algorithm 1. ", "page_idx": 6}, {"type": "text", "text": "Corollary 3. For every choice of $\\delta>0$ in $f(t)=\\log(t)^{1+\\delta}$ , there is no value of $\\eta>0$ for which Algorithm 1 achieves an instance-independent regret bound of the form $R_{T}\\leq\\mathcal{O}(T^{1-\\eta})$ . ", "page_idx": 6}, {"type": "text", "text": "The above result shows that Algorithm 1 suffers from linear instance-independent regret in $T$ , except for logarithmic terms. Moreover, the following corollary of Theorem 3 answers the question raised in the previous section. Indeed, we can prove that the unpleasant dependence on the suboptimality gaps $\\Delta_{i}$ is not a feature of Algorithm 1; instead, it cannot be avoided until the instance-dependent regret has a good order in $T$ . ", "page_idx": 6}, {"type": "text", "text": "Corollary 4. Let $\\pi$ be any policy for the bandits with ranking feedback problem that satisfies and instance-dependent regret upper bound of the form $\\begin{array}{r}{R_{T}\\leq\\dot{\\sum_{i=1}^{n}{C(\\Delta_{i})\\bar{f}(T)}}}\\end{array}$ , where $f(\\cdot)$ is $a$ subpolynomial function. Then, $C(\\Delta)$ is super-polynomial in $1/\\Delta$ . ", "page_idx": 6}, {"type": "text", "text": "Proof. We prove the opposite implication, namely that if $C(\\Delta)$ is polynomial in $1/\\Delta$ , then the instance-independent regret bound cannot be subpolynomial in $T$ . By assumption, in case of two arms with just one gap $\\Delta$ , we have: ", "page_idx": 6}, {"type": "equation", "text": "$$\nR_{T}\\leq\\sum_{\\ell=1}^{p}C_{\\ell}\\Delta^{-\\ell}f(T),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "which implies that the instance-independent regret can be bounded in the following way ", "page_idx": 6}, {"type": "equation", "text": "$$\nR_{T}\\leq\\operatorname*{sup}_{\\Delta>0}\\operatorname*{min}\\left\\{\\sum_{\\ell=1}^{p}C_{\\ell}\\Delta^{-\\ell}f(T),\\Delta T\\right\\}\\leq\\sum_{\\ell=1}^{p}\\operatorname*{sup}_{\\Delta>0}\\operatorname*{min}\\left\\{C_{\\ell}\\Delta^{-\\ell}f(T),\\Delta T\\right\\}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Notice that, for $\\Delta\\geq T^{-1/(\\ell+1)}$ the first term is less than $C T^{\\frac{\\ell}{\\alpha+1}}f(T)$ , while for $\\Delta\\leq T^{-1/(\\ell+1)}$ , the second one is less than $T^{\\frac{\\ell}{\\ell+1}}$ . Therefore, the full instance-independent regret is bounded by: ", "page_idx": 7}, {"type": "equation", "text": "$$\nR_{T}\\leq\\sum_{\\ell=1}^{p}C_{\\ell}T^{\\frac{\\ell}{\\ell+1}}f(T),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "which is polynomial in $T$ . If, by contradiction, $f(T)$ were subpolynomial, this bound would be sublinear in $T$ , but this contradicts the result of Corollary 2. \u53e3 ", "page_idx": 7}, {"type": "text", "text": "3.4 Instance-independent upper bound ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The impossibility result stated in Corollary 2 pushes for the need for an algorithm guaranteeing sublinear regret in the instance-independent case. Initially, we observe that the standard Explore-then-Commit (EC) algorithm [Auer et al., 2002] can be applied within our framework, achieving an $\\mathcal{O}(T^{2/3})$ instance-independent regret bound. In the following, we provide a brief overview of how the EC algorithm works. It divides the time horizon into two phases as follows: (i) exploration phase: the arms are pulled uniformly for the first $m\\cdot n$ rounds, ", "page_idx": 7}, {"type": "table", "img_path": "aCaspFfAhG/tmp/337e6529a9e13e17dacb55329ee0e97f5a7077cc0fd7d93520c56fd38e0e8eac.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "where $m$ is a parameter of the algorithm one can tune to minimize the regret; (ii) commitment phase: the arm maximizing the estimated reward is pulled. In the case of bandits with ranking feedback, the EC algorithm explores the arms in the first $m\\cdot n$ rounds and subsequently pulls the arm in the first position of the ranking feedback received at round $t=m\\cdot n$ . As is customary in standard (non-ranking) bandits, the best regret bound can be achieved by setting $m=\\lceil T^{2/3}\\rceil$ , thus obtaining an $\\mathcal{O}(T^{2/3})$ regret upper bound. We show that we can get a regret bound better than that of the EC algorithm. In particular, we provide the Ranking Logarithmic Phased Elimination (R-PLE) algorithm, which breaks the barrier of ${\\mathcal{O}}(T^{2/3})$ guaranteeing an $\\widetilde{\\mathcal{O}}(\\sqrt{T})$ regret bound when neglecting logarithmic terms. Due to the mathematical instruments involved, the proof of this regret bound only holds for the case of Gaussian rewards, as the ones presented in a similar setting by Garivier et al. [2016]. The pseudocode of R-PLE is reported in Algorithm 2. ", "page_idx": 7}, {"type": "text", "text": "In order to proper analyze the algorithm, we need to introduce the two following definitions. Initially, we introduce the definition of the loggrid set as follows. ", "page_idx": 7}, {"type": "text", "text": "Definition 2 (Loggrid). Given $a,b\\in\\mathbb{R}\\,s.t\\,a<b$ and a constant value $T>0$ , we define: ", "page_idx": 7}, {"type": "equation", "text": "$$\nL G(a,b,T):=\\left\\{\\lfloor T^{\\lambda_{j}b+(1-\\lambda_{j})a}\\rfloor:\\ \\lambda_{j}={\\frac{j}{\\lfloor\\log(T)\\rfloor}},\\ \\forall j=0,\\ldots,\\lfloor\\log(T)\\rfloor\\right\\}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Next, we give the notion of active set, which the algorithm employs to cancel out sub-optimal arms. ", "page_idx": 7}, {"type": "text", "text": "Definition 3 (Filtering condition). Let $S$ be the active set of the algorithm, at a certain timestep. We say that a timestep $t\\in[T]$ is fair if all active arms have been pulled the same number of times times. In any fair timestep $t\\in[T]$ , we define the active set $\\mathcal{F}_{t}(\\zeta)$ as the set of arms such that: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{F}_{t}(\\zeta):=\\left\\{i\\in S:\\forall j\\in S\\ \\sum_{\\tau=1\\mathrm{~s.t.~}\\tau\\ \\rho_{a i r}}^{t}\\{\\mathcal{R}_{\\tau}(i)>\\mathcal{R}_{\\tau}(j)\\}\\geq\\zeta\\right\\}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "This condition will be called filtering condition. ", "page_idx": 7}, {"type": "text", "text": "Initially, we observe that R-LPE differs from Algorithm 1, as it takes into account the entire history of the process, not just the most recent ranking $\\mathcal{R}_{t}$ . It also requires knowledge of $T$ . We denote with $S$ the set of active arms used by the algorithm. Initially, the set $S$ comprises all the possible arms available in the problem (Line 1). Furthermore, the set which drives the update of the decision space $S$ , namely $\\mathcal{L}$ , is initialized as the loggrid built on parameters $1/2,1,{\\bar{T}}$ (Line 2). At every round $t\\in[T]$ , R-LPE chooses the arm from the active set $S$ with the minimum number of pulls, namely $i\\in[n]$ s.t. $Z_{i}(t)$ is minimized (Line 4); ties are broken by index order. Next, the number of times arm $i_{t}$ has been pulled, namely $Z_{i_{t}}(t)$ , is updated accordingly (Line 5). The peculiarity of the algorithm is that the set $S$ changes every time the condition $\\operatorname*{min}_{i}$ $Z_{i}(t)\\in{\\mathcal{L}}$ is satisfied (Line 7). When the aforementioned condition is met, the set of active arms $S$ is flitered to avoid the exploration on sub-optimal arms. Precisely, $S$ is flitered given the time dependent parameter $\\alpha$ (Lines 8- 9). We state the following theorem providing an instance-independent regret bound to Algorithm 2. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Theorem 4. In the stochastic bandits with ranking feedback setting, when the noise is Gaussian, Algorithm 2 achieves $R_{T}\\leq62n^{4}\\log(T)^{2}T^{1/2}$ . ", "page_idx": 8}, {"type": "text", "text": "Proof. (Sketch) To prove the theorem, we define, for every pair of indices $i,j\\in[n]$ , the event: ", "page_idx": 8}, {"type": "equation", "text": "$$\nE_{i j}^{\\psi}:=\\left\\{{\\cal E}_{i j}\\quad\\mu_{j}-\\mu_{i}>\\psi\\right.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $E_{i j}$ corresponds to the event in which arm $i$ eliminates arm $j$ at some point in the process, while $\\psi$ is a constant defined in the following. The probability that at least one of these events holds is bounded (by Lemma 6 and employing a union bound) as $\\mathbb{P}(\\Psi):=\\mathbb{P}\\left(\\bigcup_{i\\neq j,i,i\\in[n]}^{n}E_{i j}^{\\psi}\\right)\\leq$ $\\mathcal{O}\\left(n^{2}\\log(T)^{2}T^{-1/2}\\psi^{-1}\\right)$ , since their number is at most $n(n-1)/2$ . ", "page_idx": 8}, {"type": "text", "text": "Therefore, if the complement of the event $\\Psi$ holds, there exists an arm $i^{\\star}$ with a gap (w.r.t. the first arm) less than $(n-1)\\psi$ , which is not eliminated until the last round. This is because at most $n-1$ eliminations can happen, and if the complement of the event $\\Psi$ holds, such eliminations concern pairs of arms with a difference in mean of at most $\\psi$ . ", "page_idx": 8}, {"type": "text", "text": "Since the arm $i^{\\star}\\in[n]$ is not eliminated until the last round under the event $\\Psi^{C}$ , the probability of event $E_{i i^{\\star}}^{*}$ , corresponding to the event in which the suboptimal arm $i\\in[n]$ survives for more than $\\mathcal{O}(\\log(T)T^{1/2}\\Delta_{i i^{\\star}}^{-1})$ pulls, is bounded by $2T^{-1/2}$ thanks to Lemma 7. As a result, employing a union bound over all possible values of $i^{\\star}$ , we can say that the probability that any event $E_{i i^{\\star}}^{*}$ with $i^{\\star}\\in[n]$ occuring is at most $2(n-1)T^{-1/2}$ . Thus, fixing $\\psi=\\Delta_{i}/(2(n-1))$ ensures that $\\Delta_{i i^{\\star}}^{-1}\\geq\\Delta_{i}/n$ , which entails $\\mathbb{P}(Z_{i}(T)\\ge\\mathcal{O}(\\log(T)T^{1/2}\\Delta_{i}^{-1}))\\le\\mathcal{O}(n^{3}\\log(T)^{2}T^{-1/2}\\Delta_{i}^{-1}))$ , and thus, it holds $\\mathbb{E}[\\Delta_{i}Z_{i}(T)]=O(n^{3}\\log(T)^{2}T^{-1/2})$ . Finally, using the Regret Decomposition Lemma [Lattimore and Szepesvari, 2017] we can conclude the proof. Proving the two lemmas, however, is not trivial since it requires to see the whole process as a discretization of a biased Brownian motion, and then applying results for this kind of stochastic processes. \u53e3 ", "page_idx": 8}, {"type": "text", "text": "At first glance, the result presented in Theorem 4 may seem unsurprising. Indeed, there are several elimination algorithms achieving $\\mathcal{O}(\\sqrt{T})$ regret bounds in different bandit settings (see, for example, [Auer and Ortner, 2010, Lattimore et al., 2020, Li and Scarlett, 2022]). Nevertheless, our setting poses several additional challenges compared to existing ones. For instance, in our framework, it is not possible to rely on concentration bounds, as the current feedback is heavily correlated with the past ones. This is precisely the reason for the anomalous growth of the regret in terms of the number of arms $n$ : due to the extremely correlated feedback, two union bounds are necessary trough the last proof, and this leads to an increase in the dependence of the order of $n$ . In the impossibility of using concentration inequalities, our analysis employs novel arguments, drawing from recent results in the theory of Brownian Motions, which allow to properly model the particular ranking feedback. ", "page_idx": 8}, {"type": "text", "text": "4 Analysis in the adversarial setting ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We focus on bandits with ranking feedback in adversarial settings. In particular, we show that no algorithm provides sublinear regret without statistical assumptions on the rewards. ", "page_idx": 8}, {"type": "text", "text": "Theorem 5. In adversarial bandits with ranking feedback, there exists a constant $\\gamma\\in(0,1)$ such that no algorithm achieves $o(T)$ regret with respect to the best arm in hindsight with probability greater than $\\gamma$ . ", "page_idx": 8}, {"type": "text", "text": "Proof. (Sketch) The proof introduces three instances in an adversarial setting in a way that no algorithm can achieve sublinear regret in all three. The main reason behind such a negative result is that ranking feedback obfuscates the value of the rewards so as not to allow the algorithm to distinguish two or more instances where the rewards are non-stationary. The three instances employed in the proof are divided into three phases such that the instances are similar in terms of rewards for the first two phases, while they are extremely different in the third phase. In summary, if the learner receives the same ranking when playing in two instances with different best arms in hindsight, it is not possible to achieve a small regret in both of them. \u53e3 ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper is supported by the FAIR (Future Artificial Intelligence Research) project, funded by the NextGenerationEU program within the PNRR-PE-AI scheme (M4C2, Investment 1.3, Line on Artificial Intelligence), by the EU Horizon project ELIAS (European Lighthouse of AI for Sustainability, No. 101120237) and by project SERICS (PE00000014) under the NRRP MUR program funded by the EU - NGEU. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "P. Auer, N. Cesa-Bianchi, Y. Freund, and R.E. Schapire. Gambling in a rigged casino: The adversarial multi-armed bandit problem. In Proceedings of IEEE 36th Annual Foundations of Computer Science, pages 322\u2013331, 1995. doi: 10.1109/SFCS.1995.492488.   \nPeter Auer and Ronald Ortner. Ucb revisited: Improved regret bounds for the stochastic multi-armed bandit problem. Periodica Mathematica Hungarica, 61(1-2):55\u201365, 2010.   \nPeter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem. Machine learning, 47(2):235\u2013256, 2002.   \nPaolo Baldi. Stochastic calculus. In Stochastic Calculus, pages 215\u2013254. Springer, 2017.   \nViktor Bengs, Robert Busa-Fekete, Adil El Mesaoudi-Paul, and Eyke H\u00fcllermeier. Preference-based online learning with dueling bandits: A survey. 2018. doi: 10.48550/ARXIV.1807.11398. URL https://arxiv.org/abs/1807.11398.   \nXiaoyu Chen, Han Zhong, Zhuoran Yang, Zhaoran Wang, and Liwei Wang. Human-in-the-loop: Provably efficient preference-based reinforcement learning with general function approximation. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 3773\u20133793. PMLR, 17\u201323 Jul 2022. URL https://proceedings.mlr.press/v162/chen22ag.html.   \nPaul Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences, 2017. URL https://arxiv.org/abs/1706. 03741.   \nJohannes F\u00fcrnkranz and Eyke H\u00fcllermeier, editors. Preference Learning. Springer, 2010. ISBN 978-3-642-14124-9. doi: 10.1007/978-3-642-14125-6. URL https://doi.org/10.1007/ 978-3-642-14125-6.   \nAur\u00e9lien Garivier, Tor Lattimore, and Emilie Kaufmann. On explore-then-commit strategies. Advances in Neural Information Processing Systems, 29, 2016.   \nTor Lattimore and Csaba Szepesvari. Bandit algorithms. 2017. URL https://tor-lattimore. com/downloads/book/book.pdf.   \nTor Lattimore, Csaba Szepesvari, and Gellert Weisz. Learning with good feature representations in bandits and in rl with a generative model. In International Conference on Machine Learning, pages 5662\u20135670. PMLR, 2020.   \nKimin Lee, Laura Smith, Anca Dragan, and Pieter Abbeel. B-pref: Benchmarking preference-based reinforcement learning, 2021. URL https://arxiv.org/abs/2111.03026.   \nTyler Lekang and Andrew Lamperski. Simple algorithms for dueling bandits, 2019. URL https: //arxiv.org/abs/1906.07611.   \nZihan Li and Jonathan Scarlett. Gaussian process bandit optimization with few batches. In International Conference on Artificial Intelligence and Statistics, pages 92\u2013107. PMLR, 2022.   \nEllen R. Novoseller, Yibing Wei, Yanan Sui, Yisong Yue, and Joel W. Burdick. Dueling posterior sampling for preference-based reinforcement learning, 2019. URL https://arxiv.org/abs/ 1908.01289.   \nAadirupa Saha and Pierre Gaillard. Versatile dueling bandits: Best-of-both-world analyses for online learning from preferences, 2022. URL https://arxiv.org/abs/2202.06694.   \nLajos Tak\u00e1cs. On a generalization of the arc-sine law. The Annals of Applied Probability, 6(3): 1035\u20131040, 1996.   \nChristian Wirth, Riad Akrour, Gerhard Neumann, and Johannes F\u00fcrnkranz. A survey of preferencebased reinforcement learning methods. J. Mach. Learn. Res., 18(1):4945\u20134990, jan 2017. ISSN 1532-4435.   \nYichong Xu, Ruosong Wang, Lin F. Yang, Aarti Singh, and Artur Dubrawski. Preference-based reinforcement learning with finite-time guarantees, 2020. URL https://arxiv.org/abs/2006. 08910.   \nYisong Yue, Josef Broder, Robert Kleinberg, and Thorsten Joachims. The k-armed dueling bandits problem. Journal of Computer and System Sciences, 78(5):1538\u20131556, 2012. ISSN 0022- 0000. doi: https://doi.org/10.1016/j.jcss.2011.12.028. URL https://www.sciencedirect. com/science/article/pii/S0022000012000281. JCSS Special Issue: Cloud Computing 2011. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "A Proofs of instance dependent stochastic analysis ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "A.1 Proof of instance dependent lower bound and lemmas ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "Lemma 1 (Separation lemma). Let $\\{G_{t}\\}_{t\\in\\mathbb{N}}$ , $\\{G_{t}^{\\prime}\\}_{t\\in\\mathbb{N}}$ be two independent random walks defined as: ", "page_idx": 11}, {"type": "equation", "text": "$$\nG_{t+1}=G_{t}+\\epsilon_{t}\\qquad a n d\\qquad G_{t+1}^{\\prime}=G_{t}^{\\prime}+\\epsilon_{t}^{\\prime},\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "where $G_{0}=G_{0}^{\\prime}=0$ and the drifts satisfy $\\mathbb{E}[\\epsilon_{t}]=p>q=\\mathbb{E}[\\epsilon_{t}^{\\prime}],$ , for each $t\\in\\mathbb{N}$ . Then, we have: ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Big(\\forall t,t^{\\prime}\\in\\mathbb{N}^{*}\\;\\;G_{t}/t\\geq G_{t^{\\prime}}^{\\prime}/t^{\\prime}\\Big)\\geq c(p,q)>0.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "Proof. Let us consider the random walk defined as follows: ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\widetilde{G}_{t+1}=\\widetilde{G}_{t}+\\epsilon_{t}-\\frac{p+q}{2}.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "Since $\\mathbb{E}\\left[\\epsilon_{t}-\\frac{p+q}{2}\\right]>0$ , and observing that a random walk with a non null drift is transient, we know that there exists a constant $c_{1}(p,q)>0$ such that: ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\widetilde{G}_{t}>0,\\,\\forall t>0\\right)=c_{1}(p,q)>0.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "On the other hand, we observe that the random walk: ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\widetilde{G}_{t+1}^{\\prime}=\\widetilde{G}_{t}^{\\prime}+\\epsilon_{t}^{\\prime}-\\frac{p+q}{2}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "satisfies the opposite inequality since $\\mathbb{E}[\\epsilon_{t}^{\\prime}-\\frac{p+q}{2}]<0$ . Therefore, for the same reason as above, we have: ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\widetilde{G}_{t}^{\\prime}<0,\\,\\forall t>0\\right)=c_{2}(p,q)>0,\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "for some constant $c_{2}(p,q)>0$ . ", "page_idx": 11}, {"type": "text", "text": "Thus, since the two processes are independent, we have: ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Big(\\bigcap_{t,t^{\\prime}=1}^{\\infty}\\{\\widetilde{G}_{t+1}>0,\\widetilde{G}_{t^{\\prime}+1}^{\\prime}<0\\}\\Big)>0,\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "which entails: ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle c_{1}(p,q)c_{2}(p,q)=\\mathbb{P}\\Big(\\displaystyle\\prod_{t,t^{\\prime}=1}^{\\infty}\\{\\widetilde{G}_{t}>0,\\widetilde{G}_{t^{\\prime}}^{\\prime}<0\\}\\Big)}\\\\ {\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\mathrm{~}}\\\\ {\\displaystyle=\\mathbb{P}\\Big(\\displaystyle\\prod_{t,t^{\\prime}=1}^{\\infty}\\,\\{G_{t}-t\\frac{p+q}{2}>0,G_{t^{\\prime}}^{\\prime}-t^{\\prime}\\frac{p+q}{2}<0\\}\\Big)}\\\\ {\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\mathbb{P}\\Big(\\displaystyle\\prod_{t,t^{\\prime}=1}^{\\infty}\\,\\{G_{t}/t>\\frac{p+q}{2},G_{t^{\\prime}}^{\\prime}/t^{\\prime}<\\frac{p+q}{2}\\}\\Big)}\\\\ {\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\le\\mathbb{P}\\Big(\\displaystyle\\prod_{t,t^{\\prime}=1}^{\\infty}\\,\\{G_{t}/t>G_{t^{\\prime}}^{\\prime}/t^{\\prime}\\}\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "The latter inequality can be easily reformulated as in the statement of the lemma, concluding the proof. \u53e3 ", "page_idx": 11}, {"type": "text", "text": "In order to prove the lower bound, we will also show the following lemma. ", "page_idx": 11}, {"type": "text", "text": "Lemma 2. Let $\\{X_{i}\\}_{i\\in[n]}$ be a sequence of i.i.d. Bernoulli random variables. Then, for every event $E\\in\\mathcal{F}_{n}$ , where ${\\mathcal{F}}_{n}$ is the filtration generated by $X_{1},\\ldots X_{n}$ , we have: ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\mathbb{P}_{X_{1},\\ldots X_{n}\\sim B e(p)}(E)\\leq\\mathbb{P}_{X_{1},\\ldots X_{n}\\sim B e(p^{\\prime})}(E)\\operatorname*{max}\\left({\\frac{p}{p^{\\prime}}},{\\frac{1-p}{1-p^{\\prime}}}\\right)^{n}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "Proof. Without loss of generality, let us assume that $p^{\\prime}<p$ . Then, we have: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{P}_{X_{1},\\ldots,X_{n}\\sim B e(p)}(E)=\\int_{\\{0,1\\}^{n}}\\mathbf{1}_{E}(\\underline{{x}})\\,\\prod_{i=1}^{n}p^{x_{i}}(1-p)^{1-x_{i}}d\\underline{{x}}}}\\\\ &{}&{\\le\\int_{\\{0,1\\}^{n}}\\mathbf{1}_{E}(\\underline{{x}})\\,\\prod_{i=1}^{n}(p/p^{\\prime})^{x_{i}}p^{x_{i}}(1-p^{\\prime})^{1-x_{i}}d\\underline{{x}}}\\\\ &{}&{\\le\\Big(\\frac{p}{p^{\\prime}}\\Big)^{n}\\int_{\\{0,1\\}^{n}}\\mathbf{1}_{E}(\\underline{{x}})\\,\\prod_{i=1}^{n}p^{x_{i}}(1-p^{\\prime})^{1-x_{i}}d\\underline{{x}}}\\\\ &{}&{=\\Big(\\frac{p}{p^{\\prime}}\\Big)^{n}\\mathbb{P}_{X_{1},\\ldots,X_{n}\\sim B e(p^{\\prime})}(E).}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where Equation (2) follows from the assumption that $p^{\\prime}<p$ . The other way round can be proved substituting $p$ and $p^{\\prime}$ . This concludes the proof. \u53e3 ", "page_idx": 12}, {"type": "text", "text": "Theorem 1 (Instance-dependent lower bound). Let $\\pi$ be any policy for the bandits with ranking feedback problem, then, for any $C:[0,+\\infty)\\to[0,+\\infty),$ , there exists a $\\Delta>0$ and a time horizon $T>0$ such that $R_{T}>C(\\Delta)\\log(T)$ . ", "page_idx": 12}, {"type": "text", "text": "Proof. Let $p_{1}=0.5$ , $p_{2}=0.5-\\Delta$ , $p_{2}^{*}=0.5+\\Delta$ , for some $0<\\Delta\\le1/4$ (independent on $T$ ) that satisfies: ", "page_idx": 12}, {"type": "equation", "text": "$$\n2C(1)\\log(1-4\\Delta)\\geq-1/2.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "We consider the following three instances: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathcal{P}:\\;\\left\\{\\begin{array}{l l}{\\nu_{1}=B e(p_{1})}&{\\qquad\\mathcal{P}^{*}:\\;\\left\\{\\begin{array}{l l}{\\nu_{1}=B e(p_{1})}&{\\qquad\\mathcal{P}^{**}:\\;\\left\\{\\begin{array}{l l}{\\nu_{1}=B e(1)}&{\\qquad\\mathcal{P}^{**}:\\;\\left\\{\\begin{array}{l l}{\\nu_{1}=B e(1)}&{\\qquad\\mathcal{P}^{**}:\\;\\left\\{\\begin{array}{l l}{\\nu_{1}=B e(1)}&{\\qquad\\mathcal{P}^{**}:\\;\\left\\{\\begin{array}{l l}{\\nu_{1}=B e(1)}&{\\qquad\\mathcal{P}^{**}:\\;\\left\\{\\begin{array}{l l}{\\nu_{1}=B e(1)}&{\\qquad\\mathcal{P}^{**}:\\;\\left\\{\\begin{array}{l l}{\\nu_{1}=B e(1)}&{\\qquad\\mathcal{P}^{**}:\\;\\left\\{\\begin{array}{l l}{\\nu_{1}=B e(1)}&{\\qquad\\mathcal{P}^{**}:\\;\\left\\nu_{1}=B e(1)}\\\\ {\\nu_{2}=B e(2)}&&{\\qquad\\mathcal{P}^{**}:\\;\\left\\nu_{1}=B e(0)}\\end{array}\\right.\\right.}}}\\end{array}\\right.\\right.}}}\\end{array}\\right.\\right.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Clearly, the optimal arm is the first in instances $\\mathcal{P}$ and $\\mathcal{P}^{**}$ , while the optimal arm is the second in instance $\\mathcal{P}^{*}$ . By contradiction, we assume that, for every time horizon $T>0$ , exists a policy $\\pi_{T}(\\cdot|H_{t})$ for each $t<T$ satisfying, in all three cases, the following condition: ", "page_idx": 12}, {"type": "equation", "text": "$$\nR_{T}(\\pi_{T})\\leq C(\\Delta)\\log(T).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "We also define the following event about the rankings received by the learner: ", "page_idx": 12}, {"type": "equation", "text": "$$\nE_{t}=\\bigcap_{\\tau=1}^{t}\\{\\mathcal{R}_{\\tau}=\\langle1,2\\rangle\\}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "The event $E_{t}$ can be interpreted as \"up to time $t$ , the learner has always observed the ranking $\\langle1,2\\rangle\"$ . Note that by applying Equation 3 to the particular case of instance $\\mathcal{P}^{**}$ , we have: ", "page_idx": 12}, {"type": "equation", "text": "$$\nC(1)\\log(T)\\geq R_{T}(\\pi_{T}|\\mathcal{P}^{**})=\\mathbb{E}_{\\mathcal{P}^{**}}[Z_{2}(T)]=\\mathbb{E}_{\\mathcal{P}^{**}}[Z_{2}(T)|E_{T}],\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where the last step holds since in instance $\\mathcal{P}^{**}$ the event $E_{T}$ holds almost surely. Therefore, we have: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathcal{P}^{**}}[Z_{2}(T)|E_{T}]\\le C(1)\\log(T).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Let $h\\in\\mathbb N$ . Then,we have: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}_{\\mathcal{P}}(Z_{2}(T)<h|E_{T})=\\mathbb{P}_{\\mathcal{P}^{**}}(Z_{2}(T)<h|E_{T})}\\\\ &{\\qquad\\qquad\\qquad\\geq1-\\displaystyle\\frac{C(1)\\log(T)}{h}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "We note that the above equality holds since, under the event $E_{T}$ , no policy can distinguish between the two instances $\\mathcal{P}^{**}$ and $\\mathcal{P}$ . On the other hand, the above inequality follows from Markov\u2019s theorem. ", "page_idx": 12}, {"type": "text", "text": "Furthermore, we notice that the event $\\{Z_{2}(T)<h\\}$ is contained in the $\\sigma-$ algebra generated by the first $h$ pulls of arm 2 (and all the pulls of arm 1, but this is irrelevant since $\\nu_{1}$ corresponds to the same distribution in the first two instances). Therefore, for all $h>0$ , from Lemma 2 we have: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\mathcal{P}^{*}}(Z_{2}(T)<h)\\ge\\Big(\\frac{0.5-\\Delta}{0.5+\\Delta}\\Big)^{h}\\mathbb{P}_{\\mathcal{P}}(Z_{2}(T)<h)\n$$", "text_format": "latex", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\geq\\left(\\frac{0.5-\\Delta}{0.5+\\Delta}\\right)^{h}\\mathbb{P}_{\\mathcal{P}}(Z_{2}(T)<h,E_{T})}\\\\ &{=\\left(\\frac{0.5-\\Delta}{0.5+\\Delta}\\right)^{h}\\mathbb{P}_{\\mathcal{P}}(Z_{2}(T)<h|E_{T})\\mathbb{P}_{\\mathcal{P}}(E_{T})}\\\\ &{\\geq\\left(\\frac{0.5-\\Delta}{0.5+\\Delta}\\right)^{h}\\left(1-\\frac{C(1)\\log(T)}{h}\\right)\\kappa(\\Delta)}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Where $\\kappa(\\Delta):=\\operatorname*{inf}_{t>0}\\mathbb{P}_{P}(E_{t})>0$ thanks to Lemma 1. ", "page_idx": 13}, {"type": "text", "text": "Here, for every $1/4\\geq x>0$ , the inequality $0.5-x\\geq(1-4x)(0.5+x)\\geq0$ holds. Thus, for all $h>0$ , we have: ", "page_idx": 13}, {"type": "equation", "text": "$$\n{\\left(\\frac{0.5-\\Delta}{0.5+\\Delta}\\right)}^{h}\\geq{\\left(1-4\\Delta\\right)}^{h}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Therefore, taking $h=2C(1)\\log(T)$ , we have: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}_{\\mathcal{P}^{*}}\\!\\left(Z_{2}(T)<h\\right)\\geq\\left(\\frac{0.5-\\Delta}{0.5+\\Delta}\\right)^{h}\\left(1-\\frac{C(1)\\log(T)}{h}\\right)\\kappa(\\Delta)}\\\\ &{\\qquad\\qquad\\qquad\\geq\\frac{\\kappa(\\Delta)}{2}\\Big(1-4\\Delta\\Big)^{2C(1)\\log(T)}}\\\\ &{\\qquad\\qquad\\qquad=\\frac{\\kappa(\\Delta)}{2}T^{2C(1)\\log(1-4\\Delta)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Thus, we can lower bound on the regret in case of instance $\\mathcal{P}^{*}$ as follows: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R_{T}(\\pi_{T}|\\mathcal{P}^{*})\\geq\\Delta\\mathbb{E}_{\\mathcal{P}^{*}}[(T-Z_{2}(T))]}\\\\ &{\\leq\\Delta(T-h)\\mathbb{P}_{\\mathcal{P}^{*}}(Z_{2}(T)<h)}\\\\ &{=\\Delta(T-2C(1)\\log(T))\\mathbb{P}_{\\mathcal{P}^{*}}(Z_{2}(T)<2C(1)\\log(T))}\\\\ &{\\geq\\frac{\\kappa(\\Delta)\\Delta(T-2C(1)\\log(T))}{2}T^{2C(1)\\log(1-4\\Delta)}}\\\\ &{\\geq\\frac{\\kappa(\\Delta)\\Delta(T-2C(1)\\log(T))}{2}T^{-1/2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "which grows polynomially with time. At this point, Equation (3) would give, for every $T>0$ , the following: ", "page_idx": 13}, {"type": "equation", "text": "$$\nC(\\Delta)\\log(T)\\geq\\frac{\\kappa(\\Delta)\\Delta(T-2C(1)\\log(T))}{2}T^{-1/2}\\geq\\Omega\\left(\\frac{\\kappa(\\Delta)\\Delta}{2}T^{1/2}\\right).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Clearly, for $T$ sufficiently big, the above statement is false, concluding the proof. ", "page_idx": 13}, {"type": "text", "text": "A.2 Proof of instance dependent upper bound ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Theorem 2 (Instance-dependent upper bound). Assume that the reward distribution of every arm is 1-subgaussian. Let $f:(0,\\infty)\\rightarrow\\mathbb{R}_{+}$ be a superlogarithmic nondecreasing function in $t$ . Then there is a term $C(f,\\Delta_{i})$ for each sub-optimal arm $i\\in[n]$ which does not depend on $T$ , such that Algorithm $^{\\,l}$ satisfies $\\begin{array}{r}{R_{T}\\overset{.}{\\leq}(1+f(T))\\sum_{i=1}^{n}\\Delta_{i}+\\log(\\dot{T})\\sum_{i=1}^{n}C(f,\\Delta_{i}).}\\end{array}$ . ", "page_idx": 13}, {"type": "text", "text": "Proof. Let $i^{*}\\in[n]$ be the optimal arm. For any suboptimal arm $i\\in[n]$ , we have: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}[Z_{i}(T)]=\\sum_{\\tau=1}^{T}\\mathbb{P}(i_{\\tau}=i)}\\\\ {\\displaystyle\\leq1+\\sum_{\\tau=n+1}^{T}\\mathbb{P}(i_{\\tau}=i,Z_{i}(\\tau-1)<f(\\tau))+\\sum_{\\tau=n+1}^{T}\\mathbb{P}(i_{\\tau}=i,Z_{i}(\\tau-1)\\geq f(\\tau)),}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where we let $i_{\\tau}\\in[n]$ be the arm pulled at time $\\tau\\in[T]$ . We split the proof in two parts, providing a bound for each term defining Equation (4). ", "page_idx": 13}, {"type": "text", "text": "Claim 1: The first term of Equation (4) is bounded by $f(T)$ . Indeed, notice that, being $f(\\cdot)$ nondecreasing, we have: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{\\tau=1}^{T}\\mathbf{1}\\{i_{\\tau}=i,Z_{i}(\\tau-1)<f(\\tau)\\}\\le\\sum_{\\tau=1}^{T}\\sum_{\\rho=1}^{f(\\tau)}\\mathbf{1}\\{i_{\\tau}=i,Z_{i}(\\tau-1)=\\rho\\}}}\\\\ &{}&{\\le\\displaystyle\\sum_{\\tau=1}^{T}\\sum_{\\rho=1}^{f(T)}\\mathbf{1}\\{i_{\\tau}=i,Z_{i}(\\tau-1)=\\rho\\}}\\\\ &{}&{=\\displaystyle\\sum_{\\rho=1}^{f(T)}\\sum_{\\tau=1}^{T}\\mathbf{1}\\{i_{\\tau}=i,Z_{i}(\\tau-1)=\\rho\\}\\le f(T).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Claim 2: The second term in Equation (4) is bounded by $C(\\Delta_{i})\\log(T)$ for some $C(\\Delta_{i})$ . We know, by design of the algorithm, that the arm $i\\in[n]$ can be pulled only if: ", "page_idx": 14}, {"type": "text", "text": "1. It has the highest empirical mean. ", "page_idx": 14}, {"type": "text", "text": "2. Every other arm has been pulled at least $f(T)$ times, including arm $i^{*}\\in[n]$ . ", "page_idx": 14}, {"type": "text", "text": "We define the event: ", "page_idx": 14}, {"type": "equation", "text": "$$\nE_{i,t}:=\\{Z_{i}(t)\\geq f(t)\\}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Then, we have: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{P}(i_{\\tau}=i,Z_{i}(\\tau-1)\\geq f(\\tau))\\leq\\mathbb{P}(\\hat{r}_{\\tau}(i)>\\hat{r}_{\\tau}(i^{*}),E_{i,t},E_{i^{*},t})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which can be true only if at least one of the following holds: ", "page_idx": 14}, {"type": "text", "text": "1. $\\hat{r}_{\\tau}(i)>\\mu_{i}+\\Delta_{i}/2$ , which, when intersected with $E_{i,\\tau}$ , holds thanks Hoeffding\u2019s inequality with probability at most: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}(\\hat{r}_{\\tau}(i)>\\mu_{i}+\\Delta_{i}/2,E_{i,\\tau})\\leq\\displaystyle\\sum_{y\\geq f(\\tau)}^{\\infty}\\mathbb{P}(\\hat{r}_{\\tau}(i)>\\mu_{i}+\\Delta_{i}/2,Z_{i}(\\tau)=y)}\\\\ &{\\qquad\\qquad\\qquad\\leq\\displaystyle\\sum_{y\\geq f(\\tau)}^{\\infty}e^{-\\frac{y\\Delta_{i}^{2}}{2}}=\\frac{e^{-\\frac{f(\\tau)\\Delta_{i}^{2}}{2}}}{1-e^{-\\Delta_{i}^{2}/2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "2. $\\hat{r}_{\\tau}(i^{*})<\\mu_{i^{*}}\\!-\\!\\Delta_{i}/2$ , which, when intersected to $E_{i^{*},\\tau}$ , is also true with the same probability as above, thanks to Hoeffding\u2019s inequality. ", "page_idx": 14}, {"type": "text", "text": "Therefore, we have proved that: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sum_{\\tau=1}^{T}\\mathbb{P}(i_{\\tau}=i,Z_{i}(\\tau-1)\\geq f(\\tau))\\leq2(1-e^{-\\Delta_{i}^{2}/2})^{-1}\\displaystyle\\sum_{\\tau=1}^{T}e^{-\\frac{f(\\tau)\\Delta_{i}^{2}}{2}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\underbrace{2(1-e^{-\\Delta_{i}^{2}/2})^{-1}\\displaystyle\\sum_{\\tau=1}^{T}e^{-\\frac{f(\\tau)\\Delta_{i}^{2}}{2}}}_{(*)}\\log(T).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We show that there exists a constant $C_{0}(\\Delta_{i})<+\\infty$ such that $(*)\\leq C_{0}(\\Delta_{i})$ in the equation above. We start observing that, since $f(\\cdot)$ is superlogathmic, we have: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{t\\to\\infty}\\operatorname*{sup}_{t e^{-\\frac{f(t)\\Delta_{i}^{2}}{2}}}=\\operatorname*{lim}_{t\\to\\infty}\\operatorname*{sup}_{t e^{-\\log(t)\\frac{f(t)\\Delta_{i}^{2}}{2\\log(t)}}=\\operatorname*{lim}_{t\\to\\infty}\\operatorname*{sup}_{t}\\left(\\frac{1}{t}\\right)^{\\frac{f(t)\\Delta_{i}^{2}}{2\\log(t)}}=0.}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Thus, we can find $t_{0}$ such that $e^{-\\frac{f(t)\\Delta_{i}^{2}}{2}}\\leq\\frac{1}{t}$ for all $t\\geq t_{0}$ , so that: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\infty}\\frac{\\sum_{\\tau=1}^{t}e^{-\\frac{f(\\tau)\\Delta_{i}^{2}}{2}}}{\\log(t)}\\leq\\operatorname*{lim}_{t\\to\\infty}\\frac{\\sum_{\\tau=1}^{t}e^{-\\frac{f(\\tau)\\Delta_{i}^{2}}{2}}}{\\sum_{\\tau=1}^{t}\\frac{1}{\\tau}}\\leq\\operatorname*{lim}_{t\\to\\infty}\\underbrace{\\sum_{\\tau=1}^{t_{0}}e^{-\\frac{f(\\tau)\\Delta_{i}^{2}}{2}}}_{\\underset{\\rightarrow0}{\\longrightarrow}}+\\underbrace{\\frac{\\sum_{\\tau=t_{0}}^{t}e^{-\\frac{f(\\tau)\\Delta_{i}^{2}}{2}}}{\\sum_{\\tau=t_{0}}^{t}\\frac{1}{\\tau}}}_{\\underset{\\leq1}{\\leq}}\\leq1\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Thus, since a sequence with finite limit superior is always bounded, we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\nC_{0}(\\Delta_{i}):=2(1-e^{-\\Delta_{i}^{2}/2})^{-1}\\operatorname*{sup}_{t>1}\\frac{\\sum_{\\tau=1}^{t}e^{-\\frac{f(\\tau)\\Delta_{i}^{2}}{2}}}{\\log(t)}<+\\infty,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "proving that for every suboptimal arm $i\\in[n]$ the following holds: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[Z_{i}(T)]\\leq1+f(T)+C_{0}(\\Delta_{i})\\log(T).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Finally, defining $C(\\Delta_{i}):=\\Delta_{i}C_{0}(\\Delta_{i})$ , we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\nR_{T}=\\sum_{i=1}^{n}\\Delta_{i}\\mathbb{E}[Z_{i}(T)],\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "concluding the proof. ", "page_idx": 15}, {"type": "text", "text": "Corollary 1. Let $\\delta>0$ and $f(t)=\\log(t)^{1+\\delta}$ be the sperlogarithmic function used in Algorithm 1, then we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\nC(f,\\Delta_{i})=\\frac{2\\Delta_{i}\\,\\left(e^{\\left(\\left(2/\\Delta_{i}^{2}\\right)^{1/\\delta}\\right)}+1\\right)}{1-e^{-\\Delta_{i}^{2}/2}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. Let $t_{0}>1$ be smallest integer such that: ", "page_idx": 15}, {"type": "equation", "text": "$$\ne^{-\\frac{\\log(t)^{1+\\delta}\\Delta_{i}^{2}}{2}}\\leq\\frac{1}{t},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "for all $t\\geq t_{0}>1$ . By rearranging the latter inequality we have that: ", "page_idx": 15}, {"type": "equation", "text": "$$\nt_{0}=\\lceil e^{\\left(\\left(2/\\Delta_{i}^{2}\\right)^{1/\\delta}\\right)}\\rceil.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "From the proof of Theorem 2, we have $C(\\Delta_{i},f):=\\Delta_{i}C_{0}(\\Delta_{i})$ , where: ", "page_idx": 15}, {"type": "equation", "text": "$$\nC_{0}(\\Delta_{i})=2(1-e^{-\\Delta_{i}^{2}/2})^{-1}\\operatorname*{sup}_{t>1}\\frac{\\sum_{\\tau=1}^{t}e^{-\\frac{f(\\tau)\\Delta_{i}^{2}}{2}}}{\\log(t)}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Furthermore, we let $t_{\\star}>1$ be the integer satisfying: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{\\sum_{\\tau=1}^{t}e^{-\\frac{f(\\tau)\\Delta_{i}^{2}}{2}}}{\\log(t)}\\leq\\frac{\\sum_{\\tau=1}^{t_{\\star}}e^{-\\frac{f(\\tau)\\Delta_{i}^{2}}{2}}}{\\log(t_{\\star})},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "for all the integers $t>1$ . Notice that such a value always exists, as the supremum limit of the above quantity is finite. Therefore, we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle C_{0}(\\Delta_{i})\\log(t_{\\star})=2(1-e^{-\\Delta_{i}^{2}/2})^{-1}\\sum_{\\tau=1}^{t_{\\star}}e^{-\\frac{f(\\tau)\\Delta_{i}^{2}}{2}}}\\\\ {\\displaystyle=2(1-e^{-\\Delta_{i}^{2}/2})^{-1}\\frac{\\sum_{\\tau=1}^{t_{\\star}}e^{-\\frac{f(\\tau)\\Delta_{i}^{2}}{2}}}{\\log(t_{\\star})}\\log(t_{\\star})}\\\\ {\\displaystyle\\leq2(1-e^{-\\Delta_{i}^{2}/2})^{-1}\\left(\\frac{\\sum_{\\tau=1}^{t_{0}}e^{-\\frac{f(\\tau)\\Delta_{i}^{2}}{2}}}{\\log(t_{\\star})}+\\frac{\\sum_{\\tau=t_{0}}^{t_{\\star}}e^{-\\frac{f(\\tau)\\Delta_{i}^{2}}{2}}}{\\sum_{\\tau=t_{0}}^{t_{\\star}}\\frac{1}{\\tau}}\\right)\\log(t_{\\star})}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Where the last step is due to the fact that for $t_{0}>1$ we have $\\begin{array}{r}{\\sum_{\\tau=t_{0}}^{t_{\\star}}\\frac{1}{\\tau}\\leq\\log(t_{\\star})}\\end{array}$ . Furthermore, thanks to the definition of $t_{0}$ , we have: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{\\sum_{\\tau=t_{0}}^{t_{\\star}}e^{-\\frac{f(\\tau)\\Delta_{i}^{2}}{2}}}{\\sum_{\\tau=t_{0}}^{t_{\\star}}\\frac{1}{\\tau}}\\leq1.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "With this consideration, we are able to conclude: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{C_{0}(\\Delta_{i})\\leq2(1-e^{-\\Delta_{i}^{2}/2})^{-1}\\left(\\frac{\\sum_{\\tau=1}^{t_{0}}e^{-\\frac{f(\\tau)\\Delta_{i}^{2}}{2}}}{\\log(t_{\\star})}+\\frac{\\sum_{\\tau=t_{0}}^{t_{\\star}}e^{-\\frac{f(\\tau)\\Delta_{i}^{2}}{2}}}{\\sum_{\\tau=t_{0}}^{t_{\\star}}\\frac{1}{\\tau}}\\right)}\\\\ &{\\qquad\\qquad\\leq2(1-e^{-\\Delta_{i}^{2}/2})^{-1}\\left(\\frac{t_{0}}{\\log(t_{\\star})}+1\\right)}\\\\ &{\\qquad\\qquad\\leq2(1-e^{-\\Delta_{i}^{2}/2})^{-1}+2(1-e^{-\\Delta_{i}^{2}/2})^{-1}e^{\\left(\\left(2/\\Delta_{i}^{2}\\right)^{1/\\delta}\\right)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Where in the last step we have used the fact that $\\log(t)>1$ for $t>2$ . Recollecting all the terms we have: ", "page_idx": 16}, {"type": "equation", "text": "$$\nC(\\Delta_{i},\\log(t)^{1+\\delta})=\\Delta_{i}C_{0}(\\Delta_{i},\\log(t)^{1+\\delta})\\leq2\\Delta_{i}(1-e^{-\\Delta_{i}^{2}/2})^{-1}(e^{\\big(2/\\Delta_{i}^{2}\\big)^{1/\\delta}})+1),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "concluding the proof. ", "page_idx": 16}, {"type": "text", "text": "B Proofs in the instance independent stochastic analysis ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "B.1 Instance dependent/independent trade-off ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Lemma 3. Let us define a random walk ", "page_idx": 16}, {"type": "equation", "text": "$$\nG_{t+1}=G_{t}+\\epsilon_{t}\\qquad\\epsilon_{t}=\\binom{1}{-1}\\quad\\mathrm{~with~prob.~}p\\qquad\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "with $G_{0}=1$ and $p=1/2+\\Delta/2>0.5$ for some $\\Delta\\in(0,1)$ . Then, we have: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\bigcup_{t=1}^{\\infty}\\{G_{t}\\le0\\}\\right)=\\left(\\frac{1-\\Delta}{1+\\Delta}\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. We define: ", "page_idx": 16}, {"type": "equation", "text": "$$\nf_{n}=\\mathbb{P}(G_{0}=n,\\exists t:G_{t}=0)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which satisfies, for $n\\geq0$ , the following recursive equation: ", "page_idx": 16}, {"type": "equation", "text": "$$\nf_{n}=p f_{n-1}+(1-p)f_{n+1}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "with $f_{n}=1$ for $n\\leq0$ . The equation corresponding to the aforementioned dynamical system is: ", "page_idx": 16}, {"type": "equation", "text": "$$\n(1-p)\\lambda^{2}-\\lambda+p=0.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The two solutions of the above equation are: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\lambda_{1,2}=\\frac{1\\pm\\sqrt{1-4p(1-p)}}{2(1-p)}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Thus, for all $n>0$ , we obtain: ", "page_idx": 16}, {"type": "equation", "text": "$$\nf_{n}=A\\left(\\frac{1+\\sqrt{1-4p(1-p)}}{2(1-p)}\\right)^{n}+B\\left(\\frac{1-\\sqrt{1-4p(1-p)}}{2(1-p)}\\right)^{n},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $A=0$ (otherwise, the equation does not define a probability) and $B=1$ (since $f_{1}\\rightarrow1$ for $\\Delta\\to0$ ). Therefore, from the definition of $p$ , we get: ", "page_idx": 16}, {"type": "equation", "text": "$$\nf_{n}=\\left({\\frac{1-\\sqrt{1-4p(1-p)}}{2(1-p)}}\\right)^{n}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle=\\left(\\frac{1-\\sqrt{1-4(1/2-\\Delta/2)(1/2+\\Delta/2)}}{1+\\Delta}\\right)^{n}}}\\\\ {{\\displaystyle=\\left(\\frac{1-\\Delta}{1+\\Delta}\\right)^{n}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The lemma holds by setting $n=1$ . ", "page_idx": 17}, {"type": "text", "text": "Theorem 3 (Instance Dependent/Independent Trade-off). Let $\\pi$ be any policy for the bandits with ranking feedback problem. If $\\pi$ satisfies the following properties: ", "page_idx": 17}, {"type": "text", "text": "\u2022 (instance-dependent regret upper bound) $\\begin{array}{r}{R_{T}\\leq\\sum_{i=1}^{n}C(\\Delta_{i})T^{\\alpha}}\\end{array}$ ", "page_idx": 17}, {"type": "text", "text": "\u2022 (instance-independent regret upper bound) $R_{T}\\le n C T^{\\beta}$ ", "page_idx": 17}, {"type": "text", "text": "then, $2\\alpha+\\beta\\geq1$ , where $\\alpha,\\beta\\ge0$ . ", "page_idx": 17}, {"type": "text", "text": "Proof. Let $p_{1}=0.5$ $.5,p_{2}=0.5-\\Delta,p_{2}^{*}=0.5+\\Delta$ . Let us consider three problems: ", "page_idx": 17}, {"type": "equation", "text": "$$\nP:\\;\\left\\{{\\begin{array}{l l}{\\nu_{1}=B e(p_{1})}&{\\qquad\\;\\;P^{*}:\\;\\left\\{{\\vphantom{\\bigg|}}^{\\nu_{1}=B e(p_{1})}\\atop\\nu_{2}=B e(p_{2})}\\right.}\\end{array}}\\right.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Clearly, the optimal arm is the first in instances $\\mathcal{P}$ and $\\mathcal{P}^{**}$ , while the optimal arm is the second in instance $\\mathcal{P}^{*}$ . ", "page_idx": 17}, {"type": "text", "text": "Let us now define the event: ", "page_idx": 17}, {"type": "equation", "text": "$$\nE_{t}=\\bigcap_{\\tau=1}^{t}\\{\\mathcal{R}_{t}=\\langle1,2\\rangle\\}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By assumption, the policy $\\pi$ has a sub- $\\cdot T^{\\alpha}$ instance-dependent regret. Therefore, in instance $\\mathcal{P}^{**}$ , we have for all $\\eta>0$ the following: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{T\\to\\infty}\\operatorname*{lim}_{\\substack{\\overline{{T}}^{\\alpha*}\\uplus\\tau}}[Z_{2}(T)|E_{T}]\\,=\\operatorname*{lim}_{T\\to\\infty}\\operatorname*{sup}_{T^{\\alpha*}\\uplus\\tau}\\frac{\\mathbb{E}_{\\mathcal{P}^{**}}[Z_{2}(T)]}{T^{\\alpha+\\eta}}=\\operatorname*{lim}_{T\\to\\infty}\\frac{R_{T}}{T^{\\alpha+\\eta}}=0,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "since in the instance $\\mathcal{P}^{**}$ , the event $E_{T}$ holds almost surely. ", "page_idx": 17}, {"type": "text", "text": "Under this event, no policy can distinguish between the instances. Therefore, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{t\\to\\infty}{\\operatorname*{lim}}\\underset{\\omega}{\\operatorname*{sup}}\\,\\frac{\\mathbb{E}_{\\mathcal{P}}[Z_{2}(t)|E_{t}]}{T^{\\alpha+\\eta}}=\\underset{T\\to\\infty}{\\operatorname*{lim}}\\frac{\\mathbb{E}_{\\mathcal{P}\\ast\\ast}[Z_{2}(t)|E_{t}]}{T^{\\alpha+\\eta}}=0}\\\\ &{\\implies\\exists C>0\\,\\forall t>0\\quad:\\quad\\mathbb{E}_{\\mathcal{P}}[Z_{2}(t)|E_{t}]\\leq C t^{\\alpha+\\eta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Where $\\mathbb{E}_{\\mathcal{P}}[\\cdot]$ is the expectation over the random variables of the rewards in instance $\\mathcal{P}$ . Therefore, by Markov\u2019s inequality, we have: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\mathcal{P}}(Z_{2}(T)>2C T^{\\alpha+\\eta}|E_{T})\\le\\frac{\\mathbb{E}_{\\mathcal{P}}[Z_{2}(T)|E_{T}]}{2C T^{\\alpha+\\eta}}\\le\\frac{C T^{\\alpha+\\eta}}{2C T^{\\alpha+\\eta}}\\le\\frac{1}{2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Now, note that for every $h>0$ , the event $\\{Z_{2}(T)<h\\}$ is contained in the $\\sigma$ \u2212algebra generated by the first $h$ pulls of arm 2 (and all the pulls of arm 1, but this is irrelevant since arm 1 corresponds to the same distribution in the first two instances). Therefore, thanks to Lemma 2, we have: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\forall h>0\\ \\ }&{\\ \\mathbb{P}_{\\mathcal{P}^{*}}(Z_{2}(T)\\le h)\\ge\\Big(\\frac{0.5-\\Delta}{0.5+\\Delta}\\Big)^{h}\\mathbb{P}_{\\mathcal{P}}(Z_{2}(T)\\le h)}\\\\ &{\\qquad\\qquad\\ge\\Big(\\frac{0.5-\\Delta}{0.5+\\Delta}\\Big)^{h}\\mathbb{P}_{\\mathcal{P}}(Z_{2}(T)\\le h,E_{T})}\\\\ &{\\qquad\\qquad=\\Big(\\frac{0.5-\\Delta}{0.5+\\Delta}\\Big)^{h}\\mathbb{P}_{\\mathcal{P}}(Z_{2}(T)\\le h|E_{T})\\mathbb{P}_{\\mathcal{P}}(E_{T}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By the previous step, we have $\\mathbb{P}_{\\mathcal{P}}(Z_{2}(T)\\le2C T^{\\alpha+\\eta}|E_{T})\\ge1/2$ , so that: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\mathcal{P}^{*}}(Z_{2}(T)\\leq2C T^{\\alpha+\\eta})\\geq\\frac{1}{2}\\Big(\\frac{0.5-\\Delta}{0.5+\\Delta}\\Big)^{2C T^{\\alpha+\\eta}}\\mathbb{P}_{P}(E_{T}),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "while, thanks to Lemma 3, we have: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\mathcal{P}}(E_{T})\\geq1-\\frac{1-\\Delta}{1+\\Delta}\\geq2\\Delta,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "meaning that: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\mathcal{P}^{*}}(Z_{2}(T)\\leq2C T^{\\alpha+\\eta})\\geq\\frac{1}{2}\\Big(\\frac{0.5-\\Delta}{0.5+\\Delta}\\Big)^{2C T^{\\alpha+\\eta}}\\frac{2\\Delta}{1+\\Delta}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We use this result to provide a lower bound for the regret in the instance-independent case. Analyzing the instance independent-regret, by definition, we have to fix $T$ as time horizon and let the arm gap $\\Delta$ my depend on $T$ . ", "page_idx": 18}, {"type": "text", "text": "Let us now fix $\\rho>1$ . With the choice $\\Delta=T^{-\\rho\\alpha}(\\leq1/4$ for sufficiently big $T$ ), we have: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}_{\\mathcal{P}^{*}}(Z_{2}(T)\\leq2C T^{\\alpha+\\eta})\\geq\\displaystyle\\frac{1}{2}\\Big(\\frac{0.5-T^{-\\rho\\alpha}}{0.5+T^{-\\rho\\alpha}}\\Big)^{2C T^{\\alpha+\\eta}}2T^{-\\rho\\alpha}}\\\\ &{\\phantom{\\geq\\displaystyle\\frac{1}{2}\\Big(1-4T^{-\\rho\\alpha}\\Big)^{2C T^{\\alpha+\\eta}}}\\frac{2T^{-\\rho\\alpha}}{1+T^{-\\rho\\alpha}}}\\\\ &{\\phantom{\\geq\\displaystyle\\Big(1-4T^{-\\rho\\alpha}\\Big)^{2C T^{\\alpha+\\eta}}\\frac{T^{-\\rho\\alpha}}{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "At this point if we choose $\\eta=\\alpha(\\rho-1)/2$ , the following fact holds: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{T\\rightarrow\\infty}{\\operatorname*{lim}}\\left(1-4T^{-\\rho\\alpha}\\right)^{2C T^{\\alpha+\\eta}}=\\underset{y\\rightarrow0^{+}}{\\operatorname*{lim}}\\left(1-4y\\right)^{C y^{\\frac{-(\\alpha+\\eta)}{\\rho\\alpha}}}}&{}\\\\ {=\\underset{y\\rightarrow0^{+}}{\\operatorname*{lim}}\\left(1-4y\\right)^{C y^{\\frac{-\\alpha(1/2+\\rho/2)}{\\rho\\alpha}}}}&{}\\\\ {=\\underset{y\\rightarrow0^{+}}{\\operatorname*{lim}}\\left(1-4y\\right)^{C y^{\\frac{-(1/2+\\rho/2)}{\\rho}}}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where in the first equality we substituted $y=T^{-\\rho\\alpha}$ . Here, $y^{\\frac{-(1/2+\\rho/2)}{\\rho}}\\,=\\,y^{-1}\\cdot y^{\\frac{\\rho-1}{2\\rho}}$ , where the second exponent is strictly positive. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\operatorname*{lim}_{T\\to\\infty}\\left(1-4T^{-\\rho\\alpha}\\right)^{2C T^{\\alpha+\\eta}}=\\displaystyle\\operatorname*{lim}_{y\\to0^{+}}\\left(\\left(1-4y\\right)^{-1/y}\\right)^{C y^{\\frac{\\rho-1}{2\\rho}}}}&{}\\\\ {\\displaystyle=\\operatorname*{lim}_{y\\to0^{+}}(1/e^{4})^{C y^{\\frac{\\rho-1}{2\\rho}}}=1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This limit shows that there is $c_{\\rho}>0$ such that: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left(1-4T^{-\\rho\\alpha}\\right)^{2C T^{\\alpha+\\eta}}\\geq c_{\\rho}\\qquad,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "when $T>0$ is sufficiently large. ", "page_idx": 18}, {"type": "text", "text": "Substituting this property in Equation (5), we get: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\mathcal{P}^{*}}(Z_{2}(T)\\le2C T^{\\alpha(1/2+\\rho/2)})\\ge\\frac{c_{\\rho}}{2}T^{-\\rho\\alpha},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "holding for every $\\rho>0$ and sufficiently large $T>0$ . ", "page_idx": 18}, {"type": "text", "text": "Thus, with $\\Delta=T^{-\\rho\\alpha}$ , we have: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\ensuremath{\\mathbb{E}_{\\mathcal P}}\\ast[R_{T}]\\geq\\Delta(T-2C T^{\\alpha(1/2+\\rho/2)})\\mathbb{P}_{\\mathcal P^{\\ast}}(Z_{2}(T)\\leq2C T^{\\alpha(1/2+\\rho/2)})}}\\\\ {{\\geq\\displaystyle\\frac{1}{2}T\\cdot\\frac{c_{\\rho}}{2}T^{-2\\rho\\alpha}=\\frac{c_{\\rho}}{4}T^{1-2\\rho\\alpha}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for all $T>0$ sufficiently big. ", "page_idx": 18}, {"type": "text", "text": "Therefore, for $\\beta\\le1-2\\rho\\alpha$ it is not possible to have an instance-independent upper regret bound. Since this is valid for every $\\rho>1$ , we can also extend the result to any $\\beta<1-2\\alpha$ , which leads to the conclusion that the necessary condition to satisfy both: ", "page_idx": 18}, {"type": "text", "text": "\u2022 (instance-dependent regret bound) ", "page_idx": 19}, {"type": "equation", "text": "$$\nR_{T}\\leq\\sum_{i=1}^{n}C(\\Delta_{i})T^{\\alpha}\\qquad\\forall T>0\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "\u2022 (instance-independent regret bound) ", "page_idx": 19}, {"type": "equation", "text": "$$\nR_{T}\\leq n C T^{\\beta}\\qquad\\forall T>0\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "for the same policy $\\pi$ is: ", "page_idx": 19}, {"type": "equation", "text": "$$\n2\\alpha+\\beta\\geq1.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "B.2 Proofs of instance independent regret upper bound ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "To derive the final instance-independent regret bound, we introduce some results from the theory of stochastic processes. The following subsections are therefore devoted to developing all the necessary results to prove the final regret bound of the algorithm. ", "page_idx": 19}, {"type": "text", "text": "B.2.1 Discretizing the Brownian motion ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we prove some results about the relationship between random walks and Brownian motions, that will be crucial in the proof of the regret bound. For this scope, we will introduce this quantity: ", "page_idx": 19}, {"type": "equation", "text": "$$\n|B_{t}+t\\mu_{0}<\\eta|=\\int_{0}^{1}\\mathbf{1}_{(-\\infty,\\eta)}(\\tau)d\\tau,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "corresponding to the Lebesgue measure of the set $\\left\\{t\\in[0,1]:B_{t}+t\\mu_{0}<\\eta\\right\\}$ . ", "page_idx": 19}, {"type": "text", "text": "We start with a lemma that bounds the increments in a standard Brownian motion. ", "page_idx": 19}, {"type": "text", "text": "Lemma 4. Let $\\{B_{t}\\}_{t\\in[0,1]}$ be a standard Brownian motion. We define: ", "page_idx": 19}, {"type": "equation", "text": "$$\nI_{i}:=[i/n,(i+1)/n],\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(\\underset{i\\in\\{0,\\ldots,n-1\\}}{\\operatorname*{sup}}\\left(\\operatorname*{sup}B_{t}-B_{i/n}\\right)\\geq\\eta\\right)=\\mathbb{P}\\left(\\underset{i\\in\\{0,\\ldots,n-1\\}}{\\operatorname*{inf}}\\big(\\underset{t\\in I_{i}}{\\operatorname*{inf}}B_{t}-B_{i/n}\\big)\\leq-\\eta\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\frac{2\\sqrt{n}\\exp\\Big(-\\frac{\\eta^{2}n}{2\\sigma^{2}}\\Big)}{\\eta/\\sigma\\sqrt{2\\pi}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. We notice that the Brownian motion satisfies: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{P}\\Bigg(\\underset{i\\in\\{0,\\dots,n-1\\}}{\\operatorname*{sup}}\\!\\!\\!\\!(\\operatorname*{sup}B_{t}-B_{i/n})\\geq\\eta\\Bigg)=\\mathbb{P}\\Bigg(\\underset{i\\in\\{0,\\dots,n-1\\}}{\\overset{n-1}{\\operatorname*{sup}}}B_{t}-B_{i/n}>\\eta\\Bigg)}\\\\ &{\\leq\\underset{i=0}{\\overset{n-1}{\\sum}}\\Bigg\\mathbb{P}\\Bigg(\\underset{i\\in\\mathcal{I}_{i}}{\\operatorname*{sup}}B_{t}-B_{i/n}>\\eta\\Bigg)}\\\\ &{=\\underset{i=0}{\\overset{n-1}{\\sum}}2\\mathbb{P}(B_{(i+1)/n}-B_{i/n}>\\eta)}\\\\ &{=\\underset{i\\in\\mathcal{I}_{i}}{\\overset{n-1}{\\sum}}2\\mathbb{P}(\\mathcal{N}(0,\\sigma^{2}/n)>\\eta)}\\\\ &{\\leq\\underset{i=0}{\\overset{n-1}{\\sum}}\\frac{2\\exp\\Big(-\\frac{n^{2}n^{2}}{2\\sigma^{2}}\\Big)}{\\eta/\\sqrt{2\\pi}}\\leq\\frac{2\\sqrt{n}\\exp\\Big(-\\frac{\\eta^{2}n^{2}}{2\\sigma^{2}}\\Big)}{\\eta/\\sqrt{2\\pi}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the second equality holds from the reflection principle (see [Baldi, 2017]), and last inequality holds since it is well known that: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{P}(N(0,\\beta^{2})>y)\\le\\frac{\\exp(-y^{2}/2\\beta^{2})}{y/\\beta\\sqrt{2\\pi}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "for tail bound on Gaussian distributions. In the exact same way, we can prove that: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\operatorname*{inf}_{i\\in\\{0,\\ldots,n-1\\}}(\\operatorname*{inf}_{t\\in I_{i}}B_{t}-B_{i/n})\\le-\\eta\\right)\\le\\frac{2\\sqrt{n}\\exp\\left(-\\frac{\\eta^{2}n}{2\\sigma^{2}}\\right)}{\\eta/\\sigma\\sqrt{2\\pi}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Together, the two results imply the thesis. ", "page_idx": 20}, {"type": "text", "text": "We are now ready to prove a theorem that links Brownian motion and random walks in terms of the probability that each of them stays in the interval $[0,\\infty)$ . ", "page_idx": 20}, {"type": "text", "text": "Lemma 5 (Discretization lemma). Let $\\left\\{G_{i}\\right\\}_{i\\in\\{0,...n-1\\}}$ be a Gaussian 0-mean unit variance random walk, $\\mu\\in\\mathbb{R},$ and $\\{B_{t}\\}_{t\\in[0,1]}$ $a$ standard Brownian motion. Then, for every $s\\in(0,1)$ , we have: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{P}\\left(|B_{t}+t\\mu_{0}>\\eta|>s\\right)-P(n,\\eta)\\le\\mathbb{P}\\left(\\displaystyle\\sum_{i=0}^{n-1}\\mathbf{1}_{(0,\\infty)}\\left(G_{i}+i\\mu\\right)\\,>s n\\right)}&{}\\\\ {\\le\\mathbb{P}\\left(|B_{t}+t\\mu_{0}>-\\eta|>s\\right)+P(n,\\eta),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{P}\\left(|B_{t}+t\\mu_{0}\\leq\\eta|\\leq s\\right)-P(n,\\eta)\\leq\\mathbb{P}\\left(\\displaystyle\\sum_{i=0}^{n-1}\\mathbf{1}_{(-\\infty,0]}\\left(G_{i}+i\\mu\\right)\\leq s n\\right)}&{}\\\\ {\\leq\\mathbb{P}\\left(|B_{t}+t\\mu_{0}\\leq-\\eta|\\leq s\\right)+P(n,\\eta),}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "$\\begin{array}{r}{P(n,\\eta)=\\frac{2\\sqrt{n}\\exp\\left(-\\eta^{2}n/2\\right)}{\\eta\\sqrt{2\\pi}}}\\end{array}$ and $\\mu_{0}={\\sqrt{n}}\\mu.$ ", "page_idx": 20}, {"type": "text", "text": "Proof. We only prove the first part, as the second one follows trivially by substituting: ", "page_idx": 20}, {"type": "equation", "text": "$$\ns\\gets1-s,\\,\\mu\\gets-\\mu,\\,G_{i}\\gets-G_{i},\\,B_{t}\\gets-B_{t}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Let $\\{B_{t}\\}_{t\\in[0,1]}$ be a standard Brownian motion. We define: ", "page_idx": 20}, {"type": "equation", "text": "$$\nI_{i}:=[i/n,(i+1)/n],\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "for all $i\\;\\in\\;\\{0,\\ldots,n-1\\}$ . Furthermore, we set $\\mu_{0}\\,=\\,{\\sqrt{n}}\\mu$ . With this definition, we have the following set of inclusions for any $s\\in[0,1]$ and $\\eta>0$ : ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\left\\lvert B_{t}+t\\mu_{0}>\\eta\\right\\rvert>s)=\\left\\{\\int_{0}^{1}\\mathbf{1}_{(\\eta,\\infty)}(B_{\\tau}+\\tau\\mu_{0})\\;d\\tau>s\\right\\}}\\\\ {\\displaystyle\\qquad\\qquad=\\left\\{\\sum_{i=0}^{n-1}\\int_{I_{i}}\\mathbf{1}_{(\\eta,\\infty)}(B_{\\tau}+\\tau\\mu_{0})\\;d\\tau>s\\right\\}}\\\\ {\\displaystyle\\qquad\\qquad\\leq\\left\\{\\sum_{i=0}^{n-1}\\operatorname*{sup}_{\\tau\\in I_{i}}\\mathbf{1}_{(\\eta,\\infty)}(B_{\\tau}+\\tau\\mu_{0})>s n\\right\\}}\\\\ {\\displaystyle\\qquad\\leq\\left\\{\\sum_{i=0}^{n-1}\\mathbf{1}_{(0,\\infty)}\\left(B_{i/n}+\\frac{i}{n}\\mu_{0}\\right)>s n\\right\\}\\cup\\left\\{\\underset{i\\in\\{0,\\ldots,n-1\\}}{\\operatorname*{sup}}\\left(\\operatorname*{sup}_{B}B_{t}-B_{i/n}\\right)\\geq0\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Moreover, using the same steps above, it also holds: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{t\\mu_{0}>-\\eta|>s\\}=\\left\\{\\int_{0}^{1}\\mathbf{1}_{(-\\eta,\\infty)}(B_{\\tau}+\\tau\\mu_{0})\\;d\\tau>s\\right\\}}\\\\ &{}&{\\supseteq\\left\\{\\displaystyle\\sum_{i=0}^{n-1}\\mathbf{1}_{(0,\\infty)}\\left(B_{i/n}+\\frac{i}{n}\\mu_{0}\\right)\\;>s n\\right\\}\\cap\\left\\{\\displaystyle\\operatorname*{inf}_{i\\in\\{0,\\ldots,n-1\\}}(\\operatorname*{inf}_{t\\in I_{i}}B_{t}-B_{i/n})\\geq-\\eta\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Now, note that the random variable $B_{i/n}$ for each $i\\in[n]$ has the same distribution of $G_{i}/\\sqrt{n}$ , thus: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathbb{P}\\left(\\displaystyle\\sum_{i=0}^{n-1}\\mathbf{1}_{(0,\\infty)}\\left(B_{i/n}+\\frac{i}{n}\\mu_{0}\\right)\\!\\!\\!}&{>s n\\right)=\\mathbb{P}\\left(\\displaystyle\\sum_{i=0}^{n-1}\\mathbf{1}_{(0,\\infty)}\\left(\\sqrt{n}B_{i/n}+\\frac{i}{\\sqrt{n}}\\mu_{0}\\right)\\!\\!\\!}&{>s n\\right)}&\\\\ &{}&{=\\mathbb{P}\\left(\\displaystyle\\sum_{i=0}^{n-1}\\mathbf{1}_{(0,\\infty)}\\left(G_{i}+i\\mu\\right)\\,>s n\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Therefore, by union bound, we have: ", "page_idx": 21}, {"type": "equation", "text": "$$\n^{>}(|B_{t}+t\\mu_{0}>\\eta|>s)\\leq\\mathbb{P}\\left(\\sum_{i=0}^{n-1}\\mathbf{1}_{(0,\\infty)}\\left(G_{i}+i\\mu\\right)\\,>s n\\right)+\\mathbb{P}\\left(\\operatorname*{sup}_{i\\in\\{0,\\ldots,n-1\\}}\\left(\\operatorname*{sup}_{t\\in I_{i}}B_{t}-B_{i/n}\\right)\\geq n\\leq2\\right),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and, ", "page_idx": 21}, {"type": "equation", "text": "$$\n^3(|B_{t}+t\\mu_{0}>-\\eta|>s)\\ge\\mathbb{P}\\left(\\sum_{i=0}^{n-1}\\mathbf{1}_{(0,\\infty)}\\left(G_{i}+i\\mu\\right)\\ >s n\\right)-\\mathbb{P}\\left(\\operatorname*{inf}_{i\\in\\{0,\\ldots,n-1\\}}(\\operatorname*{inf}_{t\\in I_{i}}B_{t}-B_{i/n})\\le-\\eta\\right)\\,.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The proof is completed applying Lemma 4 and reordering the terms. ", "page_idx": 21}, {"type": "text", "text": "Corollary 5. Let $\\{G_{i}\\}_{i\\in\\{0,\\dots n-1\\}}$ be a Gaussian 0-mean unit variance random walk, and $\\mu\\in\\mathbb{R}$ . Then, for every $s\\in(0,1)$ , we have: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left(\\sum_{i=0}^{n-1}\\mathbf{1}_{(-\\infty,0]}\\left(G_{i}+i\\mu\\right)\\leq s n\\right)\\in\\left[\\mathbb{P}\\left(\\left|B_{t}+t\\mu_{0}\\leq\\frac{2\\log(n)}{\\sqrt{n}}\\right|\\leq s\\right)-\\frac{\\sqrt{2}}{\\sqrt{\\pi}n\\log(n)},\\right.}\\\\ {\\left.\\mathbb{P}\\left(\\left|B_{t}+t\\mu_{0}\\leq-\\frac{2\\log(n)}{\\sqrt{n}}\\right|\\leq s\\right)+\\frac{\\sqrt{2}}{\\sqrt{\\pi}n\\log(n)}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\mu_{0}=\\sqrt{n}\\mu$ . ", "page_idx": 21}, {"type": "text", "text": "Proof. It is sufficient to make the substitution: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\eta=\\frac{2\\log(n)}{\\sqrt{n}},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "in Lemma 5. Then, we have: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{P(n,\\eta)=\\frac{2\\sqrt{n}\\exp\\left(-\\eta^{2}n/2\\right)}{\\eta\\sqrt{2\\pi}}}}\\\\ &{}&{=\\frac{2\\sqrt{n}\\exp\\left(-\\log(n)^{2}\\right)}{\\frac{\\log(n)}{\\sqrt{n}}\\sqrt{2\\pi}}}\\\\ &{}&{=\\frac{2n\\exp\\left(-\\log(n)^{2}\\right)}{\\log(n)\\sqrt{2\\pi}}=\\frac{\\sqrt{2}}{\\sqrt{\\pi}n\\log(n)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "concluding the proof. ", "page_idx": 21}, {"type": "text", "text": "B.2.2 Proofs of filtering inequalities ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "All the proof of this subsections will be based on the following very powerful result, which studies the time spent by a Brownian Motion with drift in the half-line $[0,\\infty)$ . ", "page_idx": 21}, {"type": "text", "text": "Theorem 6 (Tak\u00e1cs [1996]). Let $B_{t}$ be a standard Brownian motion on $t\\in[0,1]$ , and let us note as $|\\cdot|$ the Lebesgue measure of a set. For $\\mu_{0}\\in\\mathbb{R}$ and $\\eta>0$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(|B_{t}+t\\mu_{0}\\le\\eta|\\le s\\right)=2\\int_{0}^{s}\\left[\\frac{\\varphi(\\mu_{0}\\sqrt{1-\\tau})}{\\sqrt{1-\\tau}}+\\mu_{0}\\Phi(\\mu_{0}\\sqrt{1-\\tau})\\right]\\times\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left[{\\frac{\\varphi(\\eta/{\\sqrt{\\tau}}-\\mu_{0}{\\sqrt{\\tau}})}{\\sqrt{\\tau}}}-\\mu_{0}e^{2\\mu_{0}\\eta}\\Phi(-\\eta/{\\sqrt{\\tau}}-\\mu_{0}{\\sqrt{\\tau}})\\right]\\,d\\tau,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\varphi(x):={\\frac{1}{\\sqrt{2\\pi}}}e^{-x^{2}/2}\\qquad\\Phi(x):=\\int_{-\\infty}^{x}\\varphi(u)\\;d u.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Thanks to the previous theorem, we can prove the following crucial results. ", "page_idx": 22}, {"type": "text", "text": "Theorem 7. Let $T$ be a sufficiently large constant. Let $\\{G_{i}\\}_{i\\in\\{0,\\ldots n-1\\}}$ be a Gaussian 0-mean unit variance random walk, and $\\mu\\in\\mathbb{R}$ . If $\\mu\\geq C T^{-\\alpha}$ , for some $\\alpha\\in(0,1/2)$ and $C=4\\log(T)$ , then setting $n=\\lceil T^{1/2+\\alpha}\\rceil$ we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\sum_{i=0}^{n-1}\\mathbf{1}_{(-\\infty,0]}\\left(G_{i}+i\\mu\\right)\\leq T^{2\\alpha}\\right)\\geq1-2T^{-1/2}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. In the rest of the proof, we will assume, for ease of notation, that $T$ is such that $T^{1/2+\\alpha}$ an integer, so that $n=T^{1/2+\\alpha}$ . This is done without loss of generality, since substituting $n$ with $n+1$ leads to a negligble difference for $T$ sufficiently big. Applying the discretization corollary 5, we have that for every $s\\in(0,1)$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\sum_{i=0}^{n-1}\\mathbf{1}_{(-\\infty,0]}\\left(G_{i}+i\\mu\\right)\\leq s n\\right)\\geq\\mathbb{P}\\left(\\left|B_{t}+t\\mu_{0}\\leq\\frac{2\\log(n)}{\\sqrt{n}}\\right|\\leq s\\right)-\\frac{\\sqrt{2}}{\\sqrt{\\pi}n\\log(n)},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\mu_{0}=\\sqrt{n}\\mu$ . Therefore, by assumption, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mu_{0}=\\sqrt{n}\\mu\\geq(T^{1/2+\\alpha})^{1/2}C T^{-\\alpha}=C T^{1/4-\\alpha/2}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "At this point, we can apply Theorem 6 to have, for any $\\eta>0$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{P}\\left(|B_{t}+t\\mu_{0}\\le\\eta|\\le s\\right)=2\\int_{0}^{s}\\left(\\frac{\\phi(\\mu_{0}\\sqrt{1-\\tau})}{\\sqrt{1-\\tau}}+\\mu_{0}\\Phi(\\mu_{0}\\sqrt{1-\\tau})\\right)}\\\\ {\\displaystyle\\times\\left(\\phi\\left(\\frac{\\eta-\\mu_{0}\\tau}{\\sqrt{\\tau}}\\right)\\frac{1}{\\sqrt{\\tau}}-\\mu_{0}e^{2\\mu_{0}\\eta}\\Phi\\left(\\frac{-\\eta-\\mu_{0}\\tau}{\\sqrt{\\tau}}\\right)\\right)\\,d\\tau}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which means that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{P}\\left(|B_{t}+t\\mu_{0}\\le\\eta|\\le s\\right)=1-2\\int_{s}^{1}\\left(\\underbrace{\\frac{\\phi\\left(\\mu_{0}\\sqrt{1-\\tau}\\right)}{\\sqrt{1-\\tau}}}_{(1)}+\\underbrace{\\mu_{0}\\Phi(\\mu_{0}\\sqrt{1-\\tau})}_{(2)}\\right)}}\\\\ &{}&{\\times\\left(\\underbrace{\\phi\\left(\\frac{\\eta-\\mu_{0}\\tau}{\\sqrt{\\tau}}\\right)\\frac{1}{\\sqrt{\\tau}}}_{(3)}-\\underbrace{\\mu_{0}e^{2\\mu_{0}\\eta}\\Phi\\left(\\frac{-\\eta-\\mu_{0}\\tau}{\\sqrt{\\tau}}\\right)}_{(4)}\\right)\\,d\\tau.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Here, we have to consider that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bullet\\ \\eta=\\frac{2\\log(n)}{\\sqrt{n}}\\leq2\\log(T)T^{-\\alpha/2-1/4}}\\\\ &{\\bullet\\ \\mu_{0}\\geq C T^{1/4-\\alpha/2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Moreover, to have the thesis, we are interested in a value of $s$ such that $s n=T^{2\\alpha}$ , corresponding to $T^{-1/2+\\alpha}$ . Therefore, in the interval $[T^{-1/2+\\alpha},1]$ , we have ", "page_idx": 22}, {"type": "text", "text": "1. Consider term (3): ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\phi\\left(\\frac{\\eta-\\mu_{0}\\tau}{\\sqrt{\\tau}}\\right)\\frac{1}{\\sqrt{\\tau}}\\le\\phi\\left(\\frac{\\eta-\\mu_{0}T^{-1/2+\\alpha}}{T^{-1/4+\\alpha/2}}\\right)\\frac{1}{T^{-1/4+\\alpha/2}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\phi\\left(\\eta T^{1/4-\\alpha/2}-\\mu_{0}T^{-1/4+\\alpha/2}\\right)\\frac{1}{T^{-1/4+\\alpha/2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Here, since $\\begin{array}{r}{\\eta~=~\\frac{2\\log(n)}{\\sqrt{n}}~\\leq~2\\log(T)T^{-\\alpha/2-1/4}}\\end{array}$ , the part $\\eta T^{1/4-\\alpha/2}$ is bounded by $2\\log(T)$ . ", "page_idx": 23}, {"type": "text", "text": "Instead, $\\mu_{0}T^{-1/4+\\alpha/2}\\geq C T^{1/4-\\alpha/2}T^{-1/4+\\alpha/2}=C.$ ", "page_idx": 23}, {"type": "text", "text": "2. Term (4) is non-negative. ", "page_idx": 23}, {"type": "text", "text": "Therefore, for $C=4\\log(T)$ , we have that in the interval $[T^{-1/2+\\alpha},1]$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n(3)+(4)\\leq\\phi\\left(2\\log(T)\\right)\\frac{1}{T^{-1/4+\\alpha/2}}=\\frac{1}{\\sqrt{2\\pi}T^{-1/4+\\alpha/2}}e^{-2\\log(T)^{2}}\\leq\\frac{T^{-1}}{\\sqrt{2\\pi}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "With this inequality, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\triangleright\\left(|B_{t}+t\\mu_{0}\\le\\eta|\\le T^{-1/2+\\alpha}\\right)=1-2\\frac{T^{-1}}{\\sqrt{2\\pi}}\\int_{T^{-1/2+\\alpha}}^{1}\\left(\\frac{\\phi\\left(\\mu_{0}\\sqrt{1-\\tau}\\right)}{\\sqrt{1-\\tau}}+\\mu_{0}\\Phi(\\mu_{0}\\sqrt{1-\\tau})\\right)\\ d\\tau}\\\\ &{\\qquad\\qquad\\qquad\\ge1-2\\frac{T^{-1}}{\\sqrt{2\\pi}}\\int_{T^{-1/2+\\alpha}}^{1}\\frac{1}{\\sqrt{2\\pi(1-\\tau)}}+|\\mu_{0}|d\\tau}\\\\ &{\\qquad\\qquad\\qquad\\ge1-2\\frac{T^{-1}}{\\sqrt{2\\pi}}\\int_{0}^{1}\\frac{1}{\\sqrt{2\\pi(1-\\tau)}}+|\\mu_{0}|d\\tau}\\\\ &{\\qquad\\qquad\\quad=1-2\\frac{T^{-1}}{\\sqrt{2\\pi}}\\left(\\frac{\\sqrt{2}}{\\sqrt{\\pi}}+\\mu_{0}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "At this point, knownig from the assumptions that $n<T$ , we have $\\mu_{0}\\leq\\sqrt{T}$ , which implies ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(|B_{t}+t\\mu_{0}\\le\\eta|\\le T^{-1/2+\\alpha}\\right)\\ge1-\\frac{T^{-1/2}}{\\pi}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Substituting this result into Equation 6, we get, for s = T \u22121/2+\u03b1 and n \u2265T 1/2+\u03b1 ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb P\\left(\\sum_{i=0}^{n-1}\\mathbf{1}_{(-\\infty,0]}\\left(G_{i}+i\\mu\\right)\\,<T^{2\\alpha}\\right)\\ge1-\\frac{T^{-1/2}}{\\pi}-\\frac{\\sqrt{2}}{\\sqrt{\\pi}T^{1/2+\\alpha}\\log\\left(T^{1/2+\\alpha}\\right)}}}\\\\ &{}&{\\ge1-2T^{-1/2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The second result is the following ", "page_idx": 23}, {"type": "text", "text": "Theorem 8. Let $T$ be a sufficiently large constant. Let $\\left\\{G_{i}\\right\\}_{i\\in\\{0,...n-1\\}}$ be a Gaussian 0-mean unit variance random walk, and $\\mu\\,\\in\\,\\mathbb{R}$ such that $\\mu\\:\\leq\\:-C T^{-\\theta}$ , for some $\\theta~\\in~(0,1/2)$ and $C=2{\\sqrt{\\log(T)}}+2$ . Then, for any $\\alpha\\in(0,1/2)$ , setting $n=\\lfloor T^{1/2+\\alpha}\\rfloor$ we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\sum_{i=0}^{n-1}\\mathbf{1}_{(-\\infty,0]}\\left(G_{i}+i\\mu\\right)\\leq T^{2\\alpha}\\right)\\leq3T^{-1/2+\\theta}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. In the rest of the proof, we will assume, for ease of notation, that $T$ is such that $T^{1/2+\\alpha}$ an integer, so that $n=T^{1/2+\\alpha}$ . This is done without loss of generality, since substituting $n$ with $n+1$ leads to a negligble difference for $T$ sufficiently large. Applying the discretization corollary 5, we have that for every $s\\in(0,1)$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\sum_{i=0}^{n-1}\\mathbf{1}_{(-\\infty,0]}\\left(G_{i}+i\\mu\\right)\\ \\leq s n\\right)\\leq\\mathbb{P}\\left(\\left|B_{t}+t\\mu_{0}\\leq-\\frac{2\\log(n)}{\\sqrt{n}}\\right|\\leq s\\right)+\\frac{\\sqrt{2}}{\\sqrt{\\pi}n\\log(n)},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\mu_{0}=\\sqrt{n}\\mu$ . Therefore, by assumption, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mu_{0}=\\sqrt{n}\\mu\\leq-(T^{1/2+\\alpha})^{1/2}C T^{-\\theta}=-C T^{1/4+\\alpha/2-\\theta}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Differently from the previous proof, here we cannot directly apply Theorem 6, since $\\begin{array}{r}{\\eta=-\\frac{2\\log(n)}{\\sqrt{n}}<}\\end{array}$ 0. ", "page_idx": 24}, {"type": "text", "text": "Still, we can say that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{P}\\left(\\left|B_{t}+t\\mu_{0}\\leq-\\frac{2\\log(n)}{\\sqrt{n}}\\right|\\leq s\\right)=\\mathbb{P}\\left(\\left|-B_{t}-t\\mu_{0}>\\frac{2\\log(n)}{\\sqrt{n}}\\right|\\leq s\\right)\\quad}\\\\ {\\quad\\displaystyle=\\mathbb{P}\\left(\\left|-B_{t}-t\\mu_{0}\\leq\\frac{2\\log(n)}{\\sqrt{n}}\\right|>1-s\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "At this point, we set $\\begin{array}{r}{\\eta=\\frac{2\\log(n)}{\\sqrt{n}}}\\end{array}$ lo\u221agn(n), \u00b5\u02dc0 = \u2212\u00b50 and Bt = \u2212Bt (it is not necessary to rename it since its distribution is symmentric). In this way we can apply Theorem 6 having that the previous probability corresponds to ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{P}\\left(|B_{t}+t\\tilde{\\mu}_{0}\\leq\\eta|>1-s\\right)=2\\int_{1-s}^{1}\\left(\\underbrace{\\frac{\\phi\\left(\\tilde{\\mu}_{0}\\sqrt{1-\\tau}\\right)}{\\sqrt{1-\\tau}}}_{(1)}+\\underbrace{\\tilde{\\mu}_{0}\\Phi(\\tilde{\\mu}_{0}\\sqrt{1-\\tau})}_{(2)}\\right)}}\\\\ &{}&{\\times\\left(\\underbrace{\\phi\\left(\\frac{\\eta-\\tilde{\\mu}_{0}\\tau}{\\sqrt{\\tau}}\\right)\\frac{1}{\\sqrt{\\tau}}}_{(3)}-\\underbrace{\\tilde{\\mu}_{0}e^{2\\mu_{0}\\eta}\\Phi\\left(\\frac{-\\eta-\\tilde{\\mu}_{0}\\tau}{\\sqrt{\\tau}}\\right)}_{(4)}\\right)\\,d\\tau.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Here, we have to consider that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bullet\\ \\eta=\\frac{2\\log(n)}{\\sqrt{n}}\\leq2\\log(T)T^{-\\alpha/2-1/4}}\\\\ &{\\bullet\\ \\tilde{\\mu}_{0}\\geq C T^{1/4+\\alpha/2-\\theta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Moreover, to have the thesis, we are interested in a value of $s$ such that $s n=T^{2\\alpha}$ , corresponding to $T^{-1/2+\\alpha}$ . ", "page_idx": 24}, {"type": "text", "text": "Here, it is convenient to divide the proof in two cases, depending on the sign of $1/4+\\alpha/2-\\theta$ . ", "page_idx": 24}, {"type": "text", "text": "1. Assume $(1/4+\\alpha/2-\\theta>0)$ . Then, considering term (3) we have that for $\\tau\\in[1/2,1]$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n(3)\\leq\\phi\\left(\\frac{\\eta-\\tilde{\\mu}_{0}\\tau}{\\sqrt{\\tau}}\\right)\\frac{1}{\\sqrt{\\tau}}\\leq\\sqrt{2}\\phi\\left(\\sqrt{2}\\eta-\\tilde{\\mu}_{0}/\\sqrt{2}\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Moreover, since term (4) is nonnegative we also have ", "page_idx": 24}, {"type": "equation", "text": "$$\n(3)+(4)\\leq\\sqrt{2}\\phi\\left(\\sqrt{2}\\eta-\\tilde{\\mu}_{0}/\\sqrt{2}\\right)=\\frac{1}{\\sqrt{\\pi}}e^{-(\\sqrt{2}\\eta-\\tilde{\\mu}_{0}/\\sqrt{2})^{2}/2}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Being $1/4+\\alpha/2-\\theta>0$ and $\\eta<1$ , the exponent is less than $-({\\sqrt{2}}-C/{\\sqrt{2}})^{2}/2$ . Thi means that for $C=2{\\sqrt{\\log(T)}}+2$ the full term is bounded by ", "page_idx": 24}, {"type": "equation", "text": "$$\n(3)+(4)\\leq{\\frac{1}{\\sqrt{\\pi}}}e^{-({\\sqrt{2}}-C/{\\sqrt{2}})^{2}/2}={\\frac{1}{\\sqrt{\\pi}}}e^{-({\\sqrt{2\\log(T)}})^{2}/2}={\\frac{T^{-1}}{\\sqrt{\\pi}}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Substituting this inequality, we get ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(|B_{t}+t\\tilde{\\mu}_{0}\\leq\\eta|>1-T^{-1/2+\\alpha}\\right)\\leq\\frac{2T^{-1}}{\\sqrt{\\pi}}\\int_{1-T^{-1/2+\\alpha}}^{1}\\left(\\frac{\\phi(\\tilde{\\mu}_{0}\\sqrt{1-\\tau})}{\\sqrt{1-\\tau}}+\\tilde{\\mu}_{0}\\Phi(\\tilde{\\mu}_{0}\\sqrt{1-\\tau})\\right)\\,d\\tau}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\frac{2T^{-1}}{\\sqrt{\\pi}}\\int_{1-T^{-1/2+\\alpha}}^{1}\\frac{1}{\\sqrt{2\\pi(1-\\tau)}}+|\\tilde{\\mu}_{0}|d\\tau}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\frac{2T^{-1}}{\\sqrt{\\pi}}(2+T^{-1/2+\\alpha}\\tilde{\\mu}_{0})\\leq\\frac{6T^{-1}}{\\sqrt{\\pi}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "This quantity is of course less than $T^{-\\theta}$ , since $\\theta\\in(0,1/2)$ by assumption ", "page_idx": 25}, {"type": "text", "text": "2. Assume $(1/4+\\alpha/2-\\theta<0)$ . In this case, we have, being $\\tilde{\\mu}_{0}\\geq0$ , the following inequality ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(|B_{t}+t\\tilde{\\mu}_{0}\\leq\\eta|>1-s\\right)\\leq\\mathbb{P}\\left(|B_{t}\\leq\\eta|>1-s\\right).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "This simplified form leads to ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(\\left|B_{t}+t\\tilde{\\mu}_{0}\\leq\\eta\\right|>1-s\\right)\\leq2\\displaystyle\\int_{1-s}^{1}\\frac{\\phi(0)}{\\sqrt{1-\\tau}}\\phi\\left(\\frac{\\eta}{\\sqrt{\\tau}}\\right)\\frac{1}{\\sqrt{\\tau}}d\\tau}\\\\ &{\\qquad\\qquad\\qquad\\leq2\\displaystyle\\int_{1-s}^{1}\\frac{\\phi(0)}{\\sqrt{1-\\tau}}\\phi\\left(0\\right)\\frac{1}{\\sqrt{\\tau}}d\\tau}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\frac{1}{\\pi}\\int_{1-s}^{1}\\frac{1}{\\sqrt{\\tau(1-\\tau)}}d\\tau.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Since in our case $s=T^{-1/2+\\alpha}<1/2$ , this can be further simplified as ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(|B_{t}+t\\tilde{\\mu}_{0}\\leq\\eta|>1-s\\right)\\leq\\frac{1}{\\pi}\\int_{1-s}^{1}\\frac{1}{\\sqrt{\\tau(1-\\tau)}}d\\tau}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\frac{2}{\\pi}\\int_{1-s}^{1}\\frac{1}{\\sqrt{1-\\tau}}d\\tau}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\quad\\frac{y=1-\\tau}{\\pi}\\,\\frac{2}{\\pi}\\int_{0}^{s}\\frac{1}{\\sqrt{y}}d y=\\frac{4}{\\pi}\\sqrt{s}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "This leads to ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(|B_{t}+t\\tilde{\\mu}_{0}\\leq\\eta|>1-T^{-1/2+\\alpha}\\right)\\leq\\frac{4}{\\pi}T^{-1/4+\\alpha/2}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "By assumption, $1/4+\\alpha/2-\\theta<0$ the exponent is $-1/4+\\alpha/2<T^{-1/2+\\theta}$ . Therefore, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(|B_{t}+t\\tilde{\\mu}_{0}\\leq\\eta|>1-T^{-1/2+\\alpha}\\right)\\leq\\frac{4}{\\pi}T^{-1/2+\\theta}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Thus, we have proved that in both cases ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(|B_{t}+t\\tilde{\\mu}_{0}\\leq\\eta|>1-s\\right)\\leq\\frac{4}{\\pi}T^{-1/2+\\theta}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Finally, applying Equation (7) and substituting the value of $n$ , we get ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\sum_{i=0}^{n-1}\\mathbf{1}_{(-\\infty,0]}\\left(G_{i}+i\\mu\\right)\\ \\leq T^{2\\alpha}\\right)\\leq\\frac{4}{\\pi}T^{-1/2+\\theta}+\\frac{\\sqrt{2}}{\\sqrt{\\pi}T^{1/2+\\alpha}\\log(T^{1/2+\\alpha})},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "which implies ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\sum_{i=0}^{n-1}\\mathbf{1}_{(-\\infty,0]}\\left(G_{i}+i\\mu\\right)\\ \\leq T^{2\\alpha}\\right)\\leq3T^{-1/2+\\theta}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "B.2.3 Regret bound ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Before the actual proof, we are stating a simple proposition about the structure of the loggrid, which will ease the next computations. ", "page_idx": 26}, {"type": "text", "text": "Proposition 9. Let ", "text_level": 1, "page_idx": 26}, {"type": "equation", "text": "$$\nL G(1/2,1,T):=\\left\\{\\lfloor T^{\\lambda_{j}+(1-\\lambda_{j})/2}\\rfloor:\\ \\lambda_{j}={\\frac{j}{\\lfloor\\log(T)\\rfloor}},\\ \\forall j=0,\\ldots,\\lfloor\\log(T)\\rfloor\\right\\}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The following identities hold ", "page_idx": 26}, {"type": "text", "text": "1. $L G(1/2,1,T)$ can be equivalently defined as ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{L G(1/2,1,T):=\\left\\{|T^{1/2+\\frac{j}{2\\lfloor\\log(T)\\rfloor}}\\rfloor,\\,\\forall j=0,\\ldots,\\lfloor\\log(T)\\rfloor\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "2. Let $\\ell_{j}$ the $j$ \u2212th element of $L G(1/2,1,T)$ , and $\\begin{array}{r}{\\alpha_{j}=\\frac{\\log(\\ell_{j})}{\\log(T)}-1/2}\\end{array}$ . Then $\\begin{array}{r}{\\alpha_{j}=\\frac{j}{2\\left\\lfloor\\log(T)\\right\\rfloor}+}\\end{array}$ $o(T^{-1/2})$ . ", "page_idx": 26}, {"type": "text", "text": "Next, we prove the following lemmas, which concern some features of our algorithm. ", "page_idx": 26}, {"type": "text", "text": "Lemma 6. For any arm $i$ , the probability of the event $E_{i i_{0}}$ , corresponding to $i$ eliminating the another $i_{0}$ arm such that their gap is $\\Delta_{i i_{0}}:=\\mu_{i_{0}}-\\mu_{i}>0_{!}$ , is, at most ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}(E_{i i_{0}})\\leq6\\log(T)(4\\log(T)+2)T^{-1/2}\\Delta_{i i_{0}}^{-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. Let us call: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\widetilde{\\Delta}_{i i_{0}}=\\frac{\\Delta_{i i_{0}}}{4\\log(T)+2}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "At this point, there are two possibilities, ", "page_idx": 26}, {"type": "text", "text": "1. $\\tilde{\\Delta}_{i i_{0}}\\leq T^{-1/2}$ : in this case, the statement of the lemma is vacuous. 2. $\\widetilde{\\Delta}_{i i_{0}}>T^{-1/2}$ : in this case, by assumption, there are two consecutive $\\ell_{j_{\\star}},\\ell_{j_{\\star}+1}\\in{\\cal C}$ such that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\widetilde{\\Delta}_{i i_{0}}\\in\\left(\\frac{T^{1/2}}{\\ell_{j_{\\star}+1}},\\frac{T^{1/2}}{\\ell_{j_{\\star}}}\\right].\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "This is true due to the fact that that the sequence $\\ell_{j}$ spans from $T^{1/2}$ to $T$ . By Proposition 9, this can be equivalently expressed by saying that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widetilde{\\Delta}_{i i_{0}}\\in\\left(T^{-\\frac{j_{\\star+1}}{2\\lfloor\\log(T)\\rfloor}},T^{-\\frac{j_{\\star}}{2\\lfloor\\log(T)\\rfloor}}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Let us define the following family of events: ", "page_idx": 27}, {"type": "text", "text": "The probability of $E_{i i_{0}}$ is bounded by: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{P}(E_{i i_{0}})=\\mathbb{P}\\left(\\bigcup_{z=1}^{T}E_{i i_{0}}(z)\\right)=\\mathbb{P}\\left(\\bigcup_{z\\in\\mathcal{L}}E_{i i_{0}}(z)\\right)}}\\\\ &{}&{\\le\\displaystyle\\sum_{j=1}^{|\\mathcal{L}|}\\mathbb{P}(E_{i i_{0}}(\\ell_{j})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Here, we have applied the fact that, by design of the algorithm, the arms can only be discarded in fair steps for which $z\\in{\\mathcal{L}}$ and then a union bound. Here, we notice that by definition of the flitering condition, defining $t_{j}$ as the fair time-step where both arms have been played $\\ell_{j}$ times, this event can be again rewritten as: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\left\\{\\sum_{\\tau=1:\\tau}^{t_{j}}\\{\\mathcal{R}_{\\tau}(i)>\\mathcal{R}_{\\tau}(i_{0})\\}\\geq T^{2\\alpha_{j}}\\right\\},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where \u03b1j = $\\begin{array}{r}{\\alpha_{j}=\\frac{\\log(\\ell_{j})}{\\log(T)}-\\frac{1}{2}}\\end{array}$ . If we call $\\hat{\\mu}_{\\tau,i_{0}},\\hat{\\mu}_{\\tau,i}$ the empirical means of arms $i_{0},i$ after $\\tau$ pulls of each, the previous event can be interpreted as the time in which the random walk given by the difference of the rewards of the two arms stays in $(-\\infty,0]$ : ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{\\tau=1:\\tau\\mathrm{~fair~}}^{t_{j}}\\{\\mathcal{R}_{\\tau}(i)>\\mathcal{R}_{\\tau}(i_{0})\\}=\\sum_{\\tau=1}^{\\ell_{j}}\\mathbf{1}\\left\\{\\hat{\\mu}_{\\tau,i}\\geq\\hat{\\mu}_{\\tau,i_{0}}\\right\\}}}\\\\ &{}&{\\qquad=\\displaystyle\\sum_{\\tau=1}^{\\ell_{j}}\\mathbf{1}_{(-\\infty,0]}\\left(\\underbrace{\\sum_{k=1}^{\\tau}r_{i,k}-\\sum_{j=1}^{\\tau}r_{1,k}}_{G_{\\tau}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $\\sum_{k=1}^{\\tau}r_{i,k}$ is the cumulative reward of arm $i$ and $\\sum_{k=1}^{\\tau}r_{1,k}$ is the cumulative reward of arm $i_{0}$ . Th erefore, we have written this quantity as the time  spent by the random walk $G_{\\tau}$ in the interval $(-\\infty,0]$ , for $\\tau=1,\\ldots,\\ell_{j}$ . The drift term for this random walk is given by: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}[r_{i,k}-r_{1,k}]=\\mu_{i_{0}}-\\mu_{i}=-\\Delta_{i i_{0}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Therefore, we can apply Theorem 8 for the following choice of parameters, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\alpha=\\alpha_{j}=\\frac{j}{2\\left\\vert\\log(T)\\right\\vert}+o(T^{-1/2})\\left(\\mathrm{Proposition}\\,9\\right)\\mathrm{,which\\,implies\\}n=\\left\\vert T^{1/2+\\alpha_{j}}\\right\\vert=\\ell_{j}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "(b) 2\u230aljo\u22c6g+(T1 )\u230b. We can use this choice since the drift is ", "page_idx": 27}, {"type": "equation", "text": "$$\n-\\Delta_{i i_{0}}=-\\underbrace{(4\\log(T)+2)}_{\\geq2\\sqrt{\\log(T)}+2}\\underbrace{\\widetilde{\\Delta}_{i i_{0}}}_{\\geq T^{-\\frac{j_{\\star}}{2\\lfloor\\log(T)\\rfloor}}},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "therefore the assumptions of the theorem are respected. ", "page_idx": 27}, {"type": "text", "text": "Applying the theorem, we have: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}(E_{i}(2\\ell_{j}))\\le3T^{-1/2+\\theta}=3T^{-1/2+\\frac{j_{\\star}+1}{2\\lfloor\\log(T)\\rfloor}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Summing over $j$ , we get, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{P}\\left(\\bigcup_{t=1}^{T}E_{1}(t)\\right)\\le\\displaystyle\\sum_{j=1}^{|\\mathcal{L}|}\\mathbb{P}(E_{1}(2\\ell_{j}))}&{}\\\\ &{\\leq3\\log(T)T^{-1/2+\\frac{j_{\\star}+1}{2\\,\\lfloor\\log(T)\\rfloor}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "To conclude, consider that, by definition , ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\ell_{j}=T^{1/2+\\frac{j}{2\\lfloor\\log(T)\\rfloor}};\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "this means that: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widetilde{\\Delta}_{i i_{0}}\\in\\left(T^{-\\frac{j_{\\star}+1}{2\\lfloor\\log(T)\\rfloor}},T^{-\\frac{j_{\\star}}{2\\lfloor\\log(T)\\rfloor}}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "so that, in particular, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\tilde{\\Delta}_{i i_{0}}^{-1}\\geq T^{\\frac{j_{\\star}}{2\\lfloor\\log(T)\\rfloor}}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "and ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\Delta_{i i_{0}}^{-1}\\geq(4\\log(T)+2)T^{\\frac{j_{\\star}}{2\\lfloor\\log(T)\\rfloor}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Substituting in the bound we have just found results in, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{3\\log(T)T^{-1/2+\\frac{j\\star+1}{2\\lfloor\\log(T)\\rfloor}}\\leq3\\log(T)(4\\log(T)+2)T^{-1/2}T^{\\frac{1}{2\\lfloor\\log(T)\\rfloor}}\\Delta_{i i_{0}}^{-1}}\\\\ &{\\leq6\\log(T)(4\\log(T)+2)T^{-1/2}\\Delta_{i i_{0}}^{-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Lemma 7. For any arm $i\\,\\in\\,[n]$ , the probability of the event $E_{i i_{0}}^{*}$ , corresponding to arm $i\\,\\in\\,[n]$ not being eliminated after $(8\\log(T)+4)T^{1/2}\\Delta_{i i_{0}}^{-1}$ pulls if there is an active arm $i_{0}$ with $\\Delta_{i i_{0}}:=$ $\\mu_{i_{0}}-\\mu_{i}>0$ is such that: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{P}(E_{i i_{0}}^{*})\\leq2T^{-1/2}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. Define $\\ell_{j},\\alpha_{j}$ as in the previous lemma, so that $\\begin{array}{r}{\\alpha_{j}=\\frac{\\log(\\ell_{j})}{\\log(T)}-\\frac{1}{2}}\\end{array}$ . As in the previous lemma, we define: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\widetilde{\\Delta}_{i i_{0}}=\\frac{\\Delta_{i i_{0}}}{4\\log(T)+2},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "and $j_{\\star}\\in\\{1,\\ldots\\lfloor\\log(T)\\rfloor\\}$ such that: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widetilde{\\Delta}_{i i_{0}}\\in\\left(T^{-\\frac{j_{\\star+1}}{2\\lfloor\\log(T)\\rfloor}},T^{-\\frac{j_{\\star}}{2\\lfloor\\log(T)\\rfloor}}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Here, remember that by definition of the filtering condition, defining $t_{j_{\\star}}$ as the fair timestep where both arms have been played $\\ell_{j_{\\star}}$ times, $E_{i^{*}i_{0}}$ is included in the following event: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left\\{\\sum_{\\tau=1:\\tau}^{t_{j_{\\star}}}\\{\\mathcal{R}_{\\tau}(i_{0})>\\mathcal{R}_{\\tau}(i)\\}\\geq T^{2\\alpha_{j_{\\star}}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "As before, this event can be interpreted as the difference between two random walks being negative, due to the fact that: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{\\tau=1:\\tau\\mathrm{~fair~}}\\{\\mathcal{R}_{\\tau}(i_{0})>\\mathcal{R}_{\\tau}(i)\\}=\\sum_{\\tau=1}^{\\ell_{j_{\\star}}}\\mathbf{1}\\left\\{\\hat{\\mu}_{\\tau,1}\\geq\\hat{\\mu}_{\\tau,i}\\right\\}}}\\\\ &{}&{\\qquad=\\displaystyle\\sum_{\\tau=1}^{\\ell_{j_{\\star}}}\\mathbf{1}_{(-\\infty,0]}\\left(\\sum_{k=1}^{\\tau}r_{i_{0},k}-\\sum_{j=1}^{\\tau}r_{i,k}\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "In this formulation, we have written the quantity of interest for the filtering condition after $\\ell_{j\\star}\\!+\\!1$ pulls as the time spent by the random walk $G_{\\tau}$ in the interval $(-\\infty,0]$ , for $\\tau=1,\\dots\\ell_{j_{\\star}+1}$ . This time, the drift term is: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{E}[r_{i_{0},k}-r_{i,k}]=\\mu_{i_{0}}-\\mu_{i}=\\Delta_{i i_{0}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Therefore, we can apply Theorem 7 for $\\alpha=\\alpha_{j_{\\star}}$ , since, by assumption, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\Delta_{i i_{0}}=\\underbrace{(4\\log(T)+2)}_{\\geq4\\log(T)}\\underbrace{\\tilde{\\Delta}_{i i_{0}}}_{\\geq T^{-\\frac{j_{\\star+1}}{2\\lfloor\\log(T)\\rfloor}}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "This theorem leads to: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(E_{i i_{0}}^{*}\\right)\\leq1-\\mathbb{P}\\left(\\sum_{\\tau=1}^{\\ell_{j_{\\star}+1}}\\mathbf{1}_{(-\\infty,0]}\\left(G_{\\tau}\\right)\\leq T^{2\\alpha_{j_{\\star}+1}}\\right)}\\\\ &{\\qquad\\quad\\qquad\\underset{\\leq}{\\mathrm{Thm.7}}2T^{-1/2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "To get the thesis, is sufficient to reformulate the critical time-step $\\ell_{j_{\\star}+1}$ in terms of $\\Delta_{i i_{0}}$ . By definition, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\widetilde{\\Delta}_{i i_{0}}\\le T^{-\\frac{j_{\\star}}{2\\lfloor\\log(T)\\rfloor}}=\\frac{T^{1/2}}{\\ell_{j_{\\star}}}\\le2\\frac{T^{1/2}}{\\ell_{j_{\\star}+1}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Therefore, $\\begin{array}{r}{\\Delta_{i i_{0}}\\leq(8\\log(T)+4)\\frac{T^{1/2}}{\\ell_{j_{\\star}+1}}}\\end{array}$ . From this, it immediately follows, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\ell_{j_{\\star}+1}\\leq(8\\log(T)+4)T^{1/2}\\Delta_{i i_{0}}^{-1},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "concluding the proof. ", "page_idx": 29}, {"type": "text", "text": "We are finally able to prove our main result about the instance-independent regret of the algorithm. Theorem 4. In the stochastic bandits with ranking feedback setting, when the noise is Gaussian, Algorithm 2 achieves $R_{T}\\leq62n^{4}\\log(T)^{2}T^{1/2}$ . ", "page_idx": 29}, {"type": "text", "text": "Proof. Fix a sub-optimal arm $i$ with corresponding gap $\\Delta_{i}$ with respect to the optimal arm and let $\\psi$ a parameter to be chosen later. Define, for every couple of indices $i_{1},i_{0}$ the event: ", "page_idx": 29}, {"type": "equation", "text": "$$\nE_{i_{1}i_{0}}^{\\psi}:=\\left\\{\\!\\!\\begin{array}{l l}{E_{i_{1}i_{0}}}&{\\mu_{i_{0}}-\\mu_{i_{1}}>\\psi}\\\\ {\\emptyset}&{\\mathrm{else},}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where the event $E_{i_{1}i_{0}}$ is defined as arm $i_{1}$ eliminating arm $i_{0}$ in some point of the process. The probability that at least one of this events verifies is bounded by lemma 6 and union bound with: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\Psi\\right):=\\mathbb{P}\\left(\\bigcup_{i_{1}=1,i_{0}=0}^{n}E_{i_{1}i_{0}}^{\\psi}\\right)\\leq3n(n-1)\\log(T)(4\\log(T)+2)T^{-1/2}\\psi^{-1},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "as their number is at most $n(n-1)/2$ . Therefore, under the complementary of $\\Psi$ , as no elimination with gap larger than $\\psi$ happens, we are sure that an arm $i^{\\star}$ with gap (w.r.t. the first arm) less than $(n-{\\bar{1}}){\\bar{\\psi}}$ survives until the last: in fact, at most $n-1$ eliminations may happen, and all between pair of arms with a difference at most $\\psi$ . ", "page_idx": 29}, {"type": "text", "text": "As the arm $i^{\\star}$ is active until the last, the probability of event $E_{i i^{\\star}}^{*}$ that $i$ survives for more than $(8\\log(T)+4)T^{1/2}\\Delta_{i i^{\\star}}^{-1}$ pulls is bounded by Lemma 7 with $2T^{-1/2}$ . Making the union bound over all possible values of $i^{\\star}$ , which is a random variable, we can say that the probability that any of this event happens is at most $2(n-1)T^{-1/2}$ . Summarizing, we have the following bound on $Z_{i}(T)$ : ", "page_idx": 29}, {"type": "text", "text": "1. $Z_{i}(T)\\leq T$ if either $\\Psi$ verifies, which happens with probability, ", "page_idx": 29}, {"type": "equation", "text": "$$\n3n(n-1)\\log(T)(4\\log(T)+2)T^{-1/2}\\psi^{-1},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "or if $E_{i i^{\\star}}$ verifies, which happens with probability at most $2(n-1)T^{-1/2}$ ", "page_idx": 29}, {"type": "text", "text": "2. $Z_{i}(T)\\leq T(8\\log(T)+4)T^{1/2}(\\Delta_{i}-(n-1)\\psi)^{-1}$ otherwise. The last comes just from the fact that $\\Delta_{i i^{\\star}}\\geq\\Delta_{i}-(n-1)\\psi$ , by definition of $i^{\\star}$ . ", "page_idx": 29}, {"type": "text", "text": "If we take $\\begin{array}{r}{\\psi=\\frac{\\Delta}{2(n-1)}}\\end{array}$ 2(n\u2206\u22121), it results in, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\Delta_{i}Z_{i}(T)]\\leq\\Delta_{i}T\\mathbb{P}(\\cup_{i_{1}=1,i_{0}=0}^{n}E_{i_{1}i_{0}}^{\\psi})+T\\Delta_{i}\\mathbb{P}(E_{i i^{\\star}})}\\\\ &{\\qquad\\qquad\\quad+\\,\\Delta_{i}T(8\\log(T)+4)T^{1/2}(\\Delta_{i}-(n-1)\\psi)^{-1}}\\\\ &{\\qquad\\qquad\\quad\\leq\\Delta_{i}T\\left(3n(n-1)\\log(T)(4\\log(T)+2)T^{-1/2}\\psi^{-1}+2n T^{-1/2}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad+\\,\\Delta_{i}T(8\\log(T)+4)T^{1/2}(\\Delta_{i}-(n-1)\\psi)^{-1}}\\\\ &{\\quad=\\frac{\\Delta}{2(n-1)}}\\\\ &{\\quad\\leq\\quad\\Delta_{i}T\\left(6n(n-1)^{2}\\log(T)(4\\log(T)+2)T^{-1/2}\\Delta_{i}^{-1}+2n T^{-1/2}\\right)}\\\\ &{\\quad+\\,2\\Delta_{i}T(8\\log(T)+4)T^{1/2}\\Delta_{i}^{-1}}\\\\ &{=6n(n-1)^{2}\\log(T)(4\\log(T)+2)T^{1/2}+2n\\Delta_{i}T^{1/2}+2T(8\\log(T)+4)T^{1/2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Being $\\Delta_{i}\\in(0,1)$ and $n\\geq2$ , the previous quantity is bounded by, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\Delta_{i}Z_{i}(T)]\\leq62n^{3}\\log(T)^{2}T^{1/2}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "The proof is completed by using the the Regret Decomposition Lemma Lattimore and Szepesvari [2017]. \u53e3 ", "page_idx": 30}, {"type": "text", "text": "C Proof for adversarial setting ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Theorem 5. In adversarial bandits with ranking feedback, there exists a constant $\\gamma\\in(0,1)$ such that no algorithm achieves $o(T)$ regret with respect to the best arm in hindsight with probability greater than $\\gamma$ . ", "page_idx": 30}, {"type": "text", "text": "Proof. This negative result follows from the impossibility to achieve $R_{T}\\,\\leq\\,C T$ regret by any algorithm, with $C$ properly set constant and probability $1-\\bar{\\epsilon}$ , in all three instances reported next. Please notice that, this result implies that even the no-regret property cannot be achieved in the bandit with ranking feedback setting. ", "page_idx": 30}, {"type": "text", "text": "Without loss of generality we consider rewards function bounded in [0, 10]. Consider three instances, with two arms $a_{0},a_{1}$ for each and the associated rewards, defined as follows: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname{Instance}\\mathbb{\\odot}:\\left\\{a_{0}:\\frac{1}{2}\\quad\\forall t\\in\\left[\\!\\!\\!\\begin{array}{r}{1}\\end{array}\\!\\!\\!\\right],\\quad\\frac{1}{2}\\quad\\forall t\\in\\left[\\!\\!\\!\\begin{array}{r}{2}\\end{array}\\!\\!\\!\\right],\\quad\\frac{1}{2}\\quad\\forall t\\in\\left[\\!\\!\\!\\begin{array}{r}{3}\\end{array}\\!\\!\\!\\right]}\\\\ {a_{1}:0\\quad\\forall t\\in\\left[\\!\\!\\!\\begin{array}{r}{1}\\end{array}\\!\\!\\!\\right],\\quad0\\quad\\forall t\\in\\left[\\!\\!\\!\\begin{array}{r}{2}\\end{array}\\!\\!\\!\\right],\\quad0\\quad\\forall t\\in\\left[\\!\\!\\!\\begin{array}{r}{3}\\end{array}\\!\\!\\!\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname{Instance}\\bigodot:\\left\\{a_{0}:\\delta\\quad\\forall t\\in\\ensuremath{\\left[1\\right]},\\quad0\\quad\\forall t\\in\\ensuremath{\\left[2\\right]},\\quad0\\quad\\forall t\\in\\ensuremath{\\left[3\\right]}}\\\\ {a_{1}:0\\quad\\forall t\\in\\ensuremath{\\left[\\frac{1}{2}\\right]},\\quad1\\quad\\forall t\\in\\ensuremath{\\left[2\\right]},\\quad1\\quad\\forall t\\in\\ensuremath{\\left[3\\right]}}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "equation", "text": "$$\n\\operatorname{Instance}\\textcircled{3}:\\left\\{a_{0}:\\delta\\quad\\forall t\\in\\ensuremath{\\left[\\!\\!\\begin{array}{l l l}{1}\\end{array}\\!\\!\\right]},\\quad0\\quad\\forall t\\in\\ensuremath{\\left[\\!\\!\\begin{array}{l l l}{2}\\end{array}\\!\\!\\right]},\\quad10\\quad\\forall t\\in\\ensuremath{\\left[\\!\\!\\begin{array}{l}{3}\\end{array}\\!\\!\\right]}}\\\\ {a_{1}:0\\quad\\forall t\\in\\ensuremath{\\left[\\!\\!\\begin{array}{l l l}{1}\\end{array}\\!\\!\\right]},\\quad1\\quad\\forall t\\in\\ensuremath{\\left[\\!\\!\\begin{array}{l l l}{2}\\end{array}\\!\\!\\right]},\\quad0\\quad\\forall t\\in\\ensuremath{\\left[\\!\\!\\begin{array}{l}{3}\\end{array}\\!\\!\\!\\right]}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where phase $\\boxed{1}$ is made by the first $T/4$ rounds, phase $\\boxed{2}$ is made by the next $T/4$ rounds, phase 3 is made by the last $T/2$ rounds and $\\delta$ is near to $0$ . ", "page_idx": 30}, {"type": "text", "text": "In phase $\\boxed{1}$ all the instances have the same ranking feedback, as the first action gives higher rewards with respect to the second one. To make instance $\\textcircled{1}$ receive $R_{T}\\leq C T$ , it is necessary: ", "page_idx": 30}, {"type": "equation", "text": "$$\n{\\frac{1}{2}}T-{\\frac{1}{2}}\\mathbb{E}[n_{a_{0}}]\\leq C T\\Rightarrow\\mathbb{E}[n_{a_{0}}]\\geq(1-2C)T\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $n_{a_{0}}$ is the number of times the first arm has been pulled, and the expected value is taken on the randomization of the algorithm. From previous equation we obtain that in all instances: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[n_{a_{0}}^{\\left[1\\right]}\\right]\\geq(1-2C)T-\\frac{3}{4}T=(1-C_{1})T/4\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $C_{1}\\,=\\,8C$ , $n_{a_{0}}$ is the number of time the first arm has to be pulled in phase $\\boxed{1}$ and the inequality is computed considering that $a_{0}$ is played in all the next phases. By reverse Markov inequality: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(n_{a_{0}}^{\\boxed{1}}>(1-\\bar{C}_{1})T/4\\right)\\geq\\frac{\\bar{C}_{1}-C_{1}}{C_{1}}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Setting the probability equal to $9/10$ we obtain: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\bar{C}_{1}=10C_{1}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "from which follow that with probability $9/10$ we have: ", "page_idx": 31}, {"type": "equation", "text": "$$\nn_{a_{0}}^{\\boxed{1}}>(1-10C_{1})T/4\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "and consequently: ", "page_idx": 31}, {"type": "equation", "text": "$$\nn_{a_{1}}^{\\boxed{1}}\\le10C_{1}T/4.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We observe that in the second phase, instances $\\circled{2}$ and $\\circled{3}$ have the same feedback. Proceeding as done before, to make instance $\\circled{2}$ receive $R_{T}\\leq C T$ it is necessary: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\frac{3}{4}T-\\mathbb{E}\\left[n_{a_{1}}\\right]\\leq C T\\Rightarrow\\mathbb{E}[n_{a_{1}}]\\geq\\left(\\frac{3}{4}-C\\right)T\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "From previous equation we obtain that in instances $\\circled{2}$ and $\\circled{3}$ : ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[n_{a_{1}}^{\\boxed{2}}\\right]\\geq\\left(\\frac{3}{4}-C\\right)T-T/2=(1-C_{2})T/4\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where the inequality is computed considering that $a_{1}$ is played in the next phases and $C_{2}=4C$ . By reverse Markov inequality, we obtain that, with probability $9/10$ : ", "page_idx": 31}, {"type": "equation", "text": "$$\nn_{a_{1}}^{\\boxed{2}}>(1-10C_{2})T/4\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "and consequently: ", "page_idx": 31}, {"type": "equation", "text": "$$\nn_{a_{0}}^{\\boxed{2}}\\le10C_{2}T/4\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We neglect the $\\delta$ value for now, as it can be chosen to be insignificant with respect to the previous computation.   \nNow we focus on the third phase, in which instance $\\circled{2}$ should play: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[n_{a_{1}}^{\\left[3\\right]}\\right]\\geq\\left(\\frac{3}{4}-C\\right)T-T/4=(1-C_{3})T/2,\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $C_{3}=2C$ . By reverse Markov inequality, we obtain that, with probability $9/10$ : ", "page_idx": 31}, {"type": "equation", "text": "$$\nn_{a_{1}}^{\\boxed{3}}>(1-10C_{3})T/2\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "and consequently: ", "page_idx": 31}, {"type": "equation", "text": "$$\nn_{a_{0}}^{\\boxed{3}}\\le10C_{3}T/2\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Now, we compute the number of rounds needed in the third instance to switch the ranking in the third phase, namely $q$ . Notice that, until this switch, the last two instances receive the same feedback. We compute $q$ in the best-case scenario (that is, when small $q$ value is sufficient to allow the switch) that satisfies the constraints previously shown. Precisely, $q$ is computed so that the empirical mean of arm $a_{0}$ is greater then the arm $a_{1}$ one, given that $n_{a_{0}}^{\\boxed{1}}>(1-10C_{1})T/4$ and $n_{a_{1}}^{\\boxed{2}}>(1-10C_{2})T/4$ . Formally: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\frac{0(1-10C_{1})T/4+10q}{q+(1-10C_{1})T/4}\\ge\\frac{0C_{1}10T/4+(1-10C_{2})T/4+0T/2}{10C_{1}T/4+(1-10C_{2})T/4+T/2}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We now show that for proper $C$ value we can lower bound the right side with $\\frac{1}{4}$ . In particular: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\frac{0C_{1}10T/4+(1-10C_{2})T/4+0T/2}{10C_{1}T/4+(1-10C_{2})T/4+T/2}>\\frac{1}{4}\\Rightarrow C<1/200\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "which means that, for $C<\\textstyle{\\frac{1}{200}}$ , we can substitute the right side of the equation with $\\textstyle{\\frac{1}{4}}$ to simplify the computation. Moreover, notice that gap between $\\textstyle{\\frac{1}{4}}$ and $\\begin{array}{r}{\\frac{0C_{1}10T/4+(1-10C_{2})T/4+0T\\dot{/}2}{10C_{1}T/4+(1-10C_{2})T/4+T/2}}\\end{array}$ allowed us to neglect the computations with $\\delta$ . Then: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\frac{0(1-10C_{1})T/4+10q}{q+(1-10C_{1})T/4}\\geq1/4\\Rightarrow q\\geq\\frac{4}{39}\\left(\\frac{1}{4}-20C\\right)T/4\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "To achieve a contradiction, it sufficient to find $C$ so that $q+n_{a_{1}}^{\\boxed{3}}>T/2$ ; indeed, the previous inequality shows the impossibility to gain enough rewards to make the ranking change and, at the same time, guarantee the minimum rewards to make instance $\\circled{2}$ no-regret. Given that the ranking switch is a necessary condition to make instance $\\circled{3}$ no-regret, the result of impossibility follows for: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\frac{4}{39}\\left(\\frac{1}{4}-20C\\right)T/4+(1-20C)T/2>T/2\\Rightarrow C<\\frac{1}{1640}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "To conclude the proof, we show that the intersection between the events derived by reverse Markov inequality (namely $E_{i}$ with $i\\in[3],$ ) holds with constant probability: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\displaystyle\\left(\\bigcap_{i\\in[3]}E_{i}\\right)=1-\\mathbb{P}\\left(\\bigcup_{i\\in[3]}E_{i}^{c}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\geq1-\\displaystyle\\sum_{i\\in[3]}\\mathbb{P}(E_{i}^{c})}\\\\ &{\\qquad\\qquad=1-\\displaystyle\\frac{3}{10}=\\frac{7}{10}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where the inequality holds by Union Bound. Substituting all the previous results in the definition of regret we obtain, with probability $\\textstyle{\\frac{7}{10}}=1-{\\bar{\\epsilon}}$ and $C<{\\frac{1}{1640}}$ , $\\bar{R_{T}}\\geq C T=\\Omega(T)$ which concludes the proof. \u53e3 ", "page_idx": 32}, {"type": "text", "text": "D Numerical evaluation ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "This section presents a numerical evaluation of the algorithms proposed in the paper for the stochastic settings, namely, DREE and R-LPE. The goal of such a study is to show two crucial results: firstly, the comparison of our algorithms with a well-known bandit baseline, and secondly, the need to develop distinct algorithms tailored for instance-dependent and instance-independent scenarios. ", "page_idx": 32}, {"type": "text", "text": "To establish a benchmark for comparison, we consider the EC (Explore-Then-Commit) algorithm, which is one of the most popular algorithms among the explore-then-commit class providing sublinear regret guarantees. In the following, we evaluate the DREE algorithm with different choices of the $\\delta$ parameter in the function $f(t)^{\\overline{{\\mathbf{\\alpha}}}}=\\log(t)^{1+\\delta}$ ; precisely, we choose $\\delta\\;\\in\\;\\{1.0,1.5,2.0\\}$ . Furthermore, we consider four stochastic instances whose specific parameters are discussed below. In all these instances, we assume the rewards to be drawn from Gaussian random variables with unit variance, i.e., $\\sigma^{2}=1$ , and we let the time horizon be equal to $T=2\\cdot10^{5}$ . Finally, for each algorithm, we evaluate the cumulative regret averaged over 50 runs. ", "page_idx": 32}, {"type": "text", "text": "We structure the presentation of the experimental results into two groups. In the first, the instances have a small $\\Delta_{\\mathrm{min}}$ , while in the second, the instances have a large $\\Delta_{\\mathrm{min}}$ . ", "page_idx": 32}, {"type": "text", "text": "Small values of $\\Delta_{\\mathrm{min}}$ We focus on two instances with $\\Delta_{\\mathrm{min}}\\,<\\,0.05$ . In the first of these two instances, we consider $n=4$ arms, and a minimum gap of $\\Delta_{\\mathrm{min}}=0.03$ . In the second instance, we consider $n=6$ arms, with $\\Delta_{\\mathrm{min}}=0.03$ . The expected values of the rewards of each arm are reported in Section D.1, while the experimental results in terms of average cumulative regret are reported in Figures 1\u20132. We observe that in the first instance (see Figure 1) all the DREE algorithms exhibits a linear regret bound, confirming the strong sensitivity of this family of algorithms on the parameter $\\Delta_{\\mathrm{min}}$ in terms of regret bound. In contrast, the R-LPE algorithm exhibits better performances in terms of regret bound, as its theoretical guarantee are independent on the values of $\\Delta_{\\mathrm{min}}$ . Furthermore, Figure 2 shows that the DREE algorithms (with $\\delta\\in\\mathrm{1.0}$ , 1.5) achieve a better regret bound when the number of arms is increased. Indeed, these regret bounds are comparable to the ones achieved by the R-LPE algorithm. The previous result is reasonable as the presence of $\\Delta_{i}$ -s in the regret bound lowers the dependence on the number of arms. It is worth noticing that all our algorithms outperform the baseline EC. ", "page_idx": 32}, {"type": "image", "img_path": "aCaspFfAhG/tmp/a679a4f112ee62a3317c93abfb486bfd9b80c67a68835291e6dc31e483e594d7.jpg", "img_caption": ["Figure 1: Instance with $\\Delta_{\\mathrm{min}}=0.03$ and all the gaps small. "], "img_footnote": [], "page_idx": 33}, {"type": "image", "img_path": "aCaspFfAhG/tmp/72d702257f05c3198d278ec0257ea9307290d16a8c62117eefb64b278432332f.jpg", "img_caption": ["Figure 2: Instance with $\\Delta_{\\mathrm{min}}=0.03$ and the other gaps big "], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "Large values of $\\Delta_{\\mathrm{min}}$ We focus on two instances with $\\Delta_{\\mathrm{min}}\\,\\geq\\,0.25$ . In the first instance, we consider $n=4$ arms with a minimum gap of $\\Delta_{\\mathrm{min}}=0.5$ among their expected rewards. In the second instance, we instead consider a larger number of arms, specifically $n=8$ , with a minimum gap equal to $\\Delta_{\\mathrm{min}}=0.25$ . The expected values of the rewards are reported in Section D.1, while the experimental results in terms of average cumulative regret are provided in Figures 3\u20134. As it clear from both Figures 3\u20134 when $\\Delta_{\\mathrm{min}}$ is sufficiently large, the DREE algorithms (with $\\delta\\in\\{1.0,1.5\\})$ achieves better performances with respect both the EC and R-PLE algorithms in terms of cumulative regret. Furthermore, there is empirical evidence that a small $\\delta$ guarantees better performance, which is reasonable according to theory. Indeed, when $\\delta$ is small, the function $f(t)$ , which drives the exploration, is closer to a logarithm. Also, as shown in Corollary 1, when $\\Delta_{\\mathrm{min}}$ is large enough, the parameter $\\delta$ affects the dimension of $C(f,\\Delta_{i})$ more weakly, which results in a better regret bound. ", "page_idx": 33}, {"type": "image", "img_path": "aCaspFfAhG/tmp/532c1e8fa4781e5534cd2634f5e84af90178a4ea292204c2c0a64ef9d9e496ed.jpg", "img_caption": ["Figure 3: Instance with $\\Delta_{\\mathrm{min}}=0.5$ . "], "img_footnote": [], "page_idx": 34}, {"type": "image", "img_path": "aCaspFfAhG/tmp/08961e159bbe89cf177fddf5825783e86d53f536a8c2b6c2d65e58ad8817d328.jpg", "img_caption": ["Figure 4: Instance with $\\Delta_{\\mathrm{min}}=0.25$ . "], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "D.1 Experiments ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "For the sake of clarity, we report in the followings additional details on the four instances presented in Figures 1,2,3,4. Notice that, all the plots present the cumulative regret averaged for 50 runs, with $95\\%$ confidence interval. Furthermore: ", "page_idx": 34}, {"type": "text", "text": "\u2022 Instance of Figure $^{\\,l}$ : time horizon $T=2\\cdot10^{5}$ , arms $n=4$ , mean reward vector ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\pmb{\\mu}=[0.9,1.05,1.12,1.15],\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "unitary variance for each arm, $\\Delta_{\\mathrm{min}}=0.03$ ; ", "page_idx": 34}, {"type": "text", "text": "\u2022 Instance of Figure 2: time horizon $T=2\\cdot10^{5}$ , arms $n=6$ , mean reward vector ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\pmb{\\mu}=[0.03,0.07,0.1,0.08,0.97,1],\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "unitary variance for each arm, $\\Delta_{\\mathrm{min}}=0.03$ ; ", "page_idx": 34}, {"type": "text", "text": "\u2022 Instance of Figure $^3$ : time horizon $T=2\\cdot10^{5}$ , arms $n=4$ , mean reward vector ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\pmb{\\mu}=[0.05,0.25,0.5,1.0],\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "unitary variance for each arm, $\\Delta_{\\mathrm{min}}=0.5$ ; \u2022 Instance of Figure $^{4}$ : time horizon $T=2\\cdot10^{5}$ , arms $n=8$ , mean reward vector ", "page_idx": 35}, {"type": "text", "text": "$\\pmb{\\mu}=[0.05,0.05,0.1,0.15,0.25,0.5,0.75,1.0]$ , ", "page_idx": 35}, {"type": "text", "text": "unitary variance for each arm, $\\Delta_{\\mathrm{min}}=0.25$ ; ", "page_idx": 35}, {"type": "text", "text": "D.2 Detailed explanation of the experiments ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "In this section, we report all the details of the experiments performed in the paper. These are important to ensure the truthfullness of the results and the claims based on empirical validation. ", "page_idx": 35}, {"type": "text", "text": "Training details In the main paper we have presented four experiments, each corresponding to a different environment. Each experiment is performed for fifty random seeds, ad the computation is split in 10 parallel processes by the library joblib. The overall computational time for one experiment is around 337.92 seconds, that is roughly five minutes and one half. ", "page_idx": 35}, {"type": "text", "text": "Compute As stated, the numerical simulations resulted to be very fast. For this reason, it was not necessary to run them on a server, and we used a personal computer with the following specifications: ", "page_idx": 35}, {"type": "text", "text": "\u2022 CPU: 11th Gen Intel(R) Core(TM) i7-1165G7 2.80 GHz   \n\u2022 RAM: 16,0 GB   \n\u2022 Operating system: Windows 11   \n\u2022 System type: 64 bit ", "page_idx": 35}, {"type": "text", "text": "Reproducibility Due to the stochastic nature of the bandit problem, all the simulations have been repeated several times. We have performed all the experiments with 50 different random seeds, corresponding precisely to the first 50 natural numbers. The seed influences the generation of the reward by the environment, while all algorithms proposed, being deterministic, are independent on the seed. ", "page_idx": 35}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: All we say in the abstract is done in the work. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 36}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Justification: We clarifiy that the contribution is theoretical, which is the only aspect that can be seen as a limitation of this work. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 36}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We leave the complete and formal proofs in the appendix. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 37}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Justification: In the appendix we report every detail of the experiments, even the amount of CPU time used. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 37}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: We provide the code. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 38}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: These information are reported in the appendix. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 38}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: Every experiment reports, together with the regret curves, a shaded area indicating the uncertainty. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 38}, {"type": "text", "text": "", "page_idx": 39}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: These information are reported in the appendix. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 39}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: Experiments are just Python simulation, no concerns about real data. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 39}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: Only theoretical work. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 39}, {"type": "text", "text": "", "page_idx": 40}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: We did not use data. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 40}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: We only use standard Python libraries to make the experiments. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 40}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 41}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: Answered in previous questions. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 41}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: We did not do any crowdsourcing. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 41}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: We have no study participants. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 41}]