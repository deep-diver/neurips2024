[{"figure_path": "EHXyeImux0/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration of our problem statement on a simple example where two tokenizers are trained on different mixtures of English and Python data. During training, the BPE algorithm iteratively finds the pair of tokens with the highest frequency in the training data, adds it to the merge list, then applies it to the dataset before finding the next highest-frequency pair. To encode text at inference time, the learned merge rules are applied in order. The resulting order of merge rules is extremely sensitive to the proportion of different data categories present. Our goal is to solve for these proportions, a task which we call data mixture inference.", "description": "This figure illustrates the core idea of the paper: data mixture inference using byte-pair encoding (BPE) tokenizers.  Two BPE tokenizers are trained on different ratios of English and Python code. The figure shows how the order of merge rules learned by the BPE algorithm directly reflects the proportion of each language in the training data. The authors aim to reverse this process \u2013 given a tokenizer's merge list, infer the original data proportions.", "section": "1 Introduction"}, {"figure_path": "EHXyeImux0/figures/figures_1_2.jpg", "caption": "Figure 2: Training data mixture predictions for several commercial tokenizers. Complete results over 112 languages and 5 domains are given in \u00a7C; categories are grouped here for readability. We confirm that GPT-2 was trained overwhelmingly on English (99%), while GPT-3.5 is the first model in the GPT series to train on substantial code data (63%). GPT-40 is much more multilingual than its predecessors, with 39% of its corpus being non-English text. LLAMA is also multilingual, but focuses on languages using Latin or Cyrillic scripts (note this category in the figure excludes English). LLAMA 3* results are only based on the last 27,744 merges (the first 100K are copied from GPT-3.5), which we observe was primarily for multilingual adaptation.", "description": "This figure shows the results of applying the data mixture inference attack on several commercial language models' tokenizers.  It displays the proportion of English, code, Latin/Cyrillic languages, and other languages in the training data for each tokenizer.  Key findings highlighted are the overwhelmingly English training data for GPT-2, the significant code data used in GPT-3.5, the increased multilingualism in GPT-40 and LLAMA 3, and the specific language focus of LLAMA (Latin/Cyrillic scripts).", "section": "5 Attacking commercial tokenizers"}, {"figure_path": "EHXyeImux0/figures/figures_2_1.jpg", "caption": "Figure 2: Training data mixture predictions for several commercial tokenizers. Complete results over 112 languages and 5 domains are given in \u00a7C; categories are grouped here for readability. We confirm that GPT-2 was trained overwhelmingly on English (99%), while GPT-3.5 is the first model in the GPT series to train on substantial code data (63%). GPT-40 is much more multilingual than its predecessors, with 39% of its corpus being non-English text. LLAMA is also multilingual, but focuses on languages using Latin or Cyrillic scripts (note this category in the figure excludes English). LLAMA 3* results are only based on the last 27,744 merges (the first 100K are copied from GPT-3.5), which we observe was primarily for multilingual adaptation.", "description": "This figure shows the training data mixture proportions predicted by the authors' method for several commercially available large language models (LLMs).  The models are categorized along the x-axis into English, Code, Languages with Latin or Cyrillic scripts, and Other Languages. The y-axis represents the proportion of the training data that fell into each category. The figure visually demonstrates the varying levels of multilingualism and code usage in the training data of different models.  For example, it highlights GPT-2's heavy reliance on English, GPT-3.5's significant code usage, and GPT-40's increased multilingualism compared to its predecessors.  The figure also shows that Llama focuses on languages using Latin or Cyrillic scripts.", "section": "Inferring tokenizer training data mixtures"}, {"figure_path": "EHXyeImux0/figures/figures_3_1.jpg", "caption": "Figure 3: Illustration of our method on a simple example. We know that after applying in the first t \u2013 1 merges to the training data, the tth merge must be the most common pair. More explicitly, this means that ai should give a vector in which the value corresponding to the true next merge is the maximum. Our attack collects these inequalities at every time step to construct the linear program.", "description": "This figure illustrates how the proposed data mixture inference attack works.  It shows a simplified example with two languages, English and Spanish.  The BPE tokenizer learns merge rules based on the frequency of token pairs in the training data.  The attack uses these ordered merge rules to create linear program constraints. Each constraint reflects the most frequent pair at each step. Solving the linear program estimates the proportion of each language in the original training data.", "section": "3.1 Data mixture inference via linear programming"}, {"figure_path": "EHXyeImux0/figures/figures_9_1.jpg", "caption": "Figure 4: Performance remains much better than random even with large amounts of unknown data.", "description": "This figure shows the robustness of the data mixture inference attack when some languages are omitted from the training data. The x-axis represents the sum of probabilities of omitted languages, and the y-axis represents the mean squared error (MSE) of the prediction on the remaining languages.  The plot demonstrates that even when a significant portion of the languages are unknown, the attack's performance is still substantially better than random guessing.  This highlights the robustness of the method to handle cases where not all data categories are explicitly accounted for in the analysis.", "section": "Robustness analysis"}, {"figure_path": "EHXyeImux0/figures/figures_16_1.jpg", "caption": "Figure 5: Relationship between a language's proportion in training and the resulting tokenizer's encoding efficiency on that language, shown for mixtures of n = 10 languages. The encoding efficiency is defined as the byte-to-token ratio of a given tokenizer on a given language, normalized by that of a tokenizer trained only on that language. While more training data leads to better encoding efficiency, the correlation is not strong enough to recover a prediction nearly as precise as our attack. A baseline based on this relationship achieves log10 MSE of -2.22, compared to our attack's -7.66.", "description": "This figure shows the relationship between the proportion of a language in the training data and the encoding efficiency of that language by the resulting tokenizer.  The encoding efficiency is calculated as the ratio of bytes to tokens, normalized against a tokenizer trained solely on that language.  While higher proportions generally correlate with greater efficiency, the relationship isn't strong enough to accurately predict language proportions in the training data, unlike the approach presented in the paper. The figure highlights the superior accuracy of the proposed attack compared to a baseline approach that uses encoding efficiency.", "section": "4.3 Results"}, {"figure_path": "EHXyeImux0/figures/figures_16_2.jpg", "caption": "Figure 6: Scaling the amount of data used for estimating pair frequencies (\u00a7B.4.1), for mixtures of n = 5 categories. Sampling more data per category produces more precise inferences.", "description": "This figure shows how the accuracy of the data mixture inference attack changes with the amount of data used for estimating pair frequencies.  The experiment was conducted using mixtures of 5 categories. As expected, increasing the amount of data per category leads to significantly more precise inferences (lower MSE).", "section": "B.4.1 How many data samples should we use from each category?"}, {"figure_path": "EHXyeImux0/figures/figures_16_3.jpg", "caption": "Figure 7: Scaling the top T merges used in the merge list (\u00a7B.4.2). For mixtures of more categories (larger n), considering more merges (larger T) becomes more useful.", "description": "This figure shows how the accuracy of the data mixture inference attack changes depending on the number of merges (T) considered from the tokenizer's merge list. The results are shown for different numbers of categories (n) in the mixture. As expected, increasing the number of merges considered improves the accuracy of the attack, particularly when there are more categories in the mixture.", "section": "B.4.2 How many merges should we consider?"}, {"figure_path": "EHXyeImux0/figures/figures_17_1.jpg", "caption": "Figure 6: Scaling the amount of data used for estimating pair frequencies (\u00a7B.4.1), for mixtures of n = 5 categories. Sampling more data per category produces more precise inferences.", "description": "This figure shows how the accuracy of the data mixture inference attack changes with the amount of data used for estimating pair frequencies, when using mixtures of 5 categories. As expected, increasing the amount of data per category improves the accuracy of the attack, as shown by the decrease in Mean Squared Error (MSE).", "section": "B.4.1 How many data samples should we use from each category?"}, {"figure_path": "EHXyeImux0/figures/figures_17_2.jpg", "caption": "Figure 6: Scaling the amount of data used for estimating pair frequencies (\u00a7B.4.1), for mixtures of n = 5 categories. Sampling more data per category produces more precise inferences.", "description": "This figure shows how the accuracy of the data mixture inference attack changes with the amount of data used for estimating pair frequencies.  The experiment uses mixtures of 5 categories.  As expected, using more data leads to more accurate results, indicated by lower MSE values.", "section": "B.4.1 How many data samples should we use from each category?"}]