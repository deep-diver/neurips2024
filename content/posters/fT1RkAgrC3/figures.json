[{"figure_path": "fT1RkAgrC3/figures/figures_3_1.jpg", "caption": "Figure 1: The overview of over-parameter distillation framework (OPDF) for knowledge ditillation. a, We use MPO decomposition to realize the over-parameter procedure for the student model. The auxiliary tensors of the student model are trained to imitate the auxiliary tensors of the teacher model closely. b, We present an illustrative example of MPO decomposition. A parameter matrix WIxJ is decomposed into central tensor and auxiliary tensors.", "description": "This figure provides a high-level overview of the Over-parameterization Distillation Framework (OPDF) proposed in the paper.  Panel (a) shows the overall workflow: a pre-trained teacher model and a student model are input into the OPDF, which uses Matrix Product Operators (MPO) to over-parameterize the student model. The over-parameterized student model is then trained on a target task, with a tensor alignment loss (LAux) used to ensure consistency between the student and teacher models. Panel (b) details the MPO decomposition process, illustrating how a parameter matrix is decomposed into a central tensor and auxiliary tensors. The central tensor and auxiliary tensors are then optimized and re-combined to generate an enhanced student model.", "section": "4 Method"}]