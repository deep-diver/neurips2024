[{"figure_path": "e5Mv7iWfVW/figures/figures_2_1.jpg", "caption": "Figure 1: Illustration of how the angle between weight vector pairs in the query or the key affects ROPE. The larger absolute cosine similarity | cos \u03b1|, the direction of the projected 2D vector is more fixed which leads to high attention on certain relative positions regardless of the input.", "description": "This figure illustrates how the angle between weight vector pairs in the query or key of the RoPE mechanism affects the attention mechanism in LLMs.  The cosine similarity between the pairs determines how fixed the direction of the projected 2D vector is. A larger absolute cosine similarity means a more fixed direction and consequently, greater attention to certain relative positions, irrespective of the input.  In contrast, smaller cosine similarity leads to a more flexible attention pattern.", "section": "3 Investigating the Angle between Weight Vector Pairs in ROPE"}, {"figure_path": "e5Mv7iWfVW/figures/figures_4_1.jpg", "caption": "Figure 2: Attention visualization of the attention heads with the largest or the smallest average absolute cosine similarity | cos a| across the weight vector pairs in RoPE, where the key is on the left, and the query is on the right. The lower transparency means a higher attention score. We demonstrate the results for the first layer of Llama-2-7b-chat and Mistral-7B-Instruct-v0.2. The results empirically support that higher | cos a| leads to attention on basic syntactic information and lower | cos a| leads to attention on high-level semantic information. For more results of different models at different layers please refer to Appendix B.", "description": "This figure visualizes attention patterns in the first layer of two LLMs (Llama-2-7b-chat and Mistral-7B-Instruct-v0.2) for the question \"What is the capital of France?\"  It compares attention heads with the highest average absolute cosine similarity (|cos a|) between weight vector pairs in RoPE to those with the lowest.  Higher |cos a| values indicate attention focused on basic syntactic elements (prepositions, articles, etc.), while lower |cos a| values show attention on higher-level semantic information related to the meaning of the sentence.  The visualization uses transparency to represent attention strength; darker lines mean stronger attention.", "section": "3.2 Attention Visualization for Different Attention Heads"}, {"figure_path": "e5Mv7iWfVW/figures/figures_5_1.jpg", "caption": "Figure 3: We report the average absolute cosine similarity of the query and the key across the layers of different LLMs. We show that, for all the LLMs we investigate, the average absolute cosine similarity drastically decreases after the first several layers and stays small until the last layer.", "description": "This figure displays the average absolute cosine similarity of query and key weight vector pairs across different layers of various LLMs.  The plots show a consistent trend across all models: a significant decrease in cosine similarity after the initial few layers, followed by a relatively stable and low similarity for the remaining layers, before a slight increase at the very end. This suggests that different layers of LLMs focus on different aspects of information processing, potentially with early layers focusing on syntactic details and later layers on high-level semantic relations.", "section": "3.3 Analyzing Weight Vector Pair Angles Across the Layers"}, {"figure_path": "e5Mv7iWfVW/figures/figures_5_2.jpg", "caption": "Figure 4: We show the cosine similarity of each weight vector pair in the query and the key of different layers of the Llama2-7b [35]. Each column corresponds to different layers. The top row is the results for the query and the bottom row is the results for the key. The attention heads are separated with vertical red lines. We show that even within one head, the cosine similarity of different weight vector pairs differs. More results on other layers and other models are in Appendix B.", "description": "This figure visualizes the cosine similarity of weight vector pairs within the query and key components of the Llama2-7b model's self-attention mechanism across different layers (1st, 5th, 10th, and 20th). Each column represents a layer, with the top row showing query vector pairs and the bottom row showing key vector pairs.  Vertical red lines delineate individual attention heads. The visualization demonstrates the variability in cosine similarity, even within a single attention head, highlighting the diverse roles of different weight vector pairs in processing positional information.", "section": "3.4 Investigating the Cosine Similarity Across Weight Vector Pairs"}, {"figure_path": "e5Mv7iWfVW/figures/figures_6_1.jpg", "caption": "Figure 6: Comparison between the base model Mistral-7B [16] and fine-tuned version WizardLM-2 [38]. For each sub-figure, the scatter figure in the middle demonstrates the cosine similarity between weight vector pairs where each point corresponds to a weight vector pair; the x-axis corresponds to the results for the base model, and the y-axis corresponds to the fine-tuned model. In the histogram on the top and left, we report the average L2 weight distance between the weight vectors of the pre-trained model and the fine-tuned model. The y-axis of the histogram on the top and the x-axis of the histogram on the left correspond to the L2 weight distance. Generally, fine-tuning merely changes the angle between weight vector pairs. Besides the first several layers, the weight change mainly happens on weight vector pairs that are nearly orthogonal. More results are provided in Appendix B.", "description": "This figure compares the base model Mistral-7B with its fine-tuned version, WizardLM-2, focusing on the cosine similarity between weight vector pairs in their query and key components.  The scatter plots visualize this similarity, showing that fine-tuning primarily alters the angles between these pairs rather than significantly changing their magnitudes. Histograms illustrate the average L2 distance between weights in the base and fine-tuned models, confirming this observation and highlighting that changes are concentrated in nearly orthogonal vector pairs, especially in layers beyond the initial few. Appendix B offers further results.", "section": "4 Reducing the Trainable Parameters During Fine-tuning"}, {"figure_path": "e5Mv7iWfVW/figures/figures_6_2.jpg", "caption": "Figure 5: Correlation between cosine similarity of the query and key weight vector pair of the 1st layer of Llama2-7b. Each dot represents two corresponding weight vector pairs in query and key. The x-axis corresponds to the cosine similarity of the weight vector pair in the query and the y-axis corresponds to the cosine similarity of the weight vector pair in the key. (Pearson's r is 0.86)", "description": "This figure shows the strong positive correlation between the cosine similarity of weight vector pairs in the query and key of the first layer of the Llama2-7b model.  Each point represents a pair of weight vectors from the query and a corresponding pair from the key. The x-axis shows the cosine similarity for the query pair, and the y-axis displays the cosine similarity for the corresponding key pair. The strong positive correlation (Pearson's r = 0.86) indicates a close relationship between how the query and key weight vectors are oriented, suggesting a coordinated role in processing positional information within the attention mechanism.", "section": "3.4 Investigating the Cosine Similarity Across Weight Vector Pairs"}, {"figure_path": "e5Mv7iWfVW/figures/figures_7_1.jpg", "caption": "Figure 7: Average difference of weight vector pairs in the query and key across the layers of different LLMs (including Llama-2 [35], Alpaca [33], Llama-3 [35], Mistral [16], and WizardLM [38]). We show that the average difference increases after the first several layers and stays high, which is the result of the fact that only near orthogonal weight vector pairs are updated during fine-tuning.", "description": "This figure shows the average L2 distance between the weight vector pairs in the query and key of different versions of LLMs across layers.  The different lines represent different LLMs, comparing pre-trained models to their fine-tuned versions (e.g., Llama-2 vs. Alpaca-7b-chat). The observation highlights that fine-tuning primarily affects nearly orthogonal weight vector pairs, leading to a larger average distance in later layers.", "section": "4 Reducing the Trainable Parameters During Fine-tuning"}, {"figure_path": "e5Mv7iWfVW/figures/figures_13_1.jpg", "caption": "Figure 6: Comparison between the base model Mistral-7B [16] and fine-tuned version WizardLM-2 [38]. For each sub-figure, the scatter figure in the middle demonstrates the cosine similarity between weight vector pairs where each point corresponds to a weight vector pair; the x-axis corresponds to the results for the base model, and the y-axis corresponds to the fine-tuned model. In the histogram on the top and left, we report the average L2 weight distance between the weight vectors of the pre-trained model and the fine-tuned model. The y-axis of the histogram on the top and the x-axis of the histogram on the left correspond to the L2 weight distance. Generally, fine-tuning merely changes the angle between weight vector pairs. Besides the first several layers, the weight change mainly happens on weight vector pairs that are nearly orthogonal. More results are provided in Appendix B.", "description": "This figure compares the base model Mistral-7B with its fine-tuned version, WizardLM-2, focusing on the cosine similarity and L2 weight distance between weight vector pairs.  It shows that fine-tuning primarily alters the angles between weight vectors, particularly those that are nearly orthogonal, after the initial layers.", "section": "4 Reducing the Trainable Parameters During Fine-tuning"}, {"figure_path": "e5Mv7iWfVW/figures/figures_14_1.jpg", "caption": "Figure 6: Comparison between the base model Mistral-7B [16] and fine-tuned version WizardLM-2 [38]. For each sub-figure, the scatter figure in the middle demonstrates the cosine similarity between weight vector pairs where each point corresponds to a weight vector pair; the x-axis corresponds to the results for the base model, and the y-axis corresponds to the fine-tuned model. In the histogram on the top and left, we report the average L2 weight distance between the weight vectors of the pre-trained model and the fine-tuned model. The y-axis of the histogram on the top and the x-axis of the histogram on the left correspond to the L2 weight distance. Generally, fine-tuning merely changes the angle between weight vector pairs. Besides the first several layers, the weight change mainly happens on weight vector pairs that are nearly orthogonal. More results are provided in Appendix B.", "description": "This figure compares the base model Mistral-7B with its fine-tuned version WizardLM-2, focusing on the changes in cosine similarity and L2 weight distance of weight vector pairs during fine-tuning. It shows that fine-tuning primarily affects the angles between nearly orthogonal weight vector pairs, especially in layers beyond the initial ones.", "section": "4 Reducing the Trainable Parameters During Fine-tuning"}, {"figure_path": "e5Mv7iWfVW/figures/figures_14_2.jpg", "caption": "Figure 2: Attention visualization of the attention heads with the largest or the smallest average absolute cosine similarity | cos a| across the weight vector pairs in RoPE, where the key is on the left, and the query is on the right. The lower transparency means a higher attention score. We demonstrate the results for the first layer of Llama-2-7b-chat and Mistral-7B-Instruct-v0.2. The results empirically support that higher | cos a| leads to attention on basic syntactic information and lower | cos a| leads to attention on high-level semantic information. For more results of different models at different layers please refer to Appendix B.", "description": "This figure visualizes attention patterns in the first layer of two LLMs (Llama-2-7b-chat and Mistral-7B-Instruct-v0.2) to show the relationship between the angle of weight vector pairs in RoPE and the attention focus.  Attention heads with high average absolute cosine similarity (|cos a|) are shown to focus on basic syntactic information (e.g., prepositions, articles), while those with low |cos a| focus more on high-level semantics.  Lower transparency indicates higher attention.", "section": "3.2 Attention Visualization for Different Attention Heads"}, {"figure_path": "e5Mv7iWfVW/figures/figures_15_1.jpg", "caption": "Figure 4: We show the cosine similarity of each weight vector pair in the query and the key of different layers of the Llama2-7b [35]. Each column corresponds to different layers. The top row is the results for the query and the bottom row is the results for the key. The attention heads are separated with vertical red lines. We show that even within one head, the cosine similarity of different weight vector pairs differs. More results on other layers and other models are in Appendix B.", "description": "This figure visualizes the cosine similarity of weight vector pairs within the query and key components of the Llama2-7b model's self-attention mechanism across different layers (1st, 5th, 10th, and 20th). Each column represents a layer, with the top row showing query results and the bottom row showing key results.  Vertical red lines delineate individual attention heads. The visualization demonstrates that even within a single attention head, the cosine similarity varies significantly between different pairs of weight vectors.", "section": "3.4 Investigating the Cosine Similarity Across Weight Vector Pairs"}]