[{"figure_path": "chLoLUHnai/figures/figures_1_1.jpg", "caption": "Figure 1: The behavior of (GD) for optimizing a non-homogenous four-layer MLP with GELU activation function on a subset of CIFAR-10 dataset. We randomly sample 6,000 data with labels \u201cairplane\u201d and \u201cautomobile\u201d from CIFAR-10 dataset. The normalized margin is defined as mini\u2208[n] Yif (wt; xi)/||wt||4, which is close to (3). The blue curves correspond to GD with a large stepsize \u0e17\u0e35 = 0.2, where the empirical risk oscillates in the first phase but decreases monotonically in the second phase. The orange curves correspond to GD with a small stepsize \u1fc6 = 0.005, where the empirical risk decreases monotonically. Furthermore, Figure 1(b) suggests the normalized margins of both two curves increase and converge in the stable phases. Finally, Figure 1(c) suggests that large stepsize achieves a better test accuracy, consistent with larger-scale learning experiment [Hoffer et al., 2017, Goyal et al., 2017].", "description": "This figure shows the training dynamics of a four-layer MLP using gradient descent (GD) with different step sizes on a subset of the CIFAR-10 dataset.  It illustrates the two-phase training behavior often observed with large stepsize GD: an initial oscillatory phase followed by a monotonic decrease in empirical risk.  The figure compares large and small stepsizes, highlighting how the large stepsize leads to faster convergence and improved test accuracy, while both step sizes show a similar monotonic increase in normalized margin during the second phase of training.", "section": "1 Introduction"}, {"figure_path": "chLoLUHnai/figures/figures_8_1.jpg", "caption": "Figure 1: The behavior of (GD) for optimizing a non-homogenous four-layer MLP with GELU activation function on a subset of CIFAR-10 dataset. We randomly sample 6,000 data with labels \u201cairplane\u201d and \u201cautomobile\u201d from CIFAR-10 dataset. The normalized margin is defined as mini\u2208[n] Yif (wt; xi)/||wt||4, which is close to (3). The blue curves correspond to GD with a large stepsize \u0e17\u0e35 = 0.2, where the empirical risk oscillates in the first phase but decreases monotonically in the second phase. The orange curves correspond to GD with a small stepsize \u1fc6 = 0.005, where the empirical risk decreases monotonically. Furthermore, Figure 1(b) suggests the normalized margins of both two curves increase and converge in the stable phases. Finally, Figure 1(c) suggests that large stepsize achieves a better test accuracy, consistent with larger-scale learning experiment [Hoffer et al., 2017, Goyal et al., 2017].", "description": "This figure shows the training dynamics of a four-layer MLP on a subset of CIFAR-10 using gradient descent (GD) with different step sizes.  The plots illustrate the empirical risk, normalized margin, and test accuracy over training iterations. The key observation is that a larger step size leads to an initial oscillatory phase in the empirical risk, followed by a monotonic decrease, while a smaller step size results in a consistently monotonic decrease. Despite the initial oscillations, the larger step size achieves a better test accuracy and a similar final normalized margin as the smaller step size.", "section": "1 Introduction"}, {"figure_path": "chLoLUHnai/figures/figures_34_1.jpg", "caption": "Figure 2: Behavior of (GD) for two-layer networks (2) with leaky softplus activation function (see Example 3.1 with c = 0.5). We consider an XOR dataset and a subset of CIFAR-10 dataset. In both cases, we observe that (1) GD with a large stepsize achieves a faster optimization compared to GD with a small stepsize, (2) the asymptotic convergence rate of the empirical risk is O(1/(\u1fc6t)), and (3) in the stable phase, the normalized margin (nearly) monotonically increases. These observations are consistent with our theoretical understanding of large stepsize GD. More details about the experiments are explained in Section 5.", "description": "Figure 2 displays the results of experiments conducted on two-layer neural networks with leaky softplus activation. Two datasets are used: a synthetic XOR dataset and a subset of CIFAR-10.  The plots show the empirical risk, asymptotic convergence rate, and normalized margin for different step sizes of Gradient Descent (GD).  The results demonstrate that larger step sizes lead to faster optimization, an asymptotic convergence rate of O(1/(\u1fc6t)), and a nearly monotonically increasing normalized margin during the stable phase, aligning with the paper's theoretical analysis.", "section": "Experiments"}, {"figure_path": "chLoLUHnai/figures/figures_35_1.jpg", "caption": "Figure 2: Behavior of (GD) for two-layer networks (2) with leaky softplus activation function (see Example 3.1 with c = 0.5). We consider an XOR dataset and a subset of CIFAR-10 dataset. In both cases, we observe that (1) GD with a large stepsize achieves a faster optimization compared to GD with a small stepsize, (2) the asymptotic convergence rate of the empirical risk is O(1/(\u1fc6t)), and (3) in the stable phase, the normalized margin (nearly) monotonically increases. These observations are consistent with our theoretical understanding of large stepsize GD. More details about the experiments are explained in Section 5.", "description": "The figure shows the empirical risk, asymptotic convergence rate, and normalized margin for gradient descent (GD) using different step sizes on two-layer neural networks. The experiments were performed on an XOR dataset and a subset of the CIFAR-10 dataset. The results demonstrate that using a large step size leads to faster optimization and a nearly monotonically increasing normalized margin in the stable phase, which is consistent with the theoretical findings.", "section": "Experiments"}, {"figure_path": "chLoLUHnai/figures/figures_35_2.jpg", "caption": "Figure 4: Training loss and margins of a two-layer network with leaky softplus activations on a synthetic linear separable dataset. There are five samples in the dataset, which are ((0.05, 1, 2), 1), ((0.05, \u22122, 1), 1), ((\u22121, 0, 2), \u22121), ((0.05, -2, -2), 1), ((0.05, 1, -2), 1). The max margin direction is (1,0,0) with a normalized margin of 0.05. The network only has two neurons with fixed weights 1/2 and -1/2. The leaky softplus activation is f(x) = (x + \u00a2(x))/2, where \u222e is the softplus activation. The stepsize is 3. We can observe that both neurons have negative margins during the training, while the network's margin increases and becomes positive.", "description": "The figure shows training loss and normalized margins for a two-layer neural network with leaky softplus activation trained on a synthetic dataset.  Despite individual neurons having negative margins, the overall network margin increases and becomes positive, illustrating the model's behavior even with non-monotonic risk.", "section": "Extra Experiments"}]