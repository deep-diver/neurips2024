[{"figure_path": "Tg2EVad7VF/figures/figures_1_1.jpg", "caption": "Figure 1: Overview of our proposed system. We first normalize the target speech units with the denoising process from the latent diffusion model. Then speech-to-unit (S2UT) model is trained to predict normalized units, which are converted into waveform from an off-the-shelf unit-vocoder.", "description": "This figure illustrates the architecture of the proposed speech-to-speech translation system.  It shows how the target speech is first normalized using a diffusion model, then processed by a speech-to-unit (S2UT) translation model, and finally converted to synthesized speech via a unit vocoder. The system uses a multi-stage pipeline to improve translation quality by simplifying the target data distribution and incorporating a regularization strategy to improve model robustness.", "section": "Problem formulation and overview"}, {"figure_path": "Tg2EVad7VF/figures/figures_3_1.jpg", "caption": "Figure 2: Visualization of our latent diffusion model's denoising process for speech normalization. The clean latent z0 is synthetically noised (into zT) and the reverse diffusion process gradually denoise it to generate normalized speech units.", "description": "This figure illustrates the process of speech normalization using a latent diffusion model.  The process begins with encoding speech features (h) into a latent representation (z0).  Noise is then injected into this representation (zT). The core of the model is the denoising process, which iteratively removes the injected noise through multiple steps, ultimately recovering a denoised latent representation (z0). This denoised representation is then decoded back into speech features (\u0125), which are finally processed by a language modeling head to produce normalized speech units (ynorm).", "section": "3 DIFFNORM: denoising diffusion models for speech normalization"}, {"figure_path": "Tg2EVad7VF/figures/figures_4_1.jpg", "caption": "Figure 3: Visualization of CMLM for speech-to-unit translation where the model is trained with the unmasking objective to recover ynorm. When classifier-free guidance is used, with probability pdrop, we replace the encoded source speech g by a \"null\" representation g\u2205.", "description": "This figure illustrates the architecture of the Conditional Masked Language Model (CMLM) used for speech-to-unit translation.  The model takes source speech as input, encodes it using a Conformer subsampler and encoder, and then uses a decoder with classifier-free guidance to predict normalized target speech units (ynorm).  The classifier-free guidance involves randomly replacing the encoded source speech with a \"null\" representation, forcing the model to generate coherent units without direct conditioning on the source.  The predicted units are then fed into a separate diffusion model (DIFFNORM) to further refine them and generate the final normalized speech units.", "section": "4 Classifier-free guidance for non-autoregressive transformer"}, {"figure_path": "Tg2EVad7VF/figures/figures_7_1.jpg", "caption": "Figure 4: Trade-off between quality (ASR-BLEU) and latency for varying numbers of decoding iterations. Five markers correspond to {15, 10, 7, 5, 3} decoding iterations. Decreasing the number of iterations results in a decline in model performance, traded off for faster speedup. With DIFFNORM and CG, our S2UT model achieves a better quality-latency trade-off than CMLM and outperforms a strong autoregressive baseline with large speedups.", "description": "This figure shows the trade-off between the translation quality (measured by ASR-BLEU) and the inference speed (relative speedup compared to the autoregressive baseline) for different numbers of decoding iterations in the non-autoregressive speech-to-unit translation models.  It compares the performance of four models: CMLM (baseline), CMLM+CG (with classifier-free guidance), CMLM+DIFFNORM (with diffusion-based normalization), and CMLM+DIFFNORM+CG (combining both techniques).  As the number of iterations decreases, the speed increases, but the translation quality also decreases.  The combined DIFFNORM and CG model demonstrates a better balance between speed and accuracy compared to the other models and even surpasses the autoregressive baseline at high speedups.", "section": "5.2 Results"}, {"figure_path": "Tg2EVad7VF/figures/figures_8_1.jpg", "caption": "Figure 5: Visualization of reconstructed speech's log-mel spectrograms. Noticeable divergence from the original speech is highlighted in the white bounding boxes.", "description": "This figure visualizes the log-mel spectrograms of reconstructed speech for various noise injection levels (different start times T in the diffusion model).  The original spectrogram is compared to spectrograms with increasing noise.  White boxes highlight regions where significant differences between the original and reconstructed spectrograms are apparent.  This demonstrates how the amount of noise injected affects the quality of the reconstructed speech.", "section": "6.1 Effect of synthetic noise injection"}]