[{"figure_path": "9622QfVSAb/tables/tables_34_1.jpg", "caption": "Table 1: IMA score across different encoders. We report the IMA score and the task performance with the ST setup (OPT). A positive correlation exists between IMA score and the performance; the most aligned encdoers (CLIP) have the best accuracy/CIDEr on VQA and captioning tasks.", "description": "This table shows the Implicit Multimodal Alignment (IMA) score and the performance of different image encoders on various tasks using the OPT model with a single-task setup.  The higher the IMA score, the better the performance, indicating a strong positive correlation between alignment and task performance.  The CLIP-ViT-L encoder shows the highest IMA score and the best overall performance.", "section": "F.1 Implicit multimodal alignment as proxy metric for task performance?"}, {"figure_path": "9622QfVSAb/tables/tables_36_1.jpg", "caption": "Table 2: a-SubNet: a modality-agnostic subnetwork. We prune the LLMs using different post-training pruning methods, including our a-SubNet.", "description": "This table compares the performance of different LLM compression methods on various multimodal tasks.  The \"Baseline\" represents the original uncompressed LLM.  \"Wanda\" is a state-of-the-art task-specific pruning method.  \"Random mask\" is a simple baseline for comparison. The \"a-SubNet\" is the proposed modality-agnostic subnetwork compression method that uses a single subnetwork across a variety of tasks and modalities.  The table shows the performance in terms of CIDEr score (for captioning) and accuracy (for question answering) for various tasks and datasets. The numbers in parentheses show the performance relative to the Baseline.", "section": "5 Implications: performance, safety and efficiency"}]