[{"figure_path": "xutrKezbPF/figures/figures_1_1.jpg", "caption": "Figure 1: (a) Proposed: Controlled Information Flow for Knowledge Distillation (CIFD). In CIFD, we explore the idea of Rate-Distortion Modules (RDM) that use the Teacher\u2019s embeddings to generate embeddings that act like Teacher Assistants (TAs) to distill knowledge to the Student. It does so by processing the embedding through a rate constrained communication channel. RDMs are much cheaper to train as they reuse the feature extractors learned by the teacher. By varying the rate constraint, RDMs can can simulate different TAs, and enable \u201cTA\u201d based training. (b) CIFD vs. existing KD approaches.", "description": "This figure illustrates the proposed CIFD framework for knowledge distillation. (a) shows the architecture of CIFD, highlighting the use of Rate-Distortion Modules (RDMs) as lightweight alternatives to Teacher Assistants.  RDMs process teacher embeddings through a rate-constrained channel, mimicking TAs of varying capacities by adjusting the rate constraint. (b) compares the training cost of CIFD with existing knowledge distillation methods, demonstrating CIFD's significant cost reduction while maintaining or exceeding performance.", "section": "3 Controlled Information Flow for KD"}, {"figure_path": "xutrKezbPF/figures/figures_1_2.jpg", "caption": "Figure 1: (a) Proposed: Controlled Information Flow for Knowledge Distillation (CIFD). In CIFD, we explore the idea of Rate-Distortion Modules (RDM) that use the Teacher\u2019s embeddings to generate embeddings that act like Teacher Assistants (TAs) to distill knowledge to the Student. It does so by processing the embedding through a rate constrained communication channel. RDMs are much cheaper to train as they reuse the feature extractors learned by the teacher. By varying the rate constraint, RDMs can can simulate different TAs, and enable \u201cTA\u201d based training. (b) CIFD vs. existing KD approaches. ", "description": "This figure demonstrates the proposed CIFD method for knowledge distillation.  Panel (a) illustrates the architecture of CIFD, highlighting the use of Rate-Distortion Modules (RDMs) as a more efficient alternative to Teacher Assistants (TAs) for knowledge transfer from a large teacher model to a smaller student model.  Panel (b) compares the training cost of CIFD with other existing knowledge distillation methods, showing a significant reduction in cost while maintaining or improving performance.", "section": "1 Introduction"}, {"figure_path": "xutrKezbPF/figures/figures_2_1.jpg", "caption": "Figure 2: Training schemes of the proposed CIFD framework. We first train the RDM modules to mimic teacher assistants as in (a). Then we train the student model using both the trained RDMs and the teacher model as in (b).", "description": "This figure illustrates the two-stage training process of the Controlled Information Flow for Knowledge Distillation (CIFD) framework.  The first stage (a) focuses on training Rate-Distortion Modules (RDMs) to mimic the behavior of Teacher Assistants (TAs).  RDMs process the teacher's penultimate layer embeddings through a rate-constrained communication channel. This stage aims to create smaller, cheaper-to-train alternatives to TAs. The second stage (b) trains the student model, using both the trained RDMs and the original teacher model.  This stage leverages the RDMs to transfer knowledge efficiently to the smaller student model, while an Information Bottleneck Module regularizes the training process to prevent overfitting.", "section": "3 Controlled Information Flow for KD"}, {"figure_path": "xutrKezbPF/figures/figures_2_2.jpg", "caption": "Figure 2: Training schemes of the proposed CIFD framework. We first train the RDM modules to mimic teacher assistants as in (a). Then we train the student model using both the trained RDMs and the teacher model as in (b).", "description": "This figure illustrates the training process of the Controlled Information Flow for Knowledge Distillation (CIFD) framework.  It shows two stages: (a) training the Rate-Distortion Modules (RDMs) to mimic Teacher Assistants (TAs) by processing teacher embeddings through a rate-constrained bottleneck layer and (b) training the student model using the trained RDMs and the original teacher model.  The RDMs act as intermediate models between the teacher and the student, simplifying the knowledge transfer process. The Information Bottleneck Module (IBM) within the student model regularizes the training in the presence of multiple RDMs to prevent overfitting. The diagrams detail the flow of information and the loss functions used in each training stage.", "section": "3 Controlled Information Flow for KD"}, {"figure_path": "xutrKezbPF/figures/figures_5_1.jpg", "caption": "Figure 4: Relation between Masked Image Modeling (MIM), Masked Generative Distillation (MGD), and Information Bottleneck Module (IBM) for Distillation.", "description": "This figure illustrates the relationship between three different methods: Masked Image Modeling (MIM), Masked Generative Distillation (MGD), and the Information Bottleneck Module (IBM).  It visually depicts how each method approaches the problem of knowledge transfer by showing the input, processing steps, and the final objective.  MIM focuses on reconstructing masked parts of an image; MGD aims to generate a representation that matches the teacher's; and IBM seeks to find a balance between preserving useful information and minimizing redundancy. The figure helps to clarify the connections between these techniques and their relationship to the proposed Information Bottleneck Module in the paper.", "section": "3 Controlled Information Flow for KD"}, {"figure_path": "xutrKezbPF/figures/figures_8_1.jpg", "caption": "Figure 5: Effect of rate-constraint on RDM's classification performance. The graph behaves like a rate-distortion curve, information rate R is proportional to performance.", "description": "This figure shows the effect of varying the information rate (R) on the performance of the Rate-Distortion Module (RDM) in a classification task. The x-axis represents the rate parameter R, while the y-axis shows the classification accuracy. The plot demonstrates a behavior similar to a rate-distortion curve: as the rate constraint increases, the accuracy initially increases and then plateaus.  This suggests that increasing the information rate up to a certain point improves performance; however, beyond that point, further increases do not significantly improve the results.", "section": "4.3 Ablation studies"}, {"figure_path": "xutrKezbPF/figures/figures_20_1.jpg", "caption": "Figure 4: Relation between Masked Image Modeling (MIM), Masked Generative Distillation (MGD), and Information Bottleneck Module (IBM) for Distillation.", "description": "This figure illustrates the relationship between three different methods: Masked Image Modeling (MIM), Masked Generative Distillation (MGD), and the proposed Information Bottleneck Module (IBM) for knowledge distillation.  It visually depicts the architecture of each method, showing how they process input images and generate embeddings. MIM focuses on masking parts of an input image and predicting the masked parts. MGD uses the teacher model embedding and generates features, and the IBM introduces a noise mechanism to control the flow of information during distillation. The figure highlights the similarities and differences between these techniques in terms of their approach to knowledge transfer.", "section": "3 Controlled Information Flow for KD"}, {"figure_path": "xutrKezbPF/figures/figures_22_1.jpg", "caption": "Figure 7: Training the RDM for classification", "description": "This figure illustrates the training process of the Rate-Distortion Module (RDM) for classification tasks.  The input (e.g., an image) is first processed by the teacher's backbone model to extract the embedding. This embedding is then fed into the RDM encoder, where noise is added. The encoder output is passed through a rate-constrained bottleneck layer, and a decoder reconstructs the embedding. Finally, a fully-connected layer converts the reconstructed embedding into class logits. The RDM's training loss is a combination of mean squared error (MSE) between the input and reconstructed embeddings, a rate loss that penalizes high information flow, Kullback-Leibler (KL) divergence between the teacher's and RDM's class probability distributions, and cross-entropy (CE) loss between the RDM's predictions and ground truth labels. This multi-task learning approach enables the RDM to effectively learn both a compressed representation of the teacher's embedding and accurate classification information.", "section": "3.1 Controlling the information from the teacher using Rate-Distortion Theory"}, {"figure_path": "xutrKezbPF/figures/figures_23_1.jpg", "caption": "Figure 2: Training schemes of the proposed CIFD framework. We first train the RDM modules to mimic teacher assistants as in (a). Then we train the student model using both the trained RDMs and the teacher model as in (b).", "description": "This figure illustrates the two-stage training process of the Controlled Information Flow for Knowledge Distillation (CIFD) framework.  The first stage (a) focuses on training Rate-Distortion Modules (RDMs) to mimic the behavior of Teacher Assistants. These RDMs process the teacher's embeddings, adding noise to simulate different levels of information transfer. The second stage (b) trains the student model, using both the pretrained RDMs and the teacher model.  The student model incorporates an Information Bottleneck Module (IBM) to prevent overfitting and enhance generalization. The figure shows the different components including input embeddings, RDMs, teacher model, student model, and the different loss components used during training, including mean square error (MSE) loss, rate loss, and cross-entropy loss.", "section": "3 Controlled Information Flow for KD"}, {"figure_path": "xutrKezbPF/figures/figures_24_1.jpg", "caption": "Figure 1: (a) Proposed: Controlled Information Flow for Knowledge Distillation (CIFD). In CIFD, we explore the idea of Rate-Distortion Modules (RDM) that use the Teacher's embeddings to generate embeddings that act like Teacher Assistants (TAs) to distill knowledge to the Student. It does so by processing the embedding through a rate constrained communication channel. RDMs are much cheaper to train as they reuse the feature extractors learned by the teacher. By varying the rate constraint, RDMs can can simulate different TAs, and enable \u201cTA\u201d based training. (b) CIFD vs. existing KD approaches.", "description": "The figure shows the proposed CIFD framework for knowledge distillation, which uses Rate-Distortion Modules (RDMs) instead of Teacher Assistants. (a) illustrates the architecture of CIFD, highlighting the role of RDMs in mimicking TAs and distilling knowledge to the student model.  (b) compares the training cost of CIFD with existing knowledge distillation approaches, demonstrating that CIFD significantly reduces the cost while improving performance.", "section": "1 Introduction"}, {"figure_path": "xutrKezbPF/figures/figures_24_2.jpg", "caption": "Figure 9: tSNE plots of embeddings for a subset of 10 classes of the CIFAR-100 dataset. and blue clusters that are close to each other in both the teacher and the better performing RDM's embeddings are now farther apart in the poor performing RDM's embeddings.", "description": "This figure shows the t-SNE visualization of embeddings from the teacher model and two different RDM models trained on the CIFAR-100 dataset.  The visualization helps to understand how the RDMs (Rate-Distortion Modules) capture and represent information from the teacher model. The better-performing RDM shows a similar embedding distribution to the teacher, while a poorer performing RDM exhibits a more dispersed and less informative representation.", "section": "D.3 Visualizing the Teacher and RDM embeddings"}, {"figure_path": "xutrKezbPF/figures/figures_24_3.jpg", "caption": "Figure 9: tSNE plots of embeddings for a subset of 10 classes of the CIFAR-100 dataset. and blue clusters that are close to each other in both the teacher and the better performing RDM's embeddings are now farther apart in the poor performing RDM's embeddings.", "description": "This figure shows the t-SNE visualization of embeddings from the teacher model and two RDM models trained on the CIFAR-100 dataset.  The visualization helps to understand how the RDMs (Rate-Distortion Modules), which mimic teacher assistants in knowledge distillation, represent the data.  The better-performing RDM produces embeddings where clusters are well-separated and similar to the teacher's embeddings, implying effective knowledge transfer.  Conversely, the poorly performing RDM shows more scattered clusters, suggesting less effective knowledge transfer.", "section": "D.3 Visualizing the Teacher and RDM embeddings"}]