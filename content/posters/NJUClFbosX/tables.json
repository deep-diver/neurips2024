[{"figure_path": "NJUClFbosX/tables/tables_6_1.jpg", "caption": "Table 1: The mean word error rate [%] on the sys-bAbI task for 10 seeds, with \u00b1 indicating SD.", "description": "This table presents the results of the sys-bAbI task, a text understanding and reasoning task designed to assess systematic generalization. It shows the mean word error rate for different models: TPR-RNN, TPR-RNN with AID (attention-based iterative decomposition), and TPR-RNN with D3 (discrete dictionary-based decomposition).  The results are broken down for in-distribution data (w/o sys diff) and out-of-distribution data with systematic differences (w/ sys diff), along with the gap between them and the number of parameters for each model.  Lower error rates are better, indicating better systematic generalization.", "section": "4.2.1 TPR-based Memory Networks"}, {"figure_path": "NJUClFbosX/tables/tables_6_2.jpg", "caption": "Table 2: The mean accuracy [%] on the sort-of-CLEVR task for 10 seeds, with \u00b1 indicating SD.", "description": "This table presents the mean accuracy and standard deviation of the Linear Transformer model and its variants (with AID and D3) on the sort-of-CLEVR task.  The results are broken down by the type of reasoning required (Unary, Binary, Ternary) and the number of parameters in the model.  Different Dcode values (128 and 256) are tested for the D3 models, and the results with and without D3 applied to the filler (w/ F, w/o F) are shown separately.", "section": "4.2.2 Linear Transformer"}, {"figure_path": "NJUClFbosX/tables/tables_6_3.jpg", "caption": "Table 3: Perplexity on the WikiText-103 task.", "description": "This table presents the perplexity scores achieved by different models on the WikiText-103 language modeling task.  The models compared include the Linear Transformer baseline,  the Linear Transformer with the AID module, and the Linear Transformer with the proposed D3 module (with and without applying D3 to filler generation).  The table shows perplexity scores on both validation and test sets, and for different D3 configurations in terms of codebook size (Dcode). Lower perplexity indicates better performance.", "section": "4.2.2 Linear Transformer"}, {"figure_path": "NJUClFbosX/tables/tables_15_1.jpg", "caption": "Table 4: Hyper-parameter settings of the D3.", "description": "This table shows the hyperparameter settings used for the proposed Discrete Dictionary-based Decomposition (D3) layer in the experiments conducted on four different tasks: SAR, sys-bAbI, Sort-of-CLEVR, and WikiText-103.  For each task, it specifies the range of values explored for the code dimension (Dcode), the number of codebook entries (Ncode), the query dimension (Dquery), the number of top-k keys selected during sparse key access, and the dropout probability (Pdropout). Note that Dquery is dynamically determined as half the size of Dcode.  The table illustrates the choices made in adjusting these parameters for optimal performance on each specific task.", "section": "B Hyper-parameter Settings"}, {"figure_path": "NJUClFbosX/tables/tables_15_2.jpg", "caption": "Table 5: Hyper-parameters of TPR-RNN.", "description": "This table shows the hyperparameter settings used for the TPR-RNN model in the experiments.  It specifies the dimensions of the entity and relation components (Dentity and Drelation), the number of encoding components (Nenc), and the number of decoding components (Ndec).  These parameters control the model's architecture and capacity for processing sequential data in the sys-bAbI task.", "section": "3.3 Integration of D3 into Existing TPR-based Models"}, {"figure_path": "NJUClFbosX/tables/tables_15_3.jpg", "caption": "Table 6: Hyper-parameters of FWM.", "description": "This table shows the hyperparameter settings used for the Fast Weight Memory (FWM) model in the experiments.  It lists the dimensions of the LSTM layer (DLSTM), the FWM component (DFWM), the number of reads (Nreads), encoding components (Nenc), and decoding components (Ndec) for both the SAR and sys-bAbI tasks. The numbers of components depend on the number of reads, reflecting the model's dynamic memory operations.", "section": "3.2 Module Configurations"}, {"figure_path": "NJUClFbosX/tables/tables_15_4.jpg", "caption": "Table 7: Hyper-parameters of Linear Transformer.", "description": "This table shows the hyperparameter settings for the Linear Transformer model used in the experiments for the Sort-of-CLEVR and WikiText-103 tasks.  It specifies the dimensions of the heads (Dheads, which is equivalent to Dcomponent), the number of heads (Nheads), the number of encoding components (Nenc_component), and the number of decoding components (Ndec_component).  Note that the number of encoding and decoding components are both set to twice the number of heads.", "section": "4.2.2 Linear Transformer"}, {"figure_path": "NJUClFbosX/tables/tables_16_1.jpg", "caption": "Table 1: The mean word error rate [%] on the sys-bAbI task for 10 seeds, with \u00b1 indicating SD.", "description": "This table presents the results of the sys-bAbI experiment.  It compares different models (TPR-RNN, FWM, and their variations with AID and D3) on two conditions: with and without systematic differences in the training data.  The table shows the mean word error rate (%), standard deviation, the difference between the two conditions (Gap), and the number of parameters for each model. Lower word error rate and gap values indicate better performance.  This showcases the ability of D3 to improve the systematic generalization of TPR-based models.", "section": "4.2.1 TPR-based Memory Networks"}, {"figure_path": "NJUClFbosX/tables/tables_16_2.jpg", "caption": "Table 1: The mean word error rate [%] on the sys-bAbI task for 10 seeds, with \u00b1 indicating SD.", "description": "This table presents the results of the sys-bAbI experiment. It shows the mean word error rate and standard deviation for different models on two conditions: with and without systematic differences. The models compared include TPR-RNN, TPR-RNN with AID, TPR-RNN with D3, FWM, FWM with AID, FWM with D3 (without filler), and FWM with D3 (with filler). The table also indicates the difference in error rates between the two conditions (Gap) and the number of parameters for each model.", "section": "4.2.1 TPR-based Memory Networks"}, {"figure_path": "NJUClFbosX/tables/tables_17_1.jpg", "caption": "Table 2: The mean accuracy [%] on the sort-of-CLEVR task for 10 seeds, with \u00b1 indicating SD.", "description": "This table presents the mean accuracy and standard deviation achieved by different models on the sort-of-CLEVR task.  The models tested include a baseline Linear Transformer, the same model augmented with AID (Attention-based Iterative Decomposition), and the model with the proposed D3 layer (with and without filler generation).  The table shows results for three different levels of complexity in the task (Unary, Binary, and Ternary) and also shows the number of parameters used by each model.  Different values for Dcode (dimension of the codebook) are also tested for the D3 models.", "section": "4.2.2 Linear Transformer"}, {"figure_path": "NJUClFbosX/tables/tables_17_2.jpg", "caption": "Table 3: Perplexity on the WikiText-103 task.", "description": "This table presents the perplexity scores achieved by different models on the WikiText-103 task, a benchmark for language modeling.  The models compared include a Linear Transformer baseline, the same model enhanced with the AID (Attention-based Iterative Decomposition) module, and the Linear Transformer with the proposed D3 (Discrete Dictionary-based Decomposition) layer, both with and without D3 applied to filler generation. The table shows the perplexity scores for both the validation and test sets, and also shows the model's parameters. This allows for comparison of performance and parameter efficiency across the models.", "section": "4.2.2 Linear Transformer"}, {"figure_path": "NJUClFbosX/tables/tables_18_1.jpg", "caption": "Table 12: Ablation study for the effect of the codebook on the SAR task for 10 seeds.", "description": "This table presents the results of an ablation study on the SAR task, investigating the impact of removing the codebook from the D3 layer.  It shows the accuracy achieved by the baseline FWM model and several variations of the D3 model, including those with different numbers of codebook keys (Ncode) and those without a codebook.  The results highlight the importance of the codebook in achieving high accuracy, particularly in the case of unseen combinatorial data.", "section": "4.3.2 Ablation Study"}, {"figure_path": "NJUClFbosX/tables/tables_18_2.jpg", "caption": "Table 1: The mean word error rate [%] on the sys-bAbI task for 10 seeds, with \u00b1 indicating SD.", "description": "This table presents the performance of different models on the systematic bAbI (sys-bAbI) task, specifically focusing on the word error rate.  It compares the baseline TPR-RNN and FWM models against versions incorporating the AID and D3 layers, both with and without the filler being processed by D3. The results are broken down for in-distribution (without systematic differences) and out-of-distribution (with systematic differences) scenarios, highlighting the generalization ability of each model.  The \"Gap\" column shows the difference in error rate between the in-distribution and out-of-distribution settings, indicating the model's robustness to unseen data, while '#params' reflects the number of model parameters used.", "section": "4.2.1 TPR-based Memory Networks"}]