[{"figure_path": "XOVks7JHQA/figures/figures_1_1.jpg", "caption": "Figure 1: The impact of more neighbors on posterior uncertainty. NETCONF [7] or SocNL [35] underestimates posterior uncertainty of A in G2 by considering high-uncertainty node D as neighbor that reduce uncertainty. LinUProp represents the uncertainty of a posterior using interval widths, where increased uncertainty from more neighbors leads to higher posterior uncertainty. The proposed interval ways can be interpret as a generalized variance component of the expected prediction error (Sec. 4.2). Furthermore, LinUProp is interpretable due to the additive of neighbor uncertainty (Eq. (12)).", "description": "This figure illustrates how existing methods (NETCONF and SocNL) underestimate uncertainty when dealing with nodes having uncertain neighbors.  It compares their approach to the proposed LinUProp method.  LinUProp uses interval widths to represent uncertainty, showing how uncertainty increases additively with more neighbors, unlike the multiplicative approach of existing methods. The figure also highlights the interpretability of LinUProp by visually showing the contribution of each neighbor's uncertainty to the overall uncertainty.", "section": "Abstract"}, {"figure_path": "XOVks7JHQA/figures/figures_3_1.jpg", "caption": "Figure 2: An illustration of LinUProp quantifying uncertainty in posterior beliefs for each node in a 3-node chain. Inputs: (1) Uncertainty in prior beliefs of each node represented as interval widths (e1,e2,e3) (2) Edge potentials (H12 and H23, with H21 = H12 and H32 = H23 due to the undirected graph). Outputs: uncertainty in posterior beliefs of each node also represented as interval widths. LinUProp can set a different compatibility matrix for each edge, allowing it to handle edge-dependent potentials, while previous methods cannot do this.", "description": "This figure illustrates how LinUProp calculates uncertainty in a simple 3-node graph.  It shows how the uncertainty in the prior beliefs (interval widths e1, e2, e3) and the edge potentials (matrices H12 and H23) are used to compute the uncertainty in the posterior beliefs (interval widths in vec(B)). The key highlight is LinUProp's ability to handle different compatibility matrices for each edge, a feature not present in previous methods.", "section": "Linear Bound Propagation"}, {"figure_path": "XOVks7JHQA/figures/figures_6_1.jpg", "caption": "Figure 3: Case Study. (a) A 4\u00d74 grid with two classes. The nodes colored in red and green are labeled, while the rest are unlabeled. The bold unlabeled node indicates the node we aim to explain the source of uncertainty. (b) An interpretation of the uncertainty in the belief of the bold node computed by LinUProp. The colors represent the contribution of each node to the uncertainty of the bold node, with warmer colors indicating more significant contributions; the radius of the white circles indicates the belief bound width computed by LinUProp for each node, with a larger radius indicating higher uncertainty in the beliefs.", "description": "This figure shows a case study on a 4x4 grid graph with two classes.  Part (a) illustrates the graph structure, with labeled nodes (red and green) and unlabeled nodes (white), highlighting a specific unlabeled node for analysis. Part (b) visually represents the uncertainty in the belief of that highlighted node as computed by LinUProp.  The color intensity shows the contribution of each neighboring node to the overall uncertainty, while the circle size corresponds to the magnitude of the uncertainty associated with each node.", "section": "5.1 Case Study on a Simple Graph"}, {"figure_path": "XOVks7JHQA/figures/figures_6_2.jpg", "caption": "Figure 4: (a) Convergence of average belief bound width by LinUProp. (b) Scalability. Each data point represents the running time of LinUProp for 10 iterations with a certain number of edges. The y-axis is the running time in seconds.", "description": "This figure presents the convergence and scalability results of the LinUProp algorithm.  The top panel (a) shows the convergence of the average belief bound width across different datasets (Cora, Citeseer, Pubmed, Polblogs), demonstrating that the algorithm reaches convergence within approximately 10 iterations for all datasets. The bottom panel (b) demonstrates the linear scalability of LinUProp by plotting the running time (in seconds) against the number of edges in the graph for each dataset. The linear relationship between running time and the number of edges confirms that the LinUProp algorithm exhibits linear scalability.", "section": "5 Experiments"}, {"figure_path": "XOVks7JHQA/figures/figures_8_1.jpg", "caption": "Figure 5: Test accuracy for varying labeling budgets with BP inferring posterior beliefs. Each subplot title includes two components, which represent the labeling accuracy and dataset. Each column corresponds to a dataset. In each subplot, the node selection strategies based on LinUProp and its variant are represented by red \u25bc and purple \u25b2. Under the same labeling budget, the higher the test accuracy, the better.", "description": "This figure compares the performance of different node selection strategies in active learning, using belief propagation for inference.  The strategies are compared across four datasets (Cora, Citeseer, Pubmed, PolBlogs) at various labeling budgets. LinUProp and its variant (LC+BB) consistently show higher test accuracy than random selection, least confidence, and entropy-based selection.", "section": "5.2 Experiments on Real Data"}, {"figure_path": "XOVks7JHQA/figures/figures_8_2.jpg", "caption": "Figure 5: Test accuracy for varying labeling budgets with BP inferring posterior beliefs. Each subplot title includes two components, which represent the labeling accuracy and dataset. Each column corresponds to a dataset. In each subplot, the node selection strategies based on LinUProp and its variant are represented by red \u25bc and purple \u25b2. Under the same labeling budget, the higher the test accuracy, the better.", "description": "This figure shows the test accuracy for different labeling budgets on four datasets (Cora, Citeseer, PubMed, Polblogs) using Belief Propagation (BP) for posterior belief inference.  Different node selection strategies are compared: Random, Least Confidence (LC), Entropy, Belief Bound (BB) from LinUProp, LC+BB (combination of LC and BB), Certainty Score (CS) from NETCONF, and LC+CS (combination of LC and CS). The results demonstrate that LinUProp-based strategies generally achieve higher test accuracy with the same labeling budget, showcasing its effectiveness in active learning.", "section": "5.2 Experiments on Real Data"}, {"figure_path": "XOVks7JHQA/figures/figures_15_1.jpg", "caption": "Figure 7: Correctness. The two axes represent two UQ methods: the x-axis is the empirical standard deviation of the beliefs obtained by MC simulation (an estimation of the ground truth uncertainty), and the y-axis is the belief bound width computed by LinUProp. In fact, there are 16 points in this figure, corresponding to the 16 variable nodes in Fig. 3(a). Some points appear to overlap because they have similar uncertainties quantified by both methods. The Pearson Correlation Coefficient (PCC) between the uncertainties quantified by the two methods is 0.9084, indicating a strong positive correlation.", "description": "This figure shows a strong positive correlation between the uncertainties computed by Monte Carlo (MC) simulation and LinUProp. The x-axis represents the empirical standard deviation from MC simulation, approximating the ground truth uncertainty, and the y-axis represents the uncertainty from LinUProp.  The high correlation (PCC = 0.9084) validates the accuracy of LinUProp in quantifying uncertainty.", "section": "5.1 Case Study on a Simple Graph"}, {"figure_path": "XOVks7JHQA/figures/figures_16_1.jpg", "caption": "Figure 5: Test accuracy for varying labeling budgets with BP inferring posterior beliefs. Each subplot title includes two components, which represent the labeling accuracy and dataset. Each column corresponds to a dataset. In each subplot, the node selection strategies based on LinUProp and its variant are represented by red \u25bc and purple \u25b2. Under the same labeling budget, the higher the test accuracy, the better.", "description": "The figure shows the test accuracy for various labeling budgets when using belief propagation (BP) to infer posterior beliefs.  It compares different node selection strategies, highlighting the performance of LinUProp and its variant against random selection, least confidence, and entropy-based selection methods.  The results are shown for different datasets and labeling accuracies.  Higher test accuracy indicates better performance.", "section": "5.2 Experiments on Real Data"}, {"figure_path": "XOVks7JHQA/figures/figures_16_2.jpg", "caption": "Figure 5: Test accuracy for varying labeling budgets with BP inferring posterior beliefs. Each subplot title includes two components, which represent the labeling accuracy and dataset. Each column corresponds to a dataset. In each subplot, the node selection strategies based on LinUProp and its variant are represented by red \u25bc and purple \u25b2. Under the same labeling budget, the higher the test accuracy, the better.", "description": "This figure shows the test accuracy for different labeling budgets when using belief propagation (BP) to infer posterior beliefs.  The x-axis represents the size of the labeling budget, and the y-axis shows the test accuracy.  Different colors represent different node selection strategies (Random, Least Confidence, Entropy, Belief Bound (LinUProp), and a combination of Least Confidence and Belief Bound). The shaded areas indicate the standard deviation across multiple trials.  The figure demonstrates that LinUProp-based strategies consistently achieve higher test accuracy with smaller labeling budgets, particularly when the labeling accuracy is high.", "section": "5.2 Experiments on Real Data"}]