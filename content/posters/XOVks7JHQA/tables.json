[{"figure_path": "XOVks7JHQA/tables/tables_15_1.jpg", "caption": "Table 1: Statistical information and partitioning of datasets. The subsets, Vtrain, Vval and Vtest are sampled from the original node set. The remaining nodes are in Vulp. We use these subsets in the effectiveness experiments.", "description": "This table presents the statistical information and partitioning of four datasets used in the paper's experiments.  It shows the number of nodes, edges, and classes for each dataset, along with the sizes of the training, validation, and test sets used in the effectiveness experiments. The remaining nodes are in Vulp, which are unlabeled nodes and used in the active learning experiments.", "section": "B.2 More Experimental Details"}, {"figure_path": "XOVks7JHQA/tables/tables_17_1.jpg", "caption": "Table 2: Mean test accuracies and their standard deviations for a fixed labeling budget (20b) with BP inferring posterior beliefs using noisy labeled nodes. Each cell shows the mean accuracy and its standard deviation on the test set for different labeling accuracy and node selection strategies, on different datasets. BB and LC+BB are node selection strategies based on LinUProp. Bold values indicate the highest mean accuracy. Underlined values emphasize the method with the lower standard deviation between the LinUProp winner and the non-LinUProp winner. Superscripts indicate significant superiority between the LinUProp winner and the non-LinUProp winner (pairwise t-test at a 5% significance level (*), 10% significance level (\u2020)).", "description": "This table presents the mean test accuracy and standard deviation for a fixed labeling budget of 20b, using noisy labels and BP for inference.  It compares different node selection strategies, highlighting the superior performance of LinUProp-based methods (BB and LC+BB) across various datasets and labeling accuracies.", "section": "B.3.2 Results under a fixed Labeling Budgets"}, {"figure_path": "XOVks7JHQA/tables/tables_17_2.jpg", "caption": "Table 2: Mean test accuracies and their standard deviations for a fixed labeling budget (20b) with BP inferring posterior beliefs using noisy labeled nodes. Each cell shows the mean accuracy and its standard deviation on the test set for different labeling accuracy and node selection strategies, on different datasets. BB and LC+BB are node selection strategies based on LinUProp. Bold values indicate the highest mean accuracy. Underlined values emphasize the method with the lower standard deviation between the LinUProp winner and the non-LinUProp winner. Superscripts indicate significant superiority between the LinUProp winner and the non-LinUProp winner (pairwise t-test at a 5% significance level (*), 10% significance level (\u2020)).", "description": "This table presents the mean test accuracy and standard deviation for different node selection strategies (Random, LC, Entropy, BB, LC+BB) using a fixed labeling budget of 20b.  The results are shown for four datasets (Cora, Citeseer, Pubmed, Polblogs) and four labeling accuracies (70%, 80%, 90%, 100%).  Statistical significance is indicated using * and \u2020 symbols.", "section": "B.3.2 Results under a fixed Labeling Budgets"}, {"figure_path": "XOVks7JHQA/tables/tables_18_1.jpg", "caption": "Table 4: Runtime comparison including all edges across different datasets (in seconds)", "description": "This table compares the runtime performance of NETCONF and LinUProp on four different datasets.  The runtime is measured in seconds and includes the computation time for all edges in the graph.  The results show that the runtime of both methods is quite similar, though LinUProp shows a minor performance advantage in this specific experimental condition.", "section": "B.4 Additional Runtime Results"}]