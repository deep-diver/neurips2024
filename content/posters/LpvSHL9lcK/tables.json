[{"figure_path": "LpvSHL9lcK/tables/tables_7_1.jpg", "caption": "Table 1: We compare IPR-MPNN on QM9 with the base downstream GIN model [Xu et al., 2019], two graph rewiring techniques [Gutteridge et al., 2023, Qian et al., 2023], a multi-hop MPNN [Abboud et al., 2022], and the relational GIN [Schlichtkrull et al., 2018]. The best-performing method is colored in green, the second-best in blue, and third in orange. IPR-MPNN obtains the best result on all targets, except for MU, where it obtains the second-best result.", "description": "This table compares the performance of IPR-MPNN against several other state-of-the-art models on the QM9 dataset for predicting various molecular properties.  The results highlight IPR-MPNN's superior performance, achieving the best or second-best results across all properties.", "section": "5 Experimental Setup and Results"}, {"figure_path": "LpvSHL9lcK/tables/tables_7_2.jpg", "caption": "Table 2: Results on the PEPTIDES and PCQM-CONTACT datasets from the long-range graph benchmark [Dwivedi et al., 2022b]. For PCQM-CONTACT, we use both the initially proposed evaluation methodology (PCQM(1)) and the filtering methodologies proposed in T\u00f6nshoff et al. [2023] (PCQM(2) for filtering and PCQM(3) for extended filtering). Green is the best model, blue is the second, and red the third. IPR-MPNNs obtain the best predictive performance on all datasets.", "description": "This table compares the performance of IPR-MPNNs against other state-of-the-art models on two datasets from the long-range graph benchmark: PEPTIDES and PCQM-CONTACT.  The PCQM-CONTACT results are shown using three different evaluation methods, reflecting variations in data preprocessing.  The best-performing model for each metric and dataset is highlighted.", "section": "5 Experimental Setup and Results"}, {"figure_path": "LpvSHL9lcK/tables/tables_8_1.jpg", "caption": "Table 3: IPR-MPNN training, inference (seconds per epoch), and memory consumption statistics in comparison to the base GINE model [Xu et al., 2019], the GPS graph transformer [Ramp\u00e1\u0161ek et al., 2022] and the Drew model [Gutteridge et al., 2023] on the PEPTIDES-STRUCT dataset [Dwivedi et al., 2022b]. Our model has almost the same computation and memory efficiency as the base GINE model while being twice as fast and significantly more memory efficient when compared to GPS.", "description": "This table compares the training time, inference time, and memory usage of IPR-MPNN against three other models (GINE, GPS, and DREW) on the PEPTIDES-STRUCT dataset.  The results show that IPR-MPNN achieves similar efficiency to GINE while significantly outperforming GPS in terms of speed and memory.", "section": "5 Experimental Setup and Results"}, {"figure_path": "LpvSHL9lcK/tables/tables_9_1.jpg", "caption": "Table 4: Results on the ZINC [Jin et al., 2017] and OGBG-MOLHIV [Hu et al., 2020] datasets. Green is the best model, blue is the second, and red the third. The IPR-MPNN outperforms both SAT and GPS on ZINC, while obtaining the same performance as GPS on OGB-MOLHIV.", "description": "This table presents the results of the IPR-MPNN model and other state-of-the-art models on two molecular datasets: ZINC and OGB-MOLHIV.  The results show the performance of each model on specific tasks (indicated by the \u2191 or \u2193 symbols), highlighting the superior performance of the IPR-MPNN model compared to other methods.", "section": "5 Experimental Setup and Results"}, {"figure_path": "LpvSHL9lcK/tables/tables_24_1.jpg", "caption": "Table A5: Overview of used hyperparameters.", "description": "This table lists the hyperparameters used in the experiments for different datasets.  It shows the number of hidden units in upstream and downstream MPNN layers, the number of virtual nodes, and sampling strategies.  The hyperparameter settings were optimized for each dataset individually.", "section": "Additional Empirical Results and Experimental Details"}, {"figure_path": "LpvSHL9lcK/tables/tables_25_1.jpg", "caption": "Table 1: We compare IPR-MPNN on QM9 with the base downstream GIN model [Xu et al., 2019], two graph rewiring techniques [Gutteridge et al., 2023, Qian et al., 2023], a multi-hop MPNN [Abboud et al., 2022], and the relational GIN [Schlichtkrull et al., 2018]. The best-performing method is colored in green, the second-best in blue, and third in orange. IPR-MPNN obtains the best result on all targets, except for MU, where it obtains the second-best result.", "description": "This table compares the performance of IPR-MPNN against several other models on the QM9 dataset for various molecular properties.  The best performing model for each property is highlighted.  The results demonstrate IPR-MPNN's superior performance across different properties compared to existing methods.", "section": "5 Experimental Setup and Results"}, {"figure_path": "LpvSHL9lcK/tables/tables_25_2.jpg", "caption": "Table A7: Performance comparison between the base GINE and IPR-MPNN on recently-proposed heterophilic datasets.", "description": "This table compares the performance of the base GINE model and the proposed IPR-MPNN model on four recently proposed heterophilic datasets: ROMAN-EMPIRE, TOLOKERS, MINESWEEPER, and AMAZON-RATINGS.  Heterophilic datasets are those where nodes of different classes tend to be connected to each other more frequently than nodes of the same class. The table shows that the IPR-MPNN model significantly outperforms the base GINE model on all four datasets. This improvement demonstrates the effectiveness of IPR-MPNNs in handling heterophily, a common challenge for many graph neural network models.", "section": "5 Experimental Setup and Results"}, {"figure_path": "LpvSHL9lcK/tables/tables_25_3.jpg", "caption": "Table A8: Performance between a virtual node connected to the entire original graph (1VN-FC) and IPR-MPNNs with two virtual nodes with one sample (2VN1S) and two samples, respectively (2VN2S).", "description": "This table presents the performance comparison of different model variations on several datasets. The models vary in the number of virtual nodes and the number of samples used.  The results show the impact of these variations on model accuracy.", "section": "Additional Empirical Results and Experimental Details"}, {"figure_path": "LpvSHL9lcK/tables/tables_25_4.jpg", "caption": "Table 1: We compare IPR-MPNN on QM9 with the base downstream GIN model [Xu et al., 2019], two graph rewiring techniques [Gutteridge et al., 2023, Qian et al., 2023], a multi-hop MPNN [Abboud et al., 2022], and the relational GIN [Schlichtkrull et al., 2018]. The best-performing method is colored in green, the second-best in blue, and third in orange. IPR-MPNN obtains the best result on all targets, except for MU, where it obtains the second-best result.", "description": "This table compares the performance of IPR-MPNN against several other state-of-the-art methods on the QM9 dataset for predicting various molecular properties.  The results highlight IPR-MPNN's superior performance across most properties.", "section": "5 Experimental Setup and Results"}, {"figure_path": "LpvSHL9lcK/tables/tables_26_1.jpg", "caption": "Table 1: We compare IPR-MPNN on QM9 with the base downstream GIN model [Xu et al., 2019], two graph rewiring techniques [Gutteridge et al., 2023, Qian et al., 2023], a multi-hop MPNN [Abboud et al., 2022], and the relational GIN [Schlichtkrull et al., 2018]. The best-performing method is colored in green, the second-best in blue, and third in orange. IPR-MPNN obtains the best result on all targets, except for MU, where it obtains the second-best result.", "description": "This table compares the performance of IPR-MPNN against other state-of-the-art methods on the QM9 dataset for various molecular properties.  The results show that IPR-MPNN significantly outperforms existing methods in most cases, demonstrating its effectiveness for molecular property prediction.", "section": "5 Experimental Setup and Results"}, {"figure_path": "LpvSHL9lcK/tables/tables_26_2.jpg", "caption": "Table A11: Comparison between the base GIN model w/wo positional encoding and IPR-MPNN on CSL dataset. For IPR-MPNN*, we pre-calculate the graph partitioning for each data instance, and label each node with its partition ID.", "description": "This table compares the performance of different models on the CSL dataset, focusing on the impact of positional encoding and the IPR-MPNN method.  The base GIN model is compared against versions with positional encoding, the PR-MPNN method, and the IPR-MPNN method with and without pre-calculated graph partitioning (IPR-MPNN*). The accuracy for each model is presented, highlighting the performance improvements achieved by incorporating positional information and the IPR-MPNN technique.", "section": "Additional Empirical Results and Experimental Details"}, {"figure_path": "LpvSHL9lcK/tables/tables_26_3.jpg", "caption": "Table A12: More memory consumption details together with train and validation times per epoch in seconds. We compare to the base GINE model, various variants of the SAT Graph Transformer, GraphGPS, and the PR-MPNN rewiring technique. IPR-MPNNs maintain low memory usage while also being significantly faster when compared to the Graph Transformers and PR-MPNN. The experiments were performed on the OGBG-MOLHIV dataset, with the same batch size and the same machine that contains an Nvidia RTX A5000 GPU and an Intel i9-11900K CPU.", "description": "This table compares the training time, validation time, and memory usage of IPR-MPNN with other models on the OGB-MOLHIV dataset.  It shows that IPR-MPNN achieves significantly faster training and validation times with lower memory consumption compared to graph transformers and the PR-MPNN method.", "section": "Additional Empirical Results and Experimental Details"}]