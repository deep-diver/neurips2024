[{"figure_path": "LpvSHL9lcK/figures/figures_1_1.jpg", "caption": "Figure 1: Overview of how IPR-MPNNs implicitly rewire a graph through adding virtual nodes. IPR-MPNNs use an upstream MPNN to learn priors for connecting original nodes with virtual nodes via edges, parameterizing a probability mass function conditioned on exactly-k constraints. Subsequently, we sample exactly k edges from this distribution for each original node, connecting it to k virtual nodes. We input the resulting graph to a downstream model, typically an MPNN, for the final predictions task, propagating information from (1) original nodes to virtual nodes, (2) among virtual nodes, and (3) among original nodes. On the backward pass, the gradients of the loss l regarding the parameters are approximated through the derivative of the exactly-k marginals.", "description": "This figure illustrates the architecture of Implicit Probabilistically Rewired Message-Passing Neural Networks (IPR-MPNNs).  It shows how IPR-MPNNs implicitly rewire a graph by adding virtual nodes and learning to connect them to existing nodes via edges. An upstream MPNN learns prior probabilities for these connections, and exactly-k edges are sampled from this distribution.  The resulting graph is then fed into a downstream MPNN for prediction. The figure also highlights the backward pass, showing how gradients are approximated using the derivative of exactly-k marginals.", "section": "Present Work"}, {"figure_path": "LpvSHL9lcK/figures/figures_6_1.jpg", "caption": "Figure 2: Comparing model sensitivity across different layers for the two most distant nodes from graphs from the ZINC dataset. On the left, we compare the sensitivity for models with a varying number of layers. We can observe that IPR-MPNNs maintain a high sensitivity even for the last layer, while the base models have the sensitivity decaying to 0. On the right, we compare models with a different number of virtual nodes, observing that the results are similar for all of the variants.", "description": "This figure compares the sensitivity of IPR-MPNN and baseline models across different layers for the two most distant nodes in graphs from the ZINC dataset. The left panel shows that IPR-MPNN maintains high sensitivity even in the last layer, unlike baseline models which decay to zero. The right panel demonstrates similar results for IPR-MPNN models with varying numbers of virtual nodes.", "section": "5 Experimental Setup and Results"}, {"figure_path": "LpvSHL9lcK/figures/figures_9_1.jpg", "caption": "Figure 3: We compute the log of total effective resistance [Black et al., 2023] of five molecular datasets before and after rewiring the graphs using virtual nodes. Our rewiring technique consistently lowers the total effective resistance, indicating a better information flow on all of the datasets.", "description": "The figure shows a bar chart comparing the average total effective resistance (a measure of connectivity) before and after applying the proposed Implicit Probabilistically Rewired Message-Passing Neural Networks (IPR-MPNNs) method.  Five molecular datasets (Peptides, MolHIV, PCQM-Contact, ZINC, QM9) are shown.  For all datasets, the average total effective resistance is lower after applying IPR-MPNNs, indicating that the method effectively improves the flow of information within the graphs.", "section": "Experimental Setup and Results"}, {"figure_path": "LpvSHL9lcK/figures/figures_20_1.jpg", "caption": "Figure 2: Comparing model sensitivity across different layers for the two most distant nodes from graphs from the ZINC dataset. On the left, we compare the sensitivity for models with a varying number of layers. We can observe that IPR-MPNNs maintain a high sensitivity even for the last layer, while the base models have the sensitivity decaying to 0. On the right, we compare models with a different number of virtual nodes, observing that the results are similar for all of the variants.", "description": "The figure compares the model sensitivity across different layers for two distant nodes in graphs from the ZINC dataset. The left panel compares sensitivity for models with varying numbers of layers, showing IPR-MPNNs maintain high sensitivity even in the last layer unlike base models. The right panel compares models with different numbers of virtual nodes, showing similar results across all variants.", "section": "5 Experimental Setup and Results"}, {"figure_path": "LpvSHL9lcK/figures/figures_20_2.jpg", "caption": "Figure 1: Overview of how IPR-MPNNs implicitly rewire a graph through adding virtual nodes. IPR-MPNNs use an upstream MPNN to learn priors for connecting original nodes with virtual nodes via edges, parameterizing a probability mass function conditioned on exactly-k constraints. Subsequently, we sample exactly k edges from this distribution for each original node, connecting it to k virtual nodes. We input the resulting graph to a downstream model, typically an MPNN, for the final predictions task, propagating information from (1) original nodes to virtual nodes, (2) among virtual nodes, and (3) among original nodes. On the backward pass, the gradients of the loss l regarding the parameters are approximated through the derivative of the exactly-k marginals.", "description": "The figure illustrates the process of implicit graph rewiring in IPR-MPNNs using virtual nodes. An upstream MPNN learns priors for connecting original nodes to virtual nodes, sampling k edges per original node. The resulting graph is then fed into a downstream MPNN for predictions, involving message passing between original and virtual nodes.  The backward pass uses the derivative of exactly-k marginals to approximate gradients.", "section": "Present Work"}, {"figure_path": "LpvSHL9lcK/figures/figures_21_1.jpg", "caption": "Figure 1: Overview of how IPR-MPNNs implicitly rewire a graph through adding virtual nodes. IPR-MPNNs use an upstream MPNN to learn priors for connecting original nodes with virtual nodes via edges, parameterizing a probability mass function conditioned on exactly-k constraints. Subsequently, we sample exactly k edges from this distribution for each original node, connecting it to k virtual nodes. We input the resulting graph to a downstream model, typically an MPNN, for the final predictions task, propagating information from (1) original nodes to virtual nodes, (2) among virtual nodes, and (3) among original nodes. On the backward pass, the gradients of the loss l regarding the parameters are approximated through the derivative of the exactly-k marginals.", "description": "The figure illustrates the implicit graph rewiring mechanism of IPR-MPNNs.  An upstream MPNN generates priors for connecting original nodes to new virtual nodes.  Exactly k edges are sampled from this distribution, connecting each original node to k virtual nodes.  A downstream MPNN then processes this augmented graph, facilitating long-range message passing. The backward pass uses the derivative of exactly-k marginals to approximate gradients.", "section": "1 Introduction"}, {"figure_path": "LpvSHL9lcK/figures/figures_21_2.jpg", "caption": "Figure 1: Overview of how IPR-MPNNs implicitly rewire a graph through adding virtual nodes. IPR-MPNNs use an upstream MPNN to learn priors for connecting original nodes with virtual nodes via edges, parameterizing a probability mass function conditioned on exactly-k constraints. Subsequently, we sample exactly k edges from this distribution for each original node, connecting it to k virtual nodes. We input the resulting graph to a downstream model, typically an MPNN, for the final predictions task, propagating information from (1) original nodes to virtual nodes, (2) among virtual nodes, and (3) among original nodes. On the backward pass, the gradients of the loss l regarding the parameters are approximated through the derivative of the exactly-k marginals.", "description": "This figure illustrates the core idea of Implicit Probabilistically Rewired Message-Passing Neural Networks (IPR-MPNNs).  It shows how IPR-MPNNs implicitly rewire a graph by adding a small number of virtual nodes and connecting them probabilistically to the existing nodes.  An upstream MPNN learns the probability distribution for these connections, ensuring differentiability and efficient end-to-end training.  The resulting modified graph is then fed into a downstream MPNN for prediction. The process involves message passing between original nodes and virtual nodes, and between virtual nodes themselves, thereby enabling long-range information flow and addressing limitations of traditional MPNNs.", "section": "1 Introduction"}]