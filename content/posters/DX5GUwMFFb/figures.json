[{"figure_path": "DX5GUwMFFb/figures/figures_1_1.jpg", "caption": "Figure 1: Impact of reducing replay buffer size on SAC, PPO, and TD3: Decreasing the replay buffer size adversely affects learning. In contrast, AVG succeeds despite learning without a replay buffer, as shown by a \"buffer size\" of 1 in the plots. Each data point represents the mean episodic return over the final 100K steps, averaged across 30 runs. All methods were trained for 10M timesteps.", "description": "The figure shows the impact of reducing the replay buffer size on the performance of four reinforcement learning algorithms: SAC, TD3, PPO, and IAC.  The x-axis represents the replay buffer size (log scale), and the y-axis represents the average episodic return over the last 100,000 timesteps of training, averaged across 30 independent runs.  Each algorithm was trained for 10 million timesteps.  The plot shows that the performance of SAC, TD3, and PPO degrades significantly as the replay buffer size decreases, demonstrating a strong dependency on experience replay for effective learning.  In stark contrast, the proposed AVG algorithm maintains its performance even with a replay buffer size of 1 (essentially no replay buffer). This highlights AVG's robustness and suitability for resource-constrained environments where large replay buffers are infeasible.", "section": "1 Introduction"}, {"figure_path": "DX5GUwMFFb/figures/figures_3_1.jpg", "caption": "Figure 2: Effect of \u03c3 on entropy of normal and squashed normal distribution", "description": "The figure shows the relationship between the standard deviation (\u03c3) and entropy for both normal and squashed normal distributions.  It illustrates that for a normal distribution, entropy increases monotonically with \u03c3. However, for a squashed normal distribution, entropy initially increases with \u03c3, reaches a maximum, and then decreases. This highlights the impact of using a squashed normal distribution, which is commonly used in reinforcement learning to ensure the actions remain within a bounded range, on the entropy of the policy.", "section": "3 The Action Value Gradient Method"}, {"figure_path": "DX5GUwMFFb/figures/figures_5_1.jpg", "caption": "Figure 3: AVG on Gymnasium and DeepMind Control Suite tasks. Each solid learning curve is an average of 30 independent runs. The shaded regions represent a 95% confidence interval of the bootstrap distribution. Note that SAC refers to SAC with a replay buffer size of 1M. The corresponding dashed line represents the mean performance over the final 10K steps of training.", "description": "This figure compares the performance of AVG against three other incremental algorithms (IAC, SAC-1, and TD3-1) across various benchmark tasks.  It also includes the performance of the standard SAC algorithm with a large replay buffer (1M samples) for comparison.  The results show that AVG is the only incremental algorithm that consistently learns effectively, frequently achieving comparable or superior final performance to the non-incremental SAC.", "section": "AVG on Simulated Benchmark Tasks"}, {"figure_path": "DX5GUwMFFb/figures/figures_5_2.jpg", "caption": "Figure 3: AVG on Gymnasium and DeepMind Control Suite tasks. Each solid learning curve is an average of 30 independent runs. The shaded regions represent a 95% confidence interval of the bootstrap distribution. Note that SAC refers to SAC with a replay buffer size of 1M. The corresponding dashed line represents the mean performance over the final 10K steps of training.", "description": "This figure compares the performance of the proposed Action Value Gradient (AVG) algorithm to three other incremental algorithms (IAC, SAC-1, TD3-1) and one batch algorithm (SAC) across eight continuous control tasks.  AVG consistently outperforms the other incremental algorithms and achieves comparable performance to the batch algorithm (SAC) which utilizes a much larger replay buffer.  The shaded areas represent the 95% confidence interval for each learning curve showing the average episodic return over 10 million timesteps.", "section": "AVG on Simulated Benchmark Tasks"}, {"figure_path": "DX5GUwMFFb/figures/figures_6_1.jpg", "caption": "Figure 5: The gradient norm of the critic and actor networks for AVG and SAC, along with their average episodic returns. AVG\u00af denotes AVG without any normalization or scaling applied. The solid lines represent the average, whereas the light lines represent the values for the individual runs. Note that the y-axis in the plots for actor and critic gradient norms is displayed on a logarithmic scale.", "description": "This figure compares the gradient norms of the actor and critic networks for AVG and SAC, along with their average episodic returns. The results show that AVG without normalization and scaling (AVG\u00af) suffers from instability, manifesting as high variance in the gradients. In contrast, SAC demonstrates stable learning with smoothly decreasing gradient norms.", "section": "5.1 Instability Without Normalization"}, {"figure_path": "DX5GUwMFFb/figures/figures_7_1.jpg", "caption": "Figure 3: AVG on Gymnasium and DeepMind Control Suite tasks. Each solid learning curve is an average of 30 independent runs. The shaded regions represent a 95% confidence interval of the bootstrap distribution. Note that SAC refers to SAC with a replay buffer size of 1M. The corresponding dashed line represents the mean performance over the final 10K steps of training.", "description": "This figure compares the performance of the proposed Action Value Gradient (AVG) method against other incremental and batch methods across various continuous control tasks from the Gymnasium and DeepMind Control Suite.  It visually demonstrates AVG's learning curves (average of 30 runs with 95% confidence intervals) and contrasts them with those of IAC, SAC-1, and TD3-1 (all incremental methods) along with the performance of SAC (batch method with a large replay buffer). The dashed lines show the mean performance over the final 10,000 steps.", "section": "AVG on Simulated Benchmark Tasks"}, {"figure_path": "DX5GUwMFFb/figures/figures_7_2.jpg", "caption": "Figure 3: AVG on Gymnasium and DeepMind Control Suite tasks. Each solid learning curve is an average of 30 independent runs. The shaded regions represent a 95% confidence interval of the bootstrap distribution. Note that SAC refers to SAC with a replay buffer size of 1M. The corresponding dashed line represents the mean performance over the final 10K steps of training.", "description": "This figure compares the performance of the proposed Action Value Gradient (AVG) method to several other incremental and batch methods across a range of continuous control tasks from the Gymnasium and DeepMind Control Suite benchmark environments.  The solid lines show the average episodic return over 30 independent runs for each method, with shaded regions indicating the 95% confidence intervals.  The dashed lines represent the mean performance at the end of training.  The figure highlights AVG's superior performance compared to other incremental methods and its ability to compete with batch methods even with a greatly reduced replay buffer size.", "section": "AVG on Simulated Benchmark Tasks"}, {"figure_path": "DX5GUwMFFb/figures/figures_8_1.jpg", "caption": "Figure 3: AVG on Gymnasium and DeepMind Control Suite tasks. Each solid learning curve is an average of 30 independent runs. The shaded regions represent a 95% confidence interval of the bootstrap distribution. Note that SAC refers to SAC with a replay buffer size of 1M. The corresponding dashed line represents the mean performance over the final 10K steps of training.", "description": "This figure compares the performance of the proposed Action Value Gradient (AVG) algorithm against three other incremental algorithms (IAC, SAC-1, TD3-1) and a standard batch algorithm (SAC) across eight different continuous control tasks.  The results illustrate AVG's superior performance and ability to learn effectively in an incremental setting, often matching or exceeding the performance of batch methods that use replay buffers and target networks.", "section": "AVG on Simulated Benchmark Tasks"}, {"figure_path": "DX5GUwMFFb/figures/figures_8_2.jpg", "caption": "Figure 3: AVG on Gymnasium and DeepMind Control Suite tasks. Each solid learning curve is an average of 30 independent runs. The shaded regions represent a 95% confidence interval of the bootstrap distribution. Note that SAC refers to SAC with a replay buffer size of 1M. The corresponding dashed line represents the mean performance over the final 10K steps of training.", "description": "The figure shows the learning curves of AVG, IAC, SAC-1, and TD3-1 across multiple continuous control tasks in the Gymnasium and DeepMind Control Suite environments. The solid lines represent the average episodic returns over 30 independent runs, while shaded areas represent 95% confidence intervals.  For comparison, the performance of SAC with a large replay buffer (1M) is also included (grey dashed line).  The results demonstrate that AVG is the only incremental algorithm that learns effectively, often achieving a final performance comparable to, or better than, the batch method.", "section": "AVG on Simulated Benchmark Tasks"}, {"figure_path": "DX5GUwMFFb/figures/figures_9_1.jpg", "caption": "Figure 3: AVG on Gymnasium and DeepMind Control Suite tasks. Each solid learning curve is an average of 30 independent runs. The shaded regions represent a 95% confidence interval of the bootstrap distribution. Note that SAC refers to SAC with a replay buffer size of 1M. The corresponding dashed line represents the mean performance over the final 10K steps of training.", "description": "This figure compares the performance of AVG against three other algorithms (IAC, SAC-1, TD3-1) across multiple continuous control tasks from the Gymnasium and DeepMind Control Suite benchmark environments.  It shows that AVG is the only incremental algorithm that successfully learns across these different tasks, and that its performance is comparable to or even surpasses that of batch RL methods (SAC) that have access to much larger replay buffers.", "section": "AVG on Simulated Benchmark Tasks"}, {"figure_path": "DX5GUwMFFb/figures/figures_17_1.jpg", "caption": "Figure 11: Squashed Normal Distribution PDF", "description": "The figure shows the probability density function (PDF) of a squashed normal distribution.  A squashed normal distribution is used in the paper because it provides bounded actions in the range [-1, 1], which is suitable for many reinforcement learning tasks. The plot displays how the entropy of the distribution changes with the standard deviation (\u03c3).  For small \u03c3, the distribution is close to a delta function, so entropy is low. As \u03c3 increases, entropy increases, but once \u03c3 reaches a certain threshold, entropy begins to decrease again. This is because values become increasingly likely to be close to the boundaries of the [-1,1] interval. The effect of this is illustrated in the paper, because it helps prevent issues stemming from algorithms that continuously maximize entropy driving o to large values, thus approximating a uniform random policy.", "section": "B AVG Design Choices"}, {"figure_path": "DX5GUwMFFb/figures/figures_17_2.jpg", "caption": "Figure 12: Hyperparameter Evaluation via Random Search. Scatter plot of the performance of the best 25 out of 300 unique hyper-parameter configurations. Note that the y-axis represents the area under the curve for 2M timesteps, not an evaluation of the final policy for 10M timesteps.", "description": "This figure shows the result of a hyperparameter search for the AVG algorithm.  It displays a scatter plot comparing the area under the curve (AUC) of 25 of the best hyperparameter configurations (out of 300 tested) across three different MuJoCo environments. The AUC is calculated for the first 2 million timesteps of training. The plot helps to visualize the relative performance of different hyperparameter configurations, aiding in selecting the best set for AVG. Each data point represents the mean AUC across 10 random seeds, and the error bars illustrate standard deviation.", "section": "B.1 Relative Performance of Different Hyperparameter Configurations in Random Search"}, {"figure_path": "DX5GUwMFFb/figures/figures_18_1.jpg", "caption": "Figure 3: AVG on Gymnasium and DeepMind Control Suite tasks. Each solid learning curve is an average of 30 independent runs. The shaded regions represent a 95% confidence interval of the bootstrap distribution. Note that SAC refers to SAC with a replay buffer size of 1M. The corresponding dashed line represents the mean performance over the final 10K steps of training.", "description": "This figure compares the performance of the proposed Action Value Gradient (AVG) method against other incremental methods (IAC, SAC-1, TD3-1) and a batch method (SAC) across multiple continuous control tasks from the Gymnasium and DeepMind Control Suite environments.  The results show that AVG is the only incremental method that learns effectively, often achieving final performance comparable to batch methods. The shaded areas represent 95% confidence intervals, indicating the variability across multiple runs.", "section": "AVG on Simulated Benchmark Tasks"}, {"figure_path": "DX5GUwMFFb/figures/figures_21_1.jpg", "caption": "Figure 14: Performance of IAC Variants. Learning curves of the best hyper-parameter configurations found via random search for each task variant. Each solid curve is averaged over 30 independent runs. The shaded regions represent a 95% confidence interval.", "description": "The figure presents the learning curves of the Incremental Actor Critic (IAC) algorithm across three different MuJoCo continuous control tasks: dot_reacher_hard, Reacher-v4, and InvertedDoublePendulum-v4.  Four variants of the algorithm are compared: one with no entropy regularization, one using distribution entropy only in the actor, one using sample entropy, and one using both distribution and sample entropy.  The plots show the average episodic return over the course of training, with shaded regions illustrating 95% confidence intervals.  The results demonstrate how different entropy regularization approaches affect learning performance in these benchmark tasks, highlighting the role of entropy in stabilizing and improving the learning process of this incremental RL method.", "section": "E.1 Ablation Study of IAC: Distribution against Sample Entropy"}, {"figure_path": "DX5GUwMFFb/figures/figures_21_2.jpg", "caption": "Figure 14: Performance of IAC Variants. Learning curves of the best hyper-parameter configurations found via random search for each task variant. Each solid curve is averaged over 30 independent runs. The shaded regions represent a 95% confidence interval.", "description": "The figure displays the learning curves for different variants of the Incremental Actor Critic (IAC) algorithm across four MuJoCo environments (Ant-v4, HalfCheetah-v4, Hopper-v4, Inverted Double Pendulum-v4).  Each line represents a different entropy configuration (Sample Entropy, No Entropy, Dist Entropy (Actor Only), and Dist Entropy).  The shaded areas represent 95% confidence intervals.  The results highlight the impact of entropy regularization on the performance of IAC, comparing distribution entropy and sample entropy and illustrating the variability inherent in incremental learning.", "section": "E.1 Ablation Study of IAC: Distribution against Sample Entropy"}, {"figure_path": "DX5GUwMFFb/figures/figures_22_1.jpg", "caption": "Figure 6: Ablation study of normalization and scaling techniques used with AVG. We plot the learning curves of the best hyperparameter configurations for each task variant. Each solid learning curve is an average of 30 independent runs. The shaded regions represent a 95% confidence interval.", "description": "This figure shows the ablation study of normalization and scaling techniques used with the AVG algorithm.  Different variants of AVG are compared:  a baseline without any normalization/scaling, variants with observation normalization, penultimate normalization, scaled temporal difference errors, and combinations of these. The learning curves show the average episodic return over 30 independent runs for each variant, illustrating the impact of each technique on learning performance.  The shaded regions represent the 95% confidence intervals.", "section": "5.2 Disentangling the Effects of Normalization and Scaling"}, {"figure_path": "DX5GUwMFFb/figures/figures_22_2.jpg", "caption": "Figure 6: Ablation study of normalization and scaling techniques used with AVG. We plot the learning curves of the best hyperparameter configurations for each task variant. Each solid learning curve is an average of 30 independent runs. The shaded regions represent a 95% confidence interval.", "description": "This figure shows the ablation study on the effect of normalization and scaling techniques in AVG.  The learning curves for eight different AVG variants are plotted, each representing a combination of observation normalization, penultimate normalization, and TD error scaling.  The best hyperparameter configuration for each variant was used. The results demonstrate that combining all three techniques leads to the best overall performance.", "section": "5.2 Disentangling the Effects of Normalization and Scaling"}, {"figure_path": "DX5GUwMFFb/figures/figures_25_1.jpg", "caption": "Figure 3: AVG on Gymnasium and DeepMind Control Suite tasks. Each solid learning curve is an average of 30 independent runs. The shaded regions represent a 95% confidence interval of the bootstrap distribution. Note that SAC refers to SAC with a replay buffer size of 1M. The corresponding dashed line represents the mean performance over the final 10K steps of training.", "description": "This figure compares the performance of the proposed Action Value Gradient (AVG) algorithm with other incremental and batch deep reinforcement learning algorithms on various continuous control tasks from the Gymnasium and DeepMind Control Suite.  It showcases AVG's ability to learn effectively in an incremental setting (without a replay buffer), often achieving performance comparable to batch methods which require significantly more resources.", "section": "AVG on Simulated Benchmark Tasks"}, {"figure_path": "DX5GUwMFFb/figures/figures_26_1.jpg", "caption": "Figure 3: AVG on Gymnasium and DeepMind Control Suite tasks. Each solid learning curve is an average of 30 independent runs. The shaded regions represent a 95% confidence interval of the bootstrap distribution. Note that SAC refers to SAC with a replay buffer size of 1M. The corresponding dashed line represents the mean performance over the final 10K steps of training.", "description": "This figure compares the performance of the proposed Action Value Gradient (AVG) method against other incremental methods (IAC, SAC-1, TD3-1) and a batch method (SAC) on various continuous control tasks from the Gymnasium and DeepMind Control Suite.  AVG consistently shows superior performance, often matching or exceeding that of the batch method, even with a much smaller replay buffer (or none at all). The shaded areas represent confidence intervals, showing the consistency of AVG's results.", "section": "AVG on Simulated Benchmark Tasks"}]