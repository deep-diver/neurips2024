[{"figure_path": "hNlk9cIGo9/tables/tables_1_1.jpg", "caption": "Figure 1: Optimal algorithms for user-level DP SCO. We omit logarithms, fix L = R = 1 = \u025b and n = d.", "description": "This table compares the gradient complexities and assumptions of different algorithms for user-level differentially private stochastic convex optimization (DP-SCO).  It shows that the proposed algorithms (Algorithm 3) significantly improve upon previous state-of-the-art methods in terms of computational efficiency (gradient complexity) while relaxing restrictive assumptions.", "section": "Linear-Time Algorithms"}, {"figure_path": "hNlk9cIGo9/tables/tables_2_1.jpg", "caption": "Figure 1: Optimal algorithms for user-level DP SCO. We omit logarithms, fix L = R = 1 = \u025b and n = d.", "description": "This table compares the gradient complexities and assumptions of different algorithms for user-level differentially private stochastic convex optimization (DP SCO).  It shows that the proposed Algorithm 3 achieves optimal gradient complexity under less restrictive assumptions compared to prior state-of-the-art algorithms.  The comparison is made for both smooth and non-smooth loss functions.", "section": "Linear-Time Algorithms"}, {"figure_path": "hNlk9cIGo9/tables/tables_2_2.jpg", "caption": "Figure 1: Optimal algorithms for user-level DP SCO. We omit logarithms, fix L = R = 1 = \u025b and n = d.", "description": "The table compares the gradient complexity and assumptions of different algorithms for user-level differentially private stochastic convex optimization (DP-SCO).  It shows that the proposed algorithms (Algorithm 3) achieve optimal excess risk with lower gradient complexity compared to existing methods, and without requiring restrictive assumptions on the smoothness parameter or the number of users.", "section": "Linear-Time Algorithms"}]