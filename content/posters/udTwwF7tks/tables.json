[{"figure_path": "udTwwF7tks/tables/tables_8_1.jpg", "caption": "Table 3: Comparison of the two variants of IsoNet++ (IsoNet++ (Node) and IsoNet++ (Edge)) against all the state-of-the-art graph retrieval methods, across all six datasets. Performance is measured in terms average precision (MAP) and mean HITS@20. In all cases, we used 60% training, 15% validation and 25% test sets. The numbers highlighted with green and yellow indicate the best, second best method respectively, whereas the numbers with blue indicate the best method among the baselines. (MAP values for IsoNet++ (Edge) across FM, MM and MR were verified to be not exactly the same, but they match up to the third decimal place.)", "description": "This table compares the performance of IsoNet++ (Node) and IsoNet++ (Edge) against 11 state-of-the-art graph retrieval methods across six datasets.  The performance metrics used are mean average precision (MAP) and mean HITS@20.  The table highlights the best and second-best performing methods.", "section": "4 Experiments"}, {"figure_path": "udTwwF7tks/tables/tables_8_2.jpg", "caption": "Table 4: Lazy multi-round vs. eager multi-layer. First (Last) two rows report MAP for IsoNet++ (Node) (IsoNet++ (Edge)). Green shows the best method.", "description": "This table compares the performance of lazy multi-round and eager multi-layer versions of IsoNet++ for both node and edge alignment.  MAP (Mean Average Precision) is used as the evaluation metric. The results show that the lazy multi-round approach generally outperforms the eager multi-layer approach, with the best-performing method highlighted in green.", "section": "4.2 Results"}, {"figure_path": "udTwwF7tks/tables/tables_8_3.jpg", "caption": "Table 5: Node partner vs. node pair partner interaction. First (Last) two rows report MAP for multi-round (multi-layer) update. Green shows the best method.", "description": "This table compares the performance of different model variants.  The first two rows show results for multi-round updates, comparing a model with only node partner interaction against the full IsoNet++ (Node) which incorporates node pair partner interaction. The last two rows show the same comparison but for multi-layer updates.  The highest MAP values for each dataset are highlighted in green.", "section": "4.2 Results"}, {"figure_path": "udTwwF7tks/tables/tables_17_1.jpg", "caption": "Table 8: Statistics for the 6 datasets borrowed from the TUDatasets collection [27]", "description": "This table presents key statistics for six datasets used in the paper's experiments, all sourced from the TUDatasets collection [27].  The statistics provided for each dataset (AIDS, Mutag, PTC-FM, PTC-FR, PTC-MM, PTC-MR) include the average and range of the number of nodes in the query graphs (|Vq|), the average number of edges in the query graphs (|Eq|), the average and range of the number of nodes in the corpus graphs (|Vc|), the average number of edges in the corpus graphs (|Ec|), and the proportion of relevant graph pairs (pairs(1) / (pairs(1) + pairs(0))). These statistics provide context for understanding the characteristics of the data used for evaluating the proposed subgraph matching methods.", "section": "4.1 Experimental setup"}, {"figure_path": "udTwwF7tks/tables/tables_17_2.jpg", "caption": "Table 9: Number of parameters for all models used in comparison", "description": "This table lists the number of parameters for all the models used in the paper's experiments, including both the proposed IsoNet++ models and several state-of-the-art baseline models for comparison.  The number of parameters is an indicator of model complexity.", "section": "4 Experiments"}, {"figure_path": "udTwwF7tks/tables/tables_18_1.jpg", "caption": "Table 10: Best seeds for all models. For IsoNet (Edge), GMN and IsoNet++ (Node), these are computed based on MAP score on the validation split at convergence. For other models, the identification occurs after 10 epochs of training.", "description": "This table shows the best random seed used for each model during training.  The best seed was selected based on the Mean Average Precision (MAP) score achieved on the validation set after convergence (for IsoNet (Edge), GMN, and IsoNet++ (Node)) or after 10 training epochs (for all other models). The selection of the best seed helps ensure reproducibility and consistency in the experimental results.", "section": "4.1 Experimental setup"}, {"figure_path": "udTwwF7tks/tables/tables_19_1.jpg", "caption": "Table 11: Best margin for baselines used in comparison.", "description": "This table presents the best margin hyperparameter values for each of the baseline models used in the paper's experiments, across six different datasets (AIDS, Mutag, FM, FR, MM, MR).  The margin hyperparameter is used in the asymmetric hinge loss function, a ranking loss used to train the models. For each dataset and model, the table shows the margin value that resulted in the best performance during training.", "section": "4.1 Experimental setup"}, {"figure_path": "udTwwF7tks/tables/tables_20_1.jpg", "caption": "Table 3: Comparison of the two variants of IsoNet++ (IsoNet++ (Node) and IsoNet++ (Edge)) against all the state-of-the-art graph retrieval methods, across all six datasets. Performance is measured in terms average precision (MAP) and mean HITS@20. In all cases, we used 60% training, 15% validation and 25% test sets. The numbers highlighted with green and yellow indicate the best, second best method respectively, whereas the numbers with blue indicate the best method among the baselines. (MAP values for IsoNet++ (Edge) across FM, MM and MR were verified to be not exactly the same, but they match up to the third decimal place.)", "description": "This table compares the performance of IsoNet++ (Node) and IsoNet++ (Edge) against eleven other state-of-the-art graph retrieval methods across six datasets.  The performance metrics used are Mean Average Precision (MAP) and mean HITS@20.  The table highlights the best and second-best performing methods for each dataset.", "section": "4.2 Results"}, {"figure_path": "udTwwF7tks/tables/tables_20_2.jpg", "caption": "Table 3: Comparison of the two variants of IsoNet++ (IsoNet++ (Node) and IsoNet++ (Edge)) against all the state-of-the-art graph retrieval methods, across all six datasets. Performance is measured in terms average precision (MAP) and mean HITS@20. In all cases, we used 60% training, 15% validation and 25% test sets. The numbers highlighted with green and yellow indicate the best, second best method respectively, whereas the numbers with blue indicate the best method among the baselines. (MAP values for IsoNet++ (Edge) across FM, MM and MR were verified to be not exactly the same, but they match up to the third decimal place.)", "description": "This table compares the performance of IsoNet++ (Node) and IsoNet++ (Edge) against eleven state-of-the-art methods for graph retrieval on six real-world datasets.  The performance metrics used are Mean Average Precision (MAP) and mean HITS@20. The table highlights the best and second-best performing methods.", "section": "4 Experiments"}, {"figure_path": "udTwwF7tks/tables/tables_21_1.jpg", "caption": "Table 3: Comparison of the two variants of IsoNet++ (IsoNet++ (Node) and IsoNet++ (Edge)) against all the state-of-the-art graph retrieval methods, across all six datasets. Performance is measured in terms average precision (MAP) and mean HITS@20. In all cases, we used 60% training, 15% validation and 25% test sets. The numbers highlighted with green and yellow indicate the best, second best method respectively, whereas the numbers with blue indicate the best method among the baselines. (MAP values for IsoNet++ (Edge) across FM, MM and MR were verified to be not exactly the same, but they match up to the third decimal place.)", "description": "This table compares the performance of IsoNet++ (Node) and IsoNet++ (Edge) against eleven other state-of-the-art graph retrieval methods across six datasets.  Performance is evaluated using Mean Average Precision (MAP) and HITS@20.  The table highlights the best and second-best performing models.  It notes that the MAP values for IsoNet++ (Edge) on three datasets are very close but not exactly identical.", "section": "4.2 Results"}, {"figure_path": "udTwwF7tks/tables/tables_21_2.jpg", "caption": "Table 3: Comparison of the two variants of IsoNet++ (IsoNet++ (Node) and IsoNet++ (Edge)) against all the state-of-the-art graph retrieval methods, across all six datasets. Performance is measured in terms average precision (MAP) and mean HITS@20. In all cases, we used 60% training, 15% validation and 25% test sets. The numbers highlighted with green and yellow indicate the best, second best method respectively, whereas the numbers with blue indicate the best method among the baselines. (MAP values for IsoNet++ (Edge) across FM, MM and MR were verified to be not exactly the same, but they match up to the third decimal place.)", "description": "This table compares the performance of IsoNet++ (Node) and IsoNet++ (Edge) against eleven state-of-the-art graph retrieval methods across six datasets.  Performance is evaluated using Mean Average Precision (MAP) and HITS@20.  The table highlights the best and second-best performing methods.", "section": "4.2 Results"}, {"figure_path": "udTwwF7tks/tables/tables_22_1.jpg", "caption": "Table 3: Comparison of the two variants of IsoNet++ (IsoNet++ (Node) and IsoNet++ (Edge)) against all the state-of-the-art graph retrieval methods, across all six datasets. Performance is measured in terms average precision (MAP) and mean HITS@20. In all cases, we used 60% training, 15% validation and 25% test sets. The numbers highlighted with green and yellow indicate the best, second best method respectively, whereas the numbers with blue indicate the best method among the baselines. (MAP values for IsoNet++ (Edge) across FM, MM and MR were verified to be not exactly the same, but they match up to the third decimal place.)", "description": "This table compares the performance of IsoNet++ (Node) and IsoNet++ (Edge) against eleven state-of-the-art methods for graph retrieval on six real-world datasets.  Performance is evaluated using Mean Average Precision (MAP) and mean HITS@20.  The table highlights the best and second-best performing methods.", "section": "4 Experiments"}, {"figure_path": "udTwwF7tks/tables/tables_24_1.jpg", "caption": "Table 3: Comparison of the two variants of IsoNet++ (IsoNet++ (Node) and IsoNet++ (Edge)) against all the state-of-the-art graph retrieval methods, across all six datasets. Performance is measured in terms average precision (MAP) and mean HITS@20. In all cases, we used 60% training, 15% validation and 25% test sets. The numbers highlighted with green and yellow indicate the best, second best method respectively, whereas the numbers with blue indicate the best method among the baselines. (MAP values for IsoNet++ (Edge) across FM, MM and MR were verified to be not exactly the same, but they match up to the third decimal place.)", "description": "This table compares the performance of IsoNet++ (Node) and IsoNet++ (Edge) against eleven state-of-the-art graph retrieval methods across six datasets.  The performance metrics used are Mean Average Precision (MAP) and mean HITS@20.  The table highlights the best and second-best performing methods.", "section": "4 Experiments"}, {"figure_path": "udTwwF7tks/tables/tables_25_1.jpg", "caption": "Table 3: Comparison of the two variants of IsoNet++ (IsoNet++ (Node) and IsoNet++ (Edge)) against all the state-of-the-art graph retrieval methods, across all six datasets. Performance is measured in terms average precision (MAP) and mean HITS@20. In all cases, we used 60% training, 15% validation and 25% test sets. The numbers highlighted with green and yellow indicate the best, second best method respectively, whereas the numbers with blue indicate the best method among the baselines. (MAP values for IsoNet++ (Edge) across FM, MM and MR were verified to be not exactly the same, but they match up to the third decimal place.)", "description": "This table compares the performance of IsoNet++ (Node) and IsoNet++ (Edge) against eleven other state-of-the-art graph retrieval methods across six datasets.  The performance metrics used are Mean Average Precision (MAP) and mean HITS@20.  The table highlights the best and second-best performing methods for each dataset.", "section": "4.2 Results"}, {"figure_path": "udTwwF7tks/tables/tables_27_1.jpg", "caption": "Table 19: MAP and inference time trade-off of variants of multi-round lazy IsoNet++ (Node) with fixed T. Rows colored green indicate the best K according to the MAP score.", "description": "This table presents a comparison of the mean average precision (MAP) and inference time for different values of K (number of GNN layers) in the multi-round lazy IsoNet++ (Node) model, while keeping T (number of rounds) fixed at 3.  The results show how the model's performance and computational cost change as the number of layers increases. The best performing K in terms of MAP is highlighted.", "section": "G.6 Variation of IsoNet++ (Node) and IsoNet++ (Edge) with different T and K"}, {"figure_path": "udTwwF7tks/tables/tables_27_2.jpg", "caption": "Table 19: MAP and inference time trade-off of variants of multi-round lazy IsoNet++ (Node) with fixed T. Rows colored green indicate the best K according to the MAP score.", "description": "This table shows a comparison of the Mean Average Precision (MAP) and inference time (in milliseconds) for different values of K (number of GNN layers) in each round of the multi-round lazy IsoNet++ (Node) model. The number of rounds, T, is kept constant at 3. The table demonstrates the trade-off between accuracy and computational cost as K increases.", "section": "G.6 Variation of IsoNet++ (Node) and IsoNet++ (Edge) with different T and K"}, {"figure_path": "udTwwF7tks/tables/tables_27_3.jpg", "caption": "Table 21: MAP and inference time trade-off of variants of multi-layer eager IsoNet++ (Node) with increasing K. Rows colored green and yellow indicate the best and second best K according to the MAP score.", "description": "This table shows the results of experiments evaluating the trade-off between accuracy (measured by Mean Average Precision, or MAP) and inference time for different values of K in the multi-layer eager variant of IsoNet++ (Node).  The best performing K value is highlighted in green, indicating the optimal balance between accuracy and inference speed.", "section": "G.6 Variation of IsoNet++ (Node) and IsoNet++ (Edge) with different T and K"}, {"figure_path": "udTwwF7tks/tables/tables_27_4.jpg", "caption": "Table 24: MAP and inference time trade-off of variants of multi-layer eager IsoNet++ (Edge) with increasing K. Rows colored green and yellow indicate the best and second best K according to the MAP score.", "description": "This table shows the performance (MAP) and inference time of the multi-layer eager variant of IsoNet++ (Edge) with varying number of layers (K).  The best performing K value is highlighted in green, and the second best is highlighted in yellow. The inference time increases linearly with K.", "section": "G.6 Variation of IsoNet++ (Node) and IsoNet++ (Edge) with different T and K"}, {"figure_path": "udTwwF7tks/tables/tables_27_5.jpg", "caption": "Table 26: MAP and inference time trade-off of variants of IsoNet++ (Edge) with fixed K. Rows colored green and yellow indicate the best and second best K according to the MAP score.", "description": "This table shows the results of experiments comparing different numbers of propagation layers (K) in the IsoNet++ (Edge) model while keeping the number of rounds (T) fixed at 3.  It shows the Mean Average Precision (MAP) achieved and the inference time. The best and second-best performing values of K for each dataset, as judged by MAP score, are highlighted.", "section": "G.6 Variation of IsoNet++ (Node) and IsoNet++ (Edge) with different T and K"}, {"figure_path": "udTwwF7tks/tables/tables_27_6.jpg", "caption": "Table 22: MAP and inference time trade-off of variants of multi-round lazy IsoNet++ (Edge) with fixed T. Rows colored green indicate the best K according to the MAP score.", "description": "This table shows the relationship between the mean average precision (MAP) and inference time for the multi-round lazy version of the IsoNet++ (Edge) model.  The number of rounds (T) is held constant at 3, while the number of GNN layers in each round (K) is varied. The table highlights how the MAP score and inference time change as K increases. This analysis helps in understanding the trade-off between model accuracy and computational cost.", "section": "G.6 Variation of IsoNet++ (Node) and IsoNet++ (Edge) with different T and K"}, {"figure_path": "udTwwF7tks/tables/tables_27_7.jpg", "caption": "Table 23: MAP and inference time trade-off of variants of multi-round lazy IsoNet++ (Edge) with fixed K. Rows colored green and yellow indicate the best and second best T according to the MAP score.", "description": "This table shows the results of experiments on multi-round lazy IsoNet++ (Edge) model with a fixed number of GNN layers (K=5) and varying number of rounds (T=3,4,5).  The Mean Average Precision (MAP) and inference time (in milliseconds) are reported for AIDS and Mutag datasets. The best performing T value in terms of MAP is highlighted in green, and the second best in yellow. The table demonstrates the trade-off between accuracy and inference time as the number of rounds increases.", "section": "G.6 Variation of IsoNet++ (Node) and IsoNet++ (Edge) with different T and K"}, {"figure_path": "udTwwF7tks/tables/tables_27_8.jpg", "caption": "Table 23: MAP and inference time trade-off of variants of multi-round lazy IsoNet++ (Edge) with fixed K. Rows colored green and yellow indicate the best and second best T according to the MAP score.", "description": "This table shows how the mean average precision (MAP) and inference time vary for the multi-round lazy version of IsoNet++ (Edge) when the number of rounds (T) is changed while keeping the number of layers (K) constant. The best and second-best values of T according to the MAP score are highlighted in green and yellow, respectively.", "section": "G.6 Variation of IsoNet++ (Node) and IsoNet++ (Edge) with different T and K"}, {"figure_path": "udTwwF7tks/tables/tables_28_1.jpg", "caption": "Table 24: MAP and inference time trade-off of variants of multi-layer eager IsoNet++ (Edge) with increasing K. Rows colored green and yellow indicate the best and second best K according to the MAP score.", "description": "This table shows the results of experiments evaluating the trade-off between accuracy (measured by Mean Average Precision, MAP) and inference time for the multi-layer eager variant of the IsoNet++ (Edge) model.  Different values of K (number of propagation steps in the GNN) were used, and the MAP and inference time are reported for each. The best and second-best performing values of K, based on MAP, are highlighted.", "section": "G.6 Variation of IsoNet++ (Node) and IsoNet++ (Edge) with different T and K"}, {"figure_path": "udTwwF7tks/tables/tables_28_2.jpg", "caption": "Table 25: MAP and inference time trade-off of variants of GMN with increasing K. Rows colored green and yellow indicate the best and second best K according to the MAP score.", "description": "This table presents the results of experiments evaluating the impact of varying the number of propagation layers (K) in the GMN model on its mean average precision (MAP) and inference time.  The experiment involved varying K while keeping other parameters constant and measuring MAP and inference time across six datasets (AIDS, Mutag, FM, FR, MM, MR). The best and second-best performing values of K according to MAP scores are highlighted.", "section": "G.6 Variation of IsoNet++ (Node) and IsoNet++ (Edge) with different T and K"}, {"figure_path": "udTwwF7tks/tables/tables_28_3.jpg", "caption": "Table 25: MAP and inference time trade-off of variants of GMN with increasing K. Rows colored green and yellow indicate the best and second best K according to the MAP score.", "description": "This table shows the result of experiments on the effect of varying the number of propagation layers (K) on the performance of the GMN model.  It presents the Mean Average Precision (MAP) and inference time for different values of K (5, 8, 10, and 12). The best and second-best performing values of K, based on MAP scores, are highlighted in green and yellow, respectively.", "section": "G.6 Variation of IsoNet++ (Node) and IsoNet++ (Edge) with different T and K"}, {"figure_path": "udTwwF7tks/tables/tables_28_4.jpg", "caption": "Table 26: MAP and inference time trade-off of variants of IsoNet (Edge) for varying K. We observe that the model does not improve with increasing K.", "description": "This table shows the results of experiments evaluating the effect of varying the number of propagation layers (K) in IsoNet (Edge) on the mean average precision (MAP) and inference time.  The results indicate that increasing K beyond a certain point does not lead to improved MAP scores, suggesting a potential trade-off between model complexity and performance.", "section": "G.6 Variation of IsoNet++ (Node) and IsoNet++ (Edge) with different T and K"}, {"figure_path": "udTwwF7tks/tables/tables_29_1.jpg", "caption": "Table 27: Inference time contribution of embedding computation and matrix updates by multi-layer and multi-round IsoNet++ (Node) and IsoNet++ (Edge) models.", "description": "This table shows a breakdown of the inference time for different model variations.  It separates the time spent on embedding computation from the time spent on updating the alignment matrices. The results highlight that for multi-layer models, matrix updates are the most time-consuming part, while in multi-round models, embedding computation and matrix updates have a more even contribution to the total inference time. This difference stems from how frequently the alignment matrices are updated (at every layer vs. at the end of each round).", "section": "G.7 Contribution of refining alignment matrix in inference time"}, {"figure_path": "udTwwF7tks/tables/tables_30_1.jpg", "caption": "Table 3: Comparison of the two variants of IsoNet++ (IsoNet++ (Node) and IsoNet++ (Edge)) against all the state-of-the-art graph retrieval methods, across all six datasets. Performance is measured in terms average precision (MAP) and mean HITS@20. In all cases, we used 60% training, 15% validation and 25% test sets. The numbers highlighted with green and yellow indicate the best, second best method respectively, whereas the numbers with blue indicate the best method among the baselines. (MAP values for IsoNet++ (Edge) across FM, MM and MR were verified to be not exactly the same, but they match up to the third decimal place.)", "description": "This table compares the performance of IsoNet++ (Node) and IsoNet++ (Edge) against eleven state-of-the-art methods for graph retrieval on six datasets.  The performance metrics used are Mean Average Precision (MAP) and mean HITS@20.  The table highlights the best and second-best performing methods.", "section": "4 Experiments"}]