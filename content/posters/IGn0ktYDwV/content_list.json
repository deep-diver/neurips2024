[{"type": "text", "text": "SAMPa: Sharpness-aware Minimization Parallelized ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Wanyun Xie Thomas Pethick Volkan Cevher EPFL (LIONS) EPFL (LIONS) EPFL (LIONS) wanyun.xie@epfl.ch thomas.pethick@epfl.ch volkan.cevher@epfl.ch ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Sharpness-aware minimization (SAM) has been shown to improve the generalization of neural networks. However, each SAM update requires sequentially computing two gradients, effectively doubling the per-iteration cost compared to base optimizers like SGD. We propose a simple modification of SAM, termed SAMPa, which allows us to fully parallelize the two gradient computations. SAMPa achieves a twofold speedup of SAM under the assumption that communication costs between devices are negligible. Empirical results show that SAMPa ranks among the most efficient variants of SAM in terms of computational time. Additionally, our method consistently outperforms SAM across both vision and language tasks. Notably, SAMPa theoretically maintains convergence guarantees even for fixed perturbation sizes, which is established through a novel Lyapunov function. We in fact arrive at SAMPa by treating this convergence guarantee as a hard requirement\u2014an approach we believe is promising for developing SAM-based methods in general. Our code is available at https://github.com/LIONS-EPFL/SAMPa. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The rise in deep neural network (DNN) usage has spurred a resource examination of training optimization methods, particularly focusing on bolstering their generalization ability. Generalization refers to a DNN\u2019s proficiency in effectively processing and responding to new, previously unseen data originating from the same distribution as the training dataset. A DNN with robust generalizability can reliably perform well on real-world tasks, when confronted with novel data instances or when quantized. ", "page_idx": 0}, {"type": "text", "text": "Improving generalization poses a significant challenge in machine learning. Recent studies suggest that smoother loss landscapes lead to better generalization [Keskar et al., 2017, Jiang\\* et al., 2020]. Motivated by this concept, Sharpness-Aware Minimization (SAM) has emerged as a promising optimization approach [Foret et al., 2021, Zheng et al., 2021, Wu et al., 2020b]. It is the current state-of-the-art to seek flat minima by solving a min-max optimization problem, in which the inner maximizer quantifies the sharpness as the maximized change of training loss and the minimizer both the vanilla training loss and the sharpness. As a result, SAM significantly improves the generalization ability of the trained DNNs which has been observed across various supervised learning tasks in both vision and language domains [Foret et al., 2021, Bahri et al., 2021, Zhong et al., 2022]. Moreover, some variants of SAM improve its generalization further [Kwon et al., 2021, Kim et al., 2022]. ", "page_idx": 0}, {"type": "text", "text": "Although SAM and some variants achieve remarkable generalization improvement, they increase the computational overhead of the given base optimizers. In SAM algorithm [Foret et al., 2021], each update consists of two forward-backward computations: one for computing the perturbation and the other for computing the update direction. Since these two computations are not parallelizable, SAM doubles the training time compared to the standard empirical risk minimization (ERM). ", "page_idx": 0}, {"type": "text", "text": "Several variants of SAM have been proposed to improve its efficiency. A common strategy involves integrating SAM with base optimizers in an alternating fashion like RST [Zhao et al., 2022b], LookSAM [Liu et al., 2022], and AE-SAM [Jiang et al., 2023]. Moreover, ESAM [Du et al., 2022a] uses fewer samples and updates fewer parameters to decrease the computational cost. However, some of these algorithms are suboptimal and their computational time overhead cannot be ignored completely. Du et al. [2022b] utilize loss trajectory instead of a single ascent step to estimate sharpness, albeit at the expense of memory consumption due to the storage of historical outputs or past models. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Since the runtime of SAM critically depends on the sequential computation of its gradients, we ask ", "page_idx": 1}, {"type": "text", "text": "Can we perform these two gradient computations in parallel? ", "page_idx": 1}, {"type": "text", "text": "In the sequel, we will answer this question in the affirmative. Note that since the second gradient computation highly depends on the first one seeking the worst case around the neighborhood, it is challenging to break the sequential relationship between two gradients in one update. ", "page_idx": 1}, {"type": "text", "text": "To this end, we introduce a new optimization sequence that allows us to parallelize these two gradient computations completely. Furthermore, we also integrate the optimistic gradient descent method with our parallelized version of SAM. Our final algorithm, named SAMPa, not only allows for a theoretical speedup up to $2\\times$ when there is no communication overhead but also improves the generalization further. Specifically, we make the following contributions: ", "page_idx": 1}, {"type": "text", "text": "\u2022 Parallelized formulation of SAM. We propose a novel parallelized solution for SAM, which breaks the sequential nature of two gradient computations in each SAM\u2019s update. It enables the simultaneous calculation of both gradients, potentially halving the computational time compared to vanilla SAM. We also integrate this parallelized method with the optimistic gradient descent method, known for its stabilizing properties, finalized to SAMPa. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Convergence guarantees. Our theoretical analysis establishes a novel Lyapunov function, through which we prove convergence guarantees of SAMPa even with a fixed perturbation size. We arrive at SAMPa by treating this convergence guarantee as a hard requirement, which we believe is promising for developing other SAM-based methods. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Improved generalization and efficiency. Our numerical evidence shows that SAMPa significantly reduces overall computational time even with a basic implementation while achieving superior generalization performance. Indeed, SAMPa requires the least computational time compared to the other four efficient SAM variants while enhancing generalization across different tasks. Notably, the relative improvement from SAM to SAMPa is $62.07\\%$ on CIFAR-10 and $32.65\\%$ on CIFAR-100, comparable to the gains from SGD to SAM. SAMPa also shows benefits on a large-scale dataset (ImageNet-1K), image and NLP fine-tuning tasks, as well as noisy label tasks, with the capability to integrate with other SAM variants. ", "page_idx": 1}, {"type": "text", "text": "2 Background and Challenge of SAM ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "This section starts with a brief introduction to SAM and its sequential nature of gradient computations. Subsequently, we discuss naive attempts including an approach from existing literature and our initial attempt which serve as essential motivation for constructing our final algorithm in the next section. ", "page_idx": 1}, {"type": "text", "text": "2.1 SAM and its challenge ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Motivated by the concept of minimizing sharpness to enhance generalization, SAM attempts to enforce small loss around the neighborhood in the parameter space [Foret et al., 2021]. It is formalized by a minimax problem ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x}\\operatorname*{max}_{\\epsilon:\\|\\epsilon\\|\\leq\\rho}f(x+\\epsilon)\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $f$ is a model parametrized by a weight vector $x$ , and $\\rho$ is the radius of considered neighborhood. ", "page_idx": 1}, {"type": "text", "text": "The inner maximization of Equation (1) seeks for maxima around the neighborhood. To address the inner maximization problem, Foret et al. [2021] employ a first-order Taylor expansion of $f(x+\\epsilon)$ with respect to $\\epsilon$ in proximity to 0. This approximation yields: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\epsilon^{\\star}=\\underset{\\epsilon:\\|\\epsilon\\|\\leq\\rho}{\\arg\\operatorname*{max}}\\,f(x+\\epsilon)\\approx\\underset{\\epsilon:\\|\\epsilon\\|\\leq\\rho}{\\arg\\operatorname*{max}}\\,f(x)+\\langle\\nabla f(x),\\epsilon\\rangle=\\rho\\frac{\\nabla f(x)}{\\|\\nabla f(x)\\|}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "SAM first obtains the perturbed weight $\\widetilde x=x+\\epsilon^{\\star}$ by this approximated worst-case perturbation and then adopts the gradient of $\\widetilde{x}$ to update  t he original weight $x$ . Consequently, the updating rule at each iteration $t$ during practical training is delineated as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\widetilde{x}_{t}=x_{t}+\\rho\\frac{\\nabla f(x_{t})}{\\|\\nabla f(x_{t})\\|},\\quad x_{t+1}=x_{t}-\\eta_{t}\\nabla f(\\widetilde{x}_{t})\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "It is apparent from SAM, that the update requires two gradient computations for each iteration, which are on the clean weight $x_{t}$ and the perturbed weight $\\widetilde{x}_{t}$ respectively. These two computations are not parallelizable because the gradient at the perturbe d  point $\\nabla f(\\widetilde x_{t})$ highly depends on the gradient $\\nabla f(x_{t})$ through the computation of the perturbation $\\widetilde{x}_{t}$ . Therefore, SAM doubles the computational overhead as well as the training time compared to base optimizers e.g., SGD. ", "page_idx": 2}, {"type": "text", "text": "2.2 Naive attempts ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The computational overhead of SAM is primarily due to the first gradient for computing the perturbation as discussed in Section 2.1. Can we avoid this additional gradient computation? Random perturbation offers an alternative to the worst-case perturbation in SAM, as made precise below: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\widetilde{x}_{t}=x_{t}+\\rho\\frac{e_{t}}{\\vert\\vert e_{t}\\vert\\vert}\\ \\ \\ \\ \\ \\mathrm{with}\\ \\ \\ \\ \\ e_{t}\\sim{\\cal{N}}(0,I)}\\\\ &{\\quad\\boldsymbol{x}^{t+1}=\\boldsymbol{x}^{t}-\\eta_{t}\\nabla f(\\widetilde{\\boldsymbol{x}}_{t})}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Unfortunately, it has been demonstrated empirically that RandSAM does not perform as well as SAM [Foret et al., 2021, Andriushchenko and Flammarion, 2022]. The poor performance of RandSAM is maybe not surprising, considering that RandSAM does not converge even for simple convex quadratics as demonstrated in Figure 1. ", "page_idx": 2}, {"type": "text", "text": "We argue that the algorithm we construct should at least be able to solve the original minimization problem. Recently, Si and Yun [2024, Thm. 3.3] very interestingly proved that SAM converges for convex and smooth objectives even with a fixed perturbation size $\\rho$ . Fixed $\\rho$ is interesting to study, firstly, because it is commonly used and successful in practice [Foret et al., 2021, Kwon et al., 2021]. Secondly, convergence results relying on decreasing perturbation size are usually agnostic to the direction of the perturbation [Nam et al., 2023, Khanh et al., 2024], so the results cannot distinguish between RandSAM and SAM, which behaves strikingly different in practice. ", "page_idx": 2}, {"type": "text", "text": "The fact that SAM uses the gradient direction $\\nabla f(x_{t})$ in the perturbation update, turns out to play an important role when showing convergence. It is thus natural to ask whether another gradient could be used instead. Inspired by the reuse of past gradients in the optimistic gradient method [Popov, 1980, Rakhlin and Sridharan, 2013, Daskalakis et al., 2017], an intuitive attempt is using the previous gradient at the perturbed model, such that $\\nabla f(y_{t})=\\nabla f(\\widetilde{x}_{t-1})$ , as outlined in the following update: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\widetilde{x}_{t}=x_{t}+\\rho\\frac{\\nabla f(\\widetilde{x}_{t-1})}{\\|\\nabla f(\\widetilde{x}_{t-1})\\|},\\quad x_{t+1}=x_{t}-\\eta_{t}\\nabla f(\\widetilde{x}_{t})\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Notice that only one gradient computation is needed for each update. However, the empirical findings detailed in Appendix B.1 reveal that OptSAM fails to match SAM and even performs worse than SGD. In fact, such failure is already apparent in a simple toy example demonstrated in Figure 1, where OptSAM fails to converge. It is not surprising to see its failure. To be specific, in contrast with the optimistic gradient method, $\\widetilde{x}_{t}$ in OptSAM represents an ascent step from $x_{t}$ while $x_{t+1}$ denotes a descent step from $x_{t}$ , making $\\nabla f(\\widetilde x_{t})$ a poor estimate of $\\nabla f(x_{t+1})$ . In the subsequent Section 3 we detail a principled way of correcting this issue by developing SAMPa. ", "page_idx": 2}, {"type": "text", "text": "Toy example. We use a toy example $f({\\bar{x}})\\,=\\,\\|x\\|^{2}$ to test if an algorithm can be optimized. We show the convergent performance of SAM, two naive attempts in this section, and SAMPa- $\\cdot\\lambda$ that is our algorithm proposed in Section 3. The results in Figure 1 demonstrate that RandSAM and OptSAM fail to converge, whereas SAM and SAMPa- $\\cdot\\lambda$ converge successfully. ", "page_idx": 2}, {"type": "image", "img_path": "IGn0ktYDwV/tmp/142e8edf8e3bc540ecf33bcceb1ec899f4a1292f0dafdef3c0c41c2be3998ef4.jpg", "img_caption": ["Figure 1: Comparison on $f(x)=\\|x\\|^{2}$ . "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Input: Initialization $x_{0}\\in\\mathbb{R}^{d}$ , initialization $y_{0}=x_{0}$ and $g_{0}=\\nabla f(y_{0},B_{0})$ , iterations $T$ , step sizes $\\{\\eta_{t}\\}_{t=0}^{T-1}$ , neighborhood size $\\rho>0$ , interpolation ratio $\\lambda$ . ", "page_idx": 3}, {"type": "text", "text": "1 for $t=0$ to $T-1$ do ", "page_idx": 3}, {"type": "text", "text": "2 Keep minibatch $B_{t}$ and sample minibatch $B_{t+1}$ .   \n3 Compute perturbed weightxt = xt + \u03c1 \u2225ggtt\u2225.   \n4 Compute the auxiliary sequence $y_{t+1}=x_{t}-\\eta_{t}g_{t}$ .   \n5 Compute gradients $\\widetilde{\\boldsymbol{g}}_{t}=\\nabla f(\\widetilde{\\boldsymbol{x}}_{t},\\boldsymbol{B}_{t})$ and $g_{t+1}=\\nabla f(y_{t+1},B_{t+1})$ in parallel.   \n6 Obtain the final gradient $G_{t}=(1-\\lambda)\\widetilde{g}_{t}+\\lambda g_{t+1}$ .   \nUpdate weights $x_{t+1}=x_{t}-\\eta_{t}G_{t}$ . ", "page_idx": 3}, {"type": "text", "text": "3 SAM Parallelized (SAMPa) ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "As discussed in Section 2.2, we wish to ensure that our developed (parallelizable) SAM variant maintains convergence in convex smooth problems even when using a fixed perturbation size. To break the sequential nature of SAM, we seek to replace the gradient $\\nabla f(x_{t})$ with another gradient $\\nabla f(y_{t})$ computed at some auxiliary sequence $(\\bar{y_{t}})_{t\\in\\mathbb{N}}$ . Provided the importance of the gradient direction $\\nabla f(x_{t})$ in the convergence proof of SAM, we are interested in picking the sequence $(y_{t})_{t\\in\\mathbb{N}}$ such that the difference $\\|\\nabla f(\\bar{x_{t}})-\\bar{\\nabla}f(y_{t})\\|$ can be controlled. Additionally, we need to ensure that $\\nabla f(\\widetilde x_{t})$ and $\\nabla f(y_{t+1})$ can be computed in parallel. ", "page_idx": 3}, {"type": "text", "text": "Considering these design constraints, we arrive at the SAMPa method that is similar to SAM apart from the gradient used in perturbation calculation is computed at the auxiliary sequence $(y_{t})_{t\\in\\mathbb{N}}$ , as illustrated in the following update: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\~~~\\widetilde{x}_{t}=x_{t}+\\rho\\frac{\\nabla f(y_{t})}{\\|\\nabla f(y_{t})\\|}}\\\\ &{\\quad~~y_{t+1}=x_{t}-\\eta_{t}\\nabla f(y_{t})}\\\\ &{\\quad~\\boldsymbol{x}_{t+1}=x_{t}-\\eta_{t}\\nabla f(\\widetilde{x}_{t})}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the particular choice $y_{t+1}$ is a direct consequence of the analysis, as discussed in Appendix C. Importantly, $\\nabla f(\\widetilde x_{t})$ and $\\nabla f(y_{t+1})$ can be computed in parallel in this case. Intuitively, if $\\nabla f(y_{t})$ and $\\nabla f(x_{t})$ are not too different then the scheme will behave like SAM. This intuition will be made precise by our potential function used in the analysis of Section 4. ", "page_idx": 3}, {"type": "text", "text": "In SAMPa the gradient at the auxiliary sequence $\\nabla f(y_{t+1})$ is only used for the perturbation update. It is reasonable to ask whether the gradient can be reused elsewhere in the update. As $y_{t+1}$ can be viewed as an extrapolated sequence of $x_{t}$ , it is directly related to the optimistic gradient descent method [Popov, 1980, Rakhlin and Sridharan, 2013, Daskalakis et al., 2017] as outlined below: ", "page_idx": 3}, {"type": "equation", "text": "$$\ny_{t+1}=x_{t}-\\eta_{t}\\nabla f(y_{t}),\\quad x_{t+1}=x_{t}-\\eta_{t}\\nabla f(y_{t+1})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "This celebrated scheme is known for its stabilizing properties as made precise through its ability to converge even for minimax problems. By simply taking a convex combination of these two convergent schemes, $x_{t+1}=(1\\bar{-}\\lambda)\\operatorname{SAMPa}(x_{t})+\\bar{\\lambda}\\operatorname{OptGD}(x_{t}).$ , we arrive at the following update rule: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\widetilde{x}_{t}=x_{t}+\\rho\\frac{\\nabla f(y_{t})}{\\left|\\left|\\nabla f(y_{t})\\right|\\right|}}\\\\ &{y_{t+1}=x_{t}-\\eta_{t}\\nabla f(y_{t})}\\\\ &{x_{t+1}=x_{t}-\\eta_{t}(1-\\lambda)\\nabla f(\\widetilde{x}_{t})-\\eta_{t}\\lambda\\nabla f(y_{t+1})}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\lambda\\in[0,1]$ . Notice that SAMPa is obtained as the special case SAMPa-0 whereas SAMPa-1 recovers OptSAM. Importantly, SAMPa- $\\lambda$ still admits parallel gradient computations and requires the same number of gradient computations as SAMPa. ", "page_idx": 3}, {"type": "text", "text": "SAMPa with stochasticity. An interesting observation in the SAM implementation is that both gradients for perturbation and correction steps have to be computed on the same batch; otherwise, SAM\u2019s performance may deteriorate compared to the base optimizer. This is validated by our ", "page_idx": 3}, {"type": "text", "text": "empirical observation in Appendix B.2 and supported by [Li and Giannakis, 2024, Li et al., 2024].   \nTherefore, we need to be careful when deploying SAMPa in practice. ", "page_idx": 4}, {"type": "text", "text": "Considering the stochastic setting, we present the finalized algorithm named SAMPa in Algorithm 1. Note that $\\widetilde{\\boldsymbol{g}_{t}}=\\nabla f(\\widetilde{\\boldsymbol{x}}_{t},\\boldsymbol{B}_{t})$ represents the stochastic gradient estimate of the model $\\widetilde{x}_{t}$ on mini-batch $B_{t}$ , and s i milarly $g_{t+1}=\\nabla f(y_{t+1},B_{t+1})$ is the gradient of the model $y_{t+1}$ on  m ini-batch $B_{t+1}$ . This ensures that the gradient $g_{t}$ , used to calculate the perturbed weight $\\widetilde{x}_{t}$ (line 3), is computed on the same batch as the gradient $\\widetilde{g}_{t}$ . As demonstrated in line 5, SAMPa   also requires 2 gradient computations for each update. De s pite this, SAMPa only needs half of the computational time of SAM because $\\widetilde g_{t}$ and $g_{t+1}$ are calculated in parallel. ", "page_idx": 4}, {"type": "text", "text": "4 Analysis ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we will show convergence of SAMPa even with a nondecreasing perturbation radius.   \nThe analysis relies on the following standard assumptions. ", "page_idx": 4}, {"type": "text", "text": "Assumption 4.1. The function $f:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ is convex. ", "page_idx": 4}, {"type": "text", "text": "Assumption 4.2. The operator $\\nabla f:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{d}$ is $L$ -Lipschitz with $L\\in(0,\\infty)$ , i.e., ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\|\\nabla f(x)-\\nabla f(y)\\|\\leq L\\|x-y\\|\\quad\\forall x,y\\in\\mathbb{R}^{n}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The direction of the gradient used in the perturbation turns out to play a crucial role in the analysis. Specifically, we will show that the auxiliary gradient $\\nabla f(y_{t})$ in SAMPa can safely be used as a replacement of the original gradient $\\nabla f(x_{t})$ in SAM, since we will be able to control their difference. This is made precise by the following potential function used in our analysis: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{V}_{t}:=f(x_{t})+\\frac{1}{2}(1-\\eta_{t}L)\\|\\nabla f(x_{t})-\\nabla f(y_{t})\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "As the potential function suggests we will be able to telescope the last term, which means that our convergence will remarkably only depend on the initial difference $\\|\\nabla f(y_{0})-\\nabla f(x_{0})\\|$ , whose dependency we can remove entirely by choosing the initialization as $x_{0}\\,=\\,y_{0}$ . See the proof of Theorem 4.4 for details. In the following lemma we establish descent of the potential function. ", "page_idx": 4}, {"type": "text", "text": "Lemma 4.3. Suppose Assumptions 4.1 and 4.2 hold. Then SAMPa satisfies the following descent inequality for $\\rho>0$ and a decreasing sequence $(\\eta_{t})_{t\\in\\mathbb{N}}$ with $\\eta_{t}\\in(0,\\operatorname*{max}\\{1,c/L\\})$ and $c\\in(0,1)$ , ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{V}_{t+1}\\leq\\mathcal{V}_{t}-\\eta_{t}(1-\\frac{\\eta_{t}L}{2})\\|\\nabla f(x_{t})\\|^{2}+\\eta_{t}^{2}\\rho^{2}C}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\begin{array}{r}{C=\\frac{1}{2}(L^{2}+L^{3}+\\frac{1}{1-c^{2}}L^{4})}\\end{array}$ . ", "page_idx": 4}, {"type": "text", "text": "Notice that $\\eta_{t}$ is importantly squared in front of the error term $\\rho^{2}C$ , while this is not the case for the term $-\\|\\dot{\\nabla}f(x_{t})\\|^{2}$ . This allows us to control the error term while still providing convergence in terms of $\\|\\nabla f(x_{t})\\|^{2}$ as made precise by the following theorem. ", "page_idx": 4}, {"type": "text", "text": "Theorem 4.4. Suppose Assumptions 4.1 and 4.2 hold. Then SAMPa satisfies the following descent inequality for $\\rho>0$ and a decreasing sequence $(\\eta_{t})_{t\\in\\mathbb{N}}$ with $\\eta_{t}\\in\\left(0,\\operatorname*{max}\\{1,1/2\\dot{L}\\}\\right)$ , ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sum_{t=0}^{T-1}\\frac{\\eta_{t}(1-\\eta_{t}L/2)}{\\sum_{\\tau=0}^{T-1}\\eta_{\\tau}(1-\\eta_{\\tau}L/2)}\\|\\nabla f(x_{t})\\|^{2}\\leq\\frac{\\Delta_{0}+C\\rho^{2}\\sum_{t=0}^{T-1}\\eta_{t}^{2}}{\\sum_{t=0}^{T-1}\\eta_{t}(1-\\eta_{t}L/2)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{min}_{t=0,\\ldots,T-1}\\|\\nabla f(x_{t})\\|^{2}=\\mathcal{O}\\big(\\frac{L\\Delta_{0}}{T}+\\frac{\\rho\\sqrt{\\Delta_{0}C}}{\\sqrt{T}}\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Remark 4.5. Convergence follows as long as $\\textstyle\\sum_{t=0}^{\\infty}\\eta_{t}=\\infty$ and $\\textstyle\\sum_{t=0}^{\\infty}\\eta_{t}^{2}<\\infty$ , since the stepsize allows the right hand side to be made arbitrar ily small. Note tha t Theorem 4.4 even allows for an increasing perturbation radius $\\rho_{t}$ , since it suffice to assume $\\textstyle\\sum_{t=0}^{\\infty}\\eta_{t}=\\infty$ and $\\textstyle\\sum_{t=0}^{\\infty}\\eta_{t}^{2}\\rho_{t}^{2}<\\infty$ . ", "page_idx": 4}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we demonstrate the benefit of SAMPa across a variety of models, datasets and tasks. It is worth noting that to enable parallel computation of SAMPa, we perform the two gradient calculations across 2 GPUs. As shown in Algorithm 1, one GPU computes $\\nabla f({\\widetilde{x}}_{t},B_{t})$ while another computes $\\nabla f(y_{t+1},B_{t+1})$ . For implementation guidance, we provide pseudo- code in Appendix E, along with algorithms detailing the integration of SAMPa with SGD and AdamW, both used as base optimizers in this section. ", "page_idx": 4}, {"type": "text", "text": "5.1 Image classification ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "CIFAR-10/100. We follow the experimental setup of Kwon et al. [2021]. We use the CIFAR-10 and CIFAR-100 datasets [Krizhevsky et al., 2009], both consisting of 50 000 training images of size $32\\times32$ , with 10 and 100 classes, respectively. For data augmentation, we apply the commonly used random cropping after padding with 4 pixels, horizontal flipping, and normalization using the statistics of the training distribution at both train and test time. We train multiple variants of VGG [Simonyan and Zisserman, 2014], ResNet [He et al., 2016], DenseNet [Huang et al., 2017] and WideResNet [Zagoruyko and Komodakis, 2016] (see Tables 1 and 2 for details) using cross entropy loss. All experiments are conducted on NVIDIA A100 GPU. ", "page_idx": 5}, {"type": "text", "text": "The models are trained using stochastic gradient descent (SGD) with a momentum of 0.9 and a weight decay of $5\\times10^{-4}$ , both as a baseline and as the base model for SAM variants. We used a batch size of 128 and a cosine learning rate schedule that starts at 0.1. The number of epochs is set to 200 for SAM and SAMPa while SGD are given 400 epochs. This is done in order to provide a computational fair comparison as SAM and SAMPa use twice as much gradient computation. Moreover, we show SAMPa-0.2 trained with 400 epochs as a reference in the last column colored gray because SAMPa\u2019s theoretical limit of the per iteration cost is comparable to SGD. Note that all SAMPa- $\\lambda$ in this section use the same number of epochs as SAM only except for the last column of Tables 1 and 2. Label smoothing with a factor of 0.1 is employed for all methods. ", "page_idx": 5}, {"type": "text", "text": "Through a grid search over $\\left\\lbrace0.01,0.05,0.1,0.2,0.4\\right\\rbrace$ using the validation dataset on CIFAR-10 with ResNet-56, SAM is assigned $\\rho$ values of 0.05 and 0.1 on CIFAR-10 and CIFAR-100 respectively, which is consistent with existing works [Foret et al., 2021, Kwon et al., 2021]. Moreover, SAMPa-0 shares the same $\\rho$ value as SAM while SAMPa-0.2 is configured with twice the value of SAM\u2019s $\\rho$ . Additionally, $\\lambda$ for SAMPa- $\\lambda$ is set at 0.2 through a grid search from 0 to 1, with intervals of 0.1, with results detailed in Appendix B.3. ", "page_idx": 5}, {"type": "text", "text": "Training data is randomly partitioned into $90\\%$ for training and $10\\%$ for validation. To prevent overfitting on the test set, we deviate from Foret et al. [2021], Kwon et al. [2021] by selecting the model with the highest validation accuracy to report test accuracy. Results are averaged over 6 independent executions and presented in Tables 1 and 2. Compared with the enhancement from SGD to SAM, the average improvement from SAM to SAMPa-0.2 reaches $62.07\\%$ and $32.65\\%$ on CIFAR-10 and CIFAR-100, respectively. ", "page_idx": 5}, {"type": "table", "img_path": "IGn0ktYDwV/tmp/e402d40f87590fa4a5f426af5b16cb147e16d1bf7fdd0e3734e970078372c213.jpg", "table_caption": ["Table 1: Test accuracies on CIFAR-10. SAMPa-0.2 outperforms SAM across all models with halved total temporal cost.\u201cTemporal cost\u201d represents the number of sequential gradient computations per update. SAMPa-0.2 with 400 epochs is included for comprehensive comparison with SGD and SAM. "], "table_footnote": [], "page_idx": 5}, {"type": "table", "img_path": "IGn0ktYDwV/tmp/6a417f536b5aa25bb12ae0505a9ddd7170feaf53f27b9f336cfd36af58ab4e5c.jpg", "table_caption": ["Table 2: Test accuracies on CIFAR-100. SAMPa-0.2 outperforms SAM across all models with halved total temporal cost. \u201cTemporal cost\u201d represents the number of sequential gradient computations per update. SAMPa-0.2 with 400 epochs is included for a comprehensive comparison. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "ImageNet-1K. We evaluate SAM and SAMPa-0.2 on ImageNet-1K [Russakovsky et al., 2015], using 90 training epochs, a weight decay of $10^{-4}$ , and a batch size of 256. Other parameters match those of CIFAR-10. Each method undergoes 3 independent experiments, with test accuracies detailed in Table 3. Note that we omit SGD experiments due to computational constraints; however, prior research confirms SAM and its variants outperform SGD [Foret et al., 2021, Kwon et al., 2021]. ", "page_idx": 6}, {"type": "table", "img_path": "IGn0ktYDwV/tmp/a71076960f4914fc0706726d7f3947485b8613dd87649a2c7d008f8ab9cd55dd.jpg", "table_caption": ["Table 3: Top1/Top5 maximum test accuracies on ImageNet-1K. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "5.2 Efficiency comparison with efficient SAM variants ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To comprehensively evaluate the efficiency gains of SAMPa compared to other variants of SAM in practical scenarios, we conduct experiments using five additional SAM variants on the CIFAR-10 dataset with ResNet-56 (detailed configuration in Appendix B.4): LookSAM [Liu et al., 2022], AE-SAM [Jiang et al., 2023], SAF [Du et al., 2022b], MESA [Du et al., 2022b], and ESAM [Du et al., 2022a]. Specifically, LookSAM alternates between SAM and a base optimizer periodically, while AESAM selectively employs SAM when detecting local sharpness. SAF and MESA eliminate the ascent step and introduce an extra trajectory loss term to reduce sharpness. ESAM leverages two strategies, Stochastic Weight Perturbation (SWP) and Sharpness-sensitive Data Selection (SDS), for efficiency. ", "page_idx": 6}, {"type": "text", "text": "The number of sequentially computed gradients, as shown in Figure 2a, serves as a metric for computational time in an ideal scenario. Notably, SAMPa, SAF, and MESA require the fewest number of sequential gradients, each needing only half of SAM\u2019s. Specifically, SAF and MESA necessitate just one gradient computation per update, while SAMPa parallelizes two gradients per update. ", "page_idx": 6}, {"type": "text", "text": "However, real-world computational time encompasses more than just gradient computation; it includes forward and backward pass time, weight revision time, and potential communication overhead in distributed settings. Therefore, we present the actual training time in Figure 2b, revealing that SAMPa and SAF serve as the most efficient methods. LookSAM and AE-SAM, unable to entirely avoid computing two sequential gradients per update, exhibit greater time consumption than SAMPa as expected. MESA, requiring an additional forward step compared to the base optimizer during implementation, cannot halve the computation time relative to SAM\u2019s. Regarding ESAM, we solely integrate SWP in this experiment, as no efficiency advantage is observed compared to SAM when SDS is included. The reported time of $\\mathrm{SAMPa}\u20130.2$ in Figure 2b includes $7.5\\%$ communication overhead across GPUs. Achieving a nearly $2\\times$ speedup in runtime could be possible with faster interconnects between GPUs. In addition, the test accuracies and the wall-clock time per epoch are reported in Table 4. SAMPa-0.2 achieves strong performance and meanwhile requires near-minimal computational time. ", "page_idx": 6}, {"type": "image", "img_path": "IGn0ktYDwV/tmp/808b9b97e95094a90b1b24b7078e16cf0b3f4fcbc592dd35feec152d6915678d.jpg", "img_caption": ["Figure 2: Computational time comparison for efficient SAM variants. SAMPa-0.2 requires near-minimal computational time in both ideal and practical scenarios. ", "(a) Number of sequential gradients "], "img_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "IGn0ktYDwV/tmp/018f24906736164a380d7f7785e2b0aa8e1a2017a28a03f4d9644a9834b97ea5.jpg", "img_caption": ["(b) Actual running time "], "img_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "IGn0ktYDwV/tmp/bf982516372fdc0ee0c72c956b5516a3b4fc00d7d7b265601a493aa321850479.jpg", "table_caption": ["Table 4: Efficient SAM variants. The best result is in bold and the second best is underlined. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "5.3 Transfer learning ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We demonstrate the benefits of SAMPa in transfer learning across vision and language domains. ", "page_idx": 7}, {"type": "text", "text": "Image fine-tuning. We conduct transfer learning experiments using the pre-trained ViT-B/16 checkpoint from Visual Transformers [Wu et al., 2020a], fine-tuning it on CIFAR-10 and CIFAR-100 datasets. AdamW is employed as the base optimizer, with gradient clipping applied at a global norm of 1. Training runs for 10 epochs, with a peak learning rate of $10^{-4}$ . Other parameters remain consistent with those outlined in Section 5.1. Results in Table 5 show the benefits of SAMPa in image fine-tuning. ", "page_idx": 7}, {"type": "table", "img_path": "IGn0ktYDwV/tmp/456b8bd2fee70da2620cbaf816186685c2e9a81d9b0395af0767fdb05dd6c942.jpg", "table_caption": ["Table 5: Image fine-tuning. C10 and C100 represent CIFAR-10 and CIFAR-100 respectively. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "NLP fine-tuning. To explore if SAMPa can benefti the natural language processing (NLP) domain, we show empirical text classification results in this section. In particular, we use BERT-base model and finetune it on the GLUE datasets [Wang et al., 2018]. We use AdamW as the base optimizer under a linear learning rate schedule and gradient clipping with global norm 1. We set the peak learning rate to $2\\times10^{-5}$ and batch size to 32, and run 3 epochs with an exception for MRPC and WNLI which are significantly smaller datasets and where we used 5 epochs. Note that we set $\\rho=0.05$ for all datasets except for CoLA with $\\rho=0.01$ , and RTE and STS-B with $\\rho=0.005$ . The setting of $\\rho$ is uniformly applied across SAM, SAMPa-0 and SAMPa-0.1. We report the results computed over 10 independent executions in the Table 6, which demonstrates that SAMPa also beneftis in NLP domain. ", "page_idx": 7}, {"type": "table", "img_path": "IGn0ktYDwV/tmp/39b0aa61aec70543a42c20e0f1e6f438b163be6e936de4751a92f3a99618f1ed.jpg", "table_caption": ["Table 6: Test results of BERT-base fine-tuned on GLUE. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "5.4 Noisy label task ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We test on a task outside the i.i.d. setting that the method was designed for. Following Foret et al. [2021] we consider label noise, where a fraction of the labels in the training set are corrupted to another label sampled uniformly at random. Through a grid search over $\\{0.005,0.01,0.05,0.1,0.2\\}$ , we set $\\rho=0.1$ for SAM, SAMPa-0 and SAMPa-0.2 except for adjusting $\\rho=0.01$ when the noise rate is $80\\%$ . Other experimental setup is the same as in Section 5.1. We find that SAMPa-0.2 enjoys better robustness to label noise than SAM. ", "page_idx": 7}, {"type": "table", "img_path": "IGn0ktYDwV/tmp/a3ecd9cb7e8211206a5121cee81c371bfc6b12291b81c9d107c907fd9f7e78cd.jpg", "table_caption": ["Table 7: Test accuracies of ResNet-32 models trained on CIFAR-10 with label noise. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "5.5 Incorporation with other SAM variants ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We demonstrate the potential of SAMPa to enhance generalization further by integrating it with other variants of SAM. Specifically, we examine the results of combining SAMPa with five SAM variants: mSAM [Foret et al., 2021, Behdin et al., 2023], ASAM [Kwon et al., 2021], SAM-ON [Mueller et al., 2024], VaSSO [Li and Giannakis, 2024], and BiSAM [Xie et al., 2024]. Our experiments utilize Resnet-56 on CIFAR-10 trained with SAM and SAMPa-0.2, maintaining the same experimental setup as detailed in Section 5.1. Further specifics on the experimental configuration are provided in Appendix B.4. The results summarized in Table 8 underscore the seamless integration of SAMPa with these variants, leading to notable improvements in both generalization and efficiency. ", "page_idx": 7}, {"type": "table", "img_path": "IGn0ktYDwV/tmp/3f5634800a61f427ea38d9b75303b292f2960e3261d26c02957c763797520872.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "6 Related Works ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "SAM. Inspired by the strong correlation between the generalization of a model and the flat minima revealed in [Keskar et al., 2017, Jiang\\* et al., 2020], Foret et al. [2021] propose SAM seeking for a flat minima to improve generalization capability. SAM frames a minimax optimization problem that aims to achieve a minima whose neighborhoods also have low loss. To solve this minimax problem, the most popular way is using an ascent step to approximate the solution for the inner maximization problem with the fact that SAM with more ascent steps does not significantly enhance generalization [Kim et al., 2023]. Notably, SAM has demonstrated effectiveness across various supervised learning tasks in computer vision [Foret et al., 2021], with studies demonstrating the realm of NLP tasks [Bahri et al., 2021, Zhong et al., 2022]. ", "page_idx": 8}, {"type": "text", "text": "Efficient variants of SAM. Compared with base optimizers like SGD, SAM doubles computational overhead stemming from its need for an extra gradient computation for perturbation per iteration. Efforts to alleviate SAM\u2019s computational burden have yielded several strategies. Firstly, strategies integrating SAM with base optimizers in an alternating fashion have been explored. For instance, Randomized Sharpness-Aware Training (RST) [Zhao et al., 2022b] employs a Bernoulli trial to randomly alternate between the base optimizer and SAM. Similarly, LookSAM [Liu et al., 2022] periodically computes the ascent step and utilizes the previous direction to promote flatness. Additionally, Adaptive policy to Employ SAM (AE-SAM) [Jiang et al., 2023] selectively applies SAM when detecting local sharpness, as indicated by the gradient norm. ", "page_idx": 8}, {"type": "text", "text": "Efficiency improvements have also been pursued by other means. Efficient SAM (ESAM) [Du et al., 2022a] enhances efficiency by leveraging less data, employing strategies such as Stochastic Weight Perturbation and Sharpness-sensitive Data Selection to subset random variables or mini-batch elements during optimization. Moreover, Sparse SAM (SSAM) [Mi et al., 2022] and SAM-ON [Mueller et al., 2024] achieve computational gains by only perturbing a subset of the model\u2019s weights, which enhances efficiency during the backward pass when only sparse gradients are needed. Notably, Du et al. [2022b] offer alternative approaches, $S A F$ and MESA, estimating sharpness using loss trajectory instead of a single ascent step. Nonetheless, SAF requires increased memory consumption due to recording the outputs of historical models and MESA needs one extra forward pass. We compare against these methods in Section 5.2, where we find that SAMPa leads to a smaller wall-clock time. ", "page_idx": 8}, {"type": "text", "text": "7 Conclusion and Limitations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "This paper introduces Sharpness-aware Minimization Parallelized (SAMPa) that halves the temporal cost of SAM through parallelizing gradient computations. The method additionally incorporates the optimistic gradient descent method. Crucially, SAMPa beats almost all existing efficient SAM variants regarding computational time in practice. Besides efficiency, numerical experiments demonstrate that SAMPa enhances the generalization among various tasks including image classification, transfer learning in vision and language domains, and noisy label tasks. SAMPa can be integrated with other SAM variants, offering both efficiency and generalization improvements. Furthermore, we show convergence guarantees for SAMPa even with a fixed perturbation size through a novel Lyapunov function, which we believe will benefit the development of SAM-based methods. ", "page_idx": 8}, {"type": "text", "text": "Although SAMPa achieves a $2\\times$ speedup along with improved generalization, the computational resources required remain the same as SAM\u2019s, as two GPUs with equivalent memory (as discussed in Appendix D) are still needed. Future research could explore reducing costs by either: (i) eliminating the need for additional parallel computation, or (ii) reducing memory usage per GPU, making the resource requirements more affordable. Moreover, we prove convergence for SAMPa only in the specific case of $\\lambda=0$ , leaving the analysis for general $\\lambda$ as an open challenge for our future work. ", "page_idx": 8}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We thank the reviewers for their constructive feedback. This work was supported by the Swiss National Science Foundation (SNSF) under grant number 200021_205011. This work was supported by Google. This work was supported by Hasler Foundation Program: Hasler Responsible AI (project number 21043). This research was sponsored by the Army Research Office and was accomplished under Grant Number W911NF-24-1-0048. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Maksym Andriushchenko and Nicolas Flammarion. Towards understanding sharpness-aware minimization. In International Conference on Machine Learning (ICML), 2022. ", "page_idx": 9}, {"type": "text", "text": "Dara Bahri, Hossein Mobahi, and Yi Tay. Sharpness-aware minimization improves language model generalization. In Annual Meeting of the Association for Computational Linguistics, 2021.   \nKayhan Behdin, Qingquan Song, Aman Gupta, Ayan Acharya, David Durfee, Borja Ocejo, Sathiya Keerthi, and Rahul Mazumder. msam: Micro-batch-averaged sharpness-aware minimization. arXiv preprint arXiv:2302.09693, 2023.   \nConstantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng. Training GANs with optimism. arXiv preprint arXiv:1711.00141, 2017.   \nJiawei Du, Hanshu Yan, Jiashi Feng, Joey Tianyi Zhou, Liangli Zhen, Rick Siow Mong Goh, and Vincent Tan. Efficient sharpness-aware minimization for improved training of neural networks. In International Conference on Learning Representations (ICLR), 2022a.   \nJiawei Du, Daquan Zhou, Jiashi Feng, Vincent Tan, and Joey Tianyi Zhou. Sharpness-aware training for free. Advances in Neural Information Processing Systems (NeurIPS), 2022b.   \nPierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization for efficiently improving generalization. In International Conference on Learning Representations (ICLR), 2021.   \nFengxiang He, Tongliang Liu, and Dacheng Tao. Control batch size and learning rate to generalize well: Theoretical and empirical evidence. Advances in Neural Information Processing Systems (NeurIPS), 2019.   \nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Conference on Computer Vision and Pattern Recognition (CVPR), 2016.   \nGao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Conference on Computer Vision and Pattern Recognition (CVPR), 2017.   \nWeisen Jiang, Hansi Yang, Yu Zhang, and James Kwok. An adaptive policy to employ sharpnessaware minimization. arXiv preprint arXiv:2304.14647, 2023.   \nYiding Jiang\\*, Behnam Neyshabur\\*, Hossein Mobahi, Dilip Krishnan, and Samy Bengio. Fantastic generalization measures and where to find them. In International Conference on Learning Representations (ICLR), 2020.   \nNitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. In International Conference on Learning Representations (ICLR), 2017.   \nPham Duy Khanh, Hoang-Chau Luong, Boris S Mordukhovich, and Dat Ba Tran. Fundamental convergence analysis of sharpness-aware minimization. arXiv preprint arXiv:2401.08060, 2024.   \nHoki Kim, Jinseong Park, Yujin Choi, Woojin Lee, and Jaewook Lee. Exploring the effect of multi-step ascent in sharpness-aware minimization. arXiv preprint arXiv:2302.10181, 2023.   \nMinyoung Kim, Da Li, Shell X Hu, and Timothy Hospedales. Fisher SAM: Information geometry and sharpness aware minimisation. In International Conference on Machine Learning (ICML), 2022.   \nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \nJungmin Kwon, Jeongseop Kim, Hyunseo Park, and In Kwon Choi. ASAM: Adaptive sharpnessaware minimization for scale-invariant learning of deep neural networks. In International Conference on Machine Learning (ICML), 2021.   \nBingcong Li and Georgios Giannakis. Enhancing sharpness-aware optimization through variance suppression. Advances in Neural Information Processing Systems (NeurIPS), 2024.   \nTao Li, Pan Zhou, Zhengbao He, Xinwen Cheng, and Xiaolin Huang. Friendly sharpness-aware minimization. Conference on Computer Vision and Pattern Recognition (CVPR), 2024.   \nYong Liu, Siqi Mai, Xiangning Chen, Cho-Jui Hsieh, and Yang You. Towards efficient and scalable sharpness-aware minimization. In Conference on Computer Vision and Pattern Recognition (CVPR), 2022.   \nPeng Mi, Li Shen, Tianhe Ren, Yiyi Zhou, Xiaoshuai Sun, Rongrong Ji, and Dacheng Tao. Make sharpness-aware minimization stronger: A sparsified perturbation approach. Advances in Neural Information Processing Systems (NeurIPS), 2022.   \nMaximilian Mueller, Tiffany Vlaar, David Rolnick, and Matthias Hein. Normalization layers are all that sharpness-aware minimization needs. Advances in Neural Information Processing Systems (NeurIPS), 2024.   \nKyunghun Nam, Jinseok Chung, and Namhoon Lee. Almost sure last iterate convergence of sharpnessaware minimization. 2023.   \nLeonid Denisovich Popov. A modification of the arrow-hurwitz method of search for saddle points. Mat. Zametki, 28, 1980.   \nAlexander Rakhlin and Karthik Sridharan. Online learning with predictable sequences. In Conference on Learning Theory, pages 993\u20131019. PMLR, 2013.   \nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International Journal of Computer Vision (IJCV), 115, 2015.   \nDongkuk Si and Chulhee Yun. Practical sharpness-aware minimization cannot converge all the way to optima. Advances in Neural Information Processing Systems (NeurIPS), 36, 2024.   \nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.   \nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018.   \nBichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Zhicheng Yan, Masayoshi Tomizuka, Joseph Gonzalez, Kurt Keutzer, and Peter Vajda. Visual transformers: Token-based image representation and processing for computer vision. arXiv preprint arXiv:2006.03677, 2020a.   \nDongxian Wu, Shu-Tao Xia, and Yisen Wang. Adversarial weight perturbation helps robust generalization. Advances in Neural Information Processing Systems (NeurIPS), 2020b.   \nWanyun Xie, Fabian Latorre, Kimon Antonakopoulos, Thomas Pethick, and Volkan Cevher. Improving SAM requires rethinking its optimization formulation. International Conference on Machine Learning (ICML), 2024.   \nSergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "Yang Zhao, Hao Zhang, and Xiuyuan Hu. Penalizing gradient norm for efficiently improving generalization in deep learning. In International Conference on Machine Learning (ICML), 2022a. Yang Zhao, Hao Zhang, and Xiuyuan Hu. Randomized sharpness-aware training for boosting computational efficiency in deep learning. arXiv preprint arXiv:2203.09962, 2022b. Yaowei Zheng, Richong Zhang, and Yongyi Mao. Regularizing neural networks via adversarial model perturbation. In Conference on Computer Vision and Pattern Recognition (CVPR), 2021. Qihuang Zhong, Liang Ding, Li Shen, Peng Mi, Juhua Liu, Bo Du, and Dacheng Tao. Improving sharpness-aware minimization with fisher mask for better generalization on language models. arXiv preprint arXiv:2210.05497, 2022. ", "page_idx": 11}, {"type": "text", "text": "A Proofs for Section 4 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Lemma 4.3. Suppose Assumptions 4.1 and 4.2 hold. Then SAMPa satisfies the following descent inequality for $\\rho>0$ and a decreasing sequence $(\\eta_{t})_{t\\in\\mathbb{N}}$ with $\\eta_{t}\\in\\left(0,\\operatorname*{max}\\{1,c/L\\}\\right)$ and $c\\in(0,1)$ , ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{V}_{t+1}\\leq\\mathcal{V}_{t}-\\eta_{t}(1-\\frac{\\eta_{t}L}{2})\\|\\nabla f(x_{t})\\|^{2}+\\eta_{t}^{2}\\rho^{2}C}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $\\begin{array}{r}{C=\\frac{1}{2}(L^{2}+L^{3}+\\frac{1}{1-c^{2}}L^{4})}\\end{array}$ . ", "page_idx": 12}, {"type": "text", "text": "Proof. Using smoothness we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(x_{t+1})\\leq f(x_{t})+\\langle\\nabla f(x_{t}),x_{t+1}-x_{t}\\rangle+\\frac{L}{2}\\|x_{t+1}-x_{t}\\|^{2}}\\\\ &{\\qquad\\qquad=f(x_{t})-\\eta_{t}\\,\\langle\\nabla f(x_{t}),\\nabla f(\\widetilde x_{t})\\rangle+\\frac{\\eta_{t}^{2}L}{2}\\|\\nabla f(\\widetilde x_{t})\\|^{2}}\\\\ &{\\qquad\\quad=f(x_{t})-\\eta_{t}(1-\\frac{\\eta_{t}L}{2})\\|\\nabla f(x_{t})\\|^{2}+\\frac{\\eta_{t}^{2}L}{2}\\|\\nabla f(\\widetilde x_{t})-\\nabla f(x_{t})\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\quad-\\eta_{t}(1-\\eta_{t}L)\\,\\langle\\nabla f(x_{t}),\\nabla f(\\widetilde x_{t})-\\nabla f(x_{t})\\rangle}\\\\ &{\\quad\\mathrm{(Assumption~4.2)\\leq}f(x_{t})-\\eta_{t}(1-\\frac{\\eta_{t}L}{2})\\|\\nabla f(x_{t})\\|^{2}+\\frac{\\eta_{t}^{2}\\rho^{2}L^{3}}{2}}\\\\ &{\\qquad\\qquad\\qquad\\quad-\\eta_{t}(1-\\eta_{t}L)\\,\\langle\\nabla f(x_{t}),\\nabla f(\\widetilde x_{t})-\\nabla f(x_{t})\\rangle\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "The last term of (5): ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\nabla f(x_{t}),\\nabla f(\\widetilde{x}_{t})-\\nabla f(x_{t})\\rangle=\\langle\\nabla f(x_{t})-\\nabla f(y_{t})+\\nabla f(y_{t}),\\nabla f(\\widetilde{x}_{t})-\\nabla f(x_{t})\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\frac{\\|\\nabla f(y_{t})\\|}{\\rho}\\,\\langle\\widetilde{x}_{t}-x_{t},\\nabla f(\\widetilde{x}_{t})-\\nabla f(x_{t})\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\,\\langle\\nabla f(x_{t})-\\nabla f(y_{t}),\\nabla f(\\widetilde{x}_{t})-\\nabla f(x_{t})\\rangle}\\\\ &{\\qquad\\quad\\mathrm{(Assumption~4.1)}\\geq\\langle\\nabla f(x_{t})-\\nabla f(y_{t}),\\nabla f(\\widetilde{x}_{t})-\\nabla f(x_{t})\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "For the left term, using $2\\left<a,\\eta_{t}b\\right>=\\|a\\|^{2}+\\eta_{t}^{2}\\|b\\|^{2}-\\|a-\\eta_{t}b\\|^{2}$ , we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\quad\\cdots}\\\\ &{=2\\left\\langle\\nabla f(x_{t})-\\nabla f(y_{t}),\\eta_{t}(\\nabla f(x_{t})-\\nabla f(\\widetilde x_{t}))\\right\\rangle}\\\\ &{=-\\|\\nabla f(\\widetilde x_{t})-\\nabla f(y_{t})-\\big(1-\\eta_{t}\\big)(\\nabla f(\\widetilde x_{t})-\\nabla f(x_{t}))\\|^{2}+\\|\\nabla f(x_{t})-\\nabla f(y_{t})\\|^{2}}\\\\ &{\\quad\\quad+\\eta_{t}^{2}\\|\\nabla f(\\widetilde x_{t})-\\nabla f(x_{t})\\|^{2}}\\\\ &{\\le-\\frac{1}{1+e}\\|\\nabla f(\\widetilde x_{t})-\\nabla f(y_{t})\\|^{2}+\\big(\\eta_{t}^{2}+\\frac{(1-\\eta_{t})^{2}}{e}\\big)\\|\\nabla f(\\widetilde x_{t})-\\nabla f(x_{t})\\|^{2}}\\\\ &{\\quad\\quad+\\|\\nabla f(x_{t})-\\nabla f(y_{t})\\|^{2}}\\\\ &{\\le-\\frac{1}{(1+e)\\eta_{t}^{2}L^{2}}\\|\\nabla f(x_{t+1})-\\nabla f(y_{t+1})\\|^{2}+\\big(\\eta_{t}^{2}+\\frac{(1-\\eta_{t})^{2}}{e}\\big)\\|\\nabla f(\\widetilde x_{t})-\\nabla f(x_{t})\\|^{2}}\\\\ &{\\quad\\quad+\\|\\nabla f(x_{t})-\\nabla f(y_{t})\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "The first inequality is due to Young\u2019s inequality, $\\begin{array}{r}{-\\|a-b\\|^{2}\\leq-\\frac{1}{1+e}\\|a\\|^{2}+\\frac{1}{e}\\|b\\|^{2}}\\end{array}$ with $e>0$ , while the second inequality follows from ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla f(y_{t})-\\nabla f(\\widetilde x_{t})\\|^{2}=\\frac{1}{\\eta_{t}^{2}}\\|x_{t+1}-y_{t+1}\\|^{2}\\geq\\frac{1}{\\eta_{t}^{2}L^{2}}\\|\\nabla f(x_{t+1})-\\nabla f(y_{t+1})\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where the last inequality is due to Assumption 4.2. We can pick $\\begin{array}{r}{e=\\frac{1-\\eta_{t}L}{\\eta_{t}^{2}L^{2}(1-\\eta_{t+1}L)}-1}\\end{array}$ such that $\\begin{array}{r}{\\frac{1-\\eta_{t}L}{(1+e)\\eta_{t}^{2}L^{2}}=1-\\eta_{t+1}L}\\end{array}$ . To verify that $e>0$ , use that $(\\eta_{t})_{t\\in\\mathbb{N}}$ is decreasing to obtain ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1-\\eta_{t}L}{1-\\eta_{t+1}L}\\geq1\\geq\\eta_{t}^{2}L^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where the last inequality uses that $\\eta_{t}<1/L$ . Rearranging shows that $e>0$ . ", "page_idx": 12}, {"type": "text", "text": "With the particular choice of $e$ , (7) reduces to ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-2\\eta_{t}\\left\\langle\\nabla f(x_{t})-\\nabla f(y_{t}),\\nabla f(\\widetilde x_{t})-\\nabla f(x_{t})\\right\\rangle}\\\\ &{\\qquad\\qquad\\leq-\\frac{1-\\eta_{t+1}L}{1-\\eta_{t}L}\\|\\nabla f(x_{t+1})-\\nabla f(y_{t+1})\\|^{2}+\\eta_{t}^{2}(1+A_{t})\\|\\nabla f(\\widetilde x_{t})-\\nabla f(x_{t})\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\left\\Vert\\nabla f(x_{t})-\\nabla f(y_{t})\\right\\Vert^{2}}\\\\ &{\\qquad\\qquad({\\mathrm{Assumption~}4.2})\\leq-\\frac{1-\\eta_{t+1}L}{1-\\eta_{t}L}\\|\\nabla f(x_{t+1})-\\nabla f(y_{t+1})\\|^{2}+\\eta_{t}^{2}(1+A_{t})\\rho^{2}L^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\left\\Vert\\nabla f(x_{t})-\\nabla f(y_{t})\\right\\Vert^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "with $\\begin{array}{r}{A_{t}=\\frac{L^{2}(1-\\eta_{t})^{2}}{\\frac{1-\\eta_{t}L}{1-\\eta_{t+1}L}-\\eta_{t}^{2}L^{2}}}\\end{array}$ . Plugging (6) and (10) back into (5) yields ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(x_{t+1})+\\frac{1}{2}(1-\\eta_{t+1}L)\\Vert\\nabla f(x_{t+1})-\\nabla f(y_{t+1})\\Vert^{2}}\\\\ &{\\qquad\\qquad\\leq f(x_{t})+\\frac{1}{2}(1-\\eta_{t}L)\\Vert\\nabla f(x_{t})-\\nabla f(y_{t})\\Vert^{2}}\\\\ &{\\qquad\\qquad\\quad-\\eta_{t}(1-\\frac{\\eta_{t}L}{2})\\Vert\\nabla f(x_{t})\\Vert^{2}+\\frac{1}{2}\\eta_{t}^{2}\\Big((1-\\eta_{t}L)(1+A_{t})+L\\Big)L^{2}\\rho^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "What remains is to bound the latter term of (11) in terms of a constant independent of $t$ . First notice that, due to the first inequality of (9) and $\\eta_{t}L<c$ by assumption, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1-\\eta_{t}L}{1-\\eta_{t+1}L}-\\eta_{t}^{2}L^{2}>1-c^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "It follows that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{(1-\\eta_{t}L)(1-\\eta_{t})^{2}}{1-\\eta_{t}L}<\\frac{(1-\\eta_{t}L)(1-\\eta_{t})^{2}}{1-c^{2}}<\\frac{1}{1-c^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where the last inequality uses $\\begin{array}{r}{\\eta_{t}<\\frac{1}{L}}\\end{array}$ and $\\eta_{t}\\leq1$ . ", "page_idx": 13}, {"type": "text", "text": "Expanding the last term of (11), we obtain ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{2}\\big((1-\\eta_{t}L)(1+A_{t})+L\\big)L^{2}=\\frac{1}{2}\\Big(1-\\eta_{t}L+L^{2}\\frac{(1-\\eta_{t}L)(1-\\eta_{t})^{2}}{\\frac{1-\\eta_{t}L}{1-\\eta_{t+1}L}-\\eta_{t}^{2}L^{2}}+L\\Big)L^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\frac{1}{2}\\Big(1+L^{2}\\frac{(1-\\eta_{t}L)(1-\\eta_{t})^{2}}{\\frac{1-\\eta_{t}L}{1-\\eta_{t+1}L}-\\eta_{t}^{2}L^{2}}+L\\Big)L^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad(12)\\leq\\frac{1+L+\\frac{1}{1-c^{2}}L^{2}}{2}L^{2}=:C,}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "which completes the proof. ", "page_idx": 13}, {"type": "text", "text": "Theorem 4.4. Suppose Assumptions 4.1 and 4.2 hold. Then SAMPa satisfies the following descent inequality for $\\rho>0$ and a decreasing sequence $(\\eta_{t})_{t\\in\\mathbb{N}}$ with $\\eta_{t}\\in\\left(0,\\operatorname*{max}\\{1,1/2L\\}\\right)$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sum_{t=0}^{T-1}\\frac{\\eta_{t}(1-\\eta_{t}L/2)}{\\sum_{\\tau=0}^{T-1}\\eta_{\\tau}(1-\\eta_{\\tau}L/2)}\\|\\nabla f(x_{t})\\|^{2}\\leq\\frac{\\Delta_{0}+C\\rho^{2}\\sum_{t=0}^{T-1}\\eta_{t}^{2}}{\\sum_{t=0}^{T-1}\\eta_{t}(1-\\eta_{t}L/2)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{min}_{t=0,\\ldots,T-1}\\|\\nabla f(x_{t})\\|^{2}=\\mathcal{O}\\big(\\frac{L\\Delta_{0}}{T}+\\frac{\\rho\\sqrt{\\Delta_{0}C}}{\\sqrt{T}}\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof. The proof follows directly by telescoping the descent inequality from Lemma 4.3 after subtracting $\\operatorname{inf}_{x\\in\\mathbb{R}^{d}}f(x)$ from which we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sum_{t=0}^{T-1}\\frac{\\eta_{t}(1-\\eta_{t}L/2)}{\\sum_{\\tau=0}^{T-1}\\eta_{\\tau}(1-\\eta_{\\tau}L/2)}\\|\\nabla f(x_{t})\\|^{2}\\leq\\frac{\\Delta_{0}+\\frac{1}{2}\\|\\nabla f(x_{0})-\\nabla f(y_{0})\\|^{2}+C\\rho^{2}\\sum_{t=0}^{T-1}\\eta_{t}^{2}}{\\sum_{t=0}^{T-1}\\eta_{t}(1-\\eta_{t}L/2)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Using Lipschitz continuity from Assumption 4.2 we have that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\|\\nabla f(x_{0})-\\nabla f(y_{0})\\|^{2}\\leq L^{2}\\|x_{0}-y_{0}\\|^{2}=0\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where the last equality follows from picking the initialization $y_{0}=x_{0}$ . By picking $c={\\textstyle{\\frac{1}{2}}}$ , for which $C$ simplifies and $\\begin{array}{r}{\\eta_{t}<\\frac{1}{2L}}\\end{array}$ , we obtain the guarantee in (3). ", "page_idx": 13}, {"type": "text", "text": "Picking a fixed stepsize $\\eta_{t}=\\eta$ , the convergence guarantee (3) reduces to ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{T}\\sum_{t=0}^{T-1}\\|\\nabla f(x_{t})\\|^{2}\\leq\\frac{4}{3}\\big(\\frac{\\Delta_{0}}{T\\eta}+C\\rho^{2}\\eta\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Optimizing the bound suggests a stepsize of C\u2206\u03c120T . Thus, incorporating the other stepsize requirements, we set $\\begin{array}{r}{\\eta=\\operatorname*{min}\\{\\sqrt{\\frac{\\Delta_{0}}{C\\rho^{2}T}},\\operatorname*{max}\\{\\frac{1}{2L},1\\}\\}}\\end{array}$ . There are three cases. ", "page_idx": 13}, {"type": "text", "text": "Case I \u03b7 = $\\begin{array}{r}{\\eta=\\sqrt{\\frac{\\Delta_{0}}{C\\rho^{2}T}}}\\end{array}$ for which (15) reduces to ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{T}\\sum_{t=0}^{T-1}\\|\\nabla f(x_{t})\\|^{2}\\leq\\frac{8}{3}\\frac{\\rho\\sqrt{\\Delta_{0}C}}{\\sqrt{T}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Case II $\\begin{array}{r}{\\eta=\\frac{1}{2L}\\leq\\sqrt{\\frac{\\Delta_{0}}{C\\rho^{2}T}}}\\end{array}$ for which ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{T}\\sum_{t=0}^{T-1}\\|\\nabla f(x_{t})\\|^{2}\\leq\\frac{4}{3}\\big(\\frac{2L\\Delta_{0}}{T}+\\frac{\\rho\\sqrt{\\Delta_{0}C}}{\\sqrt{T}}\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Case III $\\begin{array}{r}{\\eta=1\\le\\sqrt{\\frac{\\Delta_{0}}{C\\rho^{2}T}}}\\end{array}$ we can additionally use that $\\begin{array}{r}{1\\geq\\frac{1}{2L}}\\end{array}$ , to again establish (17). Combining the three cases, we have that for any case ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{T}\\sum_{t=0}^{T-1}\\|\\nabla f(x_{t})\\|^{2}=\\mathcal{O}\\big(\\frac{L\\Delta_{0}}{T}+\\frac{\\rho\\sqrt{\\Delta_{0}C}}{\\sqrt{T}}\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Noting that the minimum is always smaller than the average completes the proof. ", "page_idx": 14}, {"type": "text", "text": "B Additional Experiments ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Failure of OptSAM ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "To demonstrate the failure of our naive attempt, described in Section 2.2 as OptSAM, we provide empirical results using identical settings in Section 5.1. As shown in Table 9, OptSAM fails to outperform SAM and even performs worse than SGD. ", "page_idx": 14}, {"type": "table", "img_path": "IGn0ktYDwV/tmp/53e20bf492da00bb167f07b7c9f47b3f2559e9a7fa104990d8769f81eb2535a9.jpg", "table_caption": ["Table 9: Test accuracies of OptSAM on CIFAR-10. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "B.2 SAM with stochasticity ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "To deploy SAM with stochasticity, we find it imperative to utilize the same batch for both gradient calculations of perturbation and correction steps. Otherwise, the performance of SAM may be even worse than the base optimizer. Our empirical observations on CIFAR-10 are shown in Table 10, which is also validated by [Li and Giannakis, 2024, Li et al., 2024]. ", "page_idx": 14}, {"type": "text", "text": "This observation demonstrates that the same batch for both perturbation and correction steps is essential. This also justifies the need for parallel gradient computation on two sequential batches in SAMPa. ", "page_idx": 14}, {"type": "table", "img_path": "IGn0ktYDwV/tmp/d1a373c27f50b63cc581dd01b832a4f4c940c4249f7d08a5d6b2ba1d5dd033d6.jpg", "table_caption": ["Table 10: Two gradients in SAM computed on the same or different batch on CIFAR-10. SAM computes them on the same batch while SAM-db is on two different batches. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "B.3 Sweep over $\\lambda$ for SAMPa ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "To investigate the impact of different values of $\\lambda$ in SAMPa, we present test accuracy curves for ResNet-56 and WRN-28-10 on CIFAR-10 in Figure 3, covering the range $\\lambda\\in[0,1]$ with the interval 0.1. Notably, SAMPa-1 corresponds to OptGD. ", "page_idx": 14}, {"type": "text", "text": "In our experiments, as reported in Table 1 and Table 2, we initially optimize $\\lambda=0.2$ using ResNet-56. This default value is applied consistently across other models to maintain a fair comparison. However, continuous tuning of $\\lambda$ may lead to improved performance on different model architectures, as demonstrated in Figure 3b. ", "page_idx": 14}, {"type": "image", "img_path": "IGn0ktYDwV/tmp/537076f7e8f2a59a1ee75cdce517221c4a69bb2f5b4c2867296f5199dd121309.jpg", "img_caption": ["Figure 3: Test accuracy curve obtained from SAMPa algorithm using a range of $\\lambda$ . "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "B.4 Hyperparameters for variants of SAM ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We present a comparison between SAMPa and various variants of SAM in Table 4 and Table 8. All algorithms in these tables utilize Resnet-56 on CIFAR-10 with hyperparameters mostly consistent with those used in Table 1. However, some variants require additional or different hyperparameters, which are listed below: ", "page_idx": 15}, {"type": "text", "text": "\u2022 LookSAM [Liu et al., 2022]: Update frequency $k=5$ , scaling factor $\\alpha=0.7$ .   \n\u2022 AE-SAM [Jiang et al., 2023]: $\\rho=0.2$ , forgetting rate $\\delta=0.9$ .   \n\u2022 MESA [Du et al., 2022b]: Starting epoch $E_{\\mathrm{start}}=5$ , coefficients $\\lambda=0.8$ , Decay factor $\\beta=0.9995$ .   \n\u2022 ESAM [Du et al., 2022a]: SWP probability $\\beta=0.5$ .   \n\u2022 mSAM [Foret et al., 2021]: Size of micro batch $m=32$ .   \n\u2022 ASAM [Kwon et al., 2021]: $\\rho=0.5$ .   \n\u2022 SAM-ON [Mueller et al., 2024]: $\\rho=0.5$ .   \n\u2022 VaSSO [Li and Giannakis, 2024]: Linearization parameter $\\theta=0.4$ .   \n\u2022 BiSAM Xie et al. [2024]: BiSAM (-log) with $\\mu=1$ . ", "page_idx": 15}, {"type": "text", "text": "We adhere to the default values specified in the original papers and maintain consistent naming conventions. Following the experimental setup detailed in Section 5.1, we set $\\rho\\times2$ for SAMPa in Section 5.5 when incorporated with the algorithms, while keeping other parameters consistent with their defaults. Note that these parameters are not tuned to ensure a fair comparison and avoid waste of computing resources. ", "page_idx": 15}, {"type": "text", "text": "B.5 mSAM with 2 GPUs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Since SAMPa parallelizes two gradient computations across two GPUs, we implement mSAM [Behdin et al., 2023], a SAM variant that achieves data parallelism, for a fair comparison of runtime. Based on experiments in Section 5.2, mSAM $\\scriptstyle({\\mathrm{m}}=2)$ uses two GPUs and each computes gradient for 64 samples. While mSAM\u2019s total computation time for batch sizes of 64 and 128 is similar, its wall-clock time is slightly longer due to the added communication overhead between GPUs. This highlights the need for gradient parallelization. ", "page_idx": 15}, {"type": "table", "img_path": "IGn0ktYDwV/tmp/615709b1ac218456e9f554d43d926b21cce2b058c74012f5c34cfddfff6973f2.jpg", "table_caption": ["Table 11: Runtime of SAM variants on 2 GPUs. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "We also provide the runtime per batch of SGD across various batch sizes in Table 12. The results show that data parallelism reduces time efficiently only when the batch size is sufficiently large. However, excessively large batch sizes can negatively affect generalization [He et al., 2019]. ", "page_idx": 15}, {"type": "table", "img_path": "IGn0ktYDwV/tmp/a829e58088dc1d5e9acbc85cc36577b175320efa73537dfb9d3a3452e925ce6f.jpg", "table_caption": ["Table 12: Runtime per batch/epoch of different batch sizes. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "B.6 SAMPa- $\\cdot\\lambda$ v.s. the gradient penalization method ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "SAMPa- $\\lambda$ takes a convex combination of the two convergent schemes $x_{t+1}=(1-\\lambda)$ $\\mathrm{SAMPa}(x_{t})+$ $\\lambda\\operatorname{OptGD}(x_{t})$ , which is similar with a gradient penalization method [Zhao et al., 2022a] doing $x_{t+1}=(1-\\lambda)\\operatorname{SAM}(x_{t})+\\lambda\\operatorname{SGD}(x_{t})$ . However, it is important to note that SAMPa- $\\cdot\\lambda$ differs in a key aspect: it computes gradients for each update on two different batches (as shown in line 6 of Algorithm 1), while the penalizing method combines gradients from the same batch. ", "page_idx": 16}, {"type": "text", "text": "We conducted preliminary experiments on CIFAR-10 using the penalizing method with the same hyperparameters as SAMPa-0.2. The results indicate similar performance in standard classification tasks but show worse outcomes with noisy labels. Further investigation into this discrepancy may provide insights into SAMPa\u2019s superior performance. ", "page_idx": 16}, {"type": "table", "img_path": "IGn0ktYDwV/tmp/2721fd77c6cf6e877f295b2cfc7b67428f204c1b9d6047c543eceba77619b8dd.jpg", "table_caption": ["Table 13: Test accuracy of the gradient penalization method. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "The choice of $y_{t+1}$ ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The particular choice of $y_{t+1}$ in SAMPa is a direct consequence of the analysis. Specifically, in Equation (8) of the proof, the choice $y_{t+1}\\,=\\,x_{t}\\,-\\,\\eta_{t}\\nabla f(y_{t})$ allows us to produce the term $\\|\\nabla f(x_{t+1})-\\nabla f(y_{t+1})\\|^{2}$ in order to telescope with $\\|\\nabla f(x_{t})-\\nabla f(y_{t})\\|^{2}$ in Equation (7). This is what we refer to in Section 3, when mentioning that we will pick $y_{t}$ such that $\\|\\nabla f(x_{t})-\\nabla f(y_{t})\\|^{2}$ (i.e. the discrepancy from SAM) can be controlled. This gives a precise guarantee explaining why $\\nabla f(x_{t})$ can be replaced by $\\nabla f(y_{t})$ . ", "page_idx": 16}, {"type": "text", "text": "Additionally, the small difference between the perturbations based on $\\nabla f(x_{t})$ and $\\nabla f(y_{t})$ suggests that $\\nabla f(y_{t})$ serves as an effective approximation of $\\nabla f(x_{t})$ in practice. In Figure 4, we track the cosine similarity and Euclidean distance between $\\nabla f(y_{t})$ and $\\nabla f(x_{t})$ throughout the training process of ResNet-56 on CIFAR-10 . We find that the cosine similarity keeps above 0.99 during the whole training process, and in most period it\u2019s around 0.998, while at the end of training it is even close to 1. This indicates that SAMPa\u2019s estimated perturbation is an excellent approximation of SAM\u2019s perturbation. ", "page_idx": 16}, {"type": "text", "text": "Moreover, the Euclidean distance decreases and is close to zero at the end of training. This matches our theoretical analysis that $\\|\\nabla f(x_{t})-\\nabla f(y_{t})\\|^{2}$ eventually becomes small, which lemma 4.3 guarantees in the convex case by establishing the decrease of the potential function $\\mathcal{V}_{t}$ . ", "page_idx": 16}, {"type": "image", "img_path": "IGn0ktYDwV/tmp/4622233ffc76df2d1c1ba464d7b4feddcb0ee734ecc411d79eb05f014a0fbfbb.jpg", "img_caption": ["(a) Cosine similarity ", "Figure 4: Difference between $\\nabla f(x_{t})$ and $\\nabla f(y_{t})$ . "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "IGn0ktYDwV/tmp/505c464acfeadeaa8f2cb62a9309e57d6c04b0d3e0d1cfbe69140e1fce8c4c25.jpg", "img_caption": ["(b) Euclidean distance "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "D Discussion of memory usage ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "From the implementation perspective, it is worth discussing the memory usage of SAMPa compared with SAM. As depicted in SAM, SAM necessitates the storage of a maximum of two sets of model weights $(x_{t},\\tilde{x}_{t})$ , along with one gradient $(\\nabla f(x_{t}))$ , and one mini-batch $(\\boldsymbol{B}_{t})$ for each update. Beneftiing from 2 GPUs\u2019 deployment, SAMPa- $\\lambda$ requires the same memory usage as SAM on each GPU, specifically needing two model weights $(x_{t},\\tilde{x}_{t}$ or $y_{t+1}$ ), one gradient $\\textstyle\\nabla f(x_{t})$ or $\\nabla f(y_{t+1}))$ , and one mini-batch $B_{t}$ or $B_{t+1}$ ). ", "page_idx": 17}, {"type": "text", "text": "We present a memory usage comparison in Table 14 for all SAM variants introduced in Section 5.2. Notably, SAMPa-0.2 requires slightly less memory per GPU, while MESA consumes approximately $23\\%$ more memory than SAM. The other three methods have comparable memory usage to SAM. However, it\u2019s important to note that memory usage is highly dependent on the size of the model and dataset, particularly for SAF and MESA, which store historical model outputs or weights. ", "page_idx": 17}, {"type": "table", "img_path": "IGn0ktYDwV/tmp/98ea32ad68a295102e0f798c0914f525c27d0d9be70e472ebf2329a362b8a114.jpg", "table_caption": ["Table 14: Memory usage on each GPU. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "E Implementation guidelines ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Our algorithm SAMPa is deployed across two GPUs to facilitate parallel training. As shown in Algorithm 1, one GPU calculates $\\overline{{\\nabla f}}(\\widetilde{\\boldsymbol{x}}_{t},\\boldsymbol{B}_{t})$ and another one takes responsibility for $\\bar{\\nabla}f(y_{t+1},B_{t+1})$ . For ease of implementation, we provide a detailed version in Algorithm 2, with the following key points: ", "page_idx": 17}, {"type": "text", "text": "\u2022 Apart from the synchronization step (line 8), all other operations can be executed in parallel on both GPUs.   \n\u2022 The optimizer state, $m$ , used in ModelUpdate $:_{m}()$ includes necessary elements such as step size, momentum, and weight decay. Crucially, to ensure that $y_{t+1}$ (line 6) is close to $x_{t+1}$ , the update for $y_{t+1}$ uses $m_{t}$ , the state associated with $x_{t}$ . Note that the optimizer state is not updated in line 6. Algorithm 2 SAMPa on two GPUs Input: Initialization $x_{0}\\in\\mathbb{R}^{d}$ , initialization $y_{0}=x_{0}$ and $g_{0}=\\nabla f(y_{0},B_{0})$ , iterations $T$ , step sizes $\\{\\eta_{t}\\}_{t=0}^{T-1}$ , neighborhood size $\\rho>0$ , interpolation ratio $\\lambda$ , optimizer state $m_{0}$ .   \n1 for $t=0$ to $T-1$ do   \n2 GPU1: Load minibatch $B_{t}$ .   \n3 GPU1: Compute perturbed weightxt = xt + \u03c1 \u2225ggtt\u2225.   \n4 GPU1: Compute gradient $\\widetilde{g}_{t}=\\nabla f(\\widetilde{\\boldsymbol{x}}_{t},\\boldsymbol{B}_{t})$ .   \n5 GPU2: Load minibatch $B_{t+1}$ .   \n6 GPU2: Compute the auxiliary sequence $y_{t+1},\\_=\\mathrm{ModelUpdate}_{m_{t}}(x_{t},g_{t}).$ .   \n7 GPU2: Compute gradient $g_{t+1}=\\nabla f(y_{t+1},B_{t+1})$ .   \n8 Both: Communicate $\\widetilde{g}_{t}$ and $g_{t+1}$ between GPU1 and GPU2. $\\triangleright$ Synchronization barrier   \n9 Both: Compute the final gradient $G_{t}=(1-\\lambda)\\widetilde{g}_{t}+\\lambda g_{t+1}$ .   \n10 Both: Update weights $x_{t+1}$ , $m_{t+1}=\\mathrm{ModelUpdate}_{m_{t}}(x_{t},G_{t})$ . $\\triangleright$ Updates optimizer state ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Claims ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: Abstract and conclusion in Section 1 clearly demonstrate our algorithm named SAMPa, a parallel version of SAM. We also claim that the theoretical result shows a convergence guarantee. The empirical results present that SAMPa not only enhances efficiency but also improves generalization. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 18}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: We discuss the limitation of our method in Section 7 including communication overhead across GPUs and general theoretical analysis. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 18}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: We demonstrate main assumptions and theoretical results in Section 4, and the proofs are provided in Appendix A. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 19}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: All experimental setups with detailed configurations are provided in Section 5 and Appendix B. Additionally, we give practical guidance in Appendix E and public code in https://github.com/LIONS-EPFL/SAMPa. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in ", "page_idx": 19}, {"type": "text", "text": "some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 20}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We provide code in https://github.com/LIONS-EPFL/SAMPa, which realizes a parallelized version of SAMPa. We also give practical guidance in Appendix E. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 20}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The main experimental setting is shown in Section 5 including data splitting, data augmentation, type of optimizers, and the choice of hyperparameters by grid search or following existing works. Some additional details are provided in Appendix B. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 20}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We report the average accuracy of multiple runs along with the variance in Section 5, which provides appropriate information about the statistical significance and reliability of our experimental results. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 21}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We report we use NVIDIA A100 GPU for our experiments in Section 5.   \nMoreover, we show actual running time in Figure 2b for several relative works. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 21}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: All datasets we use are public datasets like CIFAR-10, CIFAR-100, and ImageNet-1K. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 21}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: Our paper is foundational research about an efficient optimizer and it is not tied to particular applications. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 22}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: Our paper has no such risks. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 22}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We use public datasets and some pre-trained models, all of which are claimed and cited with original papers. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: We do not release new assets. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 23}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 23}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}]