{"references": [{"fullname_first_author": "Pierre Foret", "paper_title": "Sharpness-aware minimization for efficiently improving generalization", "publication_date": "2021-00-00", "reason": "This paper introduces Sharpness-Aware Minimization (SAM), a foundational method that the current paper builds upon and improves."}, {"fullname_first_author": "Jungmin Kwon", "paper_title": "ASAM: Adaptive Sharpness-Aware Minimization for Scale-Invariant Learning of Deep Neural Networks", "publication_date": "2021-00-00", "reason": "This paper proposes ASAM, an efficient variant of SAM, which is directly compared to in the current paper's experiments to show performance improvements."}, {"fullname_first_author": "Jiawei Du", "paper_title": "Efficient Sharpness-Aware Minimization for Improved Training of Neural Networks", "publication_date": "2022-00-00", "reason": "This paper offers ESAM, another efficient SAM variant, providing a comparative point for evaluating the computational efficiency gains of the proposed method."}, {"fullname_first_author": "Yang Zhao", "paper_title": "Penalizing Gradient Norm for Efficiently Improving Generalization in Deep Learning", "publication_date": "2022-00-00", "reason": "This paper presents a gradient-penalization method related to the proposed method, allowing for a comparison of techniques to improve generalization."}, {"fullname_first_author": "Maksym Andriushchenko", "paper_title": "Towards Understanding Sharpness-Aware Minimization", "publication_date": "2022-00-00", "reason": "This paper provides theoretical analysis of SAM, which the current paper expands upon with novel theoretical contributions for its proposed method."}]}