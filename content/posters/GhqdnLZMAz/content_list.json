[{"type": "text", "text": "Improving Decision Sparsity ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yiyang Sun Tong Wang Cynthia Rudin Duke University Yale University Duke University ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Sparsity is a central aspect of interpretability in machine learning. Typically, sparsity is measured in terms of the size of a model globally, such as the number of variables it uses. However, this notion of sparsity is not particularly relevant for decision making; someone subjected to a decision does not care about variables that do not contribute to the decision. In this work, we dramatically expand a notion of decision sparsity called the Sparse Explanation Value (SEV) so that its explanations are more meaningful. SEV considers movement along a hypercube towards a reference point. By allowing flexibility in that reference and by considering how distances along the hypercube translate to distances in feature space, we can derive sparser and more meaningful explanations for various types of function classes. We present cluster-based SEV and its variant tree-based SEV, introduce a method that improves credibility of explanations, and propose algorithms that optimize decision sparsity in machine learning models. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The notion of sparsity is a major focus of interpretability in machine learning and statistical modeling [Tibshirani, 1996, Rudin et al., 2022]. Typically, sparsity is measured globally, such as the number of variables in a model, or as the number of leaves in a decision tree [Murdoch et al., 2019]. Global sparsity is relevant in many situations, but it is less relevant for individuals subject to the model\u2019s decisions. Individuals care less about, and often do not even have access to, the global model. For them, local sparsity, or decision sparsity, meaning the amount of information critical to their own decision, is more consequential. ", "page_idx": 0}, {"type": "text", "text": "An important notion of decision sparsity has been established in the work of Sun et al. [2024], which defined the Sparse Explanation Value (SEV), in the context of binary classification, as the number of factors that need to be changed to a reference feature value in order to change the decision. In contrast to SEV, counterfactual explanations tend not to be sparse since they require small changes to many variables in order to reach the decision boundary [Sun et al., 2024]. Instead, SEV provides sparse explanations: consider a loan application that is denied because the applicant has many delinquent payments. In that case, the decision sparsity (that is, the SEV) would be 1 because only a single factor was required to change the decision, overwhelming all possible mitigating factors. The framework of SEV thus allows us to see sparsity of models in a new light. ", "page_idx": 0}, {"type": "text", "text": "Prior to this work, SEV had one basic definition: it is the minimal number of features we need to set to their reference values to filp the sign of the prediction. The reference values are typically defined as the mean of the instances in the opposite class. This calculation is easy to understand, but somewhat limiting because the reference could be far in feature space from the point being explained and the explanation could land in a low density area where explanations are not credible. As an example, for the loan decision for a 21 year old applicant, SEV could create a counterfactual such as \u201cChanging the applicant\u2019s 3-year credit history to 15 years would change the decision.\u201d While this counterfactual is valid, faithful, and sparse, it is not close because the distance between the query point and the counterfactual is so large (3 years to 15 years). In addition, this explanation is not credible because the proposed changes to the features lead to an unrealistic circumstance \u2013 6-year-olds do not typically have credit. That is, the counterfactual does not represent a typical member of the opposite class. ", "page_idx": 0}, {"type": "text", "text": "Lack of credibility is a common problem for many counterfactual explanations [Mothilal et al., 2020, Wachter et al., 2017, Laugel et al., 2017, Joshi et al., 2019]. Therefore, in this work, we propose to augment the SEV framework by adding two practical considerations, closeness of the reference point to the query, and credibility of the explanation, while also optimizing decision sparsity. ", "page_idx": 1}, {"type": "text", "text": "We propose three ways to create close, sparse and credible explanations. The first way is to create multiple possibilities for the reference, one at the center of each cluster of points (Section 4.1). Having a finite set of references keeps the references auditable, meaning that a domain expert can manually check the references prior to generating any explanations. By creating references spread throughout the opposite class, queries can be assigned to closer references than before. Second, we allow the references to be flexible, where their position can be shifted slightly from a central location in order to reduce the SEV (Section 4.4). The third way pertains to decision tree classifiers, where a reference point is placed on each opposite-class leaf, and an efficient shortest-path algorithm is used to find the nearest reference (Section 4.2). Table 1 shows a query at the top, and some SEV calculations from our methods below, showing feature values that were changed within the explanation. ", "page_idx": 1}, {"type": "table", "img_path": "GhqdnLZMAz/tmp/ee17ae708f0d1928aff42192ed833c5a94c965cd455c30997d86eea087d34748.jpg", "table_caption": ["Table 1: An example for a query in the FICO Dataset with different kinds of explanations, $\\mathrm{SEV^{1}}$ represents the SEV calculation with one single reference using population mean, $\\operatorname{\\dot{SEV}}^{\\odot}$ represents the cluster-based SEV, $\\boldsymbol{\\mathrm{SEV}}^{F}$ represents the flexible-based SEV. $\\mathrm{\\Delta}\\mathrm{ygv}^{T}$ represents the tree-based SEV The columns are four features. "], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "In addition to developing methods for calculating SEV, we propose two algorithms to optimize a machine learning model to reduce the number of points that have high SEV without sacrificing predictive performance in Section 5, one based on gradient optimization, and the other based on search. The search algorithm is exact. It uses an exhaustive enumeration of the set of accurate models to find one with (provably) optimal SEV. ", "page_idx": 1}, {"type": "text", "text": "Our notions of decision sparsity are general and can be used for any model type, including neural networks and boosted decision trees. Decision sparsity can benefti any application where individuals are subject to decisions made from predictive models \u2013 these are cases where decision sparsity is more important than global sparsity from the individual perspectives. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "The concept of SEV revolves around finding models that are simple, in that the explanations for their predictions are sparse, while recognizing that different predictions can be simple in different ways (i.e., involving different features). In this way, it relates to (i) instance-wise explanations (iii) local sparsity optimization Models, which seek to explain and provide predictions of complex models. We further comment on these below. ", "page_idx": 1}, {"type": "text", "text": "Instance-wise Explanations. Prior work has developed methods to explain predictions of black boxes [e.g., Guidotti et al., 2018, Ribeiro et al., 2016, 2018, Lundberg and Lee, 2017, Baehrens et al., 2010] for individual instances. These explanations are designed to estimate importance of features, are not necessarily faithful to the model, and are not associated with sparsity in decisions, so they are fairly distant from the purpose of the present work. Our work is on tabular data; there is a multitude of unrelated work on explanations for images [e.g., Apicella et al., 2019, 2020] and text [e.g., Lei et al., 2016, Li et al., 2016, Treviso and Martins, 2020, Bastings et al., 2019, Yu et al., 2019, 2021]. More closely related are counterfactual explanations, also called inverse classification [e.g., Mothilal et al., 2020, Wachter et al., 2017, Lash et al., 2017, Sharma et al., 2024, Virgolin and Fracaros, 2023, Guidotti et al., 2019, Poyiadzi et al., 2020, Russell, 2019, Boreiko et al., 2022, Laugel et al., 2017, Pawelczyk et al., 2020]. Counterfactual explanations are typically designed to find the closest instance to a query point with the opposite prediction, without considering sparsity of the explanation. However, extensive experiments [Delaney et al., 2023] indicate that these \u201cclosest counterfactuals\u201d tend to be unnatural for humans because the decision boundary is typically in a region where humans have no intuition for why a point belongs to one class or the other. For SEV, on the other hand, reference values represent the population commons, so they are intuitive. Thus, SEV has two advantages over standard counterfactuals: its references are meaningful because they represent population commons, and its explanations are sparse. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Local Sparsity Optimization Models While there are numerous prior works on developing post-hoc explanations, limited attention has been paid to developing models that provide sparse explanations. We are aware of only one work on this, namely the Explanation-based Optimization (ExpO) algorithm of Plumb et al. [2020] that used a neighborhood-fidelity regularizer to optimize the model to provide sparser post-hoc LIME explanations. Experiment in Appendix K in our paper shows that ExpO is both slower and provides less sparse predictions than our algorithms. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries and Motivation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The Sparse Explanation Value (SEV) is defined to measure the sparsity of individual predictions of binary classifiers. The point we are creating an explanation for is called the query. The SEV is the smallest set of feature changes from the query to a reference that can filp the prediction of the model. When we make a change to the query\u2019s feature, we align it to be equal to that of the reference point. The reference point is a \u201ccommons,\u201d i.e., a prototypical point of the opposite class as the query. In this section, we will focus on the basic definition of SEV, the selection criteria for the references, as well as three reference selection methods. ", "page_idx": 2}, {"type": "text", "text": "3.1 Recap of Sparse Explanation Values ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We define SEV following Sun et al. [2024]. For a specific binary classification dataset $\\{{x}_{i},{y}_{i}\\}_{i=1}^{n}$ , with each $\\pmb{x}_{i}~\\in~\\mathbb{R}^{p}$ , and the outcome of interest is $y_{i}\\;\\;\\in\\;\\;\\{0,1\\}$ . (This can be extended to multi-class classification by providing counterfactuals for every other class than the query\u2019s class.) We predict the outcome using a classifier ${\\bar{f}}\\ :\\ \\chi\\ \\ \\rightarrow\\ \\ \\{0,1\\}$ . ", "page_idx": 2}, {"type": "text", "text": "Without loss of generality, in this paper, we are only interested in queries predicted as positive (class 1). We focus on providing a sparse explanation from the query to a reference that serves as a population commons, denoted $\\pmb{r}$ . Human studies [Delaney et al., 2023] have shown that contrasting an instance with prototypical instances from another class provides more intuitive explanations than comparing it with instances from the same class. Thus, we define our references in the opposite class (negative class in this paper). To calculate SEV, we will align (i.e., equate) features from query $\\pmb{x}_{i}$ and reference $\\tilde{{\\boldsymbol{x}}}$ one at a time, checking at each time whether the prediction flipped. Thinking of these alignment steps as binary moves, it is convenient to represent the $2^{p}$ possible different alignment combinations as vertices on the boolean hypercube. The hypercube is defined below: ", "page_idx": 2}, {"type": "image", "img_path": "GhqdnLZMAz/tmp/b7a33219462b3f0b88b8384f9997bc51f1506c17e68f2d31d3f64eb355fcdf87.jpg", "img_caption": ["Figure 1: SEV Hypercube "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Definition 3.1 (SEV hypercube). A SEV hypercube $\\mathcal{L}_{f,i,r}$ for a model $f$ , an instance $\\pmb{x}_{i}$ with label $f(\\pmb{x}_{i})=1$ , and a reference $\\pmb{r}$ , is a graph with $2^{p}$ vertices. Here $p$ is the number of features in $\\pmb{x}_{i}$ and $\\pmb{b}_{v}\\in\\{0,1\\}^{p}$ is a Boolean vector that represents each vertex. Vertices $u$ and $v$ are adjacent when their Boolean vectors differ in one bit, $\\|b_{u}-b_{v}\\|_{0}=1$ . 0\u2019s in $b_{v}$ indicate the corresponding features are aligned, i.e., set to the feature values of the reference $\\pmb{r}$ , while 1\u2019s indicate the true feature value of instance $i$ . Thus, the actual feature values represented by the vertex $v$ is $\\pmb{x}_{i}^{r,v},:=\\pmb{b}_{v}\\odot\\pmb{x}_{i}+(\\mathbf{1}-\\pmb{b}_{v})\\odot\\pmb{r}$ , where $\\odot$ is the Hadamard product. The score of vertex $v$ is $f(\\pmb{x}_{i}^{r,v})$ , also denoted as $\\mathcal{L}_{f,i,r}(\\pmb{b}_{v})$ . ", "page_idx": 2}, {"type": "text", "text": "The SEV hypercube definition can also be extended from a hypercube to a Boolean lattice as they have the same geometric structure. There are two variants of the Sparse Explanation Value: one gradually aligns the query to the reference $\\mathrm{(SEV^{-})}$ , and the other gradually aligns the reference to the query $(\\mathrm{SEV^{+}})$ . In this paper, we focus on $\\mathrm{SEV^{-}}$ : ", "page_idx": 2}, {"type": "table", "img_path": "GhqdnLZMAz/tmp/86baae9d9aada2f9ba1108148d614db127a6948f5635f1fad92af969fa92d261.jpg", "table_caption": ["Table 2: Calculation process for $\\mathrm{SEV^{-}=1}$ "], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "Definition 3.2 $\\mathrm{(SEV^{-})}$ ). For a positively-predicted query $\\pmb{x}_{i}$ (i.e., $f(\\pmb{x}_{i})=1)$ , the Sparse Explanation Value Minus $\\mathrm{(SEV^{-})}$ is the minimum number of features in the query that must be aligned to reference $\\pmb{r}$ to elicit a negative prediction from $f$ . It is the length of the shortest path along the hypercube to obtain a negative prediction, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{SEV}^{-}(f,x_{i},\\pmb{r}):=\\operatorname*{min}_{b\\in\\{0,1\\}^{p}}\\quad\\|\\mathbf{1}-b\\|_{0}\\quad\\mathrm{s.t.}\\quad\\mathcal{L}_{f,i,r}(b)=0.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Figure 1 and Table 2 shows an example of $\\mathrm{SEV^{-}{=}1}$ in a credit risk evaluation setting. Since $p=3$ , we construct a SEV hypercube with $2^{3}\\,=\\,8$ vertices. The red vertex $(1,1,1)$ corresponds to the query. The dark blue vertex at $(0,0,0)$ represents the negatively-predicted reference value. The orange vertices are predicted to be positive, and the light blue vertices are predicted to be negative. To compute $\\mathrm{SEV^{-}}$ , we start at $(1,1,1)$ and find the shortest path to a negatively-predicted vertex. On this hypercube, $(0,1,1)$ is closest. Translating this to feature space, this means that if the query\u2019s housing situation changes from renting to the reference value \u201cowning,\u201d it would be predicted as negative. This means that $\\mathbf{S}\\mathbf{E}\\mathbf{V}^{-}$ is equal to 1 in this case. The feature vector corresponding to this closest vertex $(0,1,1)$ , is called the $\\mathbf{S}\\mathbf{E}\\mathbf{V}^{-}$ explanation for the query, denoted by xr,explfor reference $\\pmb{r}$ . ", "page_idx": 3}, {"type": "text", "text": "3.2 Motivation of Our Work: Sensitivity to Reference Points ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Since $\\mathrm{SEV^{-}}$ is determined by the path on a SEV hypercube and each hypercube is determined by the reference point, the $\\mathrm{SEV^{-}}$ is therefore sensitive to the selection of reference points. Adjusting the reference point trades off between sparsity (according to $\\mathrm{SEV^{-}}$ ) and closeness (measured by $\\ell_{2}$ , $\\ell_{\\infty}$ (see Section 6.1) or $\\ell_{0}$ (see Section 6.4) distance between the query and its assigned reference point). Note that this trade-off exists because $\\mathrm{SEV^{-}}$ tends to be small when the reference is far from the query. More detailed explanations, visualizations, and experiments are shown in Appendix B. ", "page_idx": 3}, {"type": "text", "text": "Selecting References. The reference must represent the commons, meaning the negative population, and the generated explanations should represents the negative populations as well. Moreover, the negative population may have subpopulations; e.g., Diabetes patients may have higher blood glucose levels, while hypertension patients have higher blood pressure. To have meaningful coverage of the negative population, in this work, we consider multiple references, placed within the various subpopulations. This allows each point in the positive population to be closer to a reference. Let $\\mathcal{R}$ denote possible placements of references. For query $\\pmb{x}_{i}$ , an individual-specific reference $r_{i}\\in\\mathcal{R}$ for $\\pmb{x}_{i}$ is chosen based on three criteria: it should be nearby (i.e., close), and should provide a sparse and reasonable explanation. That is, we are looking to minimize the following three objectives over placement of the reference $\\boldsymbol{r}_{i}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\lVert\\pmb{x}_{i}-\\pmb{r}_{i}\\rVert,\\pmb{r}_{i}\\in\\mathcal{R}\\quad(\\mathrm{Closeness})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{SEV}^{-}(f,\\mathbf{x}_{i},\\mathbf{r}_{i}),\\mathbf{r}_{i}\\in\\mathcal{R}\\quad\\mathrm{(Sparsity)}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "equation", "text": "$$\n-P(\\pmb{x}_{i}^{\\mathrm{expl},\\pmb{r}_{i}}|X^{-})\\quad(\\mathrm{Negated\\,Credibility}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "with the constraint that the references obey auditability, meaning that domain experts are able to check the references manually, or construct them manually. The function $\\operatorname{SEV}^{-}(f,\\pmb{x}_{i},\\pmb{r}_{i})$ in (2) represents the $\\mathrm{SEV^{-}}$ computed with the given function $f$ , query $\\pmb{x}_{i}$ , and the individual-specific reference $\\boldsymbol{r}_{i}$ for generating the hypercube. $\\pmb{x}_{i}^{\\mathrm{expl},r_{i}}$ is the sparse explanation for the sample $\\pmb{x}_{i}$ , and $P(\\cdot|X^{-})$ in the definition of credibility represents the probability density distribution of the negative population and $P(x_{i}^{\\mathrm{expl},r_{i}}|X^{-})$ is the density of the negative distribution at $\\pmb{x}_{i}^{\\mathrm{expl},r_{i}}$ . If $P(\\pmb{x}_{i}^{\\mathrm{expl},r_{i}}|X^{-})$ is large, xiexpl,riis in a high-density region. ", "page_idx": 3}, {"type": "text", "text": "4 Meaningful and Credible SEV ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We now describe cluster-based SEV, which improves closeness at the expense of SEV, and its variant, tree-based SEV, which improves all three objectives and computational efficiency. We also present methods to improve the credibility and sparsity of the explanations. ", "page_idx": 3}, {"type": "text", "text": "4.1 Cluster-based SEV: Improving Closeness ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "This approach creates multiple references for the negative population. A clustering algorithm is used to group negative samples, and the resulting cluster centroids are assigned as references. A query is assigned to its closest cluster center: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\tilde{\\pmb{r}}_{i}\\in\\arg\\operatorname*{min}_{\\pmb{r}\\in\\mathcal{C}}\\|\\pmb{x}_{i}-\\pmb{r}\\|_{2}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathcal{C}$ is the collection of centroids obtained by clustering the negative samples. We refer to the $\\mathrm{SEV^{-}}$ produced by the grouped samples as cluster-based SEV, denoted $\\mathrm{SEV^{\\odot}}$ . Figure 2 illustrates the calculation of $\\mathrm{SEV^{\\odot}}$ for two examples located in two different centroids. A red dot represents a query, while a blue dot represents a reference. ", "page_idx": 4}, {"type": "image", "img_path": "GhqdnLZMAz/tmp/57db71a3d2b3ad835bd7ef67e0b27c9c50d8cb8887146544aa769bf91bcdde4d.jpg", "img_caption": ["Figure 2: Cluster-based SEV "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "For each instance, it selects the closest centroid and considers the SEV hypercube, where each cyan point represents a negatively predicted vertex and each pink point represents a positively predicted vertex. We deduce by following the red lines that the $\\boldsymbol{\\mathrm{SEV}}^{\\odot}$ for the two queries are 2 and 1, respectively. The cluster centroids should serve as a cover for the negative class. To ensure that the cluster centroids have negative predictions, we use the soft clustering method of Bezdek et al. [1984] to constrain the predictions of the cluster centers. Details are in Appendix C. ", "page_idx": 4}, {"type": "text", "text": "4.2 Tree-based SEV: $\\mathbf{S}\\mathbf{E}\\mathbf{V}^{\\odot}$ Variant with Useful Properties and Computational Benefits ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Tree-based SEV is a special case of cluster-based SEV, where we consider each negative leaf as a reference candidate, and find the sparsest explanation (path along the tree) to the nearest reference. Here, $\\mathrm{SEV^{-}}$ and $\\ell_{0}$ distance (i.e., edit distance) are equivalent. That is, we find the minimum number of features to change in order to achieve a negative prediction. ", "page_idx": 4}, {"type": "text", "text": "We denote $\\mathrm{SEV}^{T}$ as the $\\mathrm{SEV^{-}}$ calculated based on this process. Here, we assume that trees have no trivial splits where all child leaves make the same prediction. If so, we would collapse those leaves before calculating the $\\mathrm{SEV}^{T}$ . The first theorem below refers to decision paths that have negatively predicted child leaves. This is where taking one different choice at an internal split leads to a negative leaf. ", "page_idx": 4}, {"type": "image", "img_path": "GhqdnLZMAz/tmp/85053f1d02ac2363a8c564cec6e22421ec3af34c8e91e43ee6ff5e5fd1b04852.jpg", "img_caption": ["Figure 3: SEVT Preprocessing "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Theorem 4.1. With a single decision classifier DT and a positively-predicted query $\\pmb{x}_{i}$ , define $N_{i}$ as the leaf that captures it. If $N_{i}$ has a sibling leaf, or any internal node in its decision path has a negatively-predicted child leaf, then $S E V^{T}$ is equal to 1. ", "page_idx": 4}, {"type": "text", "text": "The second theorem states that $\\mathrm{SEV^{-}}$ and minimum edit distance from the query to negative leaves are equivalent. ", "page_idx": 4}, {"type": "text", "text": "Theorem 4.2. With a single decision tree classifier DT and a positively-predicted query $\\pmb{x}_{i}$ , with the set of all negatively predicted leaves as reference points, both $S E V^{-}$ and the $\\ell_{0}$ distance (edit distance) between the query and the $S E V^{-}$ explanation are minimized. ", "page_idx": 4}, {"type": "text", "text": "The proofs of those two theorems are shown in Appendix $\\mathrm{L}$ and M. The structure of tree models yields an extremely efficient way to calculate $\\mathrm{SEV^{-}}$ . We perform an important preprocessing step before any $\\mathrm{SEV^{-}}$ ", "page_idx": 4}, {"type": "image", "img_path": "GhqdnLZMAz/tmp/83c9c74726d27e193305aa1e79f5efc49a0d4629f4d78b5e7e321d90d2685d4b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 4: Efficient $\\mathbf{S}\\mathbf{E}\\mathbf{V}^{T}$ calculation: Query (node $\\textcircled{7})$ has $\\mathrm{SEV}^{T}{=}1$ , which goes to node $\\textcircled{10}$ . The path to this node is recorded as LL at node $\\textcircled{3},$ , which is along the decision path to node $\\textcircled{7}$ ", "page_idx": 4}, {"type": "text", "text": "calculations are done, which will make $\\mathrm{SEV^{-}}$ easier to calculate for all queries at runtime. At each internal node, we record all paths to negative leaves anywhere below it in the tree. This is described in Algorithm 2 in Appendix E. E.g., if the tree has binary splits, a path from an internal node to a leaf node might require us to go left, then right, then left. In that case, we store LRL on this internal node to record this path. Then, when a query arrives at runtime (in a positive leaf, since it has a positive prediction), we traverse directly up its decision path all the way to the root node. For all internal nodes in the decision path, we observe distances to each negative leaf, which were stored during preprocessing. We traverse each of these, and the minimum distance among these is the $\\mathrm{SEV^{-}}$ . This is described in Algorithm 3 in Appendix E and illustrated in Figure 4. Note that we actually would traverse to each negative node because some internal decisions might not need to be changed along the path. In the example in Figure 4, we change the split at node $\\textcircled{3}.$ and use the value that the query already has for the split at node $\\textcircled{6}.$ , landing in node $\\overbrace{\\left(10\\right)}$ , so $\\mathrm{SEV^{-}}$ is 1 not 2. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "table", "img_path": "GhqdnLZMAz/tmp/3b5552006e4e24c8b23667203284e1f91ecbea7f3fd072ddd40ff0b23043d14b.jpg", "table_caption": ["Table 3: Illustration of $\\mathrm{SEV}^{T}$ calculation. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "Table 3 walks through the calculation again, using the names of the features (hypertension, diabetes, etc.). On the first action line, the decision path to the query is $\\textcircled{3}\\textcircled{6}\\textcircled{10}$ . That means we check $\\textcircled{1}$ and $\\circled{3}$ for negative paths, yielding path LL. We flip node $\\circled{3}$ (change Hyperlipidemia to \u2018yes\u2019) and follow the LL path. We do not change Obesity to get to the negative node, so we record the $\\mathrm{SEV}^{T}$ as 1 in that row. In our implementation, we simply stop when we reach an $\\mathrm{SEV}^{T}{=}1$ solution, but we will continue in order to illustrate how the calculation works. We go up to node $\\textcircled{1}$ and repeat the process for the LR and LLR paths. Those both have $\\mathrm{SEV}^{T}{=}2$ . ", "page_idx": 5}, {"type": "text", "text": "Note that the reference can be any point $x$ within the leaf; if the leaf is defined by thresholds such as $3<x_{1}<5$ and $x_{2}>7$ , then any point satisfying those conditions is a viable reference. Given a query, the algorithm flips some of its feature values to satisfy conditions of a leaf with the opposite prediction. Since any point in the leaf is a viable reference, we could choose the median/mean values of points in the leaf as the references, or a more meaningful value. That choice will not influence the fast calculation of SEV-T. ", "page_idx": 5}, {"type": "text", "text": "4.3 Improving Credibility for All SEV Calculations ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "As we mentioned in Section 3.2, the credibility objective encourages explanations to be located in high-density region of the negative population. Previous $\\mathrm{SEV^{-}}$ definitions focus on sparsity and closeness objectives, but did not consider credibility. It is possible to increase credibility easily while constructing an explanation: if the explanation veers out of the high-density region, we continue walking along the SEV hypercube during SEV calculations. Specifically, we continue moving towards the reference until the vertex is in a high-density region. Since the reference is in a highdensity region, walking towards it will eventually lead to a high-density point. The tree-based SEV explanations automatically satisfy high credibility: ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.3. With a single sparse decision tree classifier DT with support at least $S$ in each negative leaf, the $S E V^{T}$ explanation for query $\\pmb{x}_{i}$ always satisfies credibility at least NS \u2212, where N \u2212 is the total number of negative samples. ", "page_idx": 5}, {"type": "text", "text": "This theorem can be easily proved because $\\mathrm{SEV^{-}}$ explanations generated by $\\boldsymbol{\\mathrm{SEV}}^{T}$ are always the negative leaf nodes (which are the references), and the references are located in regions with support at least $S$ by assumption. ", "page_idx": 5}, {"type": "text", "text": "4.4 Flexible Reference SEV: Improving Sparsity ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "From Section 3.2, we know that queries further from the decision boundary tend to have lower $\\mathrm{SEV^{-}}$ . Based on this, we introduce Flexible Reference SEV (denoted $\\operatorname{SEV}^{F}$ ), which moves the reference value slightly in order to achieve a lower value of the model output $f(\\tilde{\\boldsymbol{r}})$ given a reference $\\tilde{\\pmb{r}}$ , and the decision function for classification $f(\\cdot)$ , which, in turn, is likely to lead to lower $\\mathrm{SEV^{-}}$ . The optimization for finding the optimal reference is: $\\begin{array}{r}{\\pmb{r}^{*}\\,\\in\\,\\arg\\operatorname*{min}_{\\pmb{r}}\\,f(\\pmb{r})\\quad\\mathrm{s.t}\\|\\pmb{r}-\\pmb{\\tilde{r}}\\|_{\\infty}\\,\\leq\\,\\epsilon_{F}}\\end{array}$ where the arg min is over reference candidates that are near the original reference value $\\tilde{\\pmb{r}}$ . The flexibility threshold $\\epsilon_{F}$ represents the flexibility allowed for moving the reference within a ball. We limit flexibility so the explanation stays meaningful. Since it is impractical to explore all potential combinations of feature-value candidates, we address this problem by marginalizing. Specifically, we optimize the reference over each feature independently. The detailed algorithm for calculating Flexible Reference SEV, denoted $\\mathrm{SEV}^{F}$ , is shown in Algorithm 1 in Appendix D. In Section 6.2, we show that moving the reference slightly can sometimes reduce the SEV, improving sparsity. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "5 Optimizing Models for SEV\u2212 ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Above, we showed how to calculate $\\mathrm{SEV^{-}}$ for a fixed model. In this section, we describe how to train classifiers that optimize the average $\\mathrm{SEV^{-}}$ without loss in predictive performance. We propose two methods: minimizing an easy-to-optimize surrogate objective (Section 5.1) and searching for models with the smallest SEV from a \u201cRashomon set\u201d of equally-good models (Section 5.2). In what follows, we assume that $\\mathrm{SEV^{-}}$ was calculated prior to optimization, that reference points were assigned to each query, and that these assignments do not change throughout the calculation. ", "page_idx": 6}, {"type": "text", "text": "5.1 Gradient-based SEV Optimization ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Since we want to minimize expected test $\\mathrm{SEV^{-}}$ , the most obvious approach would be to choose our model $f$ to minimize average training $\\mathrm{SEV^{-}}$ . However, since SEV calculations are not differentiable and they are combinatorial in the number of features and data points, this would be intractable. Following Sun et al. [2024], we instead design the optimization objective to penalize each sample where $\\mathrm{SEV^{-}}$ is more than 1. Thus, we propose the loss term: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\ell_{\\mathrm{SEV}_{-}\\mathrm{All}_{-}\\mathrm{Opt}-}(f):=\\frac{1}{n^{+}}\\sum_{i=1}^{n^{+}}\\operatorname*{max}\\left(\\operatorname*{min}_{j=1,\\dots,p}f((1-e_{j})\\odot x_{i}+e_{j}\\odot\\tilde{r}_{i}),\\,0.5\\right),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $e_{j}$ is the vector with a 1 in the $j^{t h}$ coordinate and 0\u2019s elsewhere, $n^{+}$ is the number of queries, and the reference point $\\tilde{\\pmb{r}}_{i}$ is specific to query $\\pmb{x}_{i}$ and chosen beforehand. Intuitively, $f\\big((\\mathbf{1}-e_{j})\\odot x_{i}+e_{j}\\odot\\tilde{\\mathbf{r}}_{i}\\big)$ is the function value of query $\\pmb{x}_{i}$ where its feature $j$ has been replaced with the reference\u2019s feature $\\begin{array}{r}{j.\\ \\operatorname*{min}_{j=1,...,p}f((1-e_{j})\\,{\\bar{\\odot}}\\,{\\pmb{x}}_{i}+{\\pmb{e}}_{j}\\,\\odot{\\pmb{\\tilde{r}}}_{i})}\\end{array}$ chooses the variable to replace that most reduces the function value. If the $\\mathrm{SEV^{-}}$ is 1, then when this replacement is made, the point now is on the negative side of the decision boundary and $f$ is less than 0.5, in which case the max chooses 0.5. If $\\mathrm{SEV^{-}}$ is more than 1, then after replacement, $f$ will still predict positive and be more than 0.5, in which case, its value will contribute to the loss. This loss is differentiable with respect to model parameters except at the \u201ccorners\u201d and not difficult to optimize. ", "page_idx": 6}, {"type": "text", "text": "To put these into an algorithm, we optimize a linear combination of different loss terms, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{f\\in\\mathcal{F}}\\ell_{\\mathrm{BCE}}(f)+C_{1}\\ell_{\\mathrm{SEV\\_All\\_Opt-}}(f)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\ell_{\\mathrm{BCE}}$ is the Binary Cross Entropy Loss to control the accuracy of the training model and $\\mathcal{F}$ is a class of classification models that estimate the probability of belonging to the positive class. $\\ell_{\\mathrm{SEV\\_All\\_Opt-}}$ is the loss term that we have just introduced above. $C_{1}$ can be chosen using crossvalidation. We define All- $\\mathbf{\\nabla}\\mathbf{Opt^{-}}$ as the method that optimizes (4). Our experiments show that this method is not only effective in shrinking the average $\\mathrm{SEV^{-}}$ but often attains the minimum possible $\\mathrm{SEV^{-}}$ value of 1 for most or all queries. ", "page_idx": 6}, {"type": "text", "text": "5.2 Search-based SEV Optimization ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "As defined in Section 4.2, our goal is to find a model with the lowest average $\\mathrm{SEV^{-}}$ among classification models with the best performance. ", "page_idx": 6}, {"type": "text", "text": "The Rashomon set [Semenova et al., 2022, Fisher et al., 2019] is defined as the set of all models from a given class with performance approximately that of the best-performing model. The first method that stores the entire Rashomon set of any nontrivial function class is called TreeFARMS [Xin et al., 2022], which stores all good sparse decision trees in a data structure. TreeFARMS allows us to optimize multiple objectives over the space of sparse trees easily by enumeration of the Rashomon set to find all accurate models, and a loop through the Rashomon set to optimize secondary objectives. We use TreeFARMS and search through the Rashomon set for a model with the lowest average $\\mathrm{SEV^{-}}$ : ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{f\\in\\mathscr{R}_{\\mathrm{set}}}\\frac{1}{n^{+}}\\sum_{i=1}^{n^{+}}\\mathrm{SEV}^{T}(f,\\pmb{x}_{i}),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where the Rashomon set is $\\mathcal{R}_{\\mathrm{set}}$ , and where we use $\\mathrm{SEV}^{T}$ as the $\\mathrm{SEV^{-}}$ for each sparse tree in the Rashomon set. Recall that Algorithms 2 and 3 show how to calculate $\\mathrm{SEV}^{T}$ . We call this search-based optimization as TOpt. ", "page_idx": 7}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Training Datasets To evaluate whether our proposed methods would achieve sparser, more credible and closer explanations, we present experiments on seven datasets: (i) UCI Adult Income dataset for predicting income levels [Dua and Graff, 2017], (ii) FICO Home Equity Line of Credit Dataset for assessing credit risk, used for the Explainable Machine Learning Challenge [FICO, 2018], (iii) UCI German Credit dataset for determining creditworthiness [Dua and Graff, 2017], (iv) MIMIC-III dataset for predicting patient outcomes in intensive care units [Johnson et al., 2016a,b], (v) COMPAS dataset [Jeff Larson and Angwin, 2016, Wang et al., 2022a] for predicting recidivism, (vi) Diabetes dataset [Strack et al., 2014] for predicting whether patients will be re-admitted within two years, and (vii) Headline dataset for predicting whether the headline is likely to be shared by readers [Chen et al., 2023]. Additional details on data and preprocessing are in Appendix A. ", "page_idx": 7}, {"type": "text", "text": "Training Models For $\\mathrm{SEV^{\\odot}}$ , we trained four baseline binary classifiers: (i, ii) logistic regression classifiers with $\\ell_{1}$ (L1LR) and $\\ell_{2}$ (L2LR) penalties, (iii) a gradient boosting decision tree classifier (GBDT), and (iv) a 2-layer multi-layer perceptron (MLP), and tested its performance with $\\mathrm{SEV}^{F}$ added, and the credibility rules added. In addition, we trained All-Opt\u2212variants of these models in which the SEV penalties described in the previous sections are implemented. For $\\boldsymbol{\\mathrm{SEV}}^{T}$ methods, we compared tree-based models from CART, C4.5, and GOSDT [Lin et al., 2020, McTavish et al., 2022] with the TOpt method proposed in Section 5.2. Details on training the methods is in Appendix F. ", "page_idx": 7}, {"type": "text", "text": "Evaluation Metrics To evaluate whether good references are selected for the queries, we evaluate sparsity and closeness (i.e., similarity of query to reference). For sparsity, we use the average number of feature changes (which is the same as $\\ell_{0}$ norm) between the query and the explanation; for closeness, we use the median $\\ell_{\\infty}$ norm between the generated explanation and the original query as the metric for $\\mathbf{S}\\mathbf{E}\\mathbf{V}^{\\textregistered}$ . For tree-based models, we use only $\\mathrm{SEV}^{\\dagger T}$ as the metric since $\\operatorname{\\bar{SEV}}^{\\bar{T}}$ and $\\ell_{0}$ norm are equivalent; for credibility, we need some way of estimating $P(\\cdot|X)$ since we cannot observe it directly, so we trained a Gaussian mixture model on the negative samples of each dataset , and used the mean log-likelihood of the generated explanations as the metric for $\\dot{\\mathbf{S}}\\mathbf{E}\\mathbf{V}^{\\odot}$ and $\\mathrm{SEV}^{F}$ , for TOpt, since it has already been a sparse decision tree, then we don\u2019t need to calculate the credibility. ", "page_idx": 7}, {"type": "text", "text": "6.1 Cluster-based SEV shows improvement in credibility and closeness ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Let us show that $\\mathbf{S}\\mathbf{E}\\mathbf{V}^{\\textregistered}$ provides improved explanations. Here, we calculated the metric for different $\\mathrm{SEV^{\\odot}}$ variants, $\\mathbf{S}\\mathbf{E}\\mathbf{V}^{\\textregistered}$ and $\\mathrm{SEV}^{\\odot+\\dot{F}}(\\mathrm{SEV}^{\\odot}$ with flexible reference), and compared to the original $\\mathrm{SEV^{1}}$ , where $\\mathrm{SEV^{1}}$ is defined as the $\\mathrm{SEV^{-}}$ calculation with single reference generated by the mean value of each numerical feature and mode value of each categorical feature of the negative population, as done in the original SEV paper [Sun et al., 2024] under various datasets and models. ", "page_idx": 7}, {"type": "text", "text": "Figure 5a shows the relationship between spasity and variants, the scatter plot between mean $\\mathrm{SEV^{-}}$ and mean $\\ell_{\\infty}$ for each explanation generated by different variants. We find that $\\mathbf{S}\\mathbf{E}\\mathbf{V}^{\\odot}$ improves closeness, which was expected since the references were designed to be closer to the queries. Interestingly, $\\operatorname{SEV}^{\\odot}$ sometimes has lower decision sparsity than $\\mathrm{S\\bar{E}V^{1}}$ . $\\operatorname{SEV}^{\\odot}$ was designed to trade off $\\mathrm{SEV^{-}}$ for closeness, so it is surprising that it sometimes performs strictly better on both metrics, particularly for the COMPAS, Diabetes, and German Credit datasets. ", "page_idx": 7}, {"type": "image", "img_path": "GhqdnLZMAz/tmp/a8f1e14dd6a987f4d12500de27abced58b5a62b6408767e1b4d849e4971dd12e.jpg", "img_caption": ["Figure 5: Explanation performance under different models and metrics. We desire lower $\\mathrm{SEV^{-}}$ for sparsity, lower $\\ell_{\\infty}$ for closeness and higher log likelihood for credibility (shaded regions) "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Interestingly, we also find that even though we do not optimize credibility for our model, Figure 5b shows that $\\dot{\\mathbf{S}}\\mathbf{E}\\mathbf{V}^{\\odot}$ improves credibility, particularly for the Adult, German, and Diabetes datasets by plotting the relationship between mean $\\mathrm{SEV^{-}}$ and mean log-likelihood of the generated explanations. It is reasonable since the references are the cluster centroids for the negative samples, so the explanations are more likely to be located in the same high-density area. More detailed values for those methods and metrics are shown in Appendix H. ", "page_idx": 8}, {"type": "text", "text": "6.2 Flexible Reference SEV can improve sparsity without losing credibility ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In Section 4.4, we proposed the flexible reference method for sparsifying $\\mathrm{SEV^{-}}$ explanations, which moves the reference slightly away from the decision boundary. The blue points in Figure 5a and 5b have already shown that with small modification of the reference, the credibility of the explanations is not affected. Figure 6a shows how $\\mathrm{SEV^{-}}$ and credibility change as we increase flexibility; $\\mathrm{SEV^{-}}$ sometimes substantially decreases while credibility is maintained. ", "page_idx": 8}, {"type": "image", "img_path": "GhqdnLZMAz/tmp/6268f251653d1b480e1b7c67264106589f83fb308006a93618c5b61dfc06485f.jpg", "img_caption": ["(a) SEV\u2212/Credibility change rate for varying flexibility (b) Median Log likelihood and $\\#$ of features changed "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 6: (a) Sparsity and Credibility as a function of the change of flexibility level (0 to $5\\%/10\\%/20\\%$ ) under different models and datasets (b) The median log-likelihood and number of features within different counterfactual explanations. Points at the upper left corner are desired. ", "page_idx": 8}, {"type": "text", "text": "6.3 $\\mathbf{S}\\mathbf{E}\\mathbf{V}^{-}$ provides the sparsest explanation compared to other counterfactual explanations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Recall that $\\mathrm{SEV^{-}}$ filps features of the query to values of the population commons. This can be viewed as a type of counterfactual explanation, though typically, counterfactual explanations aim to find the minimal distance from one class to another. In this experiment, we compare the sparsity of $\\mathrm{SEV^{-}}$ calculations to that of baseline methods from the literature on counterfactual explanations, namely Watcher [Wachter et al., 2017], REVISE [Joshi et al., 2019], Growing Sphere [Laugel et al., 2017], and DiCE [Mothilal et al., 2020]. ", "page_idx": 8}, {"type": "text", "text": "Figure 6b shows sparsity and credibility performance of all counterfactual explanation methods on different datasets under $\\ell_{2}$ logistic regression (other information, including $\\ell_{\\infty}$ norms for counterfactual explanation methods, is in Appendix G). All SEV variants are in warm colors, while competitors are in cool colors. $\\mathrm{SEV^{-}}$ methods have the sparest explanations, followed by DiCE. (A comparison of $\\mathrm{SEV^{-}}$ to DiCE is provided by Sun et al. [2024].) We point out that this comparison was made on methods that were not designed to optimize explanation sparsity. Importantly, sparsity is essential for human understanding [Rudin et al., 2022]. Moreover, it has been shown that SEV (especially $\\mathrm{SEV^{\\odot}}$ ) would have more credible explanations than competitors, while explanations remain sparse. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "6.4 All-Opt\u2212and TOpt optimize $\\mathbf{S}\\mathbf{E}\\mathbf{V}^{-}$ , preserving model performance, explanation closeness and credibility ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Even without optimization, our $\\mathrm{SEV^{-}}$ variants improve decision sparsity and/or closeness. If we are willing to retrain the prediction model as discussed in Section 5, we can improve these metrics further, creating accurate models with higher decision sparsity. Figure 7a shows that gradient-based SEV optimization can reduce the SEV without harming the closeness metric $(\\ell_{\\infty})$ and the credibility metrics (log-likelihood). The slashed bars represents the $\\mathrm{SEV^{-}}$ , $\\ell_{\\infty}$ and log likelihood metrics before optimization using different models, while the colored bars are the results after optimizing with All-Opt\u2212. We have also compared our results with ExpO [Plumb et al., 2020], which is a optimization method that maximizes the mean neighborhood fidelity of the queries, but we have found that explanations are not sparse, and it requires long training times; the detailed results are shown in Appendix K. ", "page_idx": 9}, {"type": "image", "img_path": "GhqdnLZMAz/tmp/86f856f0350ac1db2355fb135e0471ef4bac9c1d7e7936f018154fa7514fc5bd.jpg", "img_caption": ["Figure 7: (a) $\\mathrm{SEV^{-}}$ and $\\ell_{\\infty}$ before and after All- $\\cdot\\mathrm{Opt^{-}}$ on the FICO Dataset. Slashed bars are before, solid color is after. (b) All tree-based models with similar accuracy have low $\\mathrm{SEV}^{T}$ . "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "For the Tree-based SEV, we have applied the efficient computation procedure to different kinds of tree-based models, and compared them with the search-based optimization method (TOpt) for trees in Section 5. The search-based algorithm works perfectly in finding a good model without performance loss. It achieves a perfect average SEV score of 1.00. ", "page_idx": 9}, {"type": "text", "text": "Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Decision sparsity can be more useful than global model sparsity for individuals, as individuals care less about, and often do not even have access to, the global model. We presented approaches to achieving high decision sparsity, closeness and credibility, while being faithful to the model. One limitation of our method is that causal relationships may exist among features, invalidating certain transitions across the SEV hypercube. This can be addressed by searching across vertices that do not satisfy the causal relationship, though it requires knowledge of the causal graph. Another limitation is that to make the explanation more credible, the threshold to stop searching the SEV hypercube is not easy to determine. Future studies could focus on on these topics. Overall, our work has the potential to enhance a wide range of applications, including but not limited to loan approvals and employment hiring processes. Improved SEV translates directly into explanations that simply make more sense to those subjected to the decisions of models. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We acknowledge funding from the National Science Foundation under grants DGE-2022040 and CMMI-2323978. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Robert Tibshirani. Regression shrinkage and selection via the Lasso. Journal of the Royal Statistical Society (Series B), 1996.   \nCynthia Rudin, Chaofan Chen, Zhi Chen, Haiyang Huang, Lesia Semenova, and Chudi Zhong. Interpretable machine learning: Fundamental principles and 10 grand challenges. Statistics Surveys, 16(none):1 \u2013 85, 2022.   \nW James Murdoch, Chandan Singh, Karl Kumbier, Reza Abbasi-Asl, and Bin Yu. Definitions, methods, and applications in interpretable machine learning. Proceedings of the National Academy of Sciences, 116(44):22071\u201322080, 2019.   \nYiyang Sun, Zhi Chen, Vittorio Orlandi, Tong Wang, and Cynthia Rudin. Sparse and faithful explanations without sparse models. In Proceedings of Artificial Intelligence and Statistics (AISTATS), 2024.   \nRamaravind K Mothilal, Amit Sharma, and Chenhao Tan. Explaining machine learning classifiers through diverse counterfactual explanations. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, pages 607\u2013617, 2020.   \nSandra Wachter, Brent Mittelstadt, and Chris Russell. Counterfactual explanations without opening the black box: Automated decisions and the GDPR. Harvard Journal of Law & Technology, 31: 841, 2017.   \nThibault Laugel, Marie-Jeanne Lesot, Christophe Marsala, Xavier Renard, and Marcin Detyniecki. Inverse classification for comparison-based interpretability in machine learning. arXiv preprint arXiv:1712.08443, 2017.   \nShalmali Joshi, Oluwasanmi Koyejo, Warut Vijitbenjaronk, Been Kim, and Joydeep Ghosh. Towards realistic individual recourse and actionable explanations in black-box decision making systems. arXiv preprint arXiv:1907.09615, 2019.   \nRiccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Franco Turini, Fosca Giannotti, and Pedreschi Dino. A survey of methods for explaining black box models. ACM Computing Surveys, 51(5), 2018.   \nMarco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. \u201cWhy should I trust you?\u201d explaining the predictions of any classifier. In Proceedings of the 22nd International Conference on Knowledge Discovery and Data Mining, 2016.   \nMarco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Anchors: High-precision model-agnostic explanations. In Proceedings of the 32nd AAAI Conference on Artificial Intelligence, 2018.   \nScott M Lundberg and Su-In Lee. A Unified Approach to Interpreting Model Predictions. In Advances in Neural Information Processing Systems, pages 4765\u20134774, 2017.   \nDavid Baehrens, Timon Schroeter, Stefan Harmeling, Motoaki Kawanabe, Katja Hansen, and KlausRobert M\u00fcller. How to explain individual classification decisions. Journal of Machine Learning Research, 11:1803\u20131831, aug 2010.   \nAndrea Apicella, Francesco Isgr\u00f2, Roberto Prevete, and Guglielmo Tamburrini. Contrastive explanations to classification systems using sparse dictionaries. In Image Analysis and Processing\u2013ICIAP 2019: 20th International Conference, Trento, Italy, September 9\u201313, 2019, Proceedings, Part I 20, pages 207\u2013218. Springer, 2019.   \nA. Apicella, F. Isgr\u00f2, R. Prevete, and G. Tamburrini. Middle-level features for the explanation of classification systems by sparse dictionary methods. International Journal of Neural Systems, 30 (08):2050040, July 2020.   \nTao Lei, Regina Barzilay, and Tommi Jaakkola. Rationalizing neural predictions. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 107\u2013117, 2016.   \nJiwei Li, Will Monroe, and Dan Jurafsky. Understanding neural networks through representation erasure. arXiv preprint arXiv:1612.08220, 2016.   \nMarcos Treviso and Andr\u00e9 FT Martins. The explanation game: Towards prediction explainability through sparse communication. In Proceedings of the Third Blackbox NLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 107\u2013118, 2020.   \nJasmijn Bastings, Wilker Aziz, and Ivan Titov. Interpretable neural predictions with differentiable binary variables. arXiv preprint arXiv:1905.08160, 2019.   \nMo Yu, Shiyu Chang, Yang Zhang, and Tommi S Jaakkola. Rethinking cooperative rationalization: Introspective extraction and complement control. arXiv preprint arXiv:1910.13294, 2019.   \nMo Yu, Yang Zhang, Shiyu Chang, and Tommi Jaakkola. Understanding interlocking dynamics of cooperative rationalization. Advances in Neural Information Processing Systems, 34:12822\u201312835, 2021.   \nMichael T Lash, Qihang Lin, Nick Street, Jennifer G Robinson, and Jeffrey Ohlmann. Generalized inverse classification. In Proceedings of the 2017 SIAM International Conference on Data Mining, pages 162\u2013170. SIAM, 2017.   \nShubham Sharma, Alan Gee, Jette Henderson, and Joydeep Ghosh. Faster-ce: fast, sparse, transparent, and robust counterfactual explanations. In IFIP International Conference on Artificial Intelligence Applications and Innovations, pages 183\u2013196. Springer, 2024.   \nMarco Virgolin and Saverio Fracaros. On the robustness of sparse counterfactual explanations to adverse perturbations. Artificial Intelligence, 316:103840, 2023.   \nRiccardo Guidotti, Anna Monreale, Fosca Giannotti, Dino Pedreschi, Salvatore Ruggieri, and Franco Turini. Factual and counterfactual explanations for black box decision making. IEEE Intelligent Systems, 34(6):14\u201323, 2019.   \nRafael Poyiadzi, Kacper Sokol, Raul Santos-Rodriguez, Tijl De Bie, and Peter Flach. FACE: Feasible and actionable counterfactual explanations. In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society, page 344\u2013350. Association for Computing Machinery, 2020.   \nChris Russell. Efficient search for diverse coherent explanations. In Proceedings of the Conference on Fairness, Accountability, and Transparency, pages 20\u201328, 2019.   \nValentyn Boreiko, Maximilian Augustin, Francesco Croce, Philipp Berens, and Matthias Hein. Sparse visual counterfactual explanations in image space. In Pattern Recognition: 44th DAGM German Conference, DAGM GCPR 2022, pages 133\u2013148, 2022.   \nMartin Pawelczyk, Klaus Broelemann, and Gjergji Kasneci. Learning model-agnostic counterfactual explanations for tabular data. In Proceedings of the Web Conference 2020, pages 3126\u20133132, 2020.   \nEoin Delaney, Arjun Pakrashi, Derek Greene, and Mark T Keane. Counterfactual explanations for misclassified images: How human and machine explanations differ. Artificial Intelligence, 324: 103995, 2023.   \nGregory Plumb, Maruan Al-Shedivat, \u00c1ngel Alexander Cabrera, Adam Perer, Eric Xing, and Ameet Talwalkar. Regularizing black-box models for improved interpretability. Advances in Neural Information Processing Systems, 33:10526\u201310536, 2020.   \nJames C Bezdek, Robert Ehrlich, and William Full. Fcm: The fuzzy c-means clustering algorithm. Computers & Geosciences, 10(2-3):191\u2013203, 1984.   \nLesia Semenova, Cynthia Rudin, and Ronald Parr. On the existence of simpler machine learning models. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency, pages 1827\u20131858, 2022.   \nAaron Fisher, Cynthia Rudin, and Francesca Dominici. All models are wrong, but many are useful: Learning a variable\u2019s importance by studying an entire class of prediction models simultaneously. Journal of Machine Learning Research, 20(177):1\u201381, 2019.   \nRui Xin, Chudi Zhong, Zhi Chen, Takuya Takagi, Margo Seltzer, and Cynthia Rudin. Exploring the whole rashomon set of sparse decision trees. Advances in Neural Information Processing Systems, 35:14071\u201314084, 2022.   \nDheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.ics. uci.edu/ml.   \nFICO. Explainable machine learning challenge, 2018. URL https://community.fico.com/s/ explainable-machine-learning-challenge.   \nA. Johnson, T. Pollard, and R. Mark. MIMIC-III clinical database. https://physionet.org/ content/mimiciii/, 2016a.   \nAlistair E.W. Johnson, Tom J. Pollard, Lu Shen, Li wei H. Lehman, Mengling Feng, Mohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G. Mark. MIMIC-III, a freely accessible critical care database. Scientific Data, 3(1), May 2016b.   \nLauren Kirchner Jeff Larson, Surya Mattu and Julia Angwin. How we analyzed the COMPAS recidivism algorithm. ProPublica, 2016.   \nCaroline Wang, Bin Han, Bhrij Patel, and Cynthia Rudin. In Pursuit of Interpretable, Fair and Accurate Machine Learning for Criminal Recidivism Prediction. Journal of Quantitative Criminology, pages 1\u201363, 2022a.   \nBeata Strack, Jonathan P. DeShazo, Chris Gennings, Juan L. Olmo, Sebastian Ventura, Krzysztof J. Cios, and John N. Clore. Impact of HbA1c measurement on hospital readmission rates: Analysis of 70, 000 clinical database patient records. BioMed Research International, 2014:1\u201311, 2014.   \nXi Chen, Gordon Pennycook, and David Rand. What makes news sharable on social media? Journal of Quantitative Description: Digital Media, 3, 2023.   \nJimmy Lin, Chudi Zhong, Diane Hu, Cynthia Rudin, and Margo Seltzer. Generalized and scalable optimal sparse decision trees. In International Conference on Machine Learning, pages 6150\u20136160. PMLR, 2020.   \nHayden McTavish, Chudi Zhong, Reto Achermann, Ilias Karimalis, Jacques Chen, Cynthia Rudin, and Margo Seltzer. Fast sparse decision tree optimization via reference ensembles. In Proceedings of the AAAI conference on Artificial Intelligence, volume 36, pages 9604\u20139613, 2022.   \nChudi Zhong, Zhi Chen, Jiachang Liu, Margo Seltzer, and Cynthia Rudin. Exploring and interacting with the set of good sparse generalized additive models. Advances in Neural Information Processing Systems, 36, 2024.   \nYingfan Wang, Haiyang Huang, Cynthia Rudin, and Yaron Shaposhnik. Understanding how dimension reduction tools work: An empirical approach to deciphering t-sne, umap, trimap, and pacmap for data visualization. Journal of Machine Learning Research, 22(201):1\u201373, 2021.   \nF. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825\u20132830, 2011.   \nZijie J Wang, Chudi Zhong, Rui Xin, Takuya Takagi, Zhi Chen, Duen Horng Chau, Cynthia Rudin, and Margo Seltzer. Timbertrek: Exploring and curating sparse decision trees with interactive visualization. In 2022 IEEE Visualization and Visual Analytics (VIS), pages 60\u201364. IEEE, 2022b.   \nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in Neural Information Processing Systems, 32, 2019.   \nMartin Pawelczyk, Sascha Bielawski, Johannes van den Heuvel, Tobias Richter, and Gjergji Kasneci. CARLA: A python library to benchmark algorithmic recourse and counterfactual explanation algorithms. arXiv preprint arXiv:2108.00783, 2021.   \nGregory Plumb, Denali Molitor, and Ameet S Talwalkar. Model agnostic supervised local explanations. Advances in Neural Information Processing Systems, 31, 2018. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Data Description and Preprocessing ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The datasets were divided into training and test sets using an 80-20 stratification. The numerical features were transformed by standardization to have a mean of zero and a variance of one. The categorical features, which have $k$ different levels, were transformed into $k-1$ binary variables using one-hot encoding. The binary characteristics were transformed into a single dummy variable using one-hot encoding. The sizes of the datasets before and after encoding are shown in Table 4. ", "page_idx": 13}, {"type": "table", "img_path": "GhqdnLZMAz/tmp/2cad705bbe1fe696581f58ccac4fa89d2ef1f399f5ccda40faaec6c8489d87f9.jpg", "table_caption": [], "table_footnote": ["Table 4: Training Dataset Sizes "], "page_idx": 13}, {"type": "text", "text": "Below we provide more details for each dataset. ", "page_idx": 13}, {"type": "text", "text": "COMPAS ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The COMPAS dataset contains information on criminal recidivism in Broward County, Florida [Jeff Larson and Angwin, 2016]. The goal of this dataset is to predict the likelihood of recidivism within a two-year period, taking into account the following variables: gender, age, prior convictions, number of juvenile felonies/misdemeanors, and whether the current charge is a felony. ", "page_idx": 13}, {"type": "text", "text": "Adult ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The Adult data is derived from U.S. Census statistics, including information on demographics, education, employment, marital status, and financial gain/loss [Dua and Graff, 2017]. The target variable of this dataset is whether an individual\u2019s salary exceeds $\\mathbb{S}50{,}000$ . ", "page_idx": 13}, {"type": "text", "text": "MIMIC-III ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "MIMIC-III is a comprehensive database that stores a variety of medical data related to the experience of patients in the Intensive Care Unit (ICU) at Beth Israel Deaconess Medical Center [Johnson et al., 2016a,b]. The outcome of interest is determined by the binary indicator known as the \u201chospital expires flag,\u201d which indicates whether or not a patient died during their hospitalization. We chose the following set of variables as features: age, preiculos (pre-ICU length of stay), gcs (Glasgow Coma Scale), heartrate_min, heartrate_max, meanbp_min (min blood pressure), meanbp_max (max blood pressure), resprate_min, resprate_max, tempc_min, tempc_max, urineoutput, mechvent (whether the patient is on mechanical ventilation), and electivesurgery (whether the patient had elective surgery). ", "page_idx": 13}, {"type": "text", "text": "Diabetes ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The Diabetes dataset is derived from 10 years (1999-2008) of clinical care at 130 hospitals and integrated delivery networks in the United States [Dua and Graff, 2017]. It consists of more than 50 characteristics that describe patient and hospital outcomes. The dataset includes variables such as race, gender, age, admission type, time spent in hospital, specialty of admitting physician, number of lab tests performed, number of medications, and so on. We consider whether the patient will return to the hospital within 2 years as a binary indicator. ", "page_idx": 13}, {"type": "text", "text": "German Credit ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The German credit data [Dua and Graff, 2017] uses financial and demographic indicators such as checking account status, credit history, employment/marital status, etc., to predict whether an individual will default on a loan. ", "page_idx": 14}, {"type": "text", "text": "FICO ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The FICO Home Equity Line of Credit (HELOC) dataset [FICO, 2018] is used for the Explainable Machine Learning Challenge. It includes a number of financial indicators, such as the number of inquiries on a user\u2019s account, the maximum delinquency, and the number of satisfactory transactions, among others. These indicators relate to different individuals who have applied for credit. The target variable is whether a consumer has been 90 or more days delinquent at any time within a 2-year period since opening their account. ", "page_idx": 14}, {"type": "text", "text": "Headlines ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The News Headline dataset [Zhong et al., 2024] is a survey data aimed at discovering what kind of news content is shared and what factors are significantly associated with news sharing. The survey includes several factors, including, age, income, gender, ethnicity, social protection,economic protection, truth (\u201cWhat is the likelihood that the above headline is true?\u201d), familiarity (\u201cAre you familiar with the above headline (have you seen or heard about it before?)? )\u201d), Importance (\u201cAssuming the headline is completely accurate, how important would you consider this news to be?\u201d), Political Concordance (\u201cAssuming the above headline is completely accurate, how favorable would you consider it to be for Democrats versus Republicans?\u201d). The goal of this data set is to predict Sharing (\u201cIf you were to see the above article on social media, how likely would you be to share it?\u201d). ", "page_idx": 14}, {"type": "text", "text": "B Sensitivity of the reference points ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we will mainly show how sensitive $\\mathrm{SEV^{-}}$ is when we change the reference. Figure 8 shows an example of this, where moving the reference further away from the query (from $r$ to the $r^{\\prime}$ ) changes the $\\mathrm{SEV^{-}}$ from 2 to 1. In this figure, the dark blue axes represent the feature values of different reference values, while the black dashed line represents the decision boundary of a linear classifier. Areas with different colors represent data points with different $\\mathrm{SEV^{-}}$ . When the reference moves further from the decision boundary (from $\\pmb{r}$ to $\\boldsymbol{r}^{\\prime}$ ), the corresponding areas for $\\mathrm{SEV^{-}}$ will move away from the decision boundary. For example, the star located in the yellow area has an $\\mathrm{SEV^{-}}$ of 1 instead of 2 when the reference moves from $\\pmb{r}$ to $\\boldsymbol{r}^{\\prime}$ . If the reference point is $r$ , then the query needs to align the feature values along both $\\mathbf{X}$ and y-axis to reach the SEV Explanation with reference $\\pmb{r}$ (recall an example of $\\mathrm{SEV^{-}}$ explanation in Figure 2) in Section 3.2, which is the same point as $\\pmb{r}$ . However, if the reference point is $\\pmb{r}^{\\prime}$ , then the query only needs to align the feature value along the $\\mathbf{X}$ -axis to reach the SEV Explanation with $\\mathrm{SEV=1}$ , which is the light blue dot. ", "page_idx": 15}, {"type": "image", "img_path": "GhqdnLZMAz/tmp/2c9054b0c0681653e019525772233d867e3b66cdd9dccc87abff795e7d9fc18b.jpg", "img_caption": ["Figure $8{\\mathrm{:}}\\ {\\mathrm{SEV^{-}}}$ distribution "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Experiments have also shown that moving data points closer to the decision boundary might increase $\\mathrm{SEV^{-}}$ . The result on the Explainable ML Challenge loan decision data [FICO, 2018] shown in Table 5 demonstrates that altering the reference point may increase the average $\\mathrm{SEV^{-}}$ (from 3 to 5), but also introduces \u201cunexplainable\u201d samples (meaning $\\mathrm{SEV^{-}\\!\\ge\\!10}$ ). Hence, $\\mathrm{SEV^{-}}$ is sensitive to the reference. ", "page_idx": 15}, {"type": "text", "text": "Table 5: $\\mathrm{SEV^{-}}$ change by moving reference point $\\tilde{r}$ moving closer to the decision boundary to $\\tilde{r}^{\\prime}$ ", "page_idx": 15}, {"type": "table", "img_path": "GhqdnLZMAz/tmp/539e71776af16f739ca20f68475d51e4bde987c8a7b59de3056b226a01dbd190.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "C Detailed Description for Score-based Soft K-Means ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "As we have discussed in Section 4.1, $\\mathrm{SEV^{-}}$ needs to have negatively predicted reference points. Therefore, when clustering the negative population, it is necessary to avoid positively predicted cluster centers. However, for most of the existing clustering methods, it is hard to \u201cpenalize\u201d the positive predicted clusters, or their assigned samples. Therefore, we have modified the soft K-Means [Bezdek et al., 1984] algorithm so as to encourage negative clustering results. ", "page_idx": 16}, {"type": "text", "text": "The original Soft K-Means (SKM) algorithm generalizes $\\mathbf{K}$ -means clustering by assigning membership scores for multiple clusters to each point. Given a data set $X=\\{\\pmb{x}_{1},\\pmb{x}_{2},\\pmb{\\cdot}\\cdot\\cdot\\dot{\\mathbf{\\Omega}},\\pmb{x}_{n}\\}$ and $C$ clusters, the goal is to minimize the objective function $J(U,V)$ , where $U=[u_{i j}]$ is the membership matrix and $V=\\{\\mathbf{v}_{1},\\cdots,\\mathbf{v}_{C}\\}$ are the weighted cluster centroids. The objective is to minimize: ", "page_idx": 16}, {"type": "equation", "text": "$$\nJ(U,V)=\\sum_{i=1}^{n}\\sum_{j=1}^{C}u_{i j}^{m}\\|\\pmb{x}_{i}-\\mathbf{v}_{j}\\|_{2}^{2}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $u_{i j}$ is the (soft) membership score of $\\pmb{x}_{i}$ in cluster $j$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\nu_{i,j}=\\frac{1}{\\sum_{k=1}^{C}\\left(\\frac{\\left\\|x_{i}-v_{j}\\right\\|_{2}}{\\left\\|x_{i}-c_{k}\\right\\|_{2}}\\right)^{\\frac{2}{m-1}}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and $m>1$ is a parameter that controls the strength towards each neighboring point. When $m\\approx1$ the SKM is similar to the performance of hard $\\mathbf{K}$ -means clustering methods. When $m>1$ for point $i$ , it is considered to be associated with multiple clusters instead of one distinct cluster. The higher the value of $m$ , the more a point is considered to be part of multiple clusters, thereby reducing the distinctness of each cluster and creating a more integrated and interconnected clustering arrangement. To avoid the cluster group being predicted positively, we have given higher $m$ for those positive samples. Therefore, if the samples are predicted as positive, it reduces the possibility that those positively predicted samples to group as a cluster, which we can replace $m$ as $m_{i}^{\\prime}$ for each instance $\\pmb{x}_{i}$ as ", "page_idx": 16}, {"type": "equation", "text": "$$\nm_{i}^{\\prime}=2m\\cdot\\operatorname*{min}\\{f(\\pmb{x}_{i})-0.5,0\\}+1.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The value of $\\operatorname*{min}\\{f(\\pmb{x}_{i})-0.5,0\\}$ increases as $\\pmb{x}_{i}$ is classified as positive and further away from the decision boundary. As $m^{\\prime}$ increases, the negatively predicted samples are more associated with one distinct cluster, while the positively predicted samples are associated with multiple clusters with smaller weight. This makes the cluster centers less likely to be influenced by positively predicted points. Thus, we can rewrite the objective of the soft K-Means algorithm can be modified as ", "page_idx": 16}, {"type": "equation", "text": "$$\nJ^{\\prime}(U,V)=\\sum_{i=1}^{n}\\sum_{j=1}^{C}u_{i j}^{m_{i}^{\\prime}}\\Vert\\pmb{x}_{i}-\\mathbf{v}_{j}\\Vert_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We call this new objective function for encouraging negative clustering centers Score-based Soft K-Means (SSKM). In our experiments, the clustering is applied to the dataset after PaCMAP [Wang et al., 2021], and the feature mean of all samples in a cluster is considered as the cluster center of this cluster, which is eventually used as a reference point. The queries are assigned to reference points that are closest (based on $\\ell_{2}$ distance) to them in the PaCMAP embedding space for $\\mathrm{SEV^{\\odot}}$ calculation.The reason why we would like to first embed the dataset is that the dimension of the datasets might be too high for direct clustering, and PaCMAP provides an embedding that preserves both local and global structure. Figure 9 shows the probability of the negative predicted instances, as well as the clustering results using different kinds of clustering methods. The red points and stars represent the positively predicted instances and cluster centers, while the blue ones are the negatively predicted instances and cluster centers. It is evident from the Figure that that SKM is more likely to introduce positively predicted cluster centers, compared to SSKM. ", "page_idx": 16}, {"type": "text", "text": "When we calculate $\\mathbf{S}\\mathbf{E}\\mathbf{V}^{\\odot}$ in the experiments, all clustering parameters are tuned and fixed. For the rest of the datasets, the embedding using PaCMAP, and their clustering results for the negative population with their cluster centers, are shown in Figure 10. The regions with different colors represent different clusters, the blue stars in the graphs are cluster centers, and the gray points within the graphs are positive queries. All those cluster centers can be constrained to be predicted as negative by tuning the hyperparameter for Score-based Soft K-Means. Note that if one of the cluster centers cannot be constrained to be predicted as negative even with high $m$ , then it is reasonable to remove this cluster center when calculating $\\mathbf{S}\\mathbf{E}\\mathbf{V}^{\\textregistered}$ . ", "page_idx": 16}, {"type": "image", "img_path": "GhqdnLZMAz/tmp/aacc46e648c97bba6675c0e274daf7cc9bb3b881031a368fe755f9c226ca157a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Figure 9: The clustering results for FICO dataset. (Left) The probability distribution for the negatively labeled queries; (Middle) The clustering result for Original Soft K-Means Clustering; (Right) The clustering result for Score-based K-Means Clustering The red stars represent the positively predicted cluster centers, and the blue stars the negatively predicted cluster centers ", "page_idx": 17}, {"type": "image", "img_path": "GhqdnLZMAz/tmp/89a20c8b9c8524b9444b4d684b8978b6ca3fc17c3d2aeb61150db9dc4d44f031.jpg", "img_caption": ["Figure 10: Clustering Results for different datasets. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "D Detailed Algorithm for Flexible-based SEV ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "This section presents how the flexible-based SEV $(\\mathrm{SEV}^{F})$ has done to determine the flexible references. The key idea of finding the reference is to do a grid search through each of the features in the training dataset based on the original reference, and find the feature values that has the minimum model outcome. ", "page_idx": 18}, {"type": "text", "text": "Algorithm 1 Reference Search for Flexible SEV ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "1: Input: The negative samples $X^{-}$ , flexibility $\\epsilon$ , reference $\\tilde{\\pmb{r}}$ , grid size $G$   \n2: Output: Flexible reference $\\tilde{{\\pmb r}}^{\\prime}$   \n3: Initialization: $\\tilde{\\mathbf{r}}^{\\prime}\\gets\\tilde{\\mathbf{r}}$   \n4: for each feature $j\\in\\mathcal{I}$ , where $\\tilde{r}_{j}$ is the reference value of feature $j$ in $X^{-}$ do   \n5: $q_{j}\\leftarrow$ quantile $(X_{j}^{-},\\tilde{r}_{j})$ {Quantile location of $\\tilde{r}_{j}$ }   \n6: $B_{j}^{+}\\leftarrow$ percentil $:(X_{j}^{-},q_{j}+\\epsilon)$ {The upper range}   \n7: $B_{j}^{-}\\leftarrow$ percentile $(X_{j}^{-},q_{j}-\\epsilon)$ {The lower range}   \n8: $B_{j}^{(g)}\\sim\\mathrm{Uniform}[B_{j}^{-},B_{j}^{+}],g=1,\\cdot\\cdot\\cdot,G$   \n9: $P_{j}^{(g)}\\gets f([\\tilde{r}_{1},\\cdot\\cdot\\cdot\\cdot,B_{j}^{(g)},\\cdot\\cdot\\cdot\\tilde{r}_{J}]),g=1\\cdot\\cdot\\cdot G$ {Slight change to feature $j$ for prediction}   \n10: $g^{\\prime}\\gets\\mathrm{arg\\,min}_{g}\\,P_{j}^{(g)}$ {Find minimum model outcome}   \n11: $\\tilde{r}_{j}^{\\prime}\\leftarrow B_{j}^{(g^{\\prime})}$ {Update for flexible references}   \n12: end for ", "page_idx": 18}, {"type": "text", "text": "E Detailed Algorithms for Tree-based SEV ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "This section presents how the tree-based SEV is calculated through two main procedure: Algorithm 2 (Preprocessing) for collecting all negative pathways and assigning them to each internal nodes and Algorithm 3 (Efficient $\\mathbf{S}\\mathbf{E}\\mathbf{V}^{T}$ Calculation) for checking all negative pathways conditions for each query and calculating the number of feature changes. ", "page_idx": 19}, {"type": "table", "img_path": "GhqdnLZMAz/tmp/1d5f56cfd681e66329fe3beda8cb83d719c12e297874ffc5df2204d2bd0bb477.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "1: Input: $D T$ : decision tree, $D T^{-}$ : decision trees with paths to negative predictions, query value   \n$\\pmb{x}_{i}$ , $D P_{i}$ : list of internal nodes representing decision process for $\\mathbf{\\emx}_{i},p a t h_{i}$ : the encoded $D P_{i}$   \n2: Output: $\\mathrm{SEV}^{T}$   \n3: INITIALIZATION: $\\mathrm{SEV}^{T}\\!\\leftarrow0$   \n4: decision_p $a t h\\leftarrow\\mathrm{encoded}(D T,x$ )   \n5: {encoded $(D T,x_{i})$ is a function to get the string representation of the query $\\pmb{x}_{i}$ or a node node   \nfor $D T$ , e.g. \"LR\",\"LL\" mentioned in section 4.2}   \n6: for each internal node node in $D P_{i}$ do   \n7: if node has a sibling leaf node and is predicted as negative then   \n8: $\\mathrm{SEV}^{T}\\leftarrow1$ {Based on Theoerem 4.1}   \n9: Break $[\\mathrm{SEV}^{T}{=}1$ is the smallest $\\mathrm{SEV}^{T}$ , no further calculation needed}   \n10: end if   \n11: encoded_node $\\leftarrow$ encoded $D T$ , node) {Get the string representation of node}   \n12: negative_path $s\\leftarrow D T^{-}$ [encoded_node] {Get the negative pathways encoded_node have}   \n13: for each path in negative_path do   \n14: {If the negative goes the same direction as the decision path, we don\u2019t need to calculate this   \npath again}   \n15: {path[0] is the first character in path}   \n16: if decision_path[encoded_node.length]=path[0] then   \n17: Continue   \n18: end if   \n19: temp_ $s e\\nu{\\leftarrow}0$   \n20: {Go over the condition in the path}   \n21: {Check if query $x_{i}$ satisfies, if it doesn\u2019t satisfies the condition, then temp_sev should add 1}   \n22: for condition in each path do   \n23: if $\\pmb{x}_{i}$ doesn\u2019t satisfy condition AND $\\pmb{x}_{i}$ hasn\u2019t been changed yet then   \n24: temp_ $s e\\nu{\\leftarrow}t e m p{\\_}s e\\nu{\\leftarrow}1$   \n25: end if   \n26: end for   \n27: $\\mathrm{SEV}^{T}{\\leftarrow}\\operatorname*{min}\\{t e m p\\_s e\\nu,\\mathrm{SEV}^{T}\\}$ {Update $\\mathrm{SEV}^{T}$ to be the samller one}   \n28: if $\\begin{array}{r}{\\mathrm{SEV}^{T}=1}\\end{array}$ then   \n29: Break $\\begin{array}{r}{\\{\\mathrm{SEV}^{T}\\!=\\!1}\\end{array}$ is the smallest $\\mathrm{SEV}^{T}$ , no further calculation needed}   \n30: end if   \n31: end for   \n32: end for ", "page_idx": 20}, {"type": "text", "text": "Baseline models were fit using sklearn [Pedregosa et al., 2011] implementations in Python. The logistic regression models L1 LR and L2 LR were fit using regularization parameter $C\\,=\\,0.01$ . The 2-layer MLP used ReLU activation and consisted of two fully-connected layers with 128 nodes each. It was trained with early stopping. The gradient-boosted classifier used 200 trees with a max depth of 3. For tree-based methods comparisons, the decision tree classifiers were fti using sklearn [Pedregosa et al., 2011] and TreeFARMS packages [Wang et al., 2022b]. Since GOSDT methods require binary input, we used the built-in threshold guessing function in GOSDT to binarize the features with set of parameters $\\mathtt{n\\_e s t=}50$ , and max_depth $^{=1}$ . All the models are trained using a RTX2080Ti GPU, and with 4 core in Intel(R) Xeon(R) Gold 6226 CPU $\\textcircled{a}\\ 2.70\\mathrm{GHz}$ . ", "page_idx": 21}, {"type": "text", "text": "In order to test the performance of All- $\\cdot\\mathrm{Opt^{-}}$ , all models mentioned above were trained by adding the SEV losses from Section 5 to the standard loss term (BCELoss). For GBDT, the training goal is to reweigh the trees from the baseline GBDT model. The resulting loss was minimized via gradient descent in PyTorch [Paszke et al., 2019], with a batch size of 128, a learning rate of 0.1, and the Adam optimizer. To maintain high accuracy, the first 80 training epochs are warm-up epochs optimizing just Binary Cross Entropy Loss for classification (BCELoss). The next 20 epochs add the All-Opt terms and the baseline positive penalty term to encourage low SEV values. Moreover, during the optimization process, it is important to ensure that the reference has a negative prediction. If the reference is predicted as positive, then the $\\mathrm{SEV^{-}}$ may not exist, and a sparse explanation is no longer meaningful. Thus, we add a term to penalize the reference if it receives a positive prediction: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\ell_{\\mathrm{Pos\\_ref}}(f):=\\sum_{i=1}^{n}\\operatorname*{max}(f(\\Tilde{\\boldsymbol{r}}_{i}),0.5-\\theta)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\theta>0$ is a margin parameter, usually $\\theta=0.05$ . This term is $(0.5-\\theta)$ as long as the reference is predicted negative. As soon as it exceeds that amount, it is penalized (increasing linearly in $f(\\tilde{\\boldsymbol{r}}))$ . To put these into an algorithm, we optimize a linear combination of different loss terms, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{f\\in\\mathcal{F}}\\ell_{\\mathrm{BCE}}(f)+C_{1}\\ell_{\\mathrm{SEV\\_All\\_Opt-}}(f)+C_{2}\\ell_{\\mathrm{Pos\\_ref}}(f)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Therefore, we are tuning both $C_{1}$ and $C_{2}$ to find a model with sparser explanations without performance loss through grid search. For cluster-based SEV, the cluster centers are recalculated based on the new model every 5 epochs. ", "page_idx": 21}, {"type": "text", "text": "G The sparsity and meaningful performance of different counterfactual explanation methods ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this section, we provide detailed information on other kinds of counterfactual explanations generated by the CARLA package [Pawelczyk et al., 2021] on different datasets for logistic regression models. Table 6 shows the number of features changed and the $\\ell_{\\infty}$ for different counterfactual explanations. These counterfactual explanations tend to provide less sparse explanations than other $\\bar{\\mathrm{SEV^{-}}}$ variants shown in Section 6.3. For the $\\ell_{\\infty}$ calculations, we consider only the numerical features, since the categorical features\u2019 $\\ell_{\\infty}$ norm does not provide meaningful explanations. Moreover, we have calculated the average log-likelihood of the explanations using the Gaussian Mixture Model in scikit-learn Pedregosa et al. [2011]. The parameter n_components for each dataset is selected based on the clustering result mentioned in Appendix C. Here, we are using the same Gaussian Mixture Model for evaluating whether the explanation is within a high-density region. ", "page_idx": 22}, {"type": "table", "img_path": "GhqdnLZMAz/tmp/9cd257a0e829bdb778099ecf4704807107034e3542b53b498b128b5949051853.jpg", "table_caption": ["Table 6: Explanation performance in different counterfactual explanations "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "H Detailed $\\mathbf{SEV^{-}}$ for all datasets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this section, we show how $\\mathrm{SEV^{1}}$ , $\\mathrm{SEV^{\\odot}}$ , $\\mathrm{SEV}^{\\odot+F}$ can increase the similarity metrics or reduce the sparsity explanations. All the models are trained and evaluated 10 times using different splits, and evaluated for their mean $\\mathrm{SEV^{-}}$ , mean $\\ell_{\\infty}$ , as well as their explanation time for each query. ", "page_idx": 23}, {"type": "text", "text": "Table 7 shows the model performance and $\\mathrm{SEV^{1}}$ on various datasets. $\\mathrm{SEV^{1}}$ is considered as a base case for other $\\mathrm{SEV^{-}}$ variants to compare with. Table 7 shows that $\\mathrm{SEV^{1}}$ yields very high $\\ell_{\\infty}$ for each model, indicating a large distance between the query and reference, which implies low closeness according to Section 3.2. ", "page_idx": 23}, {"type": "text", "text": "Table 8 shows the model performance and $\\mathrm{SEV^{\\odot}}$ on different datasets. Similarly, The Mean $\\mathrm{SEV^{\\odot}}$ column reports the mean $\\bar{\\mathrm{SEV^{\\odot}}}$ for the model and the decrease in mean $\\mathrm{SEV^{-}}$ in percentage compared to $\\mathrm{SEV^{1}}$ (reported in the parenthesis). The Mean $\\ell_{\\infty}$ column reports the mean $\\ell_{\\infty}$ and the percentage reduction compared to $\\mathrm{\\dot{S}E V^{1}}$ . On most datasets, $\\mathbf{S}\\mathbf{E}\\mathbf{V}^{\\odot}$ increases, and $\\ell_{\\infty}$ decreases, which means that the model is providing both sparser and more meaningful explanations. For some datasets like Adult and MIMIC, the $\\mathrm{SE}\\bar{\\mathrm{V}}^{\\odot}$ increases, since the cluster-based reference points might be closer to the decision boundary of the model as each query is trying to find the closest (in $\\ell_{2}$ distance) negatively predicted reference point, which might provide less sparse explanations. ", "page_idx": 23}, {"type": "text", "text": "Table 9 shows the model performance and $\\mathrm{SEV}^{\\odot+F}$ $\\mathrm{[SEV^{\\odot}}$ with variable reference) on various datasets with different flexibility levels. The Mean $\\boldsymbol{\\mathrm{SEV}}^{F}$ column reports the mean $\\mathrm{SEV^{-}}$ for the model and the decrease in mean $\\mathrm{SEV^{-}}$ in percentage compared to $\\mathrm{SEV^{1}}$ (reported in the parenthesis). The Mean $\\ell_{\\infty}$ column reports the mean $\\ell_{\\infty}$ and the percentage reduction compared to $\\mathrm{SEV^{1}}$ . It is evident that with $\\mathrm{SEV}^{F}$ , $\\mathrm{SEV^{-}}$ decreases, but the $\\ell_{\\infty}$ norm will increase due to the flexibility of the features mentioned in section 4.4. The \u201cflexibility used\u201d column shows the proportion of queries using the flexible reference instead of the original one for calculating $\\mathrm{SEV}^{\\dot{F}}$ , and the higher the proportion, the larger decrease in $\\mathrm{SEV^{-}}$ the model can achieve. ", "page_idx": 23}, {"type": "table", "img_path": "GhqdnLZMAz/tmp/4d10437be89e0e5239936c6fbdc029a163ca2236d96947d55db696c6adeeb432.jpg", "table_caption": ["Table 7: The $\\mathrm{SEV^{1}}$ under different models "], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "GhqdnLZMAz/tmp/7b3ebd91d87d7a898c857e5e24ef5601e8ccd22f850f41d3a3aaa32b30f32a5c.jpg", "table_caption": ["Table 8: The $\\boldsymbol{\\mathrm{SEV}}^{\\odot}$ under different models "], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "GhqdnLZMAz/tmp/ada28fb6065eaf0f4864ecffcab18cb3c5b7e19becda2dd7cffb07ba31f62321.jpg", "table_caption": ["Table 9: $\\mathrm{SEV}^{\\odot+F}$ under different models "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "I All-Opt\u2212variants performance ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In this section, we will mainly show the model performance of All- $\\mathrm{Opt^{\\odot}}$ and All- $\\mathrm{Opt^{1}}$ , which are the two gradient-based optimization methods used for $\\boldsymbol{\\mathrm{SEV}}^{\\odot}$ and $S E\\mathrm{V^{1}}$ optimization. Table 10 shows the $\\mathrm{SEV^{1}}$ , $\\ell_{\\infty}$ and model performance after applying All- $\\cdot\\mathrm{Opt}^{1}$ methods for different models on different datasets with different levels of flexibility. It is evident that All- ${\\mathrm{Opt}}^{F}$ has provided a significant decrease in SEV, so that its values are close to 1, providing much sparser explanations without model performance loss and closeness/credibility loss in explanations. Similar findings are observed in Table 11. ", "page_idx": 26}, {"type": "table", "img_path": "GhqdnLZMAz/tmp/93e6ad59923e703b9ab41c572732abd2e0b06dbeef014f8b0d99e8029e11bc18.jpg", "table_caption": ["Table 10: The model performance for All-Opt1 "], "table_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "GhqdnLZMAz/tmp/96e78455211af1adbc1663182bf43888b1ffa1d842dffae9cbe1c0952d1aa5be.jpg", "table_caption": ["Table 11: The model performance for All-Opt\u00a9 "], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "J $\\mathbf{S}\\mathbf{E}\\mathbf{V}^{T}$ in tree-based models ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In this section, we show the model performance and $\\mathrm{SEV}^{T}$ values for different types of tree-based models. As discussed in section 4.2, the similarity and closeness metrics in $\\mathrm{SEV}^{T}$ are all $\\ell_{0}$ norm, so we only need to compute the mean $\\mathbf{S}\\mathbf{E}\\mathbf{V}^{T}$ for each tree. Table 12 shows that most of the tree-based models can provide sparse explanations $(\\mathbf{S}\\mathbf{E}\\mathbf{V}^{T}{\\leq{2}})$ , and we can also find a decision tree with the same model performance as the other tree-based models from $\\mathrm{SEV}^{T}{=}1$ to TOpt. ", "page_idx": 27}, {"type": "table", "img_path": "GhqdnLZMAz/tmp/3dce42893331509518993c34a1249f43a435770ae9eeab76d53bf969c9bc35c3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "K The $\\mathbf{S}\\mathbf{E}\\mathbf{V}^{1}$ results after ExpO optimization ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "For the ExpO comparison experiment, we used the fidelity metrics from Plumb et al. [2020] as the penalty term for regularizing the original model. Then we evaluated the optimized model with $\\mathrm{SEV^{-}}$ . We used two kinds of fidelity metrics as the regularization term: 1D fidelity and 1D fidelity. Both of these two penalty terms aim to optimize the model $f$ such that the local model $g$ [Ribeiro et al., 2016, Plumb et al., 2018] accurately approximates $f$ in the neighborhood $N_{x}$ , which is equivalent to minimizing: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\ell_{\\mathrm{fed}}(f,g,N_{x})=\\mathbb{E}_{{\\pmb x}^{\\prime}\\sim N_{x}}[g({\\pmb x}^{\\prime})-f({\\pmb x}^{\\prime})]^{2}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "The local model $g$ \u2019s are linear models, and the $N_{x}$ are points sampled normally around the original query. The 1D version of Fidelity regularization requires sampling the points around each feature of $\\textbf{\\em x}$ at a time, which saves time and computational complexity. Based on the above equation, we rewrite the overall objective function as: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{f\\in{\\mathcal{F}}}\\ell_{\\mathrm{BCE}}+C_{F}\\ell_{\\mathrm{fed}}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\ell_{\\mathrm{BCE}}$ is the Binary Cross Entropy Loss to control the accuracy of the training model, $C_{F}$ is the strength of the fidelity term, and the training process is the same All- $\\cdot\\mathrm{Opt^{-}}$ optimization, which we used 80 epochs for basic training process, 20 epochs for regularization. ", "page_idx": 28}, {"type": "text", "text": "In this section, we show the $\\mathrm{SEV^{-}}$ and training time for ExpO regularizer in LR and MLP models with 1D Fidelity (1DFed) and Global Fidelity (Fed) regularizers. Comparing the mean $\\mathrm{SEV^{1}}$ of Table 13 with Table 7, it is evident that with the optimization through Fed or 1DFed, the optimized models do not provide sparse explanations. In addition, it takes a long time to calculate Fed and 1DFed since the regularizer\u2019s complexity is determined by the number of queries, features, as well as the points samples around the queries. For $\\mathrm{SEV^{-}}$ , the complexity is determined only by the number of queries and the number of features, so it is much easier to calculate. ", "page_idx": 28}, {"type": "table", "img_path": "GhqdnLZMAz/tmp/4ef5be9a1734869b68ccfd734a351c3a5468d1c01ac57e156559ab1ae1af31ad.jpg", "table_caption": ["Table 13: Model performance, $\\mathrm{SEV^{1}}$ and training time of LR and MLPs after ExpO with different datasets "], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "L Proof of Theorem 4.1 ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Theorem L.1. With a single decision classifier DT and a positively-predicted query $\\pmb{x}_{i}$ , define $N_{i}$ as the leaf that captures it. If $N_{i}$ has a sibling leaf, or any internal node in its decision path has $a$ negatively-predicted child leaf, then $S E V^{T}$ is equal to $^{\\,I}$ . ", "page_idx": 29}, {"type": "text", "text": "$\\mathrm{SEV^{-}}$ is defined as the number of features that need to change within the given classification tree. If you have switched a particular node from one path to another, it adds one to $\\mathrm{SEV^{-}}$ . Therefore, for the internal nodes along the $\\mathrm{SEV^{-}}$ path, if $N_{i}$ has a sibling leaf node, if we goes up to its parent node and goes the opposite direction to change the query value for counterfactual explanation, the modified instance will be directly predicted as negative, which leads to $\\mathrm{SEV^{-}}$ being equal to 1 in this case. ", "page_idx": 29}, {"type": "text", "text": "Figure 11 shows an example for $\\boldsymbol{\\mathrm{SEV}}^{T}$ being exactly 1, and a case illustrating that if $N$ does not have a sibling or any internal node in its decision path that has a negatively-predicted child leaf, $\\mathrm{SEV}^{T}$ should be greater than or equal to 1. In Figure 11, the left trees are the full decision trees, where the blue nodes are the negatively predicted leaf nodes and the red ones are positively predicted. The red arrows graph represents the decision path for a specific instance. The person icon with a plus sign is $N_{i}$ that we would like to calculate $\\bar{\\mathbf{S}}\\bar{\\mathbf{E}}\\bar{\\mathbf{V}}^{T}$ on. The right tree is the subtree of the left tree. The person icon with a minus is the query and the blue arrows indicate a decision pathway for SEV Explanation. ", "page_idx": 29}, {"type": "text", "text": "If the query is predicted as positive in node $\\textcircled{4}.$ , it is easy to see that if we go up to node $\\copyright$ and goes the opposite direction as the decision path for $\\pmb{x}_{i}$ , then you can directly get a negative prediction. In other words, if you change the feature $C$ in the query to make it doens\u2019t satisfy the node $\\copyright$ \u2019s condition, then it can be prediction as negative, which means that $\\mathrm{SEV}^{T}{=}1$ . ", "page_idx": 29}, {"type": "text", "text": "For $\\mathrm{SEV}^{T}\\!\\geq1$ case, if the query predidcted as positive in node $\\textcircled{7},$ since it does not have a sibling leaf node, then if it goes to its parent node $\\circledmathrm{D}$ and goes the opposite direction, then it would reach node $\\textcircled{\\mathrm{E}}$ . However, if we don\u2019t know the query $\\pmb{x}_{i}$ \u2019s value, then I am unable to know whether I need to change the condition in node $\\textcircled{\\mathrm{E}}$ for higher $\\mathbf{S}\\mathbf{E}\\mathbf{V}^{T}$ . Therefore, in this case $\\mathbf{S}\\mathbf{E}\\mathbf{V}^{T}$ can be only guaranteed to be greater or equal to 1. ", "page_idx": 29}, {"type": "image", "img_path": "GhqdnLZMAz/tmp/bb411e080daa2dee08109ef1c9e77d98b3dba462a5563dfa1c06632ba6a5ae16.jpg", "img_caption": ["Figure 11: Example of $\\mathrm{SEV}^{T}{=}1$ in Theorem 4.1 "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "M Proof of Theorem 4.2 ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Theorem M.1. With a single decision tree classifier DT and a positively-predicted query $\\pmb{x}_{i}$ , with the set of all negatively predicted leaves as reference points, both $S E V^{-}$ and the $\\ell_{0}$ distance (edit distance) between the query and the $S E V^{-}$ explanation is minimized. ", "page_idx": 30}, {"type": "text", "text": "Proof (Optimality of Explanation Path): ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "The definition for $\\mathrm{SEV^{-}}$ is the minimum number of features that is needed for a positively predicted query $\\pmb{x}_{i}$ to aligned with the reference point in order to be predicted as negative. For tree-based classifiers, the decisions are all made in the leaf nodes. Since we have set of all the negatively predicted leaves as the reference points, then the $\\ell_{0}$ distance (edit distance) between the query and the $\\mathrm{SEV^{-}}$ explanation is equivalent to be the minimum $\\ell_{0}$ distance between the query and the negatively predicted leaf nodes. Each node can be considered as a list of rules of conditions that needs to be satisfied. If a query would like to be predicted as negative in a specific node, then it needs to change some of the feature values in the query so as to be predicted as negative, and the number of changed feature is $\\mathrm{SEV^{-}}$ . Therefore, $\\mathrm{SEV^{-}}$ and the $\\ell_{0}$ distance are the same in this theorem. ", "page_idx": 30}, {"type": "text", "text": "Next, we would like to show that if one of the negatively predicted leaf nodes is not considered as reference point, then $\\mathrm{SEV^{-}}$ is not minimized. It is really easy to give an counterexample: if we have a decision tree shown in Figure 12 with white nodes as root/internal nodes, blue nodes as negatively predicted node, and the red ones as positively predicted. Suppose we have a query predicted as positive, with feature values $\\{A:{\\mathrm{False}},B:{\\mathrm{~False}},C:{\\mathrm{False}}\\},$ , and only regard node $\\textcircled{1}$ as the reference point, then both feature $A$ and $C$ should be change to True, in order to do a negative prediction, in other words, if only node $\\textcircled{1}$ is the reference point, then $\\mathrm{SEV}^{-}{=}2$ . However, based on Theorem 4.1, since node $\\circled{4}$ has a sibling leaf predicted as negative, then the $\\mathrm{SEV^{-}}$ is not minimized. ", "page_idx": 30}, {"type": "image", "img_path": "GhqdnLZMAz/tmp/2af5b669758146ba458a7c78695c8a5c2459d79b5f0e17680b125e25cded9db9.jpg", "img_caption": ["Figure 12: An counterexample with fewer reference point "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "Lastly, we would like to show that with all the negative leaf nodes considered as reference points if an new reference points is added, the $\\mathrm{SEV^{-}}$ cannot be further minimized. Since we know that the reference points should be predicted as negative, so the newly aded reference should still belongs to one of the existing negative predicted leaf node, so $\\mathrm{SEV^{-}}$ cannot be further minimized. ", "page_idx": 30}, {"type": "text", "text": "To sum up, we have proved that with the set of all negatively predicted leaves as reference points, both $\\mathrm{SEV^{-}}$ and the $\\ell_{0}$ distance (edit distance) between the query and the SEV explanation is minimized. ", "page_idx": 30}, {"type": "table", "img_path": "GhqdnLZMAz/tmp/ecbec8af01d67f71c0cdcb1ce0a7de8f51ac1add04726c2e9cca12c2baaf2c65.jpg", "table_caption": ["N Some extra examples for different kinds of SEV metrics "], "table_footnote": [], "page_idx": 31}, {"type": "table", "img_path": "GhqdnLZMAz/tmp/36c5410907b9f6ce547f4a140fe11939ecfdb566a2019ee33a458a7b34f125b4.jpg", "table_caption": ["Table 15: Different SEV Variants Explanations in COMPAS datasets "], "table_footnote": [], "page_idx": 32}, {"type": "table", "img_path": "GhqdnLZMAz/tmp/bb85e667c4c19bbf8c6c32041e31f9f96d20344fa3c4ec7e9da0e5b30f9f150a.jpg", "table_caption": [""], "table_footnote": [], "page_idx": 33}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and precede the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 34}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 34}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] . ", "page_idx": 34}, {"type": "text", "text": "\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available. \u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 34}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 34}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 34}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: Our motivation and claims are made within the abstract. We have provided experimental and theoretical results for cluster-based SEV, and its variants, and propose algorithm for improving the decision sparsity. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 34}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: Yes, we have discuss the limitation of the work in the conclusion section. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. ", "page_idx": 34}, {"type": "text", "text": "\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 35}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: We have provided the theorem mostly for the tree-based SEV in the Section 4.2, and the corresponding proofs are shown in Appendix L and Appendix M. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 35}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: Yes, all the experiment details are mentioned in the Appendix F. The detailed training process for the comparison with ExpO is shown in Appendix K. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. ", "page_idx": 35}, {"type": "text", "text": "\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 36}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Justification: Yes, we have provided the code for training, and evaluation in the Experiment folder, and the script for running in Script folder. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 36}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: Yes, we have already mentioned them in the Section F. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 37}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: Yes, all the training data has been run for 10 times, which is mentioned in Section F, and all the results are calculated for error bars. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 37}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: Yes, we have error bars for the time execution for each methods and the GPU and CPU details in Appendix F. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. ", "page_idx": 37}, {"type": "text", "text": "\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 38}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: Yes, the paper conforms, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 38}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: Yes, we have mentioned the social impact in the conclusion. Our method has impact in that it provides sparser explanations for those subjected to decisions made by models, including in finance and criminal justice. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 38}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: Our paper doesn\u2019t release models that have the potential to cause harm like image generators or language models. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 39}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: Yes, we have well cited the packages. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 39}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification:The paper provides code. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 39}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 40}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 40}]