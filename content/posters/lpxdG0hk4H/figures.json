[{"figure_path": "lpxdG0hk4H/figures/figures_3_1.jpg", "caption": "Figure 1: Overview of our proposed framework ShowMaker. Our framework adopts a dual-stream design including a Reference U-Net and a Denoising U-Net, where the former takes a reference image as input for appearance encoding and the latter takes noise latent and driving poses as input for diffusion processing. We further equip the backbone with a Key Point-based Fine-grained Hand Modeling module and a Face Recapture module for fine-grained avatar synthesis.", "description": "This figure provides a high-level overview of the ShowMaker framework, a dual-stream model using a Reference U-Net for appearance encoding and a Denoising U-Net for diffusion processing.  It highlights two key components: a Key Point-based Fine-grained Hand Modeling module and a Face Recapture module, which are designed to enhance the quality of generated hand and facial regions. The overall process takes a reference image and driving poses as input and generates a high-fidelity conversational video.", "section": "3 Method"}, {"figure_path": "lpxdG0hk4H/figures/figures_4_1.jpg", "caption": "Figure 2: Architecture of the proposed Face Recapture Module.", "description": "The Face Recapture module enhances the face area of the generated image by considering both texture details and global identity. First, a face detection model crops and aligns the face from the reference image. Then, a pre-trained VAE Encoder and face recognition model (ArcFace) extract the facial texture feature (Ftex) and global identity feature (Fid), respectively. For the facial texture feature, a Multilayer Perceptron (MLP) and a self-attention layer enhance the feature. Similarly, the driving pose's facial pose information is also enhanced using MLP and self-attention. Finally, two cross-attention blocks merge the enhanced texture feature and identity feature with the enhanced facial pose information to obtain comprehensive face information (Gf). This comprehensive information is then fed into the face attention layer in the Denoising U-Net to improve the quality of the generated faces.", "section": "3.4 Face Recapture Module"}, {"figure_path": "lpxdG0hk4H/figures/figures_6_1.jpg", "caption": "Figure 3: Qualitative results compared with other methods. Our approach achieves high-fidelity gesture details and satisfactory image quality in both self and cross-driving settings.", "description": "This figure displays a qualitative comparison of the proposed ShowMaker model against other state-of-the-art methods (AnimateAnyone, MagicAnimate, MagicPose, Disco, TPS) in both self-driving (using the same identity for reference and driving video) and cross-driving (using different identities) settings.  The results showcase the superior performance of ShowMaker in generating high-fidelity videos with detailed hand gestures and facial expressions. The red boxes in certain images highlight areas where competing methods show artifacts or inaccuracies in hand or face details.  ShowMaker consistently produces cleaner, more realistic results.", "section": "4 Experiments"}, {"figure_path": "lpxdG0hk4H/figures/figures_8_1.jpg", "caption": "Figure 3: Qualitative results compared with other methods. Our approach achieves high-fidelity gesture details and satisfactory image quality in both self and cross-driving settings.", "description": "This figure displays a qualitative comparison of the proposed ShowMaker method with other state-of-the-art methods for 2D human video generation.  It shows results for both self-driving (where the driving video features the same person as the reference image) and cross-driving (where the driving video features a different person than the reference image). The results illustrate that ShowMaker produces high-fidelity results with accurate gestures, superior image quality, and good consistency across both driving conditions.  It directly compares generated video frames from ShowMaker to those from AnimateAnyone, MagicAnimate, MagicPose, Disco, and TPS across the self and cross-driving scenarios.", "section": "4 Experiments"}, {"figure_path": "lpxdG0hk4H/figures/figures_8_2.jpg", "caption": "Figure 5: Qualitative ablation results when removing different components in our framework.", "description": "This figure shows a qualitative comparison of the results obtained using the full ShowMaker model against those obtained by removing either the Key Point-based Fine-grained Hand Modeling module, the Face Recapture module, or both.  The top row shows hand gestures, highlighting the improved realism and detail when the Hand Modeling module is included. The bottom row shows facial expressions and details, illustrating the superior quality achieved with the Face Recapture module.  The differences clearly demonstrate the significant contributions of both components to the overall high-fidelity video synthesis.", "section": "4.3 Ablation Study"}, {"figure_path": "lpxdG0hk4H/figures/figures_14_1.jpg", "caption": "Figure 1: Overview of our proposed framework ShowMaker. Our framework adopts a dual-stream design including a Reference U-Net and a Denoising U-Net, where the former takes a reference image as input for appearance encoding and the latter takes noise latent and driving poses as input for diffusion processing. We further equip the backbone with a Key Point-based Fine-grained Hand Modeling module and a Face Recapture module for fine-grained avatar synthesis.", "description": "This figure presents a high-level overview of the ShowMaker framework, a dual-stream generative model for creating high-fidelity 2D human videos. The framework takes a reference image and driving poses as input and utilizes a Reference U-Net for appearance encoding and a Denoising U-Net for diffusion processing.  Two novel modules, the Key Point-based Fine-grained Hand Modeling module and the Face Recapture module, are integrated to improve the quality of the generated videos, particularly in the hands and face regions.", "section": "3 Method"}, {"figure_path": "lpxdG0hk4H/figures/figures_14_2.jpg", "caption": "Figure 7: Ablation experiments of hand module.", "description": "This figure shows the ablation study results of the Key Point-based Fine-grained Hand Modeling module.  It compares the generated hand images using four different methods: Vanilla (without the module), Hand image (using cropped hand images as input), w/o positional encoding (excluding positional encoding from the module), and Ours (using the full Key Point-based Fine-grained Hand Modeling module).  The visual difference in hand details and quality demonstrates the effectiveness of each component in the proposed module.", "section": "3.3 Key point-based Fine-grained Hand Modeling"}, {"figure_path": "lpxdG0hk4H/figures/figures_15_1.jpg", "caption": "Figure 8: The generation results of adjacent frames.", "description": "This figure shows example video frames generated by the ShowMaker model.  It demonstrates the temporal consistency of the generated videos by showing three frames (Frame 1, Frame 5, Frame 9) from two different video sequences. The consistency in the person's appearance and movement between the frames highlights the model's ability to maintain temporal coherence.", "section": "3.2 Pipeline Overview"}, {"figure_path": "lpxdG0hk4H/figures/figures_15_2.jpg", "caption": "Figure 3: Qualitative results compared with other methods. Our approach achieves high-fidelity gesture details and satisfactory image quality in both self and cross-driving settings.", "description": "This figure showcases qualitative results comparing the proposed ShowMaker model with other state-of-the-art methods for 2D human video generation.  Two scenarios are illustrated: self-driving (where the driving and reference videos are from the same person) and cross-driving (where the driving and reference videos are from different people).  The figure demonstrates ShowMaker\u2019s superiority in generating videos with high-fidelity gesture details and good image quality in both scenarios, contrasting it with artifacts and lower fidelity found in other approaches.", "section": "4 Experiments"}]