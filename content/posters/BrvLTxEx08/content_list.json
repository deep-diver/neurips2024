[{"type": "text", "text": "Learning Equilibria in Adversarial Team Markov Games: A Nonconvex-Hidden-Concave Min-Max OptimizationProblem ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Fivos Kalogiannis\\* Jingming Yan\\* University of California, Irvine University of California, Irvine Archimedes/Athena RC, Greece ", "page_idx": 0}, {"type": "text", "text": "Ioannis Panageas University of California, Irvine Archimedes/Athena RC, Greece ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We study the problem of learning a Nash equilibrium (NE) in Markov games which is a cornerstone in multi-agent reinforcement learning (MARL). In particular, we focus on infinite-horizon adversarial team Markov games (ATMGs) in which agents that share a common reward function compete against a single opponent, the adversary. These games unify two-player zero-sum Markov games and Markov potential games, resulting in a setting that encompasses both collaboration and competition. [65] provided an efficient equilibrium computation algorithm for ATMGs which presumes knowledge of the reward and transition functions and has no sample complexity guarantees. We contribute a learning algorithm that utilizes MARL policy gradient methods with iteration and sample complexity that is polynomial in the approximation error $\\epsilon$ and the natural parameters of the ATMG, resolving the main caveats of the solution by [65]. It is worth noting that previously, the existence of learning algorithms for NE was known for Markov two-player zero-sum and potential games but not for ATMGs. ", "page_idx": 0}, {"type": "text", "text": "Seen through the lens of min-max optimization, computing a NE in these games consists a nonconvex-nonconcave saddle-point problem. Min-max optimization has received extensive study. Nevertheless, the case of nonconvex-nonconcave landscapes remains elusive: in full generality, finding saddle-points is computationally intractable [33]. We circumvent the aforementioned intractability by developing techniques that exploit the hidden structure of the objective function via a nonconvex-concave reformulation. However, this introduces the challenge of a feasibility set with coupled constraints. We tackle these challenges by establishing novel techniques for optimizing weakly-smooth nonconvex functions, extending the framework of [35]. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Multi-agent reinforcement learning (MARL) investigates behaviors of multiple interacting agents within a dynamic, shared environment where the actions of each agent not only impact their individual rewards but also the overall state of the system. MARL has introduced several practical techniques that have justifiably captured public interest in recent years, particularly in skill-intensive games like starcraft, go, chess, and poker [12, 97, 101, 79, 16, 15, 14, 86], where its empirical methods have achieved super-human performance. More recently, MARL methods combined with large language models has excelled in the game of Diplomacy [6]. Despite these practical achievements, theoretical understanding of MARL has lagged behind its empirical successes. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Markov games (MGs) [95] is a rigorous and versatile mathematical structure that MARL employs to systematically formalize the strategic interactions in the dynamic settings [71]. These games extend Markov decision processes (MDPs) [88] to multiple agents, each making decisions and receiving rewards independently as the environment evolves. The joint decisions of the agents influence both individual rewards and the transition of the environment. MARL in general is occupied with leading the multi-agent system to a favorable outcome. Through the lens of game theory, the notion of a \u201cfavorable outcome' is formally defined through concepts like a Nash equilibrium and a (coarse) correlated equilibrium. Although computing Nash equilibria is generally computationally intractable\u2014even in two-player games without states [28, 24]\u2014\u2014-it becomes tractable in fully cooperative settings like Markov potential games [114, 68] and is also tractable in competitive scenarios such as two-player zero-sum Markov games [27, 103, 17]. Recent advances [65] also show computational tractability in adversarial team Markov games (ATMGs)\u2014a context that combines both cooperative and competitive dynamics among agents. More specifically, an infinite-horizon adversarial team Markov game (ATMG) is a Markov Game in which $n$ team players, compete against one adversary. Each of the team players receives the same reward and is equal to minus the reward of the adversary. ATMGs generalize both Markov zero-sum and potential games; the former can be viewed asATMGswith $n=1$ , the latter by choosing the adversary to be dummy (having one action). ", "page_idx": 1}, {"type": "text", "text": "Nash equilibrium computation in ATMGs naturally leads to a min-max optimization problem. Minmax optimization has been deeply explored across game theory, optimization, and machine learning. The past decade it has witnessed a proliferation of min-max optimization applications, notably in areas like generative adversarial networks (GANs) [49], robust machine learning [73], and adversarial training [50]. In these applications, the optimization objectives often involve nonconvex-nonconcave functions which pose substantial challenges. Typically, the aim is to approximate saddle-points of $f({\\boldsymbol{x}},{\\boldsymbol{y}})$ . In normal form games, these points correspond to Nash equilibria. This correspondence also holds true for MGs due to the gradient domination property [3]. Although we cannot aspire to cover the vast quantity of works in MARL and optimization, we select some representative works that we defer to Appendix A due to space constraints. ", "page_idx": 1}, {"type": "text", "text": "This paper aims to develop learning methods to approximate Nash equilibria in team Markov games by using only individual rewards and state observations as feedback, addressing the following question and answering one of the main caveats of the solutions provided in [65]: ", "page_idx": 1}, {"type": "text", "text": "Is it possiblefor agents to efficientlylearn Nash equilibria in adversarial team Markov games, having only access to trajectory roll-out samples and (almost\\* ) no (+) communication,i.e.,independently? ", "page_idx": 1}, {"type": "text", "text": "1.1 Our Contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Let us provide some context before stating our main results. An infinite-horizon adversarial team Markov game (ATMG) is characterized by a finite state-space $\\boldsymbol{S}$ $n$ team players, each equipped with a finite action-space $A_{i}$ $i\\in\\{1,\\ldots,n\\}$ , and one adversary with a finite action-space $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ . Each of the team players receives the same reward which is equal to minus the reward of the adversary. The adversary's value function is defined as the discounted expected sum of their rewards, where the discount factor is $\\gamma\\in[0,1)$ . An approximate Nash equilibrium is a product distribution over policy space such that no agent can improve their value by unilaterally deviating. We propose a learning algorithm that has both iteration and sample complexity polynomial in the parameters of the Markov Game and returns approximate Nash equilibria. ", "page_idx": 1}, {"type": "text", "text": "Theorem 1.1 (Informal Version of Theorem 3.3). There is a learning algorithm (ISPNG) that uses bandit feedback and guarantees convergence to an E-approximate Nash equilibrium in adversarial team Markov games, the sample and iteration complexities of which are ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathsf{p o l y}\\left(\\frac{1}{\\epsilon},|S|,\\sum_{k=1}^{n}|A_{i}|+|B|,\\frac{1}{1-\\gamma}\\right).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "We deem noteworthy that our algorithm manages to compute a Nash equilibrium in a Markov game, which combines opposing and shared agent interests, by only using a number of iterations and samples that is polynomial in the approximation error and the description of the game. Further, it manages to beat the curse of multi-agents [62]\u2014i.e., its iteration and sample complexity depends on $\\sum_{k=1}^{n}|A_{i}|$ instead of $\\textstyle\\prod_{k=1}^{n}\\left|A_{i}\\right|$ ", "page_idx": 2}, {"type": "text", "text": "In order to achieve the latter contribution, we acquired convergence guarantees for stochastic projected gradient descent in nonconvex functions when the gradient is Holder-continuous\u2014a notion of continuity weaker than that of Lipschitz. Finally, we contribute a general result that guarantees convergence to a saddle-point in functions that are nonconvex-hidden-strongly-concave. ", "page_idx": 2}, {"type": "text", "text": "1.2  Technical Overview ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The problem of computing an approximate Nash equilibrium in an adversarial team Markov game boils down to computing an approximate saddle-point $(x^{*},y^{*})$ of the adversary's value function $V(x,y)$ ; see Definition 2.3. The variables $\\textbf{\\em x}$ denote the policies of the team, each member of which aims to individually minimize $V$ .Moreover, $\\textit{\\textbf{y}}$ denotes the policy of the adversary who aims to maximize $V$ . The equivalence between saddle-points and equilibria is due to (i) the game being zero-sum between the team and the adversary and (i) the gradient domination property (see Lemma C.7) that holds per player, and has already been established in prior works [3, 68, 114]. In words, gradient domination in our setting implies that any approximate first-order stationary policy is also an approximate best response for that player. ", "page_idx": 2}, {"type": "text", "text": "The problem of computing an approximate saddle-point $(x^{*},y^{*})$ of the objective $V(x,y)$ poses computational challenges due to its nonconvex-nonconcave nature. Previous work [65] showed that one can compute an approximate saddle-point $(x^{*},y^{*})$ of $V$ , by first obtaining an approximate stationary point $x^{*}$ of $\\Phi({\\pmb x})=\\operatorname*{max}_{\\pmb y}V({\\pmb x},{\\pmb y})$ through a Moreau envelope argument and then extending it to $(x^{*},y^{*})$ . The proof of extendibility uses involved arguments that utilize the Lagrange multipliers of a carefully chosen nonlinear program (for the stationary point $x^{*}$ ), while the computation of $\\boldsymbol{y}^{*}$ requires solving another linear program. It is worth noting that the aforementioned linear program presumes access to the full description of the reward function and the transition model of the underlying Markov game when the team plays policy $x^{*}$ . This fact prevents the possibility of casting this approach into a learning algorithm. ", "page_idx": 2}, {"type": "text", "text": "Our proposed (learning) algorithm bypasses the requirement for knowledge of the reward function and the transition model, and works under the bandit feedback framework. The first idea behind our algorithm is to consider the adversary's value function as a function $F$ of the adversary's state-action visitation measure $\\lambda$ $,F\\left(x,\\lambda\\right):=V(\\mathbf{{x}},y)$ and the addition of aregularizing tm $-\\,\\frac{\\nu}{2}\\left\\|\\pmb{\\lambda}\\right\\|^{2}$ $\\nu$ can be thought of as a small positive scalar). As a result, the max function of the regularized value function, $\\begin{array}{r}{\\Phi^{\\nu}(\\pmb{x})\\stackrel{*}{:=}\\operatorname*{max}_{\\lambda\\in\\Lambda(\\pmb{x})}\\Big\\{{F\\left(\\pmb{x},\\lambda\\right)}-\\frac{\\nu}{2}\\left\\Vert\\pmb{\\lambda}\\right\\Vert^{2}\\Big\\},}\\end{array}$ is differentiable, where $\\Lambda(\\pmb{x})\\subseteq\\Delta^{|S||B|}$ denotes the feasibility set of $\\lambda$ and depends on $\\textbf{\\em x}$ . Effectively, different policies, $\\textbf{\\em x}$ , for the team induce a different single agent Markov decision process for the adversary. The addition of the regularizer allows us to apply Danskin's theorem on a function with a unique maximizer circumventing the necessity of solving a linear program; one only needs to approach that unique solution. To the best of our knowledge, this is the first work introducing a function of $\\lambda$ as a regularizing term. ", "page_idx": 2}, {"type": "text", "text": "By reformulating the regularized value function using state-action visitation measure $\\lambda$ , the problem boils down to learning an approximate saddle-point of a nonconvex-strongly-concave function with coupled constraints. Coupled constraints are a type of constraints that cannot be expressed as a Cartesian product (the main well-studied setting in min-max optimization [63]), i.e., the feasibility set $\\Lambda({\\pmb x})$ , depends on $\\textbf{\\em x}$ . The first challenge towards handling the coupled constraints is to argue that $\\nabla\\Phi^{\\nu}$ is Holder-continuous which is a notion of continuity weaker than Lipschitz continuity (see Definition 2.1). Specifically, in Theorem 3.2, we show that $\\Phi^{\\nu}(x)$ is weakly-smooth, or equivalently, $\\nabla\\Phi^{\\nu}$ is Holder-continuous. It seems unlikely that we could use Moreau envelope techniques to prove convergence of stochastic projected gradient descent on a weakly-smooth function. The next step of our proof is to transfer the weakly-smooth nonconvex optimization problem into a smooth optimization problem with inexact gradient oracles, extending the techniques from [35] to nonconvex and constrained settings. Since we only allow each player to observe the reward they received and not the action chosen by the other players (including the adversary), one last challenge we have to deal with is the inability to estimate the state-action visitation measure $\\lambda$ of the adversary, making the gradient inexact when computing $\\nabla\\Phi^{\\nu}(x)$ in both deterministic and stochastic settings. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Starting, we will introduce the notation conventions we use and split the rest of the preliminaries into two subsections. Section 2.1 provides necessary definitions whereas Section 2.2 deals with the preliminaries of (adversarial team) Markov games and the notion of Nash equilibrium. ", "page_idx": 3}, {"type": "text", "text": "Notation.  We denote $[n]:=\\{1,\\ldots,n\\}$ We use superscripts to denote the (discrete) time index, and subscripts to index the players. We use boldface for vectors and matrices; scalars will be denoted by lightface variables. We define $\\|\\cdot\\|_{2},\\|\\cdot\\|_{1},\\|\\cdot\\|_{\\infty}$ to be the $\\ell_{2}$ -norm, the $\\ell_{1}$ -norm and the $\\ell_{\\infty}$ norm respectively. The simplex of probability vectors supported on a finite set $\\boldsymbol{\\mathcal{A}}$ is noted as $\\Delta(A)$ . Unless specified otherwise, we denote $\\|\\cdot\\|_{2}$ by $\\Vert\\cdot\\Vert$ $\\mathrm{Diam}_{\\mathcal{X}}$ denotes the diameter of a compact set $\\mathcal{X}$ in $\\ell_{2}$ -distance. For simplicity in the exposition, we may sometimes use the $O(\\cdot)$ notation to suppress dependencies that are polynomial in the natural parameters of the problem and $\\tilde{O}(\\cdot)$ to further hide logarithmic factors; precise statements are given in the Appendix. For the convenience of the reader, a comprehensive overview of our notation is given in Table 1. ", "page_idx": 3}, {"type": "text", "text": "2.1  Basic Definitions and Facts ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We commence this subsection by introducing a number of concepts and statements of mathematical analysis and optimization. We define Holder continuity and the notion of a stationary point in constrained minimization and min-max optimization. ", "page_idx": 3}, {"type": "text", "text": "The notion of Holder continuity of the gradient is a weaker notion of Lipschitz gradient continuity. ", "page_idx": 3}, {"type": "text", "text": "Definition 2.1 ( $p$ Holder continuous gradient). A function $\\phi:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ is said to have a $(\\ell_{p},p)$ Holder continuous gradient if for every $z,z^{\\prime}\\in\\mathbb{R}^{d}$ ,it holds that: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla\\phi(z)-\\nabla\\phi(z^{\\prime})\\|_{2}\\leq\\ell_{p}\\|z-z^{\\prime}\\|_{2}^{p}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "When $p=1$ , we retrieve the definition of an $\\ell$ -smooth function. ", "page_idx": 3}, {"type": "text", "text": "Throughout, following standard conventions, we will refer to functions for which the gradient is $p$ -Holder continuouswith a $p<1$ as weakly-smooth. We state the notions of first-order stationarity relevanttoourwork. ", "page_idx": 3}, {"type": "text", "text": "Definition 2.2 $\\mathbf{\\sigma}_{\\epsilon}$ -FOSP). In the context of the constrained minimization problem $\\begin{array}{r}{\\operatorname*{min}_{z\\in\\mathcal{Z}}\\phi(z),}\\end{array}$ a point $z\\in{\\mathcal{Z}}$ is said to be an $\\epsilon$ -approximatestationary point $i f,$ ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\left\\langle-\\nabla_{z}\\phi(z),z^{\\prime}-z\\right\\rangle\\leq\\epsilon,\\quad\\forall z^{\\prime}\\in\\mathcal{Z}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Similarly, we will define an $\\epsilon$ -approximate saddle-point for the constrained min-max optimization problem $\\operatorname*{min}_{\\mathcal X}\\operatorname*{max}_{\\mathcal Y}f(\\pmb{x},\\pmb{y}).$ ", "page_idx": 3}, {"type": "text", "text": "Definition 2.3 $\\mathbf{\\Xi}_{\\epsilon}$ -SP). Let a function $f:\\mathcal{X}\\times\\mathcal{Y}\\to\\mathbb{R}$ . A point $({\\boldsymbol{x}},{\\boldsymbol{y}})\\in{\\mathcal{X}}\\times{\\mathcal{Y}}$ is said to be an $\\epsilon$ -approximate saddle-point(or $\\epsilon$ -FOSP for the min-max problem) $i f,$ ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{-\\nabla_{x}f({\\pmb x},{\\pmb y})^{\\top}\\left({\\pmb x}^{\\prime}-{\\pmb x}\\right)\\le\\epsilon,\\forall{\\pmb x}^{\\prime}\\in\\mathcal{X};}\\\\ {\\nabla_{y}f({\\pmb x},{\\pmb y})^{\\top}\\left({\\pmb y}^{\\prime}-{\\pmb y}\\right)\\le\\epsilon,\\forall{\\pmb y}^{\\prime}\\in\\mathcal{Y}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "2.2 Adversarial Team Markov Games ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "An adversarial team Markov game is the Markov game extension of normal-form adversarial team games [98]. The game takes place in an infinite-horizon discounted setting where a team of identicallyinterested players compete against one adversarial player, the adversary. We can formally define an adversarial team Markov game as a tuple $\\Gamma(S,[n+1],\\mathcal{A},\\mathcal{B},r,\\mathbb{P},\\gamma,\\rho)$ where: ", "page_idx": 3}, {"type": "text", "text": "\u00b7 $\\boldsymbol{S}$ is the finite set of states, or state-space, with cardinality $S:=|S|$   \n\u00b7 $[n+1]$ is the set of players, with the first $n$ players belonging to the team and the last one being the adversary;   \n\u00b7 $\\boldsymbol{\\mathcal{A}}=\\boldsymbol{\\times}_{i=1}^{n}\\,\\boldsymbol{\\mathcal{A}}_{i}$ is the finite set of the team's joint actions (or, team's action-space), while $\\boldsymbol{A}_{i}$ is the $i$ th player's action-space; respectively $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ is the adversary's action-space; further, $A:=\\operatorname*{max}_{i\\in[n]}|A_{i}|$ and $B:=|\\boldsymbol{\\beta}|$ .\uff0c   \n\u00b7 $r:S\\times A\\times B\\to[0,1]$ is the adversary's reward function;   \n\u00b7 $\\mathbb{P}:S\\times A\\times B\\to\\Delta(S)$ is transition probability function; $\\gamma\\in[0,1)$ is the discount factor;   \n\u00b7 $\\rho\\,\\in\\,\\Delta(S)$ is the initial state distribution. We assume that $\\rho$ is of full-support, $\\rho(s)>$ $0,\\forall s\\in S$ ", "page_idx": 4}, {"type": "text", "text": "Every team player $i\\in[n]$ gets the same reward and the sum of team players? rewards are equal to the adversary's loss, i.e, $\\begin{array}{r}{\\dot{\\sum_{i=1}^{n}r_{i}}(s,\\mathbf{a},b)=-r(s,\\mathbf{a},b)}\\end{array}$ ", "page_idx": 4}, {"type": "text", "text": "2.2.1 Policies, Value Function, and Visitation Measures ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this part, we describe policy classes, the value function, and the state-action visitation measures.   \nAll of these notions are indispensable for our analysis. ", "page_idx": 4}, {"type": "text", "text": "Policy Definitions. For any agent $i$ , a stationary policy $\\pi_{i}$ is defined as a mapping from any given state to a probability distribution over possible actions, where $\\pi_{i}:S\\ni s\\mapsto\\pi_{i}(\\cdot|s)\\in\\Delta(A_{i})$ .A policy $\\pi_{i}$ is described as deterministic when, for any state, it selects a particular action with probability of 1. To simplify, we denote the policy spaces for the team and the adversary as $\\Pi_{\\mathrm{team}}:\\bar{\\cal S}\\rightarrow\\Delta(\\dot{\\cal A})$ and $\\Pi_{\\mathrm{adv}}:{\\mathcal{S}}\\to\\Delta(B)$ ,respectively. Additionally, the combined policy space for all participants can be represented as $\\bar{\\Pi}:\\mathcal{S}\\rightarrow\\Delta(\\mathcal{A})\\times\\Delta(\\mathcal{B})$ ", "page_idx": 4}, {"type": "text", "text": "Direct Policy Parametrization. In the context of our work, we assume the strategy of direct policy representation for all players. Specifically, for each player $i$ within the set $[n]$ , the policy space $\\mathbf{\\mathcal{X}}_{i}$ is defined as $\\Delta(A_{i})^{S}$ , with $\\pi_{i}=x_{i}$ , such that the probability of choosing action $a$ in state $s$ \uff0c $x_{i,s,a}$ equals $\\pi_{i}(a|s)$ . By the usual game-theoretic convention, $\\pi_{-i}$ denotes the policy of all agents apart from $i$ . For the adversary, $\\boldsymbol{\\wp}$ is set as $\\Delta(B)^{S}$ , with $\\pi_{\\mathrm{adv}}=y$ , so that $y_{s,a}=\\pi_{\\mathrm{adv}}(a|s)$ ", "page_idx": 4}, {"type": "text", "text": "Having defined policies, we can introduce some standard shortcut notations such as $r(s,\\pmb{x},\\pmb{y})\\ :=\\ \\mathbb{E}_{(\\pmb{a}\\,b)\\sim(\\pmb{x},\\pmb{y})}[r(s,\\pmb{a},b)]$ ,and thevectors $\\pmb{r}(\\pmb{x})\\ \\in\\ \\mathbb{R}^{|S|\\times|\\mathcal{B}|},\\pmb{r}(\\pmb{x},\\pmb{y})\\ \\in\\ \\mathbb{R}^{|S|}$ with $\\begin{array}{r l r}{r(x)}&{{}:=}&{[\\mathbb{E}_{a\\sim x}[r(s,\\mathbf{a},b)]]_{s,b}}\\end{array}$ and $r(x,y)\\;:=\\;\\left[\\mathbb{E}_{(a,b)\\sim x}[r(s,a,b)]\\right]_{s}$ .Further, we define $\\mathbb{P}(s^{\\prime}|s,x,y)$ as $\\mathbb{P}(s^{\\prime}|s,\\pmb{x},\\pmb{y}):=\\mathbb{E}_{(\\pmb{a},\\pmb{b})\\sim(\\pmb{x},\\pmb{y})}[\\mathbb{P}(s^{\\prime}|s,\\pmb{a},\\pmb{b})]$ and the vector $\\bar{\\mathbb{P}}(s,{\\pmb x},b)\\in\\Delta(S)$ with $\\mathbb{P}(s,\\pmb{x},\\pmb{y}):=\\left[\\mathbb{E}_{(\\pmb{a},b)\\sim(\\pmb{x},\\pmb{y})}[\\mathbb{P}(s^{\\prime}|s,\\pmb{a},b)]\\right]_{s^{\\prime}}$ ", "page_idx": 4}, {"type": "text", "text": "The Value Function. The value function $V_{s}$ , for a given state $s\\in S$ , is defined as the adversary's expected total discounted reward over time under a combined policy $(\\pi_{\\mathrm{team}},\\pi_{\\mathrm{adv}})$ from the policy space $\\Pi$ with $x=\\pi_{\\mathrm{team}}$ being the aggregation of policies $(\\pmb{\\pi}_{1},\\bot...,\\pmb{\\pi}_{n})$ . Formally, this is represented as ", "page_idx": 4}, {"type": "equation", "text": "$$\nV_{s}({\\pmb x},{\\pmb y}):=\\mathbb{E}_{{\\pmb x},{\\pmb y}}\\left[\\sum_{h=0}^{\\infty}\\gamma^{h}r(s_{h},{\\pmb a}_{h},b_{h})\\bigg|\\,s_{0}=s\\right],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the expected value is calculated over the distribution of trajectories generated by the policies $\\textbf{\\em x}$ and $\\textit{\\textbf{y}}$ . If the initial state is instead sampled from a distribution $\\rho$ , the value function is expressed as $V_{\\rho}(\\pmb{x},\\pmb{y})=\\mathbb{E}_{s\\sim\\rho}\\left[V_{s}(\\pmb{x},\\pmb{y})\\right]$ ", "page_idx": 4}, {"type": "text", "text": "Visitation Measures. The important quantity of state-action visitation measures, or the expected discounted sum of visitations of a state-action pair. ", "page_idx": 4}, {"type": "text", "text": "Definition 2.4 (State-Action Visit. Measure). For any initial distribution $\\rho\\,\\in\\,\\Delta(S)$ \uff0ctransition matrix $\\mathbb{P}_{i}$ a team policy $\\textbf{\\em x}$ , and a policy $\\pmb{y}\\in\\mathcal{V}$ we define the station-actionvisitationmeasure of the adversary $\\lambda(y;x)$ as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\lambda_{s,b}(\\pmb{y};\\pmb{x}):=\\sum_{h=0}^{\\infty}\\gamma^{h}\\,\\mathbb{P}(s_{h}=s,b_{h}=b|\\pmb{x},\\pmb{y},s_{0}\\sim\\rho).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "As we will further discuss in the appendix (Appendix C.1), the correspondence between $\\textit{\\textbf{y}}$ and $\\lambda$ \"1-1\" for a fixed team policy $\\textbf{\\em x}$ . This property is crucial for our contributions. ", "page_idx": 5}, {"type": "text", "text": "Reformulation of the Value Function.  A key property of the value function $V_{\\rho}$ is that it can be rewritten as a concave function of the state-action visitation measure: ", "page_idx": 5}, {"type": "equation", "text": "$$\nV_{\\rho}(\\mathbf{\\boldsymbol{x}},\\mathbf{\\boldsymbol{y}})=r(\\mathbf{\\boldsymbol{x}})^{\\top}\\lambda(\\mathbf{\\boldsymbol{y}};\\mathbf{\\boldsymbol{x}}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Definition 2.5 $\\epsilon$ -NE). A product policy $({\\boldsymbol{x}}^{*},{\\boldsymbol{y}}^{*})\\;\\in\\;{\\boldsymbol{\\mathcal{X}}}\\times{\\boldsymbol{\\mathcal{Y}}}$ is called an $\\epsilon$ -approximate Nash equilibrium for an $\\epsilon\\geq0$ when ", "page_idx": 5}, {"type": "equation", "text": "$$\nV_{\\rho}(\\pmb{x}^{*},\\pmb{y}^{*})\\leq V_{\\rho}((\\pmb{x}_{i}^{\\prime},\\pmb{x}_{-i}^{*}),\\pmb{y}^{*})+\\epsilon,\\forall\\pmb{x}_{i}^{\\prime}\\in\\mathcal{X}_{i},\\forall i\\in[n];\n$$", "text_format": "latex", "page_idx": 5}, {"type": "equation", "text": "$$\nV_{\\rho}(\\pmb{x}^{*},\\pmb{y}^{*})\\geq V_{\\rho}(\\pmb{x}^{*},\\pmb{y}^{\\prime})-\\epsilon,\\qquad\\qquad\\forall\\pmb{y}^{\\prime}\\in\\mathcal{y}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "2.2.2  The Gradient and Visitation Measure Estimators. ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "An essential element that led to the development of policy gradient methods is the policy gradient theorem [104]. Notably, it has enabled the design of finite-sample gradient estimators. This technique fits well into the MARL independent learning protocol [27]. After all agents have proposed their policy, the MDP is run to acquire batches of trajectories from which all agents will observe the chain's state and their individual reward. These samples are utilized to estimate gradients. ", "page_idx": 5}, {"type": "text", "text": "The team agents implement a batch version of the REINFORCE estimator whose definition is deferred to the Appendix C.6.1. As for the estimators that the adversary utilizes, we define the state-action visitation measure estimator and their gradient estimator closely following [113]. ", "page_idx": 5}, {"type": "text", "text": "Definition 2.6 (State-Action Visitation Measure Estimator). Let $e_{s,b}$ bethestandardbasisforthe $(s,b)^{t h}$ entry. Let $\\tau=(s_{0},b_{0},s_{1},b_{1},\\cdot\\cdot\\cdot,s_{H-1},b_{H-1})$ denoteatrajectorywithlength $H$ sampled underinitialdistribution $\\rho$ and policy $\\textit{\\textbf{y}}$ Wedefinetheestimatorfor $\\lambda(y;x)$ with the trajectory $\\tau$ as the following ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\tilde{\\lambda}(\\tau|y):=\\sum_{h=0}^{H-1}\\gamma^{h}\\cdot e_{s_{h},b_{h}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "By applying policy gradient theorem [104] along with the chain-rule, the gradient estimator for a value-function that is nonlinear in $\\lambda(y;x)$ , is computed by the following estimator [113]. ", "page_idx": 5}, {"type": "text", "text": "Definition 2.7 (Gradient Estimator). Let $\\tau=(s_{0},b_{0},s_{1},b_{1},\\cdot\\cdot\\cdot,s_{H-1},b_{H-1})$ denote a trajectory with length $H$ sampled under initial distribution $\\rho$ and policy $\\textit{\\textbf{y}}$ Let $F(\\lambda(y))$ be the value function of the MDP w.r.t. $\\lambda(y)$ and $\\pmb{u}:=\\nabla_{\\lambda}F(\\pmb{\\lambda}(\\pmb{y}))$ .The estimator for gradient $\\nabla_{y}F(\\lambda(y))$ using the sampled trajectory $\\tau$ is defined as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\tilde{g}(\\tau|y;\\boldsymbol{u}):=\\sum_{h=0}^{H-1}\\gamma^{h}\\cdot\\boldsymbol{u}(s_{h},b_{h})\\cdot\\left(\\sum_{h^{\\prime}=0}^{h}\\nabla_{y}\\log y(b_{h^{\\prime}}|s_{h^{\\prime}})\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Sufficient Exploration. A standard, while rather naive, technique of bounding the variance of the REINFORCE gradient estimator is using $\\zeta$ -greedy policy parametrization. Effectively, every action in a player's dispose is played with a probability of at least $\\zeta$ . For our convenience, we ensure sufficient exploration by a $\\zeta$ -truncated simplex approach. Moreover, for a given feasibility set $\\mathcal{X}$ \uff0cwe denote $\\chi\\zeta$ to be the $\\zeta$ -truncated feasibility set. ", "page_idx": 5}, {"type": "text", "text": "3 Main Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We present our main results in two different subsections. In Section 3.1 we manage to attain guarantees for convergence to an approximate stationary-point to constrained nonconvex optimization with an stochastic inexact gradient oracle\u2014 we do so by extending previous results of [35]. While in Section 3.2, we apply the latter results along with RL techniques in order to design the first learning algorithm that computes a Nash equilibrium in ATMGs. ", "page_idx": 5}, {"type": "text", "text": "3.1 Stochastic Weakly-Smooth Nonconvex Optimization with Inexact Gradients ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this subsection we prove that projected gradient descent with a stohcastic inexact gradient oracle convergestoan $\\epsilon$ -FOSP in nonconvex functions with Holder continuous gradients. We will use this key result in subsequent sections. We begin by defining the inexact gradient oracle and its stochastic version. ", "page_idx": 6}, {"type": "text", "text": "Definition 3.1 (Inexact Gradient Oracle). Let a differentiable function $\\phi(z)$ and its gradient $\\nabla\\phi(z)$ We call the vector-valued function $\\ensuremath{\\boldsymbol{g}}(\\ensuremath{\\boldsymbol{z}})$ a $\\vartheta$ -inexact gradient oracle $i f,$ ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\lVert g(z)-\\nabla\\phi(z)\\rVert\\leq\\vartheta,\\quad\\forall z.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Further, given a random variable $\\xi$ in some sample space $\\Xi$ , we define a stochastic inexact gradient oracle $G\\colon\\mathcal{Z}\\times\\Xi\\rightarrow\\mathbb{R}^{d}$ . We assume that the expected value of this oracle will be equal to a $\\vartheta$ -inexact gradient oracle $\\ensuremath{\\boldsymbol{g}}(\\ensuremath{\\boldsymbol{z}})$ . Additionally to being unbiased (with respect to a $\\vartheta$ -inexact gradient oracle), we assume its variance to be bounded. ", "page_idx": 6}, {"type": "text", "text": "Assumption 3.1 (Unbiased and Bounded Variance). For a variance parameter $\\sigma^{2}>0$ , the gradient oracle $G$ ,satisfies ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\xi}[G(z,\\xi)]=g(z)\\quad\\mathrm{and}\\quad\\mathbb{E}_{\\xi}\\left[\\left\\|G(z,\\xi)-g(z)\\right\\|^{2}\\right]\\leq\\sigma^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Following, we consider the simple update rule of Mini-Batch Inexact Stochastic Projected Gradient Descent, with a batch size $M>0$ and $\\begin{array}{r}{\\hat{\\pmb g}^{t}=\\frac{1}{M}\\sum_{j=1}^{M}G\\left(\\pmb z^{t},\\xi_{j}^{t}\\right)}\\end{array}$ ", "page_idx": 6}, {"type": "equation", "text": "$$\nz^{t+1}=\\mathrm{Proj}_{\\mathcal{Z}}\\left(z^{t}-\\eta\\hat{\\pmb{g}}^{t}\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "(Inexact Stoch-PGD) ", "page_idx": 6}, {"type": "text", "text": "We can now state our convergence Theorem for (Inexact Stoch-PGD) whose proof we defer to the appendix. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3.1 (Convergence to $\\epsilon$ -FOSP; Formally in Theorem B.1). Let $\\phi:\\mathcal{Z}\\to\\mathbb{R}$ be a Lipschitz.continuous function with $(\\ell_{p},p)$ -Holder continuous gradient and a desired accuracy e. Also, let a stochastic inexact first-order oracle $G$ satisfying Assumption 3.1. The update rule (Inexact Stoch-PGD), with a step-size $\\eta=\\mathcal{O}\\left(\\epsilon^{\\frac{1-p}{p}}\\right)$ , computes an $\\epsilon$ -approximate stationary point after $T=O\\left(\\epsilon^{-{\\frac{1+p}{p}}}\\right)$ iterations. ", "page_idx": 6}, {"type": "text", "text": "3.2 Learning Nash Equilibria in Adversarial Team Markov Games ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this subsection we state our contributed Algorithm 1, or ISPNG, which converges to an $\\epsilon{-}\\mathrm{NE}$ for any ATMG, $\\Gamma$ , with an iteration and sample complexity that scales polynomially with $1/\\epsilon$ and the parameters of $\\Gamma$ . To simply describe the algorithm, the team players initialize their policies and then the following two steps are repeated for $T$ iterations: ", "page_idx": 6}, {"type": "text", "text": "1. the adversary approximately maximizes a regularized version of their value function, $V_{\\rho}^{\\nu}(x,y):=$ $\\begin{array}{r}{r(x)^{\\top}\\lambda(y;x)-\\frac{\\nu}{2}\\left\\|\\lambda(y;x)\\right\\|^{2}}\\end{array}$ using Algorithm 2, and then 2. every agent independently performs a gradient descent step on the value function. ", "page_idx": 6}, {"type": "text", "text": "During this process, all agents use only bandit feedback information in order to estimate the gradients of the value function. We remark that the learning dynamics remain uncoupled. The only instance of communication between agents is the fact that the team and the adversary take turns when updating their policies. During their turn, the adversary approximately best-responds. ", "page_idx": 6}, {"type": "text", "text": "Of particular interest is the sub-routine of Algorithm 2, V1s-REG-PG. It is effectively a directly parameterized policy gradient method for an objective function that is concave in the state-action visitation measure $\\bar{\\lambda}(\\bar{\\boldsymbol{y}};\\boldsymbol{x})\\,\\in\\,\\mathbb{R}^{|\\boldsymbol{S}||\\boldsymbol{B}|}$ . The objective function is merely the original value function plus a quadratic term, $-\\textstyle{\\frac{\\nu}{2}}\\|\\lambda(y;x)\\|^{2}$ . We remind the reader that due to the existence of this introduced regularizer, the utility of the adversary ${\\pmb u}\\,=\\,\\nabla_{\\lambda({\\pmb y};{\\pmb x})}F_{\\rho}^{\\nu}({\\pmb x},{\\pmb y})\\,=\\,{\\pmb r}({\\pmb x})\\,-\\,\\nu\\lambda({\\pmb y};{\\pmb x})$ In order to estimate a gradient, the adversary needs to collect a number of trajectories, $\\tau=$ $\\left(s_{0},b_{0},s_{1},\\ldots,s_{H-1},b_{H-1},s_{H}\\right)$ , each of length $H$ . Notably, the adversary only uses the empirical state-action visitation measure for the purpose of gradient estimation of the regularized function. ", "page_idx": 6}, {"type": "text", "text": "Algorithm 1 Independent Stochastic Policy-Nested-Gradient (ISPNG) ", "page_idx": 7}, {"type": "text", "text": "Input: Accuracy $\\epsilon>0$   \n1: Based on $\\epsilon$ , set stepsize $\\eta_{x}$ \uff0c $T_{x}$ iterations, batch size $M$ , truncation parameter $\\zeta_{x}$ , and inner-loop   \naccuracy $\\epsilon_{y}>0$ $\\triangleright$ see Theorem C.3   \n2: $\\begin{array}{r}{x_{i}^{(0)}(s,a)=\\frac{1}{|A_{i}|}}\\end{array}$ \uff0c[ $\\forall(s,a)\\in{\\mathcal{S}}\\times{\\mathcal{A}}_{i}$ $\\triangleright$ for all agents $i\\in[n]$   \n3: for $t\\gets1,2,\\ldots\\overset{}{,}T_{x}$ do   \n4: $\\begin{array}{r l}&{\\pmb{y}^{(t)}\\leftarrow\\mathbf{V}\\mathbf{IS-REG-PG}(\\pmb{x}^{(t-1)},\\epsilon_{y})}\\\\ &{\\hat{\\pmb{g}}_{i}^{(t)}\\leftarrow\\mathbf{REINFORCE}\\Big(\\pmb{x}^{(t-1)},\\pmb{y}^{(t)};\\boldsymbol{M}\\Big)}\\\\ &{\\pmb{x}_{i}^{(t)}\\leftarrow\\mathbf{Proj}_{\\pmb{\\chi}_{i}^{\\zeta_{x}}}\\Big(\\pmb{x}_{i}^{(t-1)}-\\eta_{x}\\hat{\\pmb{g}}_{i}^{(t)}\\Big)}\\end{array}$ see Algorithm 2   \n5: $\\triangleright$ for all agents $i\\in[n]$   \n6: $\\triangleright$ for all agents $i\\in[n]$   \n7: end for   \n8: $\\pmb{y}^{(T_{x}+1)}\\leftarrow\\mathbf{V}\\mathrm{IS-REG-PG}(\\pmb{x}^{T_{x}},\\epsilon_{y})$   \n9: a\\*\u2190x(t\\*) > pick the best iterate   \n10: y\\*\u2190 y(t\\*+1) ", "page_idx": 7}, {"type": "text", "text": "Algorithm 2 Visitation-Regularized Policy Gradient Algorithm (V1s-REG-PG) ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Input: An MDP, a joint strategy of the team $\\textbf{\\em x}$ , and a desired accuracy $\\epsilon>0$   \n1: Based on $\\epsilon$ , set batch size $K$ , sample traj. length $H$ , stepsize $\\eta_{y}$ , truncation parameter $\\zeta_{y}$ and   \nregularization coeff. $\\nu$ $\\triangleright$ see Theorem C.3   \n2 $\\begin{array}{r}{y^{(\\overline{{0}})}(s,b)\\gets\\frac{1}{|\\mathcal{B}|}}\\end{array}$ \uff0c $\\forall(s,b)\\in S\\times B$   \n3: for Epoch $t\\dot{\\leftarrow}\\dot{0},1,\\dots,T_{y}$ do   \n4: Independently sample $K$ trajectories, $\\boldsymbol{\\kappa}^{(t)}$ , of length $H$ under policy $\\pmb{y}^{(t)}$   \n5: $\\begin{array}{r l}&{\\dot{\\lambda}^{(t)}\\dot{\\leftarrow}\\displaystyle\\frac{1}{K}\\displaystyle\\sum_{\\tau\\in\\mathcal{K}^{(t)}}\\dot{\\lambda}(\\tau|y^{(t)}),}\\\\ &{u\\gets r(x)-\\nu\\hat{\\lambda}^{(t)}.}\\\\ &{\\dot{g}_{y}^{(t)}\\dot{\\leftarrow}\\displaystyle\\frac{1}{K}\\displaystyle\\sum_{\\tau\\in\\mathcal{K}^{(t)}}\\tilde{g}(\\tau|y^{(t)};u).}\\\\ &{y^{(t+1)}\\gets\\mathrm{Proj}_{y^{\\zeta_{y}}}(y^{(t)}+\\eta_{y}\\hat{g}_{y}^{(t)}).}\\end{array}$   \n6:   \n7: $\\triangleright\\tilde{\\pmb{g}}$ as in Definition 2.7.   \n8:   \n9: end for ", "page_idx": 7}, {"type": "text", "text": "3.3  Analyzing Independent Stochastic Policy-Nested-Gradient ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Algorithm 1, or ISPNG, is an instance of a nested-loop algorithm. As we have already informally stated, ISPNG runs gradient descent on the regularized max function $\\Phi^{\\nu}({\\pmb x})\\ =$ $\\begin{array}{r}{\\operatorname*{max}_{\\lambda\\in\\Lambda(\\boldsymbol{x})}\\left\\{\\boldsymbol{r}(\\boldsymbol{x})^{\\top}\\lambda-\\frac{\\nu}{2}\\left\\|\\boldsymbol{\\lambda}\\right\\|^{2}\\right\\}}\\end{array}$ for some parameter $\\nu$ . This function has Holder-continuous gradient and, as such, the convergence proof is underpinned by Theorem 3.1. Formally we state that: ", "page_idx": 7}, {"type": "text", "text": "Theorem 3.2 (Grad. Contunuity of Reg-Max Function). Let function $\\Phi^{\\nu}(x)$ be the maximum function of theregularizedvaluefunction of anATMG,withregularizationcoefficient $\\nu>0$ .It is thecase that, (i) $\\Phi^{\\nu}$ is differentiable, (ii) $\\nabla_{\\mathbf{x}}\\Phi^{\\nu}$ is $(1/2,\\ell_{1/2})$ -Holder continuous, i.e, ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\|\\nabla_{x}\\Phi^{\\nu}({\\pmb x})-\\nabla_{x}\\Phi^{\\nu}({\\pmb x})\\|\\leq\\ell_{1/2}\\,\\|{\\pmb x}-{\\pmb x}^{\\prime}\\|^{\\frac{1}{2}}}}\\\\ {{=\\displaystyle\\frac{30n^{\\frac{1}{4}}|{\\pmb S}|^{\\frac{5}{4}}\\left(\\sum_{i}|A_{i}|+|B|\\right)^{2}}{\\nu\\operatorname*{min}_{s}\\rho(s)(1-\\gamma)^{\\frac{13}{2}}}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "with ", "page_idx": 7}, {"type": "text", "text": "ISPNG manages to run gradient descent on function $\\Phi^{\\nu}$ though the agents can never observe the exact gradient of $\\Phi^{\\nu}$ . This is not only due to the randomness of gradient estimators but mainly because they cannot observe the adversary's actions and thus do not know the gradient w.r.t the regularizing term. Fortunately, the regularization coefficient plays a second role in bounding the inexactness error of the gradient estimates. For that reason, parameter $\\nu$ admits a careful tuning. ", "page_idx": 7}, {"type": "text", "text": "Finally, the differentiability of $\\Phi^{\\nu}$ and the per-player gradient domination property of the $V_{\\rho}$ implies that an $\\epsilon$ -FOSP $x^{*}$ and the corresponding best-response for the regularized value function, $\\boldsymbol{y}^{*}$ \uff0c constitute an $\\epsilon$ -NE, leading to the main Theorem of this subsection: ", "page_idx": 7}, {"type": "text", "text": "Theorem 3.3 (Main Result; Formally in Theorem C.3). Given a desired accuracy $\\epsilon>0$ Algorithm1 outputsa jointpolicy $(x^{*},y^{*})$ for which it holds that, ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[V_{\\rho}(x^{*},y^{*})-\\operatorname*{min}_{x_{i}^{\\prime}\\in\\mathcal{X}_{i}}V_{\\rho}(x_{i}^{\\prime},x_{-i}^{*},y^{*})\\right]\\leq\\epsilon,\\quad\\forall i\\in[n];\n$$", "text_format": "latex", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\operatorname*{max}_{\\pmb{y}^{\\prime}\\in\\mathcal{Y}}V_{\\rho}(\\pmb{x}^{*},\\pmb{y}^{\\prime})-V_{\\rho}(\\pmb{x}^{*},\\pmb{y}^{*})\\right]\\leq\\epsilon,\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "with $a$ number ofiterations and $a$ numberof samples that are poly (,n,[S],\u2265|Ai| +[B],Dmmnp()) By $D_{\\mathrm{m}}$ we denote the mismatch coeffcient $\\begin{array}{r}{D_{\\mathrm{m}}:=\\left\\|\\frac{d_{\\rho}^{x,y}}{\\rho}\\right\\|_{\\infty}}\\end{array}$ (Definition C.2). ", "page_idx": 8}, {"type": "text", "text": "4   Minimax in Nonconvex-Hidden-Strongly-Concave Functions ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Finally, we would state a more general result compared to that of Theorem 3.3. We consider the general min-max nonconvex-nonconcave optimization problem, $\\mathrm{min}_{\\mathbf{x}\\in\\mathcal{X}}\\,\\mathrm{max}_{\\mathbf{y}\\in\\mathcal{Y}}\\,f(\\mathbf{x},\\pmb{y})$ ,when an additional structural assumption holds, i.e., when $f$ is nonconvex-hidden-strongly-concave. In particular, function $f$ admits a reformulation of the form, ", "page_idx": 8}, {"type": "equation", "text": "$$\nH(x,\\pmb{u}):=f\\left(\\pmb{x},c^{-1}(\\pmb{u};\\pmb{x})\\right),\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where function $H$ is a nonconvex-strongly-concave function defined on $\\mathcal{X}\\times\\mathcal{U}$ . The sets $\\mathcal{X}$ and $\\boldsymbol{\\mathcal{U}}$ are closed and convex, while $c(\\cdot;{\\pmb x}):{\\boldsymbol y}\\,\\bar{\\to}\\,{\\boldsymbol u}$ is an invertible mapping parametrized by $\\textbf{\\em x}$ . Moreover, we will denote $\\mathcal{U}(\\pmb{x}):=\\{\\pmb{u}|\\pmb{u}=c(\\pmb{y};\\pmb{x})$ \uff0c $\\forall\\pmb{y}\\in\\mathcal{V}\\}$ . We further assume that the mapping $c$ and its inverse are Lipschitz-continuous. Specifically, ", "page_idx": 8}, {"type": "text", "text": "Assumption 4.1. For the mapping $c$ and its inverse, $c^{-1}$ , it holds that ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\|c(\\pmb{y};\\pmb{x})-c(\\pmb{y}^{\\prime};\\pmb{x}^{\\prime})\\|\\leq L_{c}(\\|\\pmb{x}-\\pmb{x}^{\\prime}\\|+\\|\\pmb{y}-\\pmb{y}^{\\prime}\\|),\\quad\\forall\\pmb{x},\\pmb{x}^{\\prime}\\in\\mathcal{X};\\pmb{y},\\pmb{y}^{\\prime}\\in\\mathcal{Y}}\\\\ &{}&{\\|c^{-1}(\\pmb{u};\\pmb{x})-c^{-1}(\\pmb{u}^{\\prime};\\pmb{x}^{\\prime})\\|\\leq L_{c^{-1}}(\\|\\pmb{x}-\\pmb{x}^{\\prime}\\|+\\|\\pmb{u}-\\pmb{u}^{\\prime}\\|),\\forall\\pmb{x},\\pmb{x}^{\\prime}\\in\\mathcal{X};\\pmb{u},\\pmb{u}^{\\prime}\\in\\mathcal{U}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "If this is the case, the maximizer $\\pmb{u}^{\\star}(\\pmb{x}):=\\operatorname*{argmax}_{\\pmb{u}\\in\\mathcal{U}(\\pmb{x})}H(\\pmb{x},\\pmb{u})$ , is Holder continuous w.r.t. $\\textbf{\\em x}$ as stated by the following Theorem. ", "page_idx": 8}, {"type": "text", "text": "Theorem 4.1 (Formally in Theorem D.2). Let function $f({\\boldsymbol{x}},{\\boldsymbol{y}})$ be nonconvex-hidden-stronglyconcave with a modulus of $\\nu\\,>\\,0$ \uff1aLet also function $H$ bea $L_{H}$ -Lipschitz continuous and $\\ell_{H}$ smooth nonconvex-strongly-concave reformulation of $f$ with an invertible mapping c for which Assumption 4.1 holds. Then, ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\|u^{\\star}({\\boldsymbol{\\mathbf{\\alpha}}})-u^{\\star}({\\boldsymbol{\\mathbf{\\alpha}}}^{\\prime})\\|\\leq L_{\\star}\\left\\|{\\boldsymbol{\\mathbf{\\alpha}}}\\right-{\\boldsymbol{\\mathbf{\\alpha}}}\\boldsymbol{x}^{\\prime}\\right\\|^{\\frac{1}{2}},\\quad\\forall{\\boldsymbol{\\mathbf{\\alpha}}},{\\boldsymbol{\\mathbf{\\alpha}}}^{\\prime}\\in\\mathcal{X}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where, $\\begin{array}{r}{L_{\\star}=O\\left(\\frac{1}{\\nu}\\right)}\\end{array}$ ", "page_idx": 8}, {"type": "text", "text": "Theorem 4.2 (Convergence to an $\\epsilon$ -SP; Formally in Theorem D.3). Let $f$ be a nonconvex-hiddenstrongly-concave function obeying to the same assumptions as $f$ in Theorem 4.1 and $\\epsilon>0$ Further assume a maximization oracle with $O(\\nu\\epsilon^{2})$ -accuracy.There exists an algorithm that computes an $\\epsilon$ aproximate saddle-point $(\\boldsymbol{x}^{*},\\boldsymbol{y}^{*})$ by making $\\begin{array}{r}{T=\\overset{\\cdot}{\\underset{\\cdot}{O}}\\left(\\frac{1}{\\nu^{2}\\epsilon^{3}}\\right)}\\end{array}$ calls to the maximization oracle. Also, the maximization oracle can be implemented by stochastic gradient ascent with iteration complexity $\\begin{array}{r}{T^{\\prime}=\\tilde{O}\\left(\\frac{1}{\\nu^{3}\\epsilon^{2}}\\right)}\\end{array}$ , and stepsize $\\eta_{y}=O(\\nu^{2}\\epsilon^{2})$ ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion, Future Work, and Limitations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Conclusions _  We expanded stochastic gradient techniques to be able to compute a stationary point in constrained optimization of nonconvex with weakly-smooth functions. We applied that result to design the first learning algorithm that computes an $\\epsilon$ -approximate Nash equilibrium in adversarial team Markov games using a finite number of samples and iterations that scale polynomially with $1/\\epsilon$ and the natural parameters of the game. ", "page_idx": 8}, {"type": "text", "text": "Future Work  We believe that some questions that require further investigation are the following: (i) Is it possible to extend the techniques of [34] to establish convergence guarantees of stochastic gradient descent on nonconvex functions with Holder-continuous gradient without batch-sampling of the gradient? (ii) Can we design a two-timescale gradient descent-ascent scheme for ATMGs that converges to a Nash equilibrium with best-iterate guarantees? (ii) Can we utilize some variancereduction techniques to achieve a better sample complexity for learning an $\\epsilon$ NEinATMGs? ", "page_idx": 9}, {"type": "text", "text": "Limitations  The main limitations of our work are (i) the notion of independent learning as presented is weaker than the one presented in $[27]-i.e.$ , our algorithm has an \u201cinner loop\", (i) the fact that we did not present an example for which the function $\\Phi^{\\nu}$ fails to be smooth; hence, it is unclear if we can prove the smoothness of this function and achieve tighter analysis. The first item can be addressed in future work by developing a two-timescale algorithmic approach. As for the second item, we remark that even if it is the case that $\\Phi^{\\nu}$ is smooth for ATMGs, our provided convergence rates would be straightforwardly improved without any qualitative modification of the algorithm. Also, we would like to highlight that this discussion is related to Remark 2. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work has been partially supported by project MIS 5154714 of the National Recovery and Resilience Plan Greece 2.0 funded by the European Union under the NextGenerationEU Program. FK carried over part of the research during an Archimedes Research Internship. IP would like to acknowledge an ICS research award and a startup grant from UCI. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] P. Abbeel and A. Y. Ng. Apprenticeship learning via inverse reinforcement learning. In Proceedings of the twenty-first international conference on Machine learning, page 1, 2004.   \n[2] J. Achiam, D. Held, A. Tamar, and P. Abbeel. Constrained policy optimization. In International conference on machine learning, pages 22-31. PMLR, 2017.   \n[3] A. Agarwal, S. M. Kakade, J. D. Lee, and G. Mahajan. On the theory of policy gradient methods: Optimality, approximation, and distribution shift. Journal of Machine Learning Research, 22(98):1-76, 2021. [4] E. Altman. Constrained Markov decision processes. Routledge, 2021.   \n[5] Q. Bai, A. S. Bedi, M. Agarwal, A. Koppel, and V. Aggarwal. Achieving zero constraint violation for constrained reinforcement learning via primal-dual approach. In Proceedings of the AAA1 Conference on Artificial Intelligence, volume 36, pages 3682-3689, 2022.   \n[6] A. Bakhtin, N. Brown, E. Dinan, G. Farina, C. Flaherty, D. Fried, A. Goff, J. Gray, H. Hu, et al. Human-level play in the game of diplomacy by combining language models with strategic reasoning. Science, 378(6624):1067-1074, 2022.   \n[7] N. Basilico, A. Celli, G. D. Nittis, and N. Gatti Team-maxmin equilibrium: Effciency bounds and algorithms. In S. Singh and S. Markovitch, editors, Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, February 4-9, 2017, pages 356-362. AAAI Press, 2017.   \n[8]  P. Bernhard and A. Rapaport. On a theorem of danskin with an application to a theorem of von neumann-sion. Nonlinear Analysis: Theory, Methods & Applications, 24(8):1163-1181, 1995.   \n[9] L. Bisi, L. Sabbioni, E. Vittori, M. Papini, and M. Restelli Risk-averse trust region optimizatin for reward-volatility reduction. arXiv preprint arXiv: 1912.03193, 2019.   \n[10] J. Bolte, T. P. Nguyen, J. Peypouquet, and B. Suter. From error bounds to the complexity of first-order descent methods for convex functions, 2016.   \n[11] C. Borgs, J. T. Chayes, N. Immorlica, A. T. Kalai, V. S. Mirrokni, and C. H. Papadimitriou. The myth of the folk theorem. Games Econ. Behav., 70(1):34 43, 2010.   \n[12] M. Bowling, N. Burch, M. Johanson, and O. Tammelin. Heads-up limit hold'em poker is solved. Science, 347(6218):145-149, 2015.   \n[13] K. Brantley, M. Dudik, T. Lykouris, S. Miryoosefi, M. Simchowitz, A. Slivkins, and W. Sun. Constrained episodic reinforcement learning in concave-convex and knapsack settings. Advances in Neural Information Processing Systems, 33:16315-16326, 2020.   \n[14] N. Brown, A. Bakhtin, A. Lerer, and Q. Gong. Combining deep reinforcement learning and search for imperfect-information games. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, 2020.   \n[15]  N. Brown and T. Sandholm. Superhuman ai for heads-up no-limit poker: Libratus beats top professionals. Science, 359(6374):418-424, 2018.   \n[16]  N. Brown and T. Sandholm. Superhuman ai for multiplayer poker. Science, 365(6456):885- 890, 2019.   \n[17]  Y. Cai, H. Luo, C.-Y. Wei, and W. Zheng. Uncoupled and convergent learning in two-player zero-sum markov games. In ICML 2023 Workshop The Many Facets of Preference-Based Learning, 2023.   \n[18]  Y. Cai, A. Oikonomou, and W. Zheng. Finite-time last-iterate convergence for learning in multi-player games. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022.   \n[19] V. Campos, A. Trott, C. Xiong, R. Socher, X. Giro-i Nieto, and J. Torres. Explore, discover and learn: Unsupervised discovery of state-covering skills. In International Conference on Machine Learning, pages 1317-1327. PMLR, 2020.   \n[20] L. Carminati, F. Cacciamani, M. Ciccone, and N. Gati. A marriage between adversarial team games and 2-player games: Enabling abstractions, no-regret learning, and subgame solving. In International Conference on Machine Learning,ICML 2022, volume 162 of Proceedings of Machine Learning Research, pages 2638-2657. PMLR, 2022.   \n[21]  A. Celli and N. Gatti. Computational results for extensive-form adversarial team games. In Proceedings of the AAAl Conference on Artificial Intelligence, volume 32, 2018.   \n[22] S. Cen, Y. Wei, and Y. Chi. Fast policy extragradient methods for competitive games with entropy regularization. In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, pages 27952- 27964, 2021.   \n[23] D. Chen, Q. Zhang, and T. T. Doan. Convergence and price of anarchy guarantees of the softmax policy gradient in markov potential games. In Decision Awareness in Reinforcement Learning Workshop at ICML 2022, 2022.   \n[24] X. Chen, X. Deng, and S. Teng. Settling the complexity of computing two-player nash equilibria. J. ACM, 56(3):14:1-14:57, 2009.   \n[25]  Y. Chow, A. Tamar, S. Mannor, and M. Pavone. Risk-sensitive and robust decision-making: a cvar optimization approach. Advances in neural information processing systems, 28, 2015.   \n[26] F. C. Chu and J. Y. Halpern. On the np-completeness of finding an optimal srategy in games with common payoffs. Int. J. Game Theory, 30(1):99-106, 2001.   \n[27] C. Daskalakis, D. J. Foster, and N. Golowich. Independent policy gradient methods for competitive reinforcement learning. Advances in neural information processing systems, 33:5527-5540, 2020.   \n[28]  C. Daskalakis, P. W. Goldberg, and C. H. Papadimitriou. The complexity of computing a nash equilibrium. SIAM J. Comput., 39(1):195-259, 2009.   \n[29]  C. Daskalakis, N. Golowich, and K. Zhang._ The complexity of markov equilibrium in stochastic games. CoRR, abs/2204.03991, 2022.   \n[30] C. Daskalakis, A. Ilyas, V. Syrgkanis, and H. Zeng. Training gans with optimism. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018.   \n[31] C. Daskalakis and I. Panageas. The limit points of (optimistic) gradient descent in min-max optimization. In S. Bengio, H. M. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montreal, Canada, pages 9256-9266, 2018.   \n[32] C. Daskalakis and I. Panageas. Last-iterate convergence: Zero-sum games and constrained min-max optimization. In A. Blum, editor, 1oth Innovations in Theoretical Computer Science Conference, ITCS 2019, January 10-12, 2019, San Diego, California, USA, volume 124 of LIPIcs, pages 27:1-27:18. Schloss Dagstuhl - Leibniz-Zentrum fir Informatik, 2019.   \n[33]  C. Daskalakis, S. Skoulakis, and M. Zampetakis. The complexity of constrained min-max optimization. In S. Khuller and V. V. Williams, editors, STOC '21: 53rd Annual ACM SIGACT Symposium on Theory of Computing, Virtual Event, Italy, June 21-25, 2021, pages 1466-1478. ACM, 2021.   \n[34] D. Davis and D. Drusvyatskiy. Stochastic model-based minimization of weakly convex functions. SIAM Journal on Optimization, 29(1):207-239, 2019.   \n[35]  O. Devolder, F. Glineur, and Y. Nesterov. First-order methods of smooth convex optimization with inexact oracle. Mathematical Programming, 146:37-75, 2014.   \n[36] J. Diakonikolas, C. Daskalakis, and M. I. Jordan. Efficient methods for structured nonconvexnonconcave min-max optimization, 2021.   \n[37] D. Ding, C.-Y. Wei, K. Zhang, and M. R. Jovanovic. Independent policy gradient for largesale markov potential games: Sharper rates, function approximation, and game-agnostic convergence. arXiv preprint arXiv:2202.04129, 2022.   \n[38] D. Drusvyatskiy and A. S. Lewis. Error bounds, quadratic growth, and linear convergence of proximal methods. Mathematics of Operations Research, 43(3):919-948, 2018.   \n[39] S. Emmons, C. Oesterheld, A. Critch, V. Conitzer, and S. Russell. For learning in symmetric teams, local optima are global nash equilibria. In International Conference on Machine Learning, ICML 2022, volume 162 of Proceedings of Machine Learning Research, pages 5924-5943. PMLR, 2022.   \n[40] L. Erez, T. Lancewicki, U. Sherman, T. Koren, and Y. Mansour. Regret minimization and convergence to equilibria in general-sum markov games. In International Conference on Machine Learning, pages 9343-9373. PMLR, 2023.   \n[41] B. Eysenbach, A. Gupta, J. Ibarz, and S. Levine. Diversity is all you need: Learning skills without a reward function. arXiv preprint arXiv: 1802.06070, 2018.   \n[42] G. Farina, A. Celli, N. Gatti, and T. Sandholm. Ex ante coordination and collusion in zero-sum multi-player extensive-form games. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, pages 9661-9671, 2018.   \n[43] I. Fatkhullin, N. He, and Y. Hu. Stochastic optimization under hidden convexity, 2023.   \n[44] T. Fiez, L. Ratliff, E. Mazumdar, E. Faulkner, and A. Narang. Global convergence to local minmax equilibrium in classes of nonconvex zero-sum games. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 29049-29063. Curran Associates, Inc., 2021.   \n[45]  L. Flokas, E.-V. Vlatakis-Gkaragkounis, and G. Piliouras. Solving min-max optimization with hidden structure via gradient descent ascent, 2021.   \n[46]  R. Fox, S. M. McAleer, W. Overman, and I. Panageas. Independent natural policy gradient always converges in markov potential games. In International Conference on Artificial Intelligence and Statistics, AISTATS 2022, volume 151 of Proceedings of Machine Learning Research, pages 4414-4425. PMLR, 2022.   \n[47] J. Garcia and F. Fernandez. A comprehensive survey on safe reinforcement learning. Journal of Machine Learning Research, 16(1):1437-1480, 2015.   \n[48] U. Ghai, Z. Lu, and E. Hazan. Non-convex online learning via algorithmic equivalence. Advances in Neural Information Processing Systems, 35:22161-22172, 2022.   \n[49] 1. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014.   \n[50] I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.   \n[51] E. Gorbunov, A. B. Taylor, and G. Gidel. Last-iterate convergence of optimistic gradient method for monotone variational inequalities. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022.   \n[3Z] N. Gregor, Dzso Kezenue, and D. wiersura. valauonal intrinsic conurol. arAiv preprinl arXiv:1611.07507, 2016.   \n[53]  T. Groves. Incentives in teams. Econometrica, 41(4):617-631, 1973.   \n[54] K. A. Hansen, T. D. Hansen, P. B. Miltersen, and T. B. Sorensen. Approximability and parameterized complexity of minmax values. In International Workshop on Internet and Network Economics, pages 684-695. Springer, 2008.   \n[55] S. Hansen, W. Dabney, A. Barreto, T. Van de Wiele, D. Warde-Farley, and V. Mnih. Fast task inference with variational intrinsic successor features. arXiv preprint arXiv: 1906.05030, 2019.   \n[56]  S. Hart and A. Mas-Colell. Uncoupled dynamics do not lead to nash equilibrium. American Economic Review, 93(5):1830-1836, 2003.   \n[57] E. Hazan, S. Kakade, K. Singh, and A. Van Soest. Provably efficient maximum entropy exploration. In International Conference on Machine Learning, pages 2681-2691. PMLR, 2019.   \n[58] S. He, Y. Jiang, H. Zhang, J. Shao, and X. Ji. Wasserstein unsupervised reinforcment leaning. In Proceedings of the AAA1 Conference on Artificial Intelligence, volume 36, pages 6884-6892, 2022.   \n[59]  Y-C. Ho and K.-C. Chu. Team decision theory and information structures in optimal control problems-part i. IEEE Transactions on Automatic Control, 17(1):15-22, 1972.   \n[60] J. Hu and M. P. Wellman. Multiagent reinforcement learning: Theoretical framework and an algorithm. In Proceedings of the Fifteenth International Conference on Machine Learning, ICML '98, page 242-250, San Francisco, CA, USA, 1998. Morgan Kaufmann Publishers Inc.   \n[61] J. Hu and M. P. Welman. Nash q-learning for general-sum stochastic games. J. Mach. Learn. Res., 4:1039-1069, 2003.   \n[62]  C. Jin, Q. Liu, Y. Wang, and T. Yu. V-learning-a simple, efficient, decentralized algorithm for multiagent rl. arXiv preprint arXiv:2110.14555, 2021.   \n[63]  C. Jin, P. Netrapalli, and M. Jordan. What is local optimality in nonconvex-nonconcave minimax optimization? In International Conference on Machine Learning, pages 4880-4889. PMLR, 2020.   \n[64]  Y. Jin, V. Muthukumar, and A. Sidford. The complexity of infinite-horizon general-sum stochastic games. arXiv preprint arXiv:2204.04186, 2022.   \n[65]  F. Kalogiannis, I. Anagnostides, I. Panageas, E.-V. Vlatakis-Gkaragkounis, V. Chatziafratis, and S. A. Stavroulakis. Efficiently computing nash equilibria in adversarial team markov games. In The Eleventh International Conference on Learning Representations, 2023.   \n[66]  F. Kalogiannis, E.-V. Vlatakis-Gkaragkounis, and I. Panageas. Teamwork makes von neumann work: Min-max optimization in two-team zero-sum games. ICLR, 2023.   \n[67]  H. Karimi, J. Nutini, and M. Schmidt. Linear convergence of gradient and proximal-gradient methods under the polyak-lojasiewicz condition. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2016, Riva del Garda, Italy, September 19-23, 2016, Proceedings, Part I 16, pages 795-811. Springer, 2016.   \n[68] S. Leonardos, W. Overman, I. Panageas, and G. Piliouras. Global convergence of multi-agent policy gradient in markov potential games. arXiv preprint arXiv:2106.01969, 2021.   \n[69]  Q. Lin, Z. Lu, and L. Xiao. An accelerated proximal coordinate gradient method. Advances in Neural Information Processing Systems, 27, 2014.   \n[70]  T. Lin, C. Jin, and M. Jordan. On gradient descent ascent for nonconvex-concave minimax problems. In International Conference on Machine Learning, pages 6083-6093. PMLR, 2020.   \n[71]  M. L. Littman. Markov games as a framework for multi-agent reinforcement learning. In Machine learning proceedings 1994, pages 157-163. Elsevier, 1994.   \n[72] H. Liu and P. Abbeel. Aps: Active pretraining with successor features. In International Conference on Machine Learning, pages 6736-6747. PMLR, 2021.   \n[73]  A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv: 1706.06083, 2017.   \n[74] C. Maheshwari, M. Wu, D. Pai, and S. Sastry. Independent and decentralized learning in markov potential games, 2022.   \n[75] J. Marschak. Elements for a theory of teams. Management Science, 1(2):127-137, 1955.   \n[76]  P. Mertikopoulos, B. Lecouat, H. Zenati, C. Foo, V. Chandrasekhar, and G. Piliouras. Optimistic mirror descent in saddle-point problems: Going the extra (gradient) mile. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.   \n[77]  A. Mladenovic, I. Sakos, G. Gidel, and G. Piliouras. Generalized natural gradient flows in hidden convex-concave games and gans. In International Conference on Learning Representations, 2021.   \n[78]  A. Mokhtari, A. E. Ozdaglar, and S. Pattathil. A unified analysis of extra-gradient and optimistic gradient methods for saddle point problems: Proximal point approach. In S. Chiappa and R. Calandra, editors, The 23rd International Conference on Artificial Inteligence and Statistics, AISTATS 2020, 26-28 August 2020, Online [Palermo, Sicily, Italy], volume 108 of Proceedings of Machine Learning Research, pages 1497-1507. PMLR, 2020.   \n[79] M. Moravcik, M. Schmid, N. Burch, V. Lisy, D. Morrill, N. Bard, T. Davis, K. Waugh, M. Johanson, and M. Bowling. Deepstack: Expert-level artificial intelligence in heads-up no-limit poker. Science, 356(6337):508-513, 2017.   \n[80]  M. Mutti, R. De Santi, P. De Bartolomeis, and M. Restelli. Challenging common assumptions in convex reinforcement learning. Advances in Neural Information Processing Systems, 35:4489-4502, 2022.   \n[81]  A. Nemirovski. Prox-method with rate of convergence o(1/t) for variational inequalities with lipschitz continuous monotone operators and smooth convex-concave saddle point problems. SIAM Journal on Optimization, 15(1):229-251, 2004.   \n[82]  Y. Nesterov. Universal gradient methods for convex optimization problems. Mathematical Programming, 152(1):381-404, 2015.   \n[83]  M. Nouiehed, M. Sanjabi, T. Huang, J. D. Lee, and M. Razaviyayn. Solving a class of nonconvex min-max games using iterative first order methods. Advances in Neural Information Processing Systems, 32, 2019.   \n[84]  K. A. Oikonomidis, E. Laude, P. Latafat, A. Themelis, and P. Patrinos. Adaptive proximal gradient methods are universal without approximation. arXiv preprint arXiv:2402.06271, 2024.   \n[85]  F. Orabona. Normalized gradients for all. arXiv preprint arXiv:2308.05621, 2023.   \n[86]  J. Perolat, B. de Vylder, D. Hennes, E. Tarassov, F. Strub, V. de Boer, P. Muller, J. T. Connor, N. Burch, T. Anthony, S. McAleer, R. Elie, S. H. Cen, Z. Wang, A. Gruslys, A. Malysheva, M. Khan, S. Ozair, F. Timbers, T. Pohlen, T. Eccles, M. Rowland, M. Lanctot, J-B. Lespiau, B. Piot, S. Omidshafiei, E. Lockhart, L. Sifre, N. Beauguerlange, R. Munos, D. Silver, S. Singh, D. Hassabis, and K. Tuyls. Mastering the game of stratego with model-free multiagent reinforcement learning, 2022.   \n[87]  M. Piccione and A. Rubinstein. On the interpretation of decision problems with imperfect recall. Games and Economic Behavior, 20(1):3-24, 1997.   \n[88]  M. L. Puterman. Markov decision processes: discrete stochastic dynamic programming. John Wiley & Sons, 2014.   \n[89]  R. Radner. Team Decision Problems. The Annals of Mathematical Statistics, 33(3):857 - 881, 1962.   \n[90] I. Sakos, E.-V. Vlatakis-Gkaragkounis, P. Mertikopoulos, and G. Piliouras. Exploiting hidden structures in non-convex games for convergence to nash equilibrium. Advances in Neural Information Processing Systems, 36, 2024.   \n[91]  M. Sayin, K. Zhang, D. Leslie, T. Basar, and A. Ozdaglar. Decentralized q-learning in zerosum markov games. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 18320-18334. Curran Associates, Inc., 2021.   \n[92] M. O. Sayin, F. Parise, and A. Ozdaglar. Fictitious play in zero-sum stochastic games. arXiv preprint arXiv:2010.04223, 2020.   \n[93] J. Schulman, S. Levine, P. Abbeel, M. I. Jordan, and P. Moritz. Trust region policy optimization. In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, volume 37 of JMLR Workshop and Conference Proceedings, pages 1889-1897. JMLR.org, 2015.   \n[94] L. Schulman and U. V. Vazirani. The duality gap for two-team zero-sum games. In 8th Innovations in Theoretical Computer Science Conference (ITCS 2017). Schloss DagstuhlLeibniz-Zentrum fuer Informatik, 2017.   \n[95]  L.S.Shapley. Stochastic games. Proceedings of the national academy of sciences, 39(10):1095- 1100, 1953.   \n[96]  A. Sharma, S. Gu, S. Levine, V. Kumar, and K. Hausman. Dynamics-aware unsupervised discovery of skill. arXiv preprint arXiv: 1907.01657, 2019.   \n[97] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, Y Chen, T. P Lillcrap, F. Hui, L. Sifre, G. van den Driessh, T. Graepel, and D. Hassabis. Mastering the game of go without human knowledge. Nat., 550(7676):354-359, 2017.   \n[98] B. V. Stengel and D. Koller. Team-maxmin equilibria. Games and Economic Behavior, 21(1-2):309-321, 1997.   \n[99]  A. Tamar, Y. Chow, M. Ghavamzadeh, and S. Mannor. Policy gradient for coherent risk measures. Advances in neural information processing systems, 28, 2015.   \n[100] A. Tamar and S. Mannor. Variance adjusted actor critic algorithms. arXiv preprint arXiv:1310.3697, 2013.   \n[101] O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik, J. Chung, D. H. Choi, R. Powell, T. Ewalds, P. Georgiev, J. Oh, D. Horgan, M. Krois, I. Danihelka, A. Huang, L. Sifre, T. Cai, J. P. Agapiou, M. Jaderberg, A. S. Vezhnevets, R. Leblond, T. Pohlen, V. Dalibard, D. Budden, Y. Sulsky, J. Molloy, T. L. Paine, C. Gulcehre, Z. Wang, T. Pfaff, Y. Wu, R.Ring, D. Yogatama, D.Wnch, K.McKiny, O. Smith, T. chaul, T. P Lillra, K. Kavukcuoglu, D. Hassabis, C. Apps, and D. Silver. Grandmaster level in starcraft I using multi-agent reinforcement learning. Nat., 575(7782):350-354, 2019.   \n[102] J. von Neumann and O. Morgenstern. Theory of Games and Economic Behavior (60th Anniversary Commemorative Edition). Princeton University Press, 2007.   \n[103] C.-Y. Wei, C.-W. Lee, M. Zhang, and H. Luo. Last-iterate convergence of decentralized optimistic gradient descent/ascent in infinite-horizon competitive markov games. In M. Belkin and S. Kpotufe, editors, Proceedings of Thirty Fourth Conference on Learning Theory, volume 134 of Proceedings of Machine Learning Research, pages 4259-4299. PMLR, 15-19 Aug 2021.   \n[104]  R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8:229-256, 1992.   \n[105] J. Yang, N. Kiyavash, and N. He. Global convergence and variance-reduced optimization for a class of nonconvex-nonconcave minimax problems. arXiv preprint arXiv:2002.09621, 2020.   \n[106]  M. Yashtini. On the global convergence rate of the gradient descent method for functions with holder continuous gradients. Optimization letters, 10:1361-1370, 2016.   \n[107] T. Yu, Y. Tian, J. Zhang, and S. Sra. Provably efficient algorithms for multi-objective competitive rl. In International Conference on Machine Learning, pages 12167-12176. PMLR, 2021.   \n[108] T. Zahavy, B. O'Donoghue, G. Desjardins, and S. Singh. Reward is enough for convex mdps. Advances in Neural Information Processing Systems, 34:25746-25759, 2021.   \n[109] T. Zahavy, Y. Schroecker, F. Behbahani, K. Baumli, S. Flennerhag, S. Hou, and S. Singh. Discovering policies with domino: Diversity optimization maintaining near optimality. arXiv preprint arXiv:2205.13521, 2022.   \n[110] B. H. Zhang, G. Farina, and T. Sandholm. Team belief DAG form: A concise representation for team-correlated game-theoretic decision making. CoRR, abs/2202.00789, 2022.   \n[111] B. H. Zhang and T. Sandholm. Team correlated equilibria in zero-sum extensive-form games via tree dec0mpositions. CoRR, abs/2109.05284, 2021.   \n[112] J. Zhang, A. Koppel, A. S. Bedi, C. Szepesvari, and M. Wang. Variational policy gradient method for reinforcement learning with general utilities. Advances in Neural Information Processing Systems, 33:4572-4583, 2020.   \n[113] J. Zhang, C. Ni, C. Szepesvari, M. Wang, et al. On the convergence and sample efficiency of variance-reduced policy gradient method. Advances in Neural Information Processing Systems, 34:2228-2240, 2021.   \n[114] R. Zhang, Z. Ren, and N. Li. Gradient play in stochastic games: stationary points, convergence, and sample complexity. arXiv preprint arXiv:2106.00198, 2021.   \n[115] S. Zhang, B. Liu, and S. Whiteson. Mean-variance policy iteration for risk-averse reinforcement learning. In Proceedings of the AAAl Conference on Artificial Intelligence, volume 35, pages 10905-10913, 2021.   \n[116] Y. Zhang and B. An. Computing team-maxmin equilibria in zero-sum multiplayer extensiveform games. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, pages 2318-2325. AAAI Press, 2020.   \n[117]  Y. Zhang and B. An. Converging to team-maxmin equilibria in zero-sum multiplayer games. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, volume 119 of Proceedings of Machine Learning Research, pages 11033-11043. PMLR, 2020. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Table of Contents ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Further Related Work 18   \nA.1 Team Games . 18   \nA.2  Reinforcement Learning . . - 19   \nA.3 Optimization .. 19 ", "page_idx": 17}, {"type": "text", "text": "B Nonconvex Weakly-Smooth Constrained Optimization 20 ", "page_idx": 17}, {"type": "text", "text": "B.1 Auxiliary Lemmas . : 20   \nB.2StochasticPGDwithInexact Gradients 21 ", "page_idx": 17}, {"type": "text", "text": "C Adversarial Team Markov Games 25 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "C.1 Further Background on Markov Decision Processes 27   \nC.2 Auxiliary Lemmas .... 30   \nC.3 Continuity of the maximizers . . 31   \nC.4 Analysis of ISPNG: Proof of Theorem 3.3 34   \nC.5 Visitation-Regularized Policy Gradient Analysis 37   \nC.6 Regarding the Gradient and Visitation Estimators . . 42 ", "page_idx": 17}, {"type": "text", "text": "D  Nonconvex-Hidden-Strongly-Concave Optimization 51 ", "page_idx": 17}, {"type": "text", "text": "A Further Related Work ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We accommodate this section to mention a brief collection of related literature in the fields of team games, reinforcement learning, and optimization. The literature is vast and we can only manage to mention some representative works. ", "page_idx": 17}, {"type": "text", "text": "A.1 Team Games ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Research on team games has been a major focus in economic and group decision theory for decades [75, 53, 89, 59]. A key modern reference is [98], which introduced the team-maxmin equilibrium (TME) for normal-form games, where the team's strategy maximizes their minimal expected payoff against any adversary response. Despite their optimality, TMEs are computationally intractable even for 3-player team games [54, 11]. Recently, practical algorithms have been developed for multiplayer games [117, 116, 7]. Team equilibria are also relevant to two-player zero-sum games with imperfect recall [87]. ", "page_idx": 17}, {"type": "text", "text": "Due to TME's intractability, TMECor, a relaxed equilibrium concept involving a correlation device, has been studied [42, 21, 7, 117, 111, 110, 20]. TMECor permits correlated strategies but can be impractical in certain scenarios [98]. TMECor is also NP-hard for imperfect-information extensiveform games (EFGs) [26], although fixed-parameter-tractable (FPT) algorithms have been developed for specific EFG classes [111, 110]. ", "page_idx": 17}, {"type": "text", "text": "The computational aspects of standard Nash equilibrium (NE) in adversarial team games are not well-understood, even in normal-form games. Von Neumann's minimax theorem [102] does not apply to team games, rendering traditional methods ineffective. [94] characterized the duality gap between teams, while in [66] it was shown that standard no-regret learning dynamics, such as gradient descent and optimistic Hedge, may fail to converge to mixed NE in binary-action adversarial team games. ", "page_idx": 17}, {"type": "text", "text": "A.2  Reinforcement Learning ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Multiagent RL _ Nash equilibrium computation has been central in multiagent RL. Notable algorithms, such as Nash-Q [60, 61], guarantee convergence to Nash equilibria only under strict game conditions. The behavior of independent policy gradient methods [93] remains poorly understood. The impossibility result by the authors of [56] precludes universal convergence to Nash equilibria even in normal-form games, aligning with the computational intractability (PPAD-completeness) of Nash equilibria in two-player general-sum games [28, 24]. Surprisingly, recent work shows similar hardness in turn-based stochastic games, making (stationary) CCEs intractable [29, 64]. ", "page_idx": 18}, {"type": "text", "text": "Thus, research has focused on specific game classes, like Markov potential games [68, 37, 114, 23, 74, 46] or two-player zero-sum Markov games [27, 103, 91, 22, 92]. As noted, adversarial Markov team games can unify and extend these settings. Identifying multi-agent settings where Nash equilibria are efficiently computable is a key open problem (see, e.g., [27]). Recent guarantees for convergence to Nash equilibria have been found in symmetric games, including symmetric team games [39]. Additionally, weaker solution concepts, relaxing either Markovian or stationarity properties, have gained attention [29, 62]. ", "page_idx": 18}, {"type": "text", "text": "Convex RL Maximizing a value function regularized by a term that is strongly-concave with respect to the state-action visitation measure is an instance of a convex RL problem. In that sense, our work is also related to that strain of literature. Convex RL [108, 112] is a framework that generalizes standard MDP problems by considering the optimization of an objective function that is convex (or concave) in the state (or state-action) visitation measures that the agent's policies induce. The value function of standard RL has an objective function linear to that measure. Common well-known problems that are unified below the lens of convex are (i) \u201cpure-exploration\u201d RL [57], where the agent maximizes the entropy of the state visitation measure, (i) imitation learning [1], where an agent minimizes the distance of the state visitation measure their policy induces and the one induced by an expert, (i) risk-averse RL [47] where the agent optimizes an objective function that is sensitive to the tail behavior of the agent and not merely their expected behavior [100, 99, 25, 9, 115, 80], (iv) constrained RL [4], where an agent optimizes their value function while making sure to satisfy a number of constraints that are dependent on their state-action visitation measure [5, 107, 13, 2], (v) diverse skills discovery, where the goal is to drive learning agents to acquire a diverse set of emergent skills [19, 41, 52, 55, 58, 72, 96, 109]. ", "page_idx": 18}, {"type": "text", "text": "A.3Optimization ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Min-max   Optimization  Min-max optimization studies  problems  of   the  form $\\mathrm{min}_{\\mathbf{x}\\in\\mathcal{X}}\\,\\mathrm{max}_{\\mathbf{y}\\in\\mathcal{Y}}\\,f(\\mathbf{x},\\pmb{y})$ . When the objective function $f$ is convex in $\\textbf{\\em x}$ and concave in $\\textit{\\textbf{y}}$ the corresponding variational inequality (VI) is monotone, and a wide range of algorithms have been proposed for computing an approximate saddle-point \u2014\u2014 see, e.g., [81, 69]. ", "page_idx": 18}, {"type": "text", "text": "It is also known that standard Gradient Descent/Ascent (GDA) exhibits time-averaged convergence while the actual trajectory of iterates might cycle [30, 31]. Methods like Extra Gradient or techniques such as optimism are used to ensure convergence [30, 31, 32, 76, 78, 18, 51]. ", "page_idx": 18}, {"type": "text", "text": "For more general objectives, we know how to compute approximate saddle-points when the weak Minty Property is satisfied [36] and for functions where one (or both) side satisfies the PLcondition [83, 44, 105]. On the negative side, we know that the problem in its full generality (nonconvexnonconcave landscape with coupled linear constraints) is computationally intractable [33]. ", "page_idx": 18}, {"type": "text", "text": "Hidden-Convex Optimization  This nascent field of optimization [43, 48] considers nonconvex objectives that can be reformulated, through a change of variables, into a convex objective. Further, in the context of game theory, the notion of hidden-monotonicity has made its appearance in [45] and the subsequent works [77, 90]. ", "page_idx": 18}, {"type": "text", "text": "Weakly-Smooth Optimization The majority of references that we encounter for weakly-smooth minimization assume convexity and concern the unconstrained setting. We mention the important references of [35, 82] while also more recent works [106, 85, 84]. ", "page_idx": 18}, {"type": "text", "text": "B  Nonconvex Weakly-Smooth Constrained Optimization ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section we prove that stochastic projected gradient descent with an stochastic inexact oracle convergestoan $\\epsilon$ -FOSP in functions with Holder continuous gradient. We complement this section with the proof of folklore lemmas of constrained optimization that show that the \u201cgradient mapping? (Definition B.1) is an appropriate surrogate of stationarity also for the family of functions we consider. ", "page_idx": 19}, {"type": "text", "text": "Definition B.1 (Gradient Mapping). We define the gradient mapping and stochastic gradient mapping, $\\boldsymbol{r}_{\\eta}$ and $\\hat{\\pmb r}_{\\eta}$ tobe: ", "page_idx": 19}, {"type": "text", "text": "B.1 Auxiliary Lemmas ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In general, demonstrating that the gradient mapping is an adequate surrogate of stationarity in differentiable constraint optimization relies on the Lipschitz continuity of the function. We make sure that this is the case when the gradient is only Holder continuous with $p<1$ ", "page_idx": 19}, {"type": "text", "text": "Lemma B.1 (Inexact-Gradient Mapping as a Stationarity Surrogate). If $\\|r_{\\eta}(z)\\|\\,\\leq\\,\\epsilon$ for some $z\\in{\\mathcal{Z}}$ , it holds that: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{z^{\\prime}\\in\\mathcal{Z},\\|z^{\\prime}-z^{+}\\|\\leq1}\\left\\langle-\\nabla\\phi(z^{+}),z^{\\prime}-z^{+}\\right\\rangle\\leq\\vartheta+\\eta^{2}\\epsilon+\\ell_{p}\\eta^{p}\\epsilon^{p},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $z^{+}:=\\operatorname{Proj}_{\\mathcal{Z}}\\left(z-\\eta\\pmb{g}(z)\\right)$ ", "page_idx": 19}, {"type": "text", "text": "Proof. In (Inexact Stoch-PGD), $\\|g(z)-\\nabla\\phi(z)\\|\\leq\\vartheta$ \uff0c $\\forall z\\in{\\mathcal{Z}}$ . Since $z^{+}:=\\operatorname{Proj}_{\\mathcal{Z}}\\left(z-\\eta\\pmb{g}(z)\\right)$ it holds that ", "page_idx": 19}, {"type": "equation", "text": "$$\nz^{+}=\\underset{z^{\\prime}\\in\\mathcal{Z}}{\\operatorname{argmin}}\\left\\lbrace\\left|z^{\\prime}-(z-\\eta\\pmb{g}(z))\\right|\\right|^{2}\\right\\rbrace.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Due to the optimality condition, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n-\\,\\left(z^{+}-z+{\\frac{1}{\\eta}}{\\pmb g}(z)\\right)\\in N_{\\mathcal{Z}}(z^{+}),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $N_{\\mathcal{Z}}(z)$ is the normal cone of $\\mathcal{Z}$ at $_{z}$ \uff0c $N_{\\mathcal{Z}}(z):=\\{v|\\,\\langle v,z^{\\prime}-z\\rangle\\leq0,\\forall z^{\\prime}\\in\\mathcal{Z}\\}$ . From the latter, we can conclude that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle-\\left(z^{+}-z+\\frac{1}{\\eta}\\nabla\\phi(z)\\right)-\\frac{1}{\\eta}\\left(-\\nabla\\phi(z)+g(z)\\right)\\in N_{\\mathcal{Z}}(z^{+})}\\\\ {\\displaystyle-\\left(z^{+}-z+\\frac{1}{\\eta}\\nabla\\phi(z)\\right)\\in N_{\\mathcal{Z}}(z^{+})+B\\left(\\frac{\\vartheta}{\\eta}\\right)}\\\\ {\\displaystyle-\\,\\frac{1}{\\eta}\\nabla\\phi(z^{+})-\\left(z^{+}-z+\\frac{1}{\\eta}\\nabla\\phi(z)-\\frac{1}{\\eta}\\nabla\\phi(z^{+})\\right)\\in N_{\\mathcal{Z}}(z^{+})+B\\left(\\frac{\\vartheta}{\\eta}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Now, we bound $\\begin{array}{r}{\\Big\\|z^{+}-z+\\frac{1}{\\eta}\\nabla\\phi(z)-\\frac{1}{\\eta}\\nabla\\phi(z^{+})\\Big\\|.}\\end{array}$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|z^{+}-z+\\displaystyle\\frac{1}{\\eta}\\nabla\\phi(z)-\\frac{1}{\\eta}\\nabla\\phi(z^{+})\\right\\|\\leq\\left\\|z^{+}-z\\right\\|+\\displaystyle\\frac{1}{\\eta}\\left\\|\\nabla\\phi(z)-\\nabla\\phi(z^{+})\\right\\|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\left\\|z^{+}-z\\right\\|+\\displaystyle\\frac{\\ell_{p}}{\\eta}\\left\\|z^{+}-z\\right\\|^{p}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\eta\\epsilon+\\displaystyle\\frac{\\ell_{p}}{\\eta^{1-p}}\\epsilon^{p}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n-\\nabla\\phi(z^{+})\\in N_{\\mathcal{Z}}(z^{+})+B\\left(\\vartheta+\\eta^{2}\\epsilon+\\ell_{p}\\eta^{p}\\epsilon^{p}\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The latter display implies the statement of the lemma. ", "page_idx": 19}, {"type": "text", "text": "We immediately have the following corollary, ", "page_idx": 20}, {"type": "text", "text": "Corollary B.1. For any $z\\in{\\mathcal{Z}}$ , denote $z^{+}:=\\mathrm{Proj}_{\\mathcal{Z}}\\left(z-\\eta\\pmb{g}(z)\\right).\\;\\mathbb{E}\\left[\\left\\|\\pmb{r}_{\\eta}(z)\\right\\|\\right]\\leq\\epsilon$ implies that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\operatorname*{max}_{\\substack{z^{\\prime}\\in\\mathcal{Z},\\|z^{\\prime}-z^{+}\\|\\leq1}}\\left\\langle-\\nabla\\phi(z^{+}),z^{\\prime}-z^{+}\\right\\rangle\\right]\\leq\\vartheta+\\eta^{2}\\epsilon+\\ell_{p}\\eta^{p}\\epsilon^{p}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "B.2 Stochastic PGD with Inexact Gradients ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The folklore proof of gradient descent for nonconvex functions relies on the Lipschitz continuity of the gradient to prove convergence to a first-order stationary point. When the gradient are not Lipschitz continuous but continuous in the weaker notion of Holder continuity implies the following fact that we will eventually use to prove a \u201cdescent lemma\". ", "page_idx": 20}, {"type": "text", "text": "Fact B.1. Let a function $\\phi:\\mathcal{Z}\\to\\mathbb{R}$ with $(p,\\ell_{p})$ -Holder continuous gradient. Then, it is the case that for all $z,z^{\\prime}$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n|\\phi(z^{\\prime})-\\phi(z)+\\langle\\nabla f(z),z^{\\prime}-z\\rangle|\\leq\\frac{\\ell_{p}}{1+p}\\left\\|z^{\\prime}-z\\right\\|^{1+p}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Following [35], we discuss functions with Holder-continuous gradient (see Definition 2.1) through the framework of inexact oracle. We show that the answer $\\bigl(\\bar{\\phi}(z),\\nabla\\phi(z)\\bigr)$ of an exact oracle for a nonconvex function satisfying Holder gradient continuity can be translated into some \u201cinexact\" information for a smooth function. Parameters $\\delta,\\ell^{\\prime}$ in Proposition B.1 can be treated as \u201cinexactness\" parameters and will be chosen as appropriate parameters of the exponent $p$ of Holder continuity. ", "page_idx": 20}, {"type": "text", "text": "Proposition B.1. For given $\\delta,\\ell_{p}$ , and a tuning of $\\begin{array}{r}{\\ell^{\\prime}:=\\frac{\\ell_{p}^{\\frac{2}{1+p}}}{\\delta^{\\frac{1-p}{1+p}}}}\\end{array}$ , it holds that for any $\\mathbf{\\boldsymbol{x}},\\mathbf{\\boldsymbol{x}}^{\\prime}$ \uff0c ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{\\ell_{p}}{1+p}\\left\\|\\pmb{x}-\\pmb{x}^{\\prime}\\right\\|^{1+p}\\leq\\frac{\\ell^{\\prime}}{2}\\left\\|\\pmb{x}-\\pmb{x}^{\\prime}\\right\\|^{2}+\\delta.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. We let $\\chi:=\\|\\pmb{x}-\\pmb{x}^{\\prime}\\|$ . By choosing the optimal $\\ell^{\\prime}$ we can verify that ", "page_idx": 20}, {"type": "equation", "text": "$$\n2\\operatorname*{max}_{\\chi\\geq0}\\left\\{\\frac{\\ell_{p}}{1+p}\\chi^{-1+p}-\\delta\\chi^{-2}\\right\\}=\\ell_{p}\\left(\\frac{\\ell_{p}}{2\\delta}\\cdot\\frac{1-p}{1+p}\\right)^{\\frac{1-p}{1+p}}\\leq\\frac{\\ell_{p}^{\\frac{2}{1+p}}}{\\delta^{\\frac{1-p}{1+p}}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "$\\begin{array}{r}{\\left(\\frac{1-p}{2(1+p)}\\right)^{\\frac{1-p}{1+p}}\\leq1}\\end{array}$ for $0\\leq p\\leq1$ . Setting $\\begin{array}{r}{\\ell^{\\prime}=\\frac{\\ell_{p}^{\\frac{2}{1+p}}}{\\delta^{\\frac{1-p}{1+p}}}}\\end{array}$ yields the desired inequality. ", "page_idx": 20}, {"type": "text", "text": "Now, we can use Proposition B.1 as in place of the \u201cdescent-lemma\u201d to Theorem C.3 to prove convergenceto an $\\epsilon$ FOSP. ", "page_idx": 20}, {"type": "text", "text": "Theorem B.1. Let $\\phi~:~{\\mathcal{Z}}~\\rightarrow~\\mathbb{R}$ be a $(p,\\ell_{p})$ -weakly smooth nonconvex function. Further, assume a stochastic inexact gradient oracle $\\hat{\\pmb g}$ .I.e., it holds that $\\mathbb{E}\\left[\\hat{\\pmb g}(z)-\\pmb g(z)\\right]~=~0$ and $\\begin{array}{r}{\\mathbb{E}\\left[\\left\\|\\hat{\\pmb{g}}(z)-\\pmb{g}(z)\\right\\|^{2}\\right]\\le\\frac{\\sigma^{2}}{M}}\\end{array}$ for some $g\\,:\\,\\mathcal{Z}\\,\\rightarrow\\,\\mathcal{Z}^{*}$ where $\\|g(z)-\\nabla\\phi(z)\\|\\,\\leq\\,\\vartheta$ \uff0c $\\forall z\\in{\\mathcal{Z}}$ .Implementing $T$ updates of the form Inexact Stoch-PGD) using $\\hat{\\pmb g}$ and a stepsize $\\begin{array}{r}{\\eta=\\frac{1}{2\\ell^{\\prime}}}\\end{array}$ guarantees that: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=0}^{T-1}\\mathbb{E}\\left[\\left\\Vert\\hat{r}_{\\eta}^{t}\\right\\Vert^{2}\\right]\\leq\\frac{8\\ell_{p}^{\\frac{2}{1+p}}\\left(\\mathbb{E}\\left[\\phi\\left(z^{0}\\right)\\right]-\\phi^{*}\\right)}{\\delta^{\\frac{1-p}{1+p}}T}+\\frac{8\\sigma^{2}}{M}+8\\ell_{p}^{\\frac{2}{1+p}}\\delta^{\\frac{2p}{1+p}}+4\\vartheta^{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We postpone the proof to state a corollary that might help the reader gain some intuition on how the iteration complexity scales with $p$ ", "page_idx": 20}, {"type": "text", "text": "$\\phi,\\hat{\\pmb g}$ $\\begin{array}{r}{\\eta=\\big(\\frac{\\epsilon^{1-p}}{2^{3-2p}\\cdot\\ell_{p}}\\big)^{\\frac{1}{p}}}\\end{array}$ $t^{*}$ $[1,\\dots,T]$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\Vert r_{\\eta}^{*}\\right\\Vert^{2}\\right]\\leq\\frac{8\\ell_{p}^{\\frac{2}{1+p}}\\left(\\mathbb{E}\\left[\\phi\\left(z^{0}\\right)\\right]-\\phi^{*}\\right)}{\\delta^{\\frac{1-p}{1+p}}T}+16\\ell_{p}^{\\frac{2}{1+p}}\\delta^{\\frac{2p}{1+p}}+8\\vartheta^{2}+\\frac{18\\sigma^{2}}{M},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\boldsymbol{r}_{\\eta}^{*}:=\\boldsymbol{r}_{\\eta}^{t^{*}}$ Furthermorebysetin theprameers as $\\begin{array}{r}{T\\ge\\frac{8^{\\frac{1+p}{p}}\\ell_{p}^{\\frac{1}{p}}\\left(\\mathbb{E}\\left[\\phi\\left(z^{0}\\right)\\right]-\\phi^{*}\\right)}{\\epsilon^{\\frac{1+p}{p}}},\\delta\\le\\frac{\\left(\\frac{\\epsilon}{8}\\right)^{\\frac{1+p}{p}}}{\\ell_{p}^{\\frac{1}{p}}}}\\end{array}$ $\\vartheta\\ \\leq\\ \\frac{\\epsilon}{8}$ . and $\\begin{array}{r}{M\\ge~\\frac{9\\sigma^{2}}{2\\epsilon^{2}}}\\end{array}$ , it is guaranteed that there will exist a $t^{\\star}\\,\\in\\,\\{0,\\ldots,T-1\\}$ such that $\\mathbb{E}[\\pmb{r}_{\\eta}(\\pmb{z}^{t^{\\star}})]\\leq\\epsilon$ ", "page_idx": 21}, {"type": "text", "text": "Proof. For the first claim, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\left[\\left\\Vert r_{\\mathfrak{g}}^{*}\\right\\Vert^{2}\\right]=\\mathbb{E}\\left[\\left\\Vert\\frac{1}{\\eta}\\left(z^{*}-\\operatorname{Proj}_{\\mathcal{L}}\\left(z^{*}-\\eta g(z^{*})\\right)\\right)\\right\\Vert^{2}\\right]}&{}\\\\ {\\leq2\\mathbb{E}\\left[\\left\\Vert\\frac{1}{\\eta}\\left(z^{*}-\\operatorname{Proj}_{\\mathcal{L}}\\left(z^{*}-\\eta g(z^{*})\\right)\\right)\\right\\Vert^{2}\\right]}&{}\\\\ {\\quad+2\\mathbb{E}\\left[\\left\\Vert\\frac{1}{\\eta}\\left(\\operatorname{Proj}_{\\mathcal{L}}\\left(z^{*}-\\eta g(z^{*})\\right)-\\operatorname{Proj}_{\\mathcal{L}}\\left(z^{*}-\\eta g(z^{*})\\right)\\right)\\right\\Vert^{2}\\right]}&{}\\\\ {\\leq2\\mathbb{E}\\left[\\left\\Vert\\frac{\\eta}{\\eta}\\right\\Vert^{2}\\right]+2\\mathbb{E}\\left[\\left\\Vert\\frac{1}{\\eta}\\left(z^{*}-\\eta g(z^{*})-z^{*}-\\eta g(z^{*})\\right)\\right\\Vert^{2}\\right]}&{}\\\\ {=2\\mathbb{E}\\left[\\left\\Vert\\frac{\\eta}{\\eta}\\right\\Vert^{2}\\right]+2\\mathbb{E}\\left[\\left\\Vert\\mathfrak{g}(z^{*})-g(z^{*})\\right\\Vert^{2}\\right]}&{}\\\\ {\\leq\\frac{8\\rho_{0}^{2}\\lambda^{2}}{\\theta^{4}\\mu^{4}\\mu^{2}}\\frac{\\left(\\mathbb{E}\\left\\Vert\\tilde{\\varphi}\\right\\Vert^{2}\\right)-6\\eta^{4}}{\\theta^{4}\\mu^{2}}+16\\theta_{0}^{2}\\frac{\\eta\\tilde{L}\\eta}{\\theta^{4}\\mu^{3}}+8\\theta^{2}+\\frac{18\\sigma^{2}}{M}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Where th last inequality follws frm TheormB. andthe fact that $\\begin{array}{r}{\\mathbb{E}\\left[\\left\\|\\hat{\\pmb{g}}(z)-\\pmb{g}(z)\\right\\|^{2}\\right]\\le\\frac{\\sigma^{2}}{M}}\\end{array}$ By setting the parameters as in the corollary, we have $\\mathbb{E}[r_{\\eta}(z^{t^{\\star}})]\\le\\epsilon$ \u53e3 ", "page_idx": 21}, {"type": "text", "text": "Remark 1. With the same parameters we choose in Corollary $B.2$ LemmaB.1guaranteesthatfor any $p\\in(0,1]$ ${\\pmb r}_{\\eta}({\\pmb z})$ is a suffcient surrogate for stationarity. In particular, $\\left\\|r_{\\eta}(\\bar{z})\\right\\|\\leq\\epsilon$ impliesthat ", "page_idx": 21}, {"type": "equation", "text": "$$\n-\\nabla\\phi(z^{+})\\in N_{\\mathcal{Z}}(z^{+})+B\\left(\\left(\\left(\\frac{8^{1-p}}{\\ell_{p}}\\right)^{\\frac{2}{p}}+9\\right)\\epsilon\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Finally, we state one more auxiliary claim before proceeding to the proof of Theorem C.3. ", "page_idx": 21}, {"type": "text", "text": "Claim B.1. Consider an iterate of (Inexact Stoch-PGD), $z^{t}$ .Also, define $z^{+}=\\mathrm{Proj}_{\\mathcal{Z}}\\left(z^{t}-\\eta\\pmb{g}(z)\\right)$ where $\\textbf{\\textit{g}}$ is the inexact-gradient oracle. It is the case that, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|z^{t+1}-z^{+}\\|\\leq\\eta^{2}\\frac{\\sigma^{2}}{M}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. The proof follows easily from arguments we have already used, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\Vert z^{t+1}-z^{+}\\right\\Vert^{2}\\right]=\\mathbb{E}\\left[\\left\\Vert\\mathrm{Proj}_{\\mathcal{Z}}\\left(z^{t}-\\eta\\hat{\\pmb{g}}^{t}\\right)-\\mathrm{Proj}_{\\mathcal{Z}}\\left(z^{t}-\\eta\\pmb{g}^{t}\\right)\\right\\Vert^{2}\\right]}\\\\ &{\\leq\\mathbb{E}\\left[\\left\\Vert\\eta\\pmb{\\hat{g}}^{t}-\\eta\\pmb{g}^{t}\\right\\Vert^{2}\\right]}\\\\ &{=\\eta^{2}\\mathbb{E}\\left[\\left\\Vert\\pmb{\\hat{g}}^{t}-\\pmb{g}^{t}\\right\\Vert^{2}\\right]}\\\\ &{\\leq\\eta^{2}\\frac{\\sigma^{2}}{M}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof of Theorem B.1 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Proof. Since $\\|g(z)-\\nabla\\phi(z)\\|\\leq\\vartheta$ , from the weakly-smooth condition, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\varphi(x^{(i)})}&{\\le\\varphi(x^{(i)})+\\mathcal{C}(\\varphi(x^{(i)}),x^{(i+1)}-x^{(i)}+\\frac{1}{\\varphi},\\log(\\log(x^{(i)}-x^{(i)})^{\\alpha+1}}\\\\ &{\\quad\\le\\varphi(x^{(i)})+\\mathcal{C}(\\varphi(x^{(i)}),x^{(i+1)}-x^{(i)})+\\frac{\\varphi}{2}\\log(x^{(i)}-x^{(i)^{\\alpha}+1}}\\\\ &{\\quad=\\varphi(x^{(i)})+\\mathcal{C}(\\varphi(x^{(i)},x^{(i+1)}-x^{(i)}+\\mathcal{C}(\\varphi(x^{(i)})-\\varphi(x^{(i)},x^{(i)}-x^{(i)}+\\mathcal{C})^{2}\\log(x^{(i)}+\\mathcal{C})^{2}}\\\\ &{\\quad=\\varphi(x^{(i)})-\\varphi(x^{(i)},x^{(i)})+\\pi(\\varphi(x^{(i)}-\\varphi(x^{(i)},x^{(i)},x^{(i)}+\\mathcal{C})^{2}\\log(x^{(i)}-x^{(i)}}\\\\ &{\\quad=\\varphi(x^{(i)})-\\varphi(x^{(i)},x^{(i)}+\\pi(\\varphi(x^{(i)}-\\varphi(x^{(i)},x^{(i)})+\\varphi(\\nabla(x^{(i)}-\\varphi(x^{(i)},x^{(i)})}\\\\ &{\\quad+\\frac{\\varphi^{(i)}}{2}\\log(x^{(i)}+\\mathcal{C})^{2}(\\varphi(x^{(i)}-\\varphi(x^{(i)},x^{(i)})+\\varphi(\\nabla(x^{(i)}-\\varphi(x^{(i)},x^{(i)})}\\\\ &{\\quad\\le\\varphi(x^{(i)})-\\varphi(x^{(i)},x^{(i)}+\\mathcal{C}(\\varphi(x^{(i)}-\\varphi(x^{(i)},x^{(i)})+\\varphi(\\nabla(x^{(i)}-\\varphi(x^{(i)},x^{(i)})}\\\\ &{\\quad+\\frac{\\varphi^{(i)}}{2}\\log(x^{(i)}+\\mathcal{C})^{2}(\\varphi(x^{(i)}-\\varphi(x^{(i)},x^{(i)})+\\frac{\\varphi}{2}\\log(x(i)-x^{(i)})^{\\alpha}}\\\\ &{\\quad\\le\\\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Where ", "page_idx": 22}, {"type": "text", "text": "\u00b7 (1) is because of Proposition B.1; ", "page_idx": 22}, {"type": "text", "text": "\u00b7 in (2), we plug-in the definition of $\\hat{\\pmb r}_{\\eta}^{t}$ \uff0c   \n\u00b7 (3) uses the fact that $\\begin{array}{r l}{\\quad}&{{}-\\left\\langle\\hat{\\pmb g}(\\pmb z^{t}),\\hat{\\pmb r}_{\\eta}^{t}\\right\\rangle\\leq-\\frac{1}{\\eta}\\left\\lVert\\hat{\\pmb r}_{\\eta}^{t}\\right\\rVert^{2}}\\end{array}$   \n\u00b7 (4) is due to Young's inequality;   \n\u00b7 in (5), we plug-in the error bound on the inexact-gradient oracle $\\left\\|\\nabla\\phi(z)-g(z)\\right\\|^{2}\\leq\\vartheta$ ", "page_idx": 22}, {"type": "text", "text": "Continuing we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\phi\\left(z^{t+1}\\right)}\\\\ &{\\leq\\phi\\!\\left(z^{t}\\right)-\\left(\\frac{\\eta}{2}-\\frac{\\ell^{\\prime}\\eta^{2}}{2}\\right)\\left\\Vert\\hat{\\boldsymbol r}_{\\eta}^{t}\\right\\Vert^{2}+\\eta\\left\\langle\\hat{g}\\left(z^{t}\\right)-g\\left(z^{t}\\right),r_{\\eta}^{t}\\right\\rangle+\\eta\\left\\langle\\hat{g}\\left(z^{t}\\right)-g\\left(z^{t}\\right),\\hat{\\boldsymbol r}_{\\eta}^{t}-r_{\\eta}^{t}\\right\\rangle}\\\\ &{\\quad+\\frac{\\eta}{2}\\vartheta^{2}+\\delta}\\\\ &{\\leq\\phi(z^{t})-\\left(\\frac{\\eta}{2}-\\frac{\\ell^{\\prime}\\eta^{2}}{2}\\right)\\left\\Vert\\hat{\\boldsymbol r}_{\\eta}^{t}\\right\\Vert^{2}+\\eta\\left\\langle\\hat{g}\\left(z^{t}\\right)-g\\left(z^{t}\\right),r_{\\eta}^{t}\\right\\rangle+\\eta\\left\\Vert\\hat{g}\\left(z^{t}\\right)-g\\left(z^{t}\\right)\\right\\Vert^{2}+\\frac{\\eta}{2}\\vartheta^{2}+\\delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Summing for $t=0,\\dots,T-1$ \uff0c ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=0}^{T-1}\\phi\\left(z^{t+1}\\right)}\\\\ &{\\leq\\sum_{t=0}^{T-1}\\left(\\phi\\left(z^{t}\\right)-\\left(\\frac{\\eta}{2}-\\frac{\\ell^{\\prime}\\eta^{2}}{2}\\right)\\|\\hat{\\boldsymbol r}_{\\eta}^{t}\\|^{2}+\\eta\\left\\langle\\hat{g}\\left(z^{t}\\right)-g\\left(z^{t}\\right),r_{\\eta}^{t}\\right\\rangle+\\eta\\left\\Vert\\hat{g}\\left(z^{t}\\right)-g\\left(z^{t}\\right)\\right\\Vert^{2}\\right)}\\\\ &{+\\frac{\\eta}{2}\\vartheta^{2}T+\\delta T.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "This is equivalent to ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=0}^{T-1}\\left(\\frac{\\eta}{2}-\\frac{\\ell^{\\prime}\\eta^{2}}{2}\\right)\\left\\Vert\\hat{r}_{\\eta}^{t}\\right\\Vert^{2}}\\\\ &{\\leq\\phi\\left(z^{0}\\right)-\\phi\\left(z^{T}\\right)+\\displaystyle\\sum_{t=0}^{T-1}\\left(\\eta\\left\\langle\\hat{g}\\left(z^{t}\\right)-g\\left(z^{t}\\right),r_{\\eta}^{t}\\right\\rangle+\\eta\\left\\Vert\\hat{g}\\left(z^{t}\\right)-g\\left(z^{t}\\right)\\right\\Vert^{2}\\right)}\\\\ &{\\quad+\\displaystyle\\frac{\\eta}{2}\\vartheta^{2}T+\\delta T}\\\\ &{\\leq\\phi\\left(z^{0}\\right)-\\phi^{*}+\\displaystyle\\sum_{t=0}^{T-1}\\left(\\eta\\left\\langle\\hat{g}\\left(z^{t}\\right)-g\\left(z^{t}\\right),r_{\\eta}^{t}\\right\\rangle+\\eta\\left\\Vert\\hat{g}\\left(z^{t}\\right)-g\\left(z^{t}\\right)\\right\\Vert^{2}\\right)+\\displaystyle\\frac{\\eta}{2}\\vartheta^{2}T+\\delta T.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Taking expectations, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left(\\frac{\\eta}{2}-\\frac{\\ell^{\\prime}\\eta^{2}}{2}\\right)\\displaystyle\\sum_{t=0}^{T-1}\\mathbb{E}\\left[\\left\\Vert\\hat{\\pmb{r}}_{\\eta}^{t}\\right\\Vert^{2}\\right]}\\\\ &{\\leq\\mathbb{E}\\left[\\phi\\left(z^{0}\\right)\\right]-\\phi^{*}+\\displaystyle\\sum_{t=0}^{T-1}\\left(\\eta\\mathbb{E}\\left[\\left\\langle\\hat{\\pmb{g}}\\left(z^{t}\\right)-\\pmb{g}\\left(z^{t}\\right),\\pmb{r}_{\\eta}^{t}\\right\\rangle\\right]+\\eta\\mathbb{E}\\left[\\left\\Vert\\hat{\\pmb{g}}\\left(z^{t}\\right)-\\pmb{g}\\left(z^{t}\\right)\\right\\Vert^{2}\\right]\\right)}\\\\ &{\\quad+\\delta T+\\frac{\\eta}{2}\\vartheta^{2}T}\\\\ &{\\leq\\mathbb{E}\\left[\\phi\\left(z^{0}\\right)\\right]-\\phi^{*}+\\eta\\displaystyle\\frac{\\sigma^{2}}{M}T+\\delta T+\\frac{\\eta}{2}\\vartheta^{2}T.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "By setting $\\begin{array}{r}{\\eta\\leftarrow\\frac{1}{2\\ell^{\\prime}}}\\end{array}$ it holds that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=0}^{T-1}\\mathbb{E}\\left[\\left\\lVert\\hat{r}_{\\eta}^{t}\\right\\rVert^{2}\\right]\\leq\\frac{8\\ell^{\\prime}\\left(\\mathbb{E}\\left[\\phi\\left(z^{0}\\right)\\right]-\\phi^{*}\\right)}{T}+\\frac{8\\sigma^{2}}{M}+8\\ell^{\\prime}\\delta+4\\vartheta^{2}}\\\\ {=\\frac{8\\ell_{p}^{\\frac{2}{1+p}}\\left(\\mathbb{E}\\left[\\phi\\left(z^{0}\\right)\\right]-\\phi^{*}\\right)}{\\delta^{\\frac{1-p}{1+p}}T}+\\frac{8\\sigma^{2}}{M}+8\\ell_{p}^{\\frac{2}{1+p}}\\delta^{\\frac{2p}{1+p}}+4\\vartheta^{2}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "This completes the proof. ", "page_idx": 23}, {"type": "text", "text": "C Adversarial Team Markov Games ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this section we the formal proofs of our claims regarding ATMGs. Before proceeding, let us provide a roadmap of the current section: ", "page_idx": 24}, {"type": "text", "text": "\u00b7 Beginning, in Table 1 we offer a concise summary of our ATMG-related notation.   \n\u00b7 We proceed to present a number of crucial facts regarding MDPs in Appendix C.1. In particular, facts regarding the state-action visitation measure.   \n\u00b7 In Appendix C.3, we demonstrate that the regularized value function has a unique maximizer that changes in Holder-continuous way w.r.t. to team policies $\\textbf{\\em x}$ . This leads to the Holdercontinuity of the gradient of the regularized maximum function $\\Phi^{\\nu}$ (see Theorem C.2).   \n\u00b7 Having established the latter, in Section 3.2 we invoke the results on gradient descent for nonconvex functions with Holder continuous gradient to get our main theorem regarding $\\epsilon$ -NE learning in ATMGs (Theorem C.3).   \n\u00b7 The tuning of the parameters of Theorem C.3 is supported by (i) Appendix C.5, where we get precise guarantees for maximizing the regularizing value function w.r.t. the adversary's policy $\\textit{\\textbf{y}}$ (Theorem C.4) (ii) Appendix C.6, where we define and analyze the gradient estimators used by the agents of the MG. ", "page_idx": 24}, {"type": "table", "img_path": "BrvLTxEx08/tmp/ab577fe42ffe7e743e53b019771fad91e7d8b24a1b2174be13180d15d4981850.jpg", "table_caption": [], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "C.1 Further Background on Markov Decision Processes ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We need additional preliminaries on Markov decision processes (MDPs). Specifically, we will discuss the properties of the (discounted) state and state-action visitation measure. These measures represent the \u201cdiscounted\u2019 expected amount of time the Markov chain\u2014induced by the players\u2019 fixed policies\u2014\u2014spends at state $s$ (respectively, at a state action pair $(s,b))$ starting from initial state $s^{\\prime}$ .Each visit is weighted by a discount factor $\\gamma^{h}$ ,where $h$ is the visit time. Notably, in [3] it is defined as a probability measure, meaning that for an initial state distribution $\\rho$ , the discounted state visitation distribution sums to 1. For convenience, we will use the unnormalized definition from [88, Chapter 6.10], which sums to $\\frac{1}{1-\\gamma}$ . This is why we refer to it as a measure instead of distribution. ", "page_idx": 26}, {"type": "text", "text": "Definition C.1 (State Visit. Measure). Given an initial state distribution $\\rho\\in\\Delta(S)$ andastationary jointpolicy $\\pi\\in\\Pi$ we define the statevisitationfrequency $d_{\\overline{{s}}}^{\\pi}$ asfollows: ", "page_idx": 26}, {"type": "equation", "text": "$$\nd_{\\overline{{s}}}^{\\pi}(s)=\\sum_{h=0}^{\\infty}\\gamma^{h}\\,\\mathbb{P}(s_{h}=s|\\pi,s_{0}=\\overline{{s}}).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Additionally, expanding the definition, we define $d_{\\rho}^{\\pi}(s)=\\mathbb{E}_{\\overline{{s}}\\sim\\rho}\\left[d_{\\overline{{s}}}^{\\pi}(s)\\right]$ ", "page_idx": 26}, {"type": "text", "text": "For convenience, the expression $d_{\\rho}^{x,y}(s)$ is utilized to represent the state visitation measure resulting from the policies $({\\boldsymbol{x}},{\\boldsymbol{y}})\\in{\\mathcal{X}}\\times{\\mathcal{Y}}$ ", "page_idx": 26}, {"type": "text", "text": "Fact C.1. Let MDP, $\\mathcal{M}(\\mathcal{S},\\mathcal{B},\\mathbb{P},r,\\gamma)$ . Let a policy $\\pi\\in\\Delta(B)^{|S|}$ . For the corresponding state-action visitation measure $\\lambda\\in\\dot{\\mathbb{R}}^{S\\times B}$ and the state visitation measure $d_{\\rho}^{\\pi}\\in\\mathbb{R}^{S}$ , it holds that, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\lambda_{s,b}(\\pi)=d_{\\rho}^{\\pi}(s)\\pi(s,b),\\quad\\forall s\\in S,\\forall b\\in\\mathcal{B}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "A quantity that is important in contemporary RL literature is that of the mismatch coefficient which we formally define here. ", "page_idx": 26}, {"type": "text", "text": "Definition C.2 (Distribution Mismatch Coefficient). Let $\\rho\\in\\Delta(S)$ be afull-support distribution overstates,and $\\Pi$ be the joint set of policies. We define the distribution mismatch coeffcient $D$ as ", "page_idx": 26}, {"type": "equation", "text": "$$\nD_{\\mathrm{m}}:=\\operatorname*{sup}_{\\pi\\in\\Pi}\\left\\|\\frac{d_{\\rho}^{\\pi}}{\\rho}\\right\\|_{\\infty},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $\\frac{d_{\\rho}^{\\pi}}{\\rho}$ denotes element-wise division. ", "page_idx": 26}, {"type": "text", "text": "The following theorem that relates policies and visitation measures is essential to our analysis. ", "page_idx": 26}, {"type": "text", "text": "Theorem C.1 ([88, Theorem 6.9.1]). Consider an adversarial Markov game $\\Gamma$ andafixedteam policy $\\textbf{\\em x}$ ", "page_idx": 26}, {"type": "text", "text": "(i) Any $\\pmb{y}\\in\\mathcal{V}$ defines a feasible state-action visitation measure $\\pmb{\\lambda}\\in\\mathbb{R}^{|S|\\times|B|}$ ;namely, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\lambda_{s,b}(\\pmb{y};\\pmb{x}):=\\sum_{\\pmb{\\bar{s}}\\in\\cal{S}}\\rho(\\overline{{\\pmb{s}}})\\cdot\\mathbb{E}_{\\pmb{y}}\\left[\\gamma^{t}\\,\\mathbb{P}(\\pmb{s}^{(t)}=\\pmb{s},b^{(t)}=b\\mid\\pmb{x},\\pmb{s}^{(0)}=\\overline{{\\pmb{s}}})\\right].\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "(i\uff09 Any feasible state-action visitation measure $\\lambda$ defines a feasible $\\pmb{y}\\in\\mathcal{V}$ namely, ", "page_idx": 26}, {"type": "equation", "text": "$$\ny_{s,b}:=\\frac{\\lambda(s,b)}{\\sum_{b^{\\prime}\\in B}\\lambda(s,b^{\\prime})},\\;\\forall(s,b)\\in\\mathcal{S}\\times\\mathcal{B}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Further, for any such $\\pmb{y}\\in\\mathcal{V}$ it holdsthat $\\lambda_{s,b}(\\pmb{y};\\pmb{x})\\,=\\,\\lambda(s,b),\\;\\forall(s,b)\\,\\in\\,\\mathcal{S}\\times\\mathcal{B},$ where $\\lambda(y;x)$ is the induced discounted state-action measure. ", "page_idx": 26}, {"type": "text", "text": "An implication of the latter theorem is the fact that $\\lambda(\\cdot;x)$ is a \u201c1-1\" mapping between policies and visitation measures. Following, we see that this mapping is also Lipschitz-continuous and smooth (see Lemmas C.1 to C.3). ", "page_idx": 26}, {"type": "text", "text": "Lemma C.1. For any initial distribution $\\rho\\in\\Delta(S)$ , function $V_{\\rho}$ is $L$ -Lipschitz and $\\ell_{}$ -smooth with $\\begin{array}{r}{L:=\\frac{\\sqrt{\\sum_{i=1}^{n}|\\mathcal{A}_{i}|+|\\mathcal{B}|}}{(1-\\gamma)^{2}}}\\end{array}$ and 2(Z=- IAa1+1B), in other words ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{|V_{\\rho}(\\pmb{x},\\pmb{y})-V_{\\rho}(\\pmb{x}^{\\prime},\\pmb{y}^{\\prime})|\\leq\\frac{\\sqrt{\\sum_{i=1}^{n}|\\mathcal{A}_{i}|+|\\mathcal{B}|}}{(1-\\gamma)^{2}}\\left\\|(\\pmb{x},\\pmb{y})-(\\pmb{x}^{\\prime},\\pmb{y}^{\\prime})\\right\\|;}\\\\ &{}&{\\left\\|\\nabla V_{\\rho}(\\pmb{x},\\pmb{y})-\\nabla V_{\\rho}(\\pmb{x}^{\\prime},\\pmb{y}^{\\prime})\\right\\|\\leq\\frac{2\\gamma\\,\\left(\\sum_{i=1}^{n}|\\mathcal{A}_{i}|+|\\mathcal{B}|\\right)}{(1-\\gamma)^{3}}\\left\\|(\\pmb{x},\\pmb{y})-(\\pmb{x}^{\\prime},\\pmb{y}^{\\prime})\\right\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Lemma C.2. Let $\\pmb{\\lambda}\\in\\mathbb{R}^{|S||B|}$ be the state-action visitation measure for the adversary, then $\\lambda$ is $L_{\\lambda}$ -Lipschitz continuous and $\\ell_{\\lambda}$ -smooth w.r.t to policy $\\left({\\pmb x},{\\pmb y}\\right)$ . Specifically, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\|\\boldsymbol{\\lambda}\\left(y;\\boldsymbol{x}\\right)-\\boldsymbol{\\lambda}\\left(y^{\\prime};\\boldsymbol{x}^{\\prime}\\right)\\|\\leq\\frac{|\\boldsymbol{S}|^{\\frac{1}{2}}\\left(\\sum_{i}|\\boldsymbol{A}_{i}|+|\\boldsymbol{B}|\\right)}{(1-\\gamma)^{2}}\\left(\\|\\boldsymbol{x}-\\boldsymbol{x}^{\\prime}\\|+\\|y-\\boldsymbol{y}^{\\prime}\\|\\right),\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\|\\nabla\\lambda\\left(y;x\\right)-\\nabla\\lambda\\left(y^{\\prime};x^{\\prime}\\right)\\|\\leq\\frac{2|S|^{\\frac{1}{2}}\\left(\\sum_{i}|A_{i}|+|B|\\right)^{\\frac{3}{2}}}{\\left(1-\\gamma\\right)^{3}}\\left(\\|x-x^{\\prime}\\|+\\|y-y^{\\prime}\\|\\right).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof.Each $\\lambda_{s,b}$ can be considered as a value function for the given state $s$ and the reward function is $r(a^{\\prime},b^{\\prime})=\\mathbb{1}(b=b^{\\prime})$ . Then by applying Lemma C.1, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\lambda_{s,b}\\left(\\pmb{y};\\pmb{x}\\right)-\\lambda_{s,b}\\left(\\pmb{y}^{\\prime};\\pmb{x}^{\\prime}\\right)|\\leq\\frac{\\sqrt{\\sum_{i}{|A_{i}|}+|B|}}{\\left(1-\\gamma\\right)^{2}}\\cdot\\left(\\|\\pmb{x}-\\pmb{x}^{\\prime}\\|+\\|\\pmb{y}-\\pmb{y}^{\\prime}\\|\\right),}\\\\ &{\\|\\nabla\\lambda_{s,b}\\left(\\pmb{y};\\pmb{x}\\right)-\\nabla\\lambda_{s,b}\\left(\\pmb{y}^{\\prime};\\pmb{x}^{\\prime}\\right)\\|\\leq\\frac{2\\gamma\\,\\left(\\sum_{i}{|A_{i}|}+|B|\\right)}{\\left(1-\\gamma\\right)^{3}}\\cdot\\left(\\|\\pmb{x}-\\pmb{x}^{\\prime}\\|+\\|\\pmb{y}-\\pmb{y}^{\\prime}\\|\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "From Equation (6) we get ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\|\\boldsymbol{\\lambda}\\left(y;\\boldsymbol{x}\\right)-\\boldsymbol{\\lambda}\\left(y^{\\prime};\\boldsymbol{x}^{\\prime}\\right)\\|_{\\infty}\\leq\\frac{\\sqrt{\\sum_{i}\\left|\\boldsymbol{A}_{i}\\right|+\\left|\\boldsymbol{B}\\right|}}{(1-\\gamma)^{2}}\\cdot\\left(\\|\\boldsymbol{x}-\\boldsymbol{x}^{\\prime}\\|+\\|y-\\pmb{y}^{\\prime}\\|\\right).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "This implies ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\lambda\\left(y;x\\right)-\\lambda\\left(y^{\\prime};x^{\\prime}\\right)\\|\\leq\\sqrt{|S||B|}\\left\\|\\lambda\\left(y;x\\right)-\\lambda\\left(y^{\\prime};x^{\\prime}\\right)\\right\\|_{\\infty}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\frac{\\sqrt{|S||B|}\\sqrt{\\sum_{i}\\left|A_{i}\\right|+|B|}}{\\left(1-\\gamma\\right)^{2}}\\cdot\\left(\\|x-x^{\\prime}\\|+\\|y-y^{\\prime}\\|\\right)}\\\\ &{\\qquad\\qquad\\qquad\\leq\\frac{\\sqrt{|S|}\\left(\\sum_{i}\\left|A_{i}\\right|+\\left|B\\right|\\right)}{\\left(1-\\gamma\\right)^{2}}\\cdot\\left(\\|x-x^{\\prime}\\|+\\|y-y^{\\prime}\\|\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Similarly, from Equation (7), we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\|\\nabla\\lambda_{s,b}\\left(y;\\boldsymbol{x}\\right)-\\nabla\\lambda_{s,b}\\left(y^{\\prime};\\boldsymbol{x}^{\\prime}\\right)\\|\\leq\\frac{2\\gamma\\left(\\sum_{i}\\left|A_{i}\\right|+\\left|B\\right|\\right)}{(1-\\gamma)^{3}}\\cdot\\left(\\left\\|\\boldsymbol{x}-\\boldsymbol{x}^{\\prime}\\right\\|+\\left\\|y-\\boldsymbol{y}^{\\prime}\\right\\|\\right).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Thus ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla\\lambda\\left(y;x\\right)-\\nabla\\lambda\\left(y^{\\prime};x^{\\prime}\\right)\\|_{F}\\leq\\sqrt{|S||B|}\\cdot\\underset{s\\in\\mathcal{S},b\\in\\mathcal{B}}{\\operatorname*{max}}\\|\\nabla\\lambda_{s,b}\\left(y;x\\right)-\\nabla\\lambda_{s,b}\\left(y^{\\prime};x^{\\prime}\\right)\\|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\frac{2\\gamma\\sqrt{|S||B|}\\,\\left(\\sum_{i}|A_{i}|+|B|\\right)}{\\left(1-\\gamma\\right)^{3}}\\cdot\\left(\\|x-x^{\\prime}\\|+\\|y-y^{\\prime}\\|\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Where $\\|\\cdot\\|_{F}$ denotes the Frobenius norm of the matrix. Finally, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|\\nabla\\lambda\\left(y;x\\right)-\\nabla\\lambda\\left(y^{\\prime};x^{\\prime}\\right)\\|\\leq\\|\\nabla\\lambda\\left(y;x\\right)-\\nabla\\lambda\\left(y^{\\prime};x^{\\prime}\\right)\\|_{F}}&{}\\\\ {\\leq\\frac{2\\gamma\\sqrt{\\left|S\\right|\\left|B\\right|}\\left(\\sum_{i}\\left|A_{i}\\right|+\\left|B\\right|\\right)}{\\left(1-\\gamma\\right)^{3}}\\cdot\\left(\\|x-x^{\\prime}\\|+\\left\\|y-y^{\\prime}\\right\\|\\right)}\\\\ {\\leq\\frac{2\\gamma\\sqrt{\\left|S\\right|\\left(\\sum_{i}\\left|A_{i}\\right|+\\left|B\\right|\\right)^{3}}}{\\left(1-\\gamma\\right)^{3}}\\cdot\\left(\\|x-x^{\\prime}\\|+\\left\\|y-y^{\\prime}\\right\\|\\right)}\\\\ {\\leq\\frac{2\\sqrt{\\left|S\\right|\\left(\\sum_{i}\\left|A_{i}\\right|+\\left|B\\right|\\right)^{3}}}{\\left(1-\\gamma\\right)^{3}}\\cdot\\left(\\|x-x^{\\prime}\\|+\\left\\|y-y^{\\prime}\\right\\|\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Lemma C.3. Consider $\\begin{array}{r}{\\lambda_{\\mathrm{inv}}(\\lambda(\\pmb{y};\\pmb{x})):=\\frac{\\lambda_{s,b}(\\pmb{y};\\pmb{x})}{\\sum_{b^{\\prime}}\\lambda_{s,b^{\\prime}}(\\pmb{y};\\pmb{x})}}\\end{array}$ whichisafunction thatmaps theavrsary's state-action visitation measure $\\lambda(y;x)\\in\\Lambda(x)$ to the adversary's policy $\\b{y}\\in\\b{\\mathcal{V}}$ . For any fixed team policy a, Ainv is L>i-Lipschitz continuous with respect A where L>inv = maxseS p((1-) Specifically, it holds that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\|y-y^{\\prime}\\|\\leq L_{\\lambda_{\\mathrm{inv}}}\\left\\|\\lambda(y;x)-\\lambda(y^{\\prime};x)\\right\\|.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. Take the partial derivative of $\\lambda_{\\mathrm{inv}}(\\lambda)$ ,wehave ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\left|\\frac{\\partial}{\\partial\\lambda_{s,b}}\\frac{\\lambda_{s,b}}{\\sum_{b^{\\prime}}\\lambda_{s,b^{\\prime}}}\\right|=\\left|\\frac{1}{\\sum_{b^{\\prime}}\\lambda_{s,b^{\\prime}}}-\\frac{\\lambda_{s,b}}{(\\sum_{b^{\\prime}}\\lambda_{s,b^{\\prime}})}\\right|}}\\\\ &{\\leq\\left|\\frac{1}{\\sum_{b^{\\prime}}\\lambda_{s,b^{\\prime}}}\\right|+\\left|\\frac{\\lambda_{s,b}}{(\\sum_{b^{\\prime}}\\lambda_{s,b^{\\prime}})}\\right|}\\\\ &{\\leq\\underset{s\\in S}{\\operatorname*{max}}\\left\\{\\left|\\frac{1}{\\rho(s)}\\right|+\\frac{1}{\\rho(s)(1-\\gamma)}\\right\\}}\\\\ &{\\leq\\underset{s\\in S}{\\operatorname*{max}}\\frac{2}{\\rho(s)(1-\\gamma)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "This implies the Lipschitz continuity. ", "page_idx": 28}, {"type": "text", "text": "The Regularized Value Function. ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Lemma C.4. Function $\\begin{array}{r}{V_{\\rho}^{\\nu}(\\pmb{x},\\pmb{y})\\,:=\\,V_{\\rho}^{\\nu}(\\pmb{x},\\pmb{y})\\,-\\,\\frac{\\nu}{2}\\,\\|\\pmb{\\lambda}\\,(\\pmb{y};\\pmb{x})\\|^{2}}\\end{array}$ is $L_{\\nu}$ -Lipschitz continuous and $\\ell_{\\nu}$ -smooth, where $\\begin{array}{r}{L_{\\nu}:=L+\\frac{\\nu L_{\\lambda}}{2(1-\\gamma)}}\\end{array}$ and $\\begin{array}{r}{\\ell_{\\nu}:=\\ell+\\frac{\\nu\\ell_{\\lambda}}{2(1-\\gamma)}+\\frac{\\nu L_{\\lambda}^{2}}{2}}\\end{array}$ ", "page_idx": 28}, {"type": "text", "text": "Proof. For Lipschitz continuity, we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\left\\lvert V_{\\rho}^{\\nu}(x,y)-V_{\\rho}^{\\nu}(x^{\\prime},y^{\\prime})\\right\\rvert}\\\\ &{\\leq\\ \\left\\lvert V_{\\rho}(x,y)-V_{\\rho}(x^{\\prime},y^{\\prime})\\right\\rvert+\\frac{\\nu}{2}\\left\\lvert\\|\\lambda(y;x)\\|^{2}-\\|\\lambda\\left(y^{\\prime};x^{\\prime}\\right)\\right\\rvert^{2}\\right\\rvert}\\\\ &{\\leq\\ \\left\\lvert V_{\\rho}(x,y)-V_{\\rho}(x^{\\prime},y^{\\prime})\\right\\rvert+\\frac{\\nu}{2}\\displaystyle\\operatorname*{max}_{x,y}\\left\\|\\lambda\\left(y;x\\right)\\right\\|\\cdot\\left\\lvert\\|\\lambda\\left(y;x\\right)\\right\\|-\\left\\|\\lambda\\left(y^{\\prime};x^{\\prime}\\right)\\right\\|\\;\\right\\rvert}\\\\ &{\\leq\\ \\left\\lvert V_{\\rho}(x,y)-V_{\\rho}(x^{\\prime},y^{\\prime})\\right\\rvert+\\frac{\\nu}{2}\\cdot\\displaystyle\\frac{1}{1-\\gamma}\\left\\|\\lambda\\left(y;x\\right)-\\lambda\\left(y^{\\prime};x^{\\prime}\\right)\\right\\|}\\\\ &{\\leq\\ \\left(L+\\displaystyle\\frac{\\nu L_{\\lambda}}{2(1-\\gamma)}\\right)\\cdot\\left(\\|x-x^{\\prime}\\|+\\|y-y^{\\prime}\\|\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "For smoothness, denote the Jacobian matrix of $\\lambda(y;x)$ w.r.t to $\\left({\\pmb x},{\\pmb y}\\right)$ by $\\mathbf{J}_{\\lambda}(x,y)$ , it holds that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|\\nabla_{x}\\left\\|\\lambda\\left(y;x\\right)\\right\\|^{2}-\\nabla_{x}\\left\\|\\lambda\\left(y^{\\prime};x^{\\prime}\\right)\\right\\|^{2}\\right\\|}\\\\ &{=\\;\\left\\|\\lambda\\left(y;x\\right)^{\\top}\\mathbf{J}_{\\lambda}\\left(x,y\\right)-\\lambda\\left(y^{\\prime};x^{\\prime}\\right)^{\\top}\\mathbf{J}_{\\lambda}\\left(x^{\\prime},y^{\\prime}\\right)\\right\\|}\\\\ &{\\leq\\;\\left\\|\\lambda\\left(y;x\\right)^{\\top}\\mathbf{J}_{\\lambda}\\left(x,y\\right)-\\lambda\\left(y;x\\right)^{\\top}\\mathbf{J}_{\\lambda}\\left(x^{\\prime},y^{\\prime}\\right)\\right\\|}\\\\ &{\\quad+\\left\\|\\lambda\\left(y;x\\right)^{\\top}\\mathbf{J}_{\\lambda}\\left(x^{\\prime},y^{\\prime}\\right)-\\lambda\\left(y^{\\prime};x^{\\prime}\\right)^{\\top}\\mathbf{J}_{\\lambda}\\left(x^{\\prime},y^{\\prime}\\right)\\right\\|}\\\\ &{\\leq\\;\\left\\|\\lambda\\left(y;x\\right)\\right\\|\\left\\|\\mathbf{J}_{\\lambda}\\left(x,y\\right)-\\mathbf{J}_{\\lambda}\\left(x^{\\prime},y^{\\prime}\\right)\\right\\|+\\left\\|\\lambda\\left(y;x\\right)-\\lambda\\left(y^{\\prime};x^{\\prime}\\right)\\right\\|\\left\\|\\mathbf{J}_{\\lambda}\\left(x^{\\prime},y^{\\prime}\\right)\\right\\|}\\\\ &{\\leq\\;\\frac{1}{1-\\gamma}\\left\\|\\mathbf{J}_{\\lambda}\\left(x,y\\right)-\\mathbf{J}_{\\lambda}\\left(x^{\\prime},y^{\\prime}\\right)\\right\\|+\\left\\|\\lambda\\left(y;x\\right)-\\lambda\\left(y^{\\prime};x^{\\prime}\\right)\\right\\|\\left\\|\\mathbf{J}_{\\lambda}\\left(x^{\\prime},y^{\\prime}\\right)\\right\\|}\\\\ &{\\leq\\;\\left(\\frac{\\ell_{\\lambda}}{1-\\gamma}+L_{\\lambda}\\cdot L_{\\lambda}\\right)\\cdot\\left(\\left\\|x-x^{\\prime}\\right\\|+\\left\\|y-y^{\\prime}\\right\\|\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Where in (8) we used the fact that $\\|\\mathbf{J}_{\\lambda}(\\mathbf{x}^{\\prime},\\pmb{y}^{\\prime})\\|\\leq L_{\\lambda}$ . We conclude that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|\\nabla V_{\\rho}^{\\nu}(\\mathbf{x},\\mathbf{y})-\\nabla V_{\\rho}^{\\nu}(\\mathbf{x}^{\\prime},\\mathbf{y}^{\\prime})\\right\\|}\\\\ &{=\\,\\left\\|\\nabla V_{\\rho}(\\mathbf{x},\\mathbf{y})-\\nabla V_{\\rho}(\\mathbf{x}^{\\prime},\\mathbf{y}^{\\prime})+\\frac{\\nu}{2}\\left(\\nabla_{x}\\left\\|\\lambda\\left(\\mathbf{y};\\mathbf{x}\\right)\\right\\|^{2}-\\nabla_{x}\\left\\|\\lambda\\left(\\mathbf{y}^{\\prime};\\mathbf{x}^{\\prime}\\right)\\right\\|^{2}\\right)\\right\\|}\\\\ &{\\leq\\,\\left\\|\\nabla V_{\\rho}(\\mathbf{x},\\mathbf{y})-\\nabla V_{\\rho}(\\mathbf{x}^{\\prime},\\mathbf{y}^{\\prime})\\right\\|+\\frac{\\nu}{2}\\left\\|\\nabla_{x}\\left\\|\\lambda\\left(\\mathbf{y};\\mathbf{x}\\right)\\right\\|^{2}-\\nabla_{x}\\left\\|\\lambda\\left(\\mathbf{y}^{\\prime};\\mathbf{x}^{\\prime}\\right)\\right\\|^{2}\\right\\|}\\\\ &{\\leq\\,\\left(\\ell+\\frac{\\nu\\ell_{\\lambda}}{2(1-\\gamma)}+\\frac{\\nu L_{\\lambda}^{2}}{2}\\right)\\cdot\\left(\\|\\mathbf{x}-\\mathbf{x}^{\\prime}\\|+\\|\\mathbf{y}-\\mathbf{y}^{\\prime}\\|\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Finally, we compute the Lipschitz continuity parameter of the reward vector that we already used in our previous claims. ", "page_idx": 29}, {"type": "text", "text": "Lemma C.5. Let ${\\pmb r}({\\pmb x})$ be the reward function for the adversary when the team is playing policy $\\textbf{\\em x}$ Then it holds that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\left\\|r(\\pmb{x})-r(\\pmb{x}^{\\prime})\\right\\|\\leq L_{r}\\left\\|\\pmb{x}-\\pmb{x}^{\\prime}\\right\\|,\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $L_{r}=\\sqrt{S}\\left(\\sum_{i=1}^{n}\\left|A_{i}\\right|+B\\right)$ ", "page_idx": 29}, {"type": "text", "text": "Proof. ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle||r({\\boldsymbol x})-r({\\boldsymbol x}^{\\prime})||\\leq\\sqrt{|\\mathcal{S}||\\mathcal{B}|}\\|r({\\boldsymbol x})-r({\\boldsymbol x}^{\\prime})\\|_{\\infty}}\\\\ {\\displaystyle=\\sqrt{|\\mathcal{S}||\\mathcal{B}|}\\operatorname*{max}_{s,b}|r({\\boldsymbol s},{\\boldsymbol x},b)-r({\\boldsymbol s},{\\boldsymbol x}^{\\prime},b)|}\\\\ {\\displaystyle=\\sqrt{|\\mathcal{S}||\\mathcal{B}|\\sum_{i=1}^{|\\mathcal{A}_{i}|}|\\mathcal{A}_{i}||{\\boldsymbol x}-{\\boldsymbol x}^{\\prime}||}}\\\\ {\\displaystyle\\leq\\sqrt{|\\mathcal{S}|}\\left(\\sum_{i=1}^{n}|\\mathcal{A}_{i}|+B\\right)\\|{\\boldsymbol x}-{\\boldsymbol x}^{\\prime}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Where (9) follows from Claim D.9. in [65]. ", "page_idx": 29}, {"type": "text", "text": "C.2  Auxiliary Lemmas ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Bounding the stationarity error on the truncated simplex. The $\\zeta$ -truncated simplex, $\\Delta^{m,\\zeta}$ is defined a the set of all probability vectors with no entry smaller than $\\zeta>0$ . More formally, for a given dimension $m$ and a $0<\\zeta\\leq\\frac{1}{m}$ , the $\\zeta$ -truncated simplex is defined to be ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\Delta^{m,\\zeta}=\\left\\{x\\ \\left|x_{i}\\geq\\zeta,\\sum_{i=1}^{m}x_{i}=1\\right.\\right\\}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Lemma C.6. [40, Lemma 15] Let $\\Delta^{m,\\zeta}$ be the $\\zeta$ truncated $m$ simplex. If $\\textstyle0\\leq\\zeta\\leq{\\frac{1}{2m}}$ , then for all $\\pmb{x}\\in\\Delta^{m}$ , there exists a $\\pmb{x}_{\\zeta}\\in\\Delta^{m,\\zeta}$ such that $\\lVert\\mathbf{x}-\\mathbf{x}_{\\zeta}\\rVert\\leq2\\zeta m$ ", "page_idx": 29}, {"type": "text", "text": "Proposition C.1 (Stationarity on the trunc. simplex). Let an $L_{f}$ -Lipschitz continuous differentiable function $f:\\Delta^{m}\\rightarrow\\mathbb{R}$ . Also, let an $\\epsilon$ -approximate stationary point $\\pmb{x}_{\\zeta}$ when the feasibility set is the truncated simplex $\\Delta^{m,\\zeta}$ such that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\left\\langle-\\nabla f(\\pmb{x}_{\\zeta}),\\pmb{x}_{\\zeta}^{\\prime}-\\pmb{x}_{\\zeta}\\right\\rangle\\leq\\epsilon,\\quad\\forall\\pmb{x}_{\\zeta}^{\\prime}\\in\\Delta^{m,\\zeta}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Then, $\\pmb{x}_{\\zeta}$ is an $(\\epsilon+2\\zeta m L_{f})$ -stationary point when the entire simplex is considered, i.e, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\langle-\\nabla f({\\pmb x}_{\\zeta}),{\\pmb x}-{\\pmb x}_{\\zeta}\\rangle\\leq\\epsilon+2\\zeta m L_{f},\\quad\\forall{\\pmb x}\\in\\Delta^{m}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Proof. Consider $\\pmb{x}_{\\zeta}^{\\prime}\\in\\Delta^{m,\\zeta}$ such that $\\left\\|\\pmb{x}-\\pmb{x}_{\\zeta}^{\\prime}\\right\\|\\leq2\\zeta m$ such point exists due to Lemma C.6, we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle-\\nabla f({\\pmb x}_{\\zeta}),{\\pmb x}-{\\pmb x}_{\\zeta}\\rangle=\\left\\langle-\\nabla f({\\pmb x}_{\\zeta}),{\\pmb x}_{\\zeta}^{\\prime}-{\\pmb x}_{\\zeta}\\right\\rangle+\\left\\langle-\\nabla f({\\pmb x}_{\\zeta}),{\\pmb x}-{\\pmb x}_{\\zeta}^{\\prime}\\right\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\leq\\epsilon+2\\zeta m L_{f}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Where n the last iequalty weue tefact that fal $\\pmb{x}_{\\zeta}^{\\prime}\\in\\Delta^{m,\\zeta}$ we have $\\left\\langle-\\nabla f(\\pmb{x}_{\\zeta}),\\pmb{x}_{\\zeta}^{\\prime}-\\pmb{x}_{\\zeta}\\right\\rangle\\leq$ $\\epsilon$ and $\\|\\nabla f(\\mathbf{x}_{\\zeta})\\|\\leq L_{f}$ \u53e3 ", "page_idx": 30}, {"type": "text", "text": "From stationarity to optimality. ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Lemma C.7 (Gradient Domination). Let a single-agent MDP with action-space $\\boldsymbol{\\mathcal{A}}$ and directlyparametrized policy $\\pmb{x}\\in\\Delta^{\\zeta}\\left(\\mathcal{A}\\right)^{\\left|S\\right|}$ . Then it holds that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\substack{x^{*}\\in\\Delta(A)^{|S|}}}V_{\\rho}(x^{\\star})-V_{\\rho}(x)\\le\\frac{1}{1-\\gamma}D_{\\operatorname*{m}}\\operatorname*{max}_{\\substack{x^{\\prime}\\in\\Delta^{\\zeta}(A)^{|S|}}}(x^{\\prime}-x)^{\\top}\\nabla_{x}V_{\\rho}(x)+\\frac{2D_{\\operatorname*{m}}\\zeta|\\mathcal{S}||A|L}{1-\\gamma}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof. The proof follows easily from Proposition C.1 and the gradient domination property [3, Lemma 4.1]. \u53e3 ", "page_idx": 30}, {"type": "text", "text": "C.3  Continuity of the maximizers ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "We begin this section by frstly introducing a proposition which we willeverage in Lemma C.8. ", "page_idx": 30}, {"type": "text", "text": "Proposition C.2. Consider the inequality of the form $\\alpha\\lambda^{2}\\leq\\beta\\lambda\\chi+\\gamma\\chi$ where $\\lambda$ and $\\chi$ are variables and $\\alpha,\\beta,\\gamma$ are coefficients. Under the constraints that $\\alpha,\\beta,\\gamma,\\lambda,\\chi\\geq0$ , there is no solution of the form $\\lambda\\leq c\\chi$ for any finite constant $c\\geq0$ ", "page_idx": 30}, {"type": "text", "text": "Proof. Solving the quadratic inequality for $\\lambda$ gives: ", "page_idx": 30}, {"type": "equation", "text": "$$\n0\\leq\\lambda\\leq\\frac{\\beta\\chi+\\sqrt{\\chi(4\\alpha\\gamma+\\beta^{2}\\chi)}}{2\\alpha}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We search for a positive constant c such that \u5165\u2264 Bx+\u221ax(4a+\u00b2x) , or equivalently, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\frac{1}{2\\alpha}\\left(\\beta+\\sqrt{\\frac{4\\alpha\\gamma}{\\chi}+\\beta^{2}}\\right)\\leq c.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "By observing that when $\\alpha,\\beta,\\gamma$ are all positive constants and as $\\chi\\rightarrow0$ \uff0c $c\\rightarrow\\infty$ , we conclude that no such finite constant $c$ exists. \u53e3 ", "page_idx": 30}, {"type": "text", "text": "We first define the maximizer for the regularized function $r(x)^{\\top}\\lambda-{\\textstyle\\frac{\\nu}{2}}\\left\\|\\lambda\\right\\|^{2}$ . Since the function is strongly-concave w.r.t. $\\lambda$ , the maximizing $\\lambda$ is unique. Specifically we denote ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\lambda^{\\star}(\\pmb{x}):=\\underset{\\lambda\\in\\Lambda(\\pmb{x})}{\\mathrm{argmax}}\\left\\lbrace\\pmb{r}(\\pmb{x})^{\\top}\\pmb{\\lambda}-\\frac{\\nu}{2}\\left\\|\\pmb{\\lambda}\\right\\|^{2}\\right\\rbrace.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Now we are ready to show an important lemma regarding to $\\lambda^{\\star}(x)$ ", "page_idx": 30}, {"type": "text", "text": "Lemma C.8 (Continuity of the max. of reg. functions). For any adversarial Markov game $\\Gamma$ $\\lambda^{\\star}(x)$ defined in (10) is $(1/2,L_{\\star})$ Holder continuous, specifically ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\left\\|\\lambda^{*}({\\boldsymbol x}_{1})-\\lambda^{*}({\\boldsymbol x}_{2})\\right\\|\\le L_{\\star}\\left\\|{\\boldsymbol x}_{1}-{\\boldsymbol x}_{2}\\right\\|^{1/2},}\\\\ {L_{\\star}:=\\frac{2(n)^{1/4}}{\\nu(1-\\gamma)^{3/2}}|{\\boldsymbol S}|^{1/2}\\left(\\sum_{k=1}^{n}|A_{k}|+|{\\boldsymbol B}|\\right)^{\\frac{3}{4}}.}\\end{array}\n$$where ", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof. Consider team policies, $\\mathbf{\\boldsymbol{x}}_{1},\\mathbf{\\boldsymbol{x}}_{2}$ . It holds true for the unique maximizers $\\lambda^{\\star}({\\pmb x}_{1}),\\lambda^{\\star}({\\pmb x}_{2})$ of the adversary's regularized value function that, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Big(r(x_{1})-\\nu\\lambda^{\\star}({\\pmb x}_{1})\\Big)^{\\top}(\\lambda_{1}-\\lambda^{\\star}({\\pmb x}_{1}))\\leq0,\\quad\\forall\\lambda_{1}\\in\\Lambda({\\pmb x}_{1});}\\\\ &{\\Big(r({\\pmb x}_{2})-\\nu\\lambda^{\\star}({\\pmb x}_{2})\\Big)^{\\top}(\\lambda_{2}-\\lambda^{\\star}({\\pmb x}_{2}))\\leq0,\\quad\\forall\\lambda_{2}\\in\\Lambda({\\pmb x}_{2}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $\\Lambda(x)$ is the set of feasible state-action visitation measures of the adversary given team's policy $\\textbf{\\em x}$ . To bound the distance between the two vectors $\\lambda^{\\star}({\\pmb x}_{1}),\\lambda^{\\star}({\\pmb x}_{2})$ , we observe that for any measure $\\overline{{\\pmb{\\lambda}}}\\in\\Lambda(\\pmb{x}_{1})\\cup\\Lambda(\\pmb{x}_{2})$ , there exist a measure $\\overline{{\\lambda}}_{1}\\in\\Lambda(x_{1})$ that shares the same adversary's policy $\\textit{\\textbf{y}}$ as ", "page_idx": 30}, {"type": "text", "text": ".\u4e091 $\\bar{\\lambda}$ It then folows from Lemma C.2 that $\\|{\\overline{{\\lambda}}}_{1}-{\\overline{{\\lambda}}}\\|\\leq L_{\\lambda}\\,\\|x_{1}-x_{2}\\|$ Thereforewe have for all ${\\overline{{\\lambda}}}\\in\\Lambda(x_{1})$ ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(r(x_{1})-\\nu\\lambda^{\\star}(x_{1})\\right)^{\\top}\\left(\\overline{{\\lambda}}-\\lambda^{\\star}(x_{1})\\right)=\\left(r(x_{1})-\\nu\\lambda^{\\star}(x_{1})\\right)^{\\top}\\left(\\overline{{\\lambda}}-\\overline{{\\lambda}}_{1}\\right)}\\\\ &{\\quad\\qquad\\qquad\\qquad\\qquad\\quad+\\left(r(x_{1})-\\nu\\lambda^{\\star}(x_{1})\\right)^{\\top}\\left(\\overline{{\\lambda}}_{1}-\\lambda^{\\star}(x_{1})\\right)}\\\\ &{\\quad\\qquad\\qquad\\qquad\\quad\\leq L_{\\lambda}\\sqrt{|{\\mathcal S}||{\\mathcal B}|}\\left(1+\\frac{\\nu}{1-\\gamma}\\right)\\|x_{1}-x_{2}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Where  the last inequality  follows from(11)and  the fact that $\\begin{array}{r l}{\\|r(\\pmb{x})-\\nu\\pmb{\\lambda}^{\\star}(\\pmb{x})\\|}&{{}\\leq}\\end{array}$ $\\begin{array}{r}{\\sqrt{|S||B|}\\left(1+\\frac{\\nu}{1-\\gamma}\\right)}\\end{array}$ for any $\\pmb{x}\\in\\mathcal{X}$ . Similarly, it also holds that for all ${\\overline{{\\lambda}}}\\in\\Lambda(x_{1})$ ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\left(r(x_{2})-\\nu\\lambda^{\\star}(x_{2})\\right)^{\\top}\\left(\\overline{{\\lambda}}-\\lambda^{\\star}(x_{2})\\right)\\leq L_{\\lambda}\\sqrt{|S||B|}\\left(1+\\frac{\\nu}{1-\\gamma}\\right)\\|x_{1}-x_{2}\\|.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Plugging in ${\\overline{{\\lambda}}}\\gets\\lambda^{\\star}(x_{2})$ and ${\\overline{{\\lambda}}}\\leftarrow\\lambda^{\\star}(x_{1})$ into (12) and (13) respectively ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(r(x_{1})-\\nu\\lambda^{\\star}(x_{1})\\right)^{\\top}(\\lambda^{\\star}(x_{2})-\\lambda^{\\star}(x_{1}))\\leq L_{\\lambda}\\sqrt{|S||\\mathcal{B}|}\\left(1+\\frac{\\nu}{1-\\gamma}\\right)\\|x_{1}-x_{2}\\|,}\\\\ &{\\left(r(x_{2})-\\nu\\lambda^{\\star}(x_{2})\\right)^{\\top}(\\lambda^{\\star}(x_{1})-\\lambda^{\\star}(x_{2}))\\leq L_{\\lambda}\\sqrt{|S||\\mathcal{B}|}\\left(1+\\frac{\\nu}{1-\\gamma}\\right)\\|x_{1}-x_{2}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Adding the two inequalities results in ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Big(\\left(r(\\pmb{x}_{1})-\\nu\\pmb{\\lambda}^{\\star}(\\pmb{x}_{1})\\right)-\\left(r(\\pmb{x}_{2})-\\nu\\pmb{\\lambda}^{\\star}(\\pmb{x}_{2})\\right)\\Big)^{\\top}\\left(\\pmb{\\lambda}^{\\star}(\\pmb{x}_{1})-\\pmb{\\lambda}^{\\star}(\\pmb{x}_{2})\\right)}\\\\ &{\\quad\\le2L_{\\lambda}\\sqrt{|\\cal S||\\cal B|}\\left(1+\\displaystyle\\frac{\\nu}{1-\\gamma}\\right)\\|\\pmb{x}_{1}-\\pmb{x}_{2}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "On the other hand, since $\\begin{array}{r}{r(x)^{\\top}\\lambda-\\frac{\\nu}{2}\\|\\lambda\\|^{2}}\\end{array}$ is $\\nu$ -strongly concave in $\\lambda$ , we have for all $\\lambda_{1}\\in\\Lambda(x_{1})$ ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(\\lambda_{1}-\\lambda^{\\star}(\\pmb{x}_{1}))^{\\top}\\Big((\\pmb{r}(\\pmb{x}_{1})-\\nu\\pmb{\\lambda}_{1})-(\\pmb{r}(\\pmb{x}_{1})-\\nu\\pmb{\\lambda}^{\\star}(\\pmb{x}_{1}))\\Big)+\\nu\\|\\pmb{\\lambda}_{1}-\\pmb{\\lambda}^{\\star}(\\pmb{x}_{1})\\|^{2}\\leq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We again use the fact that for every ${\\overline{{\\lambda}}}\\in{\\overline{{\\Lambda}}}$ it holds that there exists $\\overline{{\\lambda}}_{1}\\in\\Lambda(\\pmb{x}_{1})$ s.t.\u3002 $\\lVert\\overline{{\\lambda}}-\\overline{{\\lambda}}_{1}\\rVert\\leq$ $L_{\\lambda}\\|{\\pmb x}_{1}-{\\pmb x}_{2}\\|$ . Therefore it holds that for any ${\\overline{{\\lambda}}}\\in{\\overline{{\\Lambda}}}$ ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{0\\geq\\left(\\overline{{\\lambda}}_{1}+(\\overline{{\\lambda}}-\\overline{{\\lambda}})-\\lambda^{\\star}(x_{1})\\right)^{\\top}\\Big(\\left(r(x_{1})-\\nu\\overline{{\\lambda}}_{1}+(\\overline{{\\lambda}}-\\overline{{\\lambda}})\\right)-\\left(r(x_{1})-\\nu\\lambda^{\\star}(x_{1})\\right)\\Big)}\\\\ &{\\quad+\\nu\\|\\overline{{\\lambda}}_{1}+(\\overline{{\\lambda}}-\\overline{{\\lambda}})-\\lambda^{\\star}(x_{1})\\|^{2}}\\\\ &{=\\left((\\overline{{\\lambda}}-\\lambda^{\\star}(x_{1}))+(\\overline{{\\lambda}}_{1}-\\overline{{\\lambda}})\\right)^{\\top}\\Big(\\left(r(x_{1})-\\nu\\overline{{\\lambda_{1}}}+(\\overline{{\\lambda}}-\\overline{{\\lambda}})\\right)-\\left(r(x_{1})-\\nu\\lambda^{\\star}(x_{1})\\right)\\Big)}\\\\ &{\\quad+\\nu\\|\\overline{{\\lambda}}_{1}-\\overline{{\\lambda}}\\|^{2}+\\nu\\|\\overline{{\\lambda}}-\\lambda^{\\star}(x_{1})\\|^{2}+2\\nu\\left\\langle\\overline{{\\lambda}}_{1}-\\overline{{\\lambda}},\\overline{{\\lambda}}-\\lambda^{\\star}(x_{1})\\right\\rangle}\\\\ &{=\\left(\\overline{{\\lambda}}-\\lambda^{\\star}(x_{1})\\right)^{\\top}\\Big(\\left(r(x_{1})-\\nu\\overline{{\\lambda}}_{1}+(\\overline{{\\lambda}}-\\overline{{\\lambda}})\\right)-\\left(r(x_{1})-\\nu\\lambda^{\\star}(x_{1})\\right)\\Big)}\\\\ &{\\quad+\\nu\\|\\overline{{\\lambda}}_{1}-\\overline{{\\lambda}}\\|^{2}+\\nu\\|\\overline{{\\lambda}}-\\lambda^{\\star}(x_{1})\\|^{2}+2\\nu\\left\\langle\\overline{{\\lambda}}_{1}-\\overline{{\\lambda}},\\overline{{\\lambda}}-\\lambda^{\\star}(x_{1})\\right\\rangle}\\\\ &{\\quad+\\left(\\overline{{\\lambda}}_{1}-\\overline{{\\lambda}}\\right)^{\\top}\\Big(\\left(r(x_{1})-\\nu\\overline{{\\lambda}}_{1}+(\\overline{{\\lambda}}-\\overline{{\\lambda}})\\right)-\\left(r(x_{1})-\\nu\\lambda^{\\star}(x_{1})\\right)\\\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Rearranging, we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\overline{{\\lambda}}-\\lambda^{\\star}(x_{1})\\right)^{\\top}\\Big(\\left(r(x_{1})-\\nu\\overline{{\\lambda}}\\right)-\\left(r(x_{1})-\\nu\\lambda^{\\star}(x_{1})\\right)\\Big)+\\nu\\|\\overline{{\\lambda}}-\\lambda^{\\star}(x_{1})\\|^{2}}\\\\ &{\\leq\\underbrace{-\\nu\\left(\\overline{{\\lambda}}-\\lambda^{\\star}(x_{1})\\right)^{\\top}\\left(\\overline{{\\lambda}}-\\overline{{\\lambda}}_{1}\\right)}_{\\Omega_{1}}}\\\\ &{\\quad\\underbrace{-\\nu\\|\\overline{{\\lambda}}_{1}-\\overline{{\\lambda}}\\|^{2}-2\\nu\\left\\langle\\overline{{\\lambda}}_{1}-\\overline{{\\lambda}},\\overline{{\\lambda}}-\\lambda^{\\star}(x_{1})\\right\\rangle}_{\\Omega_{2}}}\\\\ &{\\quad\\underbrace{-(\\overline{{\\lambda}}_{1}-\\overline{{\\lambda}})^{\\top}\\Big(\\left(r(x_{1})-\\nu\\overline{{\\lambda}}_{1}+\\nu(\\overline{{\\lambda}}-\\overline{{\\lambda}})\\right)-\\left(r(x_{1})-\\nu\\lambda^{\\star}(x_{1})\\right)\\Big)}_{\\Omega_{3}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We bound $\\Omega_{1},\\Omega_{2}$ , and $\\Omega_{3}$ separately. ", "page_idx": 31}, {"type": "text", "text": "\u00b7 For $\\Omega_{1}$ , since $\\|\\overline{{\\lambda}}-\\overline{{\\lambda}}_{1}\\|\\leq L_{\\lambda}\\|x_{1}-x_{2}\\|$ , we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Omega_{1}\\leq\\left|\\nu\\left(\\overline{{\\lambda}}-\\lambda^{\\star}(\\pmb{x}_{1})\\right)^{\\top}\\left(\\overline{{\\lambda}}-\\overline{{\\lambda}}_{1}\\right)\\right|\\leq\\nu\\left\\|\\overline{{\\lambda}}-\\lambda^{\\star}(\\pmb{x}_{1})\\right\\|\\left\\|\\overline{{\\lambda}}-\\overline{{\\lambda}}_{1}\\right\\|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\displaystyle\\frac{2\\nu}{1-\\gamma}\\sqrt{|{\\cal{S}}||{\\cal{B}}|}\\cdot L_{\\lambda}\\|\\pmb{x}_{1}-\\pmb{x}_{2}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Wherewe use the fact that $\\|\\overline{{\\lambda}}-\\lambda^{\\star}({\\pmb x}_{1})\\|\\leq\\|\\overline{{\\lambda}}\\|+\\|\\pmb{\\lambda}^{\\star}({\\pmb x}_{1})\\|$ and $\\begin{array}{r}{\\|\\pmb{\\lambda}\\|\\leq\\|\\pmb{\\lambda}\\|_{1}=\\frac{1}{1-\\gamma}}\\end{array}$ ", "page_idx": 32}, {"type": "text", "text": "\u00b7For $\\Omega_{2}$ , only the second term is possibly non-negative, it holds that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\left|\\langle{\\overline{{\\lambda}}}_{1}-{\\overline{{\\lambda}}},{\\overline{{\\lambda}}}-\\lambda^{\\star}(x_{1})\\rangle\\right|\\leq L_{\\lambda}\\|x_{1}-x_{2}\\|{\\frac{2}{1-\\gamma}}{\\sqrt{|S||{\\mathcal{B}}|}}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Resulting in ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\Omega_{2}\\leq\\frac{4\\nu}{1-\\gamma}\\sqrt{|S||B|}L_{\\lambda}\\|\\pmb{x}_{1}-\\pmb{x}_{2}\\|.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "\u00b7 For $\\Omega_{3}$ ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\Omega_{3}\\le\\Big|(\\overline{{\\lambda}}_{1}-\\overline{{\\lambda}})^{\\top}\\Big(-\\nu\\overline{{\\lambda}}_{1}+\\nu\\lambda^{\\star}({\\pmb x}_{1})\\Big)\\Big|\\le\\frac{2\\nu}{1-\\gamma}\\sqrt{|S||\\beta|}L_{\\lambda}\\|{\\pmb x}_{1}-{\\pmb x}_{2}\\|.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Finally, by putting the bouds of $\\Omega_{1},\\Omega_{2},\\Omega_{3}$ and setting $\\begin{array}{r}{L^{\\prime}\\,:=\\,\\frac{8\\nu}{1-\\gamma}\\sqrt{|S||B|}L_{\\lambda}}\\end{array}$ , we have for all ${\\overline{{\\lambda}}}\\in{\\overline{{\\Lambda}}}$ ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left(\\overline{{\\lambda}}-\\lambda^{\\star}(x_{1})\\right)^{\\top}\\big(\\big(r(x_{1})-\\nu\\overline{{\\lambda}}\\big)-\\big(r(x_{1})-\\nu\\lambda^{\\star}(x_{1})\\big)\\big)+\\nu\\|\\overline{{\\lambda}}-\\lambda^{\\star}(x_{1})\\|^{2}\\leq L^{\\prime}\\|x_{1}-x_{2}\\|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Concluding, we plug ${\\overline{{\\lambda}}}\\gets\\lambda^{\\star}(x_{2})$ in (15), resulting ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{(\\lambda^{\\star}(x_{2})-\\lambda^{\\star}(x_{1}))^{\\top}\\left(\\left(r(x_{1})-\\nu\\lambda^{\\star}(x_{2})\\right)-\\left(r(x_{1})-\\nu\\lambda^{\\star}(x_{1})\\right)\\right)+\\nu\\|\\lambda^{\\star}(x_{2})-\\lambda^{\\star}(x_{1})\\|^{2}}\\\\ &{}&{\\leq L^{\\prime}\\|x_{1}-x_{2}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Adding (14) and (16), we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\colon L_{\\lambda}\\sqrt{|S||B|}\\left(1+\\frac{\\nu}{1-\\gamma}\\right)\\|x_{1}-x_{2}\\|+L^{\\prime}\\|x_{1}-x_{2}\\|}\\\\ &{\\ge(\\lambda^{\\star}(x_{2})-\\lambda^{\\star}(x_{1}))^{\\top}\\left((r(x_{1})-\\nu\\lambda^{\\star}(x_{1}))-(r(x_{2})-\\nu\\lambda^{\\star}(x_{2}))\\right)}\\\\ &{\\quad+\\left(\\lambda^{\\star}(x_{2})-\\lambda^{\\star}(x_{1})\\right)^{\\top}\\left(\\left(r(x_{1})-\\nu\\lambda^{\\star}(x_{2})\\right)-\\left(r(x_{1})-\\nu\\lambda^{\\star}(x_{1})\\right)\\right)+\\nu\\|\\lambda^{\\star}(x_{2})-\\lambda^{\\star}(x_{1})\\|^{2}}\\\\ &{=(\\lambda^{\\star}(x_{2})-\\lambda^{\\star}(x_{1}))^{\\top}\\left((r(x_{1})-\\nu\\lambda^{\\star}(x_{2}))-(r(x_{2})-\\nu\\lambda^{\\star}(x_{2}))\\right)+\\nu\\|\\lambda^{\\star}(x_{2})-\\lambda^{\\star}(x_{1})\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Rearranging we get ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nu\\|{\\boldsymbol{\\lambda}}^{\\star}(\\mathbf{\\boldsymbol{x}}_{2})-{\\boldsymbol{\\lambda}}^{\\star}(\\mathbf{\\boldsymbol{x}}_{1})\\|^{2}\\leq\\left({\\boldsymbol{\\lambda}}^{\\star}(\\mathbf{\\boldsymbol{x}}_{2})-{\\boldsymbol{\\lambda}}^{\\star}(\\mathbf{\\boldsymbol{x}}_{1})\\right)^{\\top}\\Big(\\left(r(\\mathbf{\\boldsymbol{x}}_{2})-\\nu{\\boldsymbol{\\lambda}}^{\\star}(\\mathbf{\\boldsymbol{x}}_{2})\\right)-\\left(r(\\mathbf{\\boldsymbol{x}}_{1})-\\nu{\\boldsymbol{\\lambda}}^{\\star}(\\mathbf{\\boldsymbol{x}}_{2})\\right)\\Big)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+{\\cal L}^{\\prime\\prime}\\|\\mathbf{\\boldsymbol{x}}_{1}-\\mathbf{\\boldsymbol{x}}_{2}\\|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq{\\cal L}_{r}\\|{\\boldsymbol{\\lambda}}^{\\star}(\\mathbf{\\boldsymbol{x}}_{2})-{\\boldsymbol{\\lambda}}^{\\star}(\\mathbf{\\boldsymbol{x}}_{1})\\|\\|\\mathbf{\\boldsymbol{x}}_{1}-\\mathbf{\\boldsymbol{x}}_{2}\\|+{\\cal L}^{\\prime\\prime}\\|\\mathbf{\\boldsymbol{x}}_{1}-\\mathbf{\\boldsymbol{x}}_{2}\\|,}\\\\ &{\\mathrm{\\here~}{\\cal L}^{\\prime\\prime}:=2\\left(1+\\frac{\\nu}{1-\\gamma}\\right)\\sqrt{|\\mathcal{S}||\\mathcal{B}|}L_{\\lambda}+{\\cal L}^{\\prime}=\\left(2+\\frac{10\\nu}{1-\\gamma}\\right)\\sqrt{|\\mathcal{S}||\\mathcal{B}|}L_{\\lambda}.}\\end{array}\n$$W ", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "By setting. $\\lambda\\ =\\ \\|\\lambda^{\\star}({\\pmb x}_{2})-\\lambda^{\\star}({\\pmb x}_{1})\\|$ and $\\chi\\ =\\ \\|\\pmb{x}_{1}-\\pmb{x}_{2}\\|$ , we consider the inequality $\\nu\\lambda^{2}~\\leq$ $L_{r}\\lambda\\chi+L^{\\prime\\prime}\\chi$ with coefficients $\\nu,L_{r},L^{\\prime\\prime}\\geq0$ and variables $\\lambda$ and $\\chi$ . Choosing $\\alpha\\gets\\nu$ $\\beta\\leftarrow L_{r}$ , and $\\gamma\\leftarrow L^{\\prime\\prime}$ , then Proposition C.2 implies that $\\lambda^{\\star}(x)$ is not Lipschitz-continuous w.r.t the team policy 2.Hence, we considerasolution of the form 0\u2264\u5165x+\u221ax4+\u00b2x) , where $\\textstyle{\\frac{1}{2}}-p\\geq0$ We choose $\\textstyle p={\\frac{1}{2}}$ since it yields the best convergence rate from Theorem B.1. Solving the above inequality with $\\bar{p^{\\prime}}=\\frac{1}{2}$ gives that ", "page_idx": 32}, {"type": "text", "text": "\u00b7if $\\chi=0$ , the inequality holds trivially; ", "page_idx": 33}, {"type": "text", "text": "\u00b7if $\\chi>0$ , we have ", "page_idx": 33}, {"type": "equation", "text": "$$\nc\\geq\\frac{\\beta\\chi+\\sqrt{\\chi(4\\alpha\\gamma+\\beta^{2}\\chi)}}{2\\alpha\\sqrt{\\chi}}=\\frac{\\beta\\sqrt{\\chi}}{2\\alpha}+\\frac{\\sqrt{4\\alpha\\gamma+\\beta^{2}\\chi}}{2\\alpha}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Since $\\chi=\\|\\pmb{x}_{1}-\\pmb{x}_{2}\\|\\leq\\sqrt{n|S|}\\mathrm{Diam}_{\\chi_{i}}=\\sqrt{2n|S|}$ by plugging in the coefficients, we have ", "page_idx": 33}, {"type": "equation", "text": "$$\nc=\\frac{1}{\\nu}\\sqrt{2L_{r}^{2}X+4\\nu L^{\\prime\\prime}}\\leq\\frac{2(n)^{1/4}}{\\nu(1-\\gamma)^{3/2}}|S|^{1/2}\\left(\\sum_{k=1}^{n}|\\mathcal{A}_{k}|+|\\mathcal{B}|\\right)^{\\frac{3}{4}}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "By setting $L_{\\star}=c$ , we conclude that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\|\\pmb{\\lambda}^{*}(\\pmb{x}_{1})-\\pmb{\\lambda}^{*}(\\pmb{x}_{2})\\|\\le L_{\\star}\\,\\|\\pmb{x}_{1}-\\pmb{x}_{2}\\|^{1/2}\\,.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "We are now ready to show that $\\Phi^{\\nu}(x)$ is weakly-smooth. ", "page_idx": 33}, {"type": "text", "text": "Theorem C.2 (Holder Continuous Max Value Func.). Let function $\\Phi^{\\nu}(x)$ bethemaximumfunction of the regularized value function, $\\begin{array}{r}{\\Phi^{\\nu}({\\pmb x}):=\\operatorname*{max}_{{\\pmb y}\\in\\mathcal{Y}}V_{\\rho}^{\\nu}({\\pmb x},{\\pmb y})}\\end{array}$ .It is the case that, ", "page_idx": 33}, {"type": "text", "text": "$\\Phi^{\\nu}$ is differentiable, \u3002 $\\nabla_{\\mathbf{\\alpha}}\\Phi^{\\nu}$ is $(1/2,\\ell_{1/2})$ -Holder continuous, i.e, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla_{x}\\Phi^{\\nu}(x)-\\nabla_{x}\\Phi^{\\nu}(\\pmb{x}^{\\prime})\\|\\leq\\ell_{1/2}\\left\\|\\pmb{x}-\\pmb{x}^{\\prime}\\right\\|^{1/2},}\\\\ {i t h\\;\\ell_{1/2}:=\\frac{30n^{\\frac{1}{4}}|S|^{\\frac{5}{4}}\\left(\\sum_{i}|A_{i}|+|B|\\right)^{2}}{\\nu\\operatorname*{min}_{s}\\rho(s)(1-\\gamma)^{\\frac{13}{2}}}.\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad}\\end{array}\n$$W ", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Proof. Since $\\Phi^{\\nu}(x)$ has a unique maximizer $\\lambda\\in\\Lambda(x)$ , by applying Danskin's Theorem [8] and the $\"1-1\"$ correspondence between $\\lambda$ and $\\textit{\\textbf{y}}$ (Theorem C.1), we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla_{x}\\Phi^{\\nu}({\\pmb x})-\\nabla_{x}\\Phi^{\\nu}({\\pmb x}^{\\prime})\\|=\\|\\nabla_{x}V_{\\rho}^{\\nu}({\\pmb x},y({\\pmb x}^{\\star}({\\pmb x})))-\\nabla_{x}V_{\\rho}^{\\nu}({\\pmb x}^{\\prime},y({\\pmb x}^{\\star}({\\pmb x}^{\\prime})))\\|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\ell_{\\nu}\\,(\\|{\\pmb x}-{\\pmb x}^{\\prime}\\|+\\|y({\\pmb\\lambda}^{\\star}({\\pmb x}))-y({\\pmb\\lambda}^{\\star}({\\pmb x}^{\\prime}))\\|)}\\\\ &{\\qquad\\qquad\\qquad\\leq\\ell_{\\nu}(\\|{\\pmb x}-{\\pmb x}^{\\prime}\\|+L_{\\lambda_{\\mathrm{inv}}}\\,\\|{\\pmb\\lambda}^{\\star}({\\pmb x})-{\\pmb\\lambda}^{\\star}({\\pmb x}^{\\prime})\\|)}\\\\ &{\\qquad\\qquad\\qquad\\leq\\ell_{\\nu}\\,\\Big((2n|S|)^{\\frac{1}{4}}+L_{\\lambda_{\\mathrm{inv}}}L_{\\star}\\Big)\\cdot\\|{\\pmb x}-{\\pmb x}^{\\prime}\\|^{\\frac{1}{2}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Where in the last inequality we used Lemma C.8 and the fact that $\\|{\\pmb x}-{\\pmb x}^{\\prime}\\|\\leq\\sqrt{2n|S|}$ . Plugging in the coefficients in Lemma C.8, Lemma C.3, and Lemma C.4, it yields that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\|\\nabla_{x}\\Phi^{\\nu}(x)-\\nabla_{x}\\Phi^{\\nu}(x^{\\prime})\\|\\leq\\frac{30n^{\\frac{1}{4}}|S|^{\\frac{5}{4}}\\left(\\sum_{i}|A_{i}|+|B|\\right)^{2}}{\\nu\\operatorname*{min}_{s}\\rho(s)(1-\\gamma)^{\\frac{13}{2}}}\\cdot\\|x-x^{\\prime}\\|^{\\frac{1}{2}}\\,.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "C.4  Analysis of ISPNG: Proof of Theorem 3.3 ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "In this part we show that Algorithm 1 converges to an $\\epsilon_{}$ -NE. Essentially, Algorithm 1 implements projected gradient descent on the regularized maximum function $\\Phi^{\\nu}:\\mathcal{X}\\to\\mathbb{R}$ with a stochastic $\\vartheta$ -inexact gradient oracle. Function $\\Phi^{\\nu}$ is Holder-continuous (see Theorem C.2) and as such we can invoke Theorem C.3 to prove convergence to an $\\epsilon$ -FOSP. ", "page_idx": 33}, {"type": "text", "text": "The inexactness of the gradient oracle, $\\vartheta$ , is the sum of two error sources: ", "page_idx": 33}, {"type": "text", "text": "1. the fact that the adversary can only approximately maximize the regularized value function $V_{\\rho}^{\\nu}(x,y)$ \u2014 the iteration and sample complexity of maximizing $V_{\\rho}^{\\nu}(x,\\cdot)$ is provided in TheoremC.4; ", "page_idx": 33}, {"type": "text", "text": "2. the exact estimation of $\\nabla\\Phi^{\\nu}$ requires estimation of the adversary's policy $\\textit{\\textbf{y}}$ -as we assume that the agents do not observe each other's actions this is impossible. Nevertheless, in Lemma C.9 it is proven that the inexactness error is bounded and controlled through the regularizer's coefficient $\\nu$ ", "page_idx": 34}, {"type": "text", "text": "After quantifying $\\vartheta$ as the function of the latter two terms, the optimality gap of Algorithm 2 and a term that scales with $O(\\nu)$ , we can tune the rest of the parameters accordingly. ", "page_idx": 34}, {"type": "text", "text": "The resulting $\\epsilon$ -FOSP, thanks to the gradient domination property (Lemma C.7), corresponds to an $\\epsilon$ NE. ", "page_idx": 34}, {"type": "text", "text": "Bounding the error of the inexact gradient. Following, we prove that the inexactness error of the gradient oracle is bounded by a function of the controllable parameter $\\nu$ ", "page_idx": 34}, {"type": "text", "text": "Lemma C.9 (Inexact gradient). Let $\\begin{array}{r}{V_{\\rho}^{\\nu}(\\pmb{x},\\pmb{y})\\,:=\\,V_{\\rho}(\\pmb{x},\\pmb{y})-\\,\\frac{\\nu}{2}\\sum_{s}\\|d^{\\pmb{x},\\pmb{y}}(s)\\pmb{y}(s)\\|^{2},}\\end{array}$ ${\\pmb y}^{\\nu}({\\pmb x})\\,:=$ $\\mathrm{argmax}_{y}\\{V_{\\rho}^{\\nu}(x,{\\pmb y})\\}$ , and $\\pmb{g}(\\pmb{x}):=\\nabla_{\\pmb{x}}\\dot{V}(\\pmb{x},\\pmb{y}_{\\nu}(\\pmb{x}))$ . Then, it holds that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\|g({\\boldsymbol{x}})-\\nabla_{\\boldsymbol{x}}V_{\\rho}^{\\nu}({\\boldsymbol{x}},{\\boldsymbol{y}})\\|_{2}\\leq\\frac{\\nu|S|^{\\frac{1}{2}}\\left(\\sum_{i=1}^{n}|A_{i}|+|B|\\right)}{(1-\\gamma)^{3}}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Proof. We observe that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|g(\\boldsymbol{x})-\\nabla_{\\boldsymbol{x}}V_{\\rho}^{\\nu}(\\boldsymbol{x},\\boldsymbol{y})\\|=\\left\\|\\nabla_{\\boldsymbol{x}}\\left(-\\frac{\\nu}{2}\\left\\|\\boldsymbol{\\lambda}(\\boldsymbol{y};\\boldsymbol{x})\\right\\|^{2}\\right)\\right\\|}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\nu\\left\\|\\boldsymbol{\\lambda}(\\boldsymbol{y};\\boldsymbol{x})\\right\\|\\|\\nabla_{\\boldsymbol{x}}\\boldsymbol{\\lambda}(\\boldsymbol{y};\\boldsymbol{x})\\|}\\\\ &{\\qquad\\qquad\\qquad\\leq\\frac{\\nu}{1-\\gamma}L_{\\lambda}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Where in the last inequality we use the fact that $\\begin{array}{r}{\\|\\lambda\\|\\leq\\frac{1}{1-\\gamma}}\\end{array}$ and $\\nabla_{x}\\lambda(y;x)\\le L_{\\lambda}$ ", "page_idx": 34}, {"type": "text", "text": "Learning an $\\mathbf{\\epsilon-NE}$ :We can now compile the intermediate statements to guarantee that ISPNG computes an $\\epsilon$ -NE for any desired accuracy $\\epsilon>0$ within a finite number of iterations and samples. ", "page_idx": 34}, {"type": "text", "text": "Theorem C.3. Consider an adversarial team Markov game $\\Gamma$ andAlgorithm $I$ ,ISPNG, with an outer-loop parameter tuning of: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bullet\\;T=\\frac{1061683200D_{\\mathrm{m}}^{5}n^{\\frac{1}{2}}|S|^{\\frac{9}{2}}\\left(\\sum_{i=1}^{n}|A_{i}|+|B|\\right)^{6}}{(1-\\gamma)^{24}(\\operatorname*{min}_{s}\\rho(s))^{2}\\varepsilon^{5}};}\\\\ &{\\bullet\\;\\eta_{x}=\\frac{(\\operatorname*{min}_{s}\\rho(s))^{2}(1-\\gamma)^{2}\\varepsilon^{3}}{33177600D_{\\mathrm{m}}^{3}n^{\\frac{1}{2}}|S|^{\\frac{9}{2}}\\left(\\sum_{i=1}^{n}|A_{i}|+|B|\\right)^{6}};}\\\\ &{\\bullet\\;\\zeta_{x}=\\frac{(1-\\gamma)^{3}\\epsilon}{6D_{\\mathrm{m}}|S|\\left(\\sum_{i=1}^{n}|A_{i}|+|B|\\right)^{\\frac{3}{2}}};}\\\\ &{\\bullet\\;M=\\frac{2034D_{\\mathrm{m}}^{3}|S|\\left(\\sum_{i=1}^{n}|A_{i}|+|B|\\right)^{\\frac{7}{2}}}{(1-\\gamma)^{10}(\\operatorname*{min}_{s}\\rho(s))^{4}\\varepsilon^{3}}\\operatorname*{max}\\left\\{\\frac{(1-\\gamma)^{4}(\\operatorname*{min}_{s}\\rho(s))^{4}\\left(\\sum_{i=1}^{n}|A_{i}|+|B|\\right)}{|S|},\\frac{9}{2}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Also, let the tuning of the inner-loop subroutine Algorithm 2 (V1s-REG-PG) be: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\bullet\\;\\nu=\\frac{(1-\\gamma)^{4}\\epsilon}{48D_{\\mathfrak{m}}|S|\\left(\\sum_{i=1}^{n}|A_{i}|+|B|\\right)};}&{}\\\\ {\\bullet\\;T_{\\boldsymbol{y}}=\\tilde{O}\\left(\\frac{D_{\\mathfrak{m}}^{5}\\left|S\\right|^{6}\\left(\\sum_{i=1}^{n}|A_{i}|+|B|\\right)^{9}}{(1-\\gamma)^{2}(\\operatorname*{min}_{\\mathfrak{m}}\\rho(s))^{4}\\epsilon^{5}}\\right);}\\\\ {\\bullet\\;\\eta_{y}=\\frac{(1-\\gamma)^{28}(\\operatorname*{min}_{\\mathfrak{m}}\\rho(s))^{4}\\epsilon^{4}}{978447237120D_{\\mathfrak{m}}^{4}|S|^{5}\\left(\\sum_{i=1}^{n}|A_{i}|+|B|\\right)^{8}};}\\\\ {\\bullet\\;\\zeta_{y}=\\frac{(1-\\gamma)^{15}(\\operatorname*{min}_{\\mathfrak{m}}\\rho(s))^{2}\\epsilon^{3}}{18432D_{\\mathfrak{m}}^{2}|S|^{2}\\left(\\sum_{i=1}^{n}|A_{i}|+|B|\\right)^{6}};}\\\\ {\\bullet\\;K=\\frac{19365101568D_{\\mathfrak{m}}^{4}|S|^{7}\\left(\\sum_{i=1}^{n}|A_{i}|+|B|\\right)^{12}}{(1-\\gamma)^{36}(\\operatorname*{min}_{\\mathfrak{m}}\\rho(s))^{4}\\epsilon^{6}};}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\cdot\\ H=\\frac{2}{1-\\gamma}\\log\\left(\\frac{2293235712D_{\\mathrm{m}}^{4}|S|^{4}(\\sum_{i=1}^{n}|A_{i}|+|B|)^{6}}{(1-\\gamma)^{22}(\\operatorname*{min}_{s}\\rho(s))^{2}\\epsilon^{4}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "It is the case that the output of the algorithm, $(x^{*},y^{*})$ will bean $\\epsilon$ -NE in expectation. Specifically, wehave ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[V_{\\rho}(\\pmb{x}^{*},\\pmb{y}^{*})-\\operatorname*{min}_{\\pmb{x}_{i}^{\\prime}\\in\\mathcal{X}_{i}}V_{\\rho}(\\pmb{x}_{i}^{\\prime},\\pmb{x}_{-i}^{*},\\pmb{y}^{*})\\right]\\leq\\epsilon,\\quad\\forall i\\in[n]\n$$", "text_format": "latex", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\operatorname*{max}_{\\pmb{y}^{\\prime}\\in\\mathcal{Y}}V_{\\rho}(\\pmb{x}^{*},\\pmb{y}^{\\prime})-V_{\\rho}(\\pmb{x}^{*},\\pmb{y}^{*})\\right]\\leq\\epsilon.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Proof. Let $\\boldsymbol{x}^{*},\\boldsymbol{y}^{*}$ be the final output of the algorithm, from Lemma C.7, we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[V_{\\rho}(\\boldsymbol{x}^{*},\\boldsymbol{y}^{*})-\\underset{\\boldsymbol{x}_{i}^{\\prime}\\in\\mathcal{X}_{i}}{\\mathrm{min}}\\;V_{\\rho}(\\boldsymbol{x}_{i}^{\\prime},\\boldsymbol{x}_{-i}^{*},\\boldsymbol{y}^{*})\\right]}\\\\ &{\\leq\\frac{1}{1-\\gamma}D_{\\mathrm{m}}\\mathbb{E}\\left[\\underset{\\boldsymbol{x}_{i}^{\\prime}}{\\mathrm{max}}\\left(-\\nabla V_{\\rho}(\\boldsymbol{x}^{*},\\boldsymbol{y}^{*})\\right)^{\\top}(\\boldsymbol{x}_{i}^{\\prime}-\\boldsymbol{x}_{i}^{*})\\right]+\\frac{2D_{\\mathrm{m}}\\zeta\\left\\vert\\mathcal{S}\\right\\vert\\left(\\sum_{i=1}^{n}\\left\\vert\\boldsymbol{A}_{i}\\right\\vert+\\left\\vert\\mathcal{B}\\right\\vert\\right)L}{1-\\gamma}}\\\\ &{\\leq\\frac{1}{1-\\gamma}D_{\\mathrm{m}}\\mathbb{E}\\left[\\underset{\\boldsymbol{x}_{i}^{\\prime}}{\\mathrm{max}}\\left(-\\nabla V_{\\rho}^{\\nu}(\\boldsymbol{x}^{*},\\boldsymbol{y}^{*})\\right)^{\\top}(\\boldsymbol{x}_{i}^{\\prime}-\\boldsymbol{x}_{i}^{*})\\right]+\\frac{\\nu}{1-\\gamma}L_{\\lambda}\\mathrm{Diam}_{\\boldsymbol{x}_{i}}}\\\\ &{\\quad+\\,\\frac{2D_{\\mathrm{m}}\\zeta\\left\\vert\\mathcal{S}\\right\\vert\\left(\\sum_{i=1}^{n}\\left\\vert\\mathcal{A}_{i}\\right\\vert+\\left\\vert\\mathcal{B}\\right\\vert\\right)L}{1-\\gamma}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Where (17) follows from Lemma C.9. As for the first term of (17), let us define ${\\pmb y}^{\\star}({\\pmb x})\\ =$ $\\mathrm{argmax}_{\\pmb{y}\\in\\mathcal{Y}}\\,V_{\\rho}^{\\nu}(\\pmb{x},\\pmb{y})$ and $\\pmb{x}^{+}=\\mathrm{Proj}_{\\mathcal{X}}\\left(\\pmb{x}^{t^{*}-1}-\\eta_{x}\\nabla V_{\\rho}(\\pmb{x}^{t^{*}-1},\\pmb{y}^{\\star}(\\pmb{x}^{t^{*}-1}))\\right)$ .Then it holdsthat, ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\operatorname*{max}\\left\\{\\left(-\\nabla V_{\\rho}^{\\nu}(x^{*},y^{*})\\right)^{\\top}(x_{t}^{\\prime}-x_{t}^{*})\\right\\}\\right]}\\\\ &{=\\mathbb{E}\\left[\\operatorname*{max}\\left\\{\\left(-V_{\\rho}^{\\nu}(x_{t}^{*},x_{t-1}^{*})y^{*}(x^{*})\\right)^{\\top}(x_{t}^{\\prime}-x_{t}^{*})\\right.}\\\\ &{\\qquad\\qquad\\left.+\\left.\\left(\\nabla V_{\\rho}^{\\nu}(x_{t}^{*},x_{t-1}^{*})y^{*}(x^{*})\\right)-\\nabla V_{\\rho}^{\\nu}(x^{*},y^{*})\\right)^{\\top}(x_{t}^{\\prime}-x_{t}^{*})\\right\\}\\right]}\\\\ &{\\leq\\mathbb{E}\\left[\\operatorname*{max}\\left\\{\\left(-\\nabla V_{\\rho}^{\\nu}(x_{t}^{*},x_{t-1}^{*})y^{*}(x^{*})\\right)^{\\top}(x_{t}^{\\prime}-x_{t}^{*})\\right\\}\\right]+L_{\\rho}\\mathbb{E}\\left[\\left\\Vert x_{t}^{*}-x_{t}^{*}\\right\\Vert\\right.}\\\\ &{\\quad+\\left.\\mathbb{E}\\left[\\left\\Vert\\nabla V_{\\rho}^{\\nu}(x_{t}^{*},x_{t-1}^{*})y^{*}(x^{*})\\right)-\\nabla V_{\\rho}^{\\nu}(x^{*},y^{*})\\right\\Vert\\right]\\cdot\\mathbb{D}\\operatorname*{lim}_{\\Sigma_{t}}}\\\\ &{\\leq\\mathbb{E}\\left[\\operatorname*{max}\\left\\{\\left(-\\nabla V_{\\rho}^{\\nu}(x_{t}^{*},x_{t-1}^{*})y^{*}(x^{*})\\right)^{\\top}(x_{t}^{\\prime}-x_{t}^{*})\\right\\}\\right]+L_{\\rho}\\mathbb{E}\\left[\\left\\Vert x_{t}^{*}-x_{t}^{*}\\right\\Vert\\right]}\\\\ &{\\quad+\\left.\\ell_{\\nu}\\left\\{\\left[\\left\\Vert x^{*}-x^{*}\\right\\Vert\\right]+\\left\\Vert\\mathbb{E}^{\\nu}(x^{*})-y^{*}\\right\\Vert\\right\\}\\right]\\cdot\\mathrm{Dim}_{\\Sigma_{t}}}\\\\ &{\\leq\\mathbb{E}\\left[\\operatorname*{max}\\left\\{\\left(-\\nabla V_{\\rho}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Where ", "page_idx": 35}, {"type": "text", "text": "\u00b7 Equation (18) is due to $\\left\\|\\nabla V_{\\rho}^{\\nu}(x,y)\\right\\|\\leq L_{\\nu}$ \u00b7 Equation (19) follows from the fact that $V_{\\rho}^{\\nu}(x,y)$ is $\\ell_{\\nu}$ -smooth. ", "page_idx": 35}, {"type": "text", "text": "By choosing parameters specified above and combining Corollaries B.1 and B.2, Theorem C.4, Lemma C.12, and Claim B.1, we have the desired result. On the other hand, since ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\underset{y^{\\prime}\\in\\mathcal{Y}}{\\operatorname*{max}}V_{\\rho}(\\pmb{x}^{*},\\pmb{y}^{\\prime})-V_{\\rho}(\\pmb{x}^{*},\\pmb{y}^{*})\\right]\\leq\\mathbb{E}\\left[\\underset{y^{\\prime}\\in\\mathcal{Y}}{\\operatorname*{max}}V_{\\rho}^{\\nu}(\\pmb{x}^{*},\\pmb{y}^{\\prime})-V_{\\rho}^{\\nu}(\\pmb{x}^{*},\\pmb{y}^{*})\\right]+\\frac{\\nu}{(1-\\gamma)^{2}}}\\\\ &{\\phantom{\\leq}=\\mathbb{E}\\left[V_{\\rho}^{\\nu}(\\pmb{x}^{*},\\pmb{y}^{\\star}(\\pmb{x}^{*}))-V_{\\rho}^{\\nu}(\\pmb{x}^{*},\\pmb{y}^{*})\\right]+\\frac{\\nu}{(1-\\gamma)^{2}}}\\\\ &{\\phantom{\\leq}\\leq L_{\\nu}E\\left[\\|\\pmb{y}^{\\star}(\\pmb{x}^{*})-\\pmb{y}^{*}\\|\\right]+\\frac{\\nu}{(1-\\gamma)^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Where in (20) we usethefat that $\\begin{array}{r}{\\left\\|\\lambda\\right\\|^{2}\\leq\\frac{1}{(1-\\gamma)^{2}}}\\end{array}$ . Combining Theorem C.4, Lemma C.12 and choosing parameters specified above gives the desired result. \u53e3 ", "page_idx": 36}, {"type": "text", "text": "C.5  Visitation-Regularized Policy Gradient Analysis ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "In this section, we consider the direct parameterization for the policy of the adversary. For any policy $\\pmb{y}\\in\\mathcal{V}$ , for anystate $s\\in S$ and any action $b\\in\\mathcal{B}$ ,wehave ", "page_idx": 36}, {"type": "equation", "text": "$$\ny(b|s)=y_{s,b}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Where $y_{s,b}$ denotes $(s,b)^{t h}$ entry of the policy vector $\\textit{\\textbf{y}}$ In this section, we mainly focus on solving the following policy optimization problem: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{y\\in y^{\\zeta}}V_{\\rho}^{\\nu}(\\pmb{x},\\pmb{y}):=\\operatorname*{max}_{y\\in y^{\\zeta}}\\left\\{\\pmb{r}(\\pmb{x})^{\\top}\\pmb{\\lambda}(\\pmb{y};\\pmb{x})-\\frac{\\nu}{2}\\|\\pmb{\\lambda}(\\pmb{y};\\pmb{x})\\|^{2}\\right\\}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Where $\\lambda(y;x)$ is the state-action visitation measure under policy $\\textit{\\textbf{y}}$ as in Definition 2.4. ${\\pmb r}({\\pmb x})$ is the induced pay-off vector for the adversary when the team is playing according to strategy $\\textbf{\\em x}$ and $\\nu$ is the regularization coefficient. Then by policy gradient theorem [113], denote $F^{\\nu}(\\lambda({\\pmb y};{\\pmb x}))=V_{\\rho}^{\\nu}({\\pmb x},{\\pmb y})$ wehave ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{y}F^{\\nu}(\\lambda(y;x))=[\\nabla_{y}\\lambda(y;x)]^{\\top}\\left(r(x)-\\nu\\lambda(y;x)\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\mathbb{E}_{\\rho,y}\\left[\\displaystyle\\sum_{h=0}^{\\infty}\\gamma^{h}\\cdot\\left(r(x)-\\nu\\lambda(y;x)\\right)_{s_{h},b_{h}}\\cdot\\left(\\sum_{h^{\\prime}=0}^{h}\\nabla_{y}\\log y(b_{h}^{\\prime}|s_{h^{\\prime}})\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Given the direct parameterization, we can show the following lemmas: ", "page_idx": 36}, {"type": "text", "text": "Lemma C.10. For any adversarial policy $\\textit{\\textbf{y}}$ and state-action pair $(s,b)$ , we have $\\begin{array}{r}{||\\nabla_{y}\\log y(b|s)||\\leq\\frac{1}{\\zeta}}\\end{array}$ \uff0c $\\begin{array}{r}{||\\nabla_{y}^{2}\\log y(b|s)||\\leq\\frac{1}{\\zeta^{2}}}\\end{array}$ and $\\begin{array}{r}{||\\nabla_{y}F^{\\nu}(\\lambda({\\pmb y};{\\pmb x}))||\\leq\\frac{1}{(1-\\gamma)^{2}\\zeta}+\\frac{\\nu}{(1-\\gamma)^{3}\\zeta}}\\end{array}$ for any fixed $\\textbf{\\em x}$ ", "page_idx": 36}, {"type": "text", "text": "Proof. By direct parameterization $y(b|s)=y_{s,b}$ , we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\|\\nabla_{y}\\log y(b|s)\\|=\\|\\nabla_{y}\\log y_{s,b}\\|=\\left\\|\\frac{1}{y_{s,b}}e_{s,b}\\right\\|\\leq\\frac{1}{\\zeta}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Where $e_{s,b}$ denotes the standard basis for the $(s,b)^{t h}$ entry. Similarly, we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\|\\nabla_{\\pmb{y}}^{2}\\log y(b|s)\\|=\\left\\|\\mathrm{diag}\\left(\\frac{1}{y_{s,b}^{2}}\\right)\\right\\|\\leq\\frac{1}{\\zeta^{2}}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Where $\\mathrm{diag(\\cdot)}$ denotes the standard diagonal matrix. For the policy gradient, we show that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\|\\nabla_{y}F^{\\nu}(\\lambda(y;\\mathbf{x}))\\|=\\Big\\|[\\nabla_{y}\\lambda(y;\\mathbf{x})]^{\\top}\\left(r(x)-\\nu\\lambda(y;\\mathbf{x})\\right)\\Big\\|}}\\\\ &{=\\bigg\\|\\mathbb{E}\\Big[\\displaystyle\\sum_{h=0}^{\\infty}\\gamma^{h}\\cdot\\left(r(x)_{s_{h}b_{h}}-\\nu\\lambda(y;\\mathbf{x})_{s_{h}b_{h}}\\right)\\cdot\\left(\\displaystyle\\sum_{h^{\\prime}=0}^{h}\\nabla_{y}\\log y(b_{h^{\\prime}}|s_{h^{\\prime}})\\right)\\Big]\\bigg\\|}\\\\ &{\\leq\\displaystyle\\sum_{h=0}^{\\infty}\\gamma^{h}\\cdot\\left(1+\\displaystyle\\frac{\\nu}{1-\\gamma}\\right)\\cdot\\left(h+1\\right)\\cdot\\displaystyle\\frac{1}{\\zeta}}\\\\ &{\\leq\\displaystyle\\frac{1}{(1-\\gamma)^{2}\\zeta}+\\frac{\\nu}{(1-\\gamma)^{3}\\zeta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Where (24) is due to (22). ", "page_idx": 36}, {"type": "text", "text": "Before we proceed to show the convergence towards global optimality for (21), we first define the notion of Moreau envelope and the proximal point. ", "page_idx": 37}, {"type": "text", "text": "Definition C.3 (Moreau Envelope and Proximal Point). For any $\\pmb{y}\\in\\mathcal{V}^{\\zeta}$ weuse $F_{1/\\beta}^{\\nu}(\\lambda(y;x))t o$ denotetheMoreauenvelopeoffunction $F^{\\nu}(\\lambda(y;x))$ Suchthat ", "page_idx": 37}, {"type": "equation", "text": "$$\nF_{1/\\beta}^{\\nu}(\\pmb{\\lambda}(\\pmb{y};\\pmb{x})):=\\operatorname*{max}_{z\\in\\mathcal{Y}^{\\zeta}}\\left\\{F^{\\nu}(\\pmb{\\lambda}(z;\\pmb{x}))-\\frac{\\beta}{2}\\left\\|\\pmb{\\lambda}(z;\\pmb{x})-\\pmb{\\lambda}(\\pmb{y};\\pmb{x})\\right\\|^{2}\\right\\}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Moreover, we define the proximal point $\\hat{y}_{1/\\beta}$ of Moreau envelope as following: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\hat{y}:=\\operatorname*{argmax}_{z\\in\\mathcal{Y}^{\\zeta}}\\left\\{F^{\\nu}(\\pmb{\\lambda}(y;\\pmb{x}))-\\frac{\\beta}{2}\\left\\|\\pmb{\\lambda}(z;\\pmb{x})-\\pmb{\\lambda}(y;\\pmb{x})\\right\\|^{2}\\right\\}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Now we proceed to show the following lemma: ", "page_idx": 37}, {"type": "text", "text": "Lemma C.11. When running Algorithm 2, for any $t\\geq0$ ,wehave ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\Vert y^{(t+1)}-\\hat{y}^{(t)}\\right\\Vert^{2}\\bigg|y^{(t)}\\right]\\leq(1-\\eta_{y}\\beta)\\left\\Vert y^{(t)}-\\hat{y}^{(t)}\\right\\Vert^{2}+2(1-\\eta_{y}\\beta)\\eta_{y}(1+\\eta_{y}\\ell_{\\nu})\\cdot\\mathcal{C}_{3}\\gamma^{H}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\eta_{y}^{2}\\mathbb{E}\\left[\\left\\Vert\\nabla_{y}F^{\\nu}(\\lambda(y^{(t)};x))-\\hat{g}_{y}^{(t)})\\right\\Vert^{2}\\bigg|y^{(t)}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Where C3 = SB] ", "page_idx": 37}, {"type": "text", "text": "Proof. ", "text_level": 1, "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\Big[\\big\\|y^{t+1}-\\bar{y}^{(t)}\\big\\|^{2}\\Big|y^{(t)}\\Big]}\\\\ &{=\\mathbb{E}\\Big[\\big\\|\\mathbf{P}^{\\mathrm{roj}}y(\\mathbf{\\theta}^{(t)}\\!+\\!\\eta_{\\mathcal{b}}\\bar{y}_{\\mathcal{b}}^{(t)})\\!-\\!\\mathrm{Proj}_{\\mathcal{b}}\\big((1\\!-\\!\\eta_{\\mathcal{b}}\\beta)\\bar{y}^{(t)}\\!+\\!\\eta_{\\mathcal{b}}\\beta\\mathbf{\\varphi}^{(t)}\\!+\\!\\eta_{\\mathcal{b}}\\nabla_{\\mathbf{y}}F^{\\nu}\\big(\\!\\lambda(\\mathbf{\\theta}^{(t)}\\!;\\mathbf{x})\\big)\\big)\\big\\|^{2}\\Big|y^{(t)}\\Big]}\\\\ &{\\le\\!\\mathbb{E}\\Big[\\big\\|\\mathbf{\\theta}^{(t)}\\!+\\!\\eta_{\\mathcal{b}}\\bar{y}_{\\mathcal{b}}^{(t)}\\!-\\!\\big((1\\!-\\!\\eta_{\\mathcal{b}}\\beta)\\bar{y}^{(t)}\\!+\\!\\eta_{\\mathcal{b}}\\beta\\mathbf{\\varphi}^{(t)}\\!+\\!\\eta_{\\mathcal{b}}\\nabla_{\\mathbf{y}}F^{\\nu}\\big(\\!\\lambda(\\mathbf{\\theta}^{(t)}\\!;\\mathbf{x})\\big)\\big)\\big\\|^{2}\\Big|y^{(t)}\\Big]}\\\\ &{=\\!\\mathbb{E}\\Big[\\big\\|(1\\!-\\!\\eta_{\\mathcal{b}}\\beta)(\\mathbf{\\varphi}^{(t)}\\!-\\!\\bar{y}^{(t)})\\!+\\!\\eta_{\\mathcal{b}}\\big(\\nabla_{\\mathbf{y}}F^{\\nu}\\big(\\!\\lambda(\\mathbf{\\theta}^{(t)}\\!;\\mathbf{x})\\big)\\!-\\!\\bar{y}_{\\mathcal{b}}^{(t)}\\big)\\big\\|^{2}\\Big|y^{(t)}\\Big]}\\\\ &{=\\!\\mathbb{E}\\Big[\\big\\|(1\\!-\\!\\eta_{\\mathcal{b}}\\beta)(\\mathbf{\\varphi}^{(t)}\\!-\\!\\bar{y}^{(t)})\\!+\\!\\eta_{\\mathcal{b}}\\big(\\nabla_{\\mathbf{y}}F^{\\nu}\\big(\\!\\lambda(\\mathbf{\\theta}^{(t)}\\!;\\mathbf{x})\\big)\\!-\\!\\nabla_{\\mathbf{y}}F^{\\nu}\\big(\\!\\lambda(\\mathbf{\\theta}^{(t)}\\!;\\mathbf{x})\\big) \n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Where (25) follows from Lemma 3.2 in [34]. For the first part of (26), we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|(1-\\eta_{y}\\beta)({\\boldsymbol y}^{(t)}-\\hat{{\\boldsymbol y}}^{(t)})+\\eta_{y}\\left(\\nabla_{y}F^{\\nu}(\\lambda(\\hat{{\\boldsymbol y}}^{(t)};{\\boldsymbol x}))-\\nabla_{y}F^{\\nu}(\\lambda({\\boldsymbol y}^{(t)};{\\boldsymbol x}))\\right)\\right\\|^{2}}\\\\ &{=(1-\\eta_{y}\\beta)^{2}\\left\\|{\\boldsymbol y}^{(t)}-\\hat{{\\boldsymbol y}}^{(t)}\\right\\|^{2}+\\eta_{y}^{2}\\left\\|\\nabla_{y}F^{\\nu}(\\lambda(\\hat{{\\boldsymbol y}}^{(t)};{\\boldsymbol x}))-\\nabla_{y}F^{\\nu}(\\lambda({\\boldsymbol y}^{(t)};{\\boldsymbol x}))\\right\\|^{2}}\\\\ &{\\quad+\\,2(1-\\eta_{y}\\beta)\\eta_{y}\\left\\langle{\\boldsymbol y}^{(t)}-\\hat{{\\boldsymbol y}}^{(t)},\\nabla_{y}F^{\\nu}(\\lambda(\\hat{{\\boldsymbol y}}^{(t)};{\\boldsymbol x}))-\\nabla_{y}F^{\\nu}(\\lambda({\\boldsymbol y}^{(t)};{\\boldsymbol x}))\\right\\rangle}\\\\ &{\\leq(1-\\eta_{y}\\beta)^{2}\\left\\|{\\boldsymbol y}^{(t)}-\\hat{{\\boldsymbol y}}^{(t)}\\right\\|^{2}+\\eta_{y}^{2}\\ell_{y}^{2}\\left\\|{\\boldsymbol y}^{(t)}-\\hat{{\\boldsymbol y}}^{(t)}\\right\\|^{2}+2(1-\\eta_{y}\\beta)\\eta_{y}\\ell_{\\nu}\\left\\|{\\boldsymbol y}^{(t)}-\\hat{{\\boldsymbol y}}^{(t)}\\right\\|^{2}}\\\\ &{=(1-\\eta_{y}\\beta)\\left(1-\\eta_{y}\\beta+2\\eta_{y}\\ell_{\\nu}+\\frac{\\eta_{y}^{2}\\ell_{\\nu}^{2}}{1-\\eta_{y}\\beta}\\right)\\left\\|{\\boldsymbol y}^{(t)}-\\hat{{\\boldsymbol y}}^{(t)}\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Where (27)fllows from Lemma C.4. By setting $\\eta_{y},\\beta$ such that $\\begin{array}{r}{2\\eta_{y}\\ell_{\\nu}\\le\\frac{\\eta_{y}\\beta}{2}}\\end{array}$ 2\uff0cand \uff0c $\\begin{array}{r}{\\frac{\\eta_{y}^{2}\\ell_{\\nu}^{2}}{1-\\eta_{y}\\beta}\\leq\\frac{\\eta_{y}\\beta}{2}}\\end{array}$ wehave ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|(1-\\eta_{y}\\beta)({\\pmb y}^{(t)}-\\hat{{\\pmb y}}^{(t)})+\\eta_{y}(\\nabla_{\\pmb y}F^{\\nu}(\\lambda(\\hat{{\\pmb y}}^{(t)};{\\pmb x}))-\\nabla_{\\pmb y}F^{\\nu}(\\lambda({\\pmb y}^{(t)};{\\pmb x})))\\right\\|^{2}}\\\\ &{\\leq(1-\\eta_{y}\\beta)\\left\\|{\\pmb y}^{(t)}-\\hat{{\\pmb y}}^{(t)}\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "For the third part in (26), we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\quad2(1-\\eta_{y}\\beta)\\eta_{y}\\mathbb{E}\\Big[\\big\\langle(y^{(t)}-\\hat{y}^{(t)})-\\eta_{y}\\big(\\nabla_{y}F^{\\nu}\\big(\\lambda(\\hat{y}^{(t)};\\mathbf{x})\\big)-\\nabla_{y}F^{\\nu}\\big(\\lambda(y^{(t)};\\mathbf{x})\\big)\\big),\\nabla_{y}F^{\\nu}\\big(\\lambda(y^{(t)};\\mathbf{x})\\big)-\\hat{g}_{y}^{(t)}\\big)\\Big\\rangle\\Big\\vert y^{(t)}\\Big]}&{}\\\\ &{\\leq2(1-\\eta_{y}\\beta)\\eta_{y}\\Big\\Vert(y^{(t)}-\\hat{y}^{(t)})-\\eta_{y}\\Big(\\nabla_{y}F^{\\nu}\\big(\\lambda(\\hat{y}^{(t)};\\mathbf{x})\\big)-\\nabla_{y}F^{\\nu}\\big(\\lambda(y^{(t)};\\mathbf{x})\\big)\\big)\\Big\\Vert\\cdot\\mathbb{E}\\Big[\\big\\|\\nabla_{y}F^{\\nu}\\big(\\lambda(y^{(t)};\\mathbf{x})\\big)-\\hat{g}_{y}^{(t)}\\big)\\big\\|\\Big\\vert y^{(t)}\\Big]}&{\\leq}\\\\ &{\\leq2(1-\\eta_{y}\\beta)\\eta_{y}\\big(1+\\eta_{y}\\ell_{\\nu}\\big)\\cdot\\left\\|y^{(t)}-\\hat{y}^{(t)}\\right\\|\\cdot\\mathbb{E}\\Big[\\big\\|\\nabla_{y}F^{\\nu}\\big(\\lambda(y^{(t)};\\mathbf{x})\\big)-\\hat{g}_{y}^{(t)}\\big)\\big\\|\\Big]}&{\\quad{\\scriptstyle(2)}}\\\\ &{\\leq2(1-\\eta_{y}\\beta)\\eta_{y}\\big(1+\\eta_{y}\\ell_{\\nu}\\big)\\cdot\\mathcal{C}_{3}\\gamma^{H}.}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Where ", "page_idx": 38}, {"type": "text", "text": "\u00b7 C3=1B;   \n\u00b7 (29) follows from Lemma C.4;   \n\u00b7 (30) is due to Lemma C.14. ", "page_idx": 38}, {"type": "text", "text": "Combine (26), (28), and (30), we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\Vert y^{t+1}-\\hat{y}^{(t)}\\right\\Vert^{2}\\bigg|y^{(t)}\\right]\\leq(1-\\eta_{y}\\beta)\\left\\Vert y^{(t)}-\\hat{y}^{(t)}\\right\\Vert^{2}+2(1-\\eta_{y}\\beta)\\eta_{y}(1+\\eta_{y}\\ell_{\\nu})\\cdot\\mathcal{C}_{3}\\gamma^{H}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\eta_{y}^{2}\\mathbb{E}\\left[\\left\\Vert\\nabla_{y}F^{\\nu}(\\lambda(y^{(t)};x))-\\hat{g}_{y}^{(t)})\\right\\Vert^{2}\\bigg|y^{(t)}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "We then show the result for convergence to optimality for (21). ", "page_idx": 38}, {"type": "text", "text": "Theorem C.4. By setting ny = 10e,L2 and $\\begin{array}{r}{H=\\frac{2\\log(1/\\nu\\epsilon)}{1-\\gamma}}\\end{array}$ 2log(1/ve). Afer runing Algorithm 2 for \u5165inv   \n$\\begin{array}{r}{T=\\mathcal{O}\\left(\\frac{\\ell_{\\nu}L_{\\lambda_{\\mathrm{inv}}}^{2}}{\\nu}\\log\\left(\\frac{1}{\\epsilon}\\right)+\\frac{\\ell_{\\nu}\\sigma^{2}L_{\\lambda_{\\mathrm{inv}}}^{4}}{\\nu^{2}\\epsilon}\\log\\left(\\frac{1}{\\epsilon}\\right)\\right)}\\end{array}$ iterations, we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[F^{\\nu}(\\pmb{\\lambda}(\\pmb{y}_{\\zeta}^{\\star};\\pmb{x}))-F^{\\nu}(\\pmb{\\lambda}(\\pmb{y}^{(T)};\\pmb{x}))\\right]\\leq\\epsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Where $\\pmb{y}_{\\zeta}^{\\star}$ is the unique maximizer for the optimization problem (21). ", "page_idx": 38}, {"type": "text", "text": "Proof. From Theorem 1 in [43], by setting $\\beta\\,=\\,4\\ell_{\\nu}$ \uff0c $\\alpha\\,\\leq\\,2\\eta_{y}\\ell_{\\nu}$ , and $\\begin{array}{r}{\\eta_{y}\\,\\le\\,\\frac{2}{9\\ell_{\\nu}}}\\end{array}$ . Then for any $z\\in\\mathcal{V}^{\\zeta}$ , we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\left[F_{1/\\beta}^{\\nu}(\\lambda(y^{(t+1)};x))\\right]}\\\\ &{\\geq\\mathbb{E}\\left[F^{\\nu}(\\lambda(z;x))-(1+s)\\frac{\\beta}{2}\\left\\|\\hat{y}^{(t)}-y^{(t+1)}\\right\\|^{2}-\\left(1+\\frac{1}{s}\\right)\\frac{\\beta}{2}\\left\\|\\hat{y}^{(t)}-z\\right\\|^{2}\\right]}\\\\ &{\\geq\\mathbb{E}\\left[F^{\\nu}(\\lambda(z;x))-(1+s)(1-\\eta_{y}\\beta)\\frac{\\beta}{2}\\left\\|y^{(t)}-\\hat{y}^{(t)}\\right\\|^{2}\\right]}\\\\ &{\\quad-\\left(1+\\frac{1}{s}\\right)\\frac{\\beta}{2}\\mathbb{E}[\\left\\|\\hat{y}^{(t)}-z\\right\\|^{2}]-2\\eta_{y}(1+s)(1-\\eta_{y}\\beta)(1+\\eta_{y}\\ell_{\\nu})\\cdot\\mathcal{C}_{3}\\gamma^{H}}\\\\ &{\\quad-\\left(1+s)\\frac{\\beta}{2}\\eta_{y}^{2}\\mathbb{E}\\left[\\left\\|\\nabla_{y}F^{\\nu}(\\lambda(y^{(t)};x))-\\hat{y}_{y}^{(t)}\\right\\|^{2}\\right]y^{(t)}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Where (31) is due to Lemma C.11. By setting $\\begin{array}{r}{s\\,=\\,\\frac{\\eta_{y}\\beta}{2}}\\end{array}$ , we have $\\begin{array}{r}{(1+s)(1-\\eta_{y}\\beta)\\le1-\\frac{\\eta_{y}\\beta}{2}}\\end{array}$ $1+s\\leq2$ and $\\begin{array}{r}{1+\\frac{1}{s}\\leq\\frac{3}{\\eta_{y}\\beta}}\\end{array}$ m . From Theorem 1 in [43], we get ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\left[F_{1/\\beta}^{\\nu}(\\lambda(y^{(t+1)};\\pmb{x}))\\right]}\\\\ &{\\ge(1-\\alpha)\\mathbb{E}\\left[F_{1/\\beta}^{\\nu}(\\lambda(y^{(t)};\\pmb{x}))\\right]+\\alpha F^{\\nu}(\\lambda(y_{\\zeta}^{\\star};\\pmb{x}))-\\beta\\eta_{y}^{2}\\mathbb{E}\\left[\\left\\|\\nabla_{y}F^{\\nu}(\\lambda(y^{(t)};\\pmb{x}))-\\hat{g}_{y}^{(t)})\\right\\|^{2}\\Big|y^{(t)}\\right]}\\\\ &{\\quad-\\left(\\frac{3L_{\\lambda_{\\mathrm{inv}}}^{2}\\alpha^{2}}{2\\eta_{y}}-\\frac{(1-\\alpha)\\alpha\\nu}{2}\\right)\\mathbb{E}\\left[\\left\\|\\lambda(\\hat{y}^{(t)};\\pmb{x})-\\lambda(y_{\\zeta}^{\\star};\\pmb{x})\\right\\|^{2}\\right]-2\\eta_{y}(1-\\alpha)(1+\\eta_{y}\\ell_{\\nu})\\cdot\\mathcal{C}_{3}\\gamma^{H}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Define $\\Lambda_{t}:=\\mathbb{E}\\left[F^{\\nu}(\\pmb{\\lambda}(\\pmb{y}_{\\zeta}^{\\star};\\pmb{x}))-F_{1/\\beta}^{\\nu}(\\pmb{\\lambda}(\\pmb{y}^{(t)};\\pmb{x}))\\right]$ , by setting $\\begin{array}{r}{\\left(\\frac{3L_{\\lambda_{\\mathrm{inv}}}^{2}\\alpha^{2}}{2\\eta_{y}}-\\frac{(1-\\alpha)\\alpha\\nu}{2}\\right)\\leq0}\\end{array}$ we have ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{\\d}_{t+1}\\leq(1-\\alpha)\\Lambda_{t}+\\beta\\eta_{y}^{2}\\mathbb{E}\\left[\\left\\|\\nabla_{y}F^{\\nu}(\\lambda(y^{(t)};x))-\\hat{g}_{y}^{(t)})\\right\\|^{2}\\Big|y^{(t)}\\right]+2\\eta_{y}(1-\\alpha)(1+\\eta_{y}\\ell_{\\nu})\\cdot\\mathcal{C}_{3}\\gamma^{H}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Summing over $T$ iterations, and denote $\\mathbb{E}\\left[\\left\\|\\nabla_{y}F^{\\nu}(\\lambda({\\pmb y}^{(t)};{\\pmb x}))-\\hat{{\\pmb g}}_{{\\pmb y}}^{(t)})\\right\\|^{2}\\left|{\\pmb y}^{(t)}\\right]=\\sigma^{2}$ we get ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\Lambda_{T}\\leq(1-\\alpha)^{T}\\Lambda_{0}+\\frac{4\\ell_{\\nu}\\eta_{y}^{2}}{\\alpha}\\sigma^{2}+\\frac{2\\eta_{y}(1-\\alpha)(1+\\eta_{y}\\ell_{\\nu})}{\\alpha}\\cdot\\mathcal{C}_{3}\\gamma^{H}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Bysetn H=2\u03b1m2 , and e 10e oL, fter ", "page_idx": 39}, {"type": "equation", "text": "$$\nT=\\mathcal{O}\\left(\\frac{\\ell_{\\nu}L_{\\lambda_{\\mathrm{inv}}}^{2}}{\\nu}\\log\\left(\\frac{1}{\\epsilon}\\right)+\\frac{\\ell_{\\nu}\\sigma^{2}L_{\\lambda_{\\mathrm{inv}}}^{4}}{\\nu^{2}\\epsilon}\\log\\left(\\frac{1}{\\epsilon}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "iterations, we get Ar \u2264 e. Where o\u00b2 = + C2 2H, C1 = (-c, C2 = (1-)6(2 , C3 = $\\sqrt{|S||B|}\\frac{6H}{(1\\!-\\!\\gamma)^{3}\\zeta}$ Since $F^{\\nu}(\\lambda(y;x))$ ismooh with respet tothe state-action vstation measure $\\lambda(y;x)$ We have ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{A}_{T}=\\mathbb{E}\\left[F^{\\nu}\\big(\\lambda(\\pmb{y}_{\\zeta}^{\\star};\\pmb{x})\\big)-F_{1/\\beta}^{\\nu}\\big(\\lambda(\\pmb{y}^{(T)};\\pmb{x})\\big)\\right]}\\\\ &{\\quad=\\mathbb{E}\\left[F^{\\nu}\\big(\\lambda(\\pmb{y}_{\\zeta}^{\\star};\\pmb{x})\\big)-\\operatorname*{max}_{z\\in\\mathcal{Y}}\\left\\{F^{\\nu}\\big(\\lambda(z;\\pmb{x})\\big)-\\frac{\\beta}{2}\\left\\|\\lambda(z;\\pmb{x})-\\lambda(\\pmb{y}^{(T)};\\pmb{x})\\right\\|^{2}\\right\\}\\right]}\\\\ &{\\quad\\geq\\mathbb{E}\\left[F^{\\nu}\\big(\\lambda(\\pmb{y}_{\\zeta}^{\\star};\\pmb{x})\\big)-F^{\\nu}\\big(\\lambda(\\pmb{y}^{(T)};\\pmb{x})\\big)+\\frac{\\beta}{2}\\left\\|\\lambda(\\pmb{y}^{(T)};\\pmb{x})-\\lambda(\\pmb{y}^{(T)};\\pmb{x})\\right\\|^{2}\\right]}\\\\ &{\\quad=\\mathbb{E}\\left[F^{\\nu}\\big(\\lambda(\\pmb{y}_{\\zeta}^{\\star};\\pmb{x})\\big)-F^{\\nu}\\big(\\lambda(\\pmb{y}^{(T)};\\pmb{x})\\big)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Therefore ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[F^{\\nu}(\\lambda(\\pmb{y}_{\\zeta}^{\\star};\\pmb{x}))-F^{\\nu}(\\lambda(\\pmb{y}^{(T)};\\pmb{x}))\\right]\\leq\\Lambda_{T}\\leq\\epsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Define $\\pmb{y}^{\\star}\\ \\in\\ \\mathcal{Y}$ such that $\\begin{array}{r}{\\pmb{y}^{\\star}\\ =\\ \\operatorname*{argmax}_{\\pmb{y}\\in\\mathcal{Y}}\\left\\{\\pmb{r}(\\pmb{x})^{\\top}\\pmb{\\lambda}(\\pmb{y};\\pmb{x})-\\frac{\\nu}{2}\\|\\pmb{\\lambda}(\\pmb{y};\\pmb{x})\\|^{2}\\right\\}}\\end{array}$ We bound the distance between the the optimal $\\boldsymbol{y}^{\\star}$ and $\\pmb{y}^{(T)}$ from Algorithm 2. ", "page_idx": 39}, {"type": "text", "text": "Lemma C.12. For any $\\pmb{y}\\in\\mathcal{V}^{\\zeta}$ , if $\\begin{array}{r}{\\mathbb{E}\\left[F^{\\nu}\\big(\\pmb{\\lambda}(\\pmb{y}_{\\zeta}^{\\star};\\pmb{x})\\big)-F^{\\nu}\\big(\\pmb{\\lambda}(\\pmb{y};\\pmb{x})\\big)\\right]\\leq\\epsilon.}\\end{array}$ then we have ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\Vert y^{\\star}-y\\Vert\\right]\\leq L_{\\lambda_{\\mathrm{inv}}}\\left(\\sqrt{\\frac{8L_{\\lambda}|\\beta|\\zeta}{(1-\\gamma)\\nu}}+\\sqrt{\\frac{2\\epsilon}{\\nu}}\\right).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Proof. Since $F^{\\nu}(\\lambda(y;x))$ is $\\nu$ -strongly concave with respect to $\\lambda(y;x)$ , we have ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F^{\\nu}(\\lambda(y_{\\zeta}^{\\star};\\pmb{x}))\\geq F^{\\nu}(\\lambda(y;\\pmb{x}))+\\left\\langle\\nabla_{\\lambda}F^{\\nu}(\\lambda(y_{\\zeta}^{\\star};\\pmb{x})),\\lambda(y_{\\zeta}^{\\star};\\pmb{x})-\\lambda(y;\\pmb{x})\\right\\rangle}\\\\ &{\\quad\\quad\\quad\\quad+\\displaystyle\\frac{\\nu}{2}\\left\\|\\lambda(y_{\\zeta}^{\\star};\\pmb{x})-\\lambda(y;\\pmb{x})\\right\\|^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad=F^{\\nu}(\\lambda(y;\\pmb{x}))+\\displaystyle\\frac{\\nu}{2}\\left\\|\\lambda(y_{\\zeta}^{\\star};\\pmb{x})-\\lambda(y;\\pmb{x})\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Where (32) holds because $\\lambda(y_{\\zeta}^{\\star};x)$ is the optimal solution for $F^{\\nu}(\\lambda(y;x))$ for any $\\b{y}\\in\\b{\\mathcal{V}}$ . Therefore ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\|\\lambda(\\pmb{y}_{\\zeta}^{\\star};\\pmb{x})-\\lambda(\\pmb{y};\\pmb{x})\\right\\|\\right]\\leq\\sqrt{\\frac{2}{\\nu}\\cdot\\mathbb{E}\\left[F^{\\nu}(\\lambda(\\pmb{y}_{\\zeta}^{\\star};\\pmb{x}))-F^{\\nu}(\\lambda(\\pmb{y};\\pmb{x}))\\right]}\\leq\\sqrt{\\frac{2\\epsilon}{\\nu}}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "From  the  definition  of $\\lambda(y_{\\zeta}^{\\star};x)$ \uff0cit holds that for all $\\begin{array}{r l r}{{\\pmb y}_{\\zeta}}&{{}\\in}&{{\\mathcal D}^{\\zeta}}\\end{array}$ , we have $\\left\\langle-\\nabla_{\\lambda}F^{\\nu}(\\lambda(y_{\\zeta}^{\\star};x)),\\lambda(y_{\\zeta}^{\\star};x)-\\dot{\\lambda}(y_{\\zeta};x)\\right\\rangle\\ \\leq\\ 0$ . Combine with Lemma C.6 and consider $\\pmb{y}^{\\star}\\in\\mathcal{V}$ , we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{\\langle-\\nabla_{\\lambda}F^{\\nu}(\\lambda(y_{\\zeta}^{\\star};x)),\\lambda(y_{\\zeta}^{\\star};x)-\\lambda(y^{\\star};x)\\rangle}&&{}\\\\ &{=\\left\\langle-\\nabla_{\\lambda}F^{\\nu}(\\lambda(y_{\\zeta}^{\\star};x)),\\lambda(y_{\\zeta}^{\\star};x)-\\lambda(y_{\\zeta};x)+\\lambda(y_{\\zeta};x)-\\lambda(y^{\\star};x)\\right\\rangle}&&{}\\\\ &{=\\left\\langle-\\nabla_{\\lambda}F^{\\nu}(\\lambda(y_{\\zeta}^{\\star};x)),\\lambda(y_{\\zeta}^{\\star};x)-\\lambda(y_{\\zeta};x)\\right\\rangle+\\left\\langle-\\nabla_{\\lambda}F^{\\nu}(\\lambda(y_{\\zeta}^{\\star};x)),\\lambda(y_{\\zeta};x)-\\lambda(y^{\\star};x)\\right\\rangle}&&{}\\\\ &{\\leq\\left\\langle-\\nabla_{\\lambda}F^{\\nu}(\\lambda(y_{\\zeta}^{\\star};x)),\\lambda(y_{\\zeta};x)-\\lambda(y^{\\star};x)\\right\\rangle}&&{}\\\\ &{\\leq\\left\\langle-\\nabla_{\\lambda}F^{\\nu}(\\lambda(y_{\\zeta}^{\\star};x)),\\lambda(y_{\\zeta};x)-\\lambda(y^{\\star};x)\\right\\rangle}&&{}\\\\ &{\\leq\\left\\|\\nabla_{\\lambda}F^{\\nu}(\\lambda(y_{\\zeta}^{\\star};x))\\right\\|\\cdot\\left\\|\\lambda(y_{\\zeta};x)-\\lambda(y^{\\star};x)\\right\\|}&&{}\\\\ &{\\leq\\frac{4L_{\\lambda}|B|\\zeta}{1-\\gamma}.}&&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Where ", "page_idx": 40}, {"type": "text", "text": "\u00b7 in (33) $\\pmb{y}_{\\zeta}\\in\\mathcal{V}^{\\zeta}$ is chosen such that $\\|\\pmb{y}^{\\star}-\\pmb{y}_{\\zeta}\\|\\leq2\\zeta|B|$ according to C.6;   \n\u00b7 (34) holds because $\\begin{array}{r}{\\|\\nabla_{\\lambda}F^{\\nu}(\\lambda)\\|\\leq\\frac{2}{1-\\gamma}}\\end{array}$ and $\\lambda(y;x)$ .is $L_{\\lambda}$ -continuous. ", "page_idx": 40}, {"type": "text", "text": "Since $F^{\\nu}(\\lambda)$ is $\\nu$ -strongly concave w.r.t $\\lambda$ , we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\frac\\nu2\\left\\|\\lambda(\\boldsymbol{y}_{\\zeta}^{\\star};\\boldsymbol{x})-\\lambda(\\boldsymbol{y}^{\\star};\\boldsymbol{x})\\right\\|^{2}}\\\\ &{\\le F^{\\nu}(\\lambda(\\boldsymbol{y}_{\\zeta}^{\\star};\\boldsymbol{x}))-F^{\\nu}(\\lambda(\\boldsymbol{y}^{\\star};\\boldsymbol{x})+\\left\\langle\\nabla_{\\lambda}F^{\\nu}((\\lambda(\\boldsymbol{y}_{\\zeta}^{\\star};\\boldsymbol{x})),\\lambda(\\boldsymbol{y}^{\\star};\\boldsymbol{x})-\\lambda(\\boldsymbol{y}_{\\zeta}^{\\star};\\boldsymbol{x})\\right\\rangle}\\\\ &{\\le\\displaystyle\\frac{4L_{\\lambda}|B|\\zeta}{1-\\gamma}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Thus we conclude that ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\|y^{\\star}-y\\|\\right]\\leq L_{\\lambda_{\\mathrm{inv}}}\\mathbb{E}\\left[\\|\\lambda(y^{\\star};\\pmb{x})-\\lambda(y;\\pmb{x})\\|\\right]}\\\\ &{\\qquad\\qquad\\leq L_{\\lambda_{\\mathrm{inv}}}\\left(\\|\\lambda(y^{\\star};\\pmb{x})-\\lambda(y_{\\zeta}^{\\star};\\pmb{x})\\|+\\mathbb{E}\\left[\\|\\lambda(y_{\\zeta}^{\\star};\\pmb{x})-\\lambda(y;\\pmb{x})\\|\\right]\\right)}\\\\ &{\\qquad\\qquad\\leq L_{\\lambda_{\\mathrm{inv}}}\\left(\\sqrt{\\frac{8L_{\\lambda}|\\mathcal{B}|\\zeta}{(1-\\gamma)\\nu}}+\\sqrt{\\frac{2\\epsilon}{\\nu}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "C.6 Regarding the Gradient and Visitation Estimators ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "In this subsection we will quantify the bias and variance of the gradient and state-action visitation estimators used in Algorithms 1 and 2. In particular, REINFORCE: ", "page_idx": 41}, {"type": "text", "text": "\u00b7 the gradient estimator for team agents is implemented by sampling a trajectory with horizon length, $H$ , that is drawn from a geometric distribution for the team, and \u00b7 while, the state-action visitation estimators that the adversary uses come from sampled trajectories of a fixed horizon length $H$ ", "page_idx": 41}, {"type": "text", "text": "In the former case, the estimator is unbiased while in the second case the bias decays exponentially in $H$ ", "page_idx": 41}, {"type": "text", "text": "C.6.1 REINFORCE for Vanilla Policy Gradient ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "In the present work, the team agents only need to implement a batch version of REINFORCE [104]. That is, they get estimates $\\begin{array}{r}{\\bar{\\pmb g}_{k}^{(t)}=\\frac{1}{M}\\sum_{j=1}^{\\dot{M}}\\tilde{\\pmb g}_{k}^{(t)}}\\end{array}$ where: ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\tilde{\\pmb{g}}_{i,j}^{(t)}=\\sum_{h_{j}=1}^{H_{j}}r_{i}^{(h_{j})}\\sum_{h=1}^{H_{j}}\\nabla\\log x_{i}\\left(a^{(h_{j})}|s^{(h_{j})}\\right),\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "with each $H_{j}$ is a random variable following a geometric distribution with parameter $(1-\\gamma)$ ", "page_idx": 41}, {"type": "text", "text": "Although the authors of [27] use $\\zeta$ -greedy parametrization in order to bound the variance of the estimator, policies drawn from the $\\zeta$ -truncated simplex imply the same inequality needed to bound the variance. Hence, we invoke the corresponding lemma. ", "page_idx": 41}, {"type": "text", "text": "Lemma C.13 ([27, Lemma 2]). When Equation (REINFORCE) is implemented with $H$ following a geometric distribution with a parameter $1-\\gamma$ ,and agent $k$ selects policies from the $\\zeta$ -truncated simplex on each state, it is the case that the gradient estimates satisfy: ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\left[\\hat{\\pmb{g}}_{k}^{(t)}\\right]-\\nabla_{\\pmb{x}_{k}}V_{\\rho}(\\pmb{x}^{t},\\pmb{y}^{t})=0;}\\\\ &{\\mathbb{E}\\left[\\left\\|\\hat{\\pmb{g}}_{k}^{(t)}-\\nabla_{\\pmb{x}_{k}}V_{\\rho}(\\pmb{x}^{t},\\pmb{y}^{t})\\right\\|^{2}\\right]\\leq24\\frac{|\\mathcal{A}_{k}^{2}|}{\\zeta(1-\\gamma)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "C.6.2  Gradient Estimation for Visitation-Regularized Policy Gradient ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "In this subsection we will describe (i) a state-action visitation estimator with bounded bias and variance and (ii) a gradient estimator of the regularized value function whose bias and variance are alsobounded. ", "page_idx": 41}, {"type": "text", "text": "Bounding the variance of the a gradient estimator with a deterministic choice of $H$ wassignificantly less demanding than doing so with a randomized choice. This comes at the cost with a non-zero bias that nevertheless decays exponentially in $H$ . For any policy of the adversary $\\pmb{y}\\in\\mathcal{V}$ ,weintroducethe $H$ -horizon truncated state-action visitation measure ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\lambda_{H,s,b}(\\pmb{y};\\pmb{x})_{s,b}:=\\sum_{h=0}^{H-1}\\gamma^{h}\\mathbb{P}(s_{h}=s,b_{h}=b|\\pmb{y},s_{0}\\sim\\pmb{\\rho}).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Where $\\lambda_{H,s,b}(\\pmb{y};\\pmb{x})$ denotes the $(s,b)^{t h}$ entry of $\\lambda_{H}({\\pmb y};{\\pmb x})$ . For any reward vector $\\pmb{r}$ , we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\left[\\nabla_{y}\\lambda_{H}(y;x)\\right]^{\\top}r=\\mathbb{E}\\left[\\sum_{h=0}^{H-1}\\gamma^{h}\\cdot r(s_{h},b_{h})\\cdot\\left(\\sum_{h^{\\prime}=0}^{h}\\nabla_{y}\\log y(b_{h^{\\prime}}|s_{h^{\\prime}})\\right)\\bigg|y,s_{0}\\sim\\rho\\right].\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "C.6.3  Controlling the Estimation Bias and Variance ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "In this subsection we will present a detailed analysis regarding estimators defined in Definition 2.6, Definition 2.7, and the ones used in Algorithm 2. Particularly, in Lemma C.14 we bound the bias of aforementioned estimators. This bias is inevitable for our analysis due to the fact we are sampling trajectories of a finite length $H$ over an infinite horizon. Proceeding to Lemma C.16 and Lemma C.17, we bound the variance of the state-action distribution measure estimator $\\hat{\\mathbf{\\lambda}}(t)$ and the gradient estimator $\\hat{g}_{y}^{(t)}$ W.r.t their biased means. Finally, in Lemma C.18, we bound the distance between the gradient estimator $\\hat{g}_{y}^{(t)}$ and the actual gradient $\\nabla_{\\pmb{y}}F^{\\nu}(\\pmb{\\lambda}(\\pmb{y}^{(t)};\\pmb{x}))$ ", "page_idx": 41}, {"type": "text", "text": "", "page_idx": 42}, {"type": "text", "text": "Lemma C.14 (Bounded Bias of the Estimators). For any adversary's policy $\\pmb{y}\\in\\mathcal{V}$ .We let $\\tau=$ $\\left(s_{0},b_{0},s_{1},b_{1},\\cdots\\,,s_{H-1},b_{H-1}\\right)$ be an $H$ -length trajectory  sampled  from $\\textit{\\textbf{y}}$ , then we have $\\mathbb{E}_{\\tau\\sim\\pmb{y}}\\left[\\tilde{\\pmb{\\lambda}}(\\tau|\\pmb{y})\\right]=\\pmb{\\lambda}_{H}(\\pmb{y};\\pmb{x})$ and $\\mathbb{E}_{\\tau\\sim\\pmb{y}}\\left[\\tilde{\\pmb{g}}(\\tau|\\pmb{y};\\pmb{r})\\right]=\\left[\\nabla_{\\pmb{y}}\\pmb{\\lambda}_{H}(\\pmb{y};\\pmb{x})\\right]^{\\top}\\pmb{r}.$ This implies that in Algo$\\mathrm{rithm}\\;2,\\mathbb{E}\\left[\\hat{\\pmb{\\lambda}}^{(t)}\\right]=\\pmb{\\lambda}_{H}(\\pmb{y}^{(t)};\\pmb{x})$ and $\\mathbb{E}\\left[\\hat{\\pmb{g}}_{\\pmb{y}}^{(t)}\\right]=\\left[\\nabla_{\\pmb{y}}\\pmb{\\lambda}_{H}(\\pmb{y}^{(t)};\\pmb{x})\\right]^{\\top}\\pmb{r}^{(t)}$ Moreover, we have: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\mathbb{E}\\left[\\hat{g}_{y}^{(t)}\\right]-\\nabla_{y}F^{\\nu}(\\lambda({\\pmb y}^{(t)};{\\pmb x}))\\right\\|\\leq\\left(\\frac{H+1}{(1-\\gamma)\\zeta}+\\frac{\\nu H+\\nu+1}{(1-\\gamma)^{2}\\zeta}+\\frac{\\nu}{(1-\\gamma)^{3}\\zeta}\\right)\\cdot\\gamma^{H}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Proof. From the definition, we have ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\tau\\sim\\boldsymbol{y}}\\left[\\tilde{\\lambda}(\\tau|\\boldsymbol{y})\\right]=\\lambda_{H}(\\boldsymbol{y};\\boldsymbol{x}),\\quad\\mathbb{E}_{\\tau\\sim\\boldsymbol{y}}\\left[\\tilde{g}(\\tau|\\boldsymbol{y};\\boldsymbol{r})\\right]=\\left[\\nabla_{\\boldsymbol{y}}\\lambda_{H}(\\boldsymbol{y};\\boldsymbol{x})\\right]^{\\top}\\boldsymbol{r}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Therefore, ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\hat{\\pmb{\\lambda}}^{(t)}\\right]=\\pmb{\\lambda}_{H}(\\pmb{y}^{(t)};\\pmb{x}),\\quad\\mathbb{E}\\left[\\hat{\\pmb{g}}_{\\pmb{y}}^{(t)}\\right]=\\left[\\nabla_{\\pmb{y}}\\pmb{\\lambda}_{H}(\\pmb{y}^{(t)};\\pmb{x})\\right]^{\\top}\\pmb{r}^{(t)}.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Then it holds that ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\left|\\mathbb{E}\\left[\\hat{\\lambda}^{(t)}\\right]-\\lambda(y^{(t)};x)\\right|\\right|=\\left\\|\\lambda_{H}(y^{(t)};x)-\\lambda(y^{(t)};x)\\right\\|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\left\\|\\displaystyle\\sum_{h=H}^{\\infty}\\gamma^{h}\\cdot\\mathbb{P}(s_{h}=s,b_{h}=b|y^{(t)},s_{0}\\sim\\rho)\\cdot e_{s_{h},b_{h}}\\right\\|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\gamma^{H}\\cdot\\displaystyle\\sum_{h=0}^{\\infty}(\\gamma^{h}\\cdot1)}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\frac{\\gamma^{H}}{1-\\gamma}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Similarly, we have ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|\\mathbb{E}\\left[\\hat{g}_{y}^{(t)}\\right]-\\nabla_{y}F^{\\nu}\\big(\\lambda(y^{(t)};x)\\big)\\right\\|}\\\\ &{=\\bigg\\|\\Big[\\nabla_{y}\\lambda_{H}\\big(y^{(t)};x\\big)\\Big]^{\\top}r^{(t)}-\\nabla_{y}F^{\\nu}\\big(\\lambda(y^{(t)};x)\\big)\\bigg\\|}\\\\ &{=\\bigg\\|\\Big[\\nabla_{y}\\lambda_{H}\\big(y^{(t)};x\\big)\\Big]^{\\top}\\nabla_{\\lambda}F^{\\nu}\\big(\\lambda_{H}\\big(y^{(t)};x\\big)\\big)-\\Big[\\nabla_{y}\\lambda(y^{(t)};x)\\Big]^{\\top}\\nabla_{\\lambda}F^{\\nu}\\big(\\lambda(y^{(t)};x)\\big)\\bigg\\|}\\\\ &{\\leq\\bigg\\|\\Big[\\nabla_{y}\\lambda_{H}\\big(y^{(t)};x\\big)\\Big]^{\\top}\\left(\\nabla_{\\lambda}F^{\\nu}\\big(\\lambda_{H}\\big(y^{(t)};x\\big)\\big)-\\nabla_{\\lambda}F\\big(\\lambda(y^{(t)};x)\\big)\\right)\\bigg\\|}\\\\ &{\\qquad+\\left\\|\\Big(\\Big[\\nabla_{y}\\lambda_{H}\\big(y^{(t)};x\\big)\\Big]^{\\top}-\\Big[\\nabla_{y}\\lambda(y^{(t)};x\\big)\\Big]^{\\top}\\Big)\\nabla_{\\lambda}F^{\\nu}\\big(\\lambda(y^{(t)};x)\\big)\\right\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "For the first part in the above inequality, we have ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\left[\\nabla_{v}\\partial_{v}\\bar{p}^{(i)};\\bar{u},\\partial_{v}(v^{(i)};u)\\right]^{*}\\right|\\left(\\nabla_{v}X^{\\{v^{(i)}\\}}(\\Delta t)\\bar{p}^{(i)};\\bar{u}\\right)\\right|}\\\\ &{=\\left|\\displaystyle\\sum_{k=0}^{\\lfloor\\frac{N}{2}\\gamma-k\\rfloor}\\left\\{\\frac{\\bar{p}^{(i)}(\\Delta t)\\bar{\\theta}^{(i)}(v^{(i)};u)}{\\bar{\\theta}\\Delta_{\\Delta_{t}\\Delta_{t}}}-\\frac{\\bar{p}^{(i)}(\\Delta{\\gamma}^{\\{v^{(i)}\\}}(\\Delta{\\gamma}^{\\theta};u))}{\\bar{\\theta}\\Delta_{\\Delta_{t}\\Delta_{t}}}\\right\\}\\left(\\frac{\\bar{\\gamma}}{\\displaystyle\\sum_{s=0}^{k}\\Psi_{p}\\log(\\theta_{\\Delta_{t}\\Delta_{t}})}\\right)\\right|}\\\\ &{\\le\\displaystyle\\sum_{k=0}^{\\infty}\\gamma^{k}\\gamma^{k}\\left|\\nabla_{v}F^{*}(\\Delta u(v^{(i)};u))-\\nabla_{v}X^{*}F^{*}(\\Delta u^{(i)};v)\\right|_{u=1}\\left\\}\\left\\{\\left(\\displaystyle\\sum_{s=0}^{\\infty}\\nabla_{v}\\log\\bar{p}^{(i)}(u|v|;u)\\right)\\right|}\\\\ &{\\le\\displaystyle\\sum_{k=0}^{\\lfloor\\frac{N}{2}\\gamma-k\\rfloor}\\left|\\lambda\\bar{u}(v|^{(i)};\\frac{u}{\\gamma})\\!\\!-\\!\\lambda\\bar{u}(v^{(i)};\\ln|\\Omega_{s})\\right|_{u=1}\\left\\}\\left\\{\\left(\\displaystyle\\sum_{s=0}^{\\infty}\\nabla_{v}\\log\\bar{p}^{(i)}(u|v|;u)\\right)\\right|}\\\\ &{\\le\\displaystyle\\sum_{k=0}^{\\lfloor\\frac{N}{2}\\gamma-k\\rfloor}\\left\\{\\lambda\\bar{u}(\\theta)^{(i)};\\frac{u}{\\gamma}\\right\\}\\ln|\\gamma|\\left(\\frac{\\bar{\\theta}^{(i)}}{\\gamma}\\right)\\!-\\!\\frac{1}{\\gamma}\\left(\\delta+1\\right)\\frac{1}{\\gamma}\\left\\}}\\\\ &{\\qquad\\displaystyle\\sum_{s=0}^{\\lfloor\\frac{N}{2}\\gamma-k\\rfloor}\\left\\{\\lambda\\bar{u}(v^{(i)};\\Delta)\\right\\}\\!=-\\lambda\\bar{(p^{(i)}; \n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "For the second part in (36), we have ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\left(\\left[\\nabla_{y}\\lambda_{H}(y^{(t)};x)\\right]^{\\top}-\\left[\\nabla_{y}\\lambda(y^{(t)};x)\\right]^{\\top}\\right)\\nabla_{\\lambda}F^{\\nu}(\\lambda(y^{(t)};x))\\right|}\\\\ &{=\\left|\\mathbb{E}\\left[\\displaystyle\\sum_{h=H}^{\\infty}\\gamma^{h}\\cdot\\frac{\\partial F^{\\nu}(\\lambda(y^{(t)};x))}{\\partial\\lambda_{s}\\partial_{h}\\cdot b_{h}}\\cdot\\left(\\displaystyle\\sum_{h^{\\prime}=0}^{h}\\nabla_{y}\\log y^{(t)}(b_{h^{\\prime}}|s_{h^{\\prime}})\\right)\\right]\\right|}\\\\ &{\\leq\\displaystyle\\sum_{h=H}^{\\infty}\\gamma^{h}\\cdot\\left(1+\\displaystyle\\frac{\\nu}{1-\\gamma}\\right)\\cdot(h+1)\\cdot\\displaystyle\\frac{1}{\\zeta}}\\\\ &{\\leq\\left(1+\\displaystyle\\frac{\\nu}{1-\\gamma}\\right)\\cdot\\left(\\displaystyle\\frac{H+1}{1-\\gamma}+\\displaystyle\\frac{1}{(1-\\gamma^{2})}\\right)\\cdot\\frac{1}{\\zeta}\\cdot\\gamma^{H}}\\\\ &{=\\left(\\frac{H+1}{(1-\\gamma)\\zeta}+\\frac{\\nu H+\\nu+1}{(1-\\gamma)^{2}\\zeta}+\\displaystyle\\frac{\\nu}{(1-\\gamma)^{3}\\zeta}\\right)\\cdot\\gamma^{H}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Combining (36), (38), and (39), we get the result ", "page_idx": 43}, {"type": "text", "text": "Before we proceed to analyze the variance of the estimators, we first show the Lipschitz continuity of the gradient estimator. ", "page_idx": 43}, {"type": "text", "text": "Lemma C.15. Let $\\tau=\\left\\{s_{0},b_{0},s_{1},b_{1},\\cdot\\cdot\\cdot,s_{H-1},b_{H-1}\\right\\}$ be an arbitrary $H$ -length trajectory. The gradient estimator satisfies ", "page_idx": 43}, {"type": "text", "text": "\u00b7 For any policy $\\textit{\\textbf{y}}$ , for any reward vectors $r_{1}$ and $r_{2}$ \uff0c ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\lVert\\tilde{{\\pmb g}}(\\tau|{\\pmb y};{\\pmb r}_{1})-\\tilde{{\\pmb g}}(\\tau|{\\pmb y};{\\pmb r}_{2})\\rVert\\leq\\frac{1}{(1-\\gamma)^{2}\\zeta}\\cdot\\lVert{\\pmb r}_{1}-{\\pmb r}_{2}\\rVert_{\\infty}.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "\u00b7 For any policies $\\pmb{y}_{1}$ and $\\pmb{y}_{2}$ , for any reward vectors $\\pmb{r}$ ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\|\\tilde{g}(\\tau|y_{1};r)-\\tilde{g}(\\tau|y_{2};r)\\|\\leq\\left(\\frac{1}{(1-\\gamma)^{2}\\zeta^{2}}+\\frac{\\nu}{(1-\\gamma)^{3}\\,\\zeta^{2}}\\right)\\cdot\\|y_{1}-y_{2}\\|.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Proof. ", "text_level": 1, "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\tilde{g}(\\tau|y;r_{1})-\\tilde{g}(\\tau|y;r_{2})\\|=\\displaystyle\\left\\|\\sum_{h=0}^{H-1}\\gamma^{h}\\cdot\\left(r_{1}(s_{h},b_{h})-r_{2}(s_{h},b_{h})\\right)\\cdot\\left(\\sum_{h^{\\prime}=0}^{h}\\nabla_{y}\\log y(b_{h^{\\prime}}|s_{h^{\\prime}})\\right)\\right\\|}\\\\ &{\\leq\\displaystyle\\sum_{h=0}^{H-1}\\gamma^{h}\\cdot\\|r_{1}-r_{2}\\|_{\\infty}\\cdot(h+1)\\cdot\\frac{1}{\\zeta}}\\\\ &{\\leq\\displaystyle\\frac{1}{(1-\\gamma)^{2}\\zeta}\\cdot\\|r_{1}-r_{2}\\|_{\\infty}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\tilde{g}(\\tau|y_{1},r)-\\tilde{g}(\\tau|y_{2},r)\\|}\\\\ &{\\leq\\;\\left\\|\\displaystyle\\sum_{h=0}^{H-1}{\\gamma^{h}\\cdot r(s_{h},b_{h})\\cdot\\left(\\displaystyle\\sum_{h^{\\prime}=0}^{h}(\\nabla_{y}\\log y_{1}(b_{h^{\\prime}}|s_{h^{\\prime}})-\\nabla_{y}\\log y_{2}(b_{h^{\\prime}}|s_{h^{\\prime}}))\\right)}\\right\\|}\\\\ &{\\leq\\;\\displaystyle\\sum_{h=0}^{H-1}{\\gamma^{h}\\cdot r(s_{h},b_{h})\\cdot(h+1)\\cdot\\displaystyle\\frac{1}{\\zeta^{2}}\\cdot\\|y_{1}-y_{2}\\|}}\\\\ &{\\leq\\;\\displaystyle\\frac{(1+\\frac{\\nu}{1-\\gamma})}{(1-\\gamma)^{2}\\zeta^{2}}\\cdot\\|y_{1}-y_{2}\\|}\\\\ &{=\\;\\left(\\displaystyle\\frac{1}{(1-\\gamma)^{2}\\zeta^{2}}+\\frac{\\nu}{(1-\\gamma)^{3}\\zeta^{2}}\\right)\\cdot\\|y_{1}-y_{2}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Where ", "page_idx": 44}, {"type": "text", "text": "\u00b7 (40) follows from (22);   \n\u00b7(41) is because of (23). ", "page_idx": 44}, {"type": "text", "text": "Now we analyze the variance of the estimators in the algorithm, we start with showing the following lemma. ", "page_idx": 44}, {"type": "text", "text": "Lemma C.16 (Bounded Var. of Visit. Estimator). For $\\hat{\\mathbf{\\lambda}}(t)$ in Algorithm 2, the variance is bounded. It holds that ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\|\\hat{\\pmb{\\lambda}}^{(t)}-\\pmb{\\lambda}_{H}(\\pmb{y}^{(t)};\\pmb{x})\\|^{2}\\right]\\le\\frac{1}{K(1-\\gamma)^{2}}.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Where $\\pmb{\\lambda}_{H}(\\pmb{y}^{(t)};\\pmb{x})$ is the truncated state-action visitation measure for $\\pmb{\\lambda}(\\pmb{y}^{(t)};\\pmb{x})$ defined in (35) ", "page_idx": 44}, {"type": "text", "text": "Proof. It holds that ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\lVert\\hat{\\lambda}^{(t)}-\\lambda_{H}(\\pmb{y}^{(t)};\\pmb{x})\\right\\rVert^{2}\\right]=\\mathbb{E}\\left[\\left\\lVert\\frac{1}{K}\\sum_{\\tau\\in\\mathcal{K}_{i}}\\tilde{\\lambda}(\\tau|\\pmb{y}^{(t)})-\\lambda_{H}(\\pmb{y}^{(t)};\\pmb{x})\\right\\rVert^{2}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\frac{1}{K}\\cdot\\mathbb{E}\\left[\\left\\lVert\\tilde{\\lambda}(\\tau|\\pmb{y}^{(t)})-\\lambda_{H}(\\pmb{y}^{(t)};\\pmb{x})\\right\\rVert^{2}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\frac{1}{K}\\cdot\\mathbb{E}\\left[\\left\\lVert\\tilde{\\lambda}(\\tau|\\pmb{y}^{(t)})\\right\\rVert^{2}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\leq\\frac{1}{K(1-\\gamma)^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Where: ", "page_idx": 44}, {"type": "text", "text": "\u00b7 (42) is due to $\\mathbb{E}\\left[\\hat{\\pmb{\\lambda}}^{(t)}\\right]=\\pmb{\\lambda}_{H}(\\pmb{y}^{(t)};\\pmb{x})$ and the fact that trajectories $\\tau\\in\\mathcal{K}^{\\left(t\\right)}$ are independently sampled; ", "page_idx": 44}, {"type": "text", "text": "\u00b7 (43) is because the variance is bounded by the second moment; ", "page_idx": 45}, {"type": "text", "text": "\u00b7(44) is because $\\begin{array}{r}{\\|\\tilde{\\lambda}(\\tau|y^{(t})\\|\\leq\\frac{1}{1-\\gamma}}\\end{array}$ ", "page_idx": 45}, {"type": "text", "text": "Now we analyze the ariance of gradient estimator $\\hat{g}_{y}^{(t)}$ by providing the following lemma: ", "page_idx": 45}, {"type": "text", "text": "Lemma C.17 (Bounded Var. of Grad. Estimator). For $\\hat{g}_{y}^{(t)}$ in Algorithm 2, we have ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\Vert\\hat{g}_{y}^{(t)}-\\left[\\nabla_{y}\\lambda_{H}(y^{(t)};x)\\right]^{\\top}r^{(t)}\\right\\Vert^{2}\\right]\\leq\\frac{3}{K(1-\\gamma)^{4}\\zeta^{2}}+\\frac{6\\nu}{K(1-\\gamma)^{5}\\zeta^{2}}+\\frac{9\\nu^{2}}{K(1-\\gamma)^{6}\\zeta^{2}}.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Proof. We denote $\\begin{array}{r}{\\pmb{r}^{\\star}=\\nabla_{\\lambda}F^{\\nu}\\left(\\lambda_{H}(\\pmb{y}^{(t)};\\pmb{x})\\right)=\\pmb{r}(\\pmb{x})-\\nu\\pmb{\\lambda}_{H}(\\pmb{y}^{(t)};\\pmb{x})}\\end{array}$ . Then we have ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\|\\frac{\\hat{\\mathbf{U}}}{v}\\right\\|_{\\mathcal{H}}^{(0)}-\\left[\\nabla_{\\mathbb{V}}\\lambda u(y^{(t)},v)\\right]^{\\top}r^{(t)}\\right]^{\\top}}\\\\ &{=\\mathbb{E}\\left[\\left\\|\\frac{1}{\\hat{\\mathbf{K}}}\\sum_{i<k^{(t)}}\\hat{g}(\\gamma(y^{(t)},\\gamma^{(t)})-\\frac{1}{K}\\sum_{r<k^{(t)}}\\hat{g}(\\gamma(y^{(t)},\\gamma^{*})+\\frac{1}{K}\\sum_{r<k^{(t)}}\\hat{g}(\\gamma(y^{(t)},\\gamma^{*})}\\\\ &{\\quad-\\left[\\nabla_{\\mathbb{V}}\\lambda u(y^{(t)},x)\\right]^{\\top}r^{+}+\\left[\\nabla_{\\mathbb{V}}\\lambda u(y^{(t)},x)\\right]^{\\top}r^{+}-\\left[\\nabla_{\\mathbb{V}}\\lambda u(y^{(t)},x)\\right]^{\\top}r^{(t)}\\right\\|^{2}\\right]^{\\top}r^{(t)}\\right]^{\\top}}\\\\ &{\\le3\\mathbb{E}\\left[\\left\\|\\frac{1}{K}\\sum_{r<k^{(t)}}\\Big(\\hat{g}(\\gamma(y^{(t)},\\gamma^{(t)})-\\hat{g}(\\gamma(y^{(t)},\\gamma^{*}))\\Big)\\Big\\|^{2}\\right]}\\\\ &{\\quad+3\\mathbb{E}\\left[\\left\\|\\frac{1}{K}\\sum_{r<k^{(t)}}\\bar{g}(\\gamma(y^{(t)},\\gamma^{*})-\\left[\\nabla_{\\mathbb{V}}\\lambda u(y^{(t)},x)\\right]^{\\top}r^{+}\\right\\|^{2}\\right]}\\\\ &{\\quad+3\\mathbb{E}\\left[\\left\\|\\nabla_{\\mathbb{V}}\\lambda u(y^{(t)},\\gamma^{*})\\right\\|^{\\top}r^{+}-\\left[\\nabla_{\\mathbb{V}}\\lambda u(y^{(t)},x)\\right]^{\\top}r^{(t)}\\right\\|^{2}\\bigg].}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Where (45) is due to Cauchy-Schwarz inequality. For the first part in (45), we have ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\Vert\\displaystyle\\frac{1}{K}\\sum_{\\tau\\in\\mathcal{K}^{(\\ell)}}\\left(\\tilde{g}(\\tau|y^{(t)};r^{(t)})-\\tilde{g}(\\tau|y^{(t)};r^{\\star})\\right)\\right\\Vert^{2}\\right]}\\\\ &{\\leq\\displaystyle\\frac{1}{K}\\sum_{\\tau\\in\\mathcal{K}^{(\\ell)}}\\mathbb{E}\\left[\\left\\Vert\\tilde{g}(\\tau|y^{(t)};r^{(t)})-\\tilde{g}(\\tau|y^{(t)};r^{\\star})\\right\\Vert^{2}\\right]}\\\\ &{\\leq\\displaystyle\\frac{1}{(1-\\gamma)^{4}\\zeta^{2}}\\cdot\\mathbb{E}\\left[\\left\\Vert r^{(t)}-r^{\\star}\\right\\Vert_{\\infty}^{2}\\right]}\\\\ &{\\leq\\displaystyle\\frac{\\nu^{2}}{(1-\\gamma)^{4}\\zeta^{2}}\\cdot E\\left[\\left\\Vert\\hat{\\lambda}^{(t)}-\\lambda_{H}(y^{(t)};x)\\right\\Vert\\right]}\\\\ &{\\leq\\displaystyle\\frac{\\nu^{2}}{K(1-\\gamma)^{6}\\zeta^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Where: ", "page_idx": 45}, {"type": "text", "text": "\u00b7 (46) is due to Cauchy-Schwarz inequality;   \n\u00b7 (47) follows from Lemma C.15;   \n\u00b7 (48) follows the same proof as in (37); ", "page_idx": 45}, {"type": "text", "text": "\u00b7(49) is because of Lemma C.16. ", "page_idx": 46}, {"type": "text", "text": "For the second part in (45), it holds that, ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\Vert\\displaystyle{\\frac{1}{K}}\\sum_{r\\in\\mathbb{R}^{n}}\\hat{g}(r|y^{(t)},r^{*})-\\left[\\nabla_{y}\\lambda_{H}(y^{(t)};x)\\right]^{\\top}r^{*}\\right\\Vert^{2}\\right]}\\\\ &{=\\displaystyle{\\frac{1}{K}}\\mathbb{E}\\left[\\Big\\|\\hat{g}(r|y^{(t)};r^{*})-\\left[\\nabla_{y}\\lambda_{H}(y^{(t)};x)\\right]^{\\top}r^{*}\\Big\\|^{2}\\right]}\\\\ &{\\le\\displaystyle{\\frac{1}{K}}\\mathbb{E}\\left[\\Big\\|\\hat{g}(r|y^{(t)};r^{*})\\Big\\|^{2}\\right]}\\\\ &{=\\displaystyle{\\frac{1}{K}}\\mathbb{E}\\left[\\left\\|\\displaystyle{\\frac{\\hat{h}^{(t)}}{\\displaystyle{\\sum_{h=0}^{t}}}}\\hat{r}^{*}(s_{h},h)\\cdot\\left(\\displaystyle{\\frac{\\hat{h}}{\\displaystyle{\\sum_{h=0}^{t}}}}\\nabla_{y}\\log y^{(t)}(b_{h}|x_{h}^{*})\\right)\\right\\|^{2}\\right]}\\\\ &{\\le\\displaystyle{\\frac{1}{K}}\\left(\\displaystyle{\\frac{(K-1)^{\\top}}{K\\log}}\\hat{r}^{*}\\left(1+\\frac{V-1}{L^{2}}\\right)\\cdot\\frac{1}{L^{\\frac{*}{2}}}\\left(h+1\\right)\\right)^{2}}\\\\ &{\\le\\displaystyle{\\frac{1}{K(1-1)^{*}}}\\Big(\\displaystyle{\\frac{2r}{K^{2}}}\\Big)\\cdot\\frac{2r}{K(1-1)^{*}\\zeta^{2}}+\\displaystyle{\\frac{V^{2}}{K(1-1)^{*}\\zeta^{2}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Where ", "page_idx": 46}, {"type": "text", "text": "\u00b7 (50) is due to Lemma C.14 and the fact that trajectories $\\tau$ are independently sampled;   \n\u00b7 (51) is because variance is bounded by second moment;   \n\u00b7 (52) follows from (22). ", "page_idx": 46}, {"type": "text", "text": "Finally for the last part in (45), we have ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\|\\left[\\nabla_{y}\\lambda_{H}(y^{(i)};x)\\right]^{\\top}r^{\\star}-\\left[\\nabla_{y}\\lambda_{H}(y^{(i)};x)\\right]^{\\top}r^{(i)}\\right\\|^{2}\\right]}\\\\ &{=\\mathbb{E}\\left[\\left\\|\\left[\\nabla_{y}\\lambda_{H}(y^{(i)};x)\\right]^{\\top}(r^{\\star}-r^{(i)})\\right\\|^{2}\\right]}\\\\ &{\\leq\\mathbb{E}\\left[\\left\\|\\sum_{h=0}^{H-1}\\gamma^{h}\\cdot\\|r^{\\star}-r^{(i)}\\|_{\\infty}\\cdot\\left(\\displaystyle\\sum_{h=0}^{h}\\nabla_{y}\\log^{(i)}(b_{h}|x_{h^{\\prime}})\\right)\\right\\|^{2}\\right]}\\\\ &{\\leq\\left(\\displaystyle\\sum_{h=0}^{H-1}\\gamma^{h}\\cdot\\nu\\cdot(h+1)\\cdot\\frac{1}{\\cdot}\\right)^{2}\\cdot\\mathbb{E}\\left[\\left\\|\\hat{\\lambda}^{(t)}-\\lambda_{H}(y^{(i)};x)\\right\\|^{2}\\right]}\\\\ &{\\leq\\frac{\\nu^{2}}{(1-\\gamma)^{4}\\xi^{2}}\\cdot\\frac{1}{K(1-\\gamma)^{2}}}\\\\ &{-\\frac{\\nu}{K(1-\\gamma)^{2}\\xi^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Where ", "page_idx": 46}, {"type": "text", "text": "(54) is due to (22) and (37);   \n\u00b7(55) is because of Lemma C.16. ", "page_idx": 46}, {"type": "text", "text": "Combine (45), (49), (53) and (56), we get ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\left[\\left\\|\\hat{g}_{y}^{(t)}-\\left[\\nabla_{y}\\lambda_{H}(\\pmb{y}^{(t)};\\pmb{x})\\right]^{\\top}r^{(t)}\\right\\|^{2}\\right]}\\\\ &{\\le\\cfrac{3\\nu^{2}}{K(1-\\gamma)^{6}\\zeta^{2}}+3\\left(\\cfrac{1}{K(1-\\gamma)^{4}\\zeta^{2}}+\\cfrac{2\\nu}{K(1-\\gamma)^{5}\\zeta^{2}}+\\cfrac{\\nu^{2}}{K(1-\\gamma)^{6}\\zeta^{2}}\\right)+\\cfrac{3\\nu^{2}}{K(1-\\gamma)^{6}\\zeta^{2}}}\\\\ &{=\\cfrac{3}{K(1-\\gamma)^{4}\\zeta^{2}}+\\cfrac{6\\nu}{K(1-\\gamma)^{5}\\zeta^{2}}+\\cfrac{9\\nu^{2}}{K(1-\\gamma)^{6}\\zeta^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "After bounding the variance of $\\hat{g}_{y}^{(t)}$ in Algorithm 2, we can prove the following lemma ", "page_idx": 47}, {"type": "text", "text": "Lemma C.18 (Bounded Dist. with Actual Grad.). Consider $\\pmb{y}^{(t)}$ and $\\hat{g}_{y}^{(t)}$ in Algorithm 2,it holds that ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\|\\hat{\\pmb{g}}_{\\pmb{y}}^{(t)}-\\nabla_{\\pmb{y}}F^{\\nu}(\\pmb{\\lambda}(\\pmb{y}^{(t)};\\pmb{x}))\\right\\|^{2}\\right]\\leq\\frac{\\mathcal{C}_{1}}{K}+\\mathcal{C}_{2}\\cdot\\gamma^{2H}.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Where ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\mathcal{C}_{1}=\\frac{57}{(1-\\gamma)^{6}\\zeta^{2}},\\quad\\mathcal{C}_{2}=\\frac{126H^{2}}{(1-\\gamma)^{6}\\zeta^{2}}.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Proof. Let $\\begin{array}{r}{\\pmb{r}^{\\star}=\\nabla_{\\lambda}F^{\\nu}\\left(\\lambda_{H}(\\pmb{y}^{(t)};\\pmb{x})\\right)=\\pmb{r}(\\pmb{x})-\\nu\\pmb{\\lambda}_{H}(\\pmb{y}^{(t)};\\pmb{x}).}\\end{array}$ ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\|\\bar{g}_{y}^{(t)}-\\nabla_{y}F^{\\prime}(\\lambda(y^{(t)};x))\\right\\|^{2}\\right]}\\\\ &{-\\mathbb{E}\\Bigg[\\bigg\\|\\bar{g}_{y}^{(t)}-\\left[\\nabla_{y}\\lambda_{H}(y^{(t)};x)\\right]^{\\top}r^{(t)}+\\left[\\nabla_{y}\\lambda_{H}(y^{(t)};x)\\right]^{\\top}r^{(t)}-\\left[\\nabla_{y}\\lambda_{H}(y^{(t)};x)\\right]^{\\top}r^{(t)}}\\\\ &{\\quad+\\left[\\nabla_{y}\\lambda_{H}(y^{(t)};x)\\right]^{\\top}r^{\\ast}-\\nabla_{y}F^{\\prime\\prime}(\\lambda(y^{(t)};x))\\bigg\\|^{2}\\Bigg]}\\\\ &{\\leq\\Im\\mathbb{E}\\Bigg[\\bigg\\|\\bar{g}_{y}^{(t)}-\\left[\\nabla_{y}\\lambda_{H}(y^{(t)};x)\\right]^{\\top}r^{(t)}\\bigg\\|^{2}\\Bigg]}\\\\ &{\\quad+\\Im\\mathbb{E}\\Bigg[\\bigg\\|\\left[\\nabla_{y}\\lambda_{H}(y^{(t)};x)\\right]^{\\top}r^{(t)}-\\left[\\nabla_{y}\\lambda_{H}(y^{(t)};x)\\right]^{\\top}r^{\\ast}\\bigg\\|^{2}\\Bigg]}\\\\ &{\\quad+\\Im\\mathbb{E}\\Bigg[\\bigg\\|\\left[\\nabla_{y}\\lambda_{H}(y^{(t)};x)\\right]^{\\top}r^{\\ast}-\\nabla_{y}F^{\\prime\\prime}(\\lambda(y^{(t)};x))\\bigg\\|^{2}\\Bigg].}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Notice that the first part in (57) is bounded in Lemma C.17 and the second part is bounded in (56). For the last part, observe that ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\left[\\nabla_{y}\\lambda_{H}(y^{(t)};x)\\right]^{\\top}r^{\\star}-\\nabla_{y}F^{\\nu}(\\lambda(y^{(t)};x))\\right\\|^{2}}\\\\ &{=\\left\\|\\left[\\nabla_{y}\\lambda_{H}(y^{(t)};x)\\right]^{\\top}\\nabla_{x}F^{\\nu}(\\lambda_{H}(y^{(t)};x))-\\left[\\nabla_{y}\\lambda(y^{(t)};x)\\right]^{\\top}\\nabla_{x}F^{\\nu}(\\lambda(y^{(t)};x))\\right\\|^{2}}\\\\ &{=\\Bigg\\|\\left[\\nabla_{y}\\lambda_{H}(y^{(t)};x)\\right]^{\\top}\\left(\\nabla_{x}F^{\\nu}(\\lambda_{H}(y^{(t)};x))-\\nabla_{x}F^{\\nu}(\\lambda(y^{(t)};x))\\right)}\\\\ &{\\quad+\\left(\\left[\\nabla_{y}\\lambda_{H}(y^{(t)};x)\\right]^{\\top}-\\left[\\nabla_{y}\\lambda(y^{(t)};x)\\right]^{\\top}\\right)\\nabla_{x}F^{\\nu}(\\lambda(y^{(t)};x))\\Bigg\\|^{2}}\\\\ &{\\leq2\\left\\|\\left[\\nabla_{y}\\lambda_{H}(y^{(t)};x)\\right]^{\\top}\\left(\\nabla_{x}F^{\\nu}(\\lambda_{H}(y^{(t)};x))-\\nabla_{x}F^{\\nu}(\\lambda(y^{(t)};x))\\right)\\right\\|^{2}}\\\\ &{\\quad+2\\left\\|\\left(\\left[\\nabla_{y}\\lambda_{H}(y^{(t)};x)\\right]^{\\top}-\\left[\\nabla_{y}\\lambda(y^{(t)};x)\\right]^{\\top}\\right)\\nabla_{x}F^{\\nu}(\\lambda(y^{(t)};x))\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Where (58) is follows from Cauchy-Schwarz inequality. For the first part, we have ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|\\left[\\nabla_{y}\\lambda_{H}(\\pmb{y}^{(t)};\\pmb{x})\\right]^{\\top}\\left(\\nabla_{\\lambda}F^{\\nu}(\\lambda_{H}(\\pmb{y}^{(t)};\\pmb{x}))-\\nabla_{\\lambda}F^{\\nu}(\\lambda(\\pmb{y}^{(t)};\\pmb{x}))\\right)\\right\\|^{2}}\\\\ &{\\leq\\left(\\displaystyle\\sum_{h=0}^{\\infty}\\gamma^{h}\\cdot\\nu\\|\\lambda_{H}(\\pmb{y}^{(t)};\\pmb{x})-\\lambda(\\pmb{y}^{(t)};\\pmb{x})\\|_{1}\\cdot(h+1)\\cdot\\displaystyle\\frac{1}{\\zeta}\\right)^{2}}\\\\ &{\\leq\\displaystyle\\frac{\\nu^{2}}{(1-\\gamma)^{4}\\zeta^{2}}\\cdot\\|\\lambda_{H}(\\pmb{y}^{(t)};\\pmb{x})-\\lambda(\\pmb{y}^{(t)};\\pmb{x})\\|_{1}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Where (59) is because of (37). Since ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\left\\|\\lambda_{H}(\\pmb{y}^{(t)};\\pmb{x})-\\lambda(\\pmb{y}^{(t)};\\pmb{x})\\right\\|_{1}^{2}=\\left(\\displaystyle\\sum_{h=H}^{\\infty}\\sum_{s,b}\\gamma^{t}\\,\\mathbb{P}(s_{h}=s,b_{h}=b|\\pmb{y}^{(t)},s_{0}\\sim\\rho)\\right)^{2}}}\\\\ &{}&{=\\left(\\gamma^{H}\\displaystyle\\sum_{h=0}^{\\infty}\\gamma^{h}\\cdot1\\right)^{2}}\\\\ &{}&{\\leq\\displaystyle\\frac{\\gamma^{2H}}{(1-\\gamma)^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "We have ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\left\\|\\left[\\nabla_{y}\\lambda_{H}(y^{(t)};x)\\right]^{\\top}\\left(\\nabla_{\\lambda}F^{\\nu}(\\lambda_{H}(y^{(t)};x))-\\nabla_{\\lambda}F^{\\nu}(\\lambda(y^{(t)};x))\\right)\\right\\|^{2}\\leq\\frac{\\nu^{2}}{(1-\\gamma)^{6}\\zeta^{2}}\\cdot\\gamma^{2H}.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "For the second part in (58), it holds that ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\left(\\left[\\nabla_{y}\\lambda_{H}(y^{(t)};x)\\right]^{\\top}-\\left[\\nabla_{y}\\lambda(y^{(t)};x)\\right]^{\\top}\\right)\\nabla_{\\lambda}F^{\\nu}(\\lambda(y^{(t)};x))\\right\\|^{2}}\\\\ &{=\\left\\|\\mathbb{E}\\left[\\displaystyle\\sum_{h=H}^{\\infty}\\gamma^{h}\\cdot\\nabla_{\\lambda}F^{\\nu}(\\lambda(y^{(t)};x))_{s_{h},b_{h}}\\cdot\\left(\\displaystyle\\sum_{h^{\\prime}=0}^{h}\\nabla_{y}\\log y^{(t)}(b_{h^{\\prime}}|s_{h^{\\prime}})\\right)\\right]\\right\\|^{2}}\\\\ &{\\leq\\left(\\displaystyle\\sum_{h=H}^{\\infty}\\gamma^{h}\\cdot\\left(1+\\displaystyle\\frac{\\nu}{1-\\gamma}\\right)\\cdot(h+1)\\cdot\\displaystyle\\frac{1}{\\zeta}\\right)^{2}}\\\\ &{\\leq\\left(1+\\displaystyle\\frac{\\nu}{1-\\gamma}\\right)^{2}\\cdot\\frac{1}{\\zeta^{2}}\\cdot\\left((\\frac{(H+1)^{2}}{(1-\\gamma)^{2}}+\\displaystyle\\frac{1}{(1-\\gamma)^{4}}\\right)\\cdot\\gamma^{2H}}\\\\ &{=\\left(\\frac{(H+1)^{2}}{(1-\\gamma)^{2}\\zeta^{2}}+\\frac{2\\nu(H+1)^{2}}{(1-\\gamma)^{3}\\zeta^{2}}+\\frac{\\nu^{2}(H+1)^{2}+1}{(1-\\gamma)^{4}\\zeta^{2}}+\\displaystyle\\frac{2\\nu}{(1-\\gamma)^{5}\\zeta^{2}}+\\frac{\\nu^{2}}{(1-\\gamma)^{6}}\\right)\\cdot\\gamma^{2H}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Combine (58), (60), and (61) we get ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|\\left[\\nabla_{y}\\lambda_{H}(y^{(t)};\\boldsymbol{x})\\right]^{\\top}r^{\\star}-\\nabla_{y}F^{\\nu}(\\lambda(y^{(t)};\\boldsymbol{x}))\\right\\|^{2}}\\\\ &{\\le2\\left(\\frac{(H+1)^{2}}{(1-\\gamma)^{2}\\zeta^{2}}+\\frac{2\\nu(H+1)^{2}}{(1-\\gamma)^{3}\\zeta^{2}}+\\frac{\\nu^{2}(H+1)^{2}+1}{(1-\\gamma)^{4}\\zeta^{2}}+\\frac{2\\nu}{(1-\\gamma)^{5}\\zeta^{2}}+\\frac{2\\nu^{2}}{(1-\\gamma)^{6}\\zeta^{2}}\\right)\\cdot\\gamma^{2H}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Now combine Lemma C.17, (56), (57), and (62), we get ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\|\\hat{\\pmb{g}}_{\\pmb{y}}^{(t)}-\\nabla_{\\pmb{y}}F^{\\nu}(\\pmb{\\lambda}(\\pmb{y}^{(t)};\\pmb{x}))\\right\\|^{2}\\right]\\leq\\frac{\\mathcal{C}_{1}}{K}+\\mathcal{C}_{2}\\cdot\\gamma^{2H}.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "D  Nonconvex-Hidden-Strongly-Concave Optimization ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "In this section we generalize our results to the more general setting of any constrained min-max optimization problem of the form $\\scriptstyle\\operatorname*{min}_{\\mathbf{x}\\in{\\mathcal{X}}}\\operatorname*{max}_{\\mathbf{y}\\in{\\mathcal{Y}}}$ when $f$ is nonconvex-hidden-strongly-concave. In particular: ", "page_idx": 50}, {"type": "text", "text": "\u00b7 In Theorem D.2 we prove the differentiability and Holder continuity of the max function $\\Phi(\\pmb{x})=\\operatorname*{max}_{\\pmb{y}\\in\\mathcal{Y}}f(\\pmb{x},\\pmb{y})$ by utilizing the Holder continuity of the maximizers w.r.t.to $\\textbf{\\em x}$ (Theorem D.1). \u00b7 Finally, in Theorem D.3 we prove that Algorithm 3 (SGDMAx) [70, Algorithm 4] with an appropriate tuning converges to an $\\epsilon$ -SP for nonconvex-hidden-concave functions. ", "page_idx": 50}, {"type": "text", "text": "We begin by stating the assumptions we make. ", "page_idx": 50}, {"type": "text", "text": "Assumption D.1. Let $f$ be a function defined on $\\mathcal X\\times\\mathcal X$ where $\\mathcal{X}$ and $\\boldsymbol{\\wp}$ are compact convex sets.   \n$L$ -Lipschitz continuous and $\\ell$ -smooth. ", "page_idx": 50}, {"type": "text", "text": "Assumption D.2. Let $c$ be a \u201c1-1\" mapping between $\\boldsymbol{\\wp}$ and a compact convex set $\\boldsymbol{\\mathcal{U}}$ parameterized by $x\\in\\mathcal{X}$ . Further, we assume that $c$ and its inverse $c^{-1}$ are $L_{c}$ and $L_{c^{-1}}$ -Lipschitz continuous. ", "page_idx": 50}, {"type": "text", "text": "Assumption D.3. Let $H$ be a nonconvex-strongly-concave reformulation of $f$ (as in Assumption D.1) for a mapping $c$ (as in Assumption D.2). We assume $H$ to be $L_{H}$ -Lipschitz continuous and $\\ell_{H}$ smooth. ", "page_idx": 50}, {"type": "text", "text": "Moving on, we can show that the maximizers $\\pmb{u}^{\\star}(\\cdot)$ are Holder continuous w.r.t. to $\\textbf{\\em x}$ ", "page_idx": 50}, {"type": "text", "text": "Theorem D.1 (Continuity of the maximizers). Let a function nonconvex-nonconcave function $f,c,H$ as in Assumptions D.1 to D.3. We define $\\pmb{u}^{\\star}(\\pmb{x}):=\\operatorname*{argmax}_{\\pmb{u}\\in\\mathcal{U}(\\pmb{x})}H(\\pmb{x},\\pmb{u})$ , then it is the case that ", "page_idx": 50}, {"type": "text", "text": "$\\|u^{\\star}(\\pmb{x}_{1})-u^{\\star}(\\pmb{x}_{2})\\|\\leq L_{\\star}\\,\\|\\pmb{x}_{1}-\\pmb{x}_{2}\\|^{1/2}\\,.$ Where $\\begin{array}{r}{L_{\\star}=\\frac{1}{2\\nu}\\left(2\\ell_{H}\\sqrt{\\mathrm{Diam}\\chi}+2\\sqrt{\\nu(1+2\\ell_{H})L_{c}\\mathrm{Diam}_{\\mathcal{U}}+2\\nu L_{c}L_{H}}\\right).}\\end{array}$ ", "page_idx": 50}, {"type": "text", "text": "Proof. Consider any $\\mathbf{\\mathcal{x}}_{1},\\mathbf{\\mathcal{x}}_{2}\\in\\mathcal{X}$ , since $\\pmb{u}^{\\star}$ is the maximizer, it holds that ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla H\\left(\\boldsymbol{x}_{1},\\boldsymbol{u}^{\\star}(\\boldsymbol{x}_{1})\\right)^{\\top}(\\boldsymbol{u}_{1}-\\boldsymbol{u}^{\\star}(\\boldsymbol{x}_{1}))\\leq0,\\quad\\forall\\boldsymbol{u}_{1}\\in\\mathcal{U}(\\boldsymbol{x}_{1});}\\\\ &{\\nabla H\\left(\\boldsymbol{x}_{2},\\boldsymbol{u}^{\\star}(\\boldsymbol{x}_{2})\\right)^{\\top}(\\boldsymbol{u}_{2}-\\boldsymbol{u}^{\\star}(\\boldsymbol{x}_{2}))\\leq0,\\quad\\forall\\boldsymbol{u}_{2}\\in\\mathcal{U}(\\boldsymbol{x}_{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "We now consider $\\overline{{\\pmb{u}}}$ that belong to the set $\\overline{{\\mathcal{U}}}=\\mathcal{U}(\\pmb{x}_{1})\\cup\\mathcal{U}(\\pmb{x}_{2})$ . We observe that due to the Lipschitz mapping, for every $\\overline{{\\pmb{u}}}\\in\\overline{{\\mathcal{U}}}$ , there exist a $\\pmb{u}_{1}\\in\\mathcal{U}(\\pmb{x}_{1})$ such that $\\|\\overline{{\\boldsymbol{u}}}-\\boldsymbol{u}_{1}\\|\\leq L_{c}\\|\\pmb{x}_{1}-\\pmb{x}_{2}\\|$ . Similar argument holds for $\\pmb{u}_{2}\\in\\mathcal{U}(\\pmb{x}_{2})$ . Therefore, from previous two inequalities, we have ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla H\\left(\\mathbf{\\boldsymbol{x}}_{1},\\boldsymbol{u}^{\\star}(\\boldsymbol{x}_{1})\\right)^{\\top}(\\overline{{\\boldsymbol{u}}}-\\boldsymbol{u}^{\\star}(\\boldsymbol{x}_{1}))\\leq L_{c}L_{H}\\|\\boldsymbol{x}_{1}-\\boldsymbol{x}_{2}\\|\\quad\\forall\\overline{{\\boldsymbol{u}}}\\in\\overline{{\\boldsymbol{U}}};}\\\\ &{\\nabla H\\left(\\boldsymbol{x}_{2},\\boldsymbol{u}^{\\star}(\\boldsymbol{x}_{2})\\right)^{\\top}(\\overline{{\\boldsymbol{u}}}-\\boldsymbol{u}^{\\star}(\\boldsymbol{x}_{2}))\\leq L_{c}L_{H}\\|\\boldsymbol{x}_{1}-\\boldsymbol{x}_{2}\\|\\quad\\forall\\overline{{\\boldsymbol{u}}}\\in\\overline{{\\boldsymbol{U}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Where in the above inequalities we used the fact that $\\nabla H(\\pmb{x},\\pmb{u})\\leq L_{H}$ . We plug in $\\overline{{\\pmb{u}}}\\leftarrow\\pmb{u}^{\\star}(\\pmb{x}_{2})$ and $\\overline{{\\pmb{u}}}\\leftarrow\\pmb{u}^{\\star}(\\pmb{x}_{1})$ accordingly, ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla H\\left(\\mathbf{x}_{1},\\pmb{u}^{\\star}(\\mathbf{x}_{1})\\right)^{\\top}(\\pmb{u}^{\\star}(\\mathbf{x}_{1})-\\pmb{u}^{\\star}(\\mathbf{x}_{1}))\\leq L_{c}L_{H}\\|\\mathbf{x}_{1}-\\mathbf{x}_{2}\\|,}\\\\ &{\\nabla H\\left(\\mathbf{x}_{2},\\pmb{u}^{\\star}(\\mathbf{x}_{2})\\right)^{\\top}(\\pmb{u}^{\\star}(\\mathbf{x}_{1})-\\pmb{u}^{\\star}(\\mathbf{x}_{2}))\\leq L_{c}L_{H}\\|\\mathbf{x}_{1}-\\mathbf{x}_{2}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Adding the two inequalities results in, ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left(\\nabla H\\left(x_{1},u^{\\star}(x_{1})\\right)-\\nabla H\\left(x_{2},u^{\\star}(x_{2})\\right)\\right)^{\\top}\\left(u^{\\star}(x_{1})-u^{\\star}(x_{1})\\right)\\leq2L_{c}L_{H}\\|x_{1}-x_{2}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Since $H(x,\\cdot)$ is $\\nu$ -strongly concave in $\\textbf{\\em u}$ for all $\\textbf{\\em x}$ , it holds that ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{u_{1}-u^{\\star}(x_{1}))^{\\top}\\left(\\nabla H\\left(x_{1},u_{1}\\right)-\\nabla H\\left(x_{1},u^{\\star}(x_{1})\\right)\\right)+\\nu\\|u_{1}-u^{\\star}(x_{1})\\|^{2}\\leq0\\quad\\forall u_{1}\\in\\mathcal{U}(x_{1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "We again consider feasibility set $\\overline{{\\pmb{u}}}\\,\\in\\,\\overline{{\\mathcal{U}}}$ . Since for every $\\overline{{\\pmb{u}}}\\,\\in\\,\\overline{{\\mathcal{U}}}$ , there exists $\\pmb{u}_{1}\\,\\in\\,\\mathcal{U}(\\pmb{x}_{1})$ s.t. $\\|\\overline{{\\boldsymbol{u}}}-\\mathbf{\\bar{\\boldsymbol{u}}}_{1}\\|\\leq L_{c}\\|\\pmb{x}_{1}-\\pmb{x}_{2}\\|$ .We have ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(u_{1}+(\\overline{{u}}-\\overline{{u}})-u^{\\star}(x_{1}))^{\\top}\\left(\\nabla H(x_{1},u_{1})+(\\nabla H(x_{1},\\overline{{u}})-\\nabla H(x_{1},\\overline{{u}}))-\\nabla H\\left(x_{1},u^{\\star}(x_{1})\\right)\\right)}\\\\ &{+\\nu\\|u_{1}+(\\overline{{u}}-\\overline{{u}})-u^{\\star}(x_{1})\\|^{2}\\leq0,\\quad\\forall u_{1}\\in\\mathcal{U}(x_{1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "We rearrange the latter display into ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\overline{{u}}-u^{\\star}(x_{1}))^{\\top}\\left(\\nabla H(x_{1},\\overline{{u}})-\\nabla H(x_{1},u^{\\star}(x_{1}))\\right)+\\nu\\|\\overline{{u}}-u^{\\star}(x_{1})\\|^{2}}\\\\ &{\\leq\\underbrace{-(\\overline{{u}}-u^{\\star}(x_{1}))^{\\top}\\left(\\nabla H(x_{1},u_{1})-\\nabla H(x_{1},\\overline{{u}})\\right)}_{\\Omega_{1}}}\\\\ &{\\underbrace{-(u_{1}-\\overline{{u}})^{\\top}\\left(\\nabla H(x_{1},u_{1})-\\nabla H(x_{1},u^{\\star}(x_{1}))\\right)}_{\\Omega_{2}}}\\\\ &{\\underbrace{-\\nu\\left\\|u_{1}-\\overline{{u}}\\right\\|^{2}-2\\nu\\left\\langle u_{1}-\\overline{{u}},\\overline{{u}}-u^{\\star}(x_{1})\\right\\rangle}_{\\Omega_{3}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "We bound $\\Omega_{1},\\Omega_{2}$ , and $\\Omega_{3}$ separately. ", "page_idx": 51}, {"type": "text", "text": "\u00b7 For $\\Omega_{1}$ , we have ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\left(\\overline{{\\boldsymbol u}}-\\boldsymbol u^{\\star}(\\boldsymbol x_{1})\\right)^{\\top}\\left(\\nabla H(\\boldsymbol x_{1},\\boldsymbol u_{1})-\\nabla H(\\boldsymbol x_{1},\\overline{{\\boldsymbol u}})\\right)\\leq\\mathrm{Diam}_{\\mathcal{U}}\\cdot\\ell_{H}\\|\\boldsymbol u_{1}-\\overline{{\\boldsymbol u}}\\|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\mathrm{Diam}_{\\mathcal{U}}\\ell_{H}L_{c}\\left\\|\\boldsymbol x_{1}-\\boldsymbol x_{2}\\right\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "\u00b7For $\\Omega_{2}$ , it holds that ", "page_idx": 51}, {"type": "equation", "text": "$$\n-(u_{1}-\\overline{{\\pmb u}})^{\\top}\\left(\\nabla H(x_{1},u_{1})-\\nabla H(x_{1},\\pmb u^{\\star}(x_{1}))\\right)\\le L_{c}\\left\\|x_{1}-x_{2}\\right\\|\\cdot\\ell_{H}\\mathrm{Diam}_{\\mathcal{U}}.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "\u00b7For $\\Omega_{3}$ , since the first term is always non-positive, we only need to bound the second term: ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r}{-\\left\\langle u_{1}-\\overline{{u}},\\overline{{u}}-u^{\\star}(x_{1})\\right\\rangle\\leq\\|u_{1}-\\overline{{u}}\\|\\,\\|\\overline{{u}}-u^{\\star}(x_{1})\\|}\\\\ {\\leq L_{c}\\,\\|x_{1}-x_{2}\\|\\cdot\\mathrm{Diam}_{\\mathcal{U}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Combining $\\Omega_{1},\\Omega_{2}$ , and $\\Omega_{3}$ , we conclude that ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\overline{{\\boldsymbol{u}}}-\\boldsymbol{u}^{\\star}(\\pmb{x}_{1}))^{\\top}\\left(\\nabla H(\\pmb{x}_{1},\\overline{{\\boldsymbol{u}}})-\\nabla H(\\pmb{x}_{1},\\boldsymbol{u}^{\\star}(\\pmb{x}_{1}))\\right)+\\nu\\|\\overline{{\\boldsymbol{u}}}-\\boldsymbol{u}^{\\star}(\\pmb{x}_{1})\\|^{2}}\\\\ &{\\leq(1+2\\ell_{H})L_{c}\\mathrm{Diam}_{\\mathcal{U}}\\left\\|\\pmb{x}_{1}-\\pmb{x}_{2}\\right\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Plugging in $\\overline{{\\pmb{u}}}\\leftarrow\\pmb{u}^{\\star}(\\pmb{x}_{2})$ in (64) and combine it with (63), we get ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nu\\|u^{\\star}(x_{2})-u^{\\star}(x_{1})\\|^{2}\\leq(u^{\\star}(x_{2})-u^{\\star}(x_{1}))^{\\top}\\left(\\nabla H\\left(x_{2},u^{\\star}(x_{2})\\right)-\\nabla H\\left(x_{1},u^{\\star}(x_{2})\\right)\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\;L^{\\prime\\prime}\\|x_{1}-x_{2}\\|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\ell_{H}\\|u^{\\star}(x_{2})-u^{\\star}(x_{1})\\|\\|x_{1}-x_{2}\\|+L^{\\prime\\prime}\\|x_{1}-x_{2}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Where $L^{\\prime\\prime}=(1+2\\ell_{H})L_{c}\\mathrm{Diam}_{\\mathcal{U}}+2L_{c}L_{H}.$ ", "page_idx": 51}, {"type": "text", "text": "Similarly to Lemma C.8, we can set $\\lambda=\\|\\pmb{u}^{\\star}(\\pmb{x}_{2})-\\pmb{u}^{\\star}(\\pmb{x}_{1})\\|$ and $\\chi=\\|\\pmb{x}_{1}-\\pmb{x}_{2}\\|$ and consider the inequality $\\nu\\lambda^{2}\\le\\ell_{H}\\lambda\\chi\\!+\\!L^{\\prime\\prime}\\chi$ We aim to fn the sluionof the form $\\begin{array}{r}{\\frac{\\ell_{H}\\chi+\\sqrt{\\chi(4\\nu L^{\\prime\\prime}+\\ell_{H}^{2}\\chi)}}{2\\nu}\\leq c\\sqrt{\\chi}}\\end{array}$ By setting $L_{\\star}=c$ and solve for $c$ gives ", "page_idx": 51}, {"type": "equation", "text": "$$\n{\\cal L}_{\\star}=\\frac{1}{2\\nu}\\left(2\\ell_{H}\\sqrt{\\mathrm{Diam}_{\\chi}}+2\\sqrt{\\nu(1+2\\ell_{H})L_{c}\\mathrm{Diam}_{\\mathcal{U}}+2\\nu L_{c}L_{H}}\\right).\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Finally, we show that $\\Phi$ is differentiable and Holder-continuous. ", "page_idx": 51}, {"type": "text", "text": "Theorem D.2. Let function $\\Phi$ be $\\begin{array}{r}{\\Phi(\\pmb{x}):=\\operatorname*{max}_{\\pmb{u}\\in\\mathcal{U}(\\pmb{x})}\\left\\{H(\\pmb{x},\\pmb{u})\\right\\}}\\end{array}$ Its gradient $\\nabla\\Phi$ .s $(1/2,\\ell_{1/2})$ Holder continuous, ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla\\Phi(\\pmb{x})-\\nabla\\Phi(\\pmb{x}^{\\prime})\\|\\le\\ell_{1/2}\\,\\|\\pmb{x}-\\pmb{x}^{\\prime}\\|^{\\frac12}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "where $\\ell_{1/2}:=\\left((1+L_{c^{-1}})\\sqrt{\\operatorname{Diam}_{\\chi}}+L_{c^{-1}}L_{\\star}\\right)\\ell$ ", "page_idx": 51}, {"type": "text", "text": "Proof. ", "text_level": 1, "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Vert\\nabla\\Phi(\\mathbf{x})-\\nabla\\Phi(\\mathbf{x}^{\\prime})\\Vert=\\left\\Vert\\nabla f\\left(\\mathbf{x},c^{-1}(\\mathbf{\\boldsymbol{u}}^{\\star}(\\mathbf{x});\\mathbf{x})\\right)-\\nabla f\\left(\\mathbf{x}^{\\prime},c^{-1}(\\mathbf{\\boldsymbol{u}}^{\\star}(\\mathbf{x}^{\\prime});\\mathbf{x}^{\\prime})\\right)\\right\\Vert}&{}\\\\ {\\leq\\ell\\left\\Vert\\mathbf{x}-\\mathbf{x}^{\\prime}\\right\\Vert+\\ell\\left\\Vert c^{-1}(\\mathbf{\\boldsymbol{u}}^{\\star}(\\mathbf{x});\\mathbf{x})-c^{-1}(\\mathbf{\\boldsymbol{u}}^{\\star}(\\mathbf{x}^{\\prime});\\mathbf{x}^{\\prime})\\right\\Vert}&{}\\\\ {\\leq\\ell\\left\\Vert\\mathbf{x}-\\mathbf{x}^{\\prime}\\right\\Vert+\\ell L_{c^{-1}}\\left(\\left\\Vert\\mathbf{\\boldsymbol{u}}^{\\star}(\\mathbf{x})-\\mathbf{\\boldsymbol{u}}^{\\star}(\\mathbf{x}^{\\prime})\\right\\Vert+\\left\\Vert\\mathbf{\\boldsymbol{x}}-\\mathbf{\\boldsymbol{x}}^{\\prime}\\right\\Vert\\right)}&{}\\\\ {\\leq\\left(1+L_{c^{-1}}\\right)\\ell\\left\\Vert\\mathbf{x}-\\mathbf{x}^{\\prime}\\right\\Vert+L_{c^{-1}}L_{\\star}\\ell\\left\\Vert\\mathbf{x}-\\mathbf{x}^{\\prime}\\right\\Vert^{\\frac{1}{2}}}&{}\\\\ {\\leq\\left((1+L_{c^{-1}})\\sqrt{\\mathrm{Diam}_{\\mathcal{X}}}+L_{c^{-1}}L_{\\star}\\right)\\ell\\left\\Vert\\mathbf{x}-\\mathbf{x}^{\\prime}\\right\\Vert^{\\frac{1}{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Where ", "page_idx": 52}, {"type": "text", "text": "\u00b7 in (65) we invoke the Lipschitz continuity of function $c^{-1}(\\cdot)$ (66) follows from Theorem D.1. ", "page_idx": 52}, {"type": "text", "text": "Following, SGDMAX is presented where we assume a stochastic gradient oracle $G=(G_{x},G y):$ $\\boldsymbol{\\mathcal{X}}\\times\\boldsymbol{\\mathcal{Y}}\\times\\Xi\\rightarrow\\mathbb{R}^{d}$ that is unbiased and has a bounded variance: ", "page_idx": 52}, {"type": "text", "text": "Algorithm 3 SGDMAX   \nInput: Initialization $\\pmb{x}^{(0)}$ , stepsize $\\eta_{x}$ \uff0c $T_{x}$ iterations, batch size $M$ , oracle accuracy $\\zeta$   \n1: for $t\\gets1,2,\\ldots,T$ do   \n2: $\\begin{array}{r l}&{\\pmb{y}^{(t)}\\leftarrow\\mathsf{m a x-o r a c l e}\\Big(f\\big(\\pmb{x}^{(t)},\\cdot\\big);\\zeta\\Big)}\\\\ &{\\hat{\\pmb{g}}^{(t)}\\leftarrow\\frac{1}{M}\\sum_{j=1}^{M}G_{x}\\left(\\pmb{x}^{(t-1)},\\pmb{y}^{(t)},\\xi_{j}^{(t)}\\right)}\\\\ &{\\pmb{x}^{(t)}\\leftarrow\\mathrm{Proj}_{\\mathcal{X}}\\left(\\pmb{x}_{i}^{(t-1)}-\\eta_{x}\\hat{\\pmb{g}}^{(t)}\\right)}\\end{array}$   \n3:   \n4:   \n5: end for   \n6: $\\pmb{y}^{(T+1)}\\gets\\mathsf{m a x-o r a c l e}\\Big(f(\\pmb{x}^{(T)},\\cdot);\\zeta\\Big)$ ", "page_idx": 52}, {"type": "text", "text": "Finally, we can state the theorem of convergence to an $\\epsilon$ -approximate saddle-point. ", "page_idx": 52}, {"type": "text", "text": "Theorem D.3. Let a function $f$ as the one in Theorem D.1. For a desired accuracy $\\epsilon>0$ Algorithm 3, (SGDMAx) with a tuning of $\\begin{array}{r}{T_{x}=O\\left(\\frac{\\ell_{1/2}^{2}}{\\epsilon^{3}}\\right)}\\end{array}$ $\\eta_{x}$ a max-racle accwracy $\\begin{array}{r}{\\zeta=O\\left(\\frac{\\nu\\epsilon^{2}}{\\ell^{2}}\\right)}\\end{array}$ and abatch size of $\\begin{array}{r}{M=\\operatorname*{max}\\left\\{1,\\frac{9\\sigma^{2}}{2\\epsilon^{2}}\\right\\}}\\end{array}$ guaranteesthathexisis a $t^{*}\\in[T]$ such that, ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r}{-\\nabla_{x}f\\left(\\pmb{x}^{(t^{\\star})},\\pmb{y}^{(t^{\\star}+1)}\\right)^{\\top}\\left(\\pmb{x}^{\\prime}-\\pmb{x}^{(t^{\\star})}\\right)\\leq\\epsilon,\\forall\\pmb{x}^{\\prime}\\in\\mathcal{X};}\\\\ {\\nabla_{y}f\\left(\\pmb{x}^{(t^{\\star})},\\pmb{y}^{(t^{\\star}+1)}\\right)^{\\top}\\left(\\pmb{y}^{\\prime}-\\pmb{y}^{(t^{\\star})}\\right)\\leq\\epsilon,\\forall\\pmb{y}^{\\prime}\\in\\mathcal{Y}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "$\\zeta$ can implemeled y $\\begin{array}{r}{T_{y}=\\tilde{O}\\left(\\frac{L}{L_{c}^{2}\\nu}+\\frac{L\\sigma^{2}}{L_{c}^{4}+\\nu^{2}}\\frac{1}{\\zeta}\\right)}\\end{array}$ hasge $\\begin{array}{r}{\\eta_{y}=\\operatorname*{min}\\left\\{\\frac{2}{9L},\\frac{L_{c}^{2}\\nu\\zeta}{10L\\sigma^{2}}\\right\\}}\\end{array}$ ", "page_idx": 52}, {"type": "text", "text": "Proof. The proof follows easily from the proof of projected gradient ascent in hidden-strongly.   \nconcave function found [43, Theorem 6] and Theorems B.1 and D.2. ", "page_idx": 52}, {"type": "text", "text": "Remark 2. It has been shown that when a function $f$ enjoys a hidden-strongly-concave reformulation, itsatisfiesglobal theProximal- $P E$ condition(orequivalently,globalKL condition)[43,67].While the equivalence between global $K E$ condition and quadratic growth condition has been proven [10,38] when $f$ is concave, to the authors\u2019best knowledge, this equivalence still remains unclear when $f$ is nonconcave. This means that we cannot use [83] to prove the smoothness of the maximum functionwhen the feasibility set of the maximizingvariableis constrained. ", "page_idx": 52}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 53}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 53}, {"type": "text", "text": "Justification: As claimed in the abstraction and introduction, our main contribution is the design and analysis of a learning algorithm for Adversarial Team Markov Games with polynomial iteration and sample complexity (in the parameters of the underlying Markov Game); this is captured by Theorem 3.3. ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 53}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 53}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Justification: The limitations of our work are captured in conclusion and future work for investigation. Moreover, all necessary assumptions have been properly cited. ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications wouldbe.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that infuence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational effciency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 53}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 53}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 54}, {"type": "text", "text": "Justification: The proof of the main result and auxiliary claims and lemmas can be found in the appendix and are properly referenced. All the assumptions have been properly defined in the Preliminaries section and section 3, 4, e.g., see Assumption 4.1. ", "page_idx": 54}, {"type": "text", "text": "Guidelines: ", "page_idx": 54}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 54}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 54}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 54}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 54}, {"type": "text", "text": "Guidelines: ", "page_idx": 54}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g.. to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 54}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 54}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 55}, {"type": "text", "text": "Justification: The paper does not include experiments requiring code. Guidelines: ", "page_idx": 55}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https : //nips . cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : // nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 55}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 55}, {"type": "text", "text": "Justification: The paper does not include experiments. Guidelines: ", "page_idx": 55}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 55}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 55}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 55}, {"type": "text", "text": "Justification: The paper does not include experiments. Guidelines: ", "page_idx": 55}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified. ", "page_idx": 55}, {"type": "text", "text": "\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). \u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 56}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 56}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 56}, {"type": "text", "text": "Justification: The paper does not include experiments. Guidelines: ", "page_idx": 56}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 56}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 56}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 56}, {"type": "text", "text": "Justification: The paper is of theoretical nature about learning Nash equilibria in Markov Games.   \nThe authors believe that the paper is aligned with NeurIPS Code of Ethics. ", "page_idx": 56}, {"type": "text", "text": "Guidelines: ", "page_idx": 56}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 56}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 56}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 56}, {"type": "text", "text": "Justification: The paper is of theoretical nature about learning Nash equilibria in Markov Games.   \nThe authors do not foresee any societal impacts. ", "page_idx": 56}, {"type": "text", "text": "Guidelines: ", "page_idx": 56}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 56}, {"type": "text", "text": "", "page_idx": 57}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 57}, {"type": "text", "text": "Answer: [NA]   \nJustification: The paper poses no such risks. Guidelines:   \n\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 57}, {"type": "text", "text": "", "page_idx": 57}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 57}, {"type": "text", "text": "Answer: [NA]   \nJustification: The paper does not use existing assets. Guidelines:   \n\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset'screators. ", "page_idx": 57}, {"type": "text", "text": "", "page_idx": 57}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 57}, {"type": "text", "text": "Answer: [NA]   \nJustification: The paper does not release new assets.   \nGuidelines: \u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 57}, {"type": "text", "text": "", "page_idx": 58}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 58}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 58}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 58}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 58}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 58}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 58}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 58}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 58}]