{"references": [{"fullname_first_author": "Jerry Chee", "paper_title": "Quip: 2-bit quantization of large language models with guarantees", "publication_date": "2023", "reason": "This paper proposes a 2-bit quantization method for LLMs that is both efficient and effective, which is highly relevant to the current work's focus on improving the performance of post-training quantization for LLMs."}, {"fullname_first_author": "Elias Frantar", "paper_title": "Optq: Accurate quantization for generative pre-trained transformers", "publication_date": "2023", "reason": "This paper introduces OPTQ, a gradient-free method for post-training quantization that achieves state-of-the-art results on LLMs, providing a strong baseline for comparison and potential integration with the proposed method."}, {"fullname_first_author": "Ji Lin", "paper_title": "Awq: Activation-aware weight quantization for llm compression and acceleration", "publication_date": "2023", "reason": "This paper proposes AWQ, a channel-wise scaling method for weight quantization that improves upon the baseline RTN method, serving as a relevant comparison point for evaluating the proposed technique."}, {"fullname_first_author": "Wenqi Shao", "paper_title": "Omniquant: Omnidirectionally calibrated quantization for large language models", "publication_date": "2023", "reason": "This paper introduces OmniQuant, a method that addresses the limitations of existing linear transformation-based PTQ methods by applying a non-linear transformation and is directly compared against in experiments."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Llama: Open and efficient foundation language models", "publication_date": "2023", "reason": "This paper introduces the LLaMA family of models, which serves as the experimental platform for evaluating the performance of the proposed weight magnitude reduction technique."}]}