[{"type": "text", "text": "FOOGD: Federated Collaboration for Both Out-of-distribution Generalization and Detection ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xinting Liao1, Weiming Liu1, Pengyang Zhou1, Fengyuan $\\mathbf{Y}\\mathbf{u}^{1}$ , Jiahe $\\mathbf{X}\\mathbf{u}^{1}$ , ", "page_idx": 0}, {"type": "text", "text": "Jun Wang2, Wenjie Wang3, Chaochao Chen1, Xiaolin Zheng1\u2217 ", "page_idx": 0}, {"type": "text", "text": "1 Zhejiang University, 2 OPPO Research Institute, 3 National University of Singapore {xintingliao, 21831010, zhoupy, zjuccc, xlzheng}@zju.edu.cn, junwang.lu@gmail.com, wenjiewang96@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Federated learning (FL) is a promising machine learning paradigm that collaborates with client models to capture global knowledge. However, deploying FL models in real-world scenarios remains unreliable due to the coexistence of in-distribution data and unexpected out-of-distribution (OOD) data, such as covariate-shift and semantic-shift data. Current FL researches typically address either covariate-shift data through OOD generalization or semantic-shift data via OOD detection, overlooking the simultaneous occurrence of various OOD shifts. In this work, we propose FOOGD, a method that estimates the probability density of each client and obtains reliable global distribution as guidance for the subsequent FL process. Firstly, $\\mathsf{S M}^{3}\\mathsf{D}$ in FOOGD estimates score model for arbitrary distributions without prior constraints, and detects semantic-shift data powerfully. Then SAG in FOOGD provides invariant yet diverse knowledge for both local covariate-shift generalization and client performance generalization. In empirical validations, FOOGD significantly enjoys three main advantages: (1) reliably estimating nonnormalized decentralized distributions, (2) detecting semantic shift data via score values, and (3) generalizing to covariate-shift data by regularizing feature extractor. The prejoct is open in https://github.com/XeniaLLL/FOOGD-main.git. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Federated learning (FL) [56] provides a distributed machine learning paradigm, which collaboratively models decentralized data resources. Specifically, each client models its data locally and server improves model performance by aggregating client models, which indirectly shares knowledge among clients and preserves privacy. FL further makes efforts to adapt real-world scenarios, i.e., adapting non-independent and identical distribution (non-IID) [39, 30]. ", "page_idx": 0}, {"type": "text", "text": "Beyond non-IID issues, deploying FL models in real-world also encounters different tasks of out-ofdistribution (OOD) shift [69, 26, 6], e.g., tackling covariate shifts (OOD generalization) and handling semantic shifts (OOD detection). In FL, OOD generalization task is devised to capture the invariant data-label relationships of covariate-shift data intra- and inter-client, which offers the potential of adapting unseen clients [17, 60, 70, 84]. The OOD detection task in FL aims to find semantic-shift data samples that do not belong to any known categories of all client data during FL training [83]. Both OOD generalization and detection simultaneously exist in FL, hindering the deployment of FL methods. Nevertheless, the existing work only tackles each OOD task in isolation. SCONE [3] proposes a unified margin-based framework to realize OOD generalization and OOD detection tasks in centralized machine learning. But it is infeasible to FL due to two reasons, i.e., being non-trivial in searching for consistent margin among non-IID distribution, and requiring outlier exposure of data. This motivates us to a crucial yet unexplored question: ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Can we devise a FL framework that adapts to wild data, which coexists with non-IID in-distribution (IN) data, covariate-shift (IN-C) data, and semantic-shift (OUT) data? ", "page_idx": 1}, {"type": "text", "text": "In this work, we simultaneously promote OOD generalization and detection by collaborating with clients in FL. The objectives of OOD generalization and detection vary among different clients due to their non-normalized and heterogeneous probability densities. This motivates us to build systematic and global guidance to distinguish IN, IN-C, and OUT data. As depicted in Fig. 1, for non-IID client distributions, we first estimate the probability density in each local client and then compose these local estimations for global distribution in server. Once a reliable global distribution estimation is established, we ", "page_idx": 1}, {"type": "image", "img_path": "D6MQrw9HFu/tmp/54495b55ecc75d43a59b303a8ac7a66f8a74589ab8e87a936f907ebb4257c3a4.jpg", "img_caption": ["Figure 1: Motivation of FOOGD. The distributions of two clients are non-IID, and we seek to estimate the global distribution among decentralized data. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "can leverage it to guide FL OOD tasks in deployment. However, this approach presents two challenges, i.e., CH1: How to estimate the reliable and global probability density among decentralized clients for detection? and CH2: How to enhance intra- and inter-client OOD generalization based on global distribution estimation? ", "page_idx": 1}, {"type": "text", "text": "To fill these gaps, we propose a federated collaboration framework named as FOOGD, which estimates client distribution in feature space via score matching with maximum mean discrepancy $(\\mathsf{S M}^{3}\\mathsf{D})$ and enhances the client model generalization by Stein augmented generalization (SAG). To solve CH1, inspired by the flexibility of score matching [58, 7], we originally devise $\\mathsf{S M}^{3}\\mathsf{D}$ to train score model that estimates limited and heterogeneous data distributions for each client, and aggregate score models in server as global estimation. Because the score values are vectors indicating position and changing degree of the log data density [55], $\\mathsf{S M}^{3}\\mathsf{D}$ brings the potential of discriminating OUT data in low-density areas with large change degree. However, it is unreliable to directly apply vanilla score matching for modeling decentralized data, which suffers from sparsity and multimodal complexity [72, 55]. To obtain a reliable density estimation, $\\mathsf{S M}^{3}\\mathsf{D}$ explores wider space by generating random samples via Langevin dynamic sampling, and constrains the generated samples to be similar to data samples via maximum mean discrepancy (MMD). To mitigate CH2, SAG regularizes feature invariance between data samples and its augmented version, which is measured by scorebased discrepancy. Though the existing generalization methods capture the invariance in feature space [1, 34], the vital feature information is inevitably lost due to strictly invariant constraints [87, 10]. This also deteriorates the performance of solving FL OOD generalization. With the benefits of distributional alignment based on Stein indentity [46], SAG in client model captures IN-C data in a similar feature space with IN data, which not only avoids representation collapse but also maintains diversifying information. Thus SAG makes FOOGD generalize to IN-C data from local covariate-shift distribution and unseen client distribution. ", "page_idx": 1}, {"type": "text", "text": "The main contributions are: (1) We are the first to study OOD generalization and detection in FL simultaneously, and formulate a evaluation on deploying FL methods in the wild data. (2) We propose FOOGD which estimates reliable global distributions based on arbitrary client probability densities, to guide both OOD generalization and detection. (3) We devise $\\mathtt{S M^{3}D}$ which not only explores wider probability space for density estimation, but also provides the score function values to detect OUT samples. (4) We utilize SAG to maintain the invariance between IN-C and IN data in feature space, which obtains better generalization without collapsing for FL scenarios. (5) We provide theoretical analyses and conduct extensive experiments to validate the effectiveness of FOOGD. ", "page_idx": 1}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 OOD Detection ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "OOD detection discriminates semantic shift (OUT) data during deployment time [3, 20, 53]. There are two main categories of OOD detection work, i.e., enhancing training-time regularization [48, ", "page_idx": 1}, {"type": "text", "text": "21, 75, 13], and measuring post-hoc detection function of a well-trained model [20, 74, 37]. The first category focuses on ensuring predictors produce low-confidence predictions for OOD data during training, which is effective but mainly requires access to real OUT data [21, 5, 80, 86]. By the way, selecting different auxiliary detection objectives [22, 57] unexpectedly varies the overall performance. The second category utilizes the classification logits [74], energy score [35, 48], and feature space estimation [37, 66] from pre-trained models, to detect the OUT data. This reduces the costly computation burden but rigidly relies on the data distribution captured in the pre-trained model. As one kind of methods in post-hoc way, density-based estimation methods [37, 66, 68] can relieve the cost of collecting or synthesizing representative OOD datasets, avoiding biased and ineffective detection [74, 35] and bringing the potential of densities composition. ", "page_idx": 2}, {"type": "text", "text": "2.2 OOD Generalization ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "OOD generalization targets extracting invariant feature-label relationships and maintaining the deployment performance of model with covariate-shift data in the open-world [31, 54]. To reach this goal, IRM-based work [2, 1, 34] utilizes invariant risk regularization to find invariant representations from different covariate shift data. Besides, there are various work calibrating invariant representations by distribution robust optimization methods [65, 16], feature alignment methods [15, 14], augmentaed training [10], gradient manipulation methods [24], diffusion modeling [82] and so on. SCONE [3] takes advantage of unlabeled wild mixture data to enhance generalization and build detectors simultaneously. However, SCONE is not suitable for FL, since it requires a hyper-parameter of energy margin and the outlier exposure data [83, 75]. To tackle the meta-task detection and generalization, Chen[9] propose an Energy-Based Meta-Learning (EBML) framework that learns meta-training distribution via two energy-based neural networks. However, it is tough to model two reliable energy models in decentralized models where data and computation resources are constrained. ", "page_idx": 2}, {"type": "text", "text": "2.3 Federated Learning with Wild Data ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In FL, wild data makes it challening in tackling non-IID modeling, OOD generalization, and OOD detection. Firstly, FL with non-IID data presents significant challenges in balancing global and local model performance [56, 8, 39, 43, 89, 42](Appendix C). Secondly, FL considers two aspects of generalization, i.e., (1) intra-client generality, and (2) inter-client generality. The intra-client generalization keeps the invariant relationship between data samples and class labels[26, 69, 70, 63], which is similar to centralized OOD generalization. The inter-client generality work captures invariant representation for heterogeneous client distributions, making the global model adaptive to a newly unseen client [84, 60, 17, 44]. Lastly, regarding OOD detection in FL, it is expected to detect semantic shift data out of the whole class categories set among decentralized data, yet avoid wrongly distinguishing unseen data classes of other clients. FOSTER [83] treats unseen data classes in each client as OUT, and enhances their detection capability via synthesizing virtual data with external classes of other clients. Different from the above methods, we aim to enhance OOD detection and generalization simultaneously by collaborating with different clients. Recently, FedGMM [77] utilizes a federated expectation-maximization algorithm to fit data distribution among clients by estimating Gaussian mixture models(GMM), and detects OUT data via computing GMM probability. It can only roughly capture the data distribution with the prior assumption of GMM. Meanwhile, a orthogonal paradigm of studies focus on tackling concept shifts in federated process [61, 29]. However, it overlooks the coexistence of wild data, resulting in suboptimal performance in federated tasks of OOD generalization and detection. ", "page_idx": 2}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Problem Setting ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Federated Learning Formulation with Wild Data. We first formulate the wild data in FL deployment and provide the optimization goal of FL. Empirically, we assume a dataset decentralizes among $K$ clients, i.e., $\\mathcal{D}=\\cup_{k\\in[K]}\\mathcal{D}_{k}$ . The data distribution of $k-$ th client is simulated following the real-world wild data, i.e., $\\dot{\\mathcal{D}_{k}}=\\mathcal{D}_{k}^{\\mathrm{IN}}+\\mathcal{D}_{k}^{\\mathrm{IN-C}}+\\mathcal{D}_{k}^{\\mathrm{OUT}}$ . The objective of the FL model, which simultaneously tackles OOD generalization and detection, is defined as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{argmin}_{\\pmb{\\theta}_{f},\\pmb{\\theta}_{g}}\\Sigma_{k=1}^{K}w_{k}\\mathbb{E}_{\\pmb{x}\\sim p_{\\mathcal{D}_{k}}}[\\mathcal{L}_{k}(\\pmb{\\theta}_{f},\\pmb{\\theta}_{g};\\mathcal{D}_{k})],}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "image", "img_path": "D6MQrw9HFu/tmp/e2b549353c3f62ffa7777283fdd2216d457f9b21500a4d958bf87103fc1446c9.jpg", "img_caption": ["Figure 2: Framework of FOOGD. For each client, we have main task feature extractor, a $\\mathsf{S M}^{3}\\mathsf{D}$ module estimates score model (Eq. (8)) for detection, and a SAG module regularizes feature extractor for enhancing generalization. The server aggregates models and obtains global distribution. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "where $\\mathcal{L}_{k}(\\pmb{\\theta}_{g},\\pmb{\\theta}_{f};\\mathcal{D}_{k})=\\ell_{k}^{\\mathrm{IN}}+\\ell_{k}^{\\mathrm{IN-C}}+\\ell_{k}^{\\mathrm{OUT}}$ , and $w_{k}$ represents weight ratio for the $k$ -th client. The OOD measurements $\\ell_{k}^{\\mathrm{IN}},\\ell_{k}^{\\mathrm{IN-C}},\\ell_{k}^{\\mathrm{OUT}}$ co espondingly justify the IN generalization, IN-C generalization, and OUT detection in each client $k$ ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ell_{k}^{\\mathrm{IN}}:=\\mathbb{E}_{(\\boldsymbol{x},\\boldsymbol{y})\\sim p_{\\mathcal{D}_{k}^{\\mathrm{IN}}}}\\left(\\mathbb{I}\\left\\{y_{\\mathrm{pred}}\\left(f_{\\theta}(\\boldsymbol{x})\\right)\\neq\\boldsymbol{y}\\right\\}\\right)}\\\\ &{\\ell_{k}^{\\mathrm{IN-C}}:=\\mathbb{E}_{(\\boldsymbol{x},\\boldsymbol{y})\\sim p_{\\mathcal{D}_{k}^{\\mathrm{IN-C}}}}\\left(\\mathbb{I}\\left\\{y_{\\mathrm{pred}}\\left(f_{\\theta}(\\boldsymbol{x})\\right)\\neq\\boldsymbol{y}\\right\\}\\right)}\\\\ &{\\ell_{k}^{\\mathrm{OUT}}:=\\mathbb{E}_{(\\boldsymbol{x},\\boldsymbol{y})\\sim p_{\\mathcal{D}_{k}^{\\mathrm{OUT}}}^{\\mathrm{OUT}}}\\left(\\mathbb{I}\\left\\{g_{\\theta}(\\boldsymbol{x})=\\mathrm{IN}\\right\\}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $f_{\\theta}(\\cdot)$ is main task model, $g_{\\pmb\\theta}(\\cdot)$ is detector, $\\mathbb{I}$ is indicator function, and $y_{\\mathrm{pred}}$ is predicted label. ", "page_idx": 3}, {"type": "text", "text": "Framework Overview. To optimize the FL objective in Eq. (1), we propose FOOGD whose framework overview is depicted in Fig. 2. For $K$ clients with non-IID data, FOOGD composes their local distributions and aggregate their model parameters in server. In each client, the data samples $\\textbf{\\em x}$ as well as its fourier augmented [79] counterparts $\\widehat{\\mathbf{\\xi}}^{x}$ , are fed into the same feature extractor of main task $f_{\\theta}(\\cdot)$ to obtain their latent features, $z=f_{\\theta}(x)$ and $\\widehat{z}=f_{\\theta}(\\widehat{{\\pmb x}})$ , respectively. To avoid overwhelming communication costs brought by score models, score matching with maximum mean discrepancy $\\bar{(\\mathrm{SM}^{3}\\mathrm{D})}$ trains a score model $s_{\\theta}(\\cdot)$ in feature space. This model captures the data distribution by estimating the gradient of log densities (score functions) of latent features $_{z}$ , i.e., $s_{\\theta^{*}}(z)=\\nabla_{z}\\log p_{\\theta}(z)\\overset{*}{\\approx}\\nabla_{z}\\log p_{\\mathcal{D}}(z)$ [7, 72]. Then score model serves as the detector for the objective in Eq. (2c), discriminating OUT based on the norm of score function values. Besides, Stein augmented generalization (SAG) enhances the generalization capabilities of the feature extractor $f_{\\theta}(\\cdot)$ , by the distribution regularization defined via score model. Because score model based distribution ensures that data features and their neighboring augmented samples, e.g., $_{\\textit{z}}$ and $\\widehat{z}$ , maintain a consistent probability space [46]. The local modeling iterates until performance converges. ", "page_idx": 3}, {"type": "text", "text": "In each communication round, since both main task model and score model are parameterized neural networks, it is practical to follow conventional weighted average aggregation [56], i.e., ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\{\\pmb{\\theta}_{s},\\pmb{\\theta}_{f}\\}=\\sum_{k=1}^{K}w_{k}\\{\\pmb{\\theta}_{s}^{k},\\pmb{\\theta}_{f}^{k}\\},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "with wk = kK|=D1 k||Dk|, \u2200k\u2208[K]. These collaborative processes among clients continue until the global model converges, bringing reliable and comprehensive global distribution in the form of global score model. We introduce the details later and illustrate the algorithm of FOOGD in Appendix A Algo. 1. ", "page_idx": 3}, {"type": "text", "text": "3.2 $\\mathtt{S M^{3}D}$ : Estimating Score Model for Detection ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this part, we introduce the estimation of FL data distribution and how to utilize it for detection. As shown in Fig. 1, a reliable probability density is eagerly necessary for distinguishing IN and OUT data [75, 27]. Different from existing centralized OUT aware and OUT synthesis methods [13, 21], the FL framework suffers from the accessibility of OUT data [83]. In this study, we aim to explicitly capture the local IN data distribution of clients, and subsequently compose them to reliable global distribution for discrimination. However, it remains challenging to estimate heterogeneous and non-normalized probability density without prior information during FL modeling. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Dynamic Feature Density Estimation. FOOGD estimates score model via score matching in the feature space [72, 32, 7], i.e., $p_{\\mathcal{D}}(z)$ , circumventing the need for prior distribution knowledge or distribution normalization [72]. Moreover, it alleviates the computational burden by modeling the score of latent representations in a smaller, yet more expressive and continuous space, compared to the scores of the original data [71]. Specifically, given the latent features $z=f_{\\theta}(x)$ , we perturb it via adding random noise $\\pmb{v}\\sim\\mathcal{N}(\\mathbf{0},\\pmb{I})$ to obtain $\\tilde{z}=z+\\sigma{\\pmb v}$ , which follows noise-perturbed data distribution $p_{\\sigma}(\\tilde{z}|z):=\\mathcal{N}\\left(\\tilde{z};z,\\sigma^{2}I\\right)$ . And we model it with noise conditional score model [67] $s_{\\theta}\\left(\\tilde{z},\\sigma\\right)$ by minimizing the denoising score matching (DSM) loss, i.e., ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}\\ell_{\\mathrm{DSM}}=\\frac{1}{2}\\mathbb{E}_{p_{\\mathcal{D}}(z)p_{\\sigma}(\\tilde{z}\\,|\\,z)}\\left\\|s_{\\theta}\\left(\\tilde{z},\\sigma\\right)-\\nabla_{\\tilde{z}}\\log p_{\\sigma}(\\tilde{z}\\,|\\,z)\\right\\|^{2},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the score function of $\\nabla_{\\tilde{z}}\\log p_{\\sigma}(\\tilde{z}\\mid z)$ for $d$ -dimensional features, is computed as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\nabla_{\\tilde{z}}\\log p\\left(\\tilde{z}\\mid z\\right)=\\nabla_{\\tilde{z}}\\left[\\log\\frac{1}{\\left(\\sqrt{2\\pi\\sigma^{2}}\\right)^{d}}\\exp\\left\\{-\\frac{\\|\\tilde{z}-z\\|^{2}}{2\\sigma^{2}}\\right\\}\\right]\\ \\ =-\\frac{\\tilde{z}-z}{\\sigma^{2}}=-\\frac{v}{\\sigma}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "When the noise get to zero, i.e., $\\sigma\\rightarrow0$ , we have the exact score values $s_{\\theta}\\left(\\tilde{z},\\sigma\\right)=s_{\\theta}(z)$ . However, score model based density estimation will inevitably fail once the distribution contains sparse data samples [67, 55, 67] or multiple modalities [32], as shown in Fig. 3 (a). $\\mathtt{S M^{3}D}$ is motivated to broadly explore the generated random features $z_{\\mathrm{gen}}$ that samples from the whole distribution space. In detail, $\\mathsf{S M}^{3}\\mathsf{D}$ first sample from a random distribution, e.g., Normal distribution, as the start latent features, i.e., $z^{0}\\sim\\mathcal{N}(\\mathbf{0},I)$ . Then $\\mathtt{S M^{3}D}$ utilizes $T$ -step Langevin dynamic sampling [67] (LDS) from density vector fields modeled by the score model, to derive generated latent features $z_{\\mathrm{gen}}=z^{T}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\pmb{z}^{t}=\\pmb{z}^{t-1}+\\frac{\\epsilon}{2}s\\pmb{\\theta}(\\pmb{z}^{t-1},\\sigma)+\\sqrt{\\epsilon}\\pmb{w}^{t},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "with $\\epsilon$ indicating the step size and $\\pmb{w}^{t}\\sim\\mathcal{N}(\\mathbf{0},\\pmb{I})$ introducing stochasticity in each step. Lastly, the distribution of a batch of the generated features $\\bar{Z_{\\mathrm{gen}}}=\\{\\bar{z_{\\mathrm{gen},i}}\\}_{i=1}^{B}$ , i.e., $p_{\\mathrm{gen}}(z_{\\mathrm{gen}})$ , is supposed to approximate the distribution of original features ${\\cal Z}=\\{z_{i}\\}_{i=1}^{B}$ , i.e., $p_{\\mathcal{D}}(z)$ , with the calibration of maximum mean discrepancy $(\\mathrm{MMD}(Z,Z_{\\mathrm{gen}}))$ matching: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\ell_{\\mathrm{MMD}}=\\mathbb{E}_{z_{D},z_{D}^{\\prime}\\sim p_{D}}[k(z_{D},z_{D}^{\\prime})]-2\\mathbb{E}_{z_{D}\\sim p_{D},z_{g\\mathrm{en}}^{\\prime}\\sim p_{g\\mathrm{en}}}[k(z_{D},z_{g\\mathrm{en}}^{\\prime})]+\\mathbb{E}_{z_{g\\mathrm{en}},z_{g\\mathrm{en}}^{\\prime}\\sim p_{g\\mathrm{en}}}[k(z_{g\\mathrm{en}},z_{g\\mathrm{en}}^{\\prime})].}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\begin{array}{r}{k(z,z^{\\prime})\\,=\\,\\exp(\\frac{1}{b}\\|z-z^{\\prime}\\|^{2})}\\end{array}$ with bandwidth $h$ is Gaussian kernel function [47, 49] within a unit ball in universal Reproducing Kernel Hilbert Space (RKHS). Because MMD is a non-parametric method that accurately measures the distance between two densities in RKHS, it provides reliable estimations and adapts well to complex data modalities [4]. This approach mitigates the limitations of directly using DSM to estimate distributions by exploring a wider feature space. Unfortunately, as depicted in Fig. 3 (d), simply using MMD matching does not enhance density estimation, when the target distribution is unknown or inaccurate. But it is quite necessary that the latent distribution is inaccurate and heterogeneous in FL. To fill this gap, $\\mathtt{S M^{3}D}$ seeks to harness and integrate the strengths of both density estimation paradigms, via a trade-off coefficient $\\lambda_{m}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\ell^{\\mathrm{OUT}}=(1-\\lambda_{m})\\ell_{\\mathrm{DSM}}+\\lambda_{m}\\ell_{\\mathrm{MMD}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "image", "img_path": "D6MQrw9HFu/tmp/94fd48258b52988e10f4a9b50fbe4bae4e2ce9de972120328fc27651e990e2f9.jpg", "img_caption": ["Figure 3: Motivation of $\\mathsf{S M}^{3}\\mathsf{D}$ . Red points are sampled from target data distribution, and the blue points are generated by LDS in Eq. (6). "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "In this way, $\\mathsf{S M}^{3}\\mathsf{D}$ brings an accurate and flexible implementation for non-normalized data distribution. The implementation procedure of $\\mathtt{S M^{3}D}$ is in Appendix A Algo.2. To illustrate the effectiveness of $\\mathsf{S M}^{3}\\mathsf{D}$ , we further visualize a density estimation of 2-D toy example in Fig. 3. In detail, we model the red target points by tuning a series of coefficients, i.e., $\\lambda_{m}=\\bar{\\{0,0.1,0.5,1\\}}$ in Eq. (8). As we can see, with the mutual impacts between score matching and MMD estimation, $\\mathtt{S M^{3}D}$ has more compact density estimation when $\\lambda_{m}=0.1$ , compared with blankly using score matching $\\lambda_{m}=0)$ ) or simply using MMD $\\lambda_{m}=1$ ). As a brand new objective of density estimation, $\\mathtt{S M^{3}D}$ expands the searching range and depth of score modeling, making it possible to comprehensively model data density. Moreover, with the calibration of MMD estimation, original data features and the generated samples based on the score model are effectively matched. Hence $\\mathtt{S M^{3}D}$ could ensure a more aligned and reliable density estimation for sparse and multi-modal data. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "OOD detection in clients. Remind that the score function indicates the gradient of the log density, which are actually vector fields pointing to the highest density area, as shown in the score function visualization of Fig. 2. The IN data should point to the high density and reflect the distance via its vector norm. While the OUT data cannot present this satisfying property and further exposure boldly, since the OUT data is always in low-density area [48, 55]. That is, the norm of the score $\\begin{array}{r}{\\|s_{\\theta^{\\ast}}(z)\\|=\\|\\frac{\\nabla_{z}p_{\\theta^{\\ast}}(z)}{p_{\\theta^{\\ast}}(z)}\\|}\\end{array}$   \nIt indicates the larger the norm of score is, the more likely the data sample is OUT. For negative threshold $\\tau<0$ , we have detection score function: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{IsOUT}(x)=\\mathrm{True},\\qquad\\mathrm{when}\\quad\\|s_{\\theta^{*}}(f_{\\theta^{*}}(x))\\|>-\\tau;\\quad\\mathrm{otherwise},\\quad\\mathrm{IsOUT}(x)=\\mathrm{False}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "3.3 SAG: Enhancing Feature Extractor for Gerneralization ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we will illustrate how to enhance generalization capability of feature extractor in FOOGD. In FL scenarios, solving OOD generalization needs not only to keep the local IN-C data classification correctly, but also to maintain performance consistency among all participating clients. The non-IID issue creates a contradiction between achieving both targets. This is because enhancing IN-C accuracy intra-client requires diversification across different classes, whereas inter-client generalization benefits from all IN data being closely clustered, irrespective of class distinctions. Hence, it is expected to balance the feature diversification of different classes and the feature consistency of in-distribution, to realize the consistent data-label relationships intra- and inter-client. ", "page_idx": 5}, {"type": "text", "text": "Diversifying Feature Invariance Augmentation. FOOGD regularizes invariance among client feature extractors using distribution-aware divergence between the original data $\\textbf{\\em x}$ and its augmented version $\\widehat{\\pmb{x}}=\\mathcal{T}(\\pmb{x})$ by transformation $\\tau$ . To address this, we propose SAG, which utilizes global distribution and optimizes distributional invariance between latent features of the original and augmented data. This approach maintains the distinguishable diversification of features and consistent data-label mapping across clients. ", "page_idx": 5}, {"type": "text", "text": "In the feature space, SAG regularizes original data samples to be aligned with augmented ones, i.e., aligning $z=f_{\\theta}(x)$ and $\\widehat{z}=f_{\\theta}(\\widehat{{\\pmb x}})$ . However, directly computing the norm regularization between $_{\\textit{z}}$ and $\\widehat{z}$ will cause mode   collapse  [28] in $\\mathrm{FL}$ , further degrading the estimation of score model $s_{\\theta}(\\cdot)$ based  o n $\\mathtt{S M^{3}D}$ . While contrastive methods [62, 73, 51, 50] like FedICON [69], and L-DAWA [64] ensure diversification and alignment for generalization, they rely on selecting negative samples instead of leveraging global distribution knowledge. Consequently, they fail to maintain consistent invariance among clients. Instead, SAG alternatively introduces kernelized Stein operator guided by score function, i.e., ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\boldsymbol{A}_{p}\\phi(\\boldsymbol{z})=\\phi(\\boldsymbol{z})\\nabla_{\\boldsymbol{z}}\\log{p(\\boldsymbol{z})}+\\nabla_{\\boldsymbol{z}}\\phi(\\boldsymbol{z}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\phi(z)$ is implemented with kernel function $k(\\cdot,\\cdot)$ mentioned in Eq. (7) [46], while $p(z)$ and $q(\\widehat{z})$ are the distributions for a batch of features ${\\cal Z}=\\{z_{i}\\}_{i=1}^{B}$ , and $\\widehat{\\boldsymbol{Z}}=\\{\\widehat{\\boldsymbol{z}}_{i}\\}_{i=1}^{B}$ , respectively. By utilizing the kernelized Stein operator, SAG encourages the samples o f augme nted features to align with high probability regions of the original features. Additionally, the second term of (10) improves feature diversification and prevents data from collapsing directly to the original distribution modes. According to a fundamental theory named as Stein identity, i.e., $\\mathbb{E}_{q(x)}\\left[{\\cal A}_{q}\\bar{\\phi}(x)\\right]=0$ for arbitrary distribution $q(x)$ [46], Stein operator brings the potential of measuring two data distributions with the guidance of global distribution estimation. Because score models capture local probability densities and are aggregated into a global score model on the server, they inherit distribution information from all participating clients. Specifically, we first illustrate kernelized Stein discrepancy (KSD) [46, 41, 45] that measures the distribution discrepancy between original data $p(z)$ and augmented data $q(\\widehat{z})$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{KSD}(p(z),q(\\widehat{z}))=\\mathbb{E}_{\\widehat{z},\\widehat{z}^{\\prime}\\sim q}[s_{\\theta}(\\widehat{z})^{\\top}s_{\\theta}(\\widehat{z}^{\\prime})k(\\widehat{z},\\widehat{z}^{\\prime})+s_{\\theta}(\\widehat{z})^{\\top}\\nabla_{\\widehat{z}^{\\prime}}k(\\widehat{z},\\widehat{z}^{\\prime})}\\\\ &{\\qquad\\qquad\\qquad+~s_{\\theta}(\\widehat{z}^{\\prime})^{\\top}\\nabla_{\\widehat{z}}k(\\widehat{z},\\widehat{z}^{\\prime})+\\mathrm{trace}(\\nabla_{\\widehat{z}}\\nabla_{\\widehat{z}^{\\prime}}k(\\widehat{z},\\widehat{z}^{\\prime}))].}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We provide the full induction of KSD between original data and augmented data in Appendix B.2. And $\\mathrm{KSD}(p(z),q(\\widehat{z}))$ equals zero if and only if $p(\\bar{z})$ and $q(\\widehat{z})$ are the same. By taking the derivative of KSD, we can ob tain the updating direction of moving $\\widehat{z}$ towards $_{z}$ , which not only keep the invariance of features, but also guarantee diversification avoiding collapse. Therefore, the augmented representation $\\hat{\\boldsymbol Z}$ has minimal KSD with the original latents $Z$ . This ensures that the final objective of the feature e xtractor is to minimize the subsequent classification error (between predictions $Y_{\\mathrm{pred}}$ and ground truth $\\mathbf{Y}_{\\mathrm{gr}}$ ) and achieve invariant alignment: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\ell^{\\mathrm{IN}}+\\ell^{\\mathrm{INC}}=\\mathrm{CrossEntropy}(Y_{\\mathrm{pred}},Y_{\\mathrm{gr}})+\\lambda_{a}\\,\\mathrm{KSD}(p(Z),q(\\widehat{Z})).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Besides, the score model in Eq. (11) communicates among different clients to obtain the global distribution, making it possible to be reliable guidance of invariance among clients. This makes SAG a potential generalization approach for modeling feature invariance in the overall FL scenario, even acting warm-start for unseen clients. Therefore, FOOGD is capable of both local IN-C data generalization and consistent performance generalization of clients. The algorithm of SAG can be found in Appendix A Algo. 3. ", "page_idx": 6}, {"type": "text", "text": "4 Theoretical Discussion ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we provide the error bound of modeling score model via $\\mathsf{S M}^{3}\\mathsf{D}$ in federated scenarios, and provide the error bound in Theorem 4.1. Besides, the federated training procedure of score model is the same with the main task model. This indicates that our federated learning convergence bound is unchanged, following [40]. We provide more theoretical details in Appendix B. ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.1 (Error Bound of Decentralized Score Matching via $\\mathrm{SM^{3}D}\\mathrm{^{\\circ}}$ ). Assume the original ${\\mathrm{MMD}}(Z,Z_{g e n})\\leq C$ for randomly initialized score model $s_{\\theta}(z)$ in Eq. (7), the score model achieves optimum and MMD decreases. By Lemma B.1, we can obtain the final error bound of global $s_{\\theta}(\\cdot)$ as: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\|s_{\\theta}(z)-\\nabla_{z}\\log p_{\\mathcal{D}}(z)\\|^{2}\\leq\\frac{v^{\\top}v}{\\sigma^{2}}-\\mathbb{E}_{p_{\\mathcal{D}}(z)}[\\|\\nabla_{z}\\log p_{\\mathcal{D}}(z)\\|^{2}]+\\frac{|\\mathcal{D}|}{B}C,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $C$ is the upper bound of the MMD, $B$ is batch size, and $|\\mathcal D|$ is the data amount. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "5.1 Experimental Setups ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Datasets. Following SCONE [3], we choose clear Cifar10, Cifar100 [33], and TinyImageNet [36] as the IN data, and select the corresponding corrupted versions [19], i.e., Cifar10-C, Cifar100-C and TinyImageNet-C as IN-C data. We evaluate detection with five OUT image datasets: SVHN [59], Texture [11], iSUN [78], LSUN-C and LSUN-R [81]. To simulate the non-IID scenarios, we sample data by label in a Dirichlet distribution parameterized by non-IID degree [23], i.e., $\\alpha$ , for $K$ clients. The smaller $\\alpha$ simulates the more heterogeneous client data distribution in federated settings. To evaluate FOOGD on unseen client generalization data, we also use PACS [38] dataset for leave-one-out domain generalization. Details of dataset simulation are in Appendix D.1. ", "page_idx": 6}, {"type": "text", "text": "Comparison Methods and Evaluations. We study the performance of FOOGD with the state-of-theart (SOTA) federated learning model and FedAvg-like derivant of SOTA centralized OOD methods, i.e., LogitNorm [76] (FedLN), ATOL [88] (FedATOL), T3A [25] (FedT3A). We compare FOOGD with three types of baseline models, i.e., (1) Vanilla FL model: FedAvg [56] and FedRoD [8], (2) FL with OOD detection: FOSTER [83], FedLN, and FedATOL, (3) FL with OOD generalization: FedT3A, FedIIR [17], FedTHE [26], FedICON [69]. For evaluation, we report the accuracy of IN data (ACCIN) and IN-C data (ACC-IN-C) to validate IN generalization and OOD generalization, respectively. We compute the maximum softmax probability [20] (MSP) and report the standard metrics used for OOD detection, i.e., the area under the receiver operating characteristic curve (AUROC), and the false positive rate at threshold corresponding to a true positive rate of $95\\%$ (FPR95) [20]. ", "page_idx": 6}, {"type": "text", "text": "Implementation Details. We choose WideResNet [85] as our main task model for Cifar datasets, and ResNet18 [18] for TinyImageNet and PACS, and optimize each model 5 local epochs per communication round until converging with SGD optimizer. We conduct all methods at their best and report the average results of three repetitions with different random seeds. We consider client number $K=10$ , participating ratio of 1.0 for performance comparison, and the hyperparameters $\\lambda_{m}=0.5$ , $\\lambda_{a}=0.05$ . We provide the full implementation details in Appendix D.2. ", "page_idx": 6}, {"type": "image", "img_path": "D6MQrw9HFu/tmp/6d58d9846b2e943e404ce032d6eb84672ed68f842eb8761f449f05af726c3f3a.jpg", "img_caption": ["Figure 4: T-SNE visualizations of FedAvg and FedRod with FOOGD. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "table", "img_path": "D6MQrw9HFu/tmp/ae200e2faff70547fc5ceb42a2ac87cb9549a0b8af35f533ed315feab1444e2b.jpg", "table_caption": ["Table 1: Main results of federated OOD detection and generalization on Cifar10. We report the ACC of brightness as IN-C ACC, the FPR95 and AUROC of LSUN-C as OUT performance. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "5.2 Experimental Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Performance Comparison on non-IID data. We categorized our baseline models into two groups based on whether they consider personalization. The results for Cifar10, Cifar100, and TinyImageNet are shown in Tab. 1, Tab. 2, and Tab. 7 in Appendix E.1, respectively. For the first group without considering personalization, the existing centralized OOD methods, i.e., LogitNorm (FedLN), ATOL (FedATOL) and T3A (FedT3A), are not directly competitive among different non-IID scenarios. Though FedATOL achieves satisfying results for both generalization and detection tasks on Cifar10 $\\alpha=5$ , it fails neither in smaller $\\alpha$ and dataset containing more classes (i.e., Cifar100). Meanwhile, the vanilla FedAvg degrades its performance in OOD generalization for both Cifar10 and Cifar100 data, and shows no potential of detecting OUT data samples. FedIIR pays more effort to maintain the inter-client generalization via restricting model consistency, making it less effective in non-IID settings. For the second group of personalized FL methods, personalization is quite necessary for both IN data generalization and IN-C generalization, which is similarly illustrated in FedTHE [26] and FedICON [69]. In general, personalized methods are worse in FL detection than non-personalized methods, indicating that there is a conflict between detecting OUT data and enhancing prediction in non-IID setting. More surprisingly, we also discover that personalized adaption methods also detect outliers better compared with vanilla FedRod model. FOSTER has better detection in more heterogenous data distribution, i.e., $\\alpha=0.1$ , compared with its results in $\\alpha\\,=\\,5$ , but its overall performance is supposed to enhance in the future. FOOGD is a flexible FL framework and achieves significant results for wild data tasks, i.e., IN generalization, IN-C generalization, and OUT detection, on Cifar10, Cifar100, and TinyImageNet. Specifically, FOOGD achieves comparable performance in enhancing both FedAvg and FedRod, free of the FL framework constraints. FOOGD enjoys the benefits of $\\mathsf{S\\bar{M}^{3}D}$ , achieving distinguishable detection improvement by Eq. (9). Besides, the regularization of score model with global distribution makes SAG regularize main task feature extractor better than contrastive-based methods, e.g., FedICON, and rebalanced-methods, e.g., FedTHE and original FedRoD. ", "page_idx": 7}, {"type": "table", "img_path": "D6MQrw9HFu/tmp/6cd0f20b7073fac476f053cf474284234161475673d12bcc92a485acdaf08a5d.jpg", "table_caption": ["Table 2: Main results of federated OOD detection and generalization on Cifar100. We report the ACC of brightness as IN-C ACC, the FPR95 and AUROC of LSUN-C as OUT performance. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "D6MQrw9HFu/tmp/8cbf8b1abc94cd7a550523796e40638ccf5e06c9eec9989c20d9bff2445fbfdf.jpg", "table_caption": ["Table 3: Cifar10 ablation study on varying $\\alpha$ modeled by FedAvg. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "D6MQrw9HFu/tmp/81a2e53feeb8526f5e64c7cca8702b3cf066b1611b8a579c5afd870e4df427e6.jpg", "table_caption": ["Table 4: Cifar100 ablation study on varying $\\alpha$ modeled by FedAvg. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Ablation Studies. We devise the variants of FOOGD , i.e., fix backbone, w/o $\\mathtt{S M^{3}D}$ , and w/o SAG, to study the effectiveness of our three main ideas: (1) obtaining reliable global distribution as guidance, (2) estimating score model by $\\mathsf{S M}^{3}\\mathsf{D}$ , and (3) enhancing FL method generalization by SAG, respectively. From Tab. 3 and Tab. 4, simply modeling score model enhances detection slightly, since it brings the knowledge of global distribution. When we remove $\\mathsf{S M}^{3}\\mathsf{D}$ , the estimation of data probability is severely impacted, bringing no detection capability. While the generalization performance decreases once we remove SAG. Moreover, compared with fix backbone, both w/o $\\mathsf{S\\bar{M}^{3}D}$ and w/o SAG have better generalization and detection results, indicating the necessity of regularizing feature extractor with global distribution. ", "page_idx": 8}, {"type": "text", "text": "Visualization. To explore the wild data distribution of FL OOD methods, we visualize T-SNE of data representations in Fig. 4, and the detection score distributions in Fig. 5, on Cifar10 $\\alpha=5$ for FedAvg+FOOGD , FedRoD+FOOGD and their runner-up methods, FedATOL and FedTHE, respectively. It is evident that FOOGD represents IN-C data more tight with IN data, and constructs a comparably clear decision boundary between IN data and OUT data. Besides, we also discover that FOOGD will push OUT data away from its IN and IN-C data, which validates the guidance from the global distribution. Additionally, in Fig. 5, FOOGD makes the modes among IN, IN-C, and OUT, more separable than existing methods. This also proves the effectiveness of FOOGD in detection task. ", "page_idx": 8}, {"type": "text", "text": "Extensive experiments on other IN-C and OUT data. In this part, we study the performance evaluation of FOOGD in additional IN-C and OUT datasets. In Tab. 5, we can find that FOOGD consistently enhances the detection capability for different OUT data, validating the effectiveness of estimating global distribution via $\\mathrm{SM^{3}D}$ . Meanwhile, we compute the average results of different IN-C accuracy for FL models trained on Cifar10 and Cifar100 in Fig. 6. The $+F O O G D$ in each group is short for $F e d A\\nu g\\!+\\!F O O G D$ and FedRod $+F O O G D,$ , respectively. We provide the details in Appendix E.7 Tab. 13 and Tab 14. FOOGD consistently improve the generalization in all unseen IN-C data, indicating the effectiveness of enhancing feature extractor via SAG. ", "page_idx": 8}, {"type": "image", "img_path": "D6MQrw9HFu/tmp/7c11852fe8c5df76f553ca2671d6ca4a527314f2c971670e775df0833093c86b.jpg", "img_caption": ["Figure 5: Detection score distribution of FL methods on Cifar10 $\\alpha=5.0)$ ). "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "D6MQrw9HFu/tmp/defe04ae477699dc8ef016f8ccf6a6ca06314b2af39ee92190da94beecb6e772.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Client Generalization on PACS Dataset. To validate the effectiveness of FOOGD in domain generalization tasks, i.e., each client contains one domain data and we train domain generalization model by leave-one-out, following FedIIR [17]. To compare fairly, we pretrain all models from scratch and utilize adaption methods as stated in their main paper. In terms of Tab. 6, FOOGD obtains performance improvements for FedAvg and FedRoD. Compared with existing adaption methods, FOOGD achieves outstanding results even in the toughest task, i.e., leaving Sketch domain out. This also concludes that FOOGD is capable of inter-client generalization, via utilizing global distribution knowledge. ", "page_idx": 9}, {"type": "text", "text": "Hyperparameter sensitivity studies and other empirical studies. Due to the space limitation, we leave the other relevant experiments in Appendix E. Summarily, we study four additional evaluations: (1) In Tab. 10 We compute different detection metrics, i.e., MSP, energy score, and ASH, and validate that Eq. (9) is consistently powerful in detection. (2) We ", "page_idx": 9}, {"type": "table", "img_path": "D6MQrw9HFu/tmp/8de1880ac2ee6fc8dd10d9e6aad3b4aedc882824e027423827c83a6bc973fea1.jpg", "table_caption": ["Table 6: OOD generalization task for PACS. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "vary the coefficient of $\\mathsf{S M}^{3}\\mathsf{D}\\ \\lambda_{m}=\\{0.1,0.2,0.5,0.8,1\\}$ in Fig. 11(a)-Fig. 11(b), and vary the coefficient of SAG $\\textsuperscript{\\scriptsize\\backslash}\\lambda_{a}=\\{0,0.01,0.05,0.1,0.2,0.5,0.8\\}$ in Fig. 11(c), to obtain the best modeling in FOOGD. (3) We vary the number of participating clients in Fig. 10 and found FOOGD can have better results among different participating clients. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion and Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we consider enhancing both detection and generalization capability of FL methods among non-IID settings. To realize it, we try to model global distribution by collaborating clients, and propose FOOGD, which consists of $\\mathtt{S M^{3}D}$ for estimating score model for detection, and SAG to enhance the invariant representation for generalization. We conduct extensive experiments to validate the effectiveness of FOOGD: (1) reliably and flexibly estimating non-normalized decentralized distribution, (2) detecting semantic shift data via the norm of score values, and (3) generalizing adaption of covariate shift data by regularizing feature extractor invariant distribution discrepancy. ", "page_idx": 9}, {"type": "text", "text": "In the future, we plan to integrate privacy enhancement techniques, such as differential privacy, into FOOGD. While the score model in FOOGD captures the score function of the data probability in the latent space, which is extremely difficult to be used for reconstructing the original data by attacking. The primary risk exposure for each client arises from the exchange of model parameters, i.e., the feature extractor and score model. Hence, FOOGD has a comparable level of privacy exposure as existing FL methods dealing with non-IID and OOD shifts, acquiring to be addressed comprehensively. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported in part by the Leading Expert of \u201cTen Thousands Talent Program\u201d of Zhejiang Province, China (No. 2021R52001), the National Natural Science Foundation of China (No. 62172362), and the Fundamental Research Funds for the Central Universities (No. 226202400241). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Kartik Ahuja, Karthikeyan Shanmugam, Kush Varshney, and Amit Dhurandhar. Invariant risk minimization games. In Proceedings of the 37th International Conference on Machine Learning, volume 119, pages 145\u2013155. PMLR, 2020.   \n[2] Martin Arjovsky, L\u00e9on Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. arXiv preprint arXiv:1907.02893, 2019.   \n[3] Haoyue Bai, Gregory Canal, Xuefeng Du, Jeongyeol Kwon, Robert D Nowak, and Yixuan Li. Feed two birds with one scone: Exploiting wild data for both out-of-distribution generalization and detection. In International Conference on Machine Learning, pages 1454\u20131471. PMLR, 2023.   \n[4] Ayush Bharti, Masha Naslidnyk, Oscar Key, Samuel Kaski, and Francois-Xavier Briol. Optimally-weighted estimators of the maximum mean discrepancy for likelihood-free inference. In Proceedings of the 40th International Conference on Machine Learning, volume 202, pages 2289\u20132312. PMLR, 2023.   \n[5] Chentao Cao, Zhun Zhong, Zhanke Zhou, Yang Liu, Tongliang Liu, and Bo Han. Envisioning outlier exposure by large language models for out-of-distribution detection. In Proceedings of the 41st International Conference on Machine Learning, volume 235, pages 5629\u20135659. PMLR, 2024.   \n[6] Tri Cao, Jiawen Zhu, and Guansong Pang. Anomaly detection under distribution shift. In IEEE/CVF International Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023, pages 6488\u20136500. IEEE, 2023.   \n[7] Chen-Hao Chao, Wei-Fang Sun, Bo-Wun Cheng, Yi-Chen Lo, Chia-Che Chang, Yu-Lun Liu, Yu-Lin Chang, Chia-Ping Chen, and Chun-Yi Lee. Denoising likelihood score matching for conditional score-based data generation. In International Conference on Learning Representations, 2021.   \n[8] Hong-You Chen and Wei-Lun Chao. On bridging generic and personalized federated learning for image classification. In International Conference on Learning Representations, 2021.   \n[9] Shengzhuang Chen, Long-Kai Huang, Jonathan Richard Schwarz, Yilun Du, and Ying Wei. Secure out-of-distribution task generalization with energy-based models. In Advances in Neural Information Processing Systems, volume 36, pages 67007\u201367020, 2023.   \n[10] Yongqiang Chen, Wei Huang, Kaiwen Zhou, Yatao Bian, Bo Han, and James Cheng. Understanding and improving feature learning for out-of-distribution generalization. In Advances in Neural Information Processing Systems, volume 36, pages 68221\u201368275, 2023.   \n[11] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3606\u20133613, 2014.   \n[12] Andrija Djurisic, Nebojsa Bozanic, Arjun Ashok, and Rosanne Liu. Extremely simple activation shaping for out-of-distribution detection. In The Eleventh International Conference on Learning Representations, 2022.   \n[13] Xuefeng Du, Zhaoning Wang, Mu Cai, and Yixuan Li. Vos: Learning what you don\u2019t know by virtual outlier synthesis. In Proceedings of the International Conference on Learning Representations, 2022.   \n[14] Haozhe Feng, Zhaoyang You, Minghao Chen, Tianye Zhang, Minfeng Zhu, Fei Wu, Chao Wu, and Wei Chen. Kd3a: Unsupervised multi-source decentralized domain adaptation via knowledge distillation. In Proceedings of the 38th international conference on machine learning, volume 139, pages 3274\u20133283, 2021.   \n[15] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Fran\u00e7ois Laviolette, Mario March, and Victor Lempitsky. Domain-adversarial training of neural networks. Journal of machine learning research, 17(59):1\u201335, 2016.   \n[16] Soumya Suvra Ghosal and Yixuan Li. Distributionally robust optimization with probabilistic group. In Thirty-Seventh AAAI Conference on Artificial Intelligence, pages 11809\u201311817, 2023.   \n[17] Yaming Guo, Kai Guo, Xiaofeng Cao, Tieru Wu, and Yi Chang. Out-of-distribution generalization of federated learning via implicit invariant relationships. In Proceedings of the 40th International Conference on Machine Learning, volume 202, pages 11905\u201311933. PMLR, 2023.   \n[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.   \n[19] Dan Hendrycks and Thomas G Dietterich. Benchmarking neural network robustness to common corruptions and surface variations. arXiv preprint arXiv:1807.01697, 2018.   \n[20] Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution examples in neural networks. In International Conference on Learning Representations, 2016.   \n[21] Dan Hendrycks, Mantas Mazeika, and Thomas Dietterich. Deep anomaly detection with outlier exposure. In International Conference on Learning Representations, 2018.   \n[22] Dan Hendrycks, Mantas Mazeika, Saurav Kadavath, and Dawn Song. Using self-supervised learning can improve model robustness and uncertainty. Advances in neural information processing systems, 32, 2019.   \n[23] Tzu-Ming Harry Hsu, Hang Qi, and Matthew Brown. Measuring the effects of non-identical data distribution for federated visual classification. arXiv preprint arXiv:1909.06335, 2019.   \n[24] Zeyi Huang, Haohan Wang, Eric P Xing, and Dong Huang. Self-challenging improves crossdomain generalization. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part II 16, pages 124\u2013140. Springer, 2020.   \n[25] Yusuke Iwasawa and Yutaka Matsuo. Test-time classifier adjustment module for model-agnostic domain generalization. Advances in Neural Information Processing Systems, 34:2427\u20132440, 2021.   \n[26] Liangze Jiang and Tao Lin. Test-time robust personalization for federated learning. In The Eleventh International Conference on Learning Representations, 2022.   \n[27] Wenyu Jiang, Hao Cheng, Mingcai Chen, Chongjun Wang, and Hongxin Wei. Dos: Diverse outlier sampling for out-of-distribution detection. arXiv preprint arXiv:2306.02031, 2023.   \n[28] Xue Jiang, Feng Liu, Zhen Fang, Hong Chen, Tongliang Liu, Feng Zheng, and Bo Han. Detecting out-of-distribution data through in-distribution class prior. In Proceedings of the 40th International Conference on Machine Learning, pages 15067\u201315088. PMLR, 2023.   \n[29] Ellango Jothimurugesan, Kevin Hsieh, Jianyu Wang, Gauri Joshi, and Phillip B Gibbons. Federated learning under distributed concept drift. In International Conference on Artificial Intelligence and Statistics, pages 5834\u20135853. PMLR, 2023.   \n[30] Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In International conference on machine learning, pages 5132\u20135143. PMLR, 2020.   \n[31] Julian Katz-Samuels, Julia B Nakhleh, Robert Nowak, and Yixuan Li. Training ood detectors in their natural habitats. In Proceedings of the 39th International Conference on Machine Learning, pages 10848\u201310865. PMLR, 2022.   \n[32] Frederic Koehler and Thuy-Duong Vuong. Sampling multimodal distributions with the vanilla score: Benefits of data-based initialization. arXiv preprint arXiv:2310.01762, 2023.   \n[33] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \n[34] David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai Zhang, Remi Le Priol, and Aaron Courville. Out-of-distribution generalization via risk extrapolation (rex). In Proceedings of the 38th International Conference on Machine Learning, pages 5815\u20135826. PMLR, 2021.   \n[35] Marc Lafon, Cl\u00e9ment Rambour, and Nicolas Thome. Heat: Hybrid energy based model in the feature space for out-of-distribution detection. In NeurIPS ML Safety Workshop, 2022.   \n[36] Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N, 7(7):3, 2015.   \n[37] Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for detecting out-of-distribution samples and adversarial attacks. Advances in neural information processing systems, 31, 2018.   \n[38] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Deeper, broader and artier domain generalization. In Proceedings of the IEEE international conference on computer vision, pages 5542\u20135550, 2017.   \n[39] Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated optimization in heterogeneous networks. Proceedings of Machine learning and systems, 2:429\u2013450, 2020.   \n[40] Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of fedavg on non-iid data. In International Conference on Learning Representations, 2019.   \n[41] Yingzhen Li and Richard E Turner. Gradient estimators for implicit models. In International Conference on Learning Representations, 2018.   \n[42] Xinting Liao, Chaochao Chen, Weiming Liu, Pengyang Zhou, Huabin Zhu, Shuheng Shen, Weiqiang Wang, Mengling Hu, Yanchao Tan, and Xiaolin Zheng. Joint local relational augmentation and global nash equilibrium for federated learning with non-iid data. In Proceedings of the 31st ACM International Conference on Multimedia, pages 1536\u20131545, 2023.   \n[43] Xinting Liao, Weiming Liu, Chaochao Chen, Pengyang Zhou, Fengyuan Yu, Huabin Zhu, Binhui Yao, Tao Wang, Xiaolin Zheng, and Yanchao Tan. Rethinking the representation in federated unsupervised learning with non-iid data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22841\u201322850, 2024.   \n[44] Quande Liu, Cheng Chen, Jing Qin, Qi Dou, and Pheng-Ann Heng. Feddg: Federated domain generalization on medical image segmentation via episodic learning in continuous frequency space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1013\u20131023, 2021.   \n[45] Qiang Liu, Jason Lee, and Michael Jordan. A kernelized stein discrepancy for goodness-offit tests. In Proceedings of The 33rd International Conference on Machine Learning, pages 276\u2013284. PMLR, 2016.   \n[46] Qiang Liu and Dilin Wang. Stein variational gradient descent: A general purpose bayesian inference algorithm. Advances in neural information processing systems, 29, 2016.   \n[47] Weiming Liu, Jiajie Su, Chaochao Chen, and Xiaolin Zheng. Leveraging distribution alignment via stein path for cross-domain cold-start recommendation. Advances in Neural Information Processing Systems, 34:19223\u201319234, 2021.   \n[48] Weitang Liu, Xiaoyun Wang, John Owens, and Yixuan Li. Energy-based out-of-distribution detection. Advances in neural information processing systems, 33:21464\u201321475, 2020.   \n[49] Weiming Liu, Xiaolin Zheng, Jiajie Su, Longfei Zheng, Chaochao Chen, and Mengling Hu. Contrastive proxy kernel stein path alignment for cross-domain cold-start recommendation. IEEE Transactions on Knowledge and Data Engineering, 35(11):11216\u201311230, 2023.   \n[50] Yue Liu, Xihong Yang, Sihang Zhou, Xinwang Liu, Siwei Wang, Ke Liang, Wenxuan Tu, and Liang Li. Simple contrastive graph clustering. IEEE Transactions on Neural Networks and Learning Systems, 2023.   \n[51] Yue Liu, Xihong Yang, Sihang Zhou, Xinwang Liu, Zhen Wang, Ke Liang, Wenxuan Tu, Liang Li, Jingcan Duan, and Cancan Chen. Hard sample aware network for contrastive deep graph clustering. In Proceedings of the AAAI conference on artificial intelligence, volume 37, pages 8914\u20138922, 2023.   \n[52] Zhengquan Luo, Yunlong Wang, Zilei Wang, Zhenan Sun, and Tieniu Tan. Disentangled federated learning for tackling attributes skew via invariant aggregation and diversity transferring. In International Conference on Machine Learning, volume 162, pages 14527\u201314541. PMLR, 2022.   \n[53] Zheqi Lv, Wenqiao Zhang, Zhengyu Chen, Shengyu Zhang, and Kun Kuang. Intelligent model update strategy for sequential recommendation. In Proceedings of the ACM on Web Conference 2024, pages 3117\u20133128, 2024.   \n[54] Zheqi Lv, Wenqiao Zhang, Shengyu Zhang, Kun Kuang, Feng Wang, Yongwei Wang, Zhengyu Chen, Tao Shen, Hongxia Yang, Beng Chin Ooi, et al. Duet: A tuning-free device-cloud collaborative parameters generation framework for efficient device model generalization. In Proceedings of the ACM Web Conference 2023, pages 3077\u20133085, 2023.   \n[55] Ahsan Mahmood, Junier Oliva, and Martin Andreas Styner. Multiscale score matching for out-of-distribution detection. In International Conference on Learning Representations, 2020.   \n[56] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In Artificial intelligence and statistics, pages 1273\u20131282. PMLR, 2017.   \n[57] Dheeraj Mekala, Adithya Samavedhi, Chengyu Dong, and Jingbo Shang. Selfood: Selfsupervised out-of-distribution detection via learning to rank. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 10721\u201310734, 2023.   \n[58] Chirag Modi, Robert M Gower, Charles Margossian, Yuling Yao, David Blei, and Lawrence K Saul. Variational inference with gaussian score matching. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[59] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. In Proceedings of the NIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2011.   \n[60] A Tuan Nguyen, Philip Torr, and Ser Nam Lim. Fedsr: A simple and effective domain generalization method for federated learning. Advances in Neural Information Processing Systems, 35:38831\u201338843, 2022.   \n[61] Kunjal Panchal, Sunav Choudhary, Subrata Mitra, Koyel Mukherjee, Somdeb Sarkhel, Saayan Mitra, and Hui Guan. Flash: Concept drift adaptation in federated learning. In Proceedings of the 40th International Conference on Machine Learning, volume 202, pages 26931\u201326962. PMLR, 23\u201329 Jul 2023.   \n[62] Lianyong Qi, Yuwen Liu, Weiming Liu, Shichao Pei, Xiaolong Xu, Xuyun Zhang, Yingjie Wang, and Wanchun Dou. Counterfactual user sequence synthesis augmented with continuous time dynamic preference modeling for sequential poi recommendation. In Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence (IJCAI), 2024.   \n[63] Zhe Qu, Xingyu Li, Rui Duan, Yao Liu, Bo Tang, and Zhuo Lu. Generalized federated learning via sharpness aware minimization. In Proceedings of the 39th International Conference on Machine Learning, pages 18250\u201318280. PMLR, 2022.   \n[64] Yasar Abbas Ur Rehman, Yan Gao, Pedro Porto Buarque De Gusm\u00e3o, Mina Alibeigi, Jiajun Shen, and Nicholas D Lane. L-dawa: Layer-wise divergence aware weight aggregation in federated self-supervised visual representation learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 16464\u201316473, 2023.   \n[65] Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural networks. In International Conference on Learning Representations, 2019.   \n[66] Vikash Sehwag, Mung Chiang, and Prateek Mittal. Ssd: A unified framework for self-supervised outlier detection. In International Conference on Learning Representations, 2020.   \n[67] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019.   \n[68] Yiyou Sun, Yifei Ming, Xiaojin Zhu, and Yixuan Li. Out-of-distribution detection with deep nearest neighbors. In Proceedings of the 39th International Conference on Machine Learning, pages 20827\u201320840. PMLR, 2022.   \n[69] Yue Tan, Chen Chen, Weiming Zhuang, Xin Dong, Lingjuan Lyu, and Guodong Long. Is heterogeneity notorious? taming heterogeneity to handle test-time shift in federated learning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[70] Xueyang Tang, Song Guo, and Jie Zhang. Exploiting personalized invariance for better out-ofdistribution generalization in federated learning. arXiv preprint arXiv:2211.11243, 2022.   \n[71] Arash Vahdat, Karsten Kreis, and Jan Kautz. Score-based generative modeling in latent space. Advances in neural information processing systems, 34:11287\u201311302, 2021.   \n[72] Pascal Vincent. A connection between score matching and denoising autoencoders. Neural computation, 23(7):1661\u20131674, 2011.   \n[73] Fan Wang, Chaochao Chen, Weiming Liu, Tianhao Fan, Xinting Liao, Yanchao Tan, Lianyong Qi, and Xiaolin Zheng. Ce-rcfr: Robust counterfactual regression for consensus-enabled treatment effect estimation. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 3013\u20133023, 2024.   \n[74] Haoqi Wang, Zhizhong Li, Litong Feng, and Wayne Zhang. Vim: Out-of-distribution with virtual-logit matching. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4921\u20134930, 2022.   \n[75] Qizhou Wang, Zhen Fang, Yonggang Zhang, Feng Liu, Yixuan Li, and Bo Han. Learning to augment distributions for out-of-distribution detection. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[76] Hongxin Wei, Renchunzi Xie, Hao Cheng, Lei Feng, Bo An, and Yixuan Li. Mitigating neural network overconfidence with logit normalization. In Proceedings of the 39th International Conference on Machine Learning, pages 23631\u201323644. PMLR, 2022.   \n[77] Yue Wu, Shuaicheng Zhang, Wenchao Yu, Yanchi Liu, Quanquan Gu, Dawei Zhou, Haifeng Chen, and Wei Cheng. Personalized federated learning under mixture of distributions. In Proceedings of the 40th International Conference on Machine Learning, pages 37860\u201337879. PMLR, 2023.   \n[78] Pingmei Xu, Krista A Ehinger, Yinda Zhang, Adam Finkelstein, Sanjeev R Kulkarni, and Jianxiong Xiao. Turkergaze: Crowdsourcing saliency with webcam based eye tracking. arXiv preprint arXiv:1504.06755, 2015.   \n[79] Qinwei Xu, Ruipeng Zhang, Ya Zhang, Yanfeng Wang, and Qi Tian. A fourier-based framework for domain generalization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 14383\u201314392, 2021.   \n[80] Jingkang Yang, Haoqi Wang, Litong Feng, Xiaopeng Yan, Huabin Zheng, Wayne Zhang, and Ziwei Liu. Semantically coherent out-of-distribution detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8301\u20138309, 2021.   \n[81] Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365, 2015.   \n[82] Runpeng Yu, Songhua Liu, Xingyi Yang, and Xinchao Wang. Distribution shift inversion for out-of-distribution prediction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3592\u20133602, 2023.   \n[83] Shuyang Yu, Junyuan Hong, Haotao Wang, Zhangyang Wang, and Jiayu Zhou. Turning the curse of heterogeneity in federated learning into a blessing for out-of-distribution detection. In 2023 International Conference on Learning Representations, 2023.   \n[84] Honglin Yuan, Warren Richard Morningstar, Lin Ning, and Karan Singhal. What do we mean by generalization in federated learning? In International Conference on Learning Representations, 2021.   \n[85] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016.   \n[86] Jingyang Zhang, Nathan Inkawhich, Randolph Linderman, Yiran Chen, and Hai Li. Mixture outlier exposure: Towards out-of-distribution detection in fine-grained environments. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 5531\u20135540, 2023.   \n[87] Jianyu Zhang, David Lopez-Paz, and L\u00e9on Bottou. Rich feature construction for the optimization-generalization dilemma. In International Conference on Machine Learning, pages 26397\u201326411. PMLR, 2022.   \n[88] Haotian Zheng, Qizhou Wang, Zhen Fang, Xiaobo Xia, Feng Liu, Tongliang Liu, and Bo Han. Out-of-distribution detection learning with unreliable out-of-distribution sources. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[89] Zhengyi Zhong, Weidong Bao, Ji Wang, Xiaomin Zhu, and Xiongtao Zhang. Flee: A hierarchical federated learning framework for distributed deep neural network over cloud, edge, and end device. 13(5), Oct. 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "In the supplemental materials, we provide all algorithms in Appendix A, the theoretical analysis in Appendix B, additional related work on federated learning with non-IID data in Appendix C, experimental implementation details in Appendix D, the additional experimental details in Appendix E. ", "page_idx": 15}, {"type": "text", "text": "A Algorithms ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The overall algorithm of FOOGD is in Algo. 1. In line 1:10, the server collaborates with clients to optimize the feature extractor model for representation and the score model for density estimation. The clients execute training local models separately in line 11:19. In each client, $\\mathtt{S M^{3}D}$ estimates data density based on the latent representation of feature extractor, and SAG computes the kernelized stein discrepancy based on score model to regularize the optimization of feature extractor. We update score model with $\\mathsf{S M}^{3}\\mathsf{D}$ in Algo. 2, and the training procedure of feature extractor is detailed in Algo. 3. ", "page_idx": 15}, {"type": "text", "text": "Algorithm 1 Training procedure of FOOGD ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Input: Batch size $B$ , communication rounds $T$ , number of clients $K$ , local steps $E$ , dataset ${\\mathcal{D}}=$   \n$\\cup_{k\\in[K]}{\\mathcal{D}}_{k}$   \nOutput: feature extractor and score model parameters, i.e., $\\theta_{f}^{T}$ and $\\pmb{\\theta}_{s}^{T}$   \n1: Server executes():   \n2: Initialize $\\{\\pmb{\\theta}_{f}^{0},\\pmb{\\theta}_{s}^{0}\\}$ with random distribution   \n3: for $t=0,1,...,T-1$ do   \n4: for $k=1,2,...,K$ in parallel do   \n5: Send $\\{\\pmb{\\theta}_{f}^{t},\\pmb{\\theta}_{s}^{t}\\}$ to client $k$   \n6: $\\{\\pmb{\\theta}_{f}^{t,k},\\pmb{\\theta}_{s}^{t,k}\\}\\gets\\mathbf{Client\\;executes}(k,\\{\\pmb{\\theta}_{f}^{t},\\pmb{\\theta}_{s}^{t}\\})$   \n7: end for   \n8: Update parameters of $\\{\\pmb{\\theta}_{f}^{t},\\pmb{\\theta}_{s}^{t}\\}$ by Eq. (3)   \n9: end for   \n10: return $\\{\\pmb{\\theta}_{f}^{T},\\pmb{\\theta}_{s}^{T}\\}$   \n11: Client executes $\\mathit{\\Delta}^{k,\\;}\\{\\theta_{f}^{t},\\theta_{s}^{t}\\})$ :   \n12: Assign global model to local model $\\{\\pmb{\\theta}_{f}^{k},\\pmb{\\theta}_{s}^{k}\\}\\leftarrow\\{\\pmb{\\theta}_{f}^{t},\\pmb{\\theta}_{s}^{t}\\}$   \n13: for each local epoch $e=1,2,...,E$ do   \n14: for batch of samples $(X_{1:B},Y_{1:B})\\in{\\mathcal{D}}_{k}$ do   \n15: Execute $\\pmb{\\theta}_{s}^{k}=\\mathsf{S M}^{3}\\mathsf{D}(\\pmb{X}_{1:B},\\pmb{Y}_{1:B})$ in Algorithm 2   \n16: Execute $\\pmb{\\theta}_{f}^{k}=\\mathsf{S A G}(\\pmb{X}_{1:B},\\pmb{Y}_{1:B})$ in Algorithm 3   \n17: end for   \n18: end for   \n19: return $\\theta_{k}^{E}$ to server ", "page_idx": 15}, {"type": "text", "text": "Algorithm 2 Algorithm of $\\mathsf{S M}^{3}\\mathsf{D}$ ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Input: Batch size $B$ , batch of samples $(X_{1:B},Y_{1:B})\\in{\\mathcal{D}}_{k}$ , fixed feature extractor $\\theta_{f}$ , and initialized   \nscore model $\\theta_{s}$   \nOutput: score model parameters, i.e., $\\theta_{s}$   \n1: Feature Extraction $Z_{1:B}\\leftarrow f_{\\theta}(X_{1:B})$   \n2: Sample $B$ random data points $Z^{0}$ with $z_{i}^{0}\\sim\\mathcal{N}(\\mathbf{0},I)$   \n3: Take Langevin dynamic sampling started at $Z^{0}$ to obtain generated samples $Z_{\\mathrm{gen}}$ by Eq. (6)   \n4: Perturb noise on data features to obtain $\\tilde{\\boldsymbol{Z}}\\sim\\mathcal{N}(\\boldsymbol{Z},\\sigma\\boldsymbol{I})$   \n5: Compute denoising score matching by Eq. (4)   \n6: Regularize maximum mean discrepancy between generated $Z_{\\mathrm{gen}}$ and $_{z}$ by Eq. (7)   \n7: Optimize score model $\\theta_{s}$ with objective $\\ell_{k}^{\\mathrm{OUT}}$ by Eq. (8)   \n8: return $\\theta_{s}$ ", "page_idx": 15}, {"type": "text", "text": "Input: Batch size $B$ , batch of samples $(X_{1:B},Y_{1:B})\\in\\mathcal{D}_{k}$ , fixed score model $\\theta_{s}$ , and initialized feature extractor $\\theta_{f}$ ", "page_idx": 16}, {"type": "text", "text": "Output: feature extractor parameters, i.e., $\\theta_{f}$ ", "page_idx": 16}, {"type": "text", "text": "1: Augment samples $\\widehat{\\pmb{X}}_{1:B}=\\mathcal{T}(\\pmb{X}_{1:B})$   \n2: Feature extraction $Z_{1:B}\\leftarrow f_{\\theta}(X_{1:B})$ , and $\\widehat{Z}_{1:B}\\leftarrow f_{\\theta}(\\widehat{X}_{1:B})$   \n3: Compute kernelized Stein divergence by Eq . (11)   \n4: Compute cross entropy loss between prediction ${Y_{p r e d}=\\mathrm{Classifier}(Z)}$ and ground truth $Y_{g r}$   \n5: Optimize feature extractor $\\theta_{f}$ with objective $\\ell_{k}^{\\mathrm{IN}}+\\ell_{k}^{\\mathrm{IN-C}}$ by Eq. (12)   \n6: return $\\theta_{f}$ ", "page_idx": 16}, {"type": "text", "text": "B Theoretical Analysis ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "B.1 Error Bound of $\\mathsf{S M}^{3}\\mathsf{D}$ ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Lemma B.1 (Error Bound of Decentralized Score Matching). The error bound of global score model aggregated from local scores models is ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\|\\nabla_{z}\\log p_{\\theta}(z)-\\nabla_{z}\\log p_{\\mathcal{D}}(z)\\|^{2}=\\frac{\\boldsymbol{v}^{\\top}\\boldsymbol{v}}{\\sigma^{2}}-\\mathbb{E}_{p_{\\mathcal{D}}(z)}[\\|\\nabla_{z}\\log p_{\\mathcal{D}}(z)\\|^{2}].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. For the global score model aggregated from local score models that estimate IN data probability densities, it holds: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\nabla_{z}\\log p_{\\theta}(z)=s_{\\theta}(z)=\\sum_{k=1}^{K}w_{k}s_{\\theta_{k}}(z)}}\\\\ {~~}\\\\ {{\\displaystyle=\\sum_{k=1}^{K}w_{k}\\nabla_{z}\\log p_{\\theta_{k}}(z)}}\\\\ {~~}\\\\ {{\\displaystyle=\\nabla_{z}\\sum_{k=1}^{K}w_{k}\\log p_{\\theta_{k}}(z).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then we formulate the score matching for global distribution as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|\\nabla_{z}\\log p_{\\theta}(z)-\\nabla_{z}\\log p_{D}(z)\\|^{2}}\\\\ {\\displaystyle=\\|\\sum_{k=1}^{K}w_{k}\\nabla_{z}\\log p_{\\theta_{k}}(z)-\\nabla_{z}\\log p_{D}(z)\\|^{2}}\\\\ {\\displaystyle=\\|\\sum_{k=1}^{K}w_{k}\\nabla_{z}\\log p_{\\theta_{k}}(z)-\\sum_{k=1}^{K}w_{k}\\nabla_{z}\\log p_{D}(z)\\|^{2}}\\\\ {\\displaystyle=\\|\\sum_{k=1}^{K}w_{k}[\\nabla_{z}\\log p_{\\theta_{k}}(z)-\\nabla_{z}\\log p_{D}(z)]\\|^{2}}\\\\ {\\displaystyle=\\|\\sum_{k=1}^{K}w_{k}[\\nabla_{z}\\log p_{\\theta_{k}}(z)-\\nabla_{z}\\log p_{D}(z)]\\|^{2}}\\\\ {\\displaystyle\\leq\\sum_{k=1}^{K}w_{k}\\|\\nabla_{z}\\log p_{\\theta_{k}}(z)-\\nabla_{z}\\log p_{D}(z)\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\textstyle\\sum_{k=1}^{K}w_{k}=1$ , and the last term is held by Jensen inequation. ", "page_idx": 16}, {"type": "text", "text": "In term of Vincent [72], the DSM for each local model $s_{\\theta_{k}}(z)$ is bounded as follows, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{J_{\\mathrm{DSM}}(\\theta_{k})\\overset{\\mathrm{def}}{=}\\mathbb{E}_{p(z,\\bar{z})}\\left[\\|s_{\\theta_{k}}(z)-\\nabla_{\\bar{z}}\\log p\\left(\\bar{z}\\mid z\\right)\\|^{2}\\right]}&{}\\\\ {=\\mathbb{E}_{p(z,\\bar{z})}\\left[\\|s_{\\theta_{k}}(z)\\|^{2}-2s_{\\theta_{k}}(z)^{\\top}\\nabla_{\\bar{z}}\\log p\\left(\\bar{z}\\mid z\\right)+\\|\\nabla_{\\bar{z}}\\log p\\left(\\bar{z}\\mid z\\right)\\|^{2}\\right]}&{}\\\\ {=\\mathbb{E}_{p(z)}\\left[\\|s_{\\theta_{k}}(z)\\|^{2}\\right]-2\\mathbb{E}_{p(z,\\bar{z})}\\left[s_{\\theta_{k}}(z)^{\\top}\\nabla_{\\bar{z}}\\log p\\left(\\bar{z}\\mid z\\right)\\right]+\\frac{v^{\\top}v}{\\sigma^{2}}}&{}\\\\ {=J_{\\mathrm{ESM}}(\\theta_{k})+2\\mathbb{E}_{p(z,\\bar{z})}\\left[s_{\\theta_{k}}(z)^{\\top}\\frac{v}{\\sigma}\\right]+\\frac{v^{\\top}v}{\\sigma^{2}}-\\mathbb{E}_{p(z)}[\\|\\nabla_{\\bar{z}}\\log p(z)\\|^{2}]}&{}\\\\ {=J_{\\mathrm{ESM}}(\\theta_{k})+\\frac{v^{\\top}v}{\\sigma^{2}}-\\mathbb{E}_{p(z)}[\\|\\nabla_{\\bar{z}}\\log p(z)\\|^{2}],}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\pmb{v}\\sim\\mathcal{N}(\\mathbf{0},\\pmb{I})$ and ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\nabla_{\\tilde{z}}\\log p\\left(\\tilde{z}\\mid z\\right)=\\nabla_{\\tilde{z}}\\left[\\log\\frac{1}{\\left(\\sqrt{2\\pi\\sigma^{2}}\\right)^{d}}\\exp\\left\\{-\\frac{\\|\\tilde{z}-z\\|^{2}}{2\\sigma^{2}}\\right\\}\\right]\\ \\ =-\\frac{\\tilde{z}-z}{\\sigma^{2}}=-\\frac{v}{\\sigma}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, when $\\pmb{\\theta}=\\pmb{\\theta}^{*}=\\pmb{\\theta}_{k}^{*}$ , we have $J_{\\mathrm{ESM}}(\\pmb\\theta)=J_{\\mathrm{ESM}}(\\pmb\\theta_{k})=0\\ \\ \\ \\forall k\\in[K]$ , the global score matching finally satisfies: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|s_{\\theta}(z)-\\nabla_{z}\\log p_{\\mathcal{D}}(z)\\|^{2}}\\\\ &{=\\|\\nabla_{z}\\log p_{\\theta}(z)-\\nabla_{z}\\log p_{\\mathcal{D}}(z)\\|^{2}}\\\\ &{\\le\\displaystyle\\sum_{k=1}^{K}w_{k}\\|\\nabla_{z}\\log p_{\\theta_{k}}(z)-\\nabla_{z}\\log p_{\\mathcal{D}}(z)\\|^{2}}\\\\ &{=\\frac{v^{\\top}v}{\\sigma^{2}}-\\mathbb{E}_{p_{\\mathcal{D}}(z)}[\\|\\nabla_{z}\\log p_{\\mathcal{D}}(z)\\|^{2}],}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which holds due to $\\textstyle\\sum_{k=1}^{K}w_{k}=1$ . ", "page_idx": 17}, {"type": "text", "text": "Theorem B.2 (Error Bound of Decentralized Score Matching via $\\mathrm{{SM^{3}D}}$ ). Assume the original ${\\mathrm{MMD}}(Z,Z_{g e n})\\leq C$ for randomly initialized score model $s_{\\theta}(z)$ in Eq. (7), the score model achieves optimum and MMD decreases. By Lemma B.1, we can obtain the final error bound of global $s_{\\theta}(\\cdot)$ as: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|s_{\\theta}(z)-\\nabla_{z}\\log p_{\\mathcal{D}}(z)\\|^{2}\\leq\\frac{v^{\\top}v}{\\sigma^{2}}-\\mathbb{E}_{p_{\\mathcal{D}}(z)}[\\|\\nabla_{z}\\log p_{\\mathcal{D}}(z)\\|^{2}]+\\frac{|\\mathcal{D}|}{B}C,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $C$ is the upper bound of the MMD, $B$ is batch size, and $|\\mathcal D|$ is the data amount. ", "page_idx": 17}, {"type": "text", "text": "B.2 The overall induction of Kernelized Stein Discrepancy in SAG ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Assume feature distributions $p(z)$ and $q(\\widehat{z})$ are two bounded distributions satisfying $\\begin{array}{r}{\\operatorname*{lim}_{||z||\\to\\infty}p(z)\\phi(z)=0}\\end{array}$ and $\\begin{array}{r}{\\operatorname*{lim}_{||\\hat{\\boldsymbol{z}}||\\rightarrow\\infty}q(\\hat{\\boldsymbol{z}})\\phi(\\hat{\\boldsymbol{z}})=0}\\end{array}$ . And we denote the gradient of log density in z as \u2207z log q(z) = \u2207qz (qz()z  ). ", "page_idx": 17}, {"type": "text", "text": "Lemma B.3 (Stein identity). If the $\\phi(\\cdot)$ in Stein operator $\\mathcal{A}_{q}\\phi(\\widehat{z})=\\phi(\\widehat{z})\\nabla_{\\widehat{z}}\\log q(\\widehat{z})+\\nabla_{\\widehat{z}}\\phi(\\widehat{z})$ introduced in Eq. (10) is Stein class, then we have a fundamental  propert y ca lled Stei n ident ity a s below: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\widehat{z}\\sim q}\\left[\\phi(\\widehat{z})\\nabla_{\\widehat{z}}\\log q(\\widehat{z})+\\nabla_{\\widehat{z}}\\phi(\\widehat{z})\\right]=0.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\hat{z}\\sim q}\\left[\\phi(\\hat{z})\\nabla_{\\bar{z}}\\log q(\\hat{z})+\\nabla_{\\bar{z}}\\phi(\\hat{z})\\right]=\\int_{-\\infty}^{+\\infty}q(\\hat{z})\\phi(\\hat{z})\\nabla_{\\bar{z}}\\log q(\\hat{z})+q(\\hat{z})\\nabla_{\\bar{z}}\\phi(\\hat{z})\\,d\\hat{z}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\int_{-\\infty}^{+\\infty}q(\\hat{z})\\phi(\\hat{z})\\frac{\\nabla_{\\bar{z}}q(\\hat{z})}{q(\\hat{z})}+q(\\hat{z})\\nabla_{\\bar{z}}\\phi(\\hat{z})\\,d\\hat{z}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\int_{-\\infty}^{+\\infty}\\phi(\\hat{z})\\nabla_{\\bar{z}}q(\\hat{z})+q(\\hat{z})\\nabla_{\\bar{z}}\\phi(\\hat{z})\\,d\\hat{z}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\int_{-\\infty}^{+\\infty}(\\phi(\\hat{z})q(\\hat{z}))^{\\prime}d\\hat{z}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\int_{-\\infty}^{+\\infty}(\\phi(\\hat{z})q(\\hat{z}))^{\\prime}d\\hat{z}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\phi(\\hat{z})q(\\hat{z})|_{-\\infty}^{+\\infty}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Definition B.4 (Stein Discrepancy). Stein identity induces Stein discrepancy for two distributions $p(z)$ and $q(\\widehat{z})$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname{SD}(p(z),q({\\widehat{z}}))=\\operatorname*{sup}_{\\phi\\in{\\mathcal{F}}}\\mathbb{E}_{{\\widehat{z}}\\sim q}\\left[A_{p}\\phi({\\widehat{z}})\\right]^{\\top}\\mathbb{E}_{{\\widehat{z}}^{\\prime}\\sim q}\\left[A_{p}\\phi({\\widehat{z}}^{\\prime})\\right],\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\phi(\\cdot)$ is the stein class function satisfying boundary conditions, and $\\mathcal{F}$ is the function space. ", "page_idx": 18}, {"type": "text", "text": "Lemma B.5. If $\\mathcal{F}$ is a unit ball in reproducing kernel Hilbert space (RKHS) with positive definite kernel function $k(\\cdot,\\cdot)\\in\\mathcal{F}$ , we obtain the Kernelized Stein Discrepancy for $p(z)$ and $q(\\widehat{z})$ as below: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{KSD}(p(z),q(\\widehat{z}))=\\mathbb{E}_{\\widehat{z},\\widehat{z}^{\\prime}\\sim q}[s_{\\theta}(\\widehat{z})^{\\top}s_{\\theta}(\\widehat{z}^{\\prime})k(\\widehat{z},\\widehat{z}^{\\prime})+s_{\\theta}(\\widehat{z})^{\\top}\\nabla_{\\widehat{z}^{\\prime}}k(\\widehat{z},\\widehat{z}^{\\prime})+s_{\\theta}(\\widehat{z}^{\\prime})^{\\top}\\nabla_{\\widehat{z}}k(\\widehat{z},\\widehat{z}^{\\prime})}\\\\ &{\\qquad\\qquad\\qquad+\\mathrm{\\trace{(\\nabla_{\\widehat{z}}\\nabla_{\\widehat{z}^{\\prime}}k(\\widehat{z},\\widehat{z}^{\\prime}))}{]}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. Firstly, considering the expectation of $q(\\widehat{z})$ on the Stein operator with score of $p(z)$ , we can expand it via introducing Stein identity: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\widehat{z}\\sim q}\\left[A_{p}\\phi(\\widehat{z})\\right]=\\mathbb{E}_{\\widehat{z}\\sim q}\\left[A_{p}\\phi(\\widehat{z})\\right]-\\mathbb{E}_{\\widehat{z}\\sim q}\\left[A_{q}\\phi(\\widehat{z})\\right]}\\\\ &{\\qquad\\qquad\\qquad=\\mathbb{E}_{\\widehat{z}\\sim q}\\left[A_{p}\\phi(\\widehat{z})-A_{q}\\phi(\\widehat{z})\\right]}\\\\ &{\\qquad\\qquad\\qquad=\\mathbb{E}_{\\widehat{z}\\sim q}\\left[\\phi(\\widehat{z})\\left(\\nabla_{\\widehat{z}}\\log p(\\widehat{z})-\\nabla_{\\widehat{z}}\\log q(\\widehat{z})\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then, with the property of RKHS, we have $k(\\widehat{z},\\widehat{z}^{\\prime}):=\\langle\\phi(\\widehat{z}),\\phi(\\widehat{z}^{\\prime})\\rangle_{\\mathcal{H}}.$ $s_{p}(\\widehat{z})$ and $s_{q}(\\widehat{z})$ are short for $\\nabla_{\\widehat{z}}\\log{p(\\widehat{z})}$ and $\\nabla_{\\widehat{z}}\\log{q(\\widehat{z})}$ , respectively, t h e  Stein discr epanc y can be re written as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{S}(p(z),q(\\hat{z}))}\\\\ &{=\\mathbb{E}_{\\hat{z}\\sim q}\\left[A_{p}\\phi(\\hat{z})\\right]^{\\top}\\mathbb{E}_{\\hat{z}^{\\prime}\\sim q}\\left[A_{p}\\phi(\\hat{z}^{\\prime})\\right]}\\\\ &{=\\mathbb{E}_{\\hat{z},\\hat{z}^{\\prime}\\sim q}\\left[\\left(\\nabla_{\\hat{z}}\\log p(\\hat{z})-\\nabla_{\\hat{z}}\\log q(\\hat{z})\\right)^{\\top}k(\\hat{z},\\hat{z}^{\\prime})\\left(\\nabla_{\\hat{z}^{\\prime}}\\log p(\\hat{z}^{\\prime})-\\nabla_{\\hat{z}^{\\prime}}\\log q(\\hat{z}^{\\prime})\\right)\\right]}\\\\ &{=\\mathbb{E}_{\\hat{z},\\hat{z}^{\\prime}\\sim q}\\left[\\left(s_{p}(\\hat{z})-s_{q}(\\hat{z})\\right)^{\\top}k(\\hat{z},\\hat{z}^{\\prime})\\left(s_{p}(\\hat{z}^{\\prime})-s_{q}(\\hat{z}^{\\prime})\\right)\\right]}\\\\ &{=\\mathbb{E}_{\\hat{z},\\hat{z}^{\\prime}\\sim q}\\left[\\left(s_{p}(\\hat{z})-s_{q}(\\hat{z})\\right)^{\\top}\\left(k(\\hat{z},\\hat{z}^{\\prime})s_{p}(\\hat{z}^{\\prime})+\\nabla_{\\hat{z}^{\\prime}}k(\\hat{z},\\hat{z}^{\\prime})-k(\\hat{z},\\hat{z}^{\\prime})s_{q}(\\hat{z}^{\\prime})-\\nabla_{\\hat{z}^{\\prime}}k(\\hat{z},\\hat{z}^{\\prime})\\right)\\right]}\\\\ &{=\\mathbb{E}_{\\hat{z},\\hat{z}^{\\prime}\\sim q}\\left[\\left(s_{p}(\\hat{z})-s_{q}(\\hat{z})\\right)^{\\top}\\left(k(\\hat{z},\\hat{z}^{\\prime})s_{p}(\\hat{z}^{\\prime})+\\nabla_{\\hat{z}^{\\prime}}k(\\hat{z},\\hat{z}^{\\prime})\\right)\\right],\\ }\\\\ &{=\\mathbb{E}_{\\hat{z},\\hat{z}^{\\prime}\\sim q}\\left[\\left(s_{p}(\\hat{z})-s_{q}(\\hat{z})\\right)^{\\top}\\left(k(\\hat{z},\\hat{z}^{\\prime})s_{p}(\\hat{z}^{\\prime})+\\nabla_{\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the last second equation is also held by Stein identity. ", "page_idx": 18}, {"type": "text", "text": "Next, we define the ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\boldsymbol{v}(\\boldsymbol{\\widehat{z}},\\boldsymbol{\\widehat{z}}^{\\prime})=k(\\boldsymbol{\\widehat{z}},\\boldsymbol{\\widehat{z}}^{\\prime})\\boldsymbol{s}_{p}(\\boldsymbol{\\widehat{z}}^{\\prime})+\\nabla_{\\boldsymbol{\\widehat{z}}^{\\prime}}k(\\boldsymbol{\\widehat{z}},\\boldsymbol{\\widehat{z}}^{\\prime}),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "introducing another Stein identity holds, i.e., ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\widehat{z},\\widehat{z}^{\\prime}\\sim q}\\left[s_{q}(\\widehat{z})^{\\top}v(\\widehat{z},\\widehat{z}^{\\prime})+\\nabla_{\\widehat{z}}v(\\widehat{z},\\widehat{z}^{\\prime})\\right]=\\mathbb{E}_{\\widehat{z},\\widehat{z}^{\\prime}\\sim q}\\left[A_{q}v(\\widehat{z},\\widehat{z}^{\\prime})\\right]=0.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Finally, taking $v(\\widehat{z},\\widehat{z}^{\\prime})$ back to Eq. (26) and substitute $s_{\\theta}(\\widehat{z})=\\nabla_{\\widehat{z}}\\log p_{\\theta}(\\widehat{z})$ , we can obtain the final KSD without  th  e requirement of computing score value s of $q(\\widehat{z})$ . ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{\\ESD}(p(z),q(\\hat{z}))}\\\\ &{=\\mathbb{E}_{\\hat{z}\\sim q}\\left[A_{p}\\phi(\\hat{z})\\right]^{\\top}\\mathbb{E}_{\\hat{z}^{\\prime}\\sim q}\\left[A_{p}\\phi(\\hat{z}^{\\prime})\\right]}\\\\ &{=\\mathbb{E}_{\\hat{z},\\hat{z}^{\\prime}\\sim q}\\left[\\left(\\nabla_{\\hat{z}}\\log p(\\hat{z})-\\nabla_{\\hat{z}}\\log q(\\hat{z})\\right)^{\\top}k(\\hat{z},\\hat{z}^{\\prime})\\left(\\nabla_{\\hat{z}^{\\prime}}\\log p(\\hat{z}^{\\prime})-\\nabla_{\\hat{z}^{\\prime}}\\log q(\\hat{z}^{\\prime})\\right)\\right]}\\\\ &{=\\mathbb{E}_{\\hat{z},\\hat{z}^{\\prime}\\sim q}\\left[s_{\\theta}(\\hat{z})^{\\top}s_{\\theta}(\\hat{z}^{\\prime})k(\\hat{z},\\hat{z}^{\\prime})+s_{\\theta}(\\hat{z})^{\\top}\\nabla_{\\hat{z}^{\\prime}}k(\\hat{z},\\hat{z}^{\\prime})+s_{\\theta}(\\hat{z}^{\\prime})^{\\top}\\nabla_{\\hat{z}}k(\\hat{z},\\hat{z}^{\\prime})+\\mathrm{trace}(\\nabla_{\\hat{z}}\\nabla_{\\hat{z}^{\\prime}}k(\\hat{z},\\hat{z}^{\\prime}))\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "B.3 Bound of Client Model Divergence ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this part, we first introduce mild and general assumptions [40], and induct the model updating divergence bound for each client. Because FOOGD aggregates client models similar to its original model, i.e., FedAvg and FedRod, its generalization bound is unchanged compared with the generalization bound proposed in [40]. Please kindly refer to the original paper. ", "page_idx": 19}, {"type": "text", "text": "Assumption B.6. Let $F_{k}(\\pmb\\theta)$ be the expected model objective for client $k$ , and assume $F_{1},\\cdot\\cdot\\cdot,F_{K}$ are all L-smooth, i.e., for all $\\theta_{k}$ , $\\begin{array}{r}{F_{k}(\\pmb{\\theta}_{k})\\leq F_{k}(\\pmb{\\theta}_{k})+(\\pmb{\\theta}_{k}-\\pmb{\\theta}_{k})^{\\top}\\nabla F_{k}(\\pmb{\\theta}_{k})+\\frac{L}{2}\\|\\pmb{\\theta}_{k}-\\pmb{\\theta}_{k}\\|^{2}}\\end{array}$ . ", "page_idx": 19}, {"type": "text", "text": "Assumption B.7. Let $F_{1},\\cdot\\cdot\\cdot,F_{N}$ are all $\\mu$ -strongly convex: for all $\\theta_{k}$ , $F_{k}(\\pmb\\theta_{k})\\geq F_{k}(\\pmb\\theta_{k})+(\\pmb\\theta_{k}-$ $\\begin{array}{r}{\\pmb{\\theta}_{k})^{\\top}\\nabla F_{k}(\\pmb{\\theta}_{k})+\\frac{\\mu}{2}\\|\\pmb{\\theta}_{k}-\\pmb{\\theta}_{k}\\|^{2}}\\end{array}$ . ", "page_idx": 19}, {"type": "text", "text": "Assumption B.8. Let $\\xi_{k}^{t}$ be sampled from the $k$ -th client\u2019s local data uniformly at random. The variance of stochastic gradients in each client is bounded: $\\mathbb{E}\\left\\|\\nabla F_{k}\\left(\\pmb{\\theta}_{k}^{t},\\xi_{k}^{t}\\right)-\\nabla F_{k}\\left(\\pmb{\\theta}_{k}^{t}\\right)\\right\\|^{2}\\leq\\sigma_{k}^{2}$ . ", "page_idx": 19}, {"type": "text", "text": "Assumption B.9. The expected squared norm of stochastic gradients is uniformly bounded, i.e., $\\mathbb{E}\\left\\|\\nabla\\bar{F_{k}}\\left(\\pmb{\\theta}_{k}^{t},\\xi_{k}^{t}\\right)\\right\\|^{2}\\leq V^{2}$ for all $k=1,\\cdots\\,,K$ and $t=1,\\cdot\\cdot\\cdot,T-1$ . ", "page_idx": 19}, {"type": "text", "text": "Next, we introduce the lemma related to the bound of client model divergence. ", "page_idx": 19}, {"type": "text", "text": "Lemma B.10 (Bound of Client Model Divergence). With assumption $B.9_{;}$ , $\\eta_{t}$ is non-increasing and $\\eta_{t}<2\\eta_{t+E}$ (learning rate of $t$ -th round and $E$ -th epoch) for all $t\\geq0$ , there exists $t_{0}\\leq t_{!}$ , such that $t-t_{0}\\le E-1$ and $\\pmb{\\theta}_{k}^{t_{0}}=\\pmb{\\theta}^{t_{0}}$ for all $k\\in[K]$ . It follows that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\sum_{k=1}^{K}w_{k}\\|\\pmb{\\theta}^{t}-\\pmb{\\theta}_{k}^{t}\\|^{2}\\right]\\leq4\\eta_{t}^{2}(E-1)^{2}V^{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. Let $E$ be the maximal local epoch. For any round $t>0$ , communication rounds from $t_{0}$ to $t$ exist $t-t_{0}<E-1$ . and the global model $\\pmb{\\theta}^{t_{0}}$ and each local model $\\pmb{\\theta}_{k}^{t_{0}}$ are same at round $t_{0}$ . ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\underset{k=1}{\\overset{K}{\\sum}}w_{k}|\\Big\\|^{\\ell}-\\theta_{k}^{\\ell}\\Big\\|^{2}\\right]}\\\\ &{=\\mathbb{E}\\left[\\sum_{k=1}^{K}w_{k}\\big\\|(\\theta_{k}^{\\ell}-\\theta^{(\\alpha)})-(\\theta^{\\ell}-\\theta^{(\\alpha)})\\|^{2}\\right]}\\\\ &{\\leq\\mathbb{E}\\left[\\underset{k=1}{\\overset{K}{\\sum}}w_{k}\\big\\|(\\theta_{k}^{\\ell}-\\theta^{(\\alpha)}\\|^{2}\\right]}\\\\ &{=\\mathbb{E}\\left[\\underset{k=1}{\\overset{K}{\\sum}}w_{k}\\left\\|\\underset{r=0}{\\overset{t-1}{\\sum}}\\eta_{k}\\nabla k_{k}(\\theta_{k}^{\\ell},\\xi\\xi)\\right\\|^{2}\\right]}\\\\ &{\\leq\\mathbb{E}\\left[\\underset{k=1}{\\overset{K}{\\sum}}w_{k}(t-t)\\underset{r=0}{\\overset{t-1}{\\sum}}\\eta_{k}^{2}\\left\\|\\nabla k_{k}(\\theta_{k}^{\\ell},\\xi\\xi)\\right\\|^{2}\\right]}\\\\ &{\\leq4\\eta_{k}^{2}(E-1)^{2}\\gamma^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the Eq. (31b) holds since $\\mathbb{E}(\\pmb{\\theta}_{k}^{t}-\\pmb{\\theta}^{t_{0}})=\\pmb{\\theta}^{t}-\\pmb{\\theta}^{t_{0}}$ , and $\\mathbb{E}\\|X-\\mathbb{E}(X)\\|\\leq\\mathbb{E}\\|X\\|$ , and Eq. (31d) derives from Jensen inequality. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "C Related Work ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "C.1 Federated Learning with Non-IID Data ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Federated Learning (FL) with non-IID data presents significant challenges in balancing global and local model performance. One prominent method, FedAvg [56], uses simple averaging but struggles with client heterogeneity, often degrading individual client models. To improve global performance, methods like FedProx [39] introduce regularization to keep local updates close to the global model, and SCAFFOLD [30] uses control variates to reduce variance from client heterogeneity. For local personalization, meta-learning and transfer learning techniques such as DFL [52] focus on enhancing individual client models to adapt better to local data. Lastly, methods like FedRoD [8] attempt to achieve joint global and local performance by decomposing models, aiming to balance both objectives, though extreme non-IID settings still pose challenges. However, these FL methods modeling non-IID data take no actions to OOD data, causing them less advantageous. ", "page_idx": 20}, {"type": "text", "text": "D Experimental Implementation Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "D.1 Experimental Setups ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Datasets Following SCONE [3], we choose clear TinyImageNet [36], Cifar10 and Cifar100 [33]and as the IN data. For OOD generalization, we select the corresponding synthetic covariate-shift dataset as IN-C data, by leveraging 15 common corruptions for all datasets, and 4 additional corruptions for Cifar10-C and Cifar100-C [19]. To evaluate FOOGD on unseen client data, we also perform experiments on PACS [38] for leave-one-out domain generalization. For OOD detection, we evaluate five OUT image datasets: SVHN [59], Texture [11], iSUN [78], LSUN-C and LSUN-R [81]. ", "page_idx": 20}, {"type": "text", "text": "Heterogeneous client data The original train and test datasets are split to all clients to simulate the practical non-IID scenario [23]. Specifically, we sample a proportion of instances of class $j$ to client $k$ using a Dirichlet distribution, i.e., $p_{j,k}\\sim\\mathrm{Dir}(\\alpha)$ , where $\\alpha$ denotes the non-IID degree of each class among the clients. A smaller $\\alpha$ indicates a more heterogeneous data distribution. For the PACS dataset, 3 clients are set where each client holds data from one distinct domain, and the remaining unseen domain data is used for testing. ", "page_idx": 20}, {"type": "text", "text": "OOD evaluation setups We construct three types of test sets to assess the model\u2019s classification, domain generalization, and out-of-distribution detection ability. Test set from IN dataset is used to evaluate how well the model adapts to local training distribution, i.e., model\u2019s classification ability. To simulate the non-IID distribution in real-world scenarios, we partition the IN-C dataset with the same heterogeneous distribution as the IN dataset. This setup evaluates the model\u2019s generalization ability on the IN-C dataset to determine whether existing FL methods can keep the data-label relationship in the presence of covariate shift in data features. All covariate-shift types in the IN-C dataset are test individually. For testing OOD detection ability, all clients use the same OOD test set for fair evaluation. After collecting performance data from all clients, we calculate a weighted average of the performance based on the volume of data each client holds. ", "page_idx": 20}, {"type": "text", "text": "D.2 Implemetnation Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We choose WideResNet [85] as our main task model for Cifar datasets, and ResNet18 [18] for TinyImageNet and PACS, and optimize each model 5 local epochs per communication round until converging with SGD optimizer. We conduct all methods at their best and report the average results of three repetitions with different random seeds. We consider client number $K=10$ , participating ratio of 1.0 for performance comparison, and the hyperparameters $\\lambda_{m}=0.5$ , $\\lambda_{a}=0.05$ . ", "page_idx": 20}, {"type": "text", "text": "Below are the detailed settings and hyperparameters for all federated baseline models. ", "page_idx": 20}, {"type": "text", "text": "1. FedAvg [56] is the classic federated learning method in which clients perform multiple epochs of SGD on their local data. The learning rate is set to 0.1, with a momentum of 0.9 and weight decay of 5e-4. ", "page_idx": 20}, {"type": "text", "text": "2. FedIIR [17] tries to implicitly learn invariant relationships through inter-client gradient alignment. We set the ema parameter 0.95 and penalty term 1e-3. ", "page_idx": 20}, {"type": "table", "img_path": "D6MQrw9HFu/tmp/51d67ac40c21feda76b52591eb3ac96b6291f56fedbb8edcbd942354163e43d7.jpg", "table_caption": ["Table 7: Main results of federated OOD detection and generalization on TinyImageNet. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "3. FedRoD [8] is the personalized federated learning method that adopts two classifiers to achieve both generic and personalized performance. 4. FOSTER [83] learns a class-conditional generator to synthesize virtual external-class OOD samples to enhance the detection ability. The weight for the outlier exposure term is set to 0.1. 5. FedTHE [26] also contains a personalized classifier. This method adopts an test time adaptation strategy that interpolates the personalized head and global classifier to enforce feature space alignment. We set $\\alpha=0.1$ and $\\beta=0.3$ as suggested in the original paper. 6. FedICON [69] performs different contrastive learning during training and test phrase to handle test-time shift problem. Each client finetunes their classifier with learning rate 0.01. ", "page_idx": 21}, {"type": "text", "text": "We also compare with centralized OOD generalization and detection methods, adapting them to a FedAvg-like approach for the federated learning scenario. ", "page_idx": 21}, {"type": "text", "text": "1. LogitNorm [76] applies a straightforward modification to the cross-entropy loss, imposing a constant norm on the logits to improve detection capabilities. $\\tau$ is tuned to be set as 0.04. ", "page_idx": 21}, {"type": "text", "text": "2. ATOL [88] generates OOD data to devise an auxillary OOD detection task to facilitate real OOD detection. We set the dimension of the latent to be 100, the mean and variance of the Gaussian distribution generating OOD data to be 5.0 and 0.1. ", "page_idx": 21}, {"type": "text", "text": "3. T3A [25] adjusts a trained linear classifier using a pseudo-prototype. The filter number is set to be 100 for experiments on Cifar10, Cifar100 and TinyImageNet, 50 for experiments on PACS. ", "page_idx": 21}, {"type": "text", "text": "E Extensive Experiment Results ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "To summary, we study four additional evaluations: (1) To compare the performance in domain generalization, we also provide the leave-one-out study on PACS [38] in Tab. 11, where FOOGD also obtains better results. (2) In Tab. 10 We compute different detection metrics, i.e., MSP, energy score, and ASH, and validate that Eq. (9) is consistently powerful in detection. (3) We vary the coefficient of $\\mathtt{S M}^{3}\\mathtt{D}$ $\\lambda_{m}=\\{0.1,0.2,0.\\dot{5},0.8,1\\}$ in Fig. 11(a)-Fig. 11(b), and vary the coefficient of SAG $\\lambda_{a}=\\{0,0.01,0.05,0.1,0.2,0.5,0.8\\}$ in Fig. 11(c), to obtain the best modeling in FOOGD. (4) We vary the number of participating clients in Fig. 10 and found FOOGD can have better results among different participating clients. ", "page_idx": 21}, {"type": "text", "text": "E.1 Evaluation on TinyImageNet Dataset. ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Additionally, we present the results of TinyImageNet in Tab. 7, and report our main results with variances in Tab. 15 and Tab. 16. It vividly states that FOOGD can also generalize in the task of more classes and more heterogeneous data distribution. ", "page_idx": 21}, {"type": "text", "text": "E.2 Ablation Studies ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We devise the variants of FOOGD , i.e., fix backbone, w/o $\\mathsf{S M}^{3}\\mathsf{D}$ , and $\\mathtt{W}/_{0}\\,\\mathtt{S A G}$ , to study the effectiveness of our three main ideas: (1) obtaining reliable global distribution as guidance, (2) estimating score model by $\\mathsf{S M}^{3}\\mathsf{D}$ , and (3) enhancing FL method generalization by SAG, respectively.s From Tab. ?? and its full version in Appendix E.2 Tab. 8 and Tab. 9, simply modeling score model fails in both OOD generalization and detection tasks, since feature extractor not adjusted with the global distribution. When we remove $\\mathsf{S M}^{3}\\mathsf{D}$ , the estimation of data probability is severely impacted, bringing no detection capability. On the contrary, the generalization performance decreases once we remove SAG. Moreover, compared with fix backbone, both w/o $\\mathsf{S M}^{3}\\mathsf{D}$ and w/o SAG have better generalization and detection results, indicating that it is necessary to introduce the global distribution. ", "page_idx": 21}, {"type": "table", "img_path": "D6MQrw9HFu/tmp/8459686b6b66365fc234ee0b6f419a3f29b150abd520449d6236b473b85bb4b4.jpg", "table_caption": ["Table 8: Cifar10 ablation study on varying $\\alpha$ modeled by FedAvg. "], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "D6MQrw9HFu/tmp/29ba6635c9b04357df6f7c9a5ae6a01148d4acb5d2643aa8a3a44d00f631db84.jpg", "table_caption": ["Table 9: Cifar100 ablation study on varying $\\alpha$ modeled by FedAvg. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "E.3 Toy Example for validating $\\mathtt{S M^{3}D}$ ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "To illustrate the effectiveness of $\\mathsf{S M}^{3}\\mathsf{D}$ , we further visualize a density estimation of 2-D toy example in Fig. 3. In detail, we model the red points sampled from target distribution, by tuning a series of coefficients, i.e., $\\lambda_{m}=\\{0,0.05,0.1,\\bar{0.2},0.4,0.5,0.8,1\\}$ in Eq. (8). To start with, the blue generated data contract loosely close to the red target data. Then the distribution divergence gets smaller, making generated distribution overlap with targeted distribution. While, as the effect of MMD increases, the distribution alignment dramatically worsens, even causing the blue generated data to collapse into the expectation of target distribution. As we can see, the mutual impacts between score matching and MMD estimation, $\\mathsf{S M}^{3}\\mathsf{D}$ has more compact density estimation when $\\lambda_{m}=0.1$ , compared with blankly using score matching $\\lambda_{m}=0)$ ) or simply using MMD $\\lambda_{m}=1)$ ). In the brand new objective of density estimation, $\\mathsf{S M}^{3}\\mathsf{D}$ expand the searching range and depth of score modeling, making it possible to comprehensively model data density. Moreover, with the calibration of MMD estimation, original data representation and the generated latents based on the score model are effectively matched, bringing more realistic and correct estimation. Hence $\\mathsf{S M}^{3}\\mathsf{D}$ could ensure a more aligned and reliable density estimation for sparse and multi-modal data. ", "page_idx": 22}, {"type": "text", "text": "E.4 Detection Score Methods Comparison ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "To study the effectiveness of our choice, i.e., $\\mathrm{{IsOUT}(\\cdot)}$ defined by the norm of score model in Eq. (9), we compare it with existing benchmarks, MSP [20], Energy score [3], and ASH [12]. As listed in Tab. 10, MSP is the runner-up method to detection, and it is flexible to detect in all baseline methods. However, IsOUT(\u00b7) is more competitive and reliable, since it utilizes the global distribution as guidance. ", "page_idx": 22}, {"type": "text", "text": "E.5 Extensive Visualization Results ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "To explore the wild data distribution of FL OOD methods, we visualize T-SNE of data representations in Fig. 8, and the detection score distributions in Fig. 9, on Cifar10 $\\alpha=5$ for FedAvg, FedRoD, ", "page_idx": 22}, {"type": "table", "img_path": "D6MQrw9HFu/tmp/ad7f57e7ebbf1d26fd8b637633f67d81ebdecd9871f7cdac2aa28442fd494e4e.jpg", "table_caption": ["Table 10: Metric comparison FedRoD+FOOGD on Cifar10. "], "table_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "D6MQrw9HFu/tmp/9375ea13873299d783f2b7c338aaf417ba1fb3f5f917b0672c63197f434c82ee.jpg", "img_caption": ["Figure 7: Motivation of $\\mathtt{S M^{3}D}$ . The red points are sampled from target distribution, while the blue points are generated via Langevin dynamic sampling from random noise. "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "D6MQrw9HFu/tmp/a3e6c7cdc39c1775f6161f73e7e0c944c518da1dd1ec466fd741e02912e1d294.jpg", "img_caption": ["Figure 8: T-SNE visualizations of FedAvg and FedRod with FOOGD. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "FedAvg+FOOGD , FedRoD $^{\\cdot+}$ FOOGD and their runner-up methods, FedATOL and FedTHE, respectively. It is evident that FOOGD represents IN-C data more tight with IN data, and constructs a comparably clear decision boundary between IN data and OUT data. Besides, we also discover that FOOGD will push OUT data away from its IN and IN-C data, which validates the guidance from the global distribution. Additionally, in Fig. 9, FOOGD makes the modes among IN, IN-C, and OUT, more separable than existing methods. This also proves the effectiveness of FOOGD in detection task. ", "page_idx": 23}, {"type": "text", "text": "E.6 Client Generalization on PACS Dataset ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "To validate the effectiveness of FOOGD in domain generalization tasks, i.e., each client contains one domain data and we train domain generalization model by leave-one-out, following FedIIR [17]. To obtain a fair comparison, we pretrain all models from scratch and utilize adaption methods as stated in their main paper, instead of using a public ImageNet pre-trained model. In terms of Tab. 11, FOOGD obtains performance improvements for FedAvg and FedRoD. Compared with existing adaption methods, FOOGD achieves outstanding results even in the toughest task, i.e., leaving Sketch domain out. This also concludes that FOOGD is capable of inter-client generalization, since FOOGD has utilized global distribution knowledge. ", "page_idx": 23}, {"type": "image", "img_path": "D6MQrw9HFu/tmp/a85e0bf95262439773654e8c559a1bb6255706e4b908c80ca4c7f8a364f6c5d5.jpg", "img_caption": ["Figure 9: Detection score distribution of FedAvg and FedRod with FOOGD on Cifar10 ( $\\alpha=5$ ). "], "img_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "D6MQrw9HFu/tmp/71041e757754d4a770fe139b97bd72b1f68332105472c0b27114ba4469ac2e17.jpg", "table_caption": ["Table 11: OOD generalization task for PACS. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "E.7 Extensive Experiments on Other IN-C and OUT data ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this part, we study the performance evaluation of FOOGD in additional IN-C and OUT datasets. In Tab. 12, we can find that FOOGD consistently enhances the detection capability for different OUT data, validating for the effectiveness of estimating global distribution via $\\dot{\\mathsf{S}}\\mathsf{M}^{3}\\mathsf{D}$ . Meanwhile, we compute the average results of different IN-C data on Fig. 12 and provide the details in Tab. 13 and Tab. 14. FOOGD consistently improve the generalization in all unseen IN-C data, indicating the effectiveness of enhancing feature extractor via SAG. ", "page_idx": 24}, {"type": "text", "text": "E.8 The Study of Hyper-parameter Sensitivity ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We vary the coefficient of $\\mathsf{S M}^{3}\\mathsf{D}\\ \\lambda_{m}=\\{0.1,0.2,0.5,0.8,1\\}$ in Fig. 11(a)-Fig. 11(b), and vary the coefficient of SAG $\\lambda_{a}=\\{0,0.01,0.05,0.1,0.2,0.5,0.8\\}$ in Fig. 11(c), to obtain the best modeling in FOOGD. To study the effect of different client numbers, we vary the number of participating clients $K=\\{5,10,20,50\\}$ in Fig. 10 and find FOOGD can have better results among different participating clients. ", "page_idx": 24}, {"type": "image", "img_path": "D6MQrw9HFu/tmp/3288b6b982edd38a0ee7dce20eb840a8019b17098430b6b2c481546c11f0ea5c.jpg", "img_caption": ["Figure 10: Effect of participating clients numbers $K$ . "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "D6MQrw9HFu/tmp/8865c79f74965c9a95cbc4c6801fe5a27612620a9c73f2a6fe6174b098e34f43.jpg", "img_caption": ["Figure 11: Effect of $\\lambda_{m}$ and $\\lambda_{a}$ . "], "img_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "D6MQrw9HFu/tmp/52dc74846ca28a235dd03737a5da511a8276550b534fd32491e57649ec924e6a.jpg", "table_caption": ["Table 12: Other detection results on Cifar10 $(\\alpha=0.1)$ ). "], "table_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "D6MQrw9HFu/tmp/ad089db98d263f7281dbe523f90db67871c534a8c8f594f84341523e6592315d.jpg", "img_caption": ["Figure 12: Average generalization results. $^+$ FOOGDis short for FedAvg $^+$ FOOGDand FedRoD $^+$ FOOGD, respectively. "], "img_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "D6MQrw9HFu/tmp/ab15889c305df5d4af18298168110dc777d76f6d59595102992c884119a2a31d.jpg", "table_caption": [""], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "D6MQrw9HFu/tmp/351601ffbccfe8844f1d24cdeaa08f9f5135ac7863b79d90f8c619f4bc941b04.jpg", "table_caption": [""], "table_footnote": [], "page_idx": 28}, {"type": "table", "img_path": "D6MQrw9HFu/tmp/56c54018f74abb9d4390514906f9893f93c622f044c3a8af01c2ecc79da52947.jpg", "table_caption": ["Table 15: Main results of federated OOD detection and generalization on Cifar10. We report the ACC of brightness as IN-C ACC, the FPR95 and AUROC ofLSUN-C as OUT performance. "], "table_footnote": [], "page_idx": 29}, {"type": "table", "img_path": "D6MQrw9HFu/tmp/2dcc48dd32513a1e9af1c158674aa3cb4069859d5850900999d5fe600462b5db.jpg", "table_caption": [""], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and precede the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 31}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 31}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 31}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 31}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 31}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 31}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 31}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: The abstract and introduction section include the main claims made in the paper. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 31}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper focuses on federated modeling, and we will focus on enhancing privacy-preserving ability of FOOGD in our future work. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 32}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We have provided the full set of assumptions and a complete (and correct) proof. Please refer to Section 4 in the main paper and Section B in the Appendix. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 32}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We have provided detailed implementation details in Section 5.1 of the main paper and Section A and D of the Appendix. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 33}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: We have provided publicly available dataset and experiment details information in Section 5.1 of the main paper and Section A and D of the Appendix. We commit to releasing the source code after the paper is accepted. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. ", "page_idx": 33}, {"type": "text", "text": "\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). \u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 34}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We have provided detailed implementation details and training settings in Section 5.1 of the main paper and Section A and D of the Appendix. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 34}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: Our experimental results are computed three times and report the average result. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 34}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: My research group supports me in computer resources. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 35}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: The research conducted in the paper fully adheres to the NeurIPS Code of Ethics. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 35}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: The paper discussed both potential positive societal impacts and negative societal impacts in Section 1 of the Appendix. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 35}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 36}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: The data and code used in this paper have obtained legal permissions. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 36}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 36}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "page_idx": 36}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 37}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 37}]