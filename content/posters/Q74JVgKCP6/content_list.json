[{"type": "text", "text": "Near-Optimality of Contrastive Divergence Algorithms ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Pierre Glaser  Kevin Han Huang  . Arthur Gretton Gatsby Computational Neuroscience Unit, University College London pierreglaser@gmail.com, han.huang.20@ucl.ac.uk, arthur.gretton@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We perform a non-asymptotic analysis of the contrastive divergence (CD) algorithm, a training method for unnormalized models. While prior work has established that (for exponential family distributions) the CD iterates asymptotically converge at an $O(n^{-1/3})$ rate to the true parameter of the data distribution, we show, under some regularity assumptions, that CD can achieve the parametric rate $O(n^{-1/2})$ Our analysis provides results for various data batching schemes, including the fully online and minibatch ones. We additionally show that CD can be near-optimal, in the sense that its asymptotic variance is close to the Cram\u00e9r-Rao lower bound. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Describing data using probability distributions is a central task in multiple scientific and industrial disciplines [1, 2, 3]. Since the true distribution of the data is generally unknown, such a task requires finding an estimator of the true distribution among a model class that best describes the available data. An estimator can be characterized at multiple levels of granularity: at the highest level lies consistency [4], a property which states that as the number of available data points increases, a given estimator will converge to the one best describing the data distribution. At a lower level, a consistent estimator can be further characterized by its convergence rate, a quantity upper-bounding its distance to the true distribution as a function of the number of samples. A convergence rate can be either asymptotic, e.g. hold only in the limit of an infinite sample size, or non-asymptotic, in which case the rate also holds for finite sample sizes. In their simplest form, convergence rates are provided in big- $\\cdot O$ notation, discarding finer grained information such as asymptotically dominated quantities as well as multiplicative constants. These constants play a role in the so-called asymptotic variance of the estimator, which is a precise descriptor of an estimator's statistical efficiency. Convergence rates and asymptotic variances have been the subject of extensive research in the statistical literature; in particular, well-known lower bounds exists regarding both the best possible (asymptotic) convergence rate of an estimator and its best possible asymptotic variance. These results set a clear frame of reference to interpret individual convergence rates, and are routinely present in the analysis of modern statistical algorithms such as noise-contrastive estimation [5, 6] or score matching [7, 8, 9]. ", "page_idx": 0}, {"type": "text", "text": "In this work, we focus on cases where (1) the true data distribution admits a density with respect to some known base measure, and (2) the model class is parametrized by a finite-dimensional parameter. In this setting, provided that the true distribution belongs to the model class, a celebrated result in statistical estimation states that the model maximizing the average log-likelihood both achieves the best possible asymptotic convergence rate (called the parametric rate) and the best possible asymptotic variance, called the Cram\u00e9r-Rao bound (see, e.g. [10]). While this result shows that Maximum Likelihood Estimators (MLE) are asymptotically optimal, fitting them is complicated by computational hurdles when using models with intractable normalizing constants. Such unnormalized models are common in the Machine Learning literature due to their high fexibility [11, 12]; their weakness however lies in the fact that expectations under these models have no unbiased approximation. For this reason, popular approximation algorithms such as unbiased gradient-based stochastic optimization of the empirical log-likelihood cannot a priori be used, as the gradient of the normalizing constant is given by an expectation under the model distribution. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "The Contrastive Divergence (CD) algorithm [13] is a popular approach that circumvents this issue by using a Markov Chain Monte Carlo (MCMC) algorithm to approximate the gradient of the loglikelihood. Unnormalized models trained with Contrastive Divergence have been shown to reach competitive performance in high-dimensional tasks such as image [14, 15, 16], text [17], and protein modeling [18, 19], or neuroscience [20]. A consistency analysis of the Contrastive Divergence algorithm is delicate, however: indeed, the optimization error e.g. the difference between the estimate returned by CD and the MLE, is likely to be non-negligible as compared with statistical error - the distance between the MLE and the true distribution - and thus cannot be discarded, as often done when analyzing estimators that minimize tractable objectives [8, 5]. Recent work [21] elegantly established asymptotic $O(n^{-1/3})$ consistency of the CD estimator for unnormalized exponential families when using only a finite number of MCMC steps. Key to their argument is the fact that the bias of the CD gradient estimate decreases as iterates approach the data distribution. However, as noted by the authors, their work left open the question of whether and under what conditions CD might achieve $O(n^{-1/2})$ -consistency. ", "page_idx": 1}, {"type": "text", "text": "Contributions_ In this work, we answer this question by providing a non-asymptotic analysis of the CD algorithm for unnormalized exponential families. While existing convergence bounds [21] were derived for the \u201cfull batch\" setting, where the CD gradient is estimated using the full dataset at each iteration, our analysis covers both the online setting (where data points are processed one at a time without replacement), and the offline setting with multiple data reuse strategies (including full batch). ", "page_idx": 1}, {"type": "text", "text": "In the online case (Section 3), we show, under a restricted set of assumptions compared to Jiang et al. [21], that the CD iterates can converge to the true distribution at the parametric $O(n^{-1/2})$ rate. Our analysis reveals that CD contains two sources of approximation: a bias term, and a variance term. These sources are almost independent of each other, in the sense that decreasing the bias by increasing the number of MCMC steps will not decrease the variance. The impact of these two sources of approximation transparently propagates in our resulting bounds: in particular, as the bias of the CD algorithm goes to 0, our bounds recover well-known results in online stochastic optimization [22]. Finally, we study the asymptotic variance of an estimator obtained by averaging the CD iterates, a classic acceleration technique in stochastic optimization [23]. We show that provided that the number ofsteps $m$ is sufficiently large, the asymptotic variance of this estimator matches (up to a factor 4) the Cram\u00e9r-Raobound. ", "page_idx": 1}, {"type": "text", "text": "Next, we study the offline setting (Section 4), where the CD gradient is estimated by reusing (potentially random) subsets of a finite dataset. We show that a similar result to the online setup holds, up to an additional correlation term that arises from data reuse, and present several approaches to control this term. We improve over the results of [21] by showing a non-asymptotic and nearparametric rate at $O((\\log n)^{\\bar{1}/2}n^{-1/2})$ under their conditions, and also illustrate how different rates can be obtained under a variety of conditions. Our results also show an interesting tradeoff between the effect of initialization and the statistical error as a function of batch size. ", "page_idx": 1}, {"type": "text", "text": "In summary, we establish the near-optimality of a variety of Contrastive Divergence algorithms for unnormalized exponential families in the so called \u201clong-run\" regime, where the number of MCMC steps is high enough to ensure that the CD gradient bias is sufficiently offset by the convexity of the negative log-likelihood. ", "page_idx": 1}, {"type": "text", "text": "2  Contrastive Divergence in Unnormalized Exponential Families ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Unnormalized Exponential Families Exponential families (EF) [24, 25] form a well-studied class of probability distributions, given by ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r}{p_{\\psi}(\\mathrm{d}x):=e^{\\psi^{\\top}\\phi(x)-\\log Z(\\psi)}c(\\mathrm{d}x),\\quad Z(\\psi):=\\int_{\\mathcal{X}}e^{\\psi^{\\top}\\phi(x)}c(\\mathrm{d}x).}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Here, $\\mathcal{X}\\ni x$ is the data or sample space, which we set to be a subset of $\\mathbb{R}^{d}$ for some $d\\in\\mathbb{N}^{*}$ , although our results are readily extendable to more general measurable spaces. $c$ is a measure on $\\mathcal{X}$ called the base or carrier measure. When $\\mathcal{X}\\subseteq\\mathbb{R}^{d}$ \uff0c $c$ is often set to be the corresponding Lebesgue measure. $\\psi\\in\\Psi\\subseteq\\mathbb{R}^{p}$ is a finite-dimensional parameter called the natural parameter, and $\\phi:\\mathbb{R}^{d}\\longmapsto\\mathbb{R}^{p}$ is a function called the sufficient statistics, which, alongside with the base measure, fully describes an exponential family. Finally, $\\log Z(\\psi)$ ,the log-normalizing (or cumulant) function,is a quantity ensuring that $p_{\\psi}$ integrates to 1 over $\\mathcal{X}$ . Crucially, we will not assume that $\\log Z(\\psi)$ admits a closed form expression for all $\\psi$ . The latter fact provides the practitioner with a great deal of fexibility in designing the model class: indeed, the only requirement that should be satisfied prior to performing statistical estimation is to have $Z(\\psi)<+\\infty$ for all $\\psi$ , something that can be readily verified and is often the case in practice. The drawback of unnormalized EFs is the fact that sampling (and thus approximating expectations under the model) cannot usually be performed in an unbiased manner. Instead, inference in unnormalized EFs is often performed using tools from the Bayesian Inference literature, such as MCMC [26]. Unnormalized EFs belong to the larger class of unnormalized models [27,28, 7, 6], of the form $e^{-E_{\\psi}(x)-\\log Z(\\psi)}c(\\mathrm{d}x)$ \uff0c $\\begin{array}{r}{Z(\\psi)\\bar{\\,}=\\int e^{-E\\bar{\\psi^{(x)}}}c(\\mathrm{d}x)}\\end{array}$ , for some parametrized function $E_{\\psi}:\\mathbb{R}^{d}\\longmapsto\\mathbb{R}$ referred to as the energy. Unnormalized models thus take the flexibility of unnormalized EFs one step further by allowing the (negative) unnormalized log-density to be an arbitrary function $E_{\\psi}$ of $x$ and $\\psi$ , instead of requiring a linear dependence on $\\psi$ as in Equation 1. We focus in this work on unnormalized EFs due to the multiple computational benefits they provide, as explained in the next section, but we believe that extending our analysis to more general unnormalized models is an interesting avenue for future work. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Statistical Estimation in Unnormalized Exponential Families using Contrastive Divergence We now review the Contrastive Divergence algorithm, an algorithm used to fit unnormalized models, and our main object of study in this work. The general setting is the following: we assume access to $n$ ii.d. samples $(X_{1},\\bot...\\,\\dot{X}_{n})$ drawn from some unknown distribution $p^{\\star}$ , which we assume belongs to $\\mathcal{P}_{\\psi}$ , e.g. $p^{\\star}=p_{\\psi^{\\star}}$ for some $\\psi^{\\star}\\in\\Psi$ . Given these samples, we aim to perform statistical estimation, e.g. find a parameter $\\psi_{n}$ within $\\Psi$ that should approach $\\psi^{\\star}$ as $n$ grows. ", "page_idx": 2}, {"type": "text", "text": "The starting point of the Contrastive Divergence algorithm is the unfortunate realization that Maximum Likelihood Estimation, which corresponds to minimizing the cross-entropy ${\\mathcal{L}}(\\psi)\\;:=$ $-\\mathbb{E}_{p_{n}}\\log\\mathrm{d}p_{\\psi}/\\mathrm{d}c$ between themodel $p_{\\psi}$ and the empiricaldatadisribution $\\begin{array}{r}{p_{n}:=1/\\bar{n_{}}\\sum_{i=1}^{n}\\delta_{X_{i}}}\\end{array}$ cannot be performed using exact (possibly stochastic) gradient-based optimization, as the gradient $\\nabla_{\\psi}\\mathcal{L}(\\psi)$ of $\\mathcal{L}$ with respect to the parameter $\\psi$ contains an expectation under the model distribution $p_{\\psi}$ . Indeed, the cross entropy and its gradient are given by ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\{\\mathcal{L}(\\psi)\\right.}&{=-\\frac{1}{n}\\sum_{i=1}^{n}\\phi(X_{i})^{\\top}\\psi+\\log Z(\\psi)}\\\\ {\\left.\\nabla_{\\psi}\\mathcal{L}(\\psi)\\right.}&{=-\\frac{1}{n}\\sum_{i=1}^{n}\\phi(X_{i})+\\mathbb{E}_{p_{\\psi}}\\phi.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The second line follows from the well known identity $\\nabla_{\\psi}\\log Z(\\psi)\\,:=\\,\\mathbb{E}_{p_{\\psi}}\\phi$ : we refer to [25, Proposition 3.1] for a proof. The Contrastive Divergence algorithm circumvents this issue by running approximate stochastic gradient descent (SGD) on $\\mathcal{L}$ , where the intractable expectation in $\\nabla_{\\psi}\\log Z$ is estimated using an MCMC algorithm initialized at the empirical data distribution. In more details, given a number of epochs $T$ , a sequence of data batches $B_{t,j}$ of size $B$ (e.g. $B_{t,j}\\in[1,n]^{B},1\\leq t\\leq T,1\\leq j\\leq N\\lceil n/B\\rceil)$ , and a family of Markov kernels $\\{k_{\\psi},\\psi\\in\\Psi\\}$ each with invariant distribution $p_{\\psi}$ , at the $j^{t h}$ minibatch of epoch $t$ $\\nabla_{\\psi}\\log Z(\\psi_{t,j-1})$ is approximated by $\\textstyle\\frac{1}{B}\\sum_{i\\in B_{t,j}}\\phi(\\tilde{X}_{i}^{m})$ where $\\tilde{X}_{i}^{m}$ is produced by running the recursion $\\tilde{X}_{i}^{k}\\sim k_{\\psi_{t}}(\\tilde{X}_{i}^{k-1},\\cdot)$ $\\tilde{X}_{i}^{0}=X_{i}$ up to $k=m$ . Throughout the paper, we will refer to the conditional distribution of $\\tilde{X}_{i}^{m}$ given $X_{i}$ as $k_{\\psi}^{m}(X_{i},\\cdot)$ . The resulting gradient estimate arising from combining this approximation with the other (tractable) sum over the data samples present in $\\nabla_{\\psi}\\mathcal{L}(\\psi)$ , which we refer to as the $C D$ gradient and denote as $h_{t}$ , is thus ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{h_{t,j}:=\\frac{1}{B}\\sum_{i\\in B_{t,j}}\\phi(X_{i})-\\frac{1}{B}\\sum_{i\\in B_{t,j}}\\phi(\\tilde{X}_{i}^{m})=\\frac{1}{B}\\sum_{i\\in B_{t,j}}\\left(\\phi(X_{i})-\\phi(\\tilde{X}_{i}^{m})\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Key to the behavior and analysis of the CD algorithm is the strategy employed to generate minibatches $B_{t,j}$ . The case where $T=1$ $B=1$ , and $B_{1,j}\\,=\\,\\{j\\}$ will be referred to as online CD, while the variant where $T>1$ , and each batch $B_{t,j}$ draws $B$ indices (with or without replacement) from $[1,n]$ will be referred to as offline CD. In online CD, each data point is present in one and one batch only, while in offine CD, data points are reused across batches. From a statistical perspective, we will see that online CD can be analyzed in a remarkably simple way, while offline CD introduces additional correlations that require care to be controlled. Both settings come with their advantages and drawbacks, as we will see in the next section. The CD algorithms we study will employ decreasing step size schedules $(\\eta_{t})_{t\\geq0}$ of the form $\\eta_{t}\\,=\\,C t^{-\\beta}$ , where $C>0$ is the initial leaning rate and $\\beta\\in[0,1]$ . We lay out online CD and ofline CD in Algorithms 1 and 2. Note that our algorithms include a projection step on the parameter space $\\Psi$ to account for the case where $\\Psi$ is compact. In the case $\\Psi=\\mathbb{R}^{p}$ , this step can be omitted. Next we depart from the setting of [21] and start by analyzing online CD. ", "page_idx": 2}, {"type": "image", "img_path": "Q74JVgKCP6/tmp/6d606a6aa8c93071665ed6fd6adc28f293d192ca83584e99e797989f29f326d4.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Algorithm 2 Offline CD Input: $(X_{1},\\dots,X_{n})\\overset{\\mathrm{i.i.d.}}{\\sim}p_{\\psi^{\\star}}$ Parameters: Same as Algorithm 1, plus number of epochs $T$ , batch size $B$ , batching schedule $B_{t,j}$ , initial parameter $\\psi_{0,0}$ for $t=1,\\dots,T$ do for $j=1,\\dots\\lceil n/B\\rceil$ do $[\\tilde{X}_{t,i,j}^{m}\\sim k_{\\psi_{t-1}}^{m}(X_{i},\\cdot)$ for i in $B_{t,j}]$ $\\begin{array}{r}{h_{t,j}:=\\frac{1}{B}\\sum_{i\\in B_{t,j}}(\\phi(X_{i})\\!-\\!\\phi(\\tilde{X}_{t,i,j}^{m}))}\\end{array}$ $\\psi_{t,j}\\leftarrow\\psi_{t,j-1}-\\eta_{t}h_{t,j}$ $\\psi_{t,j}\\leftarrow\\mathrm{Proj}_{\\Psi}(\\psi_{t,j})$ end for end for return $\\psi_{T}$ ", "page_idx": 3}, {"type": "text", "text": "3  Non-asymptotic analysis of Online CD ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "3.1  Preliminaries and Assumptions ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Recall that the chi-squared divergence between two probability measures $p$ and $q$ is defined as: $\\begin{array}{r}{\\chi^{2}(p,q):=\\big(\\int(\\frac{\\mathrm{d}p}{\\mathrm{d}q}(x)-1)^{2}q(\\mathrm{d}x)\\big)^{1/2}}\\end{array}$ $p\\ll q$ , and $+\\infty$ otherwise. Here, $p\\ll q$ denotes that $p$ is absolutely continuous with respect to $q$ and $\\mathrm{d}p/\\mathrm{d}q$ is the Radon-Nikodym derivative [29] of $p$ with respect to $q$ Let $L_{p_{\\psi}}^{2}(\\mathbb R^{d})$ be the space o square-integrable functions with respect to $p_{\\psi}$ . For a function $f\\in L_{p_{\\psi}}^{2}(\\mathbb R^{d})$ , we define ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\alpha(f,\\psi)=\\frac{\\big(\\int\\big(\\int(f-\\mathbb{E}_{p_{\\psi}}f)(x)\\,k(x,\\mathrm{d}y)\\big)^{2}p_{\\psi}(\\mathrm{d}x)\\big)^{1/2}}{\\big(\\int(f-\\mathbb{E}_{p_{\\psi}}f)(x)^{2}p_{\\psi}(\\mathrm{d}x)\\big)^{1/2}}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "which is a measure of how quick a Markov chain with kernel $k_{\\psi}$ mixes, relative to the function $f$ [30]. With these definitions in hand, we now state the assumptions required by our analysis of online CD. These assumptions form a strict subset of the assumptions considered in prior work [21], which required additional regularity and tail conditions on the Markov kernels $k_{\\psi}$ ", "page_idx": 3}, {"type": "text", "text": "Assumption A1. $\\mathcal{P}_{\\psi}$ is a subset of a regular and minimal [25, Section 3.2] exponential family with natural parameter domain $\\mathcal{D}\\subseteq\\mathbb{R}^{p}$ \uff0c $\\Psi$ is a convex and compact subset of $\\mathcal{D}$ , and $\\psi^{*}$ lies in the interior of $\\Psi$ ", "page_idx": 3}, {"type": "text", "text": "Assumption A2. There exists a constant $C_{\\chi}>0$ such that $\\chi^{2}(p_{\\psi^{\\star}},p_{\\psi})\\leq C_{\\chi}^{2}\\|\\psi-\\psi^{\\star}\\|^{2}$ ", "page_idx": 3}, {"type": "text", "text": "Assumption A3. $\\alpha:=\\operatorname*{sup}\\{\\alpha(f,\\psi)$ \uff0c\u3002 $f\\in\\{\\phi_{i}\\}_{i=1}^{p}\\cup\\{\\phi_{i}\\phi_{j}\\}_{i,j=1}^{p},\\,\\psi\\in\\Psi\\}<$ ,where $\\phi_{i}$ is the $i$ th component of the function $\\phi$ , and $\\phi_{i}^{2}$ is the $i$ -th component of the function $x\\longmapsto\\phi(x)^{2}\\,$ ", "page_idx": 3}, {"type": "text", "text": "A well known property of EFs [25, Proposition 3.1] is that their negative cross-entropy (against any other measure) is $C^{\\infty}$ , convex, and strictly so if the exponential family is minimal (meaning that the set of sufficient statistic functions $\\phi_{i}$ are not linearly dependent). Leaving aside the issue of intractable expectations, this convexity suggests that $\\mathcal{L}$ can be efficiently minimized using stochastic approximation algorithms [31, 22]. The compactness of $\\Psi$ provided by Assumption A1 thus ensures, by the extreme value theorem [32], the existence of finite positive constants $\\mu$ and $L$ defined as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mu:=\\operatorname*{min}_{\\psi\\in\\Psi}\\lambda_{\\operatorname*{min}}\\left(\\nabla_{\\psi}^{2}\\mathcal{L}(\\psi)\\right),\\quad L:=\\operatorname*{max}_{\\psi\\in\\Psi}\\lambda_{\\operatorname*{max}}\\left(\\nabla_{\\psi}^{2}\\mathcal{L}(\\psi)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\nabla_{\\psi}^{2}\\mathcal{L}$ is the Hessian of $\\mathcal{L}$ with respect to $\\psi$ $\\mu$ (called the strong convexity constant) and $L$ (a bound controlling the smoothness of the problem) play a critical role in the analysis of convex optimization algorithms [31]. While it is possible to obtain convergence rates in non-smooth or non-strongly-convex settings, our analysis follows the spirit of [21] by leveraging the strong convexity of the problem to compensate for the bias introduced by using CD gradients instead of unbiased stochastic gradients. ", "page_idx": 3}, {"type": "text", "text": "Assumption A2 allows to link variations in distribution space to variations in parameter space, and will be instrumental to control the bias of the CD gradient. Note that since $\\chi^{2}(p_{\\psi}{\\star},p_{\\dot{\\psi}})\\;{=}\\;$ elog Z(2u-b\\*)-(2 log Z(cb)-log Z(\u03a6\\*) - 1 provided that 2b - (\\* E D (see [33, Lemma 1]), we expect Assumption A2 to hold in many cases of interests. On the other hand, the possible exponential scaling Of $C_{\\chi}$ W.r.t $\\log{Z}$ suggests that this constant may be large in some instances. ", "page_idx": 4}, {"type": "text", "text": "Assumption A3 is a restricted spectral gap condition: it guarantees that the time required by the MCMC algorithm to estimate expectations of $\\phi$ and $\\phi^{2}$ under $p_{\\psi}$ will be uniformly bounded. This assumption is weaker than the (unrestricted) uniform spectral gap condition of [21], which requires that $\\alpha$ controls the convergence rate of all functions in $L_{p_{\\psi}}^{2}(\\mathbb R^{d})$ . Note that standard results in stochastic analysis [34] guarantee that $\\alpha\\leq1$ : thus, it only remains to ensure that $\\alpha$ is strictly less than 1. Spectral gaps are strongly dependent on two properties of distribution: their tail behavior and their multimodality. While multimodality poses the risk of pushing the constant $\\alpha$ close to 1, very heavy tails distributions may not verify the spectral gap condition at all. ", "page_idx": 4}, {"type": "text", "text": "3.2 Results ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "3.2.1 Parametric convergence of online CD ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we show that under the assumptions stated in Section 3.1, the iterates $\\psi_{t}$ produced by the online CD algorithm described in Algorithm 1 will converge to the true parameter $\\psi^{\\star}$ at the parametric rate $O(n^{-1/2})$ . To do so, we follow a well known paradigm in convex optimization [22] by deriving a recursion on the quantity $\\delta_{t}:=\\mathbb{E}\\left\\|\\psi_{t}-\\psi^{\\star}\\right\\|^{2}$ , which will allow, after unrolling, to obtain convergence rates for the iterates $\\psi_{t}$ . We aim to characterize precisely the impact of performing CD as opposed to performing online SGD on $\\mathcal{L}$ , which would consist of replacing the CD gradient $h_{t}$ of Algorithm 1 by the unbiased (stochastic) gradient, given by: ", "page_idx": 4}, {"type": "equation", "text": "$$\ng_{t}(\\psi):=-\\phi(X_{t})+\\nabla_{\\psi}\\log Z(\\psi)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "which satisfies $\\mathbb{E}g_{t}(\\psi)=\\nabla_{\\psi}\\mathcal{L}(\\psi)$ . The only stochasticity in $g_{t}$ comes from the sampling of a single data point $x_{t}$ from the true distribution, which is unavoidable in the online setting, and we have $\\begin{array}{r}{\\mathbb{E}\\|g_{t}\\dot{(}\\psi^{\\star})\\|^{2}=\\mathrm{Cov}_{p_{\\psi^{\\star}}}\\phi=:\\sigma_{\\star}^{2}\\;\\mathrm{.}}\\end{array}$ $\\sigma_{\\star}^{2}$ plays a key role in the analysis of Stochastic Gradient Descent [22]. We expect that replacing $g_{t}$ by $h_{t}$ will introduce two sources of approximation: a bias term coming from using a finite number of MCMC steps $m$ , and an additional variance term, coming from using a single sample $\\tilde{x}_{t}^{m}$ to estimate $\\nabla_{\\psi}\\log Z(\\psi_{t})$ . With that in mind, we derive a recursion on $\\delta_{t}$ in the following lemma. ", "page_idx": 4}, {"type": "text", "text": "Lemma 3.1. Let $(\\psi_{t})_{0\\leq t\\leq n}$ be the iterates fromAlgorithm1.Denote $\\delta_{t}=\\mathbb{E}\\|\\psi_{t}-\\psi^{\\star}\\|^{2}$ \uff0c $\\sigma_{\\star}=$ $(\\mathbb{E}_{p_{\\psi}\\star}\\|\\phi-\\mathbb{E}_{p_{\\psi}\\star}\\phi\\|^{2})^{1/2}$ , and $\\sigma_{t}=(\\mathbb{E}_{p_{\\psi_{t}}}\\|\\phi-\\mathbb{E}_{p_{\\psi_{t}}\\phi}\\|^{2})^{1/2}$ . Then, under Al, A2 and $A3$ for all $t\\geq1$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\delta_{t}\\leq(1-2\\eta_{t}\\tilde{\\mu}_{m,t-1}+2\\eta_{t}^{2}L^{2})\\delta_{t-1}+\\eta_{t}^{2}\\tilde{\\sigma}_{m,t-1}^{2}+4\\alpha^{m/2}\\eta_{t}^{2}\\left\\lVert\\log Z\\right\\rVert_{3,\\infty}C_{\\chi}\\delta_{t-1}^{1/2}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\|\\log Z\\|_{3,\\infty}$ is a constant, $\\tilde{\\mu}_{m,t}:=\\mu-\\alpha^{m}\\sigma_{t}C_{\\chi},$ and $\\tilde{\\sigma}_{m,t}:=(\\sigma_{\\star}^{2}+\\sigma_{t}^{2}+2\\sigma_{t}^{2}\\alpha^{2m})^{1/2}.$ ", "page_idx": 4}, {"type": "text", "text": "Lemma 3.1 is proved in Appendix D.2, which details the form of $\\|\\log Z\\|_{3,\\infty}$ , a constant that we expect to scale roughly as $d L$ . Loosely speaking, this recursion suggests that as the learning rate $\\eta_{t}$ goes to O, the two terms scaling in $\\dot{\\eta}_{t}^{2}$ will be negligible, in which case we will have: $\\delta_{t}\\,\\leq\\,\\dot{(1\\,\\overline{{\\,\\dots\\,}}\\,2\\eta_{t}\\tilde{\\mu}_{m,t-1})}\\delta_{t-1}\\,<\\,\\delta_{t-1}$ , yielding convergence of $\\delta_{t}$ to 0. We make these arguments formal in the next theorem. The reader familiar with the convex optimization literature will note the similarities between this recursion and the one derived in [22], which would apply as is to online SGD on $\\mathcal{L}$ using $g_{t}$ . The difference between the two recursions is that the roles of the strong convexity constant $\\mu$ and the noise $\\sigma_{\\star}$ are now played respectively by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\tilde{\\mu}_{m,t-1}\\ =\\mu-\\alpha^{m}\\sigma_{t-1}C_{\\chi}\\quad\\mathrm{~and~}\\quad\\tilde{\\sigma}_{m,t-1}^{2}\\ =\\sigma_{\\star}^{2}+\\sigma_{t}^{2}+2\\sigma_{t}^{2}\\alpha^{2m}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "These two modifications respectively characterize the impact of the bias and the additional variance introduced by the CD gradient. The last term in Equation 7, scaling in $\\alpha^{m/2}\\eta_{t}^{2}\\sqrt{\\delta_{t}}$ , is a residual higher order mixed term coming from relating the variance of the Markov chain sample $\\tilde{x}_{t}^{m}$ to $\\sigma_{t}^{2}$ This term can be easily controlled as done next, and disappears as $m\\rightarrow\\infty$ . Investigating the impact of $m$ in the recursion, we notice that as $m\\rightarrow\\infty$ \uff0c $\\tilde{\\mu}_{m,t}\\to\\mu$ . As we will see later, this ensures that CD will converge for a sufficiently high $m$ . On the other hand, in that same regime, $\\tilde{\\sigma}_{m,t}$ does not convergeto $\\sigma_{\\star}$ , but rather to $(\\sigma_{\\star}^{2}+\\sigma_{t}^{2})^{1/2}$ , showing the irreducible impact of the variance term. While we precisely investigate the impact of the residual variance term in the next section, we now unify $\\sigma_{\\star}$ and $\\sigma_{t}$ by introducing ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sigma:=\\operatorname*{sup}_{\\psi\\in\\Psi}(\\mathbb{E}_{p_{\\psi}}\\Vert\\phi-\\mathbb{E}_{p_{\\psi}}\\phi\\Vert^{2})^{1/2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "$\\sigma$ is an upper bound on the noise induced both by the CD gradient and by the online setup, and was used in prior work [21]. Note that by the properties of $\\log{Z},\\sigma^{2}$ also equals $\\mathrm{sup}_{\\psi\\in\\Psi}\\operatorname{tr}(\\overbar{\\nabla}_{\\psi}^{2}\\mathcal{L}(\\psi))$ where $\\operatorname{tr}(A)$ is the trace of $A\\in\\mathbb{R}^{p\\times p}$ , and thus finite by the extreme value theorem. The following theorem is obtained by invoking standard unrolling arguments in the convex optimization literature. In the next result, we use the function $\\varphi_{\\gamma}(t)$ , defined as $\\begin{array}{r}{\\varphi_{\\gamma}(t)=\\frac{t^{\\gamma}-1}{\\gamma}}\\end{array}$ $\\gamma\\neq0$ and $\\log t$ $\\gamma=0$ ", "page_idx": 5}, {"type": "text", "text": "$n\\,\\geq\\,1$ $(\\psi_{t})_{0\\leq t\\leq n}$ $^{\\,l}$ $\\delta_{t}:=\\mathbb{E}\\left\\|\\psi_{t}-\\psi^{\\star}\\right\\|^{2}$ $\\begin{array}{r}{m>\\frac{\\log(\\sigma C_{\\chi}/\\mu)}{\\log|\\alpha|}}\\end{array}$ $\\tilde{\\mu}_{m}:=\\mu-\\alpha^{m}\\sigma C_{\\chi}>0$ under Assumptions $A I$ \uff0c $A2$ and $A3;$ for $\\eta_{t}=C t^{-\\beta}$ with $C>0$ we have: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\delta_{n}\\leq\\left\\{\\!\\!\\begin{array}{l l}{2\\exp\\left(4\\tilde{L}C^{2}\\varphi_{1-2\\beta}(n)\\right)\\exp\\left(-\\frac{\\tilde{\\mu}_{m}C}{4}n^{1-\\beta}\\right)\\left(\\delta_{0}+\\frac{\\tilde{\\sigma}_{m}^{2}}{\\tilde{L}^{2}}\\right)+\\frac{4C\\tilde{\\sigma}_{m}^{2}}{\\tilde{\\mu}_{m}n^{\\beta}},}&{i f0\\leq\\beta<1}\\\\ {\\frac{\\exp\\left(2\\tilde{L}^{2}C^{2}\\right)}{n^{\\tilde{\\mu}C}}\\left(\\delta_{0}+\\frac{\\tilde{\\sigma}_{m}^{2}}{\\tilde{L}^{2}}\\right)+2\\tilde{\\sigma}_{m}^{2}C^{2}\\frac{\\varphi_{\\tilde{\\mu}_{m}C/2-1}(n)}{n^{\\tilde{\\mu}_{m}C/2}},}&{i f\\beta=1\\,,}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\tilde{\\sigma}_{m}=\\sigma^{2}(2+2\\alpha^{2m})+\\alpha^{m/2}\\left\\|\\log Z\\right\\|_{3,\\infty}^{2}C_{\\chi}^{2}$ and $\\tilde{L}=(L^{2}+\\alpha^{m/2})^{1/2}$ - Consequently, if $\\begin{array}{r}{\\eta_{n}=\\frac{C}{n}}\\end{array}$ with an initial learning rate $C>2\\tilde{\\mu}_{m}^{-1}$ we have $\\begin{array}{r}{\\sqrt{\\delta_{n}}\\leq2\\tilde{\\sigma}_{m}C\\sqrt{\\frac{\\tilde{\\mu}_{m}C}{\\tilde{\\mu}_{m}C-2}}\\frac{1}{\\sqrt{n}}+o\\big(\\frac{1}{n}\\big)}\\end{array}$ \uff1a ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.2 is proved in Appendix D.3. It shows that the iterates produced by online CD will converge to the true parameter $\\psi^{\\star}$ at the rate $O(n^{-1/2})$ provided that the number of steps $m$ is suficiently large, improving over the asymptotic $O(n^{-1/3})$ rate of [21], while imposing slightly weaker conditions on the number of steps $m$ (see [21, Theorem 2.1]). This proves that online CD can be asymptotically competitive with other methods for training unnormalized models, such as Noise Contrastive Estimation [6], or Score Matching [7]. However, the asymptotic variance of $\\psi_{t}$ (e.g. the multiplicative factor in front of the $O(n^{-1/2})$ term) is likely to be suboptimal, e.g. much larger than the Cramer-Rao bound, given by the trace of the inverse of the Fisher information matrix [25]. Given the statistical optimality of MLE, and the fact that CD in an approximate MLE method, this motivates the further goal or obtaining a CD estimator with near-optimal statistical properties. In the next section, we achieve this goal by showing that averaging the iterates $\\psi_{t}$ will produce a near statistically-optimal estimator, in a sense that we will make precise. ", "page_idx": 5}, {"type": "text", "text": "3.2.2  Towards statistical optimality with averaging ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Polyak-Ruppert averaging [23]is a simple yet surprisingly effective way to construct an asymptotically optimal estimator $\\begin{array}{r}{\\bar{\\psi}_{n}:={\\frac{1}{n}}\\sum_{t=1}^{n}\\psi_{i}}\\end{array}$ from asequence of iterates $(\\psi_{t})_{0\\leq t\\leq n}$ obtained by running a standard online SGD algorithm [22]. As shown in [22], when the objective is the cross-entropy of a model, and assuming the unbiased stochastic gradients are available, averaging yields an estimator $\\overline{{\\psi}}$ with the asymptotic variance $\\mathrm{tr}(\\mathcal{T}(\\psi^{\\star})^{-1})/n$ , where $\\mathcal{T}(\\psi):=\\mathrm{Cov}_{p_{\\psi}\\star}\\,\\phi$ is the Fisher information matrix of the data distribution $p_{\\psi^{\\star}}$ $\\mathbb{Z}(\\psi^{\\star})^{-1}$ being the Cram\u00e9r-Rao lower bound on asymptotic variances of statistical estimators [10], this estimator $\\bar{\\psi}_{n}$ is asymptotically optimal. The following theorem shows conditions under which averaging CD iterates can give rise to a near-optimal estimator. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.3 (Contrastive Divergence with Polyak-Ruppert averaging). Let $(\\psi_{t})_{t\\geq0}$ thesequence of iterates obtained by running the CD algorithm with a learning rate $\\eta_{t}=C t^{-\\beta}$ for $\\begin{array}{r}{\\bar{\\boldsymbol{\\beta}}\\in\\left(\\frac{1}{2},1\\right)}\\end{array}$ Define $\\begin{array}{r}{\\bar{\\psi}_{n}:=\\frac{1}{n}\\sum_{i=1}^{n}\\psi_{i}}\\end{array}$ Then, under the same assumptions as Theorem 3.2, and assuming additionally that m := m(n) > (1=@)log n, 2llog \u03b1f \", we have, for all n \u2265 1, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left(\\mathbb{E}\\left\\|\\overline{{\\psi}}_{n}-\\psi^{\\star}\\right\\|^{2}\\right)^{1/2}\\;\\leq\\;2\\sqrt{\\frac{\\operatorname{tr}(\\mathbb{Z}(\\psi^{\\star})^{-1})}{n}}+o(n^{-1/2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Consequently, we have that $\\begin{array}{r}{\\operatorname*{im}\\operatorname*{sup}_{n\\rightarrow\\infty}n\\mathbb{E}(\\left\\|\\overline{{\\psi}}_{n}-\\psi^{\\star}\\right\\|^{2})\\leq4\\mathrm{tr}(\\mathbb{Z}(\\psi^{\\star})^{-1}).}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.3, alongside with a statement which includes the asymptotic order of the residual term, is proved in Appendix D.4. It shows that at the cost of an increase in computational complexity of the entire algorithm from $O(n)$ to $O(n\\log n)$ $\\bar{\\psi}_{n}$ will be a near-optimal statistical estimator of $\\psi^{\\star}$ While this increase in complexity emerges from the bias of CD, the additional variance of CD results in an asymptotic variance inflated by a factor of 4 compared to the Cramer-Rao bound. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Theorem 3.3 concludes our analysis of online CD. Despite their asymptotic near-optimality, the bounds provided for online CD and its averaged version have weaknesses: the online CD iterates are not robust to choices of C. On the other hand, as shown in Appendix D.4, the bound of the averaged iterates contain higher-order terms that could be large in intermediate sample regimes. Next, we show that offline CD, which processes data points multiple times, can alleviate these issues. ", "page_idx": 6}, {"type": "text", "text": "4  Non-asymptotic analysis of ofline CD ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In practice, CD gradient approximation schemes are commonly used within an offline stochastic gradient descent (SGD) algorithm, where one is given the full size- ${\\mathbf{\\nabla}}n$ dataset upfront and each update uses some stochastic subset of the data. We study CD under offline SGD with replacement (SGDw), i.e. Algorithm 2 with batches $B_{t,j}$ being i.i.d. uniform draws of size- $B$ subsets of $[n]$ and include SGD without replacement in Appendix B.2. To do so, we follow the setting of prior work on offline CD [21], which established its asymptotic $O(n^{-\\frac{1}{3}})$ consistency. We show that by slightly strengthening a moment assumption used in [21], the offline CD iterates converge to the true parameter at a near-parametric $O((\\log n)^{{\\frac{1}{2}}}n^{-{\\frac{1}{2}}})$ rate. Our proof proceeds by controlling a \u201ctail probability\u201d term specific to the ofline setting which characterizes the strength of the correlations between the offine CD iterates and the training data. While, as we show, the assumptions of [21] provide a tail control sufficient to obtain a near-parametric rate, other strategies are possible to obtain convergence guarantees. In particular, we show that non-asymptotic convergence can be obtained by either (1) relaxing assumptions on the Markov kernel required by prior work, or (2) making a specific mixing assumption the Markov chain. ", "page_idx": 6}, {"type": "text", "text": "4.1  Background: Asymptotic consistency of offline CD in subexponential settings ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Prior work [21] has established asymptotic $O(n^{-\\frac{1}{3}})$ consistency of the (averaged) offline CD iterates in the full-batch case. We summarize their results and assumptions below. ", "page_idx": 6}, {"type": "text", "text": "Assumption A4. There exists $\\nu\\geq2$ s.t. for all $m\\in\\mathbb{N}$ , there is $\\kappa_{\\nu;m}<\\infty$ s.t. ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{sup}_{x\\in\\mathcal{X}}\\,\\operatorname*{sup}_{\\psi\\in\\Psi}\\,\\left(\\mathbb{E}\\big\\|\\phi(K_{\\psi}^{m}(x))-\\mathbb{E}[\\phi(K_{\\psi}^{m}(x))]\\big\\|^{\\nu}\\right)^{1/\\nu}\\,\\leq\\,\\kappa_{\\nu;m}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Assumption A5. There exists some $C_{m}>0$ such that, for all $\\psi_{1},\\psi_{2}\\in\\Psi$ $\\begin{array}{r}{\\operatorname*{sup}_{x\\in\\mathcal{X}}\\|\\mathbb{E}[\\phi(K_{\\psi_{1}}^{m}(x))]-}\\end{array}$ $\\mathbb{E}[\\phi(K_{\\psi_{2}}^{m}(x))]\\|\\le C_{m}\\|\\psi_{1}-\\psi_{2}\\|.$ ", "page_idx": 6}, {"type": "text", "text": "Assumption A6.There exist some $\\sigma_{m},\\zeta_{m}\\;>\\;0$ such that, for any $z~\\in~\\mathbb{R}^{p}$ With $\\|z\\|\\ \\leq\\ \\zeta_{m}$ $\\begin{array}{r}{\\mathbb{E}[e^{z^{\\top}(\\phi(K_{\\psi^{*}}^{m}(X_{1}))-\\mathbb{E}[\\phi(K_{\\psi^{*}}^{m}(X_{1}))])}]\\le e^{\\sigma_{m}^{2}\\|z\\|^{2}/2}.}\\end{array}$ ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.1 (Theorem 2.1 of [21]). Assume assumptions A1,A2, A3, $A4$ (for $\\nu=2$ $_{A5}$ and A6. Let $\\psi_{t,1}$ be the $t$ -th iterateof offline $C D$ with full-batch gradient descent and constant stepsize $\\eta_{t}=C$ i.e the iterates produced by Algorithm 2 using $B_{t,1}\\,=\\,[1,n]$ Then for any learning rate $C$ and number of Markov kernel steps m satisfying $\\mu-\\alpha^{m}\\sigma C_{\\chi}-\\textstyle\\frac{C}{2}(L+\\alpha^{m}\\sigma C_{\\chi})^{2}>0,$ we have, for some $A_{m}>0,$ ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\to\\infty}\\mathbb{P}\\left(\\operatorname*{lim}_{T\\to\\infty}\\left\\|\\frac{1}{T}\\sum_{t=1}^{T}\\psi_{t,1}^{\\mathrm{SGDw}}-\\psi^{\\star}\\right\\|>A_{m}n^{-\\frac{1}{3}}\\right)=0\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "This result shows convergence of the averaged full-batch CD iterates to the true parameter in the large $n$ and $T$ limit. As discussed, this result is asymptotic both in $n$ and $T$ : the probability of the errorexceeding $A_{m}n^{-\\frac{1}{3}}$ goes to O as $n\\to\\infty$ and $T\\rightarrow\\infty$ , but at an unknown rate. Moreover, the $O(n^{-\\frac{1}{3}})$ does not match the optimal $O(n^{-\\frac{1}{2}})$ rate. ", "page_idx": 6}, {"type": "text", "text": "4.2  Sharpening offline CD bounds in subexponential settings ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.2.1 Non-asymptotic ${\\tilde{O}}(n^{-1/2})$ -consistency ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "As a first result, we show that under the assumptions of [21] (except for a slightly stronger $\\nu>2$ moment assumption in A4),b.D in fact achieves a near-parametric rate The most general ersion ", "page_idx": 6}, {"type": "text", "text": "of our result holds for any learning rate schedule of the form $C t^{-\\beta}$ \uff0c $\\beta\\in[0,1]$ , and for offine SGD $B$ $(B=n,\\dot{N}=1,\\psi_{t,j}^{\\mathrm{SGDw}}=\\psi_{t,1}^{\\mathrm{SGDw}}$   \n$t\\geq1$ $\\eta_{t}=C$   \nholding for the other mentioned batching and step sizes schedules can be found in Appendix B. ", "page_idx": 7}, {"type": "text", "text": "$\\nu>2$ $\\tilde{\\mu}_{m}\\;=\\;\\mu-\\alpha^{m}\\sigma C_{\\chi}\\;\\stackrel{\\cdot}{>}\\;\\overline{{{4C L^{2}\\cdot L e t\\,\\delta_{t,j}^{\\mathrm{SGDw}}}}}\\vdots=\\mathbb{E}\\|\\psi_{t,j}^{\\mathrm{SGDw}}\\stackrel{\\cdot}{-}\\psi^{*}\\|^{2}.$ ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sqrt{\\delta_{T,1}^{\\mathrm{SGDw}}}\\le E_{1}^{T,1}\\sqrt{\\delta_{0,0}^{\\mathrm{SGDw}}}\\,+\\,C^{\\prime}(p,\\nu,m,\\Psi)\\Big(\\frac{\\sqrt{\\log n}}{\\sqrt{n}}+\\frac{1}{\\sqrt{n}}\\Big)\\Big(\\frac{e^{\\frac{\\tilde{\\mu}_{m}C}{2}}}{\\tilde{\\mu}_{m}C}+\\frac{E_{2}^{T,1}}{L^{2}C^{2}}\\Big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $E_{1}^{T,1},\\,E_{2}^{T,1}$ afexnl $T$ and $C^{\\prime}(p,\\nu,m,\\Psi)$ is a constant in $n,T$ Consequently, ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{T\\to\\infty}\\sqrt{\\delta_{T,1}^{\\mathrm{SGDw}}}\\le\\frac{e^{\\frac{\\tilde{\\mu}_{m}C}{2}}}{\\tilde{\\mu}_{m}C}C^{\\prime}(p,\\nu,m,\\Psi,\\beta)\\Big(\\frac{\\sqrt{\\log n}}{\\sqrt{n}}+\\frac{1}{\\sqrt{n}}\\Big)\\;.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The pecise aluesofalthe constants can be found in Theorem B for $E_{1}^{T,1},E_{2}^{T,1};$ and LemmaB.3 (for $C^{\\prime}(p,\\nu,m,\\Psi,\\beta))$ , including their expressions for $N>1$ and $\\beta\\in[0,1]$ . We comment on the main differences between our result and the one of [21]. First our bound holds for any epoch $T$ and number of samples $n$ . Second, fixing $n$ but taking $T\\rightarrow\\infty$ , the final bound matches the parametric $O({\\sqrt{n}})$ up to a $\\sqrt{\\log(n)}$ factor, a significant improvement over the $O(n^{-\\frac{1}{3}})$ rate of [21]. Finally, we control an $L_{2}$ error, which is a stronger control than a high probability bound by Markov's inequality; we hypothesize this as the reason why a slightly stronger moment assumption is required for our setup, compared to the one used for the high probability bound in [21]. ", "page_idx": 7}, {"type": "text", "text": "Inspecting Equation 9, we notice the presence of two transient terms, and a stationary term, reminiscent of the structure of upper bound of Theorem 3.2. The transient terms (i.e. the ones containing $E_{1}^{T,1}$ ad $E_{2}^{T,1}$ vaishafe $T$ Howeve, unlikein online CD where the number of updates and the number of samples are tied (e.g. values are now decoupled, and these terms can be made arbitrarily small by increasing the number of gradient steps $T$ without having to collect more samples $n$ . The stationary term, which is the only one remaining in the limit of $T\\rightarrow\\infty$ , decreases with $n$ at a rate that is independent of hyperparameters like the step size $C$ or the learning rate schedule $\\beta$ (see Lemma B.3). In that sense, offine CD compares favorably to online CD, whose rate is sensitive to $\\beta$ and $C$ , and averaged online CD, whose bound contains higher-order (in $n$ ) terms which can be large in the moderate $n$ regime. On the other hand, the stationary term in offline CD is asymptotically suboptimal: its rate is larger (while only up to a log factor) than the best-case $O({\\sqrt{n}})$ one achieved by online CD algorithms, and the leading constant does not match the optimal one. ", "page_idx": 7}, {"type": "text", "text": "4.2.2 Proof of Theorem 4.2 ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The high-level proof of Theorem 4.2 follows a similar strategy as the online one: first, derive a $\\delta_{t,1}^{\\mathrm{SGDw}}:=\\mathbb{E}\\|\\psi_{t,1}^{\\mathrm{SGDw}}-\\psi^{*}\\|^{2}$ $\\delta_{T,1}^{\\mathrm{SGDw}}$   \nderiving a controllable, uniform-in-time upper bound of the data-iterate correlations, (2) deriving and   \nunrolling a recursion on $\\delta_{t,1}^{\\mathrm{SGDw}}$ containing this newtem, and 3 controlling the tail probabilitytm   \nto obtaina fina ound on $\\delta_{T,1}^{\\mathrm{SGDw}}$ ", "page_idx": 7}, {"type": "text", "text": "Step 1:characterizing the data-iterate correlations in offline CD In offline CD, at each epoch t \u2265 1, the iterate \\$1,w and the data samples $X_{i}$ are correlated: this is because these samples were used in previous epochs $t^{\\prime}<t-1$ to obtain SGDw and the data samples Xz, which themselves influenced $\\psi_{t-1,1}$ . With such correlations, we now have $\\mathbb{P}(X_{i}|\\psi_{t-1,1})\\neq\\mathbb{P}(X_{i})$ preventing us from obtaining an unrollable recursion on ,GI $\\delta_{t,1}^{\\mathrm{SGDw}}$ by frst marginalizing $X_{i}$ out to obtain an upper boud $\\mathbb{E}\\left[\\|\\psi_{t,1}^{\\mathrm{SGDw}}-\\psi^{*}\\|^{2}|\\psi_{t-1,1}^{\\mathrm{SGDw}}\\right]$ that onlydepends on $\\lVert\\psi_{t-1,1}^{\\mathrm{SGDw}}-\\psi^{\\star}\\rVert$ andthenmarginaizing over $\\psi_{t-1,1}^{\\mathrm{SGDw}}$ tobansthismntad \"fresh samples\" (e.g. i.i.d copies of $X_{i}$ not present in the training data) to perform our update, the core of the proof lies in controlling the following quantity: ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Delta(\\psi_{t,1}^{\\mathrm{SGDw}}):=\\biggr|\\biggr|\\frac{1}{n}\\sum_{i\\leq n}\\left(\\mathbb{E}\\big[\\phi\\big(K_{i;\\psi_{t,1}^{\\mathrm{SGDw}}}^{m}(X_{i})\\big)\\big|\\psi_{t,1}^{\\mathrm{SGDw}},X_{i}\\big]-\\mathbb{E}\\big[\\phi\\big(K_{i;\\psi_{t,1}^{\\mathrm{SGDw}}}^{m}(X_{1}^{\\prime})\\big)\\big|\\ \\psi_{t,1}^{\\mathrm{SGDw}}\\big]\\right)\\biggr|}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Where $X_{1}^{\\prime}$ isan id.copyof $X_{1}$ $\\Delta(\\psi_{t,1}^{\\mathrm{SGDw}})$ isthe expected (overthedata and iterates error betwen a quantity that allows to obtain a recursion (the rightmost term) and the one actually used by offine CD (the leftmost term). To control it, we upper-bound it using a tail decomposition: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\Delta(\\psi_{t,1}^{\\mathrm{SGDw}})^{2}]\\leq\\epsilon^{2}+(\\operatorname*{sup}_{t}\\mathbb{E}[\\Delta(\\psi_{t,1}^{\\mathrm{SGDw}})^{\\nu}])^{2/\\nu}\\operatorname*{sup}_{t}\\mathbb{P}(\\Delta(\\psi_{t,1}^{\\mathrm{SGDw}})>\\epsilon)^{\\frac{\\nu-2}{\\nu}}:=\\varepsilon_{n,m,T;\\nu}^{\\mathrm{SGDw}}(\\epsilon)\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "We invoke an additional assumption to ensurethat $(\\mathbb{E}[\\Delta(\\psi_{t,1}^{\\mathrm{SGDw}})^{\\nu}])^{2/\\nu}$ is finite in theresults of [21], this is automatically implied by assumptions A4 and A6. For simplicity we assume the same bounding constant Kv:m\u00b7 ", "page_idx": 8}, {"type": "text", "text": "Assumption A7. There exists $\\nu\\geq2$ s.t. for all $m\\in\\mathbb{N}$ \uff0c $\\kappa_{\\nu;m}$ from A4 moreover verifies that ", "page_idx": 8}, {"type": "equation", "text": "$\\begin{array}{r}{\\operatorname*{sup}_{\\psi\\in\\Psi}\\,\\big(\\mathbb{E}\\big\\|\\phi(K_{\\psi}^{m}(X_{1}))-\\mathbb{E}[\\phi(K_{\\psi}^{m}(X_{1}))]\\big\\|^{\\nu}\\big)^{1/\\nu}\\,\\le\\,\\kappa_{\\nu;m}\\;.}\\end{array}$ ", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Note the similarity of this assumption with assumption A4: the only difference is that $X_{1}$ isnow random training point instead of an deterministic (arbitrary) one. Ensuring assumption A7 in addition to assumption A4 thus requires controlling a $\\nu$ -th order moment, instead of all moments as implied byassumptionA6. ", "page_idx": 8}, {"type": "text", "text": "Step 2: Deriving and unrolling the recursion on $\\delta_{t,1}^{\\mathrm{SGDw}}$ The right-hand side of Equation (10) does not depend on $t$ allowing for the derivation of anunrollablereursion on sSGDw and its subsequent unrolling, which is performed in the following theorem. For simplicity, we again assume $\\beta=0$ and $N=1$ and defer the general case to Theorem B.1 in appendix. ", "page_idx": 8}, {"type": "text", "text": "Theorem 4.3 (Convergence up to a tail control). Assume Al, A2, $A3$ $A4$ and $A7$ Let $\\eta_{t}=C$ for some $C>0$ andassumethat $\\tilde{\\mu}_{m}\\ =\\ \\mu-\\alpha^{m}\\sigma C_{\\chi}\\ >\\ 4C L^{2}$ . Then for any $\\epsilon>0$ ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\sqrt{\\delta_{T,1}^{\\mathrm{SGDw}}}\\le E_{1}^{T,1}\\sqrt{\\delta_{0,0}^{\\mathrm{SGDw}}}\\,+\\,C\\left(\\varepsilon_{n,m,T;\\nu}^{\\mathrm{SGDw}}(\\epsilon)+\\frac{5\\sigma+5\\kappa\\nu,m}{\\sqrt{n}}\\right)\\left(\\frac{e^{\\frac{\\bar{\\mu}_{m}C}{2}}}{\\tilde{\\mu}_{m}C}+\\frac{E_{2}^{T,1}}{L^{2}C^{2}}\\right)\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where SG;\u03bc (e) is defined in Equation (10) ", "page_idx": 8}, {"type": "text", "text": "Note that in the general, non-full batch $B~\\leq~n$ case, 50+5ky,m is replaced by 5g+5kv,m(see Theorem B.1). Under our bounds, obtaining consistency thus requires setting $B\\equiv B(n)\\underset{n\\rightarrow\\infty}{\\rightarrow}+\\infty$ ", "page_idx": 8}, {"type": "text", "text": "$\\varepsilon_{n,m,T;\\nu}^{\\mathrm{SGDw}}(\\bar{\\epsilon})$ minimizing $\\varepsilon_{n,m,T;\\nu}^{\\mathrm{SGDw}}(\\epsilon)$ over $\\epsilon$ yields the following result: ", "page_idx": 8}, {"type": "text", "text": "Lemma4.Assethe sepofThorem.2LnNbe sufntylage. Denote $r_{\\Psi}$ as the radius of the smallest sphere in $\\mathbb{R}^{p}$ thatcontains $\\Psi$ which is finite under A1. Then $\\operatorname*{inf}_{\\epsilon>0}\\varepsilon_{\\nu;n,m,T}^{\\mathrm{SGDw}}(\\epsilon)\\leq$ ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left(\\frac{3{\\sigma_{m}}\\sqrt{p((\\nu-2)p+2\\nu)}}{\\sqrt{\\nu-2}}+\\kappa_{\\nu;m}2^{\\frac{\\nu-2}{2\\nu}}(r_{\\Psi})^{\\frac{(\\nu-2)p}{2\\nu}}\\left(1+\\frac{2{C_{m}}(\\nu-2)^{1/2}}{{\\sigma_{m}}p^{1/2}((\\nu-2)p+2\\nu)^{1/2}}\\right)^{\\frac{(\\nu-2)p}{2\\nu}}\\right)\\frac{\\sqrt{\\log n}}{\\sqrt{n}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "To obtain this result, we control the moment term $(\\operatorname*{sup}_{t}\\mathbb{E}[\\Delta(\\psi_{t,1}^{\\mathrm{SGDw}})^{\\nu}])$ using A7,and we control the til probabity term $\\operatorname*{sup}_{t}\\mathbb{P}(\\Delta(\\psi_{t,1}^{\\mathrm{SGDw}})>\\epsilon)$ as in 21, Lemma 3.1] using an union bound, a covering argument and A6. Theorem 4.2 then follows by plugging Lemma 4.4 into Theorem 4.3. ", "page_idx": 8}, {"type": "text", "text": "4.3  Consistency of offine CD: beyond subexponential tails. ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "As discussed above, the general unrolling result of Theorem 4.3 holds without the subexponentiality assumptionso assumpion A6;tis assumptionwas onlyusedin Lemma 4.4 to control $\\hat{\\varepsilon}_{n,m,T;\\nu}^{\\mathrm{SGDw}}(\\epsilon)$ We now discuss two alternative ways to control this quantity without requiring subexponential tails. The first generalizes the idea of Jiang et al. [21], while the second exploits mixing of the Markov chain $K_{\\psi}^{\\bar{m}}(x)$ as $m\\rightarrow\\infty$ . As before we only state partial results (fuli batch, $\\beta=0$ ) and defer the full explicit bounds to Appendix B.3. ", "page_idx": 8}, {"type": "text", "text": "Control via Markov Inequality Our first alternative uses Markov Inequality to yield the following. Theorem 4.5. Assume the setup of Theorem 4.3 and additionally that A5 holds. Then ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{inf}_{\\epsilon>0}\\varepsilon_{\\nu;n,m,{\\cal T}}^{\\mathrm{SGDw}}(\\epsilon)\\ \\leq\\tilde{C}(p,\\nu,m,\\Psi)\\,n^{-\\frac{(\\nu-2)\\nu}{2(\\nu^{2}+(\\nu-2)p)}}\\ ,}\\\\ &{\\operatorname*{lim}_{T\\to\\infty}\\sqrt{\\delta_{T,1}^{\\mathrm{SGDw}}}\\ \\leq\\tilde{C}^{\\prime}(p,\\nu,m,\\Psi)\\big(n^{-\\frac{(\\nu-2)\\nu}{2(\\nu^{2}+(\\nu-2)p)}}+\\frac{1}{\\sqrt{n}}\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "where $\\tilde{C}$ and ${\\tilde{C}}^{\\prime}$ are functions whose explicit expressions are given in Lemma $B.4$ in the appendix. ", "page_idx": 9}, {"type": "text", "text": "In the case $p=1$ and $\\nu=3$ , the sub-optimal error from Theorem 4.5 reads $O(n^{-3/20})$ .Theorems 4.2 and 4.5 reveal that, depending on the tail condition imposed on the noise introduced by the Markov kernel, the convergence rate of offline CD varies: A subexponential tail, as assumed in prior work, in fact leads to near-parametric rate. Meanwhile, consistency can be obtained without assuming subexponentiality, albeit at a sub-optimal rate. ", "page_idx": 9}, {"type": "text", "text": "Control via Markov chain mixing. Alternatively, notice that $\\mathbb{E}[\\Delta(\\psi_{t,1}^{\\mathrm{SGDw}})^{2}]$ involves an average f $\\mathbb{E}\\big[\\phi\\big(K_{\\psi_{t,1}^{\\mathrm{SGDw}}}^{m}(X_{i})^{2}\\big)\\big|X_{i},\\psi_{t,1}^{\\mathrm{SGDw}}\\big]-\\mathbb{E}\\big[\\phi\\big(K_{\\psi_{t,1}^{\\mathrm{SGDw}}}^{m}(X_{1}^{\\prime})\\big)\\big|\\psi_{t,1}^{\\mathrm{SGDw}}\\big]$ When $m\\rightarrow\\infty$ the effet of initialization vanishes, and one may expect the difference to converge to zero. We defer to Lemma B.5 in the appendix to show that, under a $\\phi$ -discrepancy mixing condition ([35]) with a mixing coefficient $\\tilde{\\alpha}\\in[0,\\bar{1})$ \uff0c ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\epsilon>0}\\varepsilon_{\\nu;n,m,T}^{\\mathrm{SGDw}}(\\epsilon)\\,=\\,{\\cal O}\\bigl(\\kappa_{\\nu;m}\\tilde{\\alpha}^{\\frac{(\\nu-2)m}{3\\nu-2}}\\bigr)\\quad\\mathrm{and}\\quad\\operatorname*{lim}_{T\\to\\infty}\\sqrt{\\delta_{T,1}^{\\mathrm{SGDw}}}\\,=\\,{\\cal O}\\Bigl(\\kappa_{\\nu;m}\\tilde{\\alpha}^{\\frac{(\\nu-2)m}{3\\nu-2}}+\\frac{\\sigma+\\kappa_{\\nu;m}}{\\sqrt{n}}\\Bigr)\\ .\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "As $m\\rightarrow\\infty$ , this recovers the parametric rate $O(n^{-1/2})$ . This alternative convergence guarantee comes at the cost of requiring $m$ , the number of Markov chain steps, to grow with the sample size $n$ ", "page_idx": 9}, {"type": "text", "text": "Remark (Examples). In our main results (Theorems 3.2, 3.3 and 4.3) and the tail condition for offline SGD (Theorem 4.2), we employed a weaker set of assumptions than those in [21] (except for the mild $\\nu>2$ moment assumption in (A4)). Consequently, our results apply to all three examples studied in [21]: A bivariate Gaussian model with unknown mean and random-scan Gibbs sampler, a fully visible Boltzmann machine with random-scan Gibbs sampler, and an exponential-family random graph model with a Metropolis-Hastings sampler. ", "page_idx": 9}, {"type": "text", "text": "5 Related Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Central to this paper is the prior work of Jiang et al. [21], who provided a rigorous theoretical foundation to analyze the convergence of full-batch CD, and which we refine. The study of optimization with biased gradient descent has attracted a lot of attention in recent years [36, 37, 38, 39]. These works, while closely connected to ours, analyze algorithms with different implementation choices than the CD algorithm: i.i.d. noise setup [36], or setup where a persistent Markov chain is maintained through the iterations [36, 37, 38, 39]. The latter is akin to a variant of the CD algorithm, called the persistent CD [40]. In contrast, our analysis focus on the CD algorithm that restarts a batch of Markov chains from the data distribution at every iteration. Finally, there is a rich body of work on convergence guarantees for offline multi-pass SGD [41, 42, 43, 44, 45, 46]. A notable difference of our analysis is that we are primarily concerned with statistical errors associated with convergence to the true parameter $\\psi^{*}$ in number of samples $n$ , and not the commonly studied convergence rate in number of epochs $T$ . Consequently, most of our work for the offline setup goes into handling the correlations that accumulate by reusing data across epochs. ", "page_idx": 9}, {"type": "text", "text": "6 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we provide a non-asymptotic analysis of the Contrastive Divergence algorithms, showing, in the online setting, their potential to converge at the parametric rate and to have near-optimal asymptotic variance, and proving a near-parametric rates in the offline setting, significantly extending prior results. Our results apply to unnormalized exponential families: despite their flexibility, these models only cover log-densities with linear relationships on the model parameters. We believe that extending our results to more general forms of unnormalized models is an important direction for futurework. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "All authors acknowledge support from the Gatsby Charitable Foundation. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "[1]  Kyle Cranmer, Johann Brehmer, and Gilles Louppe. The frontier of simulation-based inference. Proceedings of the National Academy of Sciences, 117(48):30055-30062, 2020.   \n[2]  Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748-8763. PMLR, 2021.   \n[3]  Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Iya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.   \n[4]  Aad W Van der Vaart. Asymptotic statistics, volume 3. Cambridge university press, 2000.   \n[5] Holden Lee, Chirag Pabbaraju, Anish Prasad Sevekari, and Andrej Risteski. Pitfalls of gaussians as a noise distribution in nce. In The Eleventh International Conference on Learning Representations, 2023.   \n[6]  Michael Gutmann and Aapo Hyvarinen. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, 2010.   \n[7]  Aapo Hyvarinen and Peter Dayan. Estimation of non-normalized statistical models by score matching. Journal of Machine Learning Research, 2005.   \n[8] Frederic Koehler, Alexander Heckett, and Andrej Risteski. Statistical efficiency of score matching: The view from isoperimetry. In The Eleventh International Conference on Learning Representations, 2022.   \n[9]  Chirag Pabbaraju, Dhruv Rohatgi, Anish Prasad Sevekari, Holden Lee, Ankur Moitra, and Andrej Risteski. Provable benefits of score matching. Advances in Neural Information Processing Systems, 36, 2024.   \n[10] George Casella and Roger L Berger. Statistical inference. Cengage Learning, 2021.   \n[11]  Nicolas Le Roux and Yoshua Bengio. Representational power of restricted boltzmann machines and deep belief networks. Neural computation, 20(6):1631-1649, 2008.   \n[12]  Yilun Du and Igor Mordatch. Implicit generation and modeling with energy based models. Advances in Neural Information Processing Systems, 32, 2019.   \n[13]  Geoffrey E Hinton. Training products of experts by minimizing contrastive divergence. Neural computation, 2002.   \n[14] Erik Nijkamp, Mitch Hill, Tian Han, Song-Chun Zhu, and Ying Nian Wu. On the anatomy of mcmc-based maximum likelihood learning of energy-based models. In Proceedings of the AAAI Conference on Artijficial Intelligence, 2020.   \n[15] Erik Nijkamp, Mitch Hil, Song-Chun Zhu, and Ying Nian Wu. Learning non-convergent non-persistent short-run mcmc toward energy-based model. Advances in Neural Information Processing Systems, 2019.   \n[16]  Yilun Du, Shuang Li, Joshua Tenenbaum, and Igor Mordatch. Improved contrastive divergence training of energy- based models. arXiv preprint arXiv:2021.01316, 2021.   \n[17]  Lianhui Qin, Sean Welleck, Daniel Khashabi, and Yejin Choi. Cold decoding: Energy-based constrained text generationwith langevin dynamics. Advances in Neural Information Processing Systems, 35:9538-9551, 2022.   \n[18] Deqian Kong, Bo Pang, Tian Han, and Ying Nian Wu. Molecule design by latent space energybased modelingand gradual distribution shifting. In Uncertainty in Atifcial Inteligence, pages 1109-1120. PMLR, 2023.   \n[19]  Csilla Varnai, Nikolas S Burkoff, and David L Wild. Efficient parameter estimation of generalizable coarse-grained protein force fields using contrastive divergence: a maximum likelihood approach. Journal of chemical theory and computation, 9(12):5718-5733, 2013.   \n[20] Pierre Glaser, Michael Arbel, Arnaud Doucet, and Arthur Gretton. Maximum likelihood learning of energy-based models for simulation-based inference. 2022.   \n[21] Bai Jiang, Tung- Yu Wu, Yifan Jin, and Wing H Wong. Convergence of contrastive divergence algorithm in exponential family. The Annals of Statistics, 46(6A):3067-3098, 2018.   \n[22] Eric Moulines and Francis Bach. Non-asymptotic analysis of stochastic approximation algorithms for machine learning. Advances in neural information processing systems, 24, 2011.   \n[23]  Boris T Polyak and Anatoli B Juditsky. Acceleration of stochastic approximation by averaging. SIAM journal on control and optimization, 30(4):838-855, 1992.   \n[24] Lawrence D Brown. Fundamentals of statistical exponential families: with applications in statistical decision theory. Ims, 1986.   \n[25] Martin J Wainwright, Michael I Jordan, et al. Graphical models, exponential families, and variational inference. Foundations and Trends? in Machine Learning, 1(1-2):1-305, 2008.   \n[26]  Charles J Geyer. Introduction to markov chain monte carlo. Handbook of markov chain monte carlo, 20116022:45, 2011.   \n[27] Yann LeCun, Sumit Chopra, Raia Hadsell, M Ranzato, and Fujie Huang. A tutorial on energybased learning. Predicting structured data, 1(0), 2006.   \n[28]  Yang Song and Diederik P Kingma. How to train your energy-based models. arXiv preprint arXiv:2101.03288, 2021.   \n[29]  Joseph L Doob. Measure theory, volume 143. Springer Science & Business Media, 2012.   \n[30] David A Levin and Yuval Peres. Markov chains and mixing times, volume 107. American Mathematical Soc., 2017.   \n[31] Yuri Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer Science & Business Media, 2013.   \n[32] Gary Harris and Clyde Martin. Shorter notes: The roots of a polynomial vary continuously asa functionof the coefcients.Proceedingsof theAmericanMathematical Society,pages 390-392, 1987.   \n[33]  Frank Nielsen and Richard Nock. On the chi square and higher-order chi distances for approximating f-divergences. IEEE Signal Processing Letters, 21(1):10-13, 2013.   \n[34] Dominique Bakry, Ivan Gentil,Michel Ledoux, et al. Analysis and geometry of Markov difusion operators, volume 103. Springer, 2014.   \n[35] Maxim Rabinovich, Aaditya Ramdas, Michael I Jordan, and Martin J Wainwright. Functionspecific mixing times and concentration away from equilibrium. 2020.   \n[36] Belhal Karimi, Blazej Miasojedow, Eric Moulines, and Hoi-To Wai. Non-asymptotic analysis of biased stochastic approximation scheme. In Conference on Learning Theory, pages 1944-1974. PMLR, 2019.   \n[37]  Tao Sun, Yuejiao Sun, and Wotao Yin. On markov chain gradient descent. Advances in neural information processing systems, 31, 2018.   \n[38]  Thinh T Doan. Finite-time analysis of markov gradient descent. IEEE Transactions on Automatic Control, 68(4):2140-2153, 2022.   \n[39] Yves F Atchade, Gersende Fort, and Eric Moulines. On perturbed proximal gradient algorithms. Journal of Machine Learning Research, 18(10):1-33, 2017.   \n[40]  Tijmen Tieleman. Training restricted boltzmann machines using approximations to the likelihood gradient. In Proceedings of the 25th international conference on Machine learning, 2008.   \n[41]  Andre Elisseeff, Theodoros Evgeniou, Massimiliano Pontil, and Leslie Pack Kaelbing. Stability of randomized learning algorithms. Journal of Machine Learning Research, 6(1), 2005.   \n[42] Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic gradient descent. In International conference on machine learning, pages 1225-1234. PMLR, 2016.   \n[43]  Junhong Lin and Lorenzo Rosasco. Optimal rates for multi-pass stochastic gradient methods. Journal of Machine Learning Research, 18(97):1-47, 2017.   \n[44] Loucas Pillaud-Vivien, Alessandro Rudi, and Francis Bach. Statistical optimality of stochastic gradient descent on hard learning problems through multiple passes. Advances in Neural Information Processing Systems, 31,2018.   \n[45] Nicole Muicke, Gergely Neu, and Lorenzo Rosasco. Beating sgd saturation with tail-averaging and minibatching. Advances in Neural Information Processing Systems, 32, 2019.   \n[46] Difan Zou, Jingfeng Wu, Vladimir Braverman, Quanquan Gu, and Sham Kakade. Risk bounds of multi-pass sgd for least squares in the interpolation regime. Advances in Neural Information Processing Systems, 35:12909-12920, 2022.   \n[47]  Dheeraj Nagaraj, Prateek Jain, and Praneeth Netrapalli. Sgd without replacement: Sharper rates for general smooth convex functions. In International Conference on Machine Learning, pages 4703-4711. PMLR, 2019.   \n[48]  Shashank Rajput, Anant Gupta, and Dimitris Papailiopoulos. Closing the convergence gap of sgd without replacement. In International Conference on Machine Learning, pages 7964-7973. PMLR, 2020.   \n[49] Dimitri Bertsekas. Convex optimization theory, volume 1. Athena Scientific, 2009.   \n[50] Martin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cambridge university press, 2019.   \n[51]  S. W. Dharmadhikari, V. Fabian, and K. Jogdeo. Bounds on the moments of martingales. Ann. Math. Statist., pages 1719-1723, 1968. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Supplementary Material for 'Near-Optimality of Contrastive Divergence Algorithms' ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The supplementary material provides the proofs of the main results of the paper:   \nSection B states full explicit bounds for the offline CD algorithm.   \nSection C collects a list of useful tools for our proofs. These include the properties of $\\varphi_{\\gamma}$ introduced before Theorem 3.2 in the main text, as well as several contraction and integrability results. Section D provides the proofs for the online CD algorithm.   \nSections E, F and G contain the proofs about the offline CD algorithm and the tail control. ", "page_idx": 14}, {"type": "text", "text": "A Notations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Throughout the proofs, we will denote by $P_{\\psi}^{m}$ the following operator from $L_{p_{\\psi}}^{2}(\\mathbb{R})$ to itself: ", "page_idx": 14}, {"type": "equation", "text": "$$\nP_{\\psi}^{m}f(x):=\\int k^{m}(x,x^{\\prime})f(x^{\\prime})p_{\\psi}(x^{\\prime})\\mathrm{d}x^{\\prime}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $k^{m}(x,x^{\\prime})$ is the $m$ -iterated kernel of the Markov chain with transition kernel $k_{\\psi}$ , given by: ", "page_idx": 14}, {"type": "equation", "text": "$$\nk_{\\psi}^{m}(x,y):=\\int k_{\\psi}(x_{0},x_{1})\\,.\\,.\\,k_{\\psi}(x_{m-2},x_{m-1})\\,.\\,.\\,.\\,k_{\\psi}(x_{m-1},y)\\mathrm{d}x_{0}\\,.\\,.\\,.\\,\\mathrm{d}x_{m-1}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "${\\mathrm{Proj}}_{\\Psi}:\\mathbb{R}^{p}\\longmapsto\\Psi$ denotes the projection operator onto the convex set $\\Psi$ e.g. ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{Proj}_{\\Psi}(\\psi):=\\operatornamewithlimits{a r g m i n}_{\\psi^{\\prime}\\in\\Psi}\\|\\psi-\\psi^{\\prime}\\|\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We also frequently use the following function, used in standard convex optimization results [22]. ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\varphi_{\\gamma}(t)={\\left\\{\\!\\!\\!{\\frac{t^{\\gamma}-1}{\\gamma}}\\!\\!\\right.}}{\\left.\\begin{array}{l}{{\\mathrm{if}\\ \\gamma\\neq0}}\\\\ {{\\mathrm{if}\\ \\gamma=0}}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which is defined on $\\mathbb{R}_{+}\\setminus\\{0\\}$ ", "page_idx": 14}, {"type": "text", "text": "B  Additional results for offline SGD ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we provide the full statements on error bounds for SGD with replacement (SGDw), SGD with reshuffling (SGDo) and tail moment bounds, which complement the results in Section 4. Proofs are deferred to Appendix F, which make use of $L_{2}$ approximation by auxiliary gradient updates derived in Appendix E. ", "page_idx": 14}, {"type": "text", "text": "Tail probability term 9SGDw. $(\\psi_{t,j}^{\\mathrm{SGDw}})_{t\\in\\mathbb{N},j\\leq N}$ $X_{1}^{\\prime}$ i.i.d. copy of $X_{1}$ . Throughout the remaining of the appendix, we define given $\\epsilon>0$ and $n\\in\\mathbb{N}$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{\\Gamma}_{n,m,T}^{\\mathrm{SGDw}}(\\epsilon)\\!:=\\!\\operatorname*{sup}_{t\\in[T]}\\mathbb{P}\\left(\\frac{\\left\\|\\sum_{i=1}^{n}\\big(\\mathbb{E}\\left[\\phi\\left(K_{\\psi_{t-1,j}^{\\mathrm{SGDw}}}^{m}(X_{i})\\right)\\Big|X_{i},\\psi_{t-1,j}^{\\mathrm{SGDw}}\\right]-\\mathbb{E}\\left[\\phi\\left(K_{\\psi_{t-1,j}^{\\mathrm{SGDw}}}^{m}(X_{1}^{\\prime})\\right)\\Big|\\psi_{t-1,j}^{\\mathrm{SGDw}}\\right]\\right)\\right\\|}{t\\in[T]}>\\epsilon\\right)\\!.}\\\\ &{\\qquad\\qquad\\quad\\;\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and $\\vartheta_{n,m,T}^{\\mathrm{SGDo}}(\\epsilon)$ analogousling the n wecae thqat $\\varepsilon_{n,m,T;\\nu}^{\\mathrm{SGDw}}(\\epsilon)$ in the mainas $\\begin{array}{r}{\\varepsilon_{n,m,T;\\nu}^{\\mathrm{SGDw}}(\\epsilon):=\\sqrt{\\epsilon^{2}+\\kappa_{\\nu;m}^{2}\\big(\\vartheta_{n,m,T}^{\\mathrm{SGDw}}(\\epsilon)\\big)^{\\frac{\\nu-2}{\\nu}}}}\\end{array}$ coside asumnpton. ", "page_idx": 14}, {"type": "text", "text": "B.1  An explicit finite-sample bound for SGDw ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In th result below we write $\\delta_{t,j}^{\\mathrm{SGDw}}:=\\mathbb{E}\\left\\|\\psi_{t,j}^{\\mathrm{SGDw}}-\\psi^{*}\\right\\|^{2}$ and fo a fied $\\epsilon>0$ the quatity ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sigma_{n,T}^{\\mathrm{SGDw}}\\;=\\varepsilon_{n,m,T;\\nu}^{\\mathrm{SGDw}}(\\epsilon)+\\frac{5\\sigma+5\\kappa_{m}}{\\sqrt{B}}\\;=\\;\\sqrt{\\epsilon^{2}+\\kappa_{m}^{2}\\big(\\vartheta_{n,m,T}^{\\mathrm{SGDw}}(\\epsilon)\\big)^{\\frac{\\nu-2}{\\nu}}}+\\frac{5\\sigma+5\\kappa_{m}}{\\sqrt{B}}\\;.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Theorem B.1. Assume $A l$ (where $\\Psi$ may be non-compact), A2, A3, A4 and $A7.$ Let $\\eta_{t}=C t^{-\\beta}$ for some $\\beta\\in[0,1]$ and $C>0$ andassumethat $\\begin{array}{r}{m>\\frac{\\log(\\sigma C_{\\chi}/\\mu)}{\\log|\\alpha|}\\;s.t.\\;\\tilde{\\mu}_{m}=\\mu-\\alpha^{m}\\sigma C_{\\chi}>0}\\end{array}$ as in Theorem 3.2. Then for any $\\epsilon>0$ $\\sqrt{\\delta_{T,N}^{\\mathrm{SGDw}}}$ is upper bounded by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{E_{1}^{T,N}\\sqrt{\\delta_{0,0}^{\\mathrm{SGDw}}}\\,+\\,C\\sigma_{n,T}^{\\mathrm{SGDw}}\\bigg(\\frac{4e^{\\frac{-\\beta_{m}C N}{(T+1)^{1/2}}}}{\\tilde{\\mu}_{m}C}+2N(1+\\tilde{\\mu}_{m}C)^{N-1}\\varphi_{\\frac{1}{2}-L^{2}C^{2}N}(T+1)\\,E_{2}^{T,N}\\bigg)}\\\\ {\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\,\\,f o r\\,\\beta=\\frac{1}{2}\\,,}\\\\ {E_{1}^{T,N}\\sqrt{\\delta_{0,0}^{\\mathrm{SGDw}}}\\,+\\,C\\sigma_{n,T}^{\\mathrm{SGDw}}\\bigg(\\frac{4}{\\tilde{\\mu}_{m}C}+\\frac{3N(1+\\frac{L^{2}C^{2}}{2})^{N-1}\\,e^{2L^{2}C^{2}N}\\,\\log(T+1)}{(T+1)^{(\\tilde{\\mu}_{m}C N)/2}}\\bigg)}&{f o r\\,\\beta=1\\,,}\\\\ {E_{1}^{T,N}\\sqrt{\\delta_{0,0}^{\\mathrm{SGDw}}}\\,+\\,C\\sigma_{n,T}^{\\mathrm{SGDw}}\\bigg(\\frac{2^{2\\beta+1}}{\\tilde{\\mu}_{m}C}e^{\\frac{\\tilde{\\mu}_{m}C}{2(1-\\beta)}}\\frac{N}{(T+1)^{\\beta}}\\,+\\,\\frac{3^{\\beta}(1+\\tilde{\\mu}_{m}C)^{N-1}(T+2)^{\\beta}}{L^{2}C^{2}}\\,E_{2}^{T,N}\\bigg)\\,\\,o t h e r w i s e\\,,}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where ET,N and $E_{2}^{T,N}$ are two decreasing fnctions in $T$ defined by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{E_{1}^{T,N}\\,:=\\,\\exp\\Big(1-N\\tilde{\\mu}_{m}C\\varphi_{1-\\beta}(T+1)+\\frac{N L^{2}C^{2}}{2}\\varphi_{1-2\\beta}(T+1)\\Big)\\ ,}\\\\ &{E_{2}^{T,N}\\,:=\\,\\exp\\Big(-\\frac{N\\tilde{\\mu}_{m}C}{2}\\varphi_{1-\\beta}(T+1)+2N L^{2}C^{2}\\varphi_{1-2\\beta}(T+1)\\Big)\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We emphasize that the full result above holds for any $\\beta\\in[0,1]$ ,which in particular includesthe constant step size $\\beta=0$ regineconsideredby I2l.Whenl $\\beta=0$ for $E_{1}^{T,N}$ and ET,N to decay to zero as $T\\rightarrow\\infty$ ,we additionallyneedthecondition ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\tilde{\\mu}_{m}\\;=\\;\\mu-\\alpha^{m}\\sigma C_{\\chi}\\;>\\;-4C L^{2}\\;.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This is almost identical to the condition they use ((2.5), Theorem 2.1, [21]), except that they replace $4L^{2}$ by $(L+\\alpha^{m}\\sigma C_{\\chi})^{2}$ . Notably this says that an additional step size condition is needed for our results to hold in the constant step size regime, but not necessary for a decreasing step size. ", "page_idx": 15}, {"type": "text", "text": "B.2  Results for SGDo ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "SGD with reshuffling (SGDo, also called SGD without replacement) is an optimization scheme that is also widely used in practice compared to SGDo and online SGD. In the context of CD, it corresponds to Algorithm 2 with batches chosen as ", "page_idx": 15}, {"type": "equation", "text": "$$\n(B_{t,1},\\ldots,B_{t,N})\\ =\\pi(\\{1,\\ldots,n\\})\\ ,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\pi$ is a uniform draw of the permutation group on $n$ elements. We denote the iterates of SGDo $(\\psi_{t,j}^{S_{o}})_{t\\in\\mathbb{N},j\\in[N]}$ Analogously to GD,, we define, for X/ an ii.d. copy of X1, e > O and n E N, the tail probability term ", "page_idx": 15}, {"type": "text", "text": "$\\begin{array}{r}{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! $ Also denote $\\varepsilon_{n,m,T;\\nu}^{\\mathrm{SGDo}}(\\epsilon)\\;=\\;\\sqrt{\\epsilon^{2}+\\kappa_{m}^{2}\\left(\\vartheta_{n,m,T}^{\\mathrm{SGDo}}(\\epsilon)\\right)^{\\frac{\\nu-2}{\\nu}}}$ and $\\sigma_{n,T}^{\\mathrm{SGDw}}\\,=\\,\\varepsilon_{n,m,T;\\nu}^{\\mathrm{SGDo}}(\\epsilon)\\,+\\,\\frac{5\\sigma+5\\kappa_{m}}{\\sqrt{B}}.$ The folowing resultsaysthat $\\psi_{t,j}^{\\mathrm{SGDo}}$ enjoys exactlythe same convergenceguarateas ,SGDo in Theorem 4.3. The statement is identical to that of Theorem B.1 and is stated in full for completeness; see Appendix F.2 for the proof, which is a slight adaptation of the proof for Theorem B.1. As before Iwe write $\\delta_{t,j}^{\\mathrm{SGDo}}:=\\mathbb{E}\\left\\|\\psi_{t,j}^{\\mathrm{SGDo}}-\\psi^{*}\\right\\|^{2}$ ", "page_idx": 15}, {"type": "text", "text": "Theorem B.2 (Convergence of CD-SGDo). Assume $_{A I}$ (where $\\Psi$ may be non-compact), $A2$ A3, A4 and $A7.$ Let $\\eta_{t}\\,=\\,C t^{-\\beta}$ for some $\\beta\\,\\in\\,[0,1]$ and $C\\,>\\,0$ .and assume that $\\begin{array}{r}{m\\,>\\,\\frac{\\log\\left(\\sigma C_{\\chi}/\\mu\\right)}{\\log\\left|\\alpha\\right|}}\\end{array}$ s.t. $\\tilde{\\mu}_{m}=\\mu-\\alpha^{m}\\sigma C_{\\chi}>0$ as in Theorem 3.2. Then for any $\\epsilon>0$ $\\sqrt{\\delta_{T,N}^{\\mathrm{SGDo}}}$ is upper bounded by ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(E_{1}^{T,N}\\sqrt{\\delta_{0,0}^{\\mathrm{SGDo}}}\\,+\\,C\\sigma_{n,T}^{\\mathrm{SGDo}}\\Big(\\frac{4e^{\\frac{\\beta_{m}C N}{(T+1)^{1/2}}}}{\\tilde{\\mu}_{m}C}+2N(1+\\tilde{\\mu}_{m}C)^{N-1}\\varphi_{\\frac{1}{2}-L^{2}C^{2}N}(T+1)\\,E_{2}^{T,N}\\Big)\\right.}\\\\ &{\\quad\\left.\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\,f o r\\,\\beta=\\frac{1}{2}\\,,}\\\\ &{\\left.E_{1}^{T,N}\\sqrt{\\delta_{0,0}^{\\mathrm{SGDo}}}+C\\sigma_{n,T}^{\\mathrm{SGDo}}\\Big(\\frac{4}{\\tilde{\\mu}_{m}C}+\\frac{3N\\big(1+\\frac{L^{2}C^{2}}{2}\\big)^{N-1}\\,e^{2L^{2}C^{2}N}\\,\\log(T+1)}{(T+1)^{(\\tilde{\\mu}_{m}C N)/2}}\\Big)\\,\\qquad\\qquad\\qquad\\quad\\,f o r\\,\\beta=1\\,,}\\\\ &{\\left.E_{1}^{T,N}\\sqrt{\\delta_{0,0}^{\\mathrm{SGDo}}}\\,+\\,C\\sigma_{n,T}^{\\mathrm{SGDo}}\\Big(\\frac{2^{2}\\beta+1}{\\tilde{\\mu}_{m}C}e^{\\frac{\\beta_{m}C}{2(1-\\beta)}\\frac{N}{(T+1)^{\\beta}}}+\\frac{3^{\\beta}(1+\\tilde{\\mu}_{m}C)^{N-1}(T+2)^{\\beta}}{L^{2}C^{2}}\\,E_{2}^{T,N}\\Big)\\,\\begin{array}{l}{o t h e r w i s e\\,,}\\\\ {o t h e r w i s e\\,,}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where ET,N and EZ,N are two decreasing functions in $T$ definedby ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{E_{1}^{T,N}\\,:=\\,\\exp\\Big(1-N\\tilde{\\mu}_{m}C\\varphi_{1-\\beta}(T+1)+\\frac{N L^{2}C^{2}}{2}\\varphi_{1-2\\beta}(T+1)\\Big)\\ ,}\\\\ &{E_{2}^{T,N}\\,:=\\,\\exp\\Big(-\\frac{N\\tilde{\\mu}_{m}C}{2}\\varphi_{1-\\beta}(T+1)+2N L^{2}C^{2}\\varphi_{1-2\\beta}(T+1)\\Big)\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Remark. We also remark that existing works [47, 48] show that the standard SGDo typically gives a faster convergence rate in $T$ than SGDw. An analogous result for the CD setup would involve additional technical hurdles of jointly controlling the correlations across minibatches and from reusing data samples, and we defer this to future work. ", "page_idx": 16}, {"type": "text", "text": "B.3 Explicit tail control ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We now provide the full explicit tail control bounds. All results in this section hold directly for tyin,m,T(e) and oSGDo. , and we omit them here. In the result below, we denote $r_{\\Psi}$ as the radius of the smallest sphere in $\\mathbb{R}^{p}$ that contains $\\Psi$ , which is finite under A1. ", "page_idx": 16}, {"type": "text", "text": "Lemma B.3. Assume A5 and A6. Let $n\\in\\mathbb{N}$ be suficiently large s.t. $\\begin{array}{r}{\\frac{\\log n}{n}<\\frac{\\sigma_{m}^{2}\\zeta_{m}^{2}}{p+\\nu-2}}\\end{array}$ Then ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{>0}{\\operatorname{nf}}\\varepsilon_{\\nu;n,m,T}^{\\mathrm{SGDw}}(\\epsilon)\\,\\leq}\\\\ &{\\quad\\left(\\frac{3\\sigma_{m}\\sqrt{p((\\nu-2)p+2\\nu)}}{\\sqrt{\\nu-2}}+\\kappa_{\\nu;m}2^{\\frac{\\nu-2}{2\\nu}}(r_{\\Psi})^{\\frac{(\\nu-2)p}{2\\nu}}\\Big(1+\\frac{2C_{m}(\\nu-2)^{1/2}}{\\sigma_{m}p^{1/2}((\\nu-2)p+2\\nu)^{1/2}}\\Big)^{\\frac{(\\nu-2)p}{2\\nu}}\\right)\\frac{\\sqrt{\\log n}}{\\sqrt{n}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "In particular, $i f$ we additionally assume the conditions of Theorem $B.\\,I$ wehave ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{T\\to\\infty}\\sqrt{\\delta_{T,N}^{\\mathrm{SGDw}}}\\;\\leq\\;C^{\\prime}(p,\\nu,m,\\Psi,\\beta)\\Bigl(\\frac{\\sqrt{\\log n}}{\\sqrt{n}}+\\frac{1}{\\sqrt{B}}\\Bigr)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{C^{\\prime}(p,\\nu,m,\\Psi,\\beta)\\;:=\\;\\frac{8(1+5\\sigma+5\\kappa_{m})}{\\tilde{\\mu}_{m}}}\\\\ &{\\,\\,\\,\\times\\,\\,\\left(\\frac{3\\sigma_{m}\\sqrt{p((\\nu-2)p+2\\nu)}}{\\sqrt{\\nu-2}}+\\kappa_{\\nu;m}2^{\\frac{\\nu-2}{2\\nu}}(r_{\\Psi})^{\\frac{(\\nu-2)p}{2\\nu}}\\Big(1+\\frac{2C_{m}(\\nu-2)^{1/2}}{\\sigma_{m}p^{1/2}((\\nu-2)p+2\\nu)^{1/2}}\\Big)^{\\frac{(\\nu-2)p}{2\\nu}}\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Lemma B.4. Assume the conditions of Theorem 4.3 and additionally that A5 holds. Then ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\epsilon>0}{\\operatorname*{inf}}\\,\\varepsilon_{\\nu;n,m,T}^{\\mathrm{SGDw}}(\\epsilon)\\,\\le\\left(3C_{m}+\\kappa_{\\nu;m}^{\\nu/2}(r_{\\Psi})^{\\frac{(\\nu-2)p}{2\\nu}}C_{m}^{-\\frac{\\nu-2}{2}}3^{\\frac{(\\nu-2)p}{2\\nu}}\\right)n^{-\\frac{(\\nu-2)\\nu}{2(\\nu^{2}+(\\nu-2)p)}}\\,\\,,}\\\\ &{\\underset{T\\to\\infty}{\\operatorname*{lim}}\\,\\big(\\delta_{T,N}^{\\mathrm{SGDw}}\\big)^{1/2}\\,\\le\\tilde{C}^{\\prime}(p,\\nu,m,\\Psi,\\beta)\\big(n^{-\\frac{(\\nu-2)\\nu}{2(\\nu^{2}+(\\nu-2)p)}}+B^{-1/2}\\big)\\,\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{C}^{\\prime}(p,\\nu,m,\\Psi)\\;:=\\;\\frac{8(1+5\\sigma+5\\kappa m)}{\\tilde{\\mu}_{m}}\\big(3C_{m}+\\kappa_{\\nu;m}^{\\nu/2}(r_{\\Psi})^{\\frac{(\\nu-2)p}{2\\nu}}C_{m}^{-\\frac{\\nu-2}{2}}3^{\\frac{(\\nu-2)p}{2\\nu}}\\big)\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The next result considers a $\\phi$ -discrepancy mixing condition ([35]), which is a mixing assumption on $K_{\\psi}^{m}$ but with respect to a specific test function $\\phi$ , and we impose it uniformly over $\\psi\\in\\Psi$ We also recallthat $X_{1}^{\\psi}\\sim p_{\\psi}$ ", "page_idx": 17}, {"type": "text", "text": "Lemma B.5. Assume that there exist $\\tilde{\\alpha}\\in[0,1)$ and $\\tilde{C}_{K}>0$ such that, for all $\\psi\\in\\Psi$ and $x\\in\\mathscr{X}$ $\\|\\mathbb{E}[\\phi(K_{\\psi}^{m}(x))]-\\mathbb{E}[\\phi(X_{1}^{\\psi})]\\|\\le\\tilde{C}_{K}\\tilde{\\alpha}^{m}$ .Then ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{inf}_{\\epsilon>0}\\,\\varepsilon_{\\nu;n,m,T}^{\\mathrm{SGDw}}(\\epsilon)\\ \\leq\\ \\bigl(1+2^{\\frac{\\nu-2}{2\\nu}}\\kappa_{\\nu;m}(\\tilde{C}_{K})^{\\frac{\\nu-2}{2\\nu}}\\bigr)\\tilde{\\alpha}^{\\frac{(\\nu-2)m}{3\\nu-2}}\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "In particular, if we additionally assume the conditions of Theorem 4.3, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{T\\to\\infty}\\sqrt{\\delta_{T,N}^{\\mathrm{SGDw}}}\\ \\le\\ \\frac{8}{\\mu-\\alpha^{m}\\sigma C_{\\chi}}\\Bigl(\\bigl(1+2^{\\frac{\\nu-2}{2\\nu}}\\kappa_{\\nu;m}(\\tilde{C}_{K})^{\\frac{\\nu-2}{2\\nu}}\\bigr)\\tilde{\\alpha}^{\\frac{(\\nu-2)m}{3\\nu-2}}+\\frac{5\\sigma+5\\kappa_{m}}{\\sqrt{B}}\\Bigr)\\ .\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "C Auxiliary Tools ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "C.1  Properties of $\\varphi_{\\gamma}$ ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The following lemma collects some identities used in [22]. ", "page_idx": 17}, {"type": "text", "text": "Lemma C.1. $\\varphi_{\\gamma}$ satisfies the following properties: ", "page_idx": 17}, {"type": "text", "text": "(i) $\\varphi_{\\gamma}$ is increasing on $\\mathbb{R}_{+}$ for all $\\gamma$ (ii) $\\begin{array}{r}{\\varphi_{\\gamma}(t)\\leq\\frac{t^{\\gamma}}{\\gamma}\\,f o r\\,\\gamma>0,}\\end{array}$ and $\\varphi_{\\gamma}(t)\\leq-\\frac{1}{\\gamma}$ for $\\gamma<0$ (ii) $\\varphi_{1-\\beta}(t)\\geq t^{1-\\beta}$ for $\\beta\\in(0,1]$ (iv) $\\begin{array}{r}{\\varphi_{\\gamma}(t)-\\varphi_{\\gamma}(\\frac{t}{2})\\geq\\frac{1}{2}x^{\\gamma}}\\end{array}$ for $\\gamma\\in(0,1]$ ", "page_idx": 17}, {"type": "text", "text": "The next lemma provides some additional results on $\\varphi_{\\gamma}$ ", "page_idx": 17}, {"type": "text", "text": "Lemma C.2. $\\varphi_{\\gamma}$ satisfies the following properties: ", "page_idx": 17}, {"type": "text", "text": "(i) $\\varphi_{\\gamma}$ is positive on $t>0$ and increasing for every $\\gamma\\in\\mathbb{R}$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\varphi_{1-\\beta}(t_{2}+1)-\\varphi_{1-\\beta}(t_{1})\\ \\leq\\ \\sum_{t=t_{1}}^{t_{2}}t^{-\\beta}\\ \\leq\\ 2\\left(\\varphi_{1-\\beta}(t_{2}+1)-\\varphi_{1-\\beta}(t_{1})\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "If instead $\\beta<0$ wehave ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{2}\\big(\\varphi_{1-\\beta}(t_{2}+1)-\\varphi_{1-\\beta}(t_{1})\\big)\\ \\leq\\ \\sum_{t=t_{1}}^{t_{2}}t^{-\\beta}\\ \\leq\\ \\varphi_{1-\\beta}(t_{2}+1)-\\varphi_{1-\\beta}(t_{1})\\ ;}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "(ii) For $1\\leq t_{1}\\leq t_{2}$ and $\\gamma\\neq0$ ,we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}{t_{1}^{\\gamma-1}\\;\\leq\\varphi_{\\gamma}(t_{2})-\\varphi_{\\gamma}(t_{1})\\;\\leq\\;t_{2}^{\\gamma-1}}&{}&{\\qquad\\qquad i f\\,\\gamma\\geq1\\;,}\\\\ {t_{2}^{-(1-\\gamma)}\\;\\leq\\varphi_{\\gamma}(t_{2})-\\varphi_{\\gamma}(t_{1})\\;\\leq\\;t_{1}^{-(1-\\gamma)}}&{}&{}&{\\qquad\\qquad i f\\,\\gamma\\leq1\\;;}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "(iv) Let $1\\leq t_{1}<t_{2}$ and $\\kappa,\\beta\\geq0$ If $\\kappa\\neq1$ and $a>0$ ,we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sum_{t=t_{1}}^{t_{2}}(t+1)^{-\\beta}\\exp\\left(a\\,\\varphi_{1-\\kappa}(t-1)\\right)\\ \\leq\\ \\frac{(t_{2}+1)^{\\operatorname*{max}\\{\\kappa-\\beta,0\\}}}{a}\\,\\exp\\left(a\\,\\varphi_{1-\\kappa}(t_{2}+1)\\right)\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "andif $\\kappa\\neq1$ and $a<0$ ,we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sum_{t=t_{1}}^{t_{2}}(t+1)^{-\\beta}\\exp\\left(a\\,\\varphi_{1-\\kappa}(t)\\right)\\ \\leq\\ \\frac{(t_{2}+1)^{\\operatorname*{max}\\{\\kappa-\\beta,0\\}}}{(-a)}\\,\\exp\\left(a\\,\\varphi_{1-\\kappa}(t_{1})\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof of Lemma C.2. (i) follows from checking $\\gamma>0,\\gamma=0$ and $\\gamma<0$ respectively. The first set of bounds in (i) follow by noting that $t\\mapsto t^{-\\beta}$ is decreasing for $\\beta\\geq0$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sum_{t=t_{1}}^{t_{2}}t^{-\\beta}\\ \\geq\\ \\int_{t_{1}}^{t_{2}+1}t^{-\\beta}d t\\ =\\ \\varphi_{1-\\beta}(t_{2}+1)-\\varphi_{1-\\beta}(t_{1})\\ ,}\\\\ &{\\sum_{t=t_{1}}^{t_{2}}t^{-\\beta}\\ \\leq2\\sum_{t=t_{1}}^{t_{2}}(t+1)^{-\\beta}\\ \\leq\\ 2\\int_{t_{1}-1}^{t_{2}}(t+1)^{-\\beta}d t\\ =\\ 2\\big(\\varphi_{1-\\beta}(t_{2}+1)-\\varphi_{1-\\beta}(t_{1})\\big)\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The second set of bounds follows from noting that $t\\mapsto t^{-\\beta}$ is increasing for $\\beta<0$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\sum_{t=t_{1}}^{t_{2}}t^{-\\beta}\\ \\geq\\frac{1}{2}\\sum_{t=t_{1}}^{t_{2}}(t+1)^{-\\beta}\\ \\geq\\ \\frac{1}{2}\\int_{t_{1}-1}^{t_{2}}(t+1)^{-\\beta}d t\\ =\\ \\frac{1}{2}\\big(\\varphi_{1-\\beta}(t_{2}+1)-\\varphi_{1-\\beta}(t_{1})\\big)\\ ,}\\\\ {\\sum_{t=t_{1}}^{t_{2}}t^{-\\beta}\\ \\leq\\ \\int_{t_{1}}^{t_{2}+1}t^{-\\beta}d t\\ =\\ \\varphi_{1-\\beta}(t_{2}+1)-\\varphi_{1-\\beta}(t_{1})\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "For (i), we note that for $\\gamma\\neq0$ \uff0c ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\varphi_{\\gamma}(t_{2})-\\varphi_{\\gamma}(t_{1})\\;=\\frac{t_{2}^{\\gamma}-t_{1}^{\\gamma}}{\\gamma}\\;,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "so by the mean value theorem, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{inf}_{t_{1}\\leq t\\leq t_{2}}t^{\\gamma-1}\\;\\leq\\;\\varphi_{\\gamma}(t_{2})-\\varphi_{\\gamma}(t_{1})\\;\\leq\\;\\operatorname*{sup}_{t_{1}\\leq t\\leq t_{2}}t^{\\gamma-1}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The desired bounds then follow from an explicit computation of the infimum and the maximum in eachofthetwocases $\\gamma\\geq1$ and $\\gamma\\le1$ ", "page_idx": 18}, {"type": "text", "text": "For (iv), we first consider the case $\\kappa\\neq1$ and $a>0$ . Then ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sum_{t=t_{1}}^{t_{2}}(t+1)^{-\\beta}\\exp\\left(a\\,\\varphi_{1-\\kappa}(t-1)\\right)\\,=\\,e^{-\\frac{a}{1-\\kappa}}\\sum_{t=t_{1}}^{t_{2}}(t+1)^{-\\beta}\\exp\\left(\\frac{a t^{1-\\kappa}}{1-\\kappa}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq e^{-\\frac{a}{1-\\kappa}}\\operatorname*{max}_{t_{1}\\leq t\\leq t_{2}}\\left(\\frac{(t+1)^{\\kappa}}{(t+1)^{\\beta}}\\right)\\sum_{t=t_{1}}^{t_{2}}(t+1)^{-\\kappa}\\exp\\left(\\frac{a t^{1-\\kappa}}{1-\\kappa}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq e^{-\\frac{a}{1-\\kappa}}(t_{2}+1)^{\\operatorname*{max}\\{\\kappa-\\beta,0\\}}\\sum_{t=t_{1}}^{t_{2}}(t+1)^{-\\kappa}\\exp\\left(\\frac{a t^{1-\\kappa}}{1-\\kappa}\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Since, for $x\\geq0$ $x\\mapsto(x+1)^{-\\kappa}$ is decreasing and $x\\mapsto\\exp(a x^{1-\\kappa}/(1-\\kappa))$ is increasing, we have that for $t_{1}\\leq t\\leq t_{2}$ and $x\\in[t,t+1]$ \uff0c ", "page_idx": 18}, {"type": "equation", "text": "$$\n(t+1)^{-\\kappa}\\;\\leq x^{-\\kappa}\\;\\;\\qquad\\mathrm{and}\\;\\qquad\\exp(a t^{1-\\kappa}/(1-\\kappa))\\;\\leq\\;\\exp(a x^{1-\\kappa}/(1-\\kappa))\\;.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This implies that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\sum_{t=t_{1}}^{t_{2}}(t+1)^{-\\beta}\\exp\\left(a\\,\\varphi_{1-\\kappa}(t-1)\\right)}&{}\\\\ &{\\qquad\\qquad\\qquad\\leq(t_{2}+1)^{\\operatorname*{max}\\{\\kappa-\\beta,0\\}}e^{-\\frac{a}{1-\\kappa}}\\sum_{t=t_{1}}^{t_{2}}\\int_{t}^{t+1}x^{-\\kappa}\\exp\\left(\\frac{a x^{1-\\kappa}}{1-\\kappa}\\right)d x}\\\\ &{\\qquad=(t_{2}+1)^{\\operatorname*{max}\\{\\kappa-\\beta,0\\}}e^{-\\frac{a}{1-\\kappa}}\\int_{t_{1}}^{t_{2}+1}x^{-\\kappa}\\exp\\left(\\frac{a x^{1-\\kappa}}{1-\\kappa}\\right)d x}\\\\ &{\\qquad\\qquad\\leq\\frac{(t_{2}+1)^{\\operatorname*{max}\\{\\kappa-\\beta,0\\}}}{a}\\,e^{-\\frac{a}{1-\\kappa}}\\,e^{\\frac{a(t_{2}+1)^{1-\\kappa}}{1-\\kappa}}}\\\\ &{\\qquad=\\frac{(t_{2}+1)^{\\operatorname*{max}\\{\\kappa-\\beta,0\\}}}{a}\\,\\exp\\left(a\\,\\varphi_{1-\\kappa}(t_{2}+1)\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The main difference in the case $\\kappa\\neq1$ and $a<0$ is that we now use $x\\mapsto\\exp(a(x+1)^{1-\\kappa}/(1-\\kappa))$ is decreasing to obtain, for $t_{1}\\leq t\\leq t_{2}$ and $x\\in[t,t+1]$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\exp(a(t+1)^{1-\\kappa}/(1-\\kappa))\\ \\leq\\ \\exp(a x^{1-\\kappa}/(1-\\kappa))\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "A similar argument then yields ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\sum_{t=t_{1}}^{t_{2}}(t+1)^{-\\beta}\\exp\\left(a\\,\\varphi_{1-\\kappa}(t)\\right)}\\\\ &{\\leq(t_{2}+1)^{\\operatorname*{max}\\{\\kappa-\\beta,0\\}}e^{-\\frac{a}{1-\\kappa}}\\sum_{t=t_{1}}^{t_{2}}(t+1)^{-\\kappa}\\exp\\left(\\frac{a(t+1)^{1-\\kappa}}{1-\\kappa}\\right)}\\\\ &{\\leq(t_{2}+1)^{\\operatorname*{max}\\{\\kappa-\\beta,0\\}}e^{-\\frac{a}{1-\\kappa}}\\int_{t_{1}}^{t_{2}+1}x^{-\\kappa}\\exp\\left(\\frac{a x^{1-\\kappa}}{1-\\kappa}\\right)d x}\\\\ &{=\\frac{(t_{2}+1)^{\\operatorname*{max}\\{\\kappa-\\beta,0\\}}}{a}\\left(\\exp\\left(a\\,\\varphi_{1-\\kappa}(t_{2}+1)\\right)-\\exp\\left(a\\,\\varphi_{1-\\kappa}(t_{1})\\right)\\right)}\\\\ &{\\leq\\frac{(t_{2}+1)^{\\operatorname*{max}\\{\\kappa-\\beta,0\\}}}{(-a)}\\exp\\left(a\\,\\varphi_{1-\\kappa}(t_{1})\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We also need the following lemma, which is useful for controlling the accumulation of errors from the noise terms over iterations. ", "page_idx": 19}, {"type": "text", "text": "Lemma C.3. For any $a,b\\geq0,T,N\\in\\mathbb{N}$ and $\\kappa,\\beta\\geq0$ such that $b t^{-\\beta}\\!-\\!a t^{-\\kappa}\\leq1$ for all $1\\leq t\\leq T$ we have that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\prod_{t=1}^{T}(1-b t^{-\\beta}+a t^{-\\kappa})^{N}\\;\\leq\\;\\exp\\big(-b N\\,\\varphi_{1-\\beta}(T+1)+a N\\,\\varphi_{1-\\kappa}(T+1)\\big)\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Moreover, for any $\\zeta\\geq0$ we have that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sum_{t=1}^{T}t^{-\\zeta}\\left(\\sum_{j=1}^{N}(1-b t^{-\\beta}+a t^{-\\kappa})\\right)\\,\\prod_{s=t+1}^{T}(1-b s^{-\\beta}+a s^{-\\kappa})^{N}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\le Q_{1}+\\exp\\Big(-\\frac{b N}{2}\\,\\varphi_{1-\\beta}(T+1)+4a N\\,\\varphi_{1-\\kappa}(T+1)\\Big)\\,Q_{2}\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where ", "page_idx": 19}, {"type": "equation", "text": "$$\nQ_{1}\\;:=\\;\\left\\{\\frac{2^{2\\zeta+1}(T+3)^{\\operatorname*{max}\\{\\beta-\\zeta,0\\}}}{b}\\exp\\Big(\\frac{b N}{2(1-\\beta)(T+1)^{\\beta}}\\Big)\\qquad\\right.i f\\beta\\neq1\\,,\\;b>0\\;,}\\\\ {\\left.2N\\varphi_{1-\\zeta+b N/2}(T+1)\\exp\\Big(-\\frac{b N}{2}\\,\\varphi_{1-\\beta}(T+1)\\Big)\\quad\\right.i f\\beta=1\\;o r\\;b=0\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{Q_{2}\\;:=\\;\\left\\{\\frac{3^{\\zeta}(1+a)^{N-1}}{2a}(T+2)^{\\operatorname*{max}\\{\\kappa-\\zeta,0\\}}\\qquad i f\\kappa\\neq1\\;a n d\\,a>0\\;,\\right.}\\\\ {\\left.2N(1+a)^{N-1}\\,\\varphi_{1-\\zeta-2a N}(T+1)\\quad\\,i f\\kappa=1\\;o r\\,a=0\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "In the special case $\\zeta=\\beta=1<\\kappa$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sum_{t=1}^{T}t^{-\\zeta}\\left(\\sum_{j=1}^{N}(1-b t^{-\\beta}+a t^{-\\kappa})^{j-1}\\right)\\prod_{s=t+1}^{T}(1-b s^{-\\beta}+a s^{-\\kappa})^{N}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\le\\ \\frac{4}{b}+\\frac{3N(1+a)^{N-1}e^{\\frac{4a N}{\\kappa-1}}\\log(T+1)}{(T+1)^{\\frac{b N}{2}}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof of Lemma C.3. By assumption, $b t^{-\\beta}-a t^{-\\kappa}\\leq1$ for all $1\\leq t\\leq T$ Since $0\\leq1-x\\leq e^{-x}$ for all $x\\leq1$ , we have that for any $1\\leq t_{1}\\leq t_{2}\\leq T$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\prod_{t=t_{1}}^{t_{2}}(1-b t^{-\\beta}+a t^{-\\kappa})^{N}\\;\\leq\\;\\exp\\Bigg(-b N\\sum_{t=t_{1}}^{t_{2}}t^{-\\beta}+a N\\sum_{t=t_{1}}^{t_{2}}t^{-\\kappa}\\Bigg)\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Applying this to the first quantity of interest followed by noting that $a,b\\ge0$ and using Lemma C.2(i), we obtain the first bound that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\prod_{t=1}^{T}(1-b t^{-\\beta}+a t^{-\\kappa})^{N}\\,\\le\\,\\exp\\bigg(-b N\\sum_{t=1}^{T}t^{-\\beta}+a N\\sum_{t=1}^{T}t^{-\\kappa}\\bigg)}&{}\\\\ {\\,\\le\\,\\exp\\big(-b N\\,\\varphi_{1-\\beta}(T+1)+a N\\,\\varphi_{1-\\kappa}(T+1)\\big)\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For the second bound, we define ", "page_idx": 19}, {"type": "equation", "text": "$$\nt_{0}\\ :=\\ \\operatorname*{sup}\\left\\{t\\le T\\Big|\\ \\frac{b}{2}\\le a t^{-(\\kappa-\\beta)}\\right\\}\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then by noting that $1-b t^{-\\beta}+a t^{-\\kappa}\\geq0$ for all $1\\leq t\\leq T$ again,we can bound the quantity of interest as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sum_{t=1}^{r}t^{-\\zeta}\\left(\\sum_{j=1}^{N}(1-k^{-\\beta}+a^{-\\beta})^{j-1}\\right)\\prod_{s=t+1}^{r}(1-b s^{-\\beta}+a s^{-\\beta})^{N}}\\\\ &{=\\sum_{t=\\ a_{k}+1}^{r}t^{-\\zeta}\\left(\\sum_{j=1}^{N}(1-\\beta^{-\\beta}+a^{-\\beta})^{j-1}\\right)\\prod_{s=t+1}^{r}(1-b s^{-\\beta}+a s^{-\\beta})^{N}}\\\\ &{\\quad+\\left(\\prod_{s=t+1}^{r}(1-b s^{-\\beta}+a s^{-\\beta})\\right)}\\\\ &{\\quad\\times\\left(\\sum_{t=1}^{r}t^{-\\zeta}\\left(\\sum_{j=1}^{N}(1-b^{-\\beta}+a^{-\\beta})^{j-1}\\right)\\prod_{s=t+1}^{\\theta}(1-b s^{-\\beta}+a s^{-\\beta})^{N}\\right)}\\\\ &{\\le\\sum_{t=a_{k}+1}^{r}t^{-\\zeta}\\left(\\sum_{j=1}^{N}\\left(1-\\frac{b}{2}t^{-\\beta}\\right)^{j-1}\\right)\\prod_{s=t+1}^{r}\\left(1-\\frac{b}{2}s^{-\\beta}\\right)^{N}}\\\\ &{\\quad+\\left(\\prod_{s=t+1}^{r}\\left(1-\\frac{b}{2}s^{-\\beta}\\right)^{N}\\right)\\left(\\sum_{t=1}^{r}t^{-\\zeta}\\left(\\sum_{j=1}^{N}(1+a^{-\\beta})^{j-1}\\right)\\prod_{s=t+1}^{\\theta}(1+a^{-\\alpha})^{N}\\right)}\\\\ &{\\le N\\times\\underbrace{\\sum_{t=a_{k}+1}^{r}t^{-\\zeta}\\prod_{s=t+1}^{r}\\left(1-\\frac{b}{2}s^{-\\beta}\\right)^{N}}_{=x_{k}}}\\\\ &{\\quad+N(1+a)^{N-1}\\times\\left(\\prod_{s=t+1}^{r}\\left(1-\\frac{b}{2}s^{-\\beta}\\right)^{N}\\right)\\times\\left(\\sum_{t=1}^{t}t^{-\\zeta}\\prod_{s=t+1}^{r}(1+a s^{-\\alpha})^{N}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "In the last line, we have used that $\\begin{array}{r}{0\\le1-\\frac{b}{2}t^{-\\beta}\\le1}\\end{array}$ for $t\\geq t_{0}+1$ and $1+a t^{-\\kappa}\\leq1+a$ To control the three quantities, we first note that by (13) , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{S_{3}\\,\\le\\,\\exp\\Big(-\\frac{b N}{2}\\sum_{s=1}^{T}s^{-\\beta}\\Big)\\exp\\Big(\\frac{b N}{2}\\sum_{s=1}^{t_{0}}s^{-\\beta}\\Big)}\\\\ &{\\quad\\overset{(a)}{\\le}\\,\\exp\\Big(-\\frac{b N}{2}\\sum_{s=1}^{T}s^{-\\beta}\\Big)\\exp\\Big(a N\\sum_{s=1}^{t_{0}}s^{-\\kappa}\\Big)}\\\\ &{\\quad\\overset{(b)}{\\le}\\,\\exp\\Big(-\\frac{b N}{2}\\,\\varphi_{1-\\beta}(T+1)+2a N\\,\\varphi_{1-\\kappa}(T+1)\\Big)\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "In $(a)$ above, we have noted that ${\\frac{b}{2}}\\ \\leq\\ a s^{-(\\kappa-\\beta)}$ for $s\\,\\leq\\,t_{0}$ ; in $(b)$ , we have used $t_{0}\\,\\leq\\,T$ and Lemma C.2(ii) with $a,b\\ge0$ In the special case $\\beta=1<\\kappa$ , the above yields ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{S_{3}\\!\\!}&{\\le(T+1)^{-\\frac{b N}{2}}\\exp\\left(2a N\\,\\frac{1-(T+1)^{-(\\kappa-1)}}{\\kappa-1}\\right)}\\\\ &{\\le(T+1)^{-\\frac{b N}{2}}e^{\\frac{2a N}{\\kappa-1}}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We now control $S_{2}$ . By (13) again, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{S_{2}\\,\\le\\,\\sum_{t=1}^{t_{0}}t^{-\\zeta}\\exp\\Big(a N\\sum_{s=t+1}^{t_{0}}s^{-\\kappa}\\Big)}\\\\ &{\\le\\,\\sum_{t=1}^{T}t^{-\\zeta}\\exp\\Big(a N\\sum_{s=t+1}^{T}s^{-\\kappa}\\Big)}\\\\ &{\\overset{(c)}{\\le}\\,\\exp\\big(2a N\\varphi_{1-\\kappa}(T+1)\\big)\\times\\Big(\\sum_{t=1}^{T}t^{-\\zeta}\\exp\\big(-2a N\\varphi_{1-\\kappa}(t+1)\\big)\\Big)}\\\\ &{\\overset{(d)}{\\le}3^{\\zeta}\\exp\\big(2a N\\varphi_{1-\\kappa}(T+1)\\big)\\,\\sum_{t=1}^{T}(t+2)^{-\\zeta}\\exp\\big(-2a N\\varphi_{1-\\kappa}(t+1)\\big)\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "$(c)$ above, we have applied Lemma C.2(i); in $(d)$ , we have noted that $\\mathrm{sup}_{t\\in\\mathbb{N}}(t+2)^{\\beta}/t^{\\beta}=3^{\\beta}$ If $\\kappa\\neq1$ and $a>0$ we can apply Lemma C.2(iv) to get that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{S_{2}\\!\\!}&{\\leq\\frac{3^{\\zeta}}{2a N}(T+2)^{\\operatorname*{max}\\{\\kappa-\\zeta,0\\}}\\,\\exp\\big(2a N\\varphi_{1-\\kappa}(T+1)\\big)}\\\\ &{=\\frac{Q_{2}}{N(1+a+c)^{N-1}}\\,\\exp\\big(2a N\\varphi_{1-\\kappa}(T+1)\\big)\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "If $\\kappa=1$ or $a=0$ , the bound from $(c)$ above reads ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{S_{2}\\,\\leq\\,\\exp\\left(2a N\\varphi_{1-\\kappa}(T+1)\\right)\\,\\sum_{t=1}^{T}t^{-\\zeta}(t+1)^{-2a N}}\\\\ &{\\quad\\,\\leq\\,\\exp\\left(2a N\\varphi_{1-\\kappa}(T+1)\\right)\\,\\sum_{t=1}^{T}t^{-\\zeta-2a N}}\\\\ &{\\quad\\,\\leq\\,2\\,\\varphi_{1-\\zeta-2a N}(T+1)\\,\\exp\\left(2a N\\varphi_{1-\\kappa}(T+1)\\right)\\,=\\,\\frac{Q_{2}}{N(1+a)^{N-1}}\\,\\exp\\left(2a N\\varphi_{1-\\kappa}(T+1)\\right)\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where we have used Lemma C.2(ii) in the last line. Now consider the special case with $\\zeta=1<\\kappa$ the bound from $(d)$ becomes ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{S_{2}\\,\\le3\\exp\\left(2a N\\varphi_{1-\\kappa}(T+1)\\right)\\Big(\\sum_{t=1}^{T}(t+2)^{-1}\\exp\\big(-2a N\\varphi_{1-\\kappa}(t+1)\\big)\\Big)}\\\\ &{\\hphantom{\\sum_{t=1}^{T}}\\le3\\exp\\left(2a N\\,\\frac{1-(T+1)^{-(\\kappa-1)}}{\\kappa-1}\\right)\\sum_{t=1}^{T}(t+2)^{-1}}\\\\ &{\\hphantom{\\sum_{t=1}^{T}}\\le3e^{\\frac{2a N}{\\kappa-1}}\\log(T+1)\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We are left with controlling $S_{1}$ , which follows from a similar strategy as controlling $S_{2}$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{S_{1}\\overset{(13)}{\\leq}\\sum_{t=t_{0}+1}^{T}t^{-\\zeta}\\exp\\Big(-\\frac{b N}{2}\\sum_{s=t+1}^{T}s^{-\\beta}\\Big)}\\\\ &{\\leq\\sum_{t=1}^{T}t^{-\\zeta}\\exp\\Big(-\\frac{b N}{2}\\sum_{s=t+1}^{T}s^{-\\beta}\\Big)}\\\\ &{\\overset{(a)}{\\leq}\\exp\\Big(-\\frac{b N}{2}\\,\\varphi_{1-\\beta}(T+1)\\Big)\\sum_{t=1}^{T}t^{-\\zeta}\\exp\\Big(\\frac{b N}{2}\\,\\varphi_{1-\\beta}(t+1)\\Big)}\\\\ &{\\leq4^{\\zeta}\\exp\\Big(-\\frac{b N}{2}\\,\\varphi_{1-\\beta}(T+1)\\Big)\\sum_{t=1}^{T}(t+3)^{-\\zeta}\\exp\\Big(\\frac{b N}{2}\\,\\varphi_{1-\\beta}(t+1)\\Big)\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "In $(a)$ above, we used Lemma C.2(i). For $\\beta\\neq1$ and $b\\neq0$ , we can apply Lemma C.2(iv) with $\\textstyle{\\frac{b}{2}}>0$ to obtain ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{S_{1}\\,\\le4^{\\zeta}\\exp\\Big(-\\frac{b N}{2}\\,\\varphi_{1-\\beta}(T+1)\\Big)\\frac{(T+3)^{\\operatorname*{max}\\{\\beta-\\zeta,0\\}}}{b N/2}\\exp\\Big(\\frac{b N}{2}\\varphi_{1-\\beta}(T+3)\\Big)}\\\\ &{\\quad=\\frac{2^{2\\zeta+1}(T+3)^{\\operatorname*{max}\\{\\beta-\\zeta,0\\}}}{b N}\\exp\\Big(\\frac{b N}{2(1-\\beta)}\\Big((T+3)^{1-\\beta}-(T+1)^{1-\\beta}\\Big)\\Big)}\\\\ &{\\overset{(b)}{\\le}\\frac{2^{2\\zeta+1}(T+3)^{\\operatorname*{max}\\{\\beta-\\zeta,0\\}}}{b N}\\exp\\Big(\\frac{b N}{2(1-\\beta)(T+1)^{\\beta}}\\Big)\\,=\\,\\frac{Q_{1}}{N}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "In $(b)$ , we have used Lemma C.2(i) with $1-\\beta\\leq1$ . Meanwhile, if $\\beta=1$ or $b=0$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{S_{1}\\,\\le\\,\\exp\\Big(-\\frac{b N}{2}\\,\\varphi_{1-\\beta}(T+1)\\Big)\\sum_{t=1}^{T}t^{-\\zeta}(t+1)^{b N/2}}\\\\ &{\\quad\\le\\,\\exp\\Big(-\\frac{b}{2}\\,\\varphi_{1-\\beta}(T+1)\\Big)\\sum_{t=1}^{T}t^{-\\zeta+b N/2}}\\\\ &{\\quad\\le2\\varphi_{1-\\zeta+b N/2}(T+1)\\exp\\Big(-\\frac{b N}{2}\\,\\varphi_{1-\\beta}(T+1)\\Big)\\,=\\,\\frac{Q_{1}}{N}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For the special case with $\\zeta=\\beta=1$ , the bound in (14) becomes ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{S_{1}\\,\\le\\,\\exp\\Big(-\\frac{b N}{2}\\,\\varphi_{0}(T+1)\\Big)\\sum_{t=1}^{T}t^{-1}\\exp\\Big(\\frac{b N}{2}\\,\\varphi_{0}(t+1)\\Big)}\\\\ &{=(T+1)^{-\\frac{b N}{2}}\\sum_{t=1}^{T}t^{-1}(t+1)^{\\frac{b N}{2}}}\\\\ &{\\le(T+1)^{-\\frac{b N}{2}}\\sum_{t=1}^{T}t^{-(1-\\frac{b N}{2})}}\\\\ &{\\overset{(c)}{\\le}2(T+1)^{-\\frac{b N}{2}}\\,\\varphi_{\\frac{b N}{2}}(T+1)\\,=\\,2(T+1)^{-\\frac{b N}{2}}\\,\\frac{(T+1)^{b N/2}-1}{b N/2}\\,\\le\\,\\frac{4}{b N}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "m $(c)$ whaveusdit $\\begin{array}{r}{1-\\frac{b N}{2}\\leq0}\\end{array}$ $\\begin{array}{r}{1-\\frac{b N}{2}\\geq0}\\end{array}$ ", "page_idx": 21}, {"type": "text", "text": "Combining the bounds for the general cases, we obtain the first desired inequality that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sum_{t=1}^{T}t^{-\\zeta}\\left(\\sum_{j=1}^{N}(1-b t^{-\\beta}+a t^{-\\kappa})^{j-1}\\right)\\prod_{s=t+1}^{T}(1-b s^{-\\beta}+a s^{-\\kappa})^{N}}\\\\ &{\\,\\leq\\,Q_{1}+\\exp\\Big(-\\frac{b}{2}\\,\\varphi_{1-\\beta}(T+1)+u\\varphi_{1-\\xi}(T+3)+4a\\,\\varphi_{1-\\kappa}(T+1)\\Big)\\,Q_{2}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For the special case $\\zeta=\\beta=1<\\kappa,\\gamma.$ . combining the earlier bounds gives ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sum_{t=1}^{T}t^{-\\zeta}\\left(\\sum_{j=1}^{N}(1-b t^{-\\beta}+a t^{-\\kappa})^{j-1}\\right)\\prod_{s=t+1}^{T}(1-b s^{-\\beta}+a s^{-\\kappa})^{N}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\le\\ \\frac{4}{b}+\\frac{3N(1+a)^{N-1}e^{\\frac{4a N}{\\kappa-1}}\\log(T+1)}{(T+1)^{\\frac{b N}{2}}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "C.2   Contraction and integrability results ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The next result is a standard result in convex analysis, needed to handle projections performed in Algorithms 1 and 2. ", "page_idx": 22}, {"type": "text", "text": "Lemma C.4. Let $\\Psi$ a be convex subset of $\\mathbb{R}^{p}$ . Let $\\psi^{\\star}\\in\\Psi$ . Then, for all $\\psi\\in\\mathbb{R}^{p}$ , we have: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\lVert\\operatorname{Proj}_{\\Psi}(\\psi)-\\psi^{\\star}\\rVert\\leq\\lVert\\psi-\\psi^{\\star}\\rVert\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. We have: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\psi-\\psi^{\\star}\\|^{2}=\\|\\psi-\\mathrm{Proj}_{\\Psi}(\\psi)+\\mathrm{Proj}_{\\Psi}(\\psi)-\\psi^{\\star}\\|^{2}\\qquad\\qquad\\qquad\\qquad\\qquad\\quad}\\\\ {=\\|\\psi-\\mathrm{Proj}_{\\Psi}(\\psi)\\|^{2}+2\\,\\langle\\psi-\\mathrm{Proj}_{\\Psi}(\\psi),\\mathrm{Proj}_{\\Psi}(\\psi)-\\psi^{\\star}\\rangle+\\|\\mathrm{Proj}_{\\Psi}(\\psi)-\\psi^{\\star}\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Since by [49, Proposition 1.1.9], we have: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\langle\\psi-\\mathrm{Proj}_{\\Psi}(\\psi),\\psi^{\\prime}-\\mathrm{Proj}_{\\Psi}(\\psi)\\rangle\\leq0\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "for all $\\psi^{\\prime}\\in\\Psi$ , we can use this inequality at ${\\psi}^{\\prime}={\\psi}^{\\star}\\in\\Psi$ to obtain: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\lVert\\psi-\\psi^{\\star}\\rVert^{2}\\geq\\lVert\\psi-\\mathrm{Proj}_{\\Psi}(\\psi)\\rVert^{2}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and the result follows by taking the square root. ", "page_idx": 22}, {"type": "text", "text": "We now state two lemmas that guarantee an amount of integrability sufficient to our analysis. ", "page_idx": 22}, {"type": "text", "text": "Lemma C.5. Let $p,q\\in\\mathcal{P}(\\mathcal{X})$ such that ${\\frac{\\mathrm{d}p}{\\mathrm{d}q}}\\,$ exists, andsuch that $\\chi^{2}(p;q)\\,<\\,+\\infty.$ Assomethat $f\\in L_{q}^{2}(\\mathbb{R})$ Then $|\\mathbb{E}_{p}f|<+\\infty$ ", "page_idx": 22}, {"type": "text", "text": "Proof. By assumption, $f\\in L_{q}^{2}(\\ensuremath{\\mathbb{R}}^{d})$ .Moreover, $\\chi^{2}(p,q)<+\\infty$ , and thus we have $\\textstyle{\\frac{\\mathrm{d}p}{\\mathrm{d}q}}-1\\in L_{q}^{2}(\\mathbb{R})$ Thus, the inner product is finite, and ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Big|\\int f\\Big(\\frac{\\mathrm{d}p}{\\mathrm{d}q}-1\\Big)\\mathrm{d}q\\Big|=\\Big|\\int f\\mathrm{d}p-\\int f\\mathrm{d}q\\Big|=|\\mathbb{E}_{p}f-\\mathbb{E}_{q}f|:=M<+\\infty}\\\\ &{\\quad\\implies M-|\\mathbb{E}_{q}f|<\\mathbb{E}_{p}f<M+|\\mathbb{E}_{q}f|}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Lemma C.6. For all $\\psi\\in\\Psi$ for all $m\\geq1$ , and for all $k\\geq1$ , we have: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}_{p_{\\psi}\\star}\\left\\lVert P_{\\psi}^{m}\\phi\\right\\rVert^{k}<+\\infty\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. By analycity of the log partition function $\\psi\\longmapsto\\log Z(\\psi)$ , we have that $\\begin{array}{r}{\\int\\|\\phi\\|^{k}\\,d p_{\\psi}(x)<}\\end{array}$ $+\\infty$ for all $\\psi\\,\\in\\,\\Psi$ , and thus, the function $x\\,\\longmapsto\\,\\left\\|\\phi\\right\\|^{k}(x)\\,\\in\\,L_{p_{\\psi}}^{2}(\\mathbb R)$ for all $\\psi$ . Consequently, $P^{m}\\|\\phi\\|^{k}\\,\\in\\,L_{p_{\\psi}}^{2}(\\mathbb{R}^{d})$ .We can apply lemmaC.5 to $P^{m}\\left\\|\\phi\\right\\|^{k}$ to obtain $\\mathbb{E}_{\\psi^{\\star}}P^{m}\\|\\phi\\|^{k}<+\\infty$ for all $k\\geq1$ and for all $m\\geq0$ .As a byproduct, we obtain that $P^{m}\\left\\|\\phi\\right\\|^{k}\\in L_{p_{\\psi^{\\star}}}^{2}(\\mathbb R)$ , and thus so it $\\left\\|P^{m}\\phi\\right\\|^{k}$ \u53e3 ", "page_idx": 22}, {"type": "text", "text": "The following lemma is used multiple time in our analysis. ", "page_idx": 22}, {"type": "text", "text": "Lemma C.7. For any function $f\\in L_{p_{\\psi}\\star}^{2}(\\mathbb R^{d})\\cap L_{p_{\\psi}}^{2}(\\mathbb R^{d})$ we have: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\mathbb{E}_{p_{\\psi}\\star}P_{\\psi}^{m}(f-\\mathbb{E}_{p_{\\psi}}f)\\|\\le\\alpha^{m}C_{\\chi}(\\mathbb{E}_{p_{\\psi}}\\,\\|f-\\mathbb{E}_{p_{\\psi}}f\\|^{2})^{1/2}\\,\\|\\psi-\\psi^{\\star}\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. Let us note first that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbb{E}_{p_{\\psi}^{*}}P_{\\psi}^{m}(f-\\mathbb{E}_{p_{\\psi}}f)\\|^{2}=\\sum_{i=1}^{p}\\Big(\\int P_{\\psi}^{m}(f_{i}-\\mathbb{E}_{p_{\\psi}}f_{i})(x)(p_{\\psi^{*}}(x)-p_{\\psi}(x))\\mathrm{d}x\\Big)^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\sum_{i=1}^{p}\\Bigg(\\int P_{\\psi}^{m}(f_{i}-\\mathbb{E}_{p_{\\psi}}f_{i})(x)(\\frac{p_{\\psi^{*}}}{p_{\\psi}}-1)p_{\\psi}(x)\\mathrm{d}x\\Bigg)^{2}}\\\\ &{\\qquad\\qquad\\qquad\\quad\\leq\\chi^{2}(p_{\\psi},p_{\\psi^{*}})\\sum_{i=1}^{p}\\int P_{\\psi}^{m}(f_{i}-\\mathbb{E}_{p_{\\psi}}f_{i})(x)^{2}p_{\\psi}(x)\\mathrm{d}x}\\\\ &{\\qquad\\qquad\\quad\\leq\\chi^{2}(p_{\\psi},p_{\\psi^{*}})\\sum_{i=1}^{p}\\alpha^{m}\\int(f_{i}-\\mathbb{E}_{p_{\\psi}}f_{i})(x)^{2}p_{\\psi}(x)\\mathrm{d}x}\\\\ &{\\qquad\\qquad\\quad\\leq\\alpha^{m}C_{x}^{2}\\|\\psi-\\psi^{*}\\|\\mathbb{E}_{p_{\\psi}}\\left\\|f-\\mathbb{E}_{p_{\\psi}}f\\right\\|^{2}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "C.3  Miscellaneous ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Lemma C.8. Let $f:\\Psi\\to\\mathbb{R}^{p}$ be a differentiable function in the interior of $\\Psi\\subseteq\\mathbb{R}^{p}$ . For $\\psi\\in\\Psi$ define $\\begin{array}{r}{\\sigma_{\\operatorname*{min}}(\\psi):=\\operatorname*{inf}_{\\theta\\in\\Psi,\\|\\theta\\|=1}\\theta^{\\top}\\nabla\\dot{f}(\\psi)\\theta}\\end{array}$ and $\\begin{array}{r}{\\sigma_{\\operatorname*{max}}(\\psi):=\\operatorname*{sup}_{\\theta\\in\\Psi,\\|\\theta\\|=1}\\dot{\\theta}^{\\top}\\nabla f(\\psi)\\theta}\\end{array}$ with respect to the Jacobian matrix $\\nabla f(\\psi)$ . Then for any $\\psi_{1},\\psi_{2}\\in\\Psi$ , we have that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{inf}_{\\psi\\in\\Psi}\\sigma_{\\operatorname*{min}}(\\psi)\\ \\leq\\ (\\psi_{1}-\\psi_{2})^{\\top}\\big(f(\\psi_{1})-f(\\psi_{2})\\big)\\ \\leq\\ \\operatorname*{sup}_{\\psi\\in\\Psi}\\sigma_{\\operatorname*{max}}(\\psi)}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof of Lemma C.8. By the mean value theorem, there exists some $a\\in(0,1)$ such that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{(\\psi_{1}-\\psi_{2})^{\\top}\\big(f(\\psi_{1})-f(\\psi_{2})\\big)\\,=(\\psi_{1}-\\psi_{2})^{\\top}\\big(f(1\\times\\psi_{1}+0\\times\\psi_{2})-f(0\\times\\psi_{1}+0\\times\\psi_{2})\\big)}\\\\ &{=(\\psi_{1}-\\psi_{2})^{\\top}\\nabla f(a\\psi_{1}+(1-a)\\psi_{2})(\\psi_{1}-\\psi_{2})}\\\\ &{=\\|\\psi_{1}-\\psi_{2}\\|^{2}\\,{\\frac{(\\psi_{1}-\\psi_{2})^{\\top}}{\\|\\psi_{1}-\\psi_{2}\\|}}\\,\\nabla f(a\\psi_{1}+(1-a)\\psi_{2})\\,{\\frac{\\psi_{1}-\\psi_{2}}{\\|\\psi_{1}-\\psi_{2}\\|}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Plugging in the definition of $\\sigma_{\\mathrm{max}}$ gives the desired upper bound and similarly $\\sigma_{\\mathrm{min}}$ implies the lower bound. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "D Proofs for Online CD ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "D.1  Auxiliary Lemmas for Online CD ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We recall the following notations used in the next lemmas, namely $\\sigma_{\\psi}\\;:=\\;\\mathbb{E}_{p_{\\psi}}\\|\\phi-\\mathbb{E}_{p_{\\psi}}\\phi\\|^{2}$ $\\sigma_{\\star}:=\\sigma_{\\psi^{\\star}}$ and $\\sigma:=\\operatorname*{sup}_{\\psi\\in\\Psi}\\sigma_{\\psi}$ ", "page_idx": 23}, {"type": "text", "text": "We now provide two intermediary lemmas necessary to analyze the impact of variance in the CD gradient. The strategy is similar in both of them: we change the integration from $p_{\\psi^{\\star}}$ to $p_{\\psi}$ to obtain contraction, at the cost of an additional term scaling with $C_{\\chi}\\,\\|\\bar{\\psi}-\\psi^{\\star}\\|$ .Weobtain exact constants that we choose to describe in terms of the smoothness parameters of the problem, e.g. the $k^{t h}$ derivatives of the log partition function $\\log{Z}$ , which, for $k\\geq2$ , equals the $k^{t h}$ derivative of the negative cross-entropy model w.r.t $p_{\\psi^{\\star}}$ ", "page_idx": 23}, {"type": "text", "text": "Second Moment convergence  The following lemmas guarantee the second moment of a sample from $k_{\\psi}^{m}p_{\\psi^{\\star}}$ approaches the second moment of a sample from the target distribution $p_{\\psi}$ ", "page_idx": 23}, {"type": "text", "text": "Lemma D.1. Under A1, A2 and A3, for all $\\psi\\in\\Psi$ wehave: ", "page_idx": 23}, {"type": "equation", "text": "$$\n|\\mathbb{E}_{p_{\\psi}\\star}P_{\\psi}^{m}||\\phi||^{2}-\\mathbb{E}_{p_{\\psi}}||\\phi||^{2}|\\le\\alpha^{m}C_{\\chi}\\,\\|\\psi-\\psi^{\\star}\\|\\,\\|\\log Z\\|_{1,\\infty}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\|\\log Z\\|_{1,\\infty}:=\\operatorname*{sup}_{\\psi\\in\\Psi}\\sum_{i=1}^{p}(4\\partial_{i}^{1}\\log Z(\\psi)^{2}\\partial_{i}^{2}\\log Z(\\psi)+2\\partial_{i}^{2}\\log Z(\\psi)^{2}+4\\partial_{i}^{1}\\log Z(\\psi)\\partial_{i}^{3}\\log Z(\\psi)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\partial_{i}^{4}\\log Z(\\psi))^{1/2}<+\\infty}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. Applying Lemma C.7 to each $f_{i}:=\\phi_{i}^{2}$ ,wehave ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\mathbb{E}_{p_{\\psi}\\star}P_{\\psi}^{m}\\phi_{i}^{2}-\\mathbb{E}_{p_{\\psi}\\phi_{i}^{2}}|=\\alpha^{m}C_{\\chi}\\,\\|\\psi-\\psi^{\\star}\\|\\left(\\mathbb{E}_{p_{\\psi}}\\left(\\phi_{i}^{2}-\\mathbb{E}_{p_{\\psi}}\\phi_{i}^{2}\\right)^{2}\\right)^{1/2}}\\\\ {=\\alpha^{m}C_{\\chi}\\,\\|\\psi-\\psi^{\\star}\\|\\left(\\mathbb{E}_{p_{\\psi}}\\phi_{i}^{4}-\\left(\\mathbb{E}_{p_{\\psi}}\\phi_{i}^{2}\\right)^{2}\\right)^{1/2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We map the two moments to derivatives of $\\log Z(\\psi)$ ,sincethe $k^{t h}$ derivative of $\\log Z(\\psi)$ is the $k^{t h}$ cumulant. It can be shown, using the multivariate moment to cumulant mapping, that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\mathbb{E}\\phi_{i}^{4}=\\frac{\\partial\\log Z^{4}}{\\partial\\psi_{i}}^{2}+6\\frac{\\partial\\log Z^{2}}{\\partial\\psi_{i}}^{2}\\frac{\\partial^{2}\\log Z}{\\partial\\psi_{i}^{2}}+3\\left(\\frac{\\partial^{2}\\log Z}{\\partial\\psi_{i}^{2}}\\right)^{2}+4\\frac{\\partial\\log Z}{\\partial\\psi_{i}}\\frac{\\partial^{3}\\log Z}{\\partial\\psi_{i}^{3}}+\\frac{\\partial^{4}\\log Z}{\\partial\\psi_{i}^{4}}\\ ~}}\\\\ {{\\displaystyle\\qquad=\\partial_{i}^{1}\\log Z(\\psi)^{4}+6\\partial_{i}^{1}\\log Z(\\psi)^{2}\\partial_{i}^{2}\\log Z(\\psi)+3\\partial_{i}^{2}\\log Z(\\psi)^{2}+4\\partial_{i}^{1}\\log Z(\\psi)\\partial_{i}^{3}\\log Z(\\psi)}}\\\\ {{\\displaystyle\\qquad+\\,\\partial_{i}^{4}\\log Z(\\psi)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\partial_{i}^{k}\\log Z(\\psi)$ denotes the $k^{t h}$ derivative of $\\log{Z}$ with respect to $\\psi_{i}$ . On the other hand, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathbb{E}\\phi_{i}^{2}=\\partial_{i}^{1}\\log Z(\\psi)^{2}+\\partial_{i}^{2}\\log Z(\\psi)}\\\\ &{}&{\\implies(\\mathbb{E}\\phi_{i}^{2})^{2}=\\partial_{i}^{1}\\log Z(\\psi)^{4}+2\\partial_{i}^{1}\\log Z(\\psi)^{2}\\partial_{i}^{2}\\log Z(\\psi)+\\partial_{i}^{2}\\log Z(\\psi)^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "implying ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{p_{\\psi}}\\phi_{i}^{4}-(\\mathbb{E}_{p_{\\psi}^{2}}\\phi_{i}^{2})^{2}=4\\partial_{i}^{1}\\log Z(\\psi)^{2}\\partial_{i}^{2}\\log Z(\\psi)+2\\partial_{i}^{2}\\log Z(\\psi)^{2}+4\\partial_{i}^{1}\\log Z(\\psi)\\partial_{i}^{3}\\log Z(\\psi)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\,\\partial_{i}^{4}\\log Z(\\psi)}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The result follows by summing over $i$ since: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\mathbb{E}_{p_{\\psi}\\star}P_{\\psi}^{m}||\\phi||^{2}-\\mathbb{E}_{p_{\\psi}}||\\phi||^{2}|\\;\\le\\;\\sum_{i=1}^{d}|\\mathbb{E}_{p_{\\psi}\\star}P_{\\psi}^{m}\\phi_{i}^{2}-\\mathbb{E}_{p_{\\psi}}\\phi_{i}^{2}|\\;}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Note that $\\|\\log Z\\|_{1,\\infty}$ is finite since $\\Psi$ is compact and $\\log{Z}$ is analytic. ", "page_idx": 24}, {"type": "text", "text": "Squared First Moment convergence  The next lemma provides convergence (in squared absolute value) of the first moment of the $m$ -iterated markov kernel $k_{\\psi}^{m}$ ", "page_idx": 24}, {"type": "text", "text": "Lemma D.2. Under A1, A2 and A3, for all $\\psi\\in\\Psi$ wehave ", "page_idx": 24}, {"type": "equation", "text": "$$\n|\\mathbb{E}_{p_{\\psi}\\star}\\|P_{\\psi}^{m}\\phi\\|^{2}-\\|\\mathbb{E}_{p_{\\psi}\\phi}\\|^{2}|\\le\\alpha^{m}\\sigma_{\\psi}^{2}+C_{\\chi}\\alpha^{m/4}\\left\\|\\log Z\\right\\|_{2,\\infty}\\|\\psi-\\psi^{\\star}\\|\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\log Z\\|_{2,\\infty}=\\underset{\\psi\\in\\Psi}{\\operatorname*{sup}}\\sum_{i=1}^{p}\\big(F(\\psi)\\partial_{i}^{2}\\log Z(\\psi)\\big)^{1/4}+2\\left|\\partial_{i}^{1}\\log Z(\\psi)\\partial_{i}^{2}\\log Z(\\psi)^{1/2}\\right|}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and ", "page_idx": 24}, {"type": "equation", "text": "$$\nF(\\psi):=15\\partial_{i}^{2}\\log Z(\\psi)^{3}+10\\partial_{i}^{3}\\log Z(\\psi)^{2}+15\\partial_{i}^{2}\\log Z(\\psi)\\partial_{i}^{4}\\log Z(\\psi)+\\partial_{i}^{6}\\log Z(\\psi)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. We have: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{p_{\\psi}^{\\star}}(P_{\\psi}^{m}\\phi_{i})^{2}=\\mathbb{E}_{p_{\\psi}^{\\star}}(P_{\\psi}^{m}\\phi_{i}-\\mathbb{E}_{\\psi}\\phi_{i}+\\mathbb{E}_{\\psi}\\phi_{i})^{2}}\\\\ &{\\phantom{=}=\\mathbb{E}_{p_{\\psi}^{\\star}}(P^{m}\\phi_{i}-\\mathbb{E}_{p_{\\psi}}\\phi_{i})^{2}+2\\mathbb{E}_{\\psi}\\phi_{i}\\times\\mathbb{E}_{\\psi}\\mathbf{\\cdot}P^{m}(\\phi_{i}-\\mathbb{E}_{\\psi}\\phi_{i})}\\\\ &{\\phantom{=}\\;+(\\mathbb{E}_{\\psi}\\phi_{i})^{2}}\\\\ &{\\Longrightarrow\\;|\\mathbb{E}_{p_{\\psi}^{\\star}}(P_{\\psi}^{m}\\phi_{i})^{2}-(\\mathbb{E}_{\\psi}\\phi_{i})^{2}|\\leq\\underbrace{|\\mathbb{E}_{p_{\\psi}^{\\star}}(P^{m}(\\phi_{i}-\\mathbb{E}_{p_{\\psi}}\\phi_{i}))^{2}|}_{\\Delta_{1}}+2|\\mathbb{E}_{\\psi}\\phi_{i}\\mathbb{E}_{\\psi}\\mathbf{\\cdot}P^{m}(\\phi_{i}-\\mathbb{E}_{\\psi}\\phi_{i})|}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\lambda_{1}=\\underbrace{\\mathbb{E}_{p_{\\psi}}(P_{\\psi}^{m}(\\phi_{i}-\\mathbb{E}_{\\psi}\\phi_{i}))^{2}}_{\\Delta_{1,1}}+\\underbrace{(\\mathbb{E}_{p_{\\psi}}\\langle P_{\\psi}^{m}\\phi_{i}-\\mathbb{E}_{\\psi}\\phi_{i}\\rangle^{2}-\\mathbb{E}_{p_{\\psi}}(P_{\\psi}^{m}\\phi_{i}-\\mathbb{E}_{\\psi}\\phi_{i})^{2})}_{\\Delta_{1,2}}}\\\\ &{\\quad\\le\\alpha^{2m}\\mathbb{E}_{\\psi}(\\phi_{i}-\\mathbb{E}_{\\psi}\\phi_{i})^{2}+C_{\\chi}\\left\\|\\psi-\\psi^{\\star}\\right\\|\\left(\\mathbb{E}_{p_{\\psi}}(P^{m}(\\phi_{i}-\\mathbb{E}_{p_{\\psi}}\\phi_{i})^{4}\\right)^{1/2}}\\\\ &{\\quad\\le\\alpha^{2m}\\mathbb{E}_{\\psi}(\\phi_{i}-\\mathbb{E}_{\\psi}\\phi_{i})^{2}+C_{\\chi}\\left\\|\\psi-\\psi^{\\star}\\right\\|\\left(\\mathbb{E}_{p_{\\psi}}P^{m}(\\phi_{i}-\\mathbb{E}_{p_{\\psi}}\\phi_{i})^{2}\\right)^{1/4}\\left(\\mathbb{E}_{p_{\\psi}}P^{m}(\\phi_{i}-\\mathbb{E}_{p_{\\psi}}\\phi_{i})^{6}\\right)^{1/4}}\\\\ &{\\quad\\le\\alpha^{2m}\\mathbb{E}_{\\psi}(\\phi_{i}-\\mathbb{E}_{\\psi}\\phi_{i})^{2}+\\alpha^{m/2}C_{\\chi}\\left\\|\\psi-\\psi^{\\star}\\right\\|\\left(\\mathbb{E}_{p_{\\psi}}(\\phi_{i}-\\mathbb{E}_{p_{\\psi}}\\phi_{i})^{2}\\right)^{1/4}\\left(\\mathbb{E}_{p_{\\psi}}(\\phi_{i}-\\mathbb{E}_{p_{\\psi}}\\phi_{i})^{6}\\right)^{1/4}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where we used the fact that $P_{\\psi}^{m}$ is a contraction in $L_{p_{\\psi}}^{6}(\\mathbb R^{d})$ and the $L_{p_{\\psi}}^{2}(\\mathbb R^{d}){-}\\alpha$ spectral gap of $P_{\\psi}^{m}$ .As an aside, note that a simpler result can be obtained by making regularity assumption on the mapping $\\psi\\longmapsto P_{\\psi}^{m}$ . Assuming that $\\psi\\longmapsto P_{\\psi}^{m}(x)$ is uniformly $L_{m}$ -Lipschitz across $x\\in\\mathscr{X}$ for instance (as done in [21, Assumption 5]), the second term $\\Delta_{1,2}$ of $\\Delta_{1}$ could have been handled using ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta_{1,2}\\leq2\\mathbb{E}_{p_{\\psi}\\star}\\left\\|P_{\\psi}^{m}\\phi-\\mathbb{E}_{\\psi^{\\star}}\\phi\\right\\|^{2}+2\\left\\|\\mathbb{E}_{p_{\\psi}\\phi}-\\mathbb{E}_{p_{\\psi}\\star}\\phi\\right\\|^{2}}\\\\ &{\\qquad\\leq4(L_{m}\\left\\|\\psi-\\psi^{\\star}\\right\\|+\\sigma_{\\star}^{2}\\alpha^{2m}+2\\left\\|\\mathbb{E}_{p_{\\psi}\\phi}-\\mathbb{E}_{p_{\\psi}\\star}\\phi\\right\\|^{2})}\\\\ &{\\qquad\\leq4(L_{m}\\left\\|\\psi-\\psi^{\\star}\\right\\|+\\sigma_{\\star}^{2}\\alpha^{2m}+2L^{2}\\left\\|\\psi-\\psi^{\\star}\\right\\|)}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Although this result does not require possibly large constants related to sixth-order moments, it is less tight in the sense that it does not go to O as $m\\rightarrow\\infty$ . Back to the main proof, and $\\Delta_{2}$ in particular. Applying Lemma C.7 to $f:=\\phi$ ,wehave ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\Delta_{2}\\leq\\alpha^{m}\\mathbb{E}_{\\psi}\\phi_{i}C_{\\chi}\\left\\|\\psi-\\psi^{\\star}\\right\\|\\left(\\mathbb{E}_{p_{\\psi}}(\\phi_{i}-\\mathbb{E}_{p_{\\psi}}\\phi_{i})^{2}\\right)^{1/2}}}\\\\ {{\\leq\\alpha^{m}C_{\\chi}\\partial_{i}^{1}\\log Z(\\psi)\\partial_{i}^{2}\\log Z(\\psi)^{1/2}\\left\\|\\psi-\\psi^{\\star}\\right\\|}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Putting everything together, we have: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{p_{\\psi}*}(P_{\\psi}^{m}\\phi_{i})^{2}-\\mathbb{E}_{p_{\\psi}}\\phi_{i}^{2}|\\,\\le\\alpha^{2m}\\mathbb{E}_{p_{\\psi}}(\\phi_{i}-\\mathbb{E}_{p_{\\psi}}\\phi_{i})^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\,C_{\\chi}\\,\\|\\psi-\\psi^{*}\\|\\,\\Big(\\alpha^{m/2}(\\mathbb{E}_{p_{\\psi}}(\\phi_{i}-\\log\\phi_{i})^{6})^{1/4}(\\mathbb{E}_{p_{\\psi}}(\\phi_{i}-\\log\\phi_{i})^{2})^{1/2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\,2\\alpha^{m}\\partial_{i}^{1}\\log Z(\\psi)\\partial_{i}^{2}\\log Z(\\psi)^{1/2}\\Big)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\quad\\le\\alpha^{2m}\\mathbb{E}_{p_{\\psi}}(\\phi_{i}-\\mathbb{E}_{p_{\\psi}}\\phi_{i})^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\,C_{\\chi}\\alpha^{m/2}\\,\\|\\psi-\\psi^{*}\\|\\,\\Big((\\mathbb{E}_{p_{\\psi}}(\\phi_{i}-\\log\\phi_{i})^{6})^{1/4}(\\mathbb{E}_{p_{\\psi}}(\\phi_{i}-\\log\\phi_{i})^{2})^{1/2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\,2\\left|\\partial_{i}^{1}\\log Z(\\psi)\\partial_{i}^{2}\\log Z(\\psi)^{1/2}\\right|\\Big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Summing over $i$ , we obtain ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\mathbb{E}_{p_{\\psi}*}\\|P_{\\psi}^{m}\\phi\\|^{2}-(\\mathbb{E}_{p_{\\psi}}\\phi)^{2}|\\leq\\sum_{i=1}^{p}|\\mathbb{E}_{p_{\\psi}*}(P_{\\psi}^{m}\\phi_{i})^{2}-(\\mathbb{E}_{p_{\\psi}}\\phi_{i})^{2}|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\alpha^{2m}\\sum_{i=1}^{p}\\mathbb{E}_{p_{\\psi}}(\\phi_{i}-\\mathbb{E}_{p_{\\psi}}\\phi_{i})^{2}+C_{\\chi}\\alpha^{m/2}\\|\\log Z\\|_{2,\\infty}\\,\\|\\psi-\\psi^{\\star}\\|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\alpha^{2m}\\sigma_{\\psi}^{2}+C_{\\chi}\\alpha^{m/2}\\,\\|\\log Z\\|_{2,\\infty}\\,\\|\\psi-\\psi^{\\star}\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\log Z\\right\\|_{2,\\infty}=\\operatorname*{sup}_{\\psi\\in\\Psi}\\sum_{i=1}^{p}\\left(\\mathbb{E}_{p_{\\psi}}(\\phi_{i}-\\mathbb{E}_{p_{\\psi}}\\phi_{i})^{6}\\right)^{1/4}\\left(\\mathbb{E}_{p_{\\psi}}(\\phi_{i}-\\mathbb{E}_{p_{\\psi}}\\phi_{i})^{2}\\right)^{1/4}}\\\\ &{\\qquad\\qquad\\qquad+\\,2\\left|\\partial_{i}^{1}\\log Z(\\psi)\\partial_{i}^{2}\\log Z(\\psi)^{1/2}\\right|}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Similarily to the previous lemma, one can upper bound $\\mathbb{E}(\\phi_{i}-\\mathbb{E}_{p_{\\psi}}\\phi_{i})^{6}$ using the centered moment to cumulant formula: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{\\bar{c}}_{p_{\\psi}}(\\phi_{i}-\\mathbb{E}\\phi_{i})^{6}=15\\partial_{i}^{2}\\log Z(\\psi)^{3}+10\\partial_{i}^{3}\\log Z(\\psi)^{2}+15\\partial_{i}^{2}\\log Z(\\psi)\\partial_{i}^{4}\\log Z(\\psi)+\\partial_{i}^{6}\\log Z(\\psi)}\\\\ &{\\qquad\\qquad\\qquad=:F(\\psi)}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "To get a full description of $\\|\\log Z\\|_{2,\\infty}$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\lVert\\log Z\\right\\rVert_{2,\\infty}=\\operatorname*{sup}_{\\psi\\in\\Psi}\\sum_{i=1}^{p}\\left(F(\\psi)\\partial_{i}^{2}\\log Z(\\psi)\\right)^{1/4}+2\\left|\\partial_{i}^{1}\\log Z(\\psi)\\partial_{i}^{2}\\log Z(\\psi)^{1/2}\\right|\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We can now use the previous lemmas to obtain an expression on the second moment of the contrastive divergence gradient estimator, relating it to the one of the stochastic log-likelihood gradient estimator. Lemma D.3.Under A1.A2 and $A3$ wehave: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{;\\|h_{t}(\\psi,X_{t})\\|^{2}\\leq2\\sigma_{\\star}^{2}+2\\sigma_{\\psi}^{2}+2L^{2}\\,\\|\\psi-\\psi^{\\star}\\|^{2}+4(\\sigma_{\\psi}^{2}\\alpha^{2m}+\\alpha^{m/2}\\,\\|\\log Z\\|_{3,\\infty}\\,C_{X}\\,\\|\\psi-\\psi^{\\star}\\|)}\\\\ &{\\hphantom{;{\\quad}}\\cdot h e r e\\,\\|\\log Z\\|_{3,\\infty}:=2\\operatorname*{max}\\big(\\|\\log Z\\|_{1,\\infty}\\,,\\|\\log Z\\|_{2,\\infty}\\big).}\\end{array}\n$$W ", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. We rely on the following decomposition: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{h_{t}(\\psi,X_{t})\\,=\\,\\underbrace{\\big(\\phi(X_{t})-\\mathbb{E}_{p_{\\psi},\\,\\phi}\\big)}_{(1a)}+\\underbrace{\\big(\\mathbb{E}_{p_{\\psi},\\,\\phi}-\\mathbb{E}_{p_{\\psi}}\\phi\\big)}_{(1b)}+\\underbrace{\\big(\\mathbb{E}_{k_{\\psi}^{m}(X_{t},\\cdot)}\\phi-\\phi\\big(k_{\\psi}^{m}(X_{t},\\cdot)\\big)\\big)}_{(2)}}\\\\ &{\\quad+\\underbrace{\\big(\\mathbb{E}_{p_{\\psi}}\\phi-\\mathbb{E}_{k_{\\psi}^{m}(X_{t},\\cdot)}\\phi\\big)}_{(3)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "$(1a)+(1b)$ form the differentiable stochastic gradient $g_{t}$ of Equation 6. Note that (la) is meanzero, and (2) is mean-zero conditionally on $X_{t}$ . Consequently, $\\mathbb{E}\\left\\langle(2),(3)\\right\\rangle\\;=\\;\\mathbb{E}\\left\\langle(2),(1)\\right\\rangle\\;=$ $\\mathbb{E}\\left\\langle(1a),(1b)\\right\\rangle\\,=\\,\\mathbb{E}\\left\\langle(1a),(2)\\right\\rangle\\,=\\,0$ , and the only mixed-terms that remain to be controlled are $\\mathbb{E}\\left\\langle(1a),(3)\\right\\rangle$ and $\\mathbb{E}\\left\\langle(1b),(3)\\right\\rangle$ . We first control the unmixed terms, and the simple ones first: we have $\\mathbb{E}\\left\\|(1a)\\right\\|^{2}=\\sigma_{\\star}^{2}$ , as well as ${\\mathbb{E}\\left\\|(1b)\\right\\|^{2}\\leq L^{2}\\left\\|\\psi-\\psi^{\\star}\\right\\|^{2}}$ . For (2), we have: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{k_{\\psi}^{m}(x,\\cdot)}\\left\\|(2)\\right\\|^{2}=\\mathbb{E}_{k_{\\psi}^{m}(x,\\cdot)}\\|\\phi(k_{\\psi}^{m}(x,\\cdot))\\|^{2}-\\|\\mathbb{E}_{k_{\\psi}^{m}(x,\\cdot)}\\phi(k_{\\psi}^{m}(x,\\cdot))\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad=P_{\\psi}^{m}\\|\\phi(x)\\|^{2}-\\|P_{\\psi}^{m}\\phi(x)\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We can invoke Lemmas D.2 and D.1, which guarantee ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\mathbb{E}_{p_{\\psi}\\star}\\|P_{\\psi}^{m}\\phi\\|^{2}-\\left\\|\\mathbb{E}_{p_{\\psi}\\phi}\\phi\\right\\|^{2}|\\le\\alpha^{2m}\\sigma_{\\psi}^{2}+C_{\\chi}\\alpha^{m/2}\\left\\|\\log Z\\right\\|_{2,\\infty}\\|\\psi-\\psi^{\\star}\\|}\\\\ &{|\\mathbb{E}_{p_{\\psi}\\star}P_{\\psi}^{m}\\|\\phi\\|^{2}-\\mathbb{E}_{p_{\\psi}}\\|\\phi\\|^{2}|\\le\\alpha^{m}C_{\\chi}\\left\\|\\log Z\\right\\|_{1,\\infty}\\|\\psi-\\psi^{\\star}\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "to obtain ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{k_{\\psi}^{m}(x,\\cdot)}\\left\\|(2)\\right\\|^{2}=\\mathbb{E}_{p_{\\psi}}\\left\\|\\phi\\right\\|^{2}-\\left\\|\\mathbb{E}\\phi\\right\\|^{2}+\\alpha^{2m}\\sigma_{\\psi}^{2}+C_{\\chi}\\alpha^{m/2}\\left(\\left\\|\\log Z\\right\\|_{1,\\infty}+\\left\\|\\log Z\\right\\|_{2,\\infty}\\right)\\left\\|\\psi-\\psi^{\\star}\\right\\|}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\mathbb{E}\\left\\|\\phi-\\mathbb{E}\\phi\\right\\|^{2}+\\alpha^{2m}\\sigma_{\\psi}^{2}+\\alpha^{m/2}\\left\\|\\log Z\\right\\|_{3,\\infty}C_{\\chi}\\left\\|\\psi-\\psi^{\\star}\\right\\|}\\\\ &{\\mathrm{~re~}\\|\\log Z\\|_{3,\\infty}:=2\\operatorname*{max}(\\left\\|\\log Z\\right\\|_{1,\\infty},\\left\\|\\log Z\\right\\|_{2,\\infty}).}\\end{array}\n$$whe ", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "For (3), notice that (3) is precisely the term $\\Delta_{1}$ in Lemma D.2, and we can thus bound it by ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{E}_{p_{\\psi^{\\star}}}\\left\\|(3)\\right\\|^{2}\\leq\\sigma_{\\psi}^{2}\\alpha^{2m}+\\alpha^{m/2}\\left\\|\\log Z\\right\\|_{2,\\infty}C_{\\chi}\\left\\|\\psi-\\psi^{\\star}\\right\\|\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Finally, we simply bound $2\\mathbb{E}\\left\\langle(1a),(3)\\right\\rangle$ by $\\mathbb{E}\\left\\|(1a)\\right\\|^{2}+\\mathbb{E}\\left\\|(3)\\right\\|^{2}$ , and $2\\mathbb{E}\\left\\langle(1b),(3)\\right\\rangle$ by $\\mathbb{E}\\left\\|(1b)\\right\\|^{2}+$ $\\mathbb{E}\\left\\|(3)\\right\\|^{2}$ . Putting everything together, we have: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left\\|h_{t}(\\psi)\\right\\|^{2}=\\mathbb{E}\\left\\|(1a)\\right\\|^{2}+\\mathbb{E}\\left\\|(1b)\\right\\|^{2}+\\mathbb{E}\\left\\|(2)\\right\\|^{2}+\\mathbb{E}\\left\\|(3)\\right\\|^{2}+2\\mathbb{E}\\left\\langle(1a),(3)\\right\\rangle+2\\mathbb{E}\\left\\langle(1b),(3)\\right\\rangle}\\\\ &{\\qquad\\qquad\\leq2\\mathbb{E}\\left\\|(1a)\\right\\|^{2}+2\\mathbb{E}\\left\\|(1b)\\right\\|^{2}+\\mathbb{E}\\left\\|(2)\\right\\|^{2}+3\\mathbb{E}\\left\\|(3)\\right\\|^{2}}\\\\ &{\\qquad\\qquad\\leq2\\sigma_{\\star}^{2}+2L^{2}\\left\\|\\psi-\\psi^{\\star}\\right\\|^{2}+\\sigma_{\\psi}^{2}+4(\\sigma_{\\psi}^{2}\\alpha^{2m}+\\alpha^{m/2}\\left\\|\\log Z\\right\\|_{3,\\infty}C_{\\chi}\\left\\|\\psi-\\psi^{\\star}\\right\\|)}\\\\ &{\\qquad\\qquad\\leq2\\sigma_{\\star}^{2}+2\\sigma_{\\psi}^{2}+2L^{2}\\left\\|\\psi-\\psi^{\\star}\\right\\|^{2}+4(\\sigma_{\\psi}^{2}\\alpha^{2m}+\\alpha^{m/2}\\left\\|\\log Z\\right\\|_{3,\\infty}C_{\\chi}\\left\\|\\psi-\\psi^{\\star}\\right\\|)}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "D.2  Proof of the SGD recursion (Lemma 3.1) ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We are now ready to provide a SGD-style recursion for the expected squared distance to the optimum $\\mathbb{E}\\left\\|\\psi_{t}-\\psi^{\\star}\\right\\|^{2}$ ", "page_idx": 26}, {"type": "text", "text": "Lemma (Restatement of Lemma 3.1). Let $\\psi_{t}$ be the iterates produced by Algorithm 1. Denote $\\delta_{t}=\\mathbb{E}\\|\\psi_{t}-\\psi^{\\star}\\|^{2}$ \uff0c $\\sigma_{\\star}=(\\mathbb{E}_{p_{\\psi}\\star}\\|\\phi-\\mathbb{E}_{p_{\\psi}\\star}\\phi\\|^{2})^{1/2}$ , and $\\sigma_{t}=(\\mathbb{\\dot{E}}_{p_{\\psi_{t}}}\\|\\phi-\\mathbb{E}_{p_{\\psi_{t}}}^{\\cdot}\\phi\\|^{2})^{1/2}$ . Then, under Assumptions A1, A2 and $A3$ for all $t\\geq1$ , we have: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\delta_{t}\\leq(1-2\\eta_{t}\\tilde{\\mu}_{m,t-1}+2\\eta_{t}^{2}L^{2})\\delta_{t-1}+\\eta_{t}^{2}\\tilde{\\sigma}_{m,t-1}^{2}+4\\alpha^{m/2}\\eta_{t}^{2}\\left\\lVert\\log Z\\right\\rVert_{2,\\infty}C_{\\chi}\\delta_{t-1}^{1/2}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $\\|\\log Z\\|_{3,\\infty}$ is a constant, $\\tilde{\\mu}_{m,t}:=\\mu-\\alpha^{m}\\sigma_{t}C_{\\chi}$ and $\\tilde{\\sigma}_{m,t}:=(\\sigma_{\\star}^{2}+\\sigma_{t}^{2}+2\\sigma_{t}^{2}\\alpha^{2m})^{1/2}.$ ", "page_idx": 26}, {"type": "text", "text": "Proof. In this proof, we note $(\\mathcal{F}_{t})_{t\\geq0}$ , the increasing family of $\\sigma$ -algebras generated by the random variables $(X_{t})_{t\\geq0}\\sim p_{\\psi^{\\star}}$ and the markov chain samples $\\tilde{X}_{t}^{m}|X_{t},\\psi_{t}\\sim k_{\\psi_{t}}^{m}(X_{t},\\cdot)$ . We decompose ", "page_idx": 26}, {"type": "text", "text": "the integrand of $\\delta_{t}$ as follows: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\psi_{t}-\\psi_{n}^{\\star}\\|^{2}=\\|\\mathrm{Proj}_{\\Psi}\\big(\\psi_{t-1}-\\eta_{t}h_{t}\\big(\\psi_{t-1}\\big)\\big)-\\psi_{n}^{\\star}\\|^{2}}\\\\ &{\\qquad\\qquad\\leq\\|\\psi_{t-1}-\\eta_{t}h_{t}\\big(\\psi_{t-1}\\big)-\\psi_{n}^{\\star}\\|^{2}\\quad({\\mathrm{By~Lemma~C.4}})}\\\\ &{\\qquad\\qquad=\\|\\psi_{t-1}-\\psi^{\\star}\\|^{2}-2\\eta_{t}\\,\\big\\langle h_{t}\\big(\\psi_{t-1}\\big),\\psi_{t-1}-\\psi_{n}^{\\star}\\big\\rangle+\\eta_{t}^{2}\\,\\|h_{t}\\big(\\psi_{t-1}\\big)\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The first term is (up to an averaging operation) the previous iterate. The middle term will ensure (provided $m$ is large enough) contraction of the expected distance to the optimum. Finally, the third term can be described by Lemma D.3, and anssentially behaves like the second moment of a log-likelihood stochastic gradient. Indeed, noting $g(\\psi):=-\\mathbb{E}_{p_{\\psi}\\star}\\phi+\\mathbb{E}_{p_{\\psi}\\phi}$ the expectation of $g_{t}$ w.r.t $x_{t}$ (which is the gradient of the negative cross-entropy between $p_{\\psi}$ and $p_{\\psi^{\\star}}$ ), we have: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\left\\langle h_{t}(\\psi_{n,t-1}),\\psi_{t-1}-\\psi_{n}^{\\star}\\right\\rangle=\\left\\langle g(\\psi_{n,t-1}),\\psi_{t-1}-\\psi_{n}^{\\star}\\right\\rangle+\\underbrace{\\left\\langle h_{t}(\\psi_{n,t-1})-g(\\psi_{n,t-1})\\right\\rangle,\\psi_{t-1}-\\psi_{n}^{\\star}\\right\\rangle}_{\\Delta}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and applying Lemma C.7, we get that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{h_{t}(\\psi_{t-1})-g(\\psi_{t-1})=\\phi\\big(k^{m}(X_{t},\\cdot)\\big)-\\mathbb{E}_{\\psi}\\phi}\\\\ {\\implies\\mathbb{E}_{\\psi^{\\star}}\\mathbb{E}_{k_{\\psi}^{m}(X_{t},\\cdot)}\\left[h_{t}(\\psi_{t-1})-g(\\psi_{t-1})|\\mathcal{F}_{t-1}\\right]=P_{\\psi}^{m}\\phi-\\mathbb{E}_{\\psi}\\phi\\ ,\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "meaning ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\mathbb{E}\\left[\\Delta|\\mathcal{F}_{t-1}\\right]|\\leq\\left\\|\\mathbb{E}_{p_{\\psi^{\\star}}}P_{\\psi}^{m}(\\phi-\\mathbb{E}_{\\psi}\\phi)\\right\\|\\left\\|\\psi_{t-1}-\\psi^{\\star}\\right\\|}\\\\ &{\\qquad\\qquad\\leq\\alpha^{m}C_{\\chi}(\\mathbb{E}_{p_{\\psi_{t-1}}}\\left\\|f-\\mathbb{E}_{p_{\\psi_{t-1}}}f\\right\\|^{2})^{1/2}\\left\\|\\psi-\\psi^{\\star}\\right\\|^{2}}\\\\ &{\\qquad\\qquad\\leq\\alpha^{m}\\sigma_{t-1}C_{\\chi}\\left\\|\\psi-\\psi^{\\star}\\right\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "On the other hand, by applying Lemma C.8 to $g$ ,we have: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\langle g(\\psi_{t-1}),\\psi_{t-1}-\\psi^{\\star}\\rangle=\\langle g(\\psi_{t-1})-g(\\psi^{\\star}),\\psi_{t-1}-\\psi^{\\star}\\rangle\\geq\\mu\\left\\|\\psi_{t-1}-\\psi^{\\star}\\right\\|^{2}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Combining the above results, we obtain: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{L}_{p_{\\psi}*}\\mathbb{E}_{k_{\\psi}^{m}(x,\\cdot)}\\left[\\left\\|\\psi_{t}-\\psi^{\\star}\\right\\||\\mathcal{F}_{t-1}\\right]}\\\\ &{\\leq(1-2\\eta_{t}(\\mu-\\alpha^{m}\\sigma_{t-1}C_{\\chi}))\\|\\psi_{t-1}-\\psi^{\\star}\\|^{2}}\\\\ &{\\qquad+\\eta_{t}^{2}(2\\sigma_{\\star}^{2}+2\\sigma_{t-1}^{2}+2L^{2}\\left\\|\\psi_{t-1}-\\psi^{\\star}\\right\\|^{2}+4(\\sigma_{t-1}^{2}\\alpha^{2m}+\\alpha^{m/2}\\left\\|\\log Z\\right\\|_{3,\\infty}C_{\\chi}\\left\\|\\psi_{t-1}-\\psi^{\\star}\\right\\|)}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "And the result follows by integrating over $\\mathcal{F}_{t-1}$ ", "page_idx": 27}, {"type": "text", "text": "D.3Proof of Online CD convergence ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "We now prove Theorem 3.2. The recursion of Lemma 3.1 is (almost) identifiable, up to a crossterm of second order, with the one of an SGD algorithm as presented in the setting of [22]. To make the identification exact, we use the bound $4\\alpha^{m/2}\\eta_{t}^{2}\\,\\|\\log Z\\|_{3,\\infty}\\,C_{\\chi}\\delta_{t-1}^{1/2}\\;\\le\\;\\overline{{2}}\\alpha^{m/2}\\eta_{t}^{2}\\delta_{t}\\;+$ $2\\alpha^{m/2}\\eta_{t}^{2}\\left\\|\\log Z\\right\\|_{3,\\infty}^{2}C_{\\chi}^{2}$ yielding the following recursion: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\dot{\\iota}\\le(1-2\\eta_{t}(\\mu-\\alpha^{m}\\sigma C_{\\chi})+2\\eta_{t}^{2}(L^{2}+\\alpha^{m/2}))\\delta_{t-1}+(\\sigma^{2}(2+2\\alpha^{2m})+\\alpha^{m/2}\\left\\lVert\\log Z\\right\\rVert_{3,\\infty}^{2}C_{\\chi}^{2})\\eta_{t}^{2}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where we used the fact that $\\sigma_{m,t}\\leq\\sigma$ . We can use arguments existing in prior work [22, Theorem 1], noting that the only condition required to proceed is that $\\mu-\\alpha^{m}\\sigma C_{\\chi}\\,<\\,(L^{2}+1)^{1/2}$ ,which automatically holds since $\\mu<L$ . We then obtain, noting $\\tilde{\\mu}_{m}=\\mu-\\alpha^{m}\\sigma C_{\\chi}$ $\\tilde{\\sigma}_{m}=\\sigma^{2}(2+2\\alpha^{2m})+$ $\\alpha^{m/2}\\left\\|\\log Z\\right\\|_{3,\\infty}^{2}C_{\\chi}^{2}$ and $\\tilde{L}=(L^{2}+\\alpha^{m/2})^{1/2}$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\delta_{n}\\leqslant\\left\\{\\begin{array}{l l}{2\\exp\\left(4\\tilde{L}C^{2}\\varphi_{1-2\\beta}(n)\\right)\\exp\\left(-\\frac{\\tilde{\\mu}_{m}C}{4}n^{1-\\beta}\\right)\\left(\\delta_{0}+\\frac{\\sigma^{2}}{L^{2}}\\right)+\\frac{4C\\tilde{\\sigma}_{m}^{2}}{\\mu n^{\\beta}},}&{\\mathrm{~if~}0\\leqslant\\beta<1}\\\\ {\\frac{\\exp\\left(2\\tilde{L}^{2}C^{2}\\right)}{n^{\\beta C}}\\left(\\delta_{0}+\\frac{\\tilde{\\sigma}_{m}^{2}}{\\tilde{L}^{2}}\\right)+2\\tilde{\\sigma}_{m}^{2}C^{2}\\frac{\\varphi_{\\tilde{\\mu}_{m}C/2-1}(n)}{n^{\\tilde{\\mu}_{m}C/2}},}&{\\mathrm{~if~}\\beta=1.}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "D.4 Proof of online CD with averaging (Theorem 3.3) ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We start by three intermediate lemmas. ", "page_idx": 28}, {"type": "text", "text": "Lemma D.4. Under Assumptions Al, $A2$ A3, the online $C D$ iterates produced by Algorithm 1 using $\\eta_{t}=C t^{-\\beta}$ for $\\beta\\in(\\frac{1}{2},1)$ verify ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{n}\\left(\\sum_{i=1}^{n}\\mathbb{E}\\left\\|\\psi_{i}-\\psi^{\\star}\\right\\|^{1/2}\\right)^{1/2}\\leq P_{3}(n)}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $P_{3}(n)$ is a term of order $n^{-\\frac{1}{2}-\\frac{\\beta}{4}}$ ", "page_idx": 28}, {"type": "text", "text": "Proof. Let us note $\\delta_{n}=\\mathbb{E}\\left\\|\\psi_{n}-\\psi^{\\star}\\right\\|^{2}$ For the second term, specific to our setting, we have, recalling that $\\delta_{n}$ is bounded thanks to Theorem 3.2, Recalling that $\\delta_{n}$ verifies the bound of Theorem 3.2, we have: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{n^{2}}\\sum_{i=1}^{n}\\delta_{i}^{1/2}\\ \\leq\\ \\frac{2C^{1/2}\\sigma}{2\\mu^{1/2}n^{2}}\\sum_{i=1}^{n}i^{-\\beta/2}+\\frac{\\sqrt{2}}{n^{2}}\\Big(\\delta_{0}+\\frac{\\tilde{\\sigma}_{m}^{2}}{\\tilde{L}^{2}}\\Big)\\underbrace{\\sum_{i=1}^{n}e^{2\\tilde{L}^{2}C^{2}\\varphi_{1-2\\beta}(i)}e^{-\\frac{\\tilde{\\mu}_{m}C}{8}i^{1-\\beta}}}_{A_{3}}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\ \\frac{2C^{1/2}\\tilde{\\sigma}_{m}}{n^{2}\\tilde{\\mu}_{m}^{1/2}}\\varphi_{1-\\beta/2}(n)+\\frac{\\sqrt{2}}{n^{2}}\\Big(\\delta_{0}+\\frac{\\tilde{\\sigma}_{m}^{2}}{L^{2}}\\Big)A_{3}}\\\\ &{\\Longrightarrow\\ \\frac{1}{n}\\Big(\\sum_{i=1}^{n}\\delta_{i}^{1/2}\\Big)^{1/2}\\leq\\frac{1}{n}\\Big(\\frac{2C^{1/2}\\tilde{\\sigma}_{m}}{\\tilde{\\mu}^{1/2}}\\varphi_{1-\\beta/2}(n)\\Big)^{1/2}+\\frac{1}{n}\\Big(\\sqrt{2}(\\delta_{0}+\\frac{\\sigma^{2}}{\\tilde{L}^{2}})A_{3}\\Big)^{1/2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $A_{3}$ is finite if $\\beta<1$ , and $A_{3}=O(n)$ otherwise [22]. ", "page_idx": 28}, {"type": "text", "text": "Lemma D.5. Under Assumptions Al, A2, A3, the online $C D$ iterates produced by Algorithm 1 using $\\eta_{t}=C t^{-\\beta}$ for $\\beta\\in(\\frac{1}{2},1)$ verify ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left(\\mathbb{E}\\right|\\left|\\frac{1}{n}\\sum_{i=1}^{n}h_{i}(\\psi_{i-1})\\right|\\right|^{2}\\right)^{1/2}\\leq P_{2}(n)}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "for some term $P_{2}$ of order at most $n^{{\\frac{\\beta}{2}}-1}$ (an order smaller than $n^{-{\\frac{1}{2}}}$ \u4eba ", "page_idx": 28}, {"type": "text", "text": "Proof. The result follows from the fact that $h_{t}$ verifies ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{h_{t}(\\psi_{t-1})\\ =\\ \\frac{1}{\\eta t}(\\psi_{t-1}-\\psi_{t})\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "A similar quantity was handled in the case of standard SGD [22, Theorem 3], and the only condition needed to reuse their steps is that $(\\psi_{t})_{t\\leq n}$ satisfies an upper bound of the same form as the one [22, Theorem 1] derived. This is precisely the nature of our bound of $\\psi_{t}$ estblished in Theorem 3.2, with $\\tilde{\\mu}_{m},\\tilde{\\sigma}_{m},\\tilde{L}$ . Borrowing on their result, we have: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\mathbb{E}\\bigg\\|\\frac{1}{n}\\sum_{i=1}^{n}h_{i}(\\psi_{i-1})\\bigg\\|^{2}\\right)^{1/2}\\leq\\frac{4\\tilde{\\sigma}_{m\\beta}}{C^{1/2}n\\tilde{\\mu}_{m}}\\varphi_{\\beta/2}(n)+\\frac{4\\beta}{C n\\tilde{\\mu}_{m}^{1/2}}\\Big(\\delta_{0}+\\frac{\\tilde{\\sigma}_{m}^{2}}{\\tilde{L}^{2}}\\Big)^{1/2}A_{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\frac{1}{n\\tilde{\\mu}_{m}^{1/2}}\\Big(\\frac{1}{C}+2\\tilde{L}\\Big)\\delta_{0}^{1/2}+\\frac{2\\tilde{L}}{n\\tilde{\\mu}_{m}^{1/2}}\\frac{2C^{1/2}\\tilde{\\sigma}_{m}}{\\tilde{\\mu}_{m}^{1/2}}\\varphi_{1-\\beta}(n)^{1/2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\frac{4\\tilde{L}}{n\\tilde{\\mu}_{m}^{1/2}}\\Big(\\delta_{0}+\\frac{\\tilde{\\sigma}_{m}^{2}}{\\tilde{L}^{2}}\\Big)^{1/2}A_{2}^{1/2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\tilde{\\mu}_{m},\\tilde{\\sigma}_{m}$ and $\\tilde{L}$ are defined in 3.2, and ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{A_{2}\\ =\\ \\sum_{k=1}^{n}e^{\\frac{-\\tilde{\\mu}C}{16}k^{1-\\beta}+16\\tilde{L}_{1}^{4}C^{4}\\varphi_{1-2\\beta}(k)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Lemma D.6. Under Assumptions A1, A2, A3, the online $C D$ iterates produced by Algorithm I using $\\eta_{t}=C t^{-\\beta}$ for $\\beta\\in(\\textstyle{\\frac{1}{2}},1)$ verify ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{n}\\sum_{i=1}^{n}(\\mathbb{E}\\left\\|\\psi_{i}-\\psi^{\\star}\\right\\|^{4})^{1/2}\\leq P_{1}(n)}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $P_{1}(n)$ is a term in n of order $O(n^{-\\beta})$ ", "page_idx": 28}, {"type": "text", "text": "Proof. We have: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\left[\\Vert\\psi_{n}-\\psi^{*}\\Vert^{4}\\;|\\;\\mathcal{F}_{n-1}\\right]\\leqslant\\Vert\\psi_{n-1}-\\psi^{*}\\Vert^{4}+6\\eta_{n}^{2}\\;\\Vert\\psi_{n-1}-\\psi^{*}\\Vert^{2}\\,\\mathbb{E}\\left[\\Vert h_{n}\\left(\\psi_{n-1}\\right)\\Vert^{2}\\;|\\;\\mathcal{F}_{n-1}\\right]}&{}\\\\ {+\\;\\eta_{n}^{4}\\mathbb{E}\\left[\\Vert h_{n}\\left(\\psi_{n-1}\\right)\\Vert^{4}\\;|\\;\\mathcal{F}_{n-1}\\right]}&{}\\\\ {-\\;4\\eta_{n}\\;\\Vert\\psi_{n-1}-\\psi^{*}\\Vert^{2}\\left\\langle\\psi_{n-1}-\\psi^{*},\\mathbb{E}h_{n}\\right\\rangle}&{}\\\\ {+\\;4\\eta_{n}^{3}\\;\\Vert\\psi_{n-1}-\\psi^{*}\\Vert\\,\\mathbb{E}\\left[\\Vert h_{n}\\left(\\psi_{n-1}\\right)\\Vert^{3}\\;|\\;\\mathcal{F}_{n-1}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "The second and fourth terms will be controlled using results from our previous sections. For simplicity, we don't attempt to relate the moments of $\\|h_{n}\\|^{4}$ as precisely as before. Instead we use: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{p_{\\psi^{\\star}}}\\left\\|h_{n}\\right\\|^{k}\\leq2^{k-1}(\\mathbb{E}_{p_{\\psi^{\\star}}}\\mathbb{E}_{k_{p_{\\psi_{n-1}}}^{m}}\\|k_{\\psi_{n-1}}^{m}(x_{n},\\cdot)\\|^{k}+\\mathbb{E}_{p_{\\psi^{\\star}}}\\|\\phi(x_{n})\\|^{k})}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "And let us note $\\tau=\\left(\\operatorname*{sup}_{\\psi\\in\\Psi}\\mathbb{E}\\left\\|\\phi\\right\\|^{8}\\right)^{1/8}$ . and we have, for $k\\geq4$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{p_{\\psi}*}\\mathbb{E}_{k_{p_{\\psi_{n-1}}}^{m}}\\|k_{\\psi_{n-1}}^{m}(x_{n},\\cdot)\\|^{k}\\le\\mathbb{E}_{p_{\\psi_{n-1}}}\\|\\phi\\|^{k}+C_{\\chi}(\\mathbb{E}(\\|\\phi\\|^{k}-\\mathbb{E}\\left\\|\\phi\\|^{k}\\right)^{2})^{1/2}\\|\\psi_{n-1}-\\psi^{\\star}\\|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\tau^{k}+2C_{\\chi}\\tau^{k}\\|\\psi_{n-1}-\\psi^{\\star}\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Where we used the fact that $P_{\\psi_{n-1}}^{m}$ is a contraction, and $\\big(\\mathbb{E}\\left\\|\\phi\\right\\|^{k}\\big)^{1/k}$ is an increasing function of $k$ On the other hand, we simply have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{E}_{p_{\\psi}\\star}\\|\\phi(x_{n},\\cdot)\\|^{k}\\leq\\tau^{k}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "plugging this into the previous equation, we obtain ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\left[\\left\\Vert\\psi_{n}-\\psi^{*}\\right\\Vert^{4}\\mid\\mathcal{F}_{n-1}\\right]\\leqslant\\left\\Vert\\psi_{n-1}-\\psi^{*}\\right\\Vert^{4}}&{}\\\\ &{\\quad+6\\eta_{n}^{2}\\left\\Vert\\psi_{n-1}-\\psi^{*}\\right\\Vert^{2}\\left(4\\tau^{2}+4C_{\\chi}\\tau^{2}\\left\\Vert\\psi_{n-1}-\\psi^{*}\\right\\Vert\\right)}\\\\ &{\\quad+\\eta_{n}^{4}(16\\tau^{4}+16C_{\\chi}\\tau^{4}\\left\\Vert\\psi_{n-1}-\\psi^{*}\\right\\Vert)}\\\\ &{\\quad-4\\eta_{n}\\left\\Vert\\psi_{n-1}-\\psi^{*}\\right\\Vert^{2}\\langle\\psi_{n-1}-\\psi^{*},\\mathbb{E}h_{n}\\rangle}\\\\ &{\\quad+4\\eta_{n}^{3}\\left\\Vert\\psi_{n-1}-\\psi^{*}\\right\\Vert\\left(8\\tau^{3}+8C_{\\chi}\\tau^{3}\\left\\Vert\\psi_{n-1}-\\psi^{*}\\right\\Vert\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "To simplify the recursion, we use the four following inequalities: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tau^{2}\\eta_{n}^{2}\\left\\Vert\\psi_{n-1}-\\psi^{\\star}\\right\\Vert^{3}\\leq\\frac{1}{2}(\\tau^{2}\\eta_{n}^{2}(\\left\\Vert\\psi_{n-1}-\\psi^{\\star}\\right\\Vert^{2}+\\tau^{2}\\eta_{n}^{2}\\left\\Vert\\psi_{n-1}-\\psi^{\\star}\\right\\Vert^{4})}\\\\ &{\\tau^{4}\\eta_{n}^{4}\\left\\Vert\\psi_{n-1}-\\psi^{\\star}\\right\\Vert\\leq(\\tau^{4}\\eta_{n}^{4}+\\frac{1}{4}\\tau^{4}\\eta_{n}^{4}\\left\\Vert\\psi_{n-1}-\\psi^{\\star}\\right\\Vert^{4}))}\\\\ &{\\eta_{n}^{3}\\tau^{3}\\left\\Vert\\psi_{n-1}-\\psi^{\\star}\\right\\Vert\\leq\\frac{1}{2}(\\eta_{n}^{2}\\tau^{2}\\left\\Vert\\psi_{n-1}-\\psi^{\\star}\\right\\Vert^{2}+16\\eta_{n}^{4}\\tau^{4}}\\\\ &{\\tau^{3}\\eta_{n}^{3}\\left\\Vert\\psi_{n-1}-\\psi^{\\star}\\right\\Vert^{2}\\leq\\frac{1}{2}(\\eta_{n}^{4}\\tau^{4}+\\left\\Vert\\psi_{n-1}-\\psi^{\\star}\\right\\Vert^{2}\\eta_{n}^{2}\\tau^{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Injecting them in our recursion, we obtain: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\left[\\Vert\\psi_{n}-\\psi^{*}\\Vert^{4}\\;|\\;\\mathcal{F}_{n-1}\\right]\\leq\\Vert\\psi_{n-1}-\\psi^{*}\\Vert^{4}}&{}\\\\ &{\\quad+12\\eta_{n}^{2}\\Vert\\psi_{n-1}-\\psi^{*}\\Vert^{2}\\;|\\tau^{2}}\\\\ &{\\quad+12\\chi_{n}\\tau^{2}\\eta_{n}^{2}\\Vert\\psi_{n-1}-\\psi^{*}\\Vert^{2}}\\\\ &{\\quad+12C_{X}\\tau^{2}\\eta_{n}^{2}\\Vert\\psi_{n-1}-\\psi^{*}\\Vert^{4}}\\\\ &{\\quad+16\\eta_{n}^{2}\\tau^{4}}\\\\ &{\\quad+16\\zeta\\chi_{n}^{2}\\tau^{4}}\\\\ &{\\quad+4C_{X}\\eta_{n}^{4}\\tau^{4}\\Vert\\psi_{n-1}-\\psi^{*}\\Vert^{4}}\\\\ &{\\quad-4\\eta_{n}\\tilde{\\mu}_{n}\\Vert\\psi_{n-1}-\\psi^{*}\\Vert^{4}}\\\\ &{\\quad+16\\eta_{n}^{2}\\tau^{2}\\Vert\\psi_{n-1}-\\psi^{*}\\Vert^{2}}\\\\ &{\\quad+16\\eta_{n}^{4}\\tau^{4}}\\\\ &{\\quad+16\\eta_{n}^{2}C\\chi_{n}^{2}+}\\\\ &{\\quad+16\\eta_{n}^{2}C\\chi_{n}^{2}\\Vert\\psi_{n-1}-\\psi^{*}\\Vert^{4}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "which, after further simplifications, yields ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\big[\\|\\mathfrak{p}_{n}-\\psi^{*}\\|^{4}\\,|\\,\\mathcal{F}_{n-1}\\big]}\\\\ &{\\quad\\leq\\|\\mathfrak{p}_{n-1}-\\psi^{*}\\|^{4}\\,(1-4\\eta_{\\mathfrak{p}}\\tilde{h}_{n}+12C_{\\gamma}\\eta_{n}^{2}\\tau^{2}+4C_{\\gamma}\\tau^{4}\\eta_{n}^{4})}\\\\ &{\\quad+\\eta_{n}^{2}\\|\\mathfrak{p}_{n-1}-\\psi^{*}\\|^{2}\\,(28(1+C_{\\gamma}\\tau)^{2})+32\\eta_{n}^{4}\\big(\\tau^{4}(1+C_{\\gamma})}\\\\ &{\\quad\\leq\\|\\mathfrak{p}_{n-1}-\\psi^{*}\\|^{4}\\,(1-4\\eta_{n}\\tilde{h}_{n}+12(1+C_{\\gamma})\\eta_{n}^{2}\\tau^{2}+4(1+C_{\\gamma})^{2}\\eta_{n}^{4})}\\\\ &{\\quad+2\\,\\eta_{n}^{2}\\|\\mathfrak{p}_{n-1}-\\psi^{*}\\|^{2}\\,(1+C_{\\gamma})\\tau^{2}+32\\eta_{n}^{4}(1+C_{\\gamma})\\tau^{4}}\\\\ &{\\quad\\leq\\|\\mathfrak{p}_{n-1}-\\psi^{*}\\|^{4}\\,(1-4\\eta_{n}\\tilde{h}_{n}+12\\eta_{n}^{2}(1+C_{\\gamma})\\tau^{2})^{2}+4(1(1+C_{\\gamma})\\tau^{4}\\eta_{n}^{4})}\\\\ &{\\quad+28\\eta_{n}^{2}\\|\\mathfrak{p}_{n-1}-\\psi^{*}\\|^{2}\\,(1+C_{\\gamma})\\tau^{2}+32\\eta_{n}^{4}(1+C_{\\gamma})\\tau^{3}}\\\\ &{\\quad\\leq\\|\\mathfrak{p}_{n-1}-\\psi^{*}\\|^{4}\\,(1-4\\eta_{n}\\tilde{h}_{n}+12\\eta_{n}^{2}(21+C_{\\gamma})\\tau^{2}+16\\eta_{n}^{2}(21+C_{\\gamma})\\tau^{3}}\\\\ &{\\quad+4(21+C_{\\gamma})\\tau^{3}\\eta_{n}^{4})+20\\eta_{n}^{2}\\|\\mathfrak{p}_{n-1\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where we defined $\\tilde{\\tau}_{1}:=2(1+C_{\\chi})\\tau$ and $\\tilde{L}_{1}:=2(1+C_{\\chi})\\tau+L$ . This recursion is of the form of the one studied in [22, Equation 32] (note that by design, $\\tilde{L}\\geq\\tilde{\\mu}_{m}$ .) The steps performed to bound $\\mathbb{E}\\left\\|\\psi_{i}-\\psi^{\\star}\\right\\|^{4}$ thus follow from their derivations, and we obtain: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}\\left\\|\\psi_{i-1}-\\psi^{\\star}\\right\\|^{4}}\\\\ &{\\qquad\\leqslant\\frac{C\\tilde{\\tau}_{1}^{2}}{2n}\\Big(C^{1/2}\\varphi_{1-3\\beta/2}(n)+\\tilde{\\mu}_{m}^{-1/2}\\varphi_{1-\\beta}(n)\\Big)}\\\\ &{\\qquad\\qquad+\\,\\frac{\\sqrt{20}C^{1/2}\\tilde{\\tau}_{1}}{2n}A_{1}\\exp\\left(24\\tilde{L}_{1}^{4}C^{4}\\right)\\left(\\delta_{0}+\\frac{\\tilde{\\mu}_{m}\\mathbb{E}\\left\\|\\theta_{0}-\\theta^{\\star}\\right\\|^{4}}{20C\\tilde{\\tau}_{1}^{2}}+2\\tilde{\\tau}_{1}^{2}C^{3}\\tilde{\\mu}_{m}+8\\tilde{\\tau}_{1}^{2}C^{2}\\right)^{1/2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{A_{1}=\\sum_{k=1}^{n}e^{\\frac{-\\tilde{\\mu}C}{16}k^{1-\\beta}+16\\tilde{L}_{1}^{4}C^{4}\\varphi_{1-2\\beta}(k)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "and we have $A(1)<+\\infty$ if $\\beta<1$ , and $A(1)=O(n)$ otherwise. Note that this bound is dominated by $n^{-\\beta}$ , and thus by $n^{-1/2}$ $\\beta>1/2$ \u53e3 ", "page_idx": 30}, {"type": "text", "text": "We first restate the theorem in its complete form. ", "page_idx": 30}, {"type": "text", "text": "Theorem (Contrastive Divergence with Polyak-Ruppert averaging). Let $(\\psi_{t})_{t\\geq0}$ the sequence of iteratesobtainedbyrunningthe $C D$ algorithm with a learning rate $\\eta_{t}=C t^{-\\beta}$ for $\\beta\\in(0,1)$ .Define $\\begin{array}{r}{\\bar{\\psi}_{n}:=\\frac{1}{n}\\sum_{i=1}^{n}\\psi_{i}}\\end{array}$ Then, under the same assumptions as Theorem 3.2 we have, for all $n\\geq1$ and assuming $\\begin{array}{r}{m>\\frac{(1-\\beta)\\log n}{2|\\log\\alpha|}}\\end{array}$ ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left(\\mathbb{E}\\left\\|\\overline{{\\psi}}_{n}-\\psi^{\\star}\\right\\|^{2}\\right)^{1/2}\\;\\leq\\;\\frac{2\\mathrm{tr}(\\mathbb{Z}(\\psi)^{-1})}{\\sqrt{n}}+P(n)}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "$\\mathcal{T}(\\psi^{\\star}):=\\mathrm{Cov}_{p_{\\psi^{\\star}}}\\phi$ is the Fisher informationmatrix of thedata distribution,and th tem $P(n)$ is of order $O(n^{-\\operatorname*{max}\\left(-\\beta,\\frac{\\beta}{2}-1,-\\left(\\frac{\\beta}{2}+m\\frac{|\\log\\alpha|}{\\log n}\\right)\\right)})$ ", "page_idx": 31}, {"type": "text", "text": "Proof. Let $h_{n}$ be the standard online (biased) CD gradient defined in Equation 3: ", "page_idx": 31}, {"type": "equation", "text": "$$\nh_{n}(\\phi)=-\\phi(x_{n})+\\phi(k_{\\psi_{n-1}}^{m}(\\cdot,x_{n})),\\quad x_{n}\\sim p_{\\psi^{\\star}}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "And let us note ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\overline{{h}}(\\phi)=-\\mathbb{E}_{p_{\\psi}\\star}\\phi(x_{n})+\\mathbb{E}_{\\psi^{\\star}}\\mathbb{E}_{k_{\\psi_{n-1}}^{m}}\\phi(k_{\\psi_{n-1}}^{m}(x_{n},\\cdot)),\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "which corresponds to a CD gradient the negative cross entropy between the model and the true (and not empirical) data distribution, defined as: ", "page_idx": 31}, {"type": "equation", "text": "$$\nf(\\psi)=\\mathbb{E}_{p_{\\psi}\\star}\\phi(x_{i})^{\\top}\\psi+\\log Z(\\psi)\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "It holds that: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f^{\\prime\\prime}(\\psi^{\\star})(\\psi_{n-1}-\\psi^{\\star})=f^{\\prime}(\\psi_{n-1})-f^{\\prime}(\\psi^{\\star})+(f^{\\prime\\prime}(\\psi^{\\star})(\\psi_{n-1}-\\psi^{\\star})-f^{\\prime}(\\psi_{n-1})+f^{\\prime}(\\psi^{\\star}))}\\\\ &{\\qquad\\qquad\\qquad\\qquad=h_{n}(\\psi_{n-1})-f^{\\prime}(\\psi^{\\star})+(f^{\\prime\\prime}(\\psi^{\\star})(\\psi_{n-1}-\\psi^{\\star})-f^{\\prime}(\\psi_{n-1})+f^{\\prime}(\\psi^{\\star}))}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\left(f^{\\prime}(\\psi_{n-1})-\\bar{h}(\\psi_{n-1})\\right)+(\\bar{h}(\\psi_{n-1})-h_{n}(\\psi_{n-1}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "and thus, we have that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{\\i}\\sum_{i=1}^{n}\\psi_{i}-\\psi^{\\star}=\\underbrace{\\frac{1}{n}\\sum_{i=1}^{n}f^{\\prime\\prime}(\\psi^{\\star})^{-1}h_{i}(\\psi_{i-1})}_{(i)}+\\underbrace{\\frac{1}{n}\\sum_{i=1}^{n}f^{\\prime\\prime}(\\psi^{\\star})^{-1}\\big(f^{\\prime\\prime}(\\psi^{\\star})(\\psi_{i-1}-\\psi^{\\star})-f^{\\prime}(\\psi_{i-1})\\big)}_{(i)}}\\\\ {+\\underbrace{\\frac{1}{n}\\sum_{i=1}^{n}f^{\\prime\\prime}(\\psi^{\\star})^{-1}\\big(f^{\\prime}(\\psi_{i-1})-\\bar{h}(\\psi_{i-1})\\big)}_{\\displaystyle i=1}+\\underbrace{\\frac{1}{n}\\sum_{i=1}^{n}f^{\\prime\\prime}(\\psi^{\\star})^{-1}\\big(\\overline{{h}}(\\psi_{i-1})-h_{i}(\\psi_{i-1})\\big)}_{\\displaystyle i=1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We seek to obtain a bound on ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\big(\\mathbb{E}\\big\\|\\frac{1}{n}\\sum_{i=1}^{n}\\psi_{i}-\\psi^{\\star}\\big\\|^{2}\\big)^{1/2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "$(i)$ and $(i i)$ will be bounded using techniques from prior work [22]. (ii) captures the bias of the CD algorithm, while $(i v)$ captures the variance. ", "page_idx": 31}, {"type": "text", "text": "Bounding (i)  Note that the first difference (the bias term) is deterministic. Moreover, using lemma C.7,we have: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|f^{\\prime}(\\psi_{n-1})-\\bar{h}(\\psi_{n-1})\\right\\|=\\left\\|\\mathbb{E}_{p_{\\psi_{n-1}}\\phi}-\\mathbb{E}_{p_{\\psi^{\\star}}}\\mathbb{E}_{k_{\\psi_{n-1}}^{m}\\phi}\\right\\|}\\\\ &{\\qquad\\qquad\\qquad\\leq\\alpha^{m}(\\mathbb{E}_{p_{\\psi_{n-1}}}\\left\\|\\phi-\\mathbb{E}_{p_{\\psi_{n-1}}}\\phi\\right\\|^{2})^{1/2}C_{\\chi}\\left\\|\\psi_{n-1}-\\psi^{\\star}\\right\\|}\\\\ &{\\qquad\\qquad\\qquad\\leq\\alpha^{m}\\sigma C_{\\chi}\\left\\|\\psi_{n-1}-\\psi^{\\star}\\right\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "By Minkowski's inequality, we have: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(\\mathbb{E}(i i i)^{2})^{1/2}\\;\\leq\\;\\frac{\\alpha^{m}C_{\\chi}}{n}\\sum_{i=1}^{n}(\\mathbb{E}\\left\\|\\psi_{i-1}-\\psi^{\\star}\\right\\|^{2})^{1/2}\\;=\\;\\frac{\\alpha^{m}C_{\\chi}}{n}\\sum_{i=1}^{n}\\delta_{i-1}^{1/2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Recalling that $\\delta_{i}$ satisfies theorem 3.2, we have that $\\delta_{i}^{\\frac{1}{2}}$ is dominated by $n^{-\\beta/2}$ , and we thus thus have $\\sum_{i=1}^{n}\\delta_{i}^{1/2}$ which is dominated by $n^{1-\\frac{\\beta}{2}}$ . Overall, we obtain that $(\\mathbb{E}(i i i)^{2})^{1/2}$ is dominated by $\\alpha^{m}n^{-\\beta/2}$ Noting that $\\alpha^{m}n^{-\\beta/2}=n^{-(\\frac{\\beta}{2}+m\\frac{|\\log\\alpha|}{\\log n})}$ thi erm is dominatedby $n^{-1/2}$ since,by assumption, w have $\\begin{array}{r}{m>\\frac{(1-\\beta)\\log n}{2|\\log\\alpha|}}\\end{array}$ .To track the final orde of the bound, we write $(\\mathbb{E}(i i i)^{2})^{1/2}=$ $P_{4}(n)$ , where $P_{4}(n)=O\\big(n^{-(\\frac{\\beta}{2}+m\\frac{|\\log\\alpha|}{\\log n})}\\big)$ ", "page_idx": 31}, {"type": "text", "text": "Bounding (iv)  For (iv), we start by further breaking down the summand. We have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(\\psi^{\\star})^{-1}(\\overline{{h}}(\\psi_{n-1})-h_{n}(\\psi_{n-1}))}\\\\ &{\\qquad=f^{\\prime\\prime}(\\psi^{\\star})^{-1}\\underbrace{(\\phi(x_{n})-\\mathbb{E}_{p_{\\psi^{\\star}}}\\phi)}_{\\Delta_{1,n}}+f^{\\star}(\\psi^{\\star})^{-1}\\underbrace{(\\phi(k_{\\psi_{n-1}}^{m}(x_{n}))-\\mathbb{E}_{p_{\\psi^{\\star}}}\\mathbb{E}_{k_{\\psi_{n-1}}^{m}}\\phi(k_{\\psi_{n-1}}^{m}(x_{n},\\cdot)))}_{\\Delta_{2,n}}}\\\\ &{\\qquad=f^{\\prime\\prime}(\\psi^{\\star})^{-1}\\Delta_{1,n}+f^{\\prime\\prime}(\\psi^{\\star})^{-1}\\Delta_{2,n}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "and by Minkowski's inequality, we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(\\mathbb{E}(i v)^{2})^{1/2}\\;\\leq\\;\\frac{1}{n}\\left(\\mathbb{E}\\left\\|\\sum_{i=1}^{n}f^{\\prime\\prime}(\\psi^{\\star})^{-1}\\Delta_{1,n}\\right\\|^{2}\\right)^{1/2}+\\frac{1}{n}\\left(\\mathbb{E}\\left\\|\\sum_{i=1}^{n}f^{\\prime\\prime}(\\psi^{\\star})^{-1}\\Delta_{2,n}\\right\\|^{2}\\right)^{1/2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Note that this step was made possible because we are looking at the square-root of the variance, which is unlike the recursion in Lemma 3.1. This allows to separate the terms and use fewer intermediaries than in the proof of Lemma 3.1. ", "page_idx": 32}, {"type": "text", "text": "Since both $\\Delta_{1,n}$ and $\\Delta_{2,n}$ are martingale differences with respect to the filtration ${\\mathcal{F}}_{n-1}$ , the covariance terms vanish, and we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\mathbb{E}(i v)^{2})^{1/2}\\leq\\frac{1}{n}\\Big(\\sum_{i=1}^{n}\\mathbb{E}\\left\\|f^{\\prime\\prime}(\\psi^{\\star})^{-1}\\Delta_{1,n}\\right\\|^{2}\\Big)^{1/2}+\\frac{1}{n}\\Big(\\sum_{i=1}^{n}\\mathbb{E}\\left\\|f^{\\prime\\prime}(\\psi^{\\star})^{-1}\\Delta_{2,n}\\right\\|^{2}\\Big)^{1/2}}\\\\ &{\\qquad\\qquad\\leq\\frac{1}{n}\\Big(\\sum_{i=1}^{n}\\mathrm{tr}\\big(f^{\\prime\\prime}(\\psi^{\\star})^{-1}\\mathbb{E}\\Delta_{1,n}\\Delta_{1,n}^{\\top}f^{\\prime\\prime}(\\psi^{\\star})^{-1}\\big)\\Big)^{1/2}}\\\\ &{\\qquad\\qquad+\\ \\frac{1}{n}\\Big(\\sum_{i=1}^{n}\\mathrm{tr}\\big(f^{\\prime\\prime}(\\psi^{\\star})^{-1}\\mathbb{E}\\Delta_{2,n}\\Delta_{2,n}^{\\top}f^{\\prime\\prime}(\\psi^{\\star})^{-1}\\big)\\Big)^{1/2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "which is key to obtain the desired acceleration. Since $\\mathbb{E}\\Delta_{1,n}\\Delta_{1,n}^{\\top}=\\mathcal{T}(\\psi^{\\star})$ , the first sum equals tr(() For the scond em, since 2,n is mean-zero conditional on -1 wehave ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\boldsymbol{\\Delta}_{2,n}=\\phi\\bigl(k_{\\psi_{n-1}}^{m}(x_{n},\\cdot)\\bigr)-\\mathbb{E}_{p_{\\psi^{*}}}\\mathbb{E}_{k_{\\psi_{n-1}}^{m}}\\phi\\bigl(k_{\\psi_{n-1}}^{m}(x_{n},\\cdot)\\bigr)}\\\\ &{\\Longrightarrow\\mathbb{E}\\left[\\boldsymbol{\\Delta}_{2,n}\\boldsymbol{\\Delta}_{2,n}^{\\top}|\\mathcal{F}_{t-1}\\right]}\\\\ &{\\qquad=\\mathbb{E}_{p_{\\psi^{*}}}\\mathbb{E}_{k_{\\psi_{n-1}}^{m}}\\left[\\phi\\bigl(k_{\\psi_{n-1}}^{m}(x_{n},\\cdot)\\bigr)\\phi\\bigl(k_{\\psi_{n-1}}^{m}(x_{n},\\cdot)\\bigr)^{\\top}|\\mathcal{F}_{t-1}\\right]}\\\\ &{\\qquad-\\left(\\mathbb{E}_{p_{\\psi^{*}}}\\mathbb{E}_{k_{\\psi_{n-1}}^{m}}\\bigl[\\phi\\bigl(k_{\\psi_{n-1}}^{m}(\\cdot,x_{n})\\bigr)|\\mathcal{F}_{t-1}\\bigr]\\right)(\\mathbb{E}_{p_{\\psi^{*}}}\\mathbb{E}_{k_{\\psi_{n-1}}^{m}}\\phi\\bigl(k_{\\psi_{n-1}}^{m}(\\cdot,x_{n})\\bigr)|\\mathcal{F}_{t-1})^{\\top}}\\\\ &{\\qquad=\\mathbb{E}_{p_{\\psi^{*}}}P_{\\psi_{n-1}}^{m}\\left[\\phi\\phi^{\\top}|\\mathcal{F}_{t-1}\\right]-\\bigl(\\mathbb{E}_{p_{\\psi^{*}}}\\bigl[P_{\\psi_{n-1}}^{m}\\phi\\bigr|\\mathcal{F}_{t-1}\\bigr]\\bigr)(\\mathbb{E}_{p_{\\psi^{*}}}\\bigl[P_{\\psi_{n-1}}^{m}\\phi^{\\top}|\\mathcal{F}_{t-1}\\bigr])}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where we abused the notation for $P_{\\psi_{n-1}}^{m}$ to handle functions mapping to $\\mathbb{R}^{d}$ and $\\mathbb{R}^{d\\times d}$ instead of only $\\mathbb{R}$ .We now show that $\\Delta_{2,n}\\Delta_{2,n}^{\\top}$ approximately equals $I(\\psi_{n-1})$ , by inspecting the second moment Ep\\* $\\mathbb{E}_{p_{\\psi}\\star}P_{\\psi_{n-1}}^{m}\\phi\\phi^{\\top}$ and the squaredfrst moment $(\\mathbb{E}_{p_{\\psi}\\star}P_{\\psi_{n-1}}^{m}\\phi)(\\mathbb{E}_{p_{\\psi}\\star}P_{\\psi_{n-1}}^{m}\\phi)^{\\top}$ Looking at the second moment first, we have, omitting the conditioning on $\\mathcal{F}_{t-1}$ for brevity: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{p_{\\psi}\\star}P_{\\psi_{n-1}}^{m}\\phi\\boldsymbol{\\phi}^{\\top}=(\\mathbb{E}_{p_{\\psi}\\star}P_{\\psi_{n-1}}^{m}\\phi\\boldsymbol{\\phi}^{\\top}-\\mathbb{E}_{p_{\\psi_{n-1}}\\phi\\boldsymbol{\\phi}^{\\top}})+\\mathbb{E}_{p_{\\psi_{n-1}}\\phi\\boldsymbol{\\phi}^{\\top}}}\\\\ {=\\underbrace{\\mathbb{E}_{p_{\\psi}\\star}P_{\\psi_{n-1}}^{m}(\\phi\\boldsymbol{\\phi}^{\\top}-\\mathbb{E}_{p_{\\psi_{n-1}}\\phi\\boldsymbol{\\phi}^{\\top}})}_{\\Delta_{3,n}}+\\mathbb{E}_{p_{\\psi_{n-1}}\\phi\\boldsymbol{\\phi}^{\\top}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "since $\\mathbb{E}_{p_{\\psi_{n-1}}}P_{\\psi_{n-1}}^{m}(\\phi\\phi^{\\top}-\\mathbb{E}_{p_{\\psi_{n-1}}\\phi\\phi^{\\top}})=0.$ we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\Delta_{3,n}=\\mathbb{E}_{p_{\\psi},\\,P_{\\psi_{n-1}}^{m}\\left(\\phi\\phi^{\\top}-\\mathbb{E}_{p_{\\psi_{n-1}}}\\phi\\phi^{\\top}\\right)-\\mathbb{E}_{p_{\\psi_{n-1}}}P_{\\psi_{n-1}}^{m}(\\phi\\phi^{\\top}-\\mathbb{E}_{p_{\\psi_{n-1}}}\\phi\\phi^{\\top})}}}\\\\ {{\\Longrightarrow\\parallel\\!\\Delta_{3,n}\\!\\parallel_{F}\\leq\\alpha^{m}C_{\\chi}(\\mathbb{E}_{p_{\\psi_{n-1}}}\\left\\|\\phi\\phi^{\\top}-\\mathbb{E}_{p_{\\psi_{n-1}}}\\phi\\phi^{\\top}\\right\\|_{F}^{2})^{1/2}\\left\\|\\psi_{n-1}-\\psi^{\\star}\\right\\|}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "which follows from applying lemma C.7to the function $\\phi_{i}\\phi_{j}-\\mathbb{E}_{p_{\\psi_{n-1}}}\\phi_{i}\\phi_{j}$ , alongside with assumption A2. Noting $\\bar{\\sigma}^{2}:=\\operatorname*{sup}_{\\psi\\in\\Psi}\\mathbb{E}_{p_{\\psi}}\\left\\|\\phi\\phi^{\\top}-\\mathbb{E}_{p_{\\psi}}\\phi\\phi^{\\top}\\right\\|_{F}^{2}$ , which is finite by analyticity of the log-partition function, we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\|\\Delta_{3,n}\\|_{F}\\leq\\overline{{\\sigma}}\\alpha^{m}C_{\\chi}\\left\\|\\psi_{n-1}-\\psi^{\\star}\\right\\|\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "We now investigate the first moment. We have: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{p_{\\psi}*P_{\\psi_{n-1}}^{m}}\\phi=(\\underbrace{\\mathbb{E}_{p_{\\psi}*P_{\\psi_{n-1}}^{m}\\phi}-\\mathbb{E}_{p_{\\psi_{n-1}}}\\phi}_{\\Delta_{4a,n}})+\\mathbb{E}_{p_{\\psi_{n}}}\\phi}\\\\ &{\\implies(\\mathbb{E}_{p_{\\psi}*P_{\\psi_{n-1}}^{m}\\phi})(\\mathbb{E}_{p_{\\psi}*P_{\\psi_{n-1}}^{m}\\phi})^{\\top}-\\mathbb{E}_{p_{\\psi_{n-1}}\\phi}\\mathbb{E}_{p_{\\psi_{n-1}}\\phi}^{\\top}\\tau^{\\top}=\\Delta_{4a,n}\\mathbb{E}_{p_{\\psi_{n-1}}}\\phi^{\\top}+\\mathbb{E}_{p_{\\psi_{n-1}}}\\phi^{\\top}\\Delta_{4a,n}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and thus, applying Lemma C.7 on $\\Delta_{4a,n}$ , we have: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\Delta_{4,n}\\right\\|_{F}\\leq\\left\\|\\Delta_{4a,n}\\mathbb{E}_{p_{\\psi_{n-1}}}\\phi^{\\top}+\\mathbb{E}_{p_{\\psi_{n-1}}}\\phi^{\\top}\\Delta_{4a,n}\\right\\|}\\\\ &{\\qquad\\qquad\\leq\\left\\|\\Delta_{4a,n}\\mathbb{E}_{p_{\\psi_{n-1}}}\\phi^{\\top}\\right\\|+\\left\\|\\mathbb{E}_{p_{\\psi_{n-1}}}\\phi^{\\top}\\Delta_{4a,n}\\right\\|}\\\\ &{\\qquad\\qquad\\leq2\\left\\|\\Delta_{4a,n}\\right\\|\\left\\|\\mathbb{E}_{p_{\\psi_{n-1}}}\\phi\\right\\|}\\\\ &{\\qquad\\qquad\\leq2\\left\\|\\log Z\\right\\|_{3,\\infty}\\alpha^{m}C_{\\chi}\\sigma\\left\\|\\psi_{n-1}-\\psi^{\\star}\\right\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "We can now combine our two matrix moment bounds to obtain: Consequently, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbb{E}_{p_{\\psi}\\star}\\mathbb{E}_{k_{\\psi_{n-1}}^{m}}\\Delta_{2,n}\\Delta_{2,n}^{\\top}=\\mathbb{E}_{p_{\\psi_{n}}}(\\phi-\\mathbb{E}_{p_{\\psi_{n}}}\\phi)(\\phi-\\mathbb{E}_{p_{\\psi_{n}}}\\phi)^{\\top}+\\Delta_{3,n}+\\Delta_{4,n}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Note that $\\begin{array}{r}{\\mathbb{E}_{p_{\\psi_{n-1}}}(\\phi-\\mathbb{E}_{p_{\\psi_{n-1}}}\\phi)(\\phi-\\mathbb{E}_{p_{\\psi_{n-1}}}\\phi)^{\\top}=\\operatorname{Cov}_{p_{\\psi_{n-1}}}\\phi=\\nabla_{\\psi}^{2}\\mathcal{L}(\\psi_{n-1}).}\\end{array}$ By analycity of $\\mathcal{L}$ , there exsts a constant $\\begin{array}{r}{M:=\\operatorname*{sup}_{\\psi\\in\\Psi}\\left\\|\\nabla^{3}\\log Z(\\psi)\\right\\|_{\\mathrm{op}(\\|\\cdot\\|_{F},\\|\\cdot\\|_{F})}}\\end{array}$ such hat ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\|\\nabla_{\\psi}^{2}\\mathcal{L}(\\psi_{n-1})-\\nabla_{\\psi}^{2}\\mathcal{L}(\\psi^{\\star})\\|_{F}:=\\|\\Delta_{5,n}\\|_{F}\\leq M\\,\\|\\psi_{n}-\\psi^{\\star}\\|\\,.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Thus, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbb{E}_{p_{\\psi}\\star}\\mathbb{E}_{k_{\\psi_{n-1}}^{m}}\\Delta_{2,n}\\Delta_{2,n}^{\\top}=\\mathbb{Z}(\\psi^{\\star})+\\Delta_{3,n}+\\Delta_{4,n}+\\Delta_{5,n}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "with ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\|\\Delta_{3,n}+\\Delta_{4,n}+\\Delta_{5,n}\\right\\|_{F}\\leq\\left\\|\\Delta_{3,n}\\right\\|_{F}+\\left\\|\\Delta_{4,n}\\right\\|_{F}+\\left\\|\\Delta_{5,n}\\right\\|_{F}}&{}\\\\ {\\leq(1+M+\\left\\|\\log Z\\right\\|_{3,\\infty})\\left\\|\\psi_{n-1}-\\psi^{\\star}\\right\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "We are now ready to characterize the variance term alltogether. We have: Using the subadditivity of the square root $\\Delta_{2,n}$ sum, wehave: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{n}\\Big(\\sum_{i=1}^{n}\\mathrm{tr}(f^{\\prime\\prime}(\\psi^{\\star})^{-1}\\mathbb{E}_{\\psi_{n-1}}\\mathbb{E}_{p_{\\psi^{\\star}}}\\Delta_{2,i}\\Delta_{2,i}^{\\top}f^{\\prime\\prime}(\\psi^{\\star})^{-1})\\Big)^{1/2}}\\\\ &{\\leq\\frac{1}{n}\\Big(\\sum_{i=1}^{n}\\mathrm{tr}(\\mathbb{Z}(\\psi^{\\star}))^{-1}\\Big)^{1/2}+\\frac{1}{n\\mu}\\Big(\\sum_{i=1}^{n}(1+M+\\|\\log Z\\|_{3,\\infty})\\mathbb{E}\\,\\|\\psi_{n-1}-\\psi^{\\star}\\|\\Big)^{1/2}}\\\\ &{\\leq\\frac{\\mathrm{tr}(\\mathbb{Z}(\\psi^{\\star})^{-1})^{1/2}}{\\sqrt{n}}+\\frac{\\|\\log Z\\|_{4,\\infty}}{\\mu n}\\left(\\sum_{i=1}^{n}\\delta_{i-1}^{1/2}\\right)^{1/2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $\\|\\log Z\\|_{4,\\infty}:=(1+M+\\|\\log Z\\|_{3,\\infty})^{1/2}$ In these inequalities, we used the cyclicity of the trace, the fact that $\\mathrm{tr}(A^{\\top}B)\\leq\\|A B\\|_{F}\\leq\\|A\\|_{\\mathrm{op}}\\,\\|B\\|_{F}$ the fact that $\\begin{array}{r}{\\|\\mathbb{Z}(\\psi^{\\star})^{-1}\\|_{\\mathrm{op}}\\leq\\frac{1}{\\mu}}\\end{array}$ , and the identy $\\mathbb{E}\\|\\psi_{i-1}-\\psi^{\\star}\\|\\leq(\\mathbb{E}\\|\\psi_{i-1}-\\psi^{\\star}\\|^{2})^{1/2}=\\delta_{i-1}^{1/2}.$ \uff0c ", "page_idx": 33}, {"type": "text", "text": "We can combine this inequality with the rest of the term to obtain our final upper bound: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\mathbb{E}\\Big\\lVert\\frac{1}{n}\\sum_{i=1}^{n}\\psi_{i}-\\psi^{*}\\Big\\rVert^{2}\\right)^{1/2}\\leq\\frac{2\\mathrm{tr}(Z(\\psi^{\\star})^{-1})}{\\sqrt{n}}+\\frac{\\left\\lVert\\log Z\\right\\rVert_{4,\\infty}}{\\mu n}\\left(\\sum_{i=1}^{n}\\delta_{i}^{1/2}\\right)^{1/2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\left(\\mathbb{E}\\left\\lVert\\frac{1}{n}\\sum_{i=1}^{n}g_{i}(\\psi_{i-1})\\right\\rVert^{2}\\right)^{1/2}+\\frac{M}{2n\\mu}\\left(\\sum_{i=1}^{n}\\mathbb{E}\\left\\lVert\\psi_{i}-\\psi^{*}\\right\\rVert^{4}\\right)^{1/2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "We will use the readily available bounds of [22] for the last 2 terms. Before concluding the proof, we need tobound $\\mathbb{E}\\left\\|\\psi_{i}-\\psi^{\\star}\\right\\|^{4}$ ", "page_idx": 33}, {"type": "text", "text": "Bounding $\\mathbb{E}\\|\\psi_{i}-\\psi^{\\star}\\|^{4}$ We use Lemma D.6t obtan that $\\begin{array}{r}{\\frac{M}{2n\\mu}\\sum_{i=1}^{n}\\mathbb{E}\\left\\|\\psi_{i}-\\psi^{\\star}\\right\\|^{4}\\leq P_{1}(n)}\\end{array}$ with $P_{1}(n)$ of order $n^{-\\beta}$ \uff1a ", "page_idx": 33}, {"type": "text", "text": "Bounding $\\textstyle1/n\\sum_{i=1}^{n}h_{i}(\\psi_{i-1})$ We use Lemma D.5 to obtain that $\\begin{array}{r}{(\\mathbb{E}\\left\\|\\frac{1}{n}\\sum_{i=1}^{n}g_{i}\\big(\\psi_{i-1}\\big)\\right\\|^{2})^{1/2}}\\end{array}$ with $P_{2}(n)$ of order $n^{{\\frac{\\beta}{2}}-1}$ ", "page_idx": 34}, {"type": "text", "text": "Bounding $\\begin{array}{r}{\\frac{\\|\\log Z\\|_{4,\\infty}}{\\mu n}\\big(\\sum_{i=1}^{n}\\delta_{i}^{1/2}\\big)^{1/2}}\\end{array}$ We use Lemma D.4 to obtain that $\\begin{array}{r}{\\frac{\\|\\log Z\\|_{4,\\infty}}{\\mu n}\\big(\\sum_{i=1}^{n}\\delta_{i}^{1/2}\\big)^{1/2}\\leq P_{3}(n)}\\end{array}$ with $P_{3}(n)$ of order $n^{-\\frac{1}{2}+\\frac{\\beta}{4}}$ ", "page_idx": 34}, {"type": "text", "text": "Putting everything together, we have that: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\left(\\mathbb{E}\\left\\Vert\\overline{{\\psi}}_{n}-\\psi^{\\star}\\right\\Vert^{2}\\right)^{1/2}\\leq\\frac{2\\mathrm{tr}(\\overline{{\\chi}}(\\psi)^{-1})}{\\sqrt{n}}+P_{1}(n)+P_{2}(n)+P_{3}(n)+P_{4}(n)\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "$P_{1}+P_{2}+P_{3}+P_{4}$ is dominated by ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\left(-(\\frac{1}{2}+\\frac{\\beta}{4}),-\\beta,\\frac{\\beta}{2}-1,\\frac{\\beta}{2}+m\\frac{|\\log\\alpha|}{\\log n}\\right)=\\operatorname*{max}\\left(-\\beta,\\frac{\\beta}{2}-1,\\frac{\\beta}{2}+m\\frac{|\\log\\alpha|}{\\log n}\\right)\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "which concludes the proof. ", "page_idx": 34}, {"type": "text", "text": "E $L_{2}$ approximation by auxiliary gradient updates ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "In this section, we consider different gradient update schemes starting from some random initialization $\\theta_{\\mathrm{init}}$ , and control the $L_{2}$ distance between the different updates and the deterministic target $\\psi^{*}\\in\\Psi$ ", "page_idx": 34}, {"type": "text", "text": "Notation. Recall the notation that $X_{1,},\\ldots,X_{n}\\overset{\\mathrm{i.i.d.}}{\\sim}p_{\\psi^{*}},\\,B\\,=\\,n/N$ and for $\\psi\\,\\in\\,\\Psi$ \uff0c $K_{\\psi}(x)\\sim$ $k_{\\psi}^{m}(x,\\cdot)$ . We also write, for $m\\in\\mathbb{N}\\cup\\{\\infty\\}$ ", "page_idx": 34}, {"type": "equation", "text": "$$\nK_{1;\\psi}^{m}(x)\\,,\\,.\\,.\\,.\\,\\,,\\,K_{n;\\psi}^{m}(x)\\,\\stackrel{\\mathrm{i.i.d.}}{\\sim}k_{\\psi}^{m}(x,\\bullet)\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Let $\\theta^{\\mathrm{init}}$ besome $\\Psi$ -valued random initialization that is possibly correlated with $X_{1},\\ldots,X_{n}$ We capture the effect of correlation through the following quantities: For $\\epsilon>0$ and $\\nu>2$ , let ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\vartheta_{n,m}^{\\mathrm{init}}(\\epsilon)\\;:=\\mathbb{P}\\left(\\frac{\\left\\|\\sum_{i=1}^{n}\\mathbb{E}\\left[\\phi\\left(K_{\\theta^{\\mathrm{init}}}^{m}(X_{i})\\right)\\big|\\;X_{i},\\theta^{\\mathrm{init}}\\right]-\\mathbb{E}\\left[\\phi\\left(K_{\\theta_{\\mathrm{init}}}^{m}(X_{i}^{\\prime})\\right)\\big|\\;\\theta^{\\mathrm{init}}\\right]\\right\\|}{n}>\\epsilon\\right)\\,,}\\\\ &{}\\\\ &{\\varepsilon_{n,m;\\nu}^{\\mathrm{init}}(\\epsilon)\\;:=\\sqrt{\\epsilon^{2}+\\kappa_{\\nu;m}^{2}\\left(\\vartheta_{n,m}^{\\mathrm{init}}(\\epsilon)\\right)^{\\frac{2}{\\nu-2}}}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We also consider the ii.d. samples, drawn independently of $X_{1},\\ldots,X_{n}$ and on a given $\\psi\\in\\Psi$ as ", "page_idx": 34}, {"type": "equation", "text": "$$\nX_{1}^{\\psi},\\ldots,X_{n}^{\\psi}\\stackrel{\\mathrm{\\scriptsize~i.i.d.}}{\\sim}p_{\\psi}\\;.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "For notational clarity, we shall use $\\theta_{m,B}$ to denote parameters arising from an one-step update, where the subscripts $m,B$ represent performing the one-step update with length- $m$ Markov chains and with batch size $B$ . This is to be distinguished from $\\psi_{t}$ elsewhere in the text, which denotes the parameter from the actual multi-step CD algorithm and the subscript $t$ denotes the $t$ -th CD iterate. ", "page_idx": 34}, {"type": "text", "text": "Gradient update schemes. _ We consider five different updates. Let $X_{1}^{\\prime}$ be an i.i.d. copy of $X_{1}$ drawn independently of all other random variables. The SGD-with-replacement update is given by ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\theta_{m,B}^{\\mathrm{SGDw}}\\;:=\\;F_{m,B}^{\\mathrm{SGDw}}(\\theta^{\\mathrm{init}})\\;,\\quad\\mathrm{~where~}F_{m,B}^{\\mathrm{SGDw}}(\\psi)\\;:=\\;\\psi-\\frac{\\eta}{B}\\sum_{i\\in S^{w}}\\Big(\\phi(X_{i})-\\phi\\big(K_{i;\\psi}^{m}(X_{i})\\big)\\Big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "and $S^{w}$ is a uniformly drawn size- $B$ subset of $[n]$ . The SGD-without-replacement update, after renormalizing the learning rate, is given by the $N$ -fold function composition ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\theta_{m,B}^{\\mathrm{SGDo}}\\;:=\\;F_{m,B;N}^{\\mathrm{SGDo}}\\circ\\dots\\circ F_{m,B;1}^{\\mathrm{SGDo}}(\\theta^{\\mathrm{init}})\\;,}\\\\ {\\mathrm{where}\\quad F_{m,B;j}^{\\mathrm{SGDo}}(\\psi)\\;:=\\;\\psi-\\frac{\\eta}{N B}\\sum_{i\\in S_{j}^{o}}\\Big(\\phi(X_{i})-\\phi\\big(K_{i;\\psi}^{m}(X_{i})\\big)\\Big)\\quad\\mathrm{for}\\;\\mathrm{each}\\;j\\in[N]\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "and $S_{j}^{o}$ 's are disjoint size- $B$ random subsets of $[n]$ , defined by $(S_{1}^{o},\\ldots,S_{N}^{o})=\\pi([n])$ for a uniformly drawn element $\\pi$ of the permutation group on $n$ objects. The full-batch gradient update is given by ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\theta_{m}^{\\mathrm{GD}}\\;:=\\;F_{m}^{\\mathrm{GD}}(\\theta^{\\mathrm{init}})\\;,\\quad\\mathrm{~where~}\\quad F_{m}^{\\mathrm{GD}}(\\psi)\\;:=\\;\\psi-\\frac{\\eta}{n}\\sum_{i\\leq n}\\Big(\\phi(X_{i})-\\phi\\big(K_{i;\\psi}^{m}(X_{i})\\big)\\Big)\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "The full-batch gradient update with an infinite-length Markov chain is given by ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\theta_{\\infty}^{\\mathrm{GD}}\\;:=\\;F_{\\infty}^{\\mathrm{GD}}(\\theta^{\\mathrm{init}})\\;,\\qquad\\mathrm{where}\\qquad F_{\\infty}^{\\mathrm{GD}}(\\psi)\\;:=\\;\\psi-\\frac{\\eta}{n}\\sum_{i\\leq n}\\Big(\\phi(X_{i})-\\phi(X_{1}^{\\psi})\\Big)\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "The population gradient update with an infinite-length Markov chain is given by ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\theta^{\\mathrm{pop}}\\;:=\\;f^{\\mathrm{pop}}(\\theta^{\\mathrm{init}})\\;,\\qquad\\mathrm{where}\\quad f^{\\mathrm{pop}}(\\psi)\\;:=\\;\\psi-\\eta\\,\\mathbb{E}\\bigl[\\phi(X_{1})-\\phi(X_{1}^{\\psi})\\bigr]\\;,\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where we use the lowercase $f$ to emphasize that $f^{\\mathrm{pop}}$ is a deterministic function. The forthcoming results are summarized below: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\theta_{m,B}^{\\mathrm{SGDw\\\\Lemma\\E.1}}\\ \\theta_{m}^{\\mathrm{GD\\\\Lemma\\E.2}}\\ \\theta_{\\infty}^{\\mathrm{GD\\\\Lemma\\E.3}}\\ \\theta^{\\mathrm{pop\\\\Lemma\\E.4}}\\ \\psi^{*}\\ .\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Lemma E.1. Let ${\\mathcal{F}}_{n}$ be the sigma algebra generated by $\\{X_{i},K_{i;\\theta^{\\mathrm{init}}}^{m}(X_{i})\\,|\\,1\\le i\\le n\\}$ Then ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\theta_{m,B}^{\\mathrm{SGDw}}-\\theta_{m}^{\\mathrm{GD}}\\,\\big|\\,\\theta^{\\mathrm{init}},\\mathcal{F}_{n}\\right]\\ =0\\qquad a l m o s t\\,s u r e l y\\ .\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Moreover underA1 and $A7_{:}$ wehave ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbb{E}\\Big\\lVert\\theta_{m,B}^{\\mathrm{SGDw}}-\\theta_{m}^{\\mathrm{GD}}\\Big\\rVert^{2}\\ \\le\\frac{4\\eta^{2}(\\sigma^{2}+\\kappa_{\\nu;m}^{2})}{B}\\,\\mathbb{I}_{\\{B<n\\}}\\ .\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Proof of Lemma E.1. Write $A:=(A_{1},\\ldots,A_{n})$ , where ", "page_idx": 35}, {"type": "equation", "text": "$$\nA_{i}\\;:=\\;\\left(\\phi(X_{i})-\\phi\\big(K_{i;\\theta^{\\mathrm{init}}}^{m}(X_{i})\\big)\\right)-\\mathbb{E}\\big[\\phi(X_{1})-\\phi\\big(K_{i;\\theta^{\\mathrm{init}}}^{m}(X_{1})\\big)\\big]\\;.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Since $S^{w}$ is uniformly drawn from all size- $B$ subsets of $[n]$ and independently of all other variables, we have that almost surely ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{\\mathbb E}\\big[\\theta_{m,B}^{\\mathrm{SGDw}}-\\theta_{m}^{\\mathrm{GD}}\\,\\big|\\,\\theta^{\\mathrm{init}},\\mathcal{F}_{n}\\big]\\ =\\ {\\mathbb E}\\Big[\\frac{\\eta}{B}\\sum_{i\\in S^{w}}A_{i}-\\frac{\\eta}{n}\\sum_{i\\le n}A_{i}\\,\\Big|\\,A\\Big]\\ =\\ 0\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "To prove thrmaining oud, we nte that th above relatinli $\\theta_{m,B}^{\\mathrm{SGDw}}-\\theta_{m}^{\\mathrm{GD}}$ is zero-mean. By the law of total variance, we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\big\\|\\theta_{m,B}^{\\mathrm{SGDw}}-\\theta_{m}^{\\mathrm{GD}}\\big\\|^{2}\\;=\\;\\mathrm{Tr}\\,{\\mathrm{Cov}}\\big[\\theta_{m,B}^{\\mathrm{SGDw}}-\\theta_{m}^{\\mathrm{GD}}\\big]\\;=\\;\\mathrm{Tr}\\,\\mathbb{E}\\,{\\mathrm{Cov}}\\big[\\theta_{m,B}^{\\mathrm{SGDw}}-\\theta_{m}^{\\mathrm{GD}}\\,\\big|\\,\\theta^{\\mathrm{init}},\\mathcal{F}_{n}\\big]}\\\\ &{\\quad=\\;\\eta^{2}\\,\\mathrm{Tr}\\,\\mathbb{E}\\bigg[\\mathbb{E}\\bigg[\\Big(\\frac{1}{B}\\sum_{i\\in S^{w}}A_{i}\\Big)\\Big(\\frac{1}{B}\\sum_{i\\in S^{w}}A_{i}\\Big)^{\\top}\\,\\Big|\\,A\\Big]-\\Big(\\frac{1}{n}\\sum_{i\\leq n}A_{i}\\Big)\\Big(\\frac{1}{n}\\sum_{i\\leq n}A_{i}\\Big)^{\\top}\\bigg]\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "To compute the covariance, recall that $S^{w}$ is a uniformly drawn size- $B$ subset of $[n]$ with $B=n/N$ Let $\\bar{\\mathcal{P}_{N}([n])}$ be the collection of all partitions of $[n]$ into $N$ size- $B$ subsets. We can generate $S^{w}$ by the following two-step process: ", "page_idx": 35}, {"type": "text", "text": "(i) Uniformly draw a partition $P^{\\prime}=(P_{1}^{\\prime},\\dots,P_{N}^{\\prime})$ from $\\mathcal P_{N}([n])$ (i) Uniformly sample an index $K$ from $[N]$ and set $S^{w}=P_{K}^{\\prime}$ ", "page_idx": 35}, {"type": "text", "text": "Then we have, almost surely ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\Big[\\Big(\\frac{1}{B}\\sum_{i\\in S^{w}}A_{i}\\Big)\\Big(\\frac{1}{B}\\sum_{i\\in S^{w}}A_{i}\\Big)^{\\top}\\Big|\\,A\\Big]-\\Big(\\frac{1}{n}\\sum_{i\\le n}A_{i}\\Big)\\Big(\\frac{1}{n}\\sum_{i\\le n}A_{i}\\Big)^{\\top}}\\\\ &{=\\ \\frac{1}{|\\mathcal{P}_{N}([n])|}\\sum_{P^{\\prime}\\in\\mathcal{P}_{N}([n])}\\Big(\\frac{1}{N}\\sum_{k\\le N}\\frac{1}{B^{2}}\\sum_{i,j\\in P_{k}^{\\prime}}A_{i}A_{j}^{\\top}-\\frac{1}{n^{2}}\\sum_{i,j\\le n}A_{i}A_{j}^{\\top}\\Big)}\\\\ &{=\\ \\frac{1}{|\\mathcal{P}_{N}([n])|}\\sum_{P^{\\prime}\\in\\mathcal{P}_{N}([n])}\\Big(\\frac{1}{N B^{2}}\\sum_{k\\le N}\\sum_{i,j\\in P_{k}^{\\prime}}A_{i}A_{j}^{\\top}-\\frac{1}{N^{2}B^{2}}\\sum_{k,l\\le N}\\sum_{i\\in P_{k}^{\\prime},j\\in P_{l}^{\\prime}}A_{i}A_{j}^{\\top}\\Big)}\\\\ &{=\\ \\frac{1}{|\\mathcal{P}_{N}([n])|}\\sum_{P^{\\prime}\\in\\mathcal{P}_{N}([n])}\\Big(\\frac{N-1}{N^{2}B^{2}}\\sum_{k\\le N}\\sum_{i,j\\in P_{k}^{\\prime}}A_{i}A_{j}^{\\top}-\\frac{1}{N^{2}B^{2}}\\sum_{k\\neq l}\\sum_{i\\in P_{k}^{\\prime},j\\in P_{l}^{\\prime}}A_{i}A_{j}^{\\top}\\Big)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "By noting that $A_{i}$ 's are exchangeable, we obtain ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\|\\theta_{n,B}^{\\mathrm{SGD}}-\\theta_{n}^{(\\pm)}\\|^{2}}&{=\\eta^{2}\\mathrm{Tr}\\bigg\\{\\frac{N-1}{N B}A_{1}A_{1}^{\\top}+\\frac{(N-1)(B-1)}{N B}A_{1}A_{2}^{\\top}-\\frac{N-1}{N}A_{1}A_{2}^{\\top}\\bigg\\}}\\\\ &{=\\eta^{2}\\mathrm{Tr}\\mathbb{E}\\bigg[\\frac{N-1}{N B}A_{1}A_{1}^{\\top}-\\frac{N-1}{N B}A_{1}A_{2}^{\\top}\\bigg]}\\\\ &{=\\frac{\\eta^{2}(N-1)}{N B}\\big(\\mathbb{E}\\|A_{1}\\|^{2}-\\mathbb{E}(A_{1},A_{2})\\big)}\\\\ &{\\overset{(a)}{\\leq}\\frac{2\\eta^{2}}{N}\\mathbb{E}\\|A_{1}\\|^{2}}\\\\ &{=\\frac{2\\eta^{2}}{B}\\mathbb{E}\\bigg\\|\\big(\\phi(X_{1})-\\mathbb{E}\\big(\\phi(X_{1})\\big)\\big)-\\Big(\\phi\\big(K_{\\downarrow,\\theta^{\\mathrm{max}}}^{m}(X_{1})\\big)-\\mathbb{E}\\Big[\\phi\\big(K_{\\downarrow,\\theta^{\\mathrm{max}}}^{m}(X_{1})\\big)\\Big]\\Big)\\bigg\\|^{2}}\\\\ &{\\leq\\frac{4\\eta^{2}}{B}\\bigg(\\mathrm{Tr}\\big\\{\\mathrm{cos}[\\phi(X_{1})]+\\mathrm{Tr}\\mathrm{Cos}\\Big[\\phi\\big(K_{\\downarrow,\\theta^{\\mathrm{max}}}^{m}(X_{1})\\big)\\Big]\\bigg\\}}\\\\ &{\\overset{(b)}{\\leq}\\frac{4\\eta^{2}\\left(2^{2}+K_{\\mathrm{sm}}^{2}\\right)}{N}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "In $(a)$ , we have used a Cauchy-Schwarz inequality; in $(b)$ , we have used A1 and A7. Finally we note that if $B=n$ \uff0c $\\theta_{m,B}^{\\mathrm{SGDw}}=\\theta_{m}^{\\mathrm{GD}}$ almost surely, which implies the desired bound. \u53e3 ", "page_idx": 36}, {"type": "text", "text": "Lemma E.2. Denote $A_{n}$ as the sigma algebra generated by $\\theta^{\\mathrm{init}},X_{1},\\ldots,X_{n}$ .Under A1, A2, A3 and $A7$ we have that for any $\\epsilon>0$ and $\\nu>2$ ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left\\|\\mathbb{E}\\left[\\theta_{m}^{\\mathrm{GD}}-\\theta_{\\infty}^{\\mathrm{GD}}\\left|\\right.\\right|^{2}\\;\\leq\\;\\eta^{2}\\Big(\\alpha^{m}\\sigma C_{\\chi}\\,\\sqrt{\\mathbb{E}\\|\\theta^{\\mathrm{init}}-\\psi^{*}\\|^{2}}\\,+\\,\\varepsilon_{n,m;\\nu}^{\\mathrm{init}}(\\epsilon)\\Big)^{2}\\;,}\\\\ &{\\mathbb{E}\\left\\|\\theta_{m}^{\\mathrm{GD}}-\\theta_{\\infty}^{\\mathrm{GD}}\\right\\|^{2}\\;\\leq\\;\\eta^{2}\\Big(\\Big(\\alpha^{m}\\sigma C_{\\chi}\\,\\sqrt{\\mathbb{E}\\|\\theta^{\\mathrm{init}}-\\psi^{*}\\|^{2}}\\,+\\,\\varepsilon_{n,m;\\nu}^{\\mathrm{init}}(\\epsilon)\\Big)^{2}+\\frac{\\kappa_{\\nu;m}^{2}+\\sigma^{2}}{n}\\Big)\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Proof of Lemma $E.2$ . The main challenge arises from the possible correlation between $\\theta^{\\mathrm{init}}$ and $X_{1},\\ldots,X_{n}$ . First note that for any $\\epsilon>0,\\nu>2$ and a real-valued random variable $Y$ , by Holder's inequality, we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[Y^{2}]\\,=\\mathbb{E}\\bigl[Y^{2}\\,\\mathbb{I}_{\\{Y\\le\\epsilon\\}}+Y^{2}\\,\\mathbb{I}_{\\{Y>\\epsilon\\}}\\bigr]}\\\\ &{\\qquad\\le\\epsilon^{2}+\\mathbb{E}[Y^{2}\\mathbb{I}_{\\{Y>\\epsilon\\}}]\\,\\le\\,\\epsilon^{2}+(\\mathbb{E}[Y^{\\nu}])^{2/\\nu}\\mathbb{P}(Y>\\epsilon)^{(\\nu-2)/\\nu}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Also note the useful inequality that for two real-valued random vectors (possibly correlated) $V_{1},V_{2}$ wehave ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[\\|V_{1}+V_{2}\\|^{2}]\\ \\le\\ \\mathbb{E}[(\\|V_{1}\\|+\\|V_{2}\\|)^{2}]\\ \\le\\mathbb{E}\\|V_{1}\\|^{2}+2\\sqrt{(\\mathbb{E}\\|V_{1}\\|^{2})(\\mathbb{E}\\|V_{2}\\|^{2})}+\\mathbb{E}\\|V_{2}\\|^{2}}\\\\ {\\,=\\left(\\sqrt{\\mathbb{E}\\|V_{1}\\|^{2}}+\\sqrt{\\mathbb{E}\\|V_{2}\\|^{2}}\\right)^{2}\\,.\\qquad\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Now to control the first quantity of interest, by using a triangle inequality, we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left\\|\\mathbb{E}\\big[\\theta_{m}^{\\mathrm{GD}}-\\theta_{\\infty}^{\\mathrm{GD}}\\;\\big|\\;A_{n}\\big]\\right\\|^{2}\\;=\\mathbb{E}\\bigg\\|\\mathbb{E}\\Big[\\frac{\\eta}{n}\\sum_{i\\leq n}\\big(\\phi\\big(K_{i;\\theta^{\\mathrm{init}}}^{m}(X_{i})\\big)-\\phi\\big(X_{i}^{\\theta^{\\mathrm{init}}}\\big)\\big)\\,\\Big|\\,A_{n}\\Big]\\bigg\\|^{2}}\\\\ &{\\overset{(17)}{\\leq}\\eta^{2}\\;\\Big(\\sqrt{\\mathbb{E}[\\Delta_{1}^{2}]}+\\sqrt{\\mathbb{E}[\\Delta_{2}^{2}]}\\Big)^{2}\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Delta_{1}\\!:=\\!\\left\\|\\mathbb{E}\\!\\left[\\frac{1}{n}\\sum_{i\\leq n}\\left(\\phi\\!\\left(K_{i;\\theta^{\\mathrm{init}}}^{m}(X_{i})\\right)-\\mathbb{E}\\!\\left[\\phi\\!\\left(K_{i;\\theta^{\\mathrm{init}}}^{m}(X_{1}^{\\prime})\\right)\\big|\\,\\theta^{\\mathrm{init}}\\right]\\right)\\Big|A_{n}\\right]\\right\\|}\\\\ &{=\\!\\left\\|\\frac{1}{n}\\sum_{i\\leq n}\\left(\\mathbb{E}\\!\\left[\\phi\\!\\left(K_{i;\\theta^{\\mathrm{init}}}^{m}(X_{i})\\right)\\big|\\,\\theta^{\\mathrm{init}},X_{i}\\right]-\\mathbb{E}\\!\\left[\\phi\\!\\left(K_{i;\\theta^{\\mathrm{init}}}^{m}(X_{1}^{\\prime})\\right)\\big|\\,\\theta^{\\mathrm{init}}\\right]\\right)\\right\\|,}\\\\ {\\Delta_{2}\\!:=\\!\\left\\|\\mathbb{E}\\!\\left[\\frac{1}{n}\\sum_{i\\leq n}\\left(\\mathbb{E}\\!\\left[\\phi\\!\\left(K_{i;\\theta^{\\mathrm{init}}}^{m}(X_{1}^{\\prime})\\right)\\big|\\,\\theta^{\\mathrm{init}}\\right]-\\phi\\!\\left(X_{i}^{\\theta^{\\mathrm{init}}}\\!\\right)\\right)\\Big|A_{n}\\right]\\right\\|}\\\\ &{=\\!\\left\\|\\mathbb{E}\\!\\left[\\phi\\!\\left(K_{1;\\theta^{\\mathrm{init}}}^{m}(X_{1}^{\\prime})\\right)-\\phi\\!\\left(X_{1}^{\\theta^{\\mathrm{init}}}\\right)\\big|\\,\\theta^{\\mathrm{init}}\\right]\\right\\|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "and $X_{1}^{\\prime}$ is an i.i.d. copy of $X_{1}$ and in particular independent of $\\theta^{\\mathrm{init}}$ $\\Delta_{1}$ is controlled via (16): ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{E}[\\Delta_{1}^{2}]\\leq\\epsilon^{2}+(\\mathbb{E}[\\Delta_{1}^{\\nu}])^{2/\\nu}\\,\\mathbb{P}(\\Delta_{1}>\\epsilon)^{\\nu/(\\nu-2)}}}\\\\ &{\\overset{(a)}{\\leq}\\epsilon^{2}+\\Big(\\mathbb{E}\\Big\\|\\phi(K_{1;\\theta^{\\mathrm{init}}}^{m}(X_{1}))-\\mathbb{E}\\Big[\\phi(K_{1;\\theta^{\\mathrm{init}}}^{m}(X_{1}^{\\prime}))\\,\\Big|\\,\\theta^{\\mathrm{init}}\\Big]\\Big\\|^{\\nu}\\Big)^{2/\\nu}}\\\\ &{\\quad}&{\\times\\,\\mathbb{P}\\bigg(\\frac{\\big\\|\\sum_{i\\leq n}\\big(\\mathbb{E}\\big[\\phi(K_{i;\\theta^{\\mathrm{init}}}^{m}(X_{i}))\\big|\\,\\theta^{\\mathrm{init}},X_{i}\\big]-\\mathbb{E}[\\phi(K_{i;\\theta^{\\mathrm{init}}}^{m}(X_{1}^{\\prime}))\\,\\big|\\,\\theta^{\\mathrm{init}}]\\big)\\big\\|}{n}>\\epsilon\\bigg)^{\\frac{\\nu-2}{\\nu}}}\\\\ &{\\overset{(b)}{\\leq}\\epsilon^{2}+\\kappa_{\\nu;m}^{2}\\big(\\vartheta_{n,m}^{\\mathrm{init}}(\\epsilon)\\big)^{\\frac{\\nu-2}{\\nu}}}\\\\ &{\\quad}&{=\\big(\\varepsilon_{n,m;\\nu}^{\\mathrm{init}}(\\epsilon)\\big)^{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "In $(a)$ , we have plugged in the definition of $\\Delta_{1}$ and applied a Jensen's inequality with respect to the empirical average; in $(b)$ , we have used A7 to bound the $\\nu$ -th moment term as ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\Big(\\mathbb{E}\\Big\\|\\phi(K_{1;\\theta^{\\mathrm{init}}}^{m}(X_{1}))-\\mathbb{E}\\Big[\\phi(K_{1;\\theta^{\\mathrm{init}}}^{m}(X_{1}^{\\prime}))\\,\\Big|\\,\\theta^{\\mathrm{init}}\\Big]\\Big\\|^{\\nu}\\Big)^{1/\\nu}}\\\\ &{=\\Big(\\mathbb{E}\\,\\Big[\\mathbb{E}\\Big[\\Big\\|\\phi(K_{1;\\theta^{\\mathrm{init}}}^{m}(X_{1}))-\\mathbb{E}\\Big[\\phi(K_{1;\\theta^{\\mathrm{init}}}^{m}(X_{1}^{\\prime}))\\,\\Big|\\,\\theta^{\\mathrm{init}}\\Big]\\Big\\|^{\\nu}\\,\\Big|\\,\\theta^{\\mathrm{init}}\\Big]\\Big]\\Big)^{1/\\nu}}\\\\ &{\\leq\\,\\operatorname*{sup}_{\\psi\\in\\Psi}\\Big(\\mathbb{E}\\Big[\\Big\\|\\phi(K_{1;\\psi}^{m}(X_{1}))-\\mathbb{E}\\big[\\phi(K_{1;\\psi}^{m}(X_{1}^{\\prime}))\\big]\\Big\\|^{\\nu}\\Big]\\Big)^{1/\\nu}}\\\\ &{=\\,\\operatorname*{sup}_{\\psi\\in\\Psi}\\Big(\\mathbb{E}\\Big[\\Big\\|\\phi(K_{1;\\psi}^{m}(X_{1}))-\\mathbb{E}\\big[\\phi(K_{1;\\psi}^{m}(X_{1}))\\big]\\Big\\|^{\\nu}\\Big]\\Big)^{1/\\nu}\\,\\leq\\,\\kappa_{\\nu;m}}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "and recalled the definitions of $\\vartheta_{n,m}^{\\mathrm{init}}$ and einit $\\varepsilon_{n,m;\\nu}^{\\mathrm{init}}$ . On the other hand, ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\{\\Delta_{2}^{(\\Delta)}=\\mathbf{1}\\}\\int_{\\mathbb{R}^{d}}\\phi(x)(K_{1,1,0}^{\\alpha}\\phi(x)(x),x(x)-\\mathbb{E}\\{(X(X_{1}^{\\alpha,\\alpha}))(\\theta^{\\alpha,\\alpha})\\}|^{2}}\\\\ &{\\overset{(a)}{\\leq}\\mathbb{E}\\bigg\\|\\int_{\\mathbb{R}^{d}}(K_{1,1}^{\\alpha}\\phi(x)(x)\\phi(x),(x)\\mathrm{-}\\mathbb{E}\\,\\gamma_{n+1}\\{\\phi\\}|^{2})}\\\\ &{\\overset{(b)}{\\leq}\\mathbb{E}\\bigg\\|\\int_{\\mathbb{R}^{d}}\\left(K_{1,0}^{\\alpha}(\\phi-\\mathbb{E}_{\\mathbb{R}^{n}}[\\phi])(x)\\right)(x)\\exp_{r}(x)\\phi_{r}(x)d r\\bigg\\|^{2}}\\\\ &{\\overset{(c)}{\\leq}\\mathbb{E}\\bigg\\|\\int_{\\mathbb{R}^{d}}(K_{1,0}^{\\alpha}\\phi(\\cdot-\\mathbb{E}_{\\mathbb{R}^{n}}[\\phi]))(x)\\times(p_{r}(x)-p_{\\theta^{\\alpha}}(x))d r\\bigg\\|^{2}}\\\\ &{\\overset{(c)}{\\leq}\\sum_{i=1}^{d}\\mathbb{E}\\bigg(\\int_{\\mathbb{R}^{d}}\\left(K_{1,0}^{\\alpha}\\phi(\\cdot-\\mathbb{E}_{\\mathbb{R}^{n}}[\\phi])\\right)(x)^{\\top}\\sigma_{1}\\times p_{\\theta^{\\alpha}}(x)\\times\\frac{p_{r}(x)-p_{\\theta^{\\alpha}}(x)}{p_{\\theta^{\\alpha}}(x)}d x\\bigg)^{i}}\\\\ &{\\overset{(c)}{\\leq}\\sum_{i=1}^{d}\\mathbb{E}\\bigg[\\Big(\\int_{\\mathbb{R}^{d}}\\left(K_{1,0}^{\\alpha}\\rho(\\phi-\\mathbb{E}_{\\mathbb{R}^{n}}[\\phi])\\right)(x)^{\\top}\\sigma_{1}^{\\alpha}\\gamma_{2}\\overline{{\\gamma_{\\theta^{\\alpha}}(x)}}(x)d x}\\\\ &{\\qquad\\qquad\\qquad\\quad\\times\\int_{\\mathbb{R}^{d}}\\left(\\frac{p_{r}(x)-p_{\\theta^{\\alpha}}(x)}{p_{\\theta^{\\alpha}}(x)}\\right)^{2}p_{\\theta^{\\alpha}}(x)d r\\bigg)^{ \n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "In $(a)$ , we have used that $\\begin{array}{r}{(K f)(x)\\,=\\,\\int K(x,y)f(y)d y}\\end{array}$ ; in $(b)$ , we have used that the Markov operator leaves the constant function invariant; in $(c)$ , we used that $K_{\\mathrm{1;\\theta^{\\mathrm{init}}}}$ leaves $p_{\\theta^{\\mathrm{init}}}$ invariant; in $(d)$ : we denoted $(e_{l})_{l\\leq d}$ as the standard basis vectors of $\\mathbb{R}^{d}$ and multiplied and divided by $p_{\\theta^{\\mathrm{init}}}(x)$ in $(e)$ , we have used a Cauchy-Schwarz inequality; in $(f)$ , we have used the definition of the spectral gap $\\alpha$ in A3; in $(g)$ , we have used A1 and A2. Combining the bounds gives the first inequality that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left\\|\\mathbb{E}\\big[\\theta_{m}^{\\mathrm{GD}}\\!-\\!\\theta_{\\infty}^{\\mathrm{GD}}\\,\\big|\\,A_{n}\\big]\\right\\|^{2}\\;\\leq\\;\\eta^{2}\\Big(\\alpha^{m}\\sigma C_{\\chi}\\,\\sqrt{\\mathbb{E}\\|\\theta^{\\mathrm{init}}-\\psi^{*}\\|^{2}}\\,+\\,\\varepsilon_{n,m;\\nu}^{\\mathrm{init}}(\\epsilon)\\Big)^{2}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Now we handle the second quantity by conditioning on $\\theta^{\\mathrm{init}}$ and perform a bias-variance decomposition: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left\\|\\theta_{m}^{\\mathrm{GD}}-\\theta_{\\infty}^{\\mathrm{GD}}\\right\\|^{2}\\,=\\eta^{2}\\,\\mathbb{E}\\Big[\\mathbb{E}\\Big[\\Big\\|\\frac{1}{n}\\sum_{i\\leq n}\\left(\\phi\\big(K_{i;\\theta^{\\mathrm{init}}}^{m}(X_{i})\\big)-\\phi(X_{i}^{\\theta^{\\mathrm{init}}})\\big)\\Big\\|^{2}\\,\\Big|\\,\\mathcal{A}_{n}\\Big]\\,\\Big]}\\\\ &{=\\eta^{2}(Q_{B}+Q_{V})\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q_{B}:=\\mathbb{E}\\bigg\\|\\mathbb{E}\\bigg[\\frac{1}{n}\\sum_{i\\leq n}\\big(\\phi\\big(K_{i;\\theta^{\\mathrm{init}}}^{m}(X_{i})\\big)-\\phi(X_{i}^{\\theta^{\\mathrm{init}}})\\big)\\biggm|A_{n}\\bigg]\\bigg\\|^{2}\\,,}\\\\ &{Q_{V}:=\\mathbb{E}\\Big[\\mathrm{Tr}\\Big(\\mathrm{Cov}\\Big[\\frac{1}{n}\\sum_{i\\leq n}\\phi\\big(K_{i;\\theta^{\\mathrm{init}}}^{m}(X_{i})\\big)\\Big|A_{n}\\Big]+\\mathrm{Cov}\\Big[\\frac{1}{n}\\sum_{i\\leq n}\\phi(X_{i}^{\\theta^{\\mathrm{init}}})\\Big|\\theta^{\\mathrm{init}}\\Big]\\Big)\\Big]\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Note that the covariance terms separate because $X_{i}^{\\theta^{\\mathrm{init}}}$ is independent of $K_{i;\\theta^{\\mathrm{init}}}^{m}(X_{i})$ conditioning on $\\theta^{\\mathrm{init}}$ $\\eta^{2}Q_{B}$ is exactly the quantity controlled above, so it suffices to bound the variance term $Q_{V}$ . By explicitly computing the second covariance term while noting that $X_{1}^{\\theta^{\\mathrm{init}}},\\dots,X_{n}^{\\theta^{\\mathrm{init}}}$ are conditionallyid. given $\\theta^{\\mathrm{ini\\overline{{t}}}}$ and $K_{i;\\theta^{\\mathrm{init}}}^{m}(X_{i})$ 's areconditionally independent aeros $1\\leq i\\leq n$ given $A_{n}$ , we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q_{V}\\;=\\frac{\\sum_{i\\leq n}\\mathbb{E}[\\mathrm{Tr}\\cos[\\phi(K_{i;\\theta^{\\mathrm{init}}}^{m}(X_{i}))\\,|\\,\\theta^{\\mathrm{init}},X_{i}]]}{n^{2}}+\\frac{\\mathbb{E}\\left[\\mathrm{Tr}\\cos[\\phi(X_{1}^{\\theta^{\\mathrm{init}}})\\,|\\theta^{\\mathrm{init}}]\\right]}{n}}\\\\ &{\\stackrel{(a)}{=}\\frac{\\mathbb{E}[\\mathrm{Tr}\\cos[\\phi(K_{1;\\theta^{\\mathrm{init}}}^{m}(X_{1}))\\,|\\,\\theta^{\\mathrm{init}},X_{1}]]}{n}+\\frac{\\mathbb{E}\\left[\\mathrm{Tr}\\cos[\\phi(X_{1}^{\\theta^{\\mathrm{init}}})\\,|\\theta^{\\mathrm{init}}]\\right]}{n}}\\\\ &{\\;\\;\\leq\\frac{\\kappa_{\\nu;m}^{2}+\\sigma^{2}}{n}\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where we have used A4, A7 and A1 in the last line. Combining the bounds, we obtain that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\|\\theta_{m}^{\\mathrm{GD}}-\\theta_{\\infty}^{\\mathrm{GD}}\\|^{2}\\,=\\eta^{2}(Q_{B}+Q_{V})}\\\\ &{\\qquad\\qquad\\qquad\\le\\eta^{2}\\Bigl(\\Bigl(\\alpha^{m}\\sigma C_{\\chi}\\,\\sqrt{\\mathbb{E}\\|\\theta^{\\mathrm{init}}-\\psi^{*}\\|^{2}}\\,+\\,\\varepsilon_{n,m;\\nu}^{\\mathrm{init}}(\\epsilon)\\Bigr)^{2}+\\frac{\\kappa_{\\nu;m}^{2}+\\sigma^{2}}{n}\\Bigr)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Lemma E.3. Under Al, $\\begin{array}{r}{\\mathbb{E}\\,\\|\\theta_{\\infty}^{\\mathrm{GD}}-\\theta^{\\mathrm{pop}}\\|^{2}\\,\\leq\\,\\frac{4\\eta^{2}\\sigma^{2}}{n}\\;.}\\end{array}$ ", "page_idx": 38}, {"type": "text", "text": "Proof of Lemma E.3. Since both $F_{\\infty}^{\\mathrm{GD}}$ and $f^{\\mathrm{pop}}$ involve infinite-length Markov chains, the initializations do not matter and we can decouple the stochasticity of $X_{i}$ and $K_{i;\\theta^{\\mathrm{init}}}$ . In particular, ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\,\\|\\theta_{\\infty}^{\\mathrm{GD}}-\\theta^{\\mathrm{pop}}\\|^{2}\\,=\\eta^{2}\\,\\mathbb{E}\\left\\|\\frac{1}{n}\\sum_{i\\leq n}\\big(\\phi(X_{i})-\\phi\\big(X_{i}^{\\theta^{\\mathrm{init}}}\\big)\\big)-\\mathbb{E}\\big[\\phi(X_{1})-\\phi\\big(X_{1}^{\\theta^{\\mathrm{init}}}\\big)\\big]\\right\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\eta^{2}\\big(Q_{1}^{\\prime}+2\\sqrt{Q_{1}^{\\prime}Q_{2}^{\\prime}}+Q_{2}^{\\prime}\\big)~,}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q_{1}^{\\prime}\\;:=\\mathbb{E}\\left\\|\\frac{1}{n}\\sum_{i\\leq n}\\big(\\phi(X_{i})-\\mathbb{E}[\\phi(X_{1})]\\big)\\right\\|^{2}\\;=\\;\\frac{\\mathrm{Tr}\\,\\mathrm{Cov}[\\phi(X_{1})]}{n}\\;=\\;\\frac{\\mathrm{Tr}\\,\\nabla_{\\theta}^{2}\\log Z(\\psi^{*})}{n}\\;\\leq\\;\\frac{\\sigma^{2}}{n}\\;,}\\\\ &{Q_{2}^{\\prime}\\;:=\\mathbb{E}\\left\\|\\frac{1}{n}\\sum_{i\\leq n}\\big(\\phi(X_{i}^{\\theta^{\\mathrm{init}}})-\\mathbb{E}\\big[\\phi\\big(X_{1}^{\\theta^{\\mathrm{init}}}\\big)\\big]\\big)\\right\\|^{2}}\\\\ &{\\qquad=\\frac{\\mathbb{E}\\left[\\mathrm{Tr}\\,\\mathrm{Cov}[\\phi(X_{1}^{\\theta^{\\mathrm{init}}})|\\theta^{\\mathrm{init}}]\\right]}{n}\\;=\\;\\frac{\\mathbb{E}\\left[\\mathrm{Tr}\\,\\nabla_{\\theta}^{2}\\log Z(\\theta^{\\mathrm{init}})\\right]}{n}\\;\\leq\\;\\frac{\\sigma^{2}}{n}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "In the computations above, we have used the relation $\\nabla_{\\theta}^{2}\\log Z(\\theta)\\;=\\;\\operatorname{Cov}_{X\\sim p_{\\theta}}[\\phi(X)]$ and the assumption $\\operatorname*{sup}_{\\theta\\in\\Psi}\\operatorname{tr}(\\nabla_{\\theta}^{2}\\log Z(\\theta))=\\sigma^{2}$ from A1. This implies the desired bound. \u53e3 ", "page_idx": 38}, {"type": "text", "text": "Lemma E.4. Under $4l,\\mathbb{E}\\left\\|\\theta^{\\mathrm{pop}}-\\psi^{*}\\right\\|^{2}\\leq\\left(1-2\\mu\\eta+L^{2}\\eta^{2}\\right)\\mathbb{E}\\left\\|\\theta^{\\mathrm{init}}-\\psi^{*}\\right\\|^{2}\\,.$ ", "page_idx": 38}, {"type": "text", "text": "Proof of Lemma E.4. Recall that ", "page_idx": 38}, {"type": "equation", "text": "$$\nf^{\\mathrm{pop}}(\\theta^{\\prime})\\;=\\;\\theta-\\eta\\,\\mathbb{E}\\big[\\phi(X_{1})-\\phi(X_{1}^{\\theta^{\\prime}})\\big]\\;=\\;\\theta-\\eta\\,\\big(\\nabla_{\\psi}\\log Z(\\psi^{*})-\\nabla_{\\psi}\\log Z(\\theta^{\\prime})\\big)\\ .\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "By construction, $f^{\\mathrm{pop}}$ is deterministic and $f^{\\mathrm{pop}}(\\psi^{*})\\,=\\,\\psi^{*}$ . By plugging in the recursions and expanding the square, we get that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left\\|\\theta^{\\mathrm{pop}}-\\psi^{*}\\right\\|^{2}\\;=\\;\\mathbb{E}\\left\\|f^{\\mathrm{pop}}(\\theta^{\\mathrm{init}})-f^{\\mathrm{pop}}(\\psi^{*})\\right\\|^{2}}\\\\ &{\\;\\;\\;=\\mathbb{E}\\left\\|(\\theta^{\\mathrm{init}}-\\psi^{*})-\\eta(\\nabla_{\\psi}\\log Z(\\theta^{\\mathrm{init}})-\\nabla_{\\psi}\\log Z(\\psi^{*}))\\right\\|^{2}}\\\\ &{\\;\\;\\;=\\mathbb{E}\\left\\|\\theta^{\\mathrm{init}}-\\psi^{*}\\right\\|^{2}-2\\eta\\,\\mathbb{E}\\big[\\langle\\theta^{\\mathrm{init}}-\\psi^{*}\\,,\\,\\nabla_{\\psi}\\log Z(\\theta^{\\mathrm{init}})-\\nabla_{\\psi}\\log Z(\\psi^{*})\\rangle\\big]}\\\\ &{\\;\\;\\;\\;\\;+\\;\\eta^{2}\\,\\mathbb{E}\\left\\|\\nabla_{\\psi}\\log Z(\\theta^{\\mathrm{init}})-\\nabla_{\\psi}\\log Z(\\psi^{*})\\right\\|^{2}}\\\\ &{\\;\\;\\;\\leq\\mathbb{E}\\left\\|\\theta^{\\mathrm{init}}-\\psi^{*}\\right\\|^{2}-2\\mu\\eta\\,\\mathbb{E}\\left\\|\\theta^{\\mathrm{init}}-\\psi^{*}\\right\\|^{2}+L^{2}\\eta^{2}\\mathbb{E}\\left\\|\\theta^{\\mathrm{init}}-\\psi^{*}\\right\\|^{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Inthe last line, we have recalled $\\mathrm{inf}_{\\psi\\in\\Psi}\\,\\lambda_{\\mathrm{min}}\\bigl(\\nabla_{\\psi}^{2}\\log Z(\\psi)\\bigr)\\qquad=\\qquad\\mu$ and $\\operatorname*{sup}_{\\theta\\in\\Psi}\\lambda_{\\operatorname*{max}}\\big(\\nabla_{\\psi}^{2}\\log Z(\\psi)\\big)=L$ by A1 and applied Lemma C.8. Combining the coefficients gives the desired statement. \u53e3 ", "page_idx": 39}, {"type": "text", "text": "F Proofs for offline SGD ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "We prove Theorem B.1 (which directly implies Theorem 4.3) and Theorem B.2 in this section. The key ingredient of both proofs is Lemma F.1 below, which provides an iterative error bound for the SGD-with-replacement scheme by combining different approximation bounds in Appendix E. Throughout this section, we denote oS.GI $\\delta_{t,j}^{\\mathrm{SGDw}}:=\\mathbb{E}\\left\\|\\psi_{t,j}^{\\mathrm{SGDw}}-\\psi^{*}\\right\\|^{2}$ ", "page_idx": 39}, {"type": "text", "text": "Lemma F.1. Under A1, A2, A3, A4 and A7, we have that for $1\\leq j\\leq N-1,$ ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sqrt{\\delta_{t,j}^{\\mathrm{SGDw}}}\\ \\leq\\ \\left(1-\\eta_{t}\\Big(\\mu-\\alpha^{m}\\sigma C_{\\chi}-\\frac{L^{2}}{2}\\eta_{t}\\Big)\\right)\\sqrt{\\delta_{t,j-1}^{\\mathrm{SGDw}}}\\ +\\ \\eta_{t}\\Big(\\varepsilon_{n,m,t;\\nu}^{\\mathrm{SGDw}}(\\epsilon)+\\frac{5\\sigma+5\\kappa_{\\nu;m}}{\\sqrt{B}}\\Big)\\ ,}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where $\\begin{array}{r}{1-\\eta_{t}\\big(\\mu-\\alpha^{m}\\sigma C_{\\chi}-\\frac{L^{2}}{2}\\eta_{t}\\big)>0\\,.}\\end{array}$ ", "page_idx": 39}, {"type": "text", "text": "does not increase osG $\\delta_{t,j}^{\\mathrm{SGDw}}$ $\\psi_{t,j}^{\\mathrm{sGDw}}$ To apply theresuls from Appendi , we ideniy $\\theta^{\\mathrm{init}}=\\psi_{t,j-1}^{\\mathrm{SGDw}}$ sGDw and n = Nt, which allows us to Wrte $\\psi_{t,j}^{\\mathrm{SGDw}}=\\theta_{m,B}^{\\mathrm{SGDw}}$ $\\mathbb{E}\\lVert\\theta^{\\mathrm{init}}-\\psi^{*}\\rVert^{2}=\\delta_{t,j-1}^{\\mathrm{SGDw}}$ and $\\varepsilon_{n,m;\\nu}^{\\mathrm{init}}(\\epsilon)\\leq\\varepsilon_{\\nu;n,m,t}^{\\mathrm{SGDw}}(\\epsilon)$ ", "page_idx": 39}, {"type": "text", "text": "$\\begin{array}{r l}{=}&{\\mathbb{E}\\left\\lVert\\rho_{t}^{(1)}\\widehat{u}_{t}^{(2)}-\\rho_{t}^{(1)}\\widehat{u}_{t}^{(1)}-\\rho_{t}^{(1)}-\\sigma_{t}^{(2)}+\\sigma_{t}^{(3)}-\\sigma_{t}^{(2)}+\\sigma_{t}^{(3)}-\\sigma_{t}^{(1)}\\right\\rVert^{2}}\\\\ &{\\quad+\\mathbb{E}\\left\\lVert\\rho_{t}^{(1)}\\widehat{u}_{t}^{(2)}-\\rho_{t}^{(1)}\\right\\rVert^{2}+\\mathbb{E}\\left\\lVert\\rho_{t}^{(1)}-\\rho_{t}^{(1)}\\right\\rVert^{2}+\\mathbb{E}\\left\\lVert\\rho_{t}^{(2)}-\\rho_{t}^{(1)}+\\frac{\\sigma_{t}^{(3)}}{\\sigma_{t}^{(2)}}\\right\\rVert^{2}+\\mathbb{E}\\left\\lVert\\rho_{t}^{(2)}-\\rho_{t}^{(1)}\\right\\rVert^{2}}\\\\ &{\\quad+2\\mathbb{E}\\left(\\rho_{t}^{(1)}-\\rho_{t}^{(1)}-\\sigma_{t}^{(3)}-\\rho_{t}^{(1)}+\\sigma_{t}^{(2)}-\\sigma_{t}^{(3)}-\\sigma_{t}^{(2)}+2\\mathbb{E}\\left(\\rho_{t}^{(1)}-\\rho_{t}^{(1)}-\\sigma_{t}^{(2)}\\right)}\\\\ &{\\quad+2\\mathbb{E}\\left(\\rho_{t}^{(1)}-\\rho_{t}^{(1)}-\\sigma_{t}^{(3)}-\\sigma_{t}^{(1)}+\\sigma_{t}^{(3)}-\\sigma_{t}^{(1)}+2\\mathbb{E}\\left(\\rho_{t}^{(1)}-\\rho_{t}^{(2)}-\\sigma_{t}^{(3)}-\\sigma_{t}^{(3)}\\right)}\\\\ &{\\quad+2\\mathbb{E}\\left(\\rho_{t}^{(1)}-\\rho_{t}^{(1)}\\right)\\lambda_{t}\\lambda_{t}\\left(\\sigma_{t}^{(2)}-\\sigma_{t}^{(3)}+\\sigma_{t}^{(3)}\\right)+2\\mathbb{E}\\left(\\rho_{t}^{(1)}-\\rho_{t}^{(1)},\\sigma_{t}^{(2)}-\\sigma_{t}^{(3)} $ $\\begin{array}{r l}&{\\quad+2\\frac{\\sqrt{\\alpha}}{2}\\sqrt{\\left(1-2\\gamma\\right)\\left(\\frac{\\eta}{\\sqrt{\\alpha}}\\right)}-1(\\alpha\\cdot\\infty)\\sqrt{\\left(\\sigma^{2}-2\\gamma\\right)}\\frac{\\sqrt{\\alpha}}{\\sqrt{\\alpha}}+\\frac{\\sqrt{\\alpha}}{\\sqrt{\\alpha}}\\sqrt{\\beta+\\frac{\\gamma}{2}}(\\sigma^{2}-\\gamma_{\\mathrm{on}}/\\sigma^{2})}\\\\ &{\\quad+2\\frac{\\sqrt{\\alpha}}{2}\\sqrt{\\alpha}+\\frac{\\sqrt{\\alpha}}{2}\\sqrt{\\left(1-2\\gamma\\right)}\\frac{\\sqrt{\\alpha}}{\\sqrt{\\alpha}}+0}\\\\ &{\\quad+2\\eta\\left(\\sigma^{2}\\alpha\\sqrt{\\gamma}\\sqrt{\\frac{\\beta}{\\sqrt{\\alpha}}}\\frac{\\sqrt{\\alpha}}{\\sqrt{\\alpha}}\\right)+\\frac{\\sqrt{\\alpha}}{\\sqrt{\\alpha}}\\sqrt{\\left(1-\\gamma\\right)\\left(\\frac{\\sqrt{\\alpha}}{\\sqrt{\\alpha}}+\\frac{\\sqrt{\\alpha}}{\\sqrt{\\alpha}}\\right)}\\frac{\\sqrt{\\alpha}}{\\sqrt{\\alpha}}}\\\\ &{\\quad+2\\eta\\left(\\sigma^{2}\\alpha\\sqrt{\\gamma}\\sqrt{\\beta}\\frac{\\sqrt{\\alpha}}{\\sqrt{\\alpha}}\\right)+\\frac{\\sqrt{\\alpha}}{\\sqrt{\\alpha}}\\sqrt{\\alpha}\\frac{\\sqrt{\\alpha}}{\\sqrt{\\alpha}},}\\\\ &{\\quad+2\\frac{\\sqrt{\\alpha}}{2}\\sqrt{\\left(1-2\\gamma\\right)}\\sqrt{\\left(1-2\\gamma\\right)}\\frac{\\sqrt{\\alpha}}{\\sqrt{\\alpha}}+\\frac{\\sqrt{\\alpha}}{\\sqrt{\\alpha}}\\sqrt{\\beta+\\frac{\\gamma}{2}}\\sqrt{\\alpha},}\\\\ &{\\quad\\geq\\left(1-2\\gamma\\right)\\alpha+\\frac{\\sqrt{\\alpha}}{\\sqrt{\\alpha}}+\\frac{\\sqrt{\\alpha}^{2}\\sqrt{\\alpha}}{\\sqrt{\\alpha}}\\gamma^{2}+2\\gamma\\alpha^{2}\\sigma^{2}\\sqrt{\\gamma}\\sqrt{1-2\\gamma\\right)+D_{\\alpha}^{2}\\sigma^{2}}}\\\\ &{\\quad+2\\left(\\eta^{2}\\alpha^{2}\\sigma\\sqrt{\\alpha}\\frac{\\sqrt{\\alpha}}{\\sqrt{\\alpha}}\\frac{\\sqrt{\\alpha}}{\\sqrt{\\alpha}}\\right)\\left(1+\\frac{2\\sqrt{\\alpha}}{\\sqrt{\\alpha}}\\sqrt{\\alpha}+\\frac{\\sqrt{\\alpha}}{\\sqrt{\\alpha}}\\right)\\sqrt{\\alpha}+\\frac{\\sqrt{\\alpha}}{\\sqrt{\\alpha}}\\sqrt{\\alpha},}$ ", "page_idx": 40}, {"type": "text", "text": "In $(a)$ , we have expanded the square, used ${\\mathcal{F}}_{n}$ defined in Lemma E.1 and $A_{n}$ defined in Lemma E.2, and noted that $\\theta^{\\mathrm{pop}}\\mathrm{~-~}\\psi^{*}$ is almost surely constant given $\\theta^{\\mathrm{init}}$ ; in $(b)$ , we have applied Lemmas E.1, E.2, E.3 and E.4 under A1, A2, A3, A4 and A7, and used ${\\sqrt{a+b}}\\leq{\\sqrt{a}}+{\\sqrt{b}}$ for $a,b\\ge0$ in $(c)$ we have grouped the termsby powers of $\\delta_{t,j-1}^{\\mathrm{SGDw}}$ bounded the indicator function from above by 1 and used $B\\,\\leq\\,n$ to replace $n$ in the denominator by $B$ . While the computation above is complicated, we remark that the key steps are taking the conditional expectations for the crossterms in $(b)$ , which gives us a tighter bound than directly applying a triangle inequality of the form $\\mathbb{E}\\|Y_{1}+Y_{2}\\|^{2}\\leq(\\sqrt{\\mathbb{E}\\|Y_{1}\\|^{2}}+\\sqrt{\\mathbb{E}\\|Y_{2}\\|^{2}})$ . To further simplify the bounds, we seek to bound each ", "page_idx": 40}, {"type": "text", "text": "coefficient by a square, which yields ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bigl(A_{1}\\bigr)\\,=\\,\\Bigl(\\eta_{t}\\alpha^{m}\\sigma C_{\\chi}+\\sqrt{1-2\\mu\\eta_{t}+L\\eta_{t}^{2}}\\Bigr)^{2}\\,,}\\\\ &{\\bigl(A_{2}\\bigr)\\,\\le\\,2\\Bigl(\\eta_{t}\\alpha^{m}\\sigma C_{\\chi}+\\sqrt{1-2\\mu\\eta_{t}+L\\eta_{t}^{2}}\\Bigr)\\,\\times\\,\\eta_{t}\\Bigl(\\varepsilon_{n,m,t;\\nu}^{\\mathrm{SGDw}}\\bigl(\\epsilon\\bigr)+\\frac{4\\sigma+2\\kappa_{\\nu;m}}{\\sqrt{B}}\\Bigr)\\,,}\\\\ &{\\bigl(A_{3}\\bigr)\\,\\le\\,\\eta_{t}^{2}\\Bigl(\\bigl(\\varepsilon_{n,m,t;\\nu}^{\\mathrm{SGDw}}\\bigl(\\epsilon\\bigr)\\bigr)^{2}+\\frac{8\\sigma^{2}+4\\kappa_{\\nu;m}^{2}}{\\sqrt{B}}\\varepsilon_{n,m,t;\\nu}^{\\mathrm{SGDw}}\\bigl(\\epsilon\\bigr)+\\frac{21\\sigma^{2}+17\\kappa_{\\nu;m}^{2}}{B}\\Bigr)}\\\\ &{\\qquad\\le\\eta_{t}^{2}\\Bigl(\\varepsilon_{n,m,t;\\nu}^{\\mathrm{SGDw}}\\bigl(\\epsilon\\bigr)+\\frac{5\\sigma+5\\kappa_{\\nu;m}}{\\sqrt{B}}\\Bigr)^{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "This implies ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\delta_{t,j}^{\\mathrm{SGDw}}\\,\\leq(A_{1})\\times\\delta_{t,j-1}^{\\mathrm{SGDw}}+(A_{2})\\times\\sqrt{\\delta_{t,j-1}^{\\mathrm{SGDw}}}+(A_{3})}\\\\ &{\\qquad\\qquad\\leq\\left(\\left(\\eta_{t}\\alpha^{m}\\sigma C_{x}+\\sqrt{1-2\\mu\\eta_{t}+L\\eta_{t}^{2}}\\right)\\sqrt{\\delta_{t,j-1}^{\\mathrm{SGDw}}}+\\eta_{t}\\left(\\varepsilon_{n,m,t;\\nu}^{\\mathrm{SGDw}}(\\epsilon)+\\frac{5\\sigma+5\\kappa\\nu;m}{\\sqrt{B}}\\right)\\right)^{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Now note that since $\\mu\\leq L$ by the definitions in A1, $2\\mu\\eta_{t}-L^{2}\\eta_{t}^{2}\\leq2L\\eta_{t}-L^{2}\\eta_{t}^{2}\\leq1$ . By using $\\sqrt{1-x}\\leq1-\\frac{x}{2}$ for all $x\\leq1$ , we get that ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\sqrt{1-2\\mu\\eta_{t}+L^{2}\\eta_{t}^{2}}\\ \\leq\\ 1-\\mu\\eta_{t}+\\frac{L^{2}}{2}\\eta_{t}^{2}\\ .\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Substituting this into the earlier bound and taking a square-root, we obtain the desired bound that ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sqrt{\\delta_{t,j}^{\\mathrm{SGDw}}}\\ \\leq\\ \\left(1-\\eta_{t}\\Big(\\mu-\\alpha^{m}\\sigma C_{\\chi}-\\frac{L^{2}}{2}\\eta_{t}\\Big)\\right)\\sqrt{\\delta_{t,j-1}^{\\mathrm{SGDw}}}\\ +\\ \\eta_{t}\\Big(\\varepsilon_{n,m,t;\\nu}^{\\mathrm{SGDw}}(\\epsilon)+\\frac{5\\sigma+5\\kappa_{\\nu;m}}{\\sqrt{B}}\\Big)\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Moreover, since $L\\geq\\mu$ by definition from A1, we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r}{1-\\eta_{t}\\Big(\\mu-\\alpha^{m}\\sigma C_{\\chi}-\\frac{L^{2}}{2}\\eta_{t}\\Big)\\ =\\Big(\\frac{L}{\\sqrt{2}}\\eta_{t}-1\\Big)^{2}+(\\sqrt{2}\\,L-\\mu)\\eta_{t}+\\alpha^{m}\\sigma C_{\\chi}\\eta_{t}\\ >\\ 0\\ ,}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "which finishes the proof. ", "page_idx": 41}, {"type": "text", "text": "F.1 Proof of Theorem B.1 ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Under A1, A2, A3, A4 and A7, Lemma F.1 implies ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sqrt{\\delta_{t,j}^{\\mathrm{SGDw}}}\\,\\le\\left(1-\\tilde{\\mu}_{m}C t^{-\\beta}+\\frac{L^{2}C^{2}}{2}t^{-2\\beta}\\right)\\sqrt{\\delta_{t,j-1}^{\\mathrm{SGDw}}}\\,+\\,C t^{-\\beta}\\,\\sigma_{n,T}^{\\mathrm{SGDw}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "for $1\\leq t\\leq T$ and $1\\le j\\le N$ , where we have used $\\tilde{\\mu}_{m}=\\mu-\\alpha^{m}\\sigma C_{\\chi}$ $\\eta_{t}=C t^{-\\beta}$ and ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\varepsilon_{n,m,t;\\nu}^{\\mathrm{SGDw}}(\\epsilon)+\\frac{5\\sigma+5\\kappa_{\\nu;m}}{\\sqrt{B}}\\ \\leq\\ \\varepsilon_{n,m,T;\\nu}^{\\mathrm{SGDw}}(\\epsilon)+\\frac{5\\sigma+5\\kappa_{\\nu;m}}{\\sqrt{B}}\\ =\\ \\sigma_{n,T}^{\\mathrm{SGDw}}\\ .\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "By an induction on $j=1,\\ldots,N$ , we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sqrt{\\delta_{t,N}^{\\mathrm{SGDw}}}\\,\\le\\left(1-\\tilde{\\mu}_{m}C t^{-\\beta}+\\frac{L^{2}C^{2}}{2}t^{-2\\beta}\\right)^{N}\\sqrt{\\delta_{t,0}^{\\mathrm{SGDw}}}\\qquad\\qquad}\\\\ {\\qquad\\qquad+\\,\\,C\\sigma_{n,T}^{\\mathrm{SGDw}}\\,t^{-\\beta}\\,\\sum_{j=1}^{N}\\left(1-\\tilde{\\mu}_{m}C t^{-\\beta}+\\frac{L^{2}C^{2}}{2}t^{-2\\beta}\\right)^{j-1}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "By noting that osGDw $\\delta_{t,0}^{\\mathrm{SGDw}}=\\delta_{t-1,N}^{\\mathrm{SGDw}}$ almost surely for $t\\geq2$ and using another induction on $t=1,\\dots,T$ ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sqrt{\\delta_{T,N}^{\\mathrm{SGDw}}}\\,\\le\\,Q_{0}\\,\\sqrt{\\delta_{0,0}^{\\mathrm{SGDw}}}\\,+\\,C\\sigma_{n,T}^{\\mathrm{SGDw}}\\sum_{t=1}^{T}Q_{t}\\,A_{t}\\,t^{-\\beta}\\ ,}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where ", "page_idx": 41}, {"type": "text", "text": "$\\begin{array}{r}{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! $ Note that by Lemma 1 we also have that $1-\\tilde{\\mu}_{m}C t^{-\\beta}+\\frac{L^{2}C^{2}}{2}t^{-2\\beta}>0$ This allows us to aply Lemma C.3 to obtain $\\begin{array}{r}{\\kappa_{0}\\;\\leq\\;\\exp\\Big(1-N\\tilde{\\mu}_{m}C\\varphi_{1-\\beta}(T+1)+\\frac{N L^{2}C^{2}}{2}\\varphi_{1-2\\beta}(T+1)\\Big)\\;=\\;E_{1}^{T,N}\\;.}\\end{array}$ ", "page_idx": 41}, {"type": "text", "text": "To control $\\textstyle\\sum_{t=1}^{T}Q_{t}A_{t}t^{-\\beta}$ , recallthat $\\begin{array}{r}{\\beta\\in[0,1],\\tilde{\\mu}_{m}>0,\\frac{L^{2}C^{2}}{2}>0}\\end{array}$ and that ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r}{E_{2}^{T,N}\\,=\\,\\exp\\Big(-\\frac{N\\tilde{\\mu}_{m}C}{2}\\varphi_{1-\\beta}(T+1)+2N L^{2}C^{2}\\varphi_{1-2\\beta}(T+1)\\Big)\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "We can now apply Lemma C.3: If $\\beta\\notin\\{\\textstyle{\\frac{1}{2}},1\\}$ , then ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sum_{t=1}^{T}Q_{t}\\,A_{t}\\,t^{-\\beta}\\,\\le\\frac{2^{2\\beta+1}}{\\tilde{\\mu}_{m}C}e^{\\frac{\\tilde{\\mu}_{m}C}{2(1-\\beta)}\\frac{N}{(T+1)^{\\beta}}}+\\frac{3^{\\beta}(1+\\tilde{\\mu}_{m}C)^{N-1}(T+2)^{\\beta}}{L^{2}C^{2}}\\,E_{2}^{T,N}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "$\\beta=\\textstyle{\\frac{1}{2}}$ , i.e. $2\\beta=1$ , we have ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sum_{t=1}^{T}Q_{t}\\,A_{t}\\,t^{-\\beta}\\,\\leq\\frac{4}{\\tilde{\\mu}_{m}C}e^{\\frac{\\tilde{\\mu}_{m}C N}{(T+1)^{1/2}}}+2N(1+\\tilde{\\mu}_{m}C)^{N-1}\\varphi_{\\frac{1}{2}-L^{2}C^{2}N}(T+1)\\,E_{2}^{T,N}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "If $\\beta=1$ , we get that ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sum_{t=1}^{T}Q_{t}\\,A_{t}\\,t^{-\\beta}\\ \\leq\\frac{4}{\\tilde{\\mu}_{m}C}+\\frac{3N\\left(1+\\frac{L^{2}C^{2}}{2}\\right)^{N-1}e^{2L^{2}C^{2}N}\\,\\log\\left(T+1\\right)}{(T+1)^{(\\tilde{\\mu}_{m}C N)/2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Substituting the bounds into (19), we get the desired bound that </ $\\sqrt{\\delta_{T,N}^{\\mathrm{SGDw}}}$ sGDw is upper bounded by ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(E_{1}^{T,N}\\sqrt{\\delta_{0,0}^{\\mathrm{SGDw}}}\\right.+\\left.C\\sigma_{n,T}^{\\mathrm{SGDw}}\\Big(\\frac{4e^{\\frac{-\\beta_{m}C N}{(T+1)^{1/2}}}}{\\tilde{\\mu}_{m}C}+2N(1+\\tilde{\\mu}_{m}C)^{N-1}\\varphi_{\\frac{1}{2}-L^{2}C^{2}N}(T+1)\\,E_{2}^{T,N}\\Big)\\right.}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\left.\\mathrm{for}\\,\\beta=\\frac{1}{2}\\,,}\\\\ &{\\left.E_{1}^{T,N}\\sqrt{\\delta_{0,0}^{\\mathrm{SGDw}}}+C\\sigma_{n,T}^{\\mathrm{SGDw}}\\Big(\\frac{4}{\\tilde{\\mu}_{m}C}+\\frac{3N(1+\\frac{L^{2}C^{2}}{2})^{N-1}}{(T+1)(\\tilde{\\mu}_{m}C N)/2}\\log(T+1)\\Big)\\right.\\qquad\\qquad\\qquad\\mathrm{for}\\,\\beta=1\\,,}\\\\ &{\\left.E_{1}^{T,N}\\sqrt{\\delta_{0,0}^{\\mathrm{SGDw}}}+C\\sigma_{n,T}^{\\mathrm{SGDw}}\\Big(\\frac{2^{2\\beta+1}}{\\tilde{\\mu}_{m}C}e^{\\frac{\\tilde{\\mu}_{m}C}{2(1-\\beta)}}(\\frac{N}{(T+1)^{\\beta}}+\\frac{3^{\\beta}(1+\\tilde{\\mu}_{m}C)^{N-1}(T+2)^{\\beta}}{L^{2}C^{2}}\\,E_{2}^{T,N}\\Big)\\,\\mathrm{~otherwise~}.\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "F.2 Proof of Theorem B.2 ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "$\\delta_{t,j}^{\\mathrm{SGDo}}:=\\mathbb{E}\\left\\|\\psi_{t,j}^{\\mathrm{SGDo}}-\\psi^{*}\\right\\|^{2}$ $\\boldsymbol{\\mathrm E}$ $\\psi_{t,j}^{\\mathrm{SGDo}}$ we condition on $S_{t}^{o}$ , which in particular fixes $S_{t,j}^{o}$ , the last size- $B$ subset of $[n]$ chosen. We then identify $\\theta^{\\mathrm{init}}=\\psi_{t,j-1}^{\\mathrm{SGDo}}$ $\\eta=\\eta_{t}$ andthedataset used as $\\mathcal{D}_{t,j}^{o}:=(X_{i}\\,:\\,i\\in S_{t,j}^{o})$ wic allwsustoidenify $\\psi_{t,j}^{\\mathrm{SGDo}}=\\theta_{m}^{\\mathrm{GD}}$ $\\mathcal{D}_{t,j}^{o}$ $\\theta_{m}^{\\mathrm{GD}}=\\theta_{m,B}^{\\mathrm{SGDw}}$ $\\mathcal{D}_{t,j}^{o}$ that the proof of Lemma F.1 holds with $\\delta_{t,j-1}^{\\mathrm{SGDo}}$ replaced by any random initialization $\\theta^{\\mathrm{init}}$ possibly correlated with $X_{1},\\ldots,X_{n}$ , which allows us to obtain ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sqrt{\\delta_{t,j}^{\\mathrm{SGDo}}}\\ \\leq\\ \\left(1-\\eta_{t}\\Big(\\mu-\\alpha^{m}\\sigma C_{\\chi}-\\frac{L^{2}}{2}\\eta_{t}\\Big)\\right)\\sqrt{\\delta_{t,j-1}^{\\mathrm{SGDo}}}\\ +\\ \\eta_{t}\\Big(\\varepsilon_{n,m,t;\\nu}^{\\mathrm{SGDw}}(\\epsilon)+\\frac{5\\sigma+5\\kappa_{\\nu;m}}{\\sqrt{B}}\\Big)\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "$\\delta_{t,j}^{\\mathrm{SGDo}}$ $\\delta_{t,j}^{\\mathrm{SGDw}}$ d.n\u25a1$\\delta_{T,N}^{\\mathrm{SGDo}}$ sSGDw inTheorem 4.3. \u53e3", "page_idx": 42}, {"type": "text", "text": "G Proofs for tail probability bounds in offline SGD ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "We present the proofs for results in Appendix B.3 that control the tail probability terms v;n,m,T and Ev;n,m,T. ", "page_idx": 42}, {"type": "text", "text": "Proof of Lemma B.3. For $\\delta\\ >\\ 0$ , let $N_{\\delta}$ be the $\\delta$ -covering number of $\\Psi$ , which satisfies $N_{\\delta}~\\leq$ $\\left(r_{\\Psi}(1+2/\\delta)\\right)^{p}$ (Example 5.8, [50]). Note also that by the Jensen's inequality applied to $\\mathbb{E}[\\bullet|X_{1}]$ and Assumption A6, there exist some $\\sigma_{m},\\zeta_{m}>0$ such that, for any $z\\in\\mathbb{R}^{p}$ with $\\|z\\|\\leq\\zeta_{m}$ \uff0c ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathbb{E}\\big[e^{z^{\\top}(\\mathbb{E}[\\phi(K_{\\psi^{*}}^{m}(X_{1}))]\\,X_{1}]-\\mathbb{E}[\\phi(K_{\\psi^{*}}^{m}(X_{1}))])}\\big]\\,\\le\\mathbb{E}\\big[e^{z^{\\top}(\\phi(K_{\\psi^{*}}^{m}(X_{1}))-\\mathbb{E}[\\phi(K_{\\psi^{*}}^{m}(X_{1}))])}\\big]\\,\\le\\,e^{\\sigma_{m}^{2}\\|z\\|^{2}/2}\\;.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Meanwhile under A5, recycling the proof of Lemma 3.1 of [21] shows that if $C_{m}\\delta/\\sqrt{p}<\\sigma_{m}^{2}\\zeta_{m}$ ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\frac{5}{\\psi\\in\\Psi}\\left\\|\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}\\big[\\phi\\big(K_{\\psi}^{m}(X_{i})\\big)\\big|X_{i}\\big]-\\mathbb{E}\\big[\\phi\\big(K_{\\psi}^{m}(X_{i})\\big)\\big]\\right\\|>3C_{m}\\delta\\right)\\ \\leq\\ 2N_{\\delta}p\\exp\\Big(-\\frac{n C_{m}^{2}\\delta^{2}}{2p\\sigma_{m}^{2}}\\Big)}\\\\ &{}&{\\leq2(r_{\\Psi})^{p}\\exp\\Big(p\\log(1+2\\delta^{-1})-\\frac{n C_{m}^{2}\\delta^{2}}{2p\\sigma_{m}^{2}}\\Big)\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Since the probability above is an upper bound to $\\vartheta_{n,m,T}^{\\mathrm{SGDw}}(3C_{m}\\delta)$ , we get that if $C_{m}\\delta/\\sqrt{p}<\\sigma_{m}^{2}\\zeta_{m}$ \uff0c ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\varepsilon_{\\nu;n,m,T}^{\\mathrm{SGDw}}(3C_{m}\\delta)\\right)^{2}\\ =\\ 9C_{m}^{2}\\delta^{2}+\\kappa_{\\nu;m}^{2}\\Big(\\vartheta_{n,m,T}^{\\mathrm{SGDw}}\\Big(\\frac{C_{m}\\delta}{\\sqrt{p}}\\Big)\\Big)^{\\frac{\\nu-2}{\\nu}}}\\\\ &{\\qquad\\qquad\\leq9C_{m}^{2}\\delta^{2}+\\kappa_{\\nu;m}^{2}2^{\\frac{2(\\nu-2)}{\\nu}}(r_{\\Psi})^{\\frac{(\\nu-2)p}{\\nu}}\\exp\\left(\\frac{(\\nu-2)p}{\\nu}\\log(1+2\\delta^{-1})-\\frac{n(\\nu-2)C_{m}^{2}\\delta^{2}}{2\\nu p\\sigma_{m}^{2}}\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Recall that by assumption, $\\begin{array}{r}{\\frac{\\log n}{n}<\\frac{\\sigma_{m}^{2}\\zeta_{m}^{2}}{p+\\nu-2}}\\end{array}$ We now choose ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\delta\\;=\\;{\\frac{\\sigma_{m}}{C_{m}}}\\sqrt{p\\Big(p+{\\frac{2\\nu}{\\nu-2}}\\Big)\\times{\\frac{\\log n}{n}}}\\;=\\;{\\frac{\\sigma_{m}}{C_{m}}}\\sqrt{p\\times{\\frac{(\\nu-2)p+2\\nu}{\\nu-2}}\\times{\\frac{\\log n}{n}}}\\;,\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "which implies ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{inf}\\left(\\frac{S(z)\\ln w}{z\\exp\\left(\\eta-z\\right)},r(\\epsilon)\\right)^{2}\\leq\\bigg(\\epsilon_{\\nu\\rho,m,r}^{\\mathrm{SCD}}(3C_{m}\\delta)\\bigg)^{2}}\\\\ &{\\leq\\frac{9\\sigma_{m}^{2}p\\left((\\nu-2)p+2\\nu\\right)\\log\\eta}{2\\nu}+\\kappa_{\\nu\\rho,m}^{2}\\frac{(\\nu-2)\\tau^{\\frac{3}{2}}}{\\nu}(r\\psi)^{\\frac{(\\alpha-2)p}{\\nu}}(1+2\\delta^{-1})^{\\frac{(\\alpha-2)p}{\\nu}}\\exp\\left(-\\frac{(\\nu-2)p+2\\nu}{2\\nu}\\log n\\right)}\\\\ &{\\leq\\frac{9\\sigma_{m}^{2}p\\left((\\nu-2)p+2\\nu\\right)\\log\\eta}{\\nu-2\\pi}}\\\\ &{\\qquad+\\kappa_{\\nu\\rho,m}^{2}\\frac{\\nu\\sigma_{m}^{2}\\left(r\\psi\\right)^{\\frac{(\\alpha-2)p}{\\nu}}\\left(1+\\frac{2C_{m}(\\nu-2)^{1/2}}{\\nu}\\left(1+\\frac{2C_{m}(\\nu-2)^{1/2}}{\\sigma_{m}\\nu^{1/2}\\left((\\nu-2)p+2\\nu\\right)^{1/2}\\sqrt{\\kappa_{\\theta}n}}\\frac{\\sqrt{n-z})\\nu}{\\nu}\\frac{(\\nu-z)\\mu+2\\nu}{n}}\\\\ &{\\leq\\frac{9\\sigma_{m}^{2}p\\left((\\nu-2)p+2\\nu\\right)\\log n}{(\\nu-2)n}}\\\\ &{\\qquad+\\kappa_{\\nu\\rho,m}^{2}\\frac{\\nu\\sigma_{m}^{2}\\left(r\\psi\\right)^{\\frac{(\\alpha-2)p}{\\nu}}\\left(1+\\frac{2C_{m}(\\nu-2)^{1/2}}{\\sigma_{m}\\nu^{1/2}\\left((\\nu-2)p+2\\nu\\right)^{1/2}}\\frac{\\nu-z)\\nu}{\\nu}n\\frac{(\\nu-2)p+2\\nu}{n}}\\\\ &{\\leq\\left(\\frac{8\\sigma_{m}^{2}p\\left((\\nu-2)p+2\\nu\\right)+\\kappa_{\\nu\\rho,m}^{2}\\frac{(\\nu-2)}{\\nu}\\left(1+\\frac{2C_{m}(\\nu-2)^{1/2}}{\\sigma_{m}\\nu}\\left(1+\\frac{2C_{m}(\\nu-2)^{1/2}}{\\sigma_{m} \n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Taking a squareroot across and using ${\\sqrt{a+b}}\\leq{\\sqrt{a}}+{\\sqrt{b}}$ for $a,b>0$ gives the desired bound. The limiting result follows by substituting this bound into Theorem B.1. \u53e3 ", "page_idx": 43}, {"type": "text", "text": "Proof of Lemma B.4. Let $\\delta\\:>\\:0$ and $N_{\\delta}$ be defined as in the proof of Lemma B.3, with $N_{\\delta}~\\leq$ $\\left(r_{\\Psi}(1+2/\\delta)\\right)^{p}$ Let $(\\psi_{l})_{l=1}^{N_{\\delta}}$ be the centes of the covering $\\delta$ balls. The covering-ball argument of the proof of Lemma 3.1 of [21] shows that ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\vartheta_{n,m,T}^{\\mathrm{SGDw}}(3C_{m}\\delta)\\,\\le\\mathbb{P}\\Big(\\underset{\\psi\\in\\Psi}{\\operatorname*{sup}}\\,\\Big\\|\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}\\big[\\phi\\big(K_{\\psi}^{m}(X_{i})\\big)\\big|X_{i}\\big]-\\mathbb{E}\\big[\\phi\\big(K_{\\psi}^{m}(X_{i})\\big)\\big]\\Big\\|>3C_{m}\\delta\\Big)}\\\\ {\\,\\le\\,\\sum_{l=1}^{N_{\\delta}}\\mathbb{P}\\Big(\\Big\\|\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}\\big[\\phi\\big(K_{\\psi_{l}}^{m}(X_{i})\\big)\\big|X_{i}\\big]-\\mathbb{E}\\big[\\phi\\big(K_{\\psi_{l}}^{m}(X_{i})\\big)\\big]\\Big\\|\\ge C_{m}\\delta\\Big)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "By a Markov's inequality followed by the Burkholder's inequality applied to an average of i.i.d. summands (see e.g. [51] for $\\nu>2$ ), there exists a constant $C_{\\nu}>0$ depending only on $\\nu$ suchthat ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\vartheta_{n,m,T}^{\\mathrm{SGDw}}(3C_{m}\\delta)\\!\\!}&{\\leq\\sum_{l=1}^{N_{\\delta}}\\frac{\\mathbb{E}\\left\\|\\sum_{i=1}^{n}\\mathbb{E}\\left[\\phi\\left(K_{\\psi_{l}}^{m}(X_{i})\\right)\\right|X_{i}\\right]-\\mathbb{E}\\left[\\phi\\left(K_{\\psi_{l}}^{m}(X_{i})\\right)\\right]\\right\\|^{\\nu}}{n^{\\nu}C_{m}^{\\nu}\\delta^{\\nu}}}\\\\ &{\\leq\\sum_{l=1}^{N_{\\delta}}\\frac{\\mathbb{E}\\left\\|\\mathbb{E}\\left[\\phi\\left(K_{\\psi_{l}}^{m}(X_{1})\\right)\\left|X_{i}\\right]-\\mathbb{E}\\left[\\phi\\left(K_{\\psi_{l}}^{m}(X_{1})\\right)\\right]\\right\\|^{\\nu}}{n^{\\nu/2}C_{m}^{\\nu}\\delta^{\\nu}}}\\\\ &{\\leq\\frac{N_{\\delta}\\kappa_{\\nu;m}^{\\nu}}{n^{\\nu/2}C_{m}^{\\nu}\\delta^{\\nu}}\\,\\leq\\,\\frac{{(r_{\\Psi})}^{p}\\left(1+2{\\delta}^{-1}\\right){\\^{p}}\\kappa_{\\nu;m}^{\\nu}}{n^{\\nu/2}C_{m}^{\\nu}\\delta^{\\nu}}\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where we have used assumption A4 in the last line. This implies ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left(\\varepsilon_{\\nu;n,m,T}^{\\mathrm{SGDw}}(3C_{m}\\delta)\\right)^{2}\\!\\!\\!}&{=9C_{m}^{2}\\delta^{2}+\\kappa_{\\nu;m}^{2}\\Big(\\vartheta_{n,m,T}^{\\mathrm{SGDw}}(3C_{m}\\delta)\\Big)^{(\\nu-2)/\\nu}}\\\\ &{\\le9C_{m}^{2}\\delta^{2}+\\kappa_{\\nu;m}^{\\nu}(r_{\\Psi})^{\\frac{(\\nu-2)p}{\\nu}}C_{m}^{-(\\nu-2)}\\times\\frac{(1+2\\delta^{-1})^{\\frac{(\\nu-2)p}{\\nu}}}{n^{\\frac{\\nu-2}{2}}\\delta^{\\nu-2}}\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Choosing $\\delta=n^{-\\frac{(\\nu-2)\\nu}{2(\\nu^{2}+(\\nu-2)p)}}\\leq1$ , we get that ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{inf}_{\\varepsilon>0}\\left(\\varepsilon_{\\nu;n,m,T}^{\\mathrm{SGDw}}\\left(\\epsilon\\right)\\right)^{2}\\;\\leq\\;\\left(\\varepsilon_{\\nu;n,m,T}^{\\mathrm{SGDw}}(3C_{m}\\delta)\\right)^{2}}\\\\ &{\\qquad\\qquad\\leq9C_{m}^{2}n^{-\\frac{(\\nu-2)\\nu}{\\nu^{2}+(\\nu-2)p}}+\\kappa_{\\nu;m}^{\\nu}(r_{\\Psi})^{\\frac{(\\nu-2)p}{\\nu}}C_{m}^{-(\\nu-2)}3^{\\frac{(\\nu-2)p}{\\nu}}n^{-\\frac{(\\nu-2)\\nu}{\\nu^{2}+(\\nu-2)p}}}\\\\ &{\\qquad\\qquad=\\left(9C_{m}^{2}+\\kappa_{\\nu;m}^{\\nu}(r_{\\Psi})^{\\frac{(\\nu-2)p}{\\nu}}C_{m}^{-(\\nu-2)}3^{\\frac{(\\nu-2)p}{\\nu}}\\right)n^{-\\frac{(\\nu-2)\\nu}{\\nu^{2}+(\\nu-2)p}}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Taking a squareroot across and using ${\\sqrt{a+b}}\\leq{\\sqrt{a}}+{\\sqrt{b}}$ for $a,b>0$ gives the desired bound. The limiting result follows by substituting this bound into Theorem B.1. \u53e3 ", "page_idx": 44}, {"type": "text", "text": "Proof of Lemma B.5. By a Markov's inequality and a Jensen's inequality with respect to the empirical average,wehave ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\vartheta_{n,m,T}^{\\mathrm{SGDw}}(\\epsilon)\\,\\leq\\,\\underset{t\\in[T],j\\in[N]}{\\operatorname*{sup}}\\frac{\\sum_{i=1}^{n}\\mathbb{E}\\left\\|\\mathbb{E}\\left[\\phi\\left(K_{\\psi_{t-1,j}^{\\mathrm{SGDw}}}^{m}(X_{i})\\right)\\Big|X_{i},\\psi_{t-1,j}^{\\mathrm{SGDw}}\\right]-\\mathbb{E}\\left[\\phi\\left(K_{\\psi_{t-1,j}^{\\mathrm{SGDw}}}^{m}(X_{1}^{\\prime})\\right)\\Big|\\psi_{t-1,j}^{\\mathrm{SGDw}}\\right]\\right\\|}{n\\epsilon}}\\\\ &{=:\\underset{t\\in[T],j\\in[N]}{\\operatorname*{sup}}\\frac{\\sum_{i=1}^{n}A_{t j i}}{n\\epsilon}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Meanwhile by a triangle inequality and the ergodicity assumption, we have ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{A_{t j i}\\!\\!}&{\\leq\\!\\mathbb{E}\\Big\\|\\mathbb{E}\\Big[\\phi\\Big(K_{\\psi_{t-1,j}^{\\mathrm{SGDw}}}^{m}(X_{i})\\Big)\\Big|X_{i},\\psi_{t-1,j}^{\\mathrm{SGDw}}\\Big]-\\mathbb{E}\\Big[\\phi\\Big(X_{1}^{\\psi_{t-1,j}^{\\mathrm{SGDw}}}\\Big)\\Big|\\psi_{t-1,j}^{\\mathrm{SGDw}}\\Big]\\Big\\|}\\\\ &{\\quad+\\,\\mathbb{E}\\Big\\|\\mathbb{E}\\Big[\\phi\\Big(X_{1}^{\\psi_{t-1,j}^{\\mathrm{SGDw}}}\\Big)\\Big|\\psi_{t-1,j}^{\\mathrm{SGDw}}\\Big]-\\mathbb{E}\\Big[\\phi\\Big(K_{\\psi_{t-1,j}^{\\mathrm{SGDw}}}^{m}(X_{1}^{\\prime})\\Big)\\Big|\\psi_{t-1,j}^{\\mathrm{SGDw}}\\Big]\\Big\\|}\\\\ &{\\leq2\\tilde{C}_{K}\\tilde{\\alpha}^{m}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "This implies $\\vartheta_{n,m,T}^{\\mathrm{SGDw}}(\\epsilon)\\leq2\\tilde{C}_{K}\\tilde{\\alpha}^{m}\\epsilon^{-1}$ and threfore ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\big(\\varepsilon_{\\nu;n,m,T}^{\\mathrm{SGDw}}(\\epsilon)\\big)^{2}\\;\\leq\\;\\epsilon^{2}+2^{\\frac{\\nu-2}{\\nu}}\\kappa_{\\nu;m}^{2}(\\tilde{C}_{K})^{\\frac{\\nu-2}{\\nu}}\\tilde{\\alpha}^{\\frac{(\\nu-2)m}{\\nu}}\\epsilon^{-\\frac{\\nu-2}{\\nu}}\\;.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Choosing $\\epsilon=\\tilde{\\alpha}^{(\\nu-2)m/(3\\nu-2)}$ gives ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{inf}_{\\epsilon>0}\\big(\\varepsilon_{\\nu;n,m,T}^{\\mathrm{SGDw}}(\\epsilon)\\big)^{2}\\;\\leq\\big(1+2^{\\frac{\\nu-2}{\\nu}}\\kappa_{\\nu;m}^{2}(\\tilde{C}_{K})^{\\frac{\\nu-2}{\\nu}}\\big)\\tilde{\\alpha}^{\\frac{2(\\nu-2)m}{3\\nu-2}}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Taking a squareroot across and using ${\\sqrt{a+b}}\\leq{\\sqrt{a}}+{\\sqrt{b}}$ for $a,b>0$ gives the desired bound. The limiting result follows by substituting this bound into Theorem B.1. \u53e3 ", "page_idx": 44}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately refect the paper's contributions and scope? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: Our abstract summarizes the theoretical results of our paper. We cleary mention that while CD can achieve these rates, assumptions are needed to guarantee this. We also mention the model class studied in this paper, namely (unnormalized) exponential family distributions. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and refect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 45}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Justification: We mention limitations multiple times in the main body of our work: after introducing the assumptions in Section 3.1, we mention that spectral gaps may not be verified for heavy tail distributions, or act numerically unfavorably for multimodal distributions, and that constants may be large in practice. In the discussion (Section 6), we reflect back on these limitations, and we mention the limitations of our considered model class (unnormalized exponential families). ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best ", "page_idx": 45}, {"type": "text", "text": "judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 46}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: All of our theoretical results are formally proved in the supplementary material. In the main paper, we provide some proof sketches (discussion of the tail term in Section 4, paragraphs after theorems in section 3) to provide high level intuitions behind the proofs. All of the theoretical results provided by our work are proved in the supplementary material, usually by invoking (and referencing) more atomic lemmas, also proved in the supplementary material. All of our theorems and lemmas are stated with the assumptions they require. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 46}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 46}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 46}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n() If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct thedataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 46}, {"type": "text", "text": "", "page_idx": 47}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 47}, {"type": "text", "text": "Justification: The paper does not include experiments, and thus no code or data. Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 47}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 47}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 47}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 47}, {"type": "text", "text": "Justification: The paper does not include experiments ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 48}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 48}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 48}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 48}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: As a theory paper, we believe that our work does not breach the code of conduct. With this work, we aim to advance the understanding of statistically efficient algorithms, a domain which has the potential to improve the overall computational efficiency of machine learning algorithms. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 48}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 49}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 49}, {"type": "text", "text": "Justification: We believe that our work will not have the negative impacts mentioned (e.g., disinformation, surveillance, fairness, privacy, security considerations). We believe that advancing the domain of statistical effiency has the potential to improve the overall computational efficiency of machine learning algorithms, which, all else left equal, could constitute a positive societal impact. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 49}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 49}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Justification: Since our work does not include experiments nor data, and only analyzes existing models, we do not believe that our work requires any specific guidelines. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 49}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properlyrespected? ", "page_idx": 49}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 50}, {"type": "text", "text": "Justification: The paper does not use existing assets. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 50}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 50}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 50}, {"type": "text", "text": "Justification: Our work does not release any assets. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 50}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 50}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 50}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 50}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 50}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 51}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 51}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 51}]