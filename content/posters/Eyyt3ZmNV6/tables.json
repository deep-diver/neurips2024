[{"figure_path": "Eyyt3ZmNV6/tables/tables_7_1.jpg", "caption": "Table 1: The averaged largest Q% cosine similarity of our method on different watermarks.", "description": "This table presents the average of the largest Q% cosine similarity scores achieved by the proposed ZeroMark method across different watermarking techniques (BadNets, Blended, WaNet, DW) and on two datasets (CIFAR-10, TinyImageNet).  The scores are separated into 'Benign' (non-watermarked) and 'Target' (watermarked) labels to show the distinguishing capability of ZeroMark in identifying watermarked data.", "section": "5.1 The Performance of Verification Samples Generated by ZeroMark"}, {"figure_path": "Eyyt3ZmNV6/tables/tables_7_2.jpg", "caption": "Table 2: The performance on CIFAR-10. In particular, we mark the best results in bold while the value within the underline denotes the second-best results (except the benign samples).", "description": "This table presents the performance evaluation of different methods on the CIFAR-10 dataset using three metrics: MSE (Mean Squared Error), NAS (Neuron Activation Similarity), and MI (Mutual Information).  The methods compared include Vanilla (using original watermarked samples), Minimal (using minimally distorted watermarks), Distortion (using distorted watermarks), and ZeroMark (the proposed method).  The results are shown for four different watermarking techniques (BadNets, Blended, WaNet, and DW). Lower MSE and MI values, and higher NAS values indicate better performance in protecting watermark information.", "section": "5.1 The Performance of Verification Samples Generated by ZeroMark"}, {"figure_path": "Eyyt3ZmNV6/tables/tables_7_3.jpg", "caption": "Table 2: The performance on CIFAR-10. In particular, we mark the best results in bold while the value within the underline denotes the second-best results (except the benign samples).", "description": "This table presents the performance comparison of different methods for dataset ownership verification on the CIFAR-10 dataset using BadNets, Blended, WaNet, and DW watermarks.  The metrics used for comparison are MSE (Mean Squared Error), NAS (Neuron Activation Similarity), and MI (Mutual Information).  Lower MSE and MI values, and higher NAS values are better. The \"Benign\" row shows results for benign samples (no watermark), while \"Vanilla\" shows results using original watermarked samples. \"Minimal\" and \"Distortion\" represent attempts to protect the watermark by using minimal or distorted watermarks, respectively. \"ZeroMark (Ours)\" presents the results of the proposed method.", "section": "5.1 The Performance of Verification Samples Generated by ZeroMark"}, {"figure_path": "Eyyt3ZmNV6/tables/tables_7_4.jpg", "caption": "Table 4: The verification performance of our method on different watermarks.", "description": "This table presents the results of the dataset ownership verification experiment using the proposed ZeroMark method. It compares the performance across four different watermarking techniques (BadNets, Blended, WaNet, DW) under three scenarios: independent watermark (Independent-W), independent model (Independent-M), and malicious usage (Malicious).  For each scenario and watermark, it reports the change in the largest Q% cosine similarity (\u0394P) between benign and watermarked samples and the p-value from a t-test.  The p-value indicates whether the hypothesis that the suspicious model was trained on the watermarked dataset is rejected; very low p-values in the Malicious scenarios (e.g., 10^-45) suggest successful identification of malicious usage.", "section": "5.2 The Performance of Dataset Ownership Verification via ZeroMark"}]