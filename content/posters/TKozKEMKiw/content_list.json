[{"type": "text", "text": "Interpretable Decision Tree Search as a Markov Decision Process ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Finding an optimal decision tree for a supervised learning task is a challenging   \n2 combinatorial problem to solve at scale. It was recently proposed to frame this   \n3 problem as a Markov Decision Problem (MDP) and use deep reinforcement learn  \n4 ing to tackle scaling. Unfortunately, these methods are not competitive with the   \n5 current branch-and-bound state of the art. Instead, we propose to scale the res  \n6 olution of such MDPs using an information-theoretic tests generating function   \n7 that heuristically, and dynamically for every state, limits the set of admissible   \n8 test actions to a few good candidates. As a solver, we show empirically that our   \n9 algorithm is at the very least competitive with branch-and-bound alternatives. As   \n10 a machine learning tool, a key advantage of our approach is to solve for multiple   \n11 complexity-performance trade-offs at virtually no additional cost. With such a set   \n12 of solutions, a user can then select the tree that generalizes best and which has the   \n13 interpretability level that best suits their needs, which no current branch-and-bound   \n14 method allows. ", "page_idx": 0}, {"type": "text", "text": "15 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "16 Decision trees (DTs) remain the dominant machine learning model in applications where interpretabil  \n17 ity is essential [Costa and Pedreira, 2023]. Thanks to recent advances in hardware, a new class of   \n18 decision tree learning algorithms returning optimal trees has emerged [Bertsimas and Dunn, 2017,   \n19 Demirovic et al., 2022, Mazumder et al., 2022]. These algorithms are based on a branch-and-bound   \n20 solver that minimizes a regularized empirical loss, where the number of nodes is used as a regularizer.   \n21 These optimization problems have long been known to be NP-Hard [Hyafil and Rivest, 1976] and   \n22 despite hardware improvements, solvers of such problems do not scale well beyond trees of depth 3   \n23 when attributes take continuous values [Mazumder et al., 2022]. On the other hand, greedy approaches   \n24 such as CART [Breiman et al., 1984] are still considered state-of-the-art decision tree algorithms   \n25 because they scale and offer more advanced mechanisms to control the complexity of the tree. By   \n26 framing decision tree learning as a sequential decision problem, and by carefully controlling the   \n2 size of the search space, we achieve in this paper a best of both worlds, solving the combinatorial   \n28 optimization problem with accuracies close to optimal ones, while improving scaling and offering a   \n29 better control of the complexity-performance trade-off than any existing optimal algorithm.   \n30 To do so, we formulate the problem of decision tree learning as a Markov Decision Problem (MDP,   \n31 [L. Puterman, 1994]) for which the optimal policy builds a decision tree. Actions in such an MDP   \n32 include tests comparing an attribute to a threshold (a.k.a. splits). This action space could include all   \n33 possible splits or a heuristically chosen subset, yielding a continuum between optimal algorithms and   \n34 heuristic approaches. Furthermore, the reward function of the MDP encodes a trade-off between the   \n35 complexity and the performance of the learned tree. In our work, complexity takes the meaning of   \n36 simulatability [Lipton, 2018], i.e. the average number of splits the tree will perform on the train dataset.   \n37 The MDP reward is parameterized by $\\alpha$ , trading-off between train accuracy and regularization. One   \n38 of the main benefits of our formulation is that the biggest share of the computational cost is due to   \n39 the construction of the MDP transition function which is completely independent of $\\alpha$ , allowing us to   \n40 find optimal policies for a large choice of values of $\\alpha$ at virtually no additional cost.   \n41 Branch-and-Bound (BnB) algorithms similarly optimize a complexity performance trade-off   \n42 [Demirovic et al., 2022, Mazumder et al., 2022] but require the user to provide the maximum   \n43 number of test nodes as an input to their algorithm. Providing such a value a priori is difficult   \n44 since a smaller tree (e.g. with 3 test nodes) might be only marginally worse on a given dataset than   \n45 a larger tree (e.g. with 15 test nodes) with respect to the training accuracy but might generalize   \n46 better or be deemed more interpretable a posteriori by the user. As such, it is critical to consider   \n47 the multi-objective nature of the optimization problem and seek algorithms returning a set of trees   \n48 that are located on the Pareto front of the complexity-performance trade-off. To the best of our   \n49 knowledge, this has been so far neglected by BnB approaches. None of the BnB implementations   \n50 return a set of trees for different regularizer weights unlike greedy algorithms like CART or C4.5   \n51 that can return trees with different complexity-performance trade-offs using minimal complexity   \n52 post-pruning [Breiman et al., 1984], making it a more useful machine learning tool in practice. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "53 2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "54 2.1 Optimal Decision Trees. ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "55 Decision tree learning has been formulated as an optimization problem in which the goal is to   \n56 construct a tree that correctly fits the data while using a minimal number of splits. In [Bertsimas   \n57 and Dunn, 2017, Aghaei et al., 2020, Verwer and Zhang, 2019], decision tree learning is formulated   \n58 as a Mixed Integer Program (MIP). Instead of using a generic MIP solver, [Demirovic et al., 2022,   \n59 Mazumder et al., 2022] design specialized solvers based on the Branch-and-Bound (BnB) principle.   \n60 Quant-BnB [Mazumder et al., 2022] is currently the latest work in this line of research for datasets   \n61 with continuous attributes and is considered state-of-the-art. However, direct optimization is not a   \n62 convenient approach since finding the optimal tree is known to be NP-Hard [Hyafli and Rivest, 1976].   \n63 Despite hardware improvements, Quant-BnB does not scale beyond trees depth of 3. To reduce the   \n64 search space, optimal decision tree algorithms on binary datasets, such as MurTree, Blossom and   \n65 Pystreed [Demirovic et al., 2022, Demirovi\u00b4c et al., 2023, van der Linden et al., 2023], employ   \n66 heuristics to binarize a dataset with continuous attributes during a pre-processing step following   \n67 for example the Minimum Description Length Principle [Rissanen, 1978]. The tests generating   \n68 function of our MDP formulation is similar in principle except that it is state-dependent, which, as   \n69 demonstrated experimentally, greatly improves the performance of our solver. ", "page_idx": 1}, {"type": "text", "text": "70 2.2 Greedy approaches. ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "71 Greedy approaches like CART iteratively partition the training dataset by taking the most informative   \n72 splits in the sense of the Gini index or the entropy gain. CART is only one-step optimal but can   \n73 scale to very deep trees. This might lead to overfitting and algorithms such as Minimal Complexity   \n74 Post-Pruning (see Section 3.3 from [Breiman et al., 1984]) iteratively prune the deep tree, returning   \n75 a set of smaller trees with decreasing complexity and potentially improved generalization. The   \n76 trees returned by our algorithms provably dominate\u2014in the multi-objective optimization sense\u2014all   \n77 the above smaller trees in terms of train accuracy vs. average number of tests performed, and we   \n78 experimentally show that they often generalize better than the trees returned by CART. ", "page_idx": 1}, {"type": "text", "text": "79 2.3 Markov Decision Problem formulations. ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "80 In [Topin et al., 2021], a base MDP is extended to an Iterative Bounding MDP (IBMDP) allowing   \n81 the use of any Deep Reinforcement Learning (DRL) algorithm to learn DT policies solving the   \n82 base MDP. While more general and scalable, this method is not state-of-the-art for learning DTs for   \n83 supervised learning tasks. Prior to IBMDPs, [Garlapati et al., 2015] formulated the learning of DTs   \n84 for classification tasks with ordinal attributes as an MDP. To be able to handle continuous features,   \n85 [Nunes et al., 2020] used Monte-Carlo tree search [Kocsis and Szepesv\u00e1ri, 2006] in combination   \n86 with a tests generating function that limits the branching factor of the tree. Our MDP formulation is   \n87 different as it considers a regularized objective while [Nunes et al., 2020] optimize accuracy on a   \n88 validation set. Our tests generating function is also different and dramatically improves scaling as   \n89 shown in the comparison of Sec. 5.1.1, making our algorithm competitive with BnB solvers, while   \n90 [Nunes et al., 2020] only compared their algorithm against greedy approaches. A comparison of our   \n91 method with other MDP approaches is presented in the supplementary material. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "92 2.4 Interpretability of Decision Trees. ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "93 The interpretability of a decision tree is usually associated with its complexity, e.g. its depth or its   \n94 total number of nodes. For trees with 3 to 12 leaves, [Piltaver et al., 2016] observed a strong negative   \n95 correlation between the number of leaves in a tree and a \u201ccomprehensibility\u201d score given by users.   \n96 Most of the literature considers the total number of test nodes as its complexity measure, but other   \n97 definitions of complexity exist. [Lipton, 2018] coined the term simulatability, which is related to the   \n98 average number of tests performed before taking a decision. This quantity naturally arises in our   \n99 MDP formulation. We show in a qualitative study that both criteria are often correlated but on some   \n100 datasets, DPDT returns an unbalanced tree with more test nodes that are only traversed by a few   \n101 samples. ", "page_idx": 2}, {"type": "text", "text": "102 3 Decision Trees for Supervised Learning ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "103 Let us consider a training dataset $\\mathcal{D}=\\{(x_{i},y_{i})\\}_{i\\in\\{1,...,N\\}}$ , made of (data, label) pairs, $(x_{i},y_{i})\\in$   \n104 $(X,Y)$ , where $X\\subseteq\\mathbb{R}^{p}$ . A decision tree $T$ sequentially applies tests to $x_{i}\\in X$ before assigning it a   \n105 value in $Y$ , which we denote $T(x_{i})\\in Y$ . The tree has two types of nodes: test nodes that apply a   \n106 test and leaf nodes that assign a value in $Y$ . A test compares the value of an attribute with a given   \n107 threshold value, $x_{.,2}\\leq3\"$ . In this paper, we focus on binary decision trees, where decision nodes   \n108 split into a left and a right child with axis aligned splits as in [Breiman et al., 1984]. However, all our   \n109 results generalize straitghforwardly to tests involving functions of multiple attributes. Furthermore,   \n110 we look for trees with a maximum depth $D$ , where $D$ is the maximum number of tests a tree can   \n111 apply to classify a single $x_{i}\\in X$ . We let $\\tau_{D}$ be the set of all binary decision trees of depth $\\le D$ .   \n112 Given a loss $\\ell$ defined on $Y\\times Y$ we look for trees in $\\mathcal{T}_{D}$ satisfying ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{T^{*}=\\underset{T\\in{\\cal{T}}_{D}}{\\mathrm{argmin}}\\,\\mathcal{L}_{\\alpha}(T),}}\\\\ {{\\phantom{\\sum_{\\alpha}{=}\\,\\mathcal{L}_{\\alpha}(T)}=\\underset{T\\in{\\cal{T}}_{D}}{\\mathrm{argmin}}\\,\\frac{1}{N}{\\displaystyle\\sum_{i=0}^{N}}\\ell(y_{i},T(x_{i}))+\\alpha C(T),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "113 where $C:\\tau\\to\\mathbb{R}$ is a function that quantifies the complexity of a tree. It could be the number   \n114 of nodes as in [Mazumder et al., 2022]. In our work, we are interested in the expected number of   \n115 tests a tree applies on any arbitrary data $x\\,\\in\\,{\\mathcal{D}}$ . As for $\\ell$ , in a regression problem $Y\\subset\\mathbb{R}$ and   \n116 $\\ell(y_{i},T(x_{i}))$ can be $(y_{i}-{\\dot{T}}(x_{i}))^{2}$ . For supervised classification problems, $Y=\\{1,...,K\\}$ , where $K$   \n117 is the number of class labels, and $\\ell(y_{i},T(x_{i}))=\\mathbb{1}_{\\{y_{i}\\neq T(x_{i})\\}}.$ . In our work, we focus on supervised   \n118 classification but the MDP formulation extends naturally to regression. ", "page_idx": 2}, {"type": "text", "text": "119 4 Decision Tree Learning as an MDP ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "120 Our approach encodes the decision tree learning problem expressed by Eq. (2) as a finite horizon   \n121 Markov Decision Problem (MDP) $\\langle S,A,R_{\\alpha},P,D\\rangle$ . We present this MDP for a supervised clas  \n122 sification problem with continuous features, but again, our method extends to regression and to   \n123 other types of features. The state space of this MDP is made of subsets $X$ of the dataset $\\mathcal{D}$ as well   \n124 as a depth value $d$ : $S=\\{(X,d)\\ \\bar{\\in}\\ P({\\mathcal{D}})\\times\\{0,...,D\\}\\}$ , where $P(\\ensuremath{\\mathcal{D}})$ is the power set of $\\mathcal{D}$ . Let   \n125 $\\mathcal{F}=\\{f:f(.)=\\mathbb{1}_{\\{.\\leq x_{i j}\\}},\\forall i\\in\\{1,...,N\\},\\forall j\\in\\{1,...,p\\}\\}$ be a set of binary functions. We   \n126 consider only tests that compare attributes to values within the dataset because comparing attributes to   \n127 other values cannot further reduce the training objective. The action space $A$ of the MDP is then the set   \n128 of all possible binary tests as well as class assignments: $A=\\mathcal{F}\\cup\\left\\{1,...,K\\right\\}$ . When taking an action   \n129 $a\\in{\\mathcal{F}}$ , the MDP will transit from state $(X,d)$ to either its \u201cleft\" state $s_{l}=\\overline{{(X_{l},d+1)}}$ or its \u201cright\"   \n130 state $s_{r}=(X_{r},d\\!+\\!1)$ . In particular the MDP will transit to $s_{l}=(\\{(x_{i},y_{i})\\in X:a(x_{i})=1\\},d\\!+\\!1)$   \n131 with probability $\\begin{array}{r}{p_{l}=\\frac{|X_{l}|}{|X|}}\\end{array}$ or to $s_{r}=(X\\setminus X_{l},d+1)$ with probability $p_{r}=1-p_{l}$ . Furthermore,   \n132 to enforce a maximum tree depth of $D$ , whenever a state is $s=(.,D)$ then only class assignment   \n133 actions are possible in s. When taking an action in $\\{1,...,K\\}$ the MDP will transit to a terminal state   \n134 denoted $s_{d o n e}$ that is absorbing and has null rewards. The reward of taking an action $a$ in state $s$ is   \n135 given by the parameterized mapping $R_{\\alpha}:S\\times A\\to\\mathbb{R}$ that enforces a trade-off between the expected   \n136 number of tests and the classification accuracy. It is defined by: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R_{\\alpha}(s,a)=R_{\\alpha}((X,d),a),}\\\\ &{\\qquad\\quad=\\left\\{\\!-\\alpha,\\!\\!\\!\\begin{array}{l l}{\\!\\!\\!}&{\\mathrm{if}\\,a\\in\\mathcal{F},}\\\\ {\\!\\!\\!}&{\\displaystyle-\\frac{1}{|X|}\\!\\sum_{y_{i}\\in X}\\!\\mathbb{1}_{y_{i}\\neq a}\\quad\\!\\mathrm{if}\\,a\\in\\{1,...,K\\}.}\\end{array}\\!\\!\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "137 The complexity-performance trade-off is encoded by the value $0\\leq\\alpha\\leq1$ , which is the price to   \n138 pay to obtain more information by testing a feature. A more detailed study of the trade-off is given   \n139 in section 6.4. The maximum depth parameter $D$ is a time horizon, i.e. the number of actions it is   \n140 possible to take in one episode. An algorithm solving such an MDP can always return a deterministic   \n141 policy [L. Puterman, 1994] of the form: $\\pi:S\\rightarrow A$ that maximizes the expected sum of rewards   \n142 during an episode: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pi=\\underset{\\pi}{\\mathrm{argmax}}\\;J_{\\alpha}(\\pi),\\;\\;}\\\\ {J_{\\alpha}(\\pi)=\\mathbb{E}\\left[\\displaystyle\\sum_{t=0}^{D}\\!R_{\\alpha}(s_{t},\\pi(s_{t}))\\right],\\;}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "143 where the expectation is w.r.t. random variables $s_{t+1}\\sim P(s_{t},\\pi(a_{t}))$ with initial state $s_{0}=(\\mathcal{D},0)$ . ", "page_idx": 3}, {"type": "text", "text": "144 From deterministic policy to binary DT. One can transform any deterministic policy $\\pi$ of the above   \n145 MDP into a binary decision tree $T$ with a simple extraction routine $E(\\pi,s)$ , where $s\\in S$ is a state.   \n146 $E$ is defined recursively in the following manner. If $\\pi(s)$ is a class assignment then $E(\\pi,s)$ returns a   \n147 leaf node with class assignment $\\pi(s)$ . Otherwise $E(\\pi,s)$ returns a binary decision tree that has a test   \n148 node $\\pi(s)$ at its root, and $E(\\pi,s_{l})$ and $E(\\pi,s_{r})$ as, respectively, the left and right sub-trees of the   \n149 root node. To obtain $T$ from $\\pi$ , we call $E(\\pi,s_{0})$ on the initial state $s_{0}=(\\mathcal{D},0)$ .   \n150 Equivalence of objectives. When the complexity measure $C$ of $\\mathcal{L}_{\\alpha}$ is the expected number of tests   \n151 performed by a decision tree, the key property of our MDP formulation is that finding the optimal   \n152 policy in the MDP is equivalent to finding $T^{*}$ , as given by the following proposition   \n153 Proposition 1: Let $\\pi$ be a deterministic policy of the MDP and $\\pi^{*}$ one of its optimal deterministic   \n154 policies, then $J_{\\alpha}(\\pi)=-{\\mathcal{L}}_{\\alpha}(E(\\pi,s_{0}))$ and $T^{*}=E(\\pi^{*},s_{0})$ . ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "155 The proof is given in the Appendix H. ", "page_idx": 3}, {"type": "text", "text": "156 5 Algorithm ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "157 We now present the Dynamic Programming Decision Tree (DPDT) algorithm. The algorithm is made   \n158 of two essential steps. The first and most computationally expensive step constructs the MDP of   \n159 Section 4. The second step is to solve it to obtain policies maximizing Eq.(4) for different values of   \n160 $\\alpha$ . Both steps are now detailed. ", "page_idx": 3}, {"type": "text", "text": "161 5.1 Constructing the MDP ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "162 An algorithm constructing the MDP of Section 4 essentially computes the set of all possible decision   \n163 trees of maximum depth $D$ whose decision nodes are in $\\mathcal{F}$ . This specific MDP is a directed acyclic   \n164 graph. Each node of this graph corresponds to a state for which one computes the transition and   \n165 reward functions. To limit memory usage of non-terminal nodes, instead of storing all the samples   \n166 in $(X,d)$ , we only store $d$ and the binary vector of size $N$ $,x_{b i n}=(\\mathbb{1}_{\\{x_{i}\\in X\\}})_{i\\in\\{1,...,N\\}}$ . Even then,   \n167 considering all possible splits in $\\mathcal{F}$ will not scale. We thus introduce a state-dependent action space   \n168 $A_{s}$ , much smaller than $A$ and populated by the tests generating function. ", "page_idx": 3}, {"type": "text", "text": "169 5.1.1 Tests generating functions ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "170 A tests generating function is any function $\\phi$ of the form $\\phi:S\\to P(\\mathcal{F})$ , where $P({\\mathcal{F}})$ is the power   \n171 set of all possible data splits $\\mathcal{F}$ . For a state $s\\in S$ , the state-dependent action space is defined by   \n172 $A_{s}\\,=\\,\\phi({\\bar{s}})\\cup\\{1,...,K\\}$ . Because for a given state $s$ we might have that $\\phi(s)\\,\\neq\\,\\mathcal{F}$ , solving the ", "page_idx": 3}, {"type": "image", "img_path": "TKozKEMKiw/tmp/b15d7a17ebdcf47de56e7addb688d85607642b2831243b6eddaca3a49137f108.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 1: Comparison of DPDT algorithm on the Iris dataset in terms of the number of states in the MDP when using different tests generating functions. \u201cTOP $B^{\\bullet}$ are tests function returning the $B$ most informative splits for each state. \u201cExhaustive\u201d returns all possible states (equivalent to the search space of Quant-BnB). DPDT- $D_{c a r t}$ are the tests functions that make calls to the CART algorithm. ", "page_idx": 4}, {"type": "text", "text": "173 MDP with state-dependent actions $A_{s}$ is not guaranteed to yield the minimizing tree in Eq. (2), as   \n174 optimization is now carried on a subset of $\\tau_{D}$ . In this section, we compare different choices of $\\phi$ on a   \n175 sufficiently small dataset such that $\\phi(s)=\\mathcal{F},\\forall s\\in S$ remains tractable. As a baseline, we use a tests   \n176 generating function proposed in [Nunes et al., 2020], and compare with our proposed $\\phi$ in terms of   \n177 quality of the best tree vs. size of the MDP.   \n178 Exhaustive function. When $\\phi(s)=\\mathcal{F},\\forall s\\in S$ , the   \n179 MDP contains all possible data splits. In this case,   \n180 the MDP \u2018spans\u2019 all trees of depth at most $D$ and the   \n181 solution to Eq. (4) will be the optimal decision tree of   \n182 Eq. (2). In this case, the number of states in the MDP   \n183 would be of the order of $\\sum_{d=0}^{D-1}K(2N p)^{d}$ which scales   \n184 exponentially with the maximum depth of the tree:   \n185 this limits the learning to very shallow trees $(D\\leq3)$   \n186 as discussed in [Mazumder et al., 2022]. The goal   \n187 of a more heuristic choice of $\\phi$ is to have a maximal   \n188 number of splits $B=\\operatorname*{max}_{s\\in S}|\\phi(s)|$ that is orders   \n189 of magnitude smaller than that of the exhaustive case   \n190 $|\\mathcal{F}|=N p$ such that the size of the MDP, which is   \n191 now in the order of $\\sum_{d=0}^{D-1}K(2B)^{d}$ , remains tractable   \n192 for deeper trees.   \n193 Top $B$ most informative splits. [Nunes et al., 2020]   \n194 proposed to generate tests with a function that returns   \n195 for any state $\\bar{\\boldsymbol{s}}=(\\boldsymbol{X},\\boldsymbol{d})$ the $B$ most informative splits   \n196 over $X$ in the sense of entropy gain. In practice, we   \n197 noticed that the returned set of splits lacked diversity   \n198 and often consists of splits on the same attribute with minor changes to the threshold value. While ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Algorithm 1: DPDT- $K$ MDP generation   \nData: Dataset $\\mathcal{D}$ , max depth $D$   \nResult: Decision Tree Search MDP   \n$d\\leftarrow0$   \n$s_{0}\\gets[\\mathcal{D},d]$   \nMDP.AddState $\\left(s_{0}\\right)$ # MDP of Sec.   \nwhile $d<D$ do # For all states at the current depth $d$ for $\\bar{s}=(D_{s},d_{s})\\in M D P$ s.t. $d_{s}=d$ do # Test generating function follwing Sec. 5.1.1 $T_{c a r t}\\gets\\dot{\\mathbf{C}}\\mathbf{A}\\mathbf{R}\\mathbf{T}(\\mathcal{D}_{s}$ , maxdepth=K) $A_{s}\\leftarrow$ ExtractSplits $(T_{c a r t})$ for $a\\in A_{s}$ do # MDP expansion follwoing Sec. 4 MDP.AddRewardAndTransition $(s,a)$ MDP.AddStates(NextStates $(s,a)$ ) end end $d\\gets d+1$ ;   \nend ", "page_idx": 4}, {"type": "text", "text": "199 this still leads to improvements over greedy methods\u2014as shown in the study presented next\u2014it is at   \n200 the expense of a much larger MDP, i.e., search space.   \n201 Top $B$ most discriminative splits. Instead of returning the most informative splits, we propose at   \n202 every state $s=(X,d)$ , to find the most discriminative splits, i.e. the attribute comparisons with which   \n203 one can best predict the class of data points in $X$ . This is similar to the minimum description length   \n204 principle used in [Demirovic et al., 2022] that transforms a dataset with continuous attributes to a   \n205 binary dataset. However, we perform this transformation dynamically at every state while building   \n206 the MDP. In practice, this amounts to calling CART with a maximum depth $D_{c a r t}$ (a hyperparameter   \n207 of DPDT) on every state $s$ , and using the test nodes of the tree returned by CART as $\\phi(s)$ .   \n208 While restricting the action space at a given state $s$ to the actions of the tests generating function $\\phi(s)$   \n209 loses the guarantees of finding $T^{*}$ , we are still guaranteed to find trees better than those of CART:   \n210 Proposition 2: Let $\\pi^{*}$ be an optimal deterministic policy of the MDP, where the action space at every   \n211 state is restricted to the top $B$ most informative or discriminative splits. Let $T_{0}$ be the tree learned by   \n212 CART and $\\{T_{1},\\dots,T_{M}\\}$ be the set of trees returned by postprocessing pruning on $T_{0}$ , then for any   \n213 $\\alpha>0$ , $\\begin{array}{r}{\\mathcal{L}_{\\alpha}(E(\\pi^{*},s_{0}))\\leq\\operatorname*{min}_{0\\leq i\\leq M}\\mathcal{L}_{\\alpha}(T_{i})}\\end{array}$ .   \n214 The proof for Prop. 5.1.1 follows from the fact that policies generating the tree returned by CART   \n215 and all of its sub-trees (which is a superset of the trees returned by the pruning procedure) are   \n216 representable in the MDP and by virtue of the optimality of $\\pi^{*}$ and the equivalence in Prop. 4, are   \n217 worse in terms of regularized loss $\\mathcal{L}_{\\alpha}$ than the tree $E(\\pi^{*},s_{0})$ . The consequences of Prop. 5.1.1   \n218 are clearly observed experimentally in Fig. 3. While this proposition holds for the latter two test   \n219 generating functions, in practice, the tests returned by our proposed function are of much higher   \n220 quality as discussed next.   \n221 Comparing tests generating functions. We conduct a small study comparing the exhaustive $\\phi$   \n222 (labeled \u201cExhaustive\u201d) against the $\\phi$ proposed in [Nunes et al., 2020] (labeled \u201cTop B\u201d) and the one   \n223 used in our algorithm (labeled \u201cDPDT-K\u201d, where ${\\bf K}$ is the maximum depth given to CART), on the Iris   \n224 dataset. Figure 1 shows that while the latter two $\\phi$ generalize the greedy approach (labeled \u201cCART\u201d),   \n225 DPDT scales much more gracefully than when using the $\\phi$ of [Nunes et al., 2020]. With $D_{c a r t}=4$ ,   \n226 DPDT-4 finds the optimal tree in an MDP having several orders of magnitude less states (a few   \n227 hundreds vs a few millions) than the one built using the exhaustive $\\phi$ . This favorable comparison   \n228 against exhaustive methods also holds for larger datasets as shown in Sec. 6.2.   \n229 The MDP construction of DPDT-K using the tests generating function is explained in Alg. 1. Starting   \n230 from $s_{0}$ , the state containing the whole dataset, CART with a maximum depth of K is called which   \n231 generates a tree with up to $2^{\\overline{{K}}}-1$ split nodes. These splits are what constitutes $A_{s_{0}}$ , the set of binary   \n232 tests admissible at $s_{0}$ . For every such action, we compute the reward and transition probabilities to a   \n233 set of new states at depth 1. This process is then iterated for every state at depth 1, calling CART with   \n234 the same maximum depth of K on each of the states at depth 1, generating a new set of binary tests $A_{s}$   \n235 for each of these states $s$ and so on until reaching the maximum depth. Upon termination of Alg. 1,   \n236 we compute the rewards for labelling actions at every state and we call the dynamic programming   \n237 routine below to extract the optimal policy. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "238 5.2 Dynamic Programming ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "239 Having built the MDP, we backpropagate using dynamic programming the best optimal actions from   \n240 the terminal states to the initial states. We use Bellman\u2019s optimality equation to compute the value of   \n241 the best actions recursively: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q^{*}(s,a)=\\mathbb{E}\\left[r_{d+1}+\\underset{a^{\\prime}}{\\operatorname*{max}}Q^{*}(s_{d+1},a^{\\prime})\\vert s_{d}=s,a_{d}=a\\right],}\\\\ &{\\qquad\\qquad=\\underset{s^{\\prime}}{\\sum}P(s,a,s^{\\prime})\\left[R(s,a)+\\underset{a^{\\prime}}{\\operatorname*{max}}Q^{*}(s^{\\prime},a^{\\prime})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "242 Pareto front. As our reward function is a linear combination of the complexity and performance   \n243 measures, we can reach any tree \u201cspanned\u201d by the MDP that lies on the convex hull of the Pareto   \n244 front of the complexity-performance trade-off. In DPDT, we compute the optimal policy for several   \n245 choices of $\\alpha$ using a vectorial representation of the $Q$ -function that now depends on $\\alpha$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\nQ^{*}(s,a,\\alpha)=\\sum_{s^{\\prime}}\\!P(s,a,s^{\\prime})\\left[R_{\\alpha}(s,a)+\\operatorname*{max}_{a^{\\prime}}\\!Q^{*}(s^{\\prime},a^{\\prime},\\alpha)\\right].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "246 We can then find all policies greedy w.r.t. $Q^{*}\\pi^{*}(s,\\alpha)=\\underset{a\\in A}{\\mathrm{argmax}}Q^{*}(s,a,\\alpha)$ . Such policies satisfy   \n247 Eq. (4) for any value of $\\alpha$ . Given a set of values of $\\alpha$ in $[0,1]$ , we can compute in a single backward   \n248 pass $Q^{*}(s,a,\\alpha)$ and $\\pi^{*}(s,\\alpha)$ and return a set of trees, optimal for different values of $\\alpha$ (see Fig.7 for   \n249 an illustrative example). In practice, the computational cost is dominated by the construction of the   \n250 MDP 1 and one can promptly back-propagate the $Q$ -values of over $10^{3}$ values of $\\alpha$ . ", "page_idx": 5}, {"type": "text", "text": "251 6 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "252 In this section we study DPDT from different perspectives. First, in Sec. 6.2, we study DPDT in   \n253 terms of its performance as a solver for the combinatorial optimization problem of Eq. (2). Here, we   \n254 focus on smaller problems (maximum depth $\\leq3$ ) in which the optimal solution can be computed   \n255 by Branch-and-Bound (BnB) algorithms. In this first set of experiments, we only report the training   \n256 accuracy vs. the wall-clock time as done in prior work [Mazumder et al., 2022]. Then we study DPDT   \n257 for model selection (Sec. 6.3). From the perspective of the end user, a decision tree algorithm may   \n258 be used for selecting either a tree that generalizes well to unseen data or a tree that is interpretable.   \n259 We compare classification of unseen data of trees obtained by DPDT to other baselines described   \n260 below. Then, we plot the train accuracy of trees learned by CART and DPDT as a function of their   \n261 complexity to observe how a user can choose the complexity-performance trade-off. We use the 16   \n262 classification datasets with continuous attributes experimented with in [Mazumder et al., 2022].   \n263 When considering other optimal BnB baselines [Demirovic et al., 2022, van der Linden et al., 2023],   \n264 two problems arise for fair comparison with DPDT in terms of model selection. First, to obtain a set   \n265 of tree from such baselines, the optmization algorithms need to be ran as many times as trees wanted   \n266 by the user. For example, one can obtain a set of trees of depth $\\leq5$ by running MurTree $2^{5}$ times   \n267 with different maximum number of test nodes allowed in the learned trees. This could require up   \n268 to $2^{5}$ times the runtime of a single optimization. Second, MurTree and Pystreed [Demirovic et al.,   \n269 2022, van der Linden et al., 2023] require binary attributes. Learned trees are not comparable directly   \n270 with trees trained on continuous attributes because each tree node testing a binary feature actually   \n271 does at least two tests on the original continuous feature (see Appendix F.1 or Appendix D1 from   \n272 [Mazumder et al., 2022]). DPDT is coded in Python and the code is available in the supplementary   \n273 material. All experiments are run on a single core from a Intel i7-8665U CPU. All the links to   \n274 code used for the baselines are given in the Appenix A ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "275 6.1 Baselines ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "276 Quant-BnB. [Mazumder et al., 2022] propose a scalable BnB algorithm that returns optimal trees.   \n277 We emphasize that Quant-BnB is not meant to scale beyond tree depths of 3 (explicitly stated in the   \n278 Quant-BnB paper) and the authors\u2019 implementation of Quant-BnB does not support learning trees of   \n279 depth $>3$ .   \n280 MurTree, Pystreed. To use [Demirovic et al., 2022, van der Linden et al., 2023] with continuous   \n281 features datasets, the minimum length description principle is used to obtain bins in a continuous   \n282 feature domain, then a one hot encoding is applied to binarize the binned dataset. This can result in   \n283 datasets with more than 500 features. As MurTree and Pystreed memory scales with the square of   \n284 number of binary attributes, using those algorithms to find trees of depths greater than 3 often results   \n285 in Out Of Memory (OOM) errors.   \n286 Deep Reinforcement Learning. We use Custard [Topin et al., 2021] as a DRL baseline. Custard   \n287 has two hyperparameters: the DRL algorithm to learn a policy in the IBMDP and a tests generating   \n288 function that gives p tests per feature. In our experiments, Custard-5 and Custard-3 correspond to   \n289 DQN agents [Mnih et al., 2015] that can test each dataset attribute against 5 or 3 values respectively. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "290 CART [Breiman et al., 1984] is a greedy algorithm that can build suboptimal trees for any dataset. ", "page_idx": 6}, {"type": "text", "text": "291 6.2 Optimality gap ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "292 Because we use a tests generating function that heuristically reduces the search space, a first question   \n293 we want to investigate is how good is our solver for the combinatorial problem of decision tree search.   \n294 To do so, we focus on max depth 3 problems for which $T^{*}$ can be computed exactly using Quant-BnB   \n295 [Mazumder et al., 2022]. As Quant-BnB has a different complexity regularization (number of nodes in   \n296 the tree) than DPDT (average number of tests per classified data), we set the complexity regularizing   \n297 term $\\alpha$ to 0 to allow direct comparisons. This does not create an artificial learning and on 14 out of   \n298 16 datasets, trees with $\\alpha=0$ generalize best, and second best on the remaining 2. That is because at   \n299 depth 3 the risk of overfitting is small.   \n300 We run DPDT with calls to CART with maximum depth 4 or 5 as a tests generating function (DPDT-4   \n301 and DPDT-5 respectively). Quant-BnB is first run without a time limit to obtain optimal decision trees   \n302 w.r.t. Eq.(2). Quant-BnB is also run a second time with a time limit equal to DPDT-5\u2019s runtime (we   \n303 also added in the supplementary material results for Quant-BnB- $\\cdot T{+}5$ and Quant-BnB- $\\mathit{T}{+}50$ that add   \n304 extra seconds to Quant-BnB- $T$ ). CART is run with the maximum depth set to 3 and the information   \n305 gain based on entropy. All algorithms are run on the same hardware. Custard is run 5 times per dataset   \n306 because it is a stochastic algorithm. We use stable-baselines3 implementation of DQN [Raffin et al.,   \n307 2021] with default hyperparameters. A Custard run usually takes 10 minutes. We provide learning   \n308 curves in Fig. 4. The key result from Table 1 is that DPDT-5 has better train accuracies than the   \n309 other non-greedy methods when run in similar runtimes across all classification tasks. Furthermore,   \n310 the train accuracy gaps between the optimal decision trees obtained from Quant-BnB, in sometimes   \n311 several hours, and DPDT are usually small (the maximum gap is $1.5\\%$ for the bean dataset). ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "image", "img_path": "TKozKEMKiw/tmp/778478e1ac2b6220147d6043ad1804315147c7faa86d7487f30470f2c940b9b1.jpg", "img_caption": ["Figure 2: Performance gain of DTs over CART trees. Left, accuracy on unseen data gain of trees with depth $\\leq5$ selected with procedure of Sec. 6.3. Right, average number of tests of those trees. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Table 1: Train accuracy of decision tree algorithms. The \u201cQuant-BnB\u201d columns correspond to results for Quant-BnB with no time limit, i.e returing the optimal tree. The \u201cQuant-BnB- $\\mathbf{\\nabla}\\cdot\\mathbf{\\nabla}T^{*}$ column corresponds to results for Quant-BnB run for as long as DPDT-5. The \u201cGreedy\u201d columns correspond to CART with maximum depth of 3. ", "page_idx": 7}, {"type": "table", "img_path": "TKozKEMKiw/tmp/fd765d281747d4cce42a701bc438b402ce88b607df8fa2d7826b1899826839c1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "312 6.3 Selecting the best tree for unseen data ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "313 We now investigate whether DPDT is suited for model selection i.e. whether DPDT can identify an   \n314 accurate decision tree that will generalize well to unseen data for a given classification task. We used   \n315 the following model selection procedure for each classification task. First, we learn a set of decision   \n316 trees of depth $D\\leq5$ with DPDT-3, DPDT-2, and CART on a training set using different values of   \n317 $\\alpha$ for DPDT or minimal complexity post-pruning for CART. Because Quant-BnB simply cannot   \n318 compute trees of depth $>3$ , we only report the accuracy on unseen data of Quant-BnB trees from   \n319 Table 1. Because the BnB baselines MurTree and Pystreed are not designed to return a set of trees,   \n320 we brute force the computation of at most $2^{5}$ trees from each by setting the maximum tree nodes   \n321 parameter to $0,...,2^{5}-\\dot{1}$ . Then, for each baseline we evaluate each learned tree (only one tree for   \n322 Quant-BnB) on a test set and select the tree with highest test accuracy. Fig 2 reports the number of   \n323 datasets for which each baseline has better generalization performances than CART, and the number   \n324 of datasets for which DPDT-K returned trees performing less tests on average than CART trees. A   \n325 table with accuracies of the selected trees on a validation set, the runtime in seconds to obtain the set   \n326 of trees to select from, and the average number of tests performed on data in Appendix. All BnB   \n327 baselines required more than 5 minutes to generate a single tree. As such, the runtime for BnB to   \n328 obtain the whole set of trees is order of magnitudes higher than CART and DPDT. DPDT learns a set   \n329 of trees of at most depth 5 on the complexity-performance convex-hull in seconds which highlights   \n330 its ability to scale to non-shallow trees. For that purpose, DPDT built the MDP of possible solution   \n331 trees of at most depth 5 using CART as a tests generating function, and backpropagated state-action   \n332 values for 1000 different $\\alpha$ .   \n333 After applying the above selection procedure, we see on Table 2 that DPDT generalized better than   \n334 CART on 10 out of 16 datasets while CART outperformed DPDT on only one dataset. When accuracy   \n335 on test data for CART is already close to $100\\%$ , our approach can of course not largely outperform it.   \n336 However, the beneftis of our method have to also be appreciated in terms of gains in average number   \n337 of tests. We can see that when CART does not generalize well, our method can have clear gains in   \n338 generalization (e.g. avila, eeg and fault). Otherwise, when CART is close to $100\\%$ accuracy, our   \n339 method can achieve similar results with less tests. In raisin, rice and room we need two fewer tests   \n340 which is substantial when tests are expensive, e.g. an MRI scan when testing patients.   \n342 In this section, we show how a user can use DPDT to select a tree with complexity preferences. In   \n343 Figure 3, we plot the trade-offs of trees returned by CART and DPDT. The trade-off is between   \n344 accuracy and average number of tests. Because this is the trade-off that DPDT optimizes and because   \n345 the trees of CART are \u201cspanned\u201d by the MDP created by DPDT, all trees returned by DPDT will   \n346 dominate in the multi-objective sense trees returned by CART. This is well demonstrated in practice by   \n347 Figure 3 where the curve of DPDT is always above that of CART. Learned trees and their accuracies   \n348 as functions of number of nodes and tests are presented in Appendices 5 8 9. Finally, decision tree   \n349 search being a combinatorial problem, there are always limits to scalability. In Appendix 6 we scale   \n350 up to a tree depth of 10 by running DPDT-2 up to a depth of 6 then switch to DPTD-1 (i.e. greedy)   \n351 thereafter. The rationale is that a non-greedy approach is more critical closer to the root. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "image", "img_path": "TKozKEMKiw/tmp/70909ee095f3bb38c19efbd728011d79def440c1b3310d792d6be47d288abb0a.jpg", "img_caption": ["Figure 3: Complexity-performance trade-offs of CART and DPDT on two different classification datasets. CART returns a set of trees with the minimal complexity post-pruning algorithm. DPDT returns a set of trees by returning policies for 1000 different $\\alpha$ . "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "352 7 Limitations, Future Work, and Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "353 Limitations. In our opinion, both the strength and the weakness of DPDT come from the choice of   \n354 the tests generating function. If the tests generating function generates too much tests in each MDP   \n355 state, the runtime will grow and there is a risk for out-of-memory errors. This can be alleviated with   \n356 parallelizing (expanding MDP states on different processes) and caching (only expand unseen MDP   \n357 states), similar to [Demirovic et al., 2022]. A rule of thumb for running DPDT on personal CPUs is   \n358 to choose a tests generating function resulting in an MDP with at most $\\mathrm{10^{6}}$ states.   \n359 Future Work. DPDT could scale to bigger datasets by combining Custard [Topin et al., 2021] with   \n360 tests generating functions and tabular deep learning techniques [Kossen et al., 2021]. The latter is a   \n361 promising research avenue. The transformer-based architecture from [Kossen et al., 2021] takes a   \n362 whole train dataset as input and learns representations taking in account relationships between all   \n363 training samples and all labels. Test actions are then the output of such a neural architecture: the tests   \n364 generating function is learned.   \n365 Conclusion. In this work we solve MDPs whose optimal policies are decision trees optimizing a trade  \n366 off between tree accuracy and complexity. We introduced the Dynamic Programming Decision Tree   \n367 algorithm that returns several optimal policies for different reward functions. DPDT has reasonable   \n368 runtimes and is able to scale to trees with depth greater than 3 using information-theoretic tests   \n369 generating functions. To the best of our knowledge, DPDT is the first scalable decision tree search   \n370 algorithm that runs fast enough on continuous attributes to be an alternative to CART for model   \n371 selection of any-depth trees. DPDT is a promising research avenue for new algorithms offering   \n372 human users a greater control than CART over tree selection in terms of generalization performance   \n373 and interpretability. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "374 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "375 Sina Aghaei, Andres Gomez, and Phebe Vayanos. Learning optimal classification trees: Strong   \n376 max-flow formulations, 2020.   \n377 Dimitris Bertsimas and Jack Dunn. Optimal classification trees. Machine Learning, 106:1039\u20131082,   \n378 2017.   \n379 Leo Breiman, Jerome Friedman, R.A. Olshen, and Charles J. Stone. Classification And Regression   \n380 Trees. Taylor and Francis, New York, 1984.   \n381 Vinicius G Costa and Carlos E Pedreira. Recent advances in decision trees: An updated survey.   \n382 Artificial Intelligence Review, 56(5):4765\u20134800, 2023.   \n383 Emir Demirovic, Anna Lukina, Emmanuel Hebrard, Jeffrey Chan, James Bailey, Christopher Leckie,   \n384 Kotagiri Ramamohanarao, and Peter J. Stuckey. Murtree: Optimal decision trees via dynamic   \n385 programming and search. Journal of Machine Learning Research, 23(26):1\u201347, 2022. URL   \n386 http://jmlr.org/papers/v23/20-520.html.   \n387 Emir Demirovi\u00b4c, Emmanuel Hebrard, and Louis Jean. Blossom: an anytime algorithm for com  \n388 puting optimal decision trees. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara   \n389 Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International   \n390 Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research,   \n391 pages 7533\u20137562. PMLR, 23\u201329 Jul 2023. URL https://proceedings.mlr.press/v202/   \n392 demirovic23a.html.   \n393 Abhinav Garlapati, Aditi Raghunathan, Vaishnavh Nagarajan, and Balaraman Ravindran. A rein  \n394 forcement learning approach to online learning of decision trees, 2015.   \n395 Laurent Hyafil and Ronald L. Rivest. Constructing optimal binary decision trees is np-complete.   \n396 Information Processing Letters, 5(1):15\u201317, 1976. ISSN 0020-0190. doi: https://doi.org/10.1016/   \n397 0020-0190(76)90095-8. URL https://www.sciencedirect.com/science/article/pii/   \n398 0020019076900958.   \n399 Levente Kocsis and Csaba Szepesv\u00e1ri. Bandit based monte-carlo planning. In European conference   \n400 on machine learning, pages 282\u2013293. Springer, 2006.   \n401 Hecotr Kohler, Riad Akrour, and Philippe Preux. Limits of actor-critic algorithms for decision tree   \n402 policies learning in ibmdps, 2023.   \n403 Jannik Kossen, Neil Band, Clare Lyle, Aidan N Gomez, Thomas Rainforth, and Yarin Gal. Self  \n404 attention between datapoints: Going beyond individual input-output pairs in deep learning. Ad  \n405 vances in Neural Information Processing Systems, 34:28742\u201328756, 2021.   \n406 Martin L. Puterman, editor. Markov Decision Processes: Discrete Stochastic Dynamic Programming.   \n407 John Wiley & Sons, Hoboken, 1994.   \n408 Zachary C. Lipton. The mythos of model interpretability: In machine learning, the concept of   \n409 interpretability is both important and slippery. Queue, 16(3):31\u201357, jun 2018. ISSN 1542-7730.   \n410 doi: 10.1145/3236386.3241340. URL https://doi.org/10.1145/3236386.3241340.   \n411 Rahul Mazumder, Xiang Meng, and Haoyue Wang. Quant-BnB: A scalable branch-and-bound   \n412 method for optimal decision trees with continuous features. In Kamalika Chaudhuri, Stefanie   \n413 Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the   \n414 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine   \n415 Learning Research, pages 15255\u201315277. PMLR, 17\u201323 Jul 2022. URL https://proceedings.   \n416 mlr.press/v162/mazumder22a.html.   \n417 Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,   \n418 Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control   \n419 through deep reinforcement learning. nature, 518(7540):529\u2013533, 2015.   \n420 Cec\u00edlia Nunes, Mathieu De Craene, H\u00e9l\u00e8ne Langet, Oscar Camara, and Anders Jonsson. Learning   \n421 decision trees through monte carlo tree search: An empirical evaluation. WIREs Data Mining   \n422 and Knowledge Discovery, 10(3):e1348, 2020. doi: https://doi.org/10.1002/widm.1348. URL   \n423 https://wires.onlinelibrary.wiley.com/doi/abs/10.1002/widm.1348.   \n424 Rok Piltaver, Mitja Lu\u0161trek, Matja\u017e Gams, and Sanda Martin\u02c7ci\u00b4c-Ip\u0161i\u00b4c. What makes classification   \n425 trees comprehensible? Expert Systems with Applications, 62:333\u2013346, 2016. ISSN 0957-4174.   \n426 doi: https://doi.org/10.1016/j.eswa.2016.06.009. URL https://www.sciencedirect.com/   \n427 science/article/pii/S0957417416302901.   \n428 Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah   \n429 Dormann. Stable-baselines3: Reliable reinforcement learning implementations. Journal of Machine   \n430 Learning Research, 22(268):1\u20138, 2021. URL http://jmlr.org/papers/v22/20-1364.html.   \n431 J. Rissanen. Modeling by shortest data description. Automatica, 14(5):465\u2013471, 1978. ISSN 0005-   \n432 1098. doi: https://doi.org/10.1016/0005-1098(78)90005-5. URL https://www.sciencedirect.   \n433 com/science/article/pii/0005109878900055.   \n434 John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy   \n435 optimization algorithms. CoRR, abs/1707.06347, 2017. URL http://arxiv.org/abs/1707.   \n436 06347.   \n437 Olivier Sigaud and Olivier Buffet. Markov decision processes in artificial intelligence. John Wiley &   \n438 Sons, 2013.   \n439 Nicholay Topin, Stephanie Milani, Fei Fang, and Manuela Veloso. Iterative bounding MDPs:   \n440 Learning interpretable policies via non-interpretable methods. Proceedings of the AAAI Conference   \n441 on Artificial Intelligence, 35(11):9923\u20139931, May 2021. doi: 10.1609/aaai.v35i11.17192. URL   \n442 https://ojs.aaai.org/index.php/AAAI/article/view/17192.   \n443 Jacobus van der Linden, Mathijs de Weerdt, and Emir Demirovic\u00b4. Necessary and sufficient conditions   \n444 for optimal decision trees using dynamic programming. In A. Oh, T. Neumann, A. Globerson,   \n445 K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems,   \n446 volume 36, pages 9173\u20139212. Curran Associates, Inc., 2023.   \n447 Sicco Verwer and Yingqian Zhang. Learning optimal classification trees using a binary linear program   \n448 formulation. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pages   \n449 1625\u20131632, 2019. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "450 A Code links ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "451 Quant-BnB. The Julia code for Quant-BnB is available at https://github.com/   \n452 mengxianglgal/Quant-BnB.   \n453 CART. We use the scikit-learn Cython implementation of CART available at https://   \n454 scikit-learn.org/stable/modules/tree.html#tree-classification with the criterion   \n455 parameter fixed to \u201centropy\u201d.   \n456 MurTree, Pystreed. Codes are available at https://github.com/MurTree/pymurtree and at   \n457 https://github.com/AlgTUDelft/pystreed. ", "page_idx": 10}, {"type": "text", "text": "458 B On the failure of deep reinforcement learning. ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "459 For the dataset $X=\\{(1,2),(2,1),(3,4),(4,3)\\}$ , $Y=\\{0,1,2,3\\}$ both our MDP and IBMDP are   \n460 equivalent for learning the optimal decision tree of depth 2. We show on Fig. 4 that two different   \n461 DRL algorithms exhibit opposite performance: DQN can learn the optimal decision tree while PPO   \n462 [Schulman et al., 2017] cannot. For that reason, we only trained Custard using DQN as the DRL agent.   \n463 We see on Fig. 4 and Table 1 that Custard-5 converged to trees worst than CART for all classification   \n464 datasets. This shows that while more scalable, DRL approaches are still not competitive on these   \n465 types of problems. [Kohler et al., 2023] studied potential failure modes of DRL in our setting. ", "page_idx": 10}, {"type": "image", "img_path": "TKozKEMKiw/tmp/724e22e962bed5d3d3e22e3501c46f8446fa038ba9f6c3c0f588a6e8d909cf2f.jpg", "img_caption": ["Figure 4: Left, DRL to learn the optimal depth 2 tree. Right, Custard-5 to learn depth 3 decision trees on classification datasets "], "img_footnote": [], "page_idx": 11}, {"type": "text", "text": "466 C Tree plots ", "text_level": 1, "page_idx": 11}, {"type": "image", "img_path": "TKozKEMKiw/tmp/fcecca9533afc73c231da2afc6a2a34c67e0552c94305c2ed001c9f1260d6c9a.jpg", "img_caption": ["Figure 5: Trees for the fault dataset. Top: trees from DPDT. Bottom: trees from CART. A is accuracy, N the number of nodes, T the average number of tests. "], "img_footnote": [], "page_idx": 11}, {"type": "text", "text": "467 D Schematics DTDP ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "468 E Detailed res of model selection ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "469 F Comparisons with baselines operating on binary datasets ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "470 F.1 Why comparisons with baselines that binarize datasets is not fair in our favor? ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "471 Algorithms finding optimal DTs for binary datasets such as MurTree [Demirovic et al., 2022] use   \n472 a binarization method to transform a dataset with continuous attributes to a dataset with binary   \n473 attributes. However, a DT learned on the binary dataset, whenever it tests the value of a binary   \n474 attribute, can lead to up to two tests on the respective continuous attribute. Hence, DTs of a given   \n475 maximum depth on the binary dataset are actually deeper if transformed into DTs on the original   \n476 dataset with continuous attributes. Despite this, we show in Table 1 of this supplementary material   \n477 that DPDT typically finds better solutions (in terms of training accuracy) than MurTree $^+$ binarization   \n478 even though the comparison is not fair in our favor since MurTree is considering deeper trees.   \n479 To illustrate this unbalance with an example, we present a dataset with 3 samples, 2 classes, and 1   \n480 continuous attribute. After binning the continuous attribute and binarizing the dataset into 3 binary   \n481 attributes, we compute the optimal depth 1 tree like [Demirovic et al., 2022] or [Verwer and Zhang,   \n482 2019] would do. To apply this depth 1 tree to the original continuous attribute dataset, the root node   \n483 \" $'a\\in[0.2,0.22]\"$ \" should be decomposed in two decision nodes \" $\"a\\leq0.19\"$ and \" $'a\\leq0.22\"$ before   \n484 making a label assignment. So the corresponding tree that can be applied on the continuous attribute   \n485 is actually of depth 2. ", "page_idx": 11}, {"type": "image", "img_path": "TKozKEMKiw/tmp/cd72e9c9c9a312254ee4bd2eaa69001c9e683ad6f6b276a9461d35f5ee6c6bee.jpg", "img_caption": ["Figure 6: MDP for a training dataset made of three samples (illustrated with an oval and 2 diamonds), two continuous attributes ( $x$ and $y$ ), and two classes. The tests generating function generated three possible tests. There is an initial state $(\\mathcal{D},0)$ (the training dataset at depth 0), and six non-terminal states (three tests times two children states). Rewards are either $\\alpha$ or the misclassification, and transition probabilities are one, or the size of the child state over the size of the parent. "], "img_footnote": [], "page_idx": 12}, {"type": "image", "img_path": "TKozKEMKiw/tmp/e466a5b6102194e6cb937a25c62273e8c2775c2264ccc2fae50fe0461cd99a6d.jpg", "img_caption": ["Figure 7: For $\\alpha=0$ and $\\alpha=1$ , the values of $Q^{*}(s,a,\\alpha)$ are backpropagated from leaf states to the initial state and are given in squared brackets. The optimal policy $\\pi^{*}(.,\\alpha=1)$ , in pink, is a depth-0 tree with accuracy $\\frac{\\mathcal{Z}}{3}$ . The optimal policy $\\pi^{*}(.,\\alpha=0)$ , in green, is a depth-1 tree with accuracy 1. "], "img_footnote": [], "page_idx": 12}, {"type": "table", "img_path": "TKozKEMKiw/tmp/2f8288d356df90be9059e835614f276ed67ad74c45b68e66ceada9d2611175a1.jpg", "table_caption": ["Table 2: Trees of depth $\\leq5$ selected with the procedure described in Sec. 6.3. "], "table_footnote": [], "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "486 ", "page_idx": 13}, {"type": "image", "img_path": "TKozKEMKiw/tmp/3797fd79dffed7593d443431735342d2fc48355248a8750b893d5c5507c1649d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "488 F.2 Experiments ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "489 Comparing baselines such as [Verwer and Zhang, 2019] or [Demirovic et al., 2022] to DPDT or   \n490 Quant-BnB [Mazumder et al., 2022] that operate directly on continuous attributes with the same   \n491 maximum depth is not fair in favor of the latter algorithms as discussed above. Still, for the sake of   \n492 curiosity we performed comparisons on datasets of prior works. These can be split into two groups.   \n493 1) MurTree: Demirovic et al. [2022] propose an algorithm that retrieves optimal trees for large   \n494 datasets with binary features using dynamic programming. They also propose a binarization method   \n495 to retrieve suboptimal shallow trees for large datasets with continuous features. We do not run   \n496 MurTree but use of the results in Table 6 from Mazumder et al. [2022] (see the \u201capprox\u201d column)   \n497 which previously compared Quant-BnB to MurTree.   \n498 2) OCT, MFOCT, BinOCT: Bertsimas and Dunn [2017], Aghaei et al. [2020], Verwer and   \n499 Zhang [2019] propose optimal tree algorithms which formulate the learning problem as a MIP.   \n500 OCT and MFOCT can produce optimal trees for small datasets with continuous features. BinOCT   \n501 can also produce optimal trees for small datasets with continuous features after they have been   \n502 binarized. We make use of the results available at https://github.com/LucasBoTang/Optimal_   \n503 Classification_Trees.   \n504 Reproducibility: as mentioned above, we did not run the additional baselines but instead used   \n505 available results. As such runtimes were provided only when available. OCT, MFOCT, BinOCT were   \n506 run on a single core of an Intel(R) Core(TM) CPU i7-7700HQ @ 2.80GHz. MurTree was run   \n507 on a single core of a Intel Xeon 2.30GHz. According to online benchmarks the performances of   \n508 those machines are similar to our Laptopt CPU Intel\u00ae Core\u2122 i7-8665U CPU. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "509 G Markov Decision Problem formulations of the Decision Tree Learning 510 Problem ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "511 In this section we compare our Markov Decision Problem (MDP) formulation of decision tree   \n512 learning from Section 4 to that of prior work, namely [Garlapati et al., 2015] and [Topin et al.,   \n513 2021]. In a nutshell, prior work viewed the task as a deterministic and Partially Observable MDP   \n514 [Sigaud and Buffet, 2013] and used algorithms such as Q-learning [Garlapati et al., 2015] or deep   \n515 Q-learning [Topin et al., 2021] to solve them in an online fashion one datum from the dataset at a   \n516 time. Our approach is different in that it builds a stochastic and fully observable MDP. Our MDP   \n517 makes it possible to perform two operations that are critical for DPDT: i) being able to call the   \n518 tests generating function which does not operate online but needs full offline access of the dataset   \n519 ii) being able to efficiently compute through dynamic programming optimal policies for different   \n520 complexity-performance trade-offs, which is critical in practice as our improved training accuracy   \n521 compared to greedy methods would otherwise quickly lead to overfitting. High level differences ", "page_idx": 13}, {"type": "table", "img_path": "TKozKEMKiw/tmp/4f8baf68e137fc19c1dbd2284c9787559a815be4bbe6e22fc5ad62c4cd27b07a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "Table 3: Training accuracy of different decision tree learning algorithms. All algorithms learn trees of depth at most 3 on 16 classification datasets. MurTree returns decision trees for datasets binarized using using the minimum description length principle. Results for MurTree are taken from Tables 2 and 6 from [Mazumder et al., 2022]. ", "page_idx": 14}, {"type": "table", "img_path": "TKozKEMKiw/tmp/a2defeff0942688c41bc93ba43603b60d770b2b13e5adf10093403230d951b73.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "Table 4: Train/test accuracies and runtimes of different decision tree learning algorithms. Note that we are not using any regularization in this experiment (in order for all solvers to optimize the same objective function) and as such we might overfit compared to CART that does not optimize the training error as intensively. All algorithms learn trees of depth at most 5 on 8 classification datasets. A time limit of 10 minutes is set for OCT-type algorithms. DPDT is used with two different test generating functions: CART with a maximum depth of 4 and CART with a maximum depth of 5. The values in this table are averaged over 3 seeds giving 3 different train/test datasets. ", "page_idx": 14}, {"type": "text", "text": "522 between MDPs are summarized in Table 5. For the sake of self-completeness we then detail both   \n523 MDPs of [Topin et al., 2021] and [Garlapati et al., 2015] which are to be contrasted with our MDP   \n524 formulation in Section 4. ", "page_idx": 14}, {"type": "table", "img_path": "TKozKEMKiw/tmp/245183192ed26838721414a7977e307ec9d2eb7e7fb54749f70b639bc46a44b1.jpg", "table_caption": ["Table 5: MDP formulations of the decision tree learning problem "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "525 G.1 Iteratvie Bounding MDPs ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "526 An IBMDP [Topin et al., 2021] is an episodic, infinite horizon, discounted MDP. IBMDPs can be   \n527 used for learning decision trees of any base MDP. We discuss here the case where the base MDP is a   \n528 classification task. In this case, during each episode, an agent has to classify a hidden training sample   \n529 $x_{i}$ drawn uniformly from a training dataset with continuous attributes. We assume whiteout loss of   \n530 generality that the training dataset $\\mathcal{X}\\subset[0,1]^{N\\times p}$ has continuous attributes in [0, 1]. On the other   \n531 hand, the set of labels is $\\mathcal{V}=\\{1,...,K\\}$ . An IBMDP is defined as follows.   \n532 State space: the state space is the hypercube $[0,1]^{3\\cdot p}$ . A IBMDP state has two parts. The continuous   \n533 attributes of the hidden training sample $x_{i}\\;=\\;(x_{i1},...,x_{i p})$ to classify, and a lower and upper   \n534 bound $(L_{k},U_{k})$ for each of the $p$ attributes. For each attribute $x_{i k}$ , $(L_{k},U_{k})$ represents the current   \n535 agent knowledge about its hidden value. Initially, $(L_{k},U_{k})=(0,1)$ for all $k$ , which are iteratively   \n536 refined by taking tests actions.   \n537 Action space: an agent in an IBMDP can either take an assignment action $a\\in\\mathcal{V}$ , or a test action   \n538 $\\mathbb{1}_{\\{x_{i k}\\leq v\\cdot(U_{k}-L_{k})+L_{k}\\}}$ with $k\\in\\{1,\\ldots,p\\}$ and $\\begin{array}{r}{v\\in\\{\\frac{1}{d+1},...,\\frac{d}{d+1}\\}}\\end{array}$ , with $d\\in\\mathbb{N}$ a hyperparameter of   \n539 the IBMDP.   \n540 Transition function: if an agent takes a label assignment action, the IBMDP transits to a ter  \n541 minal state, a new training sample $x$ is drawn at random from $\\mathcal{X}$ , and the attributes bounds   \n542 $(L_{1},...,L_{p},U_{1},..,U_{p})$ are reset to 0 or 1. If an agent takes a test action, the attributes bounds   \n543 are refined. Let $x_{i k}$ be the value of the k-th attribute of the hidden training sample $x_{i}$ , and $(L_{k},U_{k})$   \n544 be the current bounds of $x_{i k}$ . If $\\mathbb{1}_{\\{x_{i k}\\leq v\\cdot(U_{k}-L_{k})+L_{k}\\}}$ is true, then $L_{k}$ is updated to $v\\!\\cdot\\!(U_{k}-L_{k})\\!+\\!L_{k}$ ,   \n545 else, it is $U_{k}$ that is updated to $v\\cdot\\left(U_{k}-L_{k}\\right)+L_{k}$ .   \n546 Reward function: the reward for assigning the label $y_{i}\\in\\mathcal{V}$ to the hidden training sample $x_{i}$ is   \n547 $1_{a=y_{i}}\\cdot r_{+}+1_{a\\neq y_{i}}\\cdot r_{-}$ , with $r_{+}>0$ and $r_{-}<0$ . The reward for taking a test action is $\\alpha<0$ . ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "548 G.2 MDP formulation of [Garlapati et al., 2015] ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "549 MDP formulations based on [Garlapati et al., 2015] assume categorical attributes, i.e, the training   \n550 dataset $\\mathcal{D}$ is in $\\mathbb{Z}^{N\\times p}$ . The MDP is episodic with a discount factor and a finite horizon $p+1$ . An   \n551 episode of this MDP consists of costly queries of a training sample\u2019s attributes until a label assignment   \n552 is made.   \n553 State space: a state of the above MDP has partial information about a training sample to classify.   \n554 At every step of the MDP, an agent queries a hidden attribute and updates its knowledge about the   \n555 training sample by concatenating all revealed attributes.   \n556 Acion space: at every step $t$ in the MDP, an agent can either assign a class label in $\\mathcal{V}=\\{1,...,K\\}$ ,   \n557 or, make a query $a_{t}$ of a hidden attribute of a training sample: $A_{t}=(\\{1,...,p\\}\\setminus\\bigcup_{h=0}^{t-1}a_{h})\\cup\\mathcal{Y}$ .   \n558 Transition function: the current state of the MDP contains values of previously queried attributes.   \n559 At $t=0$ , $s=\\{\\}$ . Assuming the hidden training sample to be classified during the current episode   \n560 is $x_{i}\\,=\\,(x_{i1},...,x_{i p})$ , then the deterministic transition function is: $T(s,a\\,\\bar{=}\\,x_{i j})\\,=\\,s\\cup\\bar{x}_{i j}$ or   \n561 $T(s,a\\in\\mathcal{V})=s_{t e r m i n a l}$ . At the start of a new episode, a new training sample is drawn uniformly   \n562 from $\\mathcal{D}$ .   \n563 Reward function: at time $t$ , when the hidden training sample to classify is $x_{i}$ , if the an agent takes   \n564 an assignment action $a\\in\\mathcal D$ , the reward is $1_{a=y_{i}}\\cdot r_{+}+1_{a\\neq y_{i}}\\cdot r_{-}$ , with $r_{+}>0$ and $r_{-}<0$ . So an   \n565 agent gets a positive signal for making a correct label assignment and negative signal otherwise. If   \n566 the agent takes a query action, the reward is a negative value $\\alpha$ in order to discourage taking to much   \n567 queries and control the tree complexity. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "568 H Proof of equivalence of learning objectives ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "569 In this section, we prove the equivalence between learning an optimal policy in the MDP of Section   \n570 4 and finding the minimizing tree of Eq. (2). We first define $C(T)$ , the expected number of tests   \n571 performed by tree $T$ on dataset $\\mathcal{D}$ . Here $T$ is induced by policy $\\pi$ , i.e. $T=E(\\pi,s_{0})$ . $C(T)$ can be   \n572 defined recursively as $C(T)=0$ if $T$ is a leaf node, and $C(T)=1+p_{l}C(T_{l})+p_{r}C(T_{r})$ , where   \n573 $T_{l}=E(\\pi,s_{l})$ and $T_{r}=E(\\pi,s_{r})$ . In words, when the root of $T$ is a test node, the expected number   \n574 of tests is one plus the expected number of tests of the left and right sub-trees of the root node.   \n575 For the purpose of the proof, we overload the definition of $J_{\\alpha}$ and $\\mathcal{L}_{\\alpha}$ , to make explicit the dependency   \n576 on the dataset and the maximum depth. As such, $J_{\\alpha}(\\pi)$ becomes $J_{\\alpha}(\\pi,{\\mathcal{D}},D)$ and ${\\mathcal{L}}_{\\alpha}(T)$ becomes   \n577 $\\mathcal{L}_{\\alpha}(T,\\mathcal{D})$ . Let us first show that the relation $J_{\\alpha}(\\pi,\\mathcal{D},0)=-\\mathcal{L}_{\\alpha}(T,\\mathcal{D})$ is true. If the maximum   \n578 depth is $D=0$ then $\\pi(s_{0})$ is necessarily a class assignment, in which case the expected number of   \n579 tests is zero and the relation is obviously true since the reward is minus the average classification loss.   \n580 Now assume it is true for any dataset and tree of depth at most $D$ with $D\\geq0$ and let us prove that it   \n581 holds for all trees of depth $D+1$ . For a tree $T$ of depth $D+1$ the root is necessarily a test node.   \n582 Let $T_{l}=E(\\pi,s_{l})$ and $T_{r}={E}(\\pi,s_{r})$ be the left and right sub-trees of the root node of $T$ . Since   \n583 both sub-trees are of depth at most $D$ , the relation holds and we have $J_{\\alpha}(\\pi,X_{l},D)=\\mathcal{L}_{\\alpha}(T_{l},X_{l})$   \n584 and $J_{\\alpha}(\\pi,X_{r},D)=\\mathcal{L}_{\\alpha}(T_{r},X_{r})$ , where $X_{l}$ and $X_{r}$ are the datasets of the \u201cright\" and \u201cleft\" states   \n585 to which the MDP transitions\u2014with probabilities $p_{l}$ and $p_{r}$ \u2014upon application of $\\pi(s_{0})$ in $s_{0}$ , as ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "586 described in the MDP formulation. Moreover, from the definition of the policy return we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle J_{\\alpha}(\\pi,\\mathcal{D},D+1)=-\\alpha+p_{l}*J_{\\alpha}(\\pi,\\boldsymbol{X}_{l},D)+p_{r}*J_{\\alpha}(\\pi,\\boldsymbol{X}_{r},D)}\\\\ {\\displaystyle\\quad=-\\alpha-p_{l}*\\mathcal{L}_{\\alpha}(T_{l},\\boldsymbol{X}_{l})-p_{r}*\\mathcal{L}_{\\alpha}(T_{r},D)}\\\\ {\\displaystyle\\quad=-\\alpha-p_{l}*\\Bigg(\\frac{1}{|\\boldsymbol{X}_{l}|}\\sum_{(x_{i},y_{l})\\in\\boldsymbol{X}_{l}}\\ell(y_{i},T_{l}(x_{i}))+\\alpha C(T_{l})\\Bigg)}\\\\ {\\displaystyle\\quad-\\,p_{r}*\\Bigg(\\frac{1}{|\\boldsymbol{X}_{r}|}\\sum_{(x_{i},y_{i})\\in\\boldsymbol{X}_{r}}\\ell(y_{i},T_{r}(x_{i}))+\\alpha C(T_{r})\\Bigg)}\\\\ {\\displaystyle\\quad=-\\frac{1}{N}\\sum_{(x_{i},y_{i})\\in\\boldsymbol{X}}\\ell(y_{i},T(x_{i}))-\\alpha(1+p_{l}C(T_{l})+p_{r}C(T_{r}))}\\\\ {\\displaystyle\\quad=-\\,\\mathcal{L}(T,\\mathcal{D})}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "587 I Deeper trees experiments ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "588 In this section, we push the limits of DPDT to learn trees of at most depth 10. We run two instances   \n589 of DPDT. The first one will generate a MDP using a depth dependant tests generating function.   \n590 DPDT-2... generates a MDP where actions availabe at states corresponding to depth $\\leq5$ are given by   \n591 running CART with a maximum depth of 2, and actions for other states are given by CART with a   \n592 maximum depth of 1 (the maximum information gain splits given the dataset $X$ in the state $((X,d))$ .   \n593 DPDT- $.2+1...$ generates a bigger MDP than DPDT-2... as actions available to states with depths up to   \n594 6 are given by CART run with a maximum depth of 2. On Table 6 we observe that deep trees learnt   \n595 by CART and DPDT perform similarly well on unseen data of different classificiation problems.   \n596 CART runs way faster than DPDT to compute deep trees. However, DPDT learns more interpretable   \n597 trees with respect to the average number of tests performed on data which is a very useful feature   \n598 for real-life applications such as medicine where each additional test before a diagnostic can be very   \nexpensive (for example performing an addition MRI scan). ", "page_idx": 16}, {"type": "table", "img_path": "TKozKEMKiw/tmp/d3ecb09934bae8c3b2178be87815fa18e40e0758f179c7199a3eedfc10402e2e.jpg", "table_caption": ["Table 6: Test accuracy of trees of depth $\\leq10$ selected with the procedure described in Sec. 6.2. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "599 ", "page_idx": 16}, {"type": "text", "text": "600 J Additional comparisons with Quant-BnB ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "601 In Table 7 we compare DPDT with Quant-BnB on train and test sets of different classification   \n602 problems. Quant-BnB has a time limit equal to DPDT- $.5'$ runtime on each problem. We also run   \n603 Quant-BnB with bonuses of 5 and 50 seconds to see if the latter can outperform DPDT with just a   \n604 little more time or if it would require almost twice the time (see Table 7 for DPDT- $.5'$ runtimes). We   \n605 observe that for both train and test accuracies, Quant-BnB- $\\cdot{\\mathrm{t}}{+}50$ (DPDT-5 runtime plus 50 seconds   \n606 bonus) outperforms DPDT most often. ", "page_idx": 16}, {"type": "table", "img_path": "TKozKEMKiw/tmp/fe3e51a5ae36351a957c2d910479bea5a084669214748c48a6b8bc678df5b446.jpg", "table_caption": ["Table 7: Train and Tests accuracies of DPDT and Quant-BnB for Trees of maximum depth 3 "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "607 K Additional figures for different complexity measures ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "608 We show here the complexity-performance trade-offs for all 16 datasets. We show the plot for two   \n609 complexity measures: average number of tests (what DPDT optimize) and total number of nodes   \n610 (what the post-process prunning of CART optimizes). On the first measure, the trees that DPDT finds   \n611 dominate those of CART, which matches the theory. On the second measure, even though we do not   \n612 optimize for the total number of nodes, we are still able to find better trade-offs w.r.t. this metric than   \n613 CART for several datasets. ", "page_idx": 17}, {"type": "text", "text": "614 K.1 Average number of tests vs accuracy ", "text_level": 1, "page_idx": 17}, {"type": "image", "img_path": "TKozKEMKiw/tmp/ac5eb79a7f09d3d8347116ba5674050e9d43a8441ee387e9ef8fcae87b1fe20d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Figure 8: Average number of tests-accuracies trade-offs of CART and DPDT-3 on classification training datasets. Both algorithms learn trees of depths at most 5. CART makes a trade-off with the minimal complexity post-pruning algorithm. DPDT-3 makes a trade-off by returning policies for 1000 different $\\alpha$ . ", "page_idx": 17}, {"type": "text", "text": "615 K.2 Total number of nodes vs accuracy ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "616 L Codes to reproduce experiments ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "617 Anonymized github for DPDT code: https://anonymous.4open.science/r/   \n618 reproduce-E9BD/README.md   \n619 Anonymized github of our clone of Quant-BnB code: https://anonymous.4open.science/r/   \n620 reproduce-quant-bnb-80ED/README.md ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "image", "img_path": "TKozKEMKiw/tmp/f871b97bc5e585cbf61d8b96de97f9832d1c5858c6656ef3337dec0d6f90170d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Figure 9: Nodes-accuracies trade-offs of CART and DPDT-3 on classification training datasets. Both algorithms learn trees of depths at most 5. CART makes a trade-off with the minimal complexity post-pruning algorithm. DPDT-3 makes a trade-off by returning policies for 1000 different $\\alpha$ . Even though we do not optimize for this complexity metric, we are still able to find better trade-offs than CART with post-pruning in several cases. ", "page_idx": 18}, {"type": "text", "text": "621 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "622 1. Claims   \n623 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n624 paper\u2019s contributions and scope?   \n625 Answer: [Yes]   \n626 Justification: All the algorithms and claims mentionned in the intro are studied and presented   \n627 in detail in the main paper. Please see 1 2.   \n628 Guidelines:   \n629 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n630 made in the paper.   \n631 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n632 contributions made in the paper and important assumptions and limitations. A No or   \n633 NA answer to this question will not be perceived well by the reviewers.   \n634 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n635 much the results can be expected to generalize to other settings.   \n636 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n637 are not attained by the paper.   \n638 2. Limitations   \n639 Question: Does the paper discuss the limitations of the work performed by the authors?   \n640 Answer: [Yes]   \n641 Justification: Dedicated sections in the experiments and in the conclusion for limitations.   \n642 Please see 7.   \n643 Guidelines:   \n644 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n645 the paper has limitations, but those are not discussed in the paper.   \n646 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n647 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n648 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n649 model well-specification, asymptotic approximations only holding locally). The authors   \n650 should reflect on how these assumptions might be violated in practice and what the   \n651 implications would be.   \n652 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n653 only tested on a few datasets or with a few runs. In general, empirical results often   \n654 depend on implicit assumptions, which should be articulated.   \n655 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n656 For example, a facial recognition algorithm may perform poorly when image resolution   \n657 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n658 used reliably to provide closed captions for online lectures because it fails to handle   \n659 technical jargon.   \n660 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n661 and how they scale with dataset size.   \n662 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n663 address problems of privacy and fairness.   \n664 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n665 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n666 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n667 judgment and recognize that individual actions in favor of transparency play an impor  \n668 tant role in developing norms that preserve the integrity of the community. Reviewers   \n669 will be specifically instructed to not penalize honesty concerning limitations.   \n670 3. Theory Assumptions and Proofs   \n671 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n672 a complete (and correct) proof?   \n673 Answer: [Yes]   \n674 Justification: Propositions and theorems are proven. Note that is not paper is not a theory   \n675 paper. Please see H.   \n676 Guidelines:   \n677 \u2022 The answer NA means that the paper does not include theoretical results.   \n678 \u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross  \n679 referenced.   \n680 \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n681 \u2022 The proofs can either appear in the main paper or the supplemental material, but if   \n682 they appear in the supplemental material, the authors are encouraged to provide a short   \n683 proof sketch to provide intuition.   \n684 \u2022 Inversely, any informal proof provided in the core of the paper should be complemented   \n685 by formal proofs provided in appendix or supplemental material.   \n686 \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced.   \n687 4. Experimental Result Reproducibility   \n688 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n689 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n690 of the paper (regardless of whether the code and data are provided or not)?   \n691 Answer: [Yes]   \n692 Justification: Code and data links are provided. Algorithms are described explicitly. Please   \n693 see A L.   \n694 Guidelines:   \n695 \u2022 The answer NA means that the paper does not include experiments.   \n696 \u2022 If the paper includes experiments, a No answer to this question will not be perceived   \n697 well by the reviewers: Making the paper reproducible is important, regardless of   \n698 whether the code and data are provided or not.   \n699 \u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken   \n700 to make their results reproducible or verifiable.   \n701 \u2022 Depending on the contribution, reproducibility can be accomplished in various ways.   \n702 For example, if the contribution is a novel architecture, describing the architecture fully   \n703 might suffice, or if the contribution is a specific model and empirical evaluation, it may   \n704 be necessary to either make it possible for others to replicate the model with the same   \n705 dataset, or provide access to the model. In general. releasing code and data is often   \n706 one good way to accomplish this, but reproducibility can also be provided via detailed   \n707 instructions for how to replicate the results, access to a hosted model (e.g., in the case   \n708 of a large language model), releasing of a model checkpoint, or other means that are   \n709 appropriate to the research performed.   \n710 \u2022 While NeurIPS does not require releasing code, the conference does require all submis  \n711 sions to provide some reasonable avenue for reproducibility, which may depend on the   \n712 nature of the contribution. For example   \n713 (a) If the contribution is primarily a new algorithm, the paper should make it clear how   \n714 to reproduce that algorithm.   \n715 (b) If the contribution is primarily a new model architecture, the paper should describe   \n716 the architecture clearly and fully.   \n717 (c) If the contribution is a new model (e.g., a large language model), then there should   \n718 either be a way to access this model for reproducing the results or a way to reproduce   \n719 the model (e.g., with an open-source dataset or instructions for how to construct   \n720 the dataset).   \n721 (d) We recognize that reproducibility may be tricky in some cases, in which case   \n722 authors are welcome to describe the particular way they provide for reproducibility.   \n723 In the case of closed-source models, it may be that access to the model is limited in   \n724 some way (e.g., to registered users), but it should be possible for other researchers   \n725 to have some path to reproducing or verifying the results.   \n726 5. Open access to data and code   \n727 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n728 tions to faithfully reproduce the main experimental results, as described in supplemental   \n729 material?   \n730 Answer: [Yes]   \n731 Justification: Anonymized github repo and data links are provided. Please see A L.   \n732 Guidelines:   \n733 \u2022 The answer NA means that paper does not include experiments requiring code.   \n734 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n735 public/guides/CodeSubmissionPolicy) for more details.   \n736 \u2022 While we encourage the release of code and data, we understand that this might not be   \n737 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n738 including code, unless this is central to the contribution (e.g., for a new open-source   \n739 benchmark).   \n740 \u2022 The instructions should contain the exact command and environment needed to run to   \n741 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n742 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n743 \u2022 The authors should provide instructions on data access and preparation, including how   \n744 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n745 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n746 proposed method and baselines. If only a subset of experiments are reproducible, they   \n747 should state which ones are omitted from the script and why.   \n748 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n749 versions (if applicable).   \n750 \u2022 Providing as much information as possible in supplemental material (appended to the   \n751 paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "752 6. Experimental Setting/Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "753 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n754 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n755 results?   \n756 Answer: [Yes]   \n757 Justification: Evertyhing is detailed clearly in the main paper, in the appendix and in the   \n758 code repos. Please see 6.   \n759 Guidelines:   \n760 \u2022 The answer NA means that the paper does not include experiments.   \n761 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n762 that is necessary to appreciate the results and make sense of them.   \n763 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n764 material.   \n765 7. Experiment Statistical Significance   \n766 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n767 information about the statistical significance of the experiments?   \n768 Answer: [Yes]   \n769 Justification: When processes are stochastic error values are provided in result tables. Please   \n770 see 1.   \n771 Guidelines:   \n772 \u2022 The answer NA means that the paper does not include experiments.   \n773 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n774 dence intervals, or statistical significance tests, at least for the experiments that support   \n775 the main claims of the paper.   \n776 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n777 example, train/test split, initialization, random drawing of some parameter, or overall   \n778 run with given experimental conditions).   \n779 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n780 call to a library function, bootstrap, etc.)   \n781 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n782 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n783 of the mean.   \n784 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n785 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n786 of Normality of errors is not verified.   \n787 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n788 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n789 error rates).   \n790 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n791 they were calculated and reference the corresponding figures or tables in the text.   \n792 8. Experiments Compute Resources   \n793 Question: For each experiment, does the paper provide sufficient information on the com  \n794 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n795 the experiments?   \n796 Answer: [Yes]   \n797 Justification: Exact CPU model as well as ram are provided. Please see 6.   \n798 Guidelines:   \n799 \u2022 The answer NA means that the paper does not include experiments.   \n800 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n801 or cloud provider, including relevant memory and storage.   \n802 \u2022 The paper should provide the amount of compute required for each of the individual   \n803 experimental runs as well as estimate the total compute.   \n804 \u2022 The paper should disclose whether the full research project required more compute   \n805 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n806 didn\u2019t make it into the paper).   \n807 9. Code Of Ethics   \n808 Question: Does the research conducted in the paper conform, in every respect, with the   \n809 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n810 Answer: [Yes]   \n811 Justification: Research is done ethically with na lot of concerns for reproduciblity and   \n812 validity of the results.   \n813 Guidelines:   \n814 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n815 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n816 deviation from the Code of Ethics.   \n817 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n818 eration due to laws or regulations in their jurisdiction).   \n819 10. Broader Impacts   \n820 Question: Does the paper discuss both potential positive societal impacts and negative   \n821 societal impacts of the work performed?   \n822 Answer: [NA]   \n823 Justification: No societal impact, our work simply proposes a classification/regression   \n824 tree algorithms like many before. So the ethical and societal concers are inherited from   \n825 supervised learning ones.   \n826 Guidelines:   \n827 \u2022 The answer NA means that there is no societal impact of the work performed.   \n828 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n829 impact or why the paper does not address societal impact.   \n830 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n831 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n832 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n833 groups), privacy considerations, and security considerations.   \n834 \u2022 The conference expects that many papers will be foundational research and not tied   \n835 to particular applications, let alone deployments. However, if there is a direct path to   \n836 any negative applications, the authors should point it out. For example, it is legitimate   \n837 to point out that an improvement in the quality of generative models could be used to   \n838 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n839 that a generic algorithm for optimizing neural networks could enable people to train   \n840 models that generate Deepfakes faster.   \n841 \u2022 The authors should consider possible harms that could arise when the technology is   \n842 being used as intended and functioning correctly, harms that could arise when the   \n843 technology is being used as intended but gives incorrect results, and harms following   \n844 from (intentional or unintentional) misuse of the technology.   \n845 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n846 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n847 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n848 feedback over time, improving the efficiency and accessibility of ML).   \n849 11. Safeguards   \n850 Question: Does the paper describe safeguards that have been put in place for responsible   \n851 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n852 image generators, or scraped datasets)?   \n853 Answer: [NA]   \n854 Justification: no risk (see above)   \n855 Guidelines:   \n856 \u2022 The answer NA means that the paper poses no such risks.   \n857 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n858 necessary safeguards to allow for controlled use of the model, for example by requiring   \n859 that users adhere to usage guidelines or restrictions to access the model or implementing   \n860 safety filters.   \n861 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n862 should describe how they avoided releasing unsafe images.   \n863 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n864 not require this, but we encourage authors to take this into account and make a best   \n865 faith effort. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "866 12. Licenses for existing assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "867 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n868 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n869 properly respected? ", "page_idx": 23}, {"type": "text", "text": "Justification: Appropriate credits is given when necessary and other code not from the authors are open sourced. Please see A L. ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 23}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 23}, {"type": "text", "text": "892 Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "893 Justification: Code is documented to the best we could. Please see L.   \n894 Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 23}, {"type": "text", "text": "903 14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "904 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n905 include the full text of instructions given to participants and screenshots, if applicable, as   \n906 well as details about compensation (if any)? ", "page_idx": 23}, {"type": "text", "text": "Answer:[NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: no user studies   \nGuidelines: \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. \u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 24}, {"type": "text", "text": "908   \n909   \n910   \n911   \n912   \n913   \n914   \n915   \n916   \n917   \n918   \n919   \n920   \n921   \n922   \n923   \n924   \n925   \n926   \n927   \n928   \n929   \n930   \n931   \n932   \n933   \n934   \n935   \n936 ", "page_idx": 24}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: no user studies Guidelines: \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. \u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. \u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}]