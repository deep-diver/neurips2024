[{"figure_path": "TKozKEMKiw/tables/tables_7_1.jpg", "caption": "Table 1: Train accuracy of decision tree algorithms. The \u201cQuant-BnB\u201d columns correspond to results for Quant-BnB with no time limit, i.e returing the optimal tree. The \u201cQuant-BnB-T", "description": "The table shows a comparison of training accuracy for several decision tree algorithms.  The algorithms are run on 16 different datasets and the results are shown for trees of depth 3.  Quant-BnB represents a branch-and-bound algorithm that finds optimal trees but doesn't scale well beyond depth 3.  Quant-BnB-T represents the same algorithm but with a runtime limit equal to DPDT-5 (another algorithm in the study).  DPDT-5 and DPDT-4 are variations of a Dynamic Programming Decision Tree algorithm that provides a balance between computational cost and accuracy.  Custard-5 and Custard-3 are deep reinforcement learning algorithms.  Greedy refers to the CART algorithm (Classification and Regression Trees). The table also includes runtime results for each algorithm.", "section": "6.2 Optimality gap"}, {"figure_path": "TKozKEMKiw/tables/tables_12_1.jpg", "caption": "Table 1: Train accuracy of decision tree algorithms. The \u201cQuant-BnB\u201d columns correspond to results for Quant-BnB with no time limit, i.e returing the optimal tree. The \u201cQuant-BnB-T", "description": "This table presents the training accuracy achieved by different decision tree algorithms on 16 datasets.  It compares the performance of Quant-BnB (with and without time limits), DPDT-4, DPDT-5, Custard-3, Custard-5, and a greedy approach (CART). The runtime for each algorithm is also provided.  The table highlights that while Quant-BnB achieves optimal results (without time constraints), DPDT-5 often provides close-to-optimal performance within a significantly shorter runtime.", "section": "6.2 Optimality gap"}, {"figure_path": "TKozKEMKiw/tables/tables_14_1.jpg", "caption": "Table 3: Training accuracy of different decision tree learning algorithms. All algorithms learn trees of depth at most 3 on 16 classification datasets. MurTree returns decision trees for datasets binarized using using the minimum description length principle. Results for MurTree are taken from Tables 2 and 6 from [Mazumder et al., 2022].", "description": "This table compares the training accuracy of several decision tree learning algorithms on 16 different datasets.  Each algorithm is used to learn a tree with a maximum depth of 3.  Note that MurTree uses a minimum description length principle to binarize the datasets before training, which differs from the other methods.", "section": "6.2 Optimality gap"}, {"figure_path": "TKozKEMKiw/tables/tables_14_2.jpg", "caption": "Table 1: Train accuracy of decision tree algorithms. The \u201cQuant-BnB\u201d columns correspond to results for Quant-BnB with no time limit, i.e returing the optimal tree. The \u201cQuant-BnB-T", "description": "The table compares the training accuracy of different decision tree algorithms on 16 datasets.  It includes results for Quant-BnB (without time limit, returning the optimal tree), Quant-BnB with a time limit (matching DPDT-5 runtime), DPDT-4, DPDT-5, Custard-5, Custard-3, and CART.  The table also shows the runtime for each algorithm.  The table highlights the performance differences between optimal algorithms and methods that trade-off accuracy for scalability.", "section": "6.2 Optimality gap"}, {"figure_path": "TKozKEMKiw/tables/tables_14_3.jpg", "caption": "Table 1: Train accuracy of decision tree algorithms. The \u201cQuant-BnB\u201d columns correspond to results for Quant-BnB with no time limit, i.e returing the optimal tree. The \u201cQuant-BnB-T", "description": "The table presents the training accuracy of several decision tree algorithms on 16 datasets.  It includes results for Quant-BnB (with no time limit and with a time limit matching DPDT-5 runtime), DPDT-4, DPDT-5, Custard-5, Custard-3, and a greedy approach (CART). For each algorithm, the runtime is also reported, giving an indication of the computational cost.", "section": "6.2 Optimality gap"}, {"figure_path": "TKozKEMKiw/tables/tables_16_1.jpg", "caption": "Table 6: Test accuracy of trees of depth \u2264 10 selected with the procedure described in Sec. 6.2.", "description": "This table presents the test accuracy results for decision trees with a maximum depth of 10.  The trees were selected using the procedure outlined in Section 6.2 of the paper.  The table compares the performance of two different DPDT (Dynamic Programming Decision Tree) algorithms (DPDT-2... and DPDT-2+1...), and CART (Classification and Regression Trees), across various datasets.  The \"Average Nb.Tests\" column indicates the average number of tests performed by each algorithm on the datasets. The runtimes for each algorithm are also provided.", "section": "I Deeper trees experiments"}, {"figure_path": "TKozKEMKiw/tables/tables_17_1.jpg", "caption": "Table 7: Train and Tests accuracies of DPDT and Quant-BnB for Trees of maximum depth 3", "description": "This table compares the training and testing accuracies of the DPDT algorithm with the Quant-BnB algorithm for decision trees with a maximum depth of 3.  It shows the results for three variants of DPDT (DPDT-3, DPDT-4, DPDT-5) and three runtimes for Quant-BnB (Quant-BnB-T, Quant-BnB-t+5, Quant-BnB-t+50) where the runtime is varied by adding bonus time of 5 seconds and 50 seconds respectively. This comparison aims to show how DPDT performs against the state-of-the-art (Quant-BnB) while using similar runtime resources for training and testing.", "section": "J Additional comparisons with Quant-BnB"}]