[{"type": "text", "text": "A Comprehensive Analysis on the Learning Curve in Kernel Ridge Regression ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Tin Sum Cheng, Aurelien Lucchi Department of Mathematics and Computer Science University of Basel, Switzerland tinsum.cheng@unibas.ch, aurelien.lucchi@unibas.ch ", "page_idx": 0}, {"type": "text", "text": "Anastasis Kratsios David Belius Department of Mathematics Faculty of Mathematics and Computer Science McMaster University and The Vector Institute UniDistance Suisse Ontario, Canada Switzerland david.belius@cantab.ch kratsioa@mcmaster.ca ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "This paper conducts a comprehensive study of the learning curves of kernel ridge regression (KRR) under minimal assumptions. Our contributions are three-fold: 1) we analyze the role of key properties of the kernel, such as its spectral eigen-decay, the characteristics of the eigenfunctions, and the smoothness of the kernel; 2) we demonstrate the validity of the Gaussian Equivalent Property (GEP), which states that the generalization performance of KRR remains the same when the whitened features are replaced by standard Gaussian vectors, thereby shedding light on the success of previous analyzes under the Gaussian Design Assumption; 3) we derive novel bounds that improve over existing bounds across a broad range of setting such as (in)dependent feature vectors and various combinations of eigen-decay rates in the over/underparameterized regimes. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Kernel ridge regression (KRR) is a central tool in machine learning due to its ability to provide a flexible and efficient framework for capturing intricate patterns within data. Additionally, it stands as one of the earliest endeavors in statistical machine learning, with ongoing research into its generalization properties [12, 45]. Over the past few years, kernels have experienced a resurgence in importance in the field of deep learning theory [49, 8, 6], partly because many deep neural networks (DNNs) can be interpreted as approaching specific kernel limits as they converge [25, 1, 9]. ", "page_idx": 0}, {"type": "text", "text": "One central topic in machine learning theory is the learning curve of the regressor in the fixed input dimensional setting as the sample size grows to infinity. Formally: let $n$ be the sample size, $\\bar{\\lambda}=\\lambda(n)$ be the ridge regularization parameter depending on $n$ and ${\\mathcal{R}}_{n}$ be the test error/excess risk of the ridge regression. For large $n$ , the test error ${\\mathcal{R}}_{n}$ should decay with $n$ as $\\mathcal{R}_{n}=\\mathcal{O}_{n,\\mathbb{P}}\\left(g(n)\\right)$ for some function $g:\\mathbb{R}\\rightarrow\\mathbb{R}$ such that $g(n)\\xrightarrow{n\\to\\infty}0$ . The decay of $g$ with respect to $n$ provides an upper bound on the learning curve of the ridge regressor and will be the main focus of this paper. To conduct our analysis, we concentrate on several crucial properties of the kernel, including its spectral eigen-decay and the characteristics of the eigenfunctions, which we will elaborate on next. ", "page_idx": 0}, {"type": "text", "text": "Properties of the eigenfunctions In a series of studies [10, 16, 34], the feature vectors are substituted by random Gaussian vectors following the Gaussian Design (GD) Assumption, and the learning curve is derived using the Replica method. In a separate body of research [29, 30, 31], it is demonstrated that similar learning curves occur for H\u00f6lder continuous kernels under an assumption called the Embedding Condition (EC) (see Section A for more details). Consequently, there is a fundamental mismatch between the distribution of the feature vector and the Gaussian random vectors used in [10, 16, 34]: in the former case, each coordinate is highly dependent on the others, whereas in the Gaussian Design case, each coordinate operates independently from the others. Astonishingly, however, both settings share similar learning curves. This phenomenon, initially identified by [21, 3, 42, 20], is termed the Gaussian Equivalence Property. This prompts the question: ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Q1: When and why does the Gaussian Equivalence Property exist? ", "page_idx": 1}, {"type": "text", "text": "Spectral eigen-decay Many recent papers [10, 35, 44, 36] have attempted to characterize the test error solely by the (kernel) eigenspectrum decay. It is for instance common to differentiate between different eigenspectrum decays: [29, 30, 31] assumes the embedding condition (EC) and H\u00f6lder continuity to kernel with polynomial eigen-decay; [33] assumes either polynomial or exponential eigen-decay (with noiseless labels) under the Maximum Degree-of-Freedom (MaxDof) Assumption; [36] assumes some concentration and the so-called hypercontractivity on eigenfunctions. ", "page_idx": 1}, {"type": "text", "text": "However, [14] pointed out that the characterization of generalization performance solely by the spectral eigen-decay might oversimplify the generalization performance of ridge regression. In relation to the second question, we further ask: ", "page_idx": 1}, {"type": "text", "text": "Q2: Under what conditions is the generalization error fully determined by the eigen-decay? ", "page_idx": 1}, {"type": "text", "text": "Additional assumptions and settings In addition to the two properties above, other hyperparameters or settings, including capacity of the kernels/feature vectors, the ridge regularization decay, the source condition of the target function, the noise level in the output label, and the amount of over-parameterization, play an important role in the analysis of the learning curve of ridge regression. Within the present body of research, various papers establish bounds on the test error of KRR across diverse assumptions and settings (we refer the reader to Section A for further elaboration). It is therefore of significant interest to ask: ", "page_idx": 1}, {"type": "text", "text": "Q3: Is there a unifying theory explaining the generalization under minimal assumptions? ", "page_idx": 1}, {"type": "text", "text": "Contributions We address questions $Q I$ -3 through the following contributions: ", "page_idx": 1}, {"type": "text", "text": "(i) Unified theory: We provide a unifying theory of the test error of KRR across a wide variety of settings (see Subsection 2.2 and Table 1 in Section 3).   \n(ii) Validation and GEP: We show that the generalization performance with independent (Gaussian) features and dependent (kernel) features coincides asymptotically and it solely depends on the eigen-decay under strong ridge regularization, hence validating the Gaussian Equivalent Property (GEP) (see Subsection 3.2) .   \n(iii) New and sharpened bounds: We provide novel bounds of the KRR test error that improve over prior work across various settings (see Subsections 3.2).   \n(iv) Smoothness and generalization: We relate the spectral eigen-decay to kernel smoothness (see Appendix B.2) and hence to the kernel\u2019s generalization performance. ", "page_idx": 1}, {"type": "text", "text": "2 Setting ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this section, we introduce the basic notation for ridge regression, which includes high-dimensional linear regression and kernel ridge regression as special cases. ", "page_idx": 1}, {"type": "text", "text": "2.1 Notations ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Suppose $p\\in\\mathbb{N}\\cup\\{\\infty\\}$ . Let $\\mathbf{x}=(x_{k})_{k=1}^{p}\\in\\mathbb{R}^{p}$ be a random (feature) vector sampled from some distribution $\\mu$ on $\\mathbb{R}^{p}$ . Let $n\\in\\mathbb N$ be an integer and denote by $\\mathbf{x}_{1},...,\\mathbf{x}_{n}$ n i.i.d. draw of $\\mathbf{x}$ . Denote the input matrix $\\mathbf{X}\\in\\mathbb{R}^{n\\times p}$ to be a matrix with rows $\\mathbf{x}_{i}^{\\top}$ . By fixing an orthonormal basis, we assume that the covariance matrix is diagonal: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\Sigma\\ {\\overset{\\mathrm{def.}}{=}}\\ \\mathbb{E}_{\\mu}\\left[\\mathbf{xx}^{\\top}\\right]=\\operatorname{diag}(\\lambda_{1},\\lambda_{2},...,\\lambda_{p})\\in\\mathbb{R}^{p\\times p},\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where the eigenvalues $\\lambda_{1}\\geq\\lambda_{2}\\geq...\\geq\\lambda_{p}>0$ is a decreasing sequence of positive numbers. ", "page_idx": 2}, {"type": "text", "text": "We also assume that $\\mathbf{y}\\in\\mathbb{R}^{n}$ is an output vector such that ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{y}=\\mathbf{X}\\pmb{\\theta}^{*}+\\pmb{\\epsilon}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\pmb{\\theta}^{*}\\in\\mathbb{R}^{p}$ is a deterministic vector, $\\epsilon\\in\\mathbb{R}^{n}$ is a random vector whose entries are i.i.d. drawn from a centered random variable $\\epsilon$ with variance $\\mathbb{E}\\left[\\epsilon^{2}\\right]=\\sigma^{2}<\\infty$ and independent to $\\mathbf{X}$ . ", "page_idx": 2}, {"type": "text", "text": "Then the linear regressor ", "page_idx": 2}, {"type": "equation", "text": "$$\n{\\hat{\\pmb{\\theta}}}(\\mathbf{y})\\ {\\stackrel{\\mathrm{def.}}{=}}\\ \\mathbf{X}^{\\top}(\\mathbf{X}\\mathbf{X}^{\\top}+n\\lambda\\mathbf{I}_{n})^{-1}\\mathbf{y}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "is the minimizer of the empirical mean square loss (MSE) problem: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\pmb{\\theta}\\in\\mathbb{R}^{p}}\\frac{1}{n}\\|\\mathbf{X}\\pmb{\\theta}-\\mathbf{y}\\|_{2}^{2}+\\lambda\\|\\pmb{\\theta}\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\lambda\\:\\geq\\:0$ is the ridge2. This paper focuses on bounding the test error, with which we can analyse the learning curve of the regressor. To do so, we use the following well-known bias-variance decomposition. ", "page_idx": 2}, {"type": "text", "text": "Definition 2.1 (Bias-variance decomposition). Consider input-output pairs $(\\mathbf{X},\\mathbf{y})$ of sample size $n$ and a ridge $\\lambda\\geq0$ . Define the test error $\\mathcal{R}$ to be the population mean squared error between the regressor and the true label averaged over noise. ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{R}\\stackrel{\\mathrm{def.}}{=}\\mathbb{E}_{{\\mathbf{x}},\\epsilon}\\left[\\left({\\mathbf{x}}^{\\top}\\hat{\\pmb{\\theta}}({\\mathbf{y}})-{\\mathbf{x}}^{\\top}{\\pmb{\\theta}}^{*}\\right)^{2}\\right]\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Note that $\\mathcal{R}$ is a random variable depending on the samples $(\\mathbf{X},\\mathbf{y})$ and the ridge $\\lambda\\geq0$ . Hence, we can also view $\\mathcal{R}=\\mathcal{R}_{n}$ as a random variable indexed in $n$ , where the samples $(\\mathbf{X},\\mathbf{y})$ are $n$ i.i.d. drawn input-output pairs and $\\lambda$ is chosen to depend on $n$ . ", "page_idx": 2}, {"type": "text", "text": "We decompose the test error into a bias $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ and variance $\\nu$ , which is typical for most KRR literature [32, 23, 6, 46, 29, 30, 31, 13, 14]: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{R}=\\mathcal{B}+\\mathcal{V}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{B}\\stackrel{\\mathrm{def.}}{=}\\mathbb{E}_{\\mathbf{x}}\\left[\\left(\\mathbf{x}^{\\top}\\hat{\\pmb{\\theta}}(\\mathbf{X}\\pmb{\\theta}^{*})-\\mathbf{x}^{\\top}\\pmb{\\theta}^{*}\\right)^{2}\\right],~\\mathcal{V}\\stackrel{\\mathrm{def.}}{=}\\mathbb{E}_{\\mathbf{x},\\epsilon}\\left[\\left(\\mathbf{x}^{\\top}\\hat{\\pmb{\\theta}}(\\epsilon)\\right)^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Remark 2.2 (Noiseless labels). If there is no noise in the label, that is, $\\epsilon=0$ , the test error $\\mathcal{R}$ is simply the bias $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ . Hence, the analysis of the bias term $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ in this paper is directly applicable to the noiseless labels setting. ", "page_idx": 2}, {"type": "text", "text": "We now summarize the combinations of assumptions and settings made in this paper. ", "page_idx": 2}, {"type": "text", "text": "2.2 Assumptions and Settings ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Polynomial/exponential eigen-decay We consider two types of spectral decay rates, namely, polynomial and exponential decay rates, because: 1) polynomial eigen-decay is, roughly speaking , equivalent to the case where the RKHS is comprised of at most finitely many continuous derivatives; 2) the exponential eigen-decay is, possibly up to a canonical change in the relevant function space, equivalent to the case where the RKHS consists of smooth (infinitely differentiable) functions. For the formal definition of the eigen-decay, see Assumptions (PE) and (EE). For further details and explanation on the relationship between eigen-decay and smoothness, we refer the reader to Section B. ", "page_idx": 2}, {"type": "text", "text": "Source condition Many previous works [6, 16, 7, 30] include the so-called source condition as assumptions on the target. If the task is proper, that is, $\\pmb{\\theta}^{*}\\in\\mathcal{H}=\\mathcal{H}^{1}$ , we have $s\\geq1$ . More generally, a larger source coefficient $s$ implies a smoother target $\\theta^{*}$ in the RKHS $\\mathcal{H}$ . ", "page_idx": 2}, {"type": "text", "text": "Definition 2.3 (Interpolation space). Let $s\\geq0$ be a real number. Define the interpolation space ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{H}^{s}\\ensuremath{\\stackrel{\\mathrm{def.}}{=}}\\{\\pmb{\\theta}\\in\\mathbb{R}^{p}:\\|\\pmb{\\theta}\\|_{\\Sigma^{1-s}}<\\infty\\}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Assumption (SC) (Source Condition). The source coefficient of a target coefficient $\\pmb{\\theta}^{*}$ is defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\ns=\\operatorname*{inf}\\{t>0:\\pmb\\theta^{*}\\in\\mathcal{H}^{t}\\}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "See Subsection A.5 for more elaborations for the source coefficient $s$ in polynomial or in exponential decay. ", "page_idx": 3}, {"type": "text", "text": "Strong/weak ridge We set the ridge $\\lambda=\\lambda(n)\\ge0$ to depend on the sample size $n$ . The ridge is considered strong (relative to the eigen-decay) if $\\lambda\\geqslant\\lambda_{\\operatorname*{min}\\{n,p\\}}$ , that is, if $\\lambda/\\lambda_{\\operatorname*{min}\\{n,p\\}}\\xrightarrow{n\\rightarrow\\infty}0$ ; otherwise, it is considered weak. Intuitively, the ridge is weak when it is negligible compared to the entries in the kernel matrix, effectively making it ridgeless. ", "page_idx": 3}, {"type": "text", "text": "To summarize the assumptions discussed previously, let $(\\lambda_{k})_{k=1}^{p}$ be the eigenvalues of the kernel $K$ , and $\\theta^{*}\\ {\\stackrel{\\mathrm{def.}}{=}}\\ (\\theta_{k}^{*})_{k=1}^{p}$ the coefficients of the target function being learned in the eigen-basis defined by . Then we assume either of the following assumptions: ", "page_idx": 3}, {"type": "text", "text": "Assumption $(\\bf P E)$ (Polynomial Eigen-decay). Assume that $\\lambda_{k}\\,=\\,\\Theta_{k}\\,\\bigl(k^{-1-a}\\bigr)$ , $|\\theta_{k}^{*}|=\\Theta_{k}\\left(k^{-r}\\right)$ , $\\lambda=\\Theta_{n}\\left(n^{-b}\\right)$ for some constants $a,b,r\\,>\\,0,$ , where $a+2\\neq2r$ unless specified. 3 Hence, $i f$ Assumption (SC) holds, the source coefficient is $\\textstyle s={\\frac{2r+a}{1+a}}\\,$ 21r++aa . We call the ridge \u03bb strong if b \u2208(0, 1 + a], and weak if $b\\in(1+a,\\infty]$ , under the convention that $b=\\infty$ implies $\\lambda=0$ . ", "page_idx": 3}, {"type": "text", "text": "Assumption (EE) (Exponential Eigen-decay). Assume that $\\lambda_{k}\\,=\\,\\Theta_{k}\\,\\bigl(e^{-a k}\\bigr)$ , $\\theta_{k}^{*}\\,=\\,\\Theta_{k}\\left(e^{-r k}\\right)$ , $\\lambda\\,=\\,\\Theta_{n}\\left(e^{-b n}\\right)$ for some constants $a,b,r\\;>\\;0$ , where $a~\\neq~2r$ unless specified. 4 Hence, $i f$ Assumption (SC) holds, the source coefficient is $\\begin{array}{r}{s=\\frac{2r+a}{a}=\\frac{2r}{a}+1}\\end{array}$ . We call the ridge $\\lambda$ strong $i f$ $b\\in(0,a]$ , and weak if $b\\in(a,\\infty]$ , under the convention that $b=\\infty$ implies $\\lambda=0$ . ", "page_idx": 3}, {"type": "text", "text": "Generic/independent features Our analysis centers on the assumptions regarding feature vectors, with a focus on the dependencies between coordinates, particularly exploring two cases: ", "page_idx": 3}, {"type": "text", "text": "1. Generic features (GF): include the cases where the feature vectors are dependent on each other, for example, the feature vectors from the following kernels: \u2022 dot-product kernels on hyperspheres; \u2022 kernels with bounded eigenfunctions; \u2022 radial base function (RBF) and shift-invariant kernels; \u2022 kernels on hypercubes, satisfy Assumption (GF). Most previous literature [30, 33, 36, 19] have assumptions that only a proper subset of the above kernels satisfies. Therefore, we believe that we are operating under the minimal assumptions that exist in the field.   \n2. Independent features (IF): replace the feature vector with sub-Gaussian random vector with independent coordinates. A special case is the Gaussian Design assumption (GD) used in literature [44, 35, 16]. ", "page_idx": 3}, {"type": "text", "text": "For further explanations regarding the assumptions, we refer the reader to Section A. ", "page_idx": 3}, {"type": "text", "text": "3 Main result ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We first present an overview of the test error bounds across various properties, assumptions, and regimes. Our main results, summarized in Table 1, describe the learning curve in the overparameterized regime, in terms of the bias $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ and variance $\\nu$ decomposition (see Equation (5)). Then, we will discuss the implications of our results in depth. ", "page_idx": 3}, {"type": "text", "text": "3.1 Overview ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Table 1 summarizes many of our results in the over-parameterized regime under various combinations of the assumptions described in subsection 2.2. The bounds are expressed in terms of the sample size $n$ as $O_{n}\\left(\\cdot\\right)$ or $\\tilde{O}_{n}\\left(\\cdot\\right)$ (ignoring logarithmic terms). Whenever we can also prove a matching lower bound, we replace $O_{n}\\left(\\cdot\\right)$ with $\\Theta_{n}\\left(\\cdot\\right)$ . We write $(\\cdot)_{+}\\ {\\stackrel{\\mathrm{def.}}{=}}\\ \\operatorname*{max}\\{\\cdot,0\\}$ . ", "page_idx": 4}, {"type": "table", "img_path": "IMlDpZmLnL/tmp/c52ff6329dcdfdb1b13171868df80c146a31e204713e222617a1c2b9400bbba3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "Table 1: Learning curve in the over-parameterized regime $\\textstyle{\\overleftrightarrow{p}}>n,$ : $n$ is the sample size, $\\overline{{a,r>0}}$ define the eigen-decay rates of the kernel and target function, $b>0$ controls the decay rate of the ridge regularization parameter (Assumptions (PE) and (EE)), $\\sigma^{2}\\ {\\stackrel{\\mathrm{def.}}{=}}\\ \\mathbb{E}\\ \\bigl[\\epsilon^{2}\\bigr]$ is the noise level, and $s\\,>\\,0$ is a technical parameter often determined by $a$ and $r$ (e.g. under Assumption (SC)). Here $\\tilde{s}\\ {\\stackrel{\\mathrm{def.}}{=}}\\ \\operatorname*{min}\\{s,2\\}$ . ", "page_idx": 4}, {"type": "text", "text": "Results in blue indicate either previously unstudied regimes or improvements in available rates in a studied regime. See Table 6 for more comparisons and Subsection 2.2 for details on various settings. ", "page_idx": 4}, {"type": "text", "text": "Before delving into the detailed discussion of our comprehensive results in Subsection 3.2, let us highlight some important observations from Table 1. ", "page_idx": 4}, {"type": "text", "text": "Asymptotic bounds The upper bound illustrates the asymptotic relationship between the test error and sample size $n$ as well as the following constants: $a$ related to the eigen-decay, $b$ related to the ridge, $r$ related to the target function and $\\bar{\\sigma^{2}}$ related to the label noise. ", "page_idx": 4}, {"type": "text", "text": "Independent feature (IF) versus generic features (GF) The bounds in both cases coincide under strong ridge (see the left columns of Table 1); meanwhile, under weak ridge (see the right columns of Table 1), the bounds with generic features are looser than those with independent features. In Subsection 3.2, we will explain the necessity of this difference and hence showcase the previous limitations in the literature, which has widely adopted the Gaussian Design Assumption (GD) under the weak ridge/interpolation regime. ", "page_idx": 4}, {"type": "text", "text": "Novel bound of bias under weak ridge A notably sophisticated bound (on the upper right corner of Table 1) ", "page_idx": 4}, {"type": "equation", "text": "$$\nB=\\left\\{\\mathcal{O}\\left(n^{-(1+a)\\tilde{s}}\\right),\\quad\\quad\\quad\\quad\\quad\\quad\\quad s>1\\quad}\\\\ {\\tilde{\\mathcal{O}}\\left(n^{-(\\operatorname*{min}\\{2(r-a),2-a\\})+}\\right),\\quad s\\leq1}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "is novel to the best of our knowledge. Our improvement compared to previous literature [7] under various eigen-decay (in terms of $a$ ) and target coefficients\u2019 decay (in terms of $r$ ) is shown in Figure 1. By comparison, we can see that the decay of our novel bound in Equation 6 is faster than previous results. Also, we prove that the upper bound in the middle green region ,where $s\\in(1,2)$ , is sharp. ", "page_idx": 4}, {"type": "text", "text": "Experimental validations of the results in Table 1 are given in Section 6. ", "page_idx": 4}, {"type": "text", "text": "In the under-parameterized regime, the bias $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ and variance $\\mathcal{V}$ terms can be bounded similarly as in over-parameterized regime. We postpone the details of these results to Section F. ", "page_idx": 4}, {"type": "text", "text": "3.2 Detailed discussion ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this subsection, we elaborate more on the details of our results shown in Table 1. ", "page_idx": 4}, {"type": "text", "text": "Independent and generic features Table 1 indicates that the test error exhibits the same upper bound with either independent or generic features under strong ridge conditions in the overparameterized regime. This similarity arises from the bounds in both cases being derived from the same Master inequalities that we will introduce later (see Section 4). However, under weak ridge conditions, the empirical kernel spectrum displays qualitative differences, as reported in [7, 14]. From Figure 2, we can see that $\\mathcal{V}=\\mathcal{O}\\left(1\\right)$ with Laplacian kernel in the left plot and $\\nu$ diverges with the neural tangent kernel (with 1 hidden layer) in the right plot. Hence under weak ridge and polynomial eigen-decay, the case distinction of the bound ", "page_idx": 4}, {"type": "image", "img_path": "IMlDpZmLnL/tmp/828464d432a2ce988abc2133c5a0d1fb405f9e4c4454316e506f73266679d00a.jpg", "img_caption": ["Figure 1: Phase diagram of the bound (Equation( 6)) of the bias term $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ under weak ridge and polynomial eigen-decay. $\\lambda_{k}\\;=\\;\\Theta_{k}\\left(k^{-1-\\mathbf{\\hat{a}}}\\right)$ , $|\\theta_{k}^{*}|\\,=\\,\\Theta_{k}\\left(k^{-r}\\right)$ , for some $a,r\\,>\\,0$ . Our result (Propositions $\\mathrm{D}.5\\mathrm{+}\\mathrm{D}.6\\mathrm{+}\\mathrm{E}.1\\rangle$ ) is on the left, which improves over previous result from [7] (Proposition D.6) on the right. On the left plot, the range of the source coefficient $\\textstyle s={\\frac{2r+a}{1+a}}\\,$ in Assumption (SC) is shown in gray font in each colored region. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{V}=\\left\\{\\Theta\\left(\\sigma^{2}\\right),\\begin{array}{l l}{\\mathrm{~\\Theta~Assumption~(IF)~holds,}}\\\\ {\\mathcal{O}\\left(\\sigma^{2}n^{2a}\\right),}&{\\mathrm{Assumption~(GF)~holds}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "in Table 1 is necessary, as Assumption (GF) includes the cases of Gaussian Design (GD) (or more generally independent features (IF)) and Laplacian kernel which yields $\\mathcal{V}=\\Theta$ (1), the so-called tempered overfitting from [35]; as well as the case of neural tangent kernel (NTK) which yields V\u2212n\u2212\u2192\u2212\u2212\u221e\u2192\u221e, the so-called catastrophic overfitting. In particular, our proof shows that the Gaussian Equivalent Property (GEP) does not hold under weak ridge. ", "page_idx": 5}, {"type": "image", "img_path": "IMlDpZmLnL/tmp/6801e8bd7c1d76afd8cc6efd2b5dfdeee30e069321a4a62c021192c077c9308b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 2: Variance $\\mathcal{V}$ against sample size $n$ for the Laplacian kernel (left) and the neural tangent kernel with 1 hidden-layer (right) defined on the unit 2-disk, validating Equation (7) where the variance with generic features (GF) can be as good as with independent features (IF) $(\\mathcal{V}=\\Theta_{n}\\left(1\\right))$ or qualitatively different $\\langle\\nu\\xrightarrow{n\\to\\infty}\\infty\\rangle$ ). See Section 6 for more details. ", "page_idx": 5}, {"type": "text", "text": "We are now prepared to address the first question posed in Section 1: ", "page_idx": 5}, {"type": "text", "text": "$Q I$ : When and why does the Gaussian Equivalence Property (GEP) exist? ", "page_idx": 5}, {"type": "text", "text": "The Master inequalities provide the same non-asymptotic bounds for both cases under a strong ridge. However, GEP does not hold under weak ridge! 5 ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In particular, our work implies that previous works [10, 16, 34, 44, 35] under the Gaussian Design assumption (GD) can be applied only when the ridge is strong. ", "page_idx": 6}, {"type": "text", "text": "Upon finalizing the preparation for this paper, we became aware of concurrent research conducted by [19], which also concerns the Gaussian Equivalence Property (GEP) in the non-asymptotic setting. For more comparison between their assumptions and ours, we refer the reader to Section A. ", "page_idx": 6}, {"type": "text", "text": "Importance of ridge To address the second question posed in Section 1, it is evident that in either the under-parameterized setting with any ridge (see Section F) or the over-parameterized regime with a strong ridge (see Table 1), the bounds for both $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ and $\\mathcal{V}$ remain the same, irrespective of the features: ", "page_idx": 6}, {"type": "text", "text": "Q2: Under what conditions is the generalization error fully determined by the eigen-decay? Either (i) in the under-parameterized setting; or (ii) with a strong ridge in the over-parameterized regime. ", "page_idx": 6}, {"type": "text", "text": "Several results [39, 33, 36] have suggested that the test error bound can be characterized by the covariance spectrum $\\Sigma$ , but they implicitly require the ridge $\\lambda>0$ to be larger than some threshold. This paper clearly demonstrates the necessity of the presence of a strong ridge in such analyses. ", "page_idx": 6}, {"type": "text", "text": "From Table 1, we can see that the eigen-decay also affects the test error qualitatively. As mentioned in [33], the bias term $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ decays polynomially (or, respectively, exponentially) when the eigen-decay is polynomial (or, respectively, exponential). However, we prove that $\\nu$ decays only polynomially at a rate at most $\\textstyle{\\mathcal{O}}\\left({\\frac{1}{n}}\\right)$ , regardless of the eigen-decay. Hence, in a noisy setting with polynomial eigen-decay, one can find an optimal ridge $\\lambda=\\Theta_{n}\\left(n^{-\\frac{1+a}{(1+a)\\tilde{s}+1}}\\right)$ to balance both terms $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ and $\\nu$ as in [16, 31]. In contrast, in noisy settings with exponential eigen-decay, $\\mathcal{V}$ dominates the test error. The bound of the variance term $\\nu$ with exponential eigen-decay under weak ridge is omitted in Table 1 due to the so-called catastrophic overfitting phenomenon observed in [35, 14]. ", "page_idx": 6}, {"type": "text", "text": "Improved upper bound During our research, we discovered that we can improve the upper bound of $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ (Equation 6) in the over-parameterized regime with polynomial decay, weak ridge, and Assumption (GF) by adapting the result from [31] and integrating it with the insights from [7] (see the upper right corner in Table 1 or 6, and Figure 1 for visualization). ", "page_idx": 6}, {"type": "text", "text": "Theorem 3.1 (Improved upper bound). Suppose Assumption $(G F)$ holds. Assume the eigen-spectrum and the target coefficient both have polynomial decay, that is, $\\lambda_{k}\\;=\\;\\Theta_{k}\\left(k^{-1-a}\\right)$ and $|\\theta_{k}^{*}|\\;=$ $\\Theta_{k}\\left(k^{-r}\\right)$ . Let $\\textstyle s={\\frac{2r+a}{1+a}}\\,$ be the source coefficient defined in Definition (SC). Then the kernel ridgeless regression has the following learning rate for the bias term $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{B}=\\left\\{\\!\\!\\begin{array}{l l}{\\mathcal{O}_{n}\\left(n^{-(r-a)_{+}}\\right)}&{\\ f o r\\,s<1;}\\\\ {\\Theta_{n}\\left(n^{-(2r+a)}\\right)}&{\\ f o r\\,1\\le s\\le2;}\\\\ {\\mathcal{O}_{n}\\left(n^{-2(1+a)}\\right)}&{\\ f o r\\,s>2.}\\end{array}\\!\\!\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where n is the sample size and $(\\cdot)_{+}\\overset{\\mathrm{def.}}{=}\\operatorname*{max}(\\cdot,0)$ . ", "page_idx": 6}, {"type": "text", "text": "For a detailed explanation of our improvements, refer to our novel Proposition D.5 and the known Proposition D.6 from [7] in Section D. ", "page_idx": 6}, {"type": "text", "text": "Lower bound of test error It is of theoretical interest to provide lower bounds as well. For independent features, we can prove that the upper bound is tight using the result from [46], generalizing the result from [16], which only showed upper bounds in Gaussian Design Assumption (GD). However, for generic features, we can only provide lower bounds in some but not all settings, using the result from [31]. We summarize our results in Table 2. ", "page_idx": 6}, {"type": "table", "img_path": "IMlDpZmLnL/tmp/b59ab54837c7211a7e9b6b09b11413b6a0c04cb52d1c24807b7abf54f61bc5ad.jpg", "table_caption": [], "table_footnote": ["Table 2: The table shows whether the lower bound is matching the upper bound deduced in this paper. "], "page_idx": 7}, {"type": "text", "text": "See Section E for details on our proof of matching lower bounds. We note that there is some KRR literature, such as [31, 36, 19], that discusses matching upper and lower bounds of test error under more assumptions, which is beyond the scope of this paper. For a comparison of assumptions in different papers, see Section A. ", "page_idx": 7}, {"type": "text", "text": "Finally, we summarize the above discussion by answering the third question we raised in Section 1: ", "page_idx": 7}, {"type": "text", "text": "Q3: Is there a unifying theory on the generalization performance under minimal assumptions? Yes, this paper considers assumptions (IF) and (GF) which cover a wide range of kernel settings under any regularization and source conditions. ", "page_idx": 7}, {"type": "text", "text": "4 Proof sketch ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "All the above results in Table 1 can be derived using the following proof strategy: ", "page_idx": 7}, {"type": "text", "text": "1. We prove a concentration result on the whitened features $\\mathbf{z}$ under Assumptions (GF) or (IF) (see Section G).   \n2. Using the above result, we bound the condition number of the (truncated) kernel matrix (see Section G) as in [7, 14], which will be used to bound the test error in the next step.   \n3. Combining the bound of the condition number with the result from [6, 46], which we call the Master inequalities (see the paragraph below for details), we can compute the non-asymptotic test error bound for various settings (see Section D).   \n4. In the over-parameterized regime, we derive the asymptotic behavior of the learning curves by plugging in the eigen-decay and the choice of the target function and ridge in the above non-asymptotic bound (see Section D).   \n5. Using the results from [46], we are able to show that the asymptotic upper bound for independent features (IF) is tight (see section E).   \n6. For generic features (GF), we only provide tight bound results in limited settings (see section E). As shown in Figure 2 in Section 3, the generic feature Assumption (GF) includes a broad variety of features where a universal matching lower bound does not exist.   \n7. In the under-parameterized regime, the Master inequalities also give upper bounds for $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ and $\\mathcal{V}$ , which might not be tight if the ridge $\\lambda$ is strong. However, we present another way to obtain tight bounds without the Master inequalities (see Section F for more details). ", "page_idx": 7}, {"type": "text", "text": "We summarize the proof techniques with a flowchart in Figure 4. ", "page_idx": 7}, {"type": "text", "text": "Master Inequalities We will now briefly introduce the key inequality used in our analysis (further details can be found in the Appendix). We will use the subscript $>k$ or $\\leq k$ to refer to the submatrix of a matrix consisting of columns with index $>k$ or $\\leq k$ (see definition C.3). The analysis relies on two crucial matrices: $\\mathbf{A}_{k}=\\mathbf{X}_{>k}\\mathbf{X}_{>k}^{\\top}+n\\lambda\\mathbf{I}_{n}\\in\\mathbb{R}^{n\\times n}$ , defined for any $k\\in\\mathbb{N}$ , which is employed to partition the spectrum, and a whitened input matrix $\\mathbf{Z}\\ {\\stackrel{\\mathrm{def.}}{=}}\\ \\mathbf{X}\\mathbf{\\Sigma}^{-1/2}\\,\\in\\,\\mathbb{R}^{n\\times p}$ . Additionally, we require the following definitions. ", "page_idx": 7}, {"type": "text", "text": "Definition 4.1 (Concentration coefficients [7]). Define the quantities: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\rho\\stackrel{\\mathrm{def.}}{=}\\frac{n\\,\\|\\Sigma_{>k}\\|_{\\mathrm{op}}+s_{1}(\\mathbf{A}_{k})}{s_{n}(\\mathbf{A}_{k})};\\quad\\mathrm{~}\\zeta\\stackrel{\\mathrm{def.}}{=}\\frac{s_{1}(\\mathbf{Z}_{\\leq k}^{\\top}\\mathbf{Z}_{\\leq k})}{s_{k}(\\mathbf{Z}_{\\leq k}^{\\top}\\mathbf{Z}_{\\leq k})};\\quad\\mathrm{~}\\xi\\stackrel{\\mathrm{def.}}{=}\\frac{s_{1}(\\mathbf{Z}_{\\leq k}^{\\top}\\mathbf{Z}_{\\leq k})}{n},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $s_{i}(\\cdot)$ denotes the $i$ -th largest singular value of the matrix. ", "page_idx": 7}, {"type": "text", "text": "Definition 4.2 (Effective ranks [6]). Let $k\\in\\mathbb{N}$ . Define two quantities: ", "page_idx": 8}, {"type": "equation", "text": "$$\nr_{k}\\overset{\\mathrm{def.}}{=}\\frac{{\\mathrm{Tr}}[\\Sigma_{>k}]}{\\|\\Sigma_{>k}\\|_{\\mathrm{op}}}=\\frac{\\sum_{l=k+1}^{p}\\lambda_{l}}{\\lambda_{k+1}},\\quad R_{k}\\overset{\\mathrm{def.}}{=}\\frac{{\\mathrm{Tr}}[\\Sigma_{>k}]^{2}}{{\\mathrm{Tr}}[\\Sigma_{>k}^{2}]}=\\frac{\\left(\\sum_{l=k+1}^{p}\\lambda_{l}\\right)^{2}}{\\sum_{l=k+1}^{p}\\lambda_{l}^{2}}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "The Master inequalities provide upper bounds on the bias and the (scaled) variance term of the test error in the following form: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{B\\leq\\left(\\frac{1+\\rho^{2}\\zeta^{2}\\xi^{-1}+\\rho}{\\delta}\\right)\\|\\theta_{>k}^{*}\\|_{\\Sigma_{>k}}^{2}+(\\zeta^{2}\\xi^{-2}+\\rho\\zeta^{2}\\xi^{-1})\\frac{s_{1}(\\mathbf{A}_{k})^{2}}{n^{2}}\\left\\|\\theta_{\\leq k}^{*}\\right\\|_{\\Sigma_{\\leq k}^{-1}}^{2};}\\\\ {\\mathcal{V}/\\sigma^{2}\\leq\\rho^{2}\\left(\\zeta^{2}\\xi^{-1}\\frac{k}{n}+\\frac{\\mathrm{Tr}[\\mathbf{Z}_{>k}\\Sigma_{>k}^{2}\\mathbf{Z}_{>k}^{\\top}]}{n\\,\\mathrm{Tr}[\\Sigma_{>k}^{2}]}\\frac{r_{k}(\\Sigma)^{2}}{n R_{k}(\\Sigma)}\\right).\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "The term appearing in the above bound can be categorized into two distinct components: the \"probably constant\" part (highlighted in blue) and the \"decay\" part (highlighted in gray). The \"probably constant\" part consists of terms that, with high probability, are bounded both below and above by positive constants\u2014these bounds represent a primary contribution of this paper. On the other hand, the \"decay\" part can be approximated using basic calculus, once a specific constant $k\\in\\mathbb{N}$ , smaller than the sample size $n$ , is selected. Together, these two parts allow us to derive the KRR learning rate for all combinations of eigen-spectra, features, and ridge parameters discussed in the preceding sections. Furthermore, [46] provides a corresponding lower bound under certain assumptions, demonstrating that the decay of the upper bound matches that of the lower bound. Establishing that Assumption (IF) satisfies these assumptions is another key contribution of this work. For the formal definitions of the terms in the above inequalities, we refer the reader to Propositions C.6 and C.7 in the appendix. ", "page_idx": 8}, {"type": "text", "text": "5 Related work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We briefly discuss related previous works and compare them with our results in the over-parameterized regime: (see Table 6 for more visual illustration) ", "page_idx": 8}, {"type": "text", "text": "1. [16] considered the upper bound of $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ and $\\mathcal{V}$ in polynomial eigen-decay under any ridge and under the Gaussian Design Assumption. Our result proves both the matching upper and lower bound with the same decay rate under a weaker assumption (IF). This implies that we validate the Gaussian Equivalence Property for Sub-Gaussian Design.   \n2. [30, 31] proved tight upper bounds of $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ and $\\nu$ for H\u00f6lder continuous kernels under polynomial eigen-decay, strong ridge and the so-called Embedding condition (EC). [7] recovered the upper bounds under Assumption (GF).   \n3. For polynomial decay under weak ridge, [31] provided a tight upper bound of $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ when the source condition $s>1$ ; while [7] provided a loose bound regardless of $s$ . Hence we modify the proof in [31] under Assumption (GF) instead and combine it with [7] to obtain a novel upper bound on the bias.   \n4. [7] also provided an upper bound of $\\nu$ under polynomial decay, weak ridge and Assumption (GF).   \n5. [14] showed that $\\mathcal{V}$ is bounded above and below by positive constants under polynomial eigendecay, weak ridge and Assumption (IF), and it exhibits the so-called tempered overfitting from [35].   \n6. [33] provided tight upper bounds of $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ for both polynomial and exponential eigen-decay for kernels under the so-called Maximal Degree-of-Freedom (MaxDoF) Assumption. We recover their result under Assumption (GF) instead of Assumption (MaxDoF).   \n7. We apply the result from [46] and [31] to obtain a matching lower bound in some settings and strengthen our result from $O_{n}\\left(\\cdot\\right)$ to $\\Theta_{n}\\left(\\cdot\\right)$ . ", "page_idx": 8}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Due to page constraints, this section focuses solely on experiments validating the Gaussian Equivalent Property (GEP). For detailed experiments on other contributions, refer to Section I. ", "page_idx": 8}, {"type": "text", "text": "We consider a simple example: let $\\begin{array}{r}{\\lambda_{k}\\,=\\,(\\frac{2k-1}{2}\\pi)^{-1-a}}\\end{array}$ and $\\begin{array}{r}{\\psi_{k}(\\cdot)\\,=\\,\\sqrt{2}\\sin\\left(\\frac{2k-1}{2}\\pi\\cdot\\right)}\\end{array}$ such that $\\|\\psi_{k}\\|_{L_{\\mu}^{2}}\\,=\\,1$ for $\\mu\\,=\\,\\operatorname{unif}[0,1]$ ; let $\\begin{array}{r}{\\theta_{k}^{*}\\,=\\,(\\frac{2k-1}{2}\\pi)^{-r}}\\end{array}$ . For $p\\,=\\,\\infty$ and $a\\,=\\,1$ , the regression coincides with kernel ridge regression with kernel $k(x,x^{\\prime})=\\operatorname*{min}\\{x,x^{\\prime}\\}$ defined on the interval $[0,1]$ [48]. Similar experiments have been conducted on this kernel $k$ by [30, 33]. However, to simulate regression for independent features (IF), the feature rank $p$ must be finite. In the following experiment, we choose $p\\,=\\,2000$ , and the sample size $n$ ranges from 100 to 1000, with ridge parameter $\\begin{array}{r}{\\lambda=(\\frac{2n-1}{2}\\pi)^{\\bar{-}b}}\\end{array}$ where $b\\in[0,1+a]$ . ", "page_idx": 9}, {"type": "text", "text": "In Figure 3, our experiment demonstrates the GEP, as the learning curves with kernel features (Sine features) and independent features (Gaussian features $\\mathbf{z}\\sim\\mathcal{N}(0,I_{p})$ or Rademacher features $\\mathbf{z}\\sim(\\mathrm{unif\\{\\pm1\\}})^{p})$ coincide and match the theoretical decay. ", "page_idx": 9}, {"type": "image", "img_path": "IMlDpZmLnL/tmp/1126f0de9c8fe73440a82005671e31858544cb1a66f2a39652c975bfc663338b.jpg", "img_caption": ["Figure 3: Decay of the bias term $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ and the variance term $\\mathcal{V}$ under different ridge decays and target coefficient decays. All features demonstrate the same theoretical decay, validating the GEP for independent features. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we present a unifying theory and a comprehensive analysis of the learning curve of kernel ridge regression under various settings. We elucidate the coincidence of learning curves between the Gaussian Design (GD) setting (or more generally, independent features (IF)) and the kernel setting (or more generally, generic features (GF)), and validate the Gaussian Equivalence Property under strong ridge. In addition to recovering previous results, we also improve test error bounds under specific circumstances, thus filling a gap in the existing literature. ", "page_idx": 9}, {"type": "text", "text": "Future potential work Our results also raise several theoretical questions. For instance: i) The upper bound in several regions of the phase diagram in Figure 1 is not known to be sharp. It would be of interest to either improve it or find matching lower bounds in those regions; ii) Why is there a qualitative difference in overftiting with the Laplacian kernel and the neural tangent kernel, as shown in Figure 2? Can one further distinguish different cases in the generic feature Assumption (GF) to explain such differences? Additionally, how tight would the bound $\\mathcal{V}=\\mathcal{O}\\left(\\sigma^{2}n^{2a}\\right)$ in Eq. (7) be? ", "page_idx": 9}, {"type": "text", "text": "Addressing these questions could provide deeper insights into the behavior of kernel ridge regression under various conditions and contribute to further advancing our understanding in machine learning. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "A. Kratsios acknowledges financial support from an NSERC Discovery Grant No. RGPIN-2023- 04482 and No. DGECR-2023-00230. A. Kratsios also acknowledges that resources used in preparing this research were provided, in part, by the Province of Ontario, the Government of Canada through CIFAR, and companies sponsoring the Vector Institute6. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks. In International Conference on Machine Learning, pages 322\u2013332. PMLR, 2019.   \n[2] Kendall Atkinson and Weimin Han. Spherical harmonics and approximations on the unit sphere: an introduction, volume 2044 of Lecture Notes in Mathematics. Springer, Heidelberg, 2012.   \n[3] Benjamin Aubin, Florent Krzakala, Yue Lu, and Lenka Zdeborov\u00e1. Generalization error in high-dimensional perceptrons: Approaching bayes error with convex optimization. Advances in Neural Information Processing Systems, 33:12199\u201312210, 2020.   \n[4] Francis Bach. Breaking the curse of dimensionality with convex neural networks, 2016.   \n[5] Francis Bach. High-dimensional analysis of double descent for linear regression with random projections. arXiv preprint arXiv:2303.01372, 2023.   \n[6] Peter L Bartlett, Philip M Long, G\u00e1bor Lugosi, and Alexander Tsigler. Benign overfitting in linear regression. Proceedings of the National Academy of Sciences, 117(48):30063\u201330070, 2020.   \n[7] Daniel Barzilai and Ohad Shamir. Generalization in kernel regression under realistic assumptions, 2023.   \n[8] Mikhail Belkin, Siyuan Ma, and Soumik Mandal. To understand deep learning we need to understand kernel learning. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 541\u2013549. PMLR, 10\u201315 Jul 2018.   \n[9] Alberto Bietti and Julien Mairal. On the inductive bias of neural tangent kernels. Advances in Neural Information Processing Systems, 32, 2019.   \n[10] Blake Bordelon, Abdulkadir Canatar, and Cengiz Pehlevan. Spectrum dependent learning curves in kernel regression and wide neural networks. In International Conference on Machine Learning, pages 1024\u20131034. PMLR, 2020.   \n[11] David Borthwick. Spectral theory\u2014basic concepts and applications, volume 284 of Graduate Texts in Mathematics. Springer, Cham, [2020] \u00a92020.   \n[12] Andrea Caponnetto and Ernesto De Vito. Optimal rates for the regularized least-squares algorithm. Foundations of Computational Mathematics, 7:331\u2013368, 2007.   \n[13] Tin Sum Cheng, Aurelien Lucchi, Ivan Dokmani\u00b4c, Anastasis Kratsios, and David Belius. A theoretical analysis of the test error of finite-rank kernel ridge regression. Annual Conference on Neural Information Processing Systems, 2023.   \n[14] Tin Sum Cheng, Aurelien Lucchi, Anastasis Kratsios, and David Belius. Characterizing overftiting in kernel ridgeless regression through the eigenspectrum. In The Eleventh International Conference on Learning Representations, 2024.   \n[15] Samuel N Cohen, Deqing Jiang, and Justin Sirignano. Neural q-learning for solving pdes. Journal of Machine Learning Research, 24(236), 2023.   \n[16] Hugo Cui, Bruno Loureiro, Florent Krzakala, and Lenka Zdeborov\u00e1. Generalization error rates in kernel regression: The crossover from the noiseless to noisy regime. Advances in Neural Information Processing Systems, 34:10131\u201310143, 2021.   \n[17] Harold Donnelly. Bounds for eigenfunctions of the Laplacian on compact Riemannian manifolds. J. Funct. Anal., 187(1):247\u2013261, 2001.   \n[18] Junichi Fujii, Masatoshi Fujii, Takayuki Furuta, and Ritsuo Nakamoto. Norm inequalities equivalent to heinz inequality. Proceedings of the American Mathematical Society, 118(3):827\u2013 830, 1993.   \n[19] Georgios Gavrilopoulos, Guillaume Lecu\u00e9, and Zong Shang. A geometrical analysis of kernel ridge regression and its applications, 2024.   \n[20] Sebastian Goldt, Bruno Loureiro, Galen Reeves, Florent Krzakala, Marc M\u00e9zard, and Lenka Zdeborov\u00e1. The gaussian equivalence of generative models for learning with shallow neural networks. In Mathematical and Scientific Machine Learning, pages 426\u2013471. PMLR, 2022.   \n[21] Sebastian Goldt, Marc M\u00e9zard, Florent Krzakala, and Lenka Zdeborov\u00e1. Modeling the influence of data structure on learning in neural networks: The hidden manifold model. Phys. Rev. X, 10:041044, Dec 2020.   \n[22] Moritz Haas, David Holzm\u00fcller, Ulrike Luxburg, and Ingo Steinwart. Mind the spikes: Benign overftiting of kernels and neural networks in fixed dimension. Advances in Neural Information Processing Systems, 36, 2024.   \n[23] Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani. Surprises in highdimensional ridgeless least squares interpolation. Annals of statistics, 50(2):949, 2022.   \n[24] Simon Hubbert, Emilio Porcu, Chris J. Oates, and Mark Girolami. Sobolev spaces, kernels and discrepancies over hyperspheres. Transactions on Machine Learning Research, 2023.   \n[25] Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.   \n[26] Deqing Jiang, Justin Sirignano, and Samuel N Cohen. Global convergence of deep galerkin and pinns methods for solving partial differential equations. arXiv preprint arXiv:2305.06000, 2023.   \n[27] Vladimir Koltchinskii and Karim Lounici. Concentration inequalities and moment bounds for sample covariance operators. Bernoulli, 23(1):110\u2013133, 2017.   \n[28] Lars Larsson-Cohn. $L^{p}$ -norms of Hermite polynomials and an extremal problem on Wiener chaos. Ark. Mat., 40(1):133\u2013144, 2002.   \n[29] Yicheng Li, Haobo Zhang, and Qian Lin. On the saturation effect of kernel ridge regression. In The Eleventh International Conference on Learning Representations, 2022.   \n[30] Yicheng Li, Haobo Zhang, and Qian Lin. Kernel interpolation generalizes poorly. Biometrika, 2023.   \n[31] Yicheng Li, Haobo Zhang, and Qian Lin. On the asymptotic learning curves of kernel ridge regression under power-law decay. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[32] Tengyuan Liang and Alexander Rakhlin. Just interpolate: Kernel \u201cridgeless\u201d regression can generalize. The Annals of Statistics, 48(3), Jun 2020.   \n[33] Jihao Long, Xiaojun Peng, and Lei Wu. A duality analysis of kernel ridge regression in the noiseless regime, 2024.   \n[34] Bruno Loureiro, Cedric Gerbelot, Hugo Cui, Sebastian Goldt, Florent Krzakala, Marc Mezard, and Lenka Zdeborov\u00e1. Learning curves of generic features maps for realistic datasets with a teacher-student model. Advances in Neural Information Processing Systems, 34:18137\u201318151, 2021.   \n[35] Neil Mallinar, James B. Simon, Amirhesam Abedsoltan, Parthe Pandit, Mikhail Belkin, and Preetum Nakkiran. Benign, tempered, or catastrophic: A taxonomy of overfitting, 2022.   \n[36] Theodor Misiakiewicz and Basil Saeed. A non-asymptotic theory of kernel ridge regression: deterministic equivalents, test error, and gcv estimator, 2024.   \n[37] \u00c9tienne Pardoux. Backward stochastic differential equations and viscosity solutions of systems of semilinear parabolic and elliptic PDEs of second order. In Stochastic analysis and related topics, VI (Geilo, 1996), volume 42 of Progr. Probab., pages 79\u2013127. Birkh\u00e4user Boston, Boston, MA, 1998.   \n[38] Etienne Pardoux and Shige Peng. Backward stochastic differential equations and quasilinear parabolic partial differential equations. In Stochastic Partial Differential Equations and Their Applications: Proceedings of IFIP WG 7/1 International Conference University of North Carolina at Charlotte, NC June 6\u20138, 1991, pages 200\u2013217. Springer, 2005.   \n[39] Lorenzo Rosasco, Mikhail Belkin, and Ernesto De Vito. On learning with integral operators. Journal of Machine Learning Research, 11(2), 2010.   \n[40] Saburou Saitoh and Yoshihiro Sawano. Theory of reproducing kernels and applications, volume 44 of Developments in Mathematics. Springer, Singapore, 2016.   \n[41] Isaac J Schoenberg. Positive definite functions on spheres. Duke Mathematical Journal, 9(1):96 \u2013 108, 1942.   \n[42] Mohamed El Amine Seddik, Cosme Louart, Mohamed Tamaazousti, and Romain Couillet. Random matrix theory proves that deep learning representations of gan-data behave as gaussian mixtures. In International Conference on Machine Learning, pages 8573\u20138582. PMLR, 2020.   \n[43] Barry Simon. Operator theory, volume 4. American Mathematical Soc., 2015.   \n[44] James B Simon, Madeline Dickens, Dhruva Karkada, and Michael Deweese. The eigenlearning framework: A conservation law perspective on kernel ridge regression and wide neural networks. Transactions on Machine Learning Research, 2023.   \n[45] Ingo Steinwart and Andreas Christmann. Support vector machines. Springer Science & Business Media, 2008.   \n[46] Alexander Tsigler and Peter L Bartlett. Benign overftiting in ridge regression. J. Mach. Learn. Res., 24:123\u20131, 2023.   \n[47] Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices, page 210\u2013268. Cambridge University Press, 2012.   \n[48] Martin J. Wainwright. High-Dimensional Statistics: A Non-Asymptotic Viewpoint. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2019.   \n[49] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization, 2017.   \n[50] Haobo Zhang, Yicheng Li, and Qian Lin. On the optimality of misspecified spectral algorithms, 2023.   \n[51] Nikita Zhivotovskiy. Dimension-free bounds for sums of independent matrices and simple tensors via the variational principle. Electronic Journal of Probability, 29:1\u201328, 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The appendix will be organized in the following way: ", "page_idx": 13}, {"type": "text", "text": "\u2022 The detailed discussion of the assumptions used in this paper and the comparison to previous literature is presented in Section A;   \n\u2022 an extended discussion of the eigen-decay assumptions is presented in Section B;   \n\u2022 the main tool of this paper, the Master inequalities, is presented in Section C;   \n\u2022 the details of the proof on the asymptotic in over-parameterized regime in Table 1 is presented in Section D;   \n\u2022 the proof for a matching lower bound is presented in Section E;   \n\u2022 the details of the proof on the non-asymptotic bound in under-parameterized regime is presented in Section F;   \n\u2022 the concentration result on the whitened feature ${\\bf z}$ and the conditioning of the kernel matrix are presented in Section G;   \n\u2022 related technical results from previous literature are collected in Section H;   \n\u2022 experimental results are shown in Section I;   \n\u2022 extra tables of summary can be found in Section J. ", "page_idx": 13}, {"type": "text", "text": "The summary of our proof can be found in the flowchart in Figure 4. ", "page_idx": 13}, {"type": "image", "img_path": "IMlDpZmLnL/tmp/73a69ee8acc34746d007c451ebc8f00651df740f154a78587bf601af6870dbca.jpg", "img_caption": ["Figure 4: A flowchart about the proof techniques in this paper "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "Big-O Notation We use the standard Big- $o$ notations $\\mathcal{O}.\\left(\\cdot\\right),o.\\left(\\cdot\\right),\\Omega.\\left(\\cdot\\right),\\Theta.\\left(\\cdot\\right)$ to represents the asymptotic behaviours of the bounds in this paper, where the subscript indicates that the constant in the bound is independent to the variables in the subscript. We use \u02dc\u00b7 to suppress logarithmic terms. With abuse of notation, we use the same notations for their probability versions, when the context is clear. ", "page_idx": 13}, {"type": "text", "text": "A Assumptions in details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The primary assumptions made in this paper concern the whitened feature vectors $\\mathbf{z}\\in\\mathbb{R}^{p}$ : ", "page_idx": 14}, {"type": "text", "text": "Assumption (GF) (Generic features [7]). Let $p\\in\\mathbb{N}\\cup\\{\\infty\\}$ . Assume the isotropic feature vector $\\mathbf{z}\\ {\\stackrel{\\mathrm{def.}}{=}}\\ \\sum^{-1/2}\\mathbf{x}\\in\\mathbb{R}^{p}$ and the covariance matrix $\\Sigma\\in\\mathbb{R}^{p\\times p}$ are such that: 7 ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\alpha_{k}\\overset{\\mathrm{def.}}{=}\\underset{\\mathbf{z}}{\\sim}\\operatorname*{ssinf}\\frac{\\left\\|\\mathbf{z}_{>k}\\right\\|_{\\mathbf{\\hat{Z}}_{>k}}^{2}}{\\mathrm{Tr}\\left[\\mathbf{\\Sigma}_{>k}\\right]}=\\Theta_{k}\\left(1\\right),}\\\\ &{\\beta_{k}\\overset{\\mathrm{def.}}{=}\\underset{\\mathbf{z}}{\\sim}\\operatorname*{ssup}\\operatorname*{max}\\left\\{\\frac{\\left\\|\\mathbf{z}_{\\leq k}\\right\\|_{2}^{2}}{k},\\frac{\\left\\|\\mathbf{z}_{>k}\\right\\|_{\\mathbf{\\Sigma}_{>k}}^{2}}{\\mathrm{Tr}\\left[\\mathbf{\\Sigma}_{>k}\\right]},\\frac{\\left\\|\\mathbf{z}_{>k}\\right\\|_{\\mathbf{\\Sigma}_{>k}}^{2}}{\\mathrm{Tr}\\left[\\mathbf{\\Sigma}_{>k}^{2}\\right]}\\right\\}=\\Theta_{k}\\left(1\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Remark A.1 $\\;k=0,$ ). For $k=0$ , we take the convention that $\\begin{array}{r}{\\beta_{0}\\ {\\stackrel{\\mathrm{def.}}{=}}\\ \\exp\\operatorname*{sup}_{\\mathbf{z}}\\operatorname*{max}\\left\\{{\\frac{\\|\\mathbf{z}\\|_{\\Sigma}^{2}}{\\mathrm{Tr}\\left[\\Sigma\\right]}},{\\frac{\\|\\mathbf{z}\\|_{\\Sigma^{2}}^{2}}{\\mathrm{Tr}\\left[\\Sigma^{2}\\right]}}\\right\\}.}\\end{array}$ . Remark A.2 (Intuition). The intuition is that the three fractions above have expected values of 1: $\\begin{array}{r}{\\mathbb{E}_{\\mathbf{z}}\\left[\\frac{\\left\\|\\mathbf{z}_{\\le k}\\right\\|_{2}^{2}}{k}\\right]=\\mathbb{E}_{\\mathbf{z}}\\left[\\frac{\\left\\|\\mathbf{z}_{>k}\\right\\|_{\\mathbf{z}_{>k}}^{2}}{\\mathrm{Tr}\\left[\\mathbf{\\Xi}\\_{>k}\\right]}\\right]=\\mathbb{E}_{\\mathbf{z}}\\left[\\frac{\\left\\|\\mathbf{z}_{>k}\\right\\|_{\\mathbf{z}_{>k}^{2}}^{2}}{\\mathrm{Tr}\\left[\\mathbf{\\Xi}_{>k}^{2}\\right]}\\right]=1.}\\end{array}$ Hence, Assumption (GF) imposes strong concentration on ${\\bf z}$ at each truncation point $k\\in\\mathbb{N}$ . ", "page_idx": 14}, {"type": "text", "text": "Remark A.3 (Weaken Assumption (GF)). One can further replace the essential supremum and infinum by a high probability guarantee. The argument on the test error bounds would follow analogously with a weaker probability. For simplicity reasons, we only consider the assumption in the above formulation. ", "page_idx": 14}, {"type": "text", "text": "Assumption (GF) encompasses both cases of dependent and independent features $z_{k}$ , which can have qualitatively different effects on the test error. ", "page_idx": 14}, {"type": "text", "text": "To compare the realistic kernel setting with the Gaussian Design setting (GD) used in previous literature [10, 34, 16, 44, 35], which replaces the feature vectors with Gaussian random vectors (with independent entries), we consider a weaker version of the Gaussian Design assumption commonly required in KRR literature [6, 13, 5, 36, 19]: ", "page_idx": 14}, {"type": "text", "text": "Assumption (IF) (Independent sub-Gaussian features). Suppose $p\\in\\mathbb{N}$ . Assume that the features $z_{k}$ \u2019s in the isotropic feature vector z are independent to each other and there exists a constant $G>0$ such that the sub-Gaussian norm $\\|z_{k}\\|_{\\psi_{2}}$ of any feature $z_{k}$ is bounded by $G$ . In over-parameterized regime, we also assume that there exists some constant $\\eta>1$ independent to n such that $p>\\eta n$ . Remark A.4 (Infinite rank). With Assumption (GF), the feature dimension $p$ can be taken as $\\infty$ ; with Assumption (IF), however, if no further boundedness assumption is imposed, the norm $\\|\\mathbf{x}\\|_{2}=$ $\\sqrt{\\sum_{k=1}^{p}\\lambda_{k}z_{k}^{2}}$ can be undefined if $p\\,=\\,\\infty$ . However, if the sub-Gaussian variables $z_{k}$ \u2019s are all bounded in the sense of Assumption (GF), then $p$ can be chosen to be infinity as well. Remark A.5 (Special case). If the features $z_{k}$ are additionally bounded, then Assumption (IF) becomes a special case of Assumption (GF). The former assumes independence of the features, while the latter does not. ", "page_idx": 14}, {"type": "text", "text": "Both assumptions (GF) and (IF) are much weaker than those in most KRR literature in the sense that: i) there is no explicit assumption on the input space $\\mathcal{X}$ ; ii) the distribution of the feature vector need not even be continuous! It is surprising that even with such minimal assumptions, one can derive tight bounds on the test error. ", "page_idx": 14}, {"type": "text", "text": "For a more detailed comparison between the assumptions in this paper and those in previous literature, see Section A. ", "page_idx": 14}, {"type": "text", "text": "We next compare the assumptions (IF), (GF) with the assumptions in previous literature. ", "page_idx": 14}, {"type": "text", "text": "A.1 Among assumptions with independent features ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The Gaussian Design Assumption, often employed in various literature such as [10, 16, 34, 20, 35, 44], can be considered a special case of Assumption (IF): ", "page_idx": 14}, {"type": "text", "text": "7We write $\\lVert\\mathbf{v}\\rVert_{\\mathbf{M}}\\stackrel{\\mathrm{def.}}{=}\\sqrt{\\mathbf{v}^{\\top}\\mathbf{M}\\mathbf{v}}$ for PDS matrix M and any vector v. See Definition C.1 Section C for more details. ", "page_idx": 14}, {"type": "text", "text": "Assumption (GD) (Gaussian Design). The feature vector $\\mathbf{z}\\sim\\mathcal{N}(0,\\mathbf{I}_{p})$ is distributed as isotropic Gaussian. ", "page_idx": 15}, {"type": "text", "text": "Note that Assumption (IF) is much weaker than Assumption (GD) in the sense that: i) the distribution of the feature vector ${\\bf z}$ is no longer rotational invariant or enjoys anti-concentration property; 8 ii) the distribution of each feature $z_{k}$ needs not be continuous; iii) the features $z_{k}$ \u2019s needs not be identical. ", "page_idx": 15}, {"type": "text", "text": "Meanwhile, Assumption (IF) appear commonly in KRR literature, for example [6, 46, 23, 5, 13, 14]. ", "page_idx": 15}, {"type": "text", "text": "A.2 Among assumptions with dependent features ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Examples that satisfy Assumption (GF), which is first introduced by [7], includes: ", "page_idx": 15}, {"type": "text", "text": "1. dot-product kernels on hyperspheres;   \n2. kernels with bounded eigenfunctions;   \n3. radial base function (RBF) and shift-invariant kernels;   \n4. kernels on hypercubes. ", "page_idx": 15}, {"type": "text", "text": "In particular, the features from Assumption (IF) also satisfies the weakened version of Assumption (GF). See Remark A.3 for details. ", "page_idx": 15}, {"type": "text", "text": "Before comparing our assumptions with previous literature, we first define the $\\infty$ -norm in the ridge regression setting analog to its kernel counterpart: ", "page_idx": 15}, {"type": "text", "text": "Definition A.6 ( $\\infty$ -norm). Fix a distribution $\\mu$ on $\\mathbb{R}^{p}$ and let $\\mathbf{v}\\in\\mathbb{R}^{p}$ . Define ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\mathbf{v}\\|_{\\infty}\\overset{\\mathrm{def.}}{=}\\displaystyle\\operatorname{ess}_{x\\sim\\mu}|\\mathbf{x}^{\\top}\\mathbf{v}|=\\operatorname*{sup}\\{a\\in\\mathbb{R}|a\\leq|\\mathbf{x}^{\\top}\\mathbf{v}|\\mathrm{~a.s.~for~all~}\\mathbf{x}\\sim\\mu\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Remark A.7. Note that $\\|\\cdot\\|_{\\infty}$ is indeed a semi-norm and for all $\\mathbf{v}\\in\\mathbb{R}^{p}$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\|\\mathbf{v}\\|_{\\Sigma}\\leq\\|\\mathbf{v}\\|_{\\infty}\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Choose $k=0$ and Assumption (GF) and Remark A.1 imply: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\mathbf{x}\\|_{\\infty}^{2}=\\displaystyle\\exp\\|\\mathbf{x}\\|_{2}^{2}=\\displaystyle\\exp\\|\\mathbf{z}\\|_{\\Sigma}^{2}\\leq\\beta_{0}\\displaystyle\\stackrel{\\mathrm{def.}}{=}\\displaystyle\\exp\\operatorname*{max}\\left\\{\\frac{\\|\\mathbf{z}\\|_{\\Sigma}^{2}}{\\mathrm{Tr}[\\Sigma]},\\frac{\\|\\mathbf{z}\\|_{\\Sigma^{2}}^{2}}{\\mathrm{Tr}[\\Sigma^{2}]}\\right\\}<\\infty,}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "as we assume that $\\mathrm{Tr}[\\pmb{\\Sigma}]<\\infty$ . ", "page_idx": 15}, {"type": "text", "text": "Embedding condition [29, 50, 30, 31] used the Embedding Condition to show tight bounds of learning curve with polynomial eigen-decay: ", "page_idx": 15}, {"type": "text", "text": "Assumption (EC) (Embedding Condition). Given a random vector $\\mathbf{x}\\in\\mathbb{R}^{p}$ with covariance $\\pmb{\\Sigma}=$ $\\mathbb{E}[{\\bf x x}^{\\top}]$ , let $\\mathbf{z}=\\Sigma^{-1/2}\\mathbf{x}$ be the isotropic random vector. Define the embedding coefficient: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\eta_{0}=\\operatorname*{inf}_{\\eta}\\{\\exp_{\\mathbf{x}}\\mathrm{sup}\\,\\|\\mathbf{x}\\|_{\\Sigma^{\\eta-1}}^{2}<\\infty\\}=\\operatorname*{inf}_{\\eta}\\{\\exp_{\\mathbf{z}}\\mathrm{sup}\\,\\|\\mathbf{z}\\|_{\\Sigma^{\\eta}}^{2}<\\infty\\}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Suppose the eigen-decay is polynomial: $\\lambda_{k}=\\Theta_{k}\\left(k^{-1-a}\\right)$ for some $a>0$ . Assume the embedding coefficient $\\begin{array}{r}{\\eta_{0}=\\frac{1}{1+a}}\\end{array}$ . ", "page_idx": 15}, {"type": "text", "text": "Remark A.8 (General range of $\\eta_{0}$ ). Rewrite the expression: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\|\\mathbf{x}\\|_{\\Sigma^{\\eta-1}}^{2}=\\mathbf{x}^{\\top}\\Sigma^{\\eta-1}\\mathbf{x}=\\sum_{k=1}^{p}\\lambda_{k}^{\\eta-1}x_{k}^{2}=\\sum_{k=1}^{p}\\lambda_{k}^{\\eta}z_{k}^{2}=\\Theta_{k}\\left(\\sum_{k=1}^{p}k^{-\\eta(1+a)}z_{k}^{2}\\right)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where we write $\\mathbf{x}=(x_{k})_{k=1}^{p}\\in\\mathbb{R}^{p}$ and $\\mathbf{z}=(z_{k})_{k=1}^{p}\\in\\mathbb{R}^{p}$ . In the kernel case, it is obvious that $\\eta_{0}\\leq1$ : for all inputs $x\\in\\mathscr{X}$ with embedding $\\mathbf{x}=K(x,\\cdot)$ in the feature space $\\mathcal{H}$ , $\\operatorname{sup}_{x}k(x,x)=$ ", "page_idx": 15}, {"type": "text", "text": "$\\left\\|\\mathbf{x}\\right\\|_{2}^{2}<\\infty$ . Since the eigenvalues have polynomial decay, we have $\\begin{array}{r}{\\eta_{0}\\geq\\frac{1}{1+a}}\\end{array}$ , or otherwise there were some $\\begin{array}{r}{\\eta\\in(\\eta_{0},\\frac{1}{1+a})}\\end{array}$ such that: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbf{x}}\\left[\\left\\Vert\\mathbf{x}\\right\\Vert_{\\Sigma^{\\eta-1}}^{2}\\right]=\\Omega\\left(\\mathbb{E}_{\\mathbf{z}}\\left[\\sum_{k=1}^{p}k^{-\\eta(1+a)}z_{k}^{2}\\right]\\right)=\\Omega\\left(\\sum_{k=1}^{p}k^{-\\eta(1+a)}\\right)=\\infty,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "contradicting Assumption (EC). Thus one have $\\begin{array}{r}{\\eta_{0}\\in[\\frac{1}{1+a},1]}\\end{array}$ in general. ", "page_idx": 16}, {"type": "text", "text": "Hence Assumption (GF) is a much weaker assumption than Assumption (EC) in the sense that the former one only requires $\\eta_{0}\\leq1$ , which holds in general; while the latter one requires $\\begin{array}{r}{\\eta_{0}=\\frac{1}{1+a}}\\end{array}$ which is the smallest possible size of $\\eta_{0}$ . Hence, many examples mentioned in [50], that satisfy Assumption (EC) also satisfy Assumption (GF). For instance, the examples include: ", "page_idx": 16}, {"type": "text", "text": "1. dot-product kernels on hyperspheres;   \n2. kernels with bounded eigenfunctions;   \n3. shift-invariant periodic kernels. ", "page_idx": 16}, {"type": "text", "text": "Nevertheless, [29, 50, 30, 31] also require H\u00f6lder continuity of the kernel $K$ while Assumption (GF) requires no continuity assumption. ", "page_idx": 16}, {"type": "text", "text": "Maximal degree of freedom We rewrite and simplify the assumption used in [33] with our notation: Assumption (MaxDoF) (Maximal Degree-of-Freedom). For any $\\lambda>0$ and $s\\geq1$ , define: ", "page_idx": 16}, {"type": "equation", "text": "$$\nF^{s}(\\lambda)\\stackrel{\\mathrm{def.}}{=}\\displaystyle\\operatorname*{ess}_{\\mathbf{x}}\\operatorname*{sup}_{\\mathbf{x}}\\left\\|(\\pmb{\\Sigma}+\\lambda\\mathbf{I}_{p})^{-s/2}\\mathbf{x}\\right\\|_{\\pmb{\\Sigma}^{s-1}}^{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Assume: there exists $n\\in\\mathbb N$ such that ", "page_idx": 16}, {"type": "equation", "text": "$$\nn\\ge5F^{s}(\\lambda_{n})\\operatorname*{max}\\{1,\\log(14F^{s}(\\lambda_{n}))/\\delta\\}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "for some constant $\\delta\\in(0,1)$ . ", "page_idx": 16}, {"type": "text", "text": "Usually, the constant $s$ is chosen as the source coefficient in Assumption (SC). Again, Assumption (MaxDoF) requires some extra boundedness condition on the norm of $\\mathbf{x}$ . Examples that satisfy Assumption (MaxDoF) include: ", "page_idx": 16}, {"type": "text", "text": "1. dot-product kernels on hyperspheres;   \n2. kernels with bounded eigenfunctions;   \n3. periodic kernels on hypercubes, ", "page_idx": 16}, {"type": "text", "text": "which also satify Assumption (GF). ", "page_idx": 16}, {"type": "text", "text": "Low/High-degree feature concentration [36] proved the convergence of the test error to its \u201cdeterministic equivalent\u201d using a list of assumptions for each fixed sample size $n$ . To avoid introducing extra notations, we list their assumption here in a semi-rigorous way: ", "page_idx": 16}, {"type": "text", "text": "Assumption $(\\mathbf{LH})$ (Low/High-degree feature concentration). Fix $n\\in\\mathbb N$ and $\\lambda\\geq0$ . Suppose there exists an integer $k\\leq p$ such that: ", "page_idx": 16}, {"type": "equation", "text": "$\\begin{array}{r}{l.~~r_{k}^{\\lambda~\\underline{{\\mathrm{def.}}}}~{\\frac{\\lambda+\\mathrm{Tr}\\left[\\boldsymbol{\\Sigma}_{>k}\\right]}{\\lambda_{k+1}}}\\geq2n,}\\end{array}$ ", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "2. the low-degree feature vector $\\mathbf{x}{\\le}k\\in\\mathbb{R}^{k}$ is highly concentrated; ", "page_idx": 16}, {"type": "text", "text": "3. with high probability, the high-degree feature vector $\\mathbf{x}_{>k}\\in\\mathbb{R}^{p-k}$ satisfies: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\big\\|\\mathbf{X}_{>k}\\mathbf{X}_{>k}^{\\top}-\\mathrm{Tr}[\\pmb{\\Sigma}_{>k}]\\mathbf{I}_{n}\\big\\|_{o p}\\leq C\\sqrt{\\frac{n}{r_{k}^{\\lambda}}}\\cdot(\\lambda+\\mathrm{Tr}[\\pmb{\\Sigma}_{>k}])\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $C>1$ is a constant depending on $n,k$ ", "page_idx": 16}, {"type": "text", "text": "Point 3 in Assumption (LH) is closely related to Assumption (GF) since: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\sum_{i=1}^{n}\\|\\mathbf{z}_{>k}\\|_{\\mathbf{Z}_{>k}}^{2}-\\mathrm{Tr}[\\mathbf{Z}_{>k}]=\\frac{1}{n}\\,\\mathrm{Tr}[\\mathbf{X}_{>k}\\mathbf{X}_{>k}^{\\top}-\\mathrm{Tr}[\\mathbf{Z}_{>k}]\\mathbf{I}_{n}]\\leq\\left\\|\\mathrm{Tr}[\\mathbf{X}_{>k}\\mathbf{X}_{>k}^{\\top}-\\mathrm{Tr}[\\mathbf{Z}_{>k}]\\mathbf{I}_{n}]\\right\\|_{\\mathrm{op}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{sup}_{\\mathbf{x}}\\frac{\\Vert\\mathbf{z}_{>k}\\Vert_{\\Sigma_{>k}}^{2}}{\\mathrm{Tr}[\\Sigma_{>k}]}=\\Theta_{k}\\left(1\\right)\\mathrm{,then~}\\frac{1}{n}\\sum_{i=1}^{n}\\Vert\\mathbf{z}_{>k}\\Vert_{\\Sigma_{>k}}^{2}-\\mathrm{Tr}[\\Sigma_{>k}]=\\Omega_{k}\\left(1\\right)\\!.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "According to [36], examples that satisfy Assumption (LH) include: ", "page_idx": 17}, {"type": "text", "text": "1. kernel satisfying Assumption (IF);   \n2. dot-product kernel on hyperspheres. ", "page_idx": 17}, {"type": "text", "text": "Moment-equivalent assumption [19] use a geometric argument with Dvoretzky-Milman Theorem to bound the test error from above under the assumptions: ", "page_idx": 17}, {"type": "text", "text": "Assumption (ME) ( $L^{2+\\epsilon}{-}L^{2}$ Moment-Equivalent Assumption). Fix a sample $(\\mathbf{x}_{i})_{i=1}^{n}$ . Suppose there exists some $\\epsilon,\\kappa>0$ such that: ", "page_idx": 17}, {"type": "text", "text": "1. for any $k\\,\\in\\,\\mathbb{N}$ and any function $f\\,\\in\\,{\\mathcal{H}}_{>k}$ in the high-degree feature space, we have $\\|f\\|_{L^{2+\\epsilon}}\\leq\\kappa\\,\\|f\\|_{L^{2}}$ . (a) $i f\\epsilon>2,$ , then no extra assumption is required; (b) if $\\epsilon\\in(0,2]$ , then $\\begin{array}{r}{\\kappa n^{\\frac{2-\\epsilon}{2\\epsilon+\\epsilon^{2}}}\\log n\\left(\\frac{\\sqrt{n\\,\\mathrm{Tr}[\\mathbf{\\Sigma}_{>\\,k}^{2}]}}{\\mathrm{Tr}[\\mathbf{\\Sigma}_{>\\,k}]}\\right)}\\end{array}$ is small.   \n2. with high probability, maxi=1,...,n $\\begin{array}{r}{\\operatorname*{max}_{i=1,\\dots,n}\\frac{\\Vert(\\mathbf{z}_{i})_{>k}\\Vert_{\\Sigma_{>k}}^{2}}{\\mathrm{Tr}[\\Sigma_{>k}]}\\approx1,}\\end{array}$   \n3. with high probability, maxi=1,...,n $\\begin{array}{r}{\\operatorname*{max}_{i=1,\\dots,n}\\frac{\\left\\|\\left(\\mathbf{z}_{i}\\right)_{>k}\\right\\|_{\\mathbf{\\Sigma}_{>k}^{2}}^{2}}{\\mathrm{Tr}\\left[\\mathbf{\\Sigma}_{>k}^{2}\\right]}\\approx1;}\\end{array}$   \n4. with high probability, maxi=1,...,n $\\begin{array}{r}{\\operatorname*{max}_{i=1,\\dots,n}\\frac{\\left\\|\\left(\\mathbf{z}_{i}\\right)\\leq k\\right\\|_{2}^{2}}{k}=\\mathcal{O}_{k}\\left(1\\right)\\!.}\\end{array}$ ", "page_idx": 17}, {"type": "text", "text": "While points 2-4 are implied by the weakened version of Assumption (GF), point 1 is a strong geometric assumption on the RKHS $\\mathcal{H}$ . According to [19], the addition of point 1 can drop the logarithmic terms in the upper bound of test error under weak ridge, and thus strengthening some results in Table 1 from $\\tilde{\\Theta}\\left(\\cdot\\right)$ to $\\Theta\\left(\\cdot\\right)$ . Examples satisfying Assumption (ME) includes: ", "page_idx": 17}, {"type": "text", "text": "1. rotational invariant kernel. ", "page_idx": 17}, {"type": "text", "text": "A.3 Between independent and dependent features: random features ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "A random feature setting can be considered as a midpoint between independent and dependent features: Let $\\mathbf{W}\\in\\mathbb{R}^{p\\times\\bar{d}}$ be a random matrix, typically with i.i.d. random entries, and $\\mathbf{z}\\in\\mathbb{R}^{d}$ be the input vector, usually comprising independent entries. We define the random feature $\\mathbf{x}\\in\\mathbb{R}^{p}$ as a one-hidden-layer neural network: $\\mathbf{x}\\ {\\overset{\\mathrm{def.}}{=}}\\ \\varphi(\\mathbf{W}\\mathbf{z})$ , where $\\varphi:\\mathbb{R}\\rightarrow\\mathbb{R}$ represents an activation function acting entry-wise on $\\mathbf{W}\\mathbf{z}$ . Thus, the coordinates of the pre-activation are independent, while those of the post-activation are not. ", "page_idx": 17}, {"type": "text", "text": "However, it is worth noting the Gaussian Equivalent Property highlighted in [20], which asserts that in wide neural networks, the distribution of the feature vector $\\mathbf{x}$ is approximately Gaussian. Consequently, it reduces to the case of independent features previously discussed. ", "page_idx": 17}, {"type": "text", "text": "A.4 Interpolation space ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Many KRR literature [29, 30, 31, 33] introduced the notation of interpolation space and the source condition on the $L^{2}$ -integrable target function $f^{*}$ . This can be easily translated to the notion of generalized vector norm. ", "page_idx": 17}, {"type": "text", "text": "Definition A.9 (Interpolation space). Let $s\\geq0$ be a real number. Define the interpolation space ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{H}^{s}\\ensuremath{\\stackrel{\\mathrm{def.}}{=}}\\{\\pmb{\\theta}\\in\\mathbb{R}^{p}:\\|\\pmb{\\theta}\\|_{\\Sigma^{1-s}}<\\infty\\}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Evidently, if $s_{1}\\leq s_{2}$ , then $\\|\\pmb\\theta\\|_{\\Sigma^{1-s_{2}}}\\leq\\|\\pmb\\theta\\|_{\\Sigma^{1-s_{1}}}$ and hence $\\mathcal{H}_{s_{1}}\\supset\\mathcal{H}_{s_{2}}$ . The intuition is that, in the KRR setting, the space $\\mathcal{H}^{s}$ interpolates between the $L^{2}$ -space $L_{\\mu}^{2}(\\mathcal{X})\\supset\\mathcal{H}^{0}$ and the RKHS $\\mathcal{H}=\\mathcal{H}^{1}$ . ", "page_idx": 18}, {"type": "text", "text": "A.5 Source condition ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Given the asymptotic of the eigen-decay $\\lambda_{k}$ and the target coefficient $\\pmb{\\theta}^{*}$ , we can find the largest possible source coefficient $s$ . For instance, if $\\lambda_{k}=\\Theta_{k}\\left(k^{-\\bar{1}-a}\\right)$ for some $a>0$ and $|\\theta_{k}^{*}|=\\Theta_{k}\\left(k^{\\bar{-}r}\\right)$ , then we can choose $s$ to be $\\textstyle{\\frac{2r+a}{1+a}}$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\theta^{*}\\in\\mathcal{H}^{\\ell}\\Leftrightarrow||\\theta^{*}||_{\\frac{\\ell}{3}\\ell^{1-\\ell}}^{2}<\\infty}\\\\ {\\Leftrightarrow\\displaystyle\\sum_{k=1}^{p}(\\theta^{*})^{2}\\lambda_{k}^{1-\\ell}<\\infty}\\\\ {\\Leftrightarrow\\displaystyle\\sum_{k=1}^{p}k^{-2r}k^{(1-t)(-1-a)}<\\infty}\\\\ {\\Leftrightarrow\\displaystyle\\sum_{k=1}^{p}k^{-1-a+t+t a-2r}<\\infty}\\\\ {\\Leftrightarrow-1-a+t+t a-2r<-1}\\\\ {\\Leftrightarrow t<\\frac{2r+a}{1+a}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Suppose $\\lambda_{k}=\\Theta_{k}\\left(e^{-a k}\\right),|\\theta_{k}^{*}|=\\Theta_{k}\\left(e^{-r k}\\right)$ for $a,r>0$ . Then we can choose $s$ to be $\\textstyle{\\frac{2r}{a}}+1$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\theta^{*}\\in\\mathcal{H}^{t}\\Leftrightarrow\\|\\theta^{*}\\|_{\\Sigma^{1-t}}^{2}<\\infty}\\\\ {\\Leftrightarrow\\displaystyle\\sum_{k=1}^{p}(\\theta^{*})^{2}\\lambda_{k}^{1-t}<\\infty}\\\\ {\\Leftrightarrow\\displaystyle\\sum_{k=1}^{p}e^{-2r k}(e^{-a k})^{1-t}<\\infty}\\\\ {\\Leftrightarrow\\displaystyle\\sum_{k=1}^{p}e^{-k(2r+a-a)}<\\infty}\\\\ {\\Leftrightarrow2r+a-a t>0}\\\\ {\\Leftrightarrow t<\\frac{2r}{a}+1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "A.6 Kernel ridge regression ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "KRR can be regarded as ridge regression on the feature space, where the positive definite symmetric (PDS) kernel $K$ sends each input $x\\in\\mathscr{X}$ to a vector/function $K(x,\\cdot)$ in the corresponding reproducing kernel Hilbert space (RKHS) $\\mathcal{H}$ , and the kernel regressor is simply a linear regressor in the RKHS $\\mathcal{H}$ . Here we briefly explain how to translate the notations above in the kernel ridge regression (KRR) setting. ", "page_idx": 18}, {"type": "text", "text": "With abuse of notation, let $\\mu$ be a (data-generating) distribution on an input space $\\mathcal{X}$ . Let $K:$ $\\mathcal{X}\\times\\mathcal{X}\\rightarrow\\mathbb{R}$ be a positive definite symmetric kernel with corresponding reproducing kernel Hilbert space (RKHS) $\\mathcal{H}$ . By Mercer decomposition, we have for all $x,\\mathbf{\\dot{x}}^{\\prime}\\in\\mathrm{supp}(\\mu)\\subset\\mathcal{X}$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\nK(x,x^{\\prime})=\\sum_{k=1}^{p}\\lambda_{k}\\psi_{k}(x)\\psi_{k}(x^{\\prime}),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\lambda_{k}$ \u2019s are the eigenvalues of $K$ indexed in decreasing order and $\\psi_{k}:\\mathcal{X}\\rightarrow\\mathbb{R}$ \u2019s are eigenfunctions of $K$ forming an orthonormal basis on $L_{\\mu}^{2}(\\mathcal{X}){\\mathrm{:}}\\;\\mathbb{E}_{x\\sim\\mu}\\left[\\bar{\\psi_{k}}(x)\\psi_{l}(x)\\right]=\\delta_{k l}$ for all $k,l\\in\\mathbb{N}$ . Hence for ", "page_idx": 18}, {"type": "text", "text": "each $x\\in\\operatorname{supp}(\\mu)\\subset\\mathcal{X}$ , $K(x,\\cdot)$ is a feature vector in the RKHS $\\mathcal{H}$ , which has an orthonormal basis $\\begin{array}{r}{\\left\\{\\phi_{k}=\\frac{1}{\\sqrt{\\lambda_{k}}}\\psi_{k}\\right\\}_{k=1}^{p}}\\end{array}$ ", "page_idx": 19}, {"type": "text", "text": "Let $y=f^{*}(x)+\\epsilon$ be the labels where $f^{*}\\in L_{\\mu}^{2}(\\mathcal{X})$ and $\\epsilon$ is a centered random variable with finite variance independent to $x$ . Let $(x_{i},y_{i})_{i=1}^{n}$ be a set of i.i.d. drawn input-output pairs. The kernel ridge regression is in the form: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\hat{f}\\ {\\stackrel{\\mathrm{def.}}{=}}\\ \\operatorname*{min}_{f\\in\\mathcal{H}}\\frac{1}{n}\\sum_{i=1}^{n}(f(x_{i})-y_{i})^{2}+\\lambda\\left\\|f\\right\\|_{\\mathcal{H}}^{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which minimum admits the form: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\hat{f}(x)=\\mathbf{K}_{x}^{\\top}(\\mathbf{K}+n\\lambda\\mathbf{I}_{n})^{-1}\\mathbf{y},\\,\\forall x\\in\\mathcal{X},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $K_{x}=(K(x,x_{i}))_{i=1}^{n}\\in\\mathbb{R}^{n}$ , $\\mathbf{K}=[K(x_{i},x_{j})]_{i,j=1}^{n}\\in\\mathbb{R}^{n\\times n}$ and $\\mathbf{y}=(y_{i})_{i=1}^{n}$ . Note that if we write $\\psi_{x}=(\\psi_{k}(x))_{k=1}^{p}\\in\\mathbb{R}^{p}$ , $\\Psi=[\\psi_{k}(x_{i})]_{i k}\\,\\in\\,\\mathbb{R}^{n\\times p}$ and $\\mathbf{\\deltaA}=\\mathrm{diag}\\{\\lambda_{k}\\}_{k=1}^{p}\\in\\mathbb{R}^{p\\times p}$ , by Mercer decomposition, we have $K_{x}=\\Psi\\Lambda\\psi_{x}$ , $\\mathbf{K}=\\Psi\\Lambda\\Psi^{\\top}$ . We are ready to translate our linear ridge regression to the KRR setting. See Table 5 for details. ", "page_idx": 19}, {"type": "text", "text": "B Smoothness and spectral decay rate ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we will explain the reason to assume the eigen-decay to be either polynomial or exponential in the paper. ", "page_idx": 20}, {"type": "text", "text": "Concretely, the \"polynomial decay regime\" arises as the Neural Tangent Kernel (NTK) limit of MultiLayer Perceptrons (MLPs) with an activation function of the form $\\sigma_{s}(t)\\stackrel{\\mathrm{def.}}{=}\\operatorname*{max}{0},t^{s}$ , where $s\\geq1$ , as studied in [4]. In cases where $s$ is an integer, $\\sigma_{s}$ represents the $(s-1)$ -th iterated anti-derivative of the ReLU function, making it exactly $s$ times weakly differentiable. ", "page_idx": 20}, {"type": "text", "text": "Suppose $\\mathcal{X}$ is the $d$ -dimensional sphere $\\mathbb{S}^{d}\\stackrel{\\mathrm{def.}}{=}\\{u\\in\\mathbb{R}^{d}:|u|=1\\}$ (with its usual Riemannian metric). In this scenario, every neural network function $f:\\mathbb{S}^{d}\\to\\mathbb{R}$ belongs to the Sobolev space $H^{s}(\\mathbb{S}^{d})$ (see Proposition B.7 in Appendix B.2), characterized by having $s$ continuous and integrable (weak) partial derivatives on $\\mathbb{S}^{d}$ . Additionally, the NTK limit manifests as a \"radially symmetric kernel\", specifically of the form $k(x,y)=\\kappa(x^{\\dagger}y)$ for some suitable function $\\kappa:[-1,1]\\to\\mathbb{R}$ . ", "page_idx": 20}, {"type": "text", "text": "It is worth noting that the choice of the smoothness level, and hence the decay rate, naturally emerges when approximately solving Partial Differential Equations (PDEs), as demonstrated in works such as [26, 15]. In these scenarios, it is desired that the kernel regressor exhibits the correct level of smoothness matching that of the theoretical solution to the PDE, with its higher-order derivatives also converging. It is emphasized that the solution to several PDEs only possesses finitely many derivatives, as seen in (viscosity) solutions to PDEs arising in stochastic control [37, 38]. ", "page_idx": 20}, {"type": "text", "text": "In the remainder of this section, we justify the use of Assumptions (PE) and (EE) by establishing a connection between the spectral eigen-decay and the smoothness of the Reproducing Kernel Hilbert Space (RKHS), which is independent of the rest of the paper. Readers primarily interested in the proofs presented in Table 1 may opt to skip this section and proceed to Section C during their initial reading. ", "page_idx": 20}, {"type": "text", "text": "B.1 Notations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Fix a positive integer $d\\in\\mathbb{N}$ and let $\\mathcal{X}$ be a non-empty subset of $\\mathbb{R}^{d}$ . We equip $\\mathcal{X}$ with a regular Borel probability measure $\\mu$ , which we specify shortly. We denote the space of (equivalence classes of) $\\mu$ -square-integrable on $\\mathbb{R}^{d}$ by $L_{\\mu}^{2}(\\vec{\\mathbb{R}^{d}})$ , equipped with its usual $L_{\\mu}^{2}$ -norm $\\|\\cdot\\|_{L_{\\mu}^{2}}$ . ", "page_idx": 20}, {"type": "text", "text": "Consider a kernel $\\kappa:\\mathcal{X}\\times\\mathcal{X}\\to\\mathbb{R}$ with its associated to the RKHS $(\\mathcal{H},\\langle\\cdot,\\cdot\\rangle_{\\kappa})$ (abbreviated by $\\mathcal{H}_{\\;}$ ). We denote the induced norm on $\\mathcal{H}$ by $\\|\\cdot\\|_{\\kappa}$ . By the Mercer decomposition theorem, see [40, Theorem 2.30], there is some $M\\in\\mathbb{N}\\cup\\{\\infty\\}$ such that we may write ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\kappa(x,y)=\\sum_{i=0}^{M}\\,\\lambda_{i}\\,\\psi_{i}(x)\\psi_{i}(y)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $(\\psi_{i})_{i=0}^{M}$ are normalized eigenfunctions of the linear operator $\\begin{array}{r l}{T_{\\kappa}(f)(x)}&{{}=}\\end{array}$ $\\textstyle\\int_{x\\in{\\mathcal{X}}}\\kappa(x,u)f(u)\\,d u$ on $L_{\\mu}^{2}(\\mathcal{X})$ and each eigenvalue $(\\lambda_{i})_{i=0}^{\\infty}$ is non-negative; which, form an orthonormal basis of $\\mathcal{H}$ . ", "page_idx": 20}, {"type": "text", "text": "We motivate our analysis, by the following connection between the spectral decay of dot-product kernels on spheres and the smoothness of functions in their RKHS. ", "page_idx": 20}, {"type": "text", "text": "Example B.1 (Radial Kernels and Sobolev Spaces on Spheres). In the setting of Examples B.3 and (B.5), suppose that there is a function $k:[-1,1]\\to\\mathbb{R}$ such that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\kappa(x,y)\\ {\\stackrel{\\mathrm{def.}}{=}}\\ k(\\langle x,y\\rangle)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "for each $x,y\\in\\mathbb{S}^{d}$ . As shown in [41], $\\kappa$ is a kernel if and only if there is a summable sequence of non-negative real numbers $(\\alpha_{i})_{i=0}^{\\infty}$ satisfying $\\begin{array}{r}{k(t)=\\sum_{i=0}^{\\infty}\\,\\alpha_{i}\\,\\dot{t}^{i}}\\end{array}$ for each $t\\in[-1,1]$ . Kernels of the form (9) are called radial. The RKHS $\\mathcal{H}$ associated to $\\kappa$ consists of all functions $f:\\mathbb{S}^{d}\\to\\mathbb{R}$ with ", "page_idx": 20}, {"type": "equation", "text": "$$\nf=\\sum_{j=0}^{\\infty}\\,\\sqrt{\\mu_{j}}\\,\\sum_{i=1}^{I_{j}}\\,\\alpha_{j,i}\\,\\bar{\\psi}_{j,i}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "for a square-summable real sequence $(\\alpha_{j,i})_{j,i=1}^{\\infty,J_{i}}$ ; i.e. $\\begin{array}{r}{\\sum_{i=0}^{\\infty}\\sum_{j=1}^{J_{i}}\\alpha_{j,i}^{2}\\,<\\,\\infty}\\end{array}$ (where $(\\alpha_{i,j})_{i,j=1}^{\\infty,J_{i}}$ depends only on the radial function $k$ , and thus only on the radial kernel $\\kappa$ ; and $(\\bar{\\psi}_{j,i})_{j,i=1}^{\\infty,J_{i}}$ are the ", "page_idx": 20}, {"type": "text", "text": "spherical harmonics described in Example B.3. As discussed in [24, Chapter 4], for any $k\\,>\\,0$ $u\\in\\mathcal H$ belongs to the Sobolev space $H_{0}^{k}(\\mathbb{S}^{d})$ if and only $i f$ : for each $f\\in\\mathcal H$ the coefficients $(\\mu_{j})_{j=0}^{\\infty}$ in the representation (10) satisfy ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mu_{j}\\in\\Theta((j+1)^{-2s}).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "As discussed in [22, Lemma B.1] in this case the norms of $\\mathcal{H}$ and on $H^{s}(\\mathbb{S}^{d})$ are equivalent9. ", "page_idx": 21}, {"type": "text", "text": "B.2 Preliminaries: regular domains and Sobolev spaces ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We consider the case where $\\mathcal{X}$ is a sufficiently regular open subset of the $d$ -dimensional Euclidean space so that we may comfortably describe the smoothness of functions on $\\mathcal{X}$ using classical notions of tools. We therefore assume the following. ", "page_idx": 21}, {"type": "text", "text": "Assumption (DR) (Domain Regularity). Either one of the following holds: ", "page_idx": 21}, {"type": "text", "text": "(i) Compact manifold: $(\\mathcal{X},g)$ is a $d$ -dimensional compact Riemannian manifold, with $d\\geq2$ , and $\\bar{\\mu}=V_{g}(\\bar{\\boldsymbol{x}})^{-1}\\,V_{g}$ is the uniform law on $(\\mathcal{X},g)$ and $V_{g}$ is its volume measure,   \n(ii) Bounded Euclidean domain: $\\mathcal{X}$ is a non-empty bounded open domain in $\\mathbb{R}^{d}$ with Lipschitz boundary and $\\mu$ is the normalized Lebesgue measure. ", "page_idx": 21}, {"type": "text", "text": "In cases (i)-(ii), $\\mathcal{X}$ can be viewed as a Riemannian manifold (possibly with boundary). ", "page_idx": 21}, {"type": "text", "text": "In what follows, we let $\\Delta$ denote either of the following Laplacian, depending on which of assumption (DR) we have assumed. If (i) or (ii) holds then $\\Delta$ will denote the usual Laplacian-Beltrami operator on X, note that when (iii) holds then \u2206=  id=1 \u2202\u2202x2i2 . We denote the eigenfunctions of $\\Delta$ by $(\\bar{\\psi}_{i})_{i\\in\\mathbb{N}}$ whose respective eigenvalues $\\lambda_{1}\\leq\\lambda_{2}\\leq...$ are arranged in a non-decreasing order. If (iii) holds, then $\\Delta$ will denote the Dirichlet Laplacian (see [11, Section 6.1.2]) on $\\mathcal{X}$ (with its usual Euclidean Riemannian metric) which acts on the spaces of functions vanishing on the boundary $\\partial\\mathcal{X}$ of $\\mathcal{X}$ ; namely on the homogeneous Sobolev space $\\left\\lceil H_{0}^{1}(\\mathcal{X})\\right\\rceil$ . ", "page_idx": 21}, {"type": "text", "text": "Example B.2. Under assumption $(i)_{;}$ , the eigenfunctions of $\\Delta$ are the Hermite polynomials on $L_{\\mu}^{2}(\\mathbb R^{d})$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\bar{\\psi}_{i}(x)=(-1)^{i}\\,e^{x^{2}/2}\\,\\frac{\\partial^{i}}{\\partial x^{i}}(e^{-x^{2}/2})\\qquad\\qquad f o r\\,e a c h\\,i\\in\\mathbb{N}_{+}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For concreteness, the size, in $L_{\\mu}^{1}$ -norm, of the Hermite polynomials is computed in [28, Theorem 2.1] where it it shown that $\\begin{array}{r}{\\|\\bar{\\psi}_{i}\\|_{L_{\\mu}^{1}}\\lesssim\\frac{(i!)^{1/2}}{i^{1}/2}(1+\\mathcal{O}(1/i))}\\end{array}$ . ", "page_idx": 21}, {"type": "text", "text": "Example B.3. Suppose that $\\mathcal{X}$ is the $d$ -dimensional sphere $\\mathbb{S}^{d}\\ensuremath{\\stackrel{\\mathrm{def.}}{=}}\\;\\{\\ensuremath{\\boldsymbol{x}}\\;\\in\\;\\mathbb{R}^{d+1}\\;:\\;\\|\\ensuremath{\\boldsymbol{x}}\\|_{2}\\;=\\;1\\}$ equipped with the (usual) Riemannian metric induced by inclusion into the $d+1$ -dimensional Euclidean space $\\mathbb{R}^{d+1}$ . Let $\\mu$ be the normalized (uniform) Riemannian (volume) measure on $\\mathbb{R}^{d}$ Then, $(\\bar{\\psi}_{i,j}(x))_{i=1,j=1}^{\\infty,J_{i}}$ are an orthogonrmal basis of $L_{\\mu}^{2}$ consisting of spherical Harmonics (counting multiplicities $J_{i}\\ \\in\\ \\mathbb{N}_{+}$ for each eigenvalue $i\\;\\in\\;\\mathbb{N}_{+}^{\\;\\cdot})$ ; that is the restriction of a homogeneous polynomial $f:\\mathbb{R}^{d+1}\\stackrel{}{\\rightarrow}\\mathbb{R}$ satisfying $\\Delta f\\,=\\,0$ to there sphere $\\mathbb{S}^{d}$ . Spherical harmonics are not very large (in uniform norm) $I I7_{s}$ , Theorem 1.6] and one can show that $\\|\\bar{\\psi}_{i,j}\\|\\lesssim\\lambda_{i}^{(d-1)/4}$ , for each $i\\in\\mathbb{N}_{+}$ . See $l2$ , Chapter 2] for further details on spherical harmonics. ", "page_idx": 21}, {"type": "text", "text": "B.2.1 Sobolev spaces on Riemannian manifolds ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Let $(\\mathcal{X},g)$ be a compact (smooth) Riemannian manifold, possibly with a boundary $\\partial\\mathcal{X}$ . For any $k\\in\\mathbb{N}_{+}$ , we define $\\bar{\\mathcal{C}}_{k}^{2}(M)$ be the set of smooth functions $f$ on $\\mathcal{X}$ satisfying ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\int_{M}|\\nabla^{i}f|_{g}^{2}\\,d\\mu<\\infty\\qquad\\mathrm{and}\\ \\underbrace{f(x)=0\\ (\\forall x\\in\\partial X)}_{\\mathrm{rrossionsonsing}\\qquad\\mathrm{rossizions}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\mu$ is the Riemannian volume measure on $(\\mathcal{X},g)$ , $\\nabla$ is the covariant derivative thereon. Then, the Sobolev space $H_{0}^{k}({\\mathcal{X}},g)$ is the Hilbert space obtained as the completion of $\\mathcal{C}_{k}^{p}(\\mathcal{X})$ with respect ", "page_idx": 21}, {"type": "text", "text": "to the norm ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|f\\|_{H^{k}(\\mathcal{X})}\\overset{\\mathrm{def.}}{=}\\sum_{i=1}^{k}\\,\\|\\nabla^{i}f\\|_{L_{\\mu}^{2}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Remark B.4 (The Sobolev Spaces $H_{0}^{k}({\\mathcal{X}},g)$ and $H^{k}({\\mathcal{X}},g))$ . Observe that when $\\mathcal{X}$ is a compact Riemannian manifold without boundary, i.e. when $\\partial\\mathcal{X}=\\emptyset$ ; then the condition $f(x)=0$ for all $x$ in the boundary set $\\partial\\mathcal{X}$ holds vacuously. In this case, this condition can be ignored, and the spaces $H_{0}^{k}({\\mathcal{X}},g)$ coincide with the usual Sobolev spaces $H^{k}({\\mathcal{X}},g)$ for Riemannian manifolds without boundary; where $H^{k}({\\mathcal{X}},g)$ is defined much the same way as $H_{0}^{k}({\\mathcal{X}},g)$ but without requiring the homogeneous boundary condition in (11). ", "page_idx": 22}, {"type": "text", "text": "A more convenient expression for the norm of Sobolev spaces on spheres can be arrived at by manipulating the definition of the Laplacian. We summarize this norm in the next example. ", "page_idx": 22}, {"type": "text", "text": "Example B.5 (Sobolev spaces on spheres). Fix $k\\,\\in\\,\\mathbb{N}_{+}$ . Following the discussion on $l2$ , page 120-121], the $f\\in H_{0}^{k}({\\mathbb S}^{d},g)$ if and only if the following norm ", "page_idx": 22}, {"type": "image", "img_path": "IMlDpZmLnL/tmp/82276738eb9e5ef5bd028c173edf6044b82bb6da381d2aabec30f1f038510454.jpg", "img_caption": [], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "is finite; where $c_{d}\\stackrel{\\mathrm{def.}}{=}(d-2)/2$ . That is, the norms $\\|\\cdot\\|_{k}$ and $\\|\\cdot\\|_{H^{k}(\\mathcal{X})}$ are equivalent. ", "page_idx": 22}, {"type": "text", "text": "The key takeaway of Example B.5, is that the smoothness of a function on there sphere $(\\mathbb{S}^{d},g)$ , i.e. the largest $k$ for which $f\\in H_{0}^{k}(\\mathbb{S}^{d},g)$ , can be expressed in terms of the decay rate of its projection onto the basis of spherical harmonics. Indeed, this expression (12) implies that $f\\in H_{0}^{k}(\\mathbb{S}^{d},g)$ only if $\\begin{array}{r}{\\left|\\int_{x\\in\\mathbb{S}^{d}}f(x)\\Bar{\\psi}_{i,j}(x)\\,\\mu(d x)\\right|^{2}\\in\\mathcal{O}\\big((j+c_{d})^{-2k-\\epsilon}\\big)}\\end{array}$ for all $\\varepsilon>0$ . ", "page_idx": 22}, {"type": "text", "text": "This observation was used in [24, Chapter 4] and circa [22, Lemma B.1] to characterize the smoothness of functions in the RKHS associated to certain radial kernels, e.g. certain NTK limits. We now generalize this argument, and use it to relate the spectral decay rate of a kernel to the smoothnes/regularity of the functions in its associated RKHS. ", "page_idx": 22}, {"type": "text", "text": "B.3 Standardization operator ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We require that the $L_{\\mu}^{2}(\\mathcal{X})$ norm of the eigenfunctions are non-vanishing and non-exploding. ", "page_idx": 22}, {"type": "text", "text": "Assumption (ND) (Non-degenerate $L^{2}$ -Norm). Suppose that: ", "page_idx": 22}, {"type": "text", "text": "(ii) Non-exploding: $\\operatorname*{sup}_{i\\in[M]}\\;\\|\\bar{\\psi}_{i}\\|_{L_{\\mu}^{2}(\\mathcal{X})}<\\infty.$ . ", "page_idx": 22}, {"type": "text", "text": "Again, $i f(\\bar{\\psi_{i}})_{i=0}^{\\infty}$ is orthonormal then $(i)$ and $(i i)$ necessarily hold and this is a non-assumption. ", "page_idx": 22}, {"type": "text", "text": "To generalize the discussion in Example B.1, we first define the linear standardization operator ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{A:\\mathcal{H}\\to L_{\\mu}^{2}(\\mathcal{X})}}\\\\ {{f=\\displaystyle\\sum_{i=0}^{M}\\langle f,\\psi_{i}\\rangle_{\\kappa}\\psi_{i}\\mapsto\\displaystyle\\sum_{i=0}^{M}\\,\\frac{\\langle f,\\psi_{i}\\rangle_{\\kappa}\\,\\|\\psi_{i}\\|_{\\kappa}}{\\|\\bar{\\psi}_{i}\\|_{L_{\\mu}^{2}(\\mathcal{X})}}\\,\\bar{\\psi}_{i}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Example B.6 (Standardization Is Inclusion for Radial Kernels on Spheres). In the setting of Example B.1, we have that (up to reordering) $\\psi_{i,j}=\\bar{\\psi}_{i,j}$ . Since, in that case, for $k$ large enough on has $\\mathcal{H}=H_{0}^{k}(\\mathbb{S}^{d},g)$ then $A$ is simply the inclusion operator of $H_{0}^{k}(\\mathbb{S}^{d},g)$ into $L_{\\mu}^{2}(\\mathbb S^{d})$ . ", "page_idx": 22}, {"type": "text", "text": "In what follows, when we assume Assumption (DR) (iii), we will use $C^{\\infty}({\\overline{{\\mathcal{X}}}})$ to denote the set of smooth functions on the closure $\\overline{{\\mathcal X}}$ of $\\mathcal{X}$ in the norm topology on $\\mathbb{R}^{d}$ . Under Assumption (DR) (i)-(ii), we use $C^{\\infty}({\\overline{{\\mathcal{X}}}})$ to denote $C^{\\infty}({\\mathcal{X}})$ ; that is, the set of smooth functions on the smooth manifold $\\mathcal{X}$ . ", "page_idx": 22}, {"type": "text", "text": "Proposition B.7 (Identification: Spectral Decay and Standardized Regularity). Suppose that Assumption $(D R)$ holds and that $(\\psi_{i})_{i=0}^{M}$ are orthonormal in $\\mathcal{H}$ . ", "page_idx": 23}, {"type": "text", "text": "Then, the linear operator $A$ , defined in (13), is an isometric embedding when $M\\leq\\infty$ (resp. isometric isomorphism when $M=\\infty$ ). Moreover, it characterizes the \u201cregularity\u201d of functions in $\\mathcal{H}$ via: ", "page_idx": 23}, {"type": "text", "text": "1. Sub-Exponential Decay: If $\\kappa$ has infinite-rank ( $M=\\infty$ ) and there exists an $r>0$ such that $|\\langle f,\\psi_{i}\\rangle_{\\kappa}|\\lesssim i^{-r}$ and $\\operatorname*{inf}_{z>0}\\,\\operatorname*{lim}_{i\\to\\infty}\\,|\\langle f,\\psi_{i}\\rangle_{\\kappa}|\\,i^{-r+\\varepsilon}=\\infty$ then ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\underbrace{A(f)\\in H_{0}^{\\frac{(1+r)d}{2}}(\\mathcal{X})\\setminus\\bigcap_{\\varepsilon>0}H_{0}^{\\frac{(1+r)d}{2}+\\varepsilon}(\\mathcal{X})}_{S o b o l e\\;S p a c e\\;C h a r a c t e r i z a t i o n}\\;\\underbrace{a n d\\;\\;A(f)\\in\\;\\bigcap_{r=d/2>k}C^{k}(\\mathcal{X})}_{C^{k}-S p a c e\\;D e s c r i p t i o n}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "2. Exponential Decay: If $\\kappa$ has infinite-rank $M=\\infty.$ ) and there exists some $r>0$ such that $|\\langle\\bar{f},\\psi_{i}\\rangle_{\\kappa}|\\lesssim e^{-r\\,i}$ , then ", "page_idx": 23}, {"type": "equation", "text": "$$\nA(f)\\in C^{\\infty}({\\overline{{\\mathcal{X}}}}).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "3. Finite-Rank: ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "(ii) If Assumption (DR) (ii) holds then: $\\kappa$ has finite rank $'M<\\infty,$ ) then $A(f)$ is smooth. (iii) If Assumption (DR) (iii) holds then: $\\kappa$ has finite rank $'M<\\infty,$ ) then $A(f)$ is realanalytic. ", "page_idx": 23}, {"type": "text", "text": "A direct corollary of the first statement in Proposition B.7 guarantees that $A$ preserves convergence. Consequentially, any finite-rank truncation or any sequence of learners converging to a limiting function in the RKHS do so if and only if their standardized versions do; moreover, both sequences converge at the same speed due to $A$ being an isometric embedding. ", "page_idx": 23}, {"type": "text", "text": "Corollary B.8 (Preservation of Convergence Rates). I $f(f_{N})_{N\\in\\mathbb{N}}$ is a sequence in $\\mathcal{H}$ converging to some $f\\in\\mathcal H$ then: for each $N\\in\\mathbb{N}$ on has $\\|f_{N}-f\\|_{\\kappa}=\\|A(f_{N})-A(f)\\|_{L_{\\mu}^{2}(\\mathbb{R}^{d})}$ . ", "page_idx": 23}, {"type": "text", "text": "We now are now prepared to prove Proposition B.7. ", "page_idx": 23}, {"type": "text", "text": "Proof of Proposition B.7. We first show that $A$ is a linear isometric embedding (resp. linear isomorphism). This allows us to relay the spectral decay rate of any function $f$ in $\\mathcal{H}$ to that of its \u201cstandardization\u201d $A(f)$ in $L_{\\mu}^{2}(\\mathcal{X})$ . We then use this identification, together with results from spectral theory (see [11] for references) to obtain our conclusion. ", "page_idx": 23}, {"type": "text", "text": "Step $\\mathbf{1}-A$ is a Linear Isometric Embedding (Resp. Linear Isomorphism): Observe that $A$ admits a linear left-inverse given by ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle B:L_{\\mu}^{2}(\\mathcal{X})\\rightarrow\\mathcal{H}}}\\\\ {{\\displaystyle f=\\sum_{i=0}^{\\infty}\\langle f,\\bar{\\psi_{i}}\\rangle_{L_{\\mu}^{2}(\\mathcal{X})}\\,\\bar{\\psi_{i}}\\mapsto\\displaystyle\\sum_{i=0}^{M}\\langle f,\\psi_{i}\\rangle_{L_{\\mu}^{2}(\\mathcal{X})}\\,\\|\\bar{\\psi_{i}}\\|_{L_{\\mu}^{2}(\\mathcal{X})}\\,\\psi_{i}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "In particular, $A$ is a linear isomorphism if and only if $M=\\infty$ ; since otherwise $B$ is not a two-sided (linear) inverse. Furthermore, $A$ is an isometric embedding (resp. isometric isomorphism when $M=\\infty$ ) since due to the following. For each $f\\in\\mathcal H$ we have that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|A(f)\\|_{L_{\\mu}^{2}(X)}^{2}=\\sum_{i=0}^{M}\\frac{|\\langle f,\\psi_{i}\\rangle_{\\kappa}|^{2}\\,\\|\\psi_{i}\\|_{K}^{2}}{\\|\\tilde{\\psi}_{i}\\|_{L_{\\mu}^{2}(X)}^{2}}\\,\\|\\bar{\\psi}_{i}\\|_{L_{\\mu}^{2}(X)}^{2}}\\\\ {\\displaystyle=\\sum_{i=0}^{M}\\frac{|\\langle f,\\psi_{i}\\rangle_{\\kappa}|^{2}\\,\\cdot\\,1}{\\|\\bar{\\psi}_{i}\\|_{L_{\\mu}^{2}(X)}^{2}}\\,\\|\\bar{\\psi}_{i}\\|_{L_{\\mu}^{2}(X)}^{2}}\\\\ {\\displaystyle=\\sum_{i=0}^{M}\\,\\frac{|\\langle f,\\psi_{i}\\rangle_{\\kappa}|^{2}}{\\|\\bar{\\psi}_{i}\\|_{L_{\\mu}^{2}(X)}^{2}}\\,\\|\\bar{\\psi}_{i}\\|_{L_{\\mu}^{2}(X)}^{2}}\\\\ {\\displaystyle=\\sum_{i=0}^{M}\\,|\\langle f,\\psi_{i}\\rangle_{\\kappa}|^{2}\\,1}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{}}&{{}=\\sum_{i=0}^{M}|\\langle f,\\psi_{i}\\rangle_{\\kappa}|^{2}\\,\\|\\psi_{i}\\|_{\\kappa}^{2}}\\\\ {}&{{}=\\|f\\|_{\\kappa}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where (14) and (16) held by virtue of $(\\psi_{i})_{i=0}^{M}$ being an orthonormal basis of $\\mathcal{H}$ . From (14)- (17) we conclude that for each $f\\in\\mathcal H$ we have $\\|\\dot{A}(f)\\|_{L_{\\mu}^{2}(\\mathcal{X})}=\\|f\\|_{\\kappa}$ . Thus, $A$ is an isometric embedding. ", "page_idx": 24}, {"type": "text", "text": "Finite Rank Kernel: ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Suppose that $M<\\infty$ . We consider two cases in Assumption (DR) separately. Under Assumption (DR) (ii), [11, Corollary 9.28] guarantees that each $\\bar{\\psi}_{0},\\ldots,\\bar{\\psi}_{M}$ is smooth. Since any finite sum of real analytic functions is again smooth then, in this case, $A(f)$ is smooth. Under Assumption (DR) (iii), [11, Theorem 6.8] each $\\bar{\\psi}_{0},\\ldots,\\bar{\\psi}_{M}$ is real-analytic. Since any finite sum of real analytic functions is again real-analytic then $A(f)$ is real-analytic. ", "page_idx": 24}, {"type": "text", "text": "Infinite-Finite Rank Kernel: ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Suppose that $M=\\infty$ and fix $s>0$ . By definition of $(\\psi_{i})_{i=0}^{\\infty}$ and of the Sobolev space $H^{s}(\\mathcal{X})$ we have that: for each $f\\in L_{\\mu}^{2}(\\mathcal{X})$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\|f\\|_{H^{s}(\\mathcal{X})}^{2}=\\sum_{i=0}^{\\infty}\\,\\vert\\langle f,\\bar{\\psi}_{i}\\rangle_{L_{\\mu}^{2}(\\mathcal{X})}\\vert^{2}\\,\\langle(1-\\Delta)^{s}\\bar{\\psi}_{i},\\bar{\\psi}_{i}\\rangle_{L_{\\mu}^{2}(\\mathcal{X})}}}\\\\ {{\\displaystyle=\\sum_{i=0}^{\\infty}\\,\\vert\\langle f,\\bar{\\psi}_{i}\\rangle_{L_{\\mu}^{2}(\\mathcal{X})}\\vert^{2}(1-\\bar{\\lambda}_{i})^{s}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Since $f\\in L_{\\mu}^{2}(\\mathcal{X})$ if and only if $\\|f\\|_{H^{s}(\\mathcal{X})}$ is well-defined and finite. Consequentially, the elements of the Sobolev space $H^{s}(\\mathcal{X})$ are characterized by the summability of the right-hand side of (18). That is, for each $f\\in L_{\\mu}^{2}(\\mathcal{X})$ one has ", "page_idx": 24}, {"type": "equation", "text": "$$\nf\\in H^{s}(\\mathcal{X})\\Leftrightarrow\\sum_{i=0}^{\\infty}|\\langle f,\\bar{\\psi}_{i}\\rangle_{L_{\\mu}^{2}(\\mathcal{X})}|^{2}(1-\\bar{\\lambda}_{i})^{s}<\\infty.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By Assumption (ND), both $0<\\underline{{C}}\\ {\\stackrel{\\mathrm{def.}}{=}}\\ \\operatorname*{inf}_{i\\in[M]}\\ \\|{\\bar{\\psi}}_{i}\\|_{L_{\\mu}^{2}(\\mathcal{X})}$ and $\\bar{C}\\stackrel{\\mathrm{def.}}{=}\\operatorname*{sup}_{i\\in[M]}\\,\\|\\bar{\\psi}_{i}\\|_{L_{\\mu}^{2}(\\mathcal{X})}<\\infty$ are well-defined, non-negative real numbers. Therefore, for every $i\\in\\mathbb N$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n0<\\frac{|\\langle f,\\psi_{i}\\rangle_{\\kappa}|^{2}}{\\bar{C}}\\leq\\frac{|\\langle f,\\psi_{i}\\rangle_{\\kappa}|^{2}}{\\|\\bar{\\psi}_{i}\\|_{L_{\\mu}^{2}(\\mathcal{X})}}\\leq\\frac{|\\langle f,\\psi_{i}\\rangle_{\\kappa}|^{2}}{\\underline{{C}}}<\\infty.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "From (20) we deduce that: for each $f\\in L_{\\mu}^{2}(\\mathcal{X})$ and every $s>0$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{1}{\\overline{{C}}}\\sum_{i=0}^{\\infty}\\vert\\langle f,\\psi_{i}\\rangle_{\\kappa}\\vert^{2}\\left(1-\\bar{\\lambda}_{i}\\right)^{s}\\leq\\sum_{i=0}^{\\infty}\\frac{\\vert\\langle f,\\psi_{i}\\rangle_{\\kappa}\\vert^{2}}{\\Vert\\bar{\\psi}_{i}\\Vert_{L_{\\mu}^{2}(\\mathcal{X})}}\\left(1-\\bar{\\lambda}_{i}\\right)^{s}\\leq\\frac{1}{\\underline{{C}}}\\sum_{i=0}^{\\infty}\\vert\\langle f,\\psi_{i}\\rangle_{\\kappa}\\vert^{2}\\left(1-\\bar{\\lambda}_{i}\\right)^{s}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Consequentially, (18) implies that ", "page_idx": 24}, {"type": "equation", "text": "$$\nA(f)\\in H^{s}(\\mathcal{X})\\Leftrightarrow\\sum_{i=0}^{\\infty}\\vert\\langle f,\\psi_{i}\\rangle_{\\kappa}\\vert^{2}\\,(1-\\bar{\\lambda}_{i})^{s}<\\infty.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Step 2 - Asymptotics of Laplacian Eigenspectrum Under Assumption (DR) (ii), Weyl\u2019s law for compact Riemannian manifolds (see e.g. [11, Corollary 9.35]) implies that there are dimensional (depending on $d,\\chi$ , and on $g$ ) constants $0<C_{W:1}^{g}\\leq\\Dot{C}_{W:2}^{g}<\\infty$ such that: for each $i\\in\\mathbb N$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n0<C_{W:1}^{g}\\left(\\frac{i}{v_{d}\\,V_{g}(\\mathcal{X})}\\right)^{2/d}\\le\\bar{\\lambda}_{i}\\le C_{W:2}^{g}\\left(\\frac{i}{v_{d}\\,V_{g}(\\mathcal{X})}\\right)^{2/d}<\\infty,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where vdde=f. V ({x \u2208Rd : \u2225x\u22252 \u22641}) = \u0393(1\u03c0+d/d2/2) and where $V$ is the $d$ -dimensional Lebesgue measure on $\\mathbb{R}^{d}$ . ", "page_idx": 24}, {"type": "text", "text": "Under Assumption (DR) (iii), Weyl\u2019s law (see e.g. [11, Theorem 6.27]) implies that: there are dimensional (depending on $d$ and on $\\mathcal{X}$ ) constants $0\\stackrel{\\cdot}{<}\\tilde{C}_{W:1}\\leq\\tilde{C}_{W:2}<\\infty$ such that: for each $i\\in\\mathbb N$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n0<\\tilde{C}_{W:1}\\left(\\frac{i}{v_{d}\\,V(\\mathcal{X})}\\right)^{2/d}\\le\\bar{\\lambda}_{i}\\le\\tilde{C}_{W:2}\\left(\\frac{i}{v_{d}\\,V(\\mathcal{X})}\\right)^{2/d}<\\infty.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "With some abuse of notation, for $i=1,2$ , we write $C_{W:i}$ for $C_{W:i}^{g}$ or $\\tilde{C}_{W:i}$ depending on which of Assumptions (DR) (ii) or (iii) are being used. We therefore, wild a mild abuse of notation, we collect (22) and (23) into the following single expression: ", "page_idx": 25}, {"type": "equation", "text": "$$\n0<\\tilde{C}_{W:1}\\left(c\\,i\\frac{1}{v_{d}\\,V(\\chi)}\\right)^{2/d}\\leq\\bar{\\lambda}_{i}\\leq\\tilde{C}_{W:2}\\left(c\\,i\\right)^{2/d}<\\infty,\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the constant $c>0$ is defined by ", "page_idx": 25}, {"type": "equation", "text": "$$\nc\\stackrel{\\mathrm{def.}}{=}\\left\\{\\frac{\\pi^{d/2}}{\\Gamma(1{+}d/2)\\,V_{g}(\\mathcal{X})}\\right.\\,\\,\\,\\,:\\mathrm{if~Assumptions~(DR)~(ii)~holds}\\,\\,\\,}\\\\ {\\frac{\\pi^{d/2}}{\\Gamma(1{+}d/2)\\,V(\\mathcal{X})}\\,\\,\\,\\,\\,\\,\\,:\\mathrm{if~Assumptions~(DR)~(iii)~holds}\\,\\,\\,.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Combining (21) with (24), we find that ", "page_idx": 25}, {"type": "equation", "text": "$$\nA(f)\\in H^{s}(\\mathcal{X})\\Leftrightarrow\\sum_{i=0}^{\\infty}\\,|\\langle f,\\psi_{i}\\rangle_{\\kappa}|^{2}\\,(c\\,i)^{2s/d}<\\infty.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Pulling out the common $c^{2s/d}$ factor in all summands on the right-hand side of (25) we find that: for any $s>0$ any $f\\in L_{\\mu}^{2}(\\mathcal{X})$ belongs to $H^{s}(\\mathcal{X})$ if and only if ", "page_idx": 25}, {"type": "equation", "text": "$$\nA(f)\\in H^{s}(\\mathcal{X})\\Leftrightarrow\\sum_{i=0}^{\\infty}\\,|\\langle f,\\psi_{i}\\rangle_{\\kappa}|^{2}\\,i^{2s/d}<\\infty.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": ". Sub-Exponential Decay: Suppose that there exists an $r>0$ such that $|\\langle f,\\psi_{i}\\rangle_{\\kappa}|\\lesssim i^{-r}$ and for every $\\varepsilon>0$ we have $\\operatorname*{lim}_{i\\to\\infty}\\left|\\langle f,\\psi_{i}\\rangle_{\\kappa}\\right|i^{-r+\\varepsilon}=\\infty$ then (26) implies that ", "page_idx": 25}, {"type": "equation", "text": "$$\nA(f)\\in H_{0}^{\\frac{(1+r)d}{2}}(\\mathcal X)\\setminus\\bigcap_{\\varepsilon>0}H_{0}^{\\frac{(1+r)d}{2}+\\varepsilon}(\\mathcal X).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "If Assumption (DR) (ii) or (iii) holds the Sobolev embedding theorem for Riemannian manifolds with boundary, as formulated in [11, Theorem 9.26], implies that $H_{0}^{s_{r}}(\\mathcal{X})\\subseteq$ $C^{k_{r}}(\\mathcal{X})$ where $s_{r}\\textrm{--}d/2>k_{r}$ for some $k_{r}\\,\\geq\\,0$ to be determined shortly; where for us $s_{r}\\ {\\stackrel{\\mathrm{def.}}{=}}\\ (1+r)d/2$ . Therefore, $d r/2>k_{r}$ . Consequentially, for each $f\\in\\mathcal H$ we have that ", "page_idx": 25}, {"type": "equation", "text": "$$\nA(f)\\in\\bigcap_{r d/2>k}C^{k}({\\mathcal{X}}).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "2. Exponential Decay: Suppose that there exists some $r\\,>\\,0$ such that $|\\langle f,\\psi_{i}\\rangle_{\\kappa}|\\lesssim e^{-r\\,i}$ . Then, then (26) implies that $A(f)\\in\\cap_{s>0}H_{0}^{s}(\\mathcal{X})$ . By the Sobolev Embedding Theorem, in the bounded case ([11, Theorem 9.2.6]) ", "page_idx": 25}, {"type": "equation", "text": "$$\nA(f)\\in C^{\\infty}({\\overline{{\\mathcal{X}}}}).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "C Master inequalities ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In this section, we will present and proof the Master inequalities, which is crucial in our argument. Our approach bounds each term in the bias-variance decomposition in line (5): ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathcal{R}=\\mathbb{E}_{\\epsilon}\\left[\\|\\hat{\\theta}(\\mathbf{X}\\pmb{\\theta}^{*}+\\epsilon)-\\theta^{*}\\|_{\\Sigma}^{2}\\right]=\\mathcal{B}+\\mathcal{V},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "under minimal assumptions. ", "page_idx": 26}, {"type": "text", "text": "C.1 More notations ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We first introduce some more notations. We define the empirical covariance matrix: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\hat{\\pmb{\\Sigma}}\\ \\overset{\\mathrm{def.}}{=}\\frac{1}{n}\\mathbf{X}^{\\top}\\mathbf{X}=\\frac{1}{n}\\sum_{i=1}^{n}{\\mathbf{x}}_{i}\\mathbf{x}_{i}^{\\top}\\in\\mathbb{R}^{p\\times p},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and denote by ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbf{z}\\ {\\stackrel{\\mathrm{def.}}{=}}\\ \\sum^{-1/2}\\mathbf{x}\\in\\mathbb{R}^{p}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "the whitened/isotropic feature vector where $\\mathbb{E}_{\\mu}\\left[\\mathbf{zz}^{\\top}\\right]=\\mathbf{I}_{p}\\in\\mathbb{R}^{p\\times p}$ . Denote the whitened input matrix by $\\mathbf{Z}\\stackrel{\\mathrm{def.}}{=}\\mathbf{X}\\mathbf{\\Sigma}^{-1/2}\\in\\mathbb{R}^{n\\times p}$ , then the rows of $\\mathbf{Z}$ are isotropic centered i.i.d. random vectors. ", "page_idx": 26}, {"type": "text", "text": "Definition C.1 (Generalized vector norm). We denote $\\lVert\\mathbf{v}\\rVert_{\\mathbf{M}}\\stackrel{\\mathrm{def.}}{=}\\sqrt{\\mathbf{v}^{\\top}\\mathbf{M}\\mathbf{v}}$ for any positive definite matrix $\\mathbf{M}$ and vector $\\mathbf{v}$ . ", "page_idx": 26}, {"type": "text", "text": "Remark C.2 (Examples). Let $\\mathbf{v}\\in\\mathbb{R}^{p}$ and we have $\\left\\|\\mathbf{v}\\right\\|_{2}^{2}=\\mathbf{v}^{\\top}\\mathbf{I}_{p}\\mathbf{v}=\\left\\|\\mathbf{v}\\right\\|_{\\mathbf{I}_{p}}^{2}$ , where ${\\bf\\cal I}_{p}$ is the identity matrix. Another example is that the euclidean norm of $\\mathbf{x}$ equals to the $\\Sigma$ -norm of its whitened counterpart ${\\bf z}$ : ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{x}\\right\\|_{2}^{2}=\\mathbf{x}^{\\top}\\mathbf{x}=\\mathbf{z}^{\\top}\\Sigma\\mathbf{z}=\\left\\|\\mathbf{z}\\right\\|_{\\Sigma}^{2}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Furthermore, if $\\mathbf{M}_{1}\\preccurlyeq\\mathbf{M}_{2}$ , we have $\\|\\mathbf{v}\\|_{\\mathbf{M}_{1}}\\leq\\|\\mathbf{v}\\|_{\\mathbf{M}_{2}}$ . In particular, since $\\boldsymbol{\\Sigma}\\prec\\lambda_{1}\\mathbf{I}_{p}$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\lVert\\mathbf{v}\\rVert_{\\Sigma}\\leq\\sqrt{\\lambda_{1}}\\left\\lVert\\mathbf{v}\\right\\rVert_{2}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Lastly, it is convenient to write the expected values in a generalized vector norm: fix a distribution $\\mu$ , an input block $\\mathbf{X}\\in\\mathbb{R}^{n\\times p}$ , where its rows are drawn i.i.d. by $\\mu$ , and denote by $\\begin{array}{r}{{\\hat{\\mu}}={\\frac{1}{n}}\\sum_{i=1}^{n}\\delta_{\\mathbf{x}_{i}}}\\end{array}$ the empirical distribution of $\\mu$ , then for any $\\mathbf{v}\\in\\mathbb{R}^{p}$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbf{x}\\sim\\mu}\\left[(\\mathbf{x}^{\\top}\\mathbf{v})^{2}\\right]=\\left\\lVert\\mathbf{v}\\right\\rVert_{\\Sigma}^{2};~~~~\\mathbb{E}_{\\mathbf{x}\\sim\\hat{\\mu}}\\left[(\\mathbf{x}^{\\top}\\mathbf{v})^{2}\\right]=\\left\\lVert\\mathbf{v}\\right\\rVert_{\\hat{\\Sigma}}^{2}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Definition C.3 (Truncated Terms). For any vector $\\textbf{v}\\in\\mathbb{R}^{M}$ , write $\\mathbf{v}\\;=\\;\\mathbf{v}_{\\leq k}\\;\\oplus\\;\\mathbf{v}_{>k}\\;\\in\\;\\mathbb{R}^{k}\\;\\oplus$ $\\mathbb{R}^{M-k}$ . For any square matrix $\\mathbf{M}\\,\\in\\,\\mathbb{R}^{M\\times M}$ , write ${\\bf M}\\,=\\,\\left(\\stackrel{\\bf M\\_}{*}{\\dots}\\,{\\bf\\sigma}_{{\\bf M}_{>k}}^{*}\\right)$ with M\u2264k \u2208Rk\u00d7k and $\\mathbf{M}_{>k}\\:\\in\\:\\mathbb{R}^{(M-k)\\times(M-k)}$ . Analogously, for any non-square matrix $\\mathbf{M}\\,\\in\\,\\mathbb{R}^{N\\times M}$ , we write $\\mathbf{M}=\\mathbf{M}_{\\leq k}\\oplus\\mathbf{M}_{>k}\\in\\mathbb{R}^{N\\times k}\\oplus\\mathbb{R}^{N\\times(M-k)}$ . Also write ${\\bf M}_{-k}$ ${\\boldsymbol{k}}\\ \\stackrel{\\mathrm{def.}}{=}{\\mathbf{M}}_{\\leq k-1}\\oplus{\\mathbf{M}}_{>k}\\in\\mathbb{R}^{N\\times(M-1)}$ and $\\mathbf{M}_{l:j}=[\\mathbf{M}_{i,k}]_{k=l+1}^{j}\\in\\mathbb{R}^{N\\times(j-l)}$ . ", "page_idx": 26}, {"type": "text", "text": "Denote $\\mathbf{A}\\stackrel{\\mathrm{def.}}{=}\\mathbf{X}\\mathbf{X}^{\\top}+n\\lambda\\mathbf{I}_{n}\\in\\mathbb{R}^{n\\times n}$ . For any $k\\in\\mathbb{N}$ , denote ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbf{A}_{k}=\\mathbf{X}_{>k}\\mathbf{X}_{>k}^{\\top}+n\\lambda\\mathbf{I}_{n}\\in\\mathbb{R}^{n\\times n}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The symmetric matrix ${\\bf A}_{k}$ plays an important role in the analysis. Intuitively, when $k$ is chosen appropriately, ${\\bf A}_{k}$ is approximately equal to a scaled identity, with spectrum bounded by some constants from below and from above. ", "page_idx": 26}, {"type": "text", "text": "We introduce a few more definitions that have been used in the previous literature. ", "page_idx": 26}, {"type": "text", "text": "Definition C.4 (Effective ranks [6]). Let $k\\in\\mathbb{N}$ . Define two quantities: ", "page_idx": 26}, {"type": "equation", "text": "$$\nr_{k}\\ensuremath{\\stackrel{\\mathrm{def.}}{=}}\\frac{\\mathrm{Tr}[\\ensuremath{\\Sigma}_{>k}]}{\\|\\ensuremath{\\Sigma}_{>k}\\|_{\\mathrm{op}}}=\\frac{\\sum_{l=k+1}^{p}\\lambda_{l}}{\\lambda_{k+1}},\\quad R_{k}\\ensuremath{\\stackrel{\\mathrm{def.}}{=}}\\frac{\\mathrm{Tr}[\\ensuremath{\\Sigma}_{>k}]^{2}}{\\mathrm{Tr}[\\ensuremath{\\Sigma}_{>k}^{2}]}=\\frac{\\left(\\sum_{l=k+1}^{p}\\lambda_{l}\\right)^{2}}{\\sum_{l=k+1}^{p}\\lambda_{l}^{2}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Note that we have $r_{k}\\le R_{k}\\le r_{k}^{2}$ . 10 10See [6] for details. ", "page_idx": 26}, {"type": "text", "text": "Definition C.5 (Concentration coefficients [7]). Define the quantities: 11 ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\rho_{n,k}\\stackrel{\\mathrm{def.}}{=}\\frac{n\\,\\|\\Sigma_{>k}\\|_{\\mathrm{op}}+s_{1}\\mathbf{(A}_{k})}{s_{n}\\mathbf{(A}_{k})};\\ \\ \\ \\ \\zeta_{n,k}\\stackrel{\\mathrm{def.}}{=}\\frac{s_{1}(\\mathbf{Z}_{\\leq k}^{\\top}\\mathbf{Z}_{\\leq k})}{s_{k}\\mathbf{(Z}_{\\leq k}^{\\top}\\mathbf{Z}_{\\leq k})};\\ \\ \\ \\xi_{n,k}\\stackrel{\\mathrm{def.}}{=}\\frac{s_{1}(\\mathbf{Z}_{\\leq k}^{\\top}\\mathbf{Z}_{\\leq k})}{n}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $s_{i}(\\cdot)$ denotes the $i$ -th largest singular value of the matrix. ", "page_idx": 27}, {"type": "text", "text": "To enhance readability, we will omit the subscript $^{\\cdot n,k}$ when the context is clear. The intuition of the concentration coefficients is that for fixed $k\\in\\mathbb{N}$ and $n$ large enough, $\\zeta$ and $\\xi$ concentrate around some constants; when $k$ is smaller but scales with $n,\\,\\rho$ converges to a constant, as it essentially represents the condition number of ${\\bf A}_{k}$ . As mentioned earlier, the spectrum of ${\\bf A}_{k}$ is bounded. Therefore, selecting an appropriate $k$ ensures that all three concentration coefficients converge to certain constants. ", "page_idx": 27}, {"type": "text", "text": "C.2 Statements and proofs ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Now we are ready to present the Master inequalities, which are proved in [46, 7]: ", "page_idx": 27}, {"type": "text", "text": "Proposition C.6 (Master inequality for $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ , [46, 7]). Let $k\\in\\mathbb{N}$ be an integer. For any $\\delta\\in(0,1)$ , with probability at least $1-\\delta$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\nB\\leq\\left(\\frac{1+\\rho^{2}\\zeta^{2}\\xi^{-1}+\\rho}{\\delta}\\right)\\|\\theta_{>k}^{*}\\|_{\\Sigma_{>k}}^{2}+(\\zeta^{2}\\xi^{-2}+\\rho\\zeta^{2}\\xi^{-1})\\frac{s_{1}(\\mathbf{A}_{k})^{2}}{n^{2}}\\left\\|\\theta_{\\leq k}^{*}\\right\\|_{\\Sigma_{\\leq k}^{-1}}^{2}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. We begin by bounding the terms that appear in Lemma H.8: plug in Definition C.5 of $\\rho,\\zeta,\\xi$ , since ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{s_{1}(\\mathbf{A}_{k}^{-1})^{2}}{s_{n}(\\mathbf{A}_{k}^{-1})^{2}}\\frac{s_{1}(\\mathbf{Z}_{\\leq k}^{\\top}\\mathbf{Z}_{\\leq k})}{s_{k}(\\mathbf{Z}_{\\leq k}^{\\top}\\mathbf{Z}_{\\leq k})^{2}}=\\frac{s_{1}(\\mathbf{A}_{k})^{2}}{s_{n}(\\mathbf{A}_{k})^{2}}\\frac{s_{1}(\\mathbf{Z}_{\\leq k}^{\\top}\\mathbf{Z}_{\\leq k})^{2}}{s_{k}(\\mathbf{Z}_{\\leq k}^{\\top}\\mathbf{Z}_{\\leq k})^{2}}\\frac{n}{s_{1}(\\mathbf{Z}_{\\leq k}^{\\top}\\mathbf{Z}_{\\leq k})}\\frac{1}{n}\\leq\\rho^{2}\\zeta^{2}\\xi^{-1}\\cdot\\frac{1}{n}}\\\\ &{\\frac{1}{s_{n}(\\mathbf{A}_{k}^{-1})^{2}s_{k}(\\mathbf{Z}_{\\leq k}^{\\top}\\mathbf{Z}_{\\leq k})^{2}}=\\frac{s_{1}(\\mathbf{A}_{k})^{2}}{s_{k}(\\mathbf{Z}_{\\leq k}^{\\top}\\mathbf{Z}_{\\leq k})^{2}}=\\frac{s_{1}(\\mathbf{A}_{k})^{2}}{n^{2}}\\frac{s_{1}(\\mathbf{Z}_{\\leq k}^{\\top}\\mathbf{Z}_{\\leq k})^{2}}{s_{k}(\\mathbf{Z}_{\\leq k}^{\\top}\\mathbf{Z}_{\\leq k})^{2}}\\frac{n^{2}}{s_{1}(\\mathbf{Z}_{\\leq k}^{\\top}\\mathbf{Z}_{\\leq k})^{2}}=\\frac{s_{1}(\\mathbf{A}_{k})^{2}}{n^{2}}\\cdot\\frac{1}{n^{2}}}\\\\ &{|\\Sigma_{>k}|_{\\mathrm{op}}\\,s_{1}(\\mathbf{A}_{k}^{-1})=\\frac{\\|\\Sigma_{>k}\\|_{\\mathrm{op}}}{s_{n}(\\mathbf{A}_{k})}=\\frac{n\\,\\|\\Sigma_{>k}\\|_{\\mathrm{op}}}{s_{n}(\\mathbf{A}_{k})}\\frac{1}{n}\\leq\\rho\\cdot\\frac{1}{n}}\\\\ &{|\\Sigma_{>k}\\|_{\\mathrm{o p \n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{B}\\leq\\|\\theta_{>k}^{*}\\|_{\\Sigma_{>k}}^{2}+\\rho^{2}\\zeta^{2}\\xi^{-1}\\frac{\\big\\|{\\bf X}_{>k}\\theta_{>k}^{*}\\big\\|_{2}^{2}}{n}}\\\\ &{\\quad+\\zeta^{2}\\xi^{-2}\\frac{s_{1}\\big({\\bf A}_{k}\\big)^{2}}{n^{2}}\\,\\|\\theta_{\\leq k}^{*}\\big\\|_{\\Sigma_{\\leq k}^{-1}}^{2}}\\\\ &{\\quad+\\,\\rho\\frac{\\big\\|{\\bf X}_{>k}\\theta_{>k}^{*}\\big\\|_{2}^{2}}{n}}\\\\ &{\\quad+\\,\\rho\\zeta^{2}\\xi^{-1}\\frac{s_{1}\\big({\\bf A}_{k}\\big)^{2}}{n^{2}}\\,\\|\\theta_{\\leq k}^{*}\\big\\|_{\\Sigma_{\\leq k}^{-1}}^{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We regroup the terms into: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathcal B\\leq\\|\\pmb{\\theta}_{>k}^{*}\\|_{\\pmb{\\Sigma}_{>k}}^{2}+(\\rho^{2}\\zeta^{2}\\xi^{-1}+\\rho)\\frac{\\big\\|\\mathbf{X}_{>k}\\pmb{\\theta}_{>k}^{*}\\big\\|_{2}^{2}}{n}+(\\zeta^{2}\\xi^{-2}+\\rho\\zeta^{2}\\xi^{-1})\\frac{s_{1}(\\mathbf{A}_{k})^{2}}{n^{2}}\\left\\|\\pmb{\\theta}_{\\leq k}^{*}\\right\\|_{\\mathbf{X}_{\\leq k}^{-1}}^{2}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "By Lemma G.6, we can further simplify the expression into a high probability bound: for any $\\delta\\in(0,1)$ , with a probability at least $1-\\delta$ , it holds that ", "page_idx": 27}, {"type": "equation", "text": "$$\nB\\leq\\left(1+\\frac{\\rho^{2}\\zeta^{2}\\xi^{-1}+\\rho}{\\delta}\\right)\\|\\theta_{>k}^{*}\\|_{\\Sigma_{>k}}^{2}+(\\zeta^{2}\\xi^{-2}+\\rho\\zeta^{2}\\xi^{-1})\\frac{s_{1}(\\mathbf{A}_{k})^{2}}{n^{2}}\\left\\|\\theta_{\\leq k}^{*}\\right\\|_{\\Sigma_{\\leq k}^{-1}}^{2}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Note that $1/\\delta>1$ and we obtain the result. ", "page_idx": 27}, {"type": "text", "text": "11Alternatively, one can define a smaller \u03c1n,kd $\\begin{array}{r}{\\rho_{n,k}\\ {\\stackrel{\\mathrm{def.}}{=}}\\ {\\frac{\\operatorname*{max}\\{n\\left\\|\\mathbf{\\Sigma}_{>k}\\right\\|_{\\mathrm{op}},s_{1}(\\mathbf{A}_{k})\\}}{s_{n}(\\mathbf{A}_{k})}}\\leq{\\frac{n\\left\\|\\mathbf{\\Sigma}_{>k}\\right\\|_{\\mathrm{op}}+s_{1}(\\mathbf{A}_{k})}{s_{n}(\\mathbf{A}_{k})}}.}\\end{array}$ If $s_{1}(\\mathbf{A}_{k})=$ $\\Theta(n\\lambda_{k})=\\Theta(n\\left\\|\\pmb{\\Sigma}_{>k}\\right\\|_{\\mathrm{op}})$ , the two definitions differs only by a constant. ", "page_idx": 27}, {"type": "text", "text": "Proposition C.7 (Master inequality for $\\nu$ , [46, 7]). Let $k\\in\\mathbb{N}$ be an integer. Then the variance is bounded by: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathcal{V}/\\sigma^{2}\\leq\\rho^{2}\\left(\\zeta^{2}\\xi^{-1}\\frac{k}{n}+\\frac{\\mathrm{Tr}[\\mathbf Z_{>k}\\boldsymbol\\Sigma_{>k}^{2}\\mathbf Z_{>k}^{\\top}]}{n\\,\\mathrm{Tr}[\\boldsymbol\\Sigma_{>k}^{2}]}\\frac{r_{k}(\\boldsymbol\\Sigma)^{2}}{n R_{k}(\\boldsymbol\\Sigma)}\\right),\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\sigma^{2}\\;\\overset{\\mathrm{def.}}{=}\\mathbb{E}\\;\\big[\\epsilon^{2}\\big]\\geq0$ is the noise level. ", "page_idx": 28}, {"type": "text", "text": "Proof. We start from Lemma H.9: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\gamma/\\sigma^{2}\\le\\frac{s_{1}\\big(A_{1}^{-1}\\big)^{2}\\operatorname{Tr}\\big[\\mathbf{X}_{\\le1}\\mathbf{Z}_{\\le1}^{-\\mathbf{k}}\\mathbf{X}_{\\le1}^{\\le\\mathbf{k}}\\mathbf{Z}_{\\le1}^{\\le\\mathbf{k}}\\big]}{s_{n}(\\mathbf{A}_{k}^{-1})^{2}s_{k}(\\mathbf{Z}_{\\le1}^{-\\mathbf{k}})^{2}\\mathbf{C}_{k}\\mathbf{A}_{k}^{-1}\\mathbf{Z}_{\\le1}^{-\\mathbf{k}}\\mathbf{Z}_{\\le1}^{\\le\\mathbf{k}}}+s_{1}(\\mathbf{A}_{k}^{-1})^{2}\\operatorname{Tr}\\big[\\mathbf{X}_{>k}\\mathbf{Z}_{>k}\\mathbf{A}_{>k}^{\\top}\\mathbf{Z}_{\\le k}^{\\top}\\big]}\\\\ &{\\qquad=\\frac{s_{1}\\big(A_{k}\\big)^{2}}{s_{n}(\\mathbf{A}_{k})^{2}}\\frac{\\prod\\mathbb{T}\\big[\\mathbb{Z}_{\\le k}\\mathbf{Z}_{\\le k}^{\\top}\\mathbf{Z}_{\\le k}^{\\top}\\big]}{s_{k}(\\mathbf{Z}_{\\le1}^{-\\mathbf{k}})^{2}}+\\frac{n^{2}\\big\\|\\mathbb{D}_{>k}\\big\\|\\mathbb{Z}_{0}^{\\top}\\mathbf{Tr}\\big[\\mathbb{Z}_{0}^{\\top}\\mathbf{Tr}\\big]\\big[\\mathbf{X}_{>k}\\mathbf{Z}_{>k}\\mathbf{X}_{>k}^{\\sum}\\big]}{n^{2}\\sigma^{2}\\big\\|\\mathbb{Z}_{\\ge1}^{\\mathbf{k}}\\big\\|\\mathbb{Z}_{0}^{\\top}}}\\\\ &{\\qquad\\le\\rho^{2}\\left(\\frac{\\prod\\big(\\mathbf{Z}_{\\le k}\\mathbf{Z}_{\\le k}^{\\top}\\big)}{s_{k}(\\mathbf{Z}_{\\le k}^{\\top}\\mathbf{Z}_{\\le k}^{\\top})^{2}}+\\frac{\\operatorname{Tr}\\big[\\mathbb{X}_{>k}\\mathbf{Z}_{\\ge k}\\mathbf{X}_{\\le k}^{\\top}\\big]}{n^{2}\\|\\mathbb{Z}_{>k}\\|\\mathbb{Z}_{0}^{\\top}}\\right)}\\\\ &{\\qquad\\le\\rho^{2}\\left(\\frac{k_{k}(\\mathbf{Z}_{\\le k}^\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\xi_{n,k}\\ {\\stackrel{\\mathrm{def.}}{=}}\\ {\\frac{s_{1}(\\mathbf{Z}_{\\leq k}^{\\top}\\mathbf{Z}_{\\leq k})}{n}}$ , $\\begin{array}{r}{\\zeta_{n,k}\\ {\\stackrel{\\mathrm{def.}}{=}}\\ {\\frac{s_{1}(\\mathbf{Z}_{\\leq k}^{\\top}\\mathbf{Z}_{\\leq k})}{s_{k}(\\mathbf{Z}_{\\leq k}^{\\top}\\mathbf{Z}_{\\leq k})}}}\\end{array}$ sk(Z\u22a4\u2264kZ\u2264k), \u03c1n,k = $\\begin{array}{r}{\\rho_{n,k}\\ \\overset{\\mathrm{def.}}{=}\\ \\frac{n\\|\\mathbf{\\Xi}\\Sigma_{>k}\\|_{\\mathrm{op}}+s_{1}(\\mathbf{A}_{k})}{s_{n}(\\mathbf{A}_{k})}}\\end{array}$ , $r_{k}\\ {\\stackrel{\\mathrm{def.}}{=}}\\ {\\frac{\\mathrm{Tr}[\\Sigma_{>k}]}{\\|\\Sigma_{>k}\\|_{\\mathrm{op}}}}$ , $R_{k}\\ {\\stackrel{\\mathrm{def.}}{=}}$ $\\frac{\\mathrm{Tr}[\\Sigma_{>k}]^{2}}{\\mathrm{Tr}[\\Sigma_{>k}^{2}]}$ . \u53e3 ", "page_idx": 28}, {"type": "text", "text": "C.3 Sanity check ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Recall that $\\mathcal{B}\\stackrel{\\mathrm{def.}}{=}\\|\\hat{\\pmb{\\theta}}(\\mathbf{X}\\pmb{\\theta}^{*})-\\pmb{\\theta}^{*}\\|_{\\Sigma}^{2}$ . Let $k\\in\\mathbb{N}$ be an integer. For any $\\delta\\in(0,1)$ , with probability at least $1-\\delta$ , we have ", "page_idx": 28}, {"type": "image", "img_path": "IMlDpZmLnL/tmp/2c50cf7adb92c74cbfb010fd7be8648f7a9f7eae6a722434d031b3df046b237d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "We see that: ", "page_idx": 28}, {"type": "text", "text": "1. the regressor barely learns the tail of the target (the eigenfunctions with small eigenvalues), thus the first term $\\lvert|\\pmb{\\theta}_{>k}^{*}\\rvert|_{\\pmb{\\Sigma}_{>k}}^{2}$ in the upper bound is just a multiple of the $\\Sigma$ -norm square (or $L_{\\mu}^{2}$ -norm square in KRR setting) of the target\u2019s tail; ", "page_idx": 28}, {"type": "text", "text": "2. the second term indicates that the regressor learns well if the ridge $\\lambda$ is small, showing the trad-off between (noiseless) interpolation and regularization;   \n3. the term  \u03b8\u2217\u2264k  2\u03a3\u2212\u22641k c orresponds to norm square of the target\u2019s head in the interpolation space $\\mathcal{H}^{2}$ . This term will explain the so-called saturation effect reported in [29] where the bias ceases to improve once the source coefficient $s$ in Assumption (SC) surpasses 2. Consequently, the bias term $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ bound in Table 1 is closely related to the constant min s, 2. ", "page_idx": 28}, {"type": "text", "text": "[46, 7] also derived a non-asymptotic bound of $\\mathcal{V}$ . Recall that $\\mathcal{V}\\stackrel{\\mathrm{def.}}{=}\\mathbb{E}_{\\boldsymbol{\\epsilon}}\\left[\\|\\hat{\\pmb{\\theta}}(\\boldsymbol{\\epsilon})\\|_{\\Sigma}^{2}\\right]$ . Let $k\\in\\mathbb N$ be an integer. Then ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{V}\\leq\\underbrace{\\underline{{\\sigma}}^{2}}_{\\mathrm{noise\\;level}}\\rho^{2}\\left(\\underbrace{\\zeta^{2}\\xi^{-1}}_{\\mathrm{constant}}\\frac{k}{n}+\\underbrace{\\frac{\\mathrm{Tr}[\\mathbf{Z}_{>k}\\mathbf{\\sum}_{>k}^{2}\\mathbf{Z}_{>k}^{\\top}]}{n\\,\\mathrm{Tr}[\\mathbf{Z}_{>k}^{2}]}\\frac{r_{k}(\\mathbf{\\sum})^{2}}{n R_{k}(\\mathbf{\\sum})}}_{=\\mathcal{O}\\left(\\frac{k}{n}\\right)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "We see that: ", "page_idx": 29}, {"type": "text", "text": "1. the upper bound is proportional to noise level $\\sigma^{2}=\\mathbb{E}\\left[\\epsilon^{2}\\right]\\geq0$ ;   \n2. we will show later that for $n$ large enough, the second term is approximately $\\scriptstyle{\\frac{k}{n}}$ , hence the decay rate of $\\mathcal{V}$ is at most $\\mathcal{O}_{n,k}\\left(n^{-1}\\right)$ . ", "page_idx": 29}, {"type": "text", "text": "The derivation of Eq (30) and (31), which we refer to as the Master inequalities in this paper, is purely algebraic, and the statement holds regardless of the assumptions on the eigen-decay and the features. One can optimally choose the integer $k\\in\\mathbb N$ to achieve a tight upper bound, as we will demonstrate later in this paper. Surprisingly, the Master inequalities alone are sufficient to establish tight bounds for most cases under various settings. ", "page_idx": 29}, {"type": "text", "text": "D Over-parameterized regime ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "In this section, we provide the proof for the upper bounds presented in Table 1. For all the statements in this section, the big O notation $\\tilde{\\mathcal{O}}\\left(\\cdot\\right)$ can be replaced by $O\\left(\\cdot\\right)$ if $a+2\\neq2r$ . ", "page_idx": 30}, {"type": "text", "text": "A brief summary of the propositions can be found in Table 3. ", "page_idx": 30}, {"type": "table", "img_path": "IMlDpZmLnL/tmp/db6ef7f8b552c16e5f34c33aa80f9c240d735062808f5563efcd8f84314bc3a6.jpg", "table_caption": [], "table_footnote": ["Table 3: Brief summary of propositions derived in Section D used for proving the upper bounds on Table 1. "], "page_idx": 30}, {"type": "text", "text": "D.1 Bias under strong ridge ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Proposition D.1 (Asymptotic upper bound of the bias term with polynomial decay). Suppose: $\\lambda_{k}\\overset{=}{=}\\Theta_{k}\\left(k^{-1-a}\\right)$ , $|\\theta_{k}^{\\bar{*}}|={\\mathcal{O}}_{k}\\,(k^{\\bar{-r}})$ and $\\lambda=\\Theta_{k}\\left(k^{-b}\\right)$ for some $a,b,r>0$ . Furthermore, suppose Assumption $(G F)$ (or resp. $(I F)$ ) holds, and $\\delta,\\xi,\\zeta=\\Theta_{n,k}$ (1) with probability at least $1-\\varphi$ . Then, for any $t\\in(0,1]$ , with probability at least $1-\\delta-\\varphi$ (or resp. $1-\\delta-\\varphi-e^{-n})$ , the bias is bounded by: ", "page_idx": 30}, {"type": "equation", "text": "$$\nB=\\tilde{O}_{n}\\left(\\rho^{3}n^{-C}\\right)\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "for some constant ", "page_idx": 30}, {"type": "equation", "text": "$$\nC\\stackrel{\\mathrm{def.}}{=}\\operatorname*{min}\\{t(2r+a),2\\operatorname*{min}\\{b,1+t a\\}-t(2+a-2r)_{+}\\}>0\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "with with following lower bound: ", "page_idx": 30}, {"type": "equation", "text": "$$\nC\\geq\\left\\{\\!\\!\\begin{array}{l l}{\\operatorname*{min}\\{t(2r+a)-2(t a+1-b),2(b-1+t)\\}}&{,\\;b<1+t a;}\\\\ {\\operatorname*{min}\\{t(2r+a),2t(1+a)\\}}&{,\\;b\\geq1+t a.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof. For simplicity, we only prove the statement for all $a,r$ except when $a+2=2r$ . In such case, the argument follows similarly with $O\\left(\\cdot\\right)$ replaced by $\\tilde{\\mathcal{O}}\\left(\\cdot\\right)$ . ", "page_idx": 30}, {"type": "text", "text": "For any $k,n\\in\\mathbb{N}$ , pick $k=\\tilde{\\Theta}_{n,k}\\left(n^{t}\\right)$ for some $t\\,\\in\\,(0,1]$ . Suppose the event $\\delta,\\xi,\\zeta=\\Theta_{n,k}$ (1) happens, then: ", "page_idx": 30}, {"type": "text", "text": "$\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{=\\mathcal{O}_{n,k}\\left(\\left(\\frac{1+\\rho^{2}\\zeta^{2}\\xi^{-1}+\\rho}{\\delta}\\right)\\|\\theta_{>k}^{*}\\|_{\\mathbf{X}_{>k}}^{2}+\\left(\\rho^{2}\\zeta^{2}\\xi^{-2}+\\rho^{3}\\zeta^{2}\\xi^{-1}\\right)\\left(n^{-b}+\\frac{c_{2}\\,\\mathrm{Tr}[\\mathbf{X}_{>k}]}{n}\\right)^{2}\\left\\|\\theta_{\\leq k}^{*}\\right\\|_{\\mathbf{X}_{>k}^{2}}^{2}\\right.}\\\\ &{}&{\\left.\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\left.\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\left.\\quad\\quad\\quad\\quad\\quad\\left.\\quad\\quad\\quad\\quad\\quad\\left(34\\right)\\right.}\\\\ &{=\\mathcal{O}_{n,k}\\left(\\rho^{3}\\left(k^{-(2r+a)}+\\left(n^{-b}+\\frac{k^{-a}}{n}\\right)^{2}k^{(2+a-2r)_{+}}\\right)\\right)\\right.}\\\\ &{=\\mathcal{O}_{n,k}\\left(\\rho^{3}\\left(n^{-t(2r+a)}+\\left(n^{-b}+n^{-(1+a)}\\right)^{2}n^{(2+a-2r)_{+}}\\right)\\right)}\\\\ &{=\\mathcal{O}_{n,k}\\left(\\rho^{3}\\left(n^{-t(2r+a)}+n^{-2\\operatorname*{min}\\{b,1+t\\}}n^{t(2+a-2r)_{+}}\\right)\\right)}\\\\ &{=\\mathcal{O}_{n,k}\\left(\\rho^{3}\\left(n^{-t(2r+a)}+n^{-2\\operatorname*{min}\\{b,1+t\\}+t(2+a-2r)_{+}}\\right)\\right)}\\\\ &{=\\mathcal{O}_{n,k}\\left(\\rho^{3}\\left(n^{-t(2r+a)}+n^{-2\\operatorname*{min}\\{b,1+t\\}+t(2+a-2r)_{+}}\\right)\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\left.\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\lambda\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where we apply Proposition C.6, Lemma G.8 in line (34); we apply Lemma H.10: $\\big\\|\\theta_{\\leq k}^{*}\\big\\|_{\\Sigma_{\\leq k}^{-1}}^{2}=$ $\\Theta_{k}\\left(k^{(2+a-2r)+}\\right)$ where $x_{+}\\ {\\stackrel{\\mathrm{def.}}{=}}\\ \\operatorname*{max}\\{x,0\\}$ , as well as plug in $\\operatorname{Tr}[\\Sigma_{>k}]=\\Theta_{k}\\left(k^{-a}\\right)$ in line (35); we plug in $k=\\Tilde{\\Theta}_{n,k}\\left(n^{t}\\right)$ for some $t\\in(0,1]$ in line (36). ", "page_idx": 30}, {"type": "text", "text": "Hence the bias $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ has a decay in form of ${\\mathcal{O}}_{n}\\left(\\rho^{3}n^{-C}\\right)$ where ", "page_idx": 31}, {"type": "equation", "text": "$$\nC\\stackrel{\\mathrm{def.}}{=}\\operatorname*{min}\\{t(2r+a),2\\operatorname*{min}\\{b,1+t a\\}-t(2+a-2r)_{+}\\}\\geq0\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "is a constant defined in nested minima and maxima. To bound $C$ from below, we make two case distinctions: $b<1+t a$ or $b\\geq1+t a$ . ", "page_idx": 31}, {"type": "text", "text": "For $b<1+t a$ , we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{C=\\operatorname*{min}\\{t(2-\\alpha,\\gamma,\\beta-t)+\\alpha-2\\gamma+\\alpha-\\beta\\}+}&{2+\\alpha\\geq2r}\\\\ {=\\operatorname*{min}\\{t(2+\\alpha),2,\\beta-t(2+\\alpha-2\\gamma)\\}}&{2+\\alpha<2r}\\\\ {\\operatorname*{min}\\{t(2+\\alpha),2\\},}&{(2+\\alpha+2\\gamma)}\\\\ {=\\operatorname*{min}\\{t(2+\\alpha),t(2+\\alpha)+2(\\frac{2\\beta}{1-\\gamma}-2)-2(t+1-\\beta)\\}}&{2+\\alpha\\geq2r}\\\\ {\\operatorname*{min}\\{t(2+\\alpha),2\\}}&{2+\\alpha<2r}\\\\ {\\operatorname*{min}\\{t(2+\\alpha),t(2+\\alpha)-2\\beta\\}}&{2+\\alpha\\geq2r}\\\\ {\\operatorname*{min}\\{t(2+\\alpha),t(2+\\alpha),2\\}}&{2+\\alpha\\leq2r}\\\\ {=\\operatorname*{min}\\{t(2+\\alpha+\\alpha),2\\}}&{2+\\alpha<2r}\\\\ {\\operatorname*{min}\\{t(2+\\alpha)-2(t+1-\\beta)\\}}&{2+\\alpha<2r}\\\\ {=\\left\\{t(2+\\alpha)-2(\\alpha+1-\\beta)\\right\\}}&{2+\\alpha<2r}\\\\ {\\operatorname*{min}\\{t(2+\\alpha)-2(t+1-\\beta)\\}}&{2+\\alpha\\geq2r}\\\\ {\\operatorname*{min}\\{t-1,t+\\beta\\}}&{1\\geq4+\\alpha<2r}\\\\ {=\\left\\{t(2+\\alpha)-2(t+1-\\beta)\\right\\}}&{2+\\alpha\\geq2r}\\\\ {\\operatorname*{min}\\{t(2+\\alpha)-2(t+1-\\beta)\\}}&{2+\\alpha<2r}\\\\ {\\operatorname*{min}\\{t(2+\\alpha)-2(t+1-\\beta)\\}}&{2+\\alpha<1}\\\\ {\\operatorname*{min}\\{t(2+\\alpha)-2(t+1-\\beta)\\}}&{2+\\alpha<2r}\\\\ {\\operatorname*{min}\\{t(2+\\alpha)-2(t+1-\\beta)\\}}&{2+\\alpha<1}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "For $b\\geq1+t a$ , we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{C=\\operatorname*{min}\\{t(2r+a),2(1+t a)-t(2+a-2r)_{+}\\}}\\\\ &{\\quad=\\left\\{\\!\\operatorname*{min}\\{t(2r+a),2(1+t a)-t(2+a-2r)\\}\\quad,\\quad2+a\\geq2r}\\\\ &{\\quad=\\left\\{\\!\\operatorname*{min}\\{t(2r+a),2(1+t a)\\}\\quad,\\quad2+a<2r\\right.}\\\\ &{\\quad\\geq\\left\\{\\!\\operatorname*{min}\\{t(2r+a),t(2r+a)+2(1-t)\\}\\quad,\\quad2+a\\geq2r}\\\\ &{\\quad\\mathrm{lmin}\\{t(2+a+a),2(1+t a)\\}\\quad,\\quad2+a<2r}\\\\ &{\\quad=\\left\\{\\!t(2r+a)\\right.}\\\\ &{\\quad=\\left\\{\\!\\operatorname*{min}\\{2(t+t a),2(1+t a)\\}\\quad,\\quad2+a<2r}\\end{array}\\right.}\\\\ &{\\quad=\\left\\{\\!t(2r+a)\\quad,\\quad2+a\\geq2r\\right.}\\\\ &{\\quad=\\left\\{\\!2(t+t a)\\quad,\\quad2+a\\leq2r\\right.}\\\\ &{\\quad\\geq\\operatorname*{min}\\{t(2r+a),\\quad2+a<2r\\}}\\\\ &{\\quad\\geq\\operatorname*{min}\\{t(2r+a),2t(1+a)\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Replacing $C$ by its lower bound, we obtain the claimed decay. ", "page_idx": 31}, {"type": "text", "text": "Proposition D.2 (Asymptotic upper bound of bias under strong ridge and polynomial eigen-decay). Suppose: $\\lambda_{k}\\,=\\,\\Theta_{k}\\,\\bigl(k^{\\dot{-}1-a}\\bigr)$ , $|\\theta_{k}^{*}|\\,=\\,\\mathcal{O}_{k}\\left(k^{-r}\\right)$ and $\\lambda\\,=\\,\\Theta_{k}^{\\,-}\\,\\bigl(k^{\\underline{{\\,\\,-}}b}\\bigr)$ for some $a,r\\,>\\,0$ and $b\\in$ $(0,1+a]$ . Fix a constant $\\delta\\in(0,1)$ . If, additionally, Assumption $(G F)$ (or resp. $(I F)$ ) holds, then, with probability at least $1-\\delta-o\\left(n^{-1}\\right)$ (or resp. $1-\\delta-3e^{-c_{1}n},$ ), the bias is bounded by: ", "page_idx": 31}, {"type": "equation", "text": "$$\nB=\\tilde{\\mathcal{O}}_{n}\\left(n^{-\\operatorname*{min}\\{s b,2b\\}}\\right)\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "with $s\\ {\\stackrel{\\mathrm{def.}}{=}}\\ {\\frac{2r\\!+\\!a}{1\\!+\\!a}}$ ", "page_idx": 31}, {"type": "text", "text": "Proof. Use Proposition D.1 as the backbone, and use Propositions G.1 and G.2 to obtain the high probability guarantee for $\\xi,\\zeta=\\Theta_{n,k}$ (1). Then for different value of $b$ , we plug different values ", "page_idx": 31}, {"type": "text", "text": "of $t\\in(0,1]$ and set $k=\\Theta_{n,k}\\left(n^{t}\\right)$ , such that $\\rho=\\rho_{n,k}=\\tilde{\\mathcal{O}}_{n,k}$ (1) by Proposition G.3, in order to obtain different decay rates of bias. ", "page_idx": 32}, {"type": "text", "text": "Assume $b\\in[0,a+1)$ . Pick $k=\\Theta_{k,n}\\left(n^{t}\\right)$ with $\\textstyle t={\\frac{b}{1+a}}$ . As above, it holds that $\\rho=\\tilde{\\Theta}_{n,k}\\left(1\\right)$ with high probability. This time, we plug in $\\textstyle t={\\frac{b}{1+a}}$ in the definition of $C$ . By the choice of $b$ , we have $b<1+t a$ and hence ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{C\\stackrel{\\rightharpoonup}{=}\\operatorname*{min}\\left\\{t(2r+a),2\\operatorname*{min}\\{b,1+t a\\}-t(2+a-2r)_{+}\\right\\}}\\\\ &{\\quad=\\operatorname*{min}\\left\\{t(2r+a),2b-t(2+a-2r)_{+}\\right\\}}\\\\ &{\\quad=\\left\\{\\operatorname*{min}\\{t(2r+a),2b-t(2+a-2r)\\}\\quad,\\quad2+a\\geq2r\\right.}\\\\ &{\\quad=\\left\\{\\operatorname*{min}\\{t(2r+a),2b\\}\\quad\\quad,\\quad2+a<2r\\quad\\right.}\\\\ &{\\quad=\\left\\{\\operatorname*{min}\\{t(2r+a),t(2r+a)+2(\\hat{b}-t(1+a))\\}\\quad,\\quad2+a\\geq2r\\quad\\right.}\\\\ &{\\quad\\left.\\quad\\quad\\operatorname*{min}\\{t(2r+a),2b\\}\\quad\\quad}\\\\ &{\\quad\\geq\\left\\{\\operatorname*{min}\\{t(2r+a),t(2r+a)\\}\\quad,\\quad2+a\\geq2r\\quad\\right.}\\\\ &{\\quad\\left.\\quad\\operatorname*{min}\\{t(2r+a),2b\\}\\quad\\quad,\\quad2+a<2r}\\\\ &{\\quad\\geq\\operatorname*{min}\\{t(2r+a),2b\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Finally, assume $b=a+1$ . Pick $k=\\Theta_{k,n}\\left(n^{t}\\right)$ with $\\textstyle t={\\frac{b}{1+a}}=1$ . But this time, since $b\\geq1+t a$ we plug in t =1+ba $\\textstyle t={\\frac{b}{1+a}}=1$ into the second lower bound of $C$ in line (33): ", "page_idx": 32}, {"type": "equation", "text": "$$\nC\\geq\\operatorname*{min}\\{t(2r+a),2t(1+a)\\}=\\operatorname*{min}\\left\\{{\\frac{b(2r+a)}{1+a}},{\\frac{2b(1+a)}{1+a}}\\right\\}=\\operatorname*{min}\\{s b,2b\\}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "The argument for exponential decay follows analogously as above. Recall Assumption (EE): $\\lambda_{k}=$ $\\Theta_{k}\\left(e^{-a k}\\right)$ for some $a>0$ . Suppose $|\\theta_{k}^{*}|=\\Theta_{k}\\left(e^{\\stackrel{\\bullet}{-}r k}\\right)$ and $\\begin{array}{r}{\\lambda=\\Theta_{n}\\left(\\frac{1}{n}e^{-b n}\\right)}\\end{array}$ 12 for some $b,r>0$ Then the asymptotic bounds follows similarly to the polynomial case. For sake of completeness, in this section, we list out some propositions for illustration. ", "page_idx": 32}, {"type": "text", "text": "Proposition D.3 (Asymptotic upper bound of bias under strong ridge and exponential eigen-decay). Suppose $\\lambda_{k}=\\Theta_{k}\\left(e^{-a\\dot{k}}\\right)$ , $\\lvert\\theta_{k}^{*}\\rvert=\\Theta_{k}\\left(e^{-r k}\\right)$ and $\\lambda=\\Theta_{n}\\left(e^{-\\check{b}n}\\right)$ for some $a,r>0$ and $b\\in(0,\\dot{a})$ . If Assumption $(G F)$ (or resp. $(I F)$ ) holds, then with probability at least $\\begin{array}{r}{1-\\delta-o_{n}\\left(\\frac{1}{n}\\right)}\\end{array}$ (or resp. $1-\\delta-3e^{-c_{1}n},$ ), the bias is bounded by: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathcal{B}=\\left\\{\\mathcal{O}_{n,k}\\left(e^{-\\operatorname*{min}\\left\\{s,2\\right\\}b n}\\right)\\quad,\\;a\\neq2r\\quad\\right.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "for some constants $c_{1}>0$ , where $\\begin{array}{r}{s=\\frac{2r}{a}+1>0}\\end{array}$ is the source coefficient. ", "page_idx": 32}, {"type": "text", "text": "Remark D.4. Note that the factor $n$ in the case of $a=2r$ does not play a big role asymptotically: by taking a slightly smaller $\\tilde{b}<b$ , the decay is exponential in $\\tilde{b}n$ . ", "page_idx": 32}, {"type": "text", "text": "Proof. By Proposition C.6, with probability at least $1-\\delta$ (or resp. $1-\\delta-e^{-n},$ ), the bias is bounded by: ", "page_idx": 32}, {"type": "equation", "text": "$$\nB\\leq\\left(\\frac{1+\\rho^{2}\\zeta^{2}\\xi^{-1}+\\rho}{\\delta}\\right)\\|\\pmb{\\theta}_{>k}^{*}\\|_{\\Sigma_{>k}}^{2}+c(\\zeta^{2}\\xi^{-2}+\\rho\\zeta^{2}\\xi^{-1})\\left(\\frac{s_{1}(\\mathbf{A}_{k})}{n}\\right)^{2}\\left\\|\\pmb{\\theta}_{\\leq k}^{*}\\right\\|_{\\Sigma_{\\leq k}^{-1}}^{2}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "By Proposition G.1 and G.2, there exists some $c>1$ such that, if $c\\beta_{k}k\\log k\\leq n$ (or resp. $k/c\\leq n)$ ), then with probability at least $\\begin{array}{r}{1-2\\exp\\left(-\\frac{c}{\\beta_{k}}\\frac{n}{k}\\right)}\\end{array}$ (or resp. $1-2\\exp{\\left(-c_{1}n\\right)})$ , it holds that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\xi_{k,n}\\geq{\\frac{1}{2}};\\;\\zeta_{k,n}\\leq c_{2}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Fix a $\\delta\\in(0,1)$ . With probability at least $\\begin{array}{r}{1-\\delta-2\\exp\\left(-\\frac{c}{\\beta_{k}}\\frac{n}{k}\\right)}\\end{array}$ (or resp. $1-\\delta-e^{-n}-2e^{-c_{1}n})$ , it holds that: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{B}=\\mathcal{O}_{n,k}\\left(\\rho\\left(\\left\\|\\theta_{>k}^{*}\\right\\|_{\\Sigma_{>k}}^{2}+\\left(\\frac{s_{1}\\left(\\mathbf{A}_{k}\\right)}{n}\\right)^{2}\\left\\|\\theta_{\\leq k}^{*}\\right\\|_{\\Sigma_{\\leq k}^{-1}}^{2}\\right)\\right)}\\\\ &{\\quad=\\mathcal{O}_{n,k}\\left(\\rho\\left(k^{-\\left(2r+a\\right)}+n^{-2}\\left(e^{-b n}\\right)^{2}\\left\\|\\theta_{\\leq k}^{*}\\right\\|_{\\Sigma_{\\leq k}^{-1}}^{2}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where the last inequality comes from Proposition G.4. For $b<a$ , set $\\begin{array}{r}{t\\;{\\stackrel{\\mathrm{def.}}{=}}\\;{\\frac{b}{a}}<1.}\\end{array}$ .By Proposition G.4 again, we have $\\rho=\\Theta_{k,n}$ (1) with probability at least $\\textstyle1-o_{n}\\left({\\frac{1}{n}}\\right)$ (or resp. $1-e^{-n}$ ). Set $s\\ {\\stackrel{\\mathrm{def.}}{=}}\\ {\\frac{2r\\!+\\!a}{a}}$ be the source coefficient and we write: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{B=\\mathcal{O}_{n,k}\\left(e^{-\\frac{k}{2}(2r+\\alpha)n}+\\left(e^{-\\frac{k}{2}\\alpha}\\right)^{2}\\sum_{t=1}^{k}e^{\\left(\\alpha-2r/t\\right)}\\right)}\\\\ {=\\mathcal{O}_{n,k}\\left(e^{-\\sin{\\phi}}+e^{-2\\beta n}\\sum_{t=1}^{\\frac{k}{2}}e^{\\left(\\alpha-2r/t\\right)}\\right)}\\\\ {=\\left\\{\\begin{array}{l l}{\\mathcal{O}_{n,k}\\left(e^{-\\sin{\\phi}}+e^{-2\\beta n}\\cdot e^{\\left(\\alpha-2r/\\frac{\\sin{\\phi}}{2}\\right)t}\\right)}&{,\\;n>2r}\\\\ {\\mathcal{O}_{n,k}\\left(e^{-\\sin{\\phi}}+e^{-2\\beta n}\\cdot\\frac{\\delta n}{2}\\right)}&{,\\;n<2r}\\end{array}\\right.}\\\\ {=\\left\\{\\begin{array}{l l}{\\mathcal{O}_{n,k}\\left(e^{-\\sin{\\phi}}+e^{-2\\beta n}\\cdot\\frac{\\delta n}{2}\\right)}&{,\\;n<3r}\\\\ {\\mathcal{O}_{n,k}\\left(e^{-\\sin{\\phi}}+e^{-2\\beta n}\\right)}&{,\\;n<2r}\\end{array}\\right.}\\\\ {=\\left\\{\\begin{array}{l l}{\\mathcal{O}_{n,k}\\left(e^{-\\sin{\\phi}}+e^{-\\sin{\\phi}}\\right)}&{,\\;n>2r}\\\\ {\\mathcal{O}_{n,k}\\left(e^{-\\sin{\\phi}}+e^{-2\\beta n}\\right)}&{,\\;n<2r}\\end{array}\\right.}\\\\ {=\\mathcal{O}_{n,k}\\left(e^{-\\sin{\\phi}}+\\frac{e^{-\\sin{\\phi}}}{2}\\right)}&{,\\;n\\in\\mathcal{O}_{n}}\\\\ {=\\left\\{\\mathcal{O}_{n,k}\\left(e^{-\\sin{\\phi}}+1\\right)\\right\\}}&{,\\;n\\in\\mathcal{O}_{n}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "D.2 Bias under weak ridge ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Proposition D.5 (Asymptotic upper bound of bias under weak/effectively no ridge for $s\\ >\\ 1_{.}$ ). Suppose Assumption $(G F)$ (or resp. $(I F)$ ) holds, and the source coefficient $s>1$ . Then with high probability, the bias has decay: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{B}=\\tilde{\\mathcal{O}}_{n}\\left(\\lambda_{n}^{\\operatorname*{min}\\left\\{s,2\\right\\}}\\right),\\;f o r\\;\\lambda=\\Omega_{n}\\left(\\lambda_{n}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Proof. By definition, rewrite the bias into: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{B}=\\left\\|\\boldsymbol{\\theta}^{*}-\\hat{\\boldsymbol{\\theta}}(\\mathbf{X}\\boldsymbol{\\theta}^{*})\\right\\|_{\\mathbf{\\hat{X}}}^{2}}\\\\ &{\\quad=\\left\\|\\boldsymbol{\\theta}^{*}-\\mathbf{X}^{\\top}(\\mathbf{X}\\mathbf{X}^{\\top}+n\\lambda\\mathbf{I}_{n})^{-1}(\\mathbf{X}\\boldsymbol{\\theta}^{*})\\right\\|_{\\mathbf{\\hat{X}}}^{2}}\\\\ &{\\quad=\\left\\|\\left(\\mathbf{I}_{p}-\\underbrace{\\mathbf{X}^{\\top}(\\mathbf{X}\\mathbf{X}^{\\top}+n\\lambda\\mathbf{I}_{n})^{-1}\\mathbf{X}}_{\\mathbf{P}_{\\lambda}}\\right)\\boldsymbol{\\theta}^{*}\\right\\|_{\\Sigma}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Denote $\\mathbf{P}_{\\lambda}\\ {\\stackrel{\\mathrm{def.}}{=}}\\ \\mathbf{X}^{\\top}(\\mathbf{X}\\mathbf{X}^{\\top}+n\\lambda\\mathbf{I}_{n})^{-1}\\mathbf{X}\\in\\mathbb{R}^{p\\times p}$ . By Sherman-Morrison-Woodbury formula, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbf{I}_{p}-\\mathbf{P}_{\\lambda}=\\mathbf{I}_{p}-(n\\lambda)^{-1}\\mathbf{X}^{\\top}\\left(\\mathbf{I}_{n}+(n\\lambda)^{-1}\\mathbf{X}\\mathbf{X}^{\\top}\\right)^{-1}\\mathbf{X}=\\lambda\\left(\\lambda I_{p}+{\\frac{1}{n}}\\mathbf{X}^{\\top}\\mathbf{X}\\right)^{-1}=\\lambda\\left(\\lambda I_{p}+{\\hat{\\boldsymbol{\\Sigma}}}\\right)^{-1}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "is a positive definite matrix such that the matrix ${\\bf{I}}_{p}-{\\bf{P}}_{\\lambda}$ is monotonic increasing in $\\lambda$ , that is: ", "page_idx": 33}, {"type": "equation", "text": "$$\n0\\prec\\mathbf{I}_{p}-\\mathbf{P}_{\\lambda}\\preccurlyeq\\mathbf{I}_{p}-\\mathbf{P}_{\\tilde{\\lambda}}\\preccurlyeq\\mathbf{I}_{p},\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "for all $\\tilde{\\lambda}\\geq\\lambda>0$ , since the map $\\lambda\\mapsto{\\frac{\\lambda}{\\lambda+x}}$ is monotone. Hence, for $t\\in[1,\\operatorname*{min}\\{2,s\\})$ 13, ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f^{\\prime\\prime}=\\mathrm{L}[\\gamma^{\\prime\\prime},\\lambda,\\gamma^{\\prime\\prime},\\lambda,\\gamma^{\\prime}]}\\\\ &{=\\left\\lVert\\mathrm{E}^{\\top}[\\mathcal{X}_{0},\\mathbf{P}_{0}][\\mathbf{E}_{0}^{\\top}]\\right\\rVert_{q}^{2}}\\\\ &{\\leq\\left\\lVert\\mathrm{E}^{\\top}[(\\mathbf{1}_{q}-\\mathbf{1}_{\\mathrm{S}})\\mathbf{1}_{q}^{\\top}][-1]\\right\\rVert_{q}^{2}}\\\\ &{=\\left\\lVert\\mathrm{E}^{\\top}[\\mathcal{X}_{0},\\mathbf{P}_{0}][\\mathbf{E}_{0}^{\\top}][-\\mathbf{1}_{q}^{\\top}][\\mathbf{1}_{q}^{\\top}-\\mathbf{1}_{q}^{\\top}][\\mathbf{2}_{q}^{\\top}][\\mathbf{1}_{q}^{2},\\mathbf{1}_{q}^{\\top}][\\mathbf{2}_{q}^{\\top}]\\right\\rVert_{q}^{2}}\\\\ &{\\ \\ \\ \\ \\ -\\left\\lVert\\mathrm{E}^{\\top}[\\mathbf{1}_{q},\\mathbf{P}_{0}][\\mathbf{2}_{q}^{\\top},\\mathbf{1}_{q}^{\\top}][\\mathbf{1}_{q}^{\\top}-\\mathbf{1}_{q}^{\\top}][\\mathbf{2}_{q}^{\\top},\\mathbf{1}_{q}^{\\top}][\\mathbf{2}_{q}^{\\top}][\\mathbf{1}_{q}^{\\top}][\\mathbf{2}_{q}^{\\top}]\\right\\rVert_{q}^{2}}\\\\ &{\\leq\\left\\lVert\\mathrm{E}^{\\top}[\\mathbf{1}_{q}^{\\top}(\\mathbf{1}_{q}-\\mathbf{1}_{q})\\mathbf{1}_{q}^{\\top}][\\mathbf{2}_{q}^{\\top},\\mathbf{1}_{q}^{\\top}][\\mathbf{2}_{q}^{\\top}][\\mathbf{1}_{q}-\\mathbf{1}_{q}^{\\top}][\\mathbf{2}_{q}^{\\top}][\\mathbf{1}_{q}^{\\top}-\\mathbf{1}_{q}^{\\top}][\\mathbf{2}_{q}^{\\top}][\\mathbf{1}_{q}^{\\top}][\\mathbf{2}_{q}^{\\top}]\\right\\rVert_{q}^{2}}\\\\ &{\\leq\\left\\lVert\\mathrm{E}^{\\top}[\\mathbf{1}_{q}^{\\top \n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where we rewrite line (40) in line (42); we apply the monotonicity from line (41) in line (43); we use the fact that $\\|\\mathbf{M}_{1}\\mathbf{M}_{2}\\|_{\\mathrm{op}}=\\|\\mathbf{M}_{2}\\mathbf{M}_{1}\\|_{\\mathrm{op}}$ for symmetric matrices $\\mathbf{M}_{1},\\mathbf{M}_{2}$ in line (44); we use Lemma H.17 to pull out the power $(t-1)$ from the matrix to its operator norm in line (45); we use the fact that $\\left\\|\\mathbf{M}_{1}^{1/2}\\mathbf{M}_{2}^{1/2}\\right\\|_{\\mathrm{op}}^{2}=\\left\\|\\mathbf{M}_{1}^{1/2}\\mathbf{M}_{2}^{1/2}(\\mathbf{M}_{1}^{1/2}\\mathbf{M}_{2}^{1/2})^{\\top}\\right\\|_{\\mathrm{op}}=\\left\\|\\mathbf{M}_{1}^{1/2}\\mathbf{M}_{2}\\mathbf{M}_{1}^{1/2}\\right\\|_{\\mathrm{op}}$ for any positive definite symmetric matrices $^{14}\\,\\mathbf{M}_{1},\\mathbf{M}_{2}$ in line (46); by the monotonicity from line (41) again and the fact that $\\pmb{\\Sigma}^{1/2}$ is positive definite, we choose $\\tilde{\\lambda}\\geq\\lambda\\succcurlyeq\\lambda_{n}$ to be strong in the sense of polynomial/exponential eigen-decay in line (47): $\\widetilde{\\lambda}=\\Theta_{n}\\left(n^{-1-a}\\right)$ for polynomial case and $\\tilde{\\lambda}=\\Theta_{n}\\left(e^{-a n}\\right)$ for exponential case. ", "page_idx": 34}, {"type": "text", "text": "Let $\\begin{array}{r l r l r}{{\\bf v}}&{{}\\in}&{\\mathbb{S}^{p-1}}&{}&{{}\\subset}&{\\mathbb{R}^{p}}\\end{array}$ be a unit vector such that $\\begin{array}{r l}{\\left\\lVert\\Sigma^{1/2}(\\mathbf{I}_{p}-\\mathbf{P}_{\\tilde{\\lambda}})\\Sigma^{1/2}\\right\\rVert_{\\mathrm{op}}^{2}}&{{}=}\\end{array}$ $\\left\\|\\Sigma^{1/2}(\\mathbf{I}_{p}-\\mathbf{P}_{\\tilde{\\lambda}})\\Sigma^{1/2}\\mathbf{v}\\right\\|_{2}^{2}$ . By definition, we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\left\\|\\pmb{\\Sigma}^{1/2}\\mathbf{v}\\right\\|_{\\pmb{\\Sigma}^{-1}}^{2}=\\|\\mathbf{v}\\|_{2}^{2}=1<\\infty,\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "hence the vector $\\Sigma^{1/2}\\mathbf{v}$ lies on the interpolation space $\\mathcal{H}^{2}$ . Note that the expression ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\left\\|\\Sigma^{1/2}(\\mathbf{I}_{p}-\\mathbf{P}_{\\widetilde{\\lambda}})\\Sigma^{1/2}\\right\\|_{\\mathbf{op}}^{2}=\\left\\|\\Sigma^{1/2}(\\mathbf{I}_{p}-\\mathbf{P}_{\\widetilde{\\lambda}})\\Sigma^{1/2}\\mathbf{v}\\right\\|_{2}^{2}=\\left\\|(\\mathbf{I}_{p}-\\mathbf{P}_{\\widetilde{\\lambda}})\\left(\\Sigma^{1/2}\\mathbf{v}\\right)\\right\\|_{\\Sigma}^{2}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "is just the bias term of another task $\\Sigma^{1/2}\\mathbf{v}$ with source coefficient 2 and ridge $\\tilde{\\lambda}=\\Omega_{n}\\left(\\lambda_{n}\\right)$ on the same dataset $\\mathbf{X}$ . Apply Proposition D.2 with polynomial decay and Proposition D.3 with exponential decay on that new task with strong ridge $\\tilde{\\lambda}$ , with high probability, the above term has a decay rate: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\Sigma^{1/2}(\\mathbf{I}_{p}-\\mathbf{P}_{\\widetilde{\\lambda}})\\Sigma^{1/2}\\right\\|_{\\mathrm{op}}^{2}=\\left\\|(\\mathbf{I}_{p}-\\mathbf{P}_{\\widetilde{\\lambda}})\\left(\\Sigma^{1/2}\\mathbf{v}\\right)\\right\\|_{\\Sigma}^{2}=\\widetilde{\\mathcal{O}}_{n}\\left(\\lambda_{n}^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Plug in line (48) into line (47) to obtain the decay rate of the original bias: with high probability: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathcal{B}={\\mathcal O}_{n}\\left(\\left(\\left\\|\\mathbf{\\boldsymbol{\\Sigma}}^{1/2}(\\mathbf{I}_{p}-\\mathbf{P}_{\\widetilde{\\lambda}})\\mathbf{\\boldsymbol{\\Sigma}}^{1/2}\\right\\|_{\\mathrm{op}}^{2}\\right)^{t/2}\\right)=\\tilde{\\mathcal{O}}_{n}\\left(\\left(\\boldsymbol{\\lambda}_{n}^{2}\\right)^{t/2}\\right)=\\tilde{\\mathcal{O}}_{n}\\left(\\boldsymbol{\\lambda}_{n}^{t}\\right).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "This holds for any $t\\in[1,\\operatorname*{min}\\{s,2\\})$ , hence we conclude that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{B}=\\tilde{\\mathcal{O}}_{n}\\left(\\lambda_{n}^{\\operatorname*{min}\\left\\{s,2\\right\\}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "For polynomial eigen-decay, we can still apply the Master inequality for $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ to obtain another upper bound: ", "page_idx": 35}, {"type": "text", "text": "Proposition D.6 (Asymptotic upper bound of bias under weak/no ridge for any $s>0$ ). Suppose: $\\lambda_{k}\\;\\;{\\bar{=}}\\;\\;\\Theta_{k}\\;\\bigl(k^{-1-a}\\bigr)$ , $\\bar{|\\theta_{k}^{*}|}=\\bar{O_{k}\\bar{(k^{-r})}}$ and $\\lambda\\,=\\,\\Theta_{k}\\left(k^{-b}\\right)$ for some $a,b,r\\,>\\,0$ . Fix a constant $\\delta\\in(0,1)$ . If, additionally, Assumption $^{\\prime}G F)$ (or resp. $(I F)$ ) holds, then, with probability at least $\\begin{array}{r}{1-\\delta-{\\mathcal O}_{n}\\left(\\frac{1}{\\log n}\\right)}\\end{array}$ (or resp. $1-\\delta-3e^{-c_{1}n},$ ), the bias is bounded by: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\boldsymbol{B}=\\tilde{\\mathcal{O}}_{n}\\left(\\boldsymbol{n}^{-C}\\right)\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where ", "page_idx": 35}, {"type": "text", "text": "(or resp. ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{C=\\left(\\operatorname*{min}\\{2(r-a),2-a\\}\\right)_{+},\\ b\\in[1+a,\\infty]}\\\\ &{}\\\\ &{C=\\operatorname*{min}\\{2r+a,2(1+a)\\},\\ b\\in[1+a,\\infty].)}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Proof. The proof is similar to Proposition D.2. Take $k\\,=\\,\\Theta_{\\boldsymbol{k},n}\\,\\Big(\\frac{n}{\\log n}\\Big)$ (or resp. $k\\,=\\,n/c$ for some constant $c>1$ ) and control $\\xi,\\zeta$ by Propositions G.1 and G.2. Now, by Proposition G.3, with probability at least $1-{\\frac{c_{1}}{\\log n}}$ , since $b\\geq1+a$ , ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\rho=\\tilde{\\mathcal{O}}_{n,k}\\left(n^{a}\\right).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "(or resp. $\\rho=\\mathcal{O}_{n,k}$ (1).) Now by Proposition D.1, since $b\\in[1+a,\\infty]$ and we choose $t=1$ , with high probability, ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathcal{B}=\\tilde{\\mathcal{O}}_{n,k}\\left(\\rho^{3}n^{-\\operatorname*{min}\\{t(2r+a),2(1+a t)\\}}\\right).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Plug in $\\rho=\\tilde{\\mathcal{O}}_{n,k}\\left(n^{a}\\right)$ and $t=1$ to obtain: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathcal{B}=\\tilde{\\mathcal{O}}_{n,k}\\left(n^{-\\operatorname*{min}\\{2(r-a),2-a\\}}\\right).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "The index in the above bound can be negative, causing the upper bound vacuous. By line (41), since $\\mathcal{B}=\\|(\\mathbf{I}_{p}-\\mathbf{P}_{\\lambda})\\pmb{\\theta}^{*}\\|_{\\Sigma}^{2}\\leq\\|\\pmb{\\theta}^{*}\\|_{\\Sigma}^{2}$ , there is a trivial bound that $B=\\ensuremath{\\mathcal{O}_{n}}$ (1). Combining both results, we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathcal{B}=\\tilde{\\mathcal{O}}_{n,k}\\left(n^{-(\\operatorname*{min}\\{2(r-a),2-a\\})+}\\right).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "D.3 Variance with polynomial eigen-decay ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Proposition D.7 (Asymptotic upper bound of the variance term with strong ridge). Suppose $\\lambda_{k}\\ {\\stackrel{.}{=}}\\ \\Theta_{k}\\left(k^{-1-a}\\right)$ , $|\\dot{\\theta}_{k}^{*}|\\,=\\,\\Theta_{k}\\,(\\bar{k}^{\\bar{-}r})$ , and $\\lambda=\\Theta_{n}\\left(n^{-b}\\right)$ for some $a,r\\,>\\,0$ and $b\\in(0,a+1)$ . Furthermore, suppose Assumption $(G F)$ (or resp. $(I F)$ ) holds. With probability at least $\\textstyle1-o_{n}\\left({\\frac{1}{n}}\\right)$ (or resp. $1-3e^{-c_{1}n}.$ ), the variance term is bounded: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathcal{V}=\\mathcal{O}_{n}\\left(n^{\\frac{b}{(1+a)}-1}\\right)\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "for $n\\in\\mathbb N$ large enough. ", "page_idx": 35}, {"type": "text", "text": "Proof. We divide the case where Assumption (GF) or (IF) holds. ", "page_idx": 36}, {"type": "text", "text": "Suppose Assumption (GF) holds. By Proposition G.2, there exists constants $c>1$ , $c_{1}>0$ such that, for $k\\leq{\\frac{n}{c\\log n}}$ , with probability at least $\\bar{1}-8e^{-c_{1}n/k}$ , it holds that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\xi_{n,k},\\;\\zeta_{n,k}=\\Theta_{n,k}\\left(1\\right).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Also, by definition, it always holds that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{Tr}[\\mathbf{X}_{>k}\\pmb{\\Sigma}_{>k}\\mathbf{X}_{>k}^{\\top}]}{n\\,\\mathrm{Tr}[\\pmb{\\Sigma}_{>k}^{2}]}=\\frac{\\frac{1}{n}\\sum_{i=1}^{n}\\left\\|\\mathbf{\\Sigma}_{\\mathbf{X}^{2}}^{1/2}\\mathbf{x}_{i}\\right\\|_{2}^{2}}{\\mathrm{Tr}[\\pmb{\\Sigma}_{>k}^{2}]}\\leq\\operatorname*{sup}_{\\mathbf{x}}\\frac{\\left\\|\\pmb{\\Sigma}^{1/2}\\mathbf{x}\\right\\|_{2}^{2}}{\\mathrm{Tr}[\\pmb{\\Sigma}_{>k}^{2}]}=\\beta_{k}=\\Theta_{n,k}\\left(1\\right).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "By Proposition C.7, ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathcal{V}/\\sigma^{2}=\\mathcal{O}_{n,k}\\left(\\rho^{2}\\left(\\frac{k}{n}+\\frac{r_{k}(\\Sigma)^{2}}{n R_{k}(\\Sigma)}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Let c > 1 be the constant in Proposition G.3. Choose k = \u230a1+ban\u230b\u2264c long n for $n\\in\\mathbb N$ large enough. By Proposition G.3, with probability at least $\\textstyle1-o_{n}\\left({\\frac{1}{n}}\\right)$ , ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\rho\\ {\\stackrel{\\mathrm{def.}}{=}}\\ {\\frac{n\\left\\|\\mathbf{\\boldsymbol{\\Sigma}}_{>k}\\right\\|_{\\mathrm{op}}+s_{1}(\\mathbf{\\boldsymbol{\\mathbf{A}}}_{k})}{s_{n}(\\mathbf{\\boldsymbol{\\mathbf{A}}}_{k})}}={\\mathcal{O}}_{n,k}\\left({\\frac{n\\lambda_{k}+n\\lambda}{n\\lambda}}\\right)={\\mathcal{O}}_{n,k}\\left(1\\right)\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "by the choice of $b\\in(0,1+a)$ . Combine the result with Lemma H.12, with probability at least $\\textstyle1^{\\stackrel{-}{-}}o_{n}\\left({\\frac{1}{n}}\\right)$ , we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathcal{V}=\\mathcal{O}_{n,k}\\left(\\frac{k}{n}+\\frac{k^{2}}{k n}\\right)=\\mathcal{O}_{n,k}\\left(\\frac{n^{\\frac{b}{1+a}}}{n}\\right)=\\mathcal{O}_{n,k}\\left(n^{\\frac{b}{1+a}-1}\\right)\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "to conclude the claim. ", "page_idx": 36}, {"type": "text", "text": "Suppose Assumption (IF) holds. The argument follows analogously. As in Proposition G.2, choose $\\begin{array}{r}{k=\\lfloor\\frac{b}{1+a}\\rfloor<n\\bar{/}c}\\end{array}$ for $n\\in\\mathbb N$ large enough. With probability at least $1-2e^{-c_{1}n}$ , we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\xi_{n,k},\\;\\zeta_{n,k}=\\Theta_{n,k}\\left(1\\right).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "By Proposition G.3, with probability at least $1-e^{-n}$ , ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\rho=\\Theta_{n,k}\\left(1\\right)\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "by the choice of $b\\in(0,a+1)$ . By possibly choosing a new constant $c_{1}>0$ , with probability at least $\\bar{1}-3e^{-c_{1}n}$ , we have $\\xi,\\zeta,\\rho$ bounded. The rest of the argument follows similarly. \u53e3 ", "page_idx": 36}, {"type": "text", "text": "Proposition D.8 (Asymptotic upper bound of variance term with weak/no ridge). Suppose $\\lambda_{k}=$ $\\Theta_{k}\\left(\\boldsymbol{k}^{-1-a}\\right)$ , $|\\theta_{k}^{*}|=\\bar{\\Theta}_{k}\\,\\bar{(}k^{-r})$ , and $\\lambda=\\Theta_{n}\\left(n^{-b}\\right)$ for some $a,r>0$ and $b\\in[a+1,\\infty]$ . Furthermore, suppose Assumption $(G F)$ (or resp. $(I F)$ ) holds. With probability at least $\\textstyle1-o_{n}\\left({\\frac{1}{\\log n}}\\right)$ (or resp. $1-3e^{-c_{1}n})$ , the variance term is bounded: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathcal{V}=\\mathcal{O}_{n}\\left(n^{2a}\\right).\\quad(o r\\,r e s p.\\ \\mathcal{V}=\\mathcal{O}_{n}\\left(1\\right).)\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Proof. We divide the case where Assumption (GF) or (IF) holds. ", "page_idx": 36}, {"type": "text", "text": "Suppose Assumption (GF) holds. By Proposition G.2, there exists constants $c>1$ , $c_{1}>0$ such that, for c long n, with probability at least 1 \u22128e\u2212c1n/k, it holds that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\xi_{n,k},\\;\\zeta_{n,k}=\\Theta_{n,k}\\left(1\\right).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Choose $\\textstyle k=\\left\\lfloor{\\frac{n}{c\\log n}}\\right\\rfloor$ . Then by Proposition G.3, with probability at least $\\textstyle1-o_{n}\\left({\\frac{1}{\\log n}}\\right)$ , it holds that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\rho=\\tilde{\\mathcal{O}}_{n,k}\\left(\\frac{n^{-a}+n^{-a}+n^{-b+1}}{n^{-2a}+n^{-b+1}}\\right)=\\tilde{\\mathcal{O}}_{n,k}\\left(n^{a}\\right)\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "by the choice of $b$ . Moreover, since $\\textstyle k={\\frac{n}{c\\log n}}$ , by Proposition C.7, we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathcal{V}/\\sigma^{2}={\\mathcal O}_{n,k}\\left(\\rho^{2}\\left(\\frac{k}{n}+\\frac{r_{k}(\\Sigma)^{2}}{n R_{k}(\\Sigma)}\\right)\\right)=\\tilde{\\mathcal{O}}_{n,k}\\left(n^{2a}\\frac{k}{n}\\right)=\\tilde{\\mathcal{O}}_{n,k}\\left(n^{2a}\\right).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Suppose Assumption (IF) holds, choose $k=\\lfloor n/c\\rfloor$ for some constant $c>1$ as in Propositions G.3. with probability at least $1-3e^{-c_{1}n}$ , it holds that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\xi,\\;\\zeta,\\;\\rho=\\Theta_{n,k}\\left(1\\right).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Then by Proposition C.7, we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathcal{V}/\\sigma^{2}=\\mathcal{O}_{n,k}\\left(\\rho^{2}\\left(\\frac{k}{n}+\\frac{r_{k}(\\Sigma)^{2}}{n R_{k}(\\Sigma)}\\right)\\right)=\\tilde{\\mathcal{O}}_{n,k}\\left(\\frac{k}{n}\\right)=\\tilde{\\mathcal{O}}_{n,k}\\left(1\\right).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "D.4 Variance with exponential decay ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Proposition D.9 (Asymptotic upper bound of the variance term with strong ridge). Suppose $\\lambda_{k}=$ $\\Theta_{k}\\left(e^{-a k}\\right)$ , $|\\theta_{k}^{*}|=\\Theta_{k}^{\\ }\\left(e^{-r k}\\right)$ , and $\\begin{array}{r}{\\lambda=\\Theta_{n}\\left(\\frac{1}{n}e^{-b n}\\right)}\\end{array}$ for some $a,r>0$ and $b\\in(0,a)$ . Furthermore, suppose Assumption $(G F)$ (or resp. $(I F)$ ) holds. With probability at least $\\textstyle{1-o_{n}\\left({\\frac{1}{n}}\\right)}$ (or resp. $1-3e^{-c_{1}n})$ , the variance term is bounded: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathcal{V}=\\mathcal{O}_{n}\\left(n^{\\frac{b}{a}-1}\\right)\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "for $n\\in\\mathbb N$ large enough. ", "page_idx": 37}, {"type": "text", "text": "Proof. The argument is similar to Proposition D.7. The difference is the bound of $\\rho$ and the second term in Proposition C.7: if $\\xi,\\zeta=\\Theta_{n,k}$ (1), by Lemma H.12, ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{V}/\\sigma^{2}\\leq\\rho^{2}\\left(\\zeta^{2}\\xi^{-1}\\displaystyle\\frac{k}{n}+\\frac{\\mathrm{Tr}[\\mathbf{Z}_{>k}\\boldsymbol{\\Sigma}_{>k}^{2}\\mathbf{Z}_{>k}^{\\top}]}{n\\,\\mathrm{Tr}[\\boldsymbol{\\Sigma}_{>k}^{2}]}\\displaystyle\\frac{r_{k}(\\boldsymbol{\\Sigma})^{2}}{n R_{k}(\\boldsymbol{\\Sigma})}\\right)}\\\\ &{\\quad\\quad=\\mathcal{O}_{n,k}\\left(\\rho^{2}\\left(\\displaystyle\\frac{k}{n}+\\displaystyle\\frac{1}{n^{2}}\\right)\\right)}\\\\ &{\\quad\\quad=\\mathcal{O}_{n,k}\\left(\\rho^{2}\\displaystyle\\frac{k}{n}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Choose $\\begin{array}{r}{k=\\left\\lfloor\\frac{b}{a}\\right\\rfloor<1}\\end{array}$ . Combine Proposition G.4 and the above inequality to obtain the result. ", "page_idx": 37}, {"type": "text", "text": "E Matching lower bound ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "In this section, we provide the proof for the matching lower bounds presented on Table 1. A quick summary of propositions can be found in Table 4. ", "page_idx": 38}, {"type": "table", "img_path": "IMlDpZmLnL/tmp/3cf00b23a86401b4c73e408a922f022cda2c7ce45363a7e734717f470241eca0.jpg", "table_caption": [], "table_footnote": ["Table 4: Quick summary of propositions used for proving the matching lower bounds on Table 1. "], "page_idx": 38}, {"type": "text", "text": "E.1 Bias under weak ridge and source coefficient between 1 and 2 ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Proposition E.1 (Asymptotic lower bound of bias, Proposition 4.4 in [31], Theorem 3.4 in [33]). Suppose Assumption $(P E)/(E E)$ holds. Then ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\|\\theta^{*}\\|_{\\Sigma^{1-s}}\\leq1}\\mathcal{B}=\\Omega_{n}\\left(\\lambda_{n}^{s}\\right).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where s is the source coefficient. ", "page_idx": 38}, {"type": "text", "text": "Remark E.2. Note that for both polynomial (with $2r\\neq2+a)$ and exponential decays (with $2r\\neq a$ ), by Proposition D.5, we have $\\boldsymbol{\\mathcal{B}}=\\mathcal{O}\\left(\\lambda_{n}^{\\operatorname*{max}\\{s,2\\}}\\right)$ for source coefficient $s\\geq1$ . Hence when $s\\in[1,2]$ , the upper and lower bound matches. ", "page_idx": 38}, {"type": "text", "text": "Proof. Recall the expression of the bias in line (40): ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathcal{B}=\\left\\|\\hat{\\pmb{\\theta}}(\\mathbf{X}\\pmb{\\theta}^{*})-\\pmb{\\theta}^{*}\\right\\|_{\\Sigma}^{2}=\\|(\\mathbf{I}_{p}-\\mathbf{P}_{\\lambda})\\pmb{\\theta}^{*}\\|_{\\Sigma}^{2}\\,,\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where $\\mathbf{P}\\ {\\stackrel{\\mathrm{def.}}{=}}\\ \\mathbf{X}^{\\top}(\\mathbf{X}\\mathbf{X}^{\\top}+n\\lambda\\mathbf{I}_{p})^{-1}\\mathbf{X}$ . Recall the source coefficient $s$ satisfies: $\\|\\theta^{*}\\|_{\\Sigma^{1-s}}<\\infty$ . By definition of operator norm, we have: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{\\Vert\\theta^{*}\\Vert_{\\mathbf{L}^{1-s}}\\leq1}{\\operatorname*{sup}}B=\\underset{\\Vert\\theta^{*}\\Vert_{\\mathbf{S}^{1-s}}\\leq1}{\\operatorname*{sup}}\\Vert(\\mathbf{I}_{p}-\\mathbf{P}_{\\lambda})\\theta^{*}\\Vert_{\\mathbf{S}}^{2}}\\\\ &{=\\underset{\\Vert\\theta^{*}\\Vert_{\\mathbf{S}^{1-s}}\\leq1}{\\operatorname*{sup}}\\left\\Vert\\Sigma^{1/2}(\\mathbf{I}_{p}-\\mathbf{P}_{\\lambda})\\Sigma^{(s-1)/2}\\Sigma^{(1-s)/2}\\theta^{*}\\right\\Vert_{2}^{2}}\\\\ &{=\\left\\Vert\\Sigma^{1/2}(\\mathbf{I}_{p}-\\mathbf{P}_{\\lambda})\\Sigma^{(s-1)/2}\\right\\Vert_{\\mathrm{op}}^{2}}\\\\ &{=\\left\\Vert\\Sigma^{s/2}-\\Sigma^{1/2}\\mathbf{P}_{\\lambda}\\Sigma^{(s-1)/2}\\right\\Vert_{\\mathrm{op}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "By [43], $\\|\\mathbf{M}_{1}-\\mathbf{M}_{2}\\|_{\\mathrm{op}}\\,\\geq\\,s_{n+1}(\\mathbf{M}_{1})$ for any operator $\\mathbf{M}_{2}$ with rank at most $n$ . Note that $\\mathbf{P}_{\\lambda}\\in$ $\\mathbb{R}^{p\\times p}$ is of rank $n$ , hence ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\|\\theta^{*}\\|_{\\Sigma^{1-s}}\\leq1}B=\\left\\|\\Sigma^{s/2}-\\Sigma^{1/2}\\mathbf{P}_{\\lambda}\\Sigma^{\\left(s-1\\right)/2}\\right\\|_{\\mathrm{op}}^{2}\\geq\\left(\\lambda_{n+1}^{s/2}\\right)^{2}=\\Omega_{n}\\left(\\lambda_{n}^{s}\\right).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "E.2 Independent features and prior signs ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Instead of bounding the bias from below in the worst case scenario, we can also bound it in an average sense. In this case, we need to pose an assumption on the target coefficient $\\theta^{*}$ . ", "page_idx": 38}, {"type": "text", "text": "Assumption (PS) (Prior Signs). Assume target coefficient $\\pmb{\\theta}^{*}\\in\\mathbb{R}^{p}$ is drawn from a distribution $\\vartheta$ where its entries $\\theta_{k}^{*:}$ \u2019s are drawn independently of each other and have the same distributions as the random variables $-\\theta_{k}^{*}$ \u2019s. ", "page_idx": 38}, {"type": "text", "text": "Proposition E.3 (Asymptotic lower bound of bias with polynomial decay and independent features). Suppose Assumption $(P S)$ holds and $\\lambda_{k}\\,=\\,\\Theta_{k}\\,\\big(\\dot{k}^{-1-a}\\big),\\;\\mathbb{E}_{\\pmb{\\theta}^{*}}\\,\\big[({\\theta}_{k}^{*})^{2}\\big]\\,=\\,\\mathcal{O}_{k}^{\\,^{\\star}}\\big(k^{-2r}\\big)$ and $\\lambda=\\Theta_{n}\\left(n^{-b}\\right)$ for some $a,b,r>0$ . Fix a constant $\\delta\\,\\in\\,(0,1)$ . If, additionally, Assumption $(I\\!F,$ ) holds, then, with probability at least $1-\\delta-11e^{-c_{1}n}$ , the bias is bounded by: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\theta^{*}}\\left[\\beta\\right]=\\tilde{\\Omega}_{n}\\left(n^{-b\\operatorname*{min}\\left\\{s,2\\right\\}}\\right)\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "for $\\begin{array}{r}{s\\ {\\stackrel{\\mathrm{def.}}{=}}\\ {\\frac{2r+a}{1+a}}}\\end{array}$ ", "page_idx": 39}, {"type": "text", "text": "Proof. By Lemma H.14, it holds that: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\theta^{*}}\\left[\\boldsymbol{\\mathcal{B}}\\right]\\geq\\sum_{l=1}^{p}\\frac{\\lambda_{l}\\mathbb{E}_{\\theta^{*}}\\left[\\left(\\theta_{l}^{*}\\right)^{2}\\right]}{\\left(1+\\lambda_{l}s_{n}(\\mathbf{A}_{-l})^{-1}\\left\\Vert\\mathbf{z}^{(l)}\\right\\Vert_{2}^{2}\\right)^{2}}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Hence, we want to bound the term $s_{n}(\\mathbf{A}_{-l})^{-1}$ and $\\left\\|\\mathbf{z}^{(l)}\\right\\|_{2}^{2}$ from above. ", "page_idx": 39}, {"type": "text", "text": "If Assumption (IF) holds, then apply Lemma G.7 on $\\mathbf{A}_{-l}$ instead of $\\mathbf{A}$ , $l=1,...,p$ , with probability at least $1-2e^{-n}$ , ", "page_idx": 39}, {"type": "equation", "text": "$$\ns_{n}(\\mathbf{A}_{-l})=\\left\\{\\Omega_{n}\\left(n\\lambda_{n}\\right),\\right.\\quad\\mathrm{if~}l>n=\\Omega_{n}\\left(n\\lambda_{n}\\right)=\\Omega_{n}\\left(\\mathrm{Tr}[\\Sigma_{>n}]\\right).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "By Lemma H.7, with probability at least $1-2e^{-n}$ , we have, ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{z}_{l}\\right\\|_{2}^{2}\\leq c_{1}n,\\forall l=1,...,p.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Plug in the above two inequalities, by union bound, with probability at least $1-4e^{-n}$ , ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\frac{\\lambda_{l}\\mathbb{E}\\left[(\\theta_{l}^{*})^{2}\\right]}{(1+\\lambda_{l}s_{n}(\\mathbf{A}_{-l})^{-1}\\left\\|\\mathbf{z}_{l}\\right\\|_{2}^{2})^{2}}=\\Omega_{n}\\left(\\frac{\\lambda_{l}\\mathbb{E}\\left[(\\theta_{l}^{*})^{2}\\right]}{\\left(1+\\frac{n\\lambda_{l}}{\\lambda_{n+1}r_{n}}\\right)^{2}}\\right)\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where rk = \u2225Tr\u03a3[>\u03a3k>\u2225kop] for any k. If we choose k = n/c for some constant c > 1 such that Ak = $\\Theta_{n}\\left(n\\lambda_{n}\\right)$ , then $\\mathrm{Tr}[\\dot{\\Sigma}_{>k}]=\\Theta_{n}\\left(\\mathrm{Tr}[\\Sigma_{>n}]\\right)$ . Hence ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\frac{\\lambda_{l}\\mathbb{E}\\left[(\\theta_{l}^{*})^{2}\\right]}{(1+\\lambda_{l}s_{n}(\\mathbf{A}_{-l})^{-1}\\left\\|\\mathbf{z}_{l}\\right\\|_{2}^{2})^{2}}=\\Omega_{n}\\left(\\frac{\\lambda_{l}\\mathbb{E}\\left[(\\theta_{l}^{*})^{2}\\right]}{\\left(1+\\frac{n\\lambda_{l}}{\\lambda_{k+1}r_{k}}\\right)^{2}}\\right)\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "By Lemma H.15, with probability at least $1-8e^{-n}$ , ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\theta^{*}}\\left[\\mathcal{B}\\right]=\\Omega_{n}\\left(\\underbrace{\\sum_{l=1}^{p}\\frac{\\lambda_{l}\\mathbb{E}\\left[(\\theta_{l}^{*})^{2}\\right]}{\\left(1+\\frac{n\\lambda_{l}}{\\lambda_{k+1}r_{k}}\\right)^{2}}}_{B}\\right),\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where $\\underline{{\\boldsymbol{B}}}$ is defined as in Theorem H.16. since $\\begin{array}{r}{r_{k}=\\frac{\\mathrm{Tr}\\left[\\Sigma_{>k}\\right]}{\\left\\Vert\\Sigma_{>k}\\right\\Vert_{\\mathrm{op}}}=\\Theta_{k}\\left(\\frac{k\\lambda_{k}}{\\lambda_{k}}\\right)=\\Theta_{k}\\left(k\\right)}\\end{array}$ , hence the fraction $\\begin{array}{r}{\\frac{r_{k}}{n}=\\Theta_{n,k}\\left(1\\right)}\\end{array}$ and $r_{k}$ satisfies the condition of Theorem H.16. By Theorem H.16, we have upper bound $\\overline{{B}}$ matching the lower bound $\\underline{{\\boldsymbol{\\mathcal{B}}}}$ : ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\underline{{B}}=\\Theta_{n}\\left(\\underbrace{\\|\\theta_{>k}^{*}\\|_{\\Sigma_{>k}}^{2}+\\left(\\frac{n\\lambda+\\mathrm{Tr}[\\Sigma_{>k}]}{n}\\right)^{2}\\left\\|\\theta_{\\leq k}^{*}\\right\\|_{\\Sigma_{\\leq k}^{-1}}^{2}}_{\\overline{{B}}}\\right).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "By Propositions C.6, D.2 and D.6, we have, with probability at least $1-\\delta-3e^{c_{1}n}$ , ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\mathcal{B}=\\mathcal{O}_{n}\\left(\\overline{{B}}\\right)=\\mathcal{O}_{n}\\left(n^{-b\\operatorname*{min}\\left\\{s,2\\right\\}}\\right).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "All together, with probability at least $1-\\delta-3e^{-c_{1}n}-8e^{-n}$ , ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\theta^{*}}\\left[\\boldsymbol{B}\\right]=\\Omega_{n}\\left(\\overline{{\\boldsymbol{B}}}\\right)=\\Omega_{n}\\left(n^{-b\\operatorname*{min}\\left\\{s,2\\right\\}}\\right).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Proposition E.4 (Asymptotic lower bound of bias with exponential decay and independent features under strong ridge). Suppose Assumption $(P S)$ holds and $\\begin{array}{r}{\\lambda_{k}^{\\,^{\\cdot}}=\\Theta_{k}\\left(e^{-a k}\\right)\\!,\\mathbb{E}_{\\theta^{\\ast}}\\left[(\\theta_{k}^{\\ast})^{2}\\right]=\\mathcal{O}_{k}\\left(e^{-2r k}\\right)}\\end{array}$ and $\\lambda=\\Theta_{n}\\left(e^{-b n}\\right)$ for some $a,r>0$ and $b\\in(0,a)$ . Fix a constant $\\delta\\in(0,1)$ . If, additionally, Assumption $(I F)$ holds, then, with probability at least $1-\\delta-7e^{-c_{1}n}$ , the bias is bounded by: ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\theta^{*}}\\left[\\beta\\right]=\\tilde{\\Omega}_{n}\\left(e^{-b n\\operatorname*{min}\\left\\{s,2\\right\\}}\\right)\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "for $\\begin{array}{r}{s\\ {\\stackrel{\\mathrm{def.}}{=}}\\ {\\frac{2r}{a}}+1}\\end{array}$ ", "page_idx": 40}, {"type": "text", "text": "Proof. The proof is similar to Proposition E.3. By Lemma H.14, it holds that: ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\theta^{*}}\\left[\\boldsymbol{\\mathcal{B}}\\right]\\geq\\sum_{l=1}^{p}\\frac{\\lambda_{l}\\mathbb{E}_{\\theta^{*}}\\left[\\left(\\theta_{l}^{*}\\right)^{2}\\right]}{\\left(1+\\lambda_{l}s_{n}(\\mathbf{A}_{-l})^{-1}\\left\\Vert\\mathbf{z}^{(l)}\\right\\Vert_{2}^{2}\\right)^{2}}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "By Lemma H.7, with probability at least $1-2e^{-n}$ , we have, ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{z}_{l}\\right\\|_{2}^{2}\\leq c_{1}n,\\forall l=1,...,p.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "By the choice of $b\\in(0,a)$ : ", "page_idx": 40}, {"type": "equation", "text": "$$\ns_{n}({\\bf A}_{-l})=\\Omega_{n}\\left(n\\lambda\\right)=\\Omega_{n}\\left(n e^{-b n}\\right)=\\Omega_{n}\\left(n\\lambda_{n+1}\\right).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Plug in the above two inequalities, with probability at least $1-2e^{-n}$ , ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\frac{\\lambda_{l}\\mathbb{E}\\left[(\\theta_{l}^{*})^{2}\\right]}{(1+\\lambda_{l}s_{n}(\\mathbf{A}_{-l})^{-1}\\left\\|\\mathbf{z}_{l}\\right\\|_{2}^{2})^{2}}=\\Omega_{n}\\left(\\frac{\\lambda_{l}\\mathbb{E}\\left[(\\theta_{l}^{*})^{2}\\right]}{\\left(1+\\frac{\\lambda_{l}}{\\lambda_{n+1}}\\right)^{2}}\\right)=\\Omega_{n}\\left(\\frac{\\lambda_{l}\\mathbb{E}\\left[(\\theta_{l}^{*})^{2}\\right]}{\\left(1+\\frac{n\\lambda_{l}}{\\lambda_{n+1}r_{n}}\\right)^{2}}\\right)\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where rk = \u2225Tr\u03a3[>\u03a3k>\u2225kop] = \u0398k (1). By Lemma H.15, with probability at least 1 \u22124e\u2212n, ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\theta^{*}}\\left[\\mathcal{B}\\right]=\\Omega_{n}\\left(\\underbrace{\\sum_{l=1}^{p}\\frac{\\lambda_{l}\\mathbb{E}\\left[(\\theta_{l}^{*})^{2}\\right]}{\\left(1+\\frac{n\\lambda_{l}}{\\lambda_{k+1}r_{k}}\\right)^{2}}}_{B}\\right),\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where B is defined as in Theorem H.16. since rk = \u2225Tr\u03a3[>\u03a3k>\u2225kop] = \u0398k (1), pick k such that rk > 1. This satisfies the condition of Theorem H.16. By Theorem H.16, we have upper bound $\\overline{{\\beta}}$ matching the lower bound $\\underline{{\\boldsymbol{\\mathcal{B}}}}$ : ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\underline{{B}}=\\Theta_{n}\\left(\\underbrace{\\|\\theta_{>k}^{*}\\|_{\\Sigma_{>k}}^{2}+\\left(\\frac{n\\lambda+\\mathrm{Tr}[\\Sigma_{>k}]}{n}\\right)^{2}\\left\\|\\theta_{\\leq k}^{*}\\right\\|_{\\Sigma_{\\leq k}^{-1}}^{2}}_{\\overline{{B}}}\\right).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "By Propositions C.6, and D.3, we have, with probability at least $1-\\delta-3e^{c_{1}n}$ , ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathcal{B}=\\mathcal{O}_{n}\\left(\\overline{{B}}\\right)=\\mathcal{O}_{n}\\left(e^{-b n\\operatorname*{min}\\left\\{s,2\\right\\}}\\right).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "All together, with probability at least $1-\\delta-3e^{-c_{1}n}-4e^{-n}$ , ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\theta^{*}}\\left[\\boldsymbol{\\mathcal{B}}\\right]=\\Omega_{n}\\left(\\overline{{\\boldsymbol{B}}}\\right)=\\Omega_{n}\\left(e^{-b n\\operatorname*{min}\\left\\{s,2\\right\\}}\\right).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Lemma E.5 (Asymptotic lower bound of variance with polynomial eigen-decay and independent features). Suppose $\\begin{array}{r}{\\dot{\\lambda_{k}}=\\Theta_{k}\\left(k^{-1-a}\\right)}\\end{array}$ , $\\lambda=\\Theta_{n}\\left(n^{-b}\\right)$ . Additionally, suppose Assumption $(I F)$ holds. Then with probability at least $1-c e^{-n/c}$ , $i t$ holds that ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathcal{V}=\\left\\{\\Omega_{n}\\left(n^{-1+\\frac{b}{1+a}}\\right)\\quad,\\;\\lambda\\;s t r o n g\\right.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Proof. This is a consequence of Propositions C.7, D.7 Lemma H.13 and Theorem H.16: with probability at least $1-3e^{-c_{1}n}$ , by choosing $k={\\left\\{\\begin{array}{l l}{\\lceil n^{\\frac{b}{1+a}}\\rceil}&{,\\ \\lambda{\\mathrm{~strong}}}\\\\ {n/c}&{,\\ \\lambda{\\mathrm{~weak}}}\\end{array}\\right.}$ ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathcal{V}=\\left\\{\\mathcal{O}_{n}\\left(n^{-1+\\frac{b}{1+a}}\\right)\\ \\ \\mathrm{,~}\\lambda\\,\\mathrm{strong}\\right.=\\mathcal{O}_{n}\\left(\\overline{{V}}\\right),\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where $\\begin{array}{r}{\\overline{{V}}\\stackrel{\\mathrm{def.}}{=}\\frac{k}{n}+\\frac{n\\,\\mathrm{Tr}[\\mathbf{\\Sigma}\\mathbf{\\Sigma}_{>k}^{2}]}{(n\\lambda+\\mathrm{Tr}[\\mathbf{\\Sigma}\\mathbf{\\Sigma}_{>k}])^{2}}}\\end{array}$ is defined as in Theorem H.16. Since the polynomial eigen-decay $\\lambda_{k}$ satisfies the condition in Theorem H.16, we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n{\\underline{{V}}}=\\Theta_{n,k}\\left({\\overline{{V}}}\\right).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "By Lemma H.13, $\\begin{array}{r}{\\gamma=\\left\\{\\Omega_{n}\\left(n^{-1+\\frac{b}{1+a}}\\right)\\quad,\\;\\lambda\\mathrm{~strong}\\right.}\\\\ {\\Omega_{n}\\left(1\\right)\\quad\\quad\\quad\\quad\\quad\\,,\\;\\lambda\\mathrm{~weak}}\\end{array}$ , \u03bb strong. By taking a larger constant c > 0, the above events hold with probability at least $1-c e^{-n/c}$ . \u53e3 ", "page_idx": 41}, {"type": "text", "text": "Lemma E.6 (Asymptotic lower bound of variance with exponential eigen-decay and independent features). Suppose $\\dot{\\lambda}_{k}=\\Theta_{k}\\left(e^{-a k}\\right)$ , $\\lambda=\\Theta_{n}\\left(e^{-b n}\\right)$ . Additionally, suppose Assumption $(I F)$ holds. Then with probability at least $1-c e^{-n/c}$ , it holds that ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\gamma=\\left\\{\\mathcal{O}_{n}\\left(n^{-1+\\frac{b}{1+a}}\\right)\\quad,\\ \\lambda\\,s t r o n g\\right.}\\\\ {\\mathcal{O}_{n}\\left(1\\right)\\quad\\quad\\quad\\quad\\quad\\quad\\,,\\ \\lambda\\,w e a k}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Proof. The proof follows a similar pattern to that of Proposition E.5. Ensure that the exponential eigen-decay also meets the condition outlined in Theorem H.16. Consequently, $\\underline{{V}}=\\Theta_{n,k}\\,\\big(\\overline{{V}}\\big)$ , and the remaining argument proceeds accordingly. \u53e3 ", "page_idx": 41}, {"type": "text", "text": "F Under-parameterized regime ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "For the under-parameterized regime, one can clearly use the Master inequalities in Section C to bound the test error from above by setting $k=p<n$ . However, one can even prove an asymptotic convergence in the following way. ", "page_idx": 42}, {"type": "text", "text": "we first prove the convergence of the empirical covariance matrix. ", "page_idx": 42}, {"type": "text", "text": "Lemma F.1 (Convergence of $\\hat{\\Sigma}$ ). Suppose Assumption $(G F)$ (or resp. $(I F)$ ) holds. Fix an $\\epsilon\\in(0,1)$ . Then there exists a constant $c>1$ such that, for $\\begin{array}{r}{p<\\frac{n^{1-\\epsilon}}{c\\log n}}\\end{array}$ (or resp. $p<n^{1-\\epsilon}/c)$ , with probability at least $1-o_{n}\\left(n^{-\\epsilon}\\right)$ , it holds that ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{I}_{p}-\\frac{1}{n}\\mathbf{Z}^{\\top}\\mathbf{Z}\\right\\|_{o p}=\\left\\|\\mathbf{\\boldsymbol{\\Sigma}}-\\hat{\\mathbf{\\boldsymbol{\\Sigma}}}\\right\\|_{o p}=o_{n,p}\\left(n^{-\\epsilon/2}\\right),\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where $\\pmb{\\Sigma}\\,\\overset{\\mathrm{def.}}{=}\\mathbb{E}\\,\\left[\\mathbf{x}\\mathbf{x}^{\\top}\\right]\\in\\mathbb{R}^{p\\times p}$ and $\\begin{array}{r}{\\hat{\\ensuremath{\\Sigma}}\\overset{\\mathrm{def.}}{=}\\frac{1}{n}\\sum_{i=1}^{n}\\ensuremath{\\mathbf{x}}_{i}\\ensuremath{\\mathbf{x}}_{i}^{\\top}\\in\\mathbb{R}^{p\\times p}.}\\end{array}$ ", "page_idx": 42}, {"type": "text", "text": "Proof. We argue similarly as in Proposition G.2. Fix an $\\epsilon\\in(0,1)$ . ", "page_idx": 42}, {"type": "text", "text": "Suppose Assumption (GF) holds. We have ${\\left\\|\\mathbf{z}\\right\\|}_{2}^{2}\\leq\\beta_{p}p$ by definition of $\\beta_{k}$ . Apply Theorem H.3 on the whitened input block $\\mathbf{Z}\\in\\mathbb{R}n\\times p$ : there exists some constant $c_{1}>0$ such that, with probability at least $1-2p e^{-c_{1}t^{2}}$ , we have: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\sqrt{n}-t\\sqrt{\\beta_{p}p}\\leq s_{p}(\\mathbf{Z})\\leq s_{1}(\\mathbf{Z})\\leq\\sqrt{n}+t\\sqrt{\\beta_{p}p}.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "By setting $\\begin{array}{r}{t=\\sqrt{\\frac{\\log n}{c_{1}}}}\\end{array}$ and the choice of $\\begin{array}{r}{p<\\frac{n^{1-\\epsilon}}{c\\log n}}\\end{array}$ , with probability at least $\\textstyle1-{\\frac{2}{c n^{\\epsilon}\\log n}}$ cn\u03f5 l2og n, we have ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\sqrt{n}\\left(1-\\sqrt{\\frac{\\beta_{p}}{2c_{1}}n^{-\\epsilon}}\\right)\\le s_{p}(\\mathbf{Z})\\le s_{1}(\\mathbf{Z})\\le\\sqrt{n}\\left(1+\\sqrt{\\frac{\\beta_{p}}{2c_{1}}n^{-\\epsilon}}\\right).\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Hence, for $n$ large enough, with probability at least cn\u03f5 l2og n, it holds that: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{I}_{p}-\\frac{1}{n}\\mathbf{Z}^{\\top}\\mathbf{Z}\\right\\|_{\\mathrm{op}}\\leq\\frac{2\\beta_{p}}{2c_{1}}n^{-\\epsilon}=o_{n}\\left(n^{-\\epsilon/2}\\right).\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "and hence: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\Sigma-\\hat{\\Sigma}\\right\\|_{\\mathrm{op}}\\leq\\left\\|\\Sigma^{1/2}\\right\\|_{\\mathrm{op}}\\left\\|\\mathbf{I}_{p}-\\frac{1}{n}\\mathbf{Z}^{\\top}\\mathbf{Z}\\right\\|_{\\mathrm{op}}\\left\\|\\Sigma^{1/2}\\right\\|_{\\mathrm{op}}}\\\\ &{\\qquad\\qquad\\leq\\left\\|\\Sigma^{1/2}\\right\\|_{\\mathrm{op}}\\cdot\\frac{2\\beta_{p}}{2c_{1}}n^{-\\epsilon}\\cdot\\left\\|\\Sigma^{1/2}\\right\\|_{\\mathrm{op}}}\\\\ &{\\qquad\\qquad=\\frac{\\lambda_{1}\\beta_{p}}{c_{1}}n^{-\\epsilon}}\\\\ &{\\qquad=o_{n}\\left(n^{-\\epsilon/2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Suppose Assumption (GF) holds. By Lemma H.7 and union bound, we have $\\|\\mathbf{z}_{i}\\|_{2}^{2}\\leq2p$ for all $i=1,...,n$ with probability $1-2n e^{-c_{1}n}$ . When this event happens, apply Theorem H.3 on the whitened input block $\\mathbf{Z}\\in\\mathbb{R}^{n\\times p}$ , the rest of the argument follows similarly. \u53e3 ", "page_idx": 42}, {"type": "text", "text": "Proposition F.2 (Convergence of the bias and variance terms). Suppose Assumption $(G F)$ (or resp. $(I F)_{\\cdot}$ ) holds. Then there exists an $\\epsilon\\in(0,1)$ and a constant $c>1$ such that, for $\\begin{array}{r}{p<\\frac{n^{1-\\epsilon}}{c\\log n}}\\end{array}$ (or resp. $p<n^{1-\\epsilon}/c)$ , with probability at least $1-o_{n}\\left(n^{-\\epsilon}\\right)$ , it holds that ", "page_idx": 42}, {"type": "equation", "text": "$$\nB=\\Theta_{n,p}\\left(\\lambda^{2}\\sum_{k=1}^{p}\\frac{\\lambda_{k}(\\theta_{k}^{*})^{2}}{(\\lambda_{k}+\\lambda)^{2}}\\right),\\,\\mathcal{V}=\\Theta_{n,p}\\left(\\frac{\\sigma^{2}}{n}\\sum_{k=1}^{p}\\frac{\\lambda_{k}^{2}}{(\\lambda_{k}+\\lambda)^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Proof. We begin with $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ . By line (40) and the formula $\\mathbf{M}_{1}^{-1}-\\mathbf{M}_{2}^{-1}=\\mathbf{M}_{1}^{-1}(\\mathbf{M}_{1}-\\mathbf{M}_{2})\\mathbf{M}_{2}^{-1}$ , rewrite the bias term $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ as: ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{B}=\\|(\\mathbf{I}_{p}-\\mathbf{P}_{\\lambda})\\pmb{\\theta}^{*}\\|_{\\Sigma}^{2}}\\\\ &{\\quad=\\left\\|\\lambda(\\lambda\\mathbf{I}_{p}+\\hat{\\pmb{\\Sigma}})^{-1}\\pmb{\\theta}^{*}\\right\\|_{\\Sigma}^{2}}\\\\ &{\\quad=\\left\\|\\lambda\\left((\\lambda\\mathbf{I}_{p}+\\pmb{\\Sigma})^{-1}+(\\lambda\\mathbf{I}_{p}+\\hat{\\pmb{\\Sigma}})^{-1}(\\hat{\\pmb{\\Sigma}}-\\pmb{\\Sigma})(\\lambda\\mathbf{I}_{p}+\\pmb{\\Sigma})^{-1}\\right)\\pmb{\\theta}^{*}\\right\\|_{\\Sigma}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Denote $\\Delta=(\\hat{\\Sigma}-\\Sigma)(\\lambda\\mathbf{I}_{p}+\\Sigma)^{-1}$ . Note that ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|{\\pmb{\\Delta}}\\right\\|_{\\mathrm{op}}=\\left\\|{\\pmb{\\Sigma}}^{1/2}\\left(\\frac{1}{n}{\\pmb{\\Sigma}}^{\\top}{\\pmb{\\mathrm{Z}}}-{\\pmb{\\mathrm{I}}}_{p}\\right){\\pmb{\\Sigma}}^{1/2}{\\pmb{\\Sigma}}^{-1/2}({\\lambda}{\\pmb{\\Sigma}}^{-1}+{\\pmb{\\mathrm{I}}}_{p})^{-1}{\\pmb{\\Sigma}}^{-1/2}\\right\\|_{\\mathrm{op}}}\\\\ &{\\qquad\\leq\\left\\|\\frac{1}{n}{\\pmb{\\mathrm{Z}}}^{\\top}{\\pmb{\\mathrm{Z}}}-{\\pmb{\\mathrm{I}}}_{p}\\right\\|_{\\mathrm{op}}\\cdot\\left\\|\\left({\\lambda}{\\pmb{\\Sigma}}^{-1}+{\\pmb{\\mathrm{I}}}_{p}\\right)^{-1}\\right\\|_{\\mathrm{op}}\\cdot\\left\\|{\\pmb{\\Sigma}}^{1/2}{\\pmb{\\Sigma}}^{1/2}{\\pmb{\\Sigma}}^{-1/2}{\\pmb{\\Sigma}}^{-1/2}\\right\\|_{\\mathrm{op}}}\\\\ &{\\qquad=o_{n}\\left(n^{-\\epsilon/2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "We apply the matrix difference formula iteratively: ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{(\\hat{\\Sigma}+\\lambda\\mathbf{I}_{p})^{-1}=(\\Sigma+\\lambda\\mathbf{I}_{p})^{-1}+(\\hat{\\Sigma}+\\lambda\\mathbf{I}_{p})^{-1}(\\hat{\\Sigma}-\\Sigma)(\\Sigma+\\lambda\\mathbf{I}_{p})^{-1}}}\\\\ {{\\displaystyle=(\\Sigma+\\lambda\\mathbf{I}_{p})^{-1}+(\\hat{\\Sigma}+\\lambda\\mathbf{I}_{p})^{-1}\\Delta}}\\\\ {{\\displaystyle=(\\Sigma+\\lambda\\mathbf{I}_{p})^{-1}+\\left((\\Sigma+\\lambda\\mathbf{I}_{p})^{-1}+(\\hat{\\Sigma}+\\lambda\\mathbf{I}_{p})^{-1}\\Delta\\right)\\Delta}}\\\\ {{\\displaystyle=\\dots}}\\\\ {{\\displaystyle=(\\Sigma+\\lambda\\mathbf{I}_{p})^{-1}\\sum_{t=0}^{\\infty}\\Delta^{t},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "we write: ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{B=\\left\\|\\lambda(\\hat{\\Sigma}+\\lambda\\mathbf{I}_{p})^{-1}\\theta^{*}\\right\\|_{\\Sigma}^{2}}\\\\ &{\\quad=\\left\\|\\lambda\\left((\\lambda\\mathbf{I}_{p}+\\Sigma)^{-1}+(\\lambda\\mathbf{I}_{p}+\\hat{\\Sigma})^{-1}(\\hat{\\Sigma}-\\Sigma)(\\lambda\\mathbf{I}_{p}+\\Sigma)^{-1}\\right)\\theta^{*}\\right\\|_{\\Sigma}^{2}}\\\\ &{\\quad=\\left\\|\\lambda(\\lambda\\mathbf{I}_{p}+\\Sigma)^{-1}\\theta^{*}+\\lambda(\\hat{\\Sigma}+\\lambda\\mathbf{I}_{p})^{-1}\\Delta\\theta^{*}\\right\\|_{\\Sigma}^{2}}\\\\ &{\\quad=\\left\\|\\lambda(\\lambda\\mathbf{I}_{p}+\\Sigma)^{-1}\\left(\\displaystyle\\sum_{t=0}^{\\infty}\\Delta^{t}\\right)\\theta^{*}\\right\\|_{\\Sigma}^{2}}\\\\ &{\\quad=\\Theta_{n,p}\\left(\\left\\|\\lambda(\\lambda\\mathbf{I}_{p}+\\Sigma)^{-1}\\theta^{*}\\right\\|_{\\Sigma}^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Finally, write ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\left\\|\\lambda(\\lambda\\mathbf{I}_{p}+\\Sigma)^{-1}\\theta^{*}\\right\\|_{\\Sigma}^{2}=\\lambda^{2}\\sum_{k=1}^{p}\\frac{\\lambda_{k}(\\theta_{k}^{*})^{2}}{(\\lambda_{k}+\\lambda)^{2}}.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Now we argue similarly for $\\mathcal{V}$ . By Lemma H.19: ", "page_idx": 44}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{\\gamma={\\frac{\\sigma^{2}}{n}}\\mathbf{\\mathbb{E}}_{\\mathbf{x}\\sim\\mu}\\left[\\left\\Vert\\left({\\hat{\\mathbf{S}}}+\\lambda\\mathbf{I}_{p}\\right)^{-1}\\mathbf{x}\\right\\Vert_{\\mathbf{\\hat{S}}}^{2}\\right]}\\\\ &{\\quad={\\frac{\\sigma^{2}}{n}}\\mathbf{\\mathbb{E}}_{\\mathbf{x}\\sim\\mu}\\left[\\left\\Vert\\left({\\mathbf{S}}+\\lambda\\mathbf{I}_{p}\\right)^{-1}\\sum_{t=0}^{\\infty}\\Delta^{t}\\mathbf{r}_{\\mathbf{x}}^{t}\\right\\Vert_{\\mathbf{\\hat{S}}}^{2}\\right]}\\\\ &{\\quad={\\frac{\\sigma^{2}}{n}}\\mathbf{\\mathbb{E}}_{\\mathbf{x}}\\left[\\mathbf{x}^{\\top}\\left(\\sum_{t=0}^{\\infty}\\Delta^{t}\\right)^{\\top}\\left({\\mathbf{S}}+\\lambda\\mathbf{I}_{p}\\right)^{-1}{\\hat{\\mathbf{S}}}({\\mathbf{S}}+\\lambda\\mathbf{I}_{p})^{-1}\\sum_{t=0}^{\\infty}\\Delta^{t}\\mathbf{r}_{\\mathbf{x}}\\right]}\\\\ &{\\quad={\\frac{\\sigma^{2}}{n}}\\mathbf{\\mathbb{E}}_{\\mathbf{x}}\\left[\\mathbf{x}^{\\top}\\left(\\sum_{t=0}^{\\infty}\\Delta^{t}\\right)^{\\top}(\\mathbf{S}+\\lambda\\mathbf{I}_{p})^{-1}\\left(\\Delta+{\\mathbf{S}}({\\mathbf{S}}+\\lambda\\mathbf{I}_{p})^{-1}\\right)\\sum_{t=0}^{\\infty}\\Delta^{t}\\mathbf{r}_{\\mathbf{x}}\\right]}\\\\ &{\\quad={\\frac{\\sigma^{2}}{n}}\\mathbf{\\mathbb{T}}[{\\mathbf{S}}^{2}(\\mathbf{\\Sigma}+\\lambda\\mathbf{I}_{p})^{-2}]\\left(1+{\\mathcal{O}}_{n}\\left(\\left\\Vert\\Delta\\right\\Vert_{\\mathbf{\\hat{S}}}\\right)\\right).}\\end{array}}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "We argue similarly as in Proposition F.2: with probability at least $1-o_{n}\\left(n^{-\\epsilon}\\right)$ , we have ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\small\\gamma=\\Theta_{n,p}\\left(\\frac{\\sigma^{2}}{n}\\sum_{k=1}^{p}\\frac{\\lambda_{k}^{2}}{(\\lambda_{k}+\\lambda)^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Plugging in the values of $n,p$ and other metrics $a,b,r$ in Proposition F.2, we can obtain the ", "page_idx": 44}, {"type": "text", "text": "G Concentration of features ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "In the following, we shall give sufficient conditions to control the concentration coefficients $\\zeta_{n,k},\\xi_{n,k}$ and $\\rho_{n,k}$ . Note that $\\rho_{n,k}$ , and $\\xi_{n,k}$ depends on the whitened/isotropic features ${\\bf z}$ but not on the spectrum $\\Sigma$ . We start from the easiest to the most difficult. ", "page_idx": 45}, {"type": "text", "text": "Proposition G.1 (Control on $\\xi_{n,k.}$ ). Let $k\\in\\mathbb{N}$ be an integer. Recall that $\\xi_{n,k}$ de=f. $\\frac{s_{1}(\\mathbf{Z}_{\\leq k}^{\\top}\\mathbf{Z}_{\\leq k})}{n}$ . If Assumption $(G F)$ (or resp. $(I F)$ ) holds, then with probability at least $\\begin{array}{r}{1-2\\exp\\left(-\\frac{1}{2\\beta_{k}^{2}}n\\right)}\\end{array}$ (or resp. $1-2\\exp\\left(-c_{1}k n\\right))$ , it holds that ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\xi_{n,k}\\geq{\\frac{1}{2}}.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Proof. Since the largest singular value is larger than the average of the singular values, ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\xi_{n,k}\\overset{\\mathrm{def.}}{=}\\frac{s_{1}(\\mathbf{Z}_{\\leq k}^{\\top}\\mathbf{Z}_{\\leq k})}{n}\\geq\\frac{\\frac{1}{k}\\operatorname{Tr}[\\mathbf{Z}_{\\leq k}^{\\top}\\mathbf{Z}_{\\leq k}]}{n}=\\frac{\\operatorname{Tr}[\\mathbf{Z}_{\\leq k}^{\\top}\\mathbf{Z}_{\\leq k}]}{k n}.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "By Lemma G.5, with respective probability, ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\xi_{n,k}\\geq{\\frac{1}{2}}.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Proposition G.2 (Control on $\\zeta_{n,k}$ ). Let $k\\leq n$ be an integer. Recall that $\\begin{array}{r}{\\zeta_{n,k}\\ {\\stackrel{\\mathrm{def.}}{=}}\\ {\\frac{s_{1}(\\mathbf{Z}_{\\leq k}^{\\top}\\mathbf{Z}_{\\leq k})}{s_{n}(\\mathbf{Z}_{\\leq k}^{\\top}\\mathbf{Z}_{\\leq k})}}}\\end{array}$ . If Assumption $(G F)$ (or resp. $(I F)$ ) holds, then there exists some $c>1$ such that, i $r_{c\\beta_{k}k}\\log k\\le n$ (or resp. $k/c\\leq n,$ ), then with probability at least $\\begin{array}{r}{1-2\\exp\\left(-\\frac{c}{\\beta_{k}}\\frac{n}{k}\\right)}\\end{array}$ (or resp. $1-2\\exp{\\left(-c_{1}n\\right)})$ , it holds that ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\zeta_{n,k}\\leq c_{2}.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Proof. For the case where Assumption (GF) holds, see Lemma 2 in [7]. Suppose Assumption (IF) holds, then by Theorem H.2, there exists constants $C_{1},C_{2}>0$ , such that, with probability at least $1-2\\exp\\left(-\\dot{C}_{1}t^{2}\\right)$ , the spectrum of random matrix $\\mathbf{Z}_{\\leq k}\\in\\mathbb{R}^{n\\times k}$ is bounded: ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\sqrt{n}-\\sqrt{C_{2}k}-t\\leq s_{k}(\\mathbf{Z}_{\\leq k})\\leq s_{1}(\\mathbf{Z}_{\\leq k})\\leq\\sqrt{n}+\\sqrt{C_{2}k}+t.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Set $t=\\sqrt{n}/4$ and $\\begin{array}{r}{c=\\frac{1}{16C_{2}}}\\end{array}$ so that if $k/c\\leq n$ , the bound becomes: ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\frac{1}{4}\\sqrt{n}\\leq s_{k}(\\mathbf{Z}_{\\leq k})\\leq s_{1}(\\mathbf{Z}_{\\leq k})\\leq\\frac{3}{2}\\sqrt{n},\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "aanndd $\\textstyle\\zeta_{n,k}\\leq({\\frac{3}{2}}{\\sqrt{n}})/({\\frac{1}{4}}{\\sqrt{n}})=6$ with a probability at least $\\begin{array}{r}{1-2\\exp\\left(-\\frac{C_{1}}{16}n\\right)}\\end{array}$ . Set $\\begin{array}{r}{c_{1}=\\frac{C_{1}}{16}}\\end{array}$ $c_{2}=6$ ", "page_idx": 45}, {"type": "text", "text": "Proposition G.3 (Control on $\\rho_{n,k}$ ). Let $k\\leq n$ be an integer. Recall that $\\begin{array}{r}{\\rho_{n,k}\\ {\\stackrel{\\mathrm{def.}}{=}}\\ {\\frac{n\\|\\mathbf{\\Xi}\\Sigma_{>k}\\|_{o p}+s_{1}(\\mathbf{A}_{k})}{s_{n}(\\mathbf{A}_{k})}}}\\end{array}$ If Assumption $(P E)$ holds and Assumption $^{\\prime}G F)$ (or resp. $(I F)$ ) holds, then there exists some $c>1$ such that, if k = c long n (or resp. $k=n/c)$ , then with probability at least $\\textstyle1-{\\mathcal{O}}_{n}\\left({\\frac{1}{\\log n}}\\right)$ (or resp. $1-3e^{-n},$ ), it holds that ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\rho_{n,k}=\\tilde{\\mathcal{O}}_{n}\\left(n^{a}\\right).\\quad(o r\\,r e s p.\\ \\rho_{n,k}=\\mathcal{O}_{n}\\left(1\\right).)\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "If $\\lambda=\\Theta_{n}\\left(n^{-b}\\right)$ with $b\\in(0,1+a]$ and Assumption (GF) holds. If $k=\\lceil n^{\\frac{b}{1+a}}\\rceil$ , with probability at least $\\textstyle1-O_{n}\\left({\\frac{1}{n}}\\right)$ , it holds that ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\rho_{n,k}={\\mathcal O}_{n}\\left(1\\right).\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Proof. The statement where Assumption (GF) holds is proved in Theorems 4 and 5 in [7]. The statement where Assumption (IF) holds can be proved proved via Lemma G.7: since: ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\rho=\\frac{n\\left|\\left|\\sum_{\\gamma>k}\\right|\\left|\\exp_{\\gamma}+\\mathinner{s}\\right|\\left|\\mathbf{A}_{k}\\right|}{s_{n}\\left(\\mathbf{A}_{k}\\right)}}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle=\\mathcal{O}_{k,n}\\left(\\frac{n\\lambda_{k}+n\\lambda}{n\\lambda_{n+k}+n\\lambda}\\right)}\\\\ {\\displaystyle}\\\\ {\\displaystyle=\\mathcal{O}_{k,n}\\left(\\frac{n\\lambda_{k}}{n\\lambda_{n+k}}\\right)}\\\\ {\\displaystyle}\\\\ {\\displaystyle=\\mathcal{O}_{k,n}\\left(\\frac{k^{-1-n}}{(n+k)^{-1-n}}\\right)}\\\\ {\\displaystyle=\\mathcal{O}_{k,n}\\left((1+n/k)^{1+n}\\right)}\\\\ {\\displaystyle=\\mathcal{O}_{k,n}\\left((1+c)^{1+n}\\right)}\\\\ {\\displaystyle=\\mathcal{O}_{k,n}\\left(1\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "with a probability of at least $1-3e^{-n}$ . ", "page_idx": 46}, {"type": "text", "text": "Proposition G.4 (Control on $\\rho_{n,k}$ under exponential eigen-decay). Let $k\\,\\leq\\,n$ be an integer. If $\\lambda_{k}=\\Theta_{k}\\left(e^{-a k}\\right)$ , $\\lambda=\\Theta_{n}\\left(e^{-b n}\\right)$ for some $a>0$ and $b\\in(0,a)$ , and Assumption $(G F)$ (or resp. $(I F)_{.}$ ) holds, choose $\\begin{array}{r}{k=\\left\\lceil\\frac{b}{a}n\\right\\rceil}\\end{array}$ , then with probability at least $\\textstyle1-o_{n}\\left({\\frac{1}{n}}\\right)$ (or resp. $1-\\exp{(-n)})$ , it holds that ", "page_idx": 46}, {"type": "equation", "text": "$$\ns_{1}(\\mathbf{A}_{k})=\\mathcal{O}_{n}\\left(n e^{-b n}\\right),\\;\\rho_{n,k}=\\mathcal{O}_{n}\\left(1\\right).\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Proof. For the case where Assumption (GF) holds, by Corollary 1 in [7], with probability at least $\\textstyle1-{\\dot{o}}_{n}\\left({\\frac{1}{n}}\\right)$ , it holds that ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{s_{1}(\\mathbf{A}_{k})=\\mathcal{O}_{n,k}\\left(n\\left(\\lambda_{k+1}+\\frac{\\log k\\,\\mathrm{Tr}[\\mathbf{Z}_{>k}]}{n}+\\lambda\\right)\\right)}\\\\ &{\\qquad\\qquad=\\mathcal{O}_{n,k}\\left(n\\left(e^{-a k}+e^{-b n}\\right)\\right)}\\\\ &{\\qquad\\quad=\\mathcal{O}_{n,k}\\left(n\\left(e^{-a\\cdot\\frac{b}{a}n}+e^{-b n}\\right)\\right)}\\\\ &{\\qquad\\quad=\\mathcal{O}_{n}\\left(n e^{-b n}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "For the case where Assumption (IF) holds, the upper bound of $s_{1}(\\mathbf{A}_{k})$ is proven in Lemma G.7. In both cases, the derivation of bounding $\\rho$ is the same: ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\rho=\\frac{n\\left\\|\\pmb{\\Sigma}_{>k}\\right\\|_{\\mathrm{op}}+s_{1}(\\mathbf{A}_{k})}{s_{n}(\\mathbf{A}_{k})}\\leq\\frac{n\\left\\|\\pmb{\\Sigma}_{>k}\\right\\|_{\\mathrm{op}}+s_{1}(\\mathbf{A}_{k})}{n\\lambda}=\\mathcal{O}_{n,k}\\left(\\frac{n e^{-b n}}{n e^{-b n}}\\right)=\\mathcal{O}_{n,k}\\left(1\\right).\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Lemma G.5 (Lemma 1 in [7]). Let $k$ be an integer. Suppose Assumption $(G F)$ (or resp. $(I F)$ ) holds. Then with probability at least $\\begin{array}{r}{1-2\\exp\\left(-\\frac{1}{2\\beta_{k}}n\\right)}\\end{array}$ (or. resp. $1-2\\exp\\left(-c_{1}k n\\right))$ , it holds that ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\frac12k n\\leq\\mathrm{Tr}[{\\bf Z}_{\\leq k}^{\\top}{\\bf Z}_{\\leq k}]\\leq\\frac32k n.\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Proof. If Assumption (GF) holds, then ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\mathrm{Tr}[\\mathbf Z_{\\le k}^{\\top}\\mathbf Z_{\\le k}]=\\mathrm{Tr}[\\mathbf Z_{\\le k}\\mathbf Z_{\\le k}^{\\top}]=\\sum_{i=1}^{n}\\|(\\mathbf z_{i})_{\\le k}\\|_{2}^{2}\\le\\beta_{k}k n.\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Set $M=\\beta_{k}k$ and by Hoeffding\u2019s inequality, the above trace concentrates: ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left\\{\\left|\\mathrm{Tr}[\\mathbf Z_{\\leq k}\\mathbf Z_{\\leq k}^{\\top}]-k n\\right|\\geq t\\right\\}\\leq2\\exp\\left(-\\frac{2t^{2}}{n M^{2}}\\right)\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Set $t=n k/2$ to conclude the statement. ", "page_idx": 47}, {"type": "text", "text": "Analogously, if Assumption (IF) holds, for $i\\,=\\,1,...,n$ and $l\\,=\\,1,...,k$ , $(z_{i}^{(l)})^{2}-1$ is centered sub-exponential variable with sub-exponential norm $\\left\\|(z_{i}^{(l)})^{2}-1\\right\\|_{\\psi_{1}}\\lesssim G^{2}$ . By Lemma H.7, with probability at least $1-2\\exp\\left(-c_{1}k n\\right)$ , ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\left|\\mathrm{Tr}[\\mathbf{Z}_{\\leq k}^{\\top}\\mathbf{Z}_{\\leq k}]-k n\\right|=\\left|\\sum_{i=1}^{n}\\sum_{l=1}^{k}(z_{i}^{(l)})^{2}-k n\\right|\\leq\\frac{1}{2}k n.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Lemma G.6 (Lemma 3 in [7]). For any $k\\leq n$ and $\\delta\\in(0,1)$ , with probability at least $1-\\delta$ , it holds that ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\frac1n\\left\\lVert\\mathbf{X}_{>k}\\pmb{\\theta}_{>k}^{*}\\right\\rVert_{2}^{2}\\leq\\frac1\\delta\\left\\lVert\\pmb{\\theta}_{>k}^{*}\\right\\rVert_{\\pmb{\\Sigma}_{>k}}^{2}.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Proof. Since $\\begin{array}{r}{\\mathbb{E}\\left[\\frac{1}{n}\\left\\|\\mathbf{X}_{>k}\\pmb{\\theta}_{>k}^{*}\\right\\|_{2}^{2}\\right]=\\left\\|\\pmb{\\theta}_{>k}^{*}\\right\\|_{\\pmb{\\Sigma}_{>k}}^{2}}\\end{array}$ , we use Markov\u2019s inequality to obtain the result. ", "page_idx": 47}, {"type": "text", "text": "Lemma G.7. Suppose Assumption $(I F)$ hold. Then for any integer $k\\leq n$ , ", "page_idx": 47}, {"type": "text", "text": "1. If Assumption $(P E)/(E E)$ holds, then with probability at least $1-e^{-n}$ , it holds that ", "page_idx": 47}, {"type": "equation", "text": "$$\ns_{1}({\\bf A}_{k})={\\mathcal O}_{n,k}\\left(n\\lambda_{k}+n\\lambda\\right).\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "2. If Assumption $(P E)$ holds, then with probability at least $1-2e^{-n}$ , it holds that ", "page_idx": 47}, {"type": "equation", "text": "$$\ns_{n}(\\mathbf{A}_{k})=\\Omega_{n,k}\\left(n\\lambda_{n+k}+n\\lambda\\right),\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "for p large enough. ", "page_idx": 47}, {"type": "text", "text": "Proof. First we prove statement 1. For any $k\\in\\mathbb N$ , ", "page_idx": 47}, {"type": "equation", "text": "$$\nr_{k}\\stackrel{\\mathrm{def.}}{=}\\frac{{\\mathrm{Tr}}\\left[\\Sigma_{>k}\\right]}{\\left\\Vert\\Sigma_{>k}\\right\\Vert_{\\mathrm{op}}}=\\left\\{\\Theta_{k}\\left(\\frac{\\sum_{l=k+1}^{p}l^{-1-a}}{\\left(k+1\\right)^{-1-a}}\\right)\\right.\\quad=\\left\\{\\Theta_{k}\\left(\\frac{k^{-a}}{k^{-1-a}}\\right)\\quad=\\left\\{\\Theta_{k}\\left(k\\right)\\right\\}\\frac{\\left\\Vert\\Phi_{k}\\right\\Vert}{\\left\\Vert\\Phi_{k}\\left(1\\right)\\right\\Vert_{\\mathrm{op}}}=0\\right.\\quad.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Hence, set $t=n$ in Theorem H.5, with probability at least $1-e^{-n}$ , it holds that ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\frac{1}{n}\\mathbf{X}_{>k}^{\\top}\\mathbf{X}_{>k}-\\Sigma_{>k}\\right\\|_{\\mathrm{op}}=\\mathcal{O}_{n,k}\\left(\\left\\|\\Sigma_{>k}\\right\\|_{\\mathrm{op}}\\operatorname*{max}\\left\\{\\frac{r_{k}}{n},\\sqrt{\\frac{r_{k}}{n}},\\frac{t}{n},\\sqrt{\\frac{t}{n}}\\right\\}\\right)=\\mathcal{O}_{n,k}\\left(\\lambda_{k}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "By triangle inequality, $\\begin{array}{r}{\\left\\|\\mathbf{X}_{>k}^{\\top}\\mathbf{X}_{>k}\\right\\|_{\\mathrm{op}}\\,\\leq\\,n\\left\\|\\frac{1}{n}\\mathbf{X}_{>k}^{\\top}\\mathbf{X}_{>k}-\\pmb{\\Sigma}_{>k}\\right\\|_{\\mathrm{op}}+n\\left\\|\\pmb{\\Sigma}_{>k}\\right\\|_{\\mathrm{op}}\\,=\\,\\mathcal{O}_{n,k}\\left(n\\lambda_{k}\\right)\\mathrm{.}}\\end{array}$ Hence, with the same probability, ", "page_idx": 47}, {"type": "equation", "text": "$$\ns_{1}(\\mathbf{A}_{k})=s_{1}(\\mathbf{X}_{>k}\\mathbf{X}_{>k}^{\\top}+n\\lambda\\mathbf{I}_{n})=s_{1}(\\mathbf{X}_{>k}^{\\top}\\mathbf{X}_{>k})+n\\lambda=\\mathcal{O}_{n,k}\\left(n\\lambda_{k}+n\\lambda\\right).\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Now we prove statement 2. Note that the smallest singular value of a matrix does not increase after discarding a column: for some constant $\\eta>1$ to be determined, it holds that ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r}{s_{n}(\\mathbf{X}_{>k}\\mathbf{X}_{>k}^{\\top})=s_{n}(\\mathbf{Z}_{>k}\\mathbf{Z}_{>k}^{1/2})^{2}\\geq s_{n}(\\mathbf{Z}_{k:\\eta n}\\mathbf{Z}_{k:\\eta n}^{1/2})^{2}\\geq\\lambda_{\\eta n}s_{n}(\\mathbf{Z}_{k:\\eta n})^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "By Assumption (IF), the columns of $\\mathbf{Z}_{k:\\eta n}$ are independent to each other. By Theorem H.2, with probability at least $1-2e^{-c_{1}t^{2}}$ , it holds that ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r}{s_{\\operatorname*{min}}(\\mathbf{Z}_{k:\\eta n})=s_{\\operatorname*{min}\\{n,\\eta n-k\\}}(\\mathbf{Z}_{k:\\eta n})\\geq\\sqrt{\\eta n}-\\sqrt{c_{2}n}-t.}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Choose $t=\\sqrt{n/c_{1}}$ and $\\begin{array}{r}{\\eta>\\left(2+\\frac{1}{c_{1}}+c_{2}\\right)^{2}}\\end{array}$ (given that $p>\\eta n$ large enough), then with probability at least $1-2e^{-n}$ , it holds that ", "page_idx": 47}, {"type": "equation", "text": "$$\ns_{n}(\\mathbf{Z}_{k:\\eta n})=s_{\\operatorname*{min}}(\\mathbf{Z}_{k:\\eta n})\\geq\\sqrt{n}.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Hence with the same probability, by Assumption (PE), ", "page_idx": 47}, {"type": "equation", "text": "$$\ns_{n}(\\mathbf{A}_{k})=s_{n}(\\mathbf{X}_{>k}\\mathbf{X}_{>k}^{\\top}+n\\lambda\\mathbf{I}_{n})\\geq n\\lambda_{\\eta n}+n\\lambda=\\Omega_{n,k}\\left(n\\lambda_{n}+n\\lambda\\right).\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Lemma G.8 (Theorem 2 in [7]). Suppose Assumption $(G F)$ (or resp. $(I F)$ ) holds, then with probability at least 1 (or resp. $1-2p e^{-c_{1}n}.$ ), it holds that ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\frac{s_{1}(\\mathbf{A}_{k})^{2}}{n^{2}}\\leq\\rho_{n,k}^{2}\\left(\\lambda+\\frac{c_{2}\\,\\mathrm{Tr}[\\Sigma_{>k}]}{n}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Suppose Assumptions $(I F)$ holds. Furthermore, $i f$ Assumptions $(P E)$ or $(E E)$ holds, then one obtains a probability bound which allows arbitrary large $p$ : with probability at least $1-e^{-n}$ , it holds that ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\frac{s_{1}(\\mathbf{A}_{k})^{2}}{n^{2}}\\leq c_{1}\\left(\\lambda+\\frac{\\mathrm{Tr}[\\pmb{\\Sigma}_{>k}]}{n}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Proof. Since the trace of a matrix is the sum of its eigenvalues, we obtain: ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{s_{1}\\left(\\mathbf{A}_{k}\\right)^{2}}{n^{2}}=\\frac{s_{1}\\left(\\mathbf{A}_{k}\\right)^{2}}{s_{n}\\left(\\mathbf{A}_{k}\\right)^{2}}\\frac{s_{n}\\left(\\mathbf{A}_{k}\\right)^{2}}{n^{2}}}\\\\ &{\\phantom{\\frac{s_{1}\\left(\\mathbf{A}_{k}\\right)^{2}}{s_{n,k}\\left(\\frac{1}{n}\\right)}}\\leq\\rho_{n,k}^{2}\\left(\\frac{1}{n}\\mathrm{Tr}\\left[\\frac{1}{n}\\mathbf{A}_{k}\\right]\\right)^{2}}\\\\ &{\\phantom{\\frac{s_{1}\\left(\\mathbf{A}_{k}\\right)^{2}}{s_{n}^{2}}}\\leq\\rho_{n,k}^{2}\\left(\\frac{1}{n}\\left(\\mathrm{Tr}\\left[\\lambda\\mathbf{I}_{n}\\right]+\\mathrm{Tr}\\left[\\frac{1}{n}\\sum_{i=1}^{n}(\\mathbf{x}_{i})_{>k}(\\mathbf{x}_{i})_{>k}^{\\top}\\right]\\right)\\right)^{2}}\\\\ &{\\phantom{\\frac{s_{1}\\left(\\mathbf{A}_{k}\\right)^{2}}{s_{n}^{2}}}\\leq\\rho_{n,k}^{2}\\left(\\frac{1}{n}\\left(n\\lambda+\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\|(\\mathbf{x}_{i})_{>k}\\|_{2}^{2}\\right)\\right)^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "By Assumption (GF), since ess supxTr[>\u03a3k>k2] $\\begin{array}{r}{\\cos\\operatorname*{sup}_{\\mathbf{x}}\\frac{\\|\\mathbf{x}_{>k}\\|_{2}^{2}}{\\operatorname{Tr}\\left[\\sum_{>k}\\right]}\\leq\\beta_{k}}\\end{array}$ , we have ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\frac{s_{1}(\\mathbf{A}_{k})^{2}}{n^{2}}\\leq\\rho_{n,k}^{2}\\left(\\frac{1}{n}\\left(n\\lambda+\\underset{\\mathbf{x}}{\\operatorname*{sup}}\\left\\|\\mathbf{x}_{>k}\\right\\|_{2}^{2}\\right)\\right)^{2}\\leq\\rho_{n,k}^{2}\\left(\\lambda+\\frac{\\beta_{k}\\,\\mathrm{Tr}[\\Sigma_{>k}]}{n}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "If Assumption (IF) holds, then for each $l\\,>\\,k$ , the random variable $(z^{(l)})^{2}\\mathrm{~-~}1$ is centered subexponential, hence with probability at least $1-2\\exp\\left(-c_{1}n\\right)$ , it holds that ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\left|\\sum_{i=1}^{n}(z_{i}^{(l)})^{2}-n\\right|\\leq\\frac{1}{2}n.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "By union bound, with probability at least $1-2(p-k)\\exp\\left(-c_{1}n\\right)$ , it holds that ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\sum_{i=1}^{n}\\|(\\mathbf{x}_{i})_{>k}\\|_{2}^{2}=\\sum_{l>k}\\lambda_{l}\\frac{1}{n}\\sum_{i=1}^{n}(z_{i}^{(l)})^{2}\\leq\\sum_{l>k}\\lambda_{l}\\cdot\\frac{3}{2}=\\frac{3}{2}\\operatorname{Tr}[\\Sigma_{>k}].\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "If Assumptions (IF) and (PE)/(EE) hold, then by Lemma G.7: with probability at least $1-e^{-n}$ , it holds that ", "page_idx": 48}, {"type": "equation", "text": "$$\ns_{1}(\\mathbf{A}_{k})=\\mathcal{O}_{n,k}\\left(n\\lambda_{n}+n\\lambda\\right)\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "for any $k\\in\\mathbb{N}$ . Hence with the same probability, ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\frac{s_{1}(\\mathbf{A}_{k})^{2}}{n^{2}}=\\mathcal{O}_{n}\\left(\\left(\\frac{n\\lambda+n\\lambda_{n}}{n}\\right)^{2}\\right)=\\mathcal{O}_{n}\\left(\\left(\\lambda+\\frac{\\mathrm{Tr}[\\pmb{\\Sigma}_{k}]}{n}\\right)^{2}\\right).\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "H Technical lemmata ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Theorem H.1 (Theorem 1 and Example 1 in [51]). Assume that $\\mathbf{M}_{1},\\mathbf{M}_{2},..,\\mathbf{M}_{n}$ are independent copies of a $d\\times d$ positive semi-definite symmetric random matrix M with E $[\\mathbf{M}]=\\Sigma$ , which satisfies: ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\|\\mathbf{v}^{\\top}\\mathbf{M}\\mathbf{v}\\|_{\\psi_{1}}\\leq\\kappa^{2}\\mathbf{v}^{\\top}\\mathbf{M}\\mathbf{v}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "for some constant $\\kappa\\geq1$ and for any $\\mathbf{v}\\in\\mathbb{R}^{d}$ . Then, for any $t>0$ , with probability at least $1-e^{-t}$ the inequality holds: ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\left\\|\\frac{1}{n}\\sum_{i=1}\\mathbf{M}_{i}-\\mathbf{\\deltaZ}\\right\\|_{o p}\\leq20\\kappa^{2}\\left\\|\\mathbf{\\deltaZ}\\right\\|_{o p}\\sqrt{\\frac{4r+t}{n}}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "whenever $n\\geq4r+t,$ , and de=f. \u2225Tr\u03a3[\u2225\u03a3] is the effective rank. ", "page_idx": 49}, {"type": "text", "text": "Theorem H.2 (Theorem 5.39 and Remark 5.40 in [47]). Let A be an $N\\times n$ matrix with independent rows $\\mathbf{A}_{i}$ of sub-Gaussian random vector with covariance $\\pmb{\\Sigma}\\ {\\stackrel{\\mathrm{def.}}{=}}\\ \\mathbb{E}\\left[\\mathbf{A_{i}}\\mathbf{A_{i}}^{\\top}\\right]\\in\\mathbb{R}^{n\\times n}$ . Then there exists constants $C_{5},C_{6}>0$ 15 (depending only on the sub-Gaussian norm of entries of A), such that for any $t\\geq0$ , with probability at least $\\bar{1-2e^{-C_{5}t^{2}}}$ , we have ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\left\\|\\frac{1}{N}\\mathbf{A}^{\\top}\\mathbf{A}-\\mathbf{\\Sigma}\\right\\|_{o p}\\leq\\operatorname*{max}\\{\\delta,\\delta^{2}\\}\\,\\|\\Sigma\\|_{o p}\\,.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "where $\\begin{array}{r}{\\delta=C_{6}\\sqrt{\\frac{n}{N}}+\\frac{t}{N}}\\end{array}$ . In particular, if $\\mathbf{\\nabla}\\Sigma=\\mathbf{I}_{n}$ , we have ", "page_idx": 49}, {"type": "equation", "text": "$$\n{\\sqrt{N}}-{\\sqrt{C_{6}n}}-t\\leq s_{\\operatorname*{min}}(\\mathbf{A})\\leq s_{\\operatorname*{max}}(\\mathbf{A})\\leq{\\sqrt{N}}+{\\sqrt{C_{6}n}}+t.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Theorem H.3 (Theorem 5.41 in [47]). Let A be an $N\\times n$ matrix whose row\u221as ${\\bf A}_{i}$ are independent isotropic random vectors in $\\mathbb{R}^{n}$ . Let $m>0$ be a number such that $\\left\\|\\mathbf{A}_{i}\\right\\|_{2}\\leq\\sqrt{m}$ a.s. for all $i$ . Then for every $t\\geq0$ , it holds that: ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\sqrt{N}-t\\sqrt{m}\\leq s_{\\operatorname*{min}}(\\mathbf{A})\\leq s_{\\operatorname*{max}}(\\mathbf{A})\\leq\\sqrt{N}+t\\sqrt{m}.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Theorem H.4 (Theorem 5.58 in [47]). Let A be an $N\\times n$ matrix $^{\\prime}N\\geq n,$ ) w\u221aith independent columns $\\mathbf{A}_{i}\\,\\in\\,\\mathbb{R}^{N}$ of sub-Gaussian isotropic random vector with with $\\|\\mathbf{A}_{i}\\|_{2}=\\sqrt{N}$ almost surely. Then there exists constants $C_{8},C_{9}>0$ (depending only on the sub-Gaussian norm of entries of A), such that for any $t\\geq0$ , with probability at least $\\bar{1}-2e^{-C_{8}t^{2}}$ , we have ", "page_idx": 49}, {"type": "equation", "text": "$$\n{\\sqrt{N}}-C_{9}{\\sqrt{n}}-t\\leq s_{\\operatorname*{min}}(\\mathbf{A})\\leq s_{\\operatorname*{max}}(\\mathbf{A})\\leq{\\sqrt{N}}+C_{9}{\\sqrt{n}}+t.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Theorem H.5 (Theorem 9 in [27]). Let A be an $N\\times n$ matrix with i.i.d. columns $\\mathbf{C}_{i}\\ \\in\\ \\mathbb{R}^{N}$ of sub-Gaussian random vector with covariance $\\pmb{\\Sigma}\\ {\\stackrel{\\mathrm{def.}}{=}}\\ \\mathbb{E}\\left[\\mathbf{C}_{i}\\mathbf{C}_{i}^{\\top}\\right]\\in\\mathbb{R}^{N\\times N}$ . Then there exists $a$ constant $c_{1}>0$ , such that for any $t\\geq0,$ , with probability at least $1-e^{-t}$ , we have ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\left\\|\\frac1n\\mathbf{A}^{\\top}-\\Sigma\\right\\|_{o p}\\le c_{1}\\left\\|\\Sigma\\right\\|_{o p}\\operatorname*{max}\\left\\{\\frac r n,\\sqrt{\\frac r n},\\frac t n,\\sqrt{\\frac t n}\\right\\},\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "where r = $\\begin{array}{r}{r=\\frac{\\mathrm{Tr}[\\pmb{\\Sigma}]}{\\|\\pmb{\\Sigma}\\|_{o p}}}\\end{array}$ \u2225Tr\u03a3[\u2225\u03a3] is the effective rank of the covariance \u03a3. ", "page_idx": 49}, {"type": "text", "text": "Remark H.6. Theorem H.5 is different from Theorem H.2 in a few ways: first, the upper bound in the former one contains the term $\\frac{n}{N}$ , which requires $n<N$ in order to obtain a good concentration, while that in the latter one contains the term kN=n1 \u03a3kk, which can still be bounded if N > n and the decay of $\\Sigma_{k k}$ is fast enough; second, former one requires i.i.d. columns while the latter one requires only independent rows. ", "page_idx": 49}, {"type": "text", "text": "Lemma H.7 (Sub-Exponential Deviation, see Corollary 5.17 in [47]). Let $N\\in\\mathbb{N}$ . Let $X_{1},...,X_{N}$ be independent centered random variables with sub-exponential norms bounded by $B$ . Then for any $\\delta>0$ , ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left\\{|\\sum_{i=1}^{N}X_{i}|>\\delta N\\right\\}\\leq2\\exp\\left(-C_{7}\\operatorname*{min}\\left\\{\\frac{\\delta^{2}}{B^{2}},\\frac{\\delta}{B}\\right\\}N\\right),\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "15To be precise, we set $C_{5}=(8e^{2})^{-1}\\cdot G^{-4}$ and $C_{6}=2e\\sqrt{2\\log9}\\cdot G^{2}$ . ", "page_idx": 49}, {"type": "text", "text": "where $C_{7}>0$ is an absolute constant. ", "page_idx": 50}, {"type": "text", "text": "In particular, if $X\\sim\\chi(N)$ is the Chi-square distribution, then $\\begin{array}{r}{\\mathbb{P}\\left\\{\\left|\\frac{X}{N}-1\\right|>t\\right\\}\\le2e^{-N t^{2}/8}}\\end{array}$ , $\\forall t\\in$ $(0,1)$ . ", "page_idx": 50}, {"type": "text", "text": "Lemma H.8 (Lemma 28 in [46]; Lemma 14 in [7]). Suppose that for some $k<N$ , the matrix ${\\bf A}_{k}$ is positive definite, then the following inequality holds: ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{B}\\leq\\|\\theta_{>k}^{*}\\|_{\\Sigma_{>k}}^{2}+\\frac{s_{1}(\\mathbf{A}_{k}^{-1})^{2}}{s_{n}(\\mathbf{A}_{k}^{-1})^{2}}\\frac{s_{1}(\\mathbf{Z}_{\\leq k}^{\\top}\\mathbf{Z}_{\\leq k})}{s_{k}(\\mathbf{Z}_{\\leq k}^{\\top}\\mathbf{Z}_{\\leq k})^{2}}\\,\\|\\mathbf{X}_{>k}\\theta_{>k}^{*}\\|_{2}^{2}}\\\\ &{\\quad+\\,\\frac{\\|\\theta_{\\leq k}^{*}\\|_{\\Sigma_{\\leq k}^{-1}}^{2}}{s_{n}(\\mathbf{A}_{k}^{-1})^{2}s_{k}(\\mathbf{Z}_{\\leq k}^{\\top}\\mathbf{Z}_{\\leq k})^{2}}}\\\\ &{\\quad+\\,\\|\\Sigma_{>k}\\|_{\\sigma^{p}}s_{1}(\\mathbf{A}_{k}^{-1})\\,\\|\\mathbf{X}_{>k}\\theta_{>k}^{*}\\|_{2}^{2}}\\\\ &{\\quad+\\,\\|\\Sigma_{>k}\\|_{\\sigma}\\,\\frac{s_{1}(\\mathbf{A}_{k}^{-1})}{s_{n}(\\mathbf{A}_{k}^{-1})^{2}}\\frac{s_{1}(\\mathbf{Z}_{\\leq k}^{\\top}\\mathbf{Z}_{\\leq k})}{s_{k}(\\mathbf{Z}_{\\leq k}^{\\top}\\mathbf{Z}_{\\leq k})^{2}}\\,\\|\\theta_{\\leq k}^{*}\\|_{\\Sigma_{\\leq k}^{-1}}^{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Lemma H.9 (Lemma 27 in [46]; Lemma 13 in [7]). For any $k\\in\\mathbb{N}$ , we have ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{V}/\\sigma^{2}\\leq\\frac{s_{1}\\left(\\mathbf{A}_{k}^{-1}\\right)^{2}\\operatorname{Tr}\\left[\\mathbf{X}\\leq k\\sum_{\\leq k}^{-1}\\mathbf{X}_{\\leq k}^{\\top}\\right]}{s_{n}\\left(\\mathbf{A}_{k}^{-1}\\right)^{2}s_{k}\\left(\\sum_{\\leq k}^{-1/2}\\mathbf{X}_{\\leq k}^{\\top}\\mathbf{X}_{\\leq k}\\mathbf{Z}_{\\leq k}^{-1/2}\\right)^{2}}+s_{1}(\\mathbf{A}_{k}^{-1})^{2}\\operatorname{Tr}[\\mathbf{X}_{>k}\\mathbf{Z}_{>k}\\mathbf{X}_{>k}^{\\top}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": ", where $\\sigma^{2}\\;\\overset{\\mathrm{def.}}{=}\\mathbb{E}\\;\\big[\\epsilon^{2}\\big]\\geq0$ . ", "page_idx": 50}, {"type": "text", "text": "Then the square norms  \u03b8\u2217>k  2\u03a3>k and  \u03b8\u2217\u2264k  2\u03a3\u2212\u22641k have bounds: Lemma H.10 (Lemma 18 in [7]). Assume $\\lambda_{k}=\\Theta_{k}\\left(k^{-1-a}\\right)$ and $|\\theta_{k}^{*}|={\\mathcal{O}}_{k}\\,(k^{-r}).$ for some $a,r>0$ . ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\theta_{>k}^{*}\\left\\|_{\\mathbf{Z}_{>k}}^{2}=\\Theta_{k}\\left(k^{-2r-a}\\right),\\qquad\\left\\|\\theta_{\\leq k}^{*}\\right\\|_{\\mathbf{Z}_{\\leq k}^{-1}}^{2}=\\left\\{\\Theta_{k}\\left(\\log k\\right),\\qquad2r=2+a\\atop\\Theta_{k}\\left(1\\right),\\qquad2r>2+a\\right.=\\tilde{\\Theta}_{k}\\left(k^{(2+a-2r)}+\\right),\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "where $(x)_{+}\\ {\\stackrel{\\mathrm{def.}}{=}}\\ \\operatorname*{max}\\{x,0\\}$ ", "page_idx": 50}, {"type": "text", "text": "norms  \u03b8\u2217>k  2\u03a3>k and  \u03b8\u2217\u2264k  2\u03a3\u2212\u22641k h $\\lambda_{k}=\\Theta_{k}\\left(e^{-a k}\\right)$ ave bounds: and $|\\theta_{k}^{*}|=\\mathcal{O}_{k}\\left(e^{-r k}\\right)$ for some $a,r>0$ . Then the square ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\|\\theta_{>k}^{*}\\|_{\\Sigma_{>k}}^{2}=\\Theta_{k}\\left(k^{-2r-a}\\right),\\qquad\\left\\|\\theta_{\\leq k}^{*}\\right\\|_{\\Sigma_{\\leq k}^{-1}}^{2}\\leq\\left\\{\\Theta_{k}\\left(k^{(a-2r)}\\right),\\quad2r<a}\\\\ {\\Theta_{k}\\left(1\\right),\\qquad}&{2r>a}\\end{array}\\right.=k^{\\tilde{\\Theta}_{k}\\left(\\left(a-2r\\right)+\\right)}.\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Lemma H.12. Recall that ", "page_idx": 50}, {"type": "equation", "text": "$$\nr_{k}\\overset{\\mathrm{def.}}{=}\\frac{\\mathrm{Tr}[\\pmb{\\Sigma}_{>k}]}{\\|\\pmb{\\Sigma}_{>k}\\|_{o p}},\\quad R_{k}\\overset{\\mathrm{def.}}{=}\\frac{\\mathrm{Tr}[\\pmb{\\Sigma}_{>k}]^{2}}{\\mathrm{Tr}[\\pmb{\\Sigma}_{>k}^{2}]}.\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "If $\\lambda_{k}=\\Theta_{k}\\left(k^{-1-a}\\right)$ for some $a>0$ , then ", "page_idx": 50}, {"type": "equation", "text": "$$\nr_{k}=\\Theta_{k}\\left(k\\right),\\;\\;\\;\\;R_{k}=\\Theta_{k}\\left(k\\right).\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "$l f\\,\\lambda_{k}=\\Theta_{k}\\left(e^{-a k}\\right)$ for some $a>0$ , then ", "page_idx": 50}, {"type": "equation", "text": "$$\nr_{k}=\\Theta_{k}\\left(1\\right),\\;\\;\\;\\;\\;R_{k}=\\Theta_{k}\\left(1\\right).\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Proof. By simple calculus, $\\begin{array}{r l r}{\\sum_{l=k+1}^{p}k^{-1-a}}&{=}&{\\Theta_{k}\\left(\\int_{k}^{\\infty}t^{-1-a}d t\\right)\\;\\;=\\;\\;\\Theta_{k}\\left(t^{-a}\\right)}\\end{array}$ , similarly, $\\begin{array}{r}{\\sum_{l=k+1}^{p}k^{-2-2a}\\,=\\,\\Theta_{k}\\left(\\int_{k}^{\\infty}t^{-2-2a}\\dot{d t}\\right)\\,=\\,\\Theta_{k}\\left(t^{-1-2a}\\right)}\\end{array}$ . If $\\lambda_{k}\\,=\\,\\Theta_{k}\\left(k^{-1-a}\\right)$ for some $a\\,>\\,0$ then ", "page_idx": 50}, {"type": "equation", "text": "$$\nr_{k}=\\Theta_{k}\\left(\\frac{k^{-a}}{\\lambda_{k+1}}\\right)=\\Theta_{k}\\left(k\\right),\\,\\quad R_{k}=\\Theta_{k}\\left(\\frac{k^{-2a}}{k^{-1-2a}}\\right)=\\Theta_{k}\\left(k\\right).\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "If $\\lambda_{k}=\\Theta_{k}\\left(e^{-a k}\\right)$ , then $\\begin{array}{r}{\\sum_{l=k+1}^{p}\\lambda_{l}=\\Theta_{k}\\left(\\int_{k}^{\\infty}e^{-a t}d t\\right)=\\Theta_{k}\\left(e^{-a k}\\right)}\\end{array}$ . Hence, ", "page_idx": 51}, {"type": "equation", "text": "$$\nr_{k}=\\Theta_{k}\\left(\\frac{e^{-a k}}{e^{-a\\left(k+1\\right)}}\\right)=\\Theta_{k}\\left(1\\right),\\,\\quad R_{k}=\\Theta_{k}\\left(\\frac{e^{-2a k}}{e^{-2a k}}\\right)=\\Theta_{k}\\left(1\\right).\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Lemma H.13 (Lemma 7 in [46]). Suppose Assumption $(I F)$ holds. Then there exists some constant $c>0$ such that, for any $k<n/c,$ , with probability at least $1-c e^{-n/c}$ , its holds that ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\mathcal{V}\\ge\\frac{1}{c n}\\sum_{l=1}\\operatorname*{min}\\left\\{1,\\frac{\\lambda_{l}^{2}}{\\lambda_{k+1}^{2}(1+r_{k}/n)^{2}}\\right\\}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Lemma H.14 (Lemma 8 in [46]). Suppose Assumption $(P S)$ holds. Furthermore, $i f$ Assumption $(G F)$ (or resp. $(I F)$ ) holds, then with probability $1-1$ , the following inequality holds: ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\theta^{*}\\sim\\vartheta}\\left[\\mathcal{B}\\right]\\geq\\sum_{l=1}^{p}\\frac{\\lambda_{l}\\mathbb{E}_{\\theta^{*}}\\left[(\\theta_{l}^{*})^{2}\\right]}{\\left(1+\\lambda_{l}s_{n}(\\mathbf{A}_{-l})^{-1}\\left\\|\\mathbf{z}^{(l)}\\right\\|_{2}^{2}\\right)^{2}}.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "where the expectation is taken as described in Assumption $(P S)$ , and $\\begin{array}{r}{{\\bf A}_{-l}\\stackrel{\\mathrm{def.}}{=}\\sum_{l^{\\prime}\\ne l}\\lambda_{l^{\\prime}}{\\bf z}^{(l^{\\prime})}({\\bf z}^{(l^{\\prime})})^{\\top}+}\\end{array}$ $n\\lambda\\mathbf{I}_{n}\\in\\mathbb{R}^{n\\times n}$ where $\\mathbf{z}^{(l)}\\in\\mathbb{R}^{n}$ denotes the l-th column of the whitened feature block $\\mathbf{Z}\\stackrel{\\mathrm{def.}}{=}\\mathbf{X}\\boldsymbol{\\Sigma}^{-1/2}$ . ", "page_idx": 51}, {"type": "text", "text": "Proof. By Assumption (PS), the expected value over $\\theta^{*}$ on the bias admits the following expression as in line (40): ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\theta^{*}}\\left[\\mathcal{B}\\right]=\\mathbb{E}_{\\theta^{*}}\\left[\\left\\lVert(\\mathbf{I}_{p}-\\mathbf{P}_{\\lambda})\\theta^{*}\\right\\rVert_{\\Sigma}^{2}\\right]=\\sum_{l=1}^{p}[(\\mathbf{I}_{p}-\\mathbf{P}_{\\lambda})\\Sigma(\\mathbf{I}_{p}-\\mathbf{P}_{\\lambda})]_{l l}\\cdot\\mathbb{E}_{\\theta^{*}}\\left[(\\theta_{l}^{*})^{2}\\right]\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "where P $\\mathbf{\\Sigma}_{\\lambda}\\ {\\stackrel{\\mathrm{def.}}{=}}\\ \\mathbf{X}^{\\top}(\\mathbf{X}\\mathbf{X}^{\\top}+n\\lambda\\mathbf{I}_{p})^{-1}\\mathbf{X}$ . Denote $\\mathbf{z}^{(l)}\\in\\mathbb{R}^{n}$ be the $l$ -th column of the whitened feature block $\\mathbf{Z}\\stackrel{\\mathrm{def.}}{=}\\mathbf{X}\\mathbf{\\Sigma}^{-1/2}\\in\\mathbb{R}^{n\\times p}$ . Then the $l$ -th diagonal element of the matrix $(\\mathbf{I}_{p}-\\mathbf{P}_{\\lambda})\\Sigma(\\mathbf{I}_{p}-\\mathbf{P}_{\\lambda})$ can be written as ", "page_idx": 51}, {"type": "equation", "text": "$$\n[(\\mathbf{I}_{p}-\\mathbf{P}_{\\lambda})\\pmb{\\Sigma}(\\mathbf{I}_{p}-\\mathbf{P}_{\\lambda})]_{l l}=\\sum_{l^{\\prime}=1}^{p}\\lambda_{l}\\left(1-\\lambda_{l}(\\mathbf{z}^{(l)})^{\\top}\\mathbf{A}^{-1}\\mathbf{z}^{(l)}\\right)^{2}+\\sum_{l^{\\prime}\\neq l}\\lambda_{l}\\lambda_{l^{\\prime}}^{2}\\left((\\mathbf{z}^{(i)})^{\\top}\\mathbf{A}^{-1}\\mathbf{z}^{(l^{\\prime})}\\right)^{2}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "where $\\mathbf{A}\\,{\\stackrel{\\mathrm{def.}}{=}}\\,\\mathbf{X}\\mathbf{X}^{\\top}+n\\lambda\\mathbf{I}_{p}$ . If we write $\\mathbf{A}_{-l}\\overset{\\mathrm{def.}}{=}\\mathbf{A}-\\lambda_{l}\\mathbf{z}^{(l)}(\\mathbf{z}^{(l)})^{\\top}$ , by Sherman-Morrison-Woodbury formula, we have ", "page_idx": 51}, {"type": "equation", "text": "$$\n1-\\lambda_{l}(\\mathbf{z}^{(l)})^{\\top}\\mathbf{A}^{-1}\\mathbf{z}^{(l)}=\\frac{1}{1+\\lambda_{l}(\\mathbf{z}^{(l)})^{\\top}\\mathbf{A}_{-l}^{-1}\\mathbf{z}^{(l)}},\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "and hence the averaged bias is bounded by: ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\theta^{*}}\\left[{\\mathcal{B}}\\right]\\geq\\displaystyle\\sum_{l=1}^{p}\\frac{\\lambda_{l}\\mathbb{E}_{\\theta^{*}}\\left[(\\theta_{l}^{*})^{2}\\right]}{\\left(1+\\lambda_{l}(\\mathbf{z}^{(l)})^{\\top}\\mathbf{A}_{-l}^{-1}\\mathbf{z}^{(l)}\\right)^{2}}}\\\\ &{\\qquad\\ge\\displaystyle\\sum_{l=1}^{p}\\frac{\\lambda_{l}\\mathbb{E}_{\\theta^{*}}\\left[(\\theta_{l}^{*})^{2}\\right]}{\\left(1+\\lambda_{l}s_{1}(\\mathbf{A}_{-l}^{-1})\\left\\Vert\\mathbf{z}^{(l)}\\right\\Vert_{2}^{2}\\right)^{2}}}\\\\ &{\\qquad=\\displaystyle\\sum_{l=1}^{p}\\frac{\\lambda_{l}\\mathbb{E}_{\\theta^{*}}\\left[(\\theta_{l}^{*})^{2}\\right]}{\\left(1+\\lambda_{l}s_{n}(\\mathbf{A}_{-l})^{-1}\\left\\Vert\\mathbf{z}^{(l)}\\right\\Vert_{2}^{2}\\right)^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Lemma H.15 (Lemma 9 in [6]). Suppose that $\\{X_{k}\\}_{k=1}^{p}$ is a sequence of non-negative random variables, and that $\\{t_{k}\\}_{k=1}^{p}$ is sequence of non-negative real numbers (with at least one of which strictly positive), such that for some $\\delta\\,\\in\\,(0,1)$ , with a probability at least $1-\\delta$ , $\\eta_{k}>t_{k}$ for all $k=1,...,p$ . Theb with probability at least $1-2\\delta$ , ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{p}\\eta_{k}\\geq\\frac{1}{2}\\sum_{k=1}^{p}t_{k}.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Lemma H.16 (Theorem 10 in [46]). Let $k\\in\\mathbb{N}$ be an integer. Denote ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\underline{{B}}\\stackrel{\\mathrm{def}}{=}\\displaystyle\\sum_{l=1}^{p}\\frac{\\lambda_{l}|\\theta_{l}^{*}|^{2}}{(1+\\frac{n\\lambda_{l}}{\\lambda_{k+1}r_{k}})^{2}},}\\\\ &{\\displaystyle\\overline{{B}}\\stackrel{\\mathrm{def}}{=}\\|\\theta_{\\leq k}^{*}\\|_{\\Sigma_{\\geq k}}^{2}+\\left(\\frac{n\\lambda+\\mathrm{Tr}[\\Sigma_{>k}]}{n}\\right)^{2}\\|\\theta_{\\leq k}^{*}\\|_{\\Sigma_{\\leq k}^{-1}}^{2},}\\\\ &{\\displaystyle\\underline{{V}}\\stackrel{\\mathrm{def}}{=}\\frac{1}{n}\\sum_{l=1}^{p}\\operatorname*{min}\\left\\{1,\\frac{\\lambda_{l}^{2}}{\\lambda_{k+1}^{2}(1+r_{k}/n)^{2}}\\right\\},}\\\\ &{\\displaystyle\\overline{{V}}\\stackrel{\\mathrm{def}}{=}\\frac{k}{n}+\\frac{n\\,\\mathrm{Tr}[\\Sigma_{>k}^{2}]}{(n\\lambda+\\mathrm{Tr}[\\Sigma_{\\geq k}])^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Fix constants $a>0$ and $\\textstyle b>{\\frac{1}{n}}$ . There exists a constant $c>0$ that only depends on $a,b$ such that: if either $\\begin{array}{r}{\\frac{r_{k}}{n}\\in(a,b)}\\end{array}$ or $k=\\operatorname*{min}\\{\\kappa:r_{\\kappa}>b n\\}$ , then ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r}{c^{-1}\\leq\\underline{{B}}/\\overline{{B}}\\leq1,}\\\\ {c^{-1}\\leq\\underline{{V}}/\\overline{{V}}\\leq1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Lemma H.17 (Corde\u2019s inequality, [18]). For any positive definite symmetric matrices $^{16}\\,\\mathbf{M}_{1},\\mathbf{M}_{2}$ and positive number m \u2208[0, 1], it holds that \u2225M1m M2m \u2225op \u2264\u2225M1M2\u2225omp. ", "page_idx": 52}, {"type": "text", "text": "Proposition H.18. Let $\\lambda>0$ . The bias term $\\mathcal{B}\\stackrel{\\mathrm{def.}}{=}\\mathbb{E}_{\\mathbf{x}}\\left[\\left(\\mathbf{x}^{\\top}\\hat{\\pmb{\\theta}}(\\mathbf{X}\\pmb{\\theta}^{*})-\\mathbf{x}^{\\top}\\pmb{\\theta}^{*}\\right)^{2}\\right]$ has the following expression: ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\boldsymbol{\\mathcal{B}}=\\lambda^{2}\\left\\|(\\lambda\\mathbf{I}_{p}+\\hat{\\boldsymbol{\\Sigma}})^{-1}\\pmb{\\theta}^{*}\\right\\|_{\\pmb{\\Sigma}}^{2}.\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "where $\\hat{\\Sigma}\\overset{\\mathrm{def.}}{=}\\frac{1}{n}\\mathbf{X}^{\\top}\\mathbf{X}$ . ", "page_idx": 52}, {"type": "text", "text": "Proof. By definition, rewrite the bias into: ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{B}=\\left\\|\\boldsymbol{\\theta}^{*}-\\hat{\\boldsymbol{\\theta}}(\\mathbf{X}\\boldsymbol{\\theta}^{*})\\right\\|_{\\mathbf{\\hat{X}}}^{2}}\\\\ &{\\quad=\\left\\|\\boldsymbol{\\theta}^{*}-\\mathbf{X}^{\\top}(\\mathbf{X}\\mathbf{X}^{\\top}+n\\lambda\\mathbf{I}_{n})^{-1}(\\mathbf{X}\\boldsymbol{\\theta}^{*})\\right\\|_{\\mathbf{\\hat{X}}}^{2}}\\\\ &{\\quad=\\left\\|\\left(\\mathbf{I}_{p}-\\underbrace{\\mathbf{X}^{\\top}(\\mathbf{X}\\mathbf{X}^{\\top}+n\\lambda\\mathbf{I}_{n})^{-1}\\mathbf{X}}_{\\mathbf{P}_{\\lambda}}\\right)\\boldsymbol{\\theta}^{*}\\right\\|_{\\Sigma}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Denote $\\mathbf{P}_{\\lambda}\\ {\\stackrel{\\mathrm{def.}}{=}}\\ \\mathbf{X}^{\\top}(\\mathbf{X}\\mathbf{X}^{\\top}+n\\lambda\\mathbf{I}_{n})^{-1}\\mathbf{X}\\in\\mathbb{R}^{p\\times p}$ . By Sherman-Morrison-Woodbury formula, ", "page_idx": 52}, {"type": "equation", "text": "$$\np-\\mathbf{P}_{\\lambda}=\\mathbf{I}_{p}-(n\\lambda)^{-1}\\mathbf{X}^{\\top}\\left(\\mathbf{I}_{n}+(n\\lambda)^{-1}\\mathbf{X}\\mathbf{X}^{\\top}\\right)^{-1}\\mathbf{X}=\\lambda\\left(\\lambda I_{p}+{\\frac{1}{n}}\\mathbf{X}^{\\top}\\mathbf{X}\\right)^{-1}=\\lambda\\left(\\lambda I_{p}+{\\dot{\\pmb{\\Sigma}}}\\right)^{-1}.\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Proposition H.19 (Variance expression). The variance term $\\mathcal{V}\\stackrel{\\mathrm{def.}}{=}\\mathbb{E}_{\\mathbf{x},\\epsilon}\\left[\\left(\\mathbf{x}^{\\top}\\hat{\\pmb{\\theta}^{*}}(\\epsilon)\\right)^{2}\\right]$ has the following expression: ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\mathcal{V}=\\frac{\\sigma^{2}}{n}\\operatorname{Tr}\\left[\\left(\\hat{\\Sigma}+\\lambda\\mathbf{I}_{p}\\right)^{-1}\\Sigma\\left(\\hat{\\Sigma}+\\lambda\\mathbf{I}_{p}\\right)^{-1}\\hat{\\Sigma}\\right]=\\frac{\\sigma^{2}}{n}\\mathbb{E}_{\\mathbf{x}\\sim\\mu}\\left[\\left\\Vert\\left(\\hat{\\Sigma}+\\lambda\\mathbf{I}_{p}\\right)^{-1}\\mathbf{x}\\right\\Vert_{\\hat{\\Sigma}}^{2}\\right].\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "16Or equivalently positive definite self-adjoint operator in a Hilbert space. ", "page_idx": 52}, {"type": "text", "text": "Proof. By definition, ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\mathcal{E}}:=\\mathbb{E}_{\\mathbf{x}}\\left[\\left\\|\\hat{\\mathbf{G}}^{T}\\right\\|_{1}^{2}\\right]}\\\\ &{=\\mathbb{E}_{\\mathbf{x}}\\left[\\left\\{\\mathbf{X}^{T}(\\mathbf{X}\\mathbf{X}^{T}+\\hat{\\mathbf{n}}\\mathbf{n})\\mathbf{\\hat{J}}^{-1}\\mathbf{X}\\mathbf{X}^{T}(\\mathbf{X}\\mathbf{X}^{T}+i\\mathbf{n}\\mathbf{\\hat{L}}_{0}^{\\top})\\mathbf{\\hat{J}}^{-1}\\right\\}\\right]}\\\\ &{=\\mathbb{E}_{\\mathbf{x}}\\left[\\left\\|\\mathbf{X}^{T}(\\mathbf{X}\\mathbf{X}^{T}+i\\mathbf{n}\\mathbf{\\hat{A}}_{0})\\mathbf{\\hat{J}}^{-1}\\mathbf{X}\\mathbf{X}^{T}(\\mathbf{X}\\mathbf{X}^{T}+i\\mathbf{n}\\mathbf{\\hat{A}}_{0})\\right\\|^{-1}\\right]}\\\\ &{=\\mathbb{E}_{\\mathbf{x}}\\left[\\left\\|\\mathbf{X}^{T}(\\mathbf{X}\\mathbf{X}^{T}+i\\mathbf{n}\\mathbf{\\hat{A}}_{0})\\mathbf{\\hat{J}}^{-1}\\mathbf{X}\\mathbf{X}^{T}(\\mathbf{X}\\mathbf{X}^{T}+i\\mathbf{n}\\mathbf{\\hat{A}}_{0})\\right\\|^{-1}\\right]}\\\\ &{=\\mathcal{E}^{T}\\left[\\mathbf{X}^{T}(\\mathbf{X}\\mathbf{X}^{T}+i\\mathbf{n}\\mathbf{\\hat{A}}_{0})\\mathbf{\\hat{J}}^{-1}\\mathbf{X}^{T}(\\mathbf{X}\\mathbf{X}^{T}+i\\mathbf{n}\\mathbf{\\hat{A}}_{0})\\mathbf{\\hat{J}}^{-1}\\mathbf{X}^{T}(\\mathbf{X}\\mathbf{X}^{T}+i\\mathbf{n}\\mathbf{\\hat{A}}_{0})\\right]}\\\\ &{=\\sigma^{2}\\mathbb{E}_{\\mathbf{x}}\\left[\\left\\|\\mathbf{X}^{T}(\\mathbf{X}\\mathbf{X}^{T}+\\mathbf{n}\\mathbf{\\hat{A}}_{0})\\mathbf{\\hat{J}}^{-1}\\mathbf{X}^{T}(\\mathbf{X}\\mathbf{X}^{T}+\\mathbf{n}\\mathbf{\\hat{A}}_{0})\\right\\|^{-1}\\right]}\\\\ &{=\\frac{\\sigma^{2}}{n}\\mathbb{E}_{\\mathbf{x}}\\left[\\left(\\frac{1}{n}\\mathbf{X}^{T}\\mathbf{X}+\\mathbf{X}\\mathbf{\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "I Experiments in details ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Some of the results in Table 1 have already been validated by experiments in previous literature. Hence, this section will focus on the novel result of this paper: ", "page_idx": 54}, {"type": "text", "text": "1. Same decay rate for independent (IF)/generic (GF) features under strong ridge;   \n2. Decay of $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ for $s<1$ under weak ridge;   \n3. Decay of $\\nu$ under weak ridge (tempered vs catastrophic overfitting). ", "page_idx": 54}, {"type": "text", "text": "All experiments were conducted on a computer with a $2.3\\:\\mathrm{GHz}$ Quad-Core Intel Core i7 processor.   \nThe code for the experiments is available in the supplementary materials. ", "page_idx": 54}, {"type": "text", "text": "I.1 Bias under strong ridge ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "we first consider a simple example: let $\\begin{array}{r}{\\lambda_{k}\\,=\\,(\\frac{2k-1}{2}\\pi)^{-1-a}}\\end{array}$ , $\\begin{array}{r}{\\psi_{k}(\\cdot)=\\sqrt{2}\\sin\\left(\\frac{2k-1}{2}\\pi\\cdot\\right)}\\end{array}$ such that $\\|\\psi_{k}\\|_{L_{\\mu}^{2}}\\,=\\,1$ for $\\mu\\,=\\,\\operatorname{unif}[0,1]$ ; let $\\begin{array}{r}{\\theta_{k}^{*}\\,=\\,(\\frac{2k-1}{2}\\pi)^{-r}}\\end{array}$ . For $p\\,=\\,\\infty$ and $a\\,=\\,1$ , the regression coincides with the kernel ridge regression with kernel $k(x,x^{\\prime})=\\operatorname*{min}\\{x,x^{\\prime}\\}$ defined on the interval $[0,1]$ by [48]. [30, 33] have conducted similar experiments on this kernel $k$ . However, to simulate the regression for independent features (IF), the feature rank $p$ has to be finite. In the following experiment, we choose p = 2000, the sample size n ranges from 100 to 1000, ridge \u03bb = \u03bbn = ( 2n2\u22121 . ", "page_idx": 54}, {"type": "text", "text": "The first thing to check is whether both independent features ((IF)) and generic features ((GF)) satisfy ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\boldsymbol{B}=\\mathcal{O}\\left(\\boldsymbol{n}^{-b\\tilde{s}}\\right)\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "under strong ridge, where $\\tilde{s}\\ {\\stackrel{\\mathrm{def.}}{=}}\\ \\operatorname*{min}s,2$ and $\\begin{array}{r}{s\\,=\\,\\frac{2a+r}{1+a}}\\end{array}$ . To accurately obtain the bias term $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ , we compute the exact formula: ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\boldsymbol{\\mathcal{B}}=\\lambda^{2}\\left\\|(\\lambda\\mathbf{I}_{p}+\\hat{\\Sigma})^{-1}\\pmb{\\theta}^{*}\\right\\|_{\\Sigma}^{2},\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "as shown in Proposition H.18, rather than computing the squared difference $\\left({\\hat{f}}(x)-f^{*}(x)\\right)^{2}$ by evaluating on test points as done in [31], or by using an integral function as in [33]. To demonstrate the Gaussian Equivalent Property (GEP), we also compute $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ after replacing the Sine feature vector $\\pmb{\\psi}\\,=\\,\\psi_{k}(\\boldsymbol{x})_{k=1}^{p}$ with a random Gaussian vector $\\mathbf{z}\\ \\sim\\mathcal{N}(0,1)$ or a random Rademacher vector $\\mathbf{z}\\sim(\\mathrm{unif}\\{\\pm1\\})^{p}$ . It is worth noting that the random Rademacher vector $\\mathbf{z}\\sim(\\mathrm{unif}\\{\\pm1\\})^{p}$ satisfies Assumption $(\\mathrm{IF})$ . This shows that our statement holds more generally than under the Gaussian Design Assumption ((GD)). From Figure 5, we observe that for different choices of $a$ and $r$ (and hence $s$ ), the bias decays at its theoretical rate for all three different features. ", "page_idx": 54}, {"type": "image", "img_path": "IMlDpZmLnL/tmp/deb8ec9d23f4c13c2557009f852e0fff31ac3838e96daed09df0e948b846cd59.jpg", "img_caption": [], "img_footnote": [], "page_idx": 54}, {"type": "text", "text": "Figure 5: Decay of the bias term $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ under strong ridge $\\lambda=\\lambda_{n}=\\Theta\\left(n^{-1-a}\\right)$ . $\\begin{array}{r}{\\lambda_{k}=\\left(\\frac{2k-1}{2}\\pi\\right)^{-1-a}}\\end{array}$ , $\\begin{array}{r}{\\theta_{k}^{*}=(\\frac{2k-1}{2}\\pi)^{-r}}\\end{array}$ . Theoretical decay $\\mathcal{B}=\\mathcal{O}\\left(n^{-(1+a)\\tilde{s}}\\right)=\\mathcal{O}\\left(n^{-(\\dot{1}+a)\\tilde{s}}\\right)$ , where $\\tilde{s}=\\operatorname*{min}\\{s,2\\}$ , source coefficient $\\begin{array}{r}{s=\\frac{2a+r}{1+a}}\\end{array}$ . (Left): $s=1.5$ and $\\mathcal{B}=\\mathcal{O}\\left(n^{-(1+1)\\operatorname*{min}\\{1.5,2\\}}\\right)=\\mathcal{O}\\left(n^{-3}\\right);$ ; (right): $s\\;=\\;2.33\\;>\\;2$ and $\\mathcal{B}\\;=\\;\\mathcal{O}\\left(n^{-(1+0.5)\\operatorname*{min}\\{2.33,2\\}}\\right)\\;=\\;\\mathcal{O}\\left(n^{-3}\\right)$ , showing the saturation effect mentioned in [29]. All features demonstrate the same theoretical decay, validating the GEP. ", "page_idx": 54}, {"type": "text", "text": "I.2 Bias under weak ridge ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "For weak ridge, we find that the decay rate is better than this theoretical rate (see Figure 6). One explanation for this is the estimation error of replacing the kernel $K(x,x^{\\prime})\\,=\\,\\mathrm{min}\\bar{\\{}x},x^{\\prime}\\}$ by its finite rank approximation; another reason is that the decay flattens for large sample sizes $n\\gg1000$ . However, the learning curve of independent features (Gaussian or Rademacher) behaves similarly to the dependent one (Sine). The left plot in Figure 6 ftis our theoretical result, as $s>1$ , while the right plot shows that our theoretical bound is too pessimistic. However, we suspect this is due to the fact that the eigenfunctions on the one-dimensional input space are simply sines, which are uniformly bounded. ", "page_idx": 55}, {"type": "image", "img_path": "IMlDpZmLnL/tmp/0070c6434f05dd2eb4278beff08a93569011f4989f3ee73d6eee1085770659e1.jpg", "img_caption": ["Figure 6: Decay of the bias term $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ under weak ridge. $\\lambda\\;=\\;\\Theta\\;{\\bigl(}n^{-b}{\\bigr)}$ . $\\begin{array}{r}{\\lambda_{k}~=~(\\frac{2k-1}{2}\\pi)^{-1-a}}\\end{array}$ , $\\begin{array}{r l r}{\\theta_{k}^{*}}&{{}=}&{(\\frac{2k-1}{2}\\pi)^{-r}}\\end{array}$ . Theoretical decay $\\mathcal{B}\\ =\\ \\mathcal{O}\\left(n^{-(1+a)\\tilde{s}}\\right)\\ =\\ \\mathcal{O}\\left(n^{-(1+a)\\tilde{s}}\\right)$ , where $\\begin{array}{r l}{\\tilde{s}}&{{}=}\\end{array}$ $\\operatorname*{min}\\{s,2\\}$ $\\begin{array}{r l r}{s}&{{}\\!=}&{\\!\\frac{2a+r}{1+a}}\\end{array}$ .al l (fLeeatftu)r:e $s~=~1.5~>~1$ hweiotrhe ttihceaol rebtoicuanld b $\\textit{\\textbf{B}}=$ $\\mathcal{O}\\left(n^{-(1+0.5)\\operatorname*{min}\\left\\{2.33,2\\right\\}}\\right)=\\mathcal{O}\\left(n^{-3}\\right)$ $s=0.8<1$ $\\mathcal{B}=\\mathcal{O}\\left(n^{-(1+0.25)\\operatorname*{min}\\left\\{0.8,2\\right\\}}\\right)=\\mathcal{O}\\left(n^{-1}\\right)$ for Gaussian and Rademacher (independent) features, the empirical result for Sine features is better than its theoretical bound $\\mathcal{B}=\\mathcal{O}\\left(n^{-(r-a)}\\right)=\\mathcal{O}\\left(n^{-0.25}\\right)$ . "], "img_footnote": [], "page_idx": 55}, {"type": "text", "text": "I.3 Variance under strong ridge ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Analogous to $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ , we want to check whether both independent features ((IF)) and generic features ((GF)) satisfy ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\gamma=\\mathcal{O}\\left(\\sigma^{2}n^{-1+\\frac{b}{1+a}}\\right),\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "under strong ridge, where $\\sigma^{2}\\,=\\,\\mathbb{E}\\left[\\epsilon^{2}\\right]$ is the noise level. To ease the computation, instead of computing the expression of $\\nu$ directly from Proposition H.19: ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\mathcal{V}}=\\frac{\\sigma^{2}}{n}\\operatorname{Tr}\\left[({\\hat{\\mathbf{\\Sigma}}}+\\lambda\\mathbf{I}_{p})^{-1}\\Sigma({\\hat{\\mathbf{\\Sigma}}}+\\lambda\\mathbf{I}_{p})^{-1}{\\hat{\\mathbf{\\Sigma}}}\\right]}}\\\\ {~~}\\\\ {{\\displaystyle~~=\\frac{\\sigma^{2}}{n}\\operatorname{Tr}\\left[\\Sigma({\\hat{\\mathbf{\\Sigma}}}+\\lambda\\mathbf{I}_{p})^{-1}{\\hat{\\mathbf{\\Sigma}}}({\\hat{\\mathbf{\\Sigma}}}+\\lambda\\mathbf{I}_{p})^{-1}\\right]}}\\\\ {~~}\\\\ {{\\displaystyle~~=\\frac{\\sigma^{2}}{n}\\operatorname{Tr}\\left[\\Sigma\\mathbf{U}\\mathbf{D}^{2}(\\mathbf{D}+\\lambda\\mathbf{I}_{p})^{-2}\\mathbf{U}^{\\top}\\right]}}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "where $\\hat{\\mathbf{\\Sigma}}\\hat{\\mathbf{\\Sigma}}=\\mathbf{UDU}^{\\top}$ is the singular value decomposition of $\\hat{\\Sigma}$ . Figure 7 confirms the Gaussian Equivalent Property (GEP) under strong ridge. ", "page_idx": 55}, {"type": "text", "text": "All dotted lines in Figures 5, 6 and 7 are regression of the learning curve with Gaussian features. ", "page_idx": 55}, {"type": "text", "text": "I.4 Variance under weak ridge ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "As reported in [7, 14], by setting $\\lambda=0$ , $\\hat{f}$ is indeed the norm minimum interpolant, which may demonstrates tempered or catastrophic overftiting as $n\\to\\infty$ . For this example, we focus on another setting where we take samples uniformly from a unit 2-disk and approximate $\\nu$ by evaluating the regressor of the zero function on the test point. We set $\\lambda=0$ and compute the kernel ridgeless ", "page_idx": 55}, {"type": "image", "img_path": "IMlDpZmLnL/tmp/c65e6d50c0b29d11a7554d8e827d976a96a68709908ad3d3986f0c6e2c02b964.jpg", "img_caption": ["Figure 7: Decay of the variance term $\\mathcal{V}$ under strong ridge. $\\lambda=\\Theta\\left(n^{-b}\\right)$ . $\\begin{array}{r}{\\lambda_{k}\\,=\\,(\\frac{2k-1}{2}\\pi)^{-1-a}}\\end{array}$ . Theoretical decay $\\nu=\\mathcal{O}\\left(n^{-1+\\frac{b}{1+a}}\\right)$ . (Left): Theoretical decay $\\nu=\\mathcal{O}\\left(n^{-1}\\right)$ for all features; (right): Theoretical decay $\\nu=\\mathcal{O}\\left(n^{-1/2}\\right)$ for all features. "], "img_footnote": [], "page_idx": 56}, {"type": "image", "img_path": "IMlDpZmLnL/tmp/bae1f8a457dc26b1c6e0f0323ec4fb509daf2dc8dc4bf9f90362b4a98fec3dcc.jpg", "img_caption": [], "img_footnote": [], "page_idx": 56}, {"type": "text", "text": "Figure 8: The variance against the sample size n under no ridge. (Left): Tempered overfitting with Laplacian kernel. (Right): Catastrophic overfitting with NTK. ", "page_idx": 56}, {"type": "text", "text": "regression with the Laplacian kernel $K(x,z)=e^{-\\|x-z\\|_{2}}$ and the neural tangent kernel $K(x,z)=$ $x^{\\top}z\\kappa_{0}(x^{\\top}z)\\!+\\!\\kappa_{1}(x^{\\top}z)$ where $\\begin{array}{r}{\\kappa_{0}(t)\\stackrel{\\mathrm{def.}}{=}1\\!-\\!\\frac{1}{\\pi}\\operatorname{arccos}(t),\\kappa_{1}(t)\\stackrel{\\mathrm{def.}}{=}\\frac{1}{\\pi}\\left(t(\\pi-\\operatorname{arccos}(t))+\\sqrt{1-t^{2}}\\right)\\!.}\\end{array}$ . In Figure 8, we can see that although having polynomial eigen-decay, the Laplacian kernel exhibits tempered overfitting, as termed by [35], while the NTK catastrophic overfitting. ", "page_idx": 56}, {"type": "table", "img_path": "IMlDpZmLnL/tmp/ef908596998a18323daf99a1b6601e1928e72572fbcdfb3ce24abfd12401f546.jpg", "table_caption": [], "table_footnote": [], "page_idx": 57}, {"type": "table", "img_path": "IMlDpZmLnL/tmp/7d1a81ce2315232f94767c991583179b04a3db8b7eaf08eda997af2ba43fa671.jpg", "table_caption": [], "table_footnote": ["Table 6: Various fliters for Table 1. (top): Recovered (in black), improved (in blue) and novel (denoted by \u201cthis paper\u201d) results over previous literature. (top 2): Recovered results under same assumptions. (top 3): Recovered results under weaker assumptions. (bottom): State-of-the-Art (SOTA) result. Black indicates results recovered results under our assumptions, blue indicates improved results over previous literature, orange indicates SOTA results from [31] under extra assumptions. "], "page_idx": 57}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 58}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 58}, {"type": "text", "text": "Justification: The claims asserted in Abstract and the questions raised in the introduction section 1 are directly addressed in the main section of our paper; namely in section 3 and the additional material in the appendix section. ", "page_idx": 58}, {"type": "text", "text": "Guidelines: ", "page_idx": 58}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 58}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 58}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Justification: This is a theoretical paper where assumptions are clearly stated. We admit that verifying the conditions of the assumptions (e.g., eigen-decay rate or eigen-functions of kernels in realistic datasets) can be challenging, and we limit our experiments to relatively simple examples. Further investigation into the generalization performance of kernel ridge regression on realistic datasets is left for future work. ", "page_idx": 58}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 58}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 59}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 59}, {"type": "text", "text": "Justification: All mathematical objects are clearly stated, and further explain, in section $\\S2.2$ . Guidelines: ", "page_idx": 59}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 59}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 59}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 59}, {"type": "text", "text": "Justification: The experiment is done on some synthetic settings, which details are clearly stated in I. ", "page_idx": 59}, {"type": "text", "text": "Guidelines: ", "page_idx": 59}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 59}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 60}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 60}, {"type": "text", "text": "Justification: The code for the experiments is uploaded as supplementary materials. ", "page_idx": 60}, {"type": "text", "text": "Guidelines: ", "page_idx": 60}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 60}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 60}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 60}, {"type": "text", "text": "Justification: Error bars are displayed in the figures in the paper. ", "page_idx": 60}, {"type": "text", "text": "Guidelines: ", "page_idx": 60}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 60}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 60}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 60}, {"type": "text", "text": "Justification: See Section I. ", "page_idx": 60}, {"type": "text", "text": "Guidelines: ", "page_idx": 60}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 60}, {"type": "text", "text": "", "page_idx": 61}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 61}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 61}, {"type": "text", "text": "Justification: All experiments were conducted on a computer with a 2.3 GHz Quad-Core Intel Core i7 processor. ", "page_idx": 61}, {"type": "text", "text": "Guidelines: ", "page_idx": 61}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 61}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 61}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 61}, {"type": "text", "text": "Justification: Since our paper is purely theoretical, it does not involve human subjects or participants, data-related concerns, nor are there any negative societal implications. Rather, by elucidating the theoretical foundations of some kernel ridge regression models, we help verify the reliability of commonly used machine learning models. ", "page_idx": 61}, {"type": "text", "text": "Guidelines: ", "page_idx": 61}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 61}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 61}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 61}, {"type": "text", "text": "Justification: This paper focuses on enhancing the theoretical underpinnings of machine learning, leading to predominantly positive societal impacts. By emphasizing the importance of principled machine learning and furthering our understanding of AI capabilities, it contributes to the advancement of technology in a constructive manner. ", "page_idx": 61}, {"type": "text", "text": "Guidelines: ", "page_idx": 61}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 62}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 62}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 62}, {"type": "text", "text": "Justification: As our contributions are theoretical justifications of commonly used AI models, this does not apply here. ", "page_idx": 62}, {"type": "text", "text": "Guidelines: ", "page_idx": 62}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 62}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 62}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 62}, {"type": "text", "text": "Justification: Our paper does not use existing assets. ", "page_idx": 62}, {"type": "text", "text": "Guidelines: ", "page_idx": 62}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 62}, {"type": "text", "text": "", "page_idx": 63}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 63}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 63}, {"type": "text", "text": "Justification: Our paper does not use new assets. ", "page_idx": 63}, {"type": "text", "text": "Guidelines: ", "page_idx": 63}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 63}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 63}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 63}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 63}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 63}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 63}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 63}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 63}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 63}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 64}]