{"references": [{"fullname_first_author": "Stephen Boyd", "paper_title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "publication_date": "2011-01-01", "reason": "This paper introduces the ADMM algorithm, a core solver used in CRONOS for its convergence guarantees and ability to leverage GPU acceleration."}, {"fullname_first_author": "Mert Pilanci", "paper_title": "Neural networks are convex regularizers: Exact polynomial-time convex optimization formulations for two-layer networks", "publication_date": "2020-01-01", "reason": "This foundational paper proves that training shallow neural networks can be formulated as a convex optimization problem, which is the basis for the convex reformulation used in CRONOS."}, {"fullname_first_author": "Aaron Mishkin", "paper_title": "Fast convex optimization for two-layer ReLU networks: Equivalent model classes and cone decompositions", "publication_date": "2022-01-01", "reason": "This paper presents a tractable convex program for training two-layer ReLU networks, providing a foundation for CRONOS's approach and demonstrating progress in solving the high-dimensional problem at scale."}, {"fullname_first_author": "Yatong Bai", "paper_title": "Efficient global optimization of two-layer ReLU networks: Quadratic-time algorithms and adversarial training", "publication_date": "2023-01-01", "reason": "This paper further develops the convex reformulation of two-layer ReLU networks, providing additional theoretical insights and algorithms that are relevant to CRONOS's approach."}, {"fullname_first_author": "Zachary Frangella", "paper_title": "Randomized Nystr\u00f6m preconditioning", "publication_date": "2023-01-01", "reason": "This paper introduces the Nystr\u00f6m preconditioning technique used in CRONOS to accelerate the solution of the linear system arising in the ADMM iterations, enabling scalability to large datasets."}]}