[{"type": "text", "text": "DAT: Improving Adversarial Robustness via Generative Amplitude Mix-up in Frequency Domain ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Fengpeng Li1 Kemou Li1 Haiwei $\\mathbf{W}\\mathbf{u}^{2}$ Jinyu Tian3 Jiantao Zhou1\u2217 ", "page_idx": 0}, {"type": "text", "text": "1State Key Laboratory of Internet of Things for Smart City, University of Macau 2Department of Computer Science, City University of Hong Kong 3Faculty of Innovation Engineering, Macau University of Science and Technology ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "To protect deep neural networks (DNNs) from adversarial attacks, adversarial training (AT) is developed by incorporating adversarial examples (AEs) into model training. Recent studies show that adversarial attacks disproportionately impact the patterns within the phase of the sample\u2019s frequency spectrum\u2014typically containing crucial semantic information\u2014more than those in the amplitude, resulting in the model\u2019s erroneous categorization of AEs. We find that, by mixing the amplitude of training samples\u2019 frequency spectrum with those of distractor images for AT, the model can be guided to focus on phase patterns unaffected by adversarial perturbations. As a result, the model\u2019s robustness can be improved. Unfortunately, it is still challenging to select appropriate distractor images, which should mix the amplitude without affecting the phase patterns. To this end, in this paper, we propose an optimized Adversarial Amplitude Generator (AAG) to achieve a better tradeoff between improving the model\u2019s robustness and retaining phase patterns. Based on this generator, together with an efficient AE production procedure, we design a new Dual Adversarial Training (DAT) strategy. Experiments on various datasets show that our proposed DAT leads to significantly improved robustness against diverse adversarial attacks. The source code is available at https:// github.com/Feng-peng-Li/DAT. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "DNNs have been successfully applied to various tasks [21, 32, 25]. However, recent studies reveal that DNNs are vulnerable to adversarial examples (AEs), created by applying subtle yet deceptive adversarial perturbations to benign samples [40, 51]. Such vulnerabilities have sparked considerable interests, leading to the development of numerous adversarial attacks designed to deceive DNNs [40, 19, 39, 13, 20, 5, 6]. Furthermore, serious concerns about the trustworthiness of artificial intelligence have been raised, due to these fundamental vulnerabilities [47, 37]. To mitigate these threats, adversarial training (AT) has been developed to enhance model robustness by incorporating AEs into training through a min-max strategy [40, 61, 38]. Based on the typical method, PGD-AT [40], a variety of AT strategies have been devised [38, 29, 37] (see Appendix B for related works). ", "page_idx": 0}, {"type": "text", "text": "For image signals transformed into the frequency domain using, e.g. the discrete Fourier transform (DFT), several AT works [59, 52, 53, 43] explore the adversarial attacks\u2019 behavior on sample\u2019s frequency spectrum. Frequency spectra consist of amplitude and phase; the amplitude typically captures stylistic information, whereas the phase encompasses richer semantics [7]. Recent studies [58, 63] find, as shown in Figure 1, that adversarial attacks often severely eliminate some semantics in the phase, making it difficult for models to extract features for correctly predicting AEs, while the impacts on amplitude are relatively mild. Moreover, by forcing the model to focus on phase patterns, [7, 63] confirm the model can learn more features unaffected by image corruptions, e.g., Gaussian noise and defocus blur, improving the model\u2019s performance on corrupted samples. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To explore the impact of adversarial perturbations on phase and amplitude patterns respectively, we conduct some studies (see Sec. 2 for details). We find that the standard model trained without AT has worse performance on samples with adversarial phase patterns than those with adversarial amplitude ones. Unlike the standard model, AT enhances the model\u2019s robustness against both phase- and amplitude-level adversarial perturbations, with a more noticeable improvement against adversarial perturbations on phase patterns. This indicates the potential for improving the model\u2019s robust", "page_idx": 1}, {"type": "image", "img_path": "TeQvz5AlI8/tmp/9dbbc5445b50c85dcb821a02b653f19a85a8e118bbf6c10b1524bddf71803587.jpg", "img_caption": ["Figure 1: The adversarial perturbation severely damages phase patterns (especially in red rectangular) and the frequency spectrum, while amplitude patterns are rarely impacted. The AE is generated by PGD-20 $\\ell_{\\infty}$ -bounded with radius $8/255$ . "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "ness by focusing on phase patterns unaffected by adversarial perturbations in AT. Then, by mixing the training samples\u2019 amplitude with randomly selected distractor, we observe that the robust model performance, particularly at adversarial phase patterns, is further enhanced, without impacting that at adversarial amplitude ones. This demonstrates that training samples with mixed amplitude improve the model\u2019s performance on AEs, and maintain the model\u2019s robustness on amplitude-level. ", "page_idx": 1}, {"type": "text", "text": "Inspired by these observations, we in this work propose a new Dual Adversarial Training (DAT) strategy by focusing on the phase patterns, with two adversarial procedures: adversarial amplitude generation and efficient AE production. Specifically, to guide the model to learn more phase patterns, we first attempt to mix the amplitude of training samples\u2019 frequency spectra with randomly selected distractor images. However, when the disparity between the distractor and the original image is too large, the recombined ones tend to disrupt original phase patterns, hindering the model from predicting AE correctly. Conversely, it is difficult for models to focus on phase patterns when the distractor closely resembles the original one [7]. To tackle this challenge, we propose an optimized Adversarial Amplitude Generator (AAG) to synthesize an adversarial amplitude, maximizing the model loss and limiting the model fitting amplitude patterns, thereby the model focusing on phase patterns for convergence. During the training process, the AAG and robust model are optimized jointly with the original and recombined images, together with their AEs. The robust model undergoes training by empirical risk minimization, and maximizes the model loss to update the AAG adversarially. Experiments across various benchmarks against a range of adversarial attacks confirm the superior effectiveness of our proposed DAT, surpassing state-of-the-art methods in robustness with big margins. ", "page_idx": 1}, {"type": "text", "text": "Contribution. The contributions of this work can be summarized as: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We verify that adversarial perturbations significantly influence phase patterns, resulting in the model\u2019s difficulty for predicting AEs correctly. Moreover, by mixing the amplitude of a training image with that of a distractor, we find that the model robustness against AEs can be enhanced. \u2022 We propose the novel DAT strategy with an optimized AAG to synthesize an adversarial amplitude. With the AAG, we enforce the model to better focus on phase patterns, enhancing the model\u2019s robustness. Also, an efficient AE generation module is incorporated to improve the AT\u2019s efficiency. \u2022 Experiments on multiple datasets confirm that DAT significantly enhances the model\u2019s robustness against a variety of adversarial attacks. Specifically, DAT increases the model\u2019s robustness by ${\\sim}2.1\\%$ on CIFAR-10, ${\\sim}2.2\\%$ on CIFAR-100, and ${\\sim}2.3\\%$ on Tiny ImageNet, on average. Notation. Let $\\mathbf{\\mathcal{D}}=\\{(\\mathbf{x}_{i},y_{i})\\}_{i=1}^{N}$ be a benign dataset comprising $N$ samples from $c$ classes, where each sample $\\mathbf{x}_{i}\\in\\mathcal{X}\\subseteq\\mathbb{R}^{C\\times H\\times W}$ is an image with $C$ channels, height $H$ , and width $W$ , and its label $y_{i}\\in[c]=\\{1,\\ldots,c\\}$ . $f_{\\theta}:\\mathcal{X}\\to\\mathbb{R}^{c}$ denotes a DNN function parameterized by $\\pmb{\\theta}\\in\\mathbb{R}^{d}$ , and $F_{\\pmb\\theta}(\\mathbf x)=\\arg\\operatorname*{max}_{y\\in[c]}{f_{\\pmb\\theta}(\\mathbf x)_{y}}$ is the predicted label of $\\mathbf{x}$ . Moreover, $f_{\\theta}=g\\circ h$ , where $h$ is the feature extractor and $g$ is the classifier. Let ${\\mathcal{H}}\\subseteq\\mathbb{R}^{m}$ be the $m$ -dimensional feature space, $h_{i}:\\mathcal{X}\\rightarrow\\mathbb{R}$ be a feature mapping function, and $\\mathbf{h}(\\mathbf{x})\\,=\\,\\left[h_{1}(\\mathbf{x}),\\ldots,h_{m}(\\mathbf{x})\\right]^{\\top}\\in{\\mathcal{H}}$ denote the feature map of $\\mathbf{x}$ . Specifically, features induced from the amplitude and phase patterns of $\\mathbf{x}$ are $h_{a}(\\mathbf{x})$ and $h_{p}(\\mathbf{x})$ , respectively. Define $S_{\\epsilon}[\\mathbf{x}]=\\{\\mathbf{x}^{\\prime}\\colon\\|\\mathbf{x}^{\\prime}-\\mathbf{x}\\|_{\\infty}\\}\\leqslant\\epsilon\\}$ as an $\\ell_{\\infty}$ -ball centered on $\\mathbf{x}$ with radius $\\epsilon$ . Let $\\mathcal F(\\cdot)$ and $\\mathcal{F}^{-1}(\\cdot,\\cdot)$ denote the DFT and inverse DFT (IDFT) functions. Typically, DFT is independently applied to each channel of an image $\\mathbf{x}$ within the pixel space as: ", "page_idx": 1}, {"type": "image", "img_path": "TeQvz5AlI8/tmp/b0c0a44e88f2ad6fec6114c7e9913c409ccc974dbd5a71c5a95690253018f371.jpg", "img_caption": ["Figure 2: Test accuracy $(\\%)$ on CIFAR-10 of (a) standard, (b) robust, and (c) amplitude-perturbed ResNet-18. \"Adv Amp./Pha.\" refers to $\\mathbf{x}_{a m p}^{\\prime}/\\mathbf{x}_{p h a}^{\\prime}$ . AEs are generated by PGD-20, $\\mathrm{C}\\&\\mathrm{W}_{\\infty}$ , FAB, and Square, with $\\ell_{\\infty}$ -bounded perturbation budget $\\epsilon=8/255$ and inner step $\\alpha={^2}/255$ . The robust and perturbed models are trained by PGD-AT-10 following [40]. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{F}(\\mathbf{x})(u,v)=\\sum_{h=1}^{H}\\sum_{w=1}^{W}\\mathbf{x}(h,w)\\,\\mathrm{e}^{-\\mathrm{i}2\\pi(u\\frac{h}{H}+v\\frac{w}{W})},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $(h,w)$ denotes the pixel coordinates of $\\mathbf{x}$ , and $(u,v)\\in[H]\\times[W]$ signifies coordinates in the frequency domain. The real and imaginary parts of ${\\mathcal{F}}(\\mathbf{x})$ are denoted by $\\operatorname{Re}({\\mathcal{F}}(\\mathbf{x}))$ and $\\operatorname{Im}({\\mathcal{F}}(\\mathbf{x}))$ , respectively. Then, the amplitude spectrum $\\mathcal{A}(\\mathbf{x})$ and phase spectrum $\\mathcal{P}(\\mathbf{x})$ are ", "page_idx": 2}, {"type": "equation", "text": "$$\n{A}({\\mathbf x})=\\left(\\mathrm{Re}^{2}({\\mathcal F}({\\mathbf x}))+\\mathrm{Im}^{2}({\\mathcal F}({\\mathbf x}))\\right)^{\\frac{1}{2}},\\quad{\\mathcal P}({\\mathbf x})=\\arctan\\left(\\frac{\\mathrm{Im}({\\mathcal F}({\\mathbf x}))}{\\mathrm{Re}({\\mathcal F}({\\mathbf x}))}\\right).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "2 Motivation: On Improving Adversarial Robustness in Frequency Domain ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "To investigate the approach to enhance the model robustness and show the motivation of DAT, we perform exploration experiments in this section. As shown in Figure 1, adversarial perturbations severely compromise the semantics within phase patterns, resulting in the difficulty of the model predicting AEs correctly. Consequently, we examine the distinct effects of adversarial perturbations on amplitude and phase patterns. To achieve this target, we employ the standard and robust models, trained without and with AT as [40] on $\\mathcal{D}_{t}$ (the training subset of CIFAR-10). Moreover, on training samples with perturbed amplitude, the model tends to focus on phase patterns, in order to achieve the convergence [7, 58]. Following this line, we discuss the impact of perturbing amplitude by mixing the amplitude of training samples with those of distractors randomly selected from $\\mathcal{D}_{t}$ . For $(\\mathbf{x},y)\\in\\mathcal{D}_{t}$ , the recombined sample $\\hat{\\bf x}$ is generated by amplitude-level mixing operations and IDFT, namely, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{x}}=\\mathcal{F}^{-1}(\\lambda\\cdot\\mathcal{A}(\\mathbf{x}_{0})+(1-\\lambda)\\cdot\\mathcal{A}(\\mathbf{x}),\\mathcal{P}(\\mathbf{x})),\\quad\\lambda\\sim\\mathrm{Uniform}(0,1),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathbf{x}_{\\mathrm{0}}$ is the distractor i.i.d. drawn from $\\mathcal{D}_{t}$ . We use $(\\hat{\\mathbf{x}},y)$ to construct a dataset $\\mathcal{D}_{r}$ and train the perturbed model on it as [40]. Then, for $(\\mathbf{x},y)\\in\\mathcal{D}_{e}$ (the testing subset of CIFAR-10), we generate AE $\\mathbf{x}^{\\prime}$ by four representative adversarial attacks (PGD, C&W, FAB, and Square) and utilize DFT to derive the amplitude and phase of frequency spectra of both $\\mathbf{x}$ and $\\mathbf{x}^{\\prime}$ . Furthermore, images composed of adversarial amplitude/phase and benign phase/amplitude (denoted by $\\mathbf{x}_{a m p}^{\\prime}/\\mathbf{x}_{p h a}^{\\prime})$ are obtained by ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{x}_{a m p}^{\\prime}=\\mathcal{F}^{-1}(\\mathcal{A}(\\mathbf{x}^{\\prime}),\\mathcal{P}(\\mathbf{x})),\\quad\\mathbf{x}_{p h a}^{\\prime}=\\mathcal{F}^{-1}(\\mathcal{A}(\\mathbf{x}),\\mathcal{P}(\\mathbf{x}^{\\prime})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "For each $({\\bf x},y)\\in\\mathcal{D}_{e}$ , with every adopted adversarial attack, we use $(\\mathbf{x}^{\\prime},y)$ , $(\\mathbf{x}_{a m p}^{\\prime},y)$ and $(\\mathbf{x}_{p h a}^{\\prime},y)$ to combine evaluation datasets $\\mathcal{D}_{\\mathrm{AE}}$ , $\\mathcal{D}_{a m p}$ and $\\mathcal{D}_{p h a}$ , which are used to evaluate the robustness of the standard, robust, and perturbed models, respectively. The experimental outcomes are illustrated in Figure 2, from which we can draw the following conclusion: ", "page_idx": 2}, {"type": "image", "img_path": "TeQvz5AlI8/tmp/7595b0597ed5fa3a3c2c7a7ad54089fac69c729ab3bbdf9ff0b08cc8eef14494.jpg", "img_caption": ["Figure 3: The overview of DAT, which consists of three stages: (I) adversarial amplitude generation, (II) AE generation, and (III) joint optimization. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Impact of Adversarial Attacks. As shown in Figure 2a, under all four attacks, the standard model completely cannot predict samples in $\\mathcal{D}_{\\mathrm{AE}}$ . Moreover, the standard model exhibits higher test accuracy on $\\mathcal{D}_{a m p}$ than that on $\\mathcal{D}_{p h a}$ . These results suggest that adversarial attacks have a more substantial impact on the phase patterns than amplitude ones. ", "page_idx": 3}, {"type": "text", "text": "Impact of AT. As depicted in Figure 2b, AT simultaneously enhances the model\u2019s performance on $\\mathcal{D}_{\\mathrm{AE}}$ and $\\mathcal{D}_{a m p}$ and $\\mathcal{D}_{p h a}$ , in comparison to the standard model. Notably, the robust model exhibits superior performance on $\\mathcal{D}_{p h a}$ over $\\mathcal{D}_{a m p}$ , contrary to the standard model. That indicates the AT helps the model learn more phase patterns unaffected by adversarial attacks, enhancing the model\u2019s robustness on both adversarial phase patterns and AEs. The phenomena indicate that by forcing the model to focus on the phase patterns of samples, the model\u2019s robustness can be improved further. ", "page_idx": 3}, {"type": "text", "text": "Impact of Mixing Amplitude. As shown in Figure 2c, for the perturbed model trained with AT on $\\mathcal{D}_{r}$ , compared with the robust one, its performance on $\\mathcal{D}_{\\mathrm{AE}}$ and $\\mathcal{D}_{p h a}$ is improved further, while the performance on $\\mathcal{D}_{a m p}$ is rarely changed. From these results, we can conclude that mixing the amplitude with that of the randomly selected distractor can force the model to focus on phase patterns, learning more patterns within the phase unaffected by adversarial perturbations. Then, the model\u2019s robustness on AEs is improved further, while robustness on amplitude patterns is retained. ", "page_idx": 3}, {"type": "text", "text": "Following the insight from these studies, we propose DAT to enhance the model\u2019s robustness, mixing training sample\u2019s amplitude with an adversarial one, generated by the adversarially optimized AAG. ", "page_idx": 3}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, the details of our proposed DAT are introduced and analyzed. As illustrated in Figure 3, DAT consists of three stages. It first adopts the AAG $G_{\\psi}$ to generate adversarial amplitude and obtain the recombined data for benign ones in Stage I. With the proposed loss $\\mathcal{L}_{\\sf A E}$ in Stage II, we produce the AEs for both benign and recombined samples. Then, taking both benign and recombined samples, total loss ${\\mathcal{L}}_{\\mathsf{D A T}}$ for DAT are minimized to update robust model $f_{\\theta}$ , and maximized to optimize $G_{\\psi}$ adversarially in Stage III. Stages I and III involve the adversarially trained AAG $G_{\\psi}$ and commonly updated model $f_{\\theta}$ , both of which share the ${\\mathcal{L}}_{\\mathsf{D A T}}$ and optimized jointly following the objective as: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\underset{\\theta}{\\operatorname*{min}}\\mathbb{E}_{(\\mathbf{x},y)\\sim\\mathcal{D}}\\left[\\underset{\\psi}{\\operatorname*{max}}\\,\\mathbb{E}_{\\hat{\\mathbf{x}}\\sim p(\\hat{\\mathbf{x}}|\\mathbf{x},\\psi)}\\left[\\mathcal{L}_{\\sf D A T}\\big(f_{\\theta}(\\mathbf{x}),f_{\\theta}(\\hat{\\mathbf{x}}),y\\big)\\right]\\right],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\hat{\\bf x}$ is the recombined data of $\\mathbf{x}$ , following a sample-dependent conditional distribution $p({\\hat{\\mathbf{x}}}|\\mathbf{x},\\psi)$ . Stage II encompasses an efficient AE generation method, optimized using the proposed loss $\\mathcal{L}_{\\mathrm{AE}}$ as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\mathbf{x}^{\\prime}\\in S_{\\epsilon}[\\mathbf{x}]}\\mathbb{E}_{(\\mathbf{x},y)\\sim\\mathcal{D}}\\left[\\mathcal{L}_{\\mathsf{A E}}(f_{\\theta}(\\mathbf{x}),f_{\\theta}(\\mathbf{x}^{\\prime}),y)\\right]\\!.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Following the order of these three stages, we introduce (I) AAG in Sec. 3.1 and (II) the efficient AE generation in Sec. 3.2. Subsequently, building on these components, Sec. 3.3 details (III) the joint optimization. Ultimately, Sec. 3.4 theoretically analyzes the mechanism of how the adversarial amplitude spectrum generated by AAG enforces the model to focus on the phase-level patterns. ", "page_idx": 4}, {"type": "text", "text": "3.1 Adversarial Amplitude Generator ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To explain the proposed AAG, we first introduce some constraints that the recombined $\\hat{\\bf x}$ with perturbed amplitude is expected to meet. With a small $\\epsilon_{1}>0$ and an $\\epsilon_{2}\\gg\\epsilon_{1}$ , ideally, we expect $\\hat{\\bf x}$ of $(\\mathbf{x},y)\\in\\mathcal{D}$ based on the generated adversarial amplitude to satisfy the following three conditions: ", "page_idx": 4}, {"type": "text", "text": "\u2022 C1. $|h_{p}(\\mathbf{x})-h_{p}(\\hat{\\mathbf{x}})|<\\epsilon_{1}$ : Ensuring $\\hat{\\bf x}$ retains the same semantics in the phase spectrum as $\\mathbf{x}$ . \u2022 C2. $F_{\\theta}(\\mathbf{x})=F_{\\theta}(\\hat{\\mathbf{x}})$ : Ensuring $\\hat{\\bf x}$ remains distinguishable with the same label as $\\mathbf{x}$ by $f_{\\theta}$ . \u2022 C3. $|h_{a}({\\bf x})-h_{a}(\\hat{\\bf x})|>\\epsilon_{2}$ : Making $\\hat{\\bf x}$ maximize the ${\\mathcal{L}}_{\\mathsf{D A T}}$ , causing the model\u2019s difficulty fitting the amplitude of images, and forcing the model to focus on phase patterns. ", "page_idx": 4}, {"type": "text", "text": "Let us first analyze the above conditions C1-C3 when a randomly selected distractor image is used. Since $\\hat{\\bf x}$ is recombined by the mixed amplitude with the distractor and original phase of $\\mathbf{x}$ , then, C1 can be easily satisfied. As stated in Sec. 1, when the gap between the randomly selected distractor and training sample is too large, the $\\mathcal{P}(\\mathbf{x})$ \u2019s information can be damaged, resulting in the inconsistent prediction between the $\\mathbf{x}$ and $\\hat{\\bf x}$ , destroying the C1 and C2. Conversely, it is difficult to satisfy C3, limiting the model\u2019s attention to the patterns in $\\mathcal{P}(\\mathbf{x})$ . The above statement indicates that it is difficult for $\\hat{\\bf x}$ , using the amplitude of the randomly selected distractor, to meet the above three constraints. ", "page_idx": 4}, {"type": "text", "text": "Instead of searching for an appropriate distractor, we here resort to a generative approach, i.e., developing $G_{\\psi}$ to generate an adversarial amplitude $A_{G}(\\mathbf{x})$ as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{A}_{G}(\\mathbf{x})=G_{\\psi}(\\mathbf{z},f_{\\theta}(\\mathbf{x})),\\quad\\mathrm{where~\\mathbf{z}~}\\overset{\\mathrm{i.i.d.}}{\\sim}\\mathcal{N}(\\mathbf{0},\\mathbf{I}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "For efficient training, $G_{\\psi}$ is constructed by four linear layers (detailed architecture in Appendix E.2). Moreover, the input with $f_{\\pmb\\theta}(\\mathbf x)$ can ease the difficulty of $G_{\\psi}$ \u2019s convergence. With $G_{\\psi}$ , since $\\hat{\\bf x}$ is still recombined by $A_{G}(\\mathbf{x})$ and the $\\mathcal{P}(\\mathbf{x})$ , then, C1 is satisfied. For the outer minimization in Eq. (3), we retain the label of $\\hat{\\bf x}$ same $\\mathbf{as}\\times\\mathbf{x}$ , minimizing ${\\mathcal{L}}_{\\mathsf{D A T}}$ to update $f_{\\theta}$ , meeting the C2. To meets C3, $G_{\\psi}$ is optimized adversarially by maximizing the ${\\mathcal{L}}_{\\mathsf{D A T}}$ (further details in Sec. 3.3), shown as the inner step in Eq. (3). Thereby, $\\hat{\\bf x}$ can limit $f_{\\theta}$ to fit amplitude information. To achieve the convergence, $f_{\\theta}$ has to focus on patterns in $\\mathcal{P}(\\mathbf{x})$ , learning more phase patterns unaffected by adversarial perturbations. ", "page_idx": 4}, {"type": "text", "text": "Since $G_{\\psi}$ is adversarially trained by maximizing the ${\\mathcal{L}}_{\\mathsf{D A T}}$ , $A_{G}(\\mathbf{x})$ is likely to compromise semantic integrity [63], hindering $f_{\\theta}$ to retain the prediction consistency between $\\hat{\\bf x}$ and $\\mathbf{x}$ [31]. Moreover, due to the loss of original amplitude information, using $A_{G}(\\mathbf{x})$ to replace $\\mathcal{A}(\\mathbf{x})$ entirely can also hurt the model\u2019s robustness [58]. As shown in Sec. 2, the mix-up operation on amplitude rarely has impact on the amplitude level robustness. That indicates the linear mix-up operation can preserve the energy of the amplitude spectrum and maintain the original amplitude information with less impact on the sample\u2019s original information. Otherwise, the original amplitude could be compromised, thereby hindering accurate model predictions. Following this line, a mix-up operation is employed, ensuring that a portion of the original amplitude information is preserved following: ", "page_idx": 4}, {"type": "equation", "text": "$$\nA_{m i x}(\\mathbf{x})=\\lambda\\cdot\\mathcal{A}_{G}(\\mathbf{x})+(1-\\lambda)\\cdot\\mathcal{A}(\\mathbf{x}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The mix-up operation effectively satisfies C1 and $\\mathbf{C}2$ , ensuring $\\hat{\\bf x}$ remains distinguishable by $f_{\\theta}$ , and keeping $\\mathcal{P}(\\hat{\\mathbf{x}})$ \u2019s patterns closer to those in $\\mathcal{P}(\\mathbf{x})$ in manifold. Finally, $\\hat{\\bf x}$ is obtained by IDFT as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{x}}=\\mathcal{F}^{-1}(\\mathcal{A}_{m i x}(\\mathbf{x}),\\mathcal{P}(\\mathbf{x})).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "To elaborate on $G_{\\psi}$ , we perform some experiments and provide visual results in Appendixes C and F.7. Now we can use $\\hat{\\bf x}$ as an augmentation of $\\mathbf{x}$ , introduced to Stage II for generating AEs. ", "page_idx": 4}, {"type": "text", "text": "3.2 Efficient Adversarial Example Generation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Since both $\\mathbf{x}$ and $\\hat{\\bf x}$ are fed into Stage $\\mathrm{II}$ for generating AE, it doubles the time consumption if we use the same AE generation strategies as existing methods, e.g., PGD-AT [40] and TRADES [39]. To improve the training efficiency of DAT, we now suggest an efficient AE generation strategy. Since simply reducing the iteration step results in the difficulty of AEs\u2019 reaching the actual maximum in the $\\ell_{\\infty}$ -ball [49], we propose the loss $\\mathcal{L}_{\\sf A E}$ to increase adversarial perturbation length in each iteration as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\sf A E}(f_{\\theta}(\\mathbf{x}),f_{\\theta}(\\mathbf{x}^{\\prime}),y)=\\mathcal{L}_{\\sf C E}(f_{\\theta}(\\mathbf{x}^{\\prime}),y)+\\beta\\cdot\\mathcal{D}_{\\sf K L}(f_{\\theta}(\\mathbf{x}^{\\prime}),f_{\\theta}(\\mathbf{x})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\beta$ is a weighting parameter, and $\\mathcal{L}_{\\sf C E}$ and $\\mathcal{D}_{\\sf K L}$ are cross-entropy (CE) loss and Kullback-Leibler (KL) divergence. According to [49, 50], it is effective to enlarge adversarial perturbation length for each step by maximizing the distance between $f_{\\theta}(\\mathbf{x})$ and $f_{\\pmb{\\theta}}(\\mathbf{x}^{\\prime})$ . Following this line, based on PGD-AT maximizing the $\\mathcal{L}_{\\sf C E}$ , the proposed $\\mathcal{L}_{\\sf A E}$ increases the adversarial perturbation step length by maximizing the KL divergence between benign sample and its AE. Then, with $\\mathcal{L}_{\\sf A E}$ , AEs can use fewer iterative steps to achieve the actual maximum in the $\\ell_{\\infty}$ -ball. As shown by experiments in Appendix F.5, DAT only needs $^{5}$ steps to generate AEs for both benign $\\mathbf{x}$ and recombined $\\mathbf{x}^{\\prime}$ , significantly reducing the training time while maintaining the model\u2019s robustness. Enlarging the inner step size $\\alpha$ (in Eq. (10) of Appendix B.3) can also increase the adversarial perturbation length in each iteration. Due to the fact that the current AT methods typically employ a fixed $\\alpha$ , we adopt an extra loss term for fair experimental comparisons. More details on the AE generation procedure, including pseudocodes and experimental analyses, are in Appendixes A.1, F.3 and F.5. ", "page_idx": 5}, {"type": "text", "text": "With the AAG and efficient AE generation, Stage III attempts to improve the model\u2019s robustness. ", "page_idx": 5}, {"type": "text", "text": "3.3 Joint Optimization ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "After the introduction to Stages I and $\\mathrm{II}$ , we now delve into specifics of Stage III, joint optimization, where $f_{\\theta}$ and $G_{\\psi}$ are optimized jointly following the objective as Eq. (3). In Stage III, to satisfy C2 in Sec. 3.1, keeping $\\hat{\\bf x}$ with the same label as $\\mathbf{x}$ by $f_{\\theta}$ , recombined and benign samples and their AEs are fed into DAT for the model training, also reducing the negative impact of amplitude information loss. Moreover, ${\\mathcal{L}}_{\\mathsf{D A T}}$ needs to be minimized on benign and recombined samples\u2019 AEs to enhance the robustness of $f_{\\theta}$ , enforcing $f_{\\theta}$ to focus on the phase patterns in the meanwhile. For commonly updating $\\pmb{\\theta}$ and adversarially renewing $\\psi$ , we introduce the designed loss terms ${\\mathcal{L}}_{\\mathsf{D A T}}$ as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{DAT}}(f_{\\theta}(\\mathbf{x}),f_{\\theta}(\\hat{\\mathbf{x}}),y)=\\frac{1}{2}(\\mathcal{L}_{\\mathrm{AT}}(f_{\\theta}(\\mathbf{x}),y)+\\mathcal{L}_{\\mathrm{AT}}(f_{\\theta}(\\hat{\\mathbf{x}}),y))+\\omega\\cdot\\mathcal{D}_{\\mathrm{JS}}(f_{\\theta}(\\mathbf{x}),f_{\\theta}(\\hat{\\mathbf{x}})),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where ${\\mathcal{L}}_{\\mathsf{A T}}$ and $\\mathcal{D}_{\\mathrm{JS}}$ are adversarial training and consistency regularization losses respectively, and $\\omega$ is the weighting parameter for $\\mathcal{D}_{\\mathrm{JS}}$ . We discuss ${\\mathcal{L}}_{\\mathsf{A T}}$ and $\\mathcal{D}_{\\mathrm{JS}}$ below separately. ", "page_idx": 5}, {"type": "text", "text": "Adversarial Training Loss ${\\mathcal{L}}_{\\mathsf{A T}}$ . ${\\mathcal{L}}_{\\mathsf{A T}}$ is the loss used to guide $f_{\\theta}$ to learn robust features on AEs against adversarial attacks. As shown in Figure 1, although adversarial perturbations significantly damage phase patterns, there are still some unaffected features in the phase of AEs, important for $f_{\\theta}$ to categorize AEs correctly. Since these unaffected phase patterns contain adversarial perturbations, it is difficult for $f_{\\theta}$ to learn these features only with AEs. Therefore, we utilize benign samples and their AEs in AT, guiding the model to learn their shared features, significant for improving $f_{\\theta}$ \u2019s robustness. Following this line, ${\\mathcal{L}}_{\\mathsf{A T}}$ for $(\\mathbf{x},y)\\in\\mathcal{D}$ with AE $\\mathbf{x}^{\\prime}$ can be expressed as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\sf A T}(f_{\\theta}(\\mathbf{x}),y)=\\mathcal{L}_{\\sf C E}(f_{\\theta}(\\mathbf{x}),y)+\\beta\\cdot\\mathcal{D}_{\\sf K L}(f_{\\theta}(\\mathbf{x}^{\\prime}),f_{\\theta}(\\mathbf{x})),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\beta$ is the weighting parameter identical to that in Eq. (5). $\\hat{\\bf x}$ adopts the same AT loss as that of $\\mathbf{x}$ . ", "page_idx": 5}, {"type": "text", "text": "Consistency Regularization Loss $\\mathcal{D}_{\\mathsf{J S}}$ . $\\mathcal{D}_{\\mathrm{JS}}$ is the loss used to preserve the prediction consistency between $\\mathbf{x}$ and $\\hat{\\bf x}$ . In DAT, the amplitude of $\\hat{\\bf x}$ \u2019s frequency spectrum is mixed with the adversarial one generated by $G_{\\psi}$ , maximizing ${\\mathcal{L}}_{\\mathsf{D A T}}$ , showing the large gap between $h_{a}({\\bf x})$ and $h_{a}(\\hat{\\mathbf{x}})$ . Since $\\hat{\\bf x}$ has the same phase patterns as $\\mathbf{x}$ , keeping the prediction consistency between $\\mathbf{x}$ and $\\hat{\\bf x}$ can make the model learn more phase patterns further, enhancing the model\u2019s robustness. Inspired by the mechanism, we use the Jensen\u2013Shannon (JS) divergence $\\mathcal{D}_{\\mathsf{J S}}$ to ensure the prediction consistency as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{D}_{\\mathrm{JS}}(f_{\\theta}(\\mathbf{x}),f_{\\theta}(\\hat{\\mathbf{x}}))=\\frac{1}{2}\\Big(\\mathcal{D}_{\\mathrm{KL}}\\big(\\frac{f_{\\theta}(\\mathbf{x})+f_{\\theta}(\\hat{\\mathbf{x}})}{2},f_{\\theta}(\\mathbf{x})\\big)+\\mathcal{D}_{\\mathrm{KL}}\\big(\\frac{f_{\\theta}(\\mathbf{x})+f_{\\theta}(\\hat{\\mathbf{x}})}{2},f_{\\theta}(\\hat{\\mathbf{x}})\\big)\\Big).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This concludes the introduction to ${\\mathcal{L}}_{\\mathsf{D A T}}$ . Notably, during the model training of DAT, the gap of batch normalization (BN) parameters between $\\mathbf{x}$ and $\\hat{\\bf x}$ is quite large (details in Appendix D), posing challenges to model\u2019s convergence. To remedy this, we adopt different BNs for $\\mathbf{x}$ and $\\hat{\\bf x}$ during training. Please refer to the pseudocode of DAT in Appendix A.2 for a comprehensive presentation. ", "page_idx": 5}, {"type": "text", "text": "3.4 Theoretical Analysis ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Through a convergence analysis of the empirical risk, this section theoretically discusses the effects of DAT on the model to focus on phase patterns. Detailed proofs are provided in Appendix $\\mathrm{G}$ . Concretely, we instantiate $g$ as a linear softmax classifier $\\mathbf{W}=[\\mathbf{w}_{1},...,\\mathbf{w}_{c}]\\in\\mathbb{R}^{m\\times c}$ on top of the learned features h. Generally, for $(\\mathbf{x},y)\\in\\mathcal{D}$ , suppose $\\mathcal{T}(\\mathbf{x})$ represents the augmented distribution over data points, where $\\mathbf{x}$ can be transformed as anyone in $\\{\\mathbf{x},\\bar{\\mathbf{x}^{\\prime}},\\hat{\\mathbf{x}},\\hat{\\mathbf{x}}^{\\prime}\\}$ . Then, the augmented data for $\\mathbf{x}$ can be denoted as $t(\\mathbf{x})\\sim\\mathcal{T}(\\mathbf{x})$ . Since the augmentation will increase the discrepancy between original and augmented distributions w.h.p., we can establish a common assumption. ", "page_idx": 6}, {"type": "text", "text": "Assumption 3.1. Assume $\\mathbb{E}_{\\mathcal{T}}[||\\mathbf{h}(t(\\mathbf{x}))-\\mathbf{h}(\\mathbf{x})||]>\\varepsilon_{0}$ , where $\\varepsilon_{0}>0$ is a relatively large value. Since only the amplitude spectrum is perturbed in the proposed DAT, it is reasonable that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathcal{T}}\\big[|h_{a}(t(\\mathbf{x}))-h_{a}(\\mathbf{x})|\\big]>\\mathbb{E}_{\\mathcal{T}}\\big[|h_{p}(t(\\mathbf{x}))-h_{p}(\\mathbf{x})|\\big].\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Theorem 3.2 (Weight Regularization of Amplitude Features). Grant Assumption 3.1, when the empirical risk $\\hat{R}$ is minimized with some convex loss function $\\mathcal{L}$ (e.g. CE loss): ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\hat{R}(\\mathbf{W}):=\\frac{1}{|\\mathcal{D}|}\\sum_{(\\mathbf{x},y)\\in\\mathcal{D}}\\mathbb{E}_{t(\\mathbf{x})\\sim\\mathcal{T}(\\mathbf{x})}\\left[\\mathcal{L}\\left(\\mathbf{W}^{\\top}\\mathbf{h}(t(\\mathbf{x})),y\\right)\\right],\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "we have $w_{j,a}\\to0$ for all $j\\in[c],$ , where $w_{j,a}$ is the corresponding weights of amplitude features $h_{a}$ . Corollary 3.3. Suppose the predicted probability $f_{\\mathbf{w}}(\\mathbf{x})=[p_{1},\\ldots,p_{c}]^{\\top}$ , where ", "page_idx": 6}, {"type": "equation", "text": "$$\np_{i}=\\frac{\\exp(\\mathbf{w}_{i}^{\\top}\\mathbf{h})}{\\sum_{j=1}^{c}\\exp(\\mathbf{w}_{j}^{\\top}\\mathbf{h})}=\\frac{1}{\\sum_{j=1}^{c}\\exp((\\mathbf{w}_{j}-\\mathbf{w}_{i})^{\\top}\\mathbf{h})}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "For every $i,j\\in[c]$ , we have $(w_{i,a}-w_{j,a})h_{a}\\to0$ . ", "page_idx": 6}, {"type": "text", "text": "Remark. Theorem 3.2 suggests that for weights $w_{j,a}$ corresponding to features $h_{a}$ derived from amplitude pattern, minimizing the empirical risk $\\hat{R}$ regularizes it to 0. As a result, it is difficult for the model to fit the adversarial amplitude generated by AAG. In order to converge, the model needs to reduce the reliance on $h_{a}$ by restricting $w_{j,a}$ . Hence, the model would mitigate the impact of $h_{a}$ on the predicted labels, as shown in Corollary 3.3, and pay more attention to features $h_{p}$ derived from phase patterns, capturing more phase patterns unaffected by adversarial attacks. Therefore, we can verify the effectiveness of DAT in enhancing the model\u2019s robustness. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we perform experiments to verify the effectiveness of DAT and explore the function of some DAT\u2019s parts. Sec. 4.1 shows experimental setups. Sec. 4.2 compares the model\u2019s robustness with existing AT methods with fixed $\\epsilon$ and $\\alpha$ . Sec. 4.3 compares DAT with existing AT methods over complex strategy, e.g., AWP and SWA. Sec. 4.4 presents the analysis of ablation studies. ", "page_idx": 6}, {"type": "text", "text": "4.1 Experimental Setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Experimental and Evaluation Settings. We select three datasets: CIFAR-10, CIFAR-100, and Tiny ImageNet [17]. For all experiments in this work, ResNet-18, WideResNet-28-10 (WRN-28-10), and WideResNet-34-10 (WRN-34-10) are used as model architectures (experiments are attached to Appendix F.1), with $\\beta=15$ and $\\omega=2$ (exploration in Appendix F.4). During training, the inner step size is fixed as $\\alpha={^2}/{255}$ to generate adversarial perturbation $\\ell_{\\infty}$ -bounded with constant radius $\\epsilon=8/255$ following [29, 37, 33] (details in Appendixes E.1 and E.2). ", "page_idx": 6}, {"type": "text", "text": "Baselines. We compare results with PGD-AT [40], TRADES [61], MART [54], and recent AT methods with competitive performance: LAS-AT [29], SCARL [33], and ST [37]. We also compare the DAT with OA-AT [2], DAJAT [1], and IDBH [35], which uses extra strategies, e.g., AWP [56] and SWA [28]. Introduction and details of baselines are attached in Appendixes B.2 and E.3. ", "page_idx": 6}, {"type": "text", "text": "4.2 Comparison with Common Methods ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this subsection, we compare the robustness of DAT with common methods that use fixed $\\epsilon$ and $\\alpha$ during the training procedure. Table 1 displays the results on CIFAR-10, CIFAR-100, and ", "page_idx": 6}, {"type": "table", "img_path": "TeQvz5AlI8/tmp/87fa196a5dc4b05514ef8bace3779601faf8f71a3170f2fc75d08869e171f069.jpg", "table_caption": ["Table 1: Average natural and robust accuracy $(\\%)$ of ResNet-18 against $\\ell_{\\infty}$ threat with $\\epsilon=8/255$ in 7 runs. The best results are boldfaced. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "TeQvz5AlI8/tmp/88726c4fea145445edf84f7d72aa27aa33c55633179b52acc8dd69d1df850f0d.jpg", "table_caption": ["Table 2: Average natural and robust accuracy $(\\%)$ of WRN-34-10 against $\\ell_{\\infty}$ threat with $\\epsilon=8/255$ in 7 runs. The best results are boldfaced. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Tiny ImageNet using ResNet-18. Compared with existing methods, DAT not only improves the model\u2019s robustness but also enhances the natural accuracy. On CIFAR-10, DAT achieves an average improvement of ${\\sim}2.9\\%$ against FGSM, PGD-20, and PGD-100. For challenging $\\mathrm{C}\\&\\mathrm{W}_{\\infty}$ and AA, DAT obtains ${\\sim}0.66\\%$ and ${\\sim}0.85\\%$ improvement, respectively. Specifically, since SCARL adopts a contrastive learning strategy, DAT achieves less improvement against ${\\mathrm{C}}\\&{}{\\mathbf{W}}_{\\infty}$ compared to it. CIFAR-100 contains more classes but fewer samples for each class, thereby making it challenging for AT. DAT enhances the model\u2019s robustness by ${\\sim}2.7\\%$ on average against FGSM, PGD-20, and PGD-100 compared to the existing methods. For the demanding ${\\mathrm{C}}\\&{}{\\mathbf{W}}_{\\infty}$ and AA, the model\u2019s robustness is improved by ${\\sim}1.5\\%$ and ${\\sim}1.3\\%$ . On the intricate real-world dataset Tiny ImageNet, DAT achieves ${\\sim}2.9\\%$ better performance against FGSM, PGD-20, and PGD-100. For the challenging $\\mathrm{C}\\&\\mathrm{W}_{\\infty}$ and AA, we enhance the model\u2019s robustness by $\\mathord{\\sim}1.6\\%$ and ${\\sim}1.2\\%$ . AWP is proved to be an effective method for protecting the model from robust overfitting. To further improve the model\u2019s robustness, we combine DAT with AWP, and compare the performance with the combination of various existing methods and AWP fairly. Since a few methods provide the codes for the combination with AWP, we choose only these methods for comparison. As shown in Table 1, the combination of DAT and AWP improves the model\u2019s robustness further and still outperforms previous methods. ", "page_idx": 7}, {"type": "text", "text": "Large-capacity models usually have better adversarial robustness. Thus, we also perform comparative experiments using WRN-34-10 on CIFAR-10 and CIFAR-100, as shown in Table 2. Compared to existing methods, DAT still retains better performance and has a lower negative impact on the model\u2019s natural accuracy, achieving ${\\sim}1.51\\%$ , ${\\sim}0.63\\%$ , and ${\\sim}1.31\\%$ robust accuracy improvement on CIFAR-10 with WRN-34-10. On CIFAR-100, the robustness obtained by DAT is enhanced by ${\\sim}2.56\\%$ , ${\\sim}1.68\\%$ , and ${\\sim}1.27\\%$ , showing significant improvement compared to existing methods. ", "page_idx": 7}, {"type": "table", "img_path": "TeQvz5AlI8/tmp/9ee9bcdfa6575b07af62b716175b352443a5f5fa1d65947f93ae123599f83352.jpg", "table_caption": ["Table 3: The average experimental results for methods with complex strategies against $\\ell_{\\infty}$ threat model with $\\epsilon=8/255$ in 7 runs. The best results are boldfaced. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "TeQvz5AlI8/tmp/2a4d1fdbb6d06af4bc9bd413d1f754efa4c42ce7f71a23659184cd5994851f84.jpg", "table_caption": ["Table 4: Results of ablation studies with ResNet-18 against $\\ell_{\\infty}$ with $\\epsilon=8/255$ average in 7 runs. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "4.3 Comparison with Complex Strategy Based Methods ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "After the comparison of common methods, we then demonstrate the performance of some existing methods with extra strategies, e.g., AWP and SWA. Addepalli et al. [1] find that variable $\\epsilon$ and $\\alpha$ can enhance the model\u2019s robustness, and some methods using variable $\\epsilon$ and $\\alpha$ (e.g., DAJAT, OA-AT, and IDBH) obtain competitive performance against various adversarial attacks. To further verify the effectiveness of DAT, we compare it with the latest competitive methods, e.g., DAJAT with one augmentation, OA-AT and IDBH, see Appendix B.2. As shown in Table 3, fixing $\\epsilon\\,=\\,^{8}/255$ and $\\alpha\\stackrel{=}{=}{}^{2/}255$ , we combine DAT with AWP and SWA. Compared with IDBH, on CIFAR-10 with ResNet18 and WRN-34-10, DAT with AWP and SWA achieves robust improvement ${\\sim}1.36\\%$ and ${\\sim}1.18\\%$ against PGD-20, and ${\\sim}0.45\\%$ and ${\\sim}0.48\\%$ against AA. For complex CIFAR-100 with ResNet-18 and WRN-34-10, compared with the previous best method, our method obtain ${\\sim}1.8\\%$ and ${\\sim}1.54\\%$ improvement against PGD-20, and ${\\sim}0.45\\%$ and ${\\sim}0.30\\%$ enhancement against AA. Additionally, we perform with synthesized data and augmentation strategies, attached in Appendixes F.1 and F.2. ", "page_idx": 8}, {"type": "text", "text": "4.4 Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Through experiments on CIFAR-10 and CIFAR-100, this subsection discusses the impact of different components in DAT on the model\u2019s natural accuracy and robustness against PGD-20 and AA. As shown in Table 4, we use the model with the proposed AE generation method without recombined data as the baseline. Compared with complete DAT, the robustness is decreased ${\\sim}0.3\\%$ and ${\\sim}0.5\\%$ on CIFAR-10 and CIFAR-100 without $\\mathcal{D}_{\\mathrm{JS}}$ . Due to $\\mathcal{D}_{\\mathrm{JS}}$ keeping the predictions consistent between the benign and recombined data, removing $\\mathcal{D}_{\\mathsf{J S}}$ makes the model learn less unaffected phase patterns and thus reduces the robust performance. As mentioned in Sec. 3.1, the mix-up operation combines the generated adversarial and original amplitude spectrum. Since the model still needs some original amplitude information, Table 4 shows that replacing the original amplitude spectrum with the generated one drops $\\mathord{\\sim}1.6\\%$ and ${\\sim}1.9\\%$ model\u2019s robustness on CIFAR-10 and CIFAR-100, due to damaging original phase patterns. As mentioned in Sec. 3.3, the gap of BN parameters between original and recombined data is quite large. Without split BNs, the robustness of the model is reduced ${\\sim}4.6\\%$ and ${\\sim}3.5\\%$ on CIFAR-10 and CIFAR-100, showing that the model robustness is significantly affected because of the convergence difficulty. For experiments without AAG, we mix the amplitude of training sample\u2019s frequency spectra as Sec. 2. Table 4 shows the selected amplitude does not severely reduce the natural accuracy but significantly decreases robustness ${\\sim}1.6\\%$ and ${\\sim}2.2\\%$ on CIFAR-10 and CIFAR-100, confirming the effectiveness of AAG. To comprehensively explore the effectiveness of AAG, we combine AAG with some existing AT methods on ResNet-18, see Appendixes F.6 and F.7. Additionally, we compare the time consumption in Appendix F.3. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work presents a novel Dual Adversarial Training (DAT) method to improve adversarial robustness against various adversarial attacks. We first illustrate the motivation through some exploration experiments. Subsequently, we delve into the efficient and effective DAT, discussing both its underlying motivation and detailed mechanics. Additionally, we theoretically validate the functionality of the Adversarial Amplitude Generator (AAG) and the convergence properties of the DAT model. Through experiments across multiple datasets against various adversarial attacks, we verify that the proposed DAT significantly improves model robustness. We also explore the hyper-parameters and discuss the function of specific components of DAT. In the future, we will design a more suitable amplitude generation and augmentation strategy to enhance robustness further. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported in part by Macau Science and Technology Development Fund under SKLIOTSC-2021-2023, 0072/2020/AMJ, 0022/2022/A, and 0014/2022/AFJ; in part by Research Committee at University of Macau under MYRG-GRG2023-00058-FST-UMDF and MYRG2022- 00152-FST; in part by Natural Science Foundation of Guangdong Province of China under EF2023- 00116-FST; in part by the Natural Science Foundation of China under Grant 62202009. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Sravanti Addepalli, Samyak Jain, and Venkatesh Babu R. Efficient and effective augmentation strategy for adversarial training. In Proceedings of NeurIPS, November 28 - December 9, 2022, New Orleans, LA, USA, pages 1488\u20131501, 2022.   \n[2] Sravanti Addepalli, Samyak Jain, Gaurang Sriramanan, and R. Venkatesh Babu. Scaling adversarial training to large perturbation bounds. In Proceedings of ECCV, October 23-27, 2022, Tel Aviv, Israel, pages 301\u2013316, 2022.   \n[3] Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion, and Matthias Hein. Square attack: A query-efficient black-box adversarial attack via random search. In Proceedings of ECCV, August 23-28, 2020, Glasgow, UK, pages 484\u2013501, 2020.   \n[4] Jiawang Bai, Bin Chen, Yiming Li, Dongxian Wu, Weiwei Guo, Shu-Tao Xia, and En-Hui Yang. Targeted attack for deep hashing based retrieval. In Proceedings of ECCV, August 23-28, 2020, Glasgow, UK, pages 618\u2013634, 2020.   \n[5] Jiawang Bai, Baoyuan Wu, Yong Zhang, Yiming Li, Zhifeng Li, and Shu-Tao Xia. Targeted attack against deep neural networks via flipping limited weight bits. In Proceedings of ICLR, May 3-7, 2021, Virtual Event, Austria, 2021.   \n[6] Yang Bai, Yuyuan Zeng, Yong Jiang, Shu-Tao Xia, Xingjun Ma, and Yisen Wang. Improving adversarial robustness via channel-wise activation suppressing. In Proceedings of ICLR, May 3-7, 2021, Virtual Event, Austria, 2021.   \n[7] Guangyao Chen, Peixi Peng, Li Ma, Jia Li, Lin Du, and Yonghong Tian. Amplitude-phase recombination: Rethinking robustness of convolutional neural networks in frequency domain. In Proceedings of ICCV, October 10-17, 2021, Montreal, QC, Canada, pages 448\u2013457, 2021.   \n[8] Huanran Chen, Yichi Zhang, Yinpeng Dong, and Jun Zhu. Rethinking model ensemble in transfer-based adversarial attacks. CoRR, abs/2303.09105, 2023.   \n[9] Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. ZOO: zeroth order optimization based black-box attacks to deep neural networks without training substitute models. In Proceedings of AISec@CCS, November 3, 2017, Dallas, TX, USA, pages 15\u201326, 2017.   \n[10] Shuyu Cheng, Yinpeng Dong, Tianyu Pang, Hang Su, and Jun Zhu. Improving black-box adversarial attacks with a transfer-based prior. In Proceedings of NeurIPS, December 8-14, 2019, Vancouver, BC, Canada, pages 10932\u201310942, 2019.   \n[11] Francesco Croce, Maksym Andriushchenko, Vikash Sehwag, Edoardo Debenedetti, Nicolas Flammarion, Mung Chiang, Prateek Mittal, and Matthias Hein. Robustbench: a standardized adversarial robustness benchmark. arXiv preprint arXiv:2010.09670, 2020.   \n[12] Francesco Croce and Matthias Hein. Minimally distorted adversarial examples with a fast adaptive boundary attack. In Proceedings of ICML, July 13-18, 2020, Virtual Event, pages 2196\u20132205, 2020.   \n[13] Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks. In Proceedings of ICML, July 13-18, 2020, Virtual Event, pages 2206\u20132216, 2020.   \n[14] Ekin D. Cubuk, Barret Zoph, Dandelion Man\u00e9, Vijay Vasudevan, and Quoc V. Le. Autoaugment: Learning augmentation strategies from data. In Proceedings of CVPR, June 16-20, 2019, Long Beach, CA, USA, pages 113\u2013123, 2019.   \n[15] Jiequan Cui, Zhuotao Tian, Zhisheng Zhong, Xiaojuan Qi, Bei Yu, and Hanwang Zhang. Decoupled kullback-leibler divergence loss. CoRR, abs/2305.13948, 2023.   \n[16] Tri Dao, Albert Gu, Alexander Ratner, Virginia Smith, Chris De Sa, and Christopher R\u00e9. A kernel theory of modern data augmentation. In Proceedings of ICML, June 9-15, 2019, Long Beach, California, USA, pages 1528\u20131537, 2019.   \n[17] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Proceedings of CVPR, June 20-25, 2009, Miami, Florida, USA, pages 248\u2013255, 2009.   \n[18] Terrance Devries and Graham W. Taylor. Improved regularization of convolutional neural networks with cutout. CoRR, abs/1708.04552, 2017.   \n[19] Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu, Xiaolin Hu, and Jianguo Li. Boosting adversarial attacks with momentum. In Proceedings of CVPR, June 18-22, 2018, Salt Lake City, UT, USA, pages 9185\u20139193, 2018.   \n[20] Yanbo Fan, Baoyuan Wu, Tuanhui Li, Yong Zhang, Mingyang Li, Zhifeng Li, and Yujiu Yang. Sparse adversarial attack via perturbation factorization. In Proceedings of ECCV, August 23-28, 2020, Glasgow, UK, pages 35\u201350, 2020.   \n[21] Ross Girshick. Fast r-cnn. In Proceedings of the CVPR, June 7-12, 2015, Boston, MA, USA, pages 1440\u20131448, 2015.   \n[22] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In Proceedings of ICLR, May 7-9, 2015, San Diego, CA, USA, 2015.   \n[23] Sven Gowal, Sylvestre-Alvise Rebuff,i Olivia Wiles, Florian Stimberg, Dan Andrei Calian, and Timothy A. Mann. Improving robustness using generated data. In Proceedings of NeurIPS, December 6-14, 2021, virtual, pages 4218\u20134233, 2021.   \n[24] Yiwen Guo, Qizhang Li, and Hao Chen. Backpropagating linearly improves transferability of adversarial examples. In Proceedings of NeurIPS, December 6-12, 2020, virtual, 2020.   \n[25] Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Girshick. Mask r-cnn. In Proceedings of the ICCV, October 22-29, 2017, Venice, Italy, pages 2961\u20132969, 2017.   \n[26] Zhuoxun He, Lingxi Xie, Xin Chen, Ya Zhang, Yanfeng Wang, and Qi Tian. Data augmentation revisited: Rethinking the distribution gap between clean and augmented data. CoRR, abs/1909.09148, 2019.   \n[27] Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin. Black-box adversarial attacks with limited queries and information. In Proceedings of ICML, July 10-15, 2018, Stockholmsm\u00e4ssan, Stockholm, Sweden, pages 2142\u20132151, 2018.   \n[28] Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry P. Vetrov, and Andrew Gordon Wilson. Averaging weights leads to wider optima and better generalization. In Proceedings of UAI, August 6-10, 2018, Monterey, California, USA, pages 876\u2013885, 2018.   \n[29] Xiaojun Jia, Yong Zhang, Baoyuan Wu, Ke Ma, Jue Wang, and Xiaochun Cao. LAS-AT: adversarial training with learnable attack strategy. In Proceedings of CVPR, June 18-24, 2022, New Orleans, LA, USA, pages 13388\u201313398, 2022.   \n[30] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. In Proceedings of NeurIPS, November 28 - December 9, 2022, New Orleans, LA, USA, pages 26565\u201326577, 2022.   \n[31] Minyoung Kim, Da Li, and Timothy M. Hospedales. Domain generalisation via domain adaptation: An adversarial fourier amplitude approach. In Proceedings of ICLR, May 1-5, 2023, Kigali, Rwanda, 2023.   \n[32] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. Commun. ACM, 60(6):84\u201390, 2017.   \n[33] Huafeng Kuang, Hong Liu, Yongjian Wu, and Rongrong Ji. Semantically consistent visual representation for adversarial robustness. IEEE Trans. Inf. Forensics Secur., 18:5608\u20135622, 2023.   \n[34] Alexey Kurakin, Ian J Goodfellow, and Samy Bengio. Adversarial examples in the physical world. In Artificial intelligence safety and security, pages 99\u2013112. Chapman and Hall/CRC, 2018.   \n[35] Lin Li and Michael W. Spratling. Data augmentation alone can improve adversarial training. ArXiv, abs/2301.09879, 2023.   \n[36] Qizhang Li, Yiwen Guo, and Hao Chen. Practical no-box adversarial attacks against DNNs. In Proceedings of NeurIPS, December 6-12, 2020, virtual, 2020.   \n[37] Qizhang Li, Yiwen Guo, Wangmeng Zuo, and Hao Chen. Squeeze training for adversarial robustness. In Proceedings of ICLR, May 1-5, 2023, Kigali, Rwanda, 2023.   \n[38] Xin Li, Yao Qiang, Chengyin Li, Sijia Liu, and Dongxiao Zhu. Saliency guided adversarial training for learning generalizable features with applications to medical imaging classification system. CoRR, abs/2209.04326, 2022.   \n[39] Yandong Li, Lijun Li, Liqiang Wang, Tong Zhang, and Boqing Gong. NATTACK: learning the distributions of adversarial examples for an improved black-box attack on deep neural networks. In Proceedings of ICML, June 9-15, 2019, Long Beach, California, USA, pages 3866\u20133876, 2019.   \n[40] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In Proceedings of ICLR, April 30 - May 3, 2018, Vancouver, BC, Canada, 2018.   \n[41] Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. CoRR, abs/1411.1784, 2014.   \n[42] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Jonathan Uesato, and Pascal Frossard. Robustness via curvature regularization, and vice versa. In Proceedings of CVPR, June 16-20, 2019, Long Beach, CA, USA, pages 9078\u20139086, 2019.   \n[43] Rapha\u00ebl Olivier, Bhiksha Raj, and Muhammad Shah. High-frequency adversarial defense for speech and audio. In Proceedings of ICASSP, June 6-11, 2021, Toronto, ON, Canada, pages 2995\u20132999, 2021.   \n[44] Tianyu Pang, Min Lin, Xiao Yang, Jun Zhu, and Shuicheng Yan. Robustness and accuracy could be reconcilable by (proper) definition. In Proceedings of ICML, July 17-23, 2022, Baltimore, Maryland, USA, pages 17258\u201317277, 2022.   \n[45] Nicolas Papernot, Patrick D. McDaniel, Ian J. Goodfellow, Somesh Jha, Z. Berkay Celik, and Ananthram Swami. Practical black-box attacks against machine learning. In Proceedings of AsiaCCS, April 2-6, 2017, Abu Dhabi, United Arab Emirates, pages 506\u2013519, 2017.   \n[46] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Proceedings of ICML, July 18-24, 2021, Virtual Event, pages 8748\u20138763, 2021.   \n[47] Sylvestre-Alvise Rebuff,i Sven Gowal, Dan A. Calian, Florian Stimberg, Olivia Wiles, and Timothy A. Mann. Fixing data augmentation to improve adversarial robustness. CoRR, abs/2103.01946, 2021.   \n[48] Leslie Rice, Eric Wong, and J. Zico Kolter. Overftiting in adversarially robust deep learning. In Proceedings of ICML, July 13-18, 2020, Virtual Event, pages 8093\u20138104, 2020.   \n[49] Gaurang Sriramanan, Sravanti Addepalli, Arya Baburaj, and Venkatesh Babu R. Guided adversarial attack for evaluating and enhancing adversarial defenses. In Proceedings of NeurIPS, December 6-12, 2020, virtual, pages 20297\u201320308, 2020.   \n[50] Gaurang Sriramanan, Sravanti Addepalli, Arya Baburaj, and Venkatesh Babu R. Towards efficient and effective adversarial training. In Proceedings of NeurIPS, December 6-14, 2021, virtual, pages 11821\u201311833, 2021.   \n[51] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In Proceedings of ICLR, April 14-16, 2014, Banff, AB, Canada, 2014.   \n[52] Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry. Robustness may be at odds with accuracy. In Proceedings of ICLR, May 6-9, 2019, New Orleans, LA, USA, 2019.   \n[53] Haohan Wang, Xindi Wu, Zeyi Huang, and Eric P. Xing. High-frequency component helps explain the generalization of convolutional neural networks. In Proceedings of CVPR, June 13-19, 2020, Seattle, WA, USA, pages 8681\u20138691, 2020.   \n[54] Yisen Wang, Difan Zou, Jinfeng Yi, James Bailey, Xingjun Ma, and Quanquan Gu. Improving adversarial robustness requires revisiting misclassified examples. In Proceedings of ICLR, April 26-30, 2020, Addis Ababa, Ethiopia, 2020.   \n[55] Zekai Wang, Tianyu Pang, Chao Du, Min Lin, Weiwei Liu, and Shuicheng Yan. Better diffusion models further improve adversarial training. In Proceedings of ICML, July 23-29, 2023, Honolulu, Hawaii, USA, pages 36246\u201336263, 2023.   \n[56] Dongxian Wu, Shu-Tao Xia, and Yisen Wang. Adversarial weight perturbation helps robust generalization. In Proceedings of NeurIPS, December 6-12, 2020, virtual, pages 2958\u20132969, 2020.   \n[57] Cihang Xie, Zhishuai Zhang, Yuyin Zhou, Song Bai, Jianyu Wang, Zhou Ren, and Alan L. Yuille. Improving transferability of adversarial examples with input diversity. In Proceedings of CVPR, June 16-20, 2019, Long Beach, CA, USA, pages 2730\u20132739, 2019.   \n[58] Qinwei Xu, Ruipeng Zhang, Ya Zhang, Yanfeng Wang, and Qi Tian. A fourier-based framework for domain generalization. In Proceedings of CVPR, June 19-25, 2021, virtual, pages 14383\u2013 14392, 2021.   \n[59] Dong Yin, Raphael Gontijo Lopes, Jonathon Shlens, Ekin Dogus Cubuk, and Justin Gilmer. A fourier perspective on model robustness in computer vision. In Proceedings of NeurIPS, December 8-14, 2019, Vancouver, BC, Canada, pages 13255\u201313265, 2019.   \n[60] Sangdoo Yun, Dongyoon Han, Sanghyuk Chun, Seong Joon Oh, Youngjoon Yoo, and Junsuk Choe. Cutmix: Regularization strategy to train strong classifiers with localizable features. In Proceedings of ICCV, October 27 - November 2, 2019, Seoul, Korea (South), pages 6022\u20136031, 2019.   \n[61] Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P. Xing, Laurent El Ghaoui, and Michael I. Jordan. Theoretically principled trade-off between robustness and accuracy. In Proceedings of ICML, June 9-15, 2019, Long Beach, California, USA, volume 97, pages 7472\u20137482, 2019.   \n[62] Xinze Zhang, Junzhe Zhang, Zhenhua Chen, and Kun He. Crafting adversarial examples for neural machine translation. In Proceedings of ACL/IJCNLP, August 1-6, 2021, Virtual Event, pages 1967\u20131977, 2021.   \n[63] Dawei Zhou, Nannan Wang, Heng Yang, Xinbo Gao, and Tongliang Liu. Phase-aware adversarial defense for improving adversarial robustness. In Proceedings of ICML, July 23-29, 2023, Honolulu, Hawaii, USA, pages 42724\u201342741, 2023.   \n[64] Wei Zou, Shujian Huang, Jun Xie, Xinyu Dai, and Jiajun Chen. A reinforced generation of adversarial examples for neural machine translation. In Proceedings of ACL, July 5-10, 2020, Online, pages 3486\u20133497, 2020. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Table of Contents ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Pseudocodes of AE Generation and DAT 16 ", "page_idx": 14}, {"type": "text", "text": "A.1 Pseudocode of the AE Generation Method 16   \nA.2 Pseudocode of DAT . 16 ", "page_idx": 14}, {"type": "text", "text": "B Related Work & Background 17 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Adversarial Attacks 17   \nB.2 Adversarial Training . . 17   \nB.3 Generation of Adversarial Example in Adversarial Training 18   \nB.4 Discrete Fourier Transform 18 ", "page_idx": 14}, {"type": "text", "text": "C Experiments of the Amplitude Spectrum Operation 18 ", "page_idx": 14}, {"type": "text", "text": "C.1 Visualization of the Mix-up Operation on Amplitude Spectrum . . . 18   \nC.2 Impact of the Range of Amplitude Mixture Parameters . . 19 ", "page_idx": 14}, {"type": "text", "text": "D Batch Normalization Parameter Analysis 19 ", "page_idx": 14}, {"type": "text", "text": "E Detailed Experimental Setup 20 ", "page_idx": 14}, {"type": "text", "text": "E.1 Datasets . 20   \nE.2 Training Settings . . 20   \nE.3 Baselines . . 21   \nE.4 Robustness Evaluation . 21 ", "page_idx": 14}, {"type": "text", "text": "F Additional Results and Discussion 21 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "F.1 Comparison for Different AT Methods with Generated Data 21   \nF.2 Comparison of DAT with Different Augmentations . . . . . 22   \nF.3 Comparison of Training Time Consumption 22   \nF.4 Hyper-parameter Exploration . . . 23   \nF.5 Iteration Step $K$ of AE Generation . . . 23   \nF.6 AAG with Existing AT Methods . . . . . 23   \nF.7 Impact of Different Inputs of $G_{\\psi}$ . . . . . 24   \nF.8 Single AE Generation of DAT . . . 24   \nF.9 Limitations and Future Works . . 25 ", "page_idx": 14}, {"type": "text", "text": "G Theoretical Analysis and Proof 26 ", "page_idx": 14}, {"type": "text", "text": "This appendix provides additional support to the main ideas presented in the submission. The general pipeline is as follows: ", "page_idx": 14}, {"type": "text", "text": "\u2022 $\\S\\mathrm{A}$ presents the algorithms of the proposed efficient AE generation and DAT.   \n\u2022 $\\S B$ presents comprehensive related works, including adversarial attack and adversarial training methods.   \n\u2022 $\\S C$ presents additional details of AAG (e.g., mix-up operation and $\\lambda$ range).   \n\u2022 $\\S D$ presents the gap of BN parameters for benign and recombined data, showing the reason for the split BN operation in the model training of DAT.   \n\u2022 $\\S\\mathrm{E}$ presents more information about the training settings and selected datasets.   \n\u2022 $\\S\\mathrm{F}$ presents omitted experimental results (e.g., synthesized methods, data augmentation strategies and training time, iteration $K$ and different inputs of AAG).   \n\u2022 $\\S\\mathrm{G}$ presents explanation and proof for Theorem 3.2. ", "page_idx": 14}, {"type": "text", "text": "A Pseudocodes of AE Generation and DAT ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.1 Pseudocode of the AE Generation Method ", "page_idx": 15}, {"type": "table", "img_path": "TeQvz5AlI8/tmp/b948b8af669d979ef361db06d2d2819135f0fc9f7a822928eed5016cb55e064c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "A.2 Pseudocode of DAT ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Algorithm 2 DUAL ADVERSARIAL TRAINING (DAT) ", "text_level": 1, "page_idx": 15}, {"type": "table", "img_path": "TeQvz5AlI8/tmp/201b2717857f61c957b4b9edefa00b8c960103907e8c503501d46e793654ac6d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "B Related Work & Background ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "B.1 Adversarial Attacks ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "With the heightened attention on DNN vulnerabilities, numerous adversarial attack methods have developed to study the model\u2019s robustness. FGSM [22] generates AEs by applying a gradient on benign samples in a single iteration step. Building upon I-FGSM [34], PGD [40] emerges as the strongest first-order attack, amplifying the adversarial impact of AEs. In order to reduce the difficulty of parameter settings in FGSM, the accurate and efficient Deepfool [42] is developed. To enhance the effect of PGD attack, APGD, and APGD-DLR are proposed [13]. These methods have alternative loss functions and do not require an inner step size in the AEs\u2019 generation procedure. However, these white-box attacks require detailed knowledge of the target model to generate AEs, which poses challenges in practical scenarios. To address this challenge, black-box [45, 9, 27, 10, 57, 24] and no-box [45, 36] adversarial attacks are proposed. For a comprehensive evaluation of model robustness, ensemble adversarial attacks consisting of multiple types of adversarial attacks become popular. AutoAttack (AA) [13] stands out as a representative ensemble attack approach, seamlessly integrating white-box attacks including APGD, APGD-DLR, and FAB [12], alongside a black-box attack called Square attack [3]. AA is broadly recognized as a benchmark tool for the robustness evaluation of models. ", "page_idx": 16}, {"type": "text", "text": "B.2 Adversarial Training ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In an effort to protect models from adversarial attacks, various methodologies have been developed. Adversarial training is one of the most effective methods for countering adversarial attacks [64, 62, 4, 5, 19, 20, 40, 61, 1, 35]. PGD-AT [40] formulates AT as a min-max optimization problem. AEs are first generated via PGD and then fed into the trained model to minimize empirical risk. To address the robust overfitting issues, early stopping of PGD-AT is proposed to further enhance the model\u2019s robustness [48]. However, PGD-AT only takes AEs into the training procedure. This approach, due to the data distribution shift between AEs and benign samples, can diminish the model\u2019s accuracy. To make a better tradeoff between accuracy and robustness, TRADES [61] takes both benign samples and AEs into the AT procedure. Motivated by weight perturbation methods in standard training, Adversarial Weight Perturbation (AWP) [56] adversarially perturbs both inputs and weights, markedly easing the robust overfitting issues and improving adversarial robustness. Additionally, Stochastic Weight Averaging (SWA) [28] is also a frequently used technology to reduce the negative impact of robust overftiting issues and enhance the model\u2019s robustness. Misclassification Aware adveRsarial Training (MART) [54] adopts misclassified training samples as regularizers to enhance the model\u2019s robustness against AEs generated by various adversarial attacks. Instead of generating AEs by maximizing the prediction distance between AEs and benign samples as in TRADES, Squeeze Training (ST) [37] selects a better reference target and uses collaborative examples to benign ones, produced by minimizing the prediction distance between collaborative and benign samples. Different from existing hand-crafted strategy based AT, Adversarial Training with Learnable Attack Strategy (LAS-AT) [29] generates AEs by a reinforcement learning network. Motivated by Contrastive Language-Image Pre-training (CLIP) [46], Semantic Constraint Adversarial Robust Learning (SCARL) [33] uses the text information to obtain robust semantic information of training samples, improving the model\u2019s robustness. Due to the positive impact of data augmentation on AT, Diverse Augmentation-based Joint Adversarial Training (DAJAT) [1] uses AutoAugment, SWA, and AWP to achieve an effective AT. To reduce the time consumption of AE generation, DAJAT adopts variable adversarial perturbation radius $\\epsilon$ and inner step size $\\alpha$ to reduce the iteration steps. OA-AT [2] uses SWA and variable $\\epsilon$ and $\\alpha$ to protect the model from AEs with large adversarial magnitudes. Li et al. [35] show that the effectiveness of data augmentations on robustness improvement depends on their impacts on the model\u2019s robust accuracy on test data, referred to as hardness. Moreover, the work proves that augmentation strategies with moderate hardness can protect the model from robust overftiting issues and enhance the model\u2019s robustness. Based on the phenomena, with variable adversarial perturbation radius $\\epsilon$ , Improved Diversity and Balanced Hardness (IDBH) [35] combines several data augmentation strategies to obtain significant robustness improvement against various adversarial attacks. Recently, motivated by the stable diffusion (SD) model [30], some methods [47, 23, 15, 55, 44] synthesize data by SD to enhance the model\u2019s robustness further. ", "page_idx": 16}, {"type": "text", "text": "B.3 Generation of Adversarial Example in Adversarial Training ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "PGD-AT [40] formulates a min-max strategy to inject AEs into AT. The optimization objective is ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta}\\sum_{i=1}^{N}\\operatorname*{max}_{\\mathbf{x}_{i}^{\\prime}\\in S_{\\epsilon}[\\mathbf{x}_{i}]}\\mathcal{L}_{\\sf C E}(f_{\\theta}(\\mathbf{x}_{i}^{\\prime}),y_{i}),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\mathbf{x}_{i}^{\\prime}$ is the AE for $\\mathbf{x}_{i}$ , and $\\mathcal{L}_{\\sf C E}$ represents the cross-entropy (CE) loss. To achieve a better tradeoff between natural and robust accuracy, TRADES [61] incorporates both AEs and benign samples into training as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta}\\sum_{i=1}^{N}\\bigg(\\mathcal{L}_{\\mathsf{C E}}(f_{\\theta}(\\mathbf{x}_{i}),y_{i}))+\\beta\\cdot\\operatorname*{max}_{\\mathbf{x}_{i}^{\\prime}\\in\\mathcal{S}_{\\epsilon}[\\mathbf{x}_{i}]}\\mathcal{D}_{\\mathsf{K L}}(f_{\\theta}(\\mathbf{x}_{i}^{\\prime}),f_{\\theta}(\\mathbf{x}_{i}))\\bigg),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\beta$ is a weight parameter, and $\\mathcal{D}_{\\sf K L}$ represents the Kullback-Leibler (KL) divergence. ", "page_idx": 17}, {"type": "text", "text": "For $(\\mathbf{x},y)\\in\\mathcal{D}$ , its corresponding AE $\\mathbf{x}^{\\prime}$ is initialized by adding a random noise (e.g., uniform noise in PGD and Gaussian noise in TRADES) on $\\mathbf{x}$ , then iteratively updated by $K$ steps following ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{x}^{\\prime(k+1)}=\\Pi_{S_{\\epsilon}[\\mathbf{x}]}\\big(\\mathbf{x}^{\\prime(k)}+\\alpha\\cdot\\mathrm{sign}(\\nabla_{\\mathbf{x}^{\\prime(k)}}\\mathcal{L}(f_{\\theta}(\\mathbf{x}^{\\prime(k)}),y))\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\Pi(\\cdot)$ is the projection operator, $\\alpha$ is the projection inner step size, and $k\\in\\{0,...,K-1\\}$ , with $K$ typically set to 10. ", "page_idx": 17}, {"type": "text", "text": "B.4 Discrete Fourier Transform ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "DFT transforms an image signal from the spatial domain into the frequency domain, while the inverse discrete Fourier transform (IDFT) reverses this process. Let $\\mathcal F(\\cdot)$ and $\\bar{\\mathcal{F}}^{\\bar{-}1}(\\cdot,\\cdot)$ denote the DFT and IDFT functions, respectively. Typically, DFT is independently applied to each channel of an image within the pixel space. An image $\\mathbf{x}$ can be transformed into the frequency domain as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{F}(\\mathbf{x})(u,v)=\\sum_{h=1}^{H}\\sum_{w=1}^{W}\\mathbf{x}(h,w)\\,\\mathrm{e}^{-\\mathrm{i}2\\pi(u\\frac{h}{H}+v\\frac{w}{W})},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $(h,w)$ denotes the pixel coordinates of $\\mathbf{x}$ , and $(u,v)\\in[H]\\times[W]$ signifies coordinates in the frequency domain. The real and imaginary parts of ${\\mathcal{F}}(\\mathbf{x})$ are denoted by $\\operatorname{Re}({\\mathcal{F}}(\\mathbf{x}))$ and $\\operatorname{Im}({\\mathcal{F}}(\\mathbf{x}))$ , respectively. Then, the amplitude spectrum $\\mathcal{A}(\\mathbf{x})$ and phase spectrum $\\mathcal{P}(\\mathbf{x})$ are defined as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n{A}({\\mathbf x})=\\left(\\mathrm{Re}^{2}({\\mathcal F}({\\mathbf x}))+\\mathrm{Im}^{2}({\\mathcal F}({\\mathbf x}))\\right)^{\\frac{1}{2}},\\quad{\\mathcal P}({\\mathbf x})=\\arctan\\left(\\frac{\\mathrm{Im}({\\mathcal F}({\\mathbf x}))}{\\mathrm{Re}({\\mathcal F}({\\mathbf x}))}\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "C Experiments of the Amplitude Spectrum Operation ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "To better explore the proposed AAG, we present the amplitude spectrum mix-up operation visually and explore the impact of the $\\lambda$ range on the model\u2019s robustness in this part. First, we display and explain the visual results of the recombined samples and show the impact of the mix-up operation on the model\u2019s performance. Then, the experimental results of DAT with different ranges of $\\lambda$ are shown. ", "page_idx": 17}, {"type": "text", "text": "C.1 Visualization of the Mix-up Operation on Amplitude Spectrum ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Figure 8 (see page 28) shows the benign samples, amplitude spectrum, phase spectrum, generated amplitude spectrum, and recombined data. Compared to images with mixed amplitude spectrum, data recombined by replacing the original amplitude from the generated one entirely has a great gap for the original benign samples, reducing the natural and robust accuracy of the benign test samples. To better show the impact of the different strategies on the model\u2019s performance, we perform experiments with and without the mix-up operation. Table 5 shows the DAT with mix-up has a better performance. That indicates the model still needs some original information on the amplitude spectrum, showing the effectiveness of the mix-up operation. ", "page_idx": 17}, {"type": "text", "text": "Table 5: Average experimental results of DAT with and without Mix-up on CIFAR-10 and CIFAR-100 with ResNet-18 and $\\epsilon={^8}/{255}$ in 7 runs. ", "page_idx": 18}, {"type": "table", "img_path": "TeQvz5AlI8/tmp/b27c0d40f9434c6dcdacc3f1217e383b7285e4052435e234a8c63a61269d8e8d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "TeQvz5AlI8/tmp/d0177517387f113f06278a4e6bcada0a3ecc379b414795386cd6aa694081aa7d.jpg", "img_caption": ["(a) Natural accuracy of DAT with different $\\mu$ ", "Figure 4: Natural and robust accuracy $(\\%)$ against AA of DAT with different $\\mu$ on CIFAR-10 and CIFAR-100 with ResNet-18. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "TeQvz5AlI8/tmp/85795e8da6bdc1054fbcdd3cb77e481eeccc69c862ee08f21227a28b325a6e4b.jpg", "img_caption": ["(b) Robust accuracy against AA of DAT with different $\\mu$ . "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "C.2 Impact of the Range of Amplitude Mixture Parameters ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this subsection, to fully explore the impact of the $\\lambda$ range on the model\u2019s robustness, we perform some experiments to show the impact of different $\\lambda$ on the natural and robust accuracy. As shown in the work, the mix-up operation to perturb $\\mathcal{A}(\\mathbf{x})$ by the generated $A_{G}(\\mathbf{x})$ follows ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{A}_{m i x}(\\mathbf{x})=\\lambda\\mathcal{A}_{G}(\\mathbf{x})+(1-\\lambda)\\mathcal{A}(\\mathbf{x}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $0<\\lambda<1$ , reserving some information of ${\\mathcal{A}}(x)$ . Suppose that $\\lambda\\sim\\mathrm{Uniform}(0,\\mu)$ , where $0<\\mu<1$ , we perform experiments with different values of $\\mu$ on CIFAR-10 and CIFAR-100 against AA, where $\\beta\\,=\\,15$ and $\\omega\\,=\\,2$ . As shown in Figures 4a and 4b, the test accuracy fluctuations between different values of $\\mu$ are quite large, and the best settings of $\\mu$ for natural and robust accuracy for CIFAR-10 and CIFAR-100 are different. For CIFAR-10, the model achieves the best natural performance when $\\mu=0.6$ , while its robust accuracy is better with $\\mu=0.8$ . For results on CIFAR-100, $\\mu=0.4$ makes the best natural accuracy when the model achieves better robustness with $\\mu=0.6$ . In the work, there are already two hyper-parameters: $\\omega$ and $\\beta$ . Moreover, the range of $\\lambda$ is more likely to influence the settings of $\\omega$ and $\\beta$ . Consequently, we do not discuss the impact of different ranges of $\\lambda$ on the model\u2019s robust and natural accuracy in the main paper. ", "page_idx": 18}, {"type": "text", "text": "For the real-valued $\\mathbf{x}$ , it is noticed that $\\mathcal F(\\cdot)$ need to be even-conjugate, i.e., $\\mathcal{F}(\\mathbf{x})(-u,-v)\\,=$ $\\overline{{\\mathcal{F}(\\mathbf{x})(u,v)}}$ , implying that the amplitude spectrum is symmetric. Conversely, IDFT returns realvalued signals for a symmetric amplitude spectrum. Consequently, $G_{\\psi}$ only generates $A_{G}$ with the non-redundant and non-negative part of the amplitude spectrum. ", "page_idx": 18}, {"type": "text", "text": "D Batch Normalization Parameter Analysis ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In the DAT training procedure, we use different BNs for the benign data and their recombined samples. In the ablation study, we show the model performances with and without a split BN are quite different, showing that the single BN significantly influences the model\u2019s performance. To better present the necessity of the split BN in DAT training, we show the cosine similarity of the parameters of each BN layer between benign data and recombined ones. There are 20 BN layers in ResNet-18. From Figures 5a and 5b, on both CIFAR-10 and CIFAR-100, we can see the cosine similarity gaps for BN parameters in different layers. The distance between the mean and variance of each BN layer is small, while the gaps between the $\\gamma\\mathbf{s}$ and $\\beta\\mathrm{s}$ are large, especially in the low-level layers and the ", "page_idx": 18}, {"type": "image", "img_path": "TeQvz5AlI8/tmp/1f6ee511558d8ffa344db056485f753bfc13bee91b0459d0d01b7b0830048749.jpg", "img_caption": ["(a) Cosine similarity between the BN parameters on CIFAR-10. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "TeQvz5AlI8/tmp/741590dbe639fae268a5d3d359b79824f934173f303a747337db39c9d3636b37.jpg", "img_caption": ["(b) Cosine similarity between the BN parameters on CIFAR-100. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 5: Cosine similarity between the BN parameters of the original data and recombined ones in ResNet-18 on (a) CIFAR-10 and (b) CIFAR-100. ", "page_idx": 19}, {"type": "text", "text": "last layer. Consequently, using a single BN in the training procedure of DAT is likely to result in the difficulty of model convergence, showing as the performance comparisons in the ablation study. To solve the issue, we adopt different BNs for the original data and the recombined ones. In the model training procedure of DAT, we split the BN into two groups: BN-A and BN-B, for each layer with BN, original data and its AE are processed by BN-A, while recombined one and its AE are regularized by BN-B. ", "page_idx": 19}, {"type": "text", "text": "E Detailed Experimental Setup ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we add more details about the datasets and experimental settings. Moreover, the information about the robust evaluation is further introduced. ", "page_idx": 19}, {"type": "text", "text": "E.1 Datasets ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "To evaluate the performance of DAT, we select three widely used benchmark datasets for robust evaluation: CIFAR-10, CIFAR-100, and complex Tiny ImageNet [17]. CIFAR-10 and CIFAR-100 contain $50\\!,\\!000\\,\\!32\\!\\times\\!32$ training samples and $10\\small{,}000\\;32\\!\\times\\!32$ test images, categorized into 10 and 100 classes respectively. Tiny ImageNet is a challenge 200-class real-world dataset, where there are 500 training and 50 test images for each category, where the image size is $64\\!\\times\\!64$ . Moreover, due to the samples in the test set of Tiny ImageNet without labels, we evaluate the robustness of the validation set following [29, 33]. ", "page_idx": 19}, {"type": "text", "text": "E.2 Training Settings ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We use ResNet-18, WRN-34-10, and WRN-28-10 as model architectures. Experiments with ResNet18 are performed on Ubuntu 20.04.3 LTS GPU server with Intel Xeon 5120 and $5{\\times}3090$ by PyTorch 2.0, while WRN-34-10 and WRN-28-10 experiments are performed on DGX with a H800 GPU on PyTorch 2.0. In the model training procedure, we adopt an SGD optimizer with momentum 0.9 and weight decay 5e-4. For the common experiments, the model is trained for 150 epochs for CIFAR-10 and CIFAR-100 and 100 epochs for Tiny ImageNet. Moreover, the learning rate follows the schedule [0.1, 0.01, 0.001] in decay epoch schedule [100, 110] in CIFAR-10 and CIFAR-100 and in decay epoch schedule [75, 80] for Tiny ImageNet. For these datasets, the experiments with AWP or the combination of AWP and SWA use the same learning rate schedule with decay epoch as [100, 150]. ", "page_idx": 19}, {"type": "text", "text": "For the proposed AAG, we use a four-linear layer structure to build it. Since only the non-negative part of the amplitude spectrum is produced, the output of AAG is processed by a Sigmoid function. In the model training procedure, we set the dimension of $\\mathbf{z}$ as $\\tau=100$ . Due to the input of AAG combined by ${\\bf z}$ and sample logits extracted by $f_{\\theta}$ , the input dimension of AAG is the sum of $\\tau$ and class number $c$ of $\\mathcal{D}$ . ", "page_idx": 19}, {"type": "text", "text": "The AAG is optimized by an SGD optimizer with a fixed learning rate 0.1, momentum 0.9, and weight decay 5e-4. For hyper-parameters, $\\beta$ and JS weight parameter $\\omega$ are set as 15 and 2 respectively. During the training procedure, we adopt the basic data augmentation strategies, Random Crop and Random Horizontal Flip, for all selected datasets. For AE generation, the inner step size $\\alpha$ is set to $^2\\!/\\!255$ with $K=5$ to generate adversarial perturbation $\\ell_{\\infty}$ -bounded with radius $\\epsilon={^8}/{255}$ following previous work [29, 37, 33]. ", "page_idx": 20}, {"type": "text", "text": "E.3 Baselines ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "For experimental result comparisons in this work, we select two typical PGD-AT and TRADES [40, 61] as baselines. Moreover, we adopt two types of existing methods to perform experimental result comparisons. For common type AT methods, which are trained with fixed $\\epsilon=8/255$ and inner step size $\\alpha={^2}/{255}$ and without any extra technologies, we select MART [54], ST [37], LAS-AT [29] and SCARL [33]. Additionally, some methods with complex strategies, such as AWP, SWA, variable $\\epsilon$ , and changeable $\\alpha$ are also selected as baselines. For AutoAugment-based DAJAT [1] consisting of AT, SWA, variable $\\epsilon$ and $\\alpha$ , we select it with one augmentation for a fair comparison. OA-AT [2] is a variable $\\epsilon$ based AT method with SWA. IDBH [35] is an AT method based on the augmentation combination of Cropping, CutOut, and ColorShape, and is combined with AWP and SWA strategies. ", "page_idx": 20}, {"type": "text", "text": "E.4 Robustness Evaluation ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "For the experimental results, we report the model\u2019s natural and robust accuracy. We select several widely used types of adversarial attacks to generate AEs for robustness evaluation. FGSM, PGD20, PGD-100, and $\\mathrm{C}\\&\\mathrm{W}_{\\infty}$ are selected as basic methods to evaluate the model\u2019s robustness. To better show the model\u2019s robust generalization against different adversarial attacks, we also select AA consisting of black-box and white-box methods to evaluate the robustness of the model. The adversarial perturbation of these methods is $\\ell_{\\infty}$ -bounded with radius $\\epsilon=8/255$ and inner step size $\\alpha={^2}/255$ . ", "page_idx": 20}, {"type": "text", "text": "F Additional Results and Discussion ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Due to the limitation of the pages, we perform some experiments on CIFAR-10 and CIFAR-100 to show the effectiveness of our proposed DAT further. In the first section, we show the experimental results on generated data with WRN-28-10. The experimental comparison between proposed DAT and some data augmentation strategies are presented in the second section. Then, we show the time consumption comparison between DAT and existing methods in the third part. The fourth subsection discusses the impact of different $\\beta$ and $\\omega$ . Then, the impact of iteration $K$ of AE generation on the model\u2019s performance is explored in the fifth subsection. In the sixth part, the AAG is combined with existing methods to verify its effectiveness. Then, we explore the influence of AAG\u2019s input. The last two subsections show the results comparison of single and dual AE generation of DAT and limitations of the work. ", "page_idx": 20}, {"type": "text", "text": "F.1 Comparison for Different AT Methods with Generated Data ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this subsection, following [47, 23, 15, 55, 44], we take advantage of the diffusion model to synthesize data enhance the model\u2019s robustness further. Since these existing methods are combined with strategies such as AWP, SWA, variable $\\epsilon$ and changed $\\alpha$ , we use the DAT with AWP and SWA to show the experimental results comparison. Due to methods only providing single-time results, we just use the one-time test accuracy of DAT to compare with them on WRN-28-10. From Table 6, compared with IKL-AT [15], we can see DAT achieves about ${\\sim}0.69\\%$ and ${\\sim}0.48\\%$ robustness improvement with 1M and 20M generated data on CIFAR-10. On CIFAR-100, as shown in Table 7, for current IKL-AT, the method with the best robustness against AA with WRN-28-10 on the RobustBench [11], the model robustness is enhanced by ${\\sim}0.29\\%$ and ${\\sim}0.23\\%$ with 1M and 50M synthesized samples, respectively. ", "page_idx": 20}, {"type": "text", "text": "Table 6: The average experimental results for different augmentations against $\\ell_{\\infty}$ threat model with $\\epsilon=8/255$ on CIFAR-10. #Aug. refers to the number of augmentation. The best results are boldfaced. ", "page_idx": 21}, {"type": "table", "img_path": "TeQvz5AlI8/tmp/777f5091b0670e914766487b37f10634e62027d31c6e6ae603b1a05c1329e305.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "TeQvz5AlI8/tmp/18f03ac689122b3ed45b9f188a6e57b4ad1d4a198488a601504f6247bfe06868.jpg", "table_caption": ["Table 7: The average experimental results for different augmentations against $\\ell_{\\infty}$ threat model with $\\epsilon=8/255$ on CIFAR-100. #Aug. refers to the number of augmentation. The best results are boldfaced. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "F.2 Comparison of DAT with Different Augmentations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Since the recombined data based on AAG can be regarded as an augmentation for benign samples, to better present the effectiveness of the proposed strategy, we combine the proposed AE generation method with three widely used augmentation policies in AT, such as CutOut [18], CutMix [60], and AutoAugment [14], and perform experiments on CIFAR-10 and CIFAR-100. The settings of CutMix and CutOut are as [35], while AutoAugment follows [1]. In these experiments, the baseline is AT with the proposed AE generation strategy. These selected data augmentation strategies adopt the same training strategy as DAT. As shown in Table 8, for the baseline, these augmentations enhance the model\u2019s robustness against PGD-20 and AA. Compared to other strategies, DAT achieves robustness improvement of ${\\sim}1\\%$ on CIFAR-10 and ${\\sim}1.2\\%$ on CIFAR-100. ", "page_idx": 21}, {"type": "text", "text": "F.3 Comparison of Training Time Consumption ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this part, we compare the time consumption of different methods on CIFAR-10 and CIFAR-100. From Table 9, we can obtain the time consumption of DAT is slightly higher than PGD-AT and TRADES because of added AAG. Compared with recently proposed methods, DAT takes less time and achieves better robustness. ", "page_idx": 21}, {"type": "table", "img_path": "TeQvz5AlI8/tmp/b3ecc006458c027b276a47d8ae8997ac726dfc6ab8f0f1ec61172599f9ddf5b7.jpg", "table_caption": ["Table 8: The average experimental results for different augmentations against $\\ell_{\\infty}$ threat model with $\\epsilon=8/255$ in 7 runs. The best results are boldfaced. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "Table 9: Time consumption (s) of each training epoch for different AT methods. ", "page_idx": 22}, {"type": "table", "img_path": "TeQvz5AlI8/tmp/1fff4818a5fc82f6a585b2268e23023efde77ebcc9d3d08d52f37d71dd2073ac.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "F.4 Hyper-parameter Exploration ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "After the performance comparisons, we then explore the settings of $\\beta$ and $\\omega$ by experiments on CIFAR-10 and CIFAR-100. Additionally, the $\\lambda$ \u2019s range and the iteration step $K$ of AE generation are also discussed, attached to Appendixes C.2 and F.5. ", "page_idx": 22}, {"type": "text", "text": "Setting of $\\beta$ . As shown in Figure 6a of Appendix F.4, we present the model\u2019s natural and robust accuracy with different $\\beta$ with $\\omega=2$ and $K=5$ . We can see that the curves of robust accuracy rise first and then fall. Meanwhile, natural accuracy keeps decreasing as $\\beta$ increases, indicating that $\\beta$ significantly influences the model\u2019s natural and robust accuracy. To achieve a better tradeoff between robust and natural accuracy, we set $\\beta=15$ for experiments of DAT on all datasets. ", "page_idx": 22}, {"type": "text", "text": "Setting of $\\omega$ . With $\\beta=15$ and $K=5$ , we discuss the settings of $\\omega$ . Figure 6b shows the model\u2019s accuracy changes along with $\\omega$ on both natural and AE. When $\\omega<2$ , the curves of robust accuracy rise. On the contrary, $\\omega>2$ results in the performance decreasing. Unlike the robust accuracy, the model\u2019s performance on benign samples grows with the increase of $\\omega$ . For better model robustness, $\\omega$ is set as 2 in this work. ", "page_idx": 22}, {"type": "image", "img_path": "TeQvz5AlI8/tmp/6bf4222c459abd4b0bde8d270e77582ca93bb0e11c5d92e13f94b4385ba3841c.jpg", "img_caption": ["(a) Impact of $\\beta$ on natural and robust accuracy. ", "Figure 6: The impact of $\\omega$ and $\\beta$ on the model\u2019s natural (Na.) and robust accuracy (Rob.) against AA on CIFAR-10 (C-10) and CIFAR-100 (C-100) with ResNet-18. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "TeQvz5AlI8/tmp/49861b35f60f39c9ce076c2879f19d6e85481af15d8ac50e4e5f8fd44bae455b.jpg", "img_caption": ["(b) Impact of $\\omega$ on natural and robust accuracy. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "F.5 Iteration Step $K$ of AE Generation ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The generation iteration step $K$ has an extremely important impact on the model\u2019s robust accuracy. As shown in Figure 7, the larger the iteration step, the adversarial robustness against AA of the model rises with the increase of $K$ . However, the larger $K$ will significantly increase the time consumption of AT. To achieve a tradeoff between the model\u2019s robustness and time consumption, we set the iteration step as $K=5$ . Then, the time consumption of DAT will not be increased compared with existing methods. ", "page_idx": 22}, {"type": "text", "text": "F.6 AAG with Existing AT Methods ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "To further show the effectiveness of the proposed AAG, we combine AAG with PGD-AT and TRADES and perform experiments on CIFAR-10 and CIFAR-100 against PGD-20 and AA. Compared ", "page_idx": 22}, {"type": "image", "img_path": "TeQvz5AlI8/tmp/d81f1e1699ad86baacc02beb47bab6e1c18f397a1201cf5f813d70fd759e2150.jpg", "img_caption": ["Figure 7: The impact of different iteration step $K$ on the model\u2019s performance on natural data and against AA on CIFAR-10 and CIFAR-100 with ResNet-18. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "with PGD-AT and TRADES, the combinations with AAG enhance the model\u2019s robust and natural accuracy further. ", "page_idx": 23}, {"type": "table", "img_path": "TeQvz5AlI8/tmp/3c127bdbd79e7f4649b3694b49a15f08a83cbb8b40817d15764a9d88eefac5a0.jpg", "table_caption": ["Table 10: Average experimental results of AAG with existing AT methods on CIFAR-10/100 with ResNet-18 and $\\epsilon=8/255$ in 7 runs. "], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "TeQvz5AlI8/tmp/068e94de3a1be35e6745cc4824fb8ca771d4d7442d39f4f23ad2ca96331bdd4b.jpg", "table_caption": ["Table 11: Average experimental results of different inputs of $G_{\\psi}$ with ResNet-18 and $\\epsilon=8/255$ in 7 runs. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "F.7 Impact of Different Inputs of $G_{\\psi}$ ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "For the Generative Adversarial Nets (GANs), Conditional GANs (CGANs) [41] usually have higher quality generative results than vanilla ones. Consequently, motivated by CGAN, we use a combination of the sample logits from model $f_{\\theta}$ and stochastic sampling noise vector $\\mathbf{z}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})$ following a standard Gaussian distribution as input of $G_{\\psi}$ , which is different CGAN with one-hot label. To better show the effectiveness, we perform experiments on CIFAR-10 and CIFAR-100 against the PGD-20 and AA. Table 11 shows that the input of $G_{\\psi}$ significantly influences the model\u2019s natural and robust accuracy. Moreover, the performance of input with logits has a smaller variance than in other cases, indicating the logits make the training runs more stable. As shown in Figure 9 (see page 28), the gaps between the generated amplitude spectrum of different inputs are quite small. However, the recombination from the input with ${\\bf z}$ and logits have a small gap for benign samples, with no significantly changed sample. ", "page_idx": 23}, {"type": "text", "text": "F.8 Single AE Generation of DAT ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In the training procedure of DAT, we take both benign and recombined samples\u2019 AEs into AT. To reduce the time consumption of AE generation, we propose an efficient strategy to produce AEs. ", "page_idx": 23}, {"type": "text", "text": "We can also generate AE of recombined data by mixing the generated amplitude spectrum with that in the benign sample\u2019s AE. For an example, with $(\\mathbf{x},y)\\in\\mathcal{D}$ , we use the $\\mathbf{x}$ \u2019s AE $\\mathbf{x}^{\\prime}$ to obtain the amplitude of recombined AE as ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{A}_{m i x}(\\mathbf{x}^{\\prime})=\\lambda\\mathcal{A}_{G}(\\mathbf{x}^{\\prime})+(1-\\lambda)\\mathcal{A}(\\mathbf{x}^{\\prime}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Then, $\\hat{\\mathbf{x}}^{\\prime}$ is obtained by ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{x}}^{\\prime}=\\mathcal{F}^{-1}(\\mathcal{A}_{m i x}(\\mathbf{x}^{\\prime}),\\mathcal{P}(\\mathbf{x}^{\\prime})).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "table", "img_path": "TeQvz5AlI8/tmp/7b1964f0a33dbdde03a03aff8da1f01a53beb5db1791171325a9360a51cd9479.jpg", "table_caption": ["Table 12: Average experimental results with single and dual AE on CIFAR-10 and CIFAR-100 with ResNet-18 and $\\epsilon=8/255$ in 7 runs. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "We call such a procedure to obtain recombined data and AE a single AE DAT, while the process described in the main text is Dual AE DAT. To explore the difference in performance between the two AE generation strategies, we perform experiments with the same experimental settings in Sec. E.2 on CIFAR-10 and CIFAR-100 with ResNet-18. The PGD-20 and AA are selected to evaluate the model\u2019s performance with single or dual AE generation. As shown in Table 12, we report the natural and robust results. According to these experiments, the natural accuracy gap for the DAT with single and dual AE is small, while the difference for the robust performance is large. As mentioned in the work, the amplitude spectrum of the sample influences the model\u2019s learning of the phase patterns. The single AE of DAT does not reflect the impact of mixed amplitude x\u02c6 on adversarial perturbation, resulting in a lower robust performance than dual AE for DAT. Additionally, as shown in Table 4, although the model trained with single AE has a lower robust performance than common DAT, it still enhances the model\u2019s robustness compared to the baseline. That indicates the single AE for DAT can also enforce the model to learn more robust phase patterns. ", "page_idx": 24}, {"type": "text", "text": "F.9 Limitations and Future Works ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Although the proposed DAT significantly improves the model\u2019s robustness and natural accuracy of the model, there are still some limitations of the work. Since the robust over-fitting issues, DAT without AWP needs to be trained in a limited epoch, restricting its performance. Moreover, our DAT focused on protecting from adversarial attacks. Other types of image corruptions, e.g., Gaussian noise, and defocus blur, can also influence the model\u2019s performance. DAT needs to be developed further to protect the model from various image corruptions. Due to the limitation of page length, we only provide the experimental results on three frequently used datasets and deep models in the main paper. In the future, we will add experiments on more large-scale datasets with different deep models. ", "page_idx": 24}, {"type": "text", "text": "G Theoretical Analysis and Proof ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Here we restate and prove Theorem 3.2. ", "page_idx": 25}, {"type": "text", "text": "Theorem 3.2 (Weight Regularization of Amplitude Features). Grant Assumption 3.1, when the empirical risk $\\hat{R}$ is minimized with some convex loss function $\\mathcal{L}$ (e.g. CE loss): ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\hat{R}(\\mathbf{W}):=\\frac{1}{|\\mathcal{D}|}\\sum_{(\\mathbf{x},y)\\in\\mathcal{D}}\\mathbb{E}_{t(\\mathbf{x})\\sim\\mathcal{T}(\\mathbf{x})}\\left[\\mathcal{L}\\left(\\mathbf{W}^{\\top}\\mathbf{h}(t(\\mathbf{x})),y\\right)\\right],\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "we have $w_{j,a}\\to0$ for all $j\\in[c],$ , where $w_{j,a}$ is the corresponding weights of amplitude features $h_{a}$ . ", "page_idx": 25}, {"type": "text", "text": "We consider a quadratic approximation of the objective in Theorem 3.2, and provide a further and extended analysis following [16, 26]. Using the second-order Taylor expansion, we expand each term of the objective function: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}_{t(\\mathbf{x})\\sim T(\\mathbf{x})}\\left[\\mathcal{L}\\left(\\mathbf{W}^{\\top}\\mathbf{h}(t(\\mathbf{x})),y\\right)\\right]=\\mathcal{L}(\\mathbf{W}^{\\top}\\bar{\\mathbf{h}},y)+\\frac{1}{2}\\mathbb{E}_{t(\\mathbf{x})\\sim T(\\mathbf{x})}\\left[\\Delta^{\\top}\\mathbf{H}(\\zeta,y)\\Delta\\right],\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $\\bar{\\mathbf{h}}=\\mathbb{E}_{t(\\mathbf{x})\\sim\\mathcal{T}(\\mathbf{x})}\\left[\\mathbf{h}(t(\\mathbf{x}))\\right]$ , $\\Delta=\\mathbf{W}^{\\top}\\left(\\mathbf{h}(t(\\mathbf{x}))-\\bar{\\mathbf{h}}\\right)$ , and $\\mathbf{H}$ is the Hessian matrix with $\\zeta$ denoting the remainder. We introduce Lemma G.1 to demonstrate the properties of $\\mathbf{H}$ . ", "page_idx": 25}, {"type": "text", "text": "Lemma G.1. For the CE loss with softmax, the Hessian matrix w.r.t. the loss function satisfies ", "page_idx": 25}, {"type": "text", "text": "1. $\\mathbf{H}\\succeq0$ , i.e., $\\mathbf{H}$ is semi-definite. ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "2. H is independent of the true label vector y. ", "page_idx": 25}, {"type": "text", "text": "Proof. The CE loss for a single instance $\\mathbf{x}$ , when given the true label vector $\\mathbf{y}$ (using one-hot encoding with $c$ -dimension) is defined as ", "page_idx": 25}, {"type": "equation", "text": "$$\nL=-\\sum_{i=1}^{c}y_{i}\\log(p_{i}).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The gradient of $L$ w.r.t. the logits is computed as ", "page_idx": 25}, {"type": "equation", "text": "$$\n{\\frac{\\partial{\\cal L}}{\\partial z_{k}}}=p_{k}-y_{k},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "for $k\\in[c]$ , where $p_{k}=\\sigma(\\mathbf{z})_{k}$ is the predicted probability of class $k$ . The Hessian matrix $\\mathbf{H}$ , which is the matrix of second derivatives of $L$ , has elements given by ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\frac{\\partial^{2}L}{\\partial z_{k}\\partial z_{j}}=\\frac{\\partial}{\\partial z_{j}}(p_{k}-y_{k})=\\frac{\\partial p_{k}}{\\partial z_{j}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Using the derivation properties of the softmax function, the above derivative simplifies to ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\frac{\\partial p_{k}}{\\partial z_{j}}=p_{k}(\\delta_{k j}-p_{j}),\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $\\delta_{k j}$ is the Kronecker delta, equal to 1 if $k=j$ and 0 otherwise. Thus, the Hessian matrix $\\mathbf{H}$ is ", "page_idx": 25}, {"type": "equation", "text": "$$\nH_{k j}=p_{k}(\\delta_{k j}-p_{j}).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Positive Semi-Definiteness. The Hessian matrix can be expressed in matrix form as ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbf{H}=\\mathbf{P}(\\mathbf{I}-\\mathbf{P}),\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $\\mathbf{P}$ is a diagonal matrix with diagonal entries $p_{k}$ , and $\\mathbf{I}$ is the identity matrix. The product of $\\mathbf{P}$ and ${\\bf I}-{\\bf P}$ , where $\\mathbf{P}$ is a diagonal matrix with entries between 0 and 1, ensures that $\\mathbf{H}$ is positive semi-definite. ", "page_idx": 25}, {"type": "text", "text": "Independence from Label y. The structure and values of $\\mathbf{H}$ depend solely on the predicted probabilities $\\mathbf{p}$ , which are functions of $\\mathbf{z}$ and not dependent on the specific values of $\\mathbf{y}$ . This characteristic signifies that the Hessian\u2019s form is invariant w.r.t. the true label vector $\\mathbf{y}$ , thus highlighting its independence. \u53e3 ", "page_idx": 25}, {"type": "text", "text": "In the following, we omit the subscript of the expectation in Eq. (15) for simplicity. According to [8], when minimizing $\\hat{R}$ , the first term in RHS of Eq. (15) may be no room for further improvement because the local optima has nearly the same value as the global optimum in neural networks, and we put our main focus on the second term. We first derive an upper bound for the second term in Lemma G.2, since directly optimizing it which requires a third-order derivative is intractable. ", "page_idx": 26}, {"type": "text", "text": "Lemma G.2 (stated informally, $c f$ . Theorem 3.1 in [8]). Assume that the covariance between $\\|\\mathbf{H}\\|_{F}$ and $\\lVert\\Delta\\rVert_{2}$ is zero, we can get the upper bound of the second term as ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\Delta^{\\top}\\mathbf{H}\\Delta\\right]\\leq\\mathbb{E}\\left[\\left\\lVert\\mathbf{H}\\right\\rVert_{F}\\right]\\cdot\\mathbb{E}\\left[\\left\\lVert\\Delta\\right\\rVert_{2}^{2}\\right].\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. By H\u00f6lder\u2019s inequality, we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\Delta^{\\top}\\mathbf{H}\\Delta\\right]=\\mathbb{E}\\left[\\left\\Vert\\Delta\\right\\Vert_{p}\\left\\Vert\\mathbf{H}\\Delta\\right\\Vert_{q}\\right]\\leq\\mathbb{E}\\left[\\left\\Vert\\Delta\\right\\Vert_{p}\\left\\Vert\\mathbf{H}\\right\\Vert_{r,q}\\right\\Vert\\Delta\\left\\Vert_{r}\\right]=\\mathbb{E}\\left[\\left\\Vert\\mathbf{H}\\right\\Vert_{r,q}\\right]\\cdot\\mathbb{E}\\left[\\left\\Vert\\Delta\\right\\Vert_{p}\\left\\Vert\\Delta\\right\\Vert_{r}\\right],\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $1/p+1/q=1,\\,\\|\\cdot\\|r,q$ is an induced matrix norm. When $p=q=r=2$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\Delta^{\\top}\\mathbf{H}\\Delta\\right]\\leq\\mathbb{E}\\left[\\|\\mathbf{H}\\|_{2}\\right]\\cdot\\mathbb{E}\\left[\\|\\Delta\\|_{2}^{2}\\right]\\leq\\mathbb{E}\\left[\\|\\mathbf{H}\\|_{F}\\right]\\cdot\\mathbb{E}\\left[\\|\\Delta\\|_{2}^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Intuitively, since $\\|\\mathbf{H}\\|_{F}$ represents the sharpness/flatness of loss landscape, and $\\lVert\\Delta\\rVert_{2}$ describes the translation of landscape, the two terms can be assumed independent, where their covariance can be assumed as zero [8]. Consider the latter item in RHS of Eq. (22): ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\|\\Delta\\|_{2}^{2}\\right]=\\mathbb{E}\\left[\\Delta^{\\top}\\Delta\\right]}\\\\ &{\\quad\\quad\\quad=\\mathbb{E}\\left[\\left(\\mathbf{h}(t(\\mathbf{x}))-\\bar{\\mathbf{h}}\\right)^{\\top}\\mathbf{W}\\mathbf{W}^{\\top}\\left(\\mathbf{h}(t(\\mathbf{x}))-\\bar{\\mathbf{h}}\\right)\\right]}\\\\ &{\\quad\\quad\\quad=\\mathbb{E}\\left[\\mathrm{tr}\\left(\\mathbf{W}\\mathbf{W}^{\\top}\\left(\\mathbf{h}(t(\\mathbf{x}))-\\bar{\\mathbf{h}}\\right)\\left(\\mathbf{h}(t(\\mathbf{x}))-\\bar{\\mathbf{h}}\\right)^{\\top}\\right)\\right]}\\\\ &{\\quad\\quad\\quad=\\mathrm{tr}\\left(\\mathbf{W}\\mathbf{W}^{\\top}\\mathbb{E}\\left[\\left(\\mathbf{h}(t(\\mathbf{x}))-\\bar{\\mathbf{h}}\\right)\\left(\\mathbf{h}(t(\\mathbf{x}))-\\bar{\\mathbf{h}}\\right)^{\\top}\\right]\\right)}\\\\ &{\\quad\\quad\\quad=\\mathrm{tr}\\left(\\mathbf{W}\\mathbf{W}^{\\top}\\Sigma\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $\\Sigma$ is the covariance matrix of $\\mathbf{h}(t(\\mathbf{x}))$ . For a model trained well enough, the feature extractor can completely distinguish different features, i.e., $\\Sigma$ is a diagonal matrix. Then we can obtain ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathrm{tr}\\left(\\mathbf{W}\\mathbf{W}^{\\top}\\Sigma\\right)=\\mathrm{tr}\\left(\\sum_{i=1}^{c}\\mathbf{w}_{i}\\mathbf{w}_{i}^{\\top}\\Sigma\\right)=\\sum_{i=1}^{m}\\sum_{j=1}^{c}w_{j,i}^{2}\\mathrm{Var}\\left[h_{i}(t(\\mathbf{x}))\\right],\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $w_{j,i}$ is the $j$ -th entry of $\\mathbf{w}_{i}$ . If the variance of the feature $h_{i}(t({\\bf x}))$ is large, minimizing the empirical risk $\\hat{R}$ requires $\\begin{array}{r}{\\sum_{j=1}^{c}w_{j,i}^{2}\\to0\\implies w_{j,i}\\to0}\\end{array}$ for all $j\\in[c]$ . By Assumption 3.1, it is trivial that $\\mathrm{Var}[h_{a}(t({\\bf x}))]>\\bar{\\mathrm{Var}}[h_{p}(t({\\bf x}))]$ and therefore $w_{j,a}\\to0$ . Under this circumstance, the model is enforced to learn more phase information. The claim follows. ", "page_idx": 26}, {"type": "text", "text": "Original Benign Samples Amplitude Spectrum Phase Spectrum Generated Amplitude Spectrum Mixed Amplitude Spectrum Recombined Sample with Mixe Amplitude Recombined Sample only with Generated Amplitude ", "text_level": 1, "page_idx": 27}, {"type": "image", "img_path": "TeQvz5AlI8/tmp/73bb67e9381de888ef73aee9a89cfe90d355c24d5379d3a9f78cde2f884182fd.jpg", "img_caption": ["Figure 8: Visualization of recombined samples w/wo mix-up. "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "TeQvz5AlI8/tmp/12c285c950e418689ff300c998a5b7ad6e2e4545f7099069fe3f10fa9b2b06ab.jpg", "img_caption": ["Figure 9: Visualization of recombined samples based on the amplitude spectrum generated by different inputs of AAG. "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] Justification: See Abstract in the preamble and Introduction in Sec. 1. ", "page_idx": 28}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes]   \nJustification: See Appendix F.9. ", "page_idx": 28}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: See Sec. 3.4 and proof in Appendix G ", "page_idx": 28}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] Justification: See Sec. 4 ", "page_idx": 28}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] Justification: See https://github.com/Feng-peng-Li/DAT. ", "page_idx": 28}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: See Sec. 4.1. ", "page_idx": 28}, {"type": "text", "text": "7. Experiment Statistical Significance ", "page_idx": 28}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: See Secs. 4.2, 4.4, 4.3, and Appendix F.2. ", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: See Sec. 4.1 and Appendix F.3. ", "page_idx": 28}, {"type": "text", "text": "9. Code Of Ethics ", "page_idx": 28}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: following the Code Of Ethics of NeurIPS. ", "page_idx": 29}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: Since only open access models, codes, and datasets are used in this work, broader impacts are not applicable. ", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] Justification: No such risks. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: See the supplemental materials. ", "page_idx": 29}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] Justification: See Sec. 4.1. ", "page_idx": 29}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: Not involve crowdsourcing nor research with human subjects. ", "page_idx": 29}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] Justification: Not involve crowdsourcing nor research with human subjects. ", "page_idx": 29}]