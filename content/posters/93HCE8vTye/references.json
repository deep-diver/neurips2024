{"references": [{"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-01", "reason": "This paper introduced the Transformer architecture, which is the foundation of many modern LLMs and is the main subject of analysis in this paper."}, {"fullname_first_author": "Uri Alon", "paper_title": "On the bottleneck of graph neural networks and its practical implications", "publication_date": "2021-01-01", "reason": "This paper discusses the over-squashing phenomenon in graph neural networks, which is directly related to the representational collapse discussed in this paper."}, {"fullname_first_author": "Michael Hahn", "paper_title": "Theoretical limitations of self-attention in neural sequence models", "publication_date": "2020-07-01", "reason": "This paper explores theoretical limitations of self-attention mechanisms, which are crucial to understanding the limitations of Transformers."}, {"fullname_first_author": "Francesco Di Giovanni", "paper_title": "On over-squashing in message passing neural networks: The impact of width, depth, and topology", "publication_date": "2023-07-01", "reason": "This paper provides further insights into the over-squashing phenomenon, which is a key concept in the analysis of this paper."}, {"fullname_first_author": "Sepp Hochreiter", "paper_title": "Long short-term memory", "publication_date": "1997-08-01", "reason": "This paper introduced the LSTM architecture, which addresses the vanishing gradient problem that is related to the over-squashing phenomenon discussed in this paper."}]}