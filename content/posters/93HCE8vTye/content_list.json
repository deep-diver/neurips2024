[{"type": "text", "text": "Transformers need glasses! Information over-squashing in language tasks ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Andrea Banino Google DeepMind abanino@google.com ", "page_idx": 0}, {"type": "text", "text": "Steven Kapturowski Google DeepMind skapturowski@google.com ", "page_idx": 0}, {"type": "text", "text": "Dharshan Kumaran Google DeepMind dkumaran@google.com ", "page_idx": 0}, {"type": "text", "text": "Jo\u00e3o G.M. Ara\u00fajo Google DeepMind joaogui@google.com ", "page_idx": 0}, {"type": "text", "text": "Alex Vitvitskyi Google DeepMind avlife@google.com ", "page_idx": 0}, {"type": "text", "text": "Razvan Pascanu Google DeepMind razp@google.com ", "page_idx": 0}, {"type": "text", "text": "Petar Veli\u010dkovi\u0107 Google DeepMind petarv@google.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We study how information propagates in decoder-only Transformers, which are the architectural backbone of most existing frontier large language models (LLMs). We rely on a theoretical signal propagation analysis\u2014specifically, we analyse the representations of the last token in the final layer of the Transformer, as this is the representation used for next-token prediction. Our analysis reveals a representational collapse phenomenon: we prove that certain distinct sequences of inputs to the Transformer can yield arbitrarily close representations in the final token. This effect is exacerbated by the low-precision floating-point formats frequently used in modern LLMs. As a result, the model is provably unable to respond to these sequences in different ways\u2014leading to errors in, e.g., tasks involving counting or copying. Further, we show that decoder-only Transformer language models can lose sensitivity to specific tokens in the input, which relates to the well-known phenomenon of over-squashing in graph neural networks. We provide empirical evidence supporting our claims on contemporary LLMs. Our theory also points to simple solutions towards ameliorating these issues. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In recent years the field of Natural Language Processing (NLP) has been revolutionised through the introduction of Transformer-based architectures [30]. Large Transformers trained on some version of next-token prediction, known as Large Language Models (LLMs), have demonstrated impressive performance across different tasks, including conversational agents [10, 19], understanding multi-modal inputs [1], and code completion [16]. Most contemporary LLMs specifically focus on the decoder part of the original Transformer architecture, and are commonly referred to as decoder-only Transformers. Consequently, we focus primarily on such models in this paper. ", "page_idx": 0}, {"type": "text", "text": "However, despite the impressive performance of Transformers, recent works have uncovered surprising failures that may point to fundamental issues in their architecture. For instance, ", "page_idx": 0}, {"type": "image", "img_path": "93HCE8vTye/tmp/9f3771d8d6acaf1707dacceb789bc45e6c744756ee74cd5faca8ac5a4465bf4a.jpg", "img_caption": ["Figure 1: (a) Representational Collapse (Theorem 4.2). From top to bottom, we have a series of sequences given to Transformer architectures, each comprising repeated 1 tokens with a single 0 token at the end. The color and proximity of the curved lines illustrate how these representations converge as sequence length increases. (b) Over-squashing (Theorem 5.1). Due to the architecture of decoder-only Transformers, tokens that are earlier in their input sequence will have significantly more paths through which their data can reach the representation used for next-token prediction, leading to \u2018over-squashing\u2019. This effect is depicted here for an early token (blue) and later token (red) in a five-token sequence. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Transformer-based LLMs seem to be particularly challenged by seemingly simple tasks requiring counting [32] or copying elements along input sequences [17]. We find it important to study such failure cases, as these operations are fundamental building blocks of computation, and they are often necessary for solving reasoning tasks. A common strategy to assist LLMs in solving such tasks is to supply them with \u2018tools\u2019 [e.g. 25]. We argue that, while tool use will certainly help, it is still important to improve base model capabilities in this regard, because oftentimes, even producing accurate inputs to a tool may require complex reasoning operations. Specifically, often we need to copy some part of the Transformer\u2019s input into a tool\u2014if the base model struggles with robust copying, even this operation can be in peril. ", "page_idx": 1}, {"type": "text", "text": "Accordingly, we find it important to explain why decoder-only Transformers do not perform well when it comes to such problems\u2014not just as an intellectual endeavour, but also to help guide further practical improvements. While many works have studied the computational capabilities of Transformers [22, 18], they often make assumptions which do not correspond to present practical limitations, such as infinite floating-point precision or \u2018hard attention\u2019, making their conclusions less directly practically applicable. ", "page_idx": 1}, {"type": "text", "text": "In this work, we take a different approach and study what information can be contained in the representation of the last token at the last layer, as this is ultimately the information that will be used for next-token prediction \u2014 the fundamental mechanism through which modern Transformer LLMs perform training and inference. In particular, we show that for certain distinct sequences, their last-token representations can become arbitrarily close to each other. This leads to a representational collapse, exacerbated by the lower-precision floating point types typically used by modern LLM stacks. As a result Transformers incorrectly produce the same tokens on these sequence pairs \u2014 see Figure 1 (a). ", "page_idx": 1}, {"type": "text", "text": "Furthermore, we reveal that the computation graph employed by decoder-only Transformers, with its unidirectional causal mask, contributes to the observed representational collapse. This unidirectional flow of information, converging at the final token, is in fact likely to lead to a loss of information due to over-squashing, an effect that is well studied in graph neural networks (GNNs) [2, 29, 8, 3, 11], and related to vanishing gradients [14, 5, 13]. We hope that this result will be of independent interest to the GNN community, as a practical application of over-squashing results at scale. Finally, we provide supporting empirical evidence that these issues are likely of practical interest, and propose simple solutions\u2014directly stemming from our theoretical study\u2014to help alleviate them. ", "page_idx": 1}, {"type": "text", "text": "In summary, our paper provides the following contributions: ", "page_idx": 1}, {"type": "text", "text": "\u2022 Theoretical analysis of decoder-only Transformer limitations: we formalise the concepts of \u2018representational collapse\u2019 (Section 4) and \u2018over-squashing\u2019 (Section 5) in the context of Transformer-based architectures. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Impact of floating point precision: we explore how low floating-point precision exacerbate the identified theoretical issues, causing them to manifest even in relatively short input sequences.   \n\u2022 Empirical validation of theoretical analysis: our theoretical findings are supported by realworld experiments conducted on contemporary LLMs, demonstrating practical implications of the limitations we identified. ", "page_idx": 2}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this work, we study a class of Transformers which we believe forms the basis for a large number of current LLMs. We let $\\mathbf{Q},\\mathbf{K},\\mathbf{V}\\in\\mathbb{R}^{n\\times d}$ be the query, key, and value matrices respectively on $n$ tokens and $d$ dimensions. We denote with $\\mathbf{q}_{i},\\mathbf{k}_{i},\\mathbf{v}_{i}\\in\\mathbb{R}^{d}$ the $d$ -dimensional query, key, and value vectors of the $i$ -th token. We let $\\mathbf{p}_{i j}\\,\\in\\,\\mathbb{R}^{2e}$ be the $2e$ -dimensional positional encoding information between tokens $i$ and $j$ . We focus on the case in which the positional encodings are bounded, which is the case for the large majority of positional encodings used in practice [26, 30]. The Transformer model we consider computes the values, for a single head, of the $i$ -th token at the $\\ell_{}$ -th Transformer layer $\\mathbf{v}_{i}^{(\\ell)}$ as2 ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{z}_{i}^{(\\ell)}=\\displaystyle\\sum_{j\\leq i}\\alpha_{i j}^{(\\ell)}\\operatorname{norm}_{1}^{(\\ell)}\\left(\\mathbf{v}_{i}^{(\\ell)}\\right)+\\mathbf{v}_{i}^{(\\ell)},\\mathrm{with~}\\alpha_{i j}^{(\\ell)}=\\frac{\\exp\\left(k\\left(\\mathbf{q}_{i}^{(\\ell)},\\mathbf{k}_{j}^{(\\ell)},\\mathbf{p}_{i j}\\right)\\right)}{\\sum_{w\\leq i}\\exp\\left(k\\left(\\mathbf{q}_{i}^{(\\ell)},\\mathbf{k}_{w}^{(\\ell)},\\mathbf{p}_{i w}\\right)\\right)}}\\\\ &{\\mathbf{\\phi}_{i}^{(\\ell+1)}=\\psi^{(\\ell)}\\left(\\operatorname{norm}_{2}^{(\\ell)}\\left(\\mathbf{z}_{i}^{(\\ell)}\\right)\\right)+\\mathbf{z}_{i}^{(\\ell)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "for a function $k\\,:\\,\\mathbb{R}^{d}\\,\\times\\,\\mathbb{R}^{d}\\,\\times\\,\\mathbb{R}^{2e}\\,\\rightarrow\\,\\mathbb{R}$ mapping queries, key, and positional encoding information to a scalar value, an MLP $\\psi:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{d}$ , and normalization functions at the $\\ell$ -th layer $\\mathrm{norm}_{1}^{(\\ell)}$ and $\\mathrm{norm}_{2}^{(\\ell)}$ p2\u2113 q. This specific interleaving of components is often referred to as a Pre-LN Transformer [34]. We can view the output of the $\\ell$ -th layer of a Transformer as a sequence of d-dimensional vectors vp\u2113q \u201c pvp1\u2113 q, $\\mathbf v^{(\\ell)}=(\\mathbf v_{1}^{(\\ell)},\\dots,\\mathbf v_{n}^{(\\ell)})$ . Importantly, due to the causal attention mechanism, the vector ${\\bf v}_{j}^{(\\ell)}$ , will only depend on elements vip\u2113\u00b41qfor i \u010f j. We can group the attention weights into an attention matrix at the $\\ell$ -th layer which we define element-wise as $\\mathbf{A}_{i j}^{(\\ell)}\\,=\\,\\alpha_{i j}^{(\\ell)}$ . This is a row-stochastic triangular matrix that can also be interpreted as a probabilistic directed graph. After the last transformer block a normalization is applied to the token representations: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{y}_{i}=\\mathrm{norm_{3}}\\left(\\mathbf{v}_{i}^{(L)}\\right)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "We note that the next-token prediction usually depends purely on $\\mathbf{y}_{n}$ \u2014the final representation of the last token. ", "page_idx": 2}, {"type": "text", "text": "Existing theory on Transformers. The theoretical representational capacity of Transformers has become a popular area of study, providing interesting results on what classes of problems they are able to model. For instance, it has been pointed out that Transformers are not Turing-complete, but one can apply modifications which make Transformers Turingcomplete under certain assumptions [6]. Works have also shown that Transformers using \u2018hard attention\u2019 which replaces softmax with one-hot vectors alongside the use of infinite precision makes Transformers Turing-complete [22]. This contrasts with our work, which focuses on the more standard setting of Transformers using soft-attention and finite precision, and shows the limitations imposed by it. ", "page_idx": 2}, {"type": "text", "text": "Works have also tried to study transformers capabilities through the lense of formal languages, such as Weiss et al. [33], which develops a computational model of what transformers can represent in an analogous way to how Recurrent Neural Networks are associated with finite automata, and then derive an implementable programming language that represents that model. Following that, Del\u00e9tang et al. [7] place transformers within the Chomsky Hierarchy, showing that they are quite limited and cannot learn the decision problem for simple languages, which prompted authors to show that Transformer LLMs can perform substantially better if they generate a number of decoding tokens linear in the problem input size, through scratch-pad, Chain-of-Thought (CoT) or similar [18]. Finally, Peng et al. [21] show that the Transformer block with finite precision is fundamentally limited in its ability to represent compositional functions and solve simple problems that require it. Our work similarly analyses the Transformer\u2019s inability to solve simple computational tasks, and proves that even with techniques like Chain-of-Thought that inability persists as it is inherent to the combination of architecture, next-token prediction, and limited floating point precision. ", "page_idx": 2}, {"type": "image", "img_path": "93HCE8vTye/tmp/737516bef3c0dd7d3b6281e8dc54b68efc41081198525c506db0e090b74b9199.jpg", "img_caption": ["Figure 2: Results on simple copying tasks. (a). Gemini was prompted to predict the last token (diamond) of a sequences $\\mathrm{^{\\circ}1...10^{\\circ}}$ or the first token (square) of a sequence \u201801...1\u2019. (b). Same as (a) but with hints (see 3.2 for details) (c). Same as (a) but the sequences have interleaved 0s and 1s. See C.1 for extra details "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Decay in attention mechanisms. Works have also studied the limitations of self-attention by showing that it can reach pathological states that limit what transformers are able to learn. For instance, it has been show how a great reduction in the attention entropy can lead to unstable training if occurring early, but even when occurring later in training it can still lead to significantly lower performance [35]. Further, it has been shown that specific tokens can strongly concentrate attention, leading to transformers being unable to learn to process simple languages, like PARITY and DYCK [12]. Our work will similarly focus on showing how Transformers end-up effectively ignoring many tokens in their input which leads them to fail to solve simple computational problems, studying such a phenomenon by directly analysing the representational capacity. ", "page_idx": 3}, {"type": "text", "text": "Over-squashing. Graph neural networks (GNNs) are neural networks designed to operate over graph structures. Importantly, Transformers, may be seen as types of attention-based GNNs operating over specific types of graphs. The dififculties of propagating information over a graph have been thoroughly analysed, with a notable phenomenon being that of oversquashing [2, 29, 8, 3]. Over-squashing refers to the fact that propagating information over certain graphs that exhbit \u2018bottlenecks\u2019 is likely to induce a \u2018squashing\u2019 of information. This can be made more precise by studying this effect via the notion of a commute time [11] \u2014 the expected number of steps that a random walk takes to travel from a node to another node and back. Information travelling between nodes with higher commute time will be squashed more. ", "page_idx": 3}, {"type": "text", "text": "A common way to measure over-squashing is by looking at how sensitive the representation $\\mathbf{x}_{v}^{(L)}$ of a node $\\boldsymbol{v}$ after $L$ GNN layers is to the initial representation $\\mathbf{x}_{u}^{(0)}$ of another node $u$ . In particular, the partial derivative $\\hat{\\sigma}\\mathbf{x}_{v}^{(L)}/\\hat{\\sigma}\\mathbf{x}_{u}^{(0)}$ may be shown to decay, especially for nodes with high commute times between them. Our work may be seen as acting as a bridge between the well-studied phenomenon of over-squashing in GNNs and the loss of information we analyse in decoder-only Transformers specifically for language tasks. Note that this type of derivation is typical in the study of vanishing gradients for recurrent models as well [14, 5, 20, 13]. ", "page_idx": 3}, {"type": "text", "text": "3 Motivating Examples ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "This section presents a series of experiments focused on copying and counting tasks. These experiments reveal surprising failure cases in modern decoder-only Transformer architectures, providing concrete evidence that motivates the theoretical analysis presented in the following sections. ", "page_idx": 3}, {"type": "image", "img_path": "93HCE8vTye/tmp/f9828dd2937a2fcf290424991df6a46c8e1355cbd09e3035f5031a36038f46d3.jpg", "img_caption": ["Figure 3: Gemini 1.5 being prompted to sum $1+\\cdot\\cdot\\cdot+1$ (Column 1), Count the number of ones in a sequence of 1s (Column 2), Count the number of ones in a sequence of ones and zeroes (the sequence is a Bernoulli sequence with probability of sampling a one being 0.7) (Column 3), and to counter the number of times a word appears in a sentence (Column 4). "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "We start by providing motivating examples that show surprisingly simple failure cases of frontier LLMs specifically on copying (Section 3.1) and counting (Section 3.2) tasks. By copying we specifically mean tasks that involve the \u2018copying\u2019 or \u2018recalling\u2019 of a single or multiple tokens from the prompt. Instead, by counting, we mean the task of counting how many times a specific token appears in a sequence. We focus our evaluation on Gemini 1.5 [10] as our frontier LLM (referred as Gemini) and later analyse the internal representations of the open-sourced Gemma model [27]. The goal is to showcase intriguing failure cases which will motivate our signal propagation analysis. ", "page_idx": 4}, {"type": "text", "text": "3.1 Copying ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this Section, we present surprising results on simple copying tasks. In particular, we focus on tasks that involve the copying of a single token \u2014 i.e. what is the token occurring at a particular position? The copy of a single token is in principle the most straightforward type of copying task, but still requires the LLM to accurately identify the token based on a prompt and to then propagate its information correctly. ", "page_idx": 4}, {"type": "text", "text": "Importantly, we study cases in which the LLM is prompted to copy tokens either at the start or at the end of a sequence. We avoid tasks that involve the copy of tokens at the \u2018 $n$ -th\u2019 position as most frontier LLMs do not have absolute positional information, making it very challenging for them to solve tasks that require absolute position. We focus on tasks that involve sequences of \u2018zeros\u2019 and \u2018ones\u2019 growing in length with specific patterns. ", "page_idx": 4}, {"type": "text", "text": "In Figure 2 (a), we prompt Gemini to copy the last element of a sequence $\\mathbf{\\nabla}^{*}1\\dots10^{\"}$ or the first element of a sequence $ '01\\dots1$ \u2019. The answer for both is zero, but we progressively grow the number of ones. We observe how the task seems considerably easier when asked to return the first rather than the last element. Surprisingly, already at a sequence length of only 300 elements, Gemini incorrectly starts to output \u2018one\u2019 when trying to copy the last element. In Figure 2 (b), we show that providing hints in the form of: \u201c $^*$ Hint\\* It\u2019s not necessarily a 1, check carefully\u201d, helps significantly with the performance. Finally, in Figure 2 (c), we show that replacing the constant sequence of ones with alternating ones and zeros seems to also help. We refer to the Appendix (Section C.1) for further details on the experiments. ", "page_idx": 4}, {"type": "text", "text": "These three motivating experiments seem to point towards a type of vanishing of information, caused by the growing number of ones dominating the sequence. Interestingly, such a vanishing of information effect (a) seems to depend on the position in the sequence, (b) seems to be affected by the prompting, and (c) by the items that make up the sequence. We will later argue how all three of such observations can explained by our theoretical analysis. ", "page_idx": 4}, {"type": "image", "img_path": "93HCE8vTye/tmp/9254c2544381708b2e05ae874069f7abdfad86b6fddc0f38e4dcf174fd176c4f.jpg", "img_caption": ["Figure 4: Frequency of different outputted values for Gemini 1.5 for the counting tasks. The large density at 100 suggests that Gemini is likely not counting, but instead possibly performing some crude form of subitising. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "3.2 Counting ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We now turn our attention to counting problems, i.e. tasks of the form \u2014 given a specific sequence, how many times does a particular token appear? Such problems are related to copying in the sense that they also require careful consideration of individual tokens in the sequence as ignoring even a single token may potentially lead to an incorrect output. ", "page_idx": 5}, {"type": "text", "text": "We consider four different tasks: (i) Summing $1+\\cdot\\cdot\\cdot+1$ , (ii) Counting the number of ones in a sequence of ones, (iii) Counting the number of ones in a sequence of ones and zeros, with ones being sampled with 70% probability, and (iv) Counting the number of times a specific word appears in a sentence. We consider predictions of an LLM which (1) Is instructed to only output the answer, (2) Is prompted to break down the problem (CoT-no-shot), and (3) Is prompted to break down the problem with few-shot in-context examples (CoT-few-shot). We refer to the Appendix (Section C.1) for a more detailed description of the tasks. ", "page_idx": 5}, {"type": "text", "text": "Results are presented in Figure 3. It is clear that the performance rapidly deteriorates with the sequence length. It is also interesting to see that the error seems to increase with the sequence very rapidly. For instance in task (i), the LLM is quite likely to predict the value of \u2018100\u2019 once the sequence reaches a size around or larger than 100. Such an observation provides motivating evidence for the argument that Transformers may not be in fact mechanically counting but rather perform a type of crude subitising. This explains why arguably \u2018common\u2019 numbers such as 100 are much more likely to be outputted by the LLM and why in tasks such as (i) and (ii) the values near 100 have relatively lower error. This does not happen in task (iii) as the response should actually be around 70% of the sequence length due to the sequence sampling procedure, explaining why the absolute error actually seems to increase around a sequence length of 100. Figure 4 further showcases this issue, more clearly showing how 100 is by far the most common response. ", "page_idx": 5}, {"type": "text", "text": "4 Representational Collapse ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We start our theoretical analysis by showcasing a type of loss of information which we call representational collapse. More precisely, we show that under certain conditions, we can find distinct sequences such that their final representations of the last token at the last layer become arbitrarily close as the sequence length increases. As Transformer models operate over finite machine precision, this points to a fundamental representational incapacity of Transformers to distinguish certain prompts if the sequence is long enough. ", "page_idx": 5}, {"type": "text", "text": "The key intuition is that if two sequences are similar everywhere except at the last token, as the sequences get larger, their final representations will become closer and closer until they reach a critical point which is below floating point precision. In other words, solving certain tasks would require infinite floating point precision. We will later show how this phenomenon is not only theoretical, but also occurs in practice on sequences of reasonable length. In the Appendix (Section 4), we relate representational collapse to the $L_{1}$ distance \u2013 or total variation \u2013 between the softmax distributions of the two sequences. We start by presenting a result that shows that the $L_{1}$ difference tends to 0 as the sequence length grows, under some assumption on the sequences. We point to the Appendix (Lemma B.2) for the complete statement. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Lemma 4.1 (Informal). Consider two sequences $\\mathbf{x}$ , $\\mathbf{x}^{\\ast}\\in\\mathbb{R}^{n}$ such that $\\begin{array}{r}{\\operatorname*{lim}_{n\\to\\infty}|\\mathbf{x}_{n}-\\mathbf{x}_{n}^{*}|=0}\\end{array}$ .   \nThen, the $L_{\\mathrm{1}}$ difference of their softmax tends to 0. ", "page_idx": 6}, {"type": "text", "text": "We now show, using Lemma 4.1, that we can find distinct sequences that will have arbitrarily close final representations. In particular, as language models often operate in low floating regimes, i.e. bf16, this can practically become catastrophic. The result is summarised in Theorem 4.2, which describes what we call representational collapse in this work. The complete statement is reported in the Appendix (Theorem B.3). ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.2 (Representational Collapse \u2013 informal). Let $\\mathbf{v}^{(0)}\\in\\mathbb{R}^{n\\times d}$ be a sequence and $\\mathbf{v}^{*(0)}\\ \\in\\ \\mathbb{R}^{(n+1)\\times d}$ be another sequence equal to $\\mathbf{v}^{(0)}$ with the last token of $\\mathbf{v}^{(0)}$ repeated. Assume that the positional encoding information decays to 0 with the distance. Then, their representations become arbitrarily close as n increases. ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.2 shows that it becomes increasingly challenging for a Transformer to distinguish two sequences that only differ via a repeated last token. We note that the repetition of the last token is a technical consideration to show this direct representational collapse. As we will later show in Section 5.1, it is particularly problematic in general to depend on the last token due to a type of topological \u2018squashing\u2019 present in decoder-only Transformers. ", "page_idx": 6}, {"type": "text", "text": "Measuring representational collapse. We report experiments showcasing representational collapse by measuring the internal representations of Gemma 7B [27]. For two sequences $\\mathbf{v}^{(0)}$ and $\\mathbf{v}^{*(0)}$ we report their difference in representation at the last layer $\\left\\|\\mathbf{v}^{(L)}-\\mathbf{v}^{*(L)}\\right\\|_{\\infty}$ averaged out over each head, alongside the minimum and maximum over each head. Figure 5 shows the collapse occuring on (a) prompting the model to count the number of ones in a sequence of ones, with one having an additional one, and (b) prompting the model to count the number of ones for a sequences with digits sampled uniformly ending with either a single one or two ones. The repeated digits seem to make the collapse occur much sooner with a sequence length of around 50 being near machine precision, while varying the digits seems to delay such a collapse, but a downward trend is maintained with respect to the sequence length. ", "page_idx": 6}, {"type": "text", "text": "Quantisation and Tokenisation. A common technique used to speedup the inference of an LLM is that of quantisation, a process that constructs an approximate version of an LLM that operates over lower precision datatypes. This helps drastically improve the inference speed of LLMs as modern accelerators produce significantly more FLOPs over lower precision datatypes. Of course quantisation usually comes at a cost. Our theoretical analysis points towards a potentially catastrophic loss in representation due to quantisation. In particular, a lower machine precision will mean that the convergence of representations in Theorem 4.2 will occur much sooner, and consequently the LLM will not be able to distinguish even shorter sequences. ", "page_idx": 6}, {"type": "text", "text": "In practice, the direct application of theoretical results is made more complicated due to tokenisation. In particular, a sequence of repeated tokens \u201811111\u2019 for instance may not be necessarily tokenised into 5 distinct \u20181\u2019 tokens. In principle, this should help alleviate the direct collapse of the representations. Tokenisation in general makes it more of a challenge to study such phenomena as it adds an additional layer of complexity to the experimental analysis. In our experiments, we took tokenisation into consideration and attempted to mitigate its effects. ", "page_idx": 6}, {"type": "text", "text": "A simple solution to representational collapse. An important consequence of Theorem 4.2 is that it is challenging for a Transformer to deal with a long sequence of repeated tokens. A practical solution is to this issue is to introduce additional tokens throughout the sequence to help keep the representations distant. We provide direct evidence of this in Figure 5 (c,d), where we prompt the model on a simple copying task of a long string of ones. While the representations collapse for the sequence of ones (c), adding commas every third digit (d) helps to keep the representations well-separated. ", "page_idx": 6}, {"type": "image", "img_path": "93HCE8vTye/tmp/6e09df65eb4649ea84183b61e59823c7eb5b4044af34bab07e71fc0c7e56c8e4.jpg", "img_caption": ["Figure 5: Representational collapse for counting (a, b) and copying (c, d) tasks. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "5 Over-squashing in Language Tasks ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this Section, we study a more general phenomenon related to representational collapse\u2014 over-squashing. In particular, we are interested in analysing how information from the input sequence affects the information contained within the representation of the last token in the final layer\u2014the representation ultimately used for next-token prediction. For this reason, we study the quantity ${\\hat{\\sigma}}\\mathbf{y}_{n}/\\partial\\mathbf{v}_{i}^{(0)}$ which measures how sensitive is the final token to an input token at position $i$ . ", "page_idx": 7}, {"type": "text", "text": "In graph neural network theory, the decay of such a partial derivative is often associated with the \u2018squashing\u2019 of information, leading to the phenomenon of over-squashing [29, 8, 11], a problem related to the well-known vanishing gradients problem in RNNs [14, 5, 11]. The over-squashing analysis we carry out in this work is particularly challenging due to the flexible nature of the attention mechanism and the many components that are part of decoder-only Transformers. Consequently, we make two simplifying assumptions in our analysis: (i) We summarise the effect of layer normalisation via a constant $\\beta_{i}$ for the $i$ -th layer norm component, and (ii) the attention weights are treated as independent of the input. Such simplifications are not strictly necessary for our analysis, but they greatly simplify the resulting bound we derive and do not detract from the two key takeaways: (1) the sensitivity to an input token depends on its position in the sequence and (2) the sensitivity to an input token depends on the attention weights. The result is summarised in Theorem 5.1. The full statement is reported in the Appendix (Theorem B.5). ", "page_idx": 7}, {"type": "text", "text": "Theorem 5.1 (Over-squashing in Transformers). Consider an input sequence $\\mathbf{v}_{1}^{(0)},\\ldots,\\mathbf{v}_{n}^{(0)}$ Let C \u0105 0 be some constant and \u03b1\u00afip,\u2113jq \u201c\u03b1\u03b2i,2j $\\begin{array}{r}{\\bar{\\alpha}_{i,j}^{(\\ell)}=\\frac{\\alpha_{i,j}^{(\\ell)}}{\\beta_{2}}+\\delta_{i,j}}\\end{array}$ , then: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\left\\|\\frac{\\partial\\mathbf{y}_{n}}{\\partial\\mathbf{v}_{i}^{(0)}}\\right\\|\\leqslant C\\sum_{k_{1}\\geqslant i}\\cdot\\cdot\\cdot\\sum_{k_{L}\\geqslant k_{L-1}}\\bar{\\alpha}_{n,k_{L}}^{(L-1)}\\prod_{\\ell=2}^{L-1}\\bar{\\alpha}_{k_{\\ell},k_{\\ell-1}}^{(\\ell-1)}\\bar{\\alpha}_{k_{1},i}^{(0)}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Theorem 5.1 provides intuition on how information propagates in a decoder-only Transformer. In particular, there is a topological aspect present in the bound which is directly controlled by the attention mechanism. More concretely, the sensitivity depends on the sum of the weighted paths between the token $i$ at the input and the final layer. In other words, for tokens coming sooner in the sequence, there will be more opportunity for their information to be preserved. This is clear for instance for the last token, which will only be preserved by attention mechanism if the attention $n\\to n$ is large at every layer $L$ , i.e. there is only one path. The paths instead grow very quickly for tokens coming sooner in the sequence. A related observation, in terms of path counting, was also made for deep RNNs [13]. We note that such a bound explains the better performance when copying elements at the start of the sequence in Figure 2 (a), why hints help in Figure 2 (b), and why repeating the final elements within the sequence also helps in Figure 2 (c). ", "page_idx": 8}, {"type": "text", "text": "This analysis leads to an interesting limiting case described in Proposition 5.2, that shows a type of exponential vanishing that can occur in some degenerate cases in which $\\scriptstyle{\\mathbf{y}}_{n}$ depends only on the starting input token $\\mathbf{v}_{1}^{(0)}$ . Fortunately, there are many mechanisms which prevent this from happening, but believe this to be an interesting limiting case which is a direct consequence of the topology of the causal attention mechanism. Further, it provides an interesting connection between the spectral theory of directed graphs and causal attention mechanisms. We report the formal statement in the Appendix (Proposition B.8). ", "page_idx": 8}, {"type": "text", "text": "Proposition 5.2 (Informal). Under certain assumptions on the effect of the normalisation and on the attention weights, in the limit of layers $L\\to\\infty$ the output representation will only depend on the first input token. ", "page_idx": 8}, {"type": "text", "text": "U-shape effect. Theorem 5.1 in part also helps to explain the empirically observed $U$ - shape effect\u2014the observation that LLMs seem to perform better at retrieval tasks when the information to be retrieved is located either near the start or the end of the sequence. In fact, due to the topology of the causal mechanism, we find from Theorem 5.1 that tokens at the start of the sequence have more opportunity for the information to be maintained at the end. The final tokens being also easier instead can be explained from the recency bias that is learnt by the attention mechanism during training. In auto-regressive next-token prediction, it is in fact reasonable to assume that tokens that are closer to the end will be more important and this is likely a bias that is learnt during training by the LLM. ", "page_idx": 8}, {"type": "text", "text": "6 Counting ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We finally highlight another representational problem that arises specifically in counting problems. Our analysis points to a fundamental dififculty that emerges from the normalisation of the softmax. In particular, the normalisation of the softmax makes it hard for a model to take into account the length of a sequence. This is exacerbated by the fact that positional encodings are often normalised and thus relative, meaning that they also do not hold absolute positional information. Intuitively, counting is a problem that requires some notion of \u2018unboundedness\u2019 of the representations, whilst the normalisations used inside a Transformer work against this. ", "page_idx": 8}, {"type": "text", "text": "We start by showing that without causal masking and positional embeddings, a Transformer is immediately unable to count the number of tokens in a sequence, highlighting a pathological issue which stems directly from the softmax normalisation. We note that similar issues have been already pointed out [e.g. 22]. We show the result in Proposition 6.1 and report the full statement in the Appendix (Proposition B.9). ", "page_idx": 8}, {"type": "text", "text": "Proposition 6.1. A Transformer without positional encodings and a causal attention mechanism is immediately unable to count. ", "page_idx": 8}, {"type": "text", "text": "While causal mechanisms and positional encodings help to break such representational issues, they break the permutation invariance of the Transformer, meaning that the representations will be heavily miss-aligned with the task, something which has been shown to hinder performance [9]. As permutations grow factorially with sequence length, this makes it practically very challenging for a decoder-only Transformer to learn such a property simply from the data. This explains the extreme incapacity of counting highlighted in Section ", "page_idx": 8}, {"type": "text", "text": "3. Further, as a corollary of Theorem 4.2, we have that even if a model would be able to generalise perfectly, the problem of representational collapse points to an impossibility result in counting regardless. The result is summarised in Corollary 6.2, with the full statement in the Appendix (Corollary B.10). ", "page_idx": 9}, {"type": "text", "text": "Corollary 6.2 (Informal). Counting in certain situations becomes impossible due to representational collapse and finite floating point precision. ", "page_idx": 9}, {"type": "text", "text": "Corollary 6.2 shows how our main result on representational collapse points to practical issues when it comes to certain styles of prompts. When paired with low floating point arithmetic precision, representation collapse becomes problematic. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion and Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we first presented surprising failure cases of LLMs on simple copying and counting tasks. We then discussed how such failure cases can be explained by studying what can be contained inside the representation ${\\bf y}_{n}$ and in particular how information may be lost. This lead to the unvealing of two phenomena : representational collapse and over-squashing. We showed how we can measure these phenomena in practice and proposed simple solutions to help alleviate such information loss. ", "page_idx": 9}, {"type": "text", "text": "We believe that this work uncovers an interesting framework which can be used to study failure cases of Transformers and LLMs more generally. We believe that our analysis could be extended in many practical different directions, for instance by understanding how to directly measure over-squashing or how to best use this newly-found understanding to improve current Transformer models. In our work, we focused on pointing out informationpropagation issues in Transformer-based architectures, but we hope that the findings may help better understand and improve language models available today. ", "page_idx": 9}, {"type": "text", "text": "8 Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We would like to thank Wojciech Marian Czarnecki (Google DeepMind), Simon Osindero (Google DeepMind), and Timothy Nguyen (Google DeepMind) for the valuable comments and suggestions regarding this work. We also thank Constantin Kogler (University of Oxford) for providing useful insights on the theoretical aspects of the work. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in neural information processing systems, 35:23716\u201323736, 2022. [2] Uri Alon and Eran Yahav. On the bottleneck of graph neural networks and its practical implications. In International Conference on Learning Representations, 2021.   \n[3] Federico Barbero, Ameya Velingker, Amin Saberi, Michael M. Bronstein, and Francesco Di Giovanni. Locality-aware graph rewiring in GNNs. In The Twelfth International Conference on Learning Representations, 2024. [4] Federico Barbero, Alex Vitvitskyi, Christos Perivolaropoulos, Razvan Pascanu, and Petar Veli\u010dkovi\u0107. Round and round we go! what makes rotary positional encodings useful? arXiv preprint arXiv:2410.06205, 2024. [5] Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with gradient descent is dififcult. IEEE Transactions on Neural Networks, 5(2):157\u2013166, 1994.   \n[6] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and \u0141ukasz Kaiser. Universal transformers, 2019.   \n[7] Gr\u00e9goire Del\u00e9tang, Anian Ruoss, Jordi Grau-Moya, Tim Genewein, Li Kevin Wenliang, Elliot Catt, Chris Cundy, Marcus Hutter, Shane Legg, Joel Veness, and Pedro A. Ortega. Neural networks and the chomsky hierarchy, 2023.   \n[8] Francesco Di Giovanni, Lorenzo Giusti, Federico Barbero, Giulia Luise, Pietro Lio, and Michael M Bronstein. On over-squashing in message passing neural networks: The impact of width, depth, and topology. In International Conference on Machine Learning, pages 7865\u20137885. PMLR, 2023.   \n[9] Andrew Joseph Dudzik, Tamara von Glehn, Razvan Pascanu, and Petar Veli\u010dkovi\u0107. Asynchronous algorithmic alignment with cocycles. In Learning on Graphs Conference, pages 3\u20131. PMLR, 2024.   \n[10] Team Gemini. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.   \n[11] Francesco Di Giovanni, T. Konstantin Rusch, Michael Bronstein, Andreea Deac, Marc Lackenby, Siddhartha Mishra, and Petar Veli\u010dkovi\u0107. How does over-squashing affect the power of GNNs? Transactions on Machine Learning Research, 2024.   \n[12] Michael Hahn. Theoretical limitations of self-attention in neural sequence models. Transactions of the Association for Computational Linguistics, 8:156\u2013171, 2020.   \n[13] Luca Herranz-Celotti and Jean Rouat. Stabilizing rnn gradients through pre-training, 2024.   \n[14] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735\u20131780, 1997.   \n[15] Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Payel Das, and Siva Reddy. The impact of positional encoding on length generalization in transformers. Advances in Neural Information Processing Systems, 36, 2024.   \n[16] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R\u00e9mi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation with alphacode. Science, 378(6624):1092\u20131097, 2022.   \n[17] Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157\u2013173, 2024.   \n[18] William Merrill and Ashish Sabharwal. The expresssive power of transformers with chain of thought. arXiv preprint arXiv:2310.07923, 2023.   \n[19] R OpenAI. Gpt-4 technical report. arxiv 2303.08774. View in Article, 2(5), 2023.   \n[20] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the dififculty of training recurrent neural networks. In Sanjoy Dasgupta and David McAllester, editors, Proceedings of the 30th International Conference on Machine Learning, volume 28 of Proceedings of Machine Learning Research, pages 1310\u20131318, Atlanta, Georgia, USA, 17\u201319 Jun 2013. PMLR.   \n[21] Binghui Peng, Srini Narayanan, and Christos Papadimitriou. On limitations of the transformer architecture, 2024.   \n[22] Jorge P\u00e9rez, Pablo Barcel\u00f3, and Javier Marinkovic. Attention is turing-complete. Journal of Machine Learning Research, 22(75):1\u201335, 2021.   \n[23] S Unnikrishna Pillai, Torsten Suel, and Seunghun Cha. The perron-frobenius theorem: some of its applications. IEEE Signal Processing Magazine, 22(2):62\u201375, 2005.   \n[24] Ofir Press, Noah A Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. arXiv preprint arXiv:2108.12409, 2021.   \n[25] Timo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36, 2024.   \n[26] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024.   \n[27] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivi\u00e8re, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024.   \n[28] Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, L\u00e9onard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ram\u00e9, et al. Gemma 2: Improving open language models at a practical size. arXiv preprint arXiv:2408.00118, 2024.   \n[29] Jake Topping, Francesco Di Giovanni, Benjamin Paul Chamberlain, Xiaowen Dong, and Michael M Bronstein. Understanding over-squashing and bottlenecks on graphs via curvature. 2022.   \n[30] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \n[31] Petar Veli\u010dkovi\u0107, Christos Perivolaropoulos, Federico Barbero, and Razvan Pascanu. softmax is not enough (for sharp out-of-distribution). arXiv preprint arXiv:2410.01104, 2024.   \n[32] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022.   \n[33] Gail Weiss, Yoav Goldberg, and Eran Yahav. Thinking like transformers, 2021.   \n[34] Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu. On layer normalization in the transformer architecture. In International Conference on Machine Learning, pages 10524\u201310533. PMLR, 2020. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "[35] Shuangfei Zhai, Tatiana Likhomanenko, Etai Littwin, Dan Busbridge, Jason Ramapuram, Yizhe Zhang, Jiatao Gu, and Joshua M Susskind. Stabilizing transformer training by preventing attention entropy collapse. In International Conference on Machine Learning, pages 40770\u201340803. PMLR, 2023. ", "page_idx": 12}, {"type": "text", "text": "A Broader impact ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. In particular the limitations highlighted in the work can pose some issue in terms of reliability of LLMs, so highlighting these issues can help fixing them. ", "page_idx": 13}, {"type": "text", "text": "B Proofs ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We provide formal statements and proofs for the results shown in the main text. We follow the order in which they are presented in the main text. In Section B.1, we present the proofs on representational collapse (Section 4), in Section B.2 the proofs on over-squashing (Section 5) over-squashing, and finally in Section B.3 the proofs on counting (Section 6). ", "page_idx": 13}, {"type": "text", "text": "B.1 Representational Collapse ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We start by showing that adding a new element to a sequence results in the softmax value of a specific token to decrease. In particular, we consider the case in which the tokens are bounded and show that we can use this to construct an upper bound on the softmax value for any token. ", "page_idx": 13}, {"type": "text", "text": "Lemma B.1. Consider a vector $\\mathbf{a}\\in\\mathbb{R}^{n-1}$ and two scalars $b,c\\in\\mathbb{R}$ . $\\mathsf{Z}e t\\;\\mathbf{x}=[\\mathbf{a}\\ c]^{\\boldsymbol{I}}\\in\\mathbb{R}^{n}$ and $\\mathbf{x}^{*}=[\\mathbf{a}\\ b\\ c]^{T}\\in\\mathbb{R}^{n+1}$ with all entries bounded. Then, softmax $(\\mathbf{x})_{n}>$ softmax $(\\mathbf{x}^{*})_{n+1}$ . Moreover for any $p~>~0$ we can find large enough $\\textit{n}\\in\\ \\mathbb{N}^{+}$ such that | softmax $\\mathbf{\\Psi}(\\mathbf{x})_{n}\\mathbf{\\Psi}^{-}$ softmax px\u02daqn\\`1 | < p. ", "page_idx": 13}, {"type": "text", "text": "Proof. We directly compute: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{softmax}(\\mathbf{x})_{n}=\\frac{\\exp(c)}{\\sum_{k=1}^{n-1}\\exp(\\mathbf{a}_{k})+\\exp(c)}\\qquad\\qquad}\\\\ {\\mathrm{softmax}(\\mathbf{x}^{*})_{n+1}=\\frac{\\exp(c)}{\\sum_{k=1}^{n-1}\\exp(\\mathbf{a}_{k})+\\exp(b)+\\exp(c)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "As we assume that the entries are bounded, we have that $\\begin{array}{r}{\\sum_{j=1}^{n-1}\\exp({\\mathbf a}_{j})\\;+\\;\\exp(c)\\ <}\\end{array}$ $\\begin{array}{r}{\\sum_{j=1}^{n-1}\\exp(\\mathbf{a}_{j})+\\exp(b)+\\exp(c)}\\end{array}$ , therefore softmax $(\\mathbf{x})_{n}>$ softm ax $(\\mathbf{x}^{*})_{n+1}$ . ", "page_idx": 13}, {"type": "text", "text": "For the second part of the statement, we compute: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{softmax}(\\mathbf{x})_{n}-\\mathrm{softmax}(\\mathbf{x}^{*})_{n+1}|=\\left|\\frac{\\exp(c)}{\\sum_{k=1}^{n-1}\\exp(\\mathbf{a}_{k})+\\exp(c)}-\\frac{\\exp(c)}{\\sum_{k=1}^{n-1}\\exp(\\mathbf{a}_{k})+\\exp(b)+\\exp(c)}\\right|}\\\\ {\\leqslant\\left|\\frac{\\exp(c)}{\\sum_{k=1}^{n-1}\\exp(\\mathbf{a}_{k})+\\exp(c)}\\right|+\\left|\\frac{\\exp(c)}{\\sum_{k=1}^{n-1}\\exp(\\mathbf{a}_{k})+\\exp(b)+\\exp(c)}\\right|}\\\\ {<p}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "for some $p~>~0$ , as in the last step the summands tend to $\\boldsymbol{0}$ as $n~\\rightarrow~\\infty$ . We therefore have that $|\\mathrm{softmax}(\\mathbf{x})_{n}-\\mathrm{softmax}(\\mathbf{x}^{*})_{n+1}|\\ \\rightarrow\\ 0$ as $n~\\rightarrow~\\infty$ and for large enough $n$ $|\\mathrm{softmax}(\\mathbf{x})_{n}-\\mathrm{softmax}(\\mathbf{x}^{*})_{n+1}|<p$ for any $p>0$ due to the previous statement. $\\boxed{\\begin{array}{r l}\\end{array}}$ ", "page_idx": 13}, {"type": "text", "text": "Total variation. We now study a quantity known as the total variation between two distributions of interest. Given two categorical distributions $\\mu,\\nu$ supported on the same space, we define their total variation $\\delta(\\mu,\\nu)$ \u2014 or equivalently $L_{1}$ norm $\\|\\mu-\\nu\\|_{1}$ , as: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\delta(\\mu,\\nu)=\\sum_{x}\\left|\\mu(x)-\\nu(x)\\right|.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The total variation is a distance between probabilty distributions. We note that oftentimes the quantity above is strictly called the $L_{1}$ norm, while $1/2$ of such a quantity the total variation. For the scope of our work, the factor of $1/2$ is not important so we ignore it and use the two terms synonymously. Interestingly, the total variation is intimately related to the KL-divergence, pointing towards potential connections to information theory. We leave such a connection to future work. ", "page_idx": 14}, {"type": "text", "text": "We now study how the total variation between two softmax distributions behaves in the limit of the sequence length. In particular, we show that if the positional encoding information decays to 0 with the sequence length, then the total variation between the two sequences goes to 0 with $n$ . Such a result is presented in Lemma B.2 ", "page_idx": 14}, {"type": "text", "text": "Lemma B.2. Consider two sequences $\\mathbf{x},\\mathbf{x}^{*}\\in\\mathbb{R}^{n}$ such that $\\begin{array}{r}{\\operatorname*{lim}_{n\\to\\infty}|\\mathbf{x}_{n}-\\mathbf{x}_{n}^{*}|\\,=\\,0}\\end{array}$ , with $\\mathbf{x}_{i},\\mathbf{x}_{i}^{*}$ bounded. Let y, $\\b{\\upgamma}^{*}\\in\\mathbb{R}^{n}$ be the softmax of $\\mathbf{x}$ and $\\mathbf{x}^{*}$ respectively. Then, as $n\\to\\infty$ the total variation tends to 0, i.e. $\\begin{array}{r}{\\operatorname*{lim}_{n\\to\\infty}\\delta(\\mathbf{y},\\mathbf{y}^{*})=0}\\end{array}$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. Let $\\begin{array}{r}{Z\\;=\\;\\sum_{i=1}^{n}e_{i}^{\\mathbf{x}}}\\end{array}$ and $\\begin{array}{r}{Z^{*}\\;=\\;\\sum_{i=1}^{n}e^{\\mathbf{x}_{i}^{*}}}\\end{array}$ be the partition functions for $\\mathbf{x}$ and $\\mathbf{x}^{*}$ , respectively. We  s\u0159tart by bounding th e\u0159 quantity $|Z-Z^{*}|$ . In particular, let $\\epsilon>0$ , we first claim: ", "page_idx": 14}, {"type": "equation", "text": "$$\n|Z-Z^{*}|\\leqslant\\sum_{i=1}^{n}\\Big|e^{\\mathbf{x}_{i}}-e^{\\mathbf{x}_{i}^{*}}\\Big|\\leqslant\\epsilon\\operatorname*{min}(Z,Z^{*}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Consider some $n_{0}\\geqslant1$ such that $\\left|1-e^{\\mathbf{x}_{i}^{*}-\\mathbf{x}_{i}}\\right|\\leqslant\\epsilon/2$ . We note that this always possible as $|\\mathbf{x}_{i}^{*}-\\mathbf{x}_{i}|\\rightarrow0$ by assumption. We compute: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{i=1}^{n}\\left|e^{\\mathbf{x}_{i}}-e^{\\mathbf{x}_{i}^{*}}\\right|=\\displaystyle\\sum_{i=1}^{n_{0}-1}\\left|e^{\\mathbf{x}_{i}}-e^{\\mathbf{x}_{i}^{*}}\\right|+\\displaystyle\\sum_{i=n_{0}}^{n}\\left|e^{\\mathbf{x}_{i}}-e^{\\mathbf{x}_{i}^{*}}\\right|}\\\\ {\\displaystyle=\\displaystyle\\sum_{i=1}^{n_{0}-1}\\left|e^{\\mathbf{x}_{i}}-e^{\\mathbf{x}_{i}^{*}}\\right|+\\displaystyle\\sum_{i=n_{0}}^{n}e^{\\mathbf{x}_{i}}\\left|1-e^{\\mathbf{x}_{i}^{*}-\\mathbf{x}_{i}}\\right|}\\\\ {\\displaystyle\\leqslant\\displaystyle\\sum_{i=1}^{n_{0}-1}\\left|e^{\\mathbf{x}_{i}}-e^{\\mathbf{x}_{i}^{*}}\\right|+\\displaystyle\\frac{\\epsilon}{2}\\displaystyle\\sum_{i=n_{0}}^{n}e^{\\mathbf{x}_{i}}}\\\\ {\\displaystyle\\leqslant\\displaystyle\\sum_{i=1}^{n_{0}-1}\\left|e^{\\mathbf{x}_{i}}-e^{\\mathbf{x}_{i}^{*}}\\right|+\\displaystyle\\frac{\\epsilon}{2}Z}\\\\ {\\displaystyle\\leqslant\\epsilon\\displaystyle\\sum_{i=1}^{n_{0}-1}\\left|e^{\\mathbf{x}_{i}}-e^{\\mathbf{x}_{i}^{*}}\\right|+\\displaystyle\\frac{\\epsilon}{2}Z}\\\\ {\\displaystyle\\leqslant\\epsilon Z}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Where the last step comes from the observation that the first sum is fixed and $Z$ is unbounded with $n$ . Therefore, for $n$ large enough, we can also bound the left summand by $Z\\epsilon/2$ . The same argument also holds when bounding with $Z^{*}$ instead of $Z$ , leading to the claim in Equation 3. We now proceed with the following computation: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\delta(y,y^{*})=\\frac{\\hat{\\lambda}}{i-1}\\left|\\frac{y^{*}}{2}-\\frac{e^{\\lambda}}{2^{i}}\\right|}\\\\ &{\\phantom{=}-\\frac{\\hat{\\lambda}}{i-1}\\left|\\frac{y^{*}(e^{\\lambda}-\\hat{y}^{*})}{2^{i}}\\right|}\\\\ &{\\phantom{=}-\\frac{\\hat{\\lambda}}{i-1}\\left|\\frac{y^{*}(e^{\\lambda}-\\hat{y}^{*})}{2^{i}}z^{*}\\right|}\\\\ &{\\phantom{=}-\\frac{\\hat{\\lambda}}{i-1}\\left|\\frac{y^{*}(e^{\\lambda}-\\hat{y}^{*})}{2^{i}}z^{*}-Z^{*}\\right|}\\\\ &{\\phantom{=}-\\frac{\\hat{\\lambda}}{i-1}\\left|\\frac{z^{*}(e^{\\lambda}-\\hat{y}^{*})}{2^{i}}z^{*}\\right|+\\left|\\frac{z^{*}(e^{\\lambda}-Z^{*})}{2^{i}}\\right|}\\\\ &{\\phantom{=}<\\frac{\\hat{\\lambda}}{i-1}\\left|\\frac{z^{*}(e^{\\lambda}-\\hat{z}^{*})}{2^{i}\\left(1-Z^{*}\\right)}z^{*}\\right|+\\frac{\\hat{\\lambda}}{2^{i}}z^{*}-Z^{*}\\right|}\\\\ &{\\phantom{=}-\\frac{\\hat{\\lambda}}{i-1}\\left|\\frac{z^{*}(e^{\\lambda}-\\hat{z}^{*})}{2^{i}}z^{*}-\\frac{z^{*}(e^{\\lambda}-\\hat{z}^{*})}{2^{i}\\left(1-Z^{*}\\right)}\\right|}\\\\ &{\\phantom{=}-\\frac{\\hat{\\lambda}}{i-1}z^{*}-Z^{*}\\frac{z^{*}(e^{\\lambda}-\\hat{y}^{*})}{2^{i}}z^{*}}\\\\ &{\\phantom{=}-\\frac{\\frac{\\hat{\\lambda}}{i-1}\\left|z^{*}(e^{\\lambda}-\\hat{z}^{*})\\right|}{2^{i}}z+\\frac{\\sqrt{\\frac{\\hat{\\lambda}}{i-1}\\left(z^{*}-\\hat{z}^{*}\\right)}}{2^{i}}}\\\\ &{\\phantom{=}<\\frac{\\operatorname{cmin}\\left(Z,Z^{*}\\right)}{2^{i}}+\\frac{\\operatorname{cmin}\\left(Z,Z^{*}\\\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which concludes the proof. ", "page_idx": 15}, {"type": "text", "text": "We are now ready to show the main result on representational collapse. In particular, we show that given two sequences of length $n$ and $n+1$ where the second sequence is the same as the first with a final repeated token, their representations become arbitrarily close. Importantly, we require that the information from the positional encodings decays to 0 as the distance grows between tokens. The final token repetition is important as otherwise the residual connection in the Transformer would not make the representations necessarily converge. ", "page_idx": 15}, {"type": "text", "text": "Theorem B.3 (Representational Collapse). Let $\\mathbf{x}\\in\\mathbb{R}^{n-1\\times d}$ be an underlying growing token sequence. Let $\\mathbf{v}^{(0)}=[\\mathbf{v_{\\alpha}}\\mathbf{v}_{a}]^{T}\\in\\mathbb{R}^{n\\times d}$ and $\\mathbf{v}^{*(0)}=[\\mathbf{v}\\ \\mathbf{v}_{a}\\ \\mathbf{v}_{a}]^{T}\\in\\mathbb{R}^{n+1\\times d}$ be two sequences for $a$ final repeated token $\\mathbf{x}_{a}\\in\\mathbb{R}^{d}$ , with all token representations bounded. Further, assume that the positional encodings decay with distance to 0. Then, for large enough $n\\in\\mathbb{N}^{+}$ , we have that the representations are under any $\\epsilon$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n||\\mathbf{v}_{n}^{(L)}-\\mathbf{v}_{n+1}^{*(L)}||_{1}<\\epsilon.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. We note that since the sequences are identical up to the $n$ -th element, it is sufifcient to only check the representations of the final elements in both sequences. We therefore compare the $n$ -th element of ${\\bf z}^{(0)}$ with the $n+1$ -th element of ${\\mathbf z}^{\\ast(0)}$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left|\\underline{{\\mathbf{z}}}_{n}^{(0)}-\\mathbf{z}_{n+1}^{*(0)}\\right|\\Big|_{1}=\\displaystyle\\left\\|\\sum_{i<n}\\alpha_{n,i}^{(0)}\\mathbf{v}_{i}^{(0)}+\\alpha_{n,n}^{(0)}\\mathbf{v}_{a}^{(0)}-\\left(\\sum_{i<n}\\alpha_{n+1,i}^{*(0)}\\mathbf{v}_{i}^{(0)}+\\left(\\alpha_{n+1,n}^{*(0)}+\\alpha_{n+1,n+1}^{*(0)}\\right)\\mathbf{v}_{a}^{(0)}\\right)\\right\\|_{1}}\\\\ &{\\qquad=\\left\\|\\displaystyle\\sum_{i<n}\\left(\\alpha_{n,i}^{(0)}-\\alpha_{n+1,i}^{*(0)}\\right)\\mathbf{v}_{i}^{(0)}+\\left(\\alpha_{n,n}^{(0)}-\\alpha_{n+1,n}^{*(0)}-\\alpha_{n+1,n+1}^{*(0)}\\right)\\mathbf{v}_{a}^{(0)}\\right\\|_{1}}\\\\ &{\\leqslant\\displaystyle\\sum_{i<n}\\left|\\alpha_{n,i}^{(0)}-\\alpha_{n+1,i}^{*(0)}\\right|+\\left|\\alpha_{n,n}^{(0)}-\\alpha_{n+1,n}^{*(0)}-\\alpha_{n+1,n+1}^{*(0)}\\right|}\\\\ &{\\leqslant\\displaystyle\\sum_{i<n}\\left|\\alpha_{n,i}^{(0)}-\\alpha_{n+1,i}^{*(0)}\\right|+\\left|\\alpha_{n,n}^{(0)}-\\alpha_{n+1,n}^{*(0)}\\right|+\\left|\\alpha_{n+1,n+1}^{*(0)}\\right|}\\\\ &{=\\delta\\left(\\alpha_{n,i}^{(0)},\\alpha_{n,n}^{*(0)}\\right)+\\alpha_{n+1,n+1}^{*(0)}<\\epsilon}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We assume for simplicity that the values are unit norm. This is not crucial as otherwise one would equivalently just need to consider additional constant factors as we assume the token representations are bounded. We note that the term $\\delta\\left(\\alpha_{n,:}^{(0)},\\alpha_{n,:n}^{*(0)}\\right)$ goes to $0$ with $n\\to\\infty$ thanks to Lemma B.2 and our assumptions on the positional encodings. Similarly, the term \u03b1\u02danp\\`01q,n\\`1 goes to 0 due to Lemma B.1. We have also used the fact that vpn0q ${\\bf v}_{n}^{(0)}\\,=\\,{\\bf v}_{n+1}^{*(0)}$ by construction to ignore the residual connection. As $\\mathbf{v}_{i}^{(\\ell+1)}={\\boldsymbol{\\psi}}^{(\\ell)}\\left(\\operatorname{norm}_{2}^{(\\ell)}\\left(\\mathbf{z}_{i}^{(\\ell)}\\right)\\right)+\\mathbf{v}_{i}^{(\\ell)}$ , the sequences will have arbitrarily close final token representations when entering the next layer. The result then follows via a simple inductive approach on the layers. $\\sqsupset$ ", "page_idx": 16}, {"type": "text", "text": "We also highlight a negative decay result which highlights why the assumption on the positional encodings is important. In particular, we show that given two sequences $\\textbf{x}=$ $(1\\ 0\\ 1\\ 0\\ldots)$ and $\\mathbf{x}^{*}=(0\\mathrm{~1~0~}1\\dots)$ , the total variation of the softmax does not decay to $\\boldsymbol{0}$ . This implies that there may be solutions to representational collapse depending on how the positional encodings are chosen. ", "page_idx": 16}, {"type": "text", "text": "Proposition B.4. Consider two sequences $\\mathbf{x}=(1\\;0\\;1\\;0\\dots)\\in\\mathbb{R}^{n}$ and $\\mathbf{x}^{*}=(0\\mathrm{~1~0~1~.~.~.~})\\in\\mathbb{R}^{n}$ for n even. Let y and y\u02dabe the softmax of x and $\\mathbf{x}^{*}$ , respectively. Then the total variation $\\delta(\\mathbf{y},\\mathbf{y}^{*})$ does not tend to 0 as $n\\to\\infty$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. Let $\\begin{array}{r}{Z\\;=\\;\\sum_{i=1}^{n}e_{i}^{\\mathbf{x}}}\\end{array}$ and $\\begin{array}{r}{Z^{*}\\;=\\;\\sum_{i=1}^{n}e^{\\mathbf{x}_{i}^{*}}}\\end{array}$ be the partition functions for $\\mathbf{x}$ and $\\mathbf{x}^{*}$ , respectively. We  d\u0159irectly compute: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{n\\to\\infty}{\\operatorname*{lim}}\\delta(\\mathbf{y},\\mathbf{y}^{*})=\\underset{n\\to\\infty}{\\operatorname*{lim}}\\frac{\\overline{{n}}}{i-1}\\left\\vert\\frac{\\mathbf{y}_{i}}{Z}-\\frac{\\mathbf{y}_{i}^{*}}{Z^{*}}\\right\\vert}\\\\ &{=\\underset{n\\to\\infty}{\\operatorname*{lim}}\\frac{\\mathbf{y}_{i}}{i-1}\\left\\vert\\frac{\\mathbf{y}_{i}}{\\overline{{\\mathbf{z}}}_{e}+\\frac{\\mathbf{y}_{i}^{*}}{2}}-\\frac{\\mathbf{y}_{i}^{*}}{\\frac{\\overline{{n}}}{2}e+\\frac{\\mathbf{y}_{i}^{*}}{2}}\\right\\vert}\\\\ &{=\\underset{n\\to\\infty}{\\operatorname*{lim}}\\frac{\\overline{{n}}}{i-1}\\left\\vert\\frac{e-1}{\\overline{{\\mathbf{z}}}_{e}+\\frac{\\mathbf{y}_{i}^{*}}{2}}\\right\\vert}\\\\ &{=\\underset{n\\to\\infty}{\\operatorname*{lim}}\\frac{n(e-1)}{n\\to1}}\\\\ &{=2\\frac{e-1}{e+1}>0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "B.2 Over-squashing ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We now present our results on over-squashing. In our derivations, we assume that the attention coefifcients are independent of the values and that we can summarise the effect of the layer norms via a constant factor. These assumptions are not necessary for the same derivation process to hold, but they greatly simplify the obtained bound and help more clearly point out the main takeaways. ", "page_idx": 16}, {"type": "text", "text": "Theorem B.5 (Over-squashing in Transformers). Consider an input sequence $\\mathbf{v}_{1}^{(0)},\\ldots,\\mathbf{v}_{n}^{(0)}$ (including $C o T)$ . Let $\\sigma_{\\psi}$ be the maximal Lipschitz constant of any $\\psi^{(\\ell)}$ , and $\\begin{array}{r l}{\\bar{\\alpha}_{j,i}^{(\\ell)}}&{=}\\end{array}$ $\\begin{array}{r}{\\frac{1}{\\beta^{(\\ell)}}\\left(\\alpha_{j,i}^{(\\ell)}+\\delta_{j,i}\\right)}\\end{array}$ the normalized attention coefficient, then: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left\\|\\frac{\\partial\\mathbf{y}_{n}}{\\partial\\mathbf{v}_{i}^{(0)}}\\right\\|\\leqslant\\sigma_{\\psi}^{L}\\sum_{k_{1}\\geqslant i}\\cdot\\cdot\\cdot\\sum_{k_{L}\\geqslant k_{L-1}}\\bar{\\alpha}_{n,k_{L}}^{(L-1)}\\prod_{\\ell=2}^{L-1}\\bar{\\alpha}_{k_{\\ell},k_{\\ell-1}}^{(\\ell-1)}\\bar{\\alpha}_{k_{1},i}^{(0)}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. Note that for $j\\geqslant i$ we have: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\|\\frac{\\hat{\\sigma}\\mathbf{v}_{j}^{(\\ell+1)}}{\\hat{\\sigma}\\mathbf{v}_{i}^{(\\ell)}}\\right\\|=\\left\\|\\frac{\\hat{\\sigma}}{\\hat{\\sigma}\\mathbf{v}_{j}^{(\\ell)}}\\left[\\psi^{(\\ell)}\\left(\\mathrm{norm}_{2}^{(\\ell)}\\left(\\mathbf{z}_{j}^{(\\ell)}\\right)\\right)+\\mathbf{z}_{j}^{(\\ell)}\\right]\\right\\|}&{}\\\\ &{\\phantom{\\quad\\quad}\\leqslant\\left(\\frac{\\sigma_{v(\\ell)}}{\\hat{\\sigma}_{2}^{(\\ell)}}+1\\right)\\frac{\\hat{\\sigma}\\mathbf{z}_{j}^{(\\ell)}}{\\hat{\\sigma}\\mathbf{v}_{i}^{(\\ell)}}}\\\\ &{\\phantom{\\quad\\quad\\quad}=\\left(\\frac{\\sigma_{v(\\ell)}}{\\hat{\\sigma}_{2}^{(\\ell)}}+1\\right)\\frac{\\hat{\\sigma}}{\\hat{\\sigma}\\mathbf{v}_{i}^{(\\ell)}}\\left[\\sum_{j\\in\\ i}\\alpha_{i j}^{(\\ell)}\\mathrm{\\norm}_{1}^{(\\ell)}\\left(\\mathbf{v}_{i}^{(\\ell)}\\right)+\\mathbf{v}_{i}^{(\\ell)}\\right]}\\\\ &{\\phantom{\\quad\\quad\\quad}=\\left(\\frac{\\sigma_{v(\\ell)}}{\\hat{\\beta}_{2}^{(\\ell)}}+1\\right)\\left(\\frac{\\alpha_{j,i}^{(\\ell)}}{\\hat{\\beta}_{1}^{(\\ell)}}+\\delta_{j,i}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where we let $\\beta_{i}^{(\\ell)}$ represent the effect of layer normalization $i$ at the $\\ell$ -th layer and $\\sigma_{\\psi^{(\\ell)}}$ the Lipschitz constant of $\\psi^{(\\ell)}$ . For the case when due to the causal mechanism we have that $\\hat{\\sigma}\\mathbf{v}_{j}^{(\\ell)}/\\hat{\\sigma}\\mathbf{v}_{i}^{(\\ell-1)}=0$ . We compute the following bound: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\|\\frac{\\hat{\\mathcal{O}}_{\\mathbf{y}_{n}}}{\\hat{\\mathcal{O}}_{\\mathbf{t}}^{(0)}}\\right\\|=\\left\\|\\frac{1}{\\beta_{3}}\\sum_{i_{1}}\\cdots\\sum_{k_{L}}\\frac{\\hat{\\mathcal{O}}^{\\mathbf{t}}_{\\mathbf{x}_{n}^{(L)}}}{\\hat{\\mathcal{O}}_{\\mathbf{t},L}^{(L-1)}}\\prod_{\\ell=2}^{L-1}\\frac{\\hat{\\mathcal{O}}^{\\mathbf{t}}_{k\\ell}^{(\\ell)}}{\\hat{\\mathcal{O}}^{\\mathbf{t}}_{k\\ell-1}^{(\\ell)}}\\frac{\\hat{\\mathcal{O}}^{\\mathbf{t}}_{k\\ell}^{(1)}}{\\hat{\\mathcal{O}}^{(\\ell)}}\\right\\|}\\\\ {=\\left\\|\\frac{1}{\\beta_{3}}\\sum_{i_{1}\\geq i_{1}}\\cdots\\sum_{k_{L}\\geq k_{L-1}}\\frac{\\hat{\\mathcal{O}}^{\\mathbf{t}}_{n}^{(L)}}{\\hat{\\mathcal{O}}^{\\mathbf{t}}_{k\\ell-1}^{(L-1)}}\\prod_{\\ell=2}^{L-1}\\frac{\\hat{\\mathcal{O}}^{\\mathbf{t}}_{k\\ell}^{(\\ell)}}{\\hat{\\mathcal{O}}^{\\mathbf{t}}_{k\\ell-1}^{(\\ell-1)}}\\frac{\\hat{\\mathcal{O}}^{\\mathbf{t}}_{k\\ell}^{(1)}}{\\hat{\\mathcal{O}}^{(\\ell)}}\\right\\|}\\\\ {\\leqslant\\frac{1}{\\beta_{3}}\\prod_{\\ell=1}^{L}\\left(\\frac{\\sigma_{\\Psi}}{\\beta_{2}^{(\\ell)}}+1\\right)\\sum_{k_{1}\\geq i_{1}}\\cdots\\sum_{k_{L}\\geq k_{L-1}}\\frac{\\hat{\\mathcal{O}}^{(L-1)}}{\\hat{\\mathcal{O}}_{n,k_{L}}^{(1-1)}}\\frac{I^{-1}}{\\hat{\\mathcal{O}}^{(\\ell-1)}}\\frac{\\hat{\\mathcal{O}}^{(\\ell)}}{k_{L\\ell-1}^{(\\ell-1)}}\\frac{\\hat{\\mathcal{O}}^{(\\ell)}}{\\hat{\\mathcal{O}}_{k\\ell-1}^{(0)}}}\\\\ {=C\\sum_{k_{1}\\geq i_{1}}\\cdots\\sum_{k_{L}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$\\begin{array}{r}{C=\\frac{1}{\\beta_{3}}\\prod_{\\ell=1}^{L}\\left(\\frac{\\sigma_{\\psi}}{\\beta_{2}^{(\\ell)}}+1\\right)\\!.}\\end{array}$ ", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We note that in this derivation, we use simplifying assumptions on the layer norms and attention coefifcients, more specifically we assume that they are independent of the $\\mathbf{v}_{i}\\mathbf{s}$ . Of course, there is nothing stopping us from avoiding such assumptions and pushing the partial derivatives inside these components as well. The drawback is that this would add a great deal of additional complexity to the result and potentially distract from what we believe are the two key takeaways: (1) the position of the token matters, and (2) the attention coefifcients matter. ", "page_idx": 17}, {"type": "text", "text": "Connection to the spectral theory of Markov chains. We now show some results on the spectral theory of matrices which relate to causal attention mechanisms. We emphasize that in this work, we view causal attention mechanisms as triangular row-stochastic matrices. We show that these matrices have interesting spectral properties. ", "page_idx": 17}, {"type": "text", "text": "Lemma B.6. A row-stochastic triangular matrix A has 1 as its largest eigenvalue. Moreover, such eigenvalue has multiplicity 1 if each row except the first has at least 2 non-zero entries. ", "page_idx": 17}, {"type": "text", "text": "Proof. We start by showing that A cannot have eigenvalues $\\lambda>1$ . We then provide an eigenvector with eigenvalue 1. We finally show that such an eigenvector is unique if each row has at least 2 non-zero entries. ", "page_idx": 17}, {"type": "text", "text": "Assume $\\lambda>1$ for some eigenvector $\\phi$ , we then have that $\\mathbf{A}\\phi=\\lambda\\phi$ . Consider $\\phi_{i}=\\operatorname*{max}_{k}\\phi_{k}>$ 0. Now $\\begin{array}{r}{(\\mathbf{A}\\phi)_{i}=\\sum_{j\\leqslant i}\\mathbf{A}_{i j}\\phi_{j}=\\lambda\\phi_{i}}\\end{array}$ . As the sum is a convex combination, the result cannot be larger than the already maximal element $\\phi_{i}$ . As $\\lambda>1$ , we however have that $\\lambda\\phi_{i}>\\phi_{i}$ which is a contradiction and we conclude that $\\lambda\\leqslant1$ . ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "It is easy to find an eigenvector that always has eigenvalue 1. Consider a vector $\\mathbf{x}$ which is a constant vector of 1s. Then $\\begin{array}{r}{({\\bf A}{\\bf x})_{i}=\\sum_{j\\leqslant i}{\\bf A}_{i j}={\\bf x}_{i}}\\end{array}$ , therefore $\\mathbf{x}$ is an eigenvector with eigenvalue 1. ", "page_idx": 18}, {"type": "text", "text": "Finally, we show that when each row is non-zero, the only eigenvector is the constant-valued eigenvector. Consider the largest entry $\\mathbf{y}_{i}>0$ , then we have that $\\begin{array}{r}{(\\mathbf{A}\\mathbf{y})_{i}=\\sum_{j\\leqslant i}\\mathbf{A}_{i j}\\mathbf{y}_{j}=\\mathbf{y}_{i}}\\end{array}$ . Again, as this defines a convex combination, we must have that all token s \u0159that $i$ points to (i.e. the non-zero entries) are also equal to $\\mathbf{y}_{i}$ . The condition that each row has at least two non-zero entries is important as it means that the condition ${\\bf y}_{i}={\\bf y}_{j}$ is true for all tokens. $\\sqsubset$ ", "page_idx": 18}, {"type": "text", "text": "Lemma B.7. The product of two row-stochastic matrices is again row-stochastic. Moreover, the product of two triangular row-stochastic matrices is a triangular row-stochastic matrix. ", "page_idx": 18}, {"type": "text", "text": "Proof. Let $\\mathbf{A},\\mathbf{B}\\in\\mathbb{R}^{n\\times n}$ be two row-stochastic matrices. We compute: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{j}\\left(\\mathbf{AB}\\right)_{i j}=\\sum_{j}\\sum_{k}\\mathbf{A}_{i k}\\mathbf{B}_{k j}=\\sum_{k}\\mathbf{A}_{i k}\\sum_{j}\\mathbf{B}_{k j}=1\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The final statement follows immediately from the fact that the product of two triangular matrices is triangular. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "We now show that under specific conditions, our over-squashing bound converges to a steady state in which the final token ${\\bf y}_{n}$ only depends on the initial input token $\\mathbf{v}_{1}^{(0)}$ as the number layers tends to infinity, i.e. $L\\to\\infty$ . ", "page_idx": 18}, {"type": "text", "text": "Proposition B.8. Let $\\beta_{1}^{(\\ell)},\\beta_{2}^{(\\ell)}\\,=\\,1$ , $\\beta_{3}^{1/L}=4$ , $\\sigma_{\\psi}\\,=\\,I$ . Furthermore, for simplicity, let the attention coefficients be equal at each layer and such that each row except the first of the causal mechanism has at least two non-zero elements. Then, we have as $L\\to\\infty$ that $\\hat{\\sigma}\\mathbf{y}_{n}/\\hat{\\sigma}\\mathbf{v}_{i}^{(0)}=0$ when $i\\neq1$ and $\\partial\\mathbf{y}_{n}/\\partial\\mathbf{v}_{i}^{(0)}=1$ when $i=1$ . In other words, $\\mathbf{y}_{n}$ will only be sensitive to the first token. ", "page_idx": 18}, {"type": "text", "text": "Proof. Let the associated attention matrix be $\\pmb{\\Lambda}$ . We start by re-writing the following: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\displaystyle\\left|\\frac{\\partial\\mathbf{y}_{n}}{\\partial\\mathbf{v}_{i}^{(0)}}\\right|\\right|\\leqslant\\frac{1}{\\beta_{3}}\\prod_{\\ell=1}^{L}\\left(\\frac{\\sigma_{\\psi}}{\\beta_{2}^{(\\ell)}}+1\\right)\\sum_{k_{1}\\geqslant i}\\cdots\\sum_{k_{L}\\geqslant k_{L-1}}\\bar{\\alpha}_{n,k_{L}}^{(L-1)}\\prod_{\\ell=2}^{L-1}\\bar{\\alpha}_{k_{\\ell},k_{\\ell-1}}^{(\\ell-1)}\\bar{\\alpha}_{k_{1},\\ell}^{(0)}}\\\\ &{\\qquad\\qquad=\\left(\\displaystyle\\prod_{\\ell=1}^{L}\\left[\\frac{1}{\\beta_{3}^{1/L}}\\left(\\frac{\\sigma_{\\psi}}{\\beta_{2}}+1\\right)\\left(\\frac{1}{\\beta_{1}}\\mathbf{A}+\\mathbf{I}\\right)\\right]\\right)_{n,i}}\\\\ &{\\qquad\\qquad=\\left(\\left[\\frac{1}{2}\\left(\\mathbf{A}+\\mathbf{I}\\right)\\right]^{L}\\right)_{n,i}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We now point out that $\\tilde{\\textbf{A}}=\\begin{array}{r}{\\frac{1}{2}\\left(\\mathbf{A}+\\mathbf{I}\\right)}\\end{array}$ is row-stochastic and with our assumptions is diagonalizable into $\\tilde{\\mathbf{A}}=\\Psi\\Sigma\\Phi$ . In particular, by Lemma B.7, also $\\tilde{\\mathbf{A}}^{L}$ is row-stochastic and each entry is non-negative. We now use the Perron-Frobenius theorem for non-negative matrices [23], which guarantees us that all eigenvalues $\\lambda_{k}$ of $\\tilde{\\Lambda}$ are bounded such that $|\\lambda_{k}|\\leqslant1$ . In particular, thanks to Lemma B.6, we know that there is a unique eigenvector (the constant eigenvector $\\psi_{n}$ ) with eigenvalue $\\lambda_{n}=1$ . Denote the left eigenvectors by $\\psi_{k}$ and the right eigenvectors $\\phi_{n}$ , we therefore have: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{L\\rightarrow\\infty}\\tilde{\\mathbf{A}}^{L}=\\sum_{k}\\lambda_{k}^{L}\\psi_{k}\\phi_{k}^{T}=\\psi_{n}\\phi_{n}^{T}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "In particular, one can check that $\\boldsymbol{\\phi}_{n}^{I}=\\left[1\\;0\\ldots\\;0\\right]$ , meaning that $\\psi_{n}\\phi_{n}^{I}$ has as first column a constant vector of 1s and every other entry $0$ . This completes the proof. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "B.3 Counting ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We finally show in this section our final results that apply specifically to counting tasks. We start by highlighting a potential dififculty that the softmax layer encounters when counting, namely that the normalisation used makes it hard for it to preserve a notion of magnitude present in the sequence. ", "page_idx": 19}, {"type": "text", "text": "Proposition B.9. A Transformer without positional encodings and a causal attention mechanism is immediately unable to solve the counting problem. ", "page_idx": 19}, {"type": "text", "text": "Proof. We show this statement by demonstrating that the only information preserved about the \u2018count\u2019 by the attention mechanism will be the ratio of the elements present in a sequence. In particular, sequences with the same ratio of tokens will be assigned the exact same representation \u2014 this applies as we specifically study an attention mechanism without positional encodings and causal masking. Of course, having the same ratio of elements does not mean that the count will be the same, for instance the sequences \u201810\u2019 and \u20181100\u2019 have the same ratio of digits but clearly different counts. ", "page_idx": 19}, {"type": "text", "text": "Consider a sequence of two values, $\\mathbf{v}_{z e r o}^{(0)}$ and $\\mathbf{v}_{o n e}^{(0)}$ , with $n_{0}$ and $n_{1}$ being the number of zeros and ones respectively. We ignore in our calculations the MLPs $\\psi$ and the normalizations norm as these don\u2019t affect the argument. As this specific attention mechanism is permutation equivariant, the initial zero tokens will all be mapped to: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\varepsilon_{z e r o}^{(1)}=\\sum_{j}\\frac{\\exp\\big(\\mathbf{q}_{z e r o}^{(0)T}\\mathbf{k}_{j}^{(0)}\\big)}{\\sum_{w}\\exp\\big(\\mathbf{q}_{z e r o}^{(0)T}\\mathbf{k}_{w}^{(0)}\\big)}\\mathbf{v}_{j}^{(0)}+\\mathbf{v}_{z e r o}^{(0)}}}\\\\ &{=\\frac{n_{0}\\exp\\big(\\mathbf{q}_{z e r o}^{(0)T}\\mathbf{k}_{z e r o}^{(0)}\\big)}{n_{0}\\exp\\big(\\mathbf{q}_{z e r o}^{(0)T}\\mathbf{k}_{z e r o}^{(0)}\\big)+n_{1}\\exp\\big(\\mathbf{q}_{z e r o}^{(0)T}\\mathbf{k}_{z e r o}^{(0)}\\big)}\\mathbf{v}_{z e r o}^{(0)}+\\frac{n_{1}\\exp\\big(\\mathbf{q}_{z e r o}^{(0)T}\\mathbf{k}_{o n}^{(0)}\\big)}{n_{0}\\exp\\big(\\mathbf{q}_{z e r o}^{(0)T}\\mathbf{k}_{z e r o}^{(0)}\\big)+n_{1}\\exp\\big(\\mathbf{q}_{z e r o}^{(0)/2}\\mathbf{k}_{z e r o}^{(0)}\\big)}}\\\\ &{=\\frac{\\exp\\big(\\mathbf{q}_{z e r o}^{(0)T}\\mathbf{k}_{z e r o}^{(0)}\\big)}{\\exp\\big(\\mathbf{q}_{z e r o}^{(0)T}\\mathbf{k}_{z e r o}^{(0)}\\big)+\\frac{n_{1}}{n_{0}}\\exp\\big(\\mathbf{q}_{z e r o}^{(0)T}\\mathbf{k}_{o n}^{(0)}\\big)}\\mathbf{v}_{z e r o}^{(0)}+\\frac{\\exp\\big(\\mathbf{q}_{z e r o}^{(0)T}\\mathbf{k}_{o n}^{(0)}\\big)}{\\frac{n_{0}}{n_{1}}\\exp\\big(\\mathbf{q}_{z e r o}^{(0)T}\\mathbf{k}_{z e r o}^{(0)}\\big)+\\exp\\big(\\mathbf{q}_{z e r o}^{(0)T}\\mathbf{k}_{o n}^{(0)}\\big)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Similarly, the ones will be mapped to: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\varepsilon_{o n e}^{(1)}=\\sum_{j}\\frac{\\exp{\\big(\\mathbf{q}_{o n e}^{(0)T}\\mathbf{k}_{j}^{(0)}\\big)}}{\\sum_{w}\\exp{\\big(\\mathbf{q}_{o n e}^{(0)T}\\mathbf{k}_{w}^{(0)}\\big)}}\\mathbf{v}_{j}^{(0)}+\\mathbf{v}_{o n e}^{(0)}}}\\\\ &{=\\frac{n_{0}\\exp{\\big(\\mathbf{q}_{o n e}^{(0)T}\\mathbf{k}_{z e r o}^{(0)}\\big)}}{n_{0}\\exp{\\big(\\mathbf{q}_{o n e}^{(0)T}\\mathbf{k}_{z e r o}^{(0)}\\big)}+n_{1}\\exp{\\big(\\mathbf{q}_{o n e}^{(0)T}\\mathbf{k}_{o n e}^{(0)}\\big)}}\\mathbf{v}_{z e r o}^{(0)}+\\frac{n_{1}\\exp{\\big(\\mathbf{q}_{o n e}^{(0)T}\\mathbf{k}_{o n e}^{(0)}\\big)}}{n_{0}\\exp{\\big(\\mathbf{q}_{o n e}^{(0)T}\\mathbf{k}_{z e r o}^{(0)}\\big)}+n_{1}\\exp{\\big(\\mathbf{q}_{o n e}^{(0)T}\\mathbf{k}_{z e r o}^{(0)}\\big)}}}\\\\ &{=\\frac{\\exp{\\big(\\mathbf{q}_{o n e}^{(0)T}\\mathbf{k}_{z e r o}^{(0)}\\big)}}{\\exp{\\big(\\mathbf{q}_{o n e}^{(0)T}\\mathbf{k}_{z e r o}^{(0)}\\big)}+\\frac{n_{1}}{n_{0}\\exp{\\big(\\mathbf{q}_{o n e}^{(0)T}\\mathbf{k}_{o n e}^{(0)}\\big)}}\\mathbf{v}_{z e r o}^{(0)}}+\\frac{\\exp{\\big(\\mathbf{q}_{o n e}^{(0)T}\\mathbf{k}_{o n e}^{(0)}\\big)}}{\\frac{n_{0}}{n_{1}}\\exp{\\big(\\mathbf{q}_{o r e}^{(0)T}\\mathbf{k}_{z e r o}^{(0)}\\big)}+\\exp{\\big(\\mathbf{q}_{o n e}^{(0)T}\\mathbf{k}_{o n e}^{(0)}\\big)}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Assuming that zpz1eqro \u2030 zpo1nqe (t o avoid the trivial case), we notice that the attention mechanism alongside the MLP $\\psi$ define an isomorphism between sequences at different layers, updating all zeros and ones to a different value vector. The critical fact is that the representations only depend on the ratio between $n_{0}$ and $n_{1}$ , meaning that sequences of different lengths (therefore different counts) will have the exact same representation. This is respected at each layer, meaning that the LLM after $L$ layers will assign the same representation to different sequences as long as they have the same ratio. This points to a loss of representation for the counting problem. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "Corollary B.10. Consider a task in which the goal is to count how many $\\mathbf{v}_{a}$ tokens there are in the sequence. Let $\\mathbf{v}^{(0)}=[\\mathbf{v_{\\alpha}}\\mathbf{v}_{a}]^{T}\\in\\mathbb{R}^{n\\times d}$ and $\\mathbf{v}^{*(0)}=\\left[\\mathbf{v}\\ \\mathbf{v}_{a}\\ \\mathbf{v}_{a}\\right]\\in\\mathbb{R}^{(n+1)\\times d}$ . Due to representational collapse, at least one sequence will be given the wrong count for large enough finite $n$ . ", "page_idx": 20}, {"type": "text", "text": "Proof. This statement is a direct consequence of representational collapse. In particular, as $\\mathbf{y}_{n}$ and $\\mathbf{y}_{n}^{*}$ will be indistinguishable for large enough $n$ , the Transformer will be forced to make a mistake for at least one of them. This points to an impossibility result of counting on certain sequences due to floating point error. This holds regardless of the positional encodings used (as long as they satisfy the required decay conditions) and causal mechanism. \u53e3 ", "page_idx": 20}, {"type": "text", "text": "C Experiments ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The prompting done on Gemini 1.5 in our work does not require custom resources as we use hosted Gemini instances. We run a local version of Gemma 7B on modest hardware to analyse the internal representations. ", "page_idx": 20}, {"type": "text", "text": "C.1 Experimental Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We detail the way in which we execute the prompting for the various experiments. ", "page_idx": 20}, {"type": "text", "text": "Counting experiments. For the sum experiment we prompt as:   \nPlease perform the following sum: seq. Please give the answer on the final line exactly as \u2019The final answer to your maths question is: xxxx\u2019, where \u2019xxxx\u2019 is your answer..   \nFor the ones and zero sequences, we similarly prompt as   \nPlease count the number of ones in the following sequence:seq. Please give the answer on the final line exactly as \u2019The final answer to your maths question is: xxxx\u2019, where \u2019xxxx is your answer.   \nFor the word counting experiment, we prompt as   \nPlease count the number of times \u2018car\u2019 appears in the following sentence: \u2019seq\u2019. Please give the answer on the final line exactly as \u2019The final answer to your maths question is: xxxx\u2019, where \u2019xxxx\u2019 is your answer. ", "page_idx": 20}, {"type": "text", "text": "For the CoT experiments, we supply examples of the form: ", "page_idx": 20}, {"type": "text", "text": "Let\u2019s think step by step, showing me your reasoning. Here are a few examples:   \nPlease perform the following sum: $1\\ +\\ 1\\ +\\ 1\\ +\\ 1\\ +\\ 1\\ +\\ 1\\ +\\ 1\\ +\\ 1\\ +\\ 1\\ +\\ 1\\ +\\ 1\\ +\\ 1\\ +\\ 1\\ +\\ 1\\ +\\ 1\\ +\\ 1\\ +$ + $~1~+~1$   \nWe divide the sum into groups of 5. $\\left(1~+~1~+~1~+~1~+~1\\right)~+~\\left(1~+~1~+~1~+~1~+~1~+~4~\\right)~+~\\left(3~+~1~+~2~\\right)~+~\\left(3~+~1~+~1~+~2~\\right)~+~\\left(4~+~3~+~3~\\right)~+~\\left(5~+~3~+~2~\\right)~+~\\left(6~+~5~+~6~\\right)~+~1~+~4~+~1~+~2~.$ $\\mathrm{~1)~+~1~+~1~}$   \nThe answer is then $2\\ *\\ 5\\ +\\ 2\\ =\\ 12$   \nThe final answer to your maths question is: 12   \nPlease perform the following sum: $\\texttt{l+1+1+1+1+1+1+1}$   \nWe divide the sum into groups of 5.   \n$(1\\ +\\ 1\\ +\\ 1\\ +\\ 1\\ +\\ 1)\\ +\\ 1$   \nThe answer is then $1\\ *\\ 5\\ +\\ 1\\ =\\ 6$   \nThe final answer to your maths question is: 6 ", "page_idx": 20}, {"type": "text", "text": "With similar strategies for the 4 experiments. ", "page_idx": 20}, {"type": "text", "text": "Copying experiments. For the copying experiments, we use the following prompt: Consider the following sequence: seq. What is the last digit in this sequence? Please answer exactly as \u2018The answer to your question is: <ANSWER>\u2019   \nand change appropriately the sequence as described. ", "page_idx": 20}, {"type": "text", "text": "C.2 Counting with Gemma ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We report similar results for the counting experiments using Gemma [27] in Figure 6 and 7.   \nCompared to Gemini 1.5, Gemma seems to answer less accurately on the counting prompts. ", "page_idx": 21}, {"type": "image", "img_path": "93HCE8vTye/tmp/abc22712bff64aa4a717cc9bc47dc6af2dfbdb6aeef96c8638863b98b033d897.jpg", "img_caption": ["Figure 6: Gemma 7B LLMs being prompted to (i) sum $1+\\cdot\\cdot\\cdot+1$ (left), (ii) Count the number of ones in a sequence of 1s (center), and (iii) Count the number of ones in a sequence of ones and zeroes (the sequence is a Bernoulli sequence with probability of sampling a one being 0.7) (right). "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "93HCE8vTye/tmp/1d540a86b8a44fe1df520fdfb4b2769e1ee850680a0c8a3fd1cc49519756f49b.jpg", "img_caption": ["Figure 7: Frequency of different outputs for Gemma 7B "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "C.3 Synthetic Experiments on Representational Collapse ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "To provide further experimental evidence of representational collapse with other positional encodings, we experiment using the original sinusoidal embeddings from [30]. We sample key, query, and values from a Gaussian distribution with variance $\\sigma^{2}=1/d$ , with $d$ the dimension of the embeddings. We set $d=64$ and otherwise follow the exact structure of the decoderonly Transformer presented in the original Transformer paper. We experiment with a single attention layer and check the convergence of the representations of the final token between a sequence of length $n$ and a sequence of length $n+1$ in which we simply copy the final token. We present the results in Figure 8. We see how also for sinusoidal PEs with key, queries, and values randomly sampled, the convergence still occurs. ", "page_idx": 21}, {"type": "image", "img_path": "93HCE8vTye/tmp/08e30b73e5b958fe3384df87e43bde0379802800de93ce4348ce1e9529c61c38.jpg", "img_caption": ["Figure 8: Convergence behaviour with a synthetic Transformer experiment. We sample the key, query, and values from a Gaussian distribution and apply the traditional sinusoidal PEs from [30]. We apply a logarithmic scale on the y-axis. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Finally, we test the decay of the total variation of the softmax distributions of two growing sequences, to experimentally verify Lemma B.2. We sample a sequence $\\mathbf{x}$ of length $n$ with values uniformly distributed in the range r0, 1s. We then create $\\mathbf{x}^{*}$ by adding to the first $k=200$ elements of $\\mathbf{x}$ noise which is uniformly sampled between r0, 0.1s. In Figure 9, we show how the total variation between their respective softmax distributions decays with the sequence length. ", "page_idx": 22}, {"type": "image", "img_path": "93HCE8vTye/tmp/ea8679fdbb4b29067cfd5bf0645635d0d132489ed9a0b18ac28f9215c9c5f712.jpg", "img_caption": ["Figure 9: Total variation decay of softmax distributions with growing sequence length. We sample $n$ elements uniformly from r0, 1s and then create a related sequence by taking its first $k=200$ and adding to these elements noise sampled uniformly from $[0,0.1]$ . We measure the total variation between their softmax distributions. It is clear how the total variation decays with length, in accordance with Lemma B.2. Error bars show minimum and maximum over 5 seeds. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "C.4 Effect of Positional Encodings ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We include a synthetic experiment where we verify the occurrence of the representational collapse phenomenon with different Positional Encodings, namely: Alibi [24], the original Absolute Positional Encodings (APE) [30], and No Positional Encodings (NoPE) [15]. In our synthetic experiment, we sample $n$ queries and keys independently directly from a standard Gaussian. We then construct a related sequence of length $n+1$ by repeating its last element. We report the $L_{1}$ distance between the two sequences after a single decoder-Transformer layer, as done for the other representational collapse experiments. We consider a Transformer with a hidden dimension of 64, a single attention head, and we apply normalisations to simulate layer norm. The Transformer is not trained, but only used to simulate the propagation of information of queries and keys sampled from a Gaussian distribution. The results are shown in Figure 10. Representational collapse seems to occur with all 4 positional encodings, with the convergence of the representations happening at a similar sequence lengths. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "We would like to highlight that our condition on the decay of RoPE necessary to fulfill the requirement of Theorem 4.2 is inspired by claims of the decay of RoPE coming from the original work by Su et al. [26]. However, recent work has shown that such claims may not be strictly always upheld [4], as the original claims relied on very specific conditions on the queries and keys. We are of the opinion; however, that the range of synthetic and real-world experiments in this work support our representational collapse claims in practice with RoPE. A more precise mathematical treatment of RoPE specifically is therefore left as future work. ", "page_idx": 23}, {"type": "image", "img_path": "93HCE8vTye/tmp/c1e49c73c98d9e7a86b4f9c5f78051f154160cf2bf944c066bf1ae1b980f3b19.jpg", "img_caption": ["Figure 10: We sample $n$ queries, keys, and values independently from a standard Gaussian, applying different positional encodings. We then construct sequences of length $n+1$ , by repeating the $n$ -th token. We report the $L_{1}$ distance between the last tokens of the sequences of length $n$ and $n+1$ after one decoder-only Transformer layer. We set the hidden dimension to 64, use a single attention head, and normalise appropriately to simulate the effects of LayerNorm. The y-axis is shown in log-scale. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "C.5 Ablation on Prompt Structure ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We ablate the prompt structure specifically for the copying task. In particular, we consider the prompts: (Type 1) \u201cWhat is the last digit of the following sequence? Please answer exactly as \u2018The answer to your question is: $<$ <ANSWER $>$ \u2019 \u201d. Here is the sequence: $\\{{\\mathrm{seq}}\\}$ and (Type 2) \u201cPlease answer exactly as \u2018The answer to your question is: <ANSWER $>$ \u2019. What is the last digit of the following sequence? {seq}\u201d. The results are presented in Figure 11. We find that the prompt indeed does affect the performance on the task, as the prompt affects the distribution of the attention over the layer. However, for both types of prompts, the model ends up failing, in accordance with our theory. We also show for completeness, in Figure 12, that representational collapse occurs in Gemma 7B also for the \u2018Type 1\u2019 prompt. ", "page_idx": 23}, {"type": "text", "text": "C.6 Local sliding window attention ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "A fundamental limitation of an attention mechanism that leverages the softmax function is that it cannot remain sharp, especially as the sequence length grows [31]. This is in fact a key intuition that we exploit to show our result on representational collapse. A good way to address representational collapse and the related phenomenon of over-squashing is then that of limiting the spread of the softmax function, by directly limiting the amount of tokens the attention mechanism pays attention to. This mechanism is often referred to as a local sliding window and is a major architectural change present in Gemma 2 [28]. We believe that such an architectural change elegantly addresses representational collapse and over-squashing at the source as it avoids the issues that come with growing token sequences \u2013 something which our theory often exploits. ", "page_idx": 23}, {"type": "image", "img_path": "93HCE8vTye/tmp/511af72528ddbff045dbbe8b8f74af0009263c4cdfa7eeebc8f3aab9d4dc2cfe.jpg", "img_caption": ["Figure 11: Performance of a Gemini model on the following prompts: (Type 1) \u201cWhat is the last digit of the following sequence? Please answer exactly as \u2018The answer to your question is: <ANSWER>\u2019 \u201d. Here is the sequence: $\\{{\\mathrm{seq}}\\}$ and (Type 2) \u201cPlease answer exactly as \u2018The answer to your question is: <ANSWER $>$ \u2019. What is the last digit of the following sequence? {seq}\u201d "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "93HCE8vTye/tmp/e37c024c4949195438b387854617857da45c6dde2f2d1145175976c0743db6e0.jpg", "img_caption": ["Figure 12: Representational collapse in Gemma for the prompt: \u201cWhat is the last digit of the following sequence? Please answer exactly as \u2018The answer to your question is: <ANSWER $>$ \u2019 \u201d. Here is the sequence: $\\{{\\mathrm{seq}}\\}$ and (Type 2) \u201d "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We provide extensive evaluations and analysis to support our claims. Also we specify which assumption we made and justified them either with ablation or explanations or both. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 25}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: Limitations are explained in the appropriate section. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \nThe paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speechto-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efifciency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 25}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: In the Appendix B we report the complete proofs. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We detail all the experimental procedure. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might sufifce, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to ", "page_idx": 26}, {"type": "text", "text": "the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 27}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufifcient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 27}, {"type": "text", "text": "Answer: [No] ", "page_idx": 27}, {"type": "text", "text": "Justification: In the paper we describe in depth all the details to reproduce our work. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips. cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \nThe authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 27}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: All the experimental details are shared with sufifcient details to evaluate them. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 27}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Where relevant we included the statistical significance analysis. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 27}, {"type": "text", "text": "\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufifcient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: We only run inference experiments. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. \u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 28}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: We believe to have followed the details provided by the Ethics Guidelines ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 28}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: There is no societal impact of the work performed. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. However we put a broader impact statement in the Appendix. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. \u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake proflies, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. \u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. \u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efifciency and accessibility of ML). ", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: Our work does not pose the described risks. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \nDatasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: All the relevant work has been cited and the details where all reported. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. \u2022 The authors should cite the original paper that produced the code package or dataset. ", "page_idx": 29}, {"type": "text", "text": "\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/ datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 30}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: We do not use new assets. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \nThe paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 30}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 30}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 31}, {"type": "text", "text": "1. For all authors... ", "page_idx": 31}, {"type": "text", "text": "(a) Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? [Yes]   \n(b) Did you describe the limitations of your work? [Yes] We have clearly stated the assumptions and limitations of our theory, see e.g. Section 5   \n(c) Did you discuss any potential negative societal impacts of your work? [Yes] We don\u2019t expect negative societal impacts as a direct result of the contributions in our paper.   \n(d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] ", "page_idx": 31}, {"type": "text", "text": "2. If you are including theoretical results... ", "page_idx": 31}, {"type": "text", "text": "(a) Did you state the full set of assumptions of all theoretical results? [Yes] Our results clearly state the assumptions and limitations. (b) Did you include complete proofs of all theoretical results? [Yes] ", "page_idx": 31}, {"type": "text", "text": "3. If you ran experiments... ", "page_idx": 31}, {"type": "text", "text": "(a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] We included a description of the prompting and experimental details we carry out. We have used material publicly available in our work. We commit to releasing prompting code used to generate our results.   \n(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [NA] We do not perform any training in our work and use pre-trained models.   \n(c) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] We provide a description of the resources required in Appendix Section C. ", "page_idx": 31}, {"type": "text", "text": "4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... ", "page_idx": 31}, {"type": "text", "text": "(a) If your work uses existing assets, did you cite the creators? [Yes] (b) Did you mention the license of the assets? [Yes] See Appendix Section C. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "(c) Did you include any new assets either in the supplemental material or as a URL? [NA]   \n(d) Did you discuss whether and how consent was obtained from people whose data you\u2019re using/curating? [NA] No human data collected.   \n(e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [NA] No human data collected. ", "page_idx": 31}, {"type": "text", "text": "5. If you used crowdsourcing or conducted research with human subjects... ", "page_idx": 31}, {"type": "text", "text": "(a) Did you include the full text of instructions given to participants and screenshots, if applicable? [NA]   \n(b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [NA]   \n(c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [NA] ", "page_idx": 31}]