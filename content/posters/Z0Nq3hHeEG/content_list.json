[{"type": "text", "text": "pcaGAN: Improving Posterior-Sampling cGANs via Principal Component Regularization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Matthew C. Bendel ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Rizwan Ahmad   \nDept. BME   \nThe Ohio State University   \nColumbus, OH 43210   \nahmad.46@osu.edu ", "page_idx": 0}, {"type": "text", "text": "Dept. ECE The Ohio State University Columbus, OH 43210 bendel.8@osu.edu ", "page_idx": 0}, {"type": "text", "text": "Dept. ECE The Ohio State University Columbus, OH 43210 schniter.1@osu.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In ill-posed imaging inverse problems, there can exist many hypotheses that fit both the observed measurements and prior knowledge of the true image. Rather than returning just one hypothesis of that image, posterior samplers aim to explore the full solution space by generating many probable hypotheses, which can later be used to quantify uncertainty or construct recoveries that appropriately navigate the perception/distortion trade-off. In this work, we propose a fast and accurate posterior-sampling conditional generative adversarial network (cGAN) that, through a novel form of regularization, aims for correctness in the posterior mean as well as the trace and K principal components of the posterior covariance matrix. Numerical experiments demonstrate that our method outperforms contemporary cGANs and diffusion models in imaging inverse problems like denoising, large-scale inpainting, and accelerated MRI recovery. The code for our model can be found here: https://github.com/matt-bendel/pcaGAN. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In image recovery, the goal is to recover the true image $\\textbf{\\em x}$ from noisy/distorted/incomplete measurements ${\\pmb y}=\\mathcal{M}({\\pmb x})$ . This arises in linear inverse problems such as denoising, deblurring, inpainting, and magnetic resonance imaging (MRI), as well as in non-linear inverse problems like phase-retrieval and image-to-image translation. For all such problems, it is impossible to perfectly recover $\\textbf{\\em x}$ from $\\textit{\\textbf{y}}$ . ", "page_idx": 0}, {"type": "text", "text": "In much of the literature, image recovery is posed as point estimation, where the goal is to return a single best estimate $\\widehat{\\mathbf{x}}$ . However, there are several shortcomings of this approach. First, it\u2019s not clear how to define \u201cbest,\u201d since L2- or L1-minimizing $\\widehat{\\mathbf{\\xi}}^{x}$ are often regarded as too blurry, while efforts to make $\\widehat{\\mathbf{\\xi}}^{x}$ perceptually pleasing can sacrifice agreement with the true image $\\textbf{\\em x}$ and cause hallucinations [ 1 \u20135], as we show later. Also, many applications demand not only a recovery $\\widehat{\\mathbf{\\xi}}^{x}$ but also some quantification of uncertainty in that recovery [6,7]. ", "page_idx": 0}, {"type": "text", "text": "As an alternative to point estimation, posterior-sampling-based image recovery [8\u201327] aims to generate $P\\;\\geq\\;1$ samples $\\{\\widehat{\\pmb{x}}_{i}\\}_{i=1}^{P}$ from the posterior distribution $p_{\\times|\\boldsymbol{y}}(\\cdot|\\boldsymbol{y})$ . Posterior sampling facilitates numerous strategies to quantify the uncertainty in estimating $\\textbf{\\em x}$ , or any function of $\\textbf{\\em x}$ , from $\\textit{\\textbf{y}}$ [6,7]. It also can help with visualizing uncertainty and increasing robustness to adversarial attacks [28]. That said, the design of accurate and computationally-efficient posterior samplers remains an open problem. ", "page_idx": 0}, {"type": "text", "text": "Given a training dataset of image/measurement pairs $\\{(\\mathbf{\\boldsymbol{x}}_{t},\\mathbf{\\boldsymbol{y}}_{t})\\}_{t=1}^{T}$ , our goal is to build a generator $G_{\\theta}$ that, for a given $\\textit{\\textbf{y}}$ , maps random code vectors $z\\,\\sim\\,{\\mathcal{N}}(\\mathbf{0},I)$ to samples of the posterior, i.e., $\\widehat{\\pmb{x}}\\,=\\,G_{\\pmb{\\theta}}(z,\\pmb{y})\\,\\sim\\,p_{\\times|\\pmb{y}}(\\cdot,\\pmb{y})$ . There are many ways to approximate the ideal generator. The recent literature has focused on conditional variational autoencoders (cVAEs) [12\u201314], conditional normalizing flows (cNFs) [8\u201311], conditional generative adversarial networks (cGANs) [15\u201319], and Langevin/score/diffusion-based generative models [20\u201327]. Although diffusion models have garnered a great share of recent attention, they tend to generate samples several orders-of-magnitude slower than their cNF, cVAE, and cGAN counterparts. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "With the goal of fast and accurate posterior sampling, recent progress in cGAN training has been made through regularization. For example, Ohayon et al. [29] proposed to enforce correctness in the generated $\\textit{\\textbf{y}}$ -conditional mean using a novel form of L2 regularization. Later, Bendel et al. [19] proposed to enforce correctness in the generated $\\textit{\\textbf{y}}$ -conditional mean and trace-covariance using L1 regularization plus a correctly weighted standard-deviation (SD) reward, and gave evidence that their \u201crcGAN\u201d competes with contemporary diffusion samplers on MRI and inpainting tasks. Soon after, Man et al. [30] showed that L2 regularization and a variance reward are effective when training cGANs for JPEG decoding. More details are given in Section 2. ", "page_idx": 1}, {"type": "text", "text": "In a separate line of work, Nehme et al. [31] trained a Neural Posterior Principal Components (NPPC) network to directly estimate the eigenvectors and eigenvalues of the $\\textit{\\textbf{y}}$ -conditional covariance matrix, which allows powerful insights into the nature of uncertainty in an inverse problem. ", "page_idx": 1}, {"type": "text", "text": "Our contributions. Inspired by regularized cGANs and NPPC, we propose a novel \u201cpcaGAN\u201d that encourages correctness in the $K$ principal components of the $\\textit{\\textbf{y}}$ -conditional covariance matrix, as well as the $\\textit{\\textbf{y}}$ -conditional mean and trace covariance, when sampling from the posterior. We demonstrate that our pcaGAN outperforms existing cGANs [16, 17, 19, 29] and diffusion models [22, 25\u201327] on posterior sampling tasks like denoising, large-scale inpainting, and accelerated MRI recovery, while sampling orders-of-magnitude faster than those diffusion models. We also demonstrate that pcaGAN recovers the principal components more accurately than NPPC using approximately the same runtime. ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Our approach builds on the rcGAN regularization framework from [19], which itself builds on the cGAN framework from [16], both of which we now summarize. Let $\\mathcal{X},\\mathcal{V},$ and $\\mathcal{Z}$ denote the domains of $x,z$ , and $\\textit{\\textbf{y}}$ , respectively. The goal is to design a generator $\\mathcal{G}_{\\theta}:\\mathcal{Z}\\times\\mathcal{y}\\rightarrow\\mathcal{X}$ where, for a given $\\textit{\\textbf{y}}$ , the random $\\widehat{\\pmb{x}}=G_{\\pmb{\\theta}}(z,\\pmb{y})$ induced by the code vector $z\\sim p_{z}$ (with $_{\\textit{z}}$ independent of $\\textit{\\textbf{y}}$ ) has a distribution tha t  is close to the true posterior $p_{\\times|y}(\\cdot,y)$ in the Wasserstein-1 distance, given by ", "page_idx": 1}, {"type": "equation", "text": "$$\nW_{1}\\big(p_{\\times|\\mathbf{y}}(\\cdot,\\pmb{y}),p_{\\hat{\\mathbf{x}}|\\mathbf{y}}(\\cdot,\\pmb{y})\\big)=\\operatorname*{sup}_{D\\in L_{1}}\\mathrm{E}_{\\times|\\mathbf{y}}\\{D(\\pmb{x},\\pmb{y})\\}-\\mathrm{E}_{\\hat{\\mathbf{x}}|\\mathbf{y}}\\{D(\\pmb{\\hat{x}},\\pmb{y})\\}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Here, $D:\\mathcal{X}\\times\\mathcal{Y}\\to\\mathbb{R}$ is a \u201ccritic\u201d or \u201cdiscriminator\u201d that tries to distinguish between the true $\\textbf{\\em x}$ and generated $\\widehat{\\mathbf{\\xi}}^{x}$ given $\\textit{\\textbf{y}}$ , and $L_{1}$ denotes the set of 1-Lipschitz functions. A loss is constructed by averaging (1) over ${\\pmb y}\\sim p_{\\mathrm{y}}$ , which takes the form ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathrm{E}_{\\mathbb{Y}}\\left\\{W_{1}\\big(p_{\\times|\\mathbb{Y}}(\\cdot,\\pmb{y}),p_{\\mathbb{X}|\\mathbb{Y}}(\\cdot,\\pmb{y})\\big)\\right\\}=\\operatorname*{sup}_{D\\in L_{1}}\\mathrm{E}_{\\times,\\pmb{z},\\mathbb{Y}}\\{D(\\pmb{x},\\pmb{y})-D(G_{\\theta}(\\pmb{z},\\pmb{y}),\\pmb{y})\\},\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "using the fact that the expectation commutes with the supremum [16]. $D$ is then implemented as a neural network $D_{\\phi}$ with parameters $\\phi$ . Finally, $(\\pmb\\theta,\\phi)$ are learned by minimizing ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{adv}}(\\pmb{\\theta},\\phi)\\triangleq\\mathrm{E}_{\\times,\\mathbf{z},\\mathbf{y}}\\{D_{\\phi}(\\pmb{x},\\pmb{y})-D_{\\phi}(G_{\\pmb{\\theta}}(\\pmb{z},\\pmb{y}),\\pmb{y})\\}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "over $\\pmb{\\theta}$ and minimizing $-\\mathcal{L}_{\\mathrm{adv}}(\\theta,\\phi)+\\mathcal{L}_{\\mathrm{gp}}(\\phi)$ over $\\phi$ , where ${\\mathcal{L}}_{\\mathtt{g p}}(\\phi)$ is a gradient penalty that encourages $D_{\\phi}\\in L_{1}$ [32]. In practice, the expectation in (3) is approximated by a sample average over the training data $\\{(\\boldsymbol{x}_{t},\\boldsymbol{y}_{t})\\}$ . ", "page_idx": 1}, {"type": "text", "text": "In the typical case that the training data includes only a single image $\\pmb{x}_{t}$ for each measurement vector $\\pmb{y}_{t}$ , minimizing $\\mathcal{L}_{\\sf a d v}(\\pmb{\\theta},\\phi)$ alone does not encourage the generator to produce diverse samples. Rather, it leads to a form of mode collapse where the code $_{z}$ is ignored. In [16], Adler and O\u00a8ktem proposed a two-sample discriminator that encourages diverse generator outputs without compromising the Wasserstein objective (2). In [19], Bendel et al. instead proposed to regularize $\\mathcal{L}_{\\sf a d v}(\\pmb{\\theta},\\phi)$ in a way that encourages correct posterior means and trace-covariances, i.e., ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}{\\mu_{\\widehat{x}|\\mathbb{y}}=\\mu_{{\\mathsf{x}}|\\mathbb{y}}}&{\\quad}&&{\\mathrm{for}\\,\\mu_{\\widehat{x}|\\mathbb{y}}\\triangleq\\mathrm{E}\\{\\widehat{x}|{\\boldsymbol{y}}\\}\\qquad}&&{\\mathrm{and~}\\mu_{{\\mathsf{x}}|\\mathbb{y}}\\triangleq\\mathrm{E}\\{{\\boldsymbol{x}}|{\\boldsymbol{y}}\\}}\\\\ {\\mathrm{tr}(\\Sigma_{\\widehat{x}|\\mathbb{y}})=\\mathrm{tr}(\\Sigma_{\\mathsf{x}|\\mathbb{y}})}&{\\quad}&&{\\mathrm{for}\\,\\Sigma_{\\widehat{x}|\\mathbb{y}}\\triangleq\\mathrm{Cov}\\{\\widehat{x}|{\\boldsymbol{y}}\\}\\qquad}&&{\\mathrm{and~}\\Sigma_{\\times|\\mathbb{y}}\\triangleq\\mathrm{Cov}\\{{\\boldsymbol{x}}|{\\boldsymbol{y}}\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "To do this, [19] replaced $\\mathcal{L}_{\\sf a d v}(\\pmb{\\theta},\\phi)$ with the regularized adversarial loss ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathtt{r c G A N}}(\\theta,\\phi)\\triangleq\\beta_{\\mathtt{a d v}}\\mathcal{L}_{\\mathtt{a d v}}(\\theta,\\phi)+\\mathcal{L}_{1,P_{\\mathrm{rc}}}(\\theta)-\\beta_{\\mathtt{S D}}\\mathcal{L}_{\\mathtt{S D},P_{\\mathrm{rc}}}(\\theta),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "which involves the $P_{\\mathsf{r c}}$ -sample supervised- $\\ell_{1}$ loss and standard-deviation (SD) reward terms ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathcal{L}_{1,P}(\\pmb{\\theta})\\triangleq\\mathrm{E}_{\\times,z_{1},\\dots,z_{P},\\mathbf{y}}\\left\\{\\|\\pmb{x}-\\widehat{\\pmb{x}}_{(P)}\\|_{1}\\right\\}}\\\\ &{\\mathcal{L}_{5\\mathsf{D},P}(\\pmb{\\theta})\\triangleq\\sum_{i=1}^{P}\\mathrm{E}_{\\mathbf{z}_{1},\\dots,\\mathbf{z}_{P},\\mathbf{y}}\\left\\{\\|\\widehat{\\pmb{x}}_{i}-\\widehat{\\pmb{x}}_{(P)}\\|_{1}\\right\\}\\!,}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where typically $P_{\\mathsf{r c}}=2$ . Here, $\\{{\\widehat x}_{i}\\}$ are the generated samples and $\\widehat{\\pmb{x}}_{(P)}$ is their $P$ -sample average: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{\\mathbf{x}}_{i}\\triangleq G_{\\theta}(z_{i},y)\\;\\mathrm{for}\\,i=1,\\dots,P_{r\\quad}\\;\\mathrm{~and~}\\;\\;\\widehat{\\pmb{x}}_{(P)}\\triangleq\\frac{1}{P}\\sum_{i=1}^{P}\\widehat{\\pmb{x}}_{i}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The reward weight $\\beta_{\\mathsf{S D}}$ in (6) is automatically adjusted to accomplish (5) during training [19]. We note that the $\\mathcal{L}_{1,P}(\\pmb{\\theta})$ regularization proposed in [19] is closely related to the regularization $\\mathcal{L}_{2,P}(\\pmb{\\theta})\\triangleq\\operatorname{E}_{\\mathbf{x},\\mathbf{z}_{1},\\ldots,\\mathbf{z}_{\\mathsf{P}},\\mathbf{y}}\\{\\|\\pmb{x}\\!-\\!\\widehat{\\pmb{x}}_{(P)}\\|_{2}^{2}\\}$ proposed earlier by Ohayon et al. in [29]. A detailed discussion of the advantages and disadva ntages of various cGAN regularizations can be found in [19]. ", "page_idx": 2}, {"type": "text", "text": "3 Proposed method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Whereas rcGAN aimed for correctness in the posterior mean and posterior trace-covariance statistics, our proposed pcaGAN also aims for correctness in the $K$ principal components of the posterior covariance matrix $\\Sigma_{\\widehat{\\times}|y}$ , where $K$ is user-selectable. To do this, pcaGAN adds two additional regularization terms to the rcGAN objective: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{\\mathtt{p c a G A N}}(\\theta,\\phi)\\triangleq\\mathcal{L}_{\\mathtt{r c G A N}}(\\theta,\\phi)+\\beta_{\\mathtt{p c a}}\\mathcal{L}_{\\mathtt{e v e c}}(\\theta)+\\beta_{\\mathtt{p c a}}\\mathcal{L}_{\\mathtt{e v a l}}(\\theta)}\\\\ &{\\qquad\\mathcal{L}_{\\mathtt{e v e c}}(\\theta)\\triangleq-\\mathrm{E}_{\\mathtt{y}}\\left\\{\\:\\mathrm{E}_{\\mathtt{x},\\mathtt{z}_{1},\\ldots,\\mathtt{z}_{P}|\\mathtt{y}}\\left\\{\\sum_{k=1}^{K}[\\widehat{\\mathbf{v}}_{k}^{\\top}(x-\\mu_{\\mathtt{x}|\\mathtt{y}})]^{2}|y\\right\\}\\right\\}}\\\\ &{\\qquad\\mathcal{L}_{\\mathtt{e v a l}}(\\theta)\\triangleq\\mathrm{E}_{\\mathtt{y}}\\left\\{\\:\\mathrm{E}_{\\mathtt{x},\\mathtt{z}_{1},\\ldots,\\mathtt{z}_{P}|\\mathtt{y}}\\left\\{\\sum_{k=1}^{K}\\left(1-\\lambda_{k}/\\widehat{\\lambda}_{k}\\right)^{2}\\middle|y\\right\\}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Here, $\\{(\\widehat{\\boldsymbol{v}}_{k},\\widehat{\\lambda}_{k})\\}_{k=1}^{K}$ denote the principal eigenvectors and eigenvalues of the $\\pmb{\\theta}$ -dependent generated covariance matrix $\\Sigma_{\\widehat{\\times}|y}$ and $\\{(\\boldsymbol{v}_{k},\\lambda_{k})\\}_{k=1}^{K}$ denote the principal eigenvectors and eigenvalues of the true covariance matrix $\\Sigma_{\\times|y}$ . Because (11) is the classical PCA objective [33], minimizing $\\mathcal{L}_{\\mathrm{evec}}(\\theta)$ over $\\pmb{\\theta}$ will drive the generated principal eigenvector $\\widehat{\\pmb{v}}_{k}$ towards the true principal eigenvector $\\pmb{v}_{k}$ for each $k=1,\\ldots,K$ . Likewise, minimizing $\\mathcal{L}_{\\mathrm{eval}}(\\theta)$ over $\\pmb{\\theta}$ will drive the generated principal eigenvalue $\\widehat{\\lambda}_{k}$ towards the true principal eigenvalue $\\lambda_{k}$ for each $k\\,=\\,1,\\ldots,K$ . Based on our experiments, putting $\\widehat{\\lambda}_{k}$ in the denominator works better than the numerator and the squared error in (12) works better than an absolute value. ", "page_idx": 2}, {"type": "text", "text": "In practice, the expectations in (11)-(12) are replaced by sample averages over the training data. In the typical case that the training data includes only a single image $\\pmb{x}_{t}$ for each measurement vector $\\pmb{y}_{t}$ , the quantities $\\pmb{\\mu}_{\\times|\\mathfrak{y}}$ and $\\{\\lambda_{k}\\}$ in (11)-(12) are unknown and non-trivial to estimate for each $\\pmb{y}_{t}$ . Hence, when training the pcaGAN, we approximate them with learned quantities. This must be done carefully, however. For example, if $\\pmb{\\mu}_{\\times|\\mathfrak{y}}$ in (11) was simply replaced by the $\\pmb{\\theta}$ -dependent quantity $\\pmb{\\mu}_{\\widehat{\\times}|\\mathsf{y}}$ , then minimizing $\\mathcal{L}_{\\mathrm{evec}}(\\theta)$ over $\\pmb{\\theta}$ would encourage $\\pmb{\\mu}_{\\widehat{\\times}|\\mathsf{y}}$ to become overly large in order to drive $\\mathcal{L}_{\\mathrm{evec}}(\\theta)$ to a large negative value. ", "page_idx": 2}, {"type": "text", "text": "Algorithm 1 details our proposed approach to training the pcaGAN. In particular, it describes the steps used to perform a single update of the generator parameters $\\pmb{\\theta}$ based on the training batch $\\{(\\dot{\\mathbf{x}}_{b},\\pmb{y}_{b})\\}_{b=1}^{B}$ . Before diving into the details, we offer a brief summary of Algorithm 1. For the initial epochs, the rcGAN objective $\\mathcal{L}_{\\sf r c G A N}$ alone is optimized, which allows the generated posterior mean $\\pmb{\\mu}_{\\widehat{\\times}|\\mathsf{y}}$ to converge to the vicinity of $\\pmb{\\mu}_{\\times|\\mathsf{y}}$ . Starting at $E_{\\mathrm{evec}}$ epochs, the $\\bar{\\mathcal{L}}_{\\mathrm{evec}}(\\theta)$ regularization from (11) is added, but with $\\pmb{\\mu}_{\\times|\\mathfrak{y}}$ approximated as $\\mathtt{S t o p G r a d}(\\mu_{\\widehat{\\mathbb{X}}|\\mathsf{y}})$ . The use of StopGrad forces $\\mathcal{L}_{\\mathrm{evec}}(\\theta)$ to be minimized by manipulating the eigenvectors $\\{\\widehat{\\pmb{v}}_{k}\\}_{k=1}^{K}$ and not the generated posterior mean $\\pmb{\\mu}_{\\widehat{\\times}|\\mathsf{y}}$ . These eigenvectors are computed using an SVD of centered approximate-posterior samples. To reduce the computational burden imposed by this SVD, a \u201clazy regularization\u201d [34] approach is adopted, which computes $\\mathcal{L}_{\\mathrm{evec}}(\\theta)$ only once every $M$ training steps. Training proceeds in this manner until the eigenvectors $\\{\\widehat{\\pmb{v}}_{k}\\}$ converge. Starting at $E_{\\mathrm{eval}}$ epochs, the $\\mathcal{L}_{\\mathrm{eval}}(\\pmb{\\theta})$ regularization from (12) is added, but with $\\lambda_{k}$ a pproximated as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\lambda_{k}\\approx\\mathtt{S t o p G r a d}\\big(\\frac{1}{1+P_{\\mathrm{pca}}}\\big|\\big|\\widehat{\\boldsymbol{v}}_{k}^{\\top}[\\mathbf{x}_{b}-\\mu_{\\widehat{\\mathbf{x}}|\\mathbf{y}},\\widehat{\\mathbf{x}}_{1}-\\mu_{\\widehat{\\mathbf{x}}|\\mathbf{y}},\\cdot\\cdot\\cdot,\\widehat{\\mathbf{x}}_{P_{\\mathrm{pca}}}-\\mu_{\\widehat{\\mathbf{x}}|\\mathbf{y}}]\\big|\\big|^{2}\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Require: number of estimated eigen-components $K$ , number of samples used for eigenvector and eigenvalue regularization $P_{\\mathsf{p c a}}$ , number of samples used for rcGAN regularization $P_{\\sf r c}$ , epoch at which to involve eigenvector regularization $E_{\\mathrm{evec}}$ , epoch at which to involve eigenvalue regularization $E_{\\mathrm{eval}}$ , lazy update period $M$ , adversarial loss weight $\\beta_{\\mathsf{a d v}}$ , std regularization weight $\\beta_{\\mathsf{S D}}$ , eigenvector and eigenvalue regularization weight $\\beta_{\\mathsf{p c a}}$ , training batch $\\{(\\bar{\\mathbf{x}}_{b},\\mathbf{y}_{b})\\}_{b=1}^{B}$ , current model parameters , current training epoch $e$ , the current training step $s$ ", "page_idx": 3}, {"type": "text", "text": "1: $\\begin{array}{r}{\\mathcal{L}(\\pmb{\\theta})\\leftarrow0}\\end{array}$   \n2:   \n3: for $b=1,\\ldots,B$ do   \n4: $\\begin{array}{r l}&{v=1,\\dotsc,D\\textbf{u o}}\\\\ &{z_{i}\\sim\\mathcal{N}(\\mathbf{0},I)\\ \\mathrm{for}\\ i=1,\\dotsc,P_{\\mathrm{rc}}}\\\\ &{\\hat{x}_{i}\\leftarrow G_{\\theta}(z_{i},y_{b})\\ \\mathrm{for}\\ i=1,\\dotsc,P_{\\mathrm{rc}}}\\\\ &{\\mathcal{L}(\\theta)\\leftarrow\\mathcal{L}(\\theta)-\\beta_{\\mathrm{adv}}\\sum_{i=1}^{P_{\\mathrm{rc}}}D_{\\phi}(\\hat{x}_{i},y_{b})}\\\\ &{\\hat{x}_{(P_{\\mathrm{rc}})}=\\frac{1}{P_{\\mathrm{rc}}}\\sum_{i=1}^{P_{\\mathrm{rc}}}\\hat{x}_{i}}\\\\ &{\\mathcal{L}(\\theta)\\leftarrow\\mathcal{L}(\\theta)+\\|x_{b}-\\hat{x}_{(P_{\\mathrm{rc}})}\\|_{1}-\\beta_{\\mathrm{SD}}\\sum_{i=1}^{P_{\\mathrm{rc}}}\\mathrm{E}_{z_{1},\\dotsc,z_{\\mathrm{P}},\\forall}\\left\\{\\|\\hat{x}_{i}-\\widehat{x}_{(P_{\\mathrm{rc}})}\\|_{1}\\right\\}}\\end{array}$   \n5:   \n6:   \n7:   \n8:   \n9:   \n10: if $e\\geq E_{\\mathrm{evec}}$ and $s$ mod $M=0$ then   \n11: $z_{j}\\sim\\mathcal{N}(\\mathbf{0},I)$ for $j=1,\\dotsc,P_{\\mathsf{p c a}}$   \n12: $\\begin{array}{r l}&{\\widetilde{\\boldsymbol{x}}_{j}^{\\prime}\\gets\\boldsymbol{G}_{\\boldsymbol{\\theta}}(\\boldsymbol{z}_{j},\\boldsymbol{y}_{b})\\mathrm{~for~}j=1,\\dots,\\boldsymbol{P}_{\\mathrm{pca}}}\\\\ &{\\widehat{\\mu}\\leftarrow\\mathtt{S t o p G r a d}(\\frac{1}{P_{\\mathrm{pca}}}\\sum_{j=1}^{P_{\\mathrm{pca}}}\\widehat{\\boldsymbol{x}}_{j})}\\\\ &{\\widehat{U}\\widehat{S}\\widehat{V}^{\\top}\\gets\\mathrm{SVD}([\\widehat{\\boldsymbol{x}}_{1}-\\widehat{\\boldsymbol{\\mu}},\\ldots,\\widehat{\\boldsymbol{x}}_{P_{\\mathrm{pca}}}-\\widehat{\\boldsymbol{\\mu}}]^{\\top})}\\\\ &{\\widehat{\\boldsymbol{v}}_{k}\\leftarrow[\\widehat{V}]_{:,k}\\mathrm{~for~}k=1,\\ldots,K}\\\\ &{\\underline{{\\mathcal{L}}}(\\boldsymbol{\\theta})\\leftarrow\\mathcal{L}(\\boldsymbol{\\theta})-\\beta_{\\mathrm{pca}}\\sum_{k=1}^{K}[\\widehat{v}_{k}^{\\top}(\\boldsymbol{x}_{b}-\\widehat{\\boldsymbol{\\mu}})]^{2}}\\end{array}$   \n13:   \n14:   \n15:   \n16:   \n17: end if   \n18:   \n19: if $e\\geq E_{\\tt e v a l}$ and $s$ mod $M=0$ then   \n20: 21: $\\begin{array}{r l}&{\\lambda_{k}\\gets[S]_{k k}^{z}\\;\\mathrm{~for~}\\;k=1,\\ldots,K}\\\\ &{\\widetilde{X}\\gets[x_{b}-\\widehat{\\mu},\\widehat{x}_{1}-\\widehat{\\mu},\\widehat{x}_{2}-\\widehat{\\mu},\\ldots,\\widehat{x}_{P\\mathrm{{pca}}}-\\widehat{\\mu}]^{\\intercal}}\\\\ &{\\mathcal{L}(\\theta)\\gets\\mathcal{L}(\\theta)+\\beta_{\\mathrm{pca}}\\sum_{k=1}^{K}\\big(1-\\frac{1}{\\widehat{\\lambda}_{k}}\\mathrm{StopGrad}\\big(\\frac{1}{P_{\\mathrm{pca}}+1}\\|\\widehat{v}_{k}^{\\intercal}\\widetilde{X}\\|^{2}\\big)\\big)^{2}}\\\\ &{\\underset{\\mathcal{L}^{\\prime}}{\\dots}\\qquad\\qquad\\qquad\\qquad}\\end{array}$   \n22:   \n23: end if   \n24: end for   \n25:   \n26: $\\pmb{\\theta}\\leftarrow\\mathrm{Adam}(\\pmb{\\theta},\\nabla\\mathcal{L}(\\pmb{\\theta}))$ ", "page_idx": 3}, {"type": "text", "text": "where StopGrad is used so that the optimization focuses on $\\{\\widehat{\\lambda}_{k}\\}$ . The rational behind (13) is that, when $\\widehat{\\pmb{v}}_{k}=\\pmb{v}_{k}$ and $\\mu_{\\hat{\\times}|\\mathsf{y}}=\\mu_{\\mathsf{x}|\\mathsf{y}}$ , the terms $[\\widehat{\\boldsymbol{v}}_{k}^{\\top}(\\boldsymbol{x}_{b}-\\mu_{\\widehat{\\boldsymbol{x}}|\\boldsymbol{y}})]^{2}$ and $[\\widehat{\\pmb{v}}_{k}^{\\top}(\\widehat{\\pmb{x}}_{j}-\\mu_{\\widehat{\\times}|\\mathsf{y}})]^{2}\\;\\forall j$ all equal $\\lambda_{k}$ in $\\pmb{y}_{b}$ -conditional expectation. This expectation is approximated using a $(1+P_{\\mathsf{p c a}})$ -term sample average in (13) via the squared norm. The eigenvalues $\\{\\widehat{\\lambda}_{k}\\}$ in (12) are computed using the previously described SVD and the regularization schedule is again $M$ -lazy. ", "page_idx": 3}, {"type": "text", "text": "We now provide additional details on Algorithm 1. After the loss is initialized in line 1, the following steps are executed for each measurement vector $\\pmb{y}_{b}$ in the batch. First, approximate posterior samples $\\{\\widehat{\\pmb{x}}_{i}\\}_{i=1}^{P_{\\mathrm{rc}}}$ are generated in line 5, where $P_{\\mathsf{r c}}=2$ as per the suggestion in [19]. Using these samples, the adversarial component of the loss is added in line 6 and the rcGAN regularization is added in line 8. Starting at epoch $E_{\\mathrm{evec}}$ , lines 11-16 are executed whenever the training iteration is a multiple of $M$ . Nominally, $E_{\\mathrm{evec}}$ is set where the validation PSNR of $\\widehat{\\pmb{\\mu}}$ (an empirical approximation of $\\mu_{\\widehat{\\sf x}|\\mathsf{y}})$ stabilizes and $M=100$ . Within those lines, samples $\\{\\widehat{\\pmb{x}}_{j}\\}_{j=1}^{P_{\\mathsf{p c a}}}$ are generated in line 12 (where nominally $P_{\\mathsf{p c a}}=10K)$ ), their sample mean is computed in line 13, and the SVD of the centered samples is computed in line 14. The top $K$ right singular vectors are then extracted in line 15 in order to construct the $\\mathcal{L}_{\\mathrm{evec}}(\\theta)$ regularization, which is added to the overall generator loss $\\mathcal{L}(\\pmb\\theta)$ in line 16. Starting at epoch $E_{\\mathrm{eval}}$ , where nominally $E_{\\mathsf{e v a l}}=E_{\\mathsf{e v e c}}+25$ , lines 20-22 are executed whenever the training iteration is a multiple of $M$ . In line 20, the top $K$ eigenvalues $\\{\\widehat{\\lambda}_{k}\\}$ are constructed from the previously computed singular values and, in line 22, the $\\mathcal{L}_{\\mathrm{eval}}(\\theta)$ regularization is constructed and added to the overall training loss. The construction of $\\mathcal{L}_{\\mathrm{eval}}(\\theta)$ was previously described around (13). Finally, once the losses for all batch elements have been incorporated, the gradient $\\nabla{\\mathcal{L}}(\\pmb{\\theta})$ is computed using back-propagation and a gradient-descent step of $\\pmb{\\theta}$ is performed using the Adam optimizer [35] in line 26. ", "page_idx": 3}, {"type": "image", "img_path": "Z0Nq3hHeEG/tmp/27b68d645684413ef35f8630cc752cad349bef948d1f36960a5f903edd27acf6.jpg", "img_caption": ["Figure 1: Gaussian experiment. Wasserstein-2 distance versus (a) lazy update period $M$ for pcaGAN with $d=100=K$ , (b) estimated eigen-components $K$ for pcaGAN with $d=100$ and $M=100$ , and (c) problem dimension $d$ for all methods under test with $K=d$ and $M=100$ . "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "4 Numerical experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We now present experiments with Gaussian data, MNIST denoising, MRI, and FFHQ face inpainting.   \nAdditional implementation and training details for each experiment are provided in Appendix D. ", "page_idx": 4}, {"type": "text", "text": "4.1 Recovering synthetic Gaussian data ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Here our goal is to recover $\\mathbf{\\boldsymbol{x}}\\sim\\mathcal{N}(\\boldsymbol{\\mu}_{\\times},\\boldsymbol{\\Sigma}_{\\times})\\in\\mathbb{R}^{d}$ from $\\pmb{y}=\\pmb{M}\\pmb{x}+\\pmb{w}\\in\\mathbb{R}^{d}$ , where $_M$ masks $\\textbf{\\em x}$ at even indices and noise $\\pmb{w}\\sim\\mathcal{N}(\\mathbf{0},\\sigma^{2}\\pmb{I})$ is independent of $\\textbf{\\em x}$ with $\\sigma^{2}\\,=\\,0.001$ . Since $\\textbf{\\em x}$ and $\\textit{\\textbf{y}}$ are jointly Gaussian, the posterior is Gaussian posterior with $\\pmb{\\mu}_{\\times|\\upgamma}=\\pmb{\\mu}_{\\times}+\\pmb{\\Sigma}_{\\times\\mathsf{y}}\\pmb{\\Sigma}_{\\mathsf{y}}^{-1}(\\pmb{\\mathscr{y}}-\\pmb{\\mu}_{\\mathsf{y}})$ and $\\pmb{\\Sigma}_{\\times|\\mathsf{y}}=\\pmb{\\Sigma}_{\\times}-\\pmb{\\Sigma}_{\\times\\mathsf{y}}\\pmb{\\Sigma}_{\\mathsf{y}}^{-1}\\pmb{\\Sigma}_{\\mathsf{y}\\times}$ , where $\\mu_{\\mathrm{x}},\\mu_{\\mathrm{y}},\\Sigma_{\\mathrm{x}},\\Sigma_{\\mathrm{y}}$ are marginal and $\\Sigma_{\\mathrm{xy}},\\Sigma_{\\mathrm{yx}}$ are joint statistics. ", "page_idx": 4}, {"type": "text", "text": "We generate random $\\pmb{\\mu}_{\\times}\\sim\\mathcal{N}(\\mathbf{0},\\pmb{I})$ and $\\Sigma_{\\times}$ with half-normal eigenvalues $\\lambda_{k}$ (see additional details in App. A), and consider a sequence of problem sizes $d\\,=\\,10,20,30,\\dots,100$ . For each $d$ , we generate 70 000 training, 20 000 validation, and 10 000 test samples. The generator and discriminator are simple multilayer perceptrons (see App. D.1) trained for 100 epochs with $K=d$ , $E_{\\mathrm{evec}}=10$ , $\\beta_{\\mathsf{a d v}}=\\mathsf{\\dot{1}0^{-5}}$ , and $\\bar{\\beta}_{\\mathsf{p c a}}=10^{-2}$ . ", "page_idx": 4}, {"type": "text", "text": "Competitors. We compare the proposed pcaGAN to rcGAN [19] and NPPC [31]. rcGAN uses the same generator and discriminator architectures as pcaGAN and is trained according to (6) with $\\beta_{\\mathsf{a d v}}=10^{-5}$ and $P_{\\sf r c}=2$ . For NPPC, we use the authors\u2019 implementation [36] with $K=d$ and some minor modifications to work with vector data. To evaluate performance, we use the Wasserstein-2 (W2) distance between $p_{\\times|\\mathsf{y}}$ and $\\widehat{p_{\\times|y}}$ , which in the Gaussian case reduces to ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{W}_{2}(p_{\\times|\\mathbf{y}},\\widehat{p_{\\times|\\mathbf{y}}})=\\|\\mu_{\\times|\\mathbf{y}}-\\widehat{\\mu_{\\times|\\mathbf{y}}}\\|_{2}^{2}+\\mathrm{tr}\\left[\\Sigma_{\\times|\\mathbf{y}}+\\widehat{\\Sigma_{\\times|\\mathbf{y}}}-2(\\Sigma_{\\times|\\mathbf{y}}^{1/2}\\widehat{\\Sigma_{\\times|\\mathbf{y}}}\\Sigma_{\\times|\\mathbf{y}}^{1/2})^{1/2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "For the cGANs, we compute $\\widehat{\\mu_{\\times|y}}$ and $\\widehat{\\Sigma_{\\times\\vert y}}$ empirically from $10d$ samples, while for NPPC we use the conditional mean, eigenvalues, and eigenvectors returned by the approach. ", "page_idx": 4}, {"type": "text", "text": "Results. Figure 1a examines the impact of the lazy update period $M$ on pcaGAN\u2019s W2 distance at $d=100$ with $K=d$ . Based on this figure, to balance performance with training overhead, we set $M=100$ for all future experiments. Figure 1b examines the impact of $K$ on W2 distance for the pcaGAN with $d=100$ . It shows that using $K<d$ causes a relatively mild increase in W2 distance, as expected due to the half-normal distribution on the true eigenvalues $\\lambda_{k}$ . Figure 1c shows that the proposed pcaGAN outperforms rcGAN and NPPC in W2 distance for all problem sizes $d$ . ", "page_idx": 4}, {"type": "text", "text": "4.2 MNIST denoising ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Now our goal is to recover an MNIST digit $\\pmb{x}\\in[0,1]^{28\\times28}$ from noisy measurements ${\\pmb y}={\\pmb x}+{\\pmb w}$ with $w\\sim\\mathcal{N}(\\mathbf{0},I)$ . We randomly split the MNIST training fold into 50 000 training and $10\\,000$ validation images, and we use the entire MNIST fold set for testing. For pcaGAN and rcGAN, we use a UNet [37] generator and the encoder portion of the same UNet followed by one dense layer as the discriminator. pcaGAN was trained for 125 epochs with $E_{\\mathrm{evec}}=25$ , $\\beta_{\\mathsf{a d v}}=\\mathsf{\\dot{1}0^{-5}}$ , $\\beta_{\\mathsf{p c a}}=\\mathsf{\\dot{1}0^{-1}}$ , and $K\\in\\{5,10\\}$ . ", "page_idx": 4}, {"type": "table", "img_path": "Z0Nq3hHeEG/tmp/a31faaccecb7ca1e0d4f941ad6cd8970b423cc96dc72fddf191add5ed6115959.jpg", "table_caption": ["Table 1: Average MNIST denoising results. "], "table_footnote": [], "page_idx": 5}, {"type": "image", "img_path": "Z0Nq3hHeEG/tmp/86c92038d8a8e4521964c3d828ba5767765750ad6b8905025e1e6cb15aaff8ff.jpg", "img_caption": ["Figure 2: For (a) pcaGAN and (b) NPPC, this figure shows the true image $\\textbf{\\em x}$ , noisy measurements $\\textit{\\textbf{y}}$ , the conditional mean $\\widehat{\\mu_{\\times|y}}$ , principal eigenvectors $\\{\\widehat{\\pmb{v}}_{k}\\}$ , and two perturbations of $\\widehat{\\mu_{\\times|y}}$ . "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Competitors. We again compare the proposed pcaGAN to rcGAN and NPPC. For rcGAN we used the same generator and discriminator architectures as pcaGAN and trained according to (6) with $\\beta_{\\mathsf{a d v}}=10^{-5}$ and $P_{\\mathsf{r c}}=2$ . For NPPC, we used the authors\u2019 MNIST implementation from [36]. ", "page_idx": 5}, {"type": "text", "text": "Following the NPPC paper [31], we evaluate performance using root MSE (rMSE) $\\mathrm{E}_{\\times,\\mathrm{y}}\\{\\|x-\\widehat{\\pmb{\\mu}_{\\mathrm{x}|\\mathrm{y}}}\\|_{2}\\}$ and residual error magnitude $\\mathrm{(REM}_{5})\\operatorname{E}_{\\times,\\vee}\\left\\{\\left\\|(I-\\widehat{V}_{5}\\widehat{V}_{5}^{\\top})e\\right\\|_{2}\\right\\}$ , where $e=x-\\widehat{\\mu_{\\mathrm{x}|\\mathrm{y}}}$ and $\\widehat{V}_{5}$ is an $28^{2}\\times5$ matrix whose $k$ th column equals the $k$ th prin cipal eigenvector $\\widehat{\\pmb{v}}_{k}$ . For the cGANs, we use $\\widehat{\\pmb{\\mu}_{\\times|y}}=\\widehat{\\pmb{x}}_{(P)}$ and compute $\\{\\widehat{\\pmb{v}}_{k}\\}$ from the SVD of a matrix of centered  samples $\\{\\widehat{\\pmb{x}}_{i}\\}_{i=1}^{P}$ , both with $P=100$ . For NPPC, we use the conditional means and eigenvectors returned by the approach. For performance evaluation, we also consider Conditional Frechet Inception Distance (CFID) [38] with InceptionV3 features. CFID is analogous to Frechet Inception Distance (FID) [39] but applies to conditional distributions (see App. B for more details). ", "page_idx": 5}, {"type": "text", "text": "Results. Table 1 shows rMSE, $\\mathrm{REM_{5}}$ , CFID, and the reconstruction time for a batch of 128 images on the test fold. (NPPC does not generate image samples and so CFID does not apply.) The table shows that the proposed pcaGAN wins in all metrics, except for rMSE where NPPC wins. This is not surprising because NPPC computes $\\widehat{\\mu_{\\times|y}}$ using a dedicated network trained to minimize MSE loss. NPPC also generates its eigenvectors slightly quicker than pcaGAN generates samples. Table 1 also shows that pcaGAN performance improves as $K$ increases from 5 to 10, despite the fact that $\\mathrm{REM_{5}}$ uses only the top 5 eigenvectors. Figure E.1 shows examples of the 5 principal eigenvectors and posterior mean learned by pcaGAN and NPPC. The eigenvectors of pcaGAN are more structured and less noisy than those of NPPC. Figure E.1 also shows $\\widehat{\\pmb{\\mu}_{\\times|y}}\\!+\\!\\alpha\\pmb{v}_{k}$ for $\\alpha\\in[-3,3]$ and $k\\in\\{1,4\\}$ Additional figures can be found in App. E.1. ", "page_idx": 5}, {"type": "text", "text": "4.3 Accelerated MRI ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We now consider accelerated MRI, where the goal is to recover a complex-valued multicoil image $\\textbf{\\em x}$ from masked frequency-domain (i.e., \u201ck-space\u201d) measurements $\\textit{\\textbf{y}}$ . To build the image data $\\left\\{x_{t}\\right\\}$ , we follow the approach in the rcGAN paper [19], which uses the first 8 slices of all fastMRI [40] T2 brain volumes with at least 8 coils, crops to $384\\times384$ pixels, and compresses to 8 virtual coils [41]. This yields 12 200 training, 2 376 testing, and 784 validation images. To create each $\\pmb{y}_{t}$ , we transform $\\pmb{x}_{t}$ to the $\\mathbf{k}$ -space, subsample using the Cartesian GRO mask [42] at accelerations $R=4$ and $R=8$ , and transform the zero-filled $\\mathbf{k}$ -space measurements back to the image domain. ", "page_idx": 5}, {"type": "image", "img_path": "Z0Nq3hHeEG/tmp/c5275fa16d9092e16b9a6527def38904056a041bc4315f6addd6124075422191.jpg", "img_caption": ["Figure 3: Example MRI recoveries at $R=8$ . Arrows highlight meaningful variations. "], "img_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "Z0Nq3hHeEG/tmp/28429db516b68babe705e461c11a0babb6f914c926af3e5eabbe5ca8794d3f0b.jpg", "table_caption": ["Table 2: Average MRI results at acceleration $R\\in\\{4,8\\}$ "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "We train pcaGAN for 100 epochs with $K=1$ , $E_{\\mathrm{evec}}=25\\$ , $\\beta_{\\mathsf{a d v}}=10^{-5}$ , and $\\beta_{\\mathsf{p c a}}=10^{-2}$ and select the final model using validation CFID computed with VGG-16 features. ", "page_idx": 6}, {"type": "text", "text": "Competitors. We compare the proposed pcaGAN to rcGAN [19], pscGAN [29], Adler & O\u00a8ktem\u2019s cGAN [16], the Langevin approach [22], and the E2E-VarNet [43]. All cGANs use the generator and discriminator architectures from [19] and enforce data-consistency [44]. For rcGAN and the Langevin approach, we did not modify the authors\u2019 implementations from [45] and [46] except to use the GRO sampling mask. For E2E-VarNet, we use the GRO mask, hyperparameters, and training procedure from [22]. ", "page_idx": 6}, {"type": "text", "text": "Following [19], we convert the multicoil outputs ${\\widehat{\\pmb{x}}}_{i}$ to complex-valued images using SENSE-based coil combining [47] with ESPIRiT-estimated [48 ] coil sensitivity maps, and compute performance on magnitude images. All feature-based metrics (CFID, FID, LPIPS, DISTS) were computed with AlexNet features to show that pcaGAN does not overfit to the VGG-16 features used for validation. It was shown in [49] that image-quality metrics computed using ImageNet-trained feature generators like AlexNet and VGG-16 perform comparably to metrics computed using MRI-trained feature generators in terms of correlation with radiologists\u2019 scores. ", "page_idx": 6}, {"type": "text", "text": "Results. Table 2 shows CFID, FID, APSD \u225c( P1 iP=1N1 \u2225x (P )\u2212x i\u22252)1/2, and 4-sample generation time for the methods under test. Due to its slow sample-generation time, we evaluate the CFID, FID, and APSD of the Langevin technique [22] using the 72-image test from [19]. But due to the bias of CFID at small sample sizes [38], we evaluate the other methods using all 2 376 test images $(\\mathrm{CFID^{2}})$ and again using all 14 576 training and test images $(\\mathrm{CFID^{3}})$ ). Table 2 shows that pcaGAN yields better CFID and FID than the competitors. All cGANs generated samples 3\u20134 orders-of-magnitude faster than the Langevin approach [22]. ", "page_idx": 6}, {"type": "table", "img_path": "Z0Nq3hHeEG/tmp/a70fb80ca9ba0a7cd0106b14b392f264cd53103d3e46c96c364651aa7af982dc.jpg", "table_caption": ["Table 3: Average PSNR, SSIM, LPIPS, and DISTS of $\\widehat{\\pmb{x}}_{(P)}$ versus $P$ for MRI at $R=8$ "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "Z0Nq3hHeEG/tmp/587c2ad4c0fda14e1385485487caf3098f4d684b4e0e09005f9644e1012bb221.jpg", "table_caption": ["Table 4: Average FFHQ inpainting results. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 3 shows PSNR, SSIM, LPIPS [50], and DISTS [51] for the $P$ -sample average $\\widehat{\\pmb{x}}_{(P)}$ at $P\\in$ $\\{1,2,4,8,16,32\\}$ and $R=8$ . (See App. C for $R=4.$ ) It has been shown that DIST  S correlates particularly well with radiologist scores [52]. The E2E-VarNet achieves the best PSNR, but the proposed cGAN achieves the best LPIPS and DISTS when $P=2$ and the best SSIM when $P=8$ This $P$ -dependence is related to the perception-distortion trade-off [53] and consistent with that reported in [19]. ", "page_idx": 7}, {"type": "text", "text": "Figure 3 shows zoomed versions of two recoveries ${\\widehat x}_{i}$ and the sample average $\\widehat{\\pmb{x}}_{(P)}$ with $P=32$ . Appendices E.2 and E.3 show additional plots of $\\widehat{\\pmb{x}}_{(P)}$ that visually demonstrate the perceptiondistortion trade-off. ", "page_idx": 7}, {"type": "text", "text": "4.4 Large-scale inpainting ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Our final goal is to inpaint a face image with a large randomly generated masked region. For this task, we use $256\\times256$ FFHQ face images [54] and the mask generation procedure from [17]. We randomly split the FFHQ training fold into $45\\,000$ training and 5 000 validation images, and we use the remaining 20 000 images for testing. ", "page_idx": 7}, {"type": "text", "text": "For pcaGAN, we use CoModGAN\u2019s [17] generator and discriminator architecture and train for 100 epochs using $K=2$ , $E_{\\mathrm{evec}}=25$ , $\\beta_{\\mathsf{a d v}}=\\bar{5}\\times10^{-3}$ , and $\\beta_{{\\tt p c a}}=10^{-3}$ . ", "page_idx": 7}, {"type": "text", "text": "Competitors. We compare with CoModGAN [17], pscGAN [29], rcGAN [19], and state-of-theart diffusion methods DDRM (20 NFEs) [25], DDNM (100 NFEs) [27], and DPS (1000 NFEs) [26]. CoModGAN, pscGAN, and rcGAN differ from pcaGAN only in generator regularization and CoModGAN\u2019s use of discriminator MBSD [55]. For rcGAN, DDNM, DDRM, and DPS, we use the authors\u2019 implementations from [45], [56], [57], and [58] with mask generation from [17]. FID and CFID were evaluated on our 20 000 image test set with $P\\!=\\!1$ . ", "page_idx": 7}, {"type": "text", "text": "Results. Table 4 shows test CFID, FID, LPIPS, and 40-sample generation time. The table shows that the proposed pcaGAN wins in CFID, FID and LPIPS, and that the four cGANs generate samples 3\u20134 orders-of-magnitude faster than DPS. Figure 4 shows five generated samples for each method under test, along with the true and masked image. pcaGAN shows better subjective quality than the competitors, as well as good diversity. Additional figures can be found in App. E.4. ", "page_idx": 7}, {"type": "image", "img_path": "Z0Nq3hHeEG/tmp/99bef4a9557fcb47ba2fe3c216bb7bbd2ff2ab3c67f6dd826b10860b8447dd8f.jpg", "img_caption": ["Figure 4: Example of inpainting a randomly generated mask on a $256\\!\\times\\!256$ FFHQ face image. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "5 Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "When training a cGAN, the overall goal is that the samples $\\{\\widehat{\\pmb{x}}_{i}\\}$ generated from a particular $\\textit{\\textbf{y}}$ accurately represent the true posterior $p_{\\times|y}(\\cdot|y)$ . Achieving this goal is challenging when training from paired data $\\{(\\pmb{x}_{t},\\pmb{y}_{t})\\}$ , because such datasets provide only one example of $\\textbf{\\em x}$ for each given $\\textit{\\textbf{y}}$ . Early methods like [15\u201317] focused on providing some variation among $\\left\\{\\widehat{\\pmb{x}}_{i}\\right\\}$ , but did not aim for the correct variation. The rcGAN from [19] focused on providing the correct amount of variation by enforcing $\\operatorname{tr}(\\Sigma_{\\widehat{\\mathsf{x}}|\\mathsf{y}})=\\operatorname{tr}(\\Sigma_{\\mathsf{x}|\\mathsf{y}})$ , and the proposed pcaGAN goes farther by encouraging $\\Sigma_{\\widehat{\\times}|\\mathsf{y}}$ and $\\Sigma_{\\times|y}$ to agree along $K$ principal directions. Our experiments demonstrate that pcaGAN yields a notable improvement over rcGAN and outperforms contemporary diffusion approaches like DPS [26]. ", "page_idx": 8}, {"type": "text", "text": "PCA principles have also been used in unconditional GANs, where the goal is to train a generator $G_{\\theta}$ that turns codes $z\\sim\\mathcal{N}(\\mathbf{0},I)$ into outputs $\\widehat{\\pmb{x}}=G_{\\pmb{\\theta}}(\\pmb{z})$ that match the true marginal distribution $p_{\\times}$ from which the training samples $\\left\\{x_{t}\\right\\}$ are d ra wn. For example, the eigenGAN from [59] aims to train in such a way that semantic attributes are learned (without supervision) and can be independently controlled by manipulating individual entries of $_{\\textit{z}}$ . But their goal is clearly different from ours. ", "page_idx": 8}, {"type": "text", "text": "Limitations. We acknowledge several limitations of our work. First, generating $P_{\\mathsf{p c a}}\\,=\\,10K$ samples during training can impose a burden on memory when $\\textbf{\\em x}$ is high dimensional. In the multicoil MRI experiment, $\\mathbf{\\pmb{x}}\\in\\breve{\\mathbb{R}}^{d}$ for $d=2.4\\mathrm{e}6$ , which limited us to $K=1$ at batch size 2. Second, although our focus is on designing a fast and accurate posterior sampler, more work is needed on how to best use the generated samples across different applications. Using them to compute rigorous uncertainty ", "page_idx": 8}, {"type": "text", "text": "intervals seems like a promising direction [60\u201362]. Third, the application to MRI is preliminary;   \nadditional tuning and validation is needed before it can be considered for clinical practice. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we proposed pcaGAN, a novel image-recovery cGAN that enforces correctness in the $K$ principal components of the conditional covariance matrix $\\Sigma_{\\widehat{\\times}|\\mathsf{y}}$ , as well as in the conditional mean $\\pmb{\\mu}_{\\widehat{\\times}|\\mathsf{y}}$ and trace-covariance $\\operatorname{tr}(\\Sigma_{\\widehat{\\sf x}|{\\sf y}})$ . Experiments with synthetic Gaussian data showed pcaGAN outperforming both rcGAN [19] and NPPC [31] in Wasserstein-2 distance across a range of problem sizes. Experiments on MNIST denoising, accelerated multicoil MRI, and large-scale image inpainting showed pcaGAN outperforming several other cGANs and diffusion models in CFID, FID, PSNR, SSIM, LPIPS, and DISTS metrics. Furthermore, pcaGAN generates samples 3\u20134 orders-of-magnitude faster than the tested diffusion models. The proposed pcaGAN thus provides fast and accurate posterior sampling for image recovery problems, which enables uncertainty quantification, fairness in recovery, and easy navigation of the perception/distortion trade-off. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The authors are funded in part by the National Institutes of Health under grant R01-EB029957. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] C. Belthangady and L. A. Royer, \u201cApplications, promises, and pitfalls of deep learning for fluorescence image reconstruction,\u201d Nature Methods, vol. 16, no. 12, pp. 1215\u20131225, 2019. 1   \n[2] D. P. Hoffman, I. Slavitt, and C. A. Fitzpatrick, \u201cThe promise and peril of deep learning in microscopy,\u201d Nature Methods, vol. 18, no. 2, pp. 131\u2013132, 2021. 1   \n[3] M. J. Muckley, B. Riemenschneider, A. Radmanesh, S. Kim, G. Jeong, J. Ko, Y. Jun, H. Shin, D. Hwang, M. Mostapha, et al., \u201cResults of the 2020 fastMRI challenge for machine learning MR image reconstruction,\u201d IEEE Trans. Med. Imag., vol. 40, no. 9, pp. 2306\u20132317, 2021. 1   \n[4] S. Bhadra, V. A. Kelkar, F. J. Brooks, and M. A. Anastasio, \u201cOn hallucinations in tomographic image reconstruction,\u201d IEEE Trans. Med. Imag., vol. 40, no. 11, pp. 3249\u20133260, 2021. 1 [5] N. M. Gottschling, V. Antun, A. C. Hansen, and B. Adcock, \u201cThe troublesome kernel\u2014On hallucinations, no free lunches and the accuracy-stability trade-off in inverse problems,\u201d arXiv:2001.01258, 2023. 1   \n[6] M. Abdar, F. Pourpanah, S. Hussain, D. Rezazadegan, L. Liu, M. Ghavamzadeh, P. Fieguth, X. Cao, A. Khosravi, U. R. Acharya, et al., \u201cA review of uncertainty quantification in deep learning: Techniques, applications and challenges,\u201d Info. Fusion, vol. 76, pp. 243\u2013297, 2021. 1   \n[7] B. Lambert, F. Forbes, S. Doyle, H. Dehaene, and M. Dojat, \u201cTrustworthy clinical AI solutions: A unified review of uncertainty quantification in deep learning models for medical image analysis,\u201d Artificial Intelligence in Medicine, p. 102830, 2024. 1   \n[8] L. Ardizzone, C. L\u00a8uth, J. Kruse, C. Rother, and U. K\u00a8othe, \u201cGuided image generation with conditional invertible neural networks,\u201d arXiv:1907.02392, 2019. 1, 2   \n[9] C. Winkler, D. Worrall, E. Hoogeboom, and M. Welling, \u201cLearning likelihoods with conditional normalizing flows,\u201d arXiv:1912.00042, 2019. 1, 2   \n[10] H. Sun and K. L. Bouman, \u201cDeep probabilistic imaging: Uncertainty quantification and multi-modal solution characterization for computational imaging,\u201d in Proc. AAAI Conf. Artificial Intell., vol. 35, pp. 2628\u20132637, 2021. 1, 2   \n[11] J. Wen, R. Ahmad, and P. Schniter, \u201cA conditional normalizing flow for accelerated multi-coil MR imaging,\u201d in Proc. Intl. Conf. Mach. Learn., 2023. 1, 2   \n[12] V. Edupuganti, M. Mardani, S. Vasanawala, and J. Pauly, \u201cUncertainty quantification in deep MRI reconstruction,\u201d IEEE Trans. Med. Imag., vol. 40, pp. 239\u2013250, Jan. 2021. 1   \n[13] F. Tonolini, J. Radford, A. Turpin, D. Faccio, and R. Murray-Smith, \u201cVariational inference for computational imaging inverse problems,\u201d J. Mach. Learn. Res., vol. 21, no. 179, pp. 1\u201346, 2020. 1   \n[14] K. Sohn, H. Lee, and X. Yan, \u201cLearning structured output representation using deep conditional generative models,\u201d in Proc. Neural Info. Process. Syst. Conf., 2015. 1   \n[15] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros, \u201cImage-to-image translation with conditional adversarial networks,\u201d in Proc. IEEE Conf. Comp. Vision Pattern Recog., pp. 1125\u20131134, 2017. 1, 2, 9 [16] J. Adler and O. O\u00a8ktem, \u201cDeep Bayesian inversion,\u201d arXiv:1811.05910, 2018. 1, 2, 7, 8, 9, 14 [17] S. Zhao, J. Cui, Y. Sheng, Y. Dong, X. Liang, E. I.-C. Chang, and Y. Xu, \u201cLarge scale image completion via co-modulated generative adversarial networks,\u201d in Proc. Intl. Conf. Learn. Rep., 2021. 1, 2, 8, 9, 16 [18] H. Zhao, H. Li, S. Maurer-Stroh, and L. Cheng, \u201cSynthesizing retinal and neuronal images with generative adversarial nets,\u201d Med. Image Analysis, vol. 49, 07 2018. 1, 2 [19] M. Bendel, R. Ahmad, and P. Schniter, \u201cA regularized conditional GAN for posterior sampling in inverse problems,\u201d in Proc. Neural Info. Process. Syst. Conf., 2023. 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 13, 14, 15, 16 [20] M. Welling and Y. W. Teh, \u201cBayesian learning via stochastic gradient Langevin dynamics,\u201d in Proc. Intl. Conf. Mach. Learn., pp. 681\u2013688, 2011. 1, 2 [21] Y. Song and S. Ermon, \u201cImproved techniques for training score-based generative models,\u201d in Proc. Neural Info. Process. Syst. Conf., 2020. 1, 2 [22] A. Jalal, M. Arvinte, G. Daras, E. Price, A. Dimakis, and J. Tamir, \u201cRobust compressed sensing MRI with deep generative priors,\u201d in Proc. Neural Info. Process. Syst. Conf., 2021. 1, 2, 7, 8, 14, 15 [23] Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole, \u201cScore-based generative modeling through stochastic differential equations,\u201d in Proc. Intl. Conf. Learn. Rep., 2021. 1, 2 [24] Y. Song, L. Shen, L. Xing, and S. Ermon, \u201cSolving inverse problems in medical imaging with score-based generative models,\u201d in Proc. Intl. Conf. Learn. Rep., 2022. 1, 2 [25] B. Kawar, M. Elad, S. Ermon, and J. Song, \u201cDenoising diffusion restoration models,\u201d in Proc. Neural Info. Process. Syst. Conf., 2022. 1, 2, 8 [26] H. Chung, J. Kim, M. T. McCann, M. L. Klasky, and J. C. Ye, \u201cDiffusion posterior sampling for general noisy inverse problems,\u201d in Proc. Intl. Conf. Learn. Rep., 2023. 1, 2, 8, 9, 16 [27] Y. Wang, J. Yu, and J. Zhang, \u201cZero-shot image restoration using denoising diffusion null-space model,\u201d in Proc. Intl. Conf. Learn. Rep., 2023. 1, 2, 8 [28] G. Ohayon, T. J. Adrai, M. Elad, and T. Michaeli, \u201cReasons for the superiority of stochastic estimators over deterministic ones: Robustness, consistency and perceptual quality,\u201d in Proc. Intl. Conf. Mach. Learn., pp. 26474\u201326494, 2023. 1 [29] G. Ohayon, T. Adrai, G. Vaksman, M. Elad, and P. Milanfar, \u201cHigh perceptual quality image denoising with a posterior sampling CGAN,\u201d in Proc. IEEE Intl. Conf. Comput. Vis. Workshops, vol. 10, pp. 1805\u20131813,   \n2021. 2, 3, 7, 8, 14 [30] S. Man, G. Ohayon, T. Adrai, and M. Elad, \u201cHigh-perceptual quality JPEG decoding via posterior sampling,\u201d Proc. IEEE Conf. Comp. Vision Pattern Recog. Workshop, 2023. 2 [31] E. Nehme, O. Yair, and T. Michaeli, \u201cUncertainty quantification via neural posterior principal components,\u201d in Proc. Neural Info. Process. Syst. Conf., 2023. 2, 5, 6, 10, 15 [32] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. Courville, \u201cImproved training of Wasserstein GANs,\u201d in Proc. Neural Info. Process. Syst. Conf., p. 5769\u20135779, 2017. 2 [33] I. T. Jolliffe, Principal Component Analysis, vol. 2. New York: Springer Verlag, 2002. 3 [34] T. Karras, S. Laine, M. Aittala, J. Hellsten, J. Lehtinen, and T. Aila, \u201cAnalyzing and improving the image quality of StyleGAN,\u201d in Proc. IEEE Conf. Comp. Vision Pattern Recog., pp. 8110\u20138119, 2020. 3, 16 [35] D. P. Kingma and J. Ba, \u201cAdam: A method for stochastic optimization,\u201d in Proc. Intl. Conf. Learn. Rep.,   \n2015. 5, 15 [36] E. Nehme, O. Yair, and T. Michaeli, \u201cUncertainty quantification via neural posterior principal components.\u201d Downloaded from https://github.com/EliasNehme/NPPC, Jan. 2023. 5, 6, 15 [37] O. Ronneberger, P. Fischer, and T. Brox, \u201cU-Net: Convolutional networks for biomedical image segmentation,\u201d in Proc. Intl. Conf. Med. Image Comput. Comput. Assist. Intervent., pp. 234\u2013241, 2015.   \n6 [38] M. Soloveitchik, T. Diskin, E. Morin, and A. Wiesel, \u201cConditional Frechet inception distance,\u201d arXiv:2103.11521, 2021. 6, 7, 13 [39] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter, \u201cGANs trained by a two time-scale update rule converge to a local Nash equilibrium,\u201d in Proc. Neural Info. Process. Syst. Conf., vol. 30, 2017.   \n6, 13 [40] J. Zbontar, F. Knoll, A. Sriram, M. J. Muckley, M. Bruno, A. Defazio, M. Parente, K. J. Geras, J. Katsnelson, H. Chandarana, Z. Zhang, M. Drozdzal, A. Romero, M. Rabbat, P. Vincent, J. Pinkerton, D. Wang, N. Yakubova, E. Owens, C. L. Zitnick, M. P. Recht, D. K. Sodickson, and Y. W. Lui, \u201cfastMRI: An open dataset and benchmarks for accelerated MRI,\u201d arXiv:1811.08839, 2018. 6   \n[41] T. Zhang, J. M. Pauly, S. S. Vasanawala, and M. Lustig, \u201cCoil compression for accelerated imaging with Cartesian sampling,\u201d Magn. Reson. Med., vol. 69, no. 2, pp. 571\u2013582, 2013. 7   \n[42] M. Joshi, A. Pruitt, C. Chen, Y. Liu, and R. Ahmad, \u201cTechnical report (v1.0)\u2013pseudo-random cartesian sampling for dynamic MRI,\u201d arXiv:2206.03630, 2022. 7   \n[43] A. Sriram, J. Zbontar, T. Murrell, A. Defazio, C. L. Zitnick, N. Yakubova, F. Knoll, and P. Johnson, \u201cEnd-to-end variational networks for accelerated MRI reconstruction,\u201d in Proc. Intl. Conf. Med. Image Comput. Comput. Assist. Intervent., pp. 64\u201373, 2020. 7, 8, 14, 15   \n[44] C. K. S\u00f8nderby, J. Caballero, L. Theis, W. Shi, and F. Husz\u00b4ar, \u201cAmortised MAP inference for image super-resolution,\u201d in Proc. Intl. Conf. Learn. Rep., 2017. 7   \n[45] M. Bendel, R. Ahmad, and P. Schniter, \u201cA regularized conditional GAN for posterior sampling in inverse problems.\u201d Downloaded from https://github.com/matt-bendel/rcGAN, May 2023. 7, 8, 13   \n[46] A. Jalal, M. Arvinte, G. Daras, E. Price, A. Dimakis, and J. Tamir, \u201ccsgm-mri-langevin.\u201d https:// github.com/utcsilab/csgm-mri-langevin, 2021. Accessed: 2021-12-05. 7, 15   \n[47] K. P. Pruessmann, M. Weiger, M. B. Scheidegger, and P. Boesiger, \u201cSENSE: Sensitivity encoding for fast MRI,\u201d Magn. Reson. Med., vol. 42, no. 5, pp. 952\u2013962, 1999. 7   \n[48] M. Uecker, P. Lai, M. J. Murphy, P. Virtue, M. Elad, J. M. Pauly, S. S. Vasanawala, and M. Lustig, \u201cESPIRiT\u2013an eigenvalue approach to autocalibrating parallel MRI: Where SENSE meets GRAPPA,\u201d Magn. Reson. Med., vol. 71, no. 3, pp. 990\u20131001, 2014. 7   \n[49] P. M. Adamson, A. D. Desai, J. Dominic, C. Bluethgen, J. P. Wood, A. B. Syed, R. D. Boutin, K. J. Stevens, S. Vasanawala, J. M. Pauly, A. S. Chaudhari, and B. Gunel, \u201cUsing deep feature distances for evaluating MR image reconstruction quality,\u201d in Proc. Neural Info. Process. Syst. Workshop, 2023. 7   \n[50] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, \u201cThe unreasonable effectiveness of deep features as a perceptual metric,\u201d in Proc. IEEE Conf. Comp. Vision Pattern Recog., pp. 586\u2013595, 2018. 8, 13   \n[51] K. Ding, K. Ma, S. Wang, and E. P. Simoncelli, \u201cImage quality assessment: Unifying structure and texture similarity,\u201d IEEE Trans. Pattern Anal. Mach. Intell., vol. 44, no. 5, pp. 2567\u20132581, 2020. 8, 13   \n[52] S. Kastryulin, J. Zakirov, N. Pezzotti, and D. V. Dylov, \u201cImage quality assessment for magnetic resonance imaging,\u201d IEEE Access, vol. 11, pp. 14154\u201314168, 2023. 8   \n[53] Y. Blau and T. Michaeli, \u201cThe perception-distortion tradeoff,\u201d in Proc. IEEE Conf. Comp. Vision Pattern Recog., pp. 6228\u20136237, 2018. 8   \n[54] T. Karras, S. Laine, and T. Aila, \u201cA style-based generator architecture for generative adversarial networks,\u201d in Proc. IEEE Conf. Comp. Vision Pattern Recog., pp. 4396\u20134405, 2019. 8   \n[55] T. Karras, T. Aila, S. Laine, and J. Lehtinen, \u201cProgressive growing of GANs for improved quality, stability, and variation,\u201d in Proc. Intl. Conf. Learn. Rep., 2018. 8, 14, 15   \n[56] Y. Wang, \u201cDDNM.\u201d Downloaded from https://github.com/wyhuai/DDNM, Sept. 2023. 8, 16   \n[57] B. Kawar, M. Elad, S. Ermon, and J. Song, \u201cDenoising diffusion restoration models.\u201d Downloaded from https://github.com/bahjat-kawar/ddrm, May 2022. 8, 16   \n[58] H. Chung, J. Kim, M. T. McCann, M. L. Klasky, and J. C. Ye, \u201cdiffusion-posterior-sampling.\u201d https: //github.com/DPS2022/diffusion-posterior-sampling, Mar. 2023. 8, 16   \n[59] Z. He, M. Kan, and S. Shan, \u201cEigenGAN: Layer-wise eigen-learning for GANs,\u201d in Proc. IEEE Intl. Conf. Comput. Vis., pp. 14408\u201314417, 2021. 9   \n[60] A. N. Angelopoulos, A. P. Kohli, S. Bates, M. I. Jordan, J. Malik, T. Alshaabi, S. Upadhyayula, and Y. Romano, \u201cImage-to-image regression with distribution-free uncertainty quantification and applications in imaging,\u201d in Proc. Intl. Conf. Mach. Learn., 2022. 10   \n[61] J. Teneggi, M. Tivnan, J. W. Stayman, and J. Sulam, \u201cHow to trust your diffusion model: A convex optimization approach to conformal risk control,\u201d 2023. 10   \n[62] D. Narnhofer, A. Habring, M. Holler, and T. Pock, \u201cPosterior-variance-based error quantification for inverse problems in imaging,\u201d SIAM J. Imag. Sci., vol. 17, no. 1, pp. 301\u2013333, 2024. 10   \n[63] Y. Zeng, \u201cco-mod-gan-pytorch.\u201d Downloaded from https://github.com/zengxianyu/ co-mod-gan-pytorch, Sept. 2022. 16 ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Supplementary Materials ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A Synthetic Gaussian priors ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Algorithm 2 captures how we construct the Gaussian priors used in Sec. 4.1. ", "page_idx": 12}, {"type": "table", "img_path": "Z0Nq3hHeEG/tmp/dde03bc4903f77c9ae9f4da306d10b088c5822d77b737589ca9288f2e96da555.jpg", "table_caption": [], "table_footnote": [], "page_idx": 12}, {"type": "text", "text": "We begin with dimension $d\\,=\\,100$ , generating random mean \u00b5(x100) \u2208 R100 and eigenvalues {\u03bb(k100)}1k0=01. To construct the eigenvectors {v(k10 $\\{\\pmb{v}_{k}^{(100)}\\}_{k=1}^{100}$ , we perform a QR decomposition on a $100\\!\\times\\!100$ matrix with i.i.d. $\\mathcal{N}(0,1)$ entries and set $\\pmb{v}_{k}^{(100)}$ as the $k$ th column of $Q$ . For each remaining $d\\in\\{90,80,\\ldots,10\\}$ , we construct $\\mu_{\\times}^{(d)}$ , $\\{\\lambda_{k}^{(d)}\\}$ , and $u_{k}^{(d)}$ by truncating the previous quantities to ensure some level of continuity across $d$ . ", "page_idx": 12}, {"type": "text", "text": "B Conditional Fre\u00b4chet inception distance ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We quantify posterior-approximation accuracy using conditional Fr\u00b4echet inception distance (CFID) [38], which approximates the conditional Wasserstein distance ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathrm{CWD}\\triangleq\\mathrm{E}_{\\mathbf{y}}\\{W_{2}(p_{\\times|\\mathbf{y}}(\\cdot,\\mathbf{y}),p_{\\widehat{\\mathbf{x}}|\\mathbf{y}}(\\cdot,\\mathbf{y}))\\}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "In (B.1), $W_{2}(p_{\\mathsf{a}},p_{\\mathsf{b}})$ denotes the Wasserstein-2 distance between distributions $p_{\\mathsf{a}}$ and $p_{\\mathsf{b}}$ , defined as ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{W_{2}(p_{\\mathsf{a}},p_{\\mathsf{b}})\\triangleq\\underset{p_{\\mathsf{a},\\mathsf{b}}\\in\\Pi(p_{\\mathsf{a}},p_{\\mathsf{b}})}{\\operatorname*{min}}\\,\\mathrm{E}_{\\mathsf{a},\\mathsf{b}}\\{\\|a-b\\|_{2}^{2}\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $\\begin{array}{r}{\\Pi(p_{\\mathsf{a}},p_{\\mathsf{b}})\\triangleq\\{p_{\\mathsf{a},\\mathsf{b}}:p_{\\mathsf{a}}=\\int p_{\\mathsf{a},\\mathsf{b}}\\,\\mathrm{d}b}\\end{array}$ and $\\begin{array}{r}{p_{\\mathsf{b}}=\\int p_{\\mathsf{a,b}}\\,\\mathrm{d}a\\Big\\}}\\end{array}$ denotes the set of joint distributions $p_{\\mathsf{a},\\mathsf{b}}$ with marginal distributions $p_{\\mathsf{a}}$ and $p_{\\mathsf{b}}$ . Similarly to how FID [39] is computed for marginal distributions, CFID approximates (B.1) for conditional distributions. In particular, the random vectors $x,{\\widehat{x}}$ , and $\\textit{\\textbf{y}}$ are replaced by low-dimensional embeddings $\\underline{{x}},\\widehat{\\underline{{x}}}.$ , and $\\pmb{y}$ , generated by the convolutional layers of a deep network (e.g., InceptionV3 or VGG-16 or AlexNet), and the embedding distributions $p_{\\underline{{\\times}}|\\underline{{y}}}$ and $p_{\\Xi|\\underline{{y}}}$ are approximated by multivariate Gaussians. We compute CFID using the code from [45], which is described in [19]. ", "page_idx": 12}, {"type": "text", "text": "C Additional MRI results for acceleration $R=4$ ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Table 5 shows PSNR, SSIM, LPIPS [50], and DISTS [51] for the $P$ -sample average $\\widehat{\\pmb{x}}_{(P)}$ at $P\\in$ $\\{1,2,4,8,16,32\\}$ for $R=4$ . In this case, the E2E-VarNet attains the best PSNR and SSIM, while pcaGAN performs best in LPIPS and DISTS when $P=2$ and $P=1$ , respectively. These results, ", "page_idx": 12}, {"type": "table", "img_path": "Z0Nq3hHeEG/tmp/eac74a66791776daaaa2414ae576c97aadbbd7f91d5b2af5c8cb9fa9bb817390.jpg", "table_caption": ["Table 5: Average PSNR, SSIM, LPIPS, and DISTS of $\\widehat{\\pmb{x}}_{(P)}$ versus $P$ for $R=4$ MRI "], "table_footnote": [], "page_idx": 13}, {"type": "image", "img_path": "Z0Nq3hHeEG/tmp/49ed8542f8c3a25253ec16a986aa28a7868e45e3536049e1c1330dc17762b547.jpg", "img_caption": ["Figure C.1: Example MRI recoveries at $R=4$ . Arrows highlight meaningful variations. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "in conjunction with the $R=8$ results discussed in Sec. 4.3, show that pcaGAN yields a notable improvement over rcGAN in all metrics. ", "page_idx": 13}, {"type": "text", "text": "Figure C.1 shows zoomed versions of two recoveries ${\\widehat{\\pmb{x}}}_{i}$ and the sample average $\\widehat{\\pmb{x}}_{(P)}$ with $P=32$ . ", "page_idx": 13}, {"type": "text", "text": "D Implementation details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In each experiment, all cGANs were trained using the Adam optimizer with a learning rate of $10^{-3}$ , $\\beta_{1}=0$ , and $\\beta_{2}=0.99$ as in [55]. ", "page_idx": 13}, {"type": "text", "text": "D.1 Synthetic Gaussian recovery ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "cGAN training. We choose $\\beta_{\\mathsf{a d v}}=10^{-5}$ , $n_{\\mathsf{b a t c h}}=64$ , $P_{\\sf r c}=2$ , and train for 100 epochs for both rcGAN and pcaGAN. Running PyTorch on a server with 4 Tesla A100 GPUs, each with $82\\,\\mathrm{GB}$ of memory, the cGAN training for $d=100$ takes approximately 8 hours, with training time decreasing with smaller $d$ . For pcaGAN, we choose $K=d$ for each $d$ in this experiment (unless otherwise noted) and $\\beta_{\\mathsf{p c a}}=10^{\\mathsf{-2}}$ . ", "page_idx": 13}, {"type": "text", "text": "cGAN architecture. We exploit the Gaussian nature of the problem, constructing $G_{\\theta}$ with two dense layers; one which takes in $\\textit{\\textbf{y}}$ as input and one which takes in $_{z}$ as input. The output of each layer is added, yielding $\\widehat{\\mathbf{\\xi}}^{x}$ . Similarly, $D_{\\phi}$ is comprised of a single dense layer which takes in the concatenation of $x/\\widehat{\\mathbf{\\xi}}^{}$ and $\\textit{\\textbf{y}}$ and outputs a scalar. We use this architecture for both rcGAN and pcaGAN. Note that there is no listed license for rcGAN. ", "page_idx": 13}, {"type": "text", "text": "NPPC. For NPPC, we use the suggested hyperparameters from [31] and opt to train the MMSE network before training NPPC. We use the suggested architectures from their Gaussian denoising experiment and train for 100 epochs with $n_{\\sf b a t c h}=64$ . We leverage the authors\u2019 implementation in [36], modifying it for this Gaussian problem. There is no listed license for NPPC. ", "page_idx": 14}, {"type": "text", "text": "D.2 MNIST denoising ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The MNIST dataset is available under the GNU general public license, which we respect through our use. ", "page_idx": 14}, {"type": "text", "text": "cGAN training. We choose $\\beta_{\\mathsf{a d v}}=10^{-5}$ , $n_{\\mathsf{b a t c h}}=64$ , $P_{\\sf r c}=2$ , and train for 125 epochs for both rcGAN and pcaGAN. Running PyTorch on a server with 4 Tesla A100 GPUs, each with $82\\,\\mathrm{GB}$ of memory, the cGAN training for $d=100$ takes approximately 8 hours, with training time decreasing with smaller $d$ . For pcaGAN, we train two models, one with $K=5$ and one with $K=10$ . In both cases, $\\beta_{\\mathsf{p c a}}=10^{-1}$ , $E_{\\mathrm{evec}}=25$ , and $E_{\\mathrm{eval}}=50$ . ", "page_idx": 14}, {"type": "text", "text": "cGAN architecture. For both cGANs, the generator is the standard UNet which takes in the concatenation of $\\textit{\\textbf{y}}$ and code $_{\\textit{z}}$ . The network consists of 3 pooling layers and 32 initial channels. The convolutions use a kernel of size $3\\times3$ , instance normalization, and leaky ReLU activations with a negative slope of 0.1. For the encoder portion of the UNet, we use max pooling with a kernel size of $2\\times2$ to reduce spatial resolution by a factor of 2. For the decoder portion of the UNet, we use nearest-neighbor interpolation to increase spatial resolution by a factor of 2. The discriminator is simply the encoder portion of the UNet with an additional dense layer appended to map the UNet\u2019s latent space to a scalar output. Note that there is no listed license for rcGAN. ", "page_idx": 14}, {"type": "text", "text": "NPPC. For NPPC, we do not modify the authors\u2019 implementation in [36] in any way. We first train the MMSE reconstruction network and then train the NPPC network. There is no listed license for NPPC. ", "page_idx": 14}, {"type": "text", "text": "D.3 Accelerated MRI ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "For our MRI experiments, we use the fastMRI dataset which is available under the MIT license, which we respect through our use. ", "page_idx": 14}, {"type": "text", "text": "cGAN training. We choose $\\beta_{\\mathsf{a d v}}=10^{-5}$ , $\\beta_{{\\tt p c a}}=10^{-2}$ , $n_{\\mathsf{b a t c h}}=2$ , $P_{\\sf r c}=2$ , $K=1$ , $E_{\\mathrm{evec}}=25$ and $E_{\\mathrm{eval}}=50$ for pcaGAN. For rcGAN, pscGAN , and Adler\u2019s cGAN, we use the hyperparameters and training procedure described in [19]. All models were trained for 100 epochs using the Adam optimizer [35] with a learning rate of $10^{-3}$ , $\\beta_{1}=0$ , and $\\beta_{2}=0.99$ , as in [55]. Running PyTorch on a server with 4 Tesla A100 GPUs, each with $82\\,\\mathrm{GB}$ of memory, the training of each MRI cGAN took approximately 1 day. Note that there is no listed license for rcGAN, pscGAN, or Adler and O\u00a8ktem\u2019s cGAN. ", "page_idx": 14}, {"type": "text", "text": "cGAN architecture. All four cGANs used the same generator and discriminator architectures described in [19], except that Adler and O\u00a8ktem\u2019s discriminator used extra input channels to facilitate the 3-input loss. ", "page_idx": 14}, {"type": "text", "text": "E2E-VarNet. For the Sriram et al.\u2019s E2E-VarNet [43], we use the same training procedure and hyperparameters outlined in [22] with modification to the sampling pattern as in [19]. As in [22], we use the SENSE-based coil-combined image as the ground truth instead of the RSS image. The E2E-VarNet is available under the MIT license, which we respect. ", "page_idx": 14}, {"type": "text", "text": "Langevin approach. For Jalal et al.\u2019s MRI approach [22], we do not modify the authors\u2019 implementation from [46] other than replacing the default sampling pattern with the GRO sampling mask. We borrow both generated samples and results from [19]. The authors\u2019 code is available under the MIT license, which we respect. ", "page_idx": 14}, {"type": "text", "text": "D.4 FFHQ Inpainting ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "For our inpainting experiment, we use the FFHQ dataset which is available under the Creative Commons BY-NC-SA 4.0 license, which we respect through our use. ", "page_idx": 14}, {"type": "text", "text": "cGAN training. We choose $\\beta_{\\mathsf{a d v}}\\,=\\,5\\,\\times\\,10^{-3}$ , $\\beta_{\\mathsf{p c a}}\\,=\\,10^{-3}$ , $n_{\\mathsf{b a t c h}}\\,=\\,5$ , $P_{\\sf r c}\\;=\\;2$ , $K\\,=\\,2$ , $E_{\\mathrm{evec}}=25$ , and $E_{\\mathrm{eval}}=50$ for pcaGAN. Running PyTorch on a server with 4 Tesla A100 GPUs, each with $82\\,\\mathrm{GB}$ of memory, the training of our cGAN took approximately 1.5 days. ", "page_idx": 15}, {"type": "text", "text": "cGAN architecture. As in [19], we use the CoModGAN networks from [17] which extend the StyleGAN2 [34] network. The StyleGAN2 architecture is available under the NVIDIA Source Code License, which we respect. ", "page_idx": 15}, {"type": "text", "text": "rcGAN. We follow the training procedure outlined in [19], only modifying the inpainting mask to be random. The total training time on a server with 4 NVIDIA A100 GPUs, each with $82\\:\\mathrm{GB}$ of memory, is roughly 1 day. There is no listed license for rcGAN. ", "page_idx": 15}, {"type": "text", "text": "pscGAN. We use the same training procedure outlined in [19], modifying the inpainting masks to be random and using the $\\mathcal{L}_{2,P}$ objective described briefly in Sec. 2 with $P\\!=\\!8$ . The total training time on a server with 4 NVIDIA A100 GPUs, each with 82 GB of memory, is roughly 1.5 days. There is no listed license for pscGAN. ", "page_idx": 15}, {"type": "text", "text": "CoModGAN. We use the PyTorch implementation of CoModGAN from [63] and train the model. The total training time on a server with 4 NVIDIA A100 GPUs, each with 82 GB of memory, is roughly 1 day. There is no listed license for CoModGAN, beyond the NVIDIA Source Code License. ", "page_idx": 15}, {"type": "text", "text": "D.4.1 Diffusion Methods ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "For all three diffusion methods, we use the pretrained weights from [26]. ", "page_idx": 15}, {"type": "text", "text": "DPS. We use the suggested settings for the $256\\times256$ FFHQ dataset and the code from the official PyTorch implementation [58]. We found the LPIPS-minimizing step-size $\\zeta$ via grid search over a 1000 image validation set. We generate 1 sample for all 20 000 images in our test set, using a batch-size of 1 and 1000 NFEs. The total generation time on a server with 4 NVIDIA A100 GPUs, each with 82 GB of memory, is roughly 9 days. There is no listed license for DPS. ", "page_idx": 15}, {"type": "text", "text": "DDNM. We use the code from the official PyTorch implementation [56]. We generate 1 sample for all 20 000 images in our test set, using a batch-size of 1 and 100 NFEs. The total generation time on a server with 4 NVIDIA A100 GPUs, each with 82 GB of memory, is roughly 1.5 days. There is no listed license for DDNM. ", "page_idx": 15}, {"type": "text", "text": "DDRM. We use the code from the official PyTorch implementation [57]. We generate 1 sample for all 20 000 images in our test set, using a batch-size of 1 and 20 NFEs. The total generation time on a server with 4 NVIDIA A100 GPUs, each with 82 GB of memory, is roughly 5.5 hours. There is no listed license for DDRM. ", "page_idx": 15}, {"type": "text", "text": "E Additional reconstruction plots ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "E.1 MNIST denoising ", "text_level": 1, "page_idx": 16}, {"type": "image", "img_path": "Z0Nq3hHeEG/tmp/0d9603051e0e76a30728a69e12437f0bea21d2dba620ac72da202998496d1ec3.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure E.1: For (a) pcaGAN and (b) NPPC, this figure shows the true image $\\textbf{\\em x}$ , noisy measurements $\\textit{\\textbf{y}}$ , the conditional mean $\\widehat{\\mu_{\\times|y}}$ , principal eigenvectors $\\{\\widehat{\\pmb{v}}_{k}\\}$ , and two perturbations of $\\widehat{\\mu_{\\times|y}}$ . ", "page_idx": 16}, {"type": "image", "img_path": "Z0Nq3hHeEG/tmp/e9e55a83818fdc55e198d4fa8bea2e5cdd9de5d24f03de0f202a9d6ce42cea63.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure E.2: For (a) pcaGAN and (b) NPPC, this figure shows the true image $\\textbf{\\em x}$ , noisy measurements $\\textit{\\textbf{y}}$ , the conditional mean $\\widehat{\\mu_{\\times|y}}$ , principal eigenvectors $\\{\\widehat{\\pmb{v}}_{k}\\}$ , and two perturbations of $\\widehat{\\mu_{\\times|y}}$ . ", "page_idx": 16}, {"type": "image", "img_path": "Z0Nq3hHeEG/tmp/77733dfe87554f4106bcb3551889cdc925715398b655b4205c65a0bf6cba0a35.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure E.3: For (a) pcaGAN and (b) NPPC, this figure shows the true image $\\textbf{\\em x}$ , noisy measurements $\\textit{\\textbf{y}}$ , the conditional mean $\\widehat{\\mu_{\\times|y}}$ , principal eigenvectors $\\{\\widehat{\\pmb{v}}_{k}\\}$ , and two perturbations of $\\widehat{\\mu_{\\times|y}}$ . ", "page_idx": 16}, {"type": "image", "img_path": "Z0Nq3hHeEG/tmp/7abd786d524aeac7c3b5652629d30ed9fcd6cc45d28029e0a9c34daa1c1cfb02.jpg", "img_caption": ["Figure E.4: Example $R=4$ MRI reconstruction. Row one: pixel-wise SD with $P=32$ , Row two: $\\widehat{\\pmb{x}}_{(P)}$ with $P=32$ , Row three: $\\widehat{\\pmb{x}}_{(P)}$ with $P=4$ , Row four: $\\widehat{\\pmb{x}}_{(P)}$ with $P=2$ , Rows five and six: posterior samples. The arrows indicate regions of meaningful variation across posterior samples. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "Z0Nq3hHeEG/tmp/592808b06d37e758bf7754b562d6bbcbce8407969e7e6d98695ef9082268af39.jpg", "img_caption": ["Figure E.5: Example $R=4$ MRI reconstruction. Row one: pixel-wise SD with $P=32$ , Row two: $\\widehat{\\pmb{x}}_{(P)}$ with $P=32$ , Row three: $\\widehat{\\pmb{x}}_{(P)}$ with $P=4$ , Row four: $\\widehat{\\pmb{x}}_{(P)}$ with $P=2$ , Rows five and six: posterior samples. The arrows indicate regions of meaningful variation across posterior samples. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "Z0Nq3hHeEG/tmp/2e1c4d30896c1d99fbade364e6c09835550565c67c87419f9239995321a36ef6.jpg", "img_caption": ["Figure E.6: Example $R=8$ MRI reconstruction. Row one: pixel-wise SD with $P=32$ , Row two: $\\widehat{\\pmb{x}}_{(P)}$ with $P=32$ , Row three: $\\widehat{\\pmb{x}}_{(P)}$ with $P=4$ , Row four: $\\widehat{\\pmb{x}}_{(P)}$ with $P=2$ , Rows five and six: posterior samples. The arrows indicate regions of meaningful variation across posterior samples. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "Z0Nq3hHeEG/tmp/68f3d20a054b822d4c8e796ce95c1138d996765b1e3ce45762a1ad64daf592d0.jpg", "img_caption": ["Figure E.7: Example $R=8$ MRI reconstruction. Row one: pixel-wise SD with $P=32$ , Row two: $\\widehat{\\pmb{x}}_{(P)}$ with $P=32$ , Row three: $\\widehat{\\pmb{x}}_{(P)}$ with $P=4$ , Row four: $\\widehat{\\pmb{x}}_{(P)}$ with $P=2$ , Rows five and six: posterior samples. The arrows indicate regions of meaningful variation across posterior samples. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "E.4 Inpainting ", "text_level": 1, "page_idx": 21}, {"type": "image", "img_path": "Z0Nq3hHeEG/tmp/9b384d84c3b24b4b333d2f120c7e6975f73fcc08ae418f69f2b66a187f3286e4.jpg", "img_caption": ["Figure E.8: Example of inpainting a randomly generated mask on a $256\\!\\times\\!256$ FFHQ face image. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "Z0Nq3hHeEG/tmp/84cb5d9492e0bda238743d128ef88bda050624e7a4747314c51e1fac5da8379f.jpg", "img_caption": ["Figure E.9: Example of inpainting a randomly generated mask on a $256\\!\\times\\!256$ FFHQ face image. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "Z0Nq3hHeEG/tmp/25c468d840bf730612a9d434ea6586fa722a88ef8bab647959d7168d71a6dfdc.jpg", "img_caption": ["Figure E.10: Example of inpainting a randomly generated mask on a $256\\!\\times\\!256$ FFHQ face image. "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "Z0Nq3hHeEG/tmp/1c2884da07438b60b316217ee28897b6f0e309e61add6a142e8b7a7cc74ceee0.jpg", "img_caption": ["Figure E.11: Example of inpainting a randomly generated mask on a $256\\!\\times\\!256$ FFHQ face image. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The contribution and results described in the abstract and introduction are detailed in Section 3 and Section 4, respectively. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 25}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: The limitations are discussed in Section 5. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \u201dLimitations\u201d section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 25}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: We do not provide any theoretical results in this work. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Experimental settings are described in Section 4 with additional details given in Appendix D. Should this work be accepted, we will release our source code and link to it in the camera-ready submission. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 26}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Upon acceptance, we will release source code to reproduce each experiment. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 27}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Experimental settings are described in Section 4 with additional details given in Appendix D. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 27}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [No] ", "page_idx": 27}, {"type": "text", "text": "Justification: Error bars are omitted due to computational constraints. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \u201dYes\u201d if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g., negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Compute resources are described in Appendix D. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 28}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: This research abides by the NeurIPS Code of Ethics. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 28}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The broader impacts are discussed in the Introduction (our method facilitates uncertainty quantification, fairness, and perception-distortion trade-offs) and MRI-specific impacts are discussed in Section 5. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 28}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: This paper does not pose such a risk. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Licenses are discussed in Appendix D and are properly respected. Credit is given for the various data/code/models we use throughout the paper when appropriate. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Upon release, the source code will be well-documented with instructions for reproducing the experimental results and extending the code. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 30}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: This research did not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 30}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: This research did not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}]