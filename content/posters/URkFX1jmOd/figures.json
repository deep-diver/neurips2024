[{"figure_path": "URkFX1jmOd/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration of our motivation. (a) The disentanglement process leverages physical priors. (b) The image patches are restored individually for each degradation type. (c) The proposed Disentangled Regularization improves the overall performance.", "description": "This figure illustrates the core idea of the proposed method. The left panel shows the process of disentangling different degradation types (darkness, well-lit, highlights, and light effects) from a nighttime image using physical priors.  The middle panel visually represents how the disentanglement process leads to individual restoration of image patches based on the specific degradation type. The right panel presents a visual comparison to demonstrate the improved performance achieved by the proposed method, highlighting the superior quality of image restoration.", "section": "1 Introduction"}, {"figure_path": "URkFX1jmOd/figures/figures_2_1.jpg", "caption": "Figure 2: The overall architecture of the proposed N2D3 method. The training phase contains the physical prior informed degradation disentanglement module and degradation-aware contrastive learning module. They are utilized to optimize the ResNet-based generator which is the main part in the inference phase.", "description": "This figure illustrates the architecture of the Night-to-Day via Degradation Disentanglement (N2D3) method. It shows two phases: inference and training. The inference phase uses a ResNet-based generator to translate nighttime images to daytime images.  The training phase consists of two modules: a physical prior informed degradation disentanglement module and a degradation-aware contrastive learning module. These modules work together to optimize the generator and produce high-quality daytime images by considering various degradation patterns in nighttime images.", "section": "Methods"}, {"figure_path": "URkFX1jmOd/figures/figures_3_1.jpg", "caption": "Figure 3: The first row displays nighttime images, while the second row shows the corresponding degradation disentanglement results. The color progression from blue, light blue, green to yellow corresponds to the following regions: darkness, well-lit, light effects, and high-light, respectively.", "description": "This figure shows examples of nighttime images and their corresponding degradation disentanglement maps. The disentanglement process separates the image into four regions based on the illumination conditions: darkness, well-lit, light effects, and highlights. Each region is assigned a specific color in the disentanglement map, allowing for easy identification of different illumination characteristics within the image.", "section": "3.1 Physical Priors for Nighttime Environment"}, {"figure_path": "URkFX1jmOd/figures/figures_7_1.jpg", "caption": "Figure 4: The qualitative comparison results on the Alderley dataset.", "description": "This figure shows a qualitative comparison of night-to-day image translation results on the Alderley dataset.  It compares the original nighttime images to the results produced by several different methods, including CycleGAN, ToDayGAN, ForkGAN, CUT, Decent, Santa, and the proposed N2D3 method. The comparison highlights the differences in detail preservation, artifact reduction, and overall visual quality achieved by each method. The red boxes highlight areas of particular interest and comparison.", "section": "4.2 Results on Alderley"}, {"figure_path": "URkFX1jmOd/figures/figures_7_2.jpg", "caption": "Figure 4: The qualitative comparison results on the Alderley dataset.", "description": "This figure shows a qualitative comparison of night-to-day image translation results on the Alderley dataset.  It compares the performance of several methods, including CycleGAN, ToDayGAN, ForkGAN, CUT, Decent, Santa, and the proposed N2D3 method.  Each row shows the same nighttime input image and its corresponding daytime output from each method. The results demonstrate how the different methods handle various aspects of nighttime image translation, such as color correction, detail preservation, and artifact reduction. The proposed N2D3 method aims to produce visually superior and more realistic results by focusing on disentangling different types of illumination degradations.", "section": "4.2 Results on Alderley"}, {"figure_path": "URkFX1jmOd/figures/figures_8_1.jpg", "caption": "Figure 6: The quantitative results of ablation on the number of patches of the degradation-aware sampling.", "description": "This figure shows the results of an ablation study conducted to determine the optimal number of patches used in the degradation-aware sampling component of the N2D3 model.  The graphs display FID and LPIPS scores for the Alderley and BDD100K datasets across different numbers of sampling patches (64, 128, 256, 512, 1024).  It demonstrates that an intermediate number of patches achieves the best performance, indicating a trade-off between computational cost and performance.", "section": "4.4 Ablation Study"}, {"figure_path": "URkFX1jmOd/figures/figures_12_1.jpg", "caption": "Figure 3: The first row displays nighttime images, while the second row shows the corresponding degradation disentanglement results. The color progression from blue, light blue, green to yellow corresponds to the following regions: darkness, well-lit, light effects, and high-light, respectively.", "description": "This figure shows a comparison between nighttime images and their corresponding degradation disentanglement results. Each image in the first row shows different nighttime scenes, while the second row shows the corresponding processed images after the degradation disentanglement module has been applied. The color coding (blue, light blue, green, and yellow) highlights different regions based on their degradation types: darkness, well-lit, light effects, and high-light, respectively.  This illustrates how the model separates different degradation patterns within a nighttime image.", "section": "3.1 Physical Priors for Nighttime Environment"}, {"figure_path": "URkFX1jmOd/figures/figures_13_1.jpg", "caption": "Figure 3: The first row displays nighttime images, while the second row shows the corresponding degradation disentanglement results. The color progression from blue, light blue, green to yellow corresponds to the following regions: darkness, well-lit, light effects, and high-light, respectively.", "description": "This figure shows nighttime images and their corresponding disentanglement maps. The maps segment the images into four regions based on illumination: darkness, well-lit, light effects, and high-light. Each region is represented by a different color in the disentanglement map: blue for darkness, light blue for well-lit, green for light effects, and yellow for high-light. This visualization demonstrates how the proposed method disentangles the various illumination degradations in nighttime images, which is a crucial step in the Night-to-Day image translation process.", "section": "3.1 Physical Priors for Nighttime Environment"}, {"figure_path": "URkFX1jmOd/figures/figures_13_2.jpg", "caption": "Figure 9: Qualitative comparison abalation results.", "description": "This figure shows a qualitative comparison of ablation studies on the Alderley dataset.  It compares the results of the full N2D3 model against versions missing key components: one without the illumination invariant N, and another without the degradation-aware reweighting. The comparison highlights the contribution of each module to the overall quality of the nighttime-to-daytime image translation.", "section": "4.4 Ablation Study"}, {"figure_path": "URkFX1jmOd/figures/figures_14_1.jpg", "caption": "Figure 4: The qualitative comparison results on the Alderley dataset.", "description": "This figure shows a qualitative comparison of Night-to-Day translation results on the Alderley dataset.  It compares the original nighttime images with results from several different methods, including CycleGAN, ToDayGAN, ForkGAN, CUT, Decent, Santa, and the proposed N2D3 method. The comparison highlights the strengths and weaknesses of each method in terms of preserving details, color accuracy, and overall visual quality.", "section": "4.2 Results on Alderley"}, {"figure_path": "URkFX1jmOd/figures/figures_16_1.jpg", "caption": "Figure 11: More qualitative comparison results on the BDD100K dataset.", "description": "This figure presents a qualitative comparison of night-to-day translation results on the BDD100K dataset.  It shows the original nighttime images along with the results generated by several different methods including CycleGAN, ToDayGAN, ForkGAN, CUT, Decent, Santa, and the proposed N2D3 method.  The comparison aims to highlight the visual quality and details preserved by each method, showcasing the strengths and weaknesses of different approaches in handling complex nighttime scenes.", "section": "4.3 Results on BDD100K"}]