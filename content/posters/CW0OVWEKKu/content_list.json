[{"type": "text", "text": "Exploring and Exploiting the Asymmetric Valley of Deep Neural Networks ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xin-Chun Li1,2, Jin-Lin Tang1,2, Bo Zhang1,2, Lan ${\\bf L i}^{1,2}$ , De-Chuan Zhan1,2 ", "page_idx": 0}, {"type": "text", "text": "1 School of Artificial Intelligence, Nanjing University, Nanjing, China 2 National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China {lixc, tangjl, zhangb, lil}@lamda.nju.edu.cn, zhandc@nju.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Exploring the loss landscape offers insights into the inherent principles of deep neural networks (DNNs). Recent work suggests an additional asymmetry of the valley beyond the flat and sharp ones, yet without thoroughly examining its causes or implications. Our study methodically explores the factors affecting the symmetry of DNN valleys, encompassing (1) the dataset, network architecture, initialization, and hyperparameters that influence the convergence point; and (2) the magnitude and direction of the noise for 1D visualization. Our major observation shows that the degree of sign consistency between the noise and the convergence point is a critical indicator of valley symmetry. Theoretical insights from the aspects of ReLU activation and softmax function could explain the interesting phenomenon. Our discovery propels novel understanding and applications in the scenario of Model Fusion: (1) the efficacy of interpolating separate models significantly correlates with their sign consistency ratio, and (2) imposing sign alignment during federated learning emerges as an innovative approach for model parameter alignment. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The massive number of parameters and complex structure of deep neural networks (DNNs) have catalyzed extensive research to mine their underlying mechanics [54, 33, 41]. Visualizing and exploring the loss surfaces of DNNs is the most intuitive way [42, 18], which has ignited many interesting findings, such as the monotonic linear interpolation [22, 19, 64] and linear mode connectivity [22, 14, 21, 15, 63, 2]. Loss landscape visualization has also been applied to show the optimization trajectory [52, 32, 34], understand the effectiveness of Batch Normalization [31, 60], BERT [11, 23], deep ensemble [17, 29, 21], and so on. ", "page_idx": 0}, {"type": "text", "text": "Perturbation analysis around the local minima of DNNs [7, 61], i.e., the shape of the valleys they reside in, is a very popular research topic. The concept of flat minima was originally proposed by [27], who defines the size of the connected region around the minima where the loss remains relatively unchanged as flatness. Subsequent studies debate whether the flat or sharp minima could reflect the generalization ability [37, 42, 35, 56, 16, 12, 40, 3]. The previous works constrain the valley shape to be symmetric, while recent work points out that not all DNN valleys are flat or sharp, and there also exist asymmetric valleys [24], which has not been systematically studied as far as we know. ", "page_idx": 0}, {"type": "text", "text": "This paper in-depth analyzes the factors that may affect the valley symmetry of DNNs. Previous work\u2019s analysis of valley shape primarily utilizes the 1D interpolation of $\\theta_{f}+\\lambda\\epsilon$ , where $\\theta_{f}$ represents the minima solution and $\\epsilon$ denotes a random noise. As shown in Fig. 1,  w+e believe that the valley symmetry depends both on the convergence solution and noise, with each of them being influenced by some factors. The most significant innovation in our research is considering the effect of noise direction on valley visualization, as previous work has simply taken the Gaussian noise. ", "page_idx": 0}, {"type": "image", "img_path": "CW0OVWEKKu/tmp/58d57875ab87542fad7b076e6078f363933501f37185640e9f16811677c13c3d.jpg", "img_caption": ["Figure 2: The illustration of different visualization methods for 1D visualization. The norm-scaled noise unifies the magnitude of various noise without changing their directions. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: The illustration of investigated factors that could affect the valley symmetry. The $\\epsilon$ matters a lot. ", "page_idx": 1}, {"type": "text", "text": "Specifically, we start by comprehensively and carefully determining the visualization method to plot the valley shape, which could influence the conclusion significantly [30, 42, 40]. We finally take the Norm-Scaled (NS) visualization method [30] that normalizes the noise $\\epsilon$ and further scales it to $\\lVert\\theta_{f}\\rVert$ for better determining the plot range of $\\lambda$ , where the direction of raw noise is not changed. Th \u2223e\u2223n, \u2223a\u2223fter investigating 7 common noise directions and 6 special ones, we conclude: the degree of sign consistency between the noise and the convergence solution should be a determining factor for asymmetry. This phenomenon is basically insensitive to the utilized datasets. Next, we focus on the impact of the network architecture with or without Batch Normalization (BN) [31], indicating that the BN initialization also impacts the valley symmetry. Finally, different hyperparameters lead to solutions with various valley widths but show no asymmetry consistently. ", "page_idx": 1}, {"type": "text", "text": "Aside from empirical observations, theoretical insights for our interesting findings are provided. We first declare that adding sign-consistent noise to parameters may have a larger probability of keeping activating the neurons or keeping the overwhelming score in classification tasks. Then, we show that the trace of the Hessian matrix along the sign-consistent direction is smaller, implying a flatter region. The above findings inspire applications in the fields of model fusion [46, 66, 32, 2]. This paper first explains why model aggregation based on pre-trained models often leads to performance improvements, i.e., the success of model soups [66], and then proposes constraining the sign of DNN parameters in federated learning [53, 70, 49] to facilitate aggregation. ", "page_idx": 1}, {"type": "text", "text": "Our novel contributions can be summarized as (1) exploring the valley shape under different noise directions that have not been studied yet; (2) proposing that the flat region could be expanded along the direction that has a higher sign consistency with the convergence solution; (3) pointing out the influence of BN and its initialization on valley symmetry; (4) presenting theoretical insights to explain our interesting finding; (5) explaining and inspiring effective algorithms in model fusion. ", "page_idx": 1}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Exploring the Valley Shape of DNNs. The valley around a minima solution has originally been viewed as flat or sharp [27], and the large batch size training may lead to sharp minima with poor generalization [37]. However, [12] declares that two solutions that are scale-invariant in performance may lie in regions with significantly different flatness. [42] verifies the findings of [12] by applying varying weight decay to small and large batch training, leading to results contrary to [37]. With the filter-normalized plots, [42] again observes that sharpness correlates well with generalization error. Later work also shows that the sharpness calculation should be dependent on the parameter scale [40] or the dataset [3]. Whether valley width reflects the generalization is still inconclusive. The proposal of asymmetric valley [24] further throws this debate into limbo, as valleys around minima could be flat on one side but sharp on the other side, which makes it more difficult to define flatness. This paper thoroughly studies the causes and implications of the unexplained asymmetry phenomenon. ", "page_idx": 1}, {"type": "text", "text": "Exploiting the Valley Shape of DNNs. Exploring the valley shape of DNNs could help us better understand the inherent principles of DNNs. The work [42] utilizes 2D surface plots to show that the residual connections in ResNet [26] could prevent the explosion of non-convexity when networks get deep, and [60] attributes the success of BN to its effects of making landscape significantly more smooth. The asymmetric valley [24] provides a sounding explanation for the intriguing phenomenon in stochastic weight averaging [32]. Additionally, studying the valley shape could also benefit the proposal of effective optimization algorithms, e.g., the entropy-based SGD [6], and the (adaptive) sharpness-aware minimization [16, 40]. Penalizing the gradient could also lead to solutions around flat regions [71]. We also apply the findings in this paper to the area of model fusion. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Model Fusion. Directly averaging two independent models may encounter a barrier due to training randomness or permutation invariance [4, 22, 2, 50]. However, if the two models are generated from separate training of a common pre-trained model, the model fusion may perform better than individual models, i.e., the model soups [57, 66]. A recent work [68] finds that resolving sign conflicts when merging multiple task-specific models is neccessary, which is most related to our current work. We will explain the success of model soups based on the relation between the asymmetric valley and the sign consistency of model parameters. Popular federated learning algorithms also take the parameter averaging process to fuse the individual models updated on isolated clients [53, 51]. A huge challenge is the Non-Independent and Identical Data distributions of data islands (Non-I.I.D. data) [70, 28], which could make local models too diverged to merge. Multiple regularization methods are proposed to align parameters before model fusion [45, 1, 44, 36]. We propose an effective regularization method that focuses on the sign of parameters, which is inspired by our interesting findings. ", "page_idx": 2}, {"type": "text", "text": "3 Basic Notations and Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Our major tool is plotting the 1D error curve of DNNs following the formula $\\theta_{f}+\\lambda\\epsilon$ (Fig. 2 (A)).   \n$\\theta_{f}$ denotes the converged model, and $\\epsilon$ denotes a noise vector sampled from a sp e+cific distribution.   \nMore about the visualization of DNN loss landscape could be found in [42, 48]. ", "page_idx": 2}, {"type": "text", "text": "3.1 Previous Studies: Exploring $\\theta_{f}$ with Fixed $\\epsilon$ ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The previous studies focus on studying the valley shape under different $\\theta_{f}$ , and aim to mine the shape\u2019s relation to generalization. To mitigate the influence of parameter scales and make the visualization fairly comparable between different $\\theta_{f}$ , [42] proposes the fliter normalization method to properly visualize the loss landscape. The processing of the noise is $\\epsilon^{i,j}\\leftarrow\\frac{\\epsilon^{i,j}}{\\Vert\\epsilon^{i,j}\\Vert}\\Vert\\theta_{f}^{i,j}\\Vert$ , where $i$ is the index of layer and $j$ is the index of filter. This way normalizes each filter in the noise $\\epsilon$ to have the same norm of the corresponding fliter in the converged point $\\theta_{f}$ . Further, [40] proposes a proper definition of sharpness (i.e., adaptive sharpness) based on the filter normalization, extending it to all parameters and formally defining: $T_{\\theta_{f}}={\\stackrel{\\cdot}{\\operatorname{diag}}}\\left(\\operatorname{concat}\\left(||\\mathbf{f}^{1}||_{2}\\mathbf{I}_{n_{1}},\\ldots,||\\mathbf{f}^{m}||_{2}\\mathbf{I}_{n_{m}},|w^{1}|,\\ldots,|{\\bar{w}}^{q}|\\right)\\right)$ , where $\\mathbf{f}^{j}$ with $1\\leq j\\leq m$ denotes the $j$ -th convolution fliter in $\\theta_{f}$ and $n_{j}$ is the number of parameters it owns. $w^{j}$ with $1\\leq j\\leq q$ denotes the $j$ -th parameter that is not included in any filters. I is a vector with all values as o n\u2264e.  T\u2264hen, the adaptive noise $T_{\\theta_{f}}\\epsilon$ is utilized to study the sharpness of $\\theta_{f}$ . ", "page_idx": 2}, {"type": "text", "text": "3.2 Our Study: Exploring $\\epsilon$ and $\\lambda$ with Fixed $\\theta_{f}$ ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Different from the previous studies, we aim to explore the valley shape of a fixed $\\theta_{f}$ under different $\\epsilon$ and $\\lambda$ . First, the direction of 1D interpolation in previous works is limited to the Gaussian noise, while we study the impact of different noise types. Second, setting $\\lambda$ positive or negative could obtain a valley with different flatness, i.e., the asymmetric valley [24]. Hence, under the fixed $\\theta_{f}$ , we do not need to rectify the noise direction filter-wisely. We take the visualization way used in [30], which only normalizes the noise and then re-scales it to the norm of $\\theta_{f}$ , i.e., $\\epsilon\\leftarrow\\frac{\\epsilon}{||\\epsilon||}||\\theta_{f}||$ . The utilized Norm-Scaled (NS) noise is shown in Fig. 2 (C). Compared with Filter NS noise (Fig. 2 (B)), this way does not change the direction of the noise and shows the original valley shape along the direction \u03f5. Additionally, to plot 1D error curves in the same figure, we fix $\\lambda$ in the range of $[-1,1]$ . Another scale factor $s$ is added to control the visualized width. Overall, we use the followin [g\u2212 way ]to plot 1D error curves: $\\begin{array}{r}{\\theta_{f}+\\lambda*s*\\frac{\\epsilon}{\\|\\epsilon\\|}\\|\\theta_{f}\\|}\\end{array}$ , where $\\lambda\\in[-1,1]$ , and we set $s=1.0$ by default. The Frobenius norm is used. Notably, we utilize the NS noise by default and use the Filter NS noise when comparing the valley shape under different converged points, e.g., the studying of BN initialization in Sect. 4.2.1. ", "page_idx": 2}, {"type": "table", "img_path": "CW0OVWEKKu/tmp/c49d03541b8eaa68853be24a164c049f8d286318eae04298d006ce355c135231.jpg", "table_caption": [], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 3: The valleys under 7 common noise types. The second row shows the results of replacing the sign of noise with that of $\\theta_{f}$ , leading to asymmetric valleys. (VGG16 with BN on CIFAR10) ", "page_idx": 3}, {"type": "text", "text": "4 Experimental Findings ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "This section presents the major findings about factors that respectively impact the noise and converged points. Experimental details and more verification results are in Appendix A and B. ", "page_idx": 3}, {"type": "text", "text": "4.1 Factors that Affect $\\epsilon$ ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We train VGG16 [62] with BN [31] on CIFAR10 [38] for 200 epochs and obtain the converged model. For the given $\\theta_{f}$ , we plot the 1D error curves under the following 7 common noise types: (1) $G(0,1)$ : Gaussian noise with mean as 0 and std as 1; (2) $U(-1,1)$ : uniform noise in the range $[-1,1]$ ; (3)) $\\{-1,0,1\\}$ : uniform noise with values only in $-1,0$ , (an\u2212d 1; )(4) $G(1,1)$ : Gaussian noise w i[t\u2212h me]an as 1{ \u2212and std} as 1; (5) $U(0,1)$ : uniform noise in  t\u2212he range $[0,1]$ ; (6() $\\{0,1\\}$ : uniform noise with values only in 0 and 1; (7) $\\{1\\}$ : constant noise with all values as 1. For each plot, we set $s\\in\\{0.2,1.0,2.0\\}$ and $\\lambda\\in[-1,1]$ to s h{ow} curves under various levels of width. The results are shown i n\u2208 t{he first row o}f Fig. 3 . \u2208W[\u2212e obs]erve that the valley shapes along these 7 noise directions are almost symmetric, except that the last four noise directions show slight asymmetry when $s=0.2$ . ", "page_idx": 3}, {"type": "text", "text": "Then, a fantastic idea motivates us to change the sign of the noise. The detail of this motivation is provided in Appendix A.4. Specifically, we use the following method to replace the sign of $\\epsilon$ with that of $\\theta_{f}$ : $\\epsilon\\gets|\\epsilon|*\\mathrm{sign}(\\theta_{f})$ , where $\\big|\\cdot\\big|$ returns the absolute value element-wisely and $\\mathrm{sign}(\\cdot)$ returns 1 or $-1$ base d\u2190 o\u2223n \u2223w\u2217hethe(r th)e elemen \u2223t \u22c5i\u2223s positive or negative. The corresponding results of (t\u22c5h)e 7 common n\u2212oises become completely asymmetric, which are plotted in the second row of Fig. 3. Furthermore, the valleys all follow the tendency that the positive direction is flat while the negative direction is sharp. Hence, we propose our major finding: the sign consistency between noise and converged model determines the asymmetry and the valley is flatter along the noise direction with a larger sign consistency. The finding is formulated as: $L(\\theta_{f}+a\\eta)<L(\\theta_{f}-a\\eta)$ , where $\\eta=|\\epsilon|*\\mathrm{sign}(\\bar{\\theta}_{f})$ denotes the sign-consistent noise, and $a>0$ is a co(ns ta+nt. $L(\\cdot)$ i(s th e\u2212 loss) function,  =w \u2223hi\u2223c\u2217h coul(d be) the prediction error or cross-entropy loss. The following three experimental studies could further verify this interesting finding. ", "page_idx": 3}, {"type": "text", "text": "The Manual Construction of Noise Direction. We element-wise sample the noise $\\epsilon$ from $G(0,1)$ , and then manually change its elements\u2019 sign with a given ratio $r\\in\\{0.0,0.1,\\ldots,1.0\\}$ . For example, $r=0.5$ means that we sample $50\\%$ elements in the noise and change their sign to the same as $\\theta_{f}$ . T h=en, we plot the average test error of the positive interpolations and negative interpolations, i.e., $\\mathbb{E}_{\\lambda}[\\mathrm{Error}(\\theta_{f}+\\lambda\\mathrm{NS}(\\epsilon))]$ with $\\lambda\\in[0,1]$ and $\\lambda\\in[-1,0]$ , respectively. Fig. 4 plots the average test err[ors on (tw o +group(s o)f) ]network s\u2208 a[nd d]atasets .\u2208 T[h\u2212e tes]t errors of positive and negative directions are nearly equal when $r=0\\%$ . As $r$ becomes larger, the average test error of the positive direction monotonically decreases while the negative one increases, implying that the valley shape becomes more and more asymmetric. ", "page_idx": 3}, {"type": "text", "text": "The Investigation of 6 Special Noise Directions. We then investigate several special noise directions including (1) the initialization before training, i.e., $\\epsilon_{1}=\\theta_{0}$ ; (2) the converged model itself, i.e., $\\epsilon_{2}=\\theta_{f}$ ; (3) $\\epsilon_{\\mathrm{3}}=\\mathrm{sign}(\\theta_{f})$ ; (4) $\\epsilon_{4}=\\mathrm{sign}(\\theta_{f}-\\mu)$ ; (5) $\\epsilon_{5}\\,=\\,\\mathrm{sgp}(\\theta_{f})$ ; (6) $\\epsilon_{6}\\,=\\,\\mathrm{sgp}(\\theta_{f}-\\mu)$ . Here, $\\mu$ de n=otes the m e=an val(ue f)or each  p=arame(ter  \u2212gro)up, e.g.,  t=he m(ean )value o f =\u201cconv(1. w\u2212eig)ht\u201d. $\\operatorname{sgp}(\\cdot)$ returns 1 or 0 based on whether the element is positive or not. The visualization results are pr(\u22c5o)vided in Fig. 5. The first four directions lead to asymmetry, while the last two do not. First, the elements in $\\epsilon_{2}$ and $\\epsilon_{3}$ surely have the same sign with $\\theta_{f}$ , which leads to an asymmetric valley. Because the mean values of most parameters are near zero, $\\epsilon_{4}$ performs likely as $\\epsilon_{3}.\\ \\epsilon_{5}$ and $\\epsilon_{6}$ only have the same sign with the positive parameters in $\\theta_{f}$ , and applies zero to negative parameters in $\\theta_{f}$ , which shows no asymmetry. The most interesting result is the $\\epsilon_{1}=\\theta_{0}$ , whose elements may be centered around zero according to the Kaiming initialization [25]. H owever, the BN initialization is asymmetric, which leads to asymmetric curves (Sect. 4.2.1). The results of VGG11 without BN on CIFAR100 [38] show no asymmetry when $\\epsilon=\\theta_{0}$ (Appendix B, Fig. 17). ", "page_idx": 3}, {"type": "image", "img_path": "CW0OVWEKKu/tmp/47c31738016add869b5ec58c9c68db6867b66631b97fac2226337077f87ea8ca.jpg", "img_caption": ["Figure 4: The impacts of manually constructed Gaus- Figure 5: The valley shape under 6 special noise types. (VGG16 sian noise with different with BN on CIFAR10) levels of sign consistency. "], "img_footnote": [], "page_idx": 4}, {"type": "image", "img_path": "CW0OVWEKKu/tmp/6d011a4e8925a649cf72979802d4ed10e72f7c5a1adf2624d028d4dfa1a4c6b4.jpg", "img_caption": ["Figure 6: Verification results on ImageNet with pre-trained ResNeXt101. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "The Finding Holds for ImageNet and Various Parameter Groups. We extend the findings to large-scale datasets and apply noise only to specific parameter groups. Specifically, we use the pre-trained models (e.g., ResNeXt101 [67]) downloaded from \u201ctorchvision\u201d 1. Because these models are pre-trained on ImageNet [10], we could directly use them to verify our findings without additional training. Multiple parameter groups are considered as follows: (1) \u201cALL\u201d denotes the whole parameters; (2) \u201cCLF\u201d denotes the weights in the final classifier layer; (3) \u201cFEAT\u201d denotes the weights in the layers aside from the final classifier; (4) \u201cLAYER1\u201d denotes parameters in the first several blocks; (5) \u201cCONV\u201d denotes all convolution parameters; (6) \u201cBN\u201d denotes all of the BN parameters. As shown in Fig. 6, applying sign-consistent noise could lead to asymmetric valleys. Notably, this holds for both the metrics of CE loss and prediction error. ", "page_idx": 4}, {"type": "text", "text": "4.2 Factors that Affect $\\theta_{f}$ ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Then we focus on studying the effects of BN and its initialization, then present the results under various hyperparameters. ", "page_idx": 4}, {"type": "image", "img_path": "CW0OVWEKKu/tmp/4598878b8c212bccc964d04b614f8af08179c202cc56bb87ccc320b07db719a0.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 7: The impact of BN and its initialization on the valley symmetry. (VGG16 with BN on CIFAR10) ", "page_idx": 5}, {"type": "image", "img_path": "CW0OVWEKKu/tmp/17346e82ef43920861d7d8cc4f39c95475f06e722b83bbc1b198321afe9719fe.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 8: The interpolation between two models trained with batch size as 32 and 2048. (VGG16 with BN on CIFAR10) ", "page_idx": 5}, {"type": "image", "img_path": "CW0OVWEKKu/tmp/7a8519be180201fc41f52fbdad2392aab5e67e570cda6b3085c4b0c363d0ad28.jpg", "img_caption": ["Figure 9: The impact of various hyperparameters on valley symmetry. (VGG16 with BN on CIFAR10) "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "4.2.1 BN and Initialization ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The default initialization method of BN is setting w as ones and b as zeros [31, 9]. Hence, the w, i.e., the \u201cBN.weight\u201d, may be asymmetric after convergence. We take three ways to initialize the BN weights, including (1) the elements are all ones; (2) the elements are sampled from $U(0,1)$ ; (3) the values sampled from $G(0,0.1)$ . We train three models based on these three types of BN initialization. In Fig. 7, the first row s(hows t)he initial and converged parameter distribution of a specific BN weight. The traditional initialization leads to converged BN weights with all positive values, which are nearly centered around 0.2. The uniform initialization between 0 and 1 also leads to positive converged weights. The symmetric Gaussian initialization leads to converged values symmetric around 0. Then, we plot the valley shapes under the noise direction $\\epsilon\\in\\{0,1\\}$ . Because this part involves a comparison among different convergence points, we plot the res u\u2208lt{s by }both the NS noise and Filter NS noise. As vividly displayed in Fig. 7, the first two initialization ways encounter obvious asymmetry while the Gaussian initialization shows nearly perfect symmetry. If we carefully analyze the sign consistency ratio of them, we could easily explain this phenomenon. The noise direction $\\epsilon\\in\\{0,1\\}$ has a larger overlap with the first two initialization methods because the converged BN weights are all positive, while it has a lower overlap with the initialization from $G(0,0.1)$ . This implies that the traditional BN initialization will lead to nearly all positive converged B(N weig)hts, which may influence the valley symmetry. ", "page_idx": 5}, {"type": "text", "text": "4.2.2 Hyperparameters ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Previously, [37] shows that the batch size could influence the valley width, and further [42] advocates that weight decay could also play a role in the valley width. Different from them, we aim to study whether these hyperparameters influence the valley symmetry. We train VGG16 networks on CIFAR10 with various hyperparameters. We use the SGD optimizer with a momentum value ", "page_idx": 5}, {"type": "text", "text": "Figure 10: The leftmost shows a digit sample from \u201csklearn.digits\u201d and others show the pattern of $w\\bar{+}\\lambda*\\mathrm{sign}(w)$ . $\\lambda=0.0$ shows the learned classification weight $w$ . ", "page_idx": 6}, {"type": "text", "text": "of 0.9. The default learning rate (LR) is 0.03, batch size (BS) is 256, and weight decay (WD) is 0.0005. Then, we will correspondingly set the LR in $\\{0.1,0.003\\}$ , BS in $\\{32,{\\bar{2}}048\\}$ , and WD in $\\{0.001,0.00001\\}$ . The curves are in Fig. 9. The first ro w{ applies the} $G(0,1)$ {noise, wh}ile the second r{ow changes its s}ign to the converged models\u2019 sign. Obviously, differe(nt hy)perparameters may lead to valleys with various widths, while the valleys are all symmetric. The asymmetry valleys in the second row again verify the previous findings in Sect. 4.1. ", "page_idx": 6}, {"type": "text", "text": "Then, we explore the interpolation between two solutions under different hyperparameters, which are studied in [37, 42, 22]. We take the batch size of 32 and 2048 as an example and denote the converged solution as $\\theta_{f_{1}}$ and $\\theta_{f_{2}}$ . The test error curve of $(1-\\lambda)\\theta_{f_{1}}+\\lambda\\theta_{f_{2}}$ is plotted, with $\\lambda\\in\\left[-1,2\\right]$ . Aside from the interpolation curve, we also plot the p a(ra m\u2212ete)r di st+ributions of $\\theta_{f_{1}}$ and $\\theta_{f_{2}}$ .  \u2208T[h\u2212e par]ameters are divided into five groups, including \u201cBN Weight\u201d, \u201cCLF Weight\u201d, \u201cCLF Bias\u201d, \u201cOther Weight\u201d, and \u201cOther Bias\u201d. \u201cCLF\u201d denotes the last classification layer, and \u201cOther\u201d denotes other layers aside from the BN layers and the last classification layer. To simplify the figures, we only plot the mean and standard deviation of the parameters, denoted as $\\mu_{p}$ and $\\sigma_{p}$ . Fig. 8 shows the parameter distributions and the interpolation curve. The interpolation curve shows that the small batch training (i.e., $\\lambda=0.0)$ ) lies in a sharper and nearly symmetric valley, while the large batch training (i.e., $\\lambda=1.0$ ) li e=s in a flatter but asymmetric valley. Small batch training (i.e., $\\theta_{f_{1}}$ ) leads to parameters wit h= smaller mean and std values. This is because the utilized weight decay is 0.0005, which makes the parameter scale smaller due to longer training [42]. Then, we explain the different results of valley symmetry. The interpolation formula could be re-written as $\\bar{\\theta}_{f_{1}}\\bar{+}\\,\\lambda\\big(\\theta_{f_{2}}-\\theta_{f_{1}}\\big)$ and $\\theta_{f_{2}}+(\\lambda-1)\\bar{(\\theta_{f_{2}}}-\\theta_{f_{1}})$ , which respectively shows the 1D interpolation centered  +aro(und $\\theta_{f_{1}}$ an)d $\\theta_{f_{2}}$ . If  w+ e( le t\u2212 $\\epsilon=\\theta_{f_{2}}-\\theta_{f_{1}}$ , )then we could plot the sign consistency ratio (i.e., how many parameters have the same sig n) of $\\theta_{f_{1}}$ and $\\epsilon$ , and $\\theta_{f_{2}}$ and $\\epsilon$ . The results are in Fig. 8, where we provide the values of the five parameter groups. Obviously, $\\theta_{f_{2}}$ is more consistent in the sign values, which shows a flatter region towards the positive direction. In contrast, the sign consistency ratio of $\\theta_{f_{1}}$ is smaller, which only shows slight asymmetry. ", "page_idx": 6}, {"type": "text", "text": "5 Theoretical Insights to Explain the Finding ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "This section provides theoretical insights from the aspects of ReLU activation and softmax function to explain the interesting phenomenon. The forward process of DNNs contains amounts of calculation of $W^{T}h$ , e.g., the fully connected and convolution layer. $W$ denotes the weight matrix and $h$ is a hidden representation. A special case is the final classification layer with the softmax function. Given $h\\in R^{d}$ , the ground-truth label $y\\in[C]$ , and the weight matrix $\\dot{W}\\in R^{C\\times d}$ , the softened probability vec t\u2208or is $p=\\mathrm{softmax}(W h)$ . The  c\u2208r[oss]-entropy (CE) loss function  i\u2208s $L(W)=-\\log p_{y}$ . The gradient of $w_{c}$ is $g_{w_{c}}=-(I\\{c=y\\}-p_{c})h$ , with $c\\in[C]$ and $I\\{\\cdot\\}$ being the ind(icati)o =n  f\u2212unction. This implies that the upd a=t e \u2212d(ire{ct i=on }o f\u2212 $W$ )lies in the  \u2208su[bs]pace sp{a\u22c5n}ned by hidden representations, which also holds for intermediate layers and convolution layers [59]. After adequate training steps, the parameters that activate the ReLU function or correspond to the ground-truth label should correlate well with their inputs. We show a demo classification weight learned on the \u201csklearn.digits\u201d 2. Fig. 10 shows the pattern change of $w+\\lambda*\\mathrm{sign}(w)$ with $\\lambda\\in[-1.0,1.0]$ . Clearly, the weight under $\\lambda=0.0$ correlates well with the input  s+am p\u2217le (i.(e., )Digit 0  i\u2208n [F\u2212ig. 10). ]Setting $\\lambda>0.0$ will almost k e=ep the pattern, while $\\lambda<0.0$ destroys it significantly. That is, $w+\\lambda*\\mathrm{sign}(w)$ >with $\\lambda>0.0$ may keep providing a high score for the target class, while setting $\\lambda<0.0$ may decrease the score. ", "page_idx": 6}, {"type": "text", "text": "For the ReLU activation, it also holds that $w+\\lambda*\\mathrm{sign}(w)$ will have a higher probability of keeping activating the neurons when $\\lambda>0.0$ . To simplify the analysis, we assume the learned $w$ equals $a*h+\\delta$ , where $a$ is a constant  a>nd $\\delta$ is a random Gaussian vector. Then we could easily verify that $(w+\\lambda*\\mathrm{sign}(w))^{T}h$ will have a higher probability of keeping activating neurons under a positive $\\lambda$ t(ha n+ th e\u2217 negat(ive) )one. The details and simulation results are in Appendix D.2. If the neuron outputs are only simply scaled by a factor, it will not affect the relative scores of the final classification. For example, the inequation of $w_{1}^{T}h>w_{2}^{T}h$ will not change if $h$ is scaled by a positive factor, while it does not hold for $h$ whose values  >are not activated, i.e., $h=0$ . ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Then we provide a further analysis via analyzing the Hessian matrix of the softmax weights. Specifically, the Hessian of $L(W)=-\\log p_{y}$ w.r.t. $W$ is $H=\\left(\\mathrm{diag}(p)-p p^{T}\\right)\\otimes h h^{T}$ , where $\\otimes$ denotes the Kronecker product. The trace of $H$ is $t r(H)\\,=\\,t r(\\mathrm{diag}(p)-p p^{T})\\,*\\,t r(h h^{T})$ . The first part could be calculated as $\\begin{array}{r}{\\sum_{c}p_{c}(1-p_{c})}\\end{array}$ , where $c$ is the class index. According to the above analysis, adding sign-consistent  n\u2211oise (to $w_{y}$ c)ould enlarge the score of $w_{y}^{T}h$ , which may make the $p_{y}$ larger and $p_{c\\neq y}$ smaller [47]. That is, the predicted probability vector tends to be a one-hot vector when adding sign-consistent noise, and $\\begin{array}{r}{\\sum_{c}p_{c}(1-p_{c})}\\end{array}$ will be near zero. Hence, the trace of the Hessian matrix is smaller along the sign-co n\u2211sistent direction. Since softmax is convex, and the eigenvalues of the Hessian are all positive, a smaller trace means smaller eigenvalues, which makes the loss curve flatter. The empirical observation can be found in Appendix D.3. ", "page_idx": 7}, {"type": "text", "text": "6 Applications to Model Fusion ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "6.1 Explaining the Success of Model Soups ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Commonly, the interpolation of two independently found DNN solutions may encounter a barrier [22, 63]. Surprisingly, if these two models are updated from the same pre-trained model, then the barrier will disappear, and the linear interpolation brings a positive improvement [57, 66]. The common explanation follows that the pre-trained model may possess less instability when compared to the random initialization models [20]. We guess that the sign consistency ratio may influence the parameter fusion performance of the two models. Perhaps, the sign of model parameters updated based on the pre-trained model remain nearly unchanged during the process of fine-tuning. ", "page_idx": 7}, {"type": "text", "text": "We experimentally verify our guess on two datasets, i.e., training VGG16BN with completely random initialization on CIFAR10, and training ResNet18 with pre-trained initialization from PyTorch on Flowers [58]. The datasets are first uniformly split into two partitions, and then two models with corresponding initialization are separately trained or fine-tuned for 50 epochs. The checkpoints in the $\\{1,2,3,5,10,20,30,50\\}$ -th epoch are stored. For checkpoints in the specific epoch, we denote the  t{wo models on the two} partitions as $\\theta_{A}$ and $\\theta_{B}$ , respectively. The interpolation accuracy of $(1-\\lambda)\\theta_{A}+\\lambda\\theta_{B}$ on the test set is plotted in Fig. 11. The interpolation curve of VGG16BN on (CI F\u2212AR)10  in+deed encounters a significant barrier, especially when the epoch is larger, e.g., $E=50$ . In contrast, the interpolation surpasses the individual models on Flowers, which is attributed to the pre-trained ResNet18. As an explanation, we calculate the sign consistency ratio between $\\theta_{A}$ and $\\theta_{I}$ , $\\theta_{B}$ and $\\theta_{I}$ , and $\\theta_{A}$ and $\\theta_{B}$ , denoted as \u201cSSR-IA\u201d, \u201cSSR-IB\u201d, and \u201cSSR-AB\u201d, respectively. $\\theta_{I}$ means the initialization model. We also plot the gap of model interpolation and individual models when $\\lambda=0.5$ , i.e., $\\operatorname{Acc}(0.5\\theta_{A}+0.5\\theta_{B})~{\\mathrm{\\\"-}}~0.5\\big(\\operatorname{Acc}^{}\\!\\big(\\theta_{A}\\big)+\\operatorname{Acc}\\!\\big(\\theta_{B}\\big)\\big)$ . The right of Fig. 11 clearly shows that the sign consistency  ratio could almost perfectly reflect the tendency of the interpolation gap. Notably, the sign consistency ratio of models on Flowers is higher than 0.95, which means that fine-tuning the pre-trained model does not change the parameter signs a lot, which facilitates the following parameter interpolation. The previous work [68] points out that disagreement on the sign of a given parameter\u2019s values across models is a major source of interference during model merging. Although our finding is similar to the previous work, the motivation and specific explanation differs a lot. ", "page_idx": 7}, {"type": "text", "text": "6.2 Regularizing the Sign Change in Federated Learning ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Traditional machine learning models will encounter the challenges posed by \u201cisolated data islands\u201d, e.g., the Non-I.I.D. data [53, 28]. FedAvg [53], as the most standard federated learning (FL) method, utilizes the parameter server architecture [43], and fuses collaborative models from local nodes without centralizing users\u2019 data. Specifically, the local clients receive the global model from the server and update it respectively on their devices using private data, and the server periodically averages these models for multiple communication rounds. That is, FedAvg takes the simple parameter averaging to fuse local models. However, due to the Non-I.I.D. data and permutation invariance of DNNs [50, 65, 69], the local models could become too diverged to effectively merge. Numerous efforts are paid to regularize the local models so as not to go too far away from the global model, such as the proximal term proposed by FedProx [45], the contrastive regularization proposed by MOON [44], the dynamic regularization of FedDyn [1], and the position-aware neurons proposed by FedPAN [50]. Inspired by our finding, we propose to regularize the sign change when updating local models, i.e., ", "page_idx": 7}, {"type": "image", "img_path": "CW0OVWEKKu/tmp/94a2e5729d87060861a6ba838d55ca2dfee8547c028a410ca8dd80b4b9dffc76.jpg", "img_caption": ["Figure 11: The models fine-tuned from a pre-trained model have a higher sign consistency ratio. "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "CW0OVWEKKu/tmp/b6f4e737ebdad3ee62f43b17001936a7c7c0ac43231eb42c4e4dc57255c45d5c.jpg", "table_caption": ["Table 1: Aggregation performance comparisons of FedSign with several popular FL algorithms. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathcal{L}^{k}=\\mathcal{L}_{\\mathrm{ce}}^{k}-\\gamma\\left(\\mathrm{sgp}(\\theta_{t})\\sigma(\\theta_{t}^{k})+\\mathrm{sgp}(-\\theta_{t})\\sigma(-\\theta_{t}^{k})\\right),\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $t$ denotes the communication round and $k$ is the index of client. $\\mathcal{L}_{\\mathrm{ce}}^{k}$ is the common crossentropy loss of the $k$ -th client, and $\\theta_{t}$ is the global model received from the server. $\\theta_{t}^{k}\\gets\\theta_{t}$ is the local model to be updated, and the loss could regularize the sign of $\\theta_{t}^{k}$ to be close to tha t of $\\theta_{t}$ . Because obtaining the sign of parameters is not a continuous function, we therefore apply a sigmoid function to parameters as an approximation. We name this method FedSign and list its pseudo-code in the Appendix C. Compared with other regularization methods, our proposed FedSign is well-motivated because of our finding that interpolating sign-consistent models may lead to a flatter loss region. ", "page_idx": 8}, {"type": "text", "text": "Experimental studies are verified on CIFAR10 [38] and CINIC10 [8] that are commonly utilized in previous works [69, 50]. Decentralizing the training data of these datasets by a Dirichlet distribution could simulate the Non-I.I.D. scenes as in real-world FL. The Dirichlet alpha $\\alpha$ is utilized to control the Non-I.I.D. level, with a smaller $\\alpha$ representing a more rigorous heterogeneity between clients\u2019 data. We set $\\alpha\\in\\{10.0,1.0,0.5\\}$ respectively. The number of clients is 100, and the total communication round is 200. During each round, a random set of $10\\%$ clients participate in FL and every client takes 5 epochs update on their individual data. After all communication rounds, we evaluate the model performance on a global test set on the server (i.e., the original test set of corresponding datasets). We select our hyperparameter $\\gamma$ from $\\{0.001,0.01,0.1\\}$ and report the best results. For $\\alpha=10.0$ , i.e., a relatively I.I.D. scene, a smaller $\\gamma$ i{s better. In contr}ast, $\\gamma=0.1$ or $\\gamma=0.01$ will be m =ore proper for $\\alpha=0.5$ . The performance comparison results are listed i n =Tab. 1. Fo r= FeaAvg and our proposed FedS i=gn, we rerun the experimental studies five times and list the standard deviation of accuracies, showing that the accuracy doesn\u2019t fluctuate very much. FedSign could surpass the compared methods, which shows the positive effects of regularizing the sign change in FL. ", "page_idx": 8}, {"type": "text", "text": "7 Limitations and Future Works ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Although we provide theoretical insights to explain the interesting phenomenon, no formal proofs are provided to show the conditions and scopes that lead to asymmetric valleys. Additionally, this phenomenon is only investigated in the image classification tasks. Future research includes providing formal theoretical foundations for our findings and verifying them on more tasks. According to the analysis in Sect. 5, we advocate that this phenomenon is more likely to be applicable to DNNs that contain both the ReLU and softmax. ", "page_idx": 9}, {"type": "text", "text": "8 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We explore and exploit the asymmetric valley of DNNs via numerous experimental studies and theoretical analyses. We systematically examine various factors influencing valley symmetry, highlighting the significant role of sign consistency between noise direction and the converged model. The findings offer valuable insights into practical implications, enhancing the understanding of model fusion. A novel regularization method is proposed for better model averaging in federated learning. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work is partially supported by National Science and Technology Major Project (2022ZD0114805), NSFC (62376118, 62006112, 62250069, 61921006), Collaborative Innovation Center of Novel Software Technology and Industrialization. Professor De-Chuan Zhan is the corresponding author. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Durmus Alp Emre Acar, Yue Zhao, Ramon Matas Navarro, Matthew Mattina, Paul N. Whatmough, and Venkatesh Saligrama. Federated learning based on dynamic regularization. In ICLR, 2021.   \n[2] Samuel K. Ainsworth, Jonathan Hayase, and Siddhartha S. Srinivasa. Git re-basin: Merging models modulo permutation symmetries. In The Eleventh International Conference on Learning Representations, 2023. [3] Maksym Andriushchenko, Francesco Croce, Maximilian M\u00fcller, Matthias Hein, and Nicolas Flammarion. A modern look at the relationship between sharpness and generalization. In International Conference on Machine Learning, pages 840\u2013902, 2023. [4] Stephen C. Ashmore and Michael S. Gashler. A method for finding similarity between multilayer perceptrons by forward bipartite alignment. In International Joint Conference on Neural Networks, pages 1\u20137, 2015.   \n[5] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 \u2013 mining discriminative components with random forests. In European Conference on Computer Vision, 2014. [6] Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian Borgs, Jennifer T. Chayes, Levent Sagun, and Riccardo Zecchina. Entropy-sgd: Biasing gradient descent into wide valleys. In The 5th International Conference on Learning Representations, 2017.   \n[7] Nicholas Cheney, Martin Schrimpf, and Gabriel Kreiman. On the robustness of convolutional neural networks to internal architecture and weight perturbations. CoRR, abs/1703.08245, 2017.   \n[8] Luke Nicholas Darlow, Elliot J. Crowley, Antreas Antoniou, and Amos J. Storkey. CINIC-10 is not imagenet or CIFAR-10. CoRR, abs/1810.03505, 2018. [9] Jim Davis and Logan Frank. Revisiting batch norm initialization. In Computer Vision - ECCV 2022 - 17th European Conference, pages 212\u2013228, 2022.   \n[10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pages 248\u2013255, 2009.   \n[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4171\u20134186, 2019.   \n[12] Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize for deep nets. In Proceedings of the 34th International Conference on Machine Learning, pages 1019\u20131028, 2017.   \n[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. CoRR, abs/2010.11929, 2020.   \n[14] Felix Draxler, Kambis Veschgini, Manfred Salmhofer, and Fred A. Hamprecht. Essentially no barriers in neural network energy landscape. In Proceedings of the 35th International Conference on Machine Learning, pages 1308\u20131317, 2018.   \n[15] Rahim Entezari, Hanie Sedghi, Olga Saukh, and Behnam Neyshabur. The role of permutation invariance in linear mode connectivity of neural networks. In The Tenth International Conference on Learning Representations, 2022.   \n[16] Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization for efficiently improving generalization. In The 9th International Conference on Learning Representations, 2021.   \n[17] Stanislav Fort, Huiyi Hu, and Balaji Lakshminarayanan. Deep ensembles: A loss landscape perspective. CoRR, abs/1912.02757, 2019.   \n[18] Stanislav Fort and Stanislaw Jastrzebski. Large scale structure of neural network loss landscapes. In Advances in Neural Information Processing Systems 32, pages 6706\u20136714, 2019.   \n[19] Jonathan Frankle. Revisiting \"qualitatively characterizing neural network optimization problems\". CoRR, abs/2012.06898, 2020.   \n[20] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M. Roy, and Michael Carbin. Linear mode connectivity and the lottery ticket hypothesis. In Proceedings of the 37th International Conference on Machine Learning, pages 3259\u20133269, 2020.   \n[21] Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry P. Vetrov, and Andrew Gordon Wilson. Loss surfaces, mode connectivity, and fast ensembling of dnns. In Advances in Neural Information Processing Systems 31, pages 8803\u20138812, 2018.   \n[22] Ian J. Goodfellow and Oriol Vinyals. Qualitatively characterizing neural network optimization problems. In 3rd International Conference on Learning Representations, 2015.   \n[23] Yaru Hao, Li Dong, Furu Wei, and Ke Xu. Visualizing and understanding the effectiveness of BERT. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4141\u20134150, 2019.   \n[24] Haowei He, Gao Huang, and Yang Yuan. Asymmetric valleys: Beyond sharp and flat local minima. In Advances in Neural Information Processing Systems 32, pages 2549\u20132560, 2019.   \n[25] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In 2015 IEEE International Conference on Computer Vision, pages 1026\u20131034, 2015.   \n[26] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, pages 770\u2013778, 2016.   \n[27] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Flat minima. Neural computation, 9(1):1\u201342, 1997.   \n[28] Kevin Hsieh, Amar Phanishayee, Onur Mutlu, and Phillip B. Gibbons. The non-iid data quagmire of decentralized machine learning. In ICML, pages 4387\u20134398, 2020.   \n[29] Gao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu, John E. Hopcroft, and Kilian Q. Weinberger. Snapshot ensembles: Train 1, get M for free. In 5th International Conference on Learning Representations, 2017.   \n[30] Daniel Jiwoong Im, Michael Tao, and Kristin Branson. An empirical analysis of the optimization of deep network loss surfaces. CoRR, abs/1612.04010, 2016.   \n[31] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Proceedings of the 32nd International Conference on Machine Learning, pages 448\u2013456, 2015.   \n[32] Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry P. Vetrov, and Andrew Gordon Wilson. Averaging weights leads to wider optima and better generalization. In Proceedings of the Thirty-Fourth Conference on Uncertainty in Artificial Intelligence, pages 876\u2013885, 2018.   \n[33] Arthur Jacot, Cl\u00e9ment Hongler, and Franck Gabriel. Neural tangent kernel: Convergence and generalization in neural networks. In Advances in Neural Information Processing Systems 31, pages 8580\u20138589, 2018.   \n[34] Stanislaw Jastrzebski, Maciej Szymczak, Stanislav Fort, Devansh Arpit, Jacek Tabor, Kyunghyun Cho, and Krzysztof J. Geras. The break-even point on optimization trajectories of deep neural networks. In 8th International Conference on Learning Representations, 2020.   \n[35] Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio. Fantastic generalization measures and where to find them. In The 8th International Conference on Learning Representations, 2020.   \n[36] Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank J. Reddi, Sebastian U. Stich, and Ananda Theertha Suresh. SCAFFOLD: stochastic controlled averaging for federated learning. In Proceedings of the 37th International Conference on Machine Learning, pages 5132\u20135143, 2020.   \n[37] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. In The 5th International Conference on Learning Representations, 2017.   \n[38] Alex Krizhevsky. Learning multiple layers of features from tiny images. 2012.   \n[39] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural networks. Communications of the ACM, 60(6):84\u201390, 2017.   \n[40] Jungmin Kwon, Jeongseop Kim, Hyunseo Park, and In Kwon Choi. ASAM: adaptive sharpnessaware minimization for scale-invariant learning of deep neural networks. In Proceedings of the 38th International Conference on Machine Learning, pages 5905\u20135914, 2021.   \n[41] Jaehoon Lee, Lechao Xiao, Samuel S. Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models under gradient descent. In Advances in Neural Information Processing Systems 32, pages 8570\u20138581, 2019.   \n[42] Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss landscape of neural nets. In Advances in Neural Information Processing Systems 31, pages 6391\u20136401, 2018.   \n[43] Mu Li, Li Zhou, Zichao Yang, Aaron Li, Fei Xia, David G Andersen, and Alexander Smola. Parameter server for distributed machine learning. In NeurIPS, volume 6, page 2, 2013.   \n[44] Qinbin Li, Bingsheng He, and Dawn Song. Model-contrastive federated learning. In CVPR, pages 10713\u201310722, 2021.   \n[45] Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated optimization in heterogeneous networks. In MLSys, 2020.   \n[46] Weishi Li, Yong Peng, Miao Zhang, Liang Ding, Han Hu, and Li Shen. Deep model fusion: A survey. CoRR, abs/2309.15698, 2023.   \n[47] Xin-Chun Li, Wen-Shu Fan, Shaoming Song, Yinchuan Li, Bingshuai Li, Yunfeng Shao, and De-Chuan Zhan. Asymmetric temperature scaling makes larger networks teach well again. In Advances in Neural Information Processing Systems 35, 2022.   \n[48] Xin-Chun Li, Lan Li, and De-Chuan Zhan. Visualizing, rethinking, and mining the loss landscape of deep neural networks. CoRR, abs/2405.12493, 2024.   \n[49] Xin-Chun Li, Shaoming Song, Yinchuan Li, Bingshuai Li, Yunfeng Shao, Yang Yang, and De-Chuan Zhan. MAP: model aggregation and personalization in federated learning with incomplete classes. CoRR, abs/2404.09232, 2024.   \n[50] Xin-Chun Li, Yi-Chu Xu, Shaoming Song, Bingshuai Li, Yinchuan Li, Yunfeng Shao, and De-Chuan Zhan. Federated learning with position-aware neurons. In IEEE Conference on Computer Vision and Pattern Recognition, pages 10082\u201310091, 2022.   \n[51] Tao Lin, Lingjing Kong, Sebastian U. Stich, and Martin Jaggi. Ensemble distillation for robust model fusion in federated learning. In Advances in Neural Information Processing Systems 33, 2020.   \n[52] Eliana Lorch. Visualizing deep network training trajectories with pca. 2016.   \n[53] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Ag\u00fcera y Arcas. Communication-efficient learning of deep networks from decentralized data. In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, pages 1273\u20131282, 2017.   \n[54] Guido Mont\u00fafar, Razvan Pascanu, KyungHyun Cho, and Yoshua Bengio. On the number of linear regions of deep neural networks. In Advances in Neural Information Processing Systems 27, pages 2924\u20132932, 2014.   \n[55] Yuval Netzer, Tiejie Wang, Adam Coates, A. Bissacco, Bo Wu, and A. Ng. Reading digits in natural images with unsupervised feature learning. 2011.   \n[56] Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Exploring generalization in deep learning. In Advances in Neural Information Processing Systems 30, pages 5947\u20135956, 2017.   \n[57] Behnam Neyshabur, Hanie Sedghi, and Chiyuan Zhang. What is being transferred in transfer learning? In Advances in Neural Information Processing Systems 33, 2020.   \n[58] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In Sixth Indian Conference on Computer Vision, Graphics & Image Processing, pages 722\u2013729, 2008.   \n[59] Gobinda Saha, Isha Garg, and Kaushik Roy. Gradient projection memory for continual learning. In 9th International Conference on Learning Representations, 2021.   \n[60] Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. How does batch normalization help optimization? In Advances in Neural Information Processing Systems 31, pages 2488\u20132498, 2018.   \n[61] Hai Shu and Hongtu Zhu. Sensitivity analysis of deep neural networks. In The Thirty-Third AAAI Conference on Artificial Intelligence, pages 4943\u20134950, 2019.   \n[62] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In 3rd International Conference on Learning Representations, 2015.   \n[63] Sidak Pal Singh and Martin Jaggi. Model fusion via optimal transport. In Advances in Neural Information Processing Systems 33, 2020.   \n[64] Tiffany J. Vlaar and Jonathan Frankle. What can linear interpolation of neural network loss landscapes tell us? In International Conference on Machine Learning, pages 22325\u201322341, 2022.   \n[65] Hongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris S. Papailiopoulos, and Yasaman Khazaeni. Federated learning with matched averaging. In 8th International Conference on Learning Representations, 2020.   \n[66] Mitchell Wortsman, Gabriel Ilharco, Samir Yitzhak Gadre, Rebecca Roelofs, Raphael Gontijo Lopes, Ari S. Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, and Ludwig Schmidt. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In International Conference on Machine Learning, pages 23965\u201323998, 2022.   \n[67] Saining Xie, Ross B. Girshick, Piotr Doll\u00e1r, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In IEEE Conference on Computer Vision and Pattern Recognition, pages 5987\u20135995, 2017.   \n[68] Prateek Yadav, Derek Tam, Leshem Choshen, Colin A. Raffel, and Mohit Bansal. Ties-merging: Resolving interference when merging models. In Advances in Neural Information Processing Systems 36, 2023.   \n[69] Fuxun Yu, Weishan Zhang, Zhuwei Qin, Zirui Xu, Di Wang, Chenchen Liu, Zhi Tian, and Xiang Chen. Fed2: Feature-aligned federated learning. In KDD, pages 2066\u20132074, 2021.   \n[70] Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra. Federated learning with non-iid data. CoRR, abs/1806.00582, 2018.   \n[71] Yang Zhao, Hao Zhang, and Xiuyuan Hu. Penalizing gradient norm for efficiently improving generalization in deep learning. In International Conference on Machine Learning, pages 26982\u201326992, 2022. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Experimental Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we list the datasets, networks, and training details that are utilized in the body. Finally, we provide the details that motivate us to change the sign of the noise. ", "page_idx": 13}, {"type": "text", "text": "A.1 Dataset ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The utilized datasets include \u201csklearn.digits\u201d 3, SVHN [55], CIFAR10/100 [38], CINIC10 [8], Flowers [58], Food101 [5], and ImageNet [10]. We detail these datasets as follows. ", "page_idx": 13}, {"type": "text", "text": "\u2022 \u201csklearn.digits\u201d contains 1797 samples of 10 digits, with each sample being a $8\\times8$ image. We use this to provide a simple code demo and provide theoretical verificati o\u00d7n as in Appendix D.3.   \n\u2022 SVHN [55] is the Street View House Number dataset which contains 10 numbers to classify. The raw set contains 73,257 samples for training and 26,032 samples for evaluation. The image size is $32\\times32$ .   \n\u2022 CIFAR10 and CIFAR100 [38] are subsets of the Tiny Images dataset and respectively have 10/100 classes to classify. They consist of 50,000 training images and 10,000 test images. The image size is $32\\times32$ .   \n\u2022 CINIC10 [8] is a combination of CIFAR10 and ImageNet [39], which contains 10 classes. It contains 90,000 samples for training, validation, and testing, respectively. We do not use the validation set. The image size is $32\\times32$ .   \n\u2022 Flowers [58] consists of 102 fine-grained flower categories, where we select 2,000 images for training and 2,000 images for testing.   \n\u2022 Food101 [5] consists of 101 food categories, with 101, 000 images. For each class, 250 manually reviewed test images are provided as well as 750 training images. All images were rescaled to have a maximum side length of 512 pixels.   \n\u2022 ImageNet [10] consists 1000 image categories for classification. This dataset is utilized to pre-train models as listed in \u201ctorchvision\u201d. Due to the large amounts of data, we only select 5,000 images in the \u201cval\u201d partition to plot the curves in corresponding figures. ", "page_idx": 13}, {"type": "text", "text": "A.2 Network Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We utilize VGG [62], ResNet [26], ResNeXt [67], AlexNet [39], ViT [13] in this paper. We detail their architectures as follows: ", "page_idx": 13}, {"type": "text", "text": "\u2022 VGG contains a series of networks with various layers. The paper of VGG [62] presents VGG11, VGG13, VGG16, and VGG19. We follow their architectures and report the configuration of VGG11 as an example: 64, M, 128, M, 256, 256, M, 512, 512, M, 512, 512, M. \u201cM\u201d denotes the max-pooling layer. VGG11 contains 8 convolution blocks and three fully-connected layers in [62]. The VGG architecture could use BatchNorm [31] or not, which is clearly declared in the body. If BN is used, it will be added after each convolution layer and before the ReLU activation function.   \n\u2022 ResNet introduces residual connections to plain neural networks. We take the CIFAR versions used in the paper [26] for CIFAR10/100 and CINIC10, i.e., ResNet20 with the basic block. For Flowers, we use pre-trained ResNet18 from PyTorch. ResNet commonly uses BatchNorm [31], which is added before ReLU activation.   \n\u2022 AlexNet [39] consists of five convolutional layers followed by three fully connected layers.   \n\u2022 ResNeXt [67] introduces the group convolution to ResNet and we utilize the pre-trained version downloaded from \u201ctorchvision\u201d.   \n\u2022 ViT [13] follows the transformer architecture for image classification tasks. It divides an image into a sequence of fixed-size patches, processes these patches linearly, and then feeds them into a transformer encoder to capture the global context of the image. In our paper, we ", "page_idx": 13}, {"type": "text", "text": "take 12 layers in the transformer encoder and use 8 heads in each multi-head self-attention block. We set the embedding dimension as 128 to reduce the computation burden. ", "page_idx": 14}, {"type": "text", "text": "A.3 Training Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We provide the training details for obtaining converged models. We investigate the following pairs of datasets and networks. For the series of Fig. 3 and Fig. 5, we train VGG16BN on CIFAR10, ResNet20 on SVHN, VGG11 with no BN on CIFAR100, AlexNet on Food101, and ViT on Food101. For VGG16BN, ResNet20, VGG11 with no BN, and AlexNet, we use the SGD optimizer with a momentum of 0.9. We set the learning rate as 0.03. We use a cosine annealing way to decay the learning rate across 200 training epochs. The default weight decay is 0.0005, and the default batch size is 256. For training ViT, we use the AdamW optimizer with a learning rate of 0.0001. The batch size is 256, and the weight decay is 0.0005. We also take the cosine annealing way to decay the learning rate. ", "page_idx": 14}, {"type": "text", "text": "The equation of the BN layer is $\\begin{array}{r}{\\mathbf{X}=\\mathbf{w}\\frac{\\mathbf{X}-\\mathbf{m}_{\\mathbf{X}}}{\\sqrt{\\mathbf{v}_{\\mathbf{X}}+\\eta}}+\\mathbf{b}}\\end{array}$ , where $\\mathbf{X}\\in\\mathcal{R}^{C\\times d}$ denotes the feature map with $C$ channels, and $\\mathbf{m}\\mathbf{x}\\in\\mathcal{R}^{C}$ and $\\mathbf{v}\\mathbf{x}\\in\\mathcal{R}^{C}$ are channel-wise mean and variance values of the feature map. In practice, w, $\\mathbf{b}\\in\\mathcal{R}^{C}$ are lea r\u2208naRble parameters, while $\\mathbf{m}_{\\mathbf{X}}$ and $\\mathbf{v}_{\\mathbf{X}}$ are running statistics that are calculated during t h\u2208e fRorward pass. When interpolating $\\theta_{f}+\\lambda\\epsilon$ with BN layers, we should clear these running statistics after interpolating model parameters a n+d feed the interpolated model to the dataset for another forward pass to calculate proper data distributions. The forward-again process is also utilized in previous works [32, 66]. ", "page_idx": 14}, {"type": "text", "text": "For Fig. 7, we only change the initialization method of BN layers and keep the other hyperparameters not changed. For the series of Fig. 8, we only change a specific hyperparameter including the learning rate, batch size, or weight decay. Specifically, the learning rate is varied in $\\{0.1,0.{\\bar{0}}03\\}$ , and the batch size is varied in $\\lbrace32,2048\\rbrace$ , and the weight decay is varied in $\\{0.001,0.00001\\}$ . ", "page_idx": 14}, {"type": "text", "text": "A.4 Motivation of Changing the Sign of Noise ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The asymmetric valley is initially proposed by [24], while it does not propose the inherent principles behind the phenomenon. It only points out that adding asymmetric noise (e.g., $\\epsilon\\sim\\{0,1\\})$ to DNNs may result in an asymmetric valley. The symmetric noise around zero may not show such patterns (e.g., $\\epsilon\\sim\\{-1,0,1\\})$ . This inspires us to plot valleys along three types of symmetric noise directions and four types of asymmetric noise directions, and the results are shown in Fig. 3. Indeed, the last four types of noise are not symmetric around zero and show slight asymmetric valleys. However, this is not so obvious. Notably, the utilized network in Fig. 3 has BN layers, i.e., VGG16 with BN. ", "page_idx": 14}, {"type": "text", "text": "Then, we try to apply $\\epsilon\\sim\\{0,1\\}$ to DNNs without BN, i.e., VGG11 without BN. The valleys become symmetric as shown in the first row of Fig. 13. Hence, this makes us consider the effect of BN layers. Fortunately, we find that the traditional BN initialization will initialize the values in \u201cBN.weight\u201d as 1, and the converged BN weights are all positive. The initial findings could be summarized as: ", "page_idx": 14}, {"type": "text", "text": "\u2022 If DNNs have BN, and the parameters are perturbed by noise with symmetric values around zero, the valleys are symmetric. This is shown as the top first three plots in Fig. 3. \u2022 If DNNs have BN, and the parameters are perturbed by noise with asymmetric values around zero, the valleys are slightly asymmetric. This is shown as the top last four plots in Fig. 3. \u2022 If DNNs do not have BN, and the parameters are perturbed by noise with symmetric values around zero, the valleys are symmetric. This is shown as the top first three plots in Fig. 13. \u2022 If DNNs do not have BN, and the parameters are perturbed by noise with asymmetric values around zero, the valleys are symmetric. This is shown as the top last four plots in Fig. 13. ", "page_idx": 14}, {"type": "text", "text": "That is, only the second case shows slightly asymmetric valleys, where the noise (e.g., $\\epsilon\\in\\{0,1\\}$ ) has a large sign consistency with the BN weights (e.g., $>0.0_{.}$ ). As shown in Sect. 4.2.1, if we replace the initialization of BN weights with a random Gaussi a>n initialization, then the plotted valley becomes symmetric because the converged BN weights are symmetric around zero again (Fig. 7). ", "page_idx": 14}, {"type": "text", "text": "That is, the converged \u201cBN.weight\u201d are all positive values under the common BN initialization. If we perturb them by asymmetric noise (e.g., $\\epsilon\\in\\{0,1\\}$ , $\\epsilon\\in G(1,1)$ , $\\epsilon\\in U(0,1)$ , or $\\epsilon\\in\\{1\\}$ ) that has ", "page_idx": 14}, {"type": "image", "img_path": "CW0OVWEKKu/tmp/27e85c9137d7f7923cb5ef847171b752506119e94e461966d975a9eaf9f74052.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Figure 12: The valleys under 7 common noise types. The second row shows the results of replacing the sign of noise with that of $\\theta_{f}$ , leading to asymmetric valleys. (ResNet20 on SVHN) ", "page_idx": 15}, {"type": "image", "img_path": "CW0OVWEKKu/tmp/6f3ef2d6e28efa8ccf3eaf6ec7e652dab5a783c43f07434641f93b24892a6f1b.jpg", "img_caption": ["Figure 13: The valleys under 7 common noise types. The second row shows the results of replacing the sign of noise with that of $\\theta_{f}$ , leading to asymmetric valleys. (VGG11 without BN on CIFAR100) "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "a larger sign consistency with \u201cBN.weight\u201d, the plotted valleys are asymmetric. This provides an explanation for the asymmetric valley found by [24]. ", "page_idx": 15}, {"type": "text", "text": "From the above summary, we guess that the sign consistency between parameters and the noise may lead to asymmetric valleys. Hence, a fantastic idea motivates us to change the sign of the noise to the same as $\\theta$ , which leads to obvious asymmetric valleys (the bottom row of Fig. 3 and Fig. 13). ", "page_idx": 15}, {"type": "text", "text": "B More Experimental Studies ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we provide more experimental verification results to make our work more solid. We list the supplemented results by the corresponding experimental studies in the body. ", "page_idx": 15}, {"type": "text", "text": "B.1 More Results as Fig. 3 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Fig. 3 shows the valley shape under 7 common noise types. The results are plotted via training VGG16 with BN on CIFAR10. We list more results to verify the finding is common across various networks and architectures, including: (1) ResNet20 on SVHN (Fig. 12); (2) VGG11 without BN on CIFAR100 (Fig. 13). Obviously, these results are indeed similar, which verifies again that the sign consistency ratio matters a lot in the valley symmetry. ", "page_idx": 15}, {"type": "text", "text": "We also extend the findings to the large-scale dataset and popular network architectures including: (1) AlexNet on Food101 (Fig. 14); (2) ViT on Food101 (Fig. 15). Applying noise with a higher sign ", "page_idx": 15}, {"type": "image", "img_path": "CW0OVWEKKu/tmp/bd92360487ff8089776210e7bf360b4b5361e74631636e747c7e33069d8c5f51.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Figure 14: The valleys under 7 common noise types. The second row shows the results of replacing the sign of noise with that of $\\theta_{f}$ , leading to asymmetric valleys. (AlexNet on Food101) ", "page_idx": 15}, {"type": "image", "img_path": "CW0OVWEKKu/tmp/136f2a947d9a74a5c28bff6240199bc901e3aef5d842d877b272ebf689cb2852.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure 15: The valleys under 7 common noise types. The second row shows the results of replacing the sign of noise with that of $\\theta_{f}$ , leading to asymmetric valleys. (ViT on Food101) ", "page_idx": 16}, {"type": "image", "img_path": "CW0OVWEKKu/tmp/4f8c62554ccb3637c633ce31c8f85183f1b22c1e47f090abbb3cb30a5e2572c6.jpg", "img_caption": ["Figure 16: The valley shape under 6 special noise types. (ResNet20 on SVHN) "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "consistency also leads to asymmetric valleys. However, the large models are relatively stable to some extent, and the asymmetry is not as obvious as the results on the previous datasets and networks. ", "page_idx": 16}, {"type": "text", "text": "B.2 More Results as Fig. 5 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Fig. 5 shows the valley shape under 6 special noise types. The results are plotted via training VGG16 with BN on CIFAR10. We list more results to verify the finding is nearly common across various networks and architectures, including: (1) ResNet20 on SVHN (Fig. 16); (2) VGG11 without BN on CIFAR100 (Fig. 17). Obviously, these results are indeed similar. An exceptional case is the first sub-figure in Fig. 17, i.e., using the initialization as noise leads to a symmetric valley, while the valley in Fig. 5 and Fig. 16 is asymmetric. This is because of the initialization of BN parameters, where the latter two utilize BN weights as all ones. However, Fig. 17 takes VGG11 without BN layers, which shows no asymmetry. ", "page_idx": 16}, {"type": "text", "text": "We also extend the findings to the large-scale dataset and popular network architectures including: (1) AlexNet on Food101 (Fig. 18); (2) ViT on Food101 (Fig. 19). Similar to the previous results, although the asymmetry is not as obvious as the results on the previous datasets and networks, the curves still show slight asymmetry when the sign consistency is high. ", "page_idx": 16}, {"type": "text", "text": "B.3 More Results as Fig. 6 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Fig. 6 shows the valley shape investigated on ImageNet with pre-trained ResNeXt101. We also provide similar results with the pre-trained ResNet50 as in Fig. 20. ", "page_idx": 16}, {"type": "image", "img_path": "CW0OVWEKKu/tmp/15f8bcd40f9149aae985e2f2e8e248ae825be01e5d88bb865735f83ffad1d4e4.jpg", "img_caption": ["Figure 17: The valley shape under 6 special noise types. (VGG11 without BN on CIFAR100) "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "CW0OVWEKKu/tmp/e543ecb3fd723dbbc8a32f946301ddc278ca69259a930abc081c01a86d09cb7f.jpg", "img_caption": ["Figure 18: The valley shape under 6 special noise types. (AlexNet on Food101) "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "CW0OVWEKKu/tmp/2363f6022c5f26a975e246da7917aed006f8bf0abc6ec2532bbd5e36813e22dc.jpg", "img_caption": ["Figure 19: The valley shape under 6 special noise types. (ViT on Food101) "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "B.4 More Results as Fig. 7 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Fig. 7 shows the impact of BN and its initialization on the valley symmetry. The results are plotted via training VGG16 with BN on CIFAR10. We list more results to verify the finding is common across various networks and architectures, including (1) ResNet20 on SVHN (Fig. 21); (2) ResNet20 on CIFAR100 (Fig. 22). Obviously, these results are indeed similar, which shows that the original BN initialization may lead to an asymmetric valley. Replacing the original initialization with the symmetric Gaussian distribution could lead to symmetric valleys. ", "page_idx": 17}, {"type": "text", "text": "B.5 More Results as Fig. 9 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Fig. 9 shows the impact of various hyperparameters on valley symmetry. The results are plotted via training VGG16 with BN on CIFAR10. We list more results to verify the finding is common across various architectures, i.e., ResNet20 on CIFAR10 (Fig. 23). This also verifies that the hyperparameters have less impact on the valley symmetry. ", "page_idx": 17}, {"type": "text", "text": "B.6 More Results as Fig. 8 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Fig. 8 studies the interpolation between two models that trained under different hyperparameters, e.g., learning rate (LR), batch size (BS), and weight decay (WD). The body only shows the impact of batch size on training VGG16BN. Then, we list more results to verify the finding is common across various architectures, including: (1) the impact of learning rate on training VGG16BN on CIFAR10 (Fig. 24); (2) the impact of weight decay on training VGG16BN on CIFAR10 (Fig. 25); (3) ", "page_idx": 17}, {"type": "image", "img_path": "CW0OVWEKKu/tmp/f0db8708e6ec4b68613db7c8c26b4263ac7f908206bfad22227d239eb616047e.jpg", "img_caption": ["Figure 20: Verification results on ImageNet with pre-trained ResNet50. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "CW0OVWEKKu/tmp/1a903505ff0b409cb19a88b4b5e026cdb76ea7b19b59f33718615541c1226065.jpg", "img_caption": [], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Figure 21: The impact of BN and its initialization on the valley symmetry. (ResNet20 on SVHN) ", "page_idx": 18}, {"type": "text", "text": "Figure 22: The impact of BN and its initialization on the valley symmetry. (ResNet20 on CIFAR100) ", "page_idx": 18}, {"type": "image", "img_path": "CW0OVWEKKu/tmp/22eb6269f2086577a37c0ad8e3aaa1adcfb37fd37d87172deb213dd302306c12.jpg", "img_caption": ["Figure 23: The impact of various hyperparameters on valley symmetry. (ResNet20 on CIFAR10) "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "the impact of batch size on training ResNet20 on CIFAR10 (Fig. 26). From these figures, we could again observe that the parameter scale influences the valley width, while the sign consistency ratio matters to the valley symmetry. ", "page_idx": 18}, {"type": "text", "text": "Additionally, we could further obtain the following conclusion: a larger learning rate (e.g., 0.1), a smaller batch size (e.g., 32), or a larger weight decay (e.g., 0.001) could lead to better performances, which are shown in these figures with a lower test error when compared the opposite hyperparameter. However, their parameter scales are relatively smaller than opposite ones, making their valley width sharper. And commonly, a larger parameter scale of $\\theta_{f_{1}}$ will let the sign of $\\theta_{f_{1}}-\\theta_{f_{2}}$ conform to $\\theta_{f_{1}}$ more, which leads to a flatter region along the positive direction of $\\theta_{f_{2}}$ . ", "page_idx": 18}, {"type": "image", "img_path": "CW0OVWEKKu/tmp/5ae024a50dbb6f362c620c0c59e46d14b22aa7c338101dce50028e989e86c714.jpg", "img_caption": [], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Figure 24: The interpolation between two models trained with learning rate as 0.1 and 0.003. (VGG16 with BN on CIFAR10) ", "page_idx": 18}, {"type": "text", "text": "Figure 25: The interpolation between two models trained with weight decay as 0.001 and 0.00001. (VGG16 with BN on CIFAR10) ", "page_idx": 18}, {"type": "text", "text": "Figure 26: The interpolation between two models trained with batch size as 32 and 2048. (ResNet20 on CIFAR10) ", "page_idx": 18}, {"type": "text", "text": "Algorithm 1 FedSign   \nServerProcedure:   \n1: for global round $t=0,1,2,\\ldots,T$ do   \n2: $S_{t}\\gets$ sample m a= $\\mathfrak{c}(Q\\cdot K,1)$ clients   \n3: for $k\\in S_{t}$ do   \n4: $\\widehat{\\theta}_{t}^{k}\\gets$ ClientProcedure $(k,\\theta_{t})$   \n5: end f o\u2190r   \n6: $\\begin{array}{r}{\\overline{{\\theta}}_{t+1}^{\\mathbf{\\alpha}\\mathbf{n}\\mathbf{\\alpha}}\\leftarrow\\sum_{k=1}^{|S_{t}|}\\frac{1}{|S_{t}|}\\hat{\\theta}_{t}^{k}}\\end{array}$   \n7: end for   \nClientProcedure $(k,\\theta_{t})$ :   \n1: $\\theta_{t}^{k}\\gets\\theta_{t}$   \n2: fo r l\u2190ocal epoch $e=1,2,\\dots,E$ do   \n3: for each batch  =with $B$ samples from $\\mathcal{D}^{k}$ do   \n4: Calculate the loss as in Eq. 1, upd atDe $\\theta_{t}$ using, e.g., SGD with momentum   \n5: end for   \n6: end for   \n7: Return: the updated model $\\widehat{\\theta}_{t}^{k}$ ", "page_idx": 19}, {"type": "text", "text": "C Pseudo Code of FedSign ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We provide a pseudo code of the application to federated learning, i.e., FedSign. Our proposed FedSign takes a novel regularization method to limit the change of local models\u2019 signs, whose goal is for better model fusion on the server. The pseudo-code is listed as in Algo. 1. Here, $T$ denotes the number of communication rounds, $K$ is the total number of clients, $Q$ is a client participating ratio in each round, $E$ is the local update epochs of participated clients, $B$ denotes the batch size, and $\\mathcal{D}^{k}$ denotes the private data of the $k$ -th client. In our experimental studies, we set $T=200$ , $K=100$ , $Q=10\\%$ , $E=5$ , and $B=64$ . After all communication rounds, we obtain the final a g=gregated  =model $\\theta_{T+1}$ , and th e=n we test i ts= accuracy on the global test set to evaluate the aggregation performance of FedSign. ", "page_idx": 19}, {"type": "text", "text": "D Detailed Theoretical Analysis and Verification ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Our major finding is formulated as: $L(\\theta_{f}+a\\eta)<L(\\theta_{f}-a\\eta)$ , where $\\eta=|\\boldsymbol{\\epsilon}|*\\mathrm{sign}(\\theta_{f})$ denotes the sign-consistent noise, and $a>0$ is a co(nsta n+t. $L(\\cdot)$ is (the  l\u2212oss f)unction. T h=i \u2223s \u2223fi\u2217nding (hol)ds for several settings, including (1) the c a>ses when $L(\\cdot)$ is t(h\u22c5e) prediction error or cross-entropy loss, which are verified in Fig. 6 and Fig. 20; (2) the cas(e\u22c5s) of applying noise to whole parameters, only to softmax classification layer, or other layers, which are verified in Fig. 6 and Fig. 20. ", "page_idx": 19}, {"type": "text", "text": "We present theoretical analysis from several possible aspects and finally attribute this finding to the properties of ReLU activation and softmax classification. ", "page_idx": 19}, {"type": "text", "text": "D.1 Gradient Analysis ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "First, we guess that the $\\mathrm{sign}(\\theta_{f})$ may have a correlation to the gradient. Specifically, if $\\boldsymbol{\\epsilon}=\\nabla_{\\boldsymbol{\\theta}_{f}}\\mathcal{L}(\\boldsymbol{\\theta}_{f})$ , then given a very smaller $\\lambda$ ,( we) may have the following relationship: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}\\big(\\boldsymbol{\\theta}_{f}+\\lambda\\boldsymbol{\\nabla}_{\\boldsymbol{\\theta}_{f}}\\mathcal{L}\\big)\\geq\\mathcal{L}\\big(\\boldsymbol{\\theta}_{f}\\big)\\geq\\mathcal{L}\\big(\\boldsymbol{\\theta}_{f}-\\lambda\\boldsymbol{\\nabla}_{\\boldsymbol{\\theta}_{f}}\\mathcal{L}\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the first and third terms denote the gradient ascent step and gradient descent step, respectively. If we set $\\epsilon\\;=\\;\\nabla_{\\theta_{f}}\\mathcal{L}(\\theta_{f})$ and $\\lambda\\in[-1,1]$ , the plotted curve may be asymmetric. However, we frustratingly find that $\\mathrm{sign}(\\theta_{f})$ are almost orthogonal to the $\\nabla_{\\theta_{f}}\\mathcal{L}$ . On one hand, the converged model $\\theta_{f}$ has very small gradient values, and elements in $\\nabla_{\\theta_{f}}\\mathcal{L}$ ar e nearly zero. On the other hand, adding $\\mathrm{sign}(\\theta_{f})$ to $\\theta_{f}$ cannot obtain a lower error than $\\theta_{f}$ itself, which is not the same as the gradient descent dire(ctio)n. This implies that the inherent properties of them are not the same. Hence, we try other possible explanations. ", "page_idx": 19}, {"type": "text", "text": "D.2 ReLU Activation ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The asymmetry may be related to the inherent asymmetry of the ReLU activation function. According to the analysis in Sect. 5, the gradient of parameters lies in the subspace spanned by the corresponding inputs. Hence, the converged parameters may correlate with their inputs to some extent. Fig. 10 shows the pattern of learned classification weight on the \u201csklearn.digits\u201d. The learned classification weight $w$ shows a pattern of $\\,^{\\bullet}0^{\\bullet}$ as shown in the figure. However, it is hard for us to provide a concrete expression about the correlation of weight with its corresponding inputs, even for the simplest softmax classification layer. Hence, we assume that the learned $w$ equals to $a*h+\\delta$ , where $a$ is a constant and $\\delta$ is a random Gaussian vector. $h$ denotes the hidden representati o\u2217n.  A+nd then we simulate the distribution of $(w+\\lambda*\\mathrm{sign}(w))^{T}h$ by the following Python code 1. Specifically, we set $h\\in R^{d}$ as a vector sa m(pl e+d f r\u2217om th(e d)i)stribution of $G(0,1)$ , and set $a\\,=\\,0.1$ . We take $\\delta$ as a Gau s\u2208sian vector sampled from the distribution of $G(0,1)$ .( The)n we sampl e $N=10000$ groups of $\\delta$ and plot the distribution of $(w+\\lambda*\\mathrm{sign}(w))^{T}h$ under $\\lambda\\in\\{-1.0,-0.5,0.0,0.5,1.0\\}$ . The distributions are shown in Fig. 27.  O(bv i+ou sl\u2217y, wit(h a)n) increasing $\\lambda$ \u2208, t{h\u2212e distr\u2212ibution is shifted }right. In other words, a negative $\\lambda$ may decrease the value of $(w+\\lambda*\\bar{\\mathrm{sign}}(w))^{T}h$ . In ReLU activation, the negative values are not activated, making the loss error change a lot. ", "page_idx": 20}, {"type": "text", "text": "Listing 1: Simulate ReLU ", "page_idx": 20}, {"type": "table", "img_path": "CW0OVWEKKu/tmp/24ea2a80dbd8744505f9f38e75f1a99e5dffbf275c54423d694a023e7f0f6ceb.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Aside from the simulation demo, we also provide the hidden activations for ResNet20 trained on SVHN. The results are shown in Fig. 28. We plot the activation confusion matrix of $\\theta_{f}\\!+\\!\\lambda\\!*\\!\\left|\\epsilon\\middle|\\mathrm{sign}(\\theta_{f})\\right.$ with $\\lambda\\in\\left[-1.0,1.0\\right]$ . For each $\\lambda$ , we obtain the hidden features extracted by $\\theta_{f}+\\lambda*|\\epsilon|\\mathrm{sign}(\\theta_{f})$ . We then compare the features with the original features extracted by $\\theta_{f}$ and plot the activation confusion matrix. The sum of diagonal values represents the outputs that the original model and the interpolated model commonly activate or do not activate. The value in \u201c[]\u201d shows the sum of diagonal values. Obviously, the value is larger when $\\lambda=a$ than that of $\\lambda=-a$ , with $a\\in\\{0.2,0.4,0.6,0.8,1.0\\}$ . ", "page_idx": 20}, {"type": "text", "text": "To conclude, for the ReLU activation, adding sign-consistent noise to parameters will have a higher probability of activating the neurons. If the neuron outputs are only simply scaled by a factor, it will not affect the relative scores of the final classification. For example, the inequation of $w_{1}^{T}h>w_{2}^{T}h$ will not change if $h$ is scaled by a positive factor, while it does not hold for $h$ whose values are not activated, i.e., $h=0$ . ", "page_idx": 20}, {"type": "text", "text": "D.3 Softmax Function ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Given $h\\,\\in\\,R^{d}$ , the ground-truth label $y\\,\\in\\,[C]$ , and the weight matrix $W\\,\\in\\,R^{C\\times d}$ , the softened probabili t\u2208y vector is $p=\\operatorname{softmax}(W h)$ .  T\u2208he[ cr]oss-entropy (CE) loss functi o\u2208n is $L(W)=-\\log p_{y}$ The gradient of $w_{c}$ is $g_{w_{c}}=-(I\\{c=y\\}-p_{c})h$ , with $c\\in[C]$ and $I\\{\\cdot\\}$ being the indi(cati)o n=  f\u2212unction. Specifically, the Hessian  o= f $L(W)\\,=\\,-\\log p_{y}$ w.r.t. $W$ [is $\\mathbf{\\bar{\\boldsymbol{H}}}=\\left(\\mathbf{diag}(\\boldsymbol{p})-\\boldsymbol{p}\\boldsymbol{p}^{T}\\right)\\otimes\\boldsymbol{h}\\boldsymbol{h}^{T}$ , where $\\otimes$ denotes the Kronecker product. The trace of $H$ is $t r(H)=t r(\\mathrm{diag}(p)-p p^{T})*t r(h h^{T})$ . The first part could be calculated as $\\begin{array}{r}{\\sum_{c}p_{c}(1-p_{c})}\\end{array}$ , where $c$ is (the )c l=ass (index(. ", "page_idx": 20}, {"type": "image", "img_path": "CW0OVWEKKu/tmp/540e6ac0f4a5d9421aaa4af540facfe228488ed2d973c5d319682bcacdf829fd.jpg", "img_caption": ["Figure 27: The distribution of $(w+\\lambda*\\mathrm{sign}(w))^{T}h$ with $w=0.1*h+\\delta$ . $h$ and $\\delta$ are sampled from $G(0,1)$ . "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "CW0OVWEKKu/tmp/836546d197646ccb568e73aeab37c49002b6c258ca9a50ffe2864b4f75f33d85.jpg", "img_caption": ["Figure 28: The activation confusion matrix of $\\theta_{f}+\\lambda*|\\epsilon|\\mathrm{sign}(\\theta_{f})$ with $\\lambda\\in[-1.0,1.0]$ . "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "We use the \u201csklearn.digits\u201d dataset and train a softmax classification weight via the \u201cLogisticRegression\u201d classifier. The demo code is listed in Python code 2. When the training finishes, we obtain the converged weight $W$ . Then, we perturb it via the Gaussian noise $\\epsilon$ and the sign-consistent Gaussian noise $|\\bar{\\epsilon}|*\\mathrm{sign}\\bar{(W)}$ , respectively. Given a $\\lambda\\in[-1,1]$ , we calculate several metrics: (1) the test error; (2) the cross-entropy loss; (3) the average trace of $P_{\\lambda}^{\\'}=\\mathrm{diag}(p)-p p^{T}$ , i.e., $E_{x}[t r(P_{\\lambda})]$ ; (4) the trace of the Hessian matrix, i.e., $t r(H_{\\lambda})$ ; (5) the coeffici e=nt of t(he) f\u2212irst-order appr[ox(imat)i]on of $\\mathcal{L}$ w.r.p. $\\lambda$ , i.e., $\\epsilon^{T}g_{\\lambda}$ , where $g_{\\lambda}$ denotes the gradient w.r.p. $W+\\lambda|\\epsilon|*\\mathrm{sign}(W)$ ; (6) the coefficient of the second-order approximation of $\\mathcal{L}$ w.r.p. $\\lambda$ , i.e., $\\epsilon^{T}H_{\\lambda}\\epsilon$ , where $H_{\\lambda}$ denotes the Hessian matrix w.r.p. $W+\\lambda|\\epsilon|*\\mathrm{sign}(\\bar{W})$ . The abov e Lmetrics are shown in Fig. 29. The test error and cross-entropy loss in the second row show asymmetry, because the second row takes the sign-consistent noise. More specifically, with a positive $\\lambda=a$ , the average trace of $P_{\\lambda}$ is smaller than under a negative $\\lambda=-a$ . Therefore, the trace of $H_{\\lambda=a}$ is  =smaller than that of $H_{\\lambda=-a}$ . A smaller trace of Hessian means a  f=la t\u2212ter loss region. ", "page_idx": 21}, {"type": "text", "text": "The analysis from ReLU and softmax explain the observed phenomenon in this paper, i.e., the sign-consistent noise leads to valley asymmetry and the positive direction is flatter. However, the theoretical insights are not formal proofs, which are future works. ", "page_idx": 21}, {"type": "image", "img_path": "CW0OVWEKKu/tmp/c66b29c50ff6eab52191dc4cbe9b5d881afe4d7c2ef6d21ca3b40fc571a5a30b.jpg", "img_caption": ["Figure 29: Several metrics calculated on a simple softmax classification demo. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "import numpy as np   \n2 from scipy.special import softmax   \n3 from matplotlib import pyplot as plt   \n4 from sklearn.datasets import load_digits   \n5 from sklearn.linear_model import LogisticRegression   \n6 from sklearn.metrics import log_loss   \n7   \n8 def grad_hessian(W, X, Y, Z, P):   \n9 N $,\\mathrm{~\\textsf~{~D~}~}=\\mathrm{~\\textsf~{~X~}~}$ .shape   \n10 $\\textsf{C}=\\textsf{1e n}$ (np.unique(Y))   \n11 $\\tt{\\sf I d}=\\tt{\\dot{A}}$ np.tile(np.diag(np.ones(C)), (N, 1, 1))   \n12 $\\mathrm{~\\sf~{~\\sf~{~\\sf~{~I~Y~}~}~}~}=$ np.diag(np.ones(C))[Y]   \n13 $\\texttt{X X}=$ (X[:, None , :] \\* X[:, :, None ]).reshape(N, D \\* D)   \n14 PIP = (P[:, None , :] \\* (Id - P[:, :, None ])).reshape(N, $\\texttt{C}*\\texttt{C})$   \n15 g = -1.0 \\* np.dot((OY - P).T, X) / N   \n16 H = np.dot(PIP.T, XX) / N   \n17 H = H.reshape(C, C, D, D).transpose (0, 2, 1, 3).reshape $(\\texttt{C}*\\texttt{D}$ ,   \nC \\* D)   \n18 return g, H   \n19   \n20 X, Y = load_digits(return_X_y $=$ True)   \n21 model $=$ LogisticRegression (max_iter $=\\mathtt{500}$ , fit_intercept $=$ False)   \n22 model.fit(X, Y)   \n23 W = model.coef_   \n24 noise_W $=$ np.random.randn (\\*W.shape)   \n25 sign_noise_W $=$ np.abs(noise_W) \\* (2.0 \\* (W > 0.0) - 1)   \n26   \n27 for noise in [noise_W , sign_noise_W ]:   \n28 data $\\mathrm{~\\ensuremath~{~\\mathbf~{~\\mu~}~}~}=\\mathrm{~\\ensuremath~{~\\mathbf~{~\\left[~]~}~}~}$   \n29 for lamb in np.linspace (-1.0, 1.0, 21):   \n30 $\\begin{array}{r l r}{{\\mathbb{W}}{\\bf\\Sigma}}&{{}=}&{{\\mathbb{W}}{\\bf\\Sigma}+{\\bf\\Lambda}}\\end{array}$ lamb $^\\ast$ noise   \n31 $z\\ =$ np.dot(X, Wn.T)   \n32 $\\textsf{\\textsf{P}}=$ softmax(Z, axis $=\\!1$ )   \n33 g, $\\textrm{\\textbf{H}}=$ grad_hessian(W, X, Y, Z, P)   \n34 trp $=$ np.sum(P \\* (1 - P), axis $=\\!1$ ).mean ()   \n35 trh $=$ np.sum(np.diag(H))   \n36 fd $=$ np.sum(g \\* Wn)   \n37 sd $=$ np.dot(Wn.reshape (-1), np.dot(H, Wn.reshape (-1)))   \n38 loss = log_loss(y_true $\\textstyle=\\mathtt{Y}$ , y_pred $\\scriptstyle1=\\mathbf{P}$ )   \n39 err $=$ np.mean(np.argmax(P, axis $=\\!1$ ) != Y)   \n40 data.append ([trp , trh , fd , sd , loss , err])   \n41 data $=$ np.array(data)   \n42 titles $=$ [\"GiniP\", \"Tr(H)\", \"n^Tg\", \"n^THn\", \"Loss\", \"Error\"]   \n43 plt.figure(figsize $=$ (15 , 2))   \n44 for i in range (6):   \n45 plt.subplot (1, 6, i + 1)   \n46 plt.plot(data[:, i])   \n47 plt.title(titles[i])   \n48 plt.show () ", "page_idx": 22}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The abstract and introduction clearly list the paper\u2019s scope and contributions.   \nThe contributions are summarized in Sect. 1. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 23}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: The limitations and future works are presented in Sect. 7. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \u201dLimitations\u201d section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 23}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: We do not provide strict theoretical proof because it is complex for DNNs. We only provide theoretical insights to explain the observations in our paper, which does not need too many assumptions and complete proof. The theoretical insights are provided in Sect. 5 and Appendix D. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 24}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We report our experimental details and the phenomena in our paper are easy to reproduce. The details are provided in Appendix A. A demo code to reproduce our finding is provided in Appendix D.3 and Code 2. A pseudo-code for our application to federated learning is also provided in Algo. 1. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in ", "page_idx": 24}, {"type": "text", "text": "some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 25}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We provide core codes and a demo code to reproduce the observed phenomena in our paper. We do not provide codes with external links. The demo code is in Code 1 and Code 2. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 25}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Experimental settings and details are provided in the Appendix A ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 25}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: Our paper presents interesting phenomena about DNNs, which do not need error bars. However, we provide multiple groups of experimental observations to verify that the phenomena are common across various conditions. For example, multiple groups of experimental studies are provided to support our finding, e.g., the plots in Appendix B. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \u201dYes\u201d if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 26}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The experimental studies in our paper do not need too much computation budget, which could be reproduced on mainstream devices. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 26}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Our work satisfies the NeurIPS Code of Ethics. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 26}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: Our work has no societal impact because we focus on studying the basic properties of DNNs. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 27}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: Our paper does not pose such risks. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 27}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The dataset and pre-trained used in the work are publicly available, and we were unable to find the license for the dataset we used. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: We do not release new assets. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 28}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 28}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 29}]