[{"figure_path": "CW0OVWEKKu/figures/figures_1_1.jpg", "caption": "Figure 1: The illustration of investigated factors that could affect the valley symmetry. The e matters a lot.", "description": "This figure illustrates the factors influencing the symmetry of deep neural network (DNN) loss valleys.  It shows that valley symmetry is affected by both the converged model (\u03b8f) and the noise (e) added to it during 1D visualization (\u03b8f + \u03bb\u03b5, where \u03bb is in [-1,1]). The factors influencing the converged model include dataset, architecture, initialization, and hyperparameters. Factors influencing the noise include magnitude and direction. Three types of noise visualization methods are shown: raw noise, filter norm-scaled noise, and norm-scaled noise.  The norm-scaled method normalizes the noise magnitude while preserving its direction. The figure contrasts symmetric and asymmetric valleys, highlighting the crucial role of noise direction and its relationship with the converged model in determining valley symmetry.  The sign consistency (or inconsistency) between noise and model parameters directly affects whether the valley appears symmetric or asymmetric.", "section": "1 Introduction"}, {"figure_path": "CW0OVWEKKu/figures/figures_4_1.jpg", "caption": "Figure 4: The impacts of manually constructed Gaussian noise with different levels of sign consistency.", "description": "This figure shows the results of an experiment that manually controls the sign consistency between the noise and the converged model.  The x-axis represents the ratio of elements in the noise that have their signs changed to match the converged model. The y-axis shows the average test error for positive and negative interpolations. As the sign change ratio increases, the test error of positive interpolations decreases, while the test error of negative interpolations increases, demonstrating the impact of sign consistency on the valley symmetry and flatness.  The results are shown for two different datasets and network architectures (CIFAR10 VGG16BN and SVHN ResNet20) to illustrate generalizability.", "section": "4 Experimental Findings"}, {"figure_path": "CW0OVWEKKu/figures/figures_4_2.jpg", "caption": "Figure 6: Verification results on ImageNet with pre-trained ResNeXt101.", "description": "This figure shows the verification results on ImageNet using a pre-trained ResNeXt101 model.  Different noise types were applied to different parameter groups (ALL, CLF, FEAT, LAYER1, CONV, BN), and the resulting valley shapes are shown for both the original noise and the noise with its sign replaced by the sign of the converged model. The results validate the authors' finding that sign consistency between the noise and the converged model affects valley symmetry and flatness.", "section": "4 Experimental Findings"}, {"figure_path": "CW0OVWEKKu/figures/figures_5_1.jpg", "caption": "Figure 7: The impact of BN and its initialization on the valley symmetry. (VGG16 with BN on CIFAR10)", "description": "This figure visualizes how different initializations of Batch Normalization (BN) layers affect the symmetry of the loss landscape in deep neural networks. Three different initialization methods are used: setting BN weights to 1.0, sampling them from a uniform distribution U(0, 1), and sampling them from a Gaussian distribution G(0, 0.1).  The resulting loss landscapes are shown using both the Norm-Scaled noise and the Filter-NS noise, visualizing the test error across a range of \u03bb values.  The histograms show the initial and converged distributions of the BN weights. The results demonstrate that the choice of BN weight initialization significantly impacts the valley's symmetry.  Specifically, the initializations with all positive weights (1.0 and U(0,1)) demonstrate clear asymmetry, whereas the Gaussian initialization leads to near-perfect symmetry.", "section": "4.2 BN and Initialization"}, {"figure_path": "CW0OVWEKKu/figures/figures_5_2.jpg", "caption": "Figure 8: The interpolation between two models trained with batch size as 32 and 2048. (VGG16 with BN on CIFAR10)", "description": "This figure visualizes the interpolation between two models trained with different batch sizes (32 and 2048).  The left and center plots show the parameter distributions (mean and standard deviation) of the two models for different parameter groups (BN weights, classifier layer weights and biases, other weights and biases). The right plot shows the interpolation curve of the test error between the two models, demonstrating how the error changes as one varies the weights from one model to the other. The sign consistency ratio between the two models for each parameter group is also illustrated. This helps explain why interpolating between the models may be difficult due to discrepancies in the parameters, especially considering differences in their signs.", "section": "4.2 Factors that Affect Of"}, {"figure_path": "CW0OVWEKKu/figures/figures_5_3.jpg", "caption": "Figure 9: The impact of various hyperparameters on valley symmetry. (VGG16 with BN on CIFAR10)", "description": "This figure visualizes the impact of different hyperparameters (learning rate, batch size, weight decay) on the valley symmetry of deep neural networks.  It presents 1D interpolation plots of the loss landscape around a minimum, showing how the valley shape changes depending on the hyperparameter setting and whether the sign of the noise is consistent with the direction of the model parameters at convergence. The results suggest that the hyperparameters significantly affect the valley width, but don't alter its symmetry.", "section": "4.2 Factors that Affect Of"}, {"figure_path": "CW0OVWEKKu/figures/figures_8_1.jpg", "caption": "Figure 11: The models fine-tuned from a pre-trained model have a higher sign consistency ratio.", "description": "This figure visualizes the relationship between model interpolation performance and sign consistency ratio for two datasets: CIFAR10 with VGG16BN and Flowers with ResNet18.  It shows interpolation accuracy and performance gap (difference between interpolation and average individual model accuracy) against the interpolation parameter lambda.  Separate plots show the same sign ratio (SSR) between the two models' parameters across three different measures (SSR-IA, SSR-IB, SSR-AB), as well as the ratio's evolution over different training epochs. The key finding is that models fine-tuned from pre-trained models exhibit higher sign consistency, which correlates with better interpolation performance.  The contrast between randomly initialized and pre-trained models highlights the influence of pre-training on the sign consistency and its effects on model fusion.", "section": "6 Applications to Model Fusion"}, {"figure_path": "CW0OVWEKKu/figures/figures_15_1.jpg", "caption": "Figure 3: The valleys under 7 common noise types. The second row shows the results of replacing the sign of noise with that of \u03b8f, leading to asymmetric valleys. (VGG16 with BN on CIFAR10)", "description": "This figure displays the results of 1D valley visualization using different noise types. The first row shows results using seven common noise distributions (Gaussian, uniform, etc.) applied to a VGG16 network trained on CIFAR10 dataset. The second row shows the same experiment, but this time the sign of the noise is flipped to match the sign of the converged model parameters. The figure demonstrates that the sign consistency between noise and converged model plays a crucial role in determining the valley's symmetry.", "section": "4 Experimental Findings"}, {"figure_path": "CW0OVWEKKu/figures/figures_15_2.jpg", "caption": "Figure 3: The valleys under 7 common noise types. The second row shows the results of replacing the sign of noise with that of \u03b8f, leading to asymmetric valleys. (VGG16 with BN on CIFAR10)", "description": "This figure visualizes the loss landscape of a VGG16 network with Batch Normalization (BN) trained on the CIFAR-10 dataset.  The top row displays the loss curves for seven different noise types added to the converged model parameters (\u03b8f). Each curve represents the loss along a 1D subspace defined by \u03b8f + \u03bb\u03b5, where \u03bb varies from -1 to 1 and \u03b5 is a normalized noise vector. The bottom row shows the same experiment but with the sign of the noise vector flipped to match that of the converged model parameters. The comparison highlights the effect of noise direction and sign consistency on the valley's symmetry (or asymmetry). The results demonstrate that sign consistency between noise and converged solution is a critical factor for valley asymmetry.", "section": "Experimental Findings"}, {"figure_path": "CW0OVWEKKu/figures/figures_15_3.jpg", "caption": "Figure 3: The valleys under 7 common noise types. The second row shows the results of replacing the sign of noise with that of \u03b8f, leading to asymmetric valleys. (VGG16 with BN on CIFAR10)", "description": "This figure visualizes the loss landscape of a VGG16 network with batch normalization (BN) trained on the CIFAR-10 dataset. The top row shows the loss curves along seven different noise directions, demonstrating nearly symmetric valleys. The bottom row shows the loss curves when the noise sign is flipped to match the sign of the converged model (\u03b8f). This manipulation results in pronounced asymmetric valleys, illustrating how noise direction significantly influences the loss landscape's symmetry. The asymmetry is particularly prominent when the noise direction aligns with the direction of the converged model.", "section": "4 Experimental Findings"}, {"figure_path": "CW0OVWEKKu/figures/figures_16_1.jpg", "caption": "Figure 3: The valleys under 7 common noise types. The second row shows the results of replacing the sign of noise with that of \u03b8f, leading to asymmetric valleys. (VGG16 with BN on CIFAR10)", "description": "This figure visualizes the loss landscape of a VGG16 network with Batch Normalization (BN) trained on the CIFAR-10 dataset.  The top row shows the loss curves obtained using seven different types of noise added to the model parameters. The bottom row shows the effect of flipping the sign of the noise vectors before adding them. The figure demonstrates that the sign consistency between the noise and the model parameters significantly impacts the valley symmetry, resulting in asymmetric valleys when the signs are flipped.", "section": "4 Experimental Findings"}, {"figure_path": "CW0OVWEKKu/figures/figures_16_2.jpg", "caption": "Figure 5: The valley shape under 6 special noise types. (VGG16 with BN on CIFAR10)", "description": "This figure shows the loss landscape of a VGG16 network trained on CIFAR10 with batch normalization, along six different noise directions.  Each plot shows the test error as a function of lambda (\u03bb), ranging from -1 to 1, with various noise scaling factors (s).  The noise directions used are intended to explore different aspects of the model's parameter space and its relationship to the loss landscape, aiming to demonstrate the effect of noise direction on the shape of the loss valleys and how it relates to the sign consistency between the noise and the converged model. ", "section": "4.1 Factors that Affect e"}, {"figure_path": "CW0OVWEKKu/figures/figures_16_3.jpg", "caption": "Figure 5: The valley shape under 6 special noise types. (VGG16 with BN on CIFAR10)", "description": "This figure shows the loss landscape of a VGG16 network with batch normalization trained on CIFAR10, along six different noise directions.  Each plot represents a 1D interpolation of the loss function, where the starting point is the model's converged parameters (\u03b8f), and the interpolation direction is determined by one of six noise vectors.  The noise vectors are: the initialization before training (\u03b5 = \u03b80), the converged model itself (\u03b5 = \u03b8f), the sign of the converged model (\u03b5 = sign(\u03b8f)), the sign of the converged model minus its mean (\u03b5 = sign(\u03b8f \u2212 \u03bc)), and the sign and non-sign of the converged model (\u03b5 = sgp(\u03b8f), \u03b5 = sgp(\u03b8f \u2212 \u03bc)). The plots display the test error with different widths (s = {0.2, 1.0, 2.0}) along each noise direction. The results illustrate the impact of noise direction on the symmetry of the valley, which contributes to the asymmetric valley phenomenon discussed in the paper.", "section": "4 Experimental Findings"}, {"figure_path": "CW0OVWEKKu/figures/figures_17_1.jpg", "caption": "Figure 5: The valley shape under 6 special noise types. (VGG16 with BN on CIFAR10)", "description": "This figure displays the loss landscape of a VGG16 network with batch normalization trained on CIFAR-10 dataset, but with variations in the noise vectors used for 1D interpolation.  Six different types of noise vectors are shown, each impacting the valley symmetry differently.  The results highlight the effect of noise direction and magnitude on the observed shape of the loss valley, thereby supporting the paper's claims about the relationship between noise characteristics, valley symmetry, and model convergence.", "section": "4 Experimental Findings"}, {"figure_path": "CW0OVWEKKu/figures/figures_17_2.jpg", "caption": "Figure 5: The valley shape under 6 special noise types. (VGG16 with BN on CIFAR10)", "description": "This figure visualizes the loss landscape of a VGG16 network with batch normalization trained on CIFAR10 along six different noise directions. Each subfigure represents a 1D interpolation of the loss function starting from a local minimum (\u03b8f) and moving in a direction defined by a specific noise vector (\u03b5).  The noise vectors are designed to investigate the impact of various noise characteristics on the symmetry of the loss landscape.  The different types of noise vectors include: the initialization before training (\u03b80), the converged model itself (\u03b8f), the sign of the converged model (sign(\u03b8f)), the sign of the converged model minus the mean value of each parameter group (sign(\u03b8f - \u03bc)), the sign of each element of the converged model (sgp(\u03b8f)), and the sign of each element of the converged model minus the mean value of each parameter group (sgp(\u03b8f-\u03bc)). The resulting curves illustrate how these different noise directions can lead to valleys with varying degrees of symmetry,  demonstrating the effect of the noise direction on valley shape.", "section": "4 Experimental Findings"}, {"figure_path": "CW0OVWEKKu/figures/figures_17_3.jpg", "caption": "Figure 6: Verification results on ImageNet with pre-trained ResNeXt101.", "description": "This figure displays the results of applying the sign-consistent noise to different parameter groups of a pre-trained ResNeXt101 model on the ImageNet dataset.  It visually confirms that sign consistency between the noise and the converged model parameters is a key factor in determining valley asymmetry, as indicated by consistently flatter valleys in the positive direction for all parameter groups tested.", "section": "4 Experimental Findings"}, {"figure_path": "CW0OVWEKKu/figures/figures_18_1.jpg", "caption": "Figure 7: The impact of BN and its initialization on the valley symmetry. (VGG16 with BN on CIFAR10)", "description": "This figure shows how different initializations of Batch Normalization (BN) weights affect the symmetry of the loss landscape.  Three different methods are used: all ones, uniform distribution between 0 and 1, and a Gaussian distribution with mean 0 and standard deviation 0.1. The top row displays the distributions of the BN weights initially and after convergence for each initialization method.  The bottom rows show the 1D loss curves generated using two noise types (Norm-Scaled and Filter-Norm-Scaled) with different scaling factors, further illustrating the resulting valley shapes for each initialization. The results reveal a strong correlation between the BN initialization and valley symmetry, with the Gaussian initialization showing almost perfect symmetry and the other methods leading to asymmetry. ", "section": "4.2 BN and Initialization"}, {"figure_path": "CW0OVWEKKu/figures/figures_18_2.jpg", "caption": "Figure 9: The impact of various hyperparameters on valley symmetry. (VGG16 with BN on CIFAR10)", "description": "This figure shows the impact of various hyperparameters (learning rate, batch size, and weight decay) on the symmetry of the loss landscape valleys.  It presents the results of 1D interpolation along both noise and sign-consistent noise directions under different hyperparameter settings, illustrating how changes in these parameters affect the flatness and symmetry of the loss landscape.", "section": "4.2 Factors that Affect Of"}, {"figure_path": "CW0OVWEKKu/figures/figures_18_3.jpg", "caption": "Figure 6: Verification results on ImageNet with pre-trained ResNeXt101.", "description": "This figure displays the verification results on ImageNet using a pre-trained ResNeXt101 model.  It shows the valley shape under different noise directions for various parameter groups within the model. The groups include all parameters, classifier weights and biases, feature extraction weights, parameters from the first layer(s), and convolution and Batch Normalization (BN) parameters. The results are presented to confirm the findings of the study, demonstrating how sign consistency between noise and converged models affects valley shape on a different dataset and with different parameter sets.", "section": "4 Experimental Findings"}, {"figure_path": "CW0OVWEKKu/figures/figures_21_1.jpg", "caption": "Figure 27: The distribution of (w + \u03bb * sign(w))Th with w = 0.1 * h + \u03b4. h and \u03b4 are sampled from G(0,1).", "description": "The figure shows the distribution of (w + \u03bb * sign(w))Th for different values of \u03bb, where w is a weight vector, h is a hidden representation vector, and \u03b4 is a random Gaussian vector.  The distributions are plotted before and after applying the ReLU activation function.  The experiment aims to demonstrate how the distribution shifts as \u03bb changes, and to illustrate how ReLU non-linearity impacts the distribution, which is relevant to the paper's exploration of the asymmetric valley in deep neural networks. The shift is more pronounced after the ReLU.", "section": "D.2 ReLU Activation"}, {"figure_path": "CW0OVWEKKu/figures/figures_21_2.jpg", "caption": "Figure 28: The activation confusion matrix of \u03b8f + \u03bb * |e|sign(\u03b8f) with \u03bb \u2208 [\u22121.0, 1.0].", "description": "This figure shows the activation confusion matrix for different interpolation values (\u03bb) between a converged model (\u03b8f) and its sign-consistent noise (|e|sign(\u03b8f)).  The sum of diagonal values represents the agreement between the activations of the original model and the interpolated model. The figure demonstrates that the agreement is higher for positive \u03bb values (sign consistency) than for negative \u03bb values.", "section": "D Detailed Theoretical Analysis and Verification"}, {"figure_path": "CW0OVWEKKu/figures/figures_21_3.jpg", "caption": "Figure 29: Several metrics calculated on a simple softmax classification demo.", "description": "This figure displays several metrics calculated during a simple softmax classification demonstration.  The top row shows metrics for a standard noise perturbation, and the bottom row shows the same metrics with sign-consistent noise. The metrics include: prediction error, cross-entropy loss, the average trace of a matrix (Px), the trace of the Hessian matrix (Hx), the first-order approximation of the loss (\u03b5Tg\u03bb), and the second-order approximation of the loss (\u03b5TH\u03bb \u03b5).  The comparison highlights the effect of sign-consistent noise on the shape of the loss landscape, indicating that sign-consistent noise leads to a flatter loss landscape in the positive direction.", "section": "D.3 Softmax Function"}]