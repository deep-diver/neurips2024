[{"figure_path": "eC5qdC4ZTQ/tables/tables_5_1.jpg", "caption": "Table 1: Performance of the robot arm grasping task.", "description": "This table presents the performance comparison of four different methods on a real-world robot arm grasping task. The methods include MARS + TD3-BC, Multistep + TD3-BC, Dense obs + TD3-BC, and Vanilla TD3-BC.  The performance is evaluated based on three metrics: Motion Smooth Ratio (MSR), Grasp Success Rate (GSR), and Declutter Rate (DR).  The results show that MARS + TD3-BC outperforms the other methods in all three metrics.", "section": "5.2 Fixed Intermittent-MDP Tasks (RQ2)"}, {"figure_path": "eC5qdC4ZTQ/tables/tables_8_1.jpg", "caption": "Table 1: Performance of the robot arm grasping task.", "description": "This table presents the performance comparison of four different methods for a robot arm grasping task. The methods are MARS + TD3-BC, Multistep + TD3-BC, Dense obs + TD3-BC, and Vanilla TD3-BC.  The table shows the Motion Smooth Ratio (MSR), Grasp Success Rate (GSR), and Declutter Rate (DR) for each method. MARS + TD3-BC shows the best performance across all three metrics.", "section": "5.2 Performance of real-world Robot Arm Grasping"}, {"figure_path": "eC5qdC4ZTQ/tables/tables_12_1.jpg", "caption": "Table 2: Network Structures for DRL Methods", "description": "This table details the network architecture used for different Deep Reinforcement Learning (DRL) methods in the paper. It specifies the layers, dimensions, and activation functions for both the actor and critic networks.  The actor network outputs latent actions and a transition scale, while the critic network estimates the Q-value. The table provides a structured overview of the neural network components used in the experiments.", "section": "B Experimental Details"}, {"figure_path": "eC5qdC4ZTQ/tables/tables_13_1.jpg", "caption": "Table 3: Network structures for the Multi-step action representation (MARS).", "description": "This table details the architecture of the Multi-step Action Representation (MARS) model. It breaks down the network structure into components: Conditional Encoder Network and Conditional Decoder, Prediction Network.  Each component is further divided into layers, specifying the type of layer (Fully Connected), activation function (ReLU, None, Tanh), and dimension of the layer's input or output.  The table provides a clear overview of the model's architecture, showing the flow of information and the transformations performed at each step.  The dimensions specified help in understanding the complexity and scale of the model.", "section": "4.1 Scale-Conditioned Multi-step Action Encoding and Decoding"}, {"figure_path": "eC5qdC4ZTQ/tables/tables_13_2.jpg", "caption": "Table 4: A comparison of common hyperparameter choices of algorithms. We use 'None' to denote the 'not applicable' situation.", "description": "This table presents a comparison of the hyperparameters used in different reinforcement learning algorithms across various experiments.  It shows the settings for actor learning rate, critic learning rate, representation model learning rate, discount factor, batch size, and buffer size for Frameskip-TD3, Multistep-TD3, MARS-PPO, MARS-TD3, and MARS-DDPG. Notably, the representation model learning rate is 'None' for the first two algorithms as they do not utilize a representation model.", "section": "B.2 Hyperparameter"}, {"figure_path": "eC5qdC4ZTQ/tables/tables_14_1.jpg", "caption": "Table 5: Comparison between MBRL and MFRL in intermittent control tasks, average of 3 runs.", "description": "This table compares the performance of model-based reinforcement learning (MBRL) and model-free reinforcement learning (MFRL) methods on intermittent control tasks.  It shows that MARS-TD3, a model-free approach, outperforms the model-based methods (TD-MPC and Dreamer-v2) across different intermittent task scenarios (fixed and random interruptions in Ant and Hopper environments). The results highlight the limitations of model-based methods in handling the accumulation of errors from the dynamic model prediction in multi-step decision making.", "section": "C.1 Performance of model-based reinforcement learning algorithms on Intermittent-MDP tasks"}, {"figure_path": "eC5qdC4ZTQ/tables/tables_15_1.jpg", "caption": "Table 6: The parameters of all methods are optimized by grid search. The results of applying MARS to popular RL algorithms on three random interaction interval tasks. The maximum interaction interval is set to 8. Each data in the table is in the following format: MARS-RL score | the score difference compared to the perfect dense interaction baseline.  denotes the score of MARS lower than the dense interaction baseline. \u2191 denotes the score of MARS is higher. All scores are averaged over 5 runs.", "description": "This table shows the results of applying MARS to three popular reinforcement learning algorithms (PPO, DDPG, TD3) on three random interaction interval tasks (Maze hard, Hopper, Walker).  The maximum interaction interval used was 8.  Each entry in the table shows the MARS algorithm's score and the difference between its score and the score obtained with a perfect, dense interaction baseline (i.e. no intermittent control problem).  An upward-pointing arrow indicates that MARS outperformed the baseline, and a downward-pointing arrow indicates it underperformed.  All scores are averages over 5 runs.", "section": "5.1 Random Intermittent-MDP Tasks (RQ1)"}, {"figure_path": "eC5qdC4ZTQ/tables/tables_17_1.jpg", "caption": "Table 5: Comparison between MBRL and MFRL in intermittent control tasks, average of 3 runs.", "description": "This table compares the performance of model-based reinforcement learning (MBRL) and model-free reinforcement learning (MFRL) methods on intermittent control tasks.  It shows the average reward achieved by TD-MPC, Dreamer-v2, a multi-step TD3, and MARS-TD3 on Ant and Hopper environments under both fixed and random intermittent interaction settings. The results demonstrate that the model-free approach (MARS-TD3) significantly outperforms the model-based methods.", "section": "C.1 Performance of model-based reinforcement learning algorithms on Intermittent-MDP tasks"}]