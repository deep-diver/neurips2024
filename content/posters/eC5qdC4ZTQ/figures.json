[{"figure_path": "eC5qdC4ZTQ/figures/figures_1_1.jpg", "caption": "Figure 1: Detail of Intermitted Control (top part). Two types of intermittent interactions (bottom part). For both cases, we can generate a sequence of actions in advance for the next c states based on the current state st to make the control more smooth and robust.", "description": "The figure illustrates the intermittent control problem where interactions between the decision maker and the executor are discontinuous due to various interruptions. The top part shows a bidirectional communication blockage, where the agent cannot acquire the state sent by the executor and cannot transmit actions to the executor within a period of time step. The bottom part illustrates two types of intermittent interactions: a fixed interaction interval, and a random interaction interval.  The figure also shows that for both cases, smooth and robust control can be achieved by generating a sequence of actions in advance for the next c states based on the current state.", "section": "Intermittent Control Markov Decision Process (Intermittent-MDP)"}, {"figure_path": "eC5qdC4ZTQ/figures/figures_2_1.jpg", "caption": "Figure 2: Conceptual overview of MARS.", "description": "The figure illustrates the MARS framework's architecture.  It begins with a decision maker (agent) receiving intermittent state and reward information from the environment. The decision maker uses this information to select a latent action in a compact latent action space.  This latent action is then decoded into a sequence of original multi-step actions, which is then sent to the environment to be executed. The resulting action trajectory represents the agent's actions within the environment across multiple time steps, despite the intermittent communication.", "section": "4 Multi-step Action Representation"}, {"figure_path": "eC5qdC4ZTQ/figures/figures_4_1.jpg", "caption": "Figure 2: Conceptual overview of MARS.", "description": "The figure provides a conceptual overview of the Multi-step Action Representation (MARS) framework. It illustrates the process of how MARS encodes a sequence of actions from the original action space into a compact latent space and how this latent space representation is used in conjunction with a reinforcement learning (RL) policy to generate effective actions for intermittent control tasks.  The left side depicts the MARS framework, showing the encoder mapping the original multi-step action and state information into a compact latent representation. The decoder then reconstructs the original multi-step actions from the latent representation. The right side depicts the interaction with the environment, with the RL policy selecting latent actions that are decoded into original actions to be executed in the environment.", "section": "4 Multi-step Action Representation"}, {"figure_path": "eC5qdC4ZTQ/figures/figures_6_1.jpg", "caption": "Figure 4: Comparisons of methods in simulated remote NPC control tasks with random interaction interval. The x- and y-axis denote the environment steps and average episode reward. Curves and shades denote the mean and the standard deviation over 8 runs.", "description": "This figure compares the performance of four different methods (Frameskip-TD3, MARS-TD3, Multistep-TD3, and Perfect-TD3) on six simulated robotic control tasks across different interaction delays.  The x-axis represents the number of environment steps, and the y-axis represents the average episode reward.  Each line shows the average reward over eight runs, with shaded regions indicating the standard deviation. The figure demonstrates the superior performance of MARS-TD3 in handling intermittent interactions compared to the other methods.  The Perfect-TD3 line serves as a benchmark showing optimal performance in a non-intermittent setting.", "section": "5.1 Random Intermittent-MDP Tasks (RQ1)"}, {"figure_path": "eC5qdC4ZTQ/figures/figures_7_1.jpg", "caption": "Figure 5: A complete grasp process in each interaction interval.", "description": "This figure shows the four steps involved in a single grasp process within each interaction interval in a robotic arm grasping task.  Step 1 initializes the manipulator to a specified position. Step 2 shows the robot moving to a designated location for observation using a camera. Step 3 depicts the robot grasping an object based on the observation. Finally, Step 4 shows the robot moving the grasped object to a designated box.", "section": "5.2 Fixed Intermittent-MDP Tasks (RQ2)"}, {"figure_path": "eC5qdC4ZTQ/figures/figures_8_1.jpg", "caption": "Figure 4: Comparisons of methods in simulated remote NPC control tasks with random interaction interval. The x- and y-axis denote the environment steps and average episode reward. Curves and shades denote the mean and the standard deviation over 8 runs.", "description": "This figure compares the performance of different reinforcement learning methods on simulated remote NPC control tasks with random interaction intervals.  Four different Mujoco environments (Hopper, Ant, Walker, HalfCheetah) and two maze environments (Maze-medium, Maze-hard) are used. The x-axis represents the number of environment steps, and the y-axis represents the average episode reward.  Each line represents a different method: MARS-TD3, Multistep-TD3, Frameskip-TD3, and Perfect-TD3. MARS-TD3 consistently outperforms the other methods across all environments. The shaded area around each line shows the standard deviation, indicating the variability in performance across multiple runs. The results suggest that MARS-TD3 is more robust and efficient in handling the challenges of intermittent control.", "section": "5.1 Random Intermittent-MDP Tasks (RQ1)"}, {"figure_path": "eC5qdC4ZTQ/figures/figures_14_1.jpg", "caption": "Figure 4: Comparisons of methods in simulated remote NPC control tasks with random interaction interval. The x- and y-axis denote the environment steps and average episode reward. Curves and shades denote the mean and the standard deviation over 8 runs.", "description": "This figure compares the performance of four different methods in simulated remote NPC control tasks with random interaction intervals. The methods compared are Frameskip-TD3, MARS-TD3, Multistep-TD3, and Perfect-TD3.  The x-axis represents the number of environment steps, and the y-axis represents the average episode reward.  Each line represents the average reward over 8 runs, with shaded areas indicating the standard deviation. The figure shows that MARS-TD3 consistently outperforms the other methods across all four simulated tasks (Hopper, Ant, Walker, and HalfCheetah).", "section": "5.1 Random Intermittent-MDP Tasks (RQ1)"}, {"figure_path": "eC5qdC4ZTQ/figures/figures_15_1.jpg", "caption": "Figure 4: Comparisons of methods in simulated remote NPC control tasks with random interaction interval. The x- and y-axis denote the environment steps and average episode reward. Curves and shades denote the mean and the standard deviation over 8 runs.", "description": "This figure compares the performance of four different methods in six simulated remote NPC control tasks with random interaction intervals. The x-axis represents the number of environment steps, and the y-axis represents the average episode reward.  Each line shows the performance of a specific method (Frameskip-TD3, MARS-TD3, Multistep-TD3, and Perfect-TD3). The shaded area around each line indicates the standard deviation over 8 runs.  The figure demonstrates MARS-TD3 generally outperforms other methods in these tasks.", "section": "5.1 Random Intermittent-MDP Tasks (RQ1)"}, {"figure_path": "eC5qdC4ZTQ/figures/figures_15_2.jpg", "caption": "Figure 4: Comparisons of methods in simulated remote NPC control tasks with random interaction interval. The x- and y-axis denote the environment steps and average episode reward. Curves and shades denote the mean and the standard deviation over 8 runs.", "description": "This figure compares the performance of four different methods in simulated remote NPC control tasks with random interaction intervals. The x-axis represents the number of environment steps, and the y-axis represents the average episode reward.  Four methods are compared: Frameskip-TD3, MARS-TD3, Multistep-TD3, and Perfect-TD3. Each line represents the average reward over 8 runs, and the shaded area represents the standard deviation. The figure shows that MARS-TD3 consistently outperforms the other methods, demonstrating its effectiveness in handling intermittent interactions.", "section": "5.1 Random Intermittent-MDP Tasks (RQ1)"}, {"figure_path": "eC5qdC4ZTQ/figures/figures_16_1.jpg", "caption": "Figure 4: Comparisons of methods in simulated remote NPC control tasks with random interaction interval. The x- and y-axis denote the environment steps and average episode reward. Curves and shades denote the mean and the standard deviation over 8 runs.", "description": "The figure compares the performance of different reinforcement learning methods in simulated remote NPC control tasks with random interaction intervals.  The x-axis shows the number of environment steps, and the y-axis shows the average episode reward.  Four different Mujoco environments (Hopper, Ant, Walker, HalfCheetah) and two maze environments (Maze-medium, Maze-hard) are evaluated. Each line represents the average reward obtained by a particular method (Frameskip-TD3, MARS-TD3, Multistep-TD3, Perfect-TD3), and the shaded area indicates the standard deviation across eight independent runs. This visualization helps understand how MARS-TD3 performs compared to other baselines in handling tasks with intermittent interactions.", "section": "5.1 Random Intermittent-MDP Tasks (RQ1)"}]