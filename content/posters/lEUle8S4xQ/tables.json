[{"figure_path": "lEUle8S4xQ/tables/tables_5_1.jpg", "caption": "Table 1: Comparison among various fine-tuning methods for the LLaMA-7B/13B, LLaMA2-7B, and LLaMA3-8B models on eight commonsense reasoning tasks. Non-PEFT methods are marked in gray. (1: from DORA paper, 2: from ReFT paper, 3: reproduced by us, \u2020: projected trainable parameters)", "description": "This table compares the performance of various fine-tuning methods (full fine-tuning, LoRA, DORA, Galore, LoReFT, LISA, and S2FT) on eight commonsense reasoning tasks using three different LLaMA models (LLaMA-7B, LLaMA-13B, LLaMA2-7B, and LLaMA3-8B).  The table shows the percentage of parameters used for each method, and the accuracy achieved on each task.  Non-PEFT methods are highlighted in gray to distinguish them from parameter-efficient methods.", "section": "5.1 Commonsense Reasoning"}, {"figure_path": "lEUle8S4xQ/tables/tables_6_1.jpg", "caption": "Table 2: Comparison among various fine-tuning methods for different models on seven math reasoning tasks. Non-PEFT methods are marked in gray. (1: from LLM-Adapters paper, 2: reproduced by us)", "description": "This table compares the performance of various fine-tuning methods (Full FT, Prefix-Tuning, Series Adapter, Parallel Adapter, LoRA, DoRA, and S2FT) across different language models (LLaMA-7B, LLaMA-13B, LLaMA2-7B, and LLaMA3-8B) on seven math reasoning tasks.  The table shows the percentage of parameters used by each method, and the accuracy achieved on each task. Non-PEFT methods are highlighted in gray for easy comparison.", "section": "5.2 Arithmetic Reasoning"}, {"figure_path": "lEUle8S4xQ/tables/tables_7_1.jpg", "caption": "Table 3: Performance comparison of LLM fine-tuning methods trained on the Alpaca GPT-4 dataset. We report the MT-Bench score as the evaluation metric. All baseline results are cited from LISA [48].", "description": "This table compares the performance of various LLM fine-tuning methods (Vanilla, Full FT, LoRA, Galore, LISA, and S2FT) on the Alpaca GPT-4 dataset, using the MT-Bench score as the evaluation metric.  The table shows the average scores across eight different aspects of the MT-Bench benchmark (Writing, Roleplay, Reasoning, Code, Math, Extraction, STEM, and Humanities) for both the Mistral-7B and LLaMA2-7B models.  It highlights the relative performance improvements of S2FT compared to existing methods.", "section": "5.3 Instruction Following"}, {"figure_path": "lEUle8S4xQ/tables/tables_7_2.jpg", "caption": "Table 4: Comparison of various channel selection strategies on the commonsense and arithmetic reasoning datasets for the LLama3-8B. We report the average accuracy (%) as the evaluation metric.", "description": "This table compares different channel selection strategies within the S2FT method for Llama3-8B model on commonsense and arithmetic reasoning tasks. It shows the average accuracy achieved by using different selection methods (S2FT-R, S2FT-W, S2FT-A, S2FT-S, S2FT-G) with both large and small subsets of channels.  The numbers in parentheses indicate the performance difference compared to the baseline method (S2FT-R).", "section": "5.4 Design Choices for Trainable Parameter Allocations"}, {"figure_path": "lEUle8S4xQ/tables/tables_8_1.jpg", "caption": "Table 5: Adapter Fusion Results for LoRA and S2FT trained on the commonsense and arithmetic reasoning datasets using the LLama3-8B. We report the average accuracy (%) as the evaluation metric.", "description": "This table compares the performance of LoRA and S2FT when combining adapters trained separately on commonsense and arithmetic reasoning tasks. It shows the accuracy for each task (Commonsense and Arithmetic) when using LoRA, and for S2FT with both overlapped and non-overlapped parameters.  The numbers in parentheses represent the performance difference compared to the fused model.", "section": "6.2 Serving Scalability"}, {"figure_path": "lEUle8S4xQ/tables/tables_15_1.jpg", "caption": "Table 6: Hyperparameter configurations of S2FT on various base models across three tasks.", "description": "This table details the hyperparameter settings used for training the S2FT model on three different tasks: Commonsense Reasoning, Arithmetic Reasoning, and Instruction Following.  For each task, it specifies the optimizer used (AdamW), the learning rate (LR), the learning rate scheduler (linear or cosine), the batch size, the number of warmup steps, and the number of epochs.", "section": "5.4 Design Choices for Trainable Parameter Allocations"}]