[{"type": "text", "text": "S2FT: Efficient, Scalable and Generalizable LLM Fine-tuning by Structured Sparsity ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xinyu Yang1, Jixuan Leng1, Geyang $\\mathbf{Guo}^{2}$ , Jiawei Zhao3, Ryumei Nakada4, Linjun Zhang4, Huaxiu $\\mathbf{Yao^{5}}$ , Beidi Chen1 ", "page_idx": 0}, {"type": "text", "text": "1CMU, 2Georgia Tech, 3Caltech, 4Rutgers, 5UNC-Chapel Hill xinyuya2, beidic@andrew.cmu.edu ", "page_idx": 0}, {"type": "text", "text": "https://infini-ai-lab.github.io/S2FT ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Current PEFT methods for LLMs can achieve either high quality, efficient training, or scalable serving, but not all three simultaneously. To address this limitation, we investigate sparse fine-tuning and observe a remarkable improvement in generalization ability. Utilizing this key insight, we propose a family of Structured Sparse Fine-Tuning $(\\mathbf{S}^{2}\\mathbf{F}\\mathbf{T})$ methods for LLMs, which concurrently achieve state-of-theart fine-tuning performance, training efficiency, and inference scalability. $\\mathrm{\\DeltaS^{2}F T}$ accomplishes this by \u201cselecting sparsely and computing densely\u201d. It selects a few heads and channels in the MHA and FFN modules for each Transformer Block, respectively. Next, it co-permutes weight matrices on both sides of the coupled structures in LLMs to connect the selected components in each layer into a dense submatrix. Finally, $\\mathrm{S^{2}F T}$ performs in-place gradient updates on all submatrices. Through theoretical analysis and empirical results, our method prevents overftiting and forgetting, delivers SOTA performance on both commonsense and arithmetic reasoning with $4.6\\%$ and $1.3\\bar{\\%}$ average improvements compared to LoRA, and outperforms full FT by $11.5\\%$ when generalize to various domains after instruction tuning. By integrating our partial back-propagation algorithm, $\\mathrm{S^{2}F T}$ saves the finetuning memory up to $3\\times$ and improves the latency by $1.5{-}2.7\\times$ compared to full FT, while delivering an average $10\\%$ improvement over LoRA on both metrics. We further demonstrate that $\\mathrm{S^{2}F\\bar{T}}$ can be decoupled into adapters, enabling effective fusion, fast switch, and efficient parallelism for serving multiple fine-tuned models. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recently, Large Language Models (LLMs) have achieved significant success [16, 1, 64]. With these models being applied in diverse domains, full fine-tuning (FT) is commonly employed to enhance their downstream capabilities [54, 6, 71]. However, retraining all parameters comes with three drawbacks: (i) Full FT suffers from catastrophic forgetting, where a model forgets pre-trained knowledge while acquiring new information [44, 8]. (ii) As the model and dataset sizes grow at scale, full FT becomes increasingly computation-demanding and memory-intensive [68]. (iii) It is impractical to store and serve thousands of fine-tuned LLMs on modern GPUs if each requires full parameter storage [77, 58]. ", "page_idx": 0}, {"type": "text", "text": "Parameter-efficient fine-tuning (PEFT) methods propose to address these bottlenecks by updating a small fraction of parameters [21]. Rather than merely reducing the number of learnable parameters, an ideal PEFT method should possess three key properties to be practically effective and efficient: ", "page_idx": 0}, {"type": "text", "text": "High Quality: It should exhibit both memorization and generalization capabilities, balancing the acquisition of new information from fine-tuning tasks with the retention of pre-trained knowledge. ", "page_idx": 0}, {"type": "text", "text": "Efficient Training: It should minimize the memory footprint for model gradient and optimization states, and further translate such memory efficiency into less computation and fine-tuning speedup. Scalable Serving: It should avoid adding inference overhead when serving a single PEFT model. For multiple models, new parameters should be partially stored as adapters to save memory, and allows for effective fusion [75], fast switch [33], and efficient parallelism [58] among thousands of adapters. ", "page_idx": 0}, {"type": "image", "img_path": "lEUle8S4xQ/tmp/56cc0b03a017155617146d5ee5cb7e43d708b8e2eb777ea8977560433069610d.jpg", "img_caption": ["Step 1: Select sparsely with coupled structures "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: An Overview of the $\\mathbf{S^{2}F T}$ Family for LLMs: First, we perform sparse selection of specific attention heads and channels within the coupled structures of the MHA and FFN modules. Next, we apply co-permutation to the weight matrices on both sides of these structures, enabling dense gradient computation only for the selected components. While we demonstrate $S^{2}\\mathrm{FT}$ by selecting the same heads/channels on both sides for clarity, our approach also supports asymmetric selection strategies. ", "page_idx": 1}, {"type": "text", "text": "However, achieving all the aforementioned goals simultaneously is challenging. Common PEFT approaches, such as LoRA [27], DoRA [38], and Galore [76], project the model\u2019s weights or gradients onto a low-rank subspace. While this significantly reduces memory footprint, their performance lags behind full fine-tuning in most large-scale scenarios. Recent state-of-the-art PEFT methods have aimed to improve performance but at the cost of serving efficiency. ReFT operates on a frozen base model and learns task-specific interventions on hidden representations that cannot be merged into the original model, leading to a $2.2\\times$ increase in inference latency. LISA [48] employs a coarse-grained selective method by randomly freezing most Transformer blocks during optimization, which requires significantly more trainable parameters. Consequently, in scaled serving settings like S-LoRA [58], LISA can only serve at most $\\left.\\frac{1}{10}\\right.$ as many fine-tuned models as LoRA under the same memory budget. ", "page_idx": 1}, {"type": "text", "text": "Prior to the era of LLMs, PEFT methods based on unstructured sparse fine-tuning (SpFT) have shown a strong trade-off between low number of parameters and high model performance without sacrificing serving efficiency [61, 3, 69]. We hypothesize that SpFT, which selectively updates a small subset of model parameters, can outperform LoRA and its variants in generalization capabilities. In Figure 2, our findings across various generalization tasks support this hypothesis. However, the unstructured nature of SpFT necessitates sparse operations in computation, hindering its efficient training and scalable serving on modern hardware. This makes SpFT less practical for adapting LLMs at scale. ", "page_idx": 1}, {"type": "text", "text": "In this work, we propose a family of Structured Sparse Fine-Tuning $(\\mathbf{S}^{2}\\mathbf{F}\\mathbf{T})$ methods to \u201cselect sparsely and compute densely\u201d (See Figure 1), thereby closing the efficiency gap in SpFT. Inspired by structured weight pruning techniques [45, 42], we first identify several coupled structures inherent in LLMs that are connected by intermediate activations. For example, in the multi-head attention (MHA) module, each attention head in the query, key, and value projections is linked to only a few rows in the output projection. Similarly, in the feed-forward network (FFN) module, each column in the up and gate projections corresponds to a single row in the down projection. By co-permuting the matrices on both sides of these coupled structures, we can preserve the original output of these structures, with only the order of the intermediate activations changed. Exploiting this property, our $S^{2}\\mathrm{FT}$ strategically selects a subset of attention heads for the MHA module and a subset of channels for the FFN module. We then permute the coupled structures to connect the selected components within each linear layer into a dense submatrix. Finally, through our partial back-propagation algorithm with only two-line code modification, $S^{2}\\mathrm{FT}$ performs in-place gradient updates exclusively for all selected submatrices, boosting training efficiency by eliminating redundant forward activations and backward calculation. ", "page_idx": 1}, {"type": "text", "text": "Through our theoretical analysis, we demonstrate that $\\mathrm{S^{2}F T}$ mitigates overftiting and forgetting under distribution shifts. Empirically, $\\mathrm{S^{2}F T}$ outperforms other PEFT methods on LLaMA and Mistral family models, improving $1.2{-}4.1\\%$ on commonsense reasoning tasks and $0.6\u20131.9\\%$ on arithmetic reasoning ones. It also surpasses full FT by $11.5\\%$ when generalize to various domains after instruction tuning. ", "page_idx": 1}, {"type": "text", "text": "Finally, we conduct a comprehensive analysis to verify the training efficiency and serving scalability of $\\mathrm{S^{2}F T}$ . Compared to existing PEFT methods, $S^{2}\\mathrm{FT}$ not only saves $1.4\u20133.0\\times$ memory, but also increases latency by 1.5 to $2.7\\times$ , making LLM fine-tuning more accessible. Additionally, $\\mathrm{S^{2}F T^{:}}$ \u2019s parameter updates can be decomposed into adapters, enabling adapter fusion with smaller performance drop than LoRA. Our method also results in more scalable and efficient adapter switch and parallelism through reduced matrix multiplications, showcasing strong potential for large-scale LLM serving scenarios. ", "page_idx": 1}, {"type": "text", "text": "2 Memorization or Generalization? ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we evaluate the memorization and generalization capabilities of various fine-tuning methods, including full FT, LoRA, and SpFT. We hypothesize that SpFT can generalize better to downstream tasks. To support this hypothesis, we present detailed observations and analyses. Further theoretical analysis about the generalization capabilities of the $S^{2}\\mathrm{FT}$ family can be found in Section 4. ", "page_idx": 2}, {"type": "text", "text": "Hypothesis. We hypothesize that SpFT offers superior generalization than both full FT and LoRA, while maintaining comparable memorization to LoRA with the same number of trainable parameters. ", "page_idx": 2}, {"type": "text", "text": "Experimental Setup. We fine-tune the Llama3-8B on the Math10K data [28] using SpFT, LoRA, and full FT. In addition to training losses, accuracies are measured on downstream tasks in LLM-Adapters, including near out-of-distribution (OOD) generalization on both easy (i.e, MultiArith, AddSub, SingleEq, MAWPS) and hard (i.e, GSM8K, AQuA, SVAMP) arithmetic reasoning tasks, and far OOD generalization on commonsense reasoning ones. For PEFT methods, we set three ratios of trainable parameters $\\langle p=10\\%$ , $1\\%$ , $0.1\\%$ ) and search for the optimal hyperparameters on the valid set. In SpFT, trainable parameters are selected randomly with given ratios. See details in Appendix C. ", "page_idx": 2}, {"type": "image", "img_path": "lEUle8S4xQ/tmp/87b8422e5d816d26c4b1e23adef0821f41cf89c607856c515044a368f4b30d6e.jpg", "img_caption": ["Figure 2: Accuracy comparison of SpFT, LoRA and Full FT at varying ratios of trainable parameters in various settings. SpFT exhibits strong generalization ability while full FT excels in memorization. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Observations. Figure 2 indicates several key findings. First, SpFT achieves lower training losses than LoRA when using the same ratio of trainable parameters, especially at very small ratios. This gap arises from the more complex optimization process in LoRA, which requires the simultaneous updating of two matrices [23]. Second, we observe both elevated training loss and reduced average accuracy on easier math tasks as the ratio decreases, suggesting a positive correlation between memorization abilities and trainable parameters. Notably, with only $10\\%$ of the parameters updated, PEFT methods learn comparable memorization abilities to full FT when trained on a $10\\mathbf{k}$ -sample dataset. ", "page_idx": 2}, {"type": "text", "text": "When generalizing to complex mathematical problems or commonsense reasoning tasks, the performance ranking emerges as: SpFT $>$ Full FT $>$ LoRA. SpFT effectively transfers reasoning abilities to commonsense domains, while LoRA exhibits significant performance drops in far OOD generalization. This indicates (i) freezing a larger fraction of the parameters can retain more pre-trained abilities, and (ii) approximating high-dimensional gradients with low-rank decomposition may overfti fine-tuned data and hinder the model from generalization. Since LLMs are pre-trained on high-quality data, SpFT emerges as the preferred choice for fine-tuning on task-specific data of varying quality. ", "page_idx": 2}, {"type": "text", "text": "3 The $\\mathbf{S^{2}F T}$ family of methods ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "While SpFT demonstrates strong generalization ability and good overall performance in Section 2, its unstructured nature poses challenges for efficient training and scalable serving on modern hardware (e.g., GPU). This is because of the need for sparse operations when storing and computing weights, gradients, and optimization states, which are significantly slower than their dense variants on GPU. This motivates our investigation into structured sparsity approaches that utilize only dense operations: Can structured sparsity improve hardware efficiency while preserving performance by selecting sparsely but computing densely? If so, how far can the flexibility of selection be pushed in this context? To answer this question, we design a family of Structured Sparse Fine-Tuning $(\\mathbf{S}^{2}\\mathbf{F}\\mathbf{T})$ methods with dense-only computations, making PEFT effective, efficient and scalable. We begin by discovering the coupled structure in LLMs in Section 3.1. Leveraging this property, Section 3.2 introduce the selection and permutation strategies of $\\mathrm{S^{2}F T}$ , with overall pipeline illustrated in Figure 1b. In Section 3.3, we present our partial back-propagation algorithm that enables end-to-end training latency reduction. ", "page_idx": 2}, {"type": "text", "text": "3.1 Discover Coupled Structures in LLMs ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We initiate our pursuit of flexible structured sparsity by examining the coupled structures in LLMs. ", "page_idx": 2}, {"type": "image", "img_path": "lEUle8S4xQ/tmp/40fd2398b18ca6e1037cff349acab736b8dc8fe4f338c501ddc1c114019dbdc3.jpg", "img_caption": ["(a) Basic Structure ", "(b) Residual Structure "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 3: Grouped model weights with basic structure and residual structure. All highlighted weights must be permuted simultaneously. Residual structures require additional permutation during runtime. ", "page_idx": 3}, {"type": "text", "text": "Structure Dependency in LLMs. Inspired by prior work on structured pruning [45, 17], our study start by building the dependencies between activations and weights for LLMs. Let $A$ denote an activation and $W$ denote a weight in the model. We define $\\operatorname{In}(A)$ as the set of parameters that directly contribute to the computation of $A$ , and $\\operatorname{Out}(A)$ as the set of parameters that depend on $A$ in the computation of subsequent activations. The dependency between structures can be defined as follows: ", "page_idx": 3}, {"type": "text", "text": "where $\\mathrm{Deg}^{+}(W_{1})$ represents the out-degree of weight $W_{1}$ , and $\\mathrm{Deg}^{-}(W_{2})$ represents the in-degree of weight $W_{2}$ . Each equation represents a unqiue directional dependency between activations and weights. When both equations hold simultaneously, a coupled structure exists between $W_{1}$ and $W_{2}$ . In Figure 3, we employ deep linear networks to illustrate two types of coupled structures in LLMs: ", "page_idx": 3}, {"type": "text", "text": "Basic Structures: In Figure 3a, these structures exist in both the multi-head attention (MHA) and feed-forward network (FFN) modules. Taking LLaMA as an example, in the MHA module, we consider the Query $(Q)$ , Key $(K)$ , and Value $(V)$ projections as $W_{1}$ , and the Output $(O)$ projection as $W_{2}$ , while Softmax $(Q K^{\\top})V(x)$ acting as the activation between weight matrices. Similarly, in the FFN module, the Up $(U)$ and Gate $(\\pmb{G})$ projections function as $W_{1}$ , with the Down $(D)$ projection corresponding to $W_{2}$ . Here, $U(x)\\cdot\\operatorname{SwiGLU}(G(x))$ serves as the activations connecting $W_{1}$ and $W_{2}$ . Residual Structures: In Figure 3b, this type of coupled structures exists between the MHA and FFN modules. We further consider how residual connections influence the activations in these structures. ", "page_idx": 3}, {"type": "text", "text": "Permutation Invariance of Coupled Structures. Figure 3 demonstrates that $W_{1}$ and $W_{2}$ can be co-permuted using the same order, which only affects the order of activations between them while preserving the original output from the coupled structure. Since residual dependencies require an additional runtime step to permute the residuals, we will focus on basic dependencies in our method. ", "page_idx": 3}, {"type": "text", "text": "3.2 Sparse Selection and Permutation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "At this point, all coupled structures within the model have been identified. The subsequent sparse selection and permutation processes are straightforward, with overall pipeline illustrated in Figure 1b. ", "page_idx": 3}, {"type": "text", "text": "MHA Module: There are four linear layers in a MHA module: $Q,K,V,O\\in\\mathbb{R}^{d\\times d}$ . For a model with $h$ attention heads, each head $i\\in[h]$ has its own projections denoted as $Q_{i}\\in\\mathbb{R}^{d\\times d_{h}}$ , $K_{i}\\in\\mathbb{R}^{d\\times d_{h}}$ , $V_{i}\\in\\mathbb{R}^{d\\times d_{h}}$ , and $O_{i}\\in\\mathbb{R}^{d_{h}\\times d}$ , where $d_{h}=d/h$ is the dimension per head. Let $S_{\\mathrm{MHA}}\\subseteq[h]$ denotes a small subset of attention heads. By permuting $S_{\\mathrm{MHA}}$ to the beginning of each weight matrix, we are able to update these selected heads using dense-only operations, while keeping the other ones frozen. ", "page_idx": 3}, {"type": "text", "text": "FFN Module: There are three linear layers in a FFN module: $U,G\\in\\mathbb{R}^{k\\times d}$ and $D\\in\\mathbb{R}^{d\\times k}$ . In $S^{2}\\mathrm{FT}$ , only a few channels of them require gradient updates. Let $S_{\\mathrm{FFN}}\\subseteq[d]$ denote the selected channels. We can permute $S_{\\mathrm{FFN}}$ to the beginning of each weight matrix and only fine-tune this compact subset. Next, we provide several strategies for identifying and selecting important subsets in each module. ", "page_idx": 3}, {"type": "text", "text": "1. $\\mathbf{S}^{2}\\mathbf{F}\\mathbf{T}\\mathbf{-}\\mathbf{R}$ $(\\mathbf{S}^{2}\\mathbf{F}\\mathbf{T})$ : In this strategy, a subset of channels is randomly selected and set to be trainable. 2. S2FT-W: This variant selects subsets based on the magnitude of activations on a calibration set. 3. $\\mathbf{S}^{2}\\mathbf{F}\\mathbf{T}\\mathbf{-A}$ : This variant selects subsets based on the magnitude of activations on a calibration set. 4. $\\mathbf{S}^{2}\\mathbf{F}\\mathbf{T}-\\s\\mathbf{S}$ : Top-K subsets are ranked and selected by the product of weight and activation magnitudes. 5. $\\mathbf{S}^{2}$ FT-G: This variant selects subsets based on the magnitude of gradients on a calibration set. Here, 1 and 2 can be directly applied without preprocessing. 3 and 4 only require a forward pass on a small calibration dataset. While 5 necessitates a backward pass on this dataset, it does not store optimization states and can mitigate memory footprints for activations through gradient checkpointing [18]. By default, we use $S^{2}\\bar{\\mathrm{FT}}-\\mathrm{I}$ R for a fair comparison and discuss other variants in Section 5.4. ", "page_idx": 3}, {"type": "text", "text": "3.3 Partial Back-propagation Algorithm ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Finally, we introduce our partial back-propagation algorithm with only two line modifications in PyTorch. our algorithm stores trainable channels based on their start and end positions, thereby improving training efficiency by eliminating redundant forward activations and backward calculations. ", "page_idx": 4}, {"type": "text", "text": "def setup_context(ctx, inputs, output): ", "page_idx": 4}, {"type": "text", "text": "activation, weight, bias, start, end $=$ inputs # only save partial input tensors for gradient calculation in forward ctx.save_for_backward(activation[:, start:end], weight, bias, start, end) ", "page_idx": 4}, {"type": "text", "text": "def gradient_update(parameter, gradient, start, end): ", "page_idx": 4}, {"type": "text", "text": "# only modify the assigned positions of weight matrices during optimization parameter[:, start:end].add_(gradient) ", "page_idx": 4}, {"type": "text", "text": "4 Theoretical Analysis ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we theoretically explore why $\\mathrm{S^{2}F T}$ demonstrates stronger generalization capabilities compared to LoRA. We consider a pre-trained $L$ -layer deep linear networks, which has been widely used to facilitate the theoretical analysis of complex DNNs [57, 30, 43, 22, 34, 5]. Let $f^{\\mathrm{pre}}(x):=$ $W_{L}^{\\mathrm{pre}}W_{L-1}^{\\mathrm{pre}}\\cdot..\\cdot W_{1}^{\\mathrm{pre}}x$ be the pre-trained deep linear network, where $W_{\\ell}^{\\mathrm{pre}}\\in\\mathbb{R}^{d_{\\ell}\\times d_{\\ell-1}}$ , with $d_{0}=p$ $\\begin{array}{r}{s=\\lfloor r\\cdot\\frac{d\\ell+d_{\\ell-1}}{d_{\\ell-1}}\\rfloor}\\end{array}$ $d_{L}=q$ in. eD-teunnoet teh ae l-atsh sl aoyf ear dwapittha ptlireoown -rwaitnprhke npeasrsa lmeevteel $r\\leq\\operatorname*{min}\\{d\\ell,d\\ell-1\\}$ $U\\in\\mathbb{R}^{d_{\\ell}\\times d}$ $V\\in\\mathbb{R}^{d_{\\ell-1}\\times d}$ eavsel ", "page_idx": 4}, {"type": "equation", "text": "$$\nf_{\\ell,U,V}(\\boldsymbol{x}):=\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}}(W_{\\ell}^{\\mathrm{pre}}+U V^{\\top})\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}\\boldsymbol{x},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\overline{{W}}_{\\ell}^{\\mathrm{pre}}:=W_{L}^{\\mathrm{pre}}W_{L-1}^{\\mathrm{pre}}\\cdot\\ldots W_{\\ell}^{\\mathrm{pre}}\\in\\mathbb{R}^{d_{L}\\times d_{\\ell-1}}$ and $\\underline{{W}}_{\\ell}^{\\mathrm{pre}}:=W_{\\ell}^{\\mathrm{pre}}W_{\\ell-1}^{\\mathrm{pre}}\\cdot..\\cdot W_{1}^{\\mathrm{pre}}\\in\\mathbb{R}^{d_{\\ell}\\times d_{0}}$ with $W_{0}^{\\mathrm{pre}}=I_{p}$ and $\\overline{{W}}_{L}^{\\mathrm{pre}}=I_{q}$ . In a transformer-based LLM, each row of $W_{\\ell}$ can represents either the parameters in a single head for the MHA module or that in a single channel for the FFN module. ", "page_idx": 4}, {"type": "text", "text": "Given $n$ observations $(x_{i}^{(\\mathrm{i})},y_{i}^{(\\mathrm{i})})\\subset\\mathbb{R}^{p}\\times\\mathbb{R}^{q}$ , we fine-tune $f^{\\mathrm{pre}}$ by minimizing the empirical risk $\\begin{array}{r}{\\mathcal{R}_{n}^{\\mathrm{(i)}}(f_{\\ell,U,V}):=(1/n)\\sum_{i\\in[n]}||y_{i}^{\\mathrm{(i)}}-f_{\\ell,U,V}(x_{i}^{\\mathrm{(i)}})||^{2}}\\end{array}$ via gradient descent. For LoRA, we train both low-rank matrices $(U,V)$ in Equation (3) with $d\\leftarrow r$ . For $S^{2}\\mathrm{FT}$ , we train only $V$ in Equation (3) with $d\\leftarrow s$ and fixed $U\\gets U_{S}^{\\mathrm{S}^{2}\\mathrm{FT}}:=[e_{a_{1}};e_{a_{2}};\\dotsc;e_{a_{s}}].$ , which specifies $s$ rows to fine-tune, where $S=\\{a_{1},\\ldots,a_{s}\\}\\subset[d_{\\ell}]$ and $e_{a}$ is the $a$ -th standard basis. Motivated from the results that gradient descent has implicit regularization [74, 19, 5], we directly consider the minimum norm solutions. ", "page_idx": 4}, {"type": "text", "text": "We consider a multiple linear regression setting. Assume that the in-distribution training data $(x^{\\mathrm{(i)}}$ , $\\boldsymbol{y}^{(\\mathrm{i})})\\in\\mathbb{R}^{p+q}$ and out-of-distribution test data $(x^{(0)},y^{(0)})\\in\\mathbb{R}^{p+q}$ are generated i.i.d. according to ", "page_idx": 4}, {"type": "equation", "text": "$$\ny^{(k)}=B^{(k)}x^{(k)}+\\epsilon^{(k)},\\;\\;k\\in\\{\\mathrm{i},\\mathrm{o}\\},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $B^{(k)}\\in\\mathbb{R}^{q\\times p}$ is the coefficient matrix, $x^{(k)}$ and $\\epsilon^{(k)}$ are mean zero sub-Gaussian signal and noise with covariance matrices $\\Sigma_{x}^{(k)}$ and $\\Sigma_{\\epsilon}^{\\left(k\\right)}$ , respectively. The generalization capacity is measured by the fine-tuned model\u2019s excess risk $\\begin{array}{r}{\\mathcal{E}(f):=\\mathbb{E}[\\|y^{(o)}-f(x^{(o)})\\|^{2}]-\\operatorname*{inf}_{f^{\\prime}}\\mathbb{E}[\\|y^{(o)}-f^{\\prime}(x^{(o)})\\|^{2}],}\\end{array}$ . ", "page_idx": 4}, {"type": "text", "text": "For these OOD data, LoRA suffers from forgetting, while $S^{2}\\mathrm{FT}$ can maintain pre-training knowledge. ", "page_idx": 4}, {"type": "text", "text": "Assumption 4.1 (Distribution Shift). Assume that $\\Sigma_{x}^{\\mathrm{(i)}}=\\Sigma_{x}^{\\mathrm{(o)}}=\\Sigma_{x}$ for some $\\Sigma_{x}\\,\\in\\,\\mathbb{R}^{p\\times p}$ , and $\\|(\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}^{-}}U_{S}^{\\mathrm{S^{2}F T}})(\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}}U_{S}^{\\mathrm{S^{2}F T}})^{\\dagger}(B^{(\\mathrm{o})}-B^{(\\mathrm{i})})\\Sigma_{x}^{1/2}\\|_{\\mathrm{F}}^{2}\\leq\\varepsilon^{2}\\mathcal{E}^{(\\mathrm{o})}(f^{\\mathrm{pre}})$ for some $\\varepsilon>0$ . ", "page_idx": 4}, {"type": "text", "text": "Assumption 4.1 states that while the covariate distribution remains unchanged, the label distribution conditioned on covariates may shift, but not exceeding a factor of $\\epsilon^{2}$ of the OOD risk of $f^{\\mathrm{pre}}$ . This holds for fine-tuning with proper channel selection, where primarily the output distribution is changed. Theorem 4.2 (Out-of-distribution Excess Risk, Informal). Suppose Assumption 4.1 holds. Consider $n\\to\\infty$ . If $B^{\\mathrm{(i)}}=\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}}\\tilde{B}^{\\mathrm{(i)}}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}$ holds for some $\\tilde{B}^{(\\mathrm{i})}\\in\\mathbb{R}^{d_{\\ell}\\times d_{\\ell-1}}$ , and $\\begin{array}{r}{s\\leq\\mathrm{rank}(\\Sigma_{f}^{\\mathrm{(i)}})}\\end{array}$ , then, $\\begin{array}{r}{\\mathcal{E}^{(\\circ)}({f}_{\\ell,U_{s}^{\\mathrm{s2}}\\mathrm{Fr},V^{\\mathrm{s2}}\\mathrm{Fr}})\\le(1+3\\varepsilon^{2})\\mathcal{E}^{(0)}({f}^{\\mathrm{pre}}),\\ \\ \\mathcal{E}^{(0)}({f}_{\\ell,U^{\\mathrm{loRA}},V^{\\mathrm{loRA}}})\\ge\\|(B^{(0)}-B^{(\\mathrm{i})})\\Sigma_{x}^{1/2}\\|_{\\mathrm{F}}^{2}.}\\end{array}$ ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Theorem 4.2 indicates that the OOD risk of $S^{2}\\mathrm{FT}$ is bounded above by that of $f^{\\mathrm{pre}}$ , while that of LoRA is bounded below by the label shift magnitude. If $f^{\\mathrm{pre}}$ already has low risk for OOD tasks, and the label shift is significant, $\\mathrm{S^{2}F T}$ is expected to outperform LoRA. Essentially, when the OOD task deviates significantly from the FT distribution, LoRA may forget the pre-trained knowledge and overfti to the FT data, compromising its generalization capabilities. See formal statements in Theorem E.8. ", "page_idx": 4}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we conduct a series of experiments across three diverse benchmarks covering more than 20 datasets. Our goal is to provide a rich picture of how $\\mathrm{S^{2}F T}$ performs in different scenarios. Here, we compare our method with different fine-tuning strategies and categories including: (i) Full fine-tuning (FT), (ii) reparameterized fine-tuning: LoRA [27], DoRA [38], and Galore [76], (iii) adapter-based fine-tuning: Series Adapter [26], Parallel Adapter [24], and LoReFT [67], (iv) promptbased fine-tuning: Prefix-Tuning [36], (v) sparse fine-tuning: LISA [48]. For a fair comparison, we keep a comparable number of trainable parameters in $\\mathrm{S^{2}F T}$ to that of LoRA. The design choices for trainable parameter allocations in $S^{2}\\mathrm{FT}$ will be detailed in Section 5.4. All other hyperparameters are selected via cross-validation. Detailed setups and baseline descriptions are provided in Appendix D. ", "page_idx": 5}, {"type": "text", "text": "5.1 Commonsense Reasoning ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Dataset Descriptions. The commonsense reasoning dataset comprise eight subsets: BoolQ [12], PIQA [9], SocialQA [56], HellaSwag [73], WinoGrande [55], ARC-challenge [13], ARC-easy [13], and OpenbookQA [46]. Following the experimental setup of LLM-Adapters [28], we split each dataset into training and test sets. Subsequently, we combine the training data from all eight tasks into a single fine-tuning dataset and evaluate performance on the individual test dataset for each task. ", "page_idx": 5}, {"type": "text", "text": "Results. Table 1 showcases that $S^{2}\\mathrm{FT}$ consistently outperforms existing PEFT methods across the LLaMA-7B/13B, LLaMA2-7B and LLaMA3-8B models. When compared to LoRA and DoRA, it achieves average performance gains of $4.6\\%$ and $2.8\\%$ , respectively. Additionally, $\\mathrm{S^{2}F T}$ also demonstrates superior performance against recent approaches including Galore, LoReFT, and LISA, with accuracy improvements of at least $1.0\\%$ . Remarkably, despite using less than $1\\%$ of trainable parameters, our method surpasses full FT by $0.5\\%$ . The $3.0\\%$ improvement observed on the LLaMA3-8B suggests that maintaining most pre-trained parameters frozen enables better generalization to test distributions. ", "page_idx": 5}, {"type": "text", "text": "Table 1: Comparison among various fine-tuning methods for the LLaMA-7B/13B, LLaMA2-7B, and LLaMA3-8B models on eight commonsense reasoning tasks. Non-PEFT methods are marked in gray. (1: from DoRA paper, 2: from ReFT paper, 3: reproduced by us, \u2020: projected trainable parameters) ", "page_idx": 5}, {"type": "table", "img_path": "lEUle8S4xQ/tmp/c2e6fca310c3cefc07ce0f5d72be18761c0f830a10d10d0b7d655b8eaa389da7.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "table", "img_path": "lEUle8S4xQ/tmp/5b6c7fc0330cbcdf504519fe17ea72e265e3403c27e783f552b6b22b7c94296c.jpg", "table_caption": ["Table 2: Comparison among various fine-tuning methods for different models on seven math reasoning tasks. Non-PEFT methods are marked in gray. (1: from LLM-Adapters paper, 2: reproduced by us) "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "5.2 Arithmetic Reasoning ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Dataset Descriptions. We followed Hu et al. [28] and evaluated $S^{2}\\mathrm{FT}$ on seven math reasoning tasks, including MultiArith [53], GSM8K [14], AddSub [25], AQuA [37], SingleEq [31], SVAMP [50] and MAWPS [32]. Our fine-tuning employed the Math10K dataset [28], which combines training sets from GSM8K, MAWPS, and $\\mathrm{\\AQuA}$ , augmented with LM-generated chain-of-thought steps. Therefore, these three tasks are considered ID, while the remaining four are classified as OOD tasks. ", "page_idx": 6}, {"type": "text", "text": "Results. As showcased in Table 2, $\\mathrm{S^{2}F T}$ consistently outperforms other PEFT methods for different models. On average, it achieves improvements of $1.3\\%$ and $0.9\\%$ over LoRA and DoRA, respectively. These results highlight the versatility and effectiveness of our approach across a diverse range of tasks. Additionally, we observe substantial improvements even when compared to Full FT for the LLaMA3-8B model, particularly on complex tasks such as GSM8K and AQuA. This suggests that $\\mathrm{S^{2}F T}$ better preserves the original reasoning capabilities of this stronger model while acquiring new skills from the fine-tuning data, thereby validating the enhanced generalization ability of our method. ", "page_idx": 6}, {"type": "text", "text": "5.3 Instruction Following ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Dataset Descriptions. To further showcase $\\mathrm{S^{2}F T}$ \u2019s superior generalization ability, we employ the instruction-following fine-tuning task with Alpaca GPT-4 dataset, which comprises $52\\mathrm{k}$ samples generated by GPT-4 [2] based on inputs from Alpaca [63]. Performance is measured on MT-Bench [78], featuring 80 high-quality, multi-turn questions designed to assess LLMs on eight different aspects. ", "page_idx": 6}, {"type": "text", "text": "Results. Table 3 offers a comprehensive evaluation of Full FT, LoRA, LISA, and $S^{2}\\mathrm{FT}$ across various tasks in the MT-Bench benchmark, including Writing, Roleplay, Reasoning, Code, Math, Extraction, STEM, and Humanities. It is observed that $\\mathrm{S^{2}F T>L I S A>F u l l\\;F T>L o R A/G a l o r e\\geq V}$ anilla for both Mistral-7B and LLama2-7B. This is because sparse FT methods like $\\mathrm{S^{2}F T}$ and LISA retain more pre-trained knowledge while acquiring new skills on the FT dataset, thereby generalizing better to diverse tasks in MT-Bench. Moreover, our method outperforms LISA due to its fine-grained and flexible selection strategy, enabling all layers to learn to follow instructions on the full fine-tuning set. ", "page_idx": 6}, {"type": "text", "text": "5.4 Design Choices for Trainable Parameter Allocations ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Finally, we detail how $\\mathrm{S^{2}F T}$ distribute trainable parameters across layers, modules, and channels.   \nUniform across Layers: Following Chen et al. [10], we allocate parameters to each layer uniformly. ", "page_idx": 6}, {"type": "table", "img_path": "lEUle8S4xQ/tmp/9524cc26019f866f6e892aea01ff940165ebf40e713830b548f5a7f3e198a340.jpg", "table_caption": ["Table 3: Performance comparison of LLM fine-tuning methods trained on the Alpaca GPT-4 dataset. We report the MT-Bench score as the evaluation metric. All baseline results are cited from LISA [48]. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Fine-tune Important Modules: Figure 4 analyzes the effectiveness of different components in a LLaMA-like Transformer Block for fine-tuning, including Query, Key, Value, Output, Up, Gate, and Down projections. To ensure a fair comparison, we maintain a fixed number of trainable parameters when fine-tuning each component. The results show that the effectiveness of components in finetuning follows the order: Query/Key $\\ll$ Value/Up/Gate $<$ Output/Down. This is because Query/Key are only used to measure token similarities, while others serve as persistent memories of training data. Based on this finding, we allocate our parameter budget fairly to the Output and Down projections. For the LLama3-8B and Mistral-7B models, we only fine-tune the Down projection due to the inflexible selection in multi-query attention. Further analysis of this setting is left for future research. ", "page_idx": 7}, {"type": "image", "img_path": "lEUle8S4xQ/tmp/01b8ebcc1eb78b54d219dc520b84924d1a5e6a8201b40a8a7e24356165508488.jpg", "img_caption": ["Figure 4: The impact of different components in fine-tuning, including Query, Key, Value, Output, Up, Gate, and Down projection. We fix the trainable parameter budget and only fine-tune one component. "], "img_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "lEUle8S4xQ/tmp/e75ad7c92a369b3790e047cb89f913756a1ab2548d982b87f00f0283e9552bdb.jpg", "table_caption": ["Table 4: Comparison of various channel selection strategies on the commonsense and arithmetic reasoning datasets for the LLama3-8B. We report the average accuracy $(\\%)$ as the evaluation metric. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Selection across Channels: In Section 3.2, we discuss several strategies for channel selection. In our main experiments, we employ random selection to ensure fair comparisons with baseline methods, as these approaches treat all channels with equal importance. However, the sparse structure of $\\mathrm{S^{2}F T}$ offers controllability during fine-tuning, allowing us to prioritize important channels in the selection process to further boost performance. Table 4 compared nine different strategies, incorporating five varying selection metrics (i.e., random, weight, activation, weight-activation product, and gradient), each choosing either the largest or smallest values. For $\\mathrm{S^{2}F T-}\\mathrm{\\bar{A}}$ , $\\mathrm{S^{2}F T-S}$ , and $\\mathrm{S^{2}F T-G}$ , we employ $1\\%$ of the fine-tuning data as a calibration set, introducing only negligible overhead during inference. ", "page_idx": 7}, {"type": "text", "text": "Our results demonstrate that random selection serves as a strong baseline due to its unbiased nature. Among heuristic metrics, selecting channels with the smallest activations (i.e., $\\mathrm{S^{2}F T-A}$ and $\\mathrm{S^{2}F T-S}_{\\mathrm{,}}$ ) outperforms random selection. This indicates that these channels contain less task-specific information, enabling us to inject new knowledge through fine-tuning while preserving pre-trained capabilities in other channels. In contrast, other strategies introduce bias that compromises model performance. Notably, the counterintuitive accuracy decrease in $\\mathrm{S^{2}F T}$ -G (Large) suggests that channels with large gradients contain task-related pre-trained knowledge, and modifying them will disrupt these abilities. ", "page_idx": 7}, {"type": "text", "text": "6 Analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Having demonstrated the strong generalization capability and overall performance of $\\mathrm{S^{2}F T}$ , we now further explore its training efficiency and serving scalability compared to other fine-tuning techniques. ", "page_idx": 8}, {"type": "text", "text": "6.1 Training Efficiency ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To evaluate training efficiency, we examine two crucial metrics: peak memory footprint and average training latency. These numbers are measured on a single Nvidia A100 (80G) SXM GPU. We keep a comparable number of parameters for all methods. To obtain the average latency, we fine-tune the model for 50 runs, each run including 200 iterations, with 10 warmup runs excluded in measurement. ", "page_idx": 8}, {"type": "image", "img_path": "lEUle8S4xQ/tmp/9b005af0810fe336e7dd0ccbf5a287df11a0584c7b61df297064f368860e0d16.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 5: Comparison of memory and computation efficiency during training on the LLaMA2-7B/13B with varying sequence lengths and batch sizes. Average latency and peak memory usage are reported. $\\mathrm{S^{2}F T}$ significantly improves training latency while reducing memory footprint compared to baselines. ", "page_idx": 8}, {"type": "text", "text": "In Figure 5, we thoughtfully profile $\\mathrm{S^{2}F T}$ on various model sizes, sequence lengths, and batch sizes. Compared to Full FT, $\\mathrm{S^{2}F T}$ saves $1.4\u20133.0\\times$ memory, and speedups fine-tuning by 1.5-2.7 times. When benchmarked against other PEFT methods, $\\bar{\\mathrm{S^{2}F T}}$ establishes new standards for efficiency, offering average reductions of $2\\%$ in memory usage and $9\\%$ in latency. Notably, $\\mathrm{S^{2}F T}$ outperforms the widely adopted LoRA, achieving about $10\\%$ improvement in both matrics by avoiding the need to store new parameters and perform additional calculations. Our partial back-propagation algorithm further improves efficiency by saving unnecessary forward activations and backward calculations. ", "page_idx": 8}, {"type": "text", "text": "6.2 Serving Scalability ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "While $S^{2}\\mathrm{FT}$ avoids additional inference overhead for a single fine-tuned model through in-place gradient updates, we will now discuss its scalability for serving thousands of fine-tuned models. To begin, we introduce the unmerged computation paradigm of $S^{2}\\bar{\\mathrm{FT}}$ : Given a pre-trained weight matrix $\\check{W}^{p r e}\\in\\mathbb{R}^{d\\times k}$ and its corresponding fine-tuned weight matrix $W$ with sparsity level $s$ , we define the weight difference as $\\Delta W=W{\\mathrm{-}}W^{\\mathrm{pre}}$ . Similar to Section 4, $\\Delta W$ can be decomposed into the product of a weight matrix $V\\in\\mathbb{R}^{k\\times s}$ and a permutation matrix $U\\in\\mathbb{R}^{d\\times s}$ . This decomposition allows us to \u201cunmerge\u201d an adapter $\\Delta W=U V^{\\top}$ from $W$ , thereby sharing similarities with other adapters during inference. Following Zhong et al. [79], we consider three different adapter composition scenarios: ", "page_idx": 8}, {"type": "text", "text": "Adapter Fusion. To combine knowledge from multiple trained adapters, we employ weighted fusion when fine-tuning is impractical due to limited data access or computational resources. However, this approach degrades performance. In Table 5, we compare the effectiveness of LoRA and $\\mathrm{S^{2}F T}$ when combining adapters trained separately on commonsense and arithmetic reasoning tasks, where we consider both fine-tuning overlapped and non-overlapped parameters for different adapters in $\\mathrm{S^{2}F T}$ Our results show that $\\mathrm{S^{2}\\bar{F}T}$ with non-overlapped parameters achieves the best performance, while the overlapped variant shows inferior results. This is because $S^{2}\\mathrm{FT}$ (non-overlap) modifies orthogonal low-rank spaces for different tasks. Similarly, LoRA largely retains task-specific capabilities during adapter fusion by optimizing low-rank projection matrices to create separate spaces for each adapter. ", "page_idx": 8}, {"type": "table", "img_path": "lEUle8S4xQ/tmp/361a5946c594c5475d9f2e6df3f678d1d6ec7866c05cea45dee5aca9a78eebb1.jpg", "table_caption": ["Table 5: Adapter Fusion Results for LoRA and $S^{2}\\mathrm{FT}$ trained on the commonsense and arithmetic reasoning datasets using the LLama3-8B. We report the average accuracy $(\\%)$ as the evaluation metric. "], "table_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "lEUle8S4xQ/tmp/623fdb1a450b0fa86cd0ac685ad3bf1b7158192436ca759dcc549cb89018f975.jpg", "img_caption": ["Figure 6: Comparison of latency for adapter switch and parallelism on a single linear layer. $S^{2}\\mathrm{FT}$ improves scalability for switch on GPU and CPU, while saving $22\\%$ time during parallelism on GPU. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Adapter Switch. Another way to leveraging multiple adapters is to dynamically switch between them. This process involves four steps: unfusing the old adapter, unloading it from memory, loading the new adapter, and fusing it into the model. In such setting, LoRA needs two matrix multiplications (matmul) and two additions (add) on GPU whereas $\\mathrm{S^{2}F T}$ only requires two sparse addition (scatter add). In Figure 6a, we increase the base weight dimension while maintaining a sparsity of 32 for $\\mathrm{S^{2}F T}$ and a low-rankness of 16 for LoRA. Notably, we observe that LoRA\u2019s switching time scales quadratically, while $\\mathrm{S^{2}F T}$ remains nearly constant. Moreover, in I/O-constrained scenarios such as deployment on CPU, $\\mathrm{S^{2}F T}$ further accelerates adapter switch by only updating a small fraction of the original weights, reducing the volume of I/O transfers, as time compared between scatter add and add in Figure 6b. ", "page_idx": 9}, {"type": "text", "text": "Adapter Parallelism. To serve thousands of adapters in parallel, we decompose the computation into separate batched computations for $W^{p r e}$ and $\\Delta W$ following S-LoRA [58]. While LoRA requires two matmul and one add on GPU, $\\mathrm{S^{2}F T}$ reduces this to a matmul, an add, and either a scatter or gather for $W_{1}$ and $W_{2}$ in Section 3.1. Figure 6c shows that $S^{2}\\mathrm{FT}$ achieves up to $22\\%$ faster inference than LoRA under the same memory constraints, with more speedup as the number of adapters scales. ", "page_idx": 9}, {"type": "text", "text": "7 Related Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "PEFT methods reduce the fine-tuning cost for large models, which can be categorized into 4 groups: ", "page_idx": 9}, {"type": "text", "text": "Adapter-based Fine-tuning introduces additional trainable module into the original model. Series Adapters insert components between MHA or FFN layers [51, 26], while parallel adapters add modules alongside existing components [24]. Recently, ReFT [67] was introduced to directly learn interventions on hidden representations. However, they introduce additional latency during inference. ", "page_idx": 9}, {"type": "text", "text": "Prompt-based Fine-tuning adds randomly-initialized soft tokens to the input (usually as a prefix) and train their embeddings while freezing the model weights [36, 40, 35]. These approaches result in poor performance compared to other groups, while come at the cost of significant inference overhead. ", "page_idx": 9}, {"type": "text", "text": "Reparameterized Fine-tuning utilizes low-rank projections to reduce trainable parameters while allowing operations with high-dimensional matrices. LoRA[27] and its recent variants like DoRA[38], AsyLoRA [80], and FLoRA [59], use low-rank matrices to approximate additive weight updates during training. To alleviate the limitations of low-rank structure, other work also add or multiply orthogonal matrices to enable high-rank updating, including MoRA [29], OFT [52], and BOFT [39]. These methods require no additional inference cost as the weight updates can be merged into models. ", "page_idx": 9}, {"type": "text", "text": "Sparse Fine-tuning aims to reduce the number of fine-tuned parameters by selecting a subset of pre-trained parameters that are critical to downstream tasks while discarding unimportant ones. This kind of methods are commonly used in the pre-LLM era [20, 72, 62]. However, they cannot reduce the memory footprints due to their unstructured nature. Recent approaches address this limitation through three directions: (1) developing structured variants that sacrifice selection flexibility for better hardware efficiency [48, 81], (2) incorporating sparsity into LoRA [66, 15, 41] but yield limited efficiency gains, or (3) using sparse operators for lower memory cost but slow down training [4, 49, 7]. ", "page_idx": 9}, {"type": "text", "text": "Our work is based on the last category but achieving better performance and efficiency simultaneously. Additionally, we focus on scalable inference of PEFT methods, with $S^{2}\\mathrm{FT}$ being the only approach that enables effective fusion, rapid switching, and efficient parallelism when serving multiple adapters. ", "page_idx": 9}, {"type": "text", "text": "8 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper introduces $S^{2}\\mathrm{FT}$ , a novel PEFT family that simultaneously achieves high quality, efficient training, and scalable serving for LLM fine-tuning. $\\mathrm{S^{2}F T}$ accomplishes this by selecting sparsely and compute densely. It selects a subset of heads and channels to be trainable for the MHA and FFN modules, respectively. The weight matrices from the two sides of the coupled structures in LLMs are co-permuted to connect the selected components into dense matrices, and only these parameters are updated using dense operations. We hope $\\mathrm{\\overline{{S^{2}F T}}}$ can be considered as a successor to LoRA for PEFT. ", "page_idx": 9}, {"type": "text", "text": "9 Acknowledgement ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We would like to thank Songlin Yang, Tianqi Chen, Hanshi Sun, and Chris De Sa for their helpful discussions, and the authors of LLM-Adapters, ReFT, and DoRA for providing detailed results. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 1   \n[2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 7   \n[3] Alan Ansell, Edoardo Maria Ponti, Anna Korhonen, and Ivan Vuli\u00b4c. Composable sparse fine-tuning for cross-lingual transfer. arXiv preprint arXiv:2110.07560, 2021. 2   \n[4] Alan Ansell, Ivan Vulic\u00b4, Hannah Sterz, Anna Korhonen, and Edoardo M Ponti. Scaling sparse fine-tuning to large language models. arXiv preprint arXiv:2401.16405, 2024. 10   \n[5] Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix factorization. Advances in Neural Information Processing Systems, 32, 2019. 5, 17   \n[6] Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language model for mathematics. arXiv preprint arXiv:2310.10631, 2023. 1   \n[7] Kartikeya Bhardwaj, Nilesh Prasad Pandey, Sweta Priyadarshi, Viswanath Ganapathy, Rafael Esteves, Shreya Kadambi, Shubhankar Borse, Paul Whatmough, Risheek Garrepalli, Mart Van Baalen, et al. Rapid switching and multi-adapter fusion via sparse high rank adapters. arXiv preprint arXiv:2407.16712, 2024. 10   \n[8] Dan Biderman, Jose Gonzalez Ortiz, Jacob Portes, Mansheej Paul, Philip Greengard, Connor Jennings, Daniel King, Sam Havens, Vitaliy Chiley, Jonathan Frankle, et al. Lora learns less and forgets less. arXiv preprint arXiv:2405.09673, 2024. 1   \n[9] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 7432\u20137439, 2020. 6, 16   \n[10] Jiaao Chen, Aston Zhang, Xingjian Shi, Mu Li, Alex Smola, and Diyi Yang. Parameter-efficient fine-tuning design spaces. arXiv preprint arXiv:2301.01821, 2023. 7   \n[11] Yuxin Chen, Yuejie Chi, Jianqing Fan, Cong Ma, et al. Spectral methods for data science: A statistical perspective. Foundations and Trends\u00ae in Machine Learning, 14(5):566\u2013806, 2021. 30   \n[12] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019. 6, 16   \n[13] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. 6, 16   \n[14] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. 7, 16   \n[15] Ning Ding, Xingtai Lv, Qiaosen Wang, Yulin Chen, Bowen Zhou, Zhiyuan Liu, and Maosong Sun. Sparse low-rank adaptation of pre-trained language models. arXiv preprint arXiv:2311.11696, 2023. 10   \n[16] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 1   \n[17] Gongfan Fang, Xinyin Ma, Mingli Song, Michael Bi Mi, and Xinchao Wang. Depgraph: Towards any structural pruning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16091\u201316101, 2023. 4   \n[18] Jianwei Feng and Dong Huang. Optimal gradient checkpoint search for arbitrary computation graphs. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11433\u2013 11442, 2021. 4   \n[19] Suriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro. Implicit regularization in matrix factorization. Advances in neural information processing systems, 30, 2017. 5, 17   \n[20] Demi Guo, Alexander M Rush, and Yoon Kim. Parameter-efficient transfer learning with diff pruning. arXiv preprint arXiv:2012.07463, 2020. 10   \n[21] Zeyu Han, Chao Gao, Jinyang Liu, Sai Qian Zhang, et al. Parameter-efficient fine-tuning for large models: A comprehensive survey. arXiv preprint arXiv:2403.14608, 2024. 1   \n[22] Moritz Hardt and Tengyu Ma. Identity matters in deep learning. arXiv preprint arXiv:1611.04231, 2016. 5, 17   \n[23] Soufiane Hayou, Nikhil Ghosh, and Bin Yu. Lora+: Efficient low rank adaptation of large models. arXiv preprint arXiv:2402.12354, 2024. 3   \n[24] Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards a unified view of parameter-efficient transfer learning. arXiv preprint arXiv:2110.04366, 2021. 6, 7, 10   \n[25] Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman. Learning to solve arithmetic word problems with verb categorization. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 523\u2013533, 2014. 7, 16   \n[26] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for NLP. In International conference on machine learning, pages 2790\u20132799. PMLR, 2019. 6, 7, 10   \n[27] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 2, 6, 7, 10   \n[28] Zhiqiang Hu, Yihuai Lan, Lei Wang, Wanyu Xu, Ee-Peng Lim, Roy Ka-Wei Lee, Lidong Bing, and Soujanya Poria. Llm-adapters: An adapter family for parameter-efficient fine-tuning of large language models. arXiv preprint arXiv:2304.01933, 2023. 3, 6, 7, 16   \n[29] Ting Jiang, Shaohan Huang, Shengyue Luo, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, Qi Zhang, Deqing Wang, et al. Mora: High-rank updating for parameter-efficient fine-tuning. arXiv preprint arXiv:2405.12130, 2024. 10   \n[30] Kenji Kawaguchi. Deep learning without poor local minima. Advances in neural information processing systems, 29, 2016. 5, 17   \n[31] Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Oren Etzioni, and Siena Dumas Ang. Parsing algebraic word problems into equations. Transactions of the Association for Computational Linguistics, 3:585\u2013597, 2015. 7, 16   \n[32] Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. Mawps: A math word problem repository. In Proceedings of the 2016 conference of the north american chapter of the association for computational linguistics: human language technologies, pages 1152\u20131157, 2016. 7, 16   \n[33] Rui Kong, Qiyang Li, Xinyu Fang, Qingtian Feng, Qingfeng He, Yazhu Dong, Weijun Wang, Yuanchun Li, Linghe Kong, and Yunxin Liu. Lora-switch: Boosting the efficiency of dynamic llm adapters via system-algorithm co-design. arXiv preprint arXiv:2405.17741, 2024. 1   \n[34] Thomas Laurent and James Brecht. Deep linear networks with arbitrary loss: All local minima are global. In International conference on machine learning, pages 2902\u20132907. PMLR, 2018. 5, 17   \n[35] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691, 2021. 10   \n[36] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190, 2021. 6, 7, 10   \n[37] Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale generation: Learning to solve and explain algebraic word problems. arXiv preprint arXiv:1705.04146, 2017. 7, 16   \n[38] Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang, KwangTing Cheng, and Min-Hung Chen. DoRA: Weight-Decomposed low-rank adaptation. arXiv preprint arXiv:2402.09353, 2024. 2, 6, 7, 10   \n[39] Weiyang Liu, Zeju Qiu, Yao Feng, Yuliang Xiu, Yuxuan Xue, Longhui Yu, Haiwen Feng, Zhen Liu, Juyeon Heo, Songyou Peng, et al. Parameter-efficient orthogonal finetuning via butterfly factorization. arXiv preprint arXiv:2311.06243, 2023. 10   \n[40] Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. GPT understands, too. AI Open, 2023. 10   \n[41] Zequan Liu, Jiawen Lyn, Wei Zhu, Xing Tian, and Yvette Graham. Alora: Allocating low-rank adaptation for fine-tuning large language models. arXiv preprint arXiv:2403.16187, 2024. 10   \n[42] Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Re, et al. Deja vu: Contextual sparsity for efficient llms at inference time. In International Conference on Machine Learning, pages 22137\u201322176. PMLR, 2023. 2   \n[43] Haihao Lu and Kenji Kawaguchi. Depth creates no bad local minima. arXiv preprint arXiv:1702.08580, 2017. 5, 17   \n[44] Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, and Yue Zhang. An empirical study of catastrophic forgetting in large language models during continual fine-tuning. arXiv preprint arXiv:2308.08747, 2023.   \n[45] Xinyin Ma, Gongfan Fang, and Xinchao Wang. Llm-pruner: On the structural pruning of large language models. Advances in neural information processing systems, 36:21702\u201321720, 2023. 2, 4   \n[46] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. arXiv preprint arXiv:1809.02789, 2018. 6, 16   \n[47] Ryumei Nakada, Halil Ibrahim Gulluk, Zhun Deng, Wenlong Ji, James Zou, and Linjun Zhang. Understanding multimodal contrastive learning and incorporating unpaired data. arXiv preprint arXiv:2302.06232, 2023. 31   \n[48] Rui Pan, Xiang Liu, Shizhe Diao, Renjie Pi, Jipeng Zhang, Chi Han, and Tong Zhang. LISA: Layerwise importance sampling for memory-efficient large language model fine-tuning. arXiv preprint arXiv:2403.17919, 2024. 2, 6, 8, 10   \n[49] Ashwinee Panda, Berivan Isik, Xiangyu Qi, Sanmi Koyejo, Tsachy Weissman, and Prateek Mittal. Lottery ticket adaptation: Mitigating destructive interference in llms. arXiv preprint arXiv:2406.16797, 2024. 10   \n[50] Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are NLP models really able to solve simple math word problems? In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou, editors, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2080\u20132094, Online, June 2021. Association for Computational Linguistics. 7, 16   \n[51] Jonas Pfeiffer, Ivan Vulic\u00b4, Iryna Gurevych, and Sebastian Ruder. Mad-x: An adapter-based framework for multi-task cross-lingual transfer. arXiv preprint arXiv:2005.00052, 2020. 10   \n[52] Zeju Qiu, Weiyang Liu, Haiwen Feng, Yuxuan Xue, Yao Feng, Zhen Liu, Dan Zhang, Adrian Weller, and Bernhard Sch\u00a8olkopf. Controlling text-to-image diffusion by orthogonal finetuning. Advances in Neural Information Processing Systems, 36:79320\u201379362, 2023. 10   \n[53] Subhro Roy and Dan Roth. Solving general arithmetic word problems. arXiv preprint arXiv:1608.01413, 2016. 7, 16   \n[54] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Je\u00b4re\u00b4my Rapin, et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023. 1   \n[55] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99\u2013106, 2021. 6, 16   \n[56] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Commonsense reasoning about social interactions. arXiv preprint arXiv:1904.09728, 2019. 6, 16   \n[57] Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013. 5, 17   \n[58] Ying Sheng, Shiyi Cao, Dacheng Li, Coleman Hooper, Nicholas Lee, Shuo Yang, Christopher Chou, Banghua Zhu, Lianmin Zheng, Kurt Keutzer, et al. S-lora: Serving thousands of concurrent lora adapters. arXiv preprint arXiv:2311.03285, 2023. 1, 2, 10   \n[59] Chongjie Si, Xuehui Wang, Xue Yang, Zhengqin Xu, Qingyun Li, Jifeng Dai, Yu Qiao, Xiaokang Yang, and Wei Shen. Flora: Low-rank core space for n-dimension. arXiv preprint arXiv:2405.14739, 2024. 10   \n[60] GW Stewart. On the continuity of the generalized inverse. SIAM Journal on Applied Mathematics, 17(1):33\u201345, 1969. 32   \n[61] Yi-Lin Sung, Varun Nair, and Colin A Raffel. Training neural networks with fixed sparse masks. Advances in Neural Information Processing Systems, 34:24193\u201324205, 2021. 2   \n[62] Yi-Lin Sung, Varun Nair, and Colin A Raffel. Training neural networks with fixed sparse masks. Advances in Neural Information Processing Systems, 34:24193\u201324205, 2021. 10   \n[63] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following LLaMA model. https://github. com/tatsu-lab/stanford alpaca, 2023. 7   \n[64] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 1   \n[65] J Leo van Hemmen and Tsuneya Ando. An inequality for trace ideals. Communications in Mathematical Physics, 76:143\u2013148, 1980. 32   \n[66] Haoyu Wang, Tianci Liu, Tuo Zhao, and Jing Gao. Roselora: Row and column-wise sparse low-rank adaptation of pre-trained language model for knowledge editing and fine-tuning. arXiv preprint arXiv:2406.10777, 2024. 10   \n[67] Zhengxuan Wu, Aryaman Arora, Zheng Wang, Atticus Geiger, Dan Jurafsky, Christopher D Manning, and Christopher Potts. ReFT: Representation finetuning for language models. arXiv preprint arXiv:2404.03592, 2024. 6, 10   \n[68] Lingling Xu, Haoran Xie, Si-Zhao Joe Qin, Xiaohui Tao, and Fu Lee Wang. Parameter-efficient finetuning methods for pretrained language models: A critical review and assessment. arXiv preprint arXiv:2312.12148, 2023. 1   \n[69] Runxin Xu, Fuli Luo, Zhiyuan Zhang, Chuanqi Tan, Baobao Chang, Songfang Huang, and Fei Huang. Raise a child in large language model: Towards effective and generalizable fine-tuning. arXiv preprint arXiv:2109.05687, 2021. 2   \n[70] Yi Yu, Tengyao Wang, and Richard J Samworth. A useful variant of the davis\u2013kahan theorem for statisticians. Biometrika, 102(2):315\u2013323, 2015. 30   \n[71] Li Yunxiang, Li Zihan, Zhang Kai, Dan Ruilong, and Zhang You. Chatdoctor: A medical chat model fine-tuned on llama model using medical domain knowledge. arXiv preprint arXiv:2303.14070, 2023. 1   \n[72] Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. arXiv preprint arXiv:2106.10199, 2021. 10   \n[73] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. 6, 16   \n[74] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning (still) requires rethinking generalization. Communications of the ACM, 64(3):107\u2013115, 2021. 5, 17   \n[75] Jinghan Zhang, Junteng Liu, Junxian He, et al. Composing parameter-efficient modules with arithmetic operation. Advances in Neural Information Processing Systems, 36:12589\u201312610, 2023. 1 [76] Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar, and Yuandong Tian. Galore: Memory-efficient llm training by gradient low-rank projection. arXiv preprint arXiv:2403.03507,   \n2024. 2, 6 [77] Justin Zhao, Timothy Wang, Wael Abid, Geoffrey Angus, Arnav Garg, Jeffery Kinnison, Alex Sherstinsky, Piero Molino, Travis Addair, and Devvret Rishi. Lora land: 310 fine-tuned llms that rival gpt-4, a technical report. arXiv preprint arXiv:2405.00732, 2024. 1 [78] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36, 2024. 7 [79] Ming Zhong, Yelong Shen, Shuohang Wang, Yadong Lu, Yizhu Jiao, Siru Ouyang, Donghan Yu, Jiawei Han, and Weizhu Chen. Multi-lora composition for image generation. arXiv preprint arXiv:2402.16843,   \n2024. 9 [80] Jiacheng Zhu, Kristjan Greenewald, Kimia Nadjahi, Haitz Sa\u00b4ez de Oca\u00b4riz Borde, Rickard Bru\u00a8el Gabrielsson, Leshem Choshen, Marzyeh Ghassemi, Mikhail Yurochkin, and Justin Solomon. Asymmetry in low-rank adapters of foundation models. arXiv preprint arXiv:2402.16842, 2024. 10 [81] Ligeng Zhu, Lanxiang Hu, Ji Lin, and Song Han. Lift: Efficient layer-wise fine-tuning for large model models. 2023. 10 ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Limitations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "While our work demonstrates the effectiveness of $\\mathrm{S^{2}F T}$ for LLM fine-tuning, several promising directions remain unexplored. First, extending $S^{2}\\mathrm{FT}$ to other architectures with coupled structures, such as CNNs and RNNs, can broaden its applicability. Second, verifying our approach beyond language tasks, particularly in large vision/multi-modal models, will enhance its versatility. Third, exploring more selection strategies can provide deeper insights into optimal fine-tuning protocols due to $\\mathrm{\\dot{S}^{2}F T}$ \u2019s controllability. Finally, although our work confirms the feasibility of scalable deployment, developing a practical and efficient serving system for $\\mathrm{S^{2}F T}$ remains an important next step. ", "page_idx": 15}, {"type": "text", "text": "B Broader Impacts ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Since our work focuses on PEFT, it leads to a reduction in hardware resource and energy consumption Given the growing adoption of LLMs across diverse domains and the corresponding surge in finetuning demands, S2FT should represent an important step toward more sustainable AI development. ", "page_idx": 15}, {"type": "text", "text": "C Detailed Experimental Setups for Section 2 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this study, we use SpFT, LoRA, and Full FT to fine-tune the LLaMA-3-8B model on Math10K dataset [28] for 3 epochs. The Math10K dataset combines training sets from GSM8K [14], MAWPS [32], and AQuA [37], augmented with language model-generated chain-of-thought steps. We train the model for 3 epochs with a batch size of 64. For both PEFT methods\u2013SpFT and LoRA\u2013we tune with three ratios of trainable parameters $(p)$ in each linear layer: $10\\%$ , $1\\%$ , and $0.1\\%$ . We evaluate the model\u2019s performance on both arithmetic and commonsense reasoning tasks, representing near out-of-distribution (OOD) and far OOD generalization scenarios, respectively. The arithmetic tasks comprise seven subtasks: MultiArith [53], GSM8K [14], AddSub [25], AQuA [37], SingleEq [31], SVAMP [50], and MAWPS [32]. The commonsense reasoning evaluation includes eight subtasks: BoolQ [12], PIQA [9], SocialQA [56], HellaSwag [73], WinoGrande [55], ARC-challenge [13], ARC-easy [13], and OpenbookQA [46]. Based on task complexity within arithmetic reasoning, we categorize MultiArith, AddSub, SingleEq, and MAWPS as easy subtasks, while the remaining arithmetic tasks are classified as hard subtasks. ", "page_idx": 15}, {"type": "text", "text": "D Detailed Experimental Setups and Hyperparamters for Section 5 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The detailed selection strategies and number of trainable parameters are presented in Section 5. Additional hyperparameter configurations for all tasks are provided in Table 6. We maintain the same hyperparameter settings across the LLaMA-7B, LLaMA-13B, LLaMA2-7B, and LLaMA3-8B models. ", "page_idx": 15}, {"type": "table", "img_path": "lEUle8S4xQ/tmp/4d38fed8a7da2adce042de9bbac79a0287e0da8c6b2fb8557c19362ccabb4292.jpg", "table_caption": ["Table 6: Hyperparameter configurations of $S^{2}\\mathrm{FT}$ on various base models across three tasks. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "E Proofs for Theoretical Results in Section 4 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Here we provide proofs for the results in Section 4. ", "page_idx": 15}, {"type": "text", "text": "E.1 Notation ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "For a vector $a$ , let $\\lVert a\\rVert$ be the $\\ell_{2}$ norm of $a$ . For $d_{1}\\ \\geq\\ d_{2}$ , denote a set of orthogonal matrices by $\\mathbb{O}_{d_{1},d_{2}}\\,:=\\,\\{R\\,\\in\\,\\mathbb{R}^{d_{1}\\times d_{2}}\\,:\\,R^{\\top}R\\,=\\,I_{d_{2}}\\}$ . For a matrix $A\\,\\in\\,\\mathbb{R}^{d_{1}\\times d_{2}}$ , let $\\|A\\|_{\\mathrm{F}}$ and $\\|A\\|_{\\mathrm{op}}$ be the Frobenius norm and spectral norm of $A$ , respectively. Denote the condition number of $A$ by $\\kappa_{*}(A):=\\|A\\|_{\\mathrm{op}}/\\lambda_{*}(A)$ . Let $A^{\\dagger}$ be Moore-Penrose inverse of $A$ . For a symmetric matrix $A$ , denote its effective rank by $r_{e}(A):=\\operatorname{tr}(A)/\\|A\\|_{\\operatorname{op}}$ . Note that $r_{e}(A)\\,\\leq\\,\\mathrm{rank}(A)$ always holds. For $a,b\\in\\mathbb{R}$ , we let $a\\vee b:=\\operatorname*{max}(a,b)$ and $a\\wedge b:=\\operatorname*{min}(a,b)$ . For a matrix $A\\in\\mathbb{R}^{d_{1}\\times d_{2}}$ , let $\\mathrm{SVD}_{r}(A):=\\Phi_{r}(A)\\Lambda_{r}(A)\\Psi_{r}^{\\top}(A)$ be the top- $^r$ singular value decomposition of $A$ , where $\\Phi_{r}(A)\\in$ $\\mathbb{O}_{d_{1},r}$ and $\\Psi_{r}(A)\\in\\mathbb{O}_{d_{2},r}$ are top- $^r$ left and right singular vectors of $A$ , respectively, and $\\Lambda_{r}(A)=$ $\\operatorname{diag}(\\lambda_{1}(A),\\ldots,\\lambda_{r}(A))\\in\\mathbb{R}^{r\\times r}$ is a diagonal matrix of singular values of $A$ , where $\\lambda_{j}(A)$ denotes the $j$ -th largest singular value of $A$ . Define $\\Phi_{*}(A):=\\Phi_{\\mathrm{rank}(A)}(A)$ and $\\Psi_{*}(A):=\\Psi_{\\mathrm{rank}(A)}^{\\mathring{\\mathrm{~\\,~}}}(A)$ as the left and right singular vectors of $A$ corresponding to non-zero singular values, respectively. Define the smallest positive singular value of $A$ as $\\lambda_{*}(A)=\\lambda_{\\operatorname{rank}(A)}(A)$ and let $\\Lambda_{*}(A)=\\Lambda_{\\mathrm{rank}(A)}(A)$ . For a deep learning model fine-tuned on $n$ i.i.d. samples $(x_{i}^{(\\mathrm{i})},y_{i}^{(\\mathrm{i})})\\subset\\mathbb{R}^{p}\\times\\mathbb{R}^{q}$ , we say an event $\\mathcal{F}$ occurs with high probability when $\\mathbb{P}(\\mathcal{F})=1-\\exp\\bigl(-\\Omega(\\log^{2}(n+p+q))\\bigr)$ . ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "E.2 Setup ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We consider multivariate regression task. Using $n$ i.i.d. samples $(x_{i}^{(\\mathrm{i})},y_{i}^{(\\mathrm{i})})\\ \\subset\\ \\mathbb{R}^{p}\\times\\mathbb{R}^{q}$ from in-distribution task, we fine-tune a pre-trained network $f^{\\mathrm{pre}}:\\mathbb{R}^{p}\\rightarrow\\mathbb{R}^{q}$ for better prediction. ", "page_idx": 16}, {"type": "text", "text": "Deep Linear Networks We consider deep linear networks of the form $x\\mapsto W_{L}W_{L-1}\\,.\\,.\\,.\\,W_{1}x:$ $\\mathbb{R}^{d}\\;\\stackrel{-}{\\rightarrow}\\;\\mathbb{R}^{p}$ , where $W_{\\ell}~\\in~\\mathbb{R}^{d_{\\ell}\\times d_{\\ell-1}}$ , with $d_{L}\\ =\\ q$ and $d_{0}~=~p$ . In comparison to multi-head attention transformers, each row of $W_{\\ell}$ can be viewed as corresponding to the parameters in a single head. Let $f^{\\mathrm{pre}}(x)\\,=\\,W_{L}^{\\mathrm{pre}}W_{L-1}^{\\mathrm{pre}}\\cdot\\cdot\\cdot W_{1}^{\\mathrm{pre}}x\\,:\\,\\mathbb{R}^{p}\\,\\to\\,\\mathbb{R}^{q}$ represent a pre-trained neural network. We denote $\\overline{{W}}_{\\ell}^{\\mathrm{pre}}\\;:=\\;W_{L}^{\\mathrm{pre}}W_{L-1}^{\\mathrm{pre}}\\cdot\\ldots W_{\\ell}^{\\mathrm{pre}}\\;\\in\\;\\mathbb{R}^{d_{L}\\times d_{\\ell-1}}$ as the weights up to the $\\ell_{}$ -th layer, and $\\underline{{W}}_{\\ell}^{\\mathrm{pre}}:=W_{\\ell}^{\\mathrm{pre}}W_{\\ell-1}^{\\mathrm{pre}}\\cdot.\\cdot W_{1}^{\\mathrm{pre}}\\in\\mathbb{R}^{d_{\\ell}\\times d_{0}}$ as the weights above the $\\ell_{}$ -th layer, with the promise that $\\underline{{W_{0}^{\\mathrm{pre}}}}\\,=\\,I$ . Deep linear networks have been widely used to facilitate the theoretical analysis of modern complex deep neural networks [57, 30, 43, 22, 34, 5]. ", "page_idx": 16}, {"type": "text", "text": "Fine-Tuning We employ $\\ell_{2}$ distance as the error metric. Given a pre-trained network $f^{\\mathrm{pre}}$ , we finetune its $\\ell_{}$ -th layer by minimizing the empirical in-distribution risk $\\begin{array}{r}{\\mathcal{R}_{n}^{\\mathrm{(i)}}(f):=(1/n)\\sum_{i\\in[n]}\\|y_{i}^{\\mathrm{(i)}}-}\\end{array}$ $f(x_{i}^{(\\mathrm{i})})\\Vert^{2}$ , where $(x_{i}^{(\\mathrm{i})},y_{i}^{(\\mathrm{i})})\\subset\\mathbb{R}^{p}\\times\\mathbb{R}^{q}$ are $n$ i.i.d. observations from in-distribution task. More specifically, we consider a class of rank- $d$ adaptation defined as ", "page_idx": 16}, {"type": "equation", "text": "$$\nf_{\\ell,U,V}(\\boldsymbol{x}):=\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}}(W_{\\ell}^{\\mathrm{pre}}+U V^{\\top})\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}\\boldsymbol{x},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $U\\in\\mathbb{R}^{d_{\\ell}\\times d}$ and $V\\in\\mathbb{R}^{d_{\\ell-1}\\times d}$ are parameters to fine-tune. Note that by regarding multiple consecutive layers as a single layer, our settings can be extended to multi-layer fine-tuning. ", "page_idx": 16}, {"type": "text", "text": "We specifically compare two fine-tuning methods: LoRA and $\\mathrm{S^{2}F T}$ . ", "page_idx": 16}, {"type": "text", "text": "\u2022 LoRA. For a fixed $\\ell\\in[L]$ , and low-rankness level $1\\,\\leq\\,r\\,\\leq\\,\\operatorname*{min}\\{d_{\\ell},d_{\\ell-1}\\}$ , we train the lowrank matrices $(U,V)$ in (4) by minimizing the empirical in-distribution risk via gradient descent. Motivated from the previous results that gradient descent has implicit regularization [74, 19, 5], we directly consider the minimum norm solutions: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(U^{\\mathrm{LoRA}},V^{\\mathrm{LoRA}})\\in\\underset{U,V}{\\arg\\operatorname*{min}}\\,\\|(U,V)\\|_{\\mathrm{F}}^{2}\\ \\mathrm{~s.t.~}(U,V)\\mathrm{~minimizes~}\\mathcal{R}_{n}^{(\\mathrm{i})}(f_{\\ell,U,V}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "\u2022 $\\mathbf{S}^{2}\\mathbf{F}\\mathbf{T}$ . For a fixed $\\ell\\in[L]$ , and a sparsity level $\\begin{array}{r}{s=\\lfloor r\\cdot\\frac{d\\ell+d_{\\ell-1}}{d_{\\ell-1}}\\rfloor}\\end{array}$ , we train only $V$ in (4) with the $U\\gets U_{S}^{\\mathrm{S}^{2}\\mathrm{FT}}:=[e_{a_{1}};e_{a_{2}};\\dotsc;e_{a_{s}}]$ ,a rwd hbiacshi ss pveeccitfoire s $s$ tchh tahne l-ts ht oe nftirnye -btueinneg,  1w.h eWree $S=\\{a_{1},a_{2},\\ldots,a_{s}\\}\\subset[d_{\\ell}]$ $e_{a}$ $a$ minimize the empirical in-distribution risk via gradient descent. Similar to LoRA, we consider the following minimum norm solution: ", "page_idx": 16}, {"type": "equation", "text": "$$\nV^{\\mathrm{S^{2}F T}}=\\underset{V}{\\arg\\operatorname*{min}}\\,\\|V\\|_{\\mathrm{F}}^{2}\\ \\ \\mathrm{s.t.}\\ V\\,\\operatorname*{minimizes}\\,\\mathcal{R}_{n}^{(\\mathrm{i})}(f_{\\ell,U_{S}^{\\mathrm{S^{2}F T}},V}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Data Generating Process As a simplification of the data generating process, we consider multiple linear regression. Assume that the in-distribution data $(x^{\\mathrm{(i)}},\\bar{y}^{\\mathrm{(i)}})\\in\\mathbb{R}^{\\bar{p}+q}$ and out-of-distribution data ", "page_idx": 16}, {"type": "text", "text": "$(x^{(0)},y^{(0)})\\in\\mathbb{R}^{p+q}$ are generated according to ", "page_idx": 17}, {"type": "equation", "text": "$$\ny^{(k)}=B^{(k)}x^{(k)}+\\epsilon^{(k)},\\;\\;k\\in\\{\\mathrm{i},\\mathrm{o}\\},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $B^{(k)}\\,\\in\\,\\mathbb{R}^{q\\times p}$ , and $\\epsilon^{(k)}\\,\\in\\,\\mathbb{R}^{q}$ is the error term satisfying $\\mathbb{E}[\\epsilon^{(k)}|x^{(k)}]\\,=\\,0$ . Assume that $\\Sigma_{\\epsilon}^{\\left(k\\right)}:=\\mathbb{E}[\\epsilon^{\\left(k\\right)}\\epsilon^{\\left(k\\right)\\top}]\\in\\mathbb{R}^{q\\times q}$ exists and $\\mathbb{E}[x^{(k)}]=0$ . The signal covariance matrix is denoted by $\\Sigma_{x}^{(k)}:=\\mathbb{E}[x^{(k)}x^{(k)\\top}]\\in\\mathbb{R}^{p\\times p}.$ ", "page_idx": 17}, {"type": "text", "text": "We define the in-distribution and out-of-distribution risks of $f:\\mathbb{R}^{p}\\to\\mathbb{R}^{q}$ as: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{R}^{(k)}(f)=\\mathbb{E}[\\|y^{(k)}-f(x^{(k)})\\|],\\ k\\in\\{\\mathrm{i},\\mathrm{o}\\}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "For notational brevity, we can write $W^{\\mathrm{pre}}=\\underline{{W}}_{L}^{\\mathrm{pre}}\\in\\mathbb{R}^{q\\times p}$ . Let $X^{\\mathrm{(i)}}:=(x_{1}^{\\mathrm{(i)}},\\hdots,x_{n}^{\\mathrm{(i)}})\\in\\mathbb{R}^{p\\times n}$ , $Y^{\\mathrm{(i)}}:=(y_{1}^{\\mathrm{(i)}},\\dots,y_{n}^{\\mathrm{(i)}})\\in\\mathbb{R}^{q\\times n}$ , and $\\begin{array}{r}{E^{\\mathrm{(i)}}=(\\epsilon_{1}^{\\mathrm{(i)}},\\ldots,\\epsilon_{n}^{\\mathrm{(i)}}):=Y^{\\mathrm{(i)}}-B^{\\mathrm{(i)}}X^{\\mathrm{(i)}}\\in\\mathbb{R}^{q\\times n}}\\end{array}$ . Denote the in-distribution sample covariance matrices by $\\hat{\\Sigma}_{x}^{\\mathrm{(i)}}\\,:=\\,(1/n)X^{\\mathrm{(i)}}X^{\\mathrm{(i)}\\top},\\,\\hat{\\Sigma}$ $\\hat{\\Sigma}_{\\epsilon}^{\\mathrm{(i)}}\\,:=\\,(1/n)E^{\\mathrm{(i)}}E^{\\mathrm{(i)}\\top}$ , $\\hat{\\Sigma}_{x,\\epsilon}^{\\mathrm{(i)}}:=(1/n)X^{\\mathrm{(i)}}E^{\\mathrm{(i)}\\top}$ , $\\hat{\\Sigma}_{\\epsilon,x}^{\\mathrm{(i)}}=\\hat{\\Sigma}_{x,\\epsilon}^{\\mathrm{(i)}\\top}$ . Define $\\check{\\Sigma}_{x,\\epsilon}^{(k)}=(X^{(\\mathrm{i})\\top})^{\\dagger}E^{(\\mathrm{i})\\top}$ , $\\hat{A}:=(\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}\\hat{\\Sigma}_{x}^{(\\mathrm{i})}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}\\top})^{1/2}$ $A\\ :=\\ (\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}\\Sigma_{x}^{\\mathrm{(i)}}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}\\top})^{1/2}$ , $\\Phi^{\\prime}\\;:=\\;\\Phi_{*}(\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}})$ , $\\Phi_{S}^{\\prime\\prime}\\;:=\\;\\Phi_{*}(\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}}U_{S}^{\\mathrm{S}^{2}\\mathrm{FT}})$ , $D\\;=\\;B^{(\\mathrm{i})}\\;-\\;W^{\\mathrm{pre}}\\;$ , $\\begin{array}{r}{\\hat{D}:=B^{\\mathrm{(i)}}-W^{\\mathrm{pre}}+\\check{\\Sigma}_{\\epsilon,x}^{\\mathrm{(i)}}}\\end{array}$ . Also define $M:=\\Phi^{\\prime\\top}D\\Sigma_{x}^{\\mathrm{(i)}}{W_{\\ell-1}^{\\mathrm{pre}\\top}}A^{\\dagger}$ \u2113 a+n1d $\\hat{M}:=\\Phi^{\\prime\\top}\\hat{D}\\hat{\\Sigma}_{x}^{\\mathrm{(i)}}{W}_{\\ell-1}^{\\mathrm{pre}\\top}\\hat{A}^{\\dagger}$ . Let $\\hat{\\Psi}^{\\prime}:=\\Psi_{*}(\\hat{A})$ , and $G_{\\ell}^{(\\mathrm{i},\\mathrm{o})}:=(\\underline{{W}}_{\\ell}^{\\mathrm{pre}}\\Sigma_{x}^{(\\mathrm{i})1/2})^{\\dagger}\\underline{{W}}_{\\ell}^{\\mathrm{pre}}\\Sigma_{x}^{(\\mathrm{o})1/2}$ be a matrix that captures the covariate shift at the $\\ell$ -th layer. ", "page_idx": 17}, {"type": "text", "text": "We consider fine-tuning the $\\ell_{}$ -th $\\langle\\ell\\in[L])$ layer of the pre-trained deep linear network $f^{\\mathrm{pre}}(x)=$ $W_{L}^{\\mathrm{pre}}W_{L-1}^{\\mathrm{pre}}\\cdot..\\cdot W_{1}^{\\mathrm{pre}}x$ using in-distribution observations $(x_{i}^{(\\mathrm{i})},y_{i}^{(\\mathrm{i})})_{i\\in[n]}$ . ", "page_idx": 17}, {"type": "text", "text": "To measure the performance of models, we define the excess risks of $f$ for the task $k\\in\\{\\mathrm{i},\\mathrm{o}\\}$ as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{E}^{(k)}(f):=\\mathbb{E}[\\|y^{(k)}-f(x^{(k)})\\|^{2}]-\\operatorname*{inf}_{f^{\\prime}}\\mathbb{E}[\\|y^{(k)}-f^{\\prime}(x^{(k)})\\|^{2}],\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the infimum is taken over all square integrable functions. ", "page_idx": 17}, {"type": "text", "text": "E.3 Assumptions ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We assume that $\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}\\Sigma_{x}^{\\mathrm{(i)}}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}\\top}\\neq0$ , since otherwise ${\\underline{{W}}}_{\\ell-1}^{\\mathrm{pre}}x^{\\mathrm{(i)}}=0$ almost surely and fine-tuning the $\\ell$ -th layer does not improve the performance of pre-trained model. Define the in-distribution prediction residuals for pre-trained model $f^{\\mathrm{pre}}$ by $\\Sigma_{f}^{\\mathrm{(i)}}:=\\mathbb{E}[(B^{\\mathrm{(i)}}x^{\\mathrm{(i)}}-W^{\\mathrm{pre}}x^{\\mathrm{(i)}})(B^{\\mathrm{(i)}}x^{\\mathrm{(i)}}-W^{\\mathrm{pre}}x^{\\mathrm{(i)}})^{\\top}]$ . Note that $\\mathcal{E}^{\\mathrm{(i)}}\\big(f^{\\mathrm{pre}}\\big)=\\mathrm{tr}\\Big(\\Sigma_{f}^{\\mathrm{(i)}}\\Big)$ . We also assume that $\\|\\Sigma_{f}^{\\mathrm{(i)}}\\|_{\\mathrm{op}}>0$ . Since otherwise $\\mathcal{E}^{\\mathrm{(i)}}(f^{\\mathrm{pre}})=$ $\\begin{array}{r}{\\|\\Sigma_{f}^{\\mathrm{(i)}}\\|_{\\mathrm{F}}^{2}=0}\\end{array}$ and there is no room for improvement from pre-trained model. ", "page_idx": 17}, {"type": "text", "text": "We introduce several assumptions. ", "page_idx": 17}, {"type": "text", "text": "Assumption E.1 (Sub-Gaussianity). Assume that there exist some constants $c_{1},c_{2}\\in(0,\\infty)$ such that $(x^{\\mathrm{(i)}},\\epsilon^{\\mathrm{(i)}})$ in the model 7 satisfies ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\gamma^{\\top}\\Sigma_{x}^{\\mathrm{(i)}}\\gamma\\geq c_{1}\\|\\gamma^{\\top}x^{\\mathrm{(i)}}\\|_{\\psi_{2}}^{2},\\quad\\mathrm{and}\\quad\\gamma^{\\prime\\top}\\Sigma_{\\epsilon}^{\\mathrm{(i)}}\\gamma^{\\prime}\\geq c_{2}\\|\\gamma^{\\prime\\top}\\epsilon^{\\mathrm{(i)}}\\|_{\\psi_{2}}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for any $\\gamma\\in\\mathbb{R}^{d}$ and $\\gamma^{\\prime}\\in\\mathbb{R}^{p}$ , where $\\|y\\|_{\\psi_{2}}$ is the sub-Gaussian norm defined as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|y\\|_{\\psi_{2}}:=\\operatorname*{inf}\\left\\{v>0:\\mathbb{E}[\\exp\\left(y^{2}/v^{2}\\right)]\\le2\\right\\}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for a random variable $y$ taking values in $\\mathbb{R}$ . ", "page_idx": 17}, {"type": "text", "text": "Assumption E.2 (Sufficiently Many Observations). Assume that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{n\\gg(\\kappa_{*}^{4}(A)r_{e}(A^{2})+\\kappa_{*}^{2}(\\Sigma_{x}^{(\\mathrm{i})})r_{e}(\\Sigma_{x}^{(\\mathrm{i})})+r_{e}(D\\Sigma_{x}^{(\\mathrm{i})}D^{\\top}))\\log^{2}(n+d+p),}\\\\ &{n\\gg\\cfrac{\\|\\Sigma_{\\epsilon}^{(\\mathrm{i})}\\|_{\\infty}}{\\|D\\Sigma_{x}^{(\\mathrm{i})}D^{\\top}\\|_{\\infty}}(r_{e}(\\Sigma_{\\epsilon}^{(\\mathrm{i})})+r_{e}(A^{2}))\\log^{2}(n+d+p),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and ", "page_idx": 17}, {"type": "equation", "text": "$$\nn\\gg\\kappa_{*}^{4}(\\Sigma_{x}^{(\\mathrm{i})})\\frac{r_{e}(\\Sigma_{x}^{(\\mathrm{i})})(r_{e}(\\Sigma_{\\epsilon}^{(\\mathrm{i})})+r_{e}(\\Sigma_{x}^{(\\mathrm{i})}))}{r_{e}(A^{2})}\\log^{2}(n+d+p).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Assumption E.3 (Eigengap Condition). Assume that there exists some constant $C_{\\mathrm{g}}>0$ such that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{\\lambda_{s}(\\Phi^{\\prime\\top}D\\Sigma_{x}^{\\mathrm{(i)}}\\underline{{W_{\\ell-1}^{\\mathrm{pre}}}}\\bar{A}^{\\dagger})}{\\lambda_{s}(\\Phi^{\\prime\\top}D\\Sigma_{x}^{\\mathrm{(i)}}\\underline{{W_{\\ell-1}^{\\mathrm{pre}}}}\\bar{A}^{\\dagger})-\\lambda_{s+1}(\\Phi^{\\prime\\top}D\\Sigma_{x}^{\\mathrm{(i)}}\\underline{{W_{\\ell-1}^{\\mathrm{pre}}}}\\bar{A}^{\\dagger})}\\lesssim C_{\\mathrm{g}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "holds. ", "page_idx": 18}, {"type": "text", "text": "Assumption E.3 is necessary to identify the rank- $s$ approximation of $M$ , which is used to derive the risk of LoRA. ", "page_idx": 18}, {"type": "text", "text": "Assumption E.4 (Approximate Sparsity of Heads). Assume that there exists some $S_{0}\\subset[d_{\\ell}]$ with $|S_{0}|\\le s$ and $\\delta>0$ such that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{a\\in[d_{\\ell}]\\backslash S_{0}}\\|e_{a}^{\\top}(\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}})^{\\dagger}(B^{(\\mathrm{i})}-W^{\\mathrm{pre}})\\Sigma_{x}^{(\\mathrm{i})1/2}\\|^{2}\\leq\\delta^{2}\\|(\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}})^{\\dagger}(B^{(\\mathrm{i})}-W^{\\mathrm{pre}})\\Sigma_{x}^{(\\mathrm{i})1/2}\\|_{\\mathrm{F}}^{2}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "holds. ", "page_idx": 18}, {"type": "text", "text": "Assumption E.5 (Distribution Shift). Assume that $\\Sigma_{x}^{\\mathrm{(i)}}=\\Sigma_{x}^{\\mathrm{(o)}}=\\Sigma_{x}$ for some $\\Sigma_{x}\\in\\mathbb{R}^{d\\times d}$ and that $\\|\\Phi_{*}^{\\top}(\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}}U_{S}^{\\mathrm{S^{2}F T}})(B^{(\\mathrm{o})}-B^{(\\mathrm{i})})\\Sigma_{x}^{1/2}\\|_{\\mathrm{F}}^{2}\\leq\\varepsilon^{2}\\mathcal{E}^{(\\mathrm{o})}(f^{\\mathrm{pre}})$ for some $\\varepsilon>0$ . ", "page_idx": 18}, {"type": "text", "text": "Assumption E.6 (Condition Number). Assume that $_*(M)\\lesssim1,\\kappa_{*}(\\overline{W}_{\\ell+1}^{\\mathrm{pre}})\\lesssim1,\\kappa_{*}(\\Sigma_{f}^{(\\mathrm{i})})\\lesssim1$ and $\\kappa_{*}(\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}\\Sigma_{x}^{\\mathrm{(i)}}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}\\top})\\lesssim1.$ . ", "page_idx": 18}, {"type": "text", "text": "Note that Assumption E.6 is not essential to our analysis. ", "page_idx": 18}, {"type": "text", "text": "E.4 Main Results ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We first demonstrate that LoRA and $\\mathrm{S^{2}F T}$ exhibit comparable memorization abilities. Next, we present a formal restatement of 4.2 that combine Theorems E.10, E.11, E.13, E.15, and Lemma E.14. ", "page_idx": 18}, {"type": "text", "text": "Theorem E.7. Suppose that Assumptions E.1, E.2, E.3, E.4, and E.6 hold. Choose $S$ such that $S\\supset S_{0}$ holds. Let $U^{\\mathrm{LoRA}}$ , $V^{\\mathrm{LoRA}}$ be the LoRA adaptation matrices defined in (5). Let $V^{\\mathrm{{S^{2}F T}}}$ be the $S^{2}F T$ adaptation matrices given $U_{S}^{\\mathrm{S^{2}F T}}$ defined in (6). Then, for all sufficiently large $n$ , the following holds with probability $1-\\exp\\bigl(-\\Omega(\\log^{2}(n+d+p))\\bigr)$ : for any $\\eta>0$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{r}{\\mathcal{E}^{(\\mathrm{i})}(f_{\\ell,U_{S}^{\\mathrm{s2}}\\mathrm{Fr},V^{\\mathrm{s2}}\\mathrm{Fr}})\\leq(1+\\eta)(T_{\\mathrm{bias}}^{\\mathrm{S^{2}}\\mathrm{Fr}})^{2}+(1+\\eta^{-1})(T_{\\mathrm{variance}}^{\\mathrm{S^{2}}\\mathrm{Fr}})^{2},}\\end{array}}\\\\ {\\begin{array}{r l}&{\\mathcal{E}^{(\\mathrm{i})}(f_{\\ell,U^{\\mathrm{LoRA}},V^{\\mathrm{LoRA}}})\\leq(1+\\eta)(T_{\\mathrm{bias}}^{\\mathrm{LoRA}})^{2}+(1+\\eta^{-1})(T_{\\mathrm{variance}}^{\\mathrm{LoRA}})^{2},}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{0\\leq(T_{\\mathrm{bias}}^{\\mathrm{LoRA}})^{2}-\\mathcal{E}^{(\\mathrm{i})}(f_{\\ell}^{\\mathrm{full}})\\leq(T_{\\mathrm{bias}}^{\\mathrm{S^{2}F T}})^{2}-\\mathcal{E}^{(\\mathrm{i})}(f_{\\ell}^{\\mathrm{full}})\\lesssim\\delta^{2}\\mathcal{E}^{(\\mathrm{i})}(f^{\\mathrm{pre}}),}\\\\ &{(T_{\\mathrm{variance}}^{\\mathrm{S^{2}F T}})^{2}\\lesssim(\\|\\Sigma_{\\epsilon}^{(\\mathrm{i})}\\|_{\\mathrm{op}}+\\|\\Sigma_{f}^{(\\mathrm{i})}\\|_{\\mathrm{op}})\\frac{s d_{\\ell-1}\\log^{2}(n+p+q)}{n},}\\\\ &{(T_{\\mathrm{variance}}^{\\mathrm{LoRA}})^{2}\\lesssim(\\|\\Sigma_{\\epsilon}^{(\\mathrm{i})}\\|_{\\mathrm{op}}+\\|\\Sigma_{f}^{(\\mathrm{i})}\\|_{\\mathrm{op}})\\frac{r\\left(d_{\\ell}+d_{\\ell-1}\\right)\\log^{2}(n+p+q)}{n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Theorem E.8 (Restatement of Theorem 4.2). Consider the limit $n\\to\\infty$ . Suppose that Assumption $E.5$ holds. Let $U^{\\mathrm{LoRA}}$ , $V^{\\mathrm{LoRA}}$ be the LoRA adaptation matrices defined in (15). Let $V^{\\mathrm{{S^{2}F T}}}$ be the $S^{2}F T$ adaptation matrices given $U_{S}^{\\mathrm{S^{2}F T}}$ defined in (25). If $B^{\\mathrm{(i)}}=\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}}\\tilde{B}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}$ holds for some $\\tilde{B}^{(\\mathrm{i})}\\in\\mathbb{R}^{d_{\\ell}\\times d_{\\ell-1}}$ , and $s\\leq\\mathrm{rank}(\\Sigma_{f}^{\\mathrm{(i)}})$ , then, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathscr{E}^{(\\mathrm{o})}(f_{\\ell,U_{S}^{\\mathrm{s}^{2}\\mathrm{FT}},V^{\\mathrm{s}^{2}\\mathrm{FT}}})\\leq(1+3\\varepsilon^{2})\\mathscr{E}^{(\\mathrm{o})}(f^{\\mathrm{pre}}),}\\\\ &{\\mathscr{E}^{(\\mathrm{o})}(f_{\\ell,U^{\\mathrm{LoRA}},V^{\\mathrm{LoRA}}})\\geq\\|(B^{(\\mathrm{o})}-B^{(\\mathrm{i})})\\Sigma_{x}^{1/2}\\|_{\\mathrm{F}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Intuition of the proof of Theorem $E.8$ . LoRA forgets pre-trained tasks due to its model complexity. Consider the simplest low-rank adaptation to a single-layer linear network: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Delta_{1}\\in}&{\\underset{\\Delta_{1}^{\\prime}\\in\\mathbb{R}^{d_{1}}\\times d_{0}}{\\arg\\operatorname*{min}}\\mathbb{E}[\\|y^{(\\mathrm{i})}-(W_{1}^{\\mathrm{pre}}+\\Delta_{1}^{\\prime})x^{(\\mathrm{i})}\\|^{2}].}\\\\ &{\\quad\\quad\\quad\\frac{\\Delta_{1}^{\\prime}\\in\\mathbb{R}^{d_{1}}\\times d_{0}}{\\mathrm{rank}(\\Delta_{1}^{\\prime})=s}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Assume that $\\Sigma_{x}^{\\mathrm{(i)}}=I$ , then we can show that the solution is $\\Delta_{1}=\\mathrm{SVD}_{s}(B^{\\mathrm{(i)}}-W_{1}^{\\mathrm{pre}})$ . Under the condition that the rank of $B^{(\\mathrm{i})}-W_{1}^{\\mathrm{pre}}$ is smaller than, or comparable to $s$ , LoRA fine-tuned model can learn the in-distribution best regressor in $\\ell_{2}$ sense, since $(W_{1}^{\\mathrm{pre}}+\\Delta_{1})x\\approx B^{(\\mathrm{i})}x=\\mathbb{E}[y^{(\\mathrm{i})}|x^{(\\mathrm{i})}=x]$ . Hence it makes LoRA fine-tuned model vlunerable to distribution shift. ", "page_idx": 19}, {"type": "text", "text": "On the other hand, we model $\\mathrm{S^{2}F T}$ as fine-tuning only a few heads: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Delta_{1}\\in\\operatorname*{arg\\,min}_{\\Delta_{1}^{\\prime}=\\sum_{a\\in S}e_{a}v_{a}^{\\top},v_{a}\\in\\mathbb{R}^{d_{0}}}\\mathbb{E}[\\|y^{(\\mathrm{i})}-(W_{1}^{\\mathrm{pre}}+\\Delta_{1}^{\\prime})x^{(\\mathrm{i})}\\|^{2}].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Although $S^{2}\\mathrm{FT}$ is a special case of LoRA, the constraint on the direction of low-rank matrix prevents overftiting to the in-distribution task. To see this, note that a sparse fine-tuned model can be written as ", "page_idx": 19}, {"type": "equation", "text": "$$\n(W_{1}^{\\mathrm{pre}}+\\Delta_{1})x=W_{1}^{\\mathrm{pre}}x+\\sum_{a\\in S}e_{a}e_{a}^{\\top}(B^{(\\mathrm{i})}-W_{1}^{\\mathrm{pre}})x=\\sum_{a\\in S^{c}}e_{a}e_{a}^{\\top}W_{1}^{\\mathrm{pre}}x+\\sum_{a\\in S}e_{a}e_{a}^{\\top}B^{(\\mathrm{i})}x,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $S\\subset[d_{1}]$ is a set of channels/heads with cardinality $s$ . Since sparse fine-tuned model keeps parameters from the pre-trained model, except for rows specified by $S$ , the model less forget pretraining tasks. ", "page_idx": 19}, {"type": "text", "text": "E.5 Proofs for LoRA ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "E.5.1 Excess Risk of LoRA ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Lemma E.9 (Excess Risk). Consider the minimum norm solution ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(U^{\\mathrm{LoRA}},V^{\\mathrm{LoRA}})\\in\\underset{(U,V)\\in\\mathbb R^{d_{\\ell}\\times s}\\times\\mathbb R^{d_{\\ell-1}\\times s}}{\\arg\\operatorname*{min}}\\,\\Vert(U,V)\\Vert_{\\mathrm{F}}^{2}\\ \\ s.t.\\ (U,V)\\,m i n i m i z e s\\ \\mathcal{R}_{n}^{(\\mathrm{i})}(f_{\\ell,U,V}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then, the low-rank adaptation matrix satisfies ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\boldsymbol{U}^{\\mathrm{LoRA}}\\boldsymbol{V}^{\\mathrm{LoRA\\top}}=(\\overline{{\\boldsymbol{W}}}_{\\ell+1}^{\\mathrm{pre}})^{\\dagger}\\boldsymbol{S}\\boldsymbol{V}\\boldsymbol{D}_{s}(\\overline{{\\boldsymbol{W}}}_{\\ell+1}^{\\mathrm{pre}}(\\overline{{\\boldsymbol{W}}}_{\\ell+1}^{\\mathrm{pre}})^{\\dagger}\\hat{\\boldsymbol{D}}\\hat{\\boldsymbol{\\Sigma}}_{x}^{\\mathrm{(i)}}\\underline{{\\boldsymbol{W}}}_{\\ell-1}^{\\mathrm{pre}\\top}\\hat{\\boldsymbol{A}}^{\\dagger})\\hat{\\boldsymbol{A}}^{\\dagger},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{E}^{(k)}(f_{\\ell,U^{\\mathrm{LORA}},V^{\\mathrm{LORA}}})=\\mathrm{tr}\\bigg(\\Big(B^{(k)}-W^{\\mathrm{pre}}-S V D_{s}(\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}}(\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}})^{\\dagger}\\hat{D}\\hat{\\Sigma}_{x}^{(\\mathrm{i})}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}\\top}\\hat{A}^{\\dagger})\\hat{A}^{\\dagger}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}\\Big)\\Sigma_{x}^{(k)}}\\\\ &{\\qquad\\qquad\\qquad\\quad\\cdot\\left(B^{(k)}-W^{\\mathrm{pre}}-S V D_{s}(\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}}(\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}})^{\\dagger}\\hat{D}\\hat{\\Sigma}_{x}^{(\\mathrm{i})}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}\\top}\\hat{A}^{\\dagger})\\hat{A}^{\\dagger}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}\\right)^{\\top}\\bigg)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "for $k\\in\\{\\mathrm{i},\\mathrm{o}\\}$ ", "page_idx": 19}, {"type": "text", "text": "Proof of Lemma $E.9.$ . The empirical risk of $f_{\\ell,U,V}$ for the in-distribution task can be written as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{R}_{n}^{(\\mathrm{i})}(f_{\\ell,U,V})=\\displaystyle\\frac{1}{n}\\sum_{i\\in[n]}\\|(B^{(i)}-W^{\\mathrm{pe}})x_{i}^{(\\mathrm{i})}+\\epsilon_{i}^{(\\mathrm{i})}-\\overline{{W_{\\ell+1}^{\\mathrm{pe}}}}U V^{\\top}\\underline{{W_{\\ell-1}^{\\mathrm{pe}}}}x_{i}^{(\\mathrm{i})}\\|^{2}}\\\\ &{\\quad\\quad\\quad=\\mathrm{tr}\\bigg((B^{(i)}-W^{\\mathrm{pe}}-\\overline{{W_{\\ell+1}^{\\mathrm{pe}}}}U V^{\\top}\\underline{{W_{\\ell-1}^{\\mathrm{pe}}}})\\hat{\\Sigma}_{\\mathcal{C}}^{(i)}(B^{(i)}-W^{\\mathrm{pe}}-\\overline{{W_{\\ell+1}^{\\mathrm{pe}}}}U V^{\\top}\\underline{{W_{\\ell-1}^{\\mathrm{pe}}}})^{\\top}\\bigg)}\\\\ &{\\quad\\quad\\quad\\quad\\quad+2\\mathrm{tr}\\bigg((B^{(i)}-W^{\\mathrm{pe}}-\\overline{{W_{\\ell+1}^{\\mathrm{pe}}}}U V^{\\top}\\underline{{W_{\\ell-1}^{\\mathrm{pe}}}})\\hat{\\Sigma}_{\\mathcal{C},\\epsilon}^{(i)}\\bigg)+\\mathrm{tr}\\bigg(\\hat{\\Sigma}_{\\mathcal{C}}^{(i)}\\bigg)}\\\\ &{\\quad\\quad\\quad=\\mathrm{tr}\\bigg(V^{\\top}\\underline{{W_{\\ell-1}^{\\mathrm{pe}}}}\\hat{\\Sigma}_{\\mathcal{C}-1}^{(i)}\\underline{{W_{\\ell-1}^{\\mathrm{pe}}}}V U^{\\top}\\overline{{W_{\\ell+1}^{\\mathrm{pe}}}}\\overline{{W_{\\ell+1}^{\\mathrm{pe}}}}U\\bigg)}\\\\ &{\\quad\\quad\\quad\\quad-2\\mathrm{tr}\\bigg(\\overline{{W_{\\ell+1}^{\\mathrm{pe}}}}U V^{\\top}\\underline{{W_{\\ell-1}^{\\mathrm{pe}}}}\\Big\\{\\hat{\\Sigma}_{\\mathcal{C}}^{(i)}(B^{(i)}-W^{\\mathrm{pe}})^{\\top}+\\hat{\\Sigma}_{\\mathcal{C},\\epsilon}^{(i)}\\bigg\\}\\bigg)}\\\\ &{\\quad\\quad\\quad\\quad\\quad+\\mathrm{tr}\\bigg((B^{(i)}-W^{\\mathrm{pe}})\\hat\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Since $\\begin{array}{r}{\\hat{\\Sigma}_{x,\\epsilon}^{\\mathrm{(i)}}=\\hat{\\Sigma}_{x}^{\\mathrm{(i)}}(X^{\\mathrm{(i)}\\top})^{\\dagger}E^{\\mathrm{(i)}\\top}=\\hat{\\Sigma}_{x}^{\\mathrm{(i)}}\\check{\\Sigma}_{x,\\epsilon}^{\\mathrm{(i)}}.}\\end{array}$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{R}_{n}^{(\\mathrm{i})}(f_{\\ell,U,V})=\\mathrm{tr}\\bigg(\\hat{A}V U^{\\top}\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}\\top}\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}}U V^{\\top}\\hat{A}\\bigg)-2\\,\\mathrm{tr}\\bigg(\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}}U V^{\\top}\\hat{A}\\hat{A}^{\\dagger}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}\\hat{\\Sigma}_{x}^{(\\mathrm{i})}\\hat{D}^{\\top}\\bigg)}\\\\ &{\\quad\\quad\\quad-\\,2\\,\\mathrm{tr}\\bigg(\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}}U V^{\\top}\\big(I-\\hat{A}\\hat{A}^{\\dagger}\\big)\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}\\hat{\\Sigma}_{x}^{(\\mathrm{i})}\\hat{D}^{\\top}\\bigg)}\\\\ &{\\quad\\quad\\quad\\quad+\\,\\mathrm{tr}\\bigg(D\\hat{\\Sigma}_{x}^{(\\mathrm{i})}D^{\\top}\\bigg)+2\\,\\mathrm{tr}\\bigg(D\\hat{\\Sigma}_{x,\\epsilon}^{(\\mathrm{i})}\\bigg)+\\mathrm{tr}\\bigg(\\hat{\\Sigma}_{\\epsilon}^{(\\mathrm{i})}\\bigg)}\\\\ &{\\quad\\quad\\quad=\\|\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}}U V^{\\top}\\hat{A}-\\hat{D}\\hat{\\Sigma}_{x}^{(\\mathrm{i})}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}\\top}\\hat{A}^{\\dagger}\\|_{\\mathrm{F}}^{2}-\\|\\hat{D}\\hat{\\Sigma}_{x}^{(\\mathrm{i})}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}\\top}\\hat{A}^{\\dagger}\\|_{\\mathrm{F}}^{2}}\\\\ &{\\quad\\quad\\quad+\\,\\mathrm{tr}\\bigg(D\\hat{\\Sigma}_{x}^{(\\mathrm{i})}D^{\\top}\\bigg)+2\\,\\mathrm{tr}\\bigg(D\\hat{\\Sigma}_{x,\\epsilon}^{(\\mathrm{i})}\\bigg)+\\mathrm{tr}\\bigg(\\hat{\\Sigma}_{\\epsilon}^{(\\mathrm{i})}\\bigg),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where we used $(I-\\hat{A}\\hat{A}^{\\dagger})\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}\\hat{\\Sigma}_{x}^{(\\mathrm{i})1/2}=0$ . From (9), minimizing $\\mathcal{R}_{n}^{(\\mathrm{i})}(f_{\\ell,U,V})$ is equivalent to minimizing the norm: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\overline{{W}}_{\\ell+1}^{\\mathrm{pe}}U V^{\\top}\\hat{A}-\\hat{D}\\hat{\\Sigma}_{x}^{(\\mathrm{i})}\\underline{{W}}_{\\ell-1}^{\\mathrm{pe}^{\\top}\\top}\\hat{A}^{\\dagger}\\|_{\\mathrm{F}}^{2}=\\|\\overline{{W}}_{\\ell+1}^{\\mathrm{pe}}U V^{\\top}\\hat{A}-\\overline{{W}}_{\\ell+1}^{\\mathrm{pe}}(\\overline{{W}}_{\\ell+1}^{\\mathrm{pe}})^{\\dagger}\\hat{D}\\hat{\\Sigma}_{x}^{(\\mathrm{i})}\\underline{{W}}_{\\ell-1}^{\\mathrm{pe}\\top}\\hat{A}^{\\dagger}\\|_{\\mathrm{F}}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\|(I-\\overline{{W}}_{\\ell+1}^{\\mathrm{pe}}(\\overline{{W}}_{\\ell+1}^{\\mathrm{pe}})^{\\dagger})\\hat{D}\\hat{\\Sigma}_{x}^{(\\mathrm{i})}\\underline{{W}}_{\\ell-1}^{\\mathrm{pe}\\top}\\hat{A}^{\\dagger}\\|_{\\mathrm{F}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "This is minimized by $(U^{\\prime},V^{\\prime})$ satisfying ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{U^{\\prime}V^{\\prime\\top}=(\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}})^{\\dagger}\\mathrm{SVD}_{s}(\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}}(\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}})^{\\dagger}\\hat{D}\\hat{\\Sigma}_{x}^{(\\mathrm{i})}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}\\top}\\hat{A}^{\\dagger})\\hat{A}^{\\dagger}}\\\\ &{\\qquad\\qquad+\\,(I-(\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}})^{\\dagger}\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}})A_{1}+A_{2}(I-\\hat{\\Psi}^{\\prime}\\hat{\\Psi}^{\\prime\\top}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $A_{1},A_{2}\\in\\mathbb{R}^{d_{\\ell}\\times d_{\\ell-1}}$ are arbitrary matrices. Since we particularly consider the minimum norm solution, we must have $A_{1}=0$ and $A_{2}=0$ . Hence ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}}U^{\\mathrm{LoRA}}V^{\\mathrm{LoRA\\top}}\\underline{W}_{\\ell-1}^{\\mathrm{pre}}=\\mathrm{SVD}_{s}\\big(\\overline{W}_{\\ell+1}^{\\mathrm{pre}}(\\overline{W}_{\\ell+1}^{\\mathrm{pre}})^{\\dagger}\\hat{D}\\hat{\\Sigma}_{x}^{(\\mathrm{i})}\\underline{W}_{\\ell-1}^{\\mathrm{pre}\\top}\\hat{A}^{\\dagger}\\big)\\hat{A}^{\\dagger}\\underline{W}_{\\ell-1}^{\\mathrm{pre}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Therefore, the excess risk for $k\\in\\{\\mathrm{i},\\mathrm{o}\\}$ becomes ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\xi^{(k)}(f_{\\ell,U^{\\mathrm{LoRA}},V^{\\mathrm{LoRA}}})=\\mathbb{E}\\bigg[\\bigg(B^{(k)}x^{(k)}-\\overline{{W}}_{\\ell+1}^{\\mathrm{pe}}(W_{\\ell}^{\\mathrm{pre}}+U^{\\mathrm{LoRA}}V^{\\mathrm{LoRA}\\top})\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}x^{(k)}\\bigg)^{2}\\bigg]}\\\\ &{\\qquad\\qquad\\qquad=\\operatorname{tr}\\bigg(\\bigg(B^{(k)}-W^{\\mathrm{pre}}-5\\nabla\\mathsf{D}_{s}(\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}}(\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}})^{\\dagger}\\hat{D}\\hat{\\Sigma}_{x}^{(\\mathrm{i})}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}\\top}\\hat{A}^{\\dagger})\\hat{A}^{\\dagger}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}\\bigg)\\Sigma_{x}^{(k)}}\\\\ &{\\qquad\\qquad\\qquad\\cdot\\left(B^{(k)}-W^{\\mathrm{pre}}-5\\nabla\\mathsf{D}_{s}(\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}}(\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}})^{\\dagger}\\hat{D}\\hat{\\Sigma}_{x}^{(\\mathrm{i})}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}\\top}\\hat{A}^{\\dagger})\\hat{A}^{\\dagger}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}\\right)^{\\top}\\bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "This concludes the proof. ", "page_idx": 20}, {"type": "text", "text": "E.5.2 In-distribution Excess Risk of LoRA ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Let $\\mathcal{E}^{(\\mathrm{i})}\\big(f_{\\ell}^{\\mathrm{full}}\\big)$ denote the excess risk of $f^{\\mathrm{pre}}$ after fine-tuning all the parameters of the $\\ell$ -th layer under population in-distribution risk. ", "page_idx": 20}, {"type": "text", "text": "Theorem E.10 (Restatement of Theorem E.7: LoRA Part). Suppose that Assumptions E.1, E.2 and E.3 hold. Then, the following holds with probability $1-\\exp(-\\bar{\\Omega}(\\log^{2}(n+d+\\bar{p})))$ . For any $\\eta>0$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}^{(\\mathrm{i})}(f_{\\ell,U^{\\mathrm{LoRA}},V^{\\mathrm{LoRA}}})\\leq(1+\\eta)(T_{\\mathrm{bias}}^{\\mathrm{LoRA}})^{2}+(1+\\eta^{-1})(T_{\\mathrm{variance}}^{\\mathrm{LoRA}})^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(T_{\\mathrm{bias}}^{\\mathrm{LoRA}})^{2}\\leq\\frac{0\\vee\\left(\\mathrm{rank}(D\\Sigma_{x}^{(\\mathrm{i})}D^{\\top})-s\\right)}{\\mathrm{rank}(D\\Sigma_{x}^{(\\mathrm{i})}D^{\\top})}\\kappa_{*}^{2}(D\\Sigma_{x}^{(\\mathrm{i})}D^{\\top})\\mathcal{E}^{(\\mathrm{i})}(f^{\\mathrm{pee}})+\\mathcal{E}^{(\\mathrm{i})}(f_{\\ell}^{\\mathrm{full}}),}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad}\\\\ &{T_{\\mathrm{vaiance}}^{\\mathrm{LoRA}})^{2}\\lesssim C^{2}\\kappa_{*}^{4}(M)\\|\\Sigma_{\\epsilon}^{(\\mathrm{i})}\\|_{\\mathrm{op}}\\kappa_{*}^{2}(A)\\frac{s\\left(r_{e}(\\Phi^{\\prime\\top}\\Sigma_{\\epsilon}^{(\\mathrm{i})}\\Phi^{\\prime})+r_{e}(A^{2})\\right)\\log^{2}(n+d+p)}{n}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\,C^{2}\\kappa_{*}^{4}(M)\\|D\\Sigma_{x}^{(\\mathrm{i})}D^{\\top}\\|_{\\mathrm{op}}\\frac{s\\left(\\kappa_{*}^{2}(A)r_{e}(\\Phi^{\\prime\\top}D\\Sigma_{x}^{(\\mathrm{i})}D^{\\top}\\Phi^{\\prime})+\\kappa_{*}^{6}(A)r_{e}(A^{2})\\right)\\log^{2}(n+d+p)}{n}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Note that the first term on the right hand side of (11) depends on the rank of residual matrix $\\Sigma_{f}^{\\mathrm{(i)}}=D\\Sigma_{x}^{\\mathrm{(i)}}D^{\\top}$ . It becomes zero when ran $\\mathfrak{s}\\big(\\Sigma_{f}^{\\mathrm{(i)}}\\big)\\leq s$ and small when $s/\\operatorname{rank}(\\Sigma_{f}^{\\mathrm{(i)}})\\approx1$ . ", "page_idx": 20}, {"type": "text", "text": "Proof of Theorem E.10. Let $\\overline{{W}}_{\\ell}^{\\mathrm{LoRA}}:=\\overline{{W}}_{\\ell+1}^{\\mathsf{p r e}}U^{\\mathrm{LoRA}}V^{\\mathrm{LoRA}\\top}$ . From Lemma E.9, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{E}^{(\\mathrm{i})}\\big(f_{\\ell,U^{\\mathrm{LoRA}},V^{\\mathrm{LoRA}}}\\big)=\\mathrm{tr}\\bigg((D-\\overline{{W}}_{\\ell}^{\\mathrm{LoRA}}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}})\\Sigma_{x}^{(\\mathrm{i})}(D-\\overline{{W}}_{\\ell}^{\\mathrm{LoRA}}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}})^{\\top}\\bigg)}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\|(\\overline{{W}}_{\\ell}^{\\mathrm{LoRA}}A A^{\\dagger}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}-D)\\Sigma_{x}^{(\\mathrm{i})1/2}\\|_{\\mathrm{F}}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where we used (I \u2212AA\u2020)W \u2113pr\u2212e1\u03a3(xi)1/2= 0. From Lemma E.9 ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\overline{{W}}_{\\ell}^{\\mathrm{LoRA}}A=\\mathrm{SVD}_{s}(\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}}(\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}})^{\\dagger}\\hat{D}\\hat{\\Sigma}_{x}^{(\\mathrm{i})}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}\\top}\\hat{A}^{\\dagger})\\hat{A}^{\\dagger}A.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "This gives ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\overline{{W}}_{\\ell}^{\\mathrm{LoRA}}A A^{\\dagger}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}-D)\\Sigma_{x}^{(i)1/2}\\|_{\\mathrm{F}}\\leq\\|(\\overline{{W}}_{\\ell}^{\\mathrm{LoRA}}A-\\mathrm{SVD}_{s}(\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}}(\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}})^{\\dagger}D\\Sigma_{x}^{(i)}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}A^{\\dagger}))A^{\\dagger}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\|\\mathrm{SVD}_{s}(\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}}(\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}})^{\\dagger}D\\Sigma_{x}^{(i)}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}A^{\\dagger})A^{\\dagger}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}\\Sigma_{x}^{(i)1/2}-I}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=:T_{\\mathrm{vaince}}^{\\mathrm{LoRA}}+T_{\\mathrm{bis}}^{\\mathrm{LoRA}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We bound $T_{\\mathrm{variance}}^{\\mathrm{LoRA}}$ and T LoRA separately. ", "page_idx": 21}, {"type": "text", "text": "Bound T LoRA For the term T vLaoriRanAc , since $A^{\\dagger}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}\\Sigma_{x}^{(\\mathrm{i})}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}\\top}A^{\\dagger}=A^{\\dagger}A^{2}A^{\\dagger},$ ,", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{r}_{\\mathrm{sarance}}^{\\mathrm{LoRA}}=\\|\\mathrm{SVD}_{s}(\\overline{{W}}_{\\ell+1}^{\\mathtt{p e}}(\\overline{{W}}_{\\ell+1}^{\\mathtt{p e}})^{\\dagger}\\hat{D}\\hat{\\Sigma}_{x}^{\\scriptscriptstyle(\\mathrm{i})}\\underline{{W}}_{\\ell-1}^{\\mathtt{p e}\\top}\\hat{A}^{\\dagger})\\hat{A}^{\\dagger}A-\\mathrm{SVD}_{s}\\big(\\overline{{W}}_{\\ell+1}^{\\mathtt{p e}}(\\overline{{W}}_{\\ell+1}^{\\mathtt{p e}})^{\\dagger}D\\Sigma_{x}^{\\scriptscriptstyle(\\mathrm{i})}\\underline{{W}}_{\\ell-1}^{\\mathtt{p e}\\top}A^{\\dagger}\\big)A^{\\dagger}A}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Therefore, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{\\Lambda}_{\\mathrm{variance}}^{\\mathrm{rARA}}\\leq\\|\\mathrm{SVD}_{s}(\\overline{{W}}_{\\ell+1}^{\\mathtt{p e r}}(\\overline{{W}}_{\\ell+1}^{\\mathtt{p e r}})^{\\dagger}D\\Sigma_{s}^{(\\dagger)}\\underline{{W}}_{\\ell-1}^{\\mathtt{p e r}\\top}A^{\\dagger})A^{\\dagger}A-\\mathrm{SVD}_{s}\\big(\\overline{{W}}_{\\ell+1}^{\\mathtt{p e r}}(\\overline{{W}}_{\\ell+1}^{\\mathtt{p e}})^{\\dagger}\\hat{D}\\hat{\\Sigma}_{x}^{(\\dagger)}\\underline{{W}}_{\\ell-1}^{\\mathtt{p e r}\\top}\\hat{A}^{\\dagger}\\big)A^{\\dagger}A}\\\\ &{\\qquad\\qquad+\\,\\|\\mathrm{SVD}_{s}(\\overline{{W}}_{\\ell+1}^{\\mathtt{p e r}}(\\overline{{W}}_{\\ell+1}^{\\mathtt{p e r}})^{\\dagger}\\hat{D}\\hat{\\Sigma}_{x}^{(\\dagger)}\\underline{{W}}_{\\ell-1}^{\\mathtt{p e r}\\top}\\hat{A}^{\\dagger})(\\hat{A}^{\\dagger}A-A^{\\dagger}A)\\|_{\\mathsf{F}}}\\\\ &{\\qquad\\qquad=:T_{\\mathrm{variance},1}^{\\mathrm{LoRA}}+T_{\\mathrm{variance},2}^{\\mathrm{LoRA}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We first bound $T_{\\mathrm{variance,1}}^{\\mathrm{LoRA}}$ . From Lemma F.1 and Assumption E.3, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T_{\\mathrm{variance,1}}^{\\mathrm{LoRA}}\\leq\\|\\mathrm{SVD}_{s}(\\hat{M})-\\mathrm{SVD}_{s}(M)\\|_{\\mathrm{F}}}\\\\ &{\\qquad\\qquad\\leq\\kappa_{*}^{2}(M)\\frac{\\lambda_{s}(M)}{\\lambda_{s}(M)-\\lambda_{s+1}(M)}\\sqrt{s}\\|\\hat{M}-M\\|_{\\mathrm{op}}}\\\\ &{\\qquad\\qquad\\leq\\kappa_{*}^{2}(M)C\\sqrt{s}\\|\\hat{M}-M\\|_{\\mathrm{op}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\hat{M}\\ =\\ \\overline{{W}}_{\\ell+1}^{\\mathrm{pre}}(\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}})^{\\dagger}\\hat{D}\\hat{\\Sigma}_{x}^{\\mathrm{(i)}}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}\\top}\\hat{A}^{\\dagger}$ and $M\\ =\\ \\overline{{W}}_{\\ell+1}^{\\mathrm{pre}}(\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}})^{\\dagger}D\\Sigma_{x}^{\\mathrm{(i)}}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}\\top}A^{\\dagger}$ . From Lemma F.3, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|\\hat{\\boldsymbol X}-\\boldsymbol M\\|_{\\infty}\\leq\\|\\hat{\\boldsymbol W}^{\\tau}\\hat{\\boldsymbol D}_{\\infty}^{\\tau}\\|_{L^{p^{\\prime}}(\\mathbb{R}^{n})}^{2}-\\mathrm{i}\\sqrt{\\tau}\\sum_{\\hat{\\boldsymbol X}^{(\\epsilon)}}\\frac{\\|\\boldsymbol W_{\\infty}^{\\tau}\\|_{\\infty}^{2}\\|_{\\infty}^{2}\\|_{\\infty}}{N}\\|\\hat{\\boldsymbol A}^{\\tau}\\|_{\\infty}}&{}\\\\ {+\\left\\|\\boldsymbol D\\Sigma_{\\infty}^{(0)}\\frac{\\|_{\\infty}^{2}\\|_{\\infty}^{2}\\|_{\\infty}^{2}\\|_{\\infty}^{2}\\|_{\\infty}\\|_{\\infty}^{2}\\|_{\\infty}^{2}-\\mathrm{i}\\|_{\\infty}^{4}\\|_{\\infty}\\right\\|_{\\infty}}{\\sqrt{\\binom{N}{\\epsilon}\\left(N^{\\tau}\\sum_{\\epsilon}({\\Phi}^{\\tau})^{2}\\psi^{\\tau})-r_{*}(\\boldsymbol A^{2}))\\log^{2}(n+d+p)}}}&{}\\\\ {\\overset{\\leq}\\|\\Sigma_{\\infty}^{(0)}\\|_{\\infty}^{2}(s_{*}(\\boldsymbol A))\\sqrt{\\frac{(\\tau_{*}(\\hat{\\boldsymbol V}^{\\tau})\\sum_{\\epsilon}^{(0)}(\\hat{\\boldsymbol V}^{\\tau})+r_{*}(\\boldsymbol A^{2}))\\log^{2}(n+d+p)}{N}}}&{}\\\\ {+\\left\\|\\boldsymbol D\\Sigma_{\\infty}^{(0)}D^{\\tau}\\right\\|_{\\infty}^{2}\\kappa_{*}(\\boldsymbol A)\\sqrt{\\frac{(\\tau_{*}(\\hat{\\boldsymbol V}^{\\tau})\\sum_{\\epsilon}^{(0)}(D^{\\tau}+r_{*}(\\boldsymbol A^{2}))\\log^{2}(n+d+p))}{N}}}&{}\\\\ {+\\left\\|\\boldsymbol D\\Sigma_{\\infty}^{(0)}\\frac{\\|_{\\infty}^{2}\\|_{\\infty}^{2}\\|_{\\infty}^{2}\\|_{\\infty}^{4}\\binom{N}{\\epsilon}\\sum_{\\epsilon}\\left(\\frac{(\\boldsymbol A^{\\prime})^{2}\\log^{2}(n+d+p)}{N}\\right)-\\frac{n}{n}}{}}&{}\\\\ {\\lesssim\\|\\Sigma_{\\infty}^{(0)}\\|_{\\infty}^{2}(s_{*}(\\boldsymbol A) \n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "holds on the event $\\mathcal{F}$ , where we used $\\|D\\Sigma_{x}^{\\mathrm{(i)}}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}\\top}\\|_{\\mathrm{op}}\\leq\\|D\\Sigma_{x}^{\\mathrm{(i)1/2}}\\|_{\\mathrm{op}}\\|A\\|_{\\mathrm{op}}$ . Hence ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T_{\\mathrm{vatiance,1}}^{\\mathrm{LoRA}}\\lesssim C_{\\ g}\\kappa_{*}^{2}(M)\\|\\Sigma_{\\epsilon}^{(\\mathrm{i})}\\|_{\\mathrm{op}}^{1/2}\\kappa_{*}(A)\\sqrt{\\frac{s\\left(r_{\\mathrm{e}}(\\Phi^{\\prime\\top}\\Sigma_{\\epsilon}^{(\\mathrm{i})}\\Phi^{\\prime})+r_{e}(A^{2})\\right)\\log^{2}\\left(n+d+p\\right)}{n}}}\\\\ &{\\qquad\\qquad+\\,C_{g}\\kappa_{*}^{2}(M)\\|D\\Sigma_{x}^{(\\mathrm{i})}D^{\\top}\\|_{\\mathrm{op}}^{1/2}\\sqrt{\\frac{s\\left(\\kappa_{*}^{2}(A)r_{e}(\\Phi^{\\prime\\top}D\\Sigma_{x}^{(\\mathrm{i})}D^{\\top}\\Phi^{\\prime})+\\kappa_{*}^{4}(A)r_{e}(A^{2})\\right)\\log^{2}\\left(n-2\\right)}{n}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Next we bound T vLaoriRanAc e,2. Again from Lemma F.3, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T_{\\mathrm{variance},2}^{\\mathrm{LoRA}}\\leq\\sqrt{s}\\|\\hat{D}\\hat{\\Sigma}_{x}^{(\\mathrm{i})}\\underline{{W}}_{\\ell-1}^{\\mathrm{preT}}\\|_{\\mathrm{op}}\\|\\hat{A}^{\\dagger}\\|_{\\mathrm{op}}\\|\\hat{A}^{\\dagger}-A^{\\dagger}\\|_{\\mathrm{op}}\\|A\\|_{\\mathrm{op}}}\\\\ &{\\qquad\\qquad\\lesssim\\|D\\Sigma_{x}^{(\\mathrm{i})1/2}\\|_{\\mathrm{op}}\\|\\Sigma_{x}^{(\\mathrm{i})1/2}\\underline{{W}}_{\\ell-1}^{\\mathrm{preT}}\\|_{\\mathrm{op}}\\frac{\\kappa_{*}^{2}\\left(A\\right)}{\\lambda_{*}\\left(A\\right)}\\sqrt{\\frac{s r_{e}\\left(A^{2}\\right)\\log^{2}\\left(n+d+p\\right)}{n}}}\\\\ &{\\qquad=\\|D\\Sigma_{x}^{(\\mathrm{i})1/2}\\|_{\\mathrm{op}}\\kappa_{*}^{3}(A)\\sqrt{\\frac{s r_{e}\\left(A^{2}\\right)\\log^{2}\\left(n+d+p\\right)}{n}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "holds on the event $\\mathcal{F}$ . Therefore, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{\\Lambda}_{\\mathrm{variance}}^{\\mathrm{r}\\mathrm{LRA}}\\lesssim C_{\\mathrm{g}}\\kappa_{*}^{2}(M)\\|\\boldsymbol{\\Sigma}_{\\epsilon}^{(\\mathrm{i})}\\|_{\\mathrm{op}}^{1/2}\\kappa_{*}(A)\\sqrt{\\frac{s\\left(r_{e}(\\Phi^{\\prime\\top}\\boldsymbol{\\Sigma}_{\\epsilon}^{(\\mathrm{i})}\\Phi^{\\prime})+r_{e}(A^{2})\\right)\\log^{2}(n+d+p)}{n}}}\\\\ &{\\qquad\\qquad+\\,C_{\\mathrm{g}}\\kappa_{*}^{2}(M)\\|D\\boldsymbol{\\Sigma}_{x}^{(\\mathrm{i})}D^{\\top}\\|_{\\mathrm{op}}^{1/2}\\sqrt{\\frac{s\\left(\\kappa_{*}^{2}(A)r_{e}(\\Phi^{\\prime\\top}D\\boldsymbol{\\Sigma}_{x}^{(\\mathrm{i})}D^{\\top}\\Phi^{\\prime})+\\kappa_{*}^{6}(A)r_{e}(A^{2})\\right)\\log^{2}(n+d+p)}{n}}\\|\\mathrm{c.c.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "hold with high probability. ", "page_idx": 22}, {"type": "text", "text": "Bound $T_{\\mathrm{bias}}^{\\mathrm{LoRA}}$ . Note that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(T_{\\mathrm{bias}}^{\\mathrm{LORA}})^{2}=\\Vert\\mathrm{SVD}_{s}(M)A^{\\dagger}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}\\Sigma_{x}^{(1)1/2}-D\\Sigma_{x}^{(1)1/2}\\Vert_{\\mathrm{F}}^{2}}\\\\ &{\\qquad\\qquad=\\Vert\\underbrace{\\mathrm{SVD}_{s}(M)A^{\\dagger}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}\\Sigma_{x}^{(1)1/2}-\\Phi^{\\prime}\\Phi^{\\prime\\prime}D\\Sigma_{x}^{(1)}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}(A^{2})^{\\dagger}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}\\Sigma_{x}^{(1)1/2}}_{=:T_{1}}\\Vert_{\\mathrm{F}}^{2}}\\\\ &{\\qquad\\qquad+\\Vert\\underbrace{D\\Sigma_{x}^{(1)1/2}(I-\\Sigma_{x}^{(1)1/2}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}(A^{2})^{\\dagger}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}\\Sigma_{x}^{(1)1/2})}_{=:T_{2}}\\Vert_{\\mathrm{F}}^{2}}\\\\ &{\\qquad\\qquad+\\Vert\\underbrace{(I-\\Phi^{\\prime}\\Phi^{\\prime\\top})D\\Sigma_{x}^{(1)}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}(A^{2})^{\\dagger}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}\\Sigma_{x}^{(1)1/2}}_{=:T_{3}}\\Vert_{\\mathrm{F}}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the second equality follows from the fact that cross terms are zero, i.e., $\\begin{array}{l l l l l}{\\mathrm{tr}\\big(T_{1}T_{2}^{\\top}\\big)}&{=}&{\\mathrm{tr}\\big(T_{2}T_{3}^{\\top}\\big)}&{=}&{\\mathrm{tr}\\big(T_{3}T_{1}^{\\top}\\big)}&{=}&{0}\\end{array}$ since $\\begin{array}{r l}{\\Psi_{*}(\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}\\Sigma_{x}^{(\\mathrm{i})1/2})\\Psi_{*}^{\\top}(\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}\\Sigma_{x}^{(\\mathrm{i})1/2})}&{=}\\end{array}$ $\\Sigma_{x}^{\\mathrm{(i)1/2}}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}\\top}(A^{2})^{\\dagger}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}\\Sigma_{x}^{\\mathrm{(i)1/2}}$ and ", "page_idx": 22}, {"type": "text", "text": "$(I-\\Phi^{\\prime}\\Phi^{\\prime\\intercal})\\Phi_{*}(\\mathrm{SVD}_{s}(M))=0,\\,\\,\\underbrace{W_{\\ell-1}^{\\mathrm{pre}}\\Sigma_{x}^{(\\mathrm{i})1/2}(I-\\Psi_{*}(\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}\\Sigma_{x}^{(\\mathrm{i})1/2})\\Psi_{*}^{\\intercal}(\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}\\Sigma_{x}^{(\\mathrm{i})1/2}))}_{\\ell}=0$ hold. Thus from Lemma E.17, ", "page_idx": 22}, {"type": "text", "text": "$\\begin{array}{r}{(T_{\\mathrm{bias}}^{\\mathrm{LoRA}})^{2}=\\|\\mathrm{SVD}_{s}(\\Phi^{\\prime}\\Phi^{\\prime\\top}D\\Sigma_{x}^{(\\mathrm{i})}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}\\top}A^{\\dagger})-\\Phi^{\\prime}\\Phi^{\\prime\\top}D\\Sigma_{x}^{(\\mathrm{i})}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}\\top}A^{\\dagger}\\|_{\\mathrm{F}}^{2}+\\mathcal{E}^{(\\mathrm{i})}(f_{\\ell}^{\\mathrm{full}}).}\\end{array}$ (13) Notice that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla\\nabla\\mathsf{D}_{s}(\\Phi^{\\prime}\\Phi^{\\prime\\top}D\\Sigma_{x}^{(\\mathrm{i})}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}^{\\top}}A^{\\dagger})-\\Phi^{\\prime}\\Phi^{\\prime\\top}D\\Sigma_{x}^{(\\mathrm{i})}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}^{\\top}}A^{\\dagger}\\|_{\\mathrm{F}}^{2}}\\\\ &{\\quad\\le\\{0\\vee\\big(\\mathrm{rank}(\\Phi^{\\prime}\\Phi^{\\prime\\top}D\\Sigma_{x}^{(\\mathrm{i})}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}^{\\top}}A^{\\dagger})-s\\}\\}\\|\\Phi^{\\prime}\\Phi^{\\prime\\top}D\\Sigma_{x}^{(\\mathrm{i})}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}^{\\top}}A^{\\dagger}\\|_{\\mathrm{op}}^{2}}\\\\ &{\\quad\\le\\{0\\vee\\big(\\mathrm{rank}(\\Phi^{\\prime}\\Phi^{\\prime\\top}D\\Sigma_{x}^{(\\mathrm{i})}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}^{\\top}}A^{\\dagger})-s\\}\\}\\|D\\Sigma_{x}^{(\\mathrm{i})1/2}\\|_{\\mathrm{op}}^{2}}\\\\ &{\\quad\\le\\frac{0\\vee\\big(\\mathrm{rank}(D\\Sigma_{x}^{(\\mathrm{i})}D^{\\top})-s\\big)}{\\mathrm{rank}(D\\Sigma_{x}^{(\\mathrm{i})1/2})}\\kappa_{*}^{2}(D\\Sigma_{x}^{(\\mathrm{i})}D^{\\top})\\mathcal{E}^{(\\mathrm{i})}(f^{\\mathrm{pre}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the last inequality follows since ", "page_idx": 22}, {"type": "equation", "text": "$$\nD\\Sigma_{x}^{(\\mathrm{i})1/2}\\|_{\\mathrm{F}}^{2}=\\|\\Lambda_{*}(D\\Sigma_{x}^{(\\mathrm{i})1/2})\\|_{\\mathrm{F}}^{2}\\geq\\operatorname{rank}(D\\Sigma_{x}^{(\\mathrm{i})1/2})\\lambda_{*}^{2}(D\\Sigma_{x}^{(\\mathrm{i})1/2})=\\frac{\\operatorname{rank}(D\\Sigma_{x}^{(\\mathrm{i})1/2})}{\\kappa_{*}^{2}(D\\Sigma_{x}^{(\\mathrm{i})1/2})}\\|D\\Sigma_{x}^{(\\mathrm{i})1/2}\\|_{\\mathrm{op}}^{2}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Summary Note that for any $\\eta>0$ $\\cdot\\ 0,(T_{\\mathrm{variance}}^{\\mathrm{LoRA}}+T_{\\mathrm{bias}}^{\\mathrm{LoRA}})^{2}\\leq(1+\\eta)(T_{\\mathrm{bias}}^{\\mathrm{LoRA}})^{2}+(1+1/\\eta)(T_{\\mathrm{variance}}^{\\mathrm{LoRA}})^{2}$ holds. Therefore, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathcal{E}^{(\\mathrm{i})}(f_{\\ell,U^{\\mathrm{LoRA}},V^{\\mathrm{LoRA}}})\\leq(1+\\eta)(T_{\\mathrm{bias}}^{\\mathrm{LoRA}})^{2}+(1+\\eta^{-1})(T_{\\mathrm{variance}}^{\\mathrm{LoRA}})^{2}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Combined with (12), (13), and (14), this concludes the proof. ", "page_idx": 23}, {"type": "text", "text": "E.5.3 Out-of-distribution Excess Risk of LoRA ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We define the low-rank matrix obtained by LoRA under population in-distribution risk as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(U_{\\infty}^{\\mathrm{LoRA}},V_{\\infty}^{\\mathrm{LoRA}})\\in\\underset{U,V}{\\arg\\operatorname*{min}}\\,\\|(U,V)\\|_{\\mathrm{F}}^{2}\\ \\ \\mathrm{s.t.}\\ (U,V)\\operatorname*{minimizes}\\mathcal{R}^{(\\mathrm{i})}(f_{\\ell,U,V}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Theorem E.11 (Restatement of Theorem E.8: LoRA Part). For $(U_{\\infty}^{\\mathrm{LoRA}},V_{\\infty}^{\\mathrm{LoRA}})$ , defined in (15) ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{E}^{(\\circ)}(f_{\\ell,U_{\\infty}^{\\mathrm{LoRA}},V_{\\infty}^{\\mathrm{LoRA}}})\\lesssim\\|(I-\\Phi^{\\prime}\\Phi^{\\prime\\top})B^{(\\circ)}\\Sigma_{x}^{(\\circ)1/2}\\|_{\\mathsf F}^{2}+\\|(B^{(\\circ)}-B^{(\\mathsf{i})})\\Sigma_{x}^{(\\mathsf{i})1/2}\\|_{\\mathsf F}^{2}\\|G_{\\ell-1}^{(\\mathsf{i},\\mathsf{o})}\\|_{\\mathsf{o p}}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\|(B^{(\\circ)}-W^{\\mathrm{pre}})(\\Sigma_{x}^{(\\mathsf{o})1/2}-\\Sigma_{x}^{(\\mathsf{i})1/2}G_{\\ell-1}^{(\\mathsf{i},\\mathsf{o})})\\|_{\\mathsf F}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\;\\frac{0\\vee\\,\\big(\\mathrm{rank}(D\\Sigma_{x}^{(\\mathsf{i})}D^{\\top})\\,-\\,s\\big)}{\\mathrm{rank}(D\\Sigma_{x}^{(\\mathsf{i})}D^{\\top})}\\kappa_{*}^{2}(D\\Sigma_{x}^{(\\mathsf{i})}D^{\\top})\\|G_{\\ell-1}^{(\\mathsf{i},\\mathsf{o})}\\|_{\\mathsf{o p}}^{2}\\mathcal{E}^{(\\mathsf{i})}(f^{\\mathsf{p r e}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Furthermore, for any $\\eta\\in(0,1)$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Xi^{(\\mathrm{o})}(f_{\\ell,U_{\\infty}^{\\mathrm{lost},V_{\\infty}^{\\mathrm{lost},\\lambda}}})\\geq(1-\\eta)\\Big|\\Big|(B^{(\\mathrm{o})}-B^{(\\mathrm{i})})\\Sigma_{x}^{(\\mathrm{o})1/2}\\Big|\\Big|_{\\mathrm{F}}^{2}-3(\\eta^{-1}-1)\\|(I-\\Phi^{\\prime}\\Phi^{\\prime\\top})B^{(\\mathrm{i})}\\Sigma_{x}^{(\\mathrm{o})1/2}\\|_{\\mathrm{F}}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad-\\,3(\\eta^{-1}-1)\\|(B^{(\\mathrm{i})}-W^{\\mathrm{pre}})(\\Sigma_{x}^{(\\mathrm{o})1/2}-\\Sigma_{x}^{(\\mathrm{i})1/2}G_{\\ell-1}^{(\\mathrm{i},\\mathrm{o})})\\|_{\\mathrm{F}}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad-\\,3(\\eta^{-1}-1)\\frac{0\\vee\\left(\\Gamma\\mathrm{ank}(D\\Sigma_{x}^{(\\mathrm{i})}D^{\\top})-\\,s\\right)}{\\mathrm{rank}(D\\Sigma_{x}^{(\\mathrm{i})}D)}\\kappa_{*}^{2}(D\\Sigma_{x}^{(\\mathrm{i})}D^{\\top})\\|G_{\\ell-1}^{(\\mathrm{i},\\mathrm{o})}\\|_{\\infty}\\mathcal{E}^{(\\mathrm{i})}(f^{\\mathrm{pre}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof of Theorem E.11. With a slight modification to the proof of Lemma E.9, it follows that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Xi^{(\\mathrm{o})}\\big(f_{\\ell,U_{\\infty}^{\\mathrm{LoRA}},V_{\\infty}^{\\mathrm{LoRA}}}\\big)=\\mathrm{tr}\\bigg(\\bigg(B^{(\\mathrm{o})}-W^{\\mathrm{pre}}-5\\nabla\\mathsf{D}_{s}\\big(\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}}(\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}})^{\\dagger}D\\Sigma_{x}^{(\\mathrm{i})}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}\\top}A^{\\dagger}\\big)A^{\\dagger}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}\\bigg)\\Sigma_{x}^{(\\mathrm{o})}}\\\\ &{\\qquad\\qquad\\qquad\\cdot\\bigg(B^{(\\mathrm{o})}-W^{\\mathrm{pre}}-5\\nabla\\mathsf{D}_{s}\\big(\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}}(\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}})^{\\dagger}D\\Sigma_{x}^{(\\mathrm{i})}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}\\top}A^{\\dagger}\\big)A^{\\dagger}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}\\bigg)^{\\top}\\bigg)}\\\\ &{\\qquad\\qquad=\\bigg\\|(B^{(\\mathrm{o})}-W^{\\mathrm{pre}})\\Sigma_{x}^{(\\mathrm{o})1/2}-5\\nabla\\mathsf{D}_{s}\\big(\\Phi^{\\prime}\\Phi^{\\prime\\top}D\\Sigma_{x}^{(\\mathrm{i})}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}\\top}A^{\\dagger}\\big)A^{\\dagger}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}\\Sigma_{x}^{(\\mathrm{o})1/2}\\bigg\\|_{\\mathrm{F}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Recall that $M:=\\Phi^{\\prime}\\Phi^{\\prime\\top}D\\Sigma_{x}^{\\mathrm{(i)}}{\\underline{{W}}}_{\\ell-1}^{\\mathrm{pre}\\top}A^{\\dagger}$ . Then, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|(B^{(\\theta)}-W^{\\mathrm{pr}})\\Sigma_{\\varepsilon}^{(\\theta)1/2}-S\\nabla_{\\Phi}(\\Phi^{\\varepsilon})^{\\prime}(T^{3}\\Sigma_{\\varepsilon}^{(1)}W_{\\varepsilon-1}^{\\mathrm{pr}}-A^{\\dagger})A^{\\dagger}W_{\\varepsilon-1}^{\\mathrm{pr}}\\Sigma_{\\varepsilon}^{(1)/2}\\right\\|_{F}}\\\\ &{\\quad\\lesssim\\left\\|(B^{(\\theta)}-W^{\\mathrm{pr}})\\Sigma_{\\varepsilon}^{(\\theta)1/2}-\\Phi^{\\dagger}\\Phi^{\\dagger}T^{2}D\\Sigma_{\\varepsilon}^{(1)}W_{\\varepsilon-1}^{\\mathrm{pr}}\\right\\|_{F}^{3}w_{\\varepsilon-1}^{(1)/2}\\Big\\|_{F}}\\\\ &{\\quad\\quad+\\left\\|A L^{\\mathrm{At}}W_{\\varepsilon-1}^{\\mathrm{pr}}\\Sigma_{\\varepsilon}^{(1)/2}-S\\nabla_{\\Phi}(A)A^{\\dagger}W_{\\varepsilon-1}^{\\mathrm{pr}}\\Sigma_{\\varepsilon}^{(1)/2}\\right\\|_{F}}\\\\ &{\\quad=\\left\\|(B^{(\\theta)}-W^{\\mathrm{pr}})\\Sigma_{\\varepsilon}^{(\\theta)1/2}-\\Phi^{\\dagger}T^{3}T^{2}D\\Sigma_{\\varepsilon}^{(1)/2}(W_{\\varepsilon-1}^{\\mathrm{pr}}\\Sigma_{\\varepsilon}^{(1)/2})\\mathbb{I}_{F-\\Sigma_{\\varepsilon}^{(1)/2}}^{\\mathrm{pr}}\\Sigma_{\\varepsilon-1}^{(\\theta)1/2}\\right\\|_{F}}\\\\ &{\\quad\\quad+\\left\\|M A^{\\dagger}W_{\\varepsilon-1}^{\\mathrm{pr}}\\Sigma_{\\varepsilon}^{(1)/2}-S\\nabla_{\\Phi}(A)A\\right\\|_{F}^{3}W_{\\varepsilon-1}^{\\mathrm{pr}}\\Sigma_{\\varepsilon}^{(1)/2}\\Big\\|_{F}}\\\\ &{\\quad\\leq\\|(I-\\Phi^{\\dagger}\\Phi^{\\dagger})^{\\mathrm{B}}\\Sigma_{\\varepsilon}^{(1)/2}\\|_{F}+\\|\\Phi^{\\dagger}\\Phi^{\\top}(T^{3}(B^{(\\theta)}-B^{\\dagger})\\Sigma_{\\varepsilon}^{(1)/2}\\mathcal{Q}_{\\varepsilon-1}^{(1)/2})\\|_{F}}\\\\ &{\\quad\\quad+\\left\\|\\Phi^{\\dagger}\\Phi^{\\dagger}(T^{3} \n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where we used $\\Phi^{\\prime}\\Phi^{\\prime\\top}W^{\\mathrm{pre}}=W^{\\mathrm{pre}}$ . From (14), we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\{\\mathcal{E}^{(\\circ)}(f_{\\ell,U_{\\infty}^{\\mathrm{LBA}},V_{\\infty}^{\\mathrm{LBA}}})\\}^{1/2}\\leq\\|(I-\\Phi^{\\prime}\\Phi^{\\prime\\top})B^{(\\circ)}\\Sigma_{x}^{(0)1/2}\\|_{\\mathrm{F}}+\\|(B^{(\\circ)}-B^{(\\mathrm{i})})\\Sigma_{x}^{(\\mathrm{i})1/2}\\|_{\\mathrm{F}}\\|G_{\\ell-1}^{(\\mathrm{i},0)}\\|_{\\mathrm{op}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\|(B^{(\\circ)}-W^{\\mathrm{pre}})(\\Sigma_{x}^{(0)1/2}-\\Sigma_{x}^{(\\mathrm{i})1/2}G_{\\ell-1}^{(\\mathrm{i},0)})\\|_{\\mathrm{F}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\;\\|G_{\\ell-1}^{(\\mathrm{i},0)}\\|_{\\mathrm{op}^{K_{*}}}(D\\Sigma_{x}^{(\\mathrm{i})}D^{\\top})\\sqrt{\\frac{0\\vee\\left(\\ r\\mathrm{ank}(D\\Sigma_{x}^{(\\mathrm{i})}D^{\\top})-s\\right)}{\\ r\\mathrm{ank}\\left(D\\Sigma_{x}^{(\\mathrm{i})1/2}\\right)}\\mathcal{E}^{(\\mathrm{i})}(f^{\\mathrm{pre}})},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where we used $\\begin{array}{r}{\\|A^{\\dagger}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}\\Sigma_{x}^{(0)1/2}\\|_{\\mathrm{op}}=\\|G_{\\ell-1}^{(\\mathrm{i},0)}\\|_{\\mathrm{op}}}\\end{array}$ . This gives the first claim. ", "page_idx": 24}, {"type": "text", "text": "Using $2\\,\\mathrm{tr}\\big(A B^{\\top}\\big)\\geq-\\eta\\|A\\|_{\\mathrm{F}}^{2}-(1/\\eta)\\|B\\|_{\\mathrm{F}}^{2}$ for any $\\eta>0$ and any matrices $A,B$ of the same shape, (17) can be rewritten as ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathcal{E}^{(\\theta)}(f_{\\ell,U_{\\infty}^{\\mathrm{sub}},V_{\\infty}^{\\mathrm{teak}}})=\\Big|\\Big|(B^{(\\theta)}-B^{(0)})\\Sigma_{x}^{(0)1/2}+\\underbrace{(I-\\Phi^{\\prime}\\Phi^{\\prime\\prime})(B^{(0)}-W^{\\mathrm{pe}})\\Sigma_{x}^{(0)1/2}}_{=\\mathcal{T}_{x_{1}}}}\\\\ &{\\qquad+\\underbrace{\\Phi^{\\prime}\\Phi^{\\prime\\prime}(B^{(0)}-W^{\\mathrm{pe}})(\\Sigma_{x}^{(0)1/2}-\\Sigma_{x}^{(0)1/2}\\mathcal{Q}_{\\ell-1}^{(0)})}_{=\\mathcal{T}_{x_{2}}}}\\\\ &{\\qquad+\\underbrace{M^{\\mathrm{t}}\\mathbb{W}_{\\ell-1}^{\\mathrm{pe}}\\Sigma_{x}^{(0)1/2}-8\\mathrm{Vo}_{s}(M)A^{\\dagger}\\mathbb{W}_{\\ell-1}^{\\mathrm{pe}}\\Sigma_{x}^{(0)1/2}}_{=\\mathcal{T}_{x_{3}}}\\Big|_{\\ell}^{2}\\Big|_{\\ell}^{2}}\\\\ &{=\\Big\\lVert(B^{(0)}-B^{(0)})\\Sigma_{x}^{(0)1/2}\\Big\\rVert_{\\mathrm{F}}^{2}+2\\operatorname{tr}\\Big((B^{(0)}-B^{(0)})\\Sigma_{x}^{(0)1/2}(T_{1}+T_{2}+T_{3})^{\\top}\\Big)}\\\\ &{\\quad+\\left\\lVert T_{1}+T_{2}+T_{3}\\right\\rVert_{\\mathrm{F}}^{2}}\\\\ &{\\geq(1-\\eta)\\Big\\lVert(B^{(0)}-B^{(0)})\\Sigma_{x}^{(0)1/2}\\Big\\rVert_{\\mathrm{F}}^{2}+(1-\\eta^{-1})\\lVert T_{1}+T_{2}+T_{3}\\rVert_{\\mathrm{F}}^{2},\\qquad(1-\\eta)}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Choose $\\eta\\in(0,1)$ . By a similar argument as above, and using $\\Phi^{\\prime}\\Phi^{\\prime\\top}W^{\\mathrm{pre}}=W^{\\mathrm{pre}}$ , we can show that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|T_{1}+T_{2}+T_{3}\\|_{\\mathrm{F}}^{2}\\leq3\\|T_{1}\\|_{\\mathrm{F}}^{2}+3\\|T_{2}\\|_{\\mathrm{F}}^{2}+3\\|T_{3}\\|_{\\mathrm{F}}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq3\\|(I-\\Phi^{\\prime}\\Phi^{\\prime\\top})B^{(\\mathrm{i})}\\Sigma_{x}^{(\\mathrm{o})1/2}\\|_{\\mathrm{F}}^{2}+3\\|(B^{(\\mathrm{i})}-W^{\\mathrm{pre}})(\\Sigma_{x}^{(\\mathrm{o})1/2}-\\Sigma_{x}^{(\\mathrm{i})1/2}G_{\\ell-1}^{(\\mathrm{i},\\mathrm{o})})\\|_{\\mathrm{F}}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\,3\\frac{0\\,\\lor\\,\\left(\\mathrm{rank}\\left(D\\Sigma_{x}^{(\\mathrm{i})}D^{\\top}\\right)-\\,s\\right)}{\\mathrm{rank}\\left(D\\Sigma_{x}^{(\\mathrm{i})1/2}\\right)}\\kappa_{*}^{2}(D\\Sigma_{x}^{(\\mathrm{i})}D^{\\top})\\|G_{\\ell-1}^{(\\mathrm{i},\\mathrm{o})}\\|_{\\mathrm{op}}\\mathcal{E}^{(\\mathrm{i})}(f^{\\mathrm{pe}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where we used (14) again. This concludes the proof. ", "page_idx": 24}, {"type": "text", "text": "E.6 Proofs for Structured Sparse Fine-tuning ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "E.6.1 Excess Risk of Structured Sparse Fine-tuning ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Lemma E.12 (Excess Risk). Given $S\\subset[d_{\\ell}],$ , consider the minimum norm solution ", "page_idx": 24}, {"type": "equation", "text": "$$\nV^{\\mathrm{S^{2}F T}}\\in\\underset{V\\in\\mathbb{R}^{d_{\\ell-1}\\times s}}{\\arg\\operatorname*{min}}\\|V\\|_{\\mathrm{F}}^{2}\\ \\ s.t.\\ V\\,m i n i m i z e s\\,\\mathcal{R}_{n}^{\\mathrm{(i)}}(f_{\\ell,U_{S}^{\\mathrm{S^{2}F T}},V}).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Then, the low-rank adaptation matrix satisfies ", "page_idx": 24}, {"type": "equation", "text": "$$\nU_{S}^{\\mathrm{S^{2}F T}}V^{\\mathrm{S^{2}F T\\top}}=U_{S}^{\\mathrm{S^{2}F T}}(\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}}U_{S}^{\\mathrm{S^{2}F T}})^{\\dagger}\\hat{D}\\hat{\\Sigma}_{x}^{(\\mathrm{i})}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}\\top}(\\hat{A}^{\\dagger})^{2},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\mathbf{\\Xi}}^{(k)}(f_{\\ell,U_{S}^{\\mathrm{s2r}},V^{\\mathrm{s2r}}\\Gamma})=\\mathrm{tr}\\Bigg(\\Big(B^{(k)}-W^{\\mathrm{pre}}-\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}}U_{S}^{\\mathrm{S^{2}F T}}(\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}}U_{S}^{\\mathrm{S^{2}F T}})^{\\dagger}\\hat{D}\\hat{\\Sigma}_{x}^{(i)}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}^{\\top}}(\\hat{A}^{\\dagger})^{2}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}\\Big)\\Sigma_{x}^{(k)}}\\\\ &{\\qquad\\qquad\\qquad\\quad\\cdot\\left(B^{(k)}-W^{\\mathrm{pre}}-\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}}U_{S}^{\\mathrm{S^{2}F T}}(\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}}U_{S}^{\\mathrm{S^{2}F T}})^{\\dagger}\\hat{D}\\hat{\\Sigma}_{x}^{(i)}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}^{\\top}}(\\hat{A}^{\\dagger})^{2}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}\\right)^{\\top}\\Bigg)}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "for $k\\in\\{\\mathrm{i},\\mathrm{o}\\}$ . ", "page_idx": 24}, {"type": "text", "text": "Proof. Since $\\begin{array}{l c l}{\\hat{\\Sigma}_{x,\\epsilon}^{(k)}}&{=}&{(1/n)X^{(k)}E^{(k)\\top}}\\end{array}$ and $\\begin{array}{r c l}{\\hat{\\Sigma}_{x}^{(k)}}&{=}&{(1/n)X^{(k)}X^{(k)\\top}}\\end{array}$ , we have $\\begin{array}{r l}{\\hat{\\Sigma}_{x,\\epsilon}^{\\left(k\\right)}}&{{}=}\\end{array}$ $\\hat{\\Sigma}_{x}^{(k)}(X^{(k)\\top})^{\\dagger}E^{(k)\\top}=:\\hat{\\Sigma}_{x}^{(k)}\\check{\\Sigma}_{x,\\epsilon}^{(k)}$ . Similar to (9), we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathscr{R}_{n}^{(\\mathrm{i})}(f_{\\ell,U_{S}^{\\mathrm{s^{2}F T}},V})=\\|\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}}U_{S}^{\\mathrm{S^{2}F T}}V^{\\top}\\hat{A}-\\hat{D}\\hat{\\Sigma}_{x}^{(\\mathrm{i})}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre^{\\top}}}\\hat{A}^{\\dagger}\\|_{\\mathrm{F}}^{2}-\\|\\hat{D}\\hat{\\Sigma}_{x}^{(\\mathrm{i})}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre^{\\top}}}\\hat{A}^{\\dagger}\\|_{\\mathrm{F}}^{2}}\\\\ &{\\qquad\\qquad\\qquad+\\operatorname{tr}\\Big(D\\hat{\\Sigma}_{x}^{(\\mathrm{i})}D^{\\top}\\Big)+2\\operatorname{tr}\\Big(D\\hat{\\Sigma}_{x,\\epsilon}^{(\\mathrm{i})}\\Big)+\\operatorname{tr}\\Big(\\hat{\\Sigma}_{\\epsilon}^{(\\mathrm{i})}\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Thus minimizing $\\mathcal{R}_{n}^{(\\mathrm{i})}(f_{\\ell,U_{S}^{\\mathrm{S}^{2}\\mathrm{FT}},V})$ is equivalent to minimizing the norm ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}}U_{S}^{\\mathrm{S^{2}F T}}V^{\\top}\\hat{A}-\\hat{D}\\hat{\\Sigma}_{x}^{(\\mathrm{i})}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}\\top}\\hat{A}^{\\dagger}\\|_{\\mathrm{F}}^{2}}\\\\ &{\\quad=\\|\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}}U_{S}^{\\mathrm{S^{2}F T}}V^{\\top}\\hat{A}-\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}}U_{S}^{\\mathrm{S^{2}F T}}(\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}}U_{S}^{\\mathrm{S^{2}F T}})^{\\dagger}\\hat{D}\\hat{\\Sigma}_{x}^{(\\mathrm{i})}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}\\top}\\hat{A}^{\\dagger}\\|_{\\mathrm{F}}^{2}}\\\\ &{\\quad\\quad+\\,\\|(I-(\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}}U_{S}^{\\mathrm{S^{2}F T}})(\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}}U_{S}^{\\mathrm{S^{2}F T}})^{\\dagger})\\hat{D}\\hat{\\Sigma}_{x}^{(\\mathrm{i})}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}\\top}\\hat{A}^{\\dagger}\\|_{\\mathrm{F}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "By the same argument as in the proof of Lemma E.9, the minimum norm solution $V^{\\mathrm{{S}^{2}\\mathrm{{FT}}}}$ is obtained by ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{V^{\\mathrm{{S}^{2}}\\mathrm{{FT}}}=(\\hat{A}^{\\dagger})^{2}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}\\hat{\\Sigma}_{x}^{(\\mathrm{i})}\\hat{D}^{\\top}(U_{S}^{\\mathrm{{S}^{2}}\\mathrm{{FT}}\\top}\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}\\top})^{\\dagger}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The excess risk for $k\\in\\{\\mathrm{i},\\mathrm{o}\\}$ becomes ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{^{\\because(k)}(f_{\\ell,U_{S}^{\\mathrm{S^{2}F r}},V^{\\mathrm{S^{2}F r}}})=\\mathbb{E}\\bigg[\\bigg(B^{(k)}x^{(k)}-\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}}(W_{\\ell}^{\\mathrm{pre}}+U_{S}^{\\mathrm{S^{2}F T}}V^{\\mathrm{S^{2}F T}\\top})\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}x^{(k)}\\bigg)^{2}\\bigg]}\\\\ &{\\qquad\\qquad\\qquad=\\operatorname{tr}\\bigg(\\bigg(B^{(k)}-W^{\\mathrm{pre}}-\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}}U_{S}^{\\mathrm{S^{2}F T}}(\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}}U_{S}^{\\mathrm{S^{2}F T}})^{\\dagger}\\hat{D}\\hat{\\Sigma}_{x}^{(i)}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}\\top}(\\hat{A}^{\\dagger})^{2}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}\\bigg)\\Sigma_{x}^{(k)}}\\\\ &{\\qquad\\qquad\\qquad\\cdot\\left(B^{(k)}-W^{\\mathrm{pre}}-\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}}U_{S}^{\\mathrm{S^{2}F T}}(\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}}U_{S}^{\\mathrm{S^{2}F T}})^{\\dagger}\\hat{D}\\hat{\\Sigma}_{x}^{(i)}\\underline{{W}}_{\\ell-1}^{\\mathrm{pe}\\top}(\\hat{A}^{\\dagger})^{2}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}\\right)^{\\top}\\bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "This concludes the proof. ", "page_idx": 25}, {"type": "text", "text": "E.6.2 In-distribution Excess Risk of Structured Sparse Fine-tuning ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Theorem E.13 (Restatement of Theorem E.7: $S^{2}\\mathrm{FT}$ Part). Suppose that Assumptions E.1 and E.2 hold. $F i x\\,S\\subset[d_{\\ell}]\\,w i t h\\,|S|=s$ . Then, the following holds with probability $1-\\exp{\\bigl(-\\Omega(\\log^{2}(n+d+p))\\bigr)}$ . For any $\\eta>0$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathcal{E}^{(\\mathrm{i})}(f_{\\ell,U_{S}^{\\mathrm{S^{2}F T}},V^{\\mathrm{s^{2}F T}}})\\leq(1+\\eta)(T_{\\mathrm{bias}}^{\\mathrm{S^{2}F T}})^{2}+(1+\\eta^{-1})(T_{\\mathrm{variance}}^{\\mathrm{S^{2}F T}})^{2},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $T_{\\mathrm{bias}}^{\\mathrm{S^{2}F T}}\\geq T_{\\mathrm{bias}}^{\\mathrm{LoRA}}$ and ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{(T_{\\mathrm{bias}}^{\\mathrm{S}^{2}\\mathrm{FT}})^{2}\\leq\\|(\\Phi^{\\prime}\\Phi^{\\prime\\top}-\\Phi_{S}^{\\prime\\prime}\\Phi_{S}^{\\prime\\prime}^{\\prime\\top})\\Phi_{*}(D\\Sigma_{x}^{(i)1/2})\\|_{\\mathrm{op}}^{2}\\mathcal{E}^{(i)}(f_{\\ell}^{\\mathrm{pr}})+\\mathcal{E}^{(i)}(f_{\\ell}^{\\mathrm{ful}}),}&{\\quad{\\scriptstyle(21250\\Omega^{\\prime})}}\\\\ &{}&{(T_{\\mathrm{variance}}^{\\mathrm{S}^{2}\\mathrm{FT}})^{2}\\lesssim\\|\\Sigma_{\\epsilon}^{(i)}\\|_{\\mathrm{op}}\\kappa_{*}^{2}(A)\\frac{s\\left(r_{e}(\\Phi_{S}^{\\prime\\prime}\\nabla_{e}^{\\top}\\Sigma_{e}^{(i)}\\Phi_{S}^{\\prime\\prime})+r_{e}(A^{2})\\right)\\log^{2}(n+d+p)}{n}}\\\\ &{}&{\\quad\\quad\\quad\\quad\\quad+\\,\\|D\\Sigma_{x}^{(i)}D^{\\top}\\|_{\\mathrm{op}}\\frac{s\\left(\\kappa_{*}^{2}\\left(A\\right)r_{e}\\left(\\Phi_{S}^{\\prime\\prime}D\\Sigma_{x}^{(i)}D^{\\top}\\Phi_{S}^{\\prime\\prime}\\right)+\\kappa_{*}^{8}\\left(A\\right)r_{e}\\left(A^{2}\\right)\\right)\\log^{2}(n+d+p)}{n}.}&\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Note that the term $\\|(\\Phi^{\\prime}\\Phi^{\\prime\\top}-\\Phi_{S}^{\\prime\\prime}\\Phi_{S}^{\\prime\\prime\\top})\\Phi_{*}(D\\Sigma_{x}^{(\\mathrm{i})1/2})\\|_{\\mathrm{op}}$ in (21) measures the distance between subspaces spanned by $\\Phi^{\\prime}$ and $\\Phi_{S}^{\\prime\\prime}$ in a label space, weighted by $\\Phi_{*}(\\Sigma_{f}^{\\mathrm{(i)}})$ . In high level, this quantity shows the closeness between the $\\ell$ -th layer full fine-tuning and $\\mathrm{S^{2}F T}$ . It takes small values when the important \u2018heads\u2019 for residual prediction are sparsely distributed among all heads. This aligns with the intuition that $\\mathrm{S^{2}F T}$ only selectively fine-tunes small number of coordinates, and thus relying on the information contained in those coordinates. ", "page_idx": 25}, {"type": "text", "text": "Proof of Theorem E.13. By the same argument as in the proof of Theorem E.10 combined with Lemma E.12, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathscr{E}^{(\\mathrm{i})}(f_{\\ell,U_{S}^{\\mathrm{s^{2}F T}},V^{\\mathrm{s^{2}F T}}})=\\|(\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}}U_{S}^{\\mathrm{S^{2}F T}}V^{\\mathrm{S^{2}F T\\top}}A A^{\\dagger}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}-D)\\Sigma_{x}^{(\\mathrm{i})1/2}\\|_{\\mathrm{F}}^{2},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|(\\overline{{W}}_{\\ell+1}^{\\mathtt{p r e}}U_{S}^{\\mathtt{S}^{2}\\mathrm{FT}}V^{\\mathrm{S}^{2}\\mathrm{FT}^{\\top}}A A^{\\dagger}\\underline{{W}}_{\\ell-1}^{\\mathtt{p r e}}-D)\\Sigma_{x}^{\\mathrm{(i)1}/2}\\|_{\\mathrm{F}}}\\\\ &{\\quad\\le\\|\\overline{{W}}_{\\ell+1}^{\\mathtt{p r e}}U_{S}^{\\mathrm{S}^{2}\\mathrm{FT}}(\\overline{{W}}_{\\ell+1}^{\\mathtt{p r e}}U_{S}^{\\mathtt{S}^{2}\\mathrm{FT}})^{\\dagger}(\\hat{D}\\hat{\\Sigma}_{x}^{\\mathrm{(i)}}\\underline{{W}}_{\\ell-1}^{\\mathtt{p r e}^{\\top}}(\\hat{A}^{2})^{\\dagger}-D\\Sigma_{x}^{\\mathrm{(i)}}\\underline{{W}}_{\\ell-1}^{\\mathtt{p r e}^{\\top}}(A^{2})^{\\dagger})A\\|_{\\mathrm{F}}}\\\\ &{\\quad\\quad+\\,\\|\\overline{{W}}_{\\ell+1}^{\\mathtt{p r e}}U_{S}^{\\mathrm{S}^{2}\\mathrm{FT}}(\\overline{{W}}_{\\ell+1}^{\\mathtt{p r e}}U_{S}^{\\mathtt{S}^{2}\\mathrm{FT}})^{\\dagger}D\\Sigma_{x}^{\\mathrm{(i)}}\\underline{{W}}_{\\ell-1}^{\\mathtt{p r e}^{\\top}}(A^{2})^{\\dagger}\\underline{{W}}_{\\ell-1}^{\\mathtt{p r e}}\\Sigma_{x}^{\\mathrm{(i)1}/2}-D\\Sigma_{x}^{\\mathrm{(i)1}/2}\\|_{\\mathrm{F}}}\\\\ &{\\quad=:T_{\\mathrm{variance}}^{\\mathrm{S}^{2}\\mathrm{FT}}+T_{\\mathrm{bis}}^{\\mathrm{S}^{2}\\mathrm{FT}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We bound T S2FT and T S2FT separately. ", "page_idx": 26}, {"type": "text", "text": "Bound T S2FT Note that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{variance}=\\|\\overline{{W}}_{\\ell+1}^{\\mathtt{p r e}}U_{S}^{\\mathtt{S}^{2}\\mathrm{FT}}(\\overline{{W}}_{\\ell+1}^{\\mathtt{p r e}}U_{S}^{\\mathtt{S}^{2}\\mathrm{FT}})^{\\sharp}\\hat{D}\\hat{\\Sigma}_{x}^{(\\mathrm{i})}W_{\\ell-1}^{\\mathtt{p e}^{\\top}}(\\hat{A}^{\\dagger})^{2}A-\\overline{{W}}_{\\ell+1}^{\\mathtt{p e}}U_{S}^{\\mathtt{S}^{2}\\mathrm{FT}}(\\overline{{W}}_{\\ell+1}^{\\mathtt{p e}}U_{S}^{\\mathtt{S}^{2}\\mathrm{FT}})^{\\sharp}D\\Sigma_{x}^{(\\mathrm{i})}\\underline{{W}}_{\\ell-1}^{\\mathtt{p r e}}}\\\\ &{\\quad\\quad\\quad\\quad\\leq\\|\\overline{{W}}_{\\ell+1}^{\\mathtt{p e}}U_{S}^{\\mathtt{S}^{2}\\mathrm{FT}}(\\overline{{W}}_{\\ell+1}^{\\mathtt{p e}}U_{S}^{\\mathtt{S}^{2}\\mathrm{FT}})^{\\sharp}D\\Sigma_{x}^{(\\mathrm{i})}\\underline{{W}}_{\\ell-1}^{\\mathtt{p e}^{\\top}}A^{\\dagger}-\\overline{{W}}_{\\ell+1}^{\\mathtt{p e}}U_{S}^{\\mathtt{S}^{2}\\mathrm{FT}}(\\overline{{W}}_{\\ell+1}^{\\mathtt{p e}}U_{S}^{\\mathtt{S}^{2}\\mathrm{FT}})^{\\sharp}\\hat{D}\\hat{\\Sigma}_{x}^{(\\mathrm{i})}\\underline{{W}}_{\\ell-1}^{\\mathtt{p e}^{\\top}}A^{\\dagger}}\\\\ &{\\quad\\quad\\quad\\quad+\\,\\|\\overline{{W}}_{\\ell+1}^{\\mathtt{p e}}U_{S}^{\\mathtt{S}^{2}\\mathrm{FT}}(\\overline{{W}}_{\\ell+1}^{\\mathtt{p e}}U_{S}^{\\mathtt{S}^{2}\\mathrm{FT}})^{\\sharp}\\hat{D}\\hat{\\Sigma}_{x}^{(\\mathrm{i})}\\underline{{W}}_{\\ell-1}^{\\mathtt{p e}^{\\top}}((\\hat{A}^{\\dagger})^{2}-(A^{\\dagger})^{2})A\\|_{\\mathrm{F}}}\\\\ &{\\quad\\quad\\quad=:T_{\\mathrm{variance},1 \n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "For the term T vSa2riFaTnc e,1, using Lemma F.3, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T_{\\mathrm{variance},1}^{\\mathrm{S}^{2}\\mathrm{FI}}\\le2\\sqrt{s}\\Vert\\Phi_{S}^{\\prime\\prime}T D\\Sigma_{x}^{(\\mathrm{i})}\\underline{{W}}_{\\ell-1}^{\\mathrm{peT}}-\\Phi_{S}^{\\prime\\prime}\\hat{D}\\hat{\\Sigma}_{x}^{(\\mathrm{i})}\\underline{{W}}_{\\ell-1}^{\\mathrm{peT}}\\Vert_{\\mathrm{op}}\\Vert A^{\\dagger}\\Vert_{\\mathrm{op}}}\\\\ &{\\qquad\\qquad\\lesssim\\Vert\\Sigma_{\\epsilon}^{(\\mathrm{i})}\\Vert_{\\mathrm{op}}^{1/2}\\kappa_{*}(A)\\sqrt{\\frac{s\\left(r_{e}(\\Phi_{S}^{\\prime\\prime}\\Sigma_{\\epsilon}^{(\\mathrm{i})}\\Phi_{S}^{\\prime\\prime})+r_{e}(A^{2})\\right)\\log^{2}(n+d+p)}{n}}}\\\\ &{\\qquad\\qquad\\qquad+\\left\\Vert D\\Sigma_{x}^{(\\mathrm{i})}D^{\\top}\\Vert_{\\mathrm{op}}^{1/2}\\kappa_{*}(A)\\sqrt{\\frac{s\\left(r_{e}(\\Phi_{S}^{\\prime\\prime}D\\Sigma_{x}^{(\\mathrm{i})}D^{\\top}\\Phi_{S}^{\\prime\\prime})+r_{e}(A^{2})\\right)\\log^{2}(n+d+p)}{n}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "holds on the event $\\mathcal{F}$ , where the first inequality follows since the term inside the norm is at most rank- $2s$ . Again from Lemma F.3, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T_{\\mathrm{variance},2}^{\\mathrm{S}^{2}\\mathrm{Fr}}\\leq\\sqrt{s}\\|\\Phi_{S}^{\\prime\\prime}\\bar{D}\\hat{\\Sigma}_{x}^{(\\mathrm{i})}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}^{\\top}\\top}\\|_{\\mathrm{op}}\\|(\\hat{A}^{\\dagger})^{2}-(A^{\\dagger})^{2}\\|_{\\mathrm{op}}\\|A\\|_{\\mathrm{op}}}\\\\ &{\\qquad\\qquad\\lesssim\\|D\\Sigma_{x}^{(\\mathrm{i})1/2}\\|_{\\mathrm{op}}\\|\\Sigma_{x}^{(\\mathrm{i})1/2}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}^{\\top}}\\|_{\\mathrm{op}}\\frac{\\kappa_{*}^{3}(A)}{\\lambda_{*}(A)}\\sqrt{\\frac{s r_{e}(A^{2})\\log^{2}(n+d+p)}{n}}}\\\\ &{\\qquad=\\|D\\Sigma_{x}^{(\\mathrm{i})1/2}\\|_{\\mathrm{op}}\\kappa_{*}^{4}(A)\\sqrt{\\frac{s r_{e}(A^{2})\\log^{2}(n+d+p)}{n}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "holds on the event $\\mathcal{F}$ . Therefore, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T_{\\mathrm{vatiance}}^{\\mathbb{S}^{2}\\mathrm{FT}}\\lesssim\\|\\Sigma_{\\epsilon}^{(\\mathrm{i})}\\|_{\\mathrm{op}}^{1/2}\\kappa_{*}(A)\\sqrt{\\frac{s\\big(r_{e}(\\Phi_{S}^{\\prime\\prime}\\Sigma_{\\epsilon}^{(\\mathrm{i})}\\Phi_{S}^{\\prime\\prime})+r_{e}(A^{2})\\big)\\log^{2}(n+d+p)}{n}}}\\\\ &{\\qquad\\qquad+\\,\\|D\\Sigma_{x}^{(\\mathrm{i})}D^{\\top}\\|_{\\mathrm{op}}^{1/2}\\sqrt{\\frac{s\\big(\\kappa_{*}^{2}(A)r_{e}\\big(\\Phi_{S}^{\\prime\\prime}D\\Sigma_{x}^{(\\mathrm{i})}D^{\\top}\\Phi_{S}^{\\prime\\prime}\\big)+\\kappa_{*}^{8}(A)r_{e}(A^{2})\\big)\\log^{2}(n+d+p)}{n}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Bound $T_{\\mathrm{bias}}^{\\mathrm{S^{2}F T}}$ . By the same argument as in the proof of Theorem E.10, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(T_{\\mathrm{bias}}^{\\mathrm{S^{\\prime}}})^{2}=\\|\\Phi_{S}^{\\prime\\prime}\\Phi_{S}^{\\prime\\prime^{\\top}}D\\Sigma_{x}^{(\\mathrm{i})}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre^{\\top}}}A^{\\dagger}-\\Phi^{\\prime}\\Phi^{\\prime\\top}D\\Sigma_{x}^{(\\mathrm{i})}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre^{\\top}}}A^{\\dagger}\\|_{\\mathrm{F}}^{2}+\\mathcal{E}^{(\\mathrm{i})}(f_{\\ell}^{\\mathrm{full}})}\\\\ &{\\qquad\\qquad\\leq\\|(\\Phi_{S}^{\\prime\\prime}\\Phi_{S}^{\\prime\\prime}-\\Phi^{\\prime}\\Phi^{\\prime\\top})\\Phi_{*}(D\\Sigma_{x}^{(\\mathrm{i})1/2})\\|_{\\mathrm{op}}^{2}\\|D\\Sigma_{x}^{(\\mathrm{i})}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre^{\\top}}}A^{\\dagger}\\|_{\\mathrm{F}}^{2}+\\mathcal{E}^{(\\mathrm{i})}(f_{\\ell}^{\\mathrm{full}})}\\\\ &{\\qquad\\qquad=\\|(\\Phi_{S}^{\\prime\\prime}\\Phi_{S}^{\\prime\\prime^{\\top}}-\\Phi^{\\prime}\\Phi^{\\prime\\top})\\Phi_{*}(D\\Sigma_{x}^{(\\mathrm{i})1/2})\\|_{\\mathrm{op}}^{2}\\mathcal{E}^{(\\mathrm{i})}(f_{\\ell}^{\\mathrm{pe}})+\\mathcal{E}^{(\\mathrm{i})}(f_{\\ell}^{\\mathrm{full}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where we used $\\|D\\Sigma_{x}^{\\mathrm{(i)}}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}\\top}A^{\\dagger}\\|_{\\mathrm{F}}^{2}\\leq\\|D\\Sigma_{x}^{\\mathrm{(i)1/2}}\\|_{\\mathrm{F}}^{2}=\\mathcal{E}^{\\mathrm{(i)}}(f_{\\ell}^{\\mathrm{pre}})$ . We can verify $T_{\\mathrm{bias}}^{\\mathrm{S^{2}F T}}\\geq T_{\\mathrm{bias}}^{\\mathrm{LoRA}}$ by comparing (13) and (23), since $\\mathrm{SVD}_{s}(\\Phi^{\\prime}\\Phi^{\\prime\\top}D\\Sigma_{x}^{\\mathrm{(i)}}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}\\top}A^{\\dagger})$ is the best rank- $s$ approximation of $\\Phi^{\\prime}\\Phi^{\\prime\\top}D\\Sigma_{x}^{\\mathrm{(i)}}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}\\top}A^{\\dagger}$ and $\\Phi_{S}^{\\prime\\prime}\\Phi_{S}^{\\prime\\prime\\top}D\\Sigma_{x}^{\\mathrm{(i)}}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}\\top}A^{\\dagger}$ is at most rank- $s$ . ", "page_idx": 26}, {"type": "text", "text": "Summary Note that fo $:\\mathrm{any}\\;\\eta>0,\\,(T_{\\mathrm{variance}}^{\\mathrm{S^{2}F T}}+T_{\\mathrm{bias}}^{\\mathrm{S^{2}F T}})^{2}\\le(1+\\eta)(T_{\\mathrm{bias}}^{\\mathrm{S^{2}F T}})^{2}+(1+1/\\eta)(T_{\\mathrm{variance}}^{\\mathrm{S^{2}F T}})^{2}$ holds. Thus ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathcal{E}^{(\\mathrm{i})}(f_{\\ell,U_{S}^{\\mathrm{S^{2}F T}},V^{\\mathrm{s^{2}F T}}})\\leq(1+\\eta)(T_{\\mathrm{bias}}^{\\mathrm{S^{2}F T}})^{2}+(1+\\eta^{-1})(T_{\\mathrm{variance}}^{\\mathrm{S^{2}F T}})^{2}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Combined with (22) and (24), this concludes the proof. ", "page_idx": 27}, {"type": "text", "text": "Next we characterize the bias terms $T_{\\mathrm{bias}}^{\\mathrm{LoRA}}$ and T S2FT under sparsity assumption. ", "page_idx": 27}, {"type": "text", "text": "Lemma E.14. Suppose that Assumption $E.4$ holds. Then, for a sparse fine-tuned network with the choice $S\\supset S_{0}$ , it follows that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}^{\\mathrm{(i)}}(f_{\\ell}^{\\mathrm{full}})\\leq(T_{\\mathrm{bias}}^{\\mathrm{LoRA}})^{2}\\leq(T_{\\mathrm{bias}}^{\\mathrm{S}^{2}\\mathrm{FT}})^{2}\\leq\\mathcal{E}^{\\mathrm{(i)}}(f_{\\ell}^{\\mathrm{full}})+\\delta^{2}\\kappa_{*}^{2}(\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}})\\mathcal{E}^{\\mathrm{(i)}}(f^{\\mathrm{pre}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. Note that $\\Phi_{S}^{\\prime\\prime}\\Phi_{S}^{\\prime\\prime\\top}$ is a projection into a subspace, which is contained in a subspace projected by $\\Phi^{\\prime}\\Phi^{\\prime\\top}$ . Thus ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\Phi_{S}^{\\prime\\prime}\\Phi_{S}^{\\prime\\prime\\top}D\\Sigma_{x}^{(\\mathrm{i})}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}\\top}A^{\\dagger}-\\Phi^{\\prime}\\Phi^{\\prime\\top}D\\Sigma_{x}^{(\\mathrm{i})}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}\\top}A^{\\dagger}\\|_{\\mathrm{F}}^{2}}\\\\ &{\\quad=\\|(\\Phi_{S}^{\\prime\\prime}\\Phi_{S}^{\\prime\\top}-I)\\Phi^{\\prime}\\Phi^{\\prime\\prime}\\Pi\\Sigma_{x}^{(\\mathrm{i})}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}\\top}A^{\\dagger}\\|_{\\mathrm{F}}^{2}}\\\\ &{\\quad=\\|(\\Phi_{S}^{\\prime\\prime}\\Phi_{S}^{\\prime\\top}-I)\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}}(\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}})^{\\dagger}D\\Sigma_{x}^{(\\mathrm{i})}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}\\top}A^{\\dagger}\\|_{\\mathrm{F}}^{2}}\\\\ &{\\quad=\\|(\\Phi_{S}^{\\prime\\prime}\\Phi_{S}^{\\prime\\top}-I)\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}}((I-U_{S}^{\\mathrm{S}^{2}\\mathrm{FT}}U_{S}^{\\mathrm{S}^{2}\\mathrm{FT}\\top})+U_{S}^{\\mathrm{S}^{2}\\mathrm{FT}}U_{S}^{\\mathrm{S}^{2}\\mathrm{FT}\\top})(\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}})^{\\dagger}D\\Sigma_{x}^{(\\mathrm{i})}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}\\top}A^{\\dagger}\\|_{\\mathrm{F}}^{2}}\\\\ &{\\quad=\\|(\\Phi_{S}^{\\prime\\prime}\\Phi_{S}^{\\prime\\top}-I)\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}}(I-U_{S}^{\\mathrm{S}^{2}\\mathrm{FT}}U_{S}^{\\mathrm{S}^{2}\\mathrm{FT}\\top})(\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}})^{\\dagger}D\\Sigma_{x}^{(\\mathrm{i})}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}\\top}A^{\\\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "$\\Phi_{*}(\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}}U_{S}^{\\mathrm{S^{2}F T}})$ .q uTahluitsy follows since $(\\Phi_{S}^{\\prime\\prime}\\Phi_{S}^{\\prime\\prime\\top}\\ -\\ I)\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}}U_{S}^{\\mathrm{S^{2}F T}}\\ =\\ 0$ 1U SS2FT = 0 by definition of \u03a6\u2032S\u2032 = ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\Phi_{S}^{\\prime\\prime}\\Phi_{S}^{\\prime\\prime\\top}D\\Sigma_{x}^{(\\mathrm{i})}\\underline{{W}}_{\\varepsilon\\to1}^{\\mathrm{pre}^{\\top}\\top}A^{\\dagger}-\\Phi^{\\prime}\\Phi^{\\prime\\top}D\\Sigma_{x}^{(\\mathrm{i})}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}^{\\top}}A^{\\dagger}\\|_{\\mathrm{F}}^{2}}\\\\ &{\\quad\\leq\\|\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}}\\|_{\\mathrm{op}}^{2}\\|(I-U_{S}^{\\mathrm{S}^{2}\\mathrm{FT}}U_{S}^{\\mathrm{S}^{2}\\mathrm{FT}\\top})(\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}})^{\\dagger}D\\Sigma_{x}^{(\\mathrm{i})1/2}\\|_{\\mathrm{F}}^{2}\\|\\Sigma_{x}^{(\\mathrm{i})1/2}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}^{\\top}}A^{\\dagger}\\|_{\\mathrm{op}}^{2}}\\\\ &{\\quad=\\|\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}}\\|_{\\mathrm{op}}^{2}\\|\\Sigma_{x}^{(\\mathrm{i})1/2}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}^{\\top}}A^{\\dagger}\\|_{\\mathrm{op}}^{2}\\displaystyle\\sum_{a\\in[d\\ell]\\backslash S}\\|e_{a}^{\\top}(\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}})^{\\dagger}D\\Sigma_{x}^{(\\mathrm{i})1/2}\\|^{2}}\\\\ &{\\quad\\leq\\delta^{2}\\|\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}}\\|_{\\mathrm{op}}^{2}\\|(\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}})^{\\dagger}D\\Sigma_{x}^{(\\mathrm{i})1/2}\\|_{\\mathrm{F}}^{2}}\\\\ &{\\quad\\leq\\delta^{2}\\kappa_{*}^{2}(\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}})\\|D\\Sigma_{x}^{(\\mathrm{i})1/2}\\|_{\\mathrm{F}}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the second inequality follows from $\\|\\Sigma_{x}^{\\mathrm{(i)1/2}}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}\\top}A^{\\dagger}\\|_{\\mathrm{op}}\\leq1$ , Assumption E.4 and $S\\supset S_{0}$ . The conclusion follows from (13) and (23). \u53e3 ", "page_idx": 27}, {"type": "text", "text": "E.6.3 Out-of-distribution Excess Risk of Structured Sparse Fine-tuning ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Given $S\\subset[d_{\\ell}]$ with $|S|=s$ , we define the low-rank adaptation matrix obtained by $S^{2}\\mathrm{FT}$ under population in-distribution risk as ", "page_idx": 27}, {"type": "equation", "text": "$$\nV_{\\infty}^{\\mathrm{S^{2}F T}}=\\underset{V}{\\arg\\operatorname*{min}}\\,\\|V\\|_{\\mathrm{F}}^{2}\\;\\;\\mathrm{s.t.}\\;V\\;\\mathrm{minimizes}\\;\\mathcal{R}^{(\\mathrm{i})}(f_{\\ell,U_{S}^{\\mathrm{S^{2}F T}},V}).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "dTehfeinoerde imn  (E2.51)5, (Restatement of Theorem E.8: $S^{2}\\mathrm{FT}$ Part). F $i x\\;S\\subset[d_{\\ell}]$ with $|S|=s$ . For $V_{\\infty}^{\\mathrm{S^{2}F T}}$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{E}^{(\\mathrm{o})}(f_{\\ell,U_{S}^{\\mathrm{s^{2}F T}},V_{\\mathrm{se}}^{\\mathrm{s^{2}F T}}})\\leq\\mathcal{E}^{(\\mathrm{o})}(f^{\\mathrm{pre}})+3\\big|\\big|\\Phi_{S}^{\\prime\\prime}\\Phi_{S}^{\\prime\\prime\\top}(B^{(\\mathrm{o})}-B^{(\\mathrm{i})})\\Sigma_{x}^{(\\mathrm{o})1/2}\\big|\\big|_{\\mathrm{F}}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\,3\\big|\\big|B^{(\\mathrm{i})}(\\Sigma_{x}^{(\\mathrm{o})1/2}-\\Sigma_{x}^{(\\mathrm{i})1/2}G_{\\ell-1}^{(\\mathrm{i},\\mathrm{o})})\\big|\\big|_{\\mathrm{F}}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\,3\\big|\\overline{{W}}_{\\ell}^{\\mathrm{pre}}\\big|\\big|_{\\mathrm{op}}^{2}\\big|\\big|\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}\\Sigma_{x}^{(\\mathrm{o})1/2}-\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}\\Sigma_{x}^{(\\mathrm{i})1/2}G_{\\ell-1}^{(\\mathrm{i},\\mathrm{o})}\\big|\\big|_{\\mathrm{F}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Remark E.16. If there is no covariate shift, i.e., $\\Sigma_{x}^{\\left(\\mathrm{i}\\right)}\\,=\\,\\Sigma_{x}^{\\left(\\mathrm{o}\\right)}\\,=\\,\\Sigma_{x}$ for some $\\Sigma_{x}$ , Theorem E.15 further gives the bound ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{E}^{(\\circ)}(f_{\\ell,U_{S}^{\\mathrm{S}^{2}\\mathrm{FT}},V_{\\infty}^{\\mathrm{S}^{2}\\mathrm{FT}}})\\leq\\mathcal{E}^{(\\circ)}(f^{\\mathrm{pre}})+3\\big\\|\\Phi_{S}^{\\prime\\prime}\\Phi_{S}^{\\prime\\prime\\top}(B^{(\\circ)}-B^{(\\mathrm{i})})\\Sigma_{x}^{1/2}\\big\\|_{\\mathrm{F}}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\,3\\|B^{(\\mathrm{i})}\\Sigma_{x}^{1/2}(I-(\\underline{{W_{\\ell-1}^{\\mathrm{pre}}}}\\Sigma_{x}^{1/2})^{\\dagger}\\underline{{W_{\\ell-1}^{\\mathrm{pre}}}}\\Sigma_{x}^{1/2}))\\|_{\\mathrm{F}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof of Theorem E.15. With a slight modification to Lemma E.12, we obtain ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\dot{\\tau}^{\\left(\\alpha\\right)}(f_{\\ell,U_{S}^{\\mathrm{S}^{2}\\mathrm{S}^{\\mathrm{I}}\\cap V_{S}^{\\mathrm{S}^{2}\\mathrm{Fr}}}})=\\mathrm{tr}\\bigg(\\bigg(B^{(\\circ)}-W^{\\mathrm{pec}}-\\overline{{W}}_{\\ell+1}^{\\mathrm{pec}}D_{S}^{\\mathrm{S}^{2}\\mathrm{Fr}}(\\overline{{W}}_{\\ell+1}^{\\mathrm{pec}}U_{S}^{\\mathrm{S}^{2}\\mathrm{Fr}})^{\\dagger}D\\Sigma_{x}^{\\mathrm{i}}\\underline{{W}}_{\\ell-1}^{\\mathrm{per}}(A^{\\dagger})^{2}\\underline{{W}}_{\\ell-1}^{\\mathrm{pee}}\\bigg)\\Sigma_{x}^{(\\circ)}}&{}\\\\ {\\qquad\\qquad\\cdot\\left(B^{(\\circ)}-W^{\\mathrm{pec}}-\\overline{{W}}_{\\ell+1}^{\\mathrm{pec}}U_{S}^{\\mathrm{S}^{2}\\mathrm{Fr}}(\\overline{{W}}_{\\ell+1}^{\\mathrm{pec}}U_{S}^{\\mathrm{S}^{2}\\mathrm{Fr}})^{\\dagger}D\\Sigma_{x}^{(\\circ)}\\underline{{W}}_{\\ell-1}^{\\mathrm{per}}(A^{\\dagger})^{2}\\underline{{W}}_{\\ell-1}^{\\mathrm{per}}\\right)^{\\top}\\bigg)}\\\\ {=\\bigg\\lVert(B^{(\\circ)}-W^{\\mathrm{per}})\\Sigma_{x}^{(\\circ)1/2}-\\Phi_{x}^{\\prime}\\Phi_{S}^{\\prime\\prime}\\overline{{P}}_{x}^{(\\circ)}D\\Sigma_{x}^{(\\circ)}\\underline{{W}}_{\\ell-1}^{\\mathrm{per}}(A^{\\dagger})^{2}\\underline{{W}}_{\\ell-1}^{\\mathrm{pec}}\\Sigma_{x}^{(\\circ)\\mathrm{I}/2}\\bigg\\rVert_{\\mathrm{F}}^{2}}&{}\\\\ {=\\lVert(I-\\Phi_{x}^{\\prime\\prime}\\Phi_{S}^{\\prime\\prime})(B^{(\\circ)}-W^{\\mathrm{per}})\\Sigma_{x}^{(\\circ)1/2}\\rVert_{\\mathrm{F}}^{2}}&{}\\\\ {\\qquad+\\left\\lVert\\Phi_{S}^{\\prime\\prime}\\Phi_{S}^{\\prime\\prime}\\right\\{(B^{(\\circ)}-W^{\\mathrm{per}})\\Sigma_ \n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where we used $\\begin{array}{r}{\\Sigma_{x}^{(\\mathrm{i})1/2}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}\\top}(A^{\\dagger})^{2}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}\\Sigma_{x}^{(\\mathrm{o})1/2}=(\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}\\Sigma_{x}^{(\\mathrm{i})1/2})^{\\dagger}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}\\Sigma_{x}^{(\\mathrm{o})1/2}.}\\end{array}$ . Note that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|T\\|_{\\mathrm{F}}\\leq\\big\\|\\Phi_{S}^{\\prime\\prime}\\Phi_{S}^{\\prime\\prime}\\big\\{B^{(\\circ)}\\Sigma_{x}^{(0)1/2}-B^{(\\mathrm{i})}\\Sigma_{x}^{(\\mathrm{i})1/2}(\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}\\Sigma_{x}^{(\\mathrm{i})1/2})^{\\dagger}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}\\Sigma_{x}^{(0)1/2}\\big\\}\\big\\|_{\\mathrm{F}}}\\\\ &{\\qquad+\\big\\|\\Phi_{S}^{\\prime\\prime}\\Phi_{S}^{\\prime\\prime}\\overline{{W}}_{\\ell}^{\\mathrm{pre}}\\Big\\{\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}\\Sigma_{x}^{(0)1/2}-\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}\\Sigma_{x}^{(\\mathrm{i})1/2}(\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}\\Sigma_{x}^{(\\mathrm{i})1/2})^{\\dagger}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}\\Sigma_{x}^{(0)1/2}\\Big\\}\\big\\|_{\\mathrm{F}}}\\\\ &{\\leq\\big\\|\\Phi_{S}^{\\prime\\prime}\\Phi_{S}^{\\prime\\prime}\\big(B^{(\\circ)}-B^{(\\mathrm{i})}\\big)\\Sigma_{x}^{(0)1/2}\\big\\|_{\\mathrm{F}}}\\\\ &{\\qquad+\\big\\|\\Phi_{S}^{\\prime\\prime}\\Phi_{S}^{\\prime\\prime}\\mathbf{\\Phi}^{(\\mathrm{r})}\\big(\\Sigma_{x}^{(0)1/2}-\\Sigma_{x}^{(\\mathrm{i})1/2}G_{\\ell-1}^{(\\mathrm{i},0)}\\big)\\big\\|_{\\mathrm{F}}}\\\\ &{\\qquad+\\big\\|\\Phi_{S}^{\\prime\\prime}\\Phi_{S}^{\\prime\\prime}\\overline{{W}}_{\\ell}^{\\mathrm{pre}}\\big(\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}\\Sigma_{x}^{(0)1/2}-\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}\\Sigma_{x}^{(\\mathrm{i})1/2}G_{\\ell-1}^{(\\mathrm{i},0)}\\big)\\big\\|_{\\mathrm{F}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Therefore, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{E}^{(\\mathfrak{o})}(f_{\\ell,U_{S}^{s^{2}\\mathrm{Fr}},V_{\\infty}^{s^{2}\\mathrm{Fr}}})=\\|(I-\\Phi_{S}^{\\prime\\prime}\\Phi_{S}^{\\prime\\prime\\top})(B^{(\\mathfrak{o})}-W^{\\mathrm{pre}})\\Sigma_{x}^{(\\mathfrak{o})1/2}\\|_{\\mathrm{F}}^{2}+\\|T\\|_{\\mathrm{F}}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\mathcal{E}^{(\\mathfrak{o})}(f^{\\mathrm{pre}})+3\\big\\|\\Phi_{S}^{\\prime\\prime}\\Phi_{S}^{\\prime\\prime}\\big(B^{(\\mathfrak{o})}-B^{(\\mathfrak{i})})\\Sigma_{x}^{(\\mathfrak{o})1/2}\\big\\|_{\\mathrm{F}}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\,3\\|B^{(\\mathfrak{i})}(\\Sigma_{x}^{(\\mathfrak{o})1/2}-\\Sigma_{x}^{(\\mathfrak{i})1/2}G_{\\ell-1}^{(\\mathfrak{i},\\mathfrak{o})})\\|_{\\mathrm{F}}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\,3\\|\\overrightarrow{W_{\\ell}^{\\mathrm{pre}}}\\|_{\\mathrm{op}}^{2}\\|\\underline{{W_{\\ell-1}^{\\mathrm{pre}}}}\\Sigma_{x}^{(\\mathfrak{o})1/2}-\\underline{{W_{\\ell-1}^{\\mathrm{pre}}}}\\Sigma_{x}^{(\\mathfrak{i})1/2}G_{\\ell-1}^{(\\mathfrak{i},\\mathfrak{o})}\\|_{\\mathrm{F}}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where we used $x+y+z\\leq3x^{2}+3y^{2}+3z^{2}$ . This concludes the proof. ", "page_idx": 28}, {"type": "text", "text": "E.7 Proofs for Full Fine-tuning ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Define $f_{\\ell}^{\\mathrm{full}}(x)=\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}}(W_{\\ell}^{\\mathrm{pre}}+\\Delta_{\\ell}^{\\mathrm{full}})\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}x$ as a fine-tuned network with full fine-tuning applied to the $\\ell_{}$ -th layer, evaluated under the population in-distribution risk, where $\\Delta_{\\ell}^{\\mathrm{full}}$ is obtained by ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\Delta_{\\ell}^{\\mathrm{full}}\\in\\operatornamewithlimits{a r g\\,m i n}_{\\Delta^{\\prime}\\in\\mathbb{R}^{d_{\\ell}\\times d_{\\ell-1}}}\\mathbb{E}\\bigg[\\Big(B^{(\\mathrm{i})}x^{(\\mathrm{i})}-\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}}(W_{\\ell}^{\\mathrm{pre}}+\\Delta^{\\prime})\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}x^{(\\mathrm{i})}\\Big)^{2}\\bigg].\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Lemma E.17 (In-distribution Excess Risk). For $f_{\\ell}^{\\mathrm{full}}$ , it holds that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}^{\\mathrm{(i)}}(f_{\\ell}^{\\mathrm{full}})=\\|D\\Sigma_{x}^{\\mathrm{(i)1/2}}(I-\\Sigma_{x}^{\\mathrm{(i)1/2}}\\underline{{W_{\\ell-1}^{\\mathrm{pre}}}}(A^{2})^{\\dagger}\\underline{{W_{\\ell-1}^{\\mathrm{pre}}}}\\Sigma_{x}^{\\mathrm{(i)1/2}})\\|_{\\mathrm{F}}^{2}}\\\\ {+\\;\\|(I-\\Phi^{\\prime}\\Phi^{\\prime\\top})D\\Sigma_{x}^{\\mathrm{(i)}}\\underline{{W_{\\ell-1}^{\\mathrm{pre}}}}(A^{2})^{\\dagger}\\underline{{W_{\\ell-1}^{\\mathrm{pre}}}}\\Sigma_{x}^{\\mathrm{(i)1/2}}\\|_{\\mathrm{F}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof of Lemma $E.I7.$ . Similar to the proof of Theorem E.10, we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{E}^{(\\mathrm{i})}(f_{\\ell}^{\\mathrm{full}})=\\underset{\\Delta\\in\\mathbb{R}^{d_{\\ell}\\times d_{\\ell-1}}}{\\operatorname*{min}}\\mathbb{E}\\bigg[\\Big(B^{(\\mathrm{i})}x^{(\\mathrm{i})}-\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}}(W_{\\ell}^{\\mathrm{pre}}+\\Delta)\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}x^{(\\mathrm{i})}\\Big)^{2}\\bigg]}\\\\ &{\\qquad\\qquad=\\underset{\\Delta\\in\\mathbb{R}^{d_{\\ell}\\times d_{\\ell-1}}}{\\operatorname*{min}}\\|D\\Sigma_{x}^{(\\mathrm{i})1/2}-\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}}\\Delta\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}\\Sigma_{x}^{(\\mathrm{i})1/2}\\|_{\\mathsf{F}}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|D\\Sigma_{x}^{(\\mathrm{i})1/2}-\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}}\\Delta\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}\\Sigma_{x}^{(\\mathrm{i})1/2}\\|_{\\mathrm{F}}^{2}=\\|\\underbrace{\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}}\\Delta\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}\\Sigma_{x}^{(\\mathrm{i})1/2}-\\Phi^{\\prime}\\Phi^{\\prime\\top}D\\Sigma_{x}^{(\\mathrm{i})}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}^{\\top}}A^{\\dagger}}_{=:T_{1}}\\|_{\\mathrm{F}}^{2}\\ (2\\mathrm{`})}\\\\ {+\\|\\underbrace{D\\Sigma_{x}^{(\\mathrm{i})1/2}(I-\\Sigma_{x}^{(\\mathrm{i})1/2}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}^{\\top}}(A^{2})^{\\dagger}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}\\Sigma_{x}^{(\\mathrm{i})1/2})}_{=:T_{2}}\\|_{\\mathrm{F}}^{2}}\\\\ {+\\|\\underbrace{(I-\\Phi^{\\prime}\\Phi^{\\prime\\top})D\\Sigma_{x}^{(\\mathrm{i})}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}^{\\top}}(A^{2})^{\\dagger}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}\\Sigma_{x}^{(\\mathrm{i})1/2}}_{=:T_{3}}\\|_{\\mathrm{F}}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where we used the fact that the inner products $\\mathrm{tr}\\bigl(T_{1}T_{2}^{\\intercal}\\bigr)\\,=\\,\\mathrm{tr}\\bigl(T_{2}T_{3}^{\\intercal}\\bigr)\\,=\\,\\mathrm{tr}\\bigl(T_{3}T_{1}^{\\intercal}\\bigr)\\,=\\,0$ . By choosing $\\Delta=(\\overline{{W}}_{\\ell+1}^{\\mathrm{pre}})^{\\dagger}D\\Sigma_{x}^{\\mathrm{(i)}}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}\\top}A^{\\dagger}$ for example, the term $T_{1}$ becomes 0. Thus ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}^{(\\mathrm{i})}(f_{\\ell}^{\\mathrm{full}})=\\|D\\Sigma_{x}^{(\\mathrm{i})1/2}(I-\\Sigma_{x}^{(\\mathrm{i})1/2}\\underline{{W_{\\ell-1}^{\\mathrm{pre}}}}(A^{2})^{\\dagger}\\underline{{W_{\\ell-1}^{\\mathrm{pre}}}}\\Sigma_{x}^{(\\mathrm{i})1/2})\\|_{\\mathrm{F}}^{2}}\\\\ {+\\,\\|(I-\\Phi^{\\prime}\\Phi^{\\prime\\top})D\\Sigma_{x}^{(\\mathrm{i})}\\underline{{W_{\\ell-1}^{\\mathrm{pre}}}}(A^{2})^{\\dagger}\\underline{{W_{\\ell-1}^{\\mathrm{pre}}}}\\Sigma_{x}^{(\\mathrm{i})1/2}\\|_{\\mathrm{F}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "This gives the desired result. ", "page_idx": 29}, {"type": "text", "text": "We obtain the following corollary as a direct consequence of Lemma E.17. Corollary E.18. For $f_{\\ell}^{\\mathrm{full}}$ , it holds that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{E}^{(\\mathrm{i})}(f_{\\ell}^{\\mathrm{full}})\\leq\\|\\Psi_{*}^{\\top}(D\\Sigma_{x}^{(\\mathrm{i})1/2})(I-\\Sigma_{x}^{(\\mathrm{i})1/2}}\\\\ &{\\qquad\\qquad\\quad+\\,\\|(I-\\Phi^{\\prime}\\Phi^{\\prime\\top})\\Phi_{*}(D\\Sigma_{x}^{(\\mathrm{i})1/2})\\|_{\\mathrm{op}}\\mathcal{E}^{(\\mathrm{i})}(f^{\\mathrm{pre}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "The first term on the right hand side of (27) measures the distance between two subspaces spanned by $\\Psi_{*}(D\\Sigma_{x}^{\\mathrm{(i)1/2}})$ and $\\Psi_{*}(\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}\\Sigma_{x}^{(\\mathrm{i})1/2})$ . Intuitively speaking, this quantifies the information coded at the $\\ell$ -th layer, and the necessary information to predict residuals. Thus, it bounds the maximum improvement by the $\\ell$ -th layer fine-tuning. The second term measures the subspace distance between the subspace where prediction residuals reside, and the subspace predictable by the $\\ell_{}$ -th layer finetuning. ", "page_idx": 29}, {"type": "text", "text": "F Auxiliary Results for Proofs ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Lemma F.1. Fix $\\colon s,d_{1},d_{2}\\in\\mathbb{N}^{+}$ . For any $A,B\\in\\mathbb{R}^{d_{1}\\times d_{2}}$ , $i f\\,\\|B-A\\|_{\\mathrm{op}}\\,\\leq\\,\\|A\\|_{\\mathrm{op}}$ and $\\lambda_{s}(A)>$ $\\lambda_{s+1}(A)$ hold, then, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\|S V D_{s}(B)-S V D_{s}(A)\\|_{\\mathrm{F}}\\lesssim\\kappa_{*}^{2}(A)\\frac{\\lambda_{s}(A)}{\\lambda_{s}(A)-\\lambda_{s+1}(A)}\\big(\\sqrt{s}\\|B-A\\|_{\\infty}\\wedge\\|B-A\\|_{\\mathrm{F}}\\big).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Proof. By triangle inequality, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\mathrm{SVD}_{s}(B)-\\mathrm{SVD}_{s}(A)||_{\\mathrm{F}}=\\|\\Phi_{s}(B)\\Phi_{s}^{\\top}(B)B-\\Phi_{s}(A)\\Phi_{s}^{\\top}(A)A\\|_{\\mathrm{F}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\|\\Phi_{s}(B)\\Phi_{s}^{\\top}(B)(B-A)\\|_{\\mathrm{F}}+\\|(\\Phi_{s}(B)\\Phi_{s}^{\\top}(B)-\\Phi_{s}(A)\\Phi_{s}^{\\top}(A))A\\|_{\\mathrm{F}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\sqrt{s}\\|B-A\\|_{\\mathrm{op}}+\\|\\Phi_{s}(B)\\Phi_{s}^{\\top}(B)-\\Phi_{s}(A)\\Phi_{s}^{\\top}(A)\\|_{\\mathrm{F}}\\|A\\|_{\\mathrm{op}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Using Davis-Kahan theorem (Theorem 4 from [70]), and Lemma 2.6 from [11], ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\|\\Phi_{s}(B)\\Phi_{s}^{\\top}(B)-\\Phi_{s}(A)\\Phi_{s}^{\\top}(A)\\|_{\\mathrm{F}}\\leq\\frac{6\\sqrt{2}\\|A\\|_{\\mathrm{op}}(\\sqrt{s}\\|B-A\\|_{\\mathrm{op}}\\wedge\\|B-A\\|_{\\mathrm{F}})}{\\lambda_{s}^{2}(A)-\\lambda_{s+1}^{2}(A)}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Thus ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\mathrm{SVD}_{s}(B)-\\mathrm{SVD}_{s}(A)\\|_{\\mathrm{F}}\\lesssim\\frac{\\|A\\|_{\\mathrm{op}}^{2}}{\\lambda_{s}^{2}(A)}\\frac{\\lambda_{s}^{2}(A)}{\\lambda_{s}^{2}(A)-\\lambda_{s+1}^{2}(A)}(\\sqrt{s}\\|B-A\\|_{\\mathrm{op}}\\wedge\\|B-A\\|_{\\mathrm{F}})}\\\\ {\\lesssim\\frac{\\|A\\|_{\\mathrm{op}}^{2}}{\\lambda_{s}^{2}(A)}\\frac{\\lambda_{s}(A)}{\\lambda_{s}(A)-\\lambda_{s+1}(A)}(\\sqrt{s}\\|B-A\\|_{\\mathrm{op}}\\wedge\\|B-A\\|_{\\mathrm{F}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "This concludes the proof. ", "page_idx": 29}, {"type": "text", "text": "We cite the concentration inequality for cross-covariance matrices from [47]. ", "page_idx": 30}, {"type": "text", "text": "Lemma F.2 (Proposition 9.1 from [47]). Let $Z$ and $\\tilde{Z}$ be mean zero random vectors taking values in $\\mathbb{R}^{d_{1}}$ and $\\mathbb{R}^{d_{2}}$ , respectively. Denote covariance matrices of $Z$ and $\\tilde{Z}$ by $\\Sigma_{Z}$ and $\\Sigma_{\\tilde{Z}}$ , respectively. Fix any $t>0$ . Assume that there exist constants $c_{1},c_{2}>0$ such that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\gamma^{\\top}\\Sigma_{Z}\\gamma\\geq c_{1}\\|\\gamma^{\\top}Z\\|_{\\psi_{2}}^{2}\\ \\ a n d\\ \\ \\gamma^{\\prime\\top}\\Sigma_{\\tilde{Z}}\\gamma^{\\prime}\\geq c_{2}\\|\\gamma^{\\prime\\top}\\tilde{Z}\\|_{\\psi_{2}}^{2}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "holds for any $\\gamma\\in\\mathbb{R}^{d_{1}}$ and $\\gamma^{\\prime}\\in\\mathbb{R}^{d_{2}}$ . Choose $n\\gg(r_{e}(\\Sigma_{Z})\\land r_{e}(\\Sigma_{\\tilde{Z}}))(t+\\log(d_{1}+d_{2}))$ . Let $(Z_{i},\\tilde{Z}_{i})_{i\\in[n]}$ be $n$ independent copies of $(Z,{\\tilde{Z}})$ . Then, there exists a constant $C=C(c_{1},c_{2})>0$ such that with probability at least $1-e^{-t}$ , ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\left\\|\\frac{1}{n}\\sum_{i\\in[n]}Z_{i}\\tilde{Z}_{i}^{\\top}-\\mathbb{E}[Z\\tilde{Z}^{\\top}]\\right\\|_{\\mathrm{op}}\\leq C\\|\\Sigma_{Z}\\|_{\\mathrm{op}}^{1/2}\\|\\Sigma_{\\tilde{Z}}\\|_{\\mathrm{op}}^{1/2}\\sqrt{\\frac{(r_{e}(\\Sigma_{Z})+r_{e}(\\Sigma_{\\tilde{Z}})(t+\\log(d_{1}+d_{2}))}{n}}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "hold. ", "page_idx": 30}, {"type": "text", "text": "Note that if a random variable $Z$ taking values in $\\mathbb{R}^{d}$ satisfies $\\gamma^{\\top}\\Sigma_{Z}\\gamma\\geq c\\Vert\\gamma^{\\top}Z\\Vert_{\\psi_{2}}^{2}$ for any $\\gamma\\in\\mathbb{R}^{d}$ with some constant $c>0$ , $A Z$ also satisfies $\\gamma^{\\prime\\top}\\Sigma_{A Z}\\gamma^{\\prime}\\geq c\\Vert\\gamma^{\\prime\\top}A Z\\Vert_{\\psi_{2}}^{2}$ for any $\\gamma^{\\prime}\\in\\mathbb{R}^{d^{\\prime}}$ and any matrix $A\\in\\mathbb{R}^{d^{\\prime}\\times d}$ and arbitrary $d^{\\prime}\\in\\mathbb{N}^{+}$ , where $\\Sigma_{A Z}=A\\Sigma_{Z}A^{\\top}$ . ", "page_idx": 30}, {"type": "text", "text": "We then prove the following lemma to show the existance of a \u2018good\u2019 high probability event to bound multiple inequalities. ", "page_idx": 30}, {"type": "text", "text": "Lemma F.3. Suppose that Assumptions $E.l$ and $E.2$ hold. Fix any $S\\subset[d_{\\ell}]$ . Then, there exists an event $\\mathcal{F}$ with $\\mathbb{P}(\\mathcal{F})=1-\\exp\\bigl(-\\Omega(\\log^{2}(n+d+p))\\bigr)$ such that on the event $\\mathcal{F}$ , for $\\Phi\\in\\{\\Phi^{\\prime},\\Phi_{S}^{\\prime\\prime}\\}$ , ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\Phi^{\\top}\\hat{D}\\hat{\\Sigma}_{x}^{\\mathrm{(i)}}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}\\top}\\|_{\\mathrm{op}}\\lesssim\\|D\\Sigma_{x}^{\\mathrm{(i)1/2}}\\|_{\\mathrm{op}}\\|A\\|_{\\mathrm{op}},\\ \\|\\hat{A}^{\\dagger}\\|_{\\mathrm{op}}\\lesssim\\|A^{\\dagger}\\|_{\\mathrm{op}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "and ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|(\\hat{A}^{2})^{\\dagger}-(A^{2})^{\\dagger}\\|_{\\mathrm{op}}\\lesssim\\frac{\\kappa_{*}^{2}(A)}{\\lambda_{*}^{2}(A)}\\sqrt{\\frac{r_{e}(A^{2})\\log^{2}(n+d+p)}{n}},}\\\\ &{\\qquad\\quad\\|\\hat{A}-A\\|_{\\mathrm{op}}\\lesssim\\kappa_{*}^{2}(A)\\|A\\|_{\\mathrm{op}}\\sqrt{\\frac{r_{e}(A^{2})\\log^{2}(n+d+p)}{n}},}\\\\ &{\\qquad\\quad\\|\\hat{A}^{\\dagger}-A^{\\dagger}\\|_{\\mathrm{op}}\\lesssim\\frac{\\kappa_{*}(A)}{\\lambda_{*}(A)}\\sqrt{\\frac{r_{e}(A^{2})\\log^{2}(n+d+p)}{n}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "hold. Furthermore, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\Phi^{\\top}(\\hat{D}\\hat{\\Sigma}_{x}^{(\\mathrm{i})1/2}-D\\Sigma_{x}^{(\\mathrm{i})1/2})\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}\\top}\\|_{\\mathrm{op}}}\\\\ &{\\quad\\lesssim\\|\\Sigma_{\\epsilon}^{(\\mathrm{i})}\\|_{\\mathrm{op}}^{1/2}\\|A\\|_{\\mathrm{op}}\\sqrt{\\frac{\\left(r_{e}\\left(\\Phi^{\\top}\\Sigma_{\\epsilon}^{(\\mathrm{i})}\\Phi\\right)+r_{e}(A^{2})\\right)\\log^{2}(n+d+p)}{n}}}\\\\ &{\\quad\\quad+\\,\\|D\\Sigma_{x}^{(\\mathrm{i})}D^{\\top}\\|_{\\mathrm{op}}^{1/2}\\|A\\|_{\\mathrm{op}}\\sqrt{\\frac{\\left(r_{e}\\left(\\Phi^{\\top}D\\Sigma_{x}^{(\\mathrm{i})}D^{\\top}\\Phi\\right)+r_{e}(A^{2})\\right)\\log^{2}(n+d+p)}{n}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "holds on the event $\\mathcal{F}$ . ", "page_idx": 30}, {"type": "text", "text": "Proof. We only prove for $\\Phi=\\Phi^{\\prime}$ without loss of generality. Before proving Lemma F.3, we first derive several concentration inequalities. Assumption E.2 implies ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{n\\gg r_{e}(A^{2})\\log^{2}(n+d+p),}\\\\ &{n\\gg r_{e}(\\Sigma_{x}^{(\\mathrm{i})})\\log^{2}(n+d+p),}\\\\ &{n\\gg(r_{e}(\\Sigma_{\\epsilon}^{(\\mathrm{i})})\\wedge r_{e}(\\Sigma_{x}^{(\\mathrm{i})}))\\log^{2}(n+d+p),}\\\\ &{n\\gg(r_{e}(\\Phi^{\\top}\\Sigma_{\\epsilon}^{(\\mathrm{i})}\\Phi)\\wedge r_{e}(A^{2}))\\log^{2}(n+d+p),}\\\\ &{n\\gg(r_{e}(\\Phi^{\\top}D\\Sigma_{x}^{(\\mathrm{i})}D^{\\top}\\Phi)\\wedge r_{e}(A^{2}))\\log^{2}(n+d+p).}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Using Lemma F.2, we obtain ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\hat{A}^{2}-A^{2}\\|_{\\mathrm{op}}=\\|\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}\\hat{\\Sigma}_{x}^{\\mathrm{(i)}}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}\\top}-\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}\\Sigma_{x}^{\\mathrm{(i)}}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}\\top}\\|_{\\mathrm{op}}}\\\\ &{\\qquad\\qquad\\lesssim\\|A\\|_{\\mathrm{op}}^{2}\\sqrt{\\frac{r_{e}(A^{2})\\log^{2}(n+d+p)}{n}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "and ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\dot{\\Sigma}_{\\epsilon,x}^{(i)}\\|_{\\infty}\\lesssim\\|\\Sigma_{\\epsilon}^{(i)}\\|_{\\infty}^{1/2}\\|\\Sigma_{\\epsilon}^{(i)}\\|_{\\infty}^{1/2}\\sqrt{\\frac{(r_{\\epsilon}(\\Sigma_{\\epsilon}^{(i)})+r_{\\epsilon}(\\Sigma_{x}^{(i)}))\\log^{2}(n+d+p)}{n}},\\qquad\\quad\\mathrm{(35)}}\\\\ &{\\|\\dot{\\Sigma}_{x}^{(i)}-\\Sigma_{x}^{(i)}\\|_{\\infty}\\lesssim\\|\\Sigma_{x}^{(i)}\\|_{\\infty}\\sqrt{\\frac{r_{\\epsilon}(\\Sigma_{x}^{(i)})\\log^{2}(n+d+p)}{n}},\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad(36)}\\\\ &{\\bigg|\\Phi^{\\top}\\hat{\\Sigma}_{\\epsilon,x}^{(i)}(\\Sigma_{x}^{(i)})^{\\dagger}\\Sigma_{x}^{(i)}\\underline{{W_{\\epsilon}^{n+1}}}\\bigg|_{\\infty}\\lesssim\\|\\Sigma_{\\epsilon}^{(i)}\\|_{\\infty}^{1/2}\\|A\\|_{\\infty}\\sqrt{\\frac{(r_{\\epsilon}(\\Phi^{\\top}\\Sigma_{\\epsilon}^{(i)}\\Phi)+r_{\\epsilon}(A^{2}))\\log^{2}(n+d+p)}{n}},\\qquad\\quad\\mathrm{(37)}}\\\\ &{\\bigg|\\Phi^{\\top}D(\\hat{\\Sigma}_{x}^{(i)}-\\Sigma_{x}^{(i)})\\underline{{W_{\\epsilon}^{n+1}}}\\bigg|_{\\infty}\\lesssim\\|D\\Sigma_{x}^{(i)}D^{\\top}\\|_{\\infty}^{1/2}\\|A\\|_{\\infty}\\sqrt{\\frac{(r_{\\epsilon}(\\Phi^{\\top}D\\Sigma_{x}^{(i)}D^{\\top}\\Phi)+r_{\\epsilon}(A^{2}))\\log^{2}(n+d+p)}{n}},\\qquad\\quad\\mathrm{(37)}}\\\\ &{\\bigg|\\Phi^{\\top}D(\\hat{\\Sigma}_{x}^{(i)}-\\Sigma_{x}^{(i)})\\underline{{W_{\\epsilon}^{n+1}}}\\bigg|_{\\infty}\\lesssim\\|D\\Sigma_{x}^{(i)}D^{\\top}\\|_{\\infty}^{1/2}\\|A\\|_{\\infty}\\sqrt{\\frac{(r_{\\epsilon}(\\Phi^{\\top}D\\Sigma_{x}^{(i)}D^{\\top} \n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "with high probability. Hereafter we only focus on the event $\\mathcal{F}$ where these inequalities hold. We divide the proof into 2 parts. ", "page_idx": 31}, {"type": "text", "text": "Part 1. In this part we derive (30), (31) and (32). Note that $\\|\\hat{A}^{2}-A^{2}\\|_{\\mathrm{op}}\\leq\\lambda_{*}(A^{2})/2$ holds on the event $\\mathcal{F}$ since $n\\gg\\kappa_{*}^{4}(A)r_{e}(A^{2})\\log^{2}(n+d+p)$ by Assumption E.2, and hence $\\mathrm{rank}(\\hat{A}^{2})=$ $\\operatorname{rank}(A^{2})$ . Using Theorem 5.2 from [60], ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\frac{\\|(\\hat{A}^{2})^{\\dagger}-(A^{2})^{\\dagger}\\|_{\\mathrm{op}}}{\\|(A^{2})^{\\dagger}\\|_{\\mathrm{op}}}\\lesssim\\left(1-\\frac{\\kappa_{*}(A^{2})\\|\\hat{A}^{2}-A^{2}\\|_{\\mathrm{op}}}{\\|A\\|_{\\mathrm{op}}^{2}}\\right)^{-1}\\!\\frac{\\kappa_{*}(A^{2})\\|\\hat{A}^{2}-A^{2}\\|_{\\mathrm{op}}}{\\|A\\|_{\\mathrm{op}}^{2}}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Again from Assumption E.2, (34) gives ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\|(\\hat{A}^{2})^{\\dagger}-(A^{2})^{\\dagger}\\|_{\\mathrm{op}}\\lesssim\\frac{\\kappa_{*}(A^{2})}{\\lambda_{*}(A^{2})}\\sqrt{\\frac{r_{e}(A^{2})\\log^{2}(n+d+p)}{n}}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "This yields (30). Proposition 3.2 from [65] and (34) yield, ", "page_idx": 31}, {"type": "equation", "text": "$$\n(\\Phi^{\\prime\\prime\\prime\\top}\\hat{A}^{2}\\Phi^{\\prime\\prime\\prime})^{1/2}-(\\Phi^{\\prime\\prime\\prime}{}^{\\top}A^{2}\\Phi^{\\prime\\prime\\prime})^{1/2}\\|_{\\mathrm{op}}\\leq\\frac{\\|\\Phi^{\\prime\\prime\\prime}(\\hat{A}^{2}-A^{2})\\Phi^{\\prime\\prime\\prime}\\|_{\\mathrm{op}}}{\\lambda_{*}^{1/2}(\\Phi^{\\prime\\prime\\prime}{}^{\\top}A^{2}\\Phi^{\\prime\\prime\\prime})}\\lesssim\\frac{\\|A\\|_{\\mathrm{op}}^{2}}{\\lambda_{*}(A)}\\sqrt{\\frac{r_{e}(A^{2})\\log^{2}(n+d)}{n}}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $\\begin{array}{r l r}{\\Phi^{\\prime\\prime\\prime}}&{{}:=}&{\\Phi_{*}(A^{2}).}\\end{array}$ , and we used $\\lambda_{*}(\\Phi^{\\prime\\prime\\prime\\top}A^{2}\\Phi^{\\prime\\prime\\prime})\\quad\\geq\\quad\\lambda_{*}(A^{2})$ . Since $\\begin{array}{r l}{\\hat{A}}&{{}=}\\end{array}$ $\\Phi^{\\prime\\prime\\prime}(\\Phi^{\\prime\\prime\\prime}{}^{\\top}\\hat{A}^{2}\\Phi^{\\prime\\prime\\prime})^{1/2}\\Phi^{\\prime\\prime\\prime\\top}$ and $A^{1/2}=\\Phi^{\\prime\\prime\\prime}(\\Phi^{\\prime\\prime\\prime\\top}A^{2}\\Phi^{\\prime\\prime\\prime})^{1/2}\\Phi^{\\prime\\prime\\prime\\top}$ , we obtain (31) as ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\|\\hat{A}-A\\|_{\\mathrm{op}}\\lesssim\\kappa_{*}(A)\\|A\\|_{\\mathrm{op}}\\sqrt{\\frac{r_{e}(A^{2})\\log^{2}(n+d+p)}{n}}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Again using Theorem 5.2 from [60] combined with Assumption E.2, we obtain (32) as ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\|\\hat{A}^{\\dagger}-A^{\\dagger}\\|_{\\mathrm{op}}\\lesssim\\frac{\\kappa_{*}^{2}(A)}{\\lambda_{*}(A)}\\sqrt{\\frac{r_{e}(A^{2})\\log^{2}(n+d+p)}{n}}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "This yields $\\|\\hat{A}^{\\dagger}\\|_{\\mathrm{op}}\\lesssim\\|A^{\\dagger}\\|_{\\mathrm{op}}$ . ", "page_idx": 31}, {"type": "text", "text": "Part 2. Next we derive (33). By a similar argument as Part 1, (36) and Assumption E.2, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\|(\\hat{\\Sigma}_{x}^{(\\mathrm{i})})^{\\dagger}-(\\Sigma_{x}^{(\\mathrm{i})})^{\\dagger}\\|_{\\mathrm{op}}\\lesssim\\frac{\\|\\Sigma_{x}^{(\\mathrm{i})}\\|_{\\mathrm{op}}}{\\lambda_{*}^{2}(\\Sigma_{x}^{(\\mathrm{i})})}\\sqrt{\\frac{r_{e}(\\Sigma_{x}^{(\\mathrm{i})})\\log^{2}(n+d+p)}{n}}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Since $\\hat{D}-D=\\check{\\Sigma}_{\\epsilon,x}^{\\mathrm{(i)}}=\\hat{\\Sigma}_{\\epsilon,x}^{\\mathrm{(i)}}(\\hat{\\Sigma}_{x}^{\\mathrm{(i)}})^{\\dagger}$ , ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Phi^{\\top}(\\hat{D}\\hat{\\Sigma}_{x}^{(i)}-D\\Sigma_{x}^{(i)})\\underline{{W}}_{\\ell-1}^{\\mathrm{pr}^{\\top}}\\|_{\\mathrm{op}}}\\\\ &{\\quad\\leq\\left\\|\\Phi^{\\top}(\\hat{D}-D)\\Sigma_{x}^{(i)}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}^{\\top}}\\right\\|_{\\mathrm{op}}+\\left\\|\\Phi^{\\top}D(\\hat{\\Sigma}_{x}^{(i)}-\\Sigma_{x}^{(i)})\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}^{\\top}}\\right\\|_{\\mathrm{op}}+\\left\\|\\Phi^{\\top}(\\hat{D}-D)(\\hat{\\Sigma}_{x}^{(i)}-\\Sigma_{x}^{(i)})\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}}\\right\\|_{\\mathrm{op}}}\\\\ &{\\quad=\\left\\|\\Phi^{\\top}\\hat{\\Sigma}_{\\epsilon,x}^{(i)}(\\hat{\\Sigma}_{x}^{(i)})^{\\dagger}\\Sigma_{x}^{(i)}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}^{\\top}}\\right\\|_{\\mathrm{op}}+\\left\\|\\Phi^{\\top}D(\\hat{\\Sigma}_{x}^{(i)}-\\Sigma_{x}^{(i)})\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}^{\\top}}\\right\\|_{\\mathrm{op}}+\\left\\|\\Phi^{\\top}\\hat{\\Sigma}_{\\epsilon,x}^{(i)}(\\hat{\\Sigma}_{x}^{(i)})^{\\dagger}(\\hat{\\Sigma}_{x}^{(i)}-\\Sigma_{x}^{(i)})\\right\\|_{\\mathrm{op}}}\\\\ &{\\quad\\leq\\left\\|\\Phi^{\\top}\\hat{\\Sigma}_{\\epsilon,x}^{(i)}(\\Sigma_{x}^{(i)})^{\\dagger}\\Sigma_{x}^{(i)}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}^{\\top}}\\right\\|_{\\mathrm{op}}+\\left\\|\\Phi^{\\top}\\hat{\\Sigma}_{\\epsilon,x}^{(i)}\\left((\\Sigma_{x}^{(i)})^{\\dagger}\\Sigma_{x}^{(i)}-(\\hat{\\Sigma}_{x}^{(i)})^{\\dagger}\\Sigma_{x}^{(i)}\\right)\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}^{\\top}}\\right\\|_{\\mathrm{op}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "We bound $Q_{1},Q_{2},R_{1}$ and $R_{2}$ separately. For the terms $Q_{1}$ and $Q_{2}$ , (37) and (38) give ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q_{1}\\lesssim\\|\\Sigma_{\\epsilon}^{(\\mathrm{i})}\\|_{\\mathrm{op}}^{1/2}\\|A\\|_{\\mathrm{op}}\\sqrt{\\frac{\\left(r_{e}\\big(\\Phi^{\\top}\\Sigma_{\\epsilon}^{(\\mathrm{i})}\\Phi\\big)+r_{e}\\big(A^{2}\\big)\\right)\\log^{2}\\left(n+d+p\\right)}{n}},}\\\\ &{Q_{2}\\lesssim\\|D\\Sigma_{x}^{(\\mathrm{i})}D^{\\top}\\|_{\\mathrm{op}}^{1/2}\\|A\\|_{\\mathrm{op}}\\sqrt{\\frac{\\left(r_{e}\\big(\\Phi^{\\top}D\\Sigma_{x}^{(\\mathrm{i})}D^{\\top}\\Phi\\big)+r_{e}\\big(A^{2}\\big)\\right)\\log^{2}\\left(n+d+p\\right)}{n}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "For the term $R_{1}$ , using (35) and (40), ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R_{1}\\leq\\|\\hat{\\Sigma}_{\\varepsilon,x}^{(\\mathrm{i})}\\|_{\\infty}\\|(\\Sigma_{x}^{(\\mathrm{i})})^{\\dagger}-(\\hat{\\Sigma}_{x}^{(\\mathrm{i})})^{\\dagger}\\|_{\\infty}\\|\\Sigma_{x}^{(\\mathrm{i})}\\|_{\\infty}^{1/2}\\|\\Sigma_{x}^{(\\mathrm{i})1/2}\\underline{{W_{\\varepsilon-1}^{\\mathrm{prer}}}}\\|_{\\infty}}\\\\ &{\\quad\\lesssim\\frac{\\|\\Sigma_{x}^{(\\mathrm{i})}\\|_{\\infty}^{2}\\|\\Sigma_{\\varepsilon}^{(\\mathrm{i})}\\|_{\\infty}^{1/2}\\|A\\|_{\\infty}}{\\lambda_{*}^{2}(\\Sigma_{x}^{(\\mathrm{i})})}\\sqrt{\\frac{(r_{e}(\\Sigma_{\\varepsilon}^{(\\mathrm{i})})+r_{e}(\\Sigma_{x}^{(\\mathrm{i})}))\\log^{2}(n+d+p)}{n}}\\sqrt{\\frac{r_{e}(\\Sigma_{x}^{(\\mathrm{i})})\\log^{2}(n+d+p)}{n}}}\\\\ &{\\quad\\lesssim\\kappa_{*}^{2}(\\Sigma_{x}^{(\\mathrm{i})})\\|\\Sigma_{\\epsilon}^{(\\mathrm{i})}\\|_{\\infty}^{1/2}\\|A\\|_{\\infty}\\frac{\\sqrt{r_{e}(\\Sigma_{x}^{(\\mathrm{i})})(r_{e}(\\Sigma_{\\epsilon}^{(\\mathrm{i})})+r_{e}(\\Sigma_{x}^{(\\mathrm{i})}))\\log^{2}(n+d+p)}}{n}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "For the term $R_{2}$ , using (35) and (36), ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{I}_{2}\\le\\|\\hat{\\Sigma}_{\\epsilon,x}^{(\\mathrm{i})}\\|_{\\infty}\\|(\\hat{\\Sigma}_{x}^{(\\mathrm{i})})^{\\dag}\\|_{\\infty}\\|\\hat{\\Sigma}_{x}^{(\\mathrm{i})}-\\Sigma_{x}^{(\\mathrm{i})}\\|_{\\infty}\\|(\\Sigma_{x}^{(\\mathrm{i})})^{\\dag}\\|_{\\infty}^{1/2}\\|\\Sigma_{x}^{(\\mathrm{i})1/2}\\underline{{W_{\\ell-1}^{\\mathrm{per}}}}\\|_{\\infty}}\\\\ &{\\quad\\lesssim\\|(\\Sigma_{x}^{(\\mathrm{i})})^{\\dag}\\|_{\\infty}^{3/2}\\|A\\|_{\\infty}\\|\\Sigma_{\\epsilon}^{(\\mathrm{i})}\\|_{\\infty}^{1/2}\\|\\Sigma_{x}^{(\\mathrm{i})}\\|_{\\infty}^{3/2}\\sqrt{\\frac{(r_{\\epsilon}(\\Sigma_{\\epsilon}^{(\\mathrm{i})})+r_{\\epsilon}(\\Sigma_{x}^{(\\mathrm{i})}))\\log^{2}(n+d+p)}{n}}\\sqrt{\\frac{r_{\\epsilon}(\\Sigma_{x}^{(\\mathrm{i})})\\log^{2}(\\hat{\\Sigma}_{x}^{(\\mathrm{i})}/\\Sigma_{x}^{(\\mathrm{i})})}{n}}}\\\\ &{\\quad\\lesssim\\kappa_{*}^{3/2}(\\Sigma_{x}^{(\\mathrm{i})})\\|\\Sigma_{\\epsilon}^{(\\mathrm{i})}\\|_{\\infty}^{1/2}\\|A\\|_{\\infty}\\frac{\\sqrt{r_{\\epsilon}(\\Sigma_{x}^{(\\mathrm{i})})(r_{\\epsilon}(\\Sigma_{\\epsilon}^{(\\mathrm{i})})+r_{\\epsilon}(\\Sigma_{x}^{(\\mathrm{i})}))\\log^{2}(n+d+p)}}{n},}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where we used $\\|(\\hat{\\Sigma}_{x}^{\\mathrm{(i)}})^{\\dagger}\\|_{\\mathrm{op}}\\,\\lesssim\\,\\|(\\Sigma_{x}^{\\mathrm{(i)}})^{\\dagger}\\|_{\\mathrm{op}}$ by Assumption E.2 combined with (40). Again from Assumption E.2, $R_{1}+R_{2}$ is bounded by the right hand side of (41). Therefore, ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\Phi^{\\top}(\\hat{D}\\hat{\\Sigma}_{x}^{(\\mathrm{i})}-D\\Sigma_{x}^{(\\mathrm{i})})\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}^{\\top}}\\|_{\\mathrm{op}}}\\\\ &{\\quad\\lesssim\\|\\Sigma_{\\epsilon}^{(\\mathrm{i})}\\|_{\\mathrm{op}}^{1/2}\\|A\\|_{\\mathrm{op}}\\sqrt{\\frac{\\left(r_{e}\\left(\\Phi^{\\top}\\Sigma_{\\epsilon}^{(\\mathrm{i})}\\Phi\\right)+r_{e}(A^{2})\\right)\\log^{2}(n+d+p)}{n}}}\\\\ &{\\quad\\quad+\\,\\|D\\Sigma_{x}^{(\\mathrm{i})}D^{\\top}\\|_{\\mathrm{op}}^{1/2}\\|A\\|_{\\mathrm{op}}\\sqrt{\\frac{\\left(r_{e}\\left(\\Phi^{\\top}D\\Sigma_{x}^{(\\mathrm{i})}D^{\\top}\\Phi\\right)+r_{e}\\left(A^{2}\\right)\\right)\\log^{2}(n+d+p)}{n}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Finally, from Assumption E.2, we obtain This concludes the proof. $\\|\\Phi^{\\top}\\hat{D}\\hat{\\Sigma}_{x}^{\\mathrm{(i)}}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}\\top}\\|_{\\mathrm{op}}\\lesssim\\|D\\Sigma_{x}^{\\mathrm{(i)1/2}}\\|_{\\mathrm{op}}\\|\\Sigma_{x}^{\\mathrm{(i)1/2}}\\underline{{W}}_{\\ell-1}^{\\mathrm{pre}\\top}\\|_{\\mathrm{op}}.$ ", "page_idx": 32}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "1. Claims ", "page_idx": 33}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: The abstract and introduction clearly state the contributions of this work. ", "page_idx": 33}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We discuss the limitations of the work in the Conclusion Section (section 8). ", "page_idx": 33}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: The main assumptions and theorems are provided in Section 4, while additional details and complete proofs can be found in Appendix E. ", "page_idx": 33}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 33}, {"type": "text", "text": "Justification: The paper has disclosed all the information in the method and experiment part. ", "page_idx": 33}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 33}, {"type": "text", "text": "Justification: We have the code required to reproduce our experimental results and are working towards making our code available in a public GitHub repository. ", "page_idx": 33}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 33}, {"type": "text", "text": "Justification: The experimental setting is clearly described in Section 2, Section 5 and Section 6, and we will make our code available in a public GitHub repository. ", "page_idx": 33}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 33}, {"type": "text", "text": "Justification: All statistics and results included in the paper are accompanied by confidence intervals. ", "page_idx": 33}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 33}, {"type": "text", "text": "Justification: Information for the resources required to reproduce the experiments are included in the oaoer. All experiments are run with $4\\mathrm{~x~}$ A100 (80G). For the efficiency analysis, a single A100 GPU was used. ", "page_idx": 34}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 34}, {"type": "text", "text": "Justification: The research conducted in the paper fully conforms with the NeurIPS Code of Ethics in every respect. ", "page_idx": 34}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 34}, {"type": "text", "text": "Justification: We discuss the broader impacts of our work in Appendix. ", "page_idx": 34}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 34}, {"type": "text", "text": "Justification: Our paper does not introduce any assets that have a high risk for misuse. ", "page_idx": 34}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 34}, {"type": "text", "text": "Justification: We have explicitly mentioned the citations for the datasets and have ensured that all conditions are fully respected. ", "page_idx": 34}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 34}, {"type": "text", "text": "Justification: Upon acceptance, we will make our codebase publicly available and complete documentation for our assets. ", "page_idx": 34}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: We do not include any experiments with human subjects or crowdsourcing. ", "page_idx": 34}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 34}, {"type": "text", "text": "Justification: We do not include any experiments with human subjects or crowdsourcing. ", "page_idx": 34}]