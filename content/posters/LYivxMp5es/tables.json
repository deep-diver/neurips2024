[{"figure_path": "LYivxMp5es/tables/tables_7_1.jpg", "caption": "Table 1: Inference results on Dataset v2, with a degree of connectivity 4, featuring a network of 50 nodes. This table presents the average infection rates for different models, with ResNet trained on a network of 50 nodes and GCN trained on networks of 10 nodes, under various methods (M.) tested with varying action budgets (A.) across different cases considered.", "description": "This table presents the average infection rates achieved by different models (ResNet and GCN) using various methods (reinforcement learning with different reward functions and supervised learning) and action budgets across three different cases of opinion network models.  The results show how well each method performed in controlling the spread of misinformation in a network with 50 nodes and a connectivity degree of 4.", "section": "5 Results and Discussion"}, {"figure_path": "LYivxMp5es/tables/tables_8_1.jpg", "caption": "Table 1: Inference results on Dataset v2, with a degree of connectivity 4, featuring a network of 50 nodes. This table presents the average infection rates for different models, with ResNet trained on a network of 50 nodes and GCN trained on networks of 10 nodes, under various methods (M.) tested with varying action budgets (A.) across different cases considered.", "description": "This table presents a comparison of the average infection rates achieved by different models (ResNet and GCN) using various methods (Reinforcement Learning with different reward functions and Supervised Learning) and varying action budgets across three different scenarios (Cases 1-3). The results are based on Dataset v2, which features a network of 50 nodes with a connectivity degree of 4, representing one of the most complex testing scenarios.", "section": "5 Results and Discussion"}, {"figure_path": "LYivxMp5es/tables/tables_9_1.jpg", "caption": "Table 3: This table outlines the unique attributes of our approach, including the use of a deep value network, network dynamicity across multiple cases, asynchronous communication, and the exploration of five different reward models.", "description": "This table summarizes the key features and improvements of the proposed approach compared to previous works.  It highlights four significant advancements:\n\n1. **Action-Space Variant:** The use of a deep value network allows for flexible and adaptive intervention strategies, unlike the fixed action spaces in previous work.\n2. **Expressive Models:** The model incorporates three cases of opinion network dynamics, ranging from discrete to continuous representations of opinion and trust, resulting in a more comprehensive and realistic simulation.\n3. **Realistic Communication Dynamics:** The model considers asynchronous communication between agents, making it a more accurate reflection of real-world social networks.\n4. **Wider Applications:** The study explores five distinct reward models for reinforcement learning, a more extensive range than previously studied. This broad approach improves the understanding of the effectiveness of different control strategies. ", "section": "3 Methods"}]