{"references": [{"fullname_first_author": "Elias Frantar", "paper_title": "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers", "publication_date": "2023-00-00", "reason": "This paper introduces GPTQ, a post-training quantization technique that is highly relevant to the current work's focus on efficient LLM binarization."}, {"fullname_first_author": "Yuzhang Shang", "paper_title": "PB-LLM: Partially Binarized Large Language Models", "publication_date": "2024-00-00", "reason": "This paper proposes PB-LLM, a partially binarized LLM method that directly addresses the problem of preserving important information during the binarization process and is directly compared with the proposed method."}, {"fullname_first_author": "Wei Huang", "paper_title": "BiLLM: Pushing the Limit of Post-Training Quantization for LLMs", "publication_date": "2024-00-00", "reason": "This paper introduces BiLLM, another relevant binarization technique that is compared to the proposed method in the paper, demonstrating significant advancements in the field."}, {"fullname_first_author": "Yuzhuang Xu", "paper_title": "OneBit: Towards Extremely Low-bit Large Language Models", "publication_date": "2024-00-00", "reason": "This paper presents OneBit, a dual-dimensional scaling approach for LLM binarization that is closely related to and compared against the proposed method."}, {"fullname_first_author": "Noam Shazeer", "paper_title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer", "publication_date": "2017-00-00", "reason": "This paper introduces the Mixture of Experts (MoE) approach, which serves as the foundation for the proposed BinaryMoS method's token-adaptive scaling factor mechanism."}]}