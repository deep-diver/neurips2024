[{"figure_path": "wJaCsnT9UE/tables/tables_23_1.jpg", "caption": "Table 1: Hyperparamter setting for results in Section 4.4, we report the optimal p and k after grid search. Each result in Figure 7 is averaged over three ensembles, which corresponds to 9 random seeds, the random seeds we use are {13, 17, 27, 113, 117, 127, 43, 59, 223}.", "description": "This table shows the hyperparameter settings used for the experiments presented in Section 4.4 of the paper.  It lists the optimal values for the perturbation radius (p) and the top-k percentage of data samples used for the SharpBalance method (k). The table also specifies the Td parameter, and indicates that each result shown in Figure 7 is an average across three ensembles and nine different random seeds. ", "section": "4.4 Empirical evaluation of SharpBalance"}, {"figure_path": "wJaCsnT9UE/tables/tables_25_1.jpg", "caption": "Table 2: Results of different severity levels on CIFAR10-C.", "description": "This table shows the performance of three different ensemble methods (Deep ensemble, Deep ensemble+SAM, and SharpBalance) on CIFAR-10C dataset with different corruption severities (1-5).  The numbers represent the test accuracy.  The values in parentheses show the improvement of SharpBalance over the Deep ensemble+SAM method.", "section": "F.1 Evaluation on different corruption severity"}, {"figure_path": "wJaCsnT9UE/tables/tables_25_2.jpg", "caption": "Table 3: Results of different severity levels on CIFAR100-C.", "description": "This table presents the results of the CIFAR-100-C dataset with different corruption levels. It compares the performance of three methods: Deep ensemble, Deep ensemble+SAM, and SharpBalance. The numbers represent the accuracy, and the numbers in parentheses indicate the improvement of SharpBalance compared to Deep ensemble+SAM.  SharpBalance consistently outperforms the other methods across all corruption levels.", "section": "F.1 Evaluation on different corruption severity"}, {"figure_path": "wJaCsnT9UE/tables/tables_25_3.jpg", "caption": "Table 4: Results of different severity levels on Tiny-ImageNet-C.", "description": "This table presents the results of evaluating the performance of three different ensemble methods (Deep ensemble, Deep ensemble+SAM, and SharpBalance) on the Tiny-ImageNet-C dataset with varying corruption severity levels.  The numbers represent the accuracy achieved by each method under different corruption conditions. The numbers in parentheses represent the improvement achieved by SharpBalance compared to the Deep ensemble+SAM method.  The data shows that SharpBalance consistently outperforms the other two methods across all corruption levels.", "section": "F.1 Evaluation on different corruption severity"}, {"figure_path": "wJaCsnT9UE/tables/tables_25_4.jpg", "caption": "Table 5: (Additional experiments on Transformer-architecture). The ensemble test accuracy is reported and each ensemble comprises three members. The observation is consistent with the residual network results in the main paper: SAM improves the Deep Ensemble, and SharpBalance outperforms both two baselines.", "description": "This table shows the results of additional experiments performed on different model architectures, including the Transformer-based Vision Transformer (ViT-T/16) and the ALBERT-Base model for natural language processing.  The table compares the performance of three methods: a standard deep ensemble, a deep ensemble enhanced with Sharpness-Aware Minimization (SAM), and the proposed SharpBalance method.  The performance metric used is ensemble test accuracy, evaluated on CIFAR100, CIFAR100-C (a corrupted version of CIFAR100), and the MRPC (Microsoft Research Paraphrase Corpus) dataset, respectively. The results demonstrate that SharpBalance consistently outperforms both baseline methods.", "section": "F.2 Evaluation on different model architectures"}, {"figure_path": "wJaCsnT9UE/tables/tables_27_1.jpg", "caption": "Table 6: SharpBalance outperforms EoA and SAM+ both in-distribution and OOD generalization on CIFAR10 and CIFAR100.", "description": "This table compares the performance of SharpBalance against two strong baselines, EoA and SAM+, on CIFAR10 and CIFAR100 datasets.  The results demonstrate that SharpBalance achieves superior performance in both in-distribution (ACC) and out-of-distribution (cACC) generalization compared to the baselines.", "section": "F.4 Comparison with more baselines"}, {"figure_path": "wJaCsnT9UE/tables/tables_27_2.jpg", "caption": "Table 7: (Main results: SharpBalance improves the overall ensembling performance and mitigates the reduced ensembling improvement caused by sharpness-diversity trade-off). The three-member ResNet18 ensemble is trained with different methods on three datasets. The first row reports the OOD accuracy and the second row reports the ID accuracy. The lower part of each bar with the diagonal lines represents the individual model performance. The upper part of each bar represents the ensembling improvement. The results are reported by averaging three ensembles, and each ensemble is comprised of three models.", "description": "This table shows the results of comparing three different training methods (Deep Ensemble, Deep Ensemble + SAM, and SharpBalance) on three datasets (CIFAR10, CIFAR100, and TinyImageNet) with their corrupted versions.  The results are presented as the average of three ensembles, each composed of three models. For each dataset and training method, both in-distribution (ID) accuracy and out-of-distribution (OOD) accuracy are provided. The table illustrates that SharpBalance consistently improves the ensemble performance in both ID and OOD settings.", "section": "4.4 Empirical evaluation of SharpBalance"}, {"figure_path": "wJaCsnT9UE/tables/tables_28_1.jpg", "caption": "Table 1: Hyperparamter setting for results in Section 4.4, we report the optimal p and k after grid search. Each result in Figure 7 is averaged over three ensembles, which corresponds to 9 random seeds, the random seeds we use are {13, 17, 27, 113, 117, 127, 43, 59, 223}.", "description": "This table shows the hyperparameters used in the experiments described in Section 4.4 of the paper.  It lists the optimal perturbation radius (p) and top-k% threshold for the SharpBalance method, obtained through a grid search.  Also included are the datasets used (CIFAR10, CIFAR100, and TinyImageNet), the model architecture (ResNet18), and the methods compared (Deep Ensemble, Deep Ensemble + SAM, and SharpBalance).  The random seeds used for averaging the results are also provided.", "section": "4.4 Empirical evaluation of SharpBalance"}]