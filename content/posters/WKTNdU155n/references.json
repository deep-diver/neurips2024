{"references": [{"fullname_first_author": "Ross Taylor", "paper_title": "Galactica: A large language model for science", "publication_date": "2022-11-09", "reason": "This paper introduces Galactica, a large language model specifically trained for scientific tasks, which is directly relevant to the LLaMo model's use of LLMs in the molecular domain."}, {"fullname_first_author": "Zhiyuan Liu", "paper_title": "MolCA: Molecular graph-language modeling with cross-modal projector and uni-modal adapter", "publication_date": "2023-00-00", "reason": "This paper presents MolCA, a model that integrates molecular graph and language modalities, and serves as a direct comparison and contrast to the LLaMo approach."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Llama 2: Open foundation and fine-tuned chat models", "publication_date": "2023-07-09", "reason": "LLaMo uses Llama 2 as its base language model, making this paper foundational to understanding the underlying architecture and capabilities of LLaMo."}, {"fullname_first_author": "Yizhong Wang", "paper_title": "Self-instruct: Aligning language models with self-generated instructions", "publication_date": "2023-00-00", "reason": "This paper details the self-instruct method for aligning language models, providing crucial context for LLaMo's use of instruction tuning to adapt a large language model to the molecular domain."}, {"fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-00-00", "reason": "This paper explores instruction tuning of language models, a key technique leveraged by LLaMo to improve its instruction-following capabilities and adapt to molecular tasks."}]}