[{"figure_path": "m8MElyzuwp/figures/figures_1_1.jpg", "caption": "Figure 1: (a) Meta-learning treats synthetic set updates in DC as a meta-task; (b) Data-matching indirectly matches gradients, feature distributions, and training trajectories generated during network training; (c) Our proposed DCOD, the first for object detection, decouples the bi-level optimization of methods a and b, following a Fetch and Forge two-stage approach.", "description": "This figure compares three different dataset condensation frameworks. (a) shows the meta-learning framework, where the synthetic dataset is updated by minimizing the performance risk on the original dataset's validation set. (b) shows the data-matching framework, which aligns gradients, feature distributions, or training trajectories to simulate the original data's impact. (c) introduces the proposed DCOD framework, which decouples the bi-level optimization of the previous methods into two stages: Fetch and Forge. The Fetch stage stores key information from the original dataset within the detector. The Forge stage synthesizes images from the trained detector using model inversion.  DCOD is specifically designed for object detection tasks, addressing the challenges of multitasking and high-resolution data.", "section": "Introduction"}, {"figure_path": "m8MElyzuwp/figures/figures_2_1.jpg", "caption": "Figure 2: Visualization of images synthesized using DCOD. (a) A synthetic image contains only one category of foreground. (b) A synthetic image contains multiple foregrounds of different sizes, shapes, and categories.", "description": "This figure visualizes the synthetic images generated by the DCOD method.  (a) shows examples of images with only a single object in the foreground.  (b) shows examples of images with multiple objects in the foreground, demonstrating the method's ability to generate diverse and complex scenes.", "section": "3 Dataset Condensation for Object Detection"}, {"figure_path": "m8MElyzuwp/figures/figures_3_1.jpg", "caption": "Figure 3: Overview of the DCOD framework: In the first stage, Fetch, a detector is trained on the original images, encapsulating key information from the original dataset. In the second stage, Forge, a randomly initialized synthetic set is enhanced through Foreground Background Decoupling and Incremental PatchExpand on the initial images, which are then input into the trained detector. Guided by targets, specific category targets are updated in the corresponding areas of the images. The loss of the detector serves as the task loss for condensation, while pixel-level and feature-level regularization ensure the quality of the generated images.", "description": "This figure illustrates the two-stage process of the DCOD framework. Stage-I: Fetch trains a detector on the original images to capture key information. Stage-II: Forge uses this trained detector to create synthetic images. The process involves randomly initializing synthetic images, applying Foreground Background Decoupling and Incremental PatchExpand for enhancement, and optimizing these images using the detector's loss function. Regularization ensures the quality of the generated images.", "section": "3 Dataset Condensation for Object Detection"}, {"figure_path": "m8MElyzuwp/figures/figures_7_1.jpg", "caption": "Figure 7: Visualization of performance curves on Pascal VOC using YOLOv3-SPP as the compression ratio increases.", "description": "This figure shows the performance of the Random baseline method and the DCOD method as the compression ratio increases. The performance of the full dataset serves as the theoretical upper bound. When the ratio is below 5%, DCOD shows a significant advantage over the random method, while as the ratio exceeds 20%, the performance of both methods converge.", "section": "4.2 Experimental Results"}, {"figure_path": "m8MElyzuwp/figures/figures_8_1.jpg", "caption": "Figure 5: Visualization of different iteration steps during the condensed phase. Scores are assigned using the trained YOLOv3-spp model, based on IOU@0.5. The \"GT\" column represents the true labels of images. \"iter0\" shows the initial image scores, followed by scores of synthetic images and their bounding box at iterations 500, 1000, 2000, and 3000.", "description": "This figure visualizes the iterative process of synthesizing images using the DCOD method. The leftmost column shows the ground truth images with bounding boxes. Subsequent columns depict the synthetic image generation process at different iterations (0, 500, 1000, 2000, and 3000).  Each synthetic image has a score indicating the quality of the synthesis, calculated based on the Intersection over Union (IOU) at 0.5 threshold using a trained YOLOv3-SPP model. The figure demonstrates how the model progressively refines the synthetic images over iterations, improving the accuracy of object detection.", "section": "4.4 Visualization"}, {"figure_path": "m8MElyzuwp/figures/figures_8_2.jpg", "caption": "Figure 2: Visualization of images synthesized using DCOD. (a) A synthetic image contains only one category of foreground. (b) A synthetic image contains multiple foregrounds of different sizes, shapes, and categories.", "description": "This figure shows example images generated by the DCOD model.  The top row (a) demonstrates that the model can generate images with a single object in the foreground.  The bottom row (b) illustrates the model's ability to generate images containing multiple objects of different classes, sizes, and shapes, all within a single image.", "section": "Dataset Condensation for Object Detection"}, {"figure_path": "m8MElyzuwp/figures/figures_9_1.jpg", "caption": "Figure 7: Visualization of performance curves on Pascal VOC using YOLOv3-SPP as the compression ratio increases.", "description": "This figure shows a comparison of the performance of the Random baseline method and the DCOD method as the compression ratio increases. The performance of the full dataset, marked by a gray line, serves as the theoretical upper bound. When the ratio is below 5%, DCOD shows a significant advantage over the random method, while as the ratio exceeds 20%, the performance of both methods converge. The figure includes two subplots: one for mAP and one for AP@50, both plotted against the compression ratio.  The x-axis represents the compression ratio (%), and the y-axis represents the mAP and AP@50, respectively. The performance of the full dataset is shown as a dashed horizontal line for comparison.", "section": "4.2 Experimental Results"}]