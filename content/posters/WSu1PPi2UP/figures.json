[{"figure_path": "WSu1PPi2UP/figures/figures_3_1.jpg", "caption": "Figure 1: An illustration comparing the creation of descriptions of novel entities using ELM [Tennenholtz et al., 2024] vs. EAGLE (ours). The latent embedding space Z is illustrated as a complex surface on the right. Red points on the surface are illustrated as latent embeddings of existing entities. Black points are used to illustrate hypothetical (i.e., non-existing) entities. In ELM, a utility is maximized in latent embedding space to identify an optimal point. This hypothetical embedding is then decoded back to ambient space X to form a description of the hypothetical entity. Conversely, the EAGLE agent utilizes a highly capable pre-trained LLM as an environment to search for novel entities in ambient space X. EAGLE does not use a decoder, but rather only requires an encoder ED: X \u2192 Z. More specifically, the EAGLE agent uses action prompts to change an existing entity using the environment LLM. Every new changed entity is then mapped back to the latent embedding space using the encoder ED. The utility function is optimized by the EAGLE agent through a reward signal to stir the changes to areas of high utility in the latent embedding space. A final description is then returned after H steps.", "description": "This figure compares the ELM and EAGLE methods for generating descriptions of novel entities. ELM uses a decoder to map from an optimal point in the latent embedding space to a description, whereas EAGLE uses an RL agent to iteratively steer an LLM's generation towards that optimal point.", "section": "Surfacing Optimal Entities Using LLMs"}, {"figure_path": "WSu1PPi2UP/figures/figures_9_1.jpg", "caption": "Figure 2: An illustration of different forms of coverage and bias of an action space. As actions are textual prompts, their corresponding embedding directions may either provide good coverage, or partial coverage of the underlying embedding manifold (e.g., there may be directions that are not covered by the generated actions). Moreover, actions may be uniformly biased toward specific directions. We accommodate for this using a G-optimal design which reduces this bias through a set of exploratory actions in embedding space.", "description": "This figure illustrates different scenarios of action space coverage and bias in an embedding space.  The leftmost panel depicts a scenario with good coverage where the action space spans multiple directions in the latent space. The next panel shows poor coverage, concentrating only in a small area of the space. The middle panel exhibits biased coverage, where actions cluster in specific directions. The rightmost panel shows reduced bias, implying improved and more balanced coverage of the latent space, achieved by using a G-optimal design to select the actions.", "section": "Action Space Design"}, {"figure_path": "WSu1PPi2UP/figures/figures_16_1.jpg", "caption": "Figure 1: An illustration comparing the creation of descriptions of novel entities using ELM [Tennenholtz et al., 2024] vs. EAGLE (ours). The latent embedding space Z is illustrated as a complex surface on the right. Red points on the surface are illustrated as latent embeddings of existing entities. Black points are used to illustrate hypothetical (i.e., non-existing) entities. In ELM, a utility is maximized in latent embedding space to identify an optimal point. This hypothetical embedding is then decoded back to ambient space X to form a description of the hypothetical entity. Conversely, the EAGLE agent utilizes a highly capable pre-trained LLM as an environment to search for novel entities in ambient space X. EAGLE does not use a decoder, but rather only requires an encoder ED: X \u2192 Z. More specifically, the EAGLE agent uses action prompts to change an existing entity using the environment LLM. Every new changed entity is then mapped back to the latent embedding space using the encoder ED. The utility function is optimized by the EAGLE agent through a reward signal to stir the changes to areas of high utility in the latent embedding space. A final description is then returned after H steps.", "description": "This figure illustrates the difference between ELM and EAGLE in generating descriptions of novel entities.  ELM uses a pre-trained decoder to map from an optimal point in the latent embedding space to a description in the ambient space.  EAGLE uses a pre-trained LLM as an environment and an RL agent iteratively to steer LLM generations toward better regions in the latent embedding space based on some predefined criteria. The latent space is visualized as a 3D surface with existing entities shown as red points and hypothetical entities as black points.", "section": "Surfacing Optimal Entities Using LLMs"}, {"figure_path": "WSu1PPi2UP/figures/figures_17_1.jpg", "caption": "Figure 1: An illustration comparing the creation of descriptions of novel entities using ELM [Tennenholtz et al., 2024] vs. EAGLE (ours). The latent embedding space Z is illustrated as a complex surface on the right. Red points on the surface are illustrated as latent embeddings of existing entities. Black points are used to illustrate hypothetical (i.e., non-existing) entities. In ELM, a utility is maximized in latent embedding space to identify an optimal point. This hypothetical embedding is then decoded back to ambient space X to form a description of the hypothetical entity. Conversely, the EAGLE agent utilizes a highly capable pre-trained LLM as an environment to search for novel entities in ambient space X. EAGLE does not use a decoder, but rather only requires an encoder ED: X \u2192 Z. More specifically, the EAGLE agent uses action prompts to change an existing entity using the environment LLM. Every new changed entity is then mapped back to the latent embedding space using the encoder ED. The utility function is optimized by the EAGLE agent through a reward signal to stir the changes to areas of high utility in the latent embedding space. A final description is then returned after H steps.", "description": "This figure compares two methods for generating descriptions of novel entities: ELM and EAGLE.  ELM uses a pre-trained decoder to map from latent space to ambient space, maximizing a utility function in latent space.  EAGLE uses a pre-trained LLM as an environment and reinforcement learning to iteratively refine an entity's description, using an encoder to map from ambient space to latent space. The latent embedding space is shown as a 3D surface; red points represent existing entities, and black points represent hypothetical entities.", "section": "4 Surfacing Optimal Entities Using LLMs"}, {"figure_path": "WSu1PPi2UP/figures/figures_18_1.jpg", "caption": "Figure 1: An illustration comparing the creation of descriptions of novel entities using ELM [Tennenholtz et al., 2024] vs. EAGLE (ours). The latent embedding space Z is illustrated as a complex surface on the right. Red points on the surface are illustrated as latent embeddings of existing entities. Black points are used to illustrate hypothetical (i.e., non-existing) entities. In ELM, a utility is maximized in latent embedding space to identify an optimal point. This hypothetical embedding is then decoded back to ambient space X to form a description of the hypothetical entity. Conversely, the EAGLE agent utilizes a highly capable pre-trained LLM as an environment to search for novel entities in ambient space X. EAGLE does not use a decoder, but rather only requires an encoder ED: X \u2192 Z. More specifically, the EAGLE agent uses action prompts to change an existing entity using the environment LLM. Every new changed entity is then mapped back to the latent embedding space using the encoder ED. The utility function is optimized by the EAGLE agent through a reward signal to stir the changes to areas of high utility in the latent embedding space. A final description is then returned after H steps.", "description": "This figure compares the ELM and EAGLE methods for generating descriptions of novel entities.  ELM uses a pre-trained decoder to map an optimal point in latent embedding space to a description in ambient space.  EAGLE uses a pre-trained LLM as an environment and iteratively refines a description using RL until it reaches an optimal area in the latent embedding space. The figure visually represents the latent embedding space as a complex surface, highlighting existing and hypothetical entities.", "section": "4 Surfacing Optimal Entities Using LLMs"}, {"figure_path": "WSu1PPi2UP/figures/figures_19_1.jpg", "caption": "Figure 1: An illustration comparing the creation of descriptions of novel entities using ELM [Tennenholtz et al., 2024] vs. EAGLE (ours). The latent embedding space Z is illustrated as a complex surface on the right. Red points on the surface are illustrated as latent embeddings of existing entities. Black points are used to illustrate hypothetical (i.e., non-existing) entities. In ELM, a utility is maximized in latent embedding space to identify an optimal point. This hypothetical embedding is then decoded back to ambient space X to form a description of the hypothetical entity. Conversely, the EAGLE agent utilizes a highly capable pre-trained LLM as an environment to search for novel entities in ambient space X. EAGLE does not use a decoder, but rather only requires an encoder ED: X \u2192 Z. More specifically, the EAGLE agent uses action prompts to change an existing entity using the environment LLM. Every new changed entity is then mapped back to the latent embedding space using the encoder ED. The utility function is optimized by the EAGLE agent through a reward signal to stir the changes to areas of high utility in the latent embedding space. A final description is then returned after H steps.", "description": "This figure illustrates the difference between the ELM and EAGLE methods for generating descriptions of novel entities.  ELM uses a pre-trained decoder to map an optimal point in latent embedding space to a description. In contrast, EAGLE uses a pre-trained LLM as an environment and iteratively refines a description through reinforcement learning, guided by an objective function defined in the latent embedding space.  The figure highlights the key difference: ELM requires a decoder while EAGLE leverages the LLM directly for generation, only requiring an encoder to map the generated descriptions to latent space.", "section": "4 Surfacing Optimal Entities Using LLMs"}, {"figure_path": "WSu1PPi2UP/figures/figures_21_1.jpg", "caption": "Figure 1: An illustration comparing the creation of descriptions of novel entities using ELM [Tennenholtz et al., 2024] vs. EAGLE (ours). The latent embedding space Z is illustrated as a complex surface on the right. Red points on the surface are illustrated as latent embeddings of existing entities. Black points are used to illustrate hypothetical (i.e., non-existing) entities. In ELM, a utility is maximized in latent embedding space to identify an optimal point. This hypothetical embedding is then decoded back to ambient space X to form a description of the hypothetical entity. Conversely, the EAGLE agent utilizes a highly capable pre-trained LLM as an environment to search for novel entities in ambient space X. EAGLE does not use a decoder, but rather only requires an encoder ED: X \u2192 Z. More specifically, the EAGLE agent uses action prompts to change an existing entity using the environment LLM. Every new changed entity is then mapped back to the latent embedding space using the encoder ED. The utility function is optimized by the EAGLE agent through a reward signal to stir the changes to areas of high utility in the latent embedding space. A final description is then returned after H steps.", "description": "This figure illustrates the difference between ELM and EAGLE in generating descriptions of novel entities.  ELM uses a pre-trained decoder to map an optimal latent embedding to a description. In contrast, EAGLE uses a pre-trained LLM as an environment and iteratively refines an existing entity's description through RL, guided by latent embeddings and a reward signal to maximize utility.", "section": "4 Surfacing Optimal Entities Using LLMs"}, {"figure_path": "WSu1PPi2UP/figures/figures_21_2.jpg", "caption": "Figure 1: An illustration comparing the creation of descriptions of novel entities using ELM [Tennenholtz et al., 2024] vs. EAGLE (ours). The latent embedding space Z is illustrated as a complex surface on the right. Red points on the surface are illustrated as latent embeddings of existing entities. Black points are used to illustrate hypothetical (i.e., non-existing) entities. In ELM, a utility is maximized in latent embedding space to identify an optimal point. This hypothetical embedding is then decoded back to ambient space X to form a description of the hypothetical entity. Conversely, the EAGLE agent utilizes a highly capable pre-trained LLM as an environment to search for novel entities in ambient space X. EAGLE does not use a decoder, but rather only requires an encoder ED: X \u2192 Z. More specifically, the EAGLE agent uses action prompts to change an existing entity using the environment LLM. Every new changed entity is then mapped back to the latent embedding space using the encoder ED. The utility function is optimized by the EAGLE agent through a reward signal to stir the changes to areas of high utility in the latent embedding space. A final description is then returned after H steps.", "description": "This figure compares the methods of ELM and EAGLE in generating descriptions for novel entities.  ELM uses a pre-trained decoder to map an optimal latent embedding to a description. Conversely, EAGLE iteratively uses a pre-trained LLM as an environment to generate descriptions, guided by reinforcement learning to maximize utility in the latent embedding space.", "section": "4 Surfacing Optimal Entities Using LLMs"}, {"figure_path": "WSu1PPi2UP/figures/figures_22_1.jpg", "caption": "Figure 1: An illustration comparing the creation of descriptions of novel entities using ELM [Tennenholtz et al., 2024] vs. EAGLE (ours). The latent embedding space Z is illustrated as a complex surface on the right. Red points on the surface are illustrated as latent embeddings of existing entities. Black points are used to illustrate hypothetical (i.e., non-existing) entities. In ELM, a utility is maximized in latent embedding space to identify an optimal point. This hypothetical embedding is then decoded back to ambient space X to form a description of the hypothetical entity. Conversely, the EAGLE agent utilizes a highly capable pre-trained LLM as an environment to search for novel entities in ambient space X. EAGLE does not use a decoder, but rather only requires an encoder ED: X \u2192 Z. More specifically, the EAGLE agent uses action prompts to change an existing entity using the environment LLM. Every new changed entity is then mapped back to the latent embedding space using the encoder ED. The utility function is optimized by the EAGLE agent through a reward signal to stir the changes to areas of high utility in the latent embedding space. A final description is then returned after H steps.", "description": "This figure compares and contrasts the ELM and EAGLE methods for generating descriptions of novel entities.  ELM uses a pre-trained decoder to map an optimal latent embedding to a description.  Conversely, EAGLE uses a pre-trained LLM as an environment and iteratively refines a description using reinforcement learning, guided by an objective function defined in the latent embedding space. The figure uses a 3D surface to represent the latent embedding space, illustrating how both methods aim to identify optimal points within that space, but use different approaches to do so.", "section": "4 Surfacing Optimal Entities Using LLMs"}, {"figure_path": "WSu1PPi2UP/figures/figures_42_1.jpg", "caption": "Figure 1: An illustration comparing the creation of descriptions of novel entities using ELM [Tennenholtz et al., 2024] vs. EAGLE (ours). The latent embedding space Z is illustrated as a complex surface on the right. Red points on the surface are illustrated as latent embeddings of existing entities. Black points are used to illustrate hypothetical (i.e., non-existing) entities. In ELM, a utility is maximized in latent embedding space to identify an optimal point. This hypothetical embedding is then decoded back to ambient space X to form a description of the hypothetical entity. Conversely, the EAGLE agent utilizes a highly capable pre-trained LLM as an environment to search for novel entities in ambient space X. EAGLE does not use a decoder, but rather only requires an encoder ED: X \u2192 Z. More specifically, the EAGLE agent uses action prompts to change an existing entity using the environment LLM. Every new changed entity is then mapped back to the latent embedding space using the encoder ED. The utility function is optimized by the EAGLE agent through a reward signal to stir the changes to areas of high utility in the latent embedding space. A final description is then returned after H steps.", "description": "This figure compares the two methods of generating novel entities, ELM and EAGLE.  ELM uses a decoder to translate an optimal point in the latent embedding space to a description in the ambient space. EAGLE uses a pre-trained LLM as an environment and iteratively refines a description using reinforcement learning, guided by the latent embedding space and a utility function.", "section": "4 Surfacing Optimal Entities Using LLMs"}]