[{"figure_path": "2GQeCbhxVy/tables/tables_8_1.jpg", "caption": "Table 1: We show the average Peak Signal-to-Noise Ratio (PSNR) and mean squared error (MSE) over 100 test images in the table below. We see that the Hellinger-based loss gives competitive performance relative to the adversarial regularizer and TV regularization, while the tuned adversarial regularizer slightly outperforms all methods. These promising results suggests that these new a-divergence based losses are potentially worth exploring from a practical perspective as well.", "description": "This table compares the performance of three different regularizers (Hellinger, Adversarial, and TV) on the MNIST denoising task, measured by PSNR and MSE.  The Adversarial regularizer is tested both with its original and a tuned hyperparameter.", "section": "3.1 Empirical comparison with adversarial regularization"}, {"figure_path": "2GQeCbhxVy/tables/tables_24_1.jpg", "caption": "Table 1: We show the average PSNR and MSE in recovering 100 test MNIST digits using regularizers trained via the adversarial loss. Each regularizer was parameterized by a convolutional neural network utilizing a single activation function from the following options: Leaky ReLU, ELU, Tanh, or GELU. The Leaky ReLU-based regularizer achieves the highest PSNR and lowest MSE.", "description": "This table presents the results of denoising experiments on the MNIST dataset using different activation functions in the neural network used to parameterize the regularizers.  The performance of each activation function (LeakyReLU, ELU, Tanh, GELU) is measured by Peak Signal-to-Noise Ratio (PSNR) and Mean Squared Error (MSE).  LeakyReLU shows superior performance compared to other activation functions.", "section": "3.1 Empirical comparison with adversarial regularization"}]