[{"type": "text", "text": "Dual-frame Fluid Motion Estimation with Test-time Optimization and Zero-divergence Loss ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yifei Zhang University of Chinese Academy of Sciences zhangyifei21a@mails.ucas.ac.cn ", "page_idx": 0}, {"type": "text", "text": "Huan-ang Gao AIR, Tsinghua University gha24@mails.tsinghua.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Zhou Jiang Beijing Institute of Technology jzian@bit.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Hao Zhao AIR, Tsinghua University Beijing Academy of Artificial Intelligence zhaohao@air.tsinghua.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "3D particle tracking velocimetry (PTV) is a key technique for analyzing turbulent flow, one of the most challenging computational problems of our century. At the core of 3D PTV is the dual-frame fluid motion estimation algorithm, which tracks particles across two consecutive frames. Recently, deep learning-based methods have achieved impressive accuracy in dual-frame fluid motion estimation; however, they exploit a supervised scheme that heavily depends on large volumes of labeled data. In this paper, we introduce a new method that is completely self-supervised and notably outperforms its supervised counterparts while requiring only $1\\%$ of the training samples (without labels) used by previous methods. Our method features a novel zero-divergence loss that is specific to the domain of turbulent flow. Inspired by the success of splat operation in high-dimensional filtering and random fields, we propose a splat-based implementation for this loss which is both efficient and effective. The self-supervised nature of our method naturally supports test-time optimization, leading to the development of a tailored Dynamic Velocimetry Enhancer (DVE) module. We demonstrate that strong cross-domain robustness is achieved through test-time optimization on unseen leave-one-out synthetic domains and real physical/biological domains. Code, data and models are available at https://github.com/Forrest-110/FluidMotionNet. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Measuring and understanding turbulent fluid flow is a crucial problem as it is ubiquitous in various aspects of our lives, both in nature [27; 37; 20] and within our engineered society [28; 81; 35; 26; 69; 101; 68; 93]. Throughout history, flow visualization techniques have played a vital role in quantifying and analyzing turbulent flow [4; 60; 16; 76]. Among existing flow visualization techniques, 3D particle tracking velocimetry (3D PTV), which tracks individual particles between consecutive frames, distinguishes itself with high spatial resolution and precise measurement of velocity vectors [32; 3]. Before the advent of deep learning, PTV algorithms primarily focus on designing hand-crafted features for particle matching [62; 12; 99]. With the onset of deep learning, deep neural networks (like DeepPTV [50] and GotFlow3D [53]) have been introduced to solve this task. The core algorithm of 3D PTV is dual-frame fluid motion estimation, as illustrated in Fig. 1. ", "page_idx": 0}, {"type": "text", "text": "However, it is important to note that existing state-of-the-art (SOTA) dual-frame fluid motion estimation algorithms (shown in left two panels of Fig. 1) have a limitation: they require a large amount of fully annotated data. It is known that deep learning generally requires a substantial amount of in-domain data for optimal results. This demand poses a significant challenge to the AI4Science field, especially in 3D PTV where collecting suitable data is complicated due to the need for precisely selected tracer particles, tailored illumination, and camera settings [63]. Additionally, certain scenarios like flow fields under unique geometric conditions or cytoplasmic flows in disease contexts are rare, making it nearly impossible to compile a comprehensive dataset. ", "page_idx": 0}, {"type": "image", "img_path": "WOBhJs9gqU/tmp/1bfddc1d4ee58c415897f08ef725204093f6c6cba1ecc23c0a31981353e44b7c.jpg", "img_caption": ["Figure 1: Paradigm Shift: Given two frames of flow particles $X_{t}$ and $X_{t+\\Delta t}$ , DeepPTV [50] adopts a two-stage network for large- and small-scale motion refinement. GotFlow3D [53] trains a correspondence learning network and an RNN-based residual prediction network. They are trained in a fully supervised manner with annotated data and do not support test-time optimization. Our purely self-supervised method diverges from these approaches and employs DVE (see Sec. 3.3) for on-the-fly test-time optimization. The \"Snowflake\" denotes frozen weights. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To alleviate the aforementioned challenge, in this paper, we introduce a novel purely self-supervised framework with test-time optimization designed specifically for dual-frame fluid motion estimation in the 3D PTV process, as highlighted in the right panel of Fig. 1. Concerning the intrinsic difficulties associated with PTV data collection, especially in specialized contexts [6; 74; 64], we consider working under a limited size of dataset, as little as $1\\%$ typically used by existing fully-supervised methods (notably without accessing labels). Fluid particles have special physical properties, for which we resort to the inherent zero-divergence principle of incompressible fluid velocity fields and design a novel zero-divergence self-supervised loss tailored for fluid. Per implementation, we introduce the successful idea of splat in high-dimensional filtering [2] and random fields [38] and design a splat-based zero-divergence loss that is both efficient and effective. ", "page_idx": 1}, {"type": "text", "text": "Moreover, since our method is self-supervised, it naturally supports test-time optimization. Thus we introduce a module termed Dynamic Velocimetry Enhancer (DVE), shown in the right panel of Fig. 1, which optimizes the initial predicted flow during test-time based on the specific input data on the fly, ensuring an improved level of accuracy across various testing scenarios. This is critical for cross-domain robustness. The difficulty in collecting diverse PTV data leads to the common practice of using synthetic datasets. However, since synthetic data is generated based on hand-crafted priors, it cannot accurately represent specific real-world distributions, resulting in models that lack the necessary cross-domain robustness for practical applications. ", "page_idx": 1}, {"type": "text", "text": "Through comprehensive experiments, we demonstrate that our purely self-supervised framework (right panel of Fig. 1) significantly outperforms its fully-supervised counterparts (left two panels of Fig. 1), even under data-constrained conditions (using as low as $1\\%$ data). Additionally, our cross-domain robustness analyses confirm the framework\u2019s intrinsic ability to generalize to unseen domains, including leave-one-out synthetic domains and real-world physical/biological domains, underscoring the practical utility of our approach for real-world 3D PTV applications. ", "page_idx": 1}, {"type": "text", "text": "To summarize, our main contributions are: 1. A novel self-supervised framework with test-time optimization for dual-frame fluid motion estimation, surpassing fully-supervised methods with minimal samples (as low as $1\\%$ ). 2. A splat-based zero-divergence self-supervised loss for fluid dynamics, which is both efficient and effective. 3. A test-time optimization module named Dynamic Velocimetry Enhancer (DVE) that significantly improves cross-domain robustness. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Test-time Optimization and Test-time Domain Adaptation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Test-time optimization, also known as test-time refinement (TTR), exploits the inherent structure of data in a self-supervised manner without requiring ground truth labels [14; 80; 24; 21]. Applications of TTR include point cloud registration [25], depth estimation [7; 9], object recognition [80; 89], human motion capture [86], and segmentation with user feedback [73; 77; 31]. Test-time domain adaptation (TTA), a specific form of TTR, adapts a model trained on a source domain to a new target domain using an unsupervised loss function based on the target distribution [90; 54; 104]. One significant challenge in TTR is achieving per-sample adaptation at test time without compromising inference efficiency. Recent studies [85; 67] have explored using generative models to enable efficient test-time adaptation. In this work, we introduce our DVE module (Sec. 3.3), which conducts test-time optimization but maintains efficiency when compared with prior methods. ", "page_idx": 2}, {"type": "text", "text": "2.2 Learning-based Scene Flow Estimation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We include this section because our research is closely related to point-based, learning-driven scene flow estimation from point clouds\u2014a key component in understanding scenes through point clouds [8; 84; 43; 22]. Both areas of study concentrate on learning flows or correspondences from two frames of data [97; 102; 103; 100; 55]. Advances in scene flow estimation have been driven by benchmarks such as KITTI Scene Flow [59] and FlyingThings3D [58]. Drawing from the related field of optical flow [15; 30; 79; 82], recent developments in scene flow estimation utilize methods including encoder-decoder architectures [23; 57], multi-scale representations [11; 44; 94], recurrent modules [36; 83; 92], and other strategies [42; 70]. ", "page_idx": 2}, {"type": "text", "text": "Self-supervision and Test-time Optimization for Scene Flow. Self-supervised learning has received attention for scene flow estimation from point cloud data [95; 61; 5; 40; 46; 75; 45] and monocular images [29; 10; 105]. PointPwcNet [95] introduces cycle consistency loss, inspiring Mittal et al. [61] to incorporate it with nearest neighbor loss for establishing point cloud correspondence. This method also employs Chamfer Distance [19], smoothness constraints, and Laplacian regularization for selfsupervision. SLIM [5] addresses self-supervised scene flow estimation and motion segmentation simultaneously. Flowstep3d [36] uses a soft point matching module for pairwise point correspondence. Self-supervision naturally supports test-time optimization. Pontes et al. [66] eschew model training for real-time optimization by minimizing the graph Laplacian over source points to enforce rigid flow. Li et al. [48] replace the explicit graph with a neural prior using a coordinate-based MLP to implicitly regularize the flow field. SCOOP [40] combines pre-training on a subset of data to learn soft correspondences and secures initial flows with optimization-based refinement steps. ", "page_idx": 2}, {"type": "text", "text": "Our work is distinct from these scene flow methods as our data source is a specific domain: flow particles. Fluid particles differ from typical scene flow point clouds due to their disordered local distribution [51] (see Sec. 3.2.1) and unique physical properties. We propose a graph-based feature extractor and zero-divergence regularization to leverage these properties (see Sec. 3.2). ", "page_idx": 2}, {"type": "text", "text": "2.3 Particle Tracking Velocimetry (PTV) ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Particle tracking is a fundamental tool in turbulence analysis, progressing from traditional methods like streak photography [18] to advanced techniques such as Laser Speckle Photography (LSV) [65]. This evolution establishes the foundation for Particle Tracking Velocimetry (PTV). PTV gains prominence with the development of automatic tracking algorithms [1], which represents a significant advancement over manual methods [13]. Modern PTV calculates velocities by matching particle pairs between frames [1] and has been applied widely across various fields such as materials science, hydrodynamics, biomedical research, and environmental science [28; 35; 101; 27]. ", "page_idx": 2}, {"type": "text", "text": "Deep Learning Methods of Dual-frame Fluid Motion Estimation in PTV. Before the advent of deep learning, PTV algorithms primarily focused on improving particle matching by considering group particle movement [62], using multiple time step data [12], or conducting spatial area segmentation [99]. With the onset of deep learning, deep neural networks have been designed for particle motion estimation from point cloud pairs [57; 50; 53; 70; 96; 92]. Among them, DeepPTV [50] and GotFlow3D [53] are specifically tailored for fluid flow learning and in a fully supervised manner, as demonstrated in Fig. 1. Our work follows these prior efforts, aiming to develop a data-efficient and cross-domain robust motion estimation technique through self-supervision and test-time optimization. ", "page_idx": 2}, {"type": "image", "img_path": "WOBhJs9gqU/tmp/af03050f827725a4341df2f43a68452c5e74c7277d805e3fb70d5c369e14fe52.jpg", "img_caption": ["Figure 2: Upper: Training Phase. First, we (a) use input point clouds to construct graphs, which are then passed through a trainable (b) feature extractor, and we solve a (c) optimal transport problem using self-supervised loss terms including (g) reconstruction loss, (f) smooth loss, and (e) zerodivergence loss for initial flow estimation. Lower: (h) Test-Time DVE. With the initial flow estimate $\\mathbf{F}_{\\mathrm{init}}$ , we optimize a residual $\\mathbf{R}$ to generate the final flow $\\mathbf{F}$ using another reconstruction loss $(\\mathrm{g}^{\\ast})$ . "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "3 Methods ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "3.1 Problem Formulation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To elucidate the architecture and functionality of our proposed method for dual-frame fluid motion estimation, we outline the problem as follows: The method processes two consecutive, unstructured sets of 3D particles, $\\mathbf{X_{t}}\\in\\mathbf{\\dot{R}}^{n_{1}\\times3}$ and $\\mathbf{X_{t+\\Delta}}\\mathbf{\\Delta}\\in\\mathbb{R}^{n_{2}\\times3}$ , recorded at times $\\mathbf{t}$ and $\\mathbf{t}+\\Delta\\mathbf{t}$ . It outputs the predicted flow motion $\\mathbf{F}\\in\\mathbb{R}^{n_{1}\\times3}$ , mapping each particle $\\mathbf{x}_{i}$ from $\\mathbf{X_{t}}$ to a vector $\\mathbf{f}_{i}$ that indicates its movement between the two frames, capturing the flow dynamics in the turbulent 3D environment. ", "page_idx": 3}, {"type": "text", "text": "3.2 Training with Fewer Samples ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In the training phase, we aim to learn the patterns of fluid flow using considerably fewer samples, as low as $1\\%$ of what conventional approaches require, given the inherent difficulties in gathering data for specific scientific domains. We design the network as depicted at the top of Fig. 2 to train a graph-based feature extractor (Fig. 2b) that extracts per-point features for the following soft point matching. These features initialize the flow between the point clouds using the optimal transport module (Fig. 2c), and we employ self-supervision losses, as shown in Fig. 2(e,f,g), for training. However, fluid particles exhibit complex motion features compared to typical LiDAR point clouds (Sec. 3.2.1), which complicates feature learning under self-supervision with limited data. Consequently, we employ a strong graph-based feature extractor (Sec. 3.2.2) and propose a novel zero-divergence loss (Sec. 3.2.4.3.1) tailored to address these challenges. ", "page_idx": 3}, {"type": "text", "text": "3.2.1 Complexity of Fluid Flow. A common assumption in LiDAR scene flow estimation is the smoothness of flow. However, this is not enough for fluid particles due to their unique geometric distribution, as shown in the left of Fig. 3. The fluid velocity field is smooth only at a coarse scale but remains complex at a fine local scale. Therefore, we need a strong relation-based graph feature extractor and more specific regularization to capture the intricate properties of fluid particles. ", "page_idx": 3}, {"type": "text", "text": "3.2.2 Graph-based Feature Extractor. Point cloud-based extractors, including PointNet [71] and PointNet+ $^+$ [72], are commonly used in LiDAR scene flow estimation [40]. While these extractors effectively discern broader spatial structures, their capability to grasp intricate local relationships, which is vital for analyzing fluid dynamics, can be inadequate. In contrast, graph-based feature extractors excel at capturing local patterns by considering the relationships between proximate nodes, or in our context, particles. Hence, drawing inspiration from GotFlow3D [53], we opt for a graph-based feature extraction backbone, as depicted in Fig. 2b. Initially, we construct a static nearest-neighbor graph from the input point cloud. This graph is then processed through several GeoSetConv layers [72] to form a high-dimensional geometric local feature. To further enrich the feature, we construct a dynamic graph using EdgeConv [91] based on the high-dimensional feature, forming a GNN that outputs static-dynamic features. The dynamic graph expands the receptive field and focuses on geometric feature properties. Further details can be found in Appendix A.1.1. ", "page_idx": 3}, {"type": "image", "img_path": "WOBhJs9gqU/tmp/e71816b3aa4b09aef1bbce5c95df9c141922612598fba21a9ecf0dade2dd05d3.jpg", "img_caption": ["Figure 3: LEFT: A visualization of fluid flow in Fluidflow3D data. RIGHT: The divergence loss in our training phase is obtained by splatting the original sparse flow to grid points and then minimizing the divergence loss on the resulting grid points. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "3.2.3 Solving Optimal Transport for Soft Correspondence. With the static-dynamic feature from the feature extractor (Sec. 3.2.2), we formulate the correspondence linking problem through the framework of optimal transport [87], where a higher transport cost between two points indicates a lower similarity within the extracted feature space. The optimal transport plan yields the soft correspondence weight between $\\mathbf{X_{t}}$ and $\\mathbf{X_{t+\\Delta\\Delta{\\Deltat}}}$ , as shown in Fig. 2c, which can be used to formulate an initial flow estimate $\\mathbf{F}_{\\mathrm{init}}$ . This follows the common scene flow method, thus we leave the details to Appendix A.2. ", "page_idx": 4}, {"type": "text", "text": "3.2.4 Self-supervised Losses. Since manually linking particles between sets is notably intricate, we advocate for the adoption of self-supervised losses (Fig. 2e-g). ", "page_idx": 4}, {"type": "text", "text": "3.2.4.1. Reconstruction Loss. A core principle guiding self-supervised flow learning is the fact that $\\mathbf{X_{t}}+\\mathbf{F}$ and $\\mathbf{X_{t+\\Delta\\Delta{t}}}$ should be similar. The Chamfer distance (CD) is a standard metric used to measure the shape dissimilarity between point clouds in point cloud completion. Therefore, we adopt it as our reconstruction loss. We also add a regularization term [40] to prevent degeneration: ", "page_idx": 4}, {"type": "equation", "text": "$$\nL_{\\mathrm{recon}}=\\frac{1}{|\\mathcal{Y}^{\\prime}|}\\sum_{\\mathbf{y}_{i}^{\\prime}\\in\\mathcal{Y}^{\\prime}}p_{i}\\operatorname*{min}_{\\mathbf{y}_{j}\\in\\mathcal{Y}}||\\mathbf{y}_{i}^{\\prime}-\\mathbf{y}_{j}||_{2}^{2}+\\lambda_{\\mathrm{conf}}\\frac{1}{|\\mathcal{Y}^{\\prime}|}\\sum_{\\mathbf{y}_{i}^{\\prime}\\in\\mathcal{Y}^{\\prime}}(1-p_{i})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here, $\\mathcal{V}^{\\prime}$ represents the estimated point cloud formed by $\\mathbf{X}_{\\mathbf{t}}+\\mathbf{F}_{\\mathrm{init}}$ , and $\\boldsymbol{\\wp}$ is the target point cloud formed by $\\mathbf{X_{t+\\Delta\\Delta{t}}}$ . $p_{i}$ denotes the confidence of matching in the Optimal Transport (Sec. 3.2.3), which is the weighted sum of transport costs. $\\lambda_{\\mathrm{conf}}$ term is used to avoid the trivial solution $p_{i}=0$ . ", "page_idx": 4}, {"type": "text", "text": "3.2.4.2. Smooth Loss. Given the infinitely differentiable characteristic of the velocity field, it is postulated that the field should exhibit a certain level of continuous and smooth transitions (at a coarse scale). In light of this theoretical underpinning, we introduce a smooth regularization loss to enforce and maintain this continuous behavior in the velocity field, which is defined as, ", "page_idx": 4}, {"type": "equation", "text": "$$\nL_{\\mathrm{smooth}}=\\sum_{\\mathbf{x}_{i}\\in\\mathcal{X}}\\sum_{k\\in\\mathcal{N}_{l}(\\mathbf{x}_{i})}\\frac{||\\mathbf{f}_{i}-\\mathbf{f}_{k}||_{1}}{|\\mathcal{X}||\\mathcal{N}(\\mathbf{x}_{i})|},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here, $\\mathcal{X}$ represents the point cloud formed by $\\mathbf{X_{t}}$ . $\\mathcal{N}_{l}(\\mathbf{x}_{i})$ represents the index set of the $l$ closest points to $\\mathbf{x}_{i}$ . $\\mathbf{f}_{i}$ and $\\mathbf{f}_{k}$ denote the estimated flow vectors at points $\\mathbf{x}_{i}$ and $\\mathbf{x}_{k}$ , respectively. ", "page_idx": 4}, {"type": "text", "text": "3.2.4.3.1 Zero-divergence Loss. Smooth Loss is not enough for fluid particles, as mentioned in Sec. 3.2.1. Concerning the intrinsic properties of the velocity field, we note that incompressible fluids exhibit zero divergence by definition. Moreover, compressible fluids can also be approximated as incompressible under conditions like low Mach numbers, justifying this in many engineering contexts. Hence, we introduce a zero-divergence regularization, which also compensates for the shortcomings of Smooth Loss, as we will show later. ", "page_idx": 4}, {"type": "text", "text": "3.2.4.3.2 Splat-based Implementation. Splatting, first used in high-dimensional Gaussian filtering [2], embeds input values in a high-dimensional space. Studies like [38] and [78] followed this ", "page_idx": 4}, {"type": "text", "text": "Splat-Blur-Slice pipeline. Inspired by these, we implemented a splat-based zero-divergence loss: to compute divergence, we need the partial derivative of the field. The irregular arrangement of particles in 3D complicates this. Thus, we propose \"splatting\" unstructured flow estimates onto a uniform 3D grid, then applying zero-divergence regularization at these grid points, as shown in the right of Fig. 3. In formal terms, the dense grid is denoted by $(s j,s k,s l)^{\\breve{T}}$ , with $j,k,l\\in\\mathbb{Z}$ indicating the 3D indices of the grid point. The parameter $s$ corresponds to the grid\u2019s spacing. Then, given a grid point $\\mathbf{x}=(s j,s k,s\\bar{l})^{T}$ , we employ the inverse squared distance as interpolation weights to approximate the flow at that particular point, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{f}(\\mathbf{x})={\\frac{1}{|N(\\mathbf{x})|}}\\sum_{\\mathbf{x}_{i}\\in N(\\mathbf{x})}{\\frac{\\mathbf{f}_{i}}{\\|\\mathbf{x}_{i}-\\mathbf{x}\\|_{2}^{2}+\\epsilon}}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathbf{f}_{i}$ is the estimated flow value at point $\\mathbf{x}_{i}$ . $N(\\mathbf{x})$ denotes the neighborhood among the point set of $\\mathbf{X_{t}}$ for grid point $\\mathbf{x}$ . The parameter $\\epsilon$ is introduced to maintain numerical stability. By employing splatting, we convert the variable particle distance into fixed grid spacing, thus achieving efficiency and effectiveness. ", "page_idx": 5}, {"type": "text", "text": "3.2.4.3.3 Divergence Calculation. Once Splatting has been employed, the divergence at that point,   \nissp ea cuifniietd  vbeyc $\\mathbf{x}=(s j,s k,s l)^{T}$ -, tcha en nbtrey .d eFfiinnaeldl ya, st:h $\\begin{array}{r}{(\\nabla\\cdot\\mathbf{F})(\\mathbf{x})=\\sum_{k=1}^{3}\\frac{\\mathbf{f}(\\mathbf{x}+s u_{k})-\\mathbf{f}(\\mathbf{x}-s u_{k})}{2s}}\\end{array}$ 3 f(x+suk)\u2212f(x\u2212suk), where uk formula $k$   \nas, $\\begin{array}{r}{L_{\\mathrm{div}}=\\frac{1}{J K L}\\sum_{j=0}^{J-1}\\sum_{k=0}^{K-1}\\sum_{l=0}^{L-\\bar{1}}\\big\\|\\big(\\nabla\\cdot\\bar{\\mathbf{F}}\\big)((s j,s k,s l)^{T})\\big\\|_{1}}\\end{array}$ , where $J,K$ , and $L$ represent the   \nnumber of grid points along the respective dimensions. ", "page_idx": 5}, {"type": "text", "text": "3.2.4.3.4 Zero-Divergence Loss v.s. Smooth Loss Zero-Divergence loss is similar to Smooth loss in that it computes spatial gradients and requires the norm of the gradient to be small, essentially penalizing the case that neighboring flow vectors are totally irrelevant. However, Smooth regularization is too strict for fluid particles. While the divergence constraint only requires the total divergence to be zero, it does not necessitate that any two vectors be oriented in the same direction, thus allowing for locally complex particle dynamics. In practice, we set the neighborhood set size for calculating Smooth Loss to be much larger than that for calculating Zero-Divergence Loss, because smoothness is a more coarse-scale regularization. Finally, we note that Zero-Divergence Loss is calculated along three specific axes, whereas Smooth Loss is not. Therefore, using the same method (KNN) for calculating Zero-Divergence Loss as for Smooth Loss is not efficient. ", "page_idx": 5}, {"type": "text", "text": "To summarize, our final self-supervised training loss is ", "page_idx": 5}, {"type": "equation", "text": "$$\nL_{\\mathrm{train}}=L_{\\mathrm{recon}}+\\lambda_{\\mathrm{smooth}}L_{\\mathrm{smooth}}+\\lambda_{\\mathrm{div}}L_{\\mathrm{div}}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "3.3 Efficient Test-time Optimization with Dynamic Velocimetry Enhancer ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "As shown at the bottom of Fig. 2, with the initial flow estimate from the trained network, we introduce a novel Dynamic Velocimetry Enhancer (DVE) module during the test phase for test-time optimization. This provides added flexibility to accommodate unseen situations and address potential inaccuracies arising from the limited training data context, which will be demonstrated in Sec. 4.4. In principle, our approach seeks a residual flow vector $\\mathbf{R}$ such that $\\mathbf{F}=\\mathbf{F}_{\\mathrm{init}}+\\mathbf{R}$ , which can be optimized to rectify the inaccuracies. Formally, DVE is essentially an optimization process using the $L_{\\mathrm{recon}}$ objective function, with the formulation as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{R}^{*}=\\operatorname*{argmin}_{\\mathbf{R}\\in\\mathbb{R}^{|y^{\\prime}|\\times3}}\\left(\\frac{1}{|\\mathcal{V}^{\\prime}|}\\sum_{\\mathbf{y}_{i}^{\\prime}\\in\\mathcal{Y}^{\\prime}}p_{i}\\operatorname*{min}_{\\mathbf{y}_{j}\\in\\mathcal{Y}}\\left\\|\\mathbf{y}_{i}^{\\prime}+\\mathbf{R}_{i}-\\mathbf{y}_{j}\\right\\|_{2}^{2}\\right)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This test-time supervision (Fig. $2(\\mathrm{g^{*}}))$ is similar to $L_{\\mathrm{recon}}1$ without the regularization $\\lambda_{\\mathrm{conf}}$ . Solved using an Adam optimizer, it only involves parameters from an $n_{1}\\times3$ matrix. Concerning that existing test-time optimization modules [47] are slow, DVE is very efficient, as demonstrated later in Sec. 4.1. ", "page_idx": 5}, {"type": "text", "text": "Selection of Self-supervised Losses During the Test Phase We omit both $L_{\\mathrm{smooth}}$ and $L_{\\mathrm{div}}$ during the test stage. During the training phase, our objective is to embed prior knowledge about particle flow into the network. $L_{\\mathrm{smooth}}$ and $L_{\\mathrm{div}}$ serve not only to foster a comprehensive understanding of fluid behaviors but also function as regularizers, mitigating overfitting caused by the unconstrained reconstruction loss. However, in the test phase, our focus shifts to specific sparse particle sets. ", "page_idx": 5}, {"type": "text", "text": "In certain scenarios, such as when flows adhere to boundary conditions, these particles may not strictly adhere to the expected norms of ideal smoothness or zero-divergence typical in a flow field. Additionally, given that the initial flow estimate should be sufficiently accurate, regularizers become unnecessary. Our approach to customizing the loss functions in this manner aims to enhance the robustness of our model against the complex challenges encountered in real-world applications, thereby improving data efficiency and cross-domain robustness. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We conduct comprehensive evaluations using different data domains on our proposed framework. First, we compare our method with SOTA fully supervised methods (Sec. 4.1). Next, we examine its performance under the constrained size of training data, reflecting real-world situations where domain-specific data is limited (Sec. 4.2). We then assess the framework\u2019s performance under different domains with increasing domain shift, highlighting its cross-domain robustness (Sec. 4.3). Additionally, we conduct comprehensive ablation studies on the components of our framework (Sec. 4.4) to validate their effects. Following the previous SOTA method GotFlow3D [52], our datasets include FluidFlow3D [52] and its six fluid cases, DeformationFlow [98] and AVIC [41]. Due to page limit, experimental settings including implementation details, datasets, and evaluation metrics can be found in Appendix A.3. ", "page_idx": 6}, {"type": "text", "text": "4.1 Comparison with state-of-the-art methods ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Since DeepPTV[50] is not open-sourced, we enrich the comparison by including scene-flow methods [56; 70; 96; 92] as our baselines. We benchmark our method against established fully supervised models, such as FlowNet3D [57], FLOT [70], PointPWC-Net [96], PV-RAFT [92], and GotFlow3D [53], all utilizing the FluidFlow3D training set. All baseline models are evaluated using the default hyperparameters. As shown in Figure 4, our purely self-supervised approach outperforms all the fully supervised baselines. Additionally, we introduce Ours $(I\\%)$ \u2014our method trained on just $1\\%$ of the data\u2014which still demonstrates comparable performance. ", "page_idx": 6}, {"type": "text", "text": "Comparison Across Flow Cases: We further analyze our framework\u2019s performance across six distinct flow cases from the FluidFlow3D dataset, with details available in Appendix A.3.1. For three representative cases\u2014Uniform Flow, Turbulent Channel Flow, and Forced MHD Turbulence\u2014we present detailed results in Fig. 4, while the remaining are documented in Appendix A.4.1. In the simple Uniform Flow case, our method shows slight improvement over the baseline. However, in more complex scenarios, such as Forced MHD Turbulence, our method significantly outperforms the baseline, reducing the EPE/NEPE metric by nearly half compared to the SOTA GotFlow3D. ", "page_idx": 6}, {"type": "text", "text": "Test-time Efficiency: In Figure 4, the $T_{\\mathrm{test}}$ column in the table illustrates the time consumption of each method during the test phase. Our method demonstrates time efficiency, incurring less inference time cost than even baseline supervised methods (without test-time optimization). Our method requires only a few epochs to converge (See Appendix A.5.3). Furthermore, our network\u2019s relatively small size (refer to $P_{\\mathrm{train}}$ comparison in Figure 4) facilitates a rapid forward pass. Time profiling is conducted on a single RTX 3090 Ti. ", "page_idx": 6}, {"type": "text", "text": "4.2 Training with Limited Data ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Handling rare scenarios, such as unique geometric flow fields [88] or cytoplasmic flows in disease contexts, presents challenges in assembling large datasets. To address this, we explore an evaluation setup with limited training data (still without labels) by randomly sampling from the FluidFlow3D training dataset. We established three distinct training settings: a $100\\%$ sampling rate (13,621 samples), a $10\\%$ sampling rate (1,300 samples), and a $1\\%$ sampling rate (130 samples). Testing was conducted on the FluidFlow3D test data (see Appendix A.3.1). We compared our method with FLOT [70], PV-RAFT [92], and the current SOTA GotFlow3D [53]. We present the results of the major metric, EPE, with further details in Appendix A.4.2. Fig. 5(a) illustrates the robustness of our approach to reductions in training data size. Our metrics remain stable even with significant decreases in training samples, while other methods show substantial performance declines. This disparity becomes more pronounced in complex cases, as discussed below. ", "page_idx": 6}, {"type": "table", "img_path": "WOBhJs9gqU/tmp/c716abbc3789d99c51e2c155f48728d0071bb620bd68e34f87162c7eeabc282a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "WOBhJs9gqU/tmp/1236a1784e3bc46c659c209e2bde578cd270aa6fe4ac442b9badaadef4d46fb8.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 4: (Top) Benchmarking Against Fully Supervised Methods. $P_{\\mathrm{train}}$ signifies the count of trainable parameters. $T_{\\mathrm{test}}$ stands for inference time for each sample. The best results are marked in bold. (Bottom) Performance Across Flow Cases. The best results are marked in bold, with the runners-up underlined. The subplots on the right visualize these three cases. The warmer color indicates a higher flow speed. All models are trained on full data, except Ours $(1\\%)$ . ", "page_idx": 7}, {"type": "text", "text": "Performance Drop Across Flow Cases: We further tested our method with limited training data on different flow cases from the FluidFlow3D test data mentioned above. The performance drop associated with limited data is shown in Fig. 5(b). As illustrated, complex flow cases such as Forced Isotropic Turbulence are more susceptible to limited data, while simpler flow cases like Uniform Flow and Turbulent Channel Flow maintain stable EPE as the training data decreases. ", "page_idx": 7}, {"type": "text", "text": "4.3 Analysis of Robustness Across Different Domains ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Natural fluids exhibit a range of behaviors, including convection and laminar flow. Gathering data under all possible conditions presents significant challenges. To address this, we examine the crossdomain knowledge transfer capability of our proposed method. We explore a gradual increase in domain shift: initially, we investigate fluid case domain shifts (Sec. 4.3.1), where we train on five ", "page_idx": 7}, {"type": "image", "img_path": "WOBhJs9gqU/tmp/0a5f33191c90efff1aa2bce491aa659e530119f6d0987f083fca4f07f004413b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 5: (a) Leave-one-out domain EPE Comparison: \"Flow Cases\" stands for the flow case we test on with the model trained on the rest five cases. (b) Comparison of EPE with Limited Training Data. (c) Performance Drop related to Limited Training Data. The Y-axis shows the major matric EPE, and the X-axis indicates the percentage of the training dataset utilized. ", "page_idx": 8}, {"type": "image", "img_path": "WOBhJs9gqU/tmp/2f1c90718887c5715c842a50a1cf89e0a75e755c76b08642964706d6cfa8424a.jpg", "img_caption": ["Figure 6: (a) DeformationFlow data. (b) Initial estimation by our method. (c) Time-consumption comparison between SerialTrack and $O u r s{+}S T$ . \"PerIt\" denotes time per PTV iteration. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "certain fluid cases and test on the leave-one-out domain within the same dataset. Next, we examine the Sim2Real domain shift (Sec. 4.3.2), where training occurs on a synthetic fluid dataset and testing on real-world fluid data. Lastly, we assess a more extensive Sim2Real domain shift (Sec. 4.3.3) by training on synthetic physical fluid data and testing on biological datasets. ", "page_idx": 8}, {"type": "text", "text": "4.3.1 Testing within the Same Synthetic Fluid Dataset: In this section, we employ the complete FluidFlow3D training set in a six-fold cross-validation setup, training on five sub-cases and testing on the remaining one. We benchmark our method solely against the state-of-the-art fluid motion learning method, GotFlow3D [53], as other baselines are not fluid-specific. The EPE metric results, shown in Fig.5(c) (with additional results in Appendix A.4.3), indicate that our method outperforms GotFlow3D in various scenarios, especially in complex conditions like MHD and isotropic turbulence. Moreover, our method shows consistent performance, highlighting its robustness in unfamiliar scenarios. ", "page_idx": 8}, {"type": "text", "text": "Sim2Real Experimental Setting: Synthetic data with ground-truth labels often serves as a benchmark for method evaluation. However, the domain gap between synthetic and real data can negatively impact performance, underscoring the importance of validation on real-world datasets. In this and the following section, we validate our method using two real-world datasets, DeformationFlow [98] and Aortic Valve Interstitial Cell (AVIC) [41] (details in Appendix A.3.1), to demonstrate its practical application potential. Two challenges in real-world evaluation are: 1) the lack of ground-truth labels in real-world data, and 2) the requirement for a complete PTV method for particle tracking application. Therefore, we use the following setting: our method, trained on FluidFlow3D, is integrated into PTV algorithms (specified below) to provide initial motion estimates. Due to the absence of ground truth, we primarily demonstrate the generalizability of our method across various domains through qualitative results. Quantitatively, we emphasize efficiency in the physical domain, where multiple frames are involved. By contrast, in the biological domain, we focus on validating the plausibility of our estimates, especially given the significant cell deformation, where efficiency is less critical. ", "page_idx": 8}, {"type": "text", "text": "Table 1: Comparison on AVIC data. C2E, C2N, E2N stands for 3 settings: Cyto-D treatment to Endo-1 treatment, Cyto-D treatment to Normal and Endo-1 treatment to Normal. MNDS stands for the mean neighbor distance score. ", "page_idx": 9}, {"type": "table", "img_path": "WOBhJs9gqU/tmp/9095247f63a7cc5a05f8350a00236fb7dee66459ea137dea2fd0cd478f3304bd.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "4.3.2 Testing from Synthetic to Real-World Fluid Data: We employ SerialTrack (ST) [98] as the PTV framework and designate our integrated version as $O u r s{+}S T$ . Quantitative results highlight the advantages of our method: it identifies 22,882 matches, exceeding the 22,001 particles tracked by vanilla SerialTrack, and significantly reduces tracking time by providing accurate initial estimates that expedite match finding. Results in Fig. 6 showcase the initial estimations and time efficiency of $O u r s{+}S T$ . These outcomes affirm that our method delivers sufficiently precise estimations to improve PTV, demonstrating strong simulation-to-reality (Sim2Real) capabilities. ", "page_idx": 9}, {"type": "text", "text": "4.3.3 Testing from Synthetic Physical Fluids to Biological Data: This study analyzes a dataset of AVIC images embedded in a PEG hydrogel, using microspheres to track hydrogel movements. AVICs were subjected to three conditions: regular, Cyto-D exposure, and Endo 1 treatment. Cell deformation, challenging to observe directly, was quantified by tracking nearby particles with Fmtrack [41], using our framework for initialization. Results are presented in Tab.1, and a visualization of the cell deformation we estimate is in AppendixA.4.4. We evaluate particle movement using the neighbor distance score (See Appendix A.3.2), with higher scores indicating less accurate estimations. Our results in Tab. 1 show $O u r s{+}F m$ slightly outperforming Fm-track. Notably, our method, trained exclusively on the FluidFlow3D Dataset, demonstrates strong adaptability across domains and provides insights into biological fluid dynamics. ", "page_idx": 9}, {"type": "text", "text": "4.4 Ablation study on different modules. ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In addition to the aforementioned assessments, an ablation study regarding different proposed modules including different feature extractors, Zero-divergence Loss, and DVE is performed to demonstrate their effectiveness. We show our method w/ and w/o Div Loss (Zero-Divergence Loss) and DVE module in Fig. 4. The full results are illustrated in Appendix A.5. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we introduce a test-time self-supervised framework for learning 3D fluid motion from dual-frame unstructured particle sets. We address the challenge of improving data efficiency and ensuring cross-domain robustness, which are crucial for practical applications. We demonstrate the viability of our approach through two real-world studies and suggest that our findings could inform further research into extensive real-world applications, the exploration of constraints specific to particular scenarios, and the development of novel model architectures for enhanced adaptability. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] AA Adamczyk and L Rimai. 2-dimensional particle tracking velocimetry (ptv): technique and image processing algorithms. Experiments in fluids, 6(6):373\u2013380, 1988.   \n[2] Andrew Adams, Jongmin Baek, and Myers Abraham Davis. Fast high-dimensional filtering using the permutohedral lattice. In Computer graphics forum, volume 29, pages 753\u2013762. Wiley Online Library, 2010.   \n[3] RJ Adrian. Dynamic ranges of velocity and spatial resolution of particle image velocimetry. Measurement Science and Technology, 8(12):1393, 1997.   \n[4] Ronald J Adrian. Particle-imaging techniques for experimental fluid mechanics. Annual review of fluid mechanics, 23(1):261\u2013304, 1991.   \n[5] Stefan Baur, David Emmerichs, Frank Moosmann, Peter Pinggera, Bjorn Ommer, and Andreas Geiger. Slim: Self-supervised lidar scene flow and motion segmentation. In International Conference on Computer Vision (ICCV), 2021.   \n[6] Giuseppe CA Caridi, Elena Torta, Valentina Mazzi, Claudio Chiastra, Alberto L Audenino, Umberto Morbiducci, and Diego Gallo. Smartphone-based particle image velocimetry for cardiovascular flows applications: A focus on coronary arteries. Frontiers in Bioengineering and Biotechnology, 10:1011806, 2022.   \n[7] Vincent Casser, Soeren Pirk, Reza Mahjourian, and Anelia Angelova. Depth prediction without the sensors: Leveraging structure for unsupervised learning from monocular videos. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pages 8001\u20138008, 2019.   \n[8] Xiaoxue Chen, Hao Zhao, Guyue Zhou, and Ya-Qin Zhang. Pq-transformer: Jointly parsing 3d objects and layouts from point clouds. IEEE Robotics and Automation Letters, 7(2):2519\u20132526, 2022.   \n[9] Yuhua Chen, Cordelia Schmid, and Cristian Sminchisescu. Self-supervised learning with geometric constraints in monocular video: Connecting flow, depth, and camera. In Proceedings of the IEEE/CVF international conference on computer vision, pages 7063\u20137072, 2019.   \n[10] Yuhua Chen, Cordelia Schmid, and Cristian Sminchisescu. Self-supervised learning with geometric constraints in monocular video: Connecting flow, depth, and camera. pages 7062\u2013 7071, 10 2019. doi: 10.1109/ICCV.2019.00716.   \n[11] Wencan Cheng and Jong Hwan Ko. Bi-pointflownet: Bidirectional learning for point cloud based scene flow estimation. In Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XXVIII, pages 108\u2013124. Springer, 2022.   \n[12] Christian Cierpka, Benjamin L\u00fctke, and Christian J K\u00e4hler. Higher order multi-frame particle tracking velocimetry. Experiments in Fluids, 54:1\u201312, 2013.   \n[13] Paul E Dimotakis, Francois D Debussy, and Manoochehr M Koochesfahani. Particle streak velocity field measurements in a two-dimensional mixing layer. The Physics of Fluids, 24(6): 995\u2013999, 1981.   \n[14] Carl Doersch, Yi Yang, Mel Vecerik, Dilara Gokay, Ankush Gupta, Yusuf Aytar, Joao Carreira, and Andrew Zisserman. Tapir: Tracking any point with per-frame initialization and temporal refinement. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10061\u201310072, 2023.   \n[15] Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip Hausser, Caner Hazirbas, Vladimir Golkov, Patrick Van Der Smagt, Daniel Cremers, and Thomas Brox. Flownet: Learning optical flow with convolutional networks. In Proceedings of the IEEE international conference on computer vision, pages 2758\u20132766, 2015.   \n[16] TD Dudderar and PG Simpkins. Laser speckle photography in a fluid medium. Nature, 270 (5632):45\u201347, 1977.   \n[17] C Ross Ethier and DA Steinman. Exact fully 3d navier\u2013stokes solutions for benchmarking. International Journal for Numerical Methods in Fluids, 19(5):369\u2013375, 1994.   \n[18] Arthur Fage and Hubert CH Townend. An examination of turbulent flow with an ultramicroscope. Proceedings of the Royal Society of London. Series A, Containing Papers of a Mathematical and Physical Character, 135(828):656\u2013677, 1932.   \n[19] Haoqiang Fan, Hao Su, and Leonidas J Guibas. A point set generation network for 3d object reconstruction from a single image. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 605\u2013613, 2017.   \n[20] Behrooz Ferdowsi, Carlos P Ortiz, Morgane Houssais, and Douglas J Jerolmack. River-bed armouring as a granular segregation phenomenon. Nature communications, 8(1):1363, 2017.   \n[21] Yossi Gandelsman, Yu Sun, Xinlei Chen, and Alexei Efros. Test-time training with masked autoencoders. Advances in Neural Information Processing Systems, 35:29374\u201329385, 2022.   \n[22] Huan-ang Gao, Beiwen Tian, Pengfei Li, Xiaoxue Chen, Hao Zhao, Guyue Zhou, Yurong Chen, and Hongbin Zha. From semi-supervised to omni-supervised room layout estimation using point clouds. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 2803\u20132810. IEEE, 2023.   \n[23] Xiuye Gu, Yijie Wang, Chongruo Wu, Yong Jae Lee, and Panqu Wang. Hplflownet: Hierarchical permutohedral lattice flownet for scene flow estimation on large-scale point clouds. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 3254\u20133263, 2019.   \n[24] Moritz Hardt and Yu Sun. Test-time training on nearest neighbors for large language models. arXiv preprint arXiv:2305.18466, 2023.   \n[25] Ahmed Hatem, Yiming Qian, and Yang Wang. Point-tta: Test-time adaptation for point cloud registration using multitask meta-auxiliary learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 16494\u201316504, 2023.   \n[26] Su Min Hoi, Ean Hin Ooi, Irene Mei Leng Chew, and Ji Jinn Foo. Sptv sheds light on flow dynamics of fractal-induced turbulence over a plate-fin array forced convection. Scientific Reports, 12(1):76, 2022.   \n[27] Jiarong Hong, Mostafa Toloui, Leonardo P Chamorro, Michele Guala, Kevin Howard, Sean Riley, James Tucker, and Fotis Sotiropoulos. Natural snowfall reveals large-scale flow structures in the wake of a 2.5-mw wind turbine. Nature communications, 5(1):4216, 2014.   \n[28] Pinshane Y Huang, Simon Kurasch, Jonathan S Alden, Ashivni Shekhawat, Alexander A Alemi, Paul L McEuen, James P Sethna, Ute Kaiser, and David A Muller. Imaging atomic rearrangements in two-dimensional silica glass: watching silica\u2019s dance. science, 342(6155): 224\u2013227, 2013.   \n[29] Junhwa Hur and Stefan Roth. Self-supervised monocular scene flow estimation. In CVPR, 2020.   \n[30] Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper, Alexey Dosovitskiy, and Thomas Brox. Flownet 2.0: Evolution of optical flow estimation with deep networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2462\u20132470, 2017.   \n[31] Won-Dong Jang and Chang-Su Kim. Interactive image segmentation via backpropagating refinement scheme. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5297\u20135306, 2019.   \n[32] Christian J K\u00e4hler, Sven Scharnowski, and Christian Cierpka. On the resolution limit of digital particle image velocimetry. Experiments in fluids, 52:1629\u20131639, 2012.   \n[33] Alex Khang, Andrea Gonzalez Rodriguez, Megan E Schroeder, Jacob Sansom, Emma Lejeune, Kristi S Anseth, and Michael S Sacks. Quantifying heart valve interstitial cell contractile state using highly tunable poly (ethylene glycol) hydrogels. Acta biomaterialia, 96:354\u2013367, 2019.   \n[34] Ali Rahimi Khojasteh, Sylvain Laizet, Dominique Heitz, and Yin Yang. Lagrangian and eulerian dataset of the wake downstream of a smooth cylinder at a reynolds number equal to 3900. Data in brief, 40:107725, 2022.   \n[35] Mirae Kim, Daniel Schanz, Matteo Novara, Philipp Godbersen, Eunseop Yeom, and Andreas Schr\u00f6der. Experimental study on flow and turbulence characteristics of jet impinging on cylinder using three-dimensional lagrangian particle tracking velocimetry. Scientific Reports, 13(1):10929, 2023.   \n[36] Yair Kittenplon, Yonina C Eldar, and Dan Raviv. Flowstep3d: Model unrolling for selfsupervised scene flow estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4114\u20134123, 2021.   \n[37] Artur Kopitca, Kourosh Latif,i and Quan Zhou. Programmable assembly of particles on a chladni plate. Science advances, 7(39):eabi7716, 2021.   \n[38] Philipp Kr\u00e4henb\u00fchl and Vladlen Koltun. Efficient inference in fully connected crfs with gaussian edge potentials. Advances in neural information processing systems, 24, 2011.   \n[39] Itai Lang, Dror Aiger, Forrester Cole, Shai Avidan, and Michael Rubinstein. Scoop: Self-supervised correspondence and optimization-based scene flow. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5281\u20135290, 2023.   \n[40] Itai Lang, Dror Aiger, Forrester Cole, Shai Avidan, and Michael Rubinstein. SCOOP: SelfSupervised Correspondence and Optimization-Based Scene Flow. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023.   \n[41] Emma Lejeune, Alex Khang, Jacob Sansom, and Michael S Sacks. Fm-track: A fiducial marker tracking software for studying cell mechanics in a three-dimensional environment. SoftwareX, 11:100417, 2020.   \n[42] Bing Li, Cheng Zheng, Silvio Giancola, and Bernard Ghanem. Sctn: Sparse convolutiontransformer network for scene flow estimation. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 1254\u20131262, 2022.   \n[43] Pengfei Li, Ruowen Zhao, Yongliang Shi, Hao Zhao, Jirui Yuan, Guyue Zhou, and Ya-Qin Zhang. Lode: Locally conditioned eikonal implicit scene completion from sparse lidar. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 8269\u20138276. IEEE, 2023.   \n[44] Ruibo Li, Guosheng Lin, Tong He, Fayao Liu, and Chunhua Shen. Hcrf-flow: Scene flow from point clouds with continuous high-order crfs and position-aware flow embedding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 364\u2013373, 2021.   \n[45] Ruibo Li, Guosheng Lin, and Lihua Xie. Self-point-flow: Self-supervised scene flow estimation from point clouds with optimal transport and random walk. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 15577\u201315586, June 2021.   \n[46] Ruibo Li, Chi Zhang, Guosheng Lin, Zhe Wang, and Chunhua Shen. Rigidflow: Selfsupervised scene flow learning on point clouds by local rigidity prior. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 16959\u2013 16968, June 2022.   \n[47] Xueqian Li, Jhony Kaesemodel Pontes, and Simon Lucey. Neural scene flow prior. Advances in Neural Information Processing Systems, 34:7838\u20137851, 2021.   \n[48] Xueqian Li, Jhony Kaesemodel Pontes, and Simon Lucey. Neural scene flow prior. Advances in Neural Information Processing Systems, 34:7838\u20137851, 2021.   \n[49] Yi Li, Eric Perlman, Minping Wan, Yunke Yang, Charles Meneveau, Randal Burns, Shiyi Chen, Alexander Szalay, and Gregory Eyink. A public turbulence database cluster and applications to study lagrangian evolution of velocity increments in turbulence. Journal of Turbulence, (9): N31, 2008.   \n[50] Jiaming Liang, Shengze Cai, Chao Xu, Tehuan Chen, and Jian Chu. Deepptv: particle tracking velocimetry for complex flow motion via deep neural networks. IEEE Transactions on Instrumentation and Measurement, 71:1\u201316, 2021.   \n[51] Jiaming Liang, Shengze Cai, Chao Xu, Tehuan Chen, and Jian Chu. Deepptv: Particle tracking velocimetry for complex flow motion via deep neural networks. IEEE Transactions on Instrumentation and Measurement, 71:1\u201316, 2022. doi: 10.1109/TIM.2021.3120127.   \n[52] Jiaming Liang, Chao Xu, and Shengze Cai. Gotflow3d: recurrent graph optimal transport for learning 3d flow motion in particle tracking. arXiv preprint arXiv:2210.17012, 2022.   \n[53] Jiaming Liang, Chao Xu, and Shengze Cai. Recurrent graph optimal transport for learning 3d flow motion in particle tracking. Nature Machine Intelligence, pages 1\u201313, 2023.   \n[54] Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation. In International conference on machine learning, pages 6028\u20136039. PMLR, 2020.   \n[55] Yancong Lin and Holger Caesar. Icp-flow: Lidar scene flow estimation with icp. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15501\u2013 15511, 2024.   \n[56] Xingyu Liu, Charles R. Qi, and Leonidas J. Guibas. Flownet3d: Learning scene flow in 3d point clouds, 2019.   \n[57] Xingyu Liu, Charles R Qi, and Leonidas J Guibas. Flownet3d: Learning scene flow in 3d point clouds. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 529\u2013537, 2019.   \n[58] Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fischer, Daniel Cremers, Alexey Dosovitskiy, and Thomas Brox. A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4040\u20134048, 2016.   \n[59] Moritz Menze and Andreas Geiger. Object scene flow for autonomous vehicles. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3061\u20133070, 2015.   \n[60] Roland Meynart. Instantaneous velocity field measurements in unsteady gas flow by speckle velocimetry. Applied optics, 22(4):535\u2013540, 1983.   \n[61] Himangi Mittal, Brian Okorn, and David Held. Just go with the flow: Self-supervised scene flow estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.   \n[62] Kazuo Ohmi and Sanjeeb Prasad Panday. Particle tracking velocimetry using the genetic algorithm. Journal of Visualization, 12:217\u2013232, 2009.   \n[63] Charles Pecora. Particle tracking velocimetry: A review. 2018.   \n[64] Raul Perianez. A particle-tracking model for simulating pollutant dispersion in the strait of gibraltar. Marine Pollution Bulletin, 49(7-8):613\u2013623, 2004.   \n[65] Christopher JD Pickering and Neil A Halliwell. Laser speckle photography and particle image velocimetry: photographic film noise. Applied optics, 23(17):2961\u20132969, 1984.   \n[66] Jhony Kaesemodel Pontes, James Hays, and Simon Lucey. Scene flow from point clouds with or without learning. In 2020 international conference on 3D vision (3DV), pages 261\u2013270. IEEE, 2020.   \n[67] Mihir Prabhudesai, Tsung-Wei Ke, Alexander Cong Li, Deepak Pathak, and Katerina Fragkiadaki. Diffusion-tta: Test-time adaptation of discriminative models via generative feedback. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[68] Soorya Pradeep and Thomas A Zangle. Quantitative phase velocimetry measures bulk intracellular transport of cell mass during the cell cycle. Scientific Reports, 12(1):6074, 2022.   \n[69] Horst Punzmann, Nicolas Francois, Hua Xia, Gregory Falkovich, and Michael Shats. Generation and reversal of surface flows by propagating waves. Nature Physics, 10(9):658\u2013663, 2014.   \n[70] Gilles Puy, Alexandre Boulch, and Renaud Marlet. Flot: Scene flow on point clouds guided by optimal transport. In European conference on computer vision, pages 527\u2013544. Springer, 2020.   \n[71] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 652\u2013660, 2017.   \n[72] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. Advances in neural information processing systems, 30, 2017.   \n[73] Tomas Sakinis, Fausto Milletari, Holger Roth, Panagiotis Korfiatis, Petro Kostandy, Kenneth Philbrick, Zeynettin Akkus, Ziyue Xu, Daguang Xu, and Bradley J Erickson. Interactive segmentation of medical images through fully convolutional neural networks. arXiv preprint arXiv:1903.08205, 2019.   \n[74] Torsten Seelig, Hartwig Deneke, Johannes Quaas, and Matthias Tesche. Life cycle of shallow marine cumulus clouds from geostationary satellite observations. Journal of Geophysical Research: Atmospheres, 126(22):e2021JD035577, 2021.   \n[75] Yaqi Shen, Le Hui, Jin Xie, and Jian Yang. Self-supervised 3d scene flow estimation guided by superpoints. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 5271\u20135280, June 2023.   \n[76] PG Simpkins and TD Dudderar. Laser speckle measurements of transient benard convection. Journal of Fluid Mechanics, 89(4):665\u2013671, 1978.   \n[77] Konstantin Sofiiuk, Ilia Petrov, Olga Barinova, and Anton Konushin. f-brs: Rethinking backpropagating refinement for interactive segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8623\u20138632, 2020.   \n[78] Hang Su, Varun Jampani, Deqing Sun, Subhransu Maji, Evangelos Kalogerakis, Ming-Hsuan Yang, and Jan Kautz. Splatnet: Sparse lattice networks for point cloud processing. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2530\u2013 2539, 2018.   \n[79] Deqing Sun, Xiaodong Yang, Ming-Yu Liu, and Jan Kautz. Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 8934\u20138943, 2018.   \n[80] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self-supervision for generalization under distribution shifts. In International conference on machine learning, pages 9229\u20139248. PMLR, 2020.   \n[81] Alexandra M Tayar, Fernando Caballero, Trevor Anderberg, Omar A Saleh, M Cristina Marchetti, and Zvonimir Dogic. Controlling liquid\u2013liquid phase behaviour with an active fluid. Nature Materials, pages 1\u20138, 2023.   \n[82] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part II 16, pages 402\u2013419. Springer, 2020.   \n[83] Zachary Teed and Jia Deng. Raft-3d: Scene flow using rigid-motion embeddings. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8375\u20138384, 2021.   \n[84] Beiwen Tian, Liyi Luo, Hao Zhao, and Guyue Zhou. Vibus: Data-efficient 3d scene parsing with viewpoint bottleneck and uncertainty-spectrum modeling. ISPRS Journal of Photogrammetry and Remote Sensing, 194:302\u2013318, 2022.   \n[85] Yun-Yun Tsai, Fu-Chen Chen, Albert YC Chen, Junfeng Yang, Che-Chun Su, Min Sun, and Cheng-Hao Kuo. Gda: Generalized diffusion for robust test-time adaptation. arXiv preprint arXiv:2404.00095, 2024. [86] Hsiao-Yu Tung, Hsiao-Wei Tung, Ersin Yumer, and Katerina Fragkiadaki. Self-supervised learning of motion capture. In Advances in Neural Information Processing Systems, pages 5236\u20135246, 2017. [87] C\u00e9dric Villani et al. Optimal transport: old and new, volume 338. Springer, 2009. [88] BX Wang, JH Du, and XF Peng. Internal natural, forced and mixed convection in fluidsaturated porous medium. Trans Phenom Porous Media, pages 357\u2013382, 1998.   \n[89] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Fully test-time adaptation by entropy minimization. arXiv preprint arXiv:2006.10726, 2020.   \n[90] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. arXiv preprint arXiv:2006.10726, 2020.   \n[91] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma, Michael M Bronstein, and Justin M Solomon. Dynamic graph cnn for learning on point clouds. ACM Transactions on Graphics (tog), 38(5):1\u201312, 2019. [92] Yi Wei, Ziyi Wang, Yongming Rao, Jiwen Lu, and Jie Zhou. Pv-raft: Point-voxel correlation fields for scene flow estimation of point clouds. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 6954\u20136963, 2021. [93] Freda Werdiger, Martin Donnelley, Stephen Dubsky, Rhiannon P Murrie, Richard P Carnibella, Chaminda R Samarage, Ying Y How, Graeme R Zosky, Andreas Fouras, David W Parsons, et al. Quantification of muco-obstructive lung disease variability in mice via laboratory x-ray velocimetry. Scientific Reports, 10(1):10859, 2020. [94] Wenxuan Wu, Zhiyuan Wang, Zhuwen Li, Wei Liu, and Li Fuxin. Pointpwc-net: A coarseto-fine network for supervised and self-supervised scene flow estimation on 3d point clouds. arXiv preprint arXiv:1911.12408, 2019.   \n[95] Wenxuan Wu, Zhi Yuan Wang, Zhuwen Li, Wei Liu, and Li Fuxin. Pointpwc-net: Cost volume on point clouds for (self-) supervised scene flow estimation. In European Conference on Computer Vision, pages 88\u2013107. Springer, 2020.   \n[96] Wenxuan Wu, Zhi Yuan Wang, Zhuwen Li, Wei Liu, and Li Fuxin. Pointpwc-net: Cost volume on point clouds for (self-) supervised scene flow estimation. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part V 16, pages 88\u2013107. Springer, 2020.   \n[97] Xin Wu, Hao Zhao, Shunkai Li, Yingdian Cao, and Hongbin Zha. Sc-wls: Towards interpretable feed-forward camera re-localization. In European Conference on Computer Vision, pages 585\u2013601. Springer, 2022.   \n[98] Jin Yang, Yue Yin, Alexander K. Landauer, Selda Buyuktozturk, Jing Zhang, Luke Summey, Alexander McGhee, Matt K. Fu, John O. Dabiri, and Christian Franck. SerialTrack: ScalE and Rotation Invariant Augmented Lagrangian Particle Tracking. 2022. doi: 10.48550/ARXIV. 2203.12573. URL https://arxiv.org/abs/2203.12573.   \n[99] Yang Zhang, Yuan Wang, Bin Yang, and Wenbo He. A particle tracking velocimetry algorithm based on the voronoi diagram. Measurement Science and Technology, 26(7):075302, 2015.   \n[100] Yifei Zhang, Hao Zhao, Hongyang Li, and Siheng Chen. Fastmac: Stochastic spectral sampling of correspondence graph. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17857\u201317867, 2024.   \n[101] Zeng Zhang, Misun Hwang, Todd J Kilbaugh, Anush Sridharan, and Joseph Katz. Cerebral microcirculation mapped by echo particle tracking velocimetry quantifies the intracranial pressure and detects ischemia. Nature communications, 13(1):666, 2022.   \n[102] Chengliang Zhong, Peixing You, Xiaoxue Chen, Hao Zhao, Fuchun Sun, Guyue Zhou, Xiaodong Mu, Chuang Gan, and Wenbing Huang. Snake: Shape-aware neural 3d keypoint field. Advances in Neural Information Processing Systems, 35:7052\u20137064, 2022.   \n[103] Chengliang Zhong, Yuhang Zheng, Yupeng Zheng, Hao Zhao, Li Yi, Xiaodong Mu, Ling Wang, Pengfei Li, Guyue Zhou, Chao Yang, et al. 3d implicit transporter for temporally consistent keypoint discovery. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3869\u20133880, 2023.   \n[104] Tao Zhong, Zhixiang Chi, Li Gu, Yang Wang, Yuanhao Yu, and Jin Tang. Meta-dmoe: Adapting to domain shift by meta-distillation from mixture-of-experts. Advances in Neural Information Processing Systems, 35:22243\u201322257, 2022.   \n[105] Yuliang Zou, Zelun Luo, and Jia-Bin Huang. Df-net: Unsupervised joint learning of depth and flow using cross-task consistency. In European Conference on Computer Vision, 2018. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "A.1 Methods in Detail ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "A.1.1 Feature extractor ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We draw significant inspiration from GotFlow3D\u2019s feature extractor [52] to obtain a robust representation of particle features. We briefly describe the structure of the feature extractor here. ", "page_idx": 17}, {"type": "text", "text": "The feature extractor is based on a graph neural network. Initially, a static graph $G_{\\mathrm{static}}=(V_{s},E_{s})$ is established in the 3D spatial space for the input point cloud. $V_{s}$ contains the given input points, while $E_{s}$ consists of connections between the $\\boldsymbol{\\mathrm{k}}$ -nearest neighbors of each point. This process yields the original point cloud feature $f_{i}$ for point $i$ as a hexadecimal set, including the three-dimensional coordinates, the radial distance, the azimuthal angle, and the polar angle. After processing through several GeoSetConv layers [72], we obtain a high-dimensional geometric local feature: ", "page_idx": 17}, {"type": "equation", "text": "$$\nF_{i}^{n}=\\mathrm{MaxPooling}_{j\\in N_{k}(i)}\\{\\Phi^{n}(f_{i},F_{j}^{n-1}-F_{i}^{n-1})\\},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $F_{i}^{n}$ denotes the feature extracted from the static graph at the $n$ -th layer for point $i$ , and $N_{k}(i)$ represents its $\\boldsymbol{\\mathrm{k}}$ -nearest neighbors in the static graph. $\\Phi^{n}$ stands for the $n$ -th GeoSetConv layer. $F_{i}^{0}$ is initialized by the 3D coordinates of the input point cloud. ", "page_idx": 17}, {"type": "text", "text": "Subsequently, the dynamic graph is generated. It takes these high-dimensional features $F_{i}$ as inputs and, following the same steps as the construction of the static graph, seeks the nearest neighbors based on these features to establish connections, thus obtaining the local structure on the feature manifold. However, the distinction lies in the fact that the dynamic graph possesses a greater receptive range since it accepts high-dimensional features. Moreover, it is reconstructed during every training session, whereas a static graph is constructed only once. ", "page_idx": 17}, {"type": "text", "text": "The dynamic feature is obtained in the same manner as $F_{i}$ , with the sole difference being the use of EdgeConv layers [91] instead of GeoSetConv layers. The final feature is obtained by concatenating all hierarchical features from different layers of both static and dynamic graphs. ", "page_idx": 17}, {"type": "text", "text": "A.2 Optimal Transport for Soft Correspondence ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Let $\\Phi_{\\mathcal{X}}$ and $\\Phi_{\\mathcal{Y}}$ represent the features extracted from two input particle sets, $\\mathcal{X}$ and $\\boldsymbol{\\wp}$ , respectively. We initiate by computing the point-to-point similarity matrix, given by: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbf{S}_{i,j}=\\frac{\\Phi_{\\mathbf{x}_{i}}\\cdot\\Phi_{\\mathbf{y}_{j}}^{T}}{\\Vert\\Phi_{\\mathbf{x}_{i}}\\Vert_{2}\\Vert\\Phi_{\\mathbf{y}_{j}}\\Vert_{2}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "In this context, $\\Phi_{\\mathbf{x}_{i}}$ pertains to the feature of the $i$ -th point in $\\Phi_{X}$ , and analogously for $\\Phi_{\\mathbf{y}}^{j}$ . ", "page_idx": 17}, {"type": "text", "text": "Inspired by pioneering research, we formulate the correspondence linking problem through the framework of optimal transport [39]. Assigning a mass o f|X1| to each source point xi, we consider its transport to the target point $\\mathbf{y}_{j}$ with the cost matrix defined by ${\\bf C}_{i,j}=1-{\\bf S}_{i,j}$ . In this context, a higher cost indicates a lower similarity between two points within the feature space. Our objective is to identify the optimal transport plan $\\mathbf{T}^{*}$ that satisfies, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{T}^{*}=\\arg\\underset{\\mathbf T\\in\\mathbb R_{+}^{n}}{\\operatorname*{min}}\\Bigg[\\sum_{i,j}\\mathbf T_{i,j}\\mathbf C_{i,j}+\\epsilon\\displaystyle\\sum_{i,j}\\mathbf T_{i,j}(\\log\\mathbf T_{i,j}-1)}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\lambda\\left(\\mathrm{KL}(\\mathbf T\\mathbf1,\\frac{1}{\\vert\\mathcal X\\vert}\\mathbf1)+\\mathrm{KL}(\\mathbf T^{T}\\mathbf1,\\frac{1}{\\vert\\mathcal X\\vert}\\mathbf1)\\right)\\Bigg],}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where KL denotes the Kullback-Leibler (KL) divergence, $\\mathbf{1}\\in\\mathbb{R}^{|\\mathcal{X}|\\times1}$ is a vector filled with ones, and the terms involving $\\epsilon$ and $\\lambda$ are regularizing terms with coefficients controlling their respective strengths. ", "page_idx": 17}, {"type": "text", "text": "The optimal transport plan $\\mathbf{T}^{*}$ yields the soft correspondence weight of point $x_{i}$ to point $y_{j}$ as: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbf{W}_{i,j}=\\frac{e^{\\mathbf{T}_{i,j}^{*}}}{\\sum_{k\\in\\mathcal{M}_{\\boldsymbol{y}}(\\mathbf{x}_{i})}e^{\\mathbf{T}_{i,k}^{*}}},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\mathcal{M}_{\\mathfrak{V}}(\\mathbf{x}_{i})$ represents the set of $L$ points from $\\boldsymbol{\\wp}$ corresponding to the top $L$ values of $\\mathbf{T}_{i,j}^{*}$ , and $L$ is a hyper-parameter that can be chosen for specific particle tracking case. Consequently, we can estimate the new location of a given $x_{i}$ , the estimated position is given by, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbf{y}_{i}^{*}=\\sum_{\\mathbf{y}_{j}\\in\\mathcal{Y}}W_{i,j}\\mathbf{y}_{j}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "And the confidence score $p_{i}$ , which quantifies the reliability of the estimated position $\\mathbf{y}_{i}^{*}$ for each point $\\mathbf{x}_{i}$ is given by, ", "page_idx": 18}, {"type": "equation", "text": "$$\np_{i}=\\operatorname*{max}\\left(\\sum_{k\\in\\mathcal{M}_{\\mathcal{Y}}(\\mathbf{x}_{i})}\\mathbf{W}_{i,k}\\mathbf{S}_{i,k},0\\right)\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Finally, the flow estimate $f_{i}$ is determined as, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbf{f}_{i}=\\mathbf{y}_{i}^{*}-\\mathbf{x}_{i}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "A.3 Experimental Setup ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "A.3.1 Datasets ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "FluidFlow3D Dataset The FluidFlow3D [53] is a large synthetic dataset designed for the study of 3D fluid flow. Specifically, it offers enough data for training and serves as a benchmark to evaluate the flow estimation capabilities of supervised 3D fluid flow motion learning techniques. This dataset utilizes physically accurate simulated flow structures sourced from public database [49], ensuring that the provided ground truth flows adhere to computational fluid dynamics principles. It encompasses six typical categories of flow cases, namely, uniform flow, isotropic turbulent flow, magneto-hydrodynamic (MHD) turbulence, fully developed turbulent channel flow, transitional boundary layer flow, and the Beltrami flow [17]. We utilize this synthetic dataset to evaluate our method, comparing it with baselines (see Sec. 4.1), training with restricted data capacity (see Sec. 4.2), or testing cross-domain transferability by training on and evaluating different turbulence types (see Sec. 4.3). It should be noted that, to our knowledge, this dataset is the only large-scale benchmark specifically designed for dual-frame fluid motion learning. Other fluid datasets, such as CylinderFlow[34], focus on fluid trajectories across multiple frames, requiring the integration of the full PTV process. This approach deviates from the main problem we aim to address. ", "page_idx": 18}, {"type": "text", "text": "DeformationFlow The DeformationFlow dataset [98] showcases real-world physical dynamics by recording the indentation created when a stainless steel sphere is placed on a soft polyacrylamide hydrogel due to gravitational forces. Fluorescent fluid particles within the gel were scanned both pre and post-indentation, yielding 3D volumetric images. We employ this dataset because it diverges significantly from the synthetic training set. Notably, the zero-divergence constraint is no longer applicable, making it an ideal candidate to evaluate the cross-domain generalization capability of our proposed test-time self-supervised framework. ", "page_idx": 18}, {"type": "text", "text": "Aortic Valve Interstitial Cell (AVIC) Dataset The AVIC dataset [41] represents real-world biological dynamics revealed through PTV analysis. In the dataset, AVICs, extracted from dissected porcine heart leaflets, were suspended in a peptide-modified PEG hydrogel with fluorescent microbeads and subsequently incubated for 72 hours. We employ this dataset to demonstrate the strong cross-domain versatility of our proposed framework with the data from the biological realm. ", "page_idx": 18}, {"type": "text", "text": "A.3.2 Evaluation Metrics ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "For a thorough evaluation of our framework\u2019s performance and its comparison with previous works, we utilize five prominent metrics commonly applied in particle tracking velocimetry evaluations: EPE, NEPE, Acc Strict, Acc Relax, and Outliers. ", "page_idx": 18}, {"type": "text", "text": "To introduce the metrics, we first define point error $e_{i}$ and the relative point error $e_{i}^{\\mathrm{rel}}$ ", "page_idx": 18}, {"type": "equation", "text": "$$\ne_{i}=\\|\\mathbf{f}_{i}^{*}-\\mathbf{f}_{i}^{\\mathrm{gt}}\\|_{2},\\quad e_{i}^{\\mathrm{rel}}=\\frac{e_{i}}{\\|\\mathbf{f}_{i}^{\\mathrm{gt}}\\|_{2}},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $e_{i}$ represents the Euclidean distance (L2 norm) between the predicted flow $\\mathbf{f}_{i}^{*}$ , and the groundtruth flow $\\mathbf{f}_{i}^{\\mathrm{gt}}$ , for a specific point $\\mathbf{x}_{i}$ . The relative error $e_{i}^{\\mathrm{rel}}$ provides a normalized measure, indicating the magnitude of the point error in relation to the magnitude of the ground-truth flow at point $\\mathbf{x}_{i}$ . ", "page_idx": 19}, {"type": "text", "text": "Then, the metrics used for evaluating the quality of predicted flows are defined as follows. The EPE (End Point Error), calculated as $\\begin{array}{r}{\\mathrm{EPE}=\\frac{1}{|\\mathcal{X}|}\\sum_{{\\bf x}_{i}\\in\\mathcal{X}}\\dot{e}_{i}}\\end{array}$ , representing the average point errors, and the NEPE (Normalized End Point Error), given by $\\begin{array}{r}{\\mathrm{NEPE}=\\frac{1}{|\\mathcal{X}|}\\sum_{\\mathbf{x}_{i}\\in\\mathcal{X}}e_{i}^{\\mathrm{rel}}}\\end{array}$ , reflecting the normalized average of point errors. Furthermore, the Acc Strict metric denotes the percentage of points with $e_{i}<0.05[m]$ or $e_{i}^{\\mathrm{rel}}<5\\%$ , while Acc Relax captures the points having $e_{i}<0.10[m]$ or $\\bar{e}_{i}^{\\mathrm{rel}}<10\\%$ . Lastly, Outliers indicates points where $e_{i}>0.30[m]$ or $e_{i}^{\\mathrm{rel}}>10\\%$ , signifying notable deviations between predictions and ground truth, a potential indicator of the model\u2019s challenges in generalizing across diverse data. ", "page_idx": 19}, {"type": "text", "text": "Furthermore, when performing experiments on real datasets, the absence of ground truth in actual data prompts us to incorporate additional metrics for evaluating our methodology. For the SerialTrack assessment, our evaluative metrics include Matches, Tracking ratio, UpdateNorm, and Time. We use $n_{\\mathrm{match}}$ to represent the matches identified between the source and target point clouds, and $n_{\\mathrm{{no}_{\\!}}}$ _missing signifies the count of source points that are matched in the target point cloud. Consequently, Matches is equivalent to $n_{\\mathrm{match}}$ , and Tracking ratio is expressed as the fractio n nmatch . The UpdateNorm metric captures the change in PTV parameters for each iteration, while Time measures the duration taken for a single iteration. For the $F m$ -track assessment, our primary metric is the neighbor distance score (NDS), delineated for a point $\\mathbf{x}_{i}$ as, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{NDS}_{i}=\\frac{1}{N}\\sum_{\\mathbf{x}_{j}\\in N(\\mathbf{x}_{i})}\\left\\|\\mathbf{f}_{i}-\\mathbf{f}_{j}\\right\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $N(\\mathbf{x}_{i})$ is the $\\mathbf{K}$ -nearest-neighbor set of point $\\mathbf{x}_{i},\\mathbf{f}_{i}$ denotes the flow vector at point $\\mathbf{x}_{i}$ . ", "page_idx": 19}, {"type": "text", "text": "A.3.3 Implementation Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In our method, there are some pre-defined hyperparameters. For the sake of experimental reproducibility, we list them below. ", "page_idx": 19}, {"type": "text", "text": "MODEL STRUCTURE For the feature extractor, the K-value of our K-nearest-neighbor is chosen to be 32, the embedding-dim to be 128, and the dropout rate to be 0.5. The grid size for splatting is $10\\times10\\times10$ . ", "page_idx": 19}, {"type": "text", "text": "LOSS TERM The selected number of neighboring points for the reconstruction loss $L_{\\mathrm{recon}}$ is 32. Likewise, the number of neighboring points for smooth flow loss $L_{\\mathrm{smooth}}$ is 32, and for zerodivergence loss $L_{\\mathrm{div}}$ is 2. $\\lambda_{\\mathrm{conf}}$ is 0.1, $\\lambda_{\\mathrm{smooth}}$ is 10, and $\\lambda_{\\mathrm{div}}$ is 0.1. ", "page_idx": 19}, {"type": "text", "text": "During the training phase, we utilize a mini-batch training process with a batch size of 4. To achieve convergence, we train the full-data model and $10\\%$ -data model for 100 epochs, and $1\\%$ -data model for 300 epochs. A default learning rate of 0.001 is set and the training is run on a single RTX 4070TI. ", "page_idx": 19}, {"type": "text", "text": "During the test phase, DVE runs for 150 steps with an update rate of 0.01. ", "page_idx": 19}, {"type": "text", "text": "A.4 Extended Results ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "A.4.1 Full Comparison with the state-of-the-art methods on different flow cases of FluidFlow3D dataset ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We present a comprehensive comparison with state-of-the-art methods across all flow cases in Table 2. Notably, our model, trained on only 130 samples, outperforms the current state-of-the-art, GotFlow3D, and is marked in italics. In particularly complex scenarios such as Forced MHD turbulence and Forced isotropic turbulence, our method significantly outperforms GotFlow3D, even when trained with only $1\\%$ of the training set samples. In more common scenarios, our model, trained with the full dataset, surpasses GotFlow3D in most metrics, while the model trained with limited samples still demonstrates commendable performance. ", "page_idx": 19}, {"type": "text", "text": "A.4.2 Full Results of the Training with Limited Data Experiment ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We present the full results of our limited-data training experiment here in Tab. 3. ", "page_idx": 19}, {"type": "table", "img_path": "WOBhJs9gqU/tmp/35b76668ab330f5553aeb27419969255ceebb7e6cedd1e84a38511d00f84a9b4.jpg", "table_caption": ["Table 2: This table illustrates the performance of our method relative to baseline methods, with the size of the training set indicated in parentheses after each technique\u20141300 corresponds to $10\\%$ of the full dataset, and 130 corresponds to $1\\%$ . The leading results are emphasized in bold, while the second-best ones are underlined. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "A.4.3 Full Results of the Cross-Domain Robustness Experiment within the Same Synthetic Fluid Dataset ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We present the full results of our cross-domain robustness experiment here in Fig. 7. Our technique consistently outperforms the state-of-the-art method GotFlow3D across different fluid scenarios in terms of accuracy. Furthermore, our method demonstrates a consistent performance across these scenarios, unlike the pronounced variability observed with GotFlow3D. This suggests that, after training on certain fluid cases, our model can proficiently handle scenarios it hasn\u2019t directly observed. This adaptability can be attributed to the DVE module, which allows for on-the-fly adjustments to unseen examples during testing. ", "page_idx": 20}, {"type": "text", "text": "A.4.4 Full results of the AVIC experiment ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Experimental Settings: In this work, we analyzed a dataset containing images of porcine aortic valve interstitial cells (AVICs) embedded in a polyethylene glycol (PEG) hydrogel. We employed $0.5\\ [\\mu m]$ microspheres as tracking particles to monitor the movements within the PEG hydrogel. For detailed information on AVIC encapsulation within PEG hydrogels, readers are referred to a previous study [33]. We subjected the AVICs to three distinct conditions: initially, under regular conditions; secondly, after exposure to Cytochalasin-D, a compound that disrupts actin polymerization and cellular contraction; and thirdly, following treatment with Endo 1. Direct observation of cell deformation being challenging, we tracked the motion of particles adjacent to the cells within the gel under different treatments as a proxy measure. We utilized Fm-track[41], a specialized particle tracking velocimetry (PTV) method for cell tracking, providing it with an initialization value. ", "page_idx": 20}, {"type": "text", "text": "Table 3: Comparative performance of different methods under varying training data sizes. The table lists End-Point Error (EPE), Accuracy Strict, Accuracy Relaxed, and Outliers percentages for each method (FLOT, PV-RAFT, Gotflow3D, and Ours) at $100\\%$ , $10\\%$ , and $1\\%$ training data utilization. This data highlights the resilience of our method against reductions in training size, maintaining high accuracy and low outlier rates across all sampling levels. ", "page_idx": 21}, {"type": "table", "img_path": "WOBhJs9gqU/tmp/48cecb56e6ade59b385bb49ead8e26ec1ec464ce43293c6669e1670747966637.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "WOBhJs9gqU/tmp/858841212c0e239af5be91478b80eb829a681b5530392bf34188f818db5e0e0a.jpg", "img_caption": ["Figure 7: Cross-domain Robustness Analysis. Our method is compared with the state-of-the-art method GotFlow3D. Both techniques are trained on five flow cases and evaluated on the rest different case, which is indicated on the X-axis. The Y-axis showcases the metric of evaluation. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "To illustrate the estimated cellular deformation before and after treatment with Cytochalasin-D, we provide a visualization in Figure 8(a). This figure successfully captures the distinctions between untreated cells and those treated with Cytochalasin-D. The left-hand side of Figure 8(a) displays the gel particle flow field, where the color indicates the angle between the cell surface\u2019s normal and the flow vector. In the central region of this flow field, which is further magnified on the right-hand side of the subplot, a directional flow and specific vortex patterns are evident, attributes linked to cell contraction. Further analysis is provided in Figure 8(b), which elaborates on the flow field profiles across the $x,y$ , and $z$ axes, shedding light on the degree of cellular deformation in each dimension and aiding in the interpretation of the observed biological phenomenon. ", "page_idx": 21}, {"type": "image", "img_path": "WOBhJs9gqU/tmp/749bfb64ca1cad4a939df8953ca08dd69e0f8c2aafac62028cbb29edf07b51c8.jpg", "img_caption": ["a ", "Figure 8: Visualization of Our Results on AVIC. (a) Enhanced Fm-track using our method illustrates cellular deformation pre and post-treatment with Cytochalasin-D. (b) Display of flow field profiles across the x, y, and z axes. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "A.4.5 Subset Selection Robustness ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Different data subset selection may influence our results on limited data. We control experiment randomness with seeds, conducting multiple runs to show our model performance on various subset data. See results in Table. 4. ", "page_idx": 22}, {"type": "text", "text": "A.5 Ablation study on different modules ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "A.5.1 Feature extractor ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We compared two feature extractors: our graph-based feature extractor and Pointnet $^{++}$ [72], a generic point cloud feature extractor, to assess their efficiency in extracting features from fluid particles. Both extractors have an embedding dimension of 128 and use a K-value of 32 for the K-nearest-neighbor algorithm. The results in Tab. 5 show that our feature extractor consistently outperforms Pointnet++ across all metrics, indicating that Pointnet $^{++}$ has limitations in fully capturing the features of fluid particles. ", "page_idx": 22}, {"type": "table", "img_path": "WOBhJs9gqU/tmp/c0d7726902db76297a55460f753e0b8aebc8fb63bcf4a498e27ffe3241ce8ae1.jpg", "table_caption": ["Table 4: Various runs with different seeds. "], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "WOBhJs9gqU/tmp/848a8ed0102c79b5bebf4d891c47d207c617d478f8926712d1310d79af4280cb.jpg", "table_caption": ["Table 5: Comparison between our feature extractor with Pointnet++. Both models are trained on the full Fluidflow3D dataset. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "A.5.2 Zero-Divergence Loss ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "When investigating the intrinsic properties of the velocity field, it is understood that incompressible fluids, by definition, exhibit zero divergence. Consequently, we have introduced a regulation specifically targeting zero-divergence in the estimated velocity field. However, this regulation is not entirely accurate, as many fluids can be compressed. Therefore, this section scrutinizes the effectiveness of the zero-divergence regulation. We tested it using the FluidFlow3D test data(See Table. 6) and its six fluid cases(See Table 7). ", "page_idx": 23}, {"type": "text", "text": "As shown in Table 6, the zero-divergence regulation appears ineffective for models trained on full samples when tested on the FluidFlow3D-NORM dataset. However, this regulation becomes more effective for models trained on fewer samples. Furthermore, the lower the sample count, the greater the benefit gained from zero-divergence regulation. Our interpretation is that the zero-divergence regulation provides additional prior information, which can be learned with a sufficiently large sample size but may be challenging to access when the sample size is limited. Thus, our zero-divergence regulation plays a compensatory role that improves data efficiency. ", "page_idx": 23}, {"type": "text", "text": "We conducted further tests to determine the suitability of the zero-divergence assumption for various fluid cases. Our findings, illustrated in Figure 7, revealed that our regulation performs excellently in fluid cases that exhibit zero divergence, such as Beltrami Flow, Turbulent Channel Flow, Transitional Boundary Flow, and Uniform Flow. Specifically, our model trained with 130 samples (with zerodivergence regulation) outperformed an unregulated model trained with 1300 samples across all metrics in Uniform Flow. In more complex fluid cases, such as Forced MHD Turbulence, where the zero-divergence law does not hold, employing zero-divergence may capture incorrect fluid features if the model is trained with insufficient data. However, with a full training dataset, it still enhances performance. ", "page_idx": 23}, {"type": "table", "img_path": "WOBhJs9gqU/tmp/e88bdfe0c72a7e929015823bbf29fb285941ecad698d0a9714661adb453f05b0.jpg", "table_caption": ["Table 6: Comparison on FluidFlow3D test data. $\\checkmark/\\times$ means our method with/without the zerodivergence loss term. Better results are marked in bold. "], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "WOBhJs9gqU/tmp/445dd05a246154fccee31c60ea63cb1b5c2ee5bbe458a3d3aecda3126645ec18.jpg", "table_caption": ["Table 7: Comparison on different flow cases. $\\checkmark/\\times$ means our method with/without the zerodivergence loss term. Better results are marked in bold. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "Table 8: Comparison on FluidFlow3D test data. $\\checkmark/\\times$ means our method with/without the DVE module. Better results are marked in bold. ", "page_idx": 24}, {"type": "table", "img_path": "WOBhJs9gqU/tmp/2cbfb69c5108528f112fe6f5acabb691c2bdb98a81efc472e8ba989fc57ef025.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "A.5.3 Dynamic Velocimetry Enhancer(DVE) ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We have developed the Dynamic Velocimetry Enhancer (DVE), which is used in the testing phase of the process to optimize the initial flow and can effectively improve cross-domain robustness and data efficiency. To illustrate this, we trained models with and without the DVE module on different sizes of training sets and compared their performance on various test sets. We tested it using the FluidFlow3D test data(See Table. 8) and its six fluid cases(See Table 9). ", "page_idx": 24}, {"type": "text", "text": "As evidenced by Tab. 8, the model without the DVE module is less effective than the model with the DVE module, even though the latter is trained on only $1\\%$ of the samples. Furthermore, we observe that the model without the DVE module is highly sensitive to the training size, and the performance of each metric rapidly declines as the training size decreases. In contrast, the model with the DVE module exhibits a certain degree of robustness. This demonstrates the significant impact of our DVE module on data efficiency. ", "page_idx": 24}, {"type": "table", "img_path": "WOBhJs9gqU/tmp/4248e738cd26c3bd2b88f2e5c6d5a27615f7a302318953f4b30c10cf9ecc5a7b.jpg", "table_caption": ["Table 9: Comparison on different flow cases. $\\checkmark/\\times$ means our method with/without the DVE module. Better results are marked in bold. "], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "WOBhJs9gqU/tmp/25766c17c5f16a9f97536078327c2a3de1e8e73ead78490107181a232a3e02ae.jpg", "table_caption": ["Table 10: Runtime Profiling of Our Method. Different Train Size settings share the same inference time T_test. The inference process includes the Forward of the trained network and our DVE. "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "Tab. 9 shows the performance of the DVE module also varies across different flow cases. In simple cases, the presence or absence of the DVE module has minimal impact on the metrics, since the initial flow already closely matches the ground truth with a relatively small error. However, in challenging cases where the initial flow is far from the ground truth, the DVE module plays a crucial role. It significantly enhances the model\u2019s accuracy through a straightforward and focused optimization process. ", "page_idx": 25}, {"type": "text", "text": "Time Profiling of DVE A significant challenge in test-time optimization is implementing persample adaptation at test time without adversely affecting inference efficiency. We demonstrate the time profiling of our method in Table 10 to illustrate the efficiency of our proposed test-time optimization paradigm. Additionally, we present the convergence graph of DVE to study the influence of the number of refinement steps. As shown in Figure 9, DVE can converge within 150 epochs due to its simple structure, resulting in fast inference and high efficiency. ", "page_idx": 25}, {"type": "text", "text": "A.6 Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Our research addresses the challenge of estimating dual-frame fluid motion. However, this field suffers from a scarcity of large-scale benchmark datasets. To mitigate this issue, we incorporate real", "page_idx": 25}, {"type": "image", "img_path": "WOBhJs9gqU/tmp/61f19c8ff183f6c442c018f0d36c60e2e9ab049a1edf3237ad2030ba9806c59e.jpg", "img_caption": ["Figure 9: Convergence graph of EPE with respect to the number of refinement steps (iterations). "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "world datasets and conduct extensive studies across diverse flow cases. Nonetheless, it is anticipated that a broader variety of fluid motion data will become available in the future. ", "page_idx": 26}, {"type": "text", "text": "A.7 Impact Statements ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Our research focuses on advancing a dual-frame fluid motion estimation method designed for optimal data efficiency and cross-domain robustness in turbulent flow analysis. This innovative approach is anticipated to empower scientists to analyze fluid dynamics with significantly reduced data requirements while enhancing overall method robustness. It is crucial to acknowledge potential limitations, particularly in real-world applications like blood analysis in medical science, where errors arising from our method could potentially be harmful. Our intent is to continually refine and improve our methodology to minimize any such unintended consequences. ", "page_idx": 26}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Please see Abstract and Introduction. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 27}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: Please see Appendix A.6. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 27}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: Our work does not involve theoretical provement. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 28}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We provide all needed implementation details in Appendix A.3.3. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 28}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We provide link to demo data and code in the Abstract. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 29}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We provide experimental settings both in each experiment section of the main paper and in the appendix. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 29}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: Our method is deterministic. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We provide time profiling of our method in Appendix 10. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 30}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We confirm this. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 30}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We provide impact statement in Appendix A.7. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: Our work poses no such risks. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 31}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We have cited original papers. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 31}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 32}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We have open-sourced well-documented code. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 32}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: Our work does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 32}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: Our work does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 32}]