[{"type": "text", "text": "ClavaDDPM: Multi-relational Data Synthesis with Cluster-guided Diffusion Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Wei $\\mathrm{Pang}^{1,2}$ , Masoumeh Shafieinejad2, Lucy Liu3, Stephanie Hazlewood3, and Xi He 1,2\u2217 ", "page_idx": 0}, {"type": "text", "text": "1University of Waterloo 2Vector Institute 3Royal Bank of Canada w3pang@uwaterloo.ca, masoumeh@vectorinstitute.ai, lucy.z.liu@rbc.com, stephanie.hazlewood@rbc.com, xi.he@uwaterloo.ca ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recent research in tabular data synthesis has focused on single tables, whereas realworld applications often involve complex data with tens or hundreds of interconnected tables. Previous approaches to synthesizing multi-relational (multi-table)2 data fall short in two key aspects: scalability for larger datasets and capturing long-range dependencies, such as correlations between attributes spread across different tables. Inspired by the success of diffusion models in tabular data modeling, we introduce Cluster Latent Variable guided Denoising Diffusion Probabilistic Models (ClavaDDPM). This novel approach leverages clustering labels as intermediaries to model relationships between tables, specifically focusing on foreign key constraints. ClavaDDPM leverages the robust generation capabilities of diffusion models while incorporating efficient algorithms to propagate the learned latent variables across tables. This enables ClavaDDPM to capture long-range dependencies effectively. Extensive evaluations on multi-table datasets of varying sizes show that ClavaDDPM significantly outperforms existing methods for these long-range dependencies while remaining competitive on utility metrics for single-table data. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Motivation. Synthetic data has attracted significant interest for its ability to tackle key challenges in accessing high-quality training datasets. These challenges include: i) data scarcity [14, 53], ii) privacy [2, 19], and iii) bias and fairness [46]. The interest in synthetic data has extended to various commercial settings as well, notably in healthcare [18] and finance [36] sectors. The synthesis of tabular data, among all data modalities, is a critical task with approximately $79\\%$ of data scientists working with it on a daily basis [45]. While the literature on tabular data synthesis has predominantly focused on single table (relation) data, datasets in real-world scenarios often comprise multiple interconnected tables and raise new challenges to traditional single-table learning [38, 3, 12, 22]. These challenges have even enforced a join-as-one approach [15, 17], where the multi relations are first joined as a single table. However, with more than a couple of relations (let alone tens or hundreds of them as in the finance sector) this approach is neither desirable nor feasible. ", "page_idx": 0}, {"type": "text", "text": "Challenges. Synthetic Data Vault [35] and PrivLava [5] are recent efforts to synthesize multirelational data using hierarchical and marginal-based approaches. These methods exhibit significant limitations in processing speed and scalability, both with respect to the number of tables and the domain size of table attributes, and they often lack robustness in capturing intricate dependencies. Alternatively, diffusion models have emerged as powerful tools for data synthesis, demonstrating remarkable success in various domains [37]. These models are particularly noted for their strong capabilities in controlled generation. Despite their potential, the application of diffusion models to tabular data synthesis has been limited to unconditional models [25, 50, 28, 24], leaving a gap in effectively addressing the multi-table synthesis problem. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Solution. To address these challenges, we introduce ClavaDDPM (Cluster Latent Variable guided Denoising Diffusion Probabilistic Models). Our novel approach leverages the controlled generation capabilities of diffusion models by utilizing clustering labels as intermediaries to model the relationships between tables, focusing on the foreign-key constraints between parent and child tables. This integration of classifier guidance within the diffusion framework allows ClavaDDPM to effectively capture complex multi-table dependencies, offering a significant advancement over existing methods. ", "page_idx": 1}, {"type": "text", "text": "Contributions. In this work, we: 1) provide a complete formulation of the multi-relational modeling process, as well as the essential underlying assumptions being made, 2) propose an efficient framework to generate multi-relational data that preserves long-range dependencies between tables, 3) propose relationship-aware clustering as a proxy for modeling parent-child constraints, and apply the controlled generation capabilities of diffusion models to tabular data synthesis, 4) apply an approximate nearest neighbor search-based matching technique, as a universal solution to the multi-parent relational synthesis problem for a child table with multiple parents, 5) establish a comprehensive multi-relational benchmark, and propose long-range dependency as a new metric to measure synthetic data quality specific to multi-table cases, and 6) show that ClavaDDPM significantly outperforms existing methods for these long-range dependency metrics while remaining competitive on utility metrics for single-table data. ", "page_idx": 1}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Single-table synthesis models. Bayesian network [48] is a traditional approach for synthetic data generation for tabular data. They represent the joint probability distribution for a set of variables with graphical models. CTGAN [47] is a tabular generator that considers each categorical value as a condition. CTAB-GAN [52] includes mixed data types of continuous and categorical variables. Several studies have explored how GAN-based models can contribute to fairness and bias removal [44, 45]. In privacy, GAN-based solutions boosted with differential privacy have not been as successful as their Baysian-network-based competitors [34, 51]. Recent popular Diffusion Models, [20, 40, 42, 41], offer a different paradigm for generative modeling. TabDDPM [25] utilizes denoising diffusion models, treating numerical and categorical data with two disjoint diffusion processes. STaSy [24] uses score-based generative modeling in its training strategy. CoDi [28] processes continuous and discrete variables separately by two co-evolved diffusion models. Unlike the previous three which perform in data space, TabSyn [50] deploys a transformer-based variational autoencoder and applies latent diffusion models. Privacy and fairness research for diffusion models are currently limited to a few studies in computer vision [26, 11, 16]. ", "page_idx": 1}, {"type": "text", "text": "Multi-table synthesis models. There have been few proposals for synthetic data generation for multi-relational data. A study proposed this synthesis through graph variational autoencoders [31], the presented evaluation is nevertheless very limited. The Synthetic Data Vault [35] uses the Gaussian copula process to model the parent-child relationship. SDV iterates through each row in the table and performs a conditional primary key lookup in the entire database using the ID of that row, making a set of distributions and covariance matrices for each match. This inhibits an efficient application of SDV to the numerous tables case. PrivLava [5], synthesizes relational data with foreign keys under differential privacy. The key idea of PrivLava is to model the data distribution using graphical models, with latent variables included to capture the inter-relational correlations caused by foreign keys. ", "page_idx": 1}, {"type": "text", "text": "3 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Multi-relational databases. A multi-relational database $\\mathcal{R}$ consists of $m$ tables (or relations) $(R_{1},...,R_{m})$ . Each table is a collection of rows, which are defined over a sequence of attributes. ", "page_idx": 1}, {"type": "image", "img_path": "Zb2ixT19VF/tmp/17f4c6ab10e8bcf034453f0893152817d4bffc6273bc2b2eaf83add1d2be1133.jpg", "img_caption": ["Figure 1: Berka sample tables (left), and the foreign key constraint graph for Berka (right) "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "One of the attributes, let\u2019s consider the first attribute without loss of generality, is the primary key of table $R$ , which serves as the unique identifier for each row in the table. No rows in the same table have repeated values for the primary key attribute. We use Berka database [4] as our running example in this work, as in Figure 1. Note the Account $I D$ , the primary key for the Account table in Berka. ", "page_idx": 2}, {"type": "text", "text": "Given a table $R_{j}$ , we say a relation $R_{i}$ has a foreign key constraint with $R_{j}$ , or $R_{i}$ refers to $R_{j}$ , if $R_{i}$ has an attribute known as foreign key that refers to the primary key of $R_{j}$ : for every row $r_{i}\\in R_{i}$ , there exists a row $r_{j}\\,\\in\\,R_{j}$ such that $r_{j}$ \u2019s primary key value equals to $r_{i}$ \u2019s foreign key value. For example, the Account $I D$ of the Loan table refers to the primary key of the Account table. If an account row is removed from the Account $I D$ table, so would all the referring rows in the Loan table to this account, for foreign key constraint to hold. Note that the primary key of a table can consist of multiple attributes. In this paper, we focus on the case of a single attribute that is common in practice. Also note that all keys are considered row identifiers and are thus not treated or modeled alongside the actual table attributes in this work. ", "page_idx": 2}, {"type": "text", "text": "A multi-relational database under foreign key constraints forms a directed acyclic graph (DAG), ", "page_idx": 2}, {"type": "equation", "text": "$$\n{\\mathcal{G}}=\\left({\\mathcal{R}},{\\mathcal{E}}\\right),{\\mathcal{E}}=\\left\\{\\left(R_{i}\\to R_{j}\\right)\\,|\\,i,j\\in\\left\\{1,\\ldots,m\\right\\},i\\neq j,R_{i}{\\mathrm{~refers~to~}}R_{j}\\right\\}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "with the tables $\\mathcal{R}$ being the set of nodes, and $\\mathcal{E}$ being the set of edges. In addition, for $R_{i}$ referring to $R_{j}$ , we also call this a parent-child relationship, where $R_{j}$ is the parent and $R_{i}$ is the child. We use the maximum depth to denote the number of nodes on the longest path in $\\mathcal{G}$ . Figure 1 shows the corresponding graph to Berka database and its maximum depth is 4. ", "page_idx": 2}, {"type": "text", "text": "Multi-relational synthesis problem. Given a multi-relational database $\\mathcal{R}=\\{R_{1},\\ldots,R_{m}\\}$ , we would like to generate a synthetic version $\\tilde{\\mathcal{R}}\\,=\\,\\{\\tilde{R}_{1},\\...\\,,\\tilde{R}_{m}\\}$ that has the same structure and foreign-key constraints as $\\mathcal{R}$ and preserves attribute correlations within $\\mathcal{R}$ , including 1) the intercolumn correlations within the same table; 2) the intra-group correlations within the same foreign key group; 3) the inter-table correlations. The first aspect has been well defined, measured, and tackled in the literature of single-table synthesis [52, 25, 50] while the other two aspects are raised due to foreign-key constraints between tables [5]. For instance, in Berka database (Figure 1), the foreign key constraint between the Loan table and the Account table via Account $I D$ adds an important intra-group correlation for the combinations of loans associated with an account and many 1-hop inter-table correlations between columns in the Loan table and the columns in the Account table. Even for the Loan table and the Demographic table that are indirectly constrained by foreign keys, their columns are correlated as well, e.g., how is the average salary in a district related to the status of loans, an example for 2-hop inter-table correlation. ", "page_idx": 2}, {"type": "text", "text": "Classifier-guided DDPM. DDPM [20] uses two Markov chains, a forward chain that perturbs data to noise through a series of Gaussian transitions, and a reverse chain that converts noise back to data with the same number of steps of Gaussian transitions (Equation 2). ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad q\\left(\\pmb{x}_{t}\\mid\\pmb{x}_{t-1}\\right):=\\mathcal{N}\\left(\\pmb{x}_{t};\\sqrt{1-\\beta_{t}}\\pmb{x}_{t-1},\\beta_{t}\\pmb{I}\\right)}\\\\ &{\\quad p_{\\theta}\\left(\\pmb{x}_{t-1}\\mid\\pmb{x}_{t}\\right):=\\mathcal{N}\\left(\\pmb{x}_{t-1};\\pmb{\\mu}_{\\theta}\\left(\\pmb{x}_{t},t\\right),\\pmb{\\Sigma}_{\\theta}\\left(\\pmb{x}_{t},t\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Prior work [40] shows that given label $\\textit{\\textbf{y}}$ , the conditional reverse process has the form ", "page_idx": 2}, {"type": "equation", "text": "$$\np_{\\theta,\\phi}\\left(\\pmb{x}_{t}\\mid\\pmb{x}_{t+1},\\pmb{y}\\right)\\propto p_{\\theta}\\left(\\pmb{x}_{t}\\mid\\pmb{x}_{t+1}\\right)p_{\\phi}\\left(\\pmb{y}\\mid\\pmb{x}_{t}\\right).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "By approximating $\\log p_{\\phi}\\left(\\pmb{y}\\mid\\pmb{x}_{t}\\right)$ using Taylor expansion around $\\mathbf{\\boldsymbol{x}}_{t}=\\pmb{\\boldsymbol{\\mu}}$ , the conditional reverse process (Equation 3) can be approximated with a perturbed Gaussian transition [10] ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\log\\left(p_{\\theta,\\phi}\\left(\\pmb{x}_{t}\\mid\\pmb{x}_{t+1},\\pmb{y}\\right)\\right)\\approx\\log\\left(p\\left(\\boldsymbol{z}\\right)\\right)+C,\\;\\;\\boldsymbol{z}\\sim\\mathcal{N}\\left(\\mu+\\pmb{\\Sigma}\\pmb{g},\\pmb{\\Sigma}\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $C$ is a constant and $g=\\nabla_{\\pmb{x}_{t}}\\log\\left(P_{\\phi}\\left(\\pmb{y}\\mid\\pmb{x}_{t}\\right)\\right)\\mid_{\\pmb{x}_{t}=\\pmb{\\mu}}$ computed from the classifier $P_{\\phi}$ . ", "page_idx": 3}, {"type": "text", "text": "4 ClavaDDPM ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Here, we elaborate on the training and synthesis process of ClavaDDPM, and each design\u2019s rationale. ", "page_idx": 3}, {"type": "text", "text": "4.1 Modeling generative process for two-table relational databases ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Notations. Consider a database of two tables $\\mathscr{R}\\,=\\,\\{R_{1},R_{2}\\}$ , e.g. $\\{L o a n,\\;A c c o u n t\\}$ in Berka, where the child table $R_{1}$ refers to parent table $R_{2}$ . To model the entire database, we first use $\\mathbf{\\deltaX}$ and $\\mathbf{\\deltaY}$ as the variables for the child table $R_{1}$ and parent table $R_{2}$ , respectively (dropping their primary key attributes and indexing their respective row variables starting from one). In this section, we use boldface to represent random variables. e.g. $Y\\sim Y$ , where $Y$ is the data of $R_{2}$ , and $\\mathbf{Y}$ is the random variable $Y$ being sampled from. In addition, we use subscript to represent the parent row some data or random variable refers to. e.g. $\\pmb{x}_{j}$ represents the child random variable who refers to parent $\\pmb{y}_{j}$ Refer to Appendix A for a complete list of notations used and the corresponding design choices. ", "page_idx": 3}, {"type": "text", "text": "Assumptions. 1) The parent table has no constraints itself. Hence, we can follow previous work on single-table synthesis [13, 25, 47, 50, 52] to make an i.i.d assumption on the rows in the parent table. The parent table $\\mathbf{Y}$ can be modeled as a list of i.i.d. row variables $\\left\\{y_{j}\\ |\\ j=1,\\ldots,|R_{2}|\\right\\}$ , where $j$ is the index or the primary key value of the $j$ th row, and each row follows a distribution $p(y)$ . ", "page_idx": 3}, {"type": "text", "text": "2) The i.i.d assumption does not apply to the child table rows $(x_{j}$ \u2019s) as they are constrained by their respective parent rows. Consider two loans associated with the same account id; if one\u2019s status is $i n$ debt $\\mathrm{(^{\\leftarrow}C^{\\bullet})}$ , the other one is likely so too. To capture this dependency, we make a Bayesian modeling assumption that, although child rows associated with the same parent row are not independent, they are conditionally independent of child rows associated with other parent rows, given their respective parent. For example, consider an account table (parent) and a loan table (child). Loans related to the same account (i.e., the same parent) are not independent due to shared account-specific factors. However, loans from different accounts can be considered conditionally independent when accounting for their respective account-level information. Hence, we model $\\mathbf{\\deltaX}$ by $\\left\\{\\pmb{g}_{j}\\ \\right|^{2}\\}=1,\\ldots,\\left|\\boldsymbol{R}_{2}\\right|\\}$ , where each group $\\pmb{g}_{j}=\\{\\pmb{x}_{j}^{i}\\mid i=1,\\ldots,\\lvert\\pmb{g}_{j}\\rvert\\}$ represents a set of child table rows referring to the parent row yj. ", "page_idx": 3}, {"type": "text", "text": "3) Without violating the assumptions made above, we further make an i.i.d assumption on $(g_{j},y_{j})$ , which leads to an approximated distribution for the parent-child tables: ", "page_idx": 3}, {"type": "equation", "text": "$$\nP(X=X,Y=Y)\\approx\\prod_{j=1}^{|R_{2}|}P\\left(g_{j}=g_{j},y_{j}=y_{j}\\right)\\quad{\\mathrm{~or~}}\\quad p(X,Y)=\\prod_{j}p(g_{j},y_{j})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\textstyle X\\,=\\,\\cup_{j=1}^{|R_{2}|}g_{j}$ and $g_{j}\\,=\\,\\{x_{j}^{1},\\dotsc,x_{j}^{|g_{j}|}\\}$ x|jgj|}. This model allows us to capture the inter-table correlations (the correlation between tuples from different tables) and the intra-group correlations. ", "page_idx": 3}, {"type": "text", "text": "Modeling. Despite the simplified formulation with several aforementioned assumptions, learning the distribution $p\\left(g_{j},y_{j}\\right)$ is non-trivial. In particular, $(g_{j},y_{j})$ cannot be flattened into a matrix form for learning since the set structured attributes in $\\boldsymbol{g}_{\\boldsymbol{j}}$ , e.g., the size of a group variable $\\boldsymbol{g}_{\\boldsymbol{j}}$ is not fixed. ", "page_idx": 3}, {"type": "text", "text": "A naive solution is to model a conditional distribution of the group given the parent row ", "page_idx": 3}, {"type": "equation", "text": "$$\np\\left(g_{j},y_{j}\\right)=p\\left(g_{j}\\mid y_{j}\\right)p\\left(y_{j}\\right)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Direct modeling of Equation (6) still has the same issue as before for the foreign key group $\\boldsymbol{g}_{\\boldsymbol{j}}$ , which can take an arbitrary number of child rows. In particular, when modeling ${\\pmb g}_{j}\\,=\\,f\\left({\\pmb x}_{j}\\right)$ for some function $f$ , there is no trivial structured support for $\\boldsymbol{g}_{j}$ if we model for $\\pmb{g}_{j}$ using only the attributes or features of the child rows. Furthermore, the conditioning space of the parent row $\\textit{\\textbf{y}}$ can be very large (e.g., Account table has a domain size of more than 11,000), which can lead to poorly learned conditional distribution if we treat $\\textit{\\textbf{y}}$ as labels in the classifier-guided DDPM. The original space of $\\textit{\\textbf{y}}$ is high-dimensional and noisy and does not guarantee any spatial proximity or smoothness. In the context of deep modeling, this drastically worsens the quality of conditional sampling. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "To address these two challenges, we introduce latent random variables $^c$ such that $\\boldsymbol{g}_{\\boldsymbol{j}}$ is independent from $\\pmb{y}_{j}$ conditioned on $^c$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\pmb{g}_{j}\\bot\\!\\!\\mid_{\\!\\!\\!\\textbf{\\em y}_{j}}\\mid\\!\\!\\boldsymbol{c}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "With this assumption, we get an indirect modeling of inter-table correlations through $^c$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\np(g_{j},y_{j})=\\sum_{c}p(g_{j},y_{j}|c)p(c)=\\sum_{c}p(g_{j}|c)p(y_{j}|c)p(c)=\\sum_{c}p\\left(g_{j}\\mid c\\right)p\\left(y,c\\right)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Compared to Equation (6), when $^c$ is selected to be lying on a low-dimensional, compact manifold, the latent conditional distribution $p\\left(g_{j}\\mid c\\right)$ will be easier to model than $p\\left(g_{j}\\mid y_{j}\\right)$ . ", "page_idx": 4}, {"type": "text", "text": "For each foreign key group $\\boldsymbol{g}_{\\boldsymbol{j}}$ , we model its size explicitly with a variable $s_{j}$ . By making an tahses ulamtepntit orna nthdaot $s_{j}$ airsi acbolne t, iownea lelsys einntdiaelpley nddeefnint efdr oa mg ietnse crahtiilvde  rporwo cveasrsi afoblr $\\{\\pmb{x}_{j}^{1},\\cdot\\cdot\\cdot,\\pmb{x}_{j}^{s_{j}}\\}$ tsg isviezne $^c$ $g_{j}$ $s_{j}\\sim s_{j}$ , then sample $g_{j}$ child row variables. In addition, we make an i.i.d assumption on the child row variables given the latent variable. Hence, we have ", "page_idx": 4}, {"type": "equation", "text": "$$\np(g_{j}|c)=p(s_{j}|c)\\prod_{i=1}^{s_{j}}p(x_{j}^{i}|c)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Putting all together, we have the final formulation of the generative process for a two-table case: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{p\\left(X,Y\\right)\\approx\\prod_{j=1}^{|R_{2}|}p\\left(g_{j},y_{j}\\right)}}&{\\mathrm{i.i.d~assumption~on~}\\left(g_{j},y_{j}\\right)\\mathrm{Equation~}(5)}\\\\ &{}&{\\approx\\displaystyle\\prod_{j=1}^{|R_{2}|}\\sum_{c}p\\big(s_{j}|c\\big)\\displaystyle\\prod_{i=1}^{s_{j}}p\\big(x_{j}^{i}|c\\big)p\\big(y_{j},c\\big)=\\displaystyle\\prod_{j=1}^{|R_{2}|}\\sum_{c}p\\left(y_{j},c\\right)p\\left(s_{j}\\mid c\\right)\\displaystyle\\prod_{i=1}^{s_{j}}p\\left(x_{j}^{i}\\mid c\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Unlike our naive method that uses a direct modeling of $p\\left(x_{j}\\mid y_{j}\\right)$ , we model $p\\left(x_{j}\\mid c_{j}\\right)$ , which greatly reduced the condition space, thus better capturing the inter-table correlation between $\\mathbf{\\deltaX}$ and $\\bar{\\mathbf{Y}}$ . On the other hand, the intra-group correlations are intrinsically addressed, because our modeling of $(g_{j},y_{j})$ and the corresponding dependency assumptions enforce that two child rows are drawn from the same distribution if and only if they belong to the same foreign key group. ", "page_idx": 4}, {"type": "text", "text": "Based on Equation (10), we introduce our generative process for two-table case. ", "page_idx": 4}, {"type": "text", "text": "Phase I: Latent learning and table augmentation: (1) Learn latent variable $c$ on the joint space $(X;Y)$ , such that each parent row $y_{j}$ corresponds to a learned latent variable $c_{j}$ . (2) Augment the parent table into $T_{Y}=(Y;C)$ , where $C$ corresponds to the latent variable values $c_{j}$ for each row $y_{j}$ . ", "page_idx": 4}, {"type": "text", "text": "Phase II: Training: (3) Train diffusion model $p_{\\theta}\\left(y,c\\right)$ on augmented table $T_{Y}$ and child diffusion model $p_{\\phi}\\left(x\\right)$ . (4) Given learned latents and child table, train classifier $p_{\\psi}\\left(c\\mid x\\right)$ . (5) Estimate the foreign key group size distributions conditioned on latent variables $p\\left(s\\mid c\\right)$ . ", "page_idx": 4}, {"type": "text", "text": "Phase III: Synthesis: (6) Synthesize the augmented parent table $\\tilde{T}_{Y}=(\\tilde{Y};\\tilde{C})\\sim p_{\\theta}\\left(\\cdot,\\cdot\\right)$ . (7) For each synthesized latent variable $\\tilde{c}_{j}\\,\\in\\,C$ , sample group size $\\tilde{s}_{j}\\sim p\\left(\\cdot\\mid\\tilde{c}_{j}\\right)$ . (8) Given $\\tilde{s}_{j}$ , sample each child row within the foreign key group $\\boldsymbol{x}_{j}^{i}\\sim p_{\\phi,\\psi}\\left(\\cdot\\mid\\tilde{c}_{j}\\right)$ , where $p_{\\phi,\\psi}$ performs classifier guided sampling by perturbing $p_{\\phi}$ with the gradient of $p_{\\psi}$ . We denote steps (7) and (8) by $\\tilde{X}\\sim p(\\cdot|C)$ . ", "page_idx": 4}, {"type": "text", "text": "4.2 Extension to more parent-child constraints ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We learn the latent variables between a parent and a child pair in a bottom-up fashion (starting from the leaf nodes in $\\mathcal{G}$ ) and pass all the latent variable values to the parent table for the next set of latent variables at higher levels. Given a parent-child pair $(Y,X)$ , the child table $X$ also has $k$ leaf node children, $Z_{1},\\ldots,Z_{k}$ . Let $c_{X,Z_{i}}$ represent the latent variables learned on the joint space $(X;Z_{i})$ . The augmented table for $X$ is formed by appending all its latent variable values, i.e., $T_{X}=(X;C_{X,Z_{1}};\\ldots;C_{X,Z_{k}})$ . Then, the latent variable $c_{Y,X}$ is learned on the joint space of $(Y;T_{X})$ instead of $(Y;X)$ . Therefore, our latent learning process follows a bottom-up topological order, ensuring each child table is already augmented by the time we learn the latent variable to augment its parent. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "The training phase and the synthesis phase are similar to the two-table case, by handling the parentchild tables in a top-down topological order using the augmented tables. We detail the end-to-end algorithms for the complex data in Appendix B. However, we would like to highlight a special case when a table $X$ has multiple parents $Y_{1},\\ldots,Y_{k}$ . During synthesis, we will have $k$ synthetic latent variable $\\tilde{C}_{1},\\hdots,\\tilde{C}_{k}$ corresponding to the $k$ parents, and thus $k$ copies of synthetic child tables $\\tilde{X}_{1}\\sim p(\\cdot\\mid\\tilde{C}_{1}),...\\,,\\tilde{X}_{k}\\sim p(\\cdot\\mid\\bar{\\tilde{C}}_{k})$ . Unifying these diverged synthetic tables presents a challenge and we present a universal solution in Section 4.3.3. ", "page_idx": 5}, {"type": "text", "text": "Extending the model to include more tables allows for capturing longer-range dependencies, beyond just those between adjacent tables. For example, as shown in Figure 1, the dependency between the Demographic table and the Credit Card table can also be captured and quantified. Further details are provided in Section 5. ", "page_idx": 5}, {"type": "text", "text": "4.3 Design choices for ClavaDDPM ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We detail how design decisions for ClavaDDPM meet our goals and align with our assumptions. ", "page_idx": 5}, {"type": "text", "text": "4.3.1 Relationship-aware clustering ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Given the conditional independence between the parent row and its foreign key group (Equation (7)), it is important to model the latent variable $^c$ such that it can effectively capture the inter-table correlation within the same foreign key group. In ClavaDDPM, we learn $^c$ using Gaussian Mixture Models (GMM) in the weighted joint space of $\\mathbf{\\deltaX}$ and $\\mathbf{\\deltaY}$ , denoted as $H=(X;\\lambda Y)$ , where $\\lambda$ is a weight scalar controlling the importance of child and parent tables when being clustered. Concretely, we consider $k$ clusters, and model the distribution of $\\mathbf{\\bar{\\boldsymbol{h}}}=(\\mathbf{\\boldsymbol{x}};\\lambda\\mathbf{\\boldsymbol{y}})$ with Gaussian distributed around its corresponding centroid $^c$ , i.e., $\\begin{array}{r}{P\\left(h\\right)=\\sum_{c=1}^{k}P\\left(c\\right)P\\left(h\\mid c\\right)=\\sum_{c=1}^{k}\\pi_{c}\\mathcal{N}\\left(h;\\mu_{c},\\Sigma_{c}\\right).}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "Note that diagonal GMMs are universal approximators, given enough mixtures of Gaussian distributions [49]. Therefore, we can further enforce diagonal covariance, i.e., $\\Sigma_{c}=\\mathrm{diag}\\left(.\\dots,\\pmb{\\sigma}_{l}^{2},.\\dots\\right)$ , which, being properly optimized, immediately satisfies our assumptions that the foreign key groups are conditionally independent of their parent rows given $^c$ . In addition, the family of Gaussian Process Latent Variable Models (GPLVM) [30, 27, 33] has been used as an embedding technique to find low-dimensional manifolds that map to a noisy, high-dimensional space. This satisfies our need to learn a stochastic map between the noisy parent space and a condensed latent space. Thus, we can achieve a better trade-off by sacrificing some information fidelity during this quantization process while making the conditional space better shaped. ", "page_idx": 5}, {"type": "text", "text": "However, such clustering in the joint space $(X;\\lambda Y)$ could potentially lead to inconsistency when we create the augmented table $T_{Y}=(Y;C)$ . Though we add a weight $\\lambda$ to the parent rows such that child rows with the same parent rows are likely to be assigned to the same cluster, there is still some chance that they end with different clusters. In particular, for each parent row $y_{j}\\in Y$ , its child rows are assigned to different clusters. In ClavaDDPM, we impose a majority voting step to find the most popular cluster label in each foreign key group and assign it to the parent row $y_{j}$ . In practice, the voting agree rates tend to be high, and this can be further enforced by assigning a higher weight to the parent table (increasing $\\lambda$ ) during GMM clustering. We evaluate the choice of $\\lambda$ and voting agree rates in our ablation study in Section 5.3. ", "page_idx": 5}, {"type": "text", "text": "While alternative latent learning algorithms could potentially be applied, such as TabSyn [50] that demonstrated the utility of latent encoding of tabular data with VAE, this work focuses on demonstrating the effectiveness of a simple diagonal Gaussian Mixture Model (GMM) for ClavaDDPM. Our experiments (detailed in Section 5) reveal that ClavaDDPM with a diagonal GMM achieves state-of-the-art results while maintaining low computational overhead. We leave the exploration of more complex latent learning techniques for future work. ", "page_idx": 5}, {"type": "text", "text": "4.3.2 Learning with DDPM ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Gaussian diffusion backbone. We consider one of the state-of-the-art diffusion models for single tabular data, TabDDPM [25], as the backbone model. TabDDPM models numerical data with Gaussian diffusion (DDPM [20]), and models categorical data with multinomial diffusion ([21]) with one-hot encoding, and carries out disjoint diffusion processes. However, the modeling of multinomial diffusion suffers significant performance overheads, and poses challenges to guided sampling. Instead, ClavaDDPM uses a single Gaussian diffusion backbone to model both numerical and categorical data in a unified space, where categorical data is mapped to the numerical space through label-encoding. To be specific, for a categorical feature with $m$ distinct values $C=\\{c_{1},\\ldots,c_{m}\\}$ , a label encoding $E:C\\to\\{0,\\dots,m-1\\}$ maps each unique category $c_{i}$ to an assigned unique integer value. For a table row $x=[x_{n u m};\\cdot\\cdot\\cdot;x_{c a t_{i}};\\cdot\\cdot\\cdot]$ , where $x_{n u m}$ represents all the numerical features and $x_{c a t_{i}}$ represent a categorical feature, we obtain the unified feature by $x_{u n i}=[x_{n u m};\\cdot\\cdot\\cdot;E\\left(x_{c a t_{i}}\\right);\\cdot\\cdot\\cdot\\cdot]$ Based on this encoding, we learn $p_{\\theta}\\left(y,c\\right)$ on the augmented parent table $T_{Y}\\,=\\,(Y;C)$ through training a Gaussian diffusion model on the unified feature space $\\left(Y_{u n i};E\\left(C\\right)\\right)$ . ", "page_idx": 6}, {"type": "text", "text": "Classifier guided synthesis. As defined in Equation (10), we model $p\\left(x_{j}^{i}\\mid c_{j}\\right)$ by leveraging classifier-guided sampling of diffusion models, following [10]. In practice, with the sheer power of diffusion models, we jointly model $p\\left(x\\mid c\\right)$ for the entire table without distinguishing $j$ . First, we train a Gaussian diffusion model $p_{\\phi}$ on child table row $\\textbf{\\em x}$ , with its reverse process modeled with $x_{t}\\sim$ $\\mathcal{N}(\\mathbf{\\mathscr{x}}_{t+1};\\pmb{\\mathscr{\\mu}}_{\\phi_{t+1}},\\pmb{\\Sigma}_{\\phi_{t+1}})$ . Then, we train a classifier that classifies cluster labels based on $\\textbf{\\em x}$ . The conditional reverse process can be approximated by $\\begin{array}{r}{\\mathbf{\\emph{x}}_{t}\\mid c\\sim\\mathcal{N}(\\mathbf{\\emph{x}}_{t+1};\\mu_{\\phi_{t+1}}+\\eta\\Sigma_{\\phi_{t+1}}g_{\\psi_{t+1}},\\Sigma_{\\phi_{t+1}}),}\\end{array}$ where g\u03c8t+1 $g_{\\psi_{t+1}}=\\nabla_{\\pmb{x}_{t+1}}\\log\\left(p_{\\phi}\\left(c\\mid\\pmb{x}_{t+1}\\right)\\right)$ and $\\eta$ is a scale parameter controlling the strength of conditioning. One can regard $\\eta$ as a hyper parameter measuring the trade-off between single-table generation quality and inter-table correlations, to be demonstrated by our ablation study in Section 5.3. ", "page_idx": 6}, {"type": "text", "text": "4.3.3 Multi-parent dilemma: matching ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Consider the case where some child table $X$ has two parent tables $Y_{1},Y_{2}$ . Our parent-child synthesis modeling paradigm would lead to two divergent synthetic child tables $\\tilde{X}_{1}\\sim X\\mid Y_{1}$ , and $\\tilde{X}_{2}\\sim X\\mid Y_{2}$ Each synthetic table encodes its own parent-child relationship, i.e. the foreign keys. Combining $\\tilde{X}_{1}$ and $\\tilde{X}_{2}$ so that the synthetic child table contains foreign keys from both parents $p_{1}$ and $p_{2}$ is non-trivial, and we call it a multi-parent dilemma. One possible approach is to explicitly constrain the model sample space of $X\\mid Y_{2}$ to be the synthetic data $\\tilde{X}_{1}$ , as used in PrivLava [5]. However, this approach is not applicable to diffusion models that sample from a continuous space. ", "page_idx": 6}, {"type": "text", "text": "We provide a universal solution for all generative models. Consider some real data point $x$ with two parent rows $y_{1}^{j}$ and $y_{2}^{k}$ . Ideally, some synthetic data point $\\tilde{x}$ following the same distribution as real data point $x$ should be sampled from $\\boldsymbol{x}\\mid\\boldsymbol{y}_{1}^{j},\\boldsymbol{y}_{2}^{k}$ . This can be approximated by finding the intersection of two conditional distributions $\\textbf{\\em x}\\vert\\textbf{\\em y}_{1}$ and $\\textbf{\\em x}\\mid\\pmb{y}_{2}$ . Specifically, we estimate $\\tilde{x}$ by finding two synthetic data points $\\tilde{x}_{1}\\in\\tilde{X}_{1}$ and $\\tilde{x}_{2}\\in\\tilde{X}_{2}$ , such that $\\tilde{x}_{1}\\sim x\\mid y_{1}^{j}$ and $\\tilde{x}_{2}\\sim x\\mid y_{2}^{k}$ , and the two points are close enough. We reason as follows: although $\\tilde{x}_{1}$ was sampled from $\\textbf{\\em x}\\vert\\textbf{\\em y}_{1}$ , as long as it is close enough to some other synthetic data point $\\tilde{x}_{2}$ sampled from $\\textbf{\\em x}\\mid\\pmb{y}_{2}$ , then $\\tilde{x}_{1}$ will also be within in the high density region of the distribution $\\textbf{\\em x}\\mid\\pmb{y}_{2}$ , indicating a high probability that $\\tilde{\\pmb{x}}_{1}$ follows $\\textbf{\\em x}|\\,\\pmb{y}_{1},\\pmb{y}_{2}$ . Symmetrically, the same reasoning also holds for $\\tilde{x}_{2}$ . ", "page_idx": 6}, {"type": "text", "text": "Therefore, we can estimate the true sample data point by $\\tilde{x}=f\\left(\\tilde{x}_{1},\\tilde{x}_{2}\\right)$ if $\\tilde{x}_{1}$ is close to $\\tilde{x}_{2}$ , where $f$ can simply be an interpolation between two data points in practice. We call this a matching process between two divergent synthetic tables $\\tilde{X}_{1}$ and $\\tilde{X}_{2}$ , and this can be done efficiently using approximate nearest neighbor search. Although we call this a \"matching\", it does not require finding a one-to-one mapping. Note that this estimate can be further improved by resampling $\\tilde{X}_{1}$ and $\\tilde{X}_{2}$ and estimate X\u02dc with more data points rather than just a pair, and the trade-off is a larger computational overhead, and we leave this for future research. Empirically, sampling $\\tilde{X}_{1}$ and $\\tilde{X}_{2}$ only once is already strong, and an ablation study on the effectiveness of parent matching is in Section 5.3. ", "page_idx": 6}, {"type": "text", "text": "5 Evaluation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We evaluate ClavaDDPM\u2019s performance in multi-relational data synthesis, using both single-table and multi-tables utility metrics (including the new long-range dependency). We present an end-to-end comparison of ClavaDDPM to the SOTA baselines, followed by an ablation study for ClavaDDPM. ", "page_idx": 7}, {"type": "text", "text": "5.1 Experimental setup ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Real-world datasets. We experiment with five real-world multi-relational datasets including California [6], Instacart 05 [23], Berka [4], Movie Lens [39, 32], and CCS [32]. These datasets vary in the number of tables, the maximum depth, the number of constraints, and complexity. Among all, Berka, Movie Lens, and $C C S$ exhibits complex multi-parent and multi-children structures. We use Berka in our work for ablation study and model anatomy. Details can be found in Appendix C.1. ", "page_idx": 7}, {"type": "text", "text": "Baselines. We adopt two multi-relational synthesis models in literature as our baselines: PrivLava [5] as a representative of state-of-the-art marginal-based methods, and SDV [35] as a statistical method specially designed for multi-relational synthesis. We also introduce two multi-relational synthesis pipelines, SingleT(ST) and Denorm(D), as our additional baselines. SingleT learns and generates each table individually, but it also assigns foreign keys to each synthetic child table accordingly to the real foreign key group size distribution such that the group size information is preserved. Denorm follows the baseline idea that joins table first, but it is hard to join all tables into a single table. Hence, Denorm first applies single-table backbone model to generate the joined table between every parent-child table pair and then split it. For these two pipelines, we use CTGAN [47] and TabDDPM [25] as the single-table backbone models, representing the SOTA tabular synthesis algorithms with GAN-based models and diffusion-based models. The details can be found in Appendix C.2. ", "page_idx": 7}, {"type": "text", "text": "Evaluation metrics. We evaluate the quality of the synthetic data using: 1) cardinality to measure the foreign key group size distribution for the intra-group correlations; 2) column-wise density estimation ( $^{\\,l}$ -way) to estimate the density of every single column for all tables; 3) pair-wise column correlation ( $\\mathit{\\check{k}}$ -hop) for the correlations of columns from tables at distance $k$ , e.g., 0-hop refers to columns within the same table and 1-hop refers to a column and another column from its parent or child table; 4) average 2-way, which computes the average of all $k$ -hop column-pair correlations, taking into consideration of both short-range ( $\\mathrm{\\Delta}k=0$ ) and longer-range $(k>0)$ dependencies. For each measure, we report the complement of Kolmogorov-Smirnov (KS) statistic and total variation (TV) distance between the real data and the synthetic data, ranging from 0 (the worst utility) to 1 (the best utility). The reported results are averaged over 3 randomly sampled synthetic data. ", "page_idx": 7}, {"type": "text", "text": "We also consider higher-order single-table evaluation metrics for some representative tables as prior work [50]. We include their details and experiemntal results in Appendix D due to space constraints. ", "page_idx": 7}, {"type": "text", "text": "All experiments are conducted with an NVIDIA A6000 GPU and 32 CPU cores, with a time limit of 7 days. If an algorithm fails to complete within the time limit, we report TLE (time limit exceeded). Implementation details and hyperparameter specifics are in Appendix C.3. ", "page_idx": 7}, {"type": "text", "text": "5.2 End-to-end evaluation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We conducted multi-table synthesis experiments on five multi-table datasets and report the averaged utility with standard deviation for all algorithms in Table 1. First, the evaluation shows that ClavaDDPM has an overall advantage against all the baseline models in terms of correlation modeling, and is surpassing the baselines by larger margins for longer-range dependencies. e.g. in Instacart 05, our model outperforms the best baseline by $58.29\\%$ on 2-hop correlations, and in Berka, our model exceeds the best baseline by $20.24\\%$ on 3-hop correlations. For single-column densities and cardinality distributions, ClavaDDPM exhibits a competitive result compared to the state-of-the-art baseline models. We also evaluate ClavaDDPM against baselines on high-order single-table metrics (Appendix D.3), which shows that our model has advantages in preserving data fidelity, generating diverse data, and achieving high machine learning efficacy. ", "page_idx": 7}, {"type": "text", "text": "It is worth noting that ClavaDDPM, despite its complexity and capability, is more efficient and robust than some simpler baselines. PrivLava demonstrates strong performance on the California dataset (the simplest data), but fails to converge on all the other datasets. SDV also tends to fail on complex datasets, and is limited to datasets with at most 5 tables and maximum depth of 2 [9]. Although TabDDPM shares a similar model backbone with ClavaDDPM, its synthesis fails to complete within 7 days on multiple datasets, while ClavaDDPM completes all experiments within 2 days. ", "page_idx": 7}, {"type": "table", "img_path": "Zb2ixT19VF/tmp/2c3916192b4720faf8a3051144e8a8754b7548800c15ba1c46e575d3d93ff48a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "Zb2ixT19VF/tmp/7bd0deac2fd2da65db9478d82ef3b466e8f715b8d36046c06b15f019cf5ae3ee.jpg", "table_caption": ["Table 1: End-to-end results. DNC denotes Did Not Converge, and TLE denotes Time Limit Exceeded. ST stands for Single-T and D stands for Denorm. Statistical metrics described in Section 5.1 are reported. "], "table_footnote": ["Table 2: Ablation studies on number of clusters $\\overline{{k}}$ , parent scale $\\bar{\\lambda}$ , and classifier gradient scale $\\eta$ . Note that $\\eta$ and matching have no effect on agree rates. Statistical metrics described in Section 5.1 are reported. "], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "5.3 Ablation study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Gaussian diffusion backbone. To decouple the effect of our Gaussian diffusion only backbone with the latent conditioning training paradigm, we also included two models in Table 1: ST-ClavaDDPM and D-ClavaDDPM, which use the Gaussian diffusion model in ClavaDDPM as backbone model, but are trained and synthesized following Single-T and Denorm. Compared to other baselines, ST-ClavaDDPM exhibits superiority in modeling both single column densities and column-pair correlations. ST-ClavaDDPM significantly outperforms its sibling ST-TabDDPM, which proves the effectiveness of using Guassian diffusion for tabular data synthesis solely. On the other hand, ST-ClavaDDPM falls short on longer-range correlations when compared to the full ClavaDDPM model. This observation provides solid evidence to the efficacy of our multi-table training paradigm. ", "page_idx": 8}, {"type": "text", "text": "Besides the study of the single-table backbone models, we perform a comprehensive ablation study using Berka (for it has the most complex multi-table structure) on each component of ClavaDDPM and provide empirical tuning suggestions. The full results are in Table 2. ", "page_idx": 8}, {"type": "text", "text": "Number of clusters $k$ . We study the necessity of using latent cluster conditioning: (i) no conditioning with $k=1$ ; (ii) many clusters with $k=1000$ to approximate a direct conditioning on parent rows rather than latent variables. When $k=1$ , the quality of long-range correlation degrades drastically. When $k\\:=\\:1000$ , we still get reasonably strong performance, which showcases ClavaDDPM\u2019s robustness. Compared to the default setting $k\\,=\\,20$ ), the metrics are lower in all of cardinality distribution, single column densities, and column correlations \u2014 proper latent variable learning leads to better results than direct conditioning on parent rows. We also report a new metric avg agree-rate, the average of all per-table agree rates for the labels within each foreign key group (Section 4.3.1). This measure highly depends on $k$ , but a higher rate does not always imply a better performance (e.g., $\\scriptstyle{\\mathrm{k}}=1$ achieves perfect rates). We provide more insights on how it varies with the next parameter. We also conducted finer-grained experiments to examine the effect of $k$ on model performance, as shown in Appendix D.2. ", "page_idx": 9}, {"type": "text", "text": "Parent scale $\\lambda$ . Varying the parent scale parameter $\\lambda$ changes the agree-rates as shown in Table 2, but the downstream model performance does not vary too much. This result indicates that the relation-aware clustering process is robust against such factors, and the GMM model is capable of capturing nuances in data patterns. The detailed discussion is in Appendix D.1. ", "page_idx": 9}, {"type": "text", "text": "Classifier gradient scale $\\eta$ . This parameter controls the magnitude of classifier gradients when performing guided sampling, and thus the trade-off between the sample quality and conditional sampling accuracy. Table 2 shows that, when $\\eta=0$ , which essentially disables classifier conditioning, the single column densities (1-way) are slightly higher than the default setting. However, it falls short in capturing long-range correlations. When $\\eta=2$ , the conditioning is emphasized with a higher weight, which significantly improves the modeling of multi-hop correlations compared to $\\eta=0$ case. ", "page_idx": 9}, {"type": "text", "text": "Comparing with no matching for multi-parent dilemma. Berka (Figure 1) suffers from the multiparent dilemma , where the Disposition table has two parent tables, Account and Client. Our abalation study switch the table matching technique to a naive merging of two synthetic table (Appendix C.2). The experiment result show that even if trained with the same hyper parameters and model structures, ClavaDDPM with matching is significantly stronger than the no-matching setup in terms of long-range correlations, with 3-hop correlations $16.70\\%$ higher than no-matching. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We proposed ClavaDDPM as a solution to the intricate problem of synthetic data generation for multi-relational data. ClavaDDPM utilizes clustering on a child table to learn the latent variable that connects the table to its parents, then feeding them to the diffusion models to synthesis the tables. We presented ClavaDDPM\u2019s seamless extension to multiple parents and children cases, and established a comprehensive multi-relational benchmark for a through evaluation \u2013 introducing a new holistic multi-table metric long-range dependency. We demonstrated ClavaDDPM not only competes closely with the existing work on single-table synthesis metrics, but also it outperforms them in ranged (inter-table) dependencies. We deliberately selected the more complex public databases to exhibit ClavaDDPM\u2019s scalability, and introduce it as a confident candidate for a broader impact in industry. ", "page_idx": 9}, {"type": "text", "text": "We focused on foreign key constraints in this work, and made the assumption that child rows are conditionally independent given corresponding parent rows. This brings three natural follow-up research directions: i) extension to the scenarios where this prior information is not available and these relationships need to be discovered first[29], ii) further relaxing the assumptions, and iii) inspecting multi-relational data synthesis with other integrity constraints (e.g, denial constraints[15], general assertions for business rules). Furthermore, we evaluated ClavaDDPM\u2019s privacy with the common (in tabular data literature) DCR metric. Nonetheless, we think it is worthwhile to: i) evaluate the resiliency of ClavaDDPM against stronger privacy attacks[43], and ii) investigate the efficacy of boosting ClavaDDPM with privacy guarantees such as differential privacy. Similarly, the impacts of our design on fairness and bias removal, as another motivating pillar in synthetic data generation, is well worth exploring as future work. We believe the thorough multi-relational modeling formulation we presented in this work, can serve as a strong foundation to build private and fair solutions upon. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was supported by NSERC through a Discovery Grant, an alliance grant, the Canada CIFAR AI Chairs program. Resources used in preparing this research were provided, in part, by the Province of Ontario, the Government of Canada through CIFAR, and companies sponsoring the Vector Institute. We thank the reviewers and program chairs for their detailed comments, which greatly improved our paper. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] A. Alaa, B. Van Breugel, E. S. Saveliev, and M. van der Schaar. How faithful is your synthetic data? sample-level metrics for evaluating and auditing generative models. In International Conference on Machine Learning, pages 290\u2013306. PMLR, 2022. [2] S. A. Assefa, D. Dervovic, M. Mahfouz, R. E. Tillman, P. Reddy, and M. Veloso. Generating synthetic data in finance: opportunities, challenges and pitfalls. In Proceedings of the First ACM International Conference on AI in Finance, pages 1\u20138, 2020. [3] A. Atserias, M. Grohe, and D. Marx. Size bounds and query plans for relational joins. SIAM Journal on Computing, 42(4):1737\u20131767, 2013. [4] P. Berka et al. Guide to the financial data set. PKDD2000 discovery challenge, 2000. [5] K. Cai, X. Xiao, and G. Cormode. Privlava: synthesizing relational data with foreign keys under differential privacy. Proceedings of the ACM on Management of Data, 1(2):1\u201325, 2023.   \n[6] M. Center. Integrated public use microdata series, international: Version 7.3 [data set]. minneapolis, mn: Ipums, 2020.   \n[7] N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer. Smote: synthetic minority over-sampling technique. Journal of artificial intelligence research, 16:321\u2013357, 2002. [8] T. Chen and C. Guestrin. Xgboost: A scalable tree boosting system. In Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining, pages 785\u2013794, 2016. [9] DataCebo. Hmasynthesizer - synthetic data vault, 2023. Accessed on: May 20, 2024.   \n[10] P. Dhariwal and A. Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:8780\u20138794, 2021.   \n[11] T. Dockhorn, T. Cao, A. Vahdat, and K. Kreis. Differentially Private Diffusion Models. Transactions on Machine Learning Research, 2023.   \n[12] W. Dong and K. Yi. Residual sensitivity for differentially private multi-way joins. pages 432\u2013444, 06 2021.   \n[13] C. Dwork, F. McSherry, K. Nissim, and A. Smith. Calibrating noise to sensitivity in private data analysis. In Theory of Cryptography: Third Theory of Cryptography Conference, TCC 2006, New York, NY, USA, March 4-7, 2006. Proceedings 3, pages 265\u2013284. Springer, 2006.   \n[14] J. Fonseca and F. Bacao. Tabular and latent space synthetic data generation: a literature review. Journal of Big Data, 10(1):115, 2023.   \n[15] C. Ge, S. Mohapatra, X. He, and I. F. Ilyas. Kamino: constraint-aware differentially private data synthesis. Proc. VLDB Endow., 14(10):1886\u20131899, jun 2021.   \n[16] S. Ghalebikesabi, L. Berrada, S. Gowal, I. Ktena, R. Stanforth, J. Hayes, S. De, S. L. Smith, O. Wiles, and B. Balle. Differentially private diffusion models generate useful synthetic images. ArXiv, abs/2302.13861, 2023.   \n[17] B. Ghazi, X. Hu, R. Kumar, and P. Manurangsi. Differentially private data release over multiple tables. In Proceedings of the 42nd ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems, PODS \u201923, page 207\u2013219, New York, NY, USA, 2023. Association for Computing Machinery.   \n[18] A. Gonzales, G. Guruswamy, and S. R. Smith. Synthetic data in health care: A narrative review. PLOS Digital Health, 2(1):1\u201316, 01 2023.   \n[19] M. Hernandez, G. Epelde, A. Alberdi, R. Cilla, and D. Rankin. Synthetic data generation for tabular health records: A systematic review. Neurocomputing, 493:28\u201345, 2022.   \n[20] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851, 2020.   \n[21] E. Hoogeboom, D. Nielsen, P. Jaini, P. Forr\u00e9, and M. Welling. Argmax flows and multinomial diffusion: Learning categorical distributions. Advances in Neural Information Processing Systems, 34:12454\u201312465, 2021.   \n[22] X. Hu, S. Sintos, J. Gao, P. K. Agarwal, and J. Yang. Computing complex temporal join queries efficiently. In Proceedings of the 2022 International Conference on Management of Data, SIGMOD \u201922, page 2076\u20132090, New York, NY, USA, 2022. Association for Computing Machinery.   \n[23] jeremy stanley, M. Risdal, sharathrao, and W. Cukierski. Instacart market basket analysis, 2017.   \n[24] J. Kim, C. Lee, and N. Park. Stasy: Score-based tabular data synthesis. arXiv preprint arXiv:2210.04018, 2022.   \n[25] A. Kotelnikov, D. Baranchuk, I. Rubachev, and A. Babenko. Tabddpm: Modelling tabular data with diffusion models. In International Conference on Machine Learning, pages 17564\u201317579. PMLR, 2023.   \n[26] I. Ktena, O. Wiles, I. Albuquerque, S.-A. Rebuff,i R. Tanno, A. G. Roy, S. Azizi, D. Belgrave, P. Kohli, T. Cemgil, A. Karthikesalingam, and S. Gowal. Generative models improve fairness of medical classifiers under distribution shifts. Nature Medicine, 30(4):1166\u20131173, apr 2024.   \n[27] N. Lawrence and A. Hyv\u00e4rinen. Probabilistic non-linear principal component analysis with gaussian process latent variable models. Journal of machine learning research, 6(11), 2005.   \n[28] C. Lee, J. Kim, and N. Park. Codi: co-evolving contrastive diffusion models for mixed-type tabular synthesis. ICML\u201923. JMLR.org, 2023.   \n[29] F. Li, B. Wu, K. Yi, and Z. Zhao. Wander join: Online aggregation via random walks. SIGMOD \u201916, page 615\u2013629, New York, NY, USA, 2016. Association for Computing Machinery.   \n[30] P. Li and S. Chen. A review on gaussian process latent variable models. CAAI Transactions on Intelligence Technology, 1(4):366\u2013376, 2016.   \n[31] C. A. Mami, A. Coser, E. Medvet, A. T. Boudewijn, M. Volpe, M. Whitworth, B. Svara, G. Sgroi, D. Panflio, and S. Saccani. Generating realistic synthetic relational data through graph variational autoencoders. arXiv preprint arXiv:2211.16889, 2022.   \n[32] J. Motl and O. Schulte. The ctu prague relational learning repository. arXiv preprint arXiv:1511.03086, 2015.   \n[33] H. Nickisch and C. E. Rasmussen. Gaussian mixture modeling with gaussian process latent variable models. In Joint Pattern Recognition Symposium, pages 272\u2013282. Springer, 2010.   \n[34] NIST. 2018 differential privacy synthetic data challenge, 2018. Accessed: 2024-05-17.   \n[35] N. Patki, R. Wedge, and K. Veeramachaneni. The synthetic data vault. In 2016 IEEE international conference on data science and advanced analytics (DSAA), pages 399\u2013410. IEEE, 2016.   \n[36] V. K. Potluru, D. Borrajo, A. Coletta, N. Dalmasso, Y. El-Laham, E. Fons, M. Ghassemi, S. Gopalakrishnan, V. Gosai, E. Krea\u02c7ci\u00b4c, G. Mani, S. Obitayo, D. Paramanand, N. Raman, M. Solonin, S. Sood, S. Vyetrenko, H. Zhu, M. Veloso, and T. Balch. Synthetic data applications in finance. arXiv preprint arXiv:2401.00081, 2024.   \n[37] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684\u201310695, 2022.   \n[38] O. Schulte and Z. Qian. Factorbase: multi-relational structure learning with sql all the way. In International Journal of Data Science and Analytics, Int J Data Sci Anal 7, page 289\u2013309. Springer International Publishing AG, 2019.   \n[39] O. Schulte, Z. Qian, A. E. Kirkpatrick, X. Yin, and Y. Sun. Fast learning of relational dependency networks. Machine Learning, 103:377\u2013406, 2016.   \n[40] J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pages 2256\u20132265. PMLR, 2015.   \n[41] J. Song, C. Meng, and S. Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020.   \n[42] Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020.   \n[43] T. Stadler, B. Oprisanu, and C. Troncoso. Synthetic data \u2013 anonymisation groundhog day. In 31st USENIX Security Symposium (USENIX Security 22), pages 1451\u20131468, Boston, MA, Aug. 2022. USENIX Association.   \n[44] B. van Breugel, T. Kyono, J. Berrevoets, and M. van der Schaar. Decaf: Generating fair synthetic data using causally-aware generative networks. In Advances in Neural Information Processing Systems, volume 34, pages 22221\u201322233. Curran Associates, Inc., 2021.   \n[45] B. van Breugel, N. Seedat, F. Imrie, and M. van der Schaar. Can you rely on your model evaluation? improving model evaluation with synthetic test data. In Advances in Neural Information Processing Systems, 2023.   \n[46] B. van Breugel and M. van der Schaar. Beyond privacy: Navigating the opportunities and challenges of synthetic data. arXiv preprint arXiv:2304.03722, 2023.   \n[47] L. Xu, M. Skoularidou, A. Cuesta-Infante, and K. Veeramachaneni. Modeling tabular data using conditional gan. Advances in neural information processing systems, 32, 2019.   \n[48] J. Young, P. Graham, and R. Penny. Using bayesian networks to create synthetic data. Journal of Official Statistics, 25(4):549\u2013567, Dec 2009.   \n[49] R. Zemel, R. Urtasun, and S. Fidler. Mixture of gaussians and em. https://www.cs.toronto. edu/\\~urtasun/courses/CSC411_Fall16/13_mog.pdf.   \n[50] H. Zhang, J. Zhang, B. Srinivasan, Z. Shen, X. Qin, C. Faloutsos, H. Rangwala, and G. Karypis. Mixed-type tabular data synthesis with score-based diffusion in latent space. In The twelfth International Conference on Learning Representations, 2024.   \n[51] Z. Zhang, T. Wang, N. Li, J. Honorio, M. Backes, S. He, J. Chen, and Y. Zhang. PrivSyn: Differentially private data synthesis. In 30th USENIX Security Symposium (USENIX Security 21), pages 929\u2013946. USENIX Association, Aug. 2021.   \n[52] Z. Zhao, A. Kunar, R. Birke, and L. Y. Chen. Ctab-gan: Effective table data synthesizing. In Asian Conference on Machine Learning, pages 97\u2013112. PMLR, 2021.   \n[53] S. Zheng and N. Charoenphakdee. Diffusion models for missing value imputation in tabular data. arXiv preprint arXiv:2210.17128, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Notation Summary ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We use boldface to represent random variables. e.g. $Y\\sim Y$ , where $Y$ is the data of $R_{2}$ , and $\\mathbf{\\deltaY}$ is the random variable $Y$ being sampled from. In addition, we use subscript to represent the parent row some data or random variable refers to. e.g., $\\pmb{x}_{j}$ represents the child random variable that refers to parent $\\pmb{y}_{j}$ . The important notations used in the paper are summarized in Table 3. ", "page_idx": 13}, {"type": "table", "img_path": "Zb2ixT19VF/tmp/d90ab2e133cb959925c94b469db1a169a6e21aff112868764b42a29632319dff.jpg", "table_caption": ["Table 3: Notation summary "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "B Algorithm Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "B.1 Diagram for two-table relational databases ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Figure 2 summarizes the generative process for two-table cases. ", "page_idx": 13}, {"type": "image", "img_path": "Zb2ixT19VF/tmp/b3d7b312d4098029da07b23c6b3e25c1571661c7bfc4e55a55ec7f6c50a07b2b.jpg", "img_caption": ["Figure 2: ClavaDDPM overview for a two-table relational database "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "B.2 End-to-end algorithms for more tables ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We detail the end-to-end algorithms for the three phases of ClavaDDPM, including (i) latent learning and table augmentation, (ii) training, and (iii) synthesis. ", "page_idx": 13}, {"type": "text", "text": "Latent learning and table augmentation. As shown in Algorithm 1, given a database $\\mathcal{R}\\,=$ $\\{R_{1},\\ldots,R_{m}\\}$ and foreign key constraint graph $\\mathcal{G}$ , we learn the set of latent variables $C_{i,j}$ for every pair of parent-child $(R_{i}\\to R_{j})\\in\\mathcal G.\\mathcal E$ and augment all the latent variables to the parent table and the child table, denoted by $T_{j}$ and $T_{i}^{\\prime}$ , respectively. We initialize each augment table with its original table (line 1). This algorithm follows a bottom-up topological order starting from the leaf child with its parent (line 2), ensuring each child table is already augmented by the time we learn the latent variable to augment its parent. For each parent-child pair $R_{i}\\to R_{j}$ , we join $T_{i}$ (not $T_{i}^{\\prime}$ ) with $R_{j}$ into a single table $(X;Y)$ (line 3) and then run the clustering algorithm using GMM and maximum voting described in Section 4.3.1. We append the corresponding clustering labels $C_{i,j}$ to the augmented parent table $T_{j}$ and augmented child table $T_{i}$ , respectively. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "table", "img_path": "Zb2ixT19VF/tmp/bf7cfdb075aba9e53fffac5b2e7cb12e71ffa91e1d8c5a3fc84cde01ad7e7041.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "Training. As shown in Algorithm 2, the training phase takes in the augmented parent tables $\\{T_{1},\\dotsc,\\bar{T}_{m}\\}$ and the foreign key constraint graph $\\mathcal{G}$ . For each augmented table $T_{j}$ , we train a diffusion model $p_{T_{j}}$ (lines 2-4). Then, for each parent-child pair $R_{i}\\to R_{j}$ (lines 5-7), we train a child classifier $p_{\\phi}(c_{i,j}|x)$ with $R_{i}$ \u2019s child augment table $T_{i}^{\\prime}$ , where the latent column $C_{i,j}$ is used as labels, and all remaining columns are used as training data (including the augmented latent columns corresponding to $R_{i}$ \u2019s children). Using the same table, we also estimate the foreign key group size distribution conditioned on the latent variable $p(s|c_{i,j})$ . ", "page_idx": 14}, {"type": "text", "text": "Algorithm 2 ClavaDDPM: Training ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Input: augmented parent tables $\\{T_{1},...,T_{m}\\}$ , augmented child tables $\\{T_{1}^{\\prime},\\dots,T_{m}^{\\prime}\\}$ , foreign key constraint graph $\\mathcal{G}$ Output: diffusion models $\\mathcal{D}$ , classifiers $\\mathcal{C}$ , group size distributions $\\boldsymbol{S}$ 1: Initialize $\\mathcal{D},\\mathcal{C},\\mathcal{S}\\leftarrow\\emptyset$ 2: for $R_{j}$ in $\\mathcal{G R}$ do 3: Train $p_{T_{j}}$ with $T_{j}$ , and add to $\\mathcal{D}$ 4: end for 5: for $\\left[R_{i}\\to R_{j}\\right)$ in topological order of $\\mathcal{G}$ do 6: Learn classifier $p_{\\phi}(c_{i,j}|x)$ and $p(s|c_{i,j})$ using with $T_{i}^{\\prime}$ (ignoring irrelevant latent columns) and add to $\\mathcal{C}$ and $\\boldsymbol{S}$ respectively 7: end for ", "page_idx": 14}, {"type": "text", "text": "Synthesis. Algorithm 3 takes in learned diffusion models $\\mathcal{D}$ , classifiers $\\mathcal{C}$ , group size distributions $\\boldsymbol{S}$ , and the DAG representation of the database $\\mathcal{G}$ , and outputs the synthetic database $\\tilde{\\mathcal{R}}\\;=\\;\\left\\{\\tilde{R}_{1},\\ldots,\\tilde{R}_{m}\\right\\}$ . We first initialize the synthetic augmented tables to be empty (line 1). Then, for root augmented tables, since they have no parents to condition on, they can be directly synthesized from their diffusion models (line 2-4). Next, we traverse the database in topological order to synthesize the remaining augmented tables (line 5-16): If we have already synthesized $\\tilde{T}_{i}$ before, which means we encounter the multi-parent dilemma, we just store the old version and continue to generate a new version (line 6-9). For each parent-child relationship $R_{i}\\to R_{j}$ , we must have already sampled the augmented parent table $\\tilde{T}_{j}$ . This is because we follow the topological order of a DAG, and all root augmented tables have been synthesized as base cases. Therefore, we can obtain the synthetic latent variables $\\Tilde{C}_{i,j}$ from the synthetic augmented parent $\\tilde{T}_{j}$ (line 10). Then, we iterate through each synthetic latent value $\\tilde{c}_{i,j}$ and perform a two-step sampling: (1) use the learned group size distribution to conditionally sample a group size $\\tilde{s}$ (line 12); (2) sample s\u02dc rows of data conditioned on $\\tilde{c}_{i,j}$ using classifier guided sampling (line 13). We repeat this process until the augmented child table $\\tilde{T}_{i}$ is fully synthesized. We simply obtain synthetic tables from synthetic augmented tables by removing all synthetic latent columns (line 17-19). Finally, for all the encountered multi-parent dilemmas, we follow Section 4.3.3 to match the divergent versions. ", "page_idx": 14}, {"type": "text", "text": "Algorithm 3 ClavaDDPM: Synthesis Input: diffusion models $\\mathcal{D}$ , classifiers $\\mathcal{C}$ , group size distributions $\\boldsymbol{S}$ , foreign key constraint graph   \n$\\mathcal{G}$ Output: Synthetic tables $\\left\\{\\tilde{R}_{1},\\ldots,\\tilde{R}_{m}\\right\\}$ 1: Initialize $\\tilde{T}_{1},\\dots,\\tilde{T}_{m}\\gets\\emptyset,\\dots,\\emptyset$ 2: for $R_{j}$ in root nodes do 3: Sample $\\tilde{T}_{j}\\sim p_{T_{j}}$ 4: end for 5: for $\\left(R_{i}\\to R_{j}\\right)$ ) in topological order of $\\mathcal{G}$ do 6: if $\\tilde{T}_{i}$ already synthesized then 7: Store $\\tilde{T_{i}}$ as $\\tilde{T}_{i,k}$ , where $k$ is the parent that synthesized $\\tilde{T}_{i,k}$ 8: Reinitialize $\\tilde{T}_{i}\\gets\\emptyset$ 9: end if   \n10: Split $\\tilde{T}_{j}$ into $\\left(\\cdot\\;;\\tilde{C}_{i,j}\\right)$   \n11: for $\\tilde{c}_{i,j}$ in $\\Tilde{C}_{i,j}$ do   \n12: Sample $\\tilde{s}\\sim p\\left(s\\mid\\tilde{c}\\right)$   \n13: Classifier-guided sample $\\tilde{s}$ rows of data: $\\tilde{t}_{i}\\sim p\\left(t_{i}\\mid\\tilde{c}_{i,j}\\right)$   \n14: Append $\\tilde{t}_{i}$ to $\\tilde{T}_{i}$   \n15: end for   \n16: end for   \n17: for $\\tilde{T}_{j}$ in all synthetic augmented tables do   \n18: $\\tilde{R}_{j}\\gets$ all latent columns removed from $\\tilde{T}_{j}$   \n19: end for   \n20: for $\\tilde{R}_{j}$ with multiple synthetic versions do   \n21: $\\overset{\\,\\,\\,\\sim\\,}{R_{j}}\\gets\\mathbf{MATCHING}\\Big(\\Tilde{R}_{j,p_{1}},\\,\\cdot\\,\\cdot\\,,\\Tilde{R}_{j,p_{q}}\\Big)$   \n22: end for ", "page_idx": 15}, {"type": "table", "img_path": "Zb2ixT19VF/tmp/bb095ff5f3610bed57553f1d4ca964d2a4c036de1440b717fa2e36d0c6c3b578.jpg", "table_caption": [], "table_footnote": ["Table 4: Dataset Specifics "], "page_idx": 15}, {"type": "text", "text": "C Experimental Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "C.1 Datasets ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Here we describe the real-world datasets used in our evaluation in detail. The specifics of datasets are in Table 4. ", "page_idx": 15}, {"type": "text", "text": "California: The California dataset is a real-world census database ([6]) on household information. It consists of two tables in the form of a basic parent-child relationship. ", "page_idx": 15}, {"type": "text", "text": "Instacart 05: The Instacart 05 is created by downsampling 5-percent from the Kaggle competition dataset Instacart ([23]), which is a real-world transaction dataset of instacart orders. This dataset consists of 6 tables in total with a maximum depth of 3. ", "page_idx": 15}, {"type": "text", "text": "Berka: The Berka dataset is a real-world financial transaction dataset ([4]), consisting of 8 tables with a maximum depth of 4. This will be the main dataset in our work for ablation study and model anatomy. ", "page_idx": 15}, {"type": "text", "text": "Movie Lens: The Movie Lens dataset ([39], [32]) consists of 7 tables with a maximum depth of 2.   \nThis dataset exhibits complex multi-parent and multi-children structures. ", "page_idx": 15}, {"type": "text", "text": "CCS: The CCS dataset ([32]) is a real-world transactional dataset Czech debit card company. It consists of 5 tables with a maximum depth of 2, which exhibits complex multi-parent and multichildren patterns. ", "page_idx": 16}, {"type": "text", "text": "C.2 Baselines ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We adopt two multi-relational synthesis models in literature as our baselines: PrivLava [5] as a representative of state-of-the-art marginal-based methods, and SDV [35] as a statistical method specially designed for multi-relational synthesis. In addition, we introduce two types of multirelational synthesis pipelines, SingleT and Denorm, as our additional baselines. For the additional baselines, we use CTGAN ([47]) and TabDDPM ([25]) as backbone models, representing the stateof-the-art tabular synthesis algorithms with GAN-based models and diffusion-based models. In the following, we describe the high-level ideas of Single-T and Denorm. ", "page_idx": 16}, {"type": "text", "text": "Single-T: Given a single-table backbone model, we first learn and synthesize each table individually. Then, for each parent-child table pair $(p,c)$ , we assign foreign keys to the synthetic child table $\\tilde{R_{c}}$ by randomly sampling group sizes in the real table $R_{c}$ , which enforces the synthetic group size distributions to be similar to real ones. ", "page_idx": 16}, {"type": "text", "text": "Denorm: For each parent-child table pair $(p,c)$ , we join the table into $R_{p,c}$ , then use the single-table backbone model to synthesize the joint table $\\tilde{R}_{p,c}$ . Finally, we split $\\tilde{R}_{p,c}$ into two synthetic tables $\\tilde{R}_{p}$ and $\\tilde{R}_{c}$ as follows: (1) Lexicographically sort $\\tilde{R}_{p,c}$ , where the parent columns are prioritized. This guarantees that similar parent records are grouped together. (2) From the real table $R_{c}$ , randomly sample group sizes $\\tilde{g}$ with replacement. Then, for each sampled $\\tilde{g}$ , the consecutive $\\tilde{g}$ rows in $\\tilde{R}_{p,c}$ will be taken as a synthetic foreign key group $\\tilde{g}_{p,c}$ . The child columns part of $\\tilde{g}_{p,c}$ will be assigned the same foreign key and appended to the child synthetic table $\\tilde{R}_{c}$ . Then, we randomly sample a parent row in $\\tilde{g}_{p,c}$ and append to the parent synthetic table $\\tilde{R}_{p}$ . We follow the exact same way as in ClavaDDPM to extend 2-table Denorm to the entire database. ", "page_idx": 16}, {"type": "text", "text": "Random matching: We conduct ablation study by training a ClavaDDPM model with the same setup as the default setting, while instead of performing table matching to handle the multi-table dilemma, it performs a naive merging of two synthetic tables. For the diverged synthetic tables $\\tilde{R}_{D,A}$ and $\\tilde{R}_{D,C}$ , where $\\tilde{R}_{D,A}$ is the Disposition table synthesized conditioned on the Account table, and $\\tilde{R}_{D,C}$ is conditioned on the Client table, we simply keep $\\tilde{R}_{D,A}$ , and randomly assign the $(D,C)$ foreign keys from $\\tilde{R}_{D,C}$ to $\\tilde{R}_{D,A}$ . ", "page_idx": 16}, {"type": "text", "text": "C.3 Implementation Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "C.3.1 Classifier Training ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We use an MLP for classifier with layers 128, 256, 512, 1024, 512, 256, 128. The output layer size is adapted to the number of clusters $k$ . We use learning rate of $1e-4$ , and optimize with AdamW optimizer, and use cross entropy loss as objective. The overall training paradigm follows [10], where we incorporate timestep information by encoding the timesteps into sinusoidal embeddings, which are then added to the data. For experiments on California, we train the classifier for 10000 iterations, and for all other datasets we train 20000 iterations. ", "page_idx": 16}, {"type": "text", "text": "C.3.2 Hyper Parameters ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Baseline models. PrivLava was run under a non-private setup by setting privacy budget $\\epsilon=50$ , and the datasets are prepossessed spesifically for PrivLava to have domain sizes less than 200. ", "page_idx": 16}, {"type": "text", "text": "For all models with ClavaDDPM or TabDDPM backbones, we use the same set of hyper parameters. We set diffusion timesteps to 2000, and use learning rate of $6e-4$ . In terms of model architecture, we use MLP with layer sizes 512, 1024, 1024, 1024, 1024, 512. The model architecture details are following the implementation of TabDDPM [25]. All DDPM-based models are trained 100, 000 iterations on California dataset, and 200, 000 on other datasets. ", "page_idx": 16}, {"type": "table", "img_path": "Zb2ixT19VF/tmp/5f1b7a55df45cc5167cb6e40a05cafb4ad8999ca944ec422904ddd7550a3c287.jpg", "table_caption": [], "table_footnote": ["Table 5: Hyper parameters of ClavaDDPM on each dataset. "], "page_idx": 17}, {"type": "text", "text": "We conducted CTGAN experiments using the interface from SDV library, and follows the default parameters, where the learning rates for the generator and discriminator are both $2e-4$ , and is trained 300 epochs. ", "page_idx": 17}, {"type": "text", "text": "PrivLava\u2019s code is not publicly available, and we directly followed the authors\u2019 settings. Note that PrivLava requires a privacy budget searching process, and $\\epsilon=50$ is the largest working privacy budget according to our experiments, where larger $\\epsilon$ leads to failure. We consider this as large enough to resemble a non-private setting. ", "page_idx": 17}, {"type": "text", "text": "For SDV, we used the default setting of their HMASynthesizer, which by default uses a Gaussian Copula synthesizer. ", "page_idx": 17}, {"type": "text", "text": "ClavaDDPM settings. We list the major hyper parameters used by ClavaDDPM for each dataset in Table 5, and we provide an empirical guidance for hyper parameter tuning: it is suggested to use number of clusters $k$ to be at least 20, and classifier scale $\\eta$ to be in [0.5, 2]. We empirically find ClavaDDPM consistently perform well in such a range across all datasets. Parent scale $\\lambda$ is a less sensitive factor, and $\\lambda=1$ is a stable starting point for tuning. In general, ClavaDDPM is robust, with a small hyper parameter space, and there is very little need for tuning. ", "page_idx": 17}, {"type": "text", "text": "C.3.3 Metrics ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "C2ST. The Classifier Two Sample Test trains a logistic regression classifier to distinguish synthetic data from real data. We consider this metric as a high-level reflection of data fidelity. ", "page_idx": 17}, {"type": "text", "text": "Machine Learning Efficacy. Different from prior works that evaluate MLE utilities [25, 52, 50], who work on datasets with predefined machine learning tasks, the five real-world multi-relational datasets we use do not come with a designated downstream task. In addition, the prior knowledge about which column will be used for downstream predictions will introduce significant inductive bias to the training process, especially for models capable of performing task-oriented training. To avoid such issue, we evaluate machine learning efficacy on each of the columns. To be specific, each time we select a column as target, and train an XGBoost [8] model on remaining columns. For categorical target columns, we perform regression and evaluate $R^{2}$ , and for categorical target columns we perform classification and evaluate $F_{1}$ . The overall MLE is measured by average $\\breve{R}^{2}$ and average $F_{1}$ across all columns. ", "page_idx": 17}, {"type": "text", "text": "To evaluate the single-table MLE on synthetic data generated from multi-table synthesis process, instead of performing an independent train-test split on each table, we split by foreign key relationship. e.g. for California dataset, we first perform a random $90\\%$ , $10\\%$ split on the parent table Household, and then we follow the foreign key constraints to assign corresponding child rows, i.e. Individuals to the corresponding buckets. Note that although this splitting method does not lead to the same train/test ratio on child table, we consider such sampling to be foreign key relationship preserving, which is a more important property in the context of multi-table synthesis. ", "page_idx": 17}, {"type": "text", "text": "D Additional Experiments ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "D.1 Agree rate discussion ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "As introduced in Section 4.3.1, within each foreign key group, we perform a majority voting to synchronize the assigned cluster label among the group. To measure the consistency of such majority voting process, we introduce the measurement of agree rate, which computes the average ratio of agreeing on the mode within each foreign key group, and the metric avg agree-rate is the average of all per-table agree rates within a multi-table dataset. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "equation", "text": "$$\nA(g)={\\frac{m_{g}}{|g|}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "$A(g)$ represents the agree rate of some group $g$ , where $m_{g}$ represents the number of records that are assigned the mode cluster within $g$ , and the avg agree-rate ", "page_idx": 18}, {"type": "equation", "text": "$$\nA_{\\mathrm{AVG}}={\\frac{1}{|G|}}\\sum_{g\\in G}A(g)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "is computed as the average of the agree rates across all groups. ", "page_idx": 18}, {"type": "text", "text": "However, our experiment results in Table 2 indicate that the relationship-aware clustering process is robust against such factors, and the GMM model is capable of capturing nuances in data patterns. In our experiments on Berka dataset, ClavaDDPM\u2019s clustering process achieves a consistent agree rate around $81\\%$ , which is practically high enough given we have 20 clusters. Intuitively, when parent scale approaches infinity, the clustering is performed completely on parent table, which will lead to a perfect agree rate. Also note that a higher agree rate does not always imply a better performance, and the disagreement can potentially come from the intrinsic parent-child relationships. e.g. when child data is intrinsically independent of parent data, it is reasonable to have noisy learned latent variables, leading to low agree rates. However, in such cases the noisy latent variable would not degenerate model performance, because the best strategy will be a direct sampling of child table, rather than conditioning on some enforced prior distribution. In addition, as shown in Table 2, agree rates are highly affected by the number of clusters chosen, and there exists a trade-off between the granularity in clustering and consistency. In an extreme case where we have $k=1$ cluster, indicating an infinitely coarse-grained latent learning, it trivially achieves perfect agree rates. ", "page_idx": 18}, {"type": "text", "text": "D.2 Selecting Number of Clusters $k$ ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We conducted finer-grained experiments to examine the effect of the number of clusters $k$ on model performance, as shown in Figure 3, which offer empirical insights for selecting $k$ . Based on the results, (1) a binary search approach could be used to efficiently find a suitable $k$ , and (2) while it may require more computational resources, opting for a larger $k$ is generally a safe choice. ", "page_idx": 18}, {"type": "image", "img_path": "Zb2ixT19VF/tmp/7a637ff96cca174f1f919461f00a0e033ddc207caa216e20c4f4fc89e6233629.jpg", "img_caption": ["Figure 3: Smoothed model performance on Berka dataset regarding different $k$ (measured by AVG 2-way), where $k=\\infty$ represents assigning each row a unique class. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "D.3 High-order Single-table Evaluation ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We also consider higher-order single-table evaluation metrics for the quality of some representative tables as prior work [50]: 1) $\\alpha$ -precision and $\\beta$ -recall [1] to measure fidelity and diversity of synthetic data; 2) Machine Learning Efficacy (MLE) to measure the downstream-task utility; 3) Classifier Two Sample Test (C2ST) to measure if the synthetic data is distinguishable from real data by machine learning models. ", "page_idx": 18}, {"type": "table", "img_path": "Zb2ixT19VF/tmp/5aecb8b0a40294510fc8b2e112817a18176dbda95d8bbd1967c751dc2087184b.jpg", "table_caption": [], "table_footnote": ["Table 6: High-level single-table metrics evaluated on the Household table and the Individual table from the California dataset. "], "page_idx": 19}, {"type": "table", "img_path": "Zb2ixT19VF/tmp/6562311c10f81afa0a3e02ffe9fc6d639abe9bdff4cf7d0957a1ed6732068ff5.jpg", "table_caption": ["Table 7: Median DCR comparison between ClavaDDPM and SMOTE. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "We evaluated high-order single-table metrics on the California dataset across all baseline models and ClavaDDPM. Following [50], for the evaluation of MLE we perform a $90\\%$ , $10\\%$ train-test split, where the $F_{1}$ and $R^{2}$ metrics are evaluated on the $10\\%$ holdout set. Note that although PrivLava has an advantage on the California dataset when evaluated with statistical tests (Table 1), ClavaDDPM exhibits competitive, or even stronger performance than PrivLava on higher-order metrics. Especially for MLE, ClavaDDPM surpasses PrivLava by $13.07\\%$ in terms of average $R^{2}$ in Individual table, and also beats PrivLava on average $F_{1}$ in both tables. Also notice that the baseline ST-CLAVADDPM dominates in high-order metric evaluations, demonstrating the strength of our Gaussian diffusion-only backbone model. ", "page_idx": 19}, {"type": "text", "text": "ClavaDDPM achieves a second-highest $\\beta.$ -recall on Household table and ranks first in $\\beta$ -recall on Individual table with large margin, gaining a $7.65\\%$ advantage over the best baseline without ClavaDDPM backbone. This serves as strong evidence that ClavaDDPM is not only data fidelity preserving, but is also capable of generating highly diverse data. ", "page_idx": 19}, {"type": "text", "text": "D.4 Privacy Sanity Check ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We follow TabDDPM [25] to perform a privacy sanity check against SMOTE [7], which is an interpolation-based method that generates new data through convex combination of a real data point with its nearest neighbors. We use the median Distance to Closest Record (DCR) [52] to quantify the privacy level. We compare the median DCR, as well as DCR distributions of ClavaDDPM against SMOTE on selected tables. ", "page_idx": 19}, {"type": "text", "text": "As shown in table 7, ClavaDDPM although neither specialized in privacy preserving, nor in single table synthesis, it still maintains a reasonable privacy level. The charts 4 demonstrates the distributions of DCR scores, where ClavaDDPM is in blue. The overall distribution is more leaning to the right side, indicating an overall higher DCR distribution. ", "page_idx": 19}, {"type": "image", "img_path": "Zb2ixT19VF/tmp/7b3c32a1d10032e19c476a40967ff1aa0f77507fb7d56cedab6faa8ec4fb0d06.jpg", "img_caption": ["Figure 4: DCR distributions of four selected tables. The $y$ axis is log-scaled for better presentation. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "E Complexity Analysis ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Given a multi-relational database $\\mathcal{G}\\,=\\,(\\mathcal{R},\\mathcal{E})$ , with $m$ tables, $n$ foreign key constraints, and $p$ rows per table. For a $p$ -row table, we denote the time complexity of performing GMM clustering as $c_{\\mathrm{GMM}}(p)$ , training a diffusion model as $c_{\\mathrm{diff}}(p)$ , training a classifier as $c_{\\mathrm{class}}(p)$ , synthesizing as $\\ensuremath{c_{\\mathrm{syn}}}(p)$ , ANN searching as $c_{\\mathrm{ANN}}(p)$ . ", "page_idx": 20}, {"type": "text", "text": "Phase 1: latent learning and table augmentation ", "text_level": 1, "page_idx": 20}, {"type": "equation", "text": "$$\nn\\cdot c_{\\mathrm{GMM}}(p)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Phase 2: training ", "text_level": 1, "page_idx": 20}, {"type": "equation", "text": "$$\nn\\cdot c_{\\mathrm{class}}(p)+m\\cdot c_{\\mathrm{diff}}(p)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Note that in practice this phase is dominated by diffusion training, primarily influenced by $m$ . ", "page_idx": 20}, {"type": "text", "text": "Phase 3: synthesis ", "text_level": 1, "page_idx": 20}, {"type": "equation", "text": "$$\nn\\cdot c_{\\mathrm{syn}}(p)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Additional step: matching ", "text_level": 1, "page_idx": 20}, {"type": "equation", "text": "$$\nn\\cdot c_{\\mathrm{ANN}}(p)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Note that the runtime in this phase is negligible compared to the earlier phases, particularly with the FAISS implementation in the non-unique matching setup. ", "page_idx": 20}, {"type": "text", "text": "Total ", "text_level": 1, "page_idx": 20}, {"type": "equation", "text": "$$\nn\\left(c_{\\mathrm{GMM}(p)+c_{\\mathrm{class}}}(p)+c_{\\mathrm{syn}}(p)+c_{\\mathrm{ANN}}(p)\\right)+m c_{\\mathrm{diff}}(p)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "the overall runtime is dominated by Phase 2 (training) and Phase 3 (synthesis), with the critical factors being $m,n$ , and $p$ . The model remains robust against the number of clusters in Phase 1, as the impact on runtime is minimal due to the dominance of the later phases. ", "page_idx": 20}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We provide empirical evidence in the evaluation section for the claims made in the abstract and introduction. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: In the conclusion section (second paragraph), we discuss the limitations of our work and our interest in addressing them as future work. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We list all the assumptions required to derive our generative process. We do not have any proof. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We detail the datasets, baseline algorithms, implementation details of our algorithms, and evaluation metrics in the appendix. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 22}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? [Yes] Justification: We upload supplementary materials including code for reproducibility. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 23}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We provide the basic experimental setting/details, ablation study in the evaluation section, and details in the appendix. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 23}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We repeat experiments for each configuration three times and report their averaged performance with standard deviations. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We include computing specifics in the evaluation section. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethic https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: We carefully go through the NeurIPS Code of Ethic and ensure we follow it. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 24}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: We discuss the broader impacts of our work in the conclusion section. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: We use public datasets for experiments. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 25}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: We add citations and resources for the datasets and source code used in the evaluation. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 25}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 26}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We document our new code and and provide references for all code adopted from public resources. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 26}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 26}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 26}]