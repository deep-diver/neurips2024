{"references": [{"fullname_first_author": "Zahavy", "paper_title": "Discovering policies with domino: Diversity optimization maintaining near optimality", "publication_date": "2023-05-01", "reason": "This paper introduces DOMINO, a key algorithm that DUPLEX builds upon and improves, focusing on achieving a balance between diversity and near-optimality in reinforcement learning."}, {"fullname_first_author": "Borsa", "paper_title": "Universal successor features approximators", "publication_date": "2019-05-06", "reason": "This paper introduces Universal Successor Feature Approximators (USFAs), a crucial component of DUPLEX that enhances generalization and improves policy diversity estimates."}, {"fullname_first_author": "Barreto", "paper_title": "Successor features for transfer in reinforcement learning", "publication_date": "2017-00-00", "reason": "This foundational paper introduces successor features, a core concept in DUPLEX's approach to estimating policies' expected behavior and promoting diversity."}, {"fullname_first_author": "Schaul", "paper_title": "Universal value function approximators", "publication_date": "2015-00-00", "reason": "This paper introduces Universal Value Function Approximators (UVFAs), which inspired the concept of universal estimators in DUPLEX for handling multiple tasks and policies."}, {"fullname_first_author": "Haarnoja", "paper_title": "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor", "publication_date": "2018-07-10", "reason": "This paper introduces the Soft Actor-Critic (SAC) algorithm, which DUPLEX uses as a foundation and incorporates entropy regularization for improved successor feature estimation."}]}