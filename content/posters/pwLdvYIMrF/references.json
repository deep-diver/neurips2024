{"references": [{"fullname_first_author": "T. Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-XX-XX", "reason": "This paper is foundational to the field of large language models (LLMs), which are central to the topic of continual knowledge learning (CKL) explored in the main paper."}, {"fullname_first_author": "J. Jang", "paper_title": "Towards continual knowledge learning of language models", "publication_date": "2021-XX-XX", "reason": "This paper is one of the earliest works on continual knowledge learning (CKL) for LLMs, which the main paper builds upon and contrasts with."}, {"fullname_first_author": "J. Jang", "paper_title": "TemporalWiki: A lifelong benchmark for training and evaluating ever-evolving language models", "publication_date": "2022-XX-XX", "reason": "This paper introduces a benchmark dataset for evaluating CKL methods which is used in the current paper for comparison and analysis."}, {"fullname_first_author": "F. Petroni", "paper_title": "Language models as knowledge bases?", "publication_date": "2019-XX-XX", "reason": "This paper is highly relevant due to its introduction of the LAMA dataset, which forms the basis for the LAMA-CKL benchmark used in the main paper."}, {"fullname_first_author": "T. Dettmers", "paper_title": "QLoRA: Efficient finetuning of quantized LLMs", "publication_date": "2024-XX-XX", "reason": "This paper presents a highly efficient fine-tuning method, QLoRA, which is used as a baseline in the main paper's experiments."}]}