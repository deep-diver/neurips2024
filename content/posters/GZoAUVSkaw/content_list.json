[{"type": "text", "text": "First-Order Minimax Bilevel Optimization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yifan Yang\\*, Zhaofeng $\\mathbf{Si}^{*}$ , Siwei Lyu and Kaiyi Ji Department of Computer Science and Engineering University at Buffalo Buffalo, NY 14260 Lyyang99, zhaofeng, siweilyu, kaiyiji}@buffalo.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Multi-block minimax bilevel optimization has been studied recently due to its great potential in multi-task learning, robust machine learning, and few-shot learning. However, due to the complex three-level optimization structure, existing algorithms often suffer from issues such as high computing costs due to the second-order model derivatives or high memory consumption in storing all blocks\u2019 parameters. In this paper, we tackle these challenges by proposing two novel fully first-order algorithms named FOSL and MemCS. FOSL features a fully single-loop structure by updating all three variables simultaneously, and MemCS is a memory-efficient double-loop algorithm with cold-start initialization. We provide a comprehensive convergence analysis for both algorithms under full and partial block participation, and show that their sample complexities match or outperform those of the same type of methods in standard bilevel optimization. We evaluate our methods in two applications: the recently proposed multi-task deep AUC maximization and a novel rank-based robust meta-learning. Our methods consistently improve over existing methods with better performance over various datasets. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In this paper, we study a general multi-block minimax bilevel optimization problem given by ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\underset{x\\in\\mathbb{R}^{d_{x}}}{\\mathrm{min}}\\underset{y\\in\\mathbb{R}^{d_{y}}}{\\mathrm{max}}F(x,y,\\mathbf z^{*}):=\\frac{1}{n}\\sum_{i=1}^{n}f_{i}\\big(x,y,z_{i}^{*}(x)\\big)=\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}_{\\xi}\\big[f_{i}\\big(x,y,z_{i}^{*}(x);\\xi_{i}\\big)\\big]}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\mathrm{s.t.}\\ z_{i}^{*}(x)=\\underset{z\\in\\mathbb{R}^{d_{z}}}{\\mathrm{arg}\\mathrm{min}}\\ g_{i}(x,z)=\\mathbb{E}_{\\zeta}\\big[g_{i}(x,z;\\zeta_{i})\\big]}\\end{array}\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "where the upper- and lower-level function $f_{i}$ and $g_{i}$ for block $i$ take the expectation form w.r.t. the random variables $\\xi,\\,\\zeta$ and are jointly continuously differentiable, $\\mathbf{z}^{*}=\\left(z_{1}^{*}(x),...,z_{n}^{*}(x)\\right)\\in\\mathbb{R}^{d_{z}\\times n}$ contains all lower-level optimal solutions, and $n$ is the number of blocks. The above problem has various applications in machine learning, including deep AUC maximization [24, 23], meta-learning [16, 3], hyperparameter optimization [17], and robust learning [17]. This paper focuses on the setting with a nonconvex-strongly-concave minimax upper-level problem and a strongly-convex lower-level problem. ", "page_idx": 0}, {"type": "text", "text": "To date, barring a few works on optimizing special cases of this problem [17, 24], the solution algorithm to its general form has not been well studied. The primary obstacle lies in the significant computational cost per iteration, arising from the inherent structure of multi-block minimax bilevel optimization. To address this challenge, [17] considered a special case where $y$ is the simplex variable, and introduced a single-loop gradient descent-ascent algorithm, based on the two-timescale bilevel framework in [22]. [24] proposed a single-loop matrix-vector-based algorithm for a special case of our problem, where each upper-level function $f_{i}$ is evaluated only at the $i_{t h}$ coordinateof $y$ .However, these methods require computing the expensive second-order derivatives (i.e., the Hessian matrix or Hessian-vector product) per iteration, and the more efficient first-order approaches have not been explored yet. In this paper, we propose two efficient first-order minimax bilevel algorithms and further apply them to two novel ML applications. Our contributions are summarized as follows. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "\u00b7 By converting the original minimax bilevel problem into a simple minimax problem, we first propose a fully first-order single-loop algorithm named FOSL, which is easy to implement by updating $x,y$ and $\\mathbf{z}$ simultaneously, and is computationally efficient without the calculation of any second-order Hessian or Jacobian matrices. We provide a convergence analysis for FOSL under a practical block sampling without replacement setting and show that its sample complexity matches the best-known result of the same type of methods in standard bilevel optimization. Technically, we characterize the gap between the reformulated and original problems and need to deal with the interplay among four variables in the error analysis. \u00b7 In the settings where the number of blocks is substantial (e.g., in few-shot meta-learning), it becomes impractical to store all block-specific parameters to perform the single-loop optimization. To this end, we also propose a memory-efficient method named MemCS via a cold-start initialization, which randomly initializes a new weight for each sampled block, without saving it for the next iteration. We then analyze the convergence of MemCS under the partial-block and full-block participation, and show that it can achieve a better sample complexity than the same type of methods in standard bilevel optimization. \u00b7 We further apply our approaches to two ML applications: deep AUC maximization and robust meta-learning. The first application pertains to the established field of AUC Maximization, while the second explores a novel application known as Rank-based Robust MetaLearning. We show the effectiveness of our methods over a variety of datasets including CIFAR100, CelebA, CheXpert, OGBG-MolPCBA, Mini-ImageNet and Tiered-ImageNet. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "(Minimax) bilevel optimization. Bilevel optimization, introduced in [2], has been extensively studied, with constraint-based methods [13, 20, 50, 51] and gradient-based methods [1, 45, 14, 49, 57] emerging as two predominant types of approaches. The constraint-based methods (e.g., [34, 38, 30, 35, 54]) reformulated the lower-level problem as a value-function-based constraint, and solved it via different constrained optimization algorithms. More recently, [16, 23] studied the minimax bilevel optimization problem and proposed single-loop algorithms with applications to robust machine learning and deep AUC maximization. In this paper, we propose two efficient, fully first-order algorithms with solid performance guarantees. In recent years, there has been a growing interest in gradient-based methods due to their efficiency in solving machine-learning problems. Within this category, Iterative Differentiation (ITD) based methods [6, 7, 14, 39, 49, 27] and Approximate Implicit Differentiation (AID) based methods [1, 5, 33, 45, 41, 14, 27, 22] are two important classes distinguished by their approaches to approximating hypergradients. ", "page_idx": 1}, {"type": "text", "text": "Deep AUC maximization (DAM). DAM methods are aimed to mitigate the impact of imbalanced data in binary classification by directly maximizing the area under the ROC curve (AUC), a performance metric less affected by imbalanced data. As the AUC is difficult to optimize directly, research on DAM primarily focuses on devising effective optimization methods for its continuous surrogates [21, 4, 46, 8]. [37] proposed to reformulate the deep AUC maximization problem as a minimax optimization problem, providing the foundation for stochastic DAM algorithms developed in recent years [59, 60, 18, 24]. Among them, the most relevant work [24] formulated the DAM problem as a multi-block minimax optimization problem. In this work, we will use this form of DAM to demonstrate the effectiveness of our algorithm. A more comprehensive overview of DAM methods can be found in the survey [56]. ", "page_idx": 1}, {"type": "text", "text": "Robust meta-learning. Meta-learning provides effective solutions to multi-task learning in few-shot learning settings. In meta-learning, one trains a meta-model that can be quickly turned into a model that adapts to new tasks with only a few updates. Meta-learning algorithms in real-world applications must be robust to handle corrupted or low-quality data such as outliers. The majority of robust metalearning methods encompass filtering [55], re-weighting [48, 28, 31, 36], and re-labeling[43, 52, 61] on the sample level. Moreover, some other works focus on improving task-level robustness [28, 58]. In this work, we show that robust meta-learning can be formulated as a minimax bilevel optimization problem and solved with the proposed algorithm. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "3 Algorithms ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Reformulation as a Minimax Problem ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Motivated by [34, 38, 30] in single-machine bilevel optimization, we reformulate the lower-level problem as a value-function-based constraint and aim to solve the following equivalent problem: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x}\\operatorname*{max}_{y}\\frac{1}{n}\\sum_{i=1}^{n}f_{i}(x,y,z_{i})\\quad\\mathrm{s.t.}~g_{i}(x,z_{i})-g_{i}(x,z_{i}^{*})\\leq0,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $z_{i}^{*}:=\\arg\\operatorname*{min}_{z}g_{i}(x,z)$ Inspired by [30], we form a Lagrangian $\\mathcal{L}_{i}$ with Lagrangian multiplier $\\lambda\\geq0$ to approximate the original problem for each block $i$ in eq. (2), as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{i}(x,y,z_{i},v_{i})=f_{i}(x,y,z_{i})+\\lambda\\big(g_{i}(x,z_{i})-g_{i}(x,v_{i})\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $v_{i}$ is used to approximate the lower-level solution $z_{i}^{*}(x)$ of the $i_{t h}$ block. Then, we turn to solve the following surrogate minimax problem: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x,\\mathbf z}\\operatorname*{max}_{y,\\mathbf v}\\mathcal{L}\\big(x,y,\\mathbf z,\\mathbf v\\big):=\\frac{1}{n}\\sum_{i=1}^{n}\\mathcal{L}_{i}\\big(x,y,\\mathbf z e_{i},\\mathbf v e_{i}\\big),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathbf{z}\\;=\\;(z_{1},...,z_{n})\\;\\in\\;\\mathbb{R}^{d_{z}\\times n}$ \uff0c $\\mathbf{v}\\;=\\;(v_{1},...,v_{n})\\ \\in\\ \\mathbb{R}^{d_{z}\\times n}$ and the standard basis vector $e_{i}$ has only one non-zero element of 1 at the $i_{t h}$ coordinate. We show later in Section 4.2 that the gap between the gradients $\\nabla F(x,y^{*}(x),\\mathbf z^{*}(x))$ and $\\nabla\\mathcal{L}(x,y^{*}(x),\\mathbf{z}_{\\lambda}^{*}(x),\\mathbf{z}^{*}(x))$ of the original and surrogate problems can be effectively bounded by $\\mathcal{O}(1/\\lambda)$ , where $y^{*}(x)$ denotes the maximize of outer-objective $F(x,\\cdot,\\mathbf{z}^{*}(x))$ and each vector $z_{\\lambda,i}^{*}(x)$ in $\\mathbf{z}_{\\lambda}^{\\ast}(x):=(z_{\\lambda,1}^{\\ast}(x),...,z_{\\lambda,n}^{\\ast}(x))$ denotes the minimizer of the Lagrangian function $\\mathcal{L}_{i}(x,y^{*}(x),\\cdot,v)$ (where $z_{\\lambda,i}^{*}(x)$ has not reliance on $v$ ). This validates the effectiveness of the Lagrangian approximation for $\\lambda$ sufficiently large. Next, we propose two efficient algorithms, namely FOSL and MemCS, to solve the surrogate problem in eq. (3). ", "page_idx": 2}, {"type": "text", "text": "3.2FOSL: Fully First-Order Single-Loop Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "As shown in Algorithm 1, we first sample a subset $I_{t}\\subset\\mathcal{S}:=\\{1,...,n\\}$ of blocks without replacement. Noting that $z_{i}$ and $v_{i}$ are both block-specific variables, we then apply a stochastic ascent and descent step to update $v_{i}$ and $z_{i}$ for all block $i\\in I_{t}$ as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{v_{i,t+1}=v_{i,t}+\\eta_{v}\\big(-\\nabla_{z}g_{i}\\big(x_{t},v_{i,t};\\xi_{v,i}^{t}\\big)\\big)}\\\\ &{z_{i,t+1}=z_{i,t}-\\eta_{z}\\nabla_{z}\\mathcal{L}_{i}\\big(x_{t},y_{t},z_{i,t},v_{i,t};\\xi_{z,i}^{t}\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where the gradient of ${\\mathcal{L}}_{i}$ w.r.t. $z$ has no dependence on $v$ . Since the solutions w.r.t. variables $x$ and $y$ depend on all blocks, we use the average of stochastic gradient estimators from the selected blocks in $I_{t}$ to update $y$ and $x$ as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{y_{t+1}=y_{t}+\\eta_{y}\\frac{1}{\\left|I_{t}\\right|}\\displaystyle\\sum_{i\\in I_{t}}\\nabla_{y}f_{i}\\big(x_{t},y_{t},v_{i,t};\\xi_{y,i}^{t}\\big)}\\\\ &{x_{t+1}=x_{t}-\\eta_{x}\\frac{1}{\\left|I_{t}\\right|}\\displaystyle\\sum_{i\\in I_{t}}\\nabla_{x}\\mathcal{L}_{i}\\big(x_{t},y_{t},z_{i,t},v_{i,t};\\xi_{x,i}^{t}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Note that our algorithm takes a simpler fully single-loop structure via updating $\\{v_{i,t},z_{i,t}\\}_{i\\in I_{t}},x_{t}$ and $y_{t}$ simultaneously at each iteration. Hence, it can also benefit from the hardware parallelism. In addition, different from the methods in [17, 24] that need to compute the higher order Hessian- or Jacobian-vector products, our method only uses the first-order gradients. ", "page_idx": 2}, {"type": "text", "text": "3.3 MemCS: Memory-Efficient Cold-Start Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Note that in the single-loop optimization of Algorithm 1, all block-specific parameters $v_{i,t}$ and $z_{i,t}$ of blocks in $I_{t}$ need to be stored for the updates at iteration $t+1$ . However, in some ML applications, ", "page_idx": 2}, {"type": "text", "text": "Algorithm 1 Fully First-Order Single-Loop Method (FOSL) ", "page_idx": 3}, {"type": "text", "text": "1: Input: initialization $\\{x_{0},y_{0},z_{0},v_{0}\\}$ , number of iteration rounds $T$ , learning rates $\\{\\eta_{x},\\eta_{y},\\eta_{z},\\eta_{v}\\}$   \n2: for $t=0,1,2,...,T$ do   \n3: Sample blocks $I_{t}\\subset S$   \n4: for $i\\in I_{t}$ do   \n5: $\\begin{array}{r l}&{v_{i,t+1}=v_{i,t}-\\eta_{v}\\nabla_{z}g_{i}\\big(x_{t},v_{i,t};\\xi_{v,i}^{t}\\big)}\\\\ &{z_{i,t+1}=z_{i,t}-\\eta_{z}\\nabla_{z}\\mathcal{L}_{i}\\big(x_{t},y_{t},z_{i,t},v_{i,t};\\xi_{z,i}^{t}\\big)}\\end{array}$   \n6:   \n7: end for   \n8: $\\begin{array}{r l}&{y_{t+1}=y_{t}+\\eta_{y}\\frac{1}{|I_{t}|}\\sum_{i\\in I_{t}}\\nabla_{y}f_{i}\\big(x_{t},y_{t},v_{i,t};\\xi_{y,i}^{t}\\big)}\\\\ &{x_{t+1}=x_{t}-\\eta_{x}\\frac{1}{|I_{t}|}\\sum_{i\\in I_{t}}\\nabla_{x}\\mathcal{L}_{i}\\big(x_{t},y_{t},z_{i,t},v_{i,t};\\xi_{x,i}^{t}\\big)}\\end{array}$   \n9:   \n10: end for ", "page_idx": 3}, {"type": "text", "text": "Algorithm 2 Memory-Efficient Cold-Start (MemCS) ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "1: Input: initialization $\\{x_{0},y_{0}\\}$ , number of iteration rounds $T$ , learning rates $\\{\\eta_{x},\\eta_{y},\\eta_{z},\\eta_{v}\\}$   \n2: for $t=0,1,2,...,T-1$ do   \n3: Sample blocks $I_{t}\\subset S$   \n4: for $i\\in I_{t}$ do   \n5: Random initializations v,t, 2,t   \n6: for $k=0,1,2,...,K-1$ do   \n7: $\\boldsymbol{v}_{i,t}^{k+1}=\\boldsymbol{v}_{i,t}^{k}-\\eta_{v}\\nabla_{z}g_{i}\\big(\\boldsymbol{x}_{t},\\boldsymbol{v}_{i,t}^{k}\\big)$   \n8: $z_{i,t}^{k+1}=z_{i,t}^{k}-\\eta_{z}\\nabla_{z}\\mathcal{L}_{i}(x_{t},y_{t},z_{i,t}^{k},v_{i,t}^{k})$   \n9: end for   \n10: end for   \n11: $\\begin{array}{r l}&{y_{t+1}=y_{t}+\\eta_{y}\\frac{1}{|I_{t}|}\\sum_{i\\in I_{t}}\\nabla_{y}f_{i}\\big(x_{t},y_{t},v_{i,t}^{K}\\big)}\\\\ &{x_{t+1}=x_{t}-\\eta_{x}\\frac{1}{|I_{t}|}\\sum_{i\\in I_{t}}\\nabla_{x}\\mathcal{L}_{i}\\big(x_{t},y_{t},z_{i,t}^{K},v_{i,t}^{K}\\big)}\\end{array}$   \n12:   \n13: end for ", "page_idx": 3}, {"type": "text", "text": "such as few-shot meta-learning, the number of blocks/tasks is often large, and hence Algorithm 1 can suffer from significant memory consumption. To address this challenge, we propose a memoryefficient method named MemCS in Algorithm 2. Differently from the single-loop updates in FOSL, MemCS contains a sub-loop of $K$ steps of gradient descent in updating the block-specific variables $v_{i,t}^{k}$ and $z_{i,t}^{k}$ $k=0,...,K-1$ $v_{i,t}^{0}$ and $z_{i,t}^{0}$ Afer obtaningheouputs ,\\* f thisub-lop,the remaining step is toudate y and t via gradient asen adescent similarly as in FOSL. ", "page_idx": 3}, {"type": "text", "text": "4 Main Results ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "4.1  Assumptions ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Definition 4.1. A mapping $f$ is $L$ -Lipschitz continuous if $\\|f(x_{1})-f(x_{2})\\|\\leq L\\|x_{1}-x_{2}\\|$ , for any $x_{1},x_{2}$ . We say $f$ is $L$ -smooth if $\\nabla f$ is $L$ -Lipschitz continuous. ", "page_idx": 3}, {"type": "text", "text": "Since the overall objective is nonconvex w.r.t. $x$ , we aim to find an $\\epsilon$ -accurate stationary point. ", "page_idx": 3}, {"type": "text", "text": "Definition 4.2. We call $\\bar{x}$ as an $\\epsilon$ -accurate stationary point of the objective function $\\Phi(x)$ if $\\mathbb{E}\\|\\nabla\\Phi(\\bar{x})\\|^{2}\\leq\\epsilon$ ,where $\\epsilon\\in(0,1]$ and $\\bar{x}$ is the output of an algorithm. ", "page_idx": 3}, {"type": "text", "text": "We use the following assumptions in the subsequent description. Note that these assumptions are widely adopted in existing studies [24, 17]. ", "page_idx": 3}, {"type": "text", "text": "Assumption 4.3. For any $x\\in\\mathbb{R}^{d_{x}}$ \uff0c\uff0c $\\boldsymbol{y}\\in\\mathbb{R}^{d_{y}}$ \uff0c $z\\in\\mathbb{R}^{d_{z}}$ and $i\\in\\{1,2,...,n\\}$ $f_{i}(x,y)$ and $g_{i}(x,y)$ are twice continuously differentiable, $f_{i}(x,y,z)$ is $\\mu_{f}$ -strongly concave w.r.t. $y$ and $g_{i}(x,z)$ is $\\mu_{g}$ -strongly convex w.r.t. $z$ ", "page_idx": 3}, {"type": "text", "text": "The following assumption imposes the Lipschitz continuity on the upper- and lower-level functions and their derivatives. ", "page_idx": 3}, {"type": "text", "text": "Assumption 4.4. For any $x\\,\\in\\,\\mathbb{R}^{d_{x}}$ \uff0c $z\\,\\in\\,\\mathbb{R}^{d_{z}}$ and $i\\,\\in\\,\\{1,2,...,n\\}$ \uff0c\u3002 $f_{i}(x,y,z)$ is $L_{f,0}$ -Lipschitz continuous w.r.t. $x$ $g_{i}(x,z)$ is $L_{g,0}$ -Lipschitz continuous w.r.t. $x$ \uff0c $f_{i}(x,y,z)$ is $L_{f,1}$ -smooth and $g_{i}(x,y)$ is $L_{g,1}$ -smooth. In addition, the second-order derivatives $\\nabla^{2}f_{i}(x,y,z)$ and $\\nabla^{2}g_{i}(x,y)$ are $L_{f,2^{-}}$ and $L_{g,2}$ -Lipschitz continuous. ", "page_idx": 4}, {"type": "text", "text": "Next, we make a bounded variance assumption for the gradients in the stochastic setting ", "page_idx": 4}, {"type": "text", "text": "Assumption 4.5.There exist constants $\\sigma_{f}$ aind $\\sigma_{g}$ such that thevariances $\\mathbb{E}\\|\\nabla f_{i}(x,y,z)\\,-$ $\\nabla f_{i}(x,y,z;\\xi)\\|^{2}\\leq\\sigma_{f}^{2}$ and $\\begin{array}{r}{\\mathbb{E}\\|\\nabla g_{i}(x,z)-\\nabla g_{i}(x,z;\\zeta)\\|^{2}\\leq\\sigma_{g}^{2}}\\end{array}$ ", "page_idx": 4}, {"type": "text", "text": "The following assumption on block heterogeneity measures the similarities of the upper-level gradients $\\nabla_{y}f_{i}(x,z)$ for all $i$ . This has not been discussed in previous works, but it is necessary for our approach as we explore a more general outer-maximization solution $y^{*}(x)$ for $F$ , rather than for single $f_{i}$ ", "page_idx": 4}, {"type": "text", "text": "Assumption 4.6. For any $x\\in\\mathbb{R}^{d_{x}}$ \uff0c $z\\in\\mathbb{R}^{d_{z}}$ , there exist constants $\\beta_{t h}\\geq1$ and $\\sigma_{t h}\\geq0$ such that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}\\|\\nabla_{y}f_{i}(x,y,z)\\|^{2}\\leq\\beta_{t h}^{2}\\mathbb{E}\\|\\nabla_{y}F(x,y,z)\\|^{2}+\\sigma_{t h}^{2}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We have $\\beta_{t h}=1$ and $\\sigma_{t h}=0$ when all $g_{i}$ 's are identical. ", "page_idx": 4}, {"type": "text", "text": "4.2 Convergence analysis ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "For simplicity, we fix the number of involved blocks $|I_{t}|=P$ for all $t$ Let $y^{*}(x)$ be the maximizer of $F$ function w.r.t. $y$ . Then, the overall objective of the original problem in eq. (1) w.r.t. $x$ is given by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Phi(x):=F\\big(x,y^{*}(x),\\mathbf{z}^{*}(x)\\big),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbf{z}^{\\ast}(x)$ is the lower-level minimizer and $y^{*}(x)$ is the maximizer of $F(x,\\cdot,\\mathbf{z}^{*}(x))$ . In addition, recall that the objective function of the surrogate problem in eq. (3) w.r.t. $x$ is given by ${\\mathcal{L}}^{*}(x):=$ $\\mathcal{L}\\big(x,y^{*}(x),\\mathbf{z}_{\\lambda}^{*}(x),\\mathbf{z}^{*}(x)\\big)$ . We next characterize the difference between the gradients of the original and the surrogate problems. ", "page_idx": 4}, {"type": "text", "text": "Proposition 4.7. Under Assumptions 4.3, 4.4, the gap between $\\nabla\\Phi(x)$ and $\\nabla{\\mathcal{L}}^{*}(x)$ can bebounded as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\left\\|\\nabla\\Phi(x)-\\nabla{\\mathcal{L}}^{*}(x)\\right\\|={\\mathcal{O}}(1/\\lambda).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Due to the limit of space, the proof of Proposition 4.7 can be found in Lemma D.5 in the appendix. For a properlylarge $\\lambda$ , Proposition 4.7 guarantees that the stationary points of the original and surrogate problems are close to each other. However, too large $\\lambda$ can explode the gradient estimation variance, resulting in a much slower convergence rate. This trade-off makes the selection of $\\lambda$ important, as shown in our theorems later. ", "page_idx": 4}, {"type": "text", "text": "We next give an upper bound on the gradient norm $\\mathbb{E}\\|\\nabla{\\mathcal{L}}^{*}(x_{t})\\|^{2}$ of the surrogate problem. Denote $h_{x}^{t}:=\\nabla_{x}\\mathcal{L}_{i}(x_{t},y_{t},z^{t},v^{t};\\xi_{x,i}^{t})$ and its expectation as $\\widetilde{h}_{x}^{t}$ ", "page_idx": 4}, {"type": "text", "text": "Proposition 4.8. Under Assumptions 4.3, 4.4, 4.5, the consecutive iterates of Algorithm $^{\\,l}$ satisfy: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{\\bar{z}}\\|\\nabla\\mathcal{L}^{*}(x_{t})\\|^{2}\\leq\\displaystyle\\frac{2}{\\eta_{x}}\\mathbb{E}\\big[\\mathcal{L}^{*}(x_{t+1})-\\mathcal{L}^{*}(x_{t})\\big]-\\mathbb{E}\\|\\widetilde{h}_{x}^{t}\\|^{2}+\\eta_{x}L_{*,1}\\mathbb{E}\\|h_{x}^{t}\\|^{2}+3L_{f,1}^{2}\\mathbb{E}\\big\\|y_{t}-y^{*}(x_{t})\\big\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad+\\displaystyle\\frac{3L_{\\lambda,1}^{2}}{n}\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}\\Big[\\big\\|z_{i,t}-z_{\\lambda,i}^{*}(x_{t})\\big\\|^{2}+\\big\\|v_{i,t}-z_{i}^{*}(x_{t})\\big\\|^{2}\\Big]}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "forall $t\\in\\{0,1,...,T-1\\}$ where $L_{\\lambda,1}$ and $L_{*,1}$ are given in Lemma D.1 and Lemma D.6 in the appendix respectively. ", "page_idx": 4}, {"type": "text", "text": "The proof of Proposition 4.8 can be found in Lemma E.1 in the appendix. The same result can be obtained for Algorithm 2 by replacing $v_{i,t}$ and $z_{i,t}$ with $\\ v_{i,t}^{K}$ and $z_{i,t}^{K}$ This proposition shows that the convergence rate of our algorithm relies on how fast the iterates $y_{t},z_{i,t}$ and $v_{i,t}$ converge to their optimal solutions at each iteration $t$ . We next characterize the distance of $y_{t}$ to its optimal solution. ", "page_idx": 4}, {"type": "text", "text": "Proposition 4.9. Under Assumptions 4.3, 4.4, 4.5, the iterates of $y_{t}$ generated according to Algorithm $^{\\,I}$ satisfy ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\|y_{t+1}-y^{*}(x_{t+1})\\|^{2}-\\mathbb{E}\\|y_{t}-y^{*}(x_{t})\\|^{2}\\leq-\\mathcal{O}(\\eta_{y})\\cdot\\mathbb{E}\\|y_{t}-y^{*}(x_{t})\\|^{2}+\\mathcal{O}\\bigg(\\displaystyle\\frac{\\eta_{y}^{2}}{|I_{t}|}\\bigg)\\cdot\\big(\\sigma_{f}^{2}+\\sigma_{t h}^{2}\\big)}\\\\ &{\\qquad\\qquad+\\mathcal{O}(\\eta_{y})\\cdot\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}\\|v_{i,t}-z_{i}^{*}(x_{t})\\|^{2}+\\mathcal{O}\\bigg(\\displaystyle\\frac{\\eta_{x}^{2}}{\\eta_{y}}\\bigg)\\mathbb{E}\\|\\tilde{h}_{x}^{t}\\|^{2}+\\mathcal{O}(\\eta_{x}^{2})\\mathbb{E}\\big\\|h_{x}^{t}\\big\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "for all $t\\in\\{0,...,T-1\\}$ ", "page_idx": 5}, {"type": "text", "text": "The proof of Proposition 4.9 refers to Lemma E.3 in the appendix. It can be seen that with properly small stepsizes $\\eta_{x}$ and $\\eta_{y}$ , there exists a descent of the optimal distance $\\mathbb{E}\\|y_{t}-y^{*}(x_{t})\\|^{2}$ , which is critical for the final convergence analysis. Similar results are obtained for $v_{i,t}$ and $z_{i,t}$ . Combining the above propositions and the auxiliary lemmas in the appendix, we get the following result for Algorithm 1. ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.10 (Convergence of FOSL). Suppose Assumptions 4.3, 4.4, 4.5, 4.6 are satisfied. Set parameters $\\eta_{x}=\\mathcal{O}(T^{-\\frac{5}{7}})$ \uff0c $\\eta_{y}=\\mathcal{O}(T^{-\\frac{4}{7}})$ \uff0c $\\eta_{z}=\\mathcal{O}(T^{-\\frac{5}{7}})$ \uff0c $\\eta_{v}=\\mathcal{O}(T^{-\\frac{4}{7}})$ and $\\lambda={\\mathcal{O}}(T^{{\\frac{1}{7}}})$ . Then, wehave ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{T}\\displaystyle\\sum_{t=0}^{T-1}\\mathbb{E}\\|\\nabla\\Phi(x_{t})\\|^{2}\\leq\\frac{2C_{g a p}}{\\lambda^{2}}+\\frac{4\\big(\\Psi_{0}-\\Psi_{T}\\big)}{T\\eta_{x}}+\\frac{4\\eta_{x}\\lambda^{2}}{P}\\bigg(1+\\frac{\\eta_{x}}{\\eta_{y}}+\\frac{\\eta_{x}\\lambda^{2}}{(\\eta_{z}\\lambda)}+\\frac{\\eta_{x}\\lambda^{2}}{\\eta_{v}}\\bigg)C_{2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad+4\\big(\\eta_{y}+(\\eta_{z}\\lambda)\\lambda^{2}+\\eta_{v}\\lambda^{2}\\big)C_{3}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\mathcal{O}(T^{-\\frac{2}{7}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $C_{g a p}$ is defined in Lemma $D.5$ $C_{2},C_{3}$ are defined in eq. (39) in the appendix, and $\\Psi_{t}:=$ $\\begin{array}{r}{\\begin{array}{r}{\\Sigma^{*}(x_{t})\\ \\dot{+}\\ \\dot{K}_{y}\\mathbb{E}\\|y_{t}-y^{*}(x_{t})\\|^{2}+K_{z}\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}\\|z_{i,t}-z_{\\lambda,i}^{*}(x_{t})\\|^{2}+K_{v}\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}\\|v_{i,t}-z_{i}^{*}(x_{t})\\|^{2}.}\\end{array}}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "Next, we characterize the sample complexity for FOSL ", "page_idx": 5}, {"type": "text", "text": "Corollary 4.11. Under the same setting of Theorem $4.l0,$ our algorithm finds an $\\epsilon$ -accuratestationary solution after $T\\,=\\,\\mathcal{O}(\\epsilon^{-\\frac{7}{2}})$ interactions. The total sample complexity for all involved blocks is $P T={\\mathcal O}(P\\epsilon^{-\\frac{7}{2}})$ ", "page_idx": 5}, {"type": "text", "text": "Compared with existing works [17, 24], our algorithm is free from second-order derivative computations. In addition, the sample complexity of our algorithm matches the best result [30] of the same type of methods in standard single-block bilevel optimization. ", "page_idx": 5}, {"type": "text", "text": "Next, we analyze the convergence for Algorithm 2 under the partial- and full-block participation. ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.12 (Convergence of MemCS). Suppose Assumptions 4.3, 4.4, 4.5, 4.6 are satisfied. Assumethereexistssome $B\\;>\\;0$ such that $\\|\\bar{z}_{i}^{*}(x_{t})\\|\\,\\leq\\,\\bar{B}$ for any $x_{t}$ \uff0c $i\\,=\\,1,...,N$ For the partial-block participation, by setting parameters $\\eta_{x}\\,=\\,\\mathcal{O}(P^{\\frac{1}{5}}T^{-\\frac{2}{3}}),\\,\\eta_{y}\\,=\\,\\mathcal{O}(P^{-\\frac{1}{5}}T^{-\\frac{1}{2}}),\\,\\eta_{z}\\,=$ $\\mathcal{O}(P^{-\\frac{1}{10}}T^{-\\frac{1}{6}})$ $\\eta_{v}=\\mathcal{O}(1)$ $\\lambda=\\mathcal{O}(P^{\\frac{1}{10}}T^{\\frac{1}{6}})$ and taking $\\epsilon_{s u b}=\\mathcal{O}(P^{-\\frac{2}{5}}T^{-\\frac{2}{3}})$ , we have ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{T}\\displaystyle\\sum_{t=0}^{T-1}\\mathbb{E}\\|\\nabla\\Phi(x_{t})\\|^{2}\\leq\\frac{2C_{g a p}}{\\lambda^{2}}+\\frac{2(\\Psi_{0}-\\Psi_{T})}{T\\eta_{x}}+\\frac{4\\eta_{x}\\lambda^{2}}{P}\\Big(1+\\frac{\\eta_{x}}{\\eta_{y}}\\Big)C_{2}+\\frac{\\eta_{y}}{P}\\frac{24L_{f,1}^{2}\\sigma_{t h}^{2}}{\\mu_{f}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\,4\\bigg(3L_{\\lambda,1}^{2}+\\frac{12L_{f,1}^{4}}{\\mu_{f}^{2}}\\bigg)\\epsilon_{s u b}\\leq\\mathcal{O}(P^{-\\frac{1}{5}}T^{-\\frac{1}{3}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $L_{\\lambda,1}:=3\\lambda L_{g,1}.$ $C_{g a p}$ is defined by Lemma $D.5$ in the appendix and $\\Psi_{t}:=\\mathcal{L}^{*}(x_{t})+K_{y}\\mathbb{E}\\|y_{t}-$ $y^{\\ast}(x_{t})\\Vert^{2}$ .For the full-block participation, by setting $\\eta_{x}\\,=\\,\\mathcal{O}(1)$ \uff0c $\\eta_{y}\\,=\\,\\mathcal{O}(1)$ \uff0c $\\eta_{z}\\,=\\,\\mathcal{O}(T^{-\\frac{1}{2}})$ \uff0c $\\eta_{v}=\\mathcal{O}(1)$ \uff0c $\\lambda=\\mathcal{O}(T^{\\frac{1}{2}})$ and taking $\\epsilon_{s u b}=\\mathcal{O}(T^{-2})$ we have ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=0}^{T-1}\\mathbb{E}\\|\\nabla\\Phi(x_{t})\\|^{2}\\leq\\frac{2C_{g a p}}{\\lambda^{2}}+\\frac{4\\big(\\Psi_{0}-\\Psi_{T}\\big)}{T\\eta_{x}}+12\\bigg(L_{\\lambda,1}^{2}+\\frac{4L_{f,1}^{4}}{\\mu_{f}^{2}}\\bigg)\\epsilon_{s u b}\\leq\\mathcal{O}(T^{-1}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Note that the assumption $\\|z_{i}^{*}(x_{t})\\|\\leq B$ can be removed when the domain of $x$ is a closed convex set and projected gradient descent is used to update $x$ [12, 15]. We next characterize the sample complexity for MemCS. ", "page_idx": 5}, {"type": "text", "text": "\u00b7 For partial-block participation, our algorithm finds an e-accurate stationary solution of $\\Phi(x)$ after $T=\\mathcal{O}(P^{-\\frac{3}{5}}\\epsilon^{-3})$ outer iterations and $K=\\mathcal{O}(\\log\\frac{1}{\\epsilon})$ inner iterations.The total sample complexity for all involved blocks is $P K T=\\tilde{\\mathcal{O}}(P^{\\frac{2}{5}}\\epsilon^{-3})$   \n\u00b7For full-block participation, our algorithm finds an e-accurate stationary solution of $\\Phi(x)$ after $T=\\mathcal{O}(\\bar{\\epsilon}^{-1})$ outeriterationsand $\\dot{K}=\\mathcal{O}(\\log\\frac{1}{\\epsilon})$ inner iterations.The total sample complexity for all involved blocks is $n K T=\\widetilde{\\mathcal{O}}(n\\epsilon^{-1})$ ", "page_idx": 6}, {"type": "text", "text": "Note that the per-block sample complexity of our MemCS algorithm takes an order of $\\epsilon^{-1}$ , which improves that of the same-type $\\mathrm{F^{2}S\\bar{A}}$ [30j by an order of $\\epsilon^{-0.5}$ , based on a refined analysis on the smoothness of the overall objective function. Corollary 4.13 also shows that MemCS achieves a linear convergence speedup w.r.t. the number $P$ of blocks. As far as we know, this is the first linear speedup result in multi-block minimax bilevel optimization. ", "page_idx": 6}, {"type": "text", "text": "5 Applications and Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we conduct extensive experiments in two applications: deep AUC maximization and rank-based robust meta-learning. More experimental results such as time and space comparison are provided in Appendix B. ", "page_idx": 6}, {"type": "text", "text": "5.1 Deep AUC Maximization ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "5.1.1 Formulation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Deep AUC Maximization (DAM) addresses machine learning challenges presented by imbalanced datasets. In particular, the AUC (Area Under the ROC Curve) measures the likelihood that a positive sample's prediction score will be higher than that of a negative sample. As outlined by [24], the DAM issue is structured as a multi-block minimax bilevel optimization problem: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{w},a,b}\\operatorname*{max}_{\\boldsymbol{\\alpha}}\\sum_{j=1}^{m}\\Phi_{j}\\big(\\mathbf{u}_{j}^{*}(\\mathbf{w}_{j}),a_{j},b_{j},\\alpha_{j}\\big)\\quad s.t.\\ \\mathbf{u}_{j}^{*}(\\mathbf{w}_{j})=\\arg\\operatorname*{min}_{\\mathbf{u}_{j}}g_{j}\\big(\\mathbf{u}_{j},\\mathbf{w}_{j}\\big),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Where $\\begin{array}{r}{:g_{j}(\\mathbf{u}_{j},\\mathbf{w}_{j}):=\\frac{1}{2}\\big\\|\\mathbf{u}_{j}-\\big(\\mathbf{w}_{j}-\\tilde{\\eta}\\nabla L_{A V G}(\\mathbf{w}_{j})\\big)\\big\\|^{2},\\,L_{A V G}(\\mathbf{w}_{j}):=\\frac{1}{n}\\sum_{i=1}^{n}\\ell\\big(\\mathbf{w}_{j};x_{i},y_{i}\\big),}\\end{array}$ $\\ell$ denotes the task loss (e.g., the cross-entropy loss in binary classification tasks), and $\\Phi_{j}$ denotesthe sample-level AUC loss function. The detailed formulation can be found in Appendix A.1. With the method in Section 3, we reformulate this problem as a single-level minimax optimization problem: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{w},\\mathbf{u},a,b}\\operatorname*{max}_{\\alpha,\\mathbf{v}}\\mathcal{L}(\\mathbf{w},\\mathbf{u},a,b,\\alpha,\\mathbf{v}),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Where $\\begin{array}{r}{\\mathcal{L}(\\mathbf{w},\\mathbf{u},a,b,\\boldsymbol{\\alpha},\\mathbf{v})\\;:=\\;\\sum_{j=1}^{m}\\Phi_{j}(\\mathbf{u}_{j},a_{j},b_{j},\\boldsymbol{\\alpha}_{j})+\\lambda\\big(g_{j}(\\mathbf{u}_{j},\\mathbf{w}_{j})-g_{j}(\\mathbf{v}_{j},\\mathbf{w}_{j})\\big)}\\end{array}$ is the Lagrangian function of AUC loss function, and $\\mathbf{v}_{j}$ is the approximate optimal solution of $g_{j}$ ", "page_idx": 6}, {"type": "text", "text": "5.1.2 Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Settings. Following the work [24], we assess our methodology using four datasets, namely CIFAR100 [29], CelebA [40], CheXpert [26] and OGBG-MolPCBA [25], whose details are provided in Appendix B.1. We evaluate the effectiveness of our algorithm by comparing it with direct optimization on multi-block minimax AUC loss (mAUC) and compositional training on mAUC loss (mAUC-CT). The test AUC scores of mAUC and mAUC-CT for different datasets in Table 1 are derived from the original paper. Detailed configuration of experiments can be found in Appendix B.2. ", "page_idx": 6}, {"type": "text", "text": "Results. The results of deep AUC maximization on different datasets are shown in Table 1. The results indicate that our FOSL outperforms the mAUC method in terms of test AUC scores on all datasets and achieves comparative or better performance than mAUC-CT on various datasets. We proceed to visualize the AUC loss during the initial stages of training for all methods on CelebA, as depicted in Figure 1a and 1b. The figures illustrate that, in the initial stage, our method and mAUC-CT [24] exhibit a faster loss drop than mAUC, and our method achieves the fastest overall convergence rate. Furthermore, our approach exhibits a smaller fluctuation compared to other baseline methods. ", "page_idx": 6}, {"type": "text", "text": "Table 1: Test AUC score with $95\\%$ confidence interval on different datasets for AUC maximization. ", "page_idx": 7}, {"type": "table", "img_path": "GZoAUVSkaw/tmp/0206306bfad5feab84b9a7f7fc3ee3922c0918064cdd69ee57f310c2c48c0674.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "GZoAUVSkaw/tmp/fc3cc88edd22e23d8e528faa7528b8231322acf77c3197aa87bcad3e739322b7.jpg", "img_caption": ["Figure 1: Visualization results of FOSL experiments. (a) Training AUC loss over iteration rounds during the initial stages of training. (b) Training AUC loss over time during the initial training phase. (c) Impact of $\\lambda$ on test AUC score throughout training on the CIFAR100 dataset. (d) Impact of $\\lambda$ On test AUC score throughout training on the CelebA dataset. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Impact of $\\lambda$ .To evaluate the impact of the hyper-parameter $\\lambda$ on training with FOSL algorithm, we conduct an ablation study on the CIFAR100 and the CelebA datasets. The test AUC scores along with training are depicted in Figure 1c and 1d. As shown in Figure 1c, our method sustains robust performance across a wide range of choices for $\\lambda$ . For example, training within a $\\lambda$ range of [2, 8] shows that the speed of convergence and the final test performance are not sensitive to the change of $\\lambda$ . A similar observation also holds for the CelebA dataset as shown in Figure 1d. ", "page_idx": 7}, {"type": "text", "text": "5.2  Robust Meta-learning with Rank-based Loss ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "5.2.1 Formulation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Our objective is to devise a robust meta-learning approach wherein, during each iteration, tasks with large loss values are filtered out, and the meta-model is updated with the remaining tasks. This approach effectively reduces the impact of tasks with noisy samples (noisy tasks), because deep learning models tend to acquire clean and simple patterns in their initial training stages [19], such that noisy samples/tasks often have large loss values. Further justification can be found in Figure 2. ", "page_idx": 7}, {"type": "text", "text": "We first define $g_{[i]}$ as the $i_{t h}$ largest element of the set ${\\mathcal{G}}=\\{g_{1},g_{2},...,g_{n}\\}$ , such that $g_{[n]}\\leq g_{[n-1]}\\leq$ $\\cdots\\leq g_{[1]}$ . Denote the task-specific loss as $g_{i}(\\phi,w_{i})$ , where $\\phi$ is the parameter of the meta-model and $w_{i}$ is the task-specific parameter. The proposed formulation is then given by: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\phi}F(\\phi,\\mathbf{w}^{*}):=\\frac{1}{k}\\sum_{i=n-k+1}^{n}g_{[i]}\\big(\\phi,w_{[i]}^{*}(\\phi)\\big)\\quad s.t.\\ \\boldsymbol{w}_{i}^{*}(\\phi)=\\arg\\operatorname*{min}_{w_{i}}g_{i}(\\phi,w_{i}),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Wwhere $g_{[i]}\\left(\\phi,w_{[i]}^{*}(\\phi)\\right)$ is the $i_{t h}$ largest task-specific loss given $\\mathbf{w}^{*}:=\\left[w_{i}^{*}(\\phi),...,w_{n}^{*}(\\phi)\\right]^{T}$ , and $w_{[i]}^{*}(\\phi)$ is the corresponding optimal task-specific parameter. ", "page_idx": 7}, {"type": "text", "text": "With the Lemma 1 in [42], by introducing an auxiliary variable $\\gamma$ , we can reformulate the problem as: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\phi}\\operatorname*{max}_{\\gamma}F(\\phi,\\mathbf{w}^{*},\\gamma)=\\frac{1}{k}\\sum_{i=1}^{n}f_{i}\\big(\\phi,w_{i}^{*}(\\phi),\\gamma\\big)=\\frac{1}{k}\\sum_{i=1}^{n}\\Big\\{g_{i}^{*}(\\phi)-[g_{i}^{*}(\\phi)-\\gamma]_{+}-\\frac{n-k}{n}\\gamma\\Big\\}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Details about the derivation of the above formulation can be found in Appendix A.2. This formulation poses a non-convex optimization challenge for the primal variable $\\phi$ , making it challenging to address using conventional optimization methods. Nevertheless, our proposed algorithm enables efficient resolution of this problem by reformulating the problem into a singlelevel minimax optimization problem as: $\\begin{array}{r}{\\operatorname*{min}_{\\phi,\\mathbf{w}}\\operatorname*{max}_{\\gamma,\\mathbf{v}}\\mathcal{L}\\big(\\phi,\\mathbf{w},\\gamma,\\mathbf{v}\\big)}\\end{array}$ \uff0cwhere $\\mathcal{L}(\\boldsymbol{\\phi},\\mathbf{w},\\gamma,\\mathbf{v})\\;:=$ $\\begin{array}{r}{\\frac{1}{n}\\sum_{i=1}^{n}f_{i}(\\phi,w_{i},\\gamma)\\!+\\!\\lambda\\big(g_{i}(\\phi,w_{i})\\!-\\!g_{i}(\\phi,v_{i})\\big)}\\end{array}$ is the Lagrangian function of the rank based loss function, $\\mathbf{v}$ is an approximate optimal task-specific parameter of the lower-level problem. ", "page_idx": 7}, {"type": "table", "img_path": "", "table_caption": ["Table 2:Test accuracy $(\\%)$ on  the  MiniImageNet and the Tiered-ImageNet datasets for meta-learning. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 3: Test accuracy $(\\%)$ on Mini-ImageNet and Tiered-ImageNet with different noisy ratio for Flip setting. ", "page_idx": 8}, {"type": "image", "img_path": "GZoAUVSkaw/tmp/60dfab17dff2cecaa0aef66d632eeb5c20aff04cfaa9d9d542e7f88f8f32c6d9.jpg", "img_caption": ["Figure 2: The portion of tasks being noisy during training for MAML and MemCS on Mini-ImageNet. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.2.2 Results ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Settings. We perform meta-learning experiments on few-shot learning tasks, focusing on the capability of rapid adaptation to new tasks with limited samples. Adhering to standard few-shot learning configurations, we carry out 5-ways 5-shot learning experiments on Mini-ImageNet [53] (referred to as Mini in Table 2) and Tiered-ImageNet [47] (referred to as Tiered in Table 2), where each task involves a 5-class classification task, with five samples per class used as training data. Since our robust meta-learning formulation is built on that of MAML [6], we compare our method with MAML on both clean dataset and noisy dataset to evaluate the effectiveness and robustness of our algorithm. In the noisy setting, we adopt a standard noisy training scheme in meta-learning, where the labels in a noisy task are randomly flipped. Specifically, we employ two label flipping strategies: Flip, where in each iteration, a certain portion of tasks $60\\%$ in Table 2) are designated as noisy, and each sample within the task is assigned to one of all labels with equal probability; and Rand, where a random noisy ratio is assigned to each task in every iteration, determining the proportion of samples to be flipped by randomly assigning another label to them. Detailed configuration of experiments can be found in Appendix B.2. ", "page_idx": 8}, {"type": "text", "text": "Results. Table 2 displays the test accuracies. These results show that in the presence of noisy tasks, both MAML and our MemCS method undergo a decline in performance across both datasets, yet our approach manages to sustain a reasonable performance. To further evaluate the resilience of our MemCS method against MAML, we executed additional experiments with escalating noise levels in the Flip scenario, with these findings detailed in Table 3. The data clearly show a performance decrease for both methodologies as the noise ratio intensifies. Nonetheless, our approach exhibits a notably more gradual decline in performance as the noise ratio escalates, especially at higher noise levels, signifying superior robustness compared to MAML. ", "page_idx": 8}, {"type": "text", "text": "Discussion. To show the effectiveness of our approach in facilitating robust learning, we have visualized the average losses for both clean and noisy tasks separately within the MAML training framework, as demonstrated in Figure 2. The graphical representation uncovers a consistent pattern where the losses associated with noisy tasks consistently exceed those related to clean tasks throughout the training period. This pattern underscores our approach's capacity to lessen the detrimental effects of noisy tasks. Further, we examine the noisy tasks in the update mechanism at five distinct intervals during the training phase, illustrated in Figure 2. The findings show that our methodology successfully deters the influence of noisy tasks on the meta-model's updates across both Rand and Flip scenarios, maintaining this protective measure throughout the training duration. ", "page_idx": 8}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this paper, we propose two fully first-order algorithms designed to address the challenges posed by multi-block minimax bilevel optimization problems: a fully single-loop algorithm, FOSL, and a memory-efficient double-loop algorithm with cold-start initialization, MemCs. We show that our methods can achieve comparative and even better per-block sample complexities than other methods with the same type in standard bilevel optimization. The experimental results indicate that our methods consistently demonstrate superior performance and robustness in applications on deep AUC maximization and robust meta-learning. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Yifan Yang and Kaiyi Ji were partially supported by NSF grants CCF-2311274 and ECCS-2326592.   \nZhaofeng Si and Siwei Lyu were partially supported by an NSF research grant IS-2008532. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1]  Michael Arbel and Julien Mairal.  Amortized implicit differentiation for stochastic bilevel optimization. arXiv preprint arXiv:2111.14580, 2021.   \n[2]  Jerome Bracken and James T McGill. Mathematical programs with optimization problems in the constraints. Operations Research, 21(1):37-44, 1973.   \n[3] Liam Collins, Aryan Mokhtari, and Sanjay Shakkottai. Task-robust model-agnostic metalearning. Advances in Neural Information Processing Systems, 33:18860-18871, 2020.   \n[4]  Corinna Cortes and Mehryar Mohri. Auc optimization vs. error rate minimization. Advances in Neural Information Processing Systems, 16, 2003.   \n[5]  Justin Domke. Generic methods for optimization-based modeling. In Artificial Intelligence and Statistics, pages 318-326. PMLR, 2012.   \n[6]  Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International Conference on Machine Learning, pages 1126-1135. PMLR, 2017.   \n[7] Luca Franceschi, Michele Donini, Paolo Frasconi, and Massimiliano Pontil. Forward and reverse gradient-based hyperparameter optimization. In International Conference on Machine Learning, pages 1165-1173. PMLR, 2017.   \n[8]  Wei Gao, Rong Jin, Shenghuo Zhu, and Zhi-Hua Zhou. One-pass auc optimization. In International Conference on Machine Learning, pages 906-914. PMLR, 2013.   \n[9]  Wei Gao and Zhi-Hua Zhou. On the consistency of auc pairwise optimization. arXiv preprint arXiv:1208.0645, 2012.   \n[10] Camille Garcin, Maximilien Servajean, Alexis Joly, and Joseph Salmon. Stochastic smoothing of the top-k calibrated hinge loss for deep imbalanced classification. In International Conference on Machine Learning, pages 7208-7222. PMLR, 2022.   \n[11]  Guillaume Garrigos and Robert M Gower. Handbook of convergence theorems for (stochastic) gradient methods. arXiv preprint arXiv:2301.11235, 2023.   \n[12]  Saeed Ghadimi and Mengdi Wang. Approximation methods for bilevel programming. arXiv preprint arXiv:1802.02246, 2018.   \n[13] Stephen Gould, Basura Fernando, Anoop Cherian, Peter Anderson, Rodrigo Santa Cruz, and Edison Guo. On differentiating parameterized argmin and argmax problems with application to bi-level optimization. arXiv preprint arXiv:1607.05447, 2016.   \n[14]  Riccardo Grazzi, Luca Franceschi, Massimiliano Pontil, and Saverio Salzo. On the iteration complexity of hypergradient computation. In International Conference on Machine Learning, pages 3748-3758. PMLR, 2020.   \n[15] Riccardo Grazzi, Massimiliano Pontil, and Saverio Salzo. Bilevel optimization with a lowerlevel contraction: Optimal sample complexity without warm-start. Journal of Machine Learning Research, 24(167):1-37, 2023.   \n[16]  Alex Gu, Songtao Lu, Parikshit Ram, and Lily Weng. Nonconvex min-max bilevel optimization for task robust meta learning. In International Conference on Machine Learning, 2021.   \n[17]  Alex Gu, Songtao Lu, Parikshit Ram, and Lily Weng. Min-max multi-objective bilevel optimization with applications in robust machine learning. In International Conference on Learning Representations, 2023.   \n[18] Zhishuai Guo, Mingrui Liu, Zhuoning Yuan, Li Shen, Wei Liu, and Tianbao Yang. Communication-efficient distributed stochastic auc maximization with deep neural networks. In International Conference on Machine Learning, pages 3864-3874. PMLR, 2020.   \n[19] Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels. Advances in Neural Information Processing Systems, 31, 2018.   \n[20] Pierre Hansen, Brigitte Jaumard, and Gilles Savard. New branch-and-bound rules for linear bilevel programming. SIAM Journal on scientific and Statistical Computing, 13(5):1194-1217, 1992.   \n[21]  Ralf Herbrich. Large margin rank boundaries for ordinal regression. Advances in Large Margin Classifers, pages 115-132, 2000.   \n[22] Mingyi Hong, Hoi-To Wai, Zhaoran Wang, and Zhuoran Yang. A two-timescale stochastic algorithm framework for bilevel optimization: Complexity analysis and application to actorcritic. SIAM Journal on Optimization, 33(1):147-180, 2023.   \n[23] Quanqi Hu, Bokun Wang, and Tianbao Yang. A stochastic momentum method for min-max bilevel optimization. 2021.   \n[24]  Quanqi Hu, Yongjian Zhong, and Tianbao Yang. Multi-block min-max bilevel optimization with applications in multi-task deep auc maximization. Advances in Neural InformationProcessing Systems, 35:29552-29565, 2022.   \n[25]  Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. Advances in neural information processing systems, 33:22118-22133, 2020.   \n[26] Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute, Henrik Marklund, Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, et al. Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pages 590-597, 2019.   \n[27] Kaiyi Ji, Junjie Yang, and Yingbin Liang. Bilevel optimization: Convergence analysis and enhanced design. In International Conference on Machine Learning, pages 4882-4892. PMLR, 2021.   \n[28] Krishnateja Killamsetty, Changbin Li, Chen Zhao, Feng Chen, and Rishabh Iyer. A nested bi-level optimization framework for robust few shot learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 7176-7184, 2022.   \n[29]  Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \n[30] Jeongyeol Kwon, Dohyun Kwon, Stephen Wright, and Robert D Nowak. A fully frst-order method for stochasticbilevel optimization. In International Conference on Machine Learning, pages 18083-18113. PMLR, 2023.   \n[31] Kuang-Huei L, Xiaodong He, Lei Zhang, and Linjun Yang. Cleannet: Transfer learning for scalable image classifier training with label noise. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5447-5456, 2018.   \n[32] Lihua Lei, Cheng Ju, Jianbo Chen, and Michael I Jordan. Non-convex finite-sum optimization via scsg methods. Advances in Neural Information Processing Systems, 30, 2017.   \n[33] Renjie Liao, Yuwen Xiong, Ethan Fetaya, Lisa Zhang, KiJung Yoon, Xaq Pitkow, Raquel Urtasun, and Richard Zemel. Reviving and improving recurrent back-propagation In International Conference on Machine Learning, pages 3082-3091. PMLR, 20i8.   \n[34]  Gui-Hua Lin, Mengwei Xu, and Jane J Ye. On solving simple bilevel programs with a nonconvex lower level program. Mathematical Programming, 144(1-2):277-305, 2014.   \n[35] Bo Liu, Mao Ye, Stephen Wright, Peter Stone, and Qiang Liu. Bome! bilevel optimization made easy: A simple first-order approach. Advances in Neural Information Processing Systems, 35:17248-17262, 2022.   \n[36] Chenghao Liu, Zhihao Wang, Doyen Sahoo, Yuan Fang, Kun Zhang, and Steven CH Hoi. Adaptive task sampling for meta-learning. In Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XVIll 16, pages 752-769. Springer, 2020.   \n[37]  Mingrui Liu, Zhuoning Yuan, Yiming Ying, and Tianbao Yang. Stochastic auc maximization with deep neural networks. arXiv preprint arXiv:1908.10831, 2019.   \n[38] Risheng Liu, Xuan Liu, Xiaoming Yuan, Shangzhi Zeng, and Jin Zhang. A value-function-based interior-point method for non-convex bi-level optimization. In International Conference on Machine Learning, pages 6882-6892. PMLR, 2021.   \n[39] Risheng Liu, Yaohua Liu, Shangzhi Zeng, and Jin Zhang. Towards gradient-based bilevel optimization with non-convex followers and beyond. Advances in Neural Information Processing Systems, 34:8662-8675, 2021.   \n[40] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Proceedings of International Conference on Computer Vision (ICCV), December 2015.   \n[41] Jonathan Lorraine, Paul Vicol, and David Duvenaud. Optimizing millions of hyperparameters by implicit differentiation. In International Conference on Artificial Intelligence and Statistics, pages 1540-1552. PMLR, 2020.   \n[42] Siwei Lyu, Yanbo Fan, Yiming Ying, and Bao-Gang Hu. Average top-k aggregate loss for supervised learning. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(1):76- 86, 2020.   \n[43] Soufiane Mallem, Abul Hasnat, and Amir Nakib. Effcient meta label correction based on meta learning and bi-level optimization. Engineering Applications of Artificial Intelligence, 117:105517, 2023.   \n[44]  Yuri Nesterov and Boris T Polyak. Cubic regularization of newton method and its global performance. Mathematical Programming, 108(1):177-205, 2006.   \n[45]  Fabian Pedregosa. Hyperparameter optimization with approximate gradient. In International Conference on Machine Learning, pages 737-746. PMLR, 2016.   \n[46]  Alain Rakotomamonjy. Optimizing area under roc curve with svms. In ROCAl, pages 71-80, 2004.   \n[47] Mengye Ren, Eleni Triantafillou, Sachin Ravi, Jake Snell, Kevin Swersky, Joshua B Tenenbaum, Hugo Larochelle, and Richard S Zemel. Meta-learning for semi-supervised few-shot classification. arXiv preprint arXiv: 1803.00676, 2018.   \n[48] Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun, Leaning to reweight exampls for robust deep learning. In International Conference on Machine Learning, pages 4334-4343. PMLR, 2018.   \n[49]  Amirreza Shaban, Ching-An Cheng, Nathan Hatch, and Byron Boots. Truncated backpropagation for bilevel optimization. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 1723-1732. PMLR, 2019.   \n[50]  Chenggen Shi, Jie Lu, and Guangquan Zhang. An extended kuhn-tucker approach for linear bilevel programming. Applied Mathematics and Computation, 162(1):51-63, 2005.   \n[51] Ankur Sinha, Pekka Malo, and Kalyanmoy Deb. A review on bilevel optimization: From classical to evolutionary approaches and applications. IEEE Transactions on Evolutionary Computation, 22(2):276-295, 2017.   \n[52] Haoliang Sun, Chenhui Guo, Qi Wei, Zhongyi Han, and Yilong Yin. Learning to rectify for robust learning with noisy labels. Pattern Recognition, 124:108467, 2022.   \n[53] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one shot learning. Advances in neural information processing systems, 29, 2016.   \n[54] Xiaoyu Wang, Rui Pan, Renjie Pi, and Tong Zhang. Effective bilevel optimization via minimax reformulation. arXiv preprint arXiv:2305.13153, 2023.   \n[55]  Zhen Wang, Guosheng Hu, and Qinghua Hu. Training noise-robust deep neural networks via meta-learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4524-4533, 2020.   \n[56] Tianbao Yang and Yiming Ying. Auc maximization in the era of big data and ai: A survey. ACM Computing Surveys, 55(8):1-37, 2022.   \n[57]  Yifan Yang, Peiyao Xiao, and Kaiyi Ji. Achieving $O(\\epsilon^{-1.5})$ complexity in Hessian/Jacobian-free stochastic bilevel optimization. arXiv preprint arXiv:2312.03807, 2023.   \n[58] Huaxiu Yao, Yu Wang, Ying Wei, Peilin Zhao, Mehrdad Mahdavi, Defu Lian, and Chelsea Finn. Meta-learning with an adaptive task scheduler. Advances in Neural Information Processing Systems, 34:7497-7509, 2021.   \n[59]  Zhuoning Yuan, Zhishuai Guo, Nitesh Chawla, and Tianbao Yang. Compositional training for end-to-end deep auc maximization. In International Conference on Learning Representations, 2021.   \n[60] Zhuoning Yuan, Yan Yan, Milan Sonka, and Tianbao Yang. Large-scale robust deep auc maximization: A new surrogate loss and empirical studies on medical image classification. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3040-3049, 2021.   \n[61] Guoqing Zheng, Ahmed Hassan Awadallah, and Susan Dumais. Meta label correction for noisy label learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 11053-11061, 2021. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A  Specifications of Applications ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we provide a detailed introduction to the formulation utilized in Section 5. ", "page_idx": 13}, {"type": "text", "text": "A.1  Deep AUC Maximization (DAM) ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "For a classifier model $f(\\mathbf{w})$ , we have the AUC score function as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{A U C\\big(f(\\mathbf{w})\\big)=P r\\big(f(\\mathbf{w};x)\\geq f(\\mathbf{w};x^{\\prime})|y=1,y^{\\prime}=-1\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $P r(X)$ denote probability of an event $X$ being true. One of the surrogate loss (AUC square loss [9]) is given by: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{w}}\\frac{1}{n_{+}n_{-}}\\sum_{y_{i}=1}\\sum_{y_{j}=-1}\\Big(c-\\big(f(\\mathbf{w};x_{i})-f(\\mathbf{w};x_{j})\\big)\\Big)^{2},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $n_{+}$ and $n_{-}$ are the numbers of positive examples and negative examples, respectively, and $c$ is the margin parameter. One can transfer this problem into an equivalent minimax optimization problem according to Proposition 1 in [37] by: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{w},a,b}\\operatorname*{max}_{\\alpha}\\Phi(\\mathbf{w},a,b,\\alpha):=\\frac{1}{n}\\sum_{i=1}^{n}\\phi(\\mathbf{w},a,b,\\alpha;x_{i},y_{i}),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\phi(\\mathbf{w},a,b,\\alpha;x_{i},y_{i})=(1-p)\\big(f(\\mathbf{w};x_{i})-a\\big)^{2}\\cdot\\mathbb{I}_{y_{i}=1}+p\\big(f(\\mathbf{w};x_{i})-b\\big)^{2}\\cdot\\mathbb{I}_{y_{i}=-1}-p(1-p)\\alpha^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\,2\\alpha\\big(p(1-p)c+p f(\\mathbf{w};x_{i})\\cdot\\mathbb{I}_{y_{i}=-1}-(1-p)f(\\mathbf{w};x_{i})\\cdot\\mathbb{I}_{y_{i}=1}\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $a,b$ are margin parameters, $p=n_{+}/n$ . This formulation decomposed the individual examples, which is more favorable in stochastic scenarios. [59] proposed a compositional training algorithm for this problem. The compositional objective function is formulated as: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{w},a,b}\\operatorname*{max}_{\\alpha}\\Phi(\\mathbf{w}-\\alpha\\nabla L_{A V G}(\\mathbf{w}),a,b,\\alpha),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Where $\\begin{array}{r}{L_{A V G}=\\frac{1}{n}\\sum_{i=1}^{n}\\ell(\\mathbf{w};x_{i},y_{i})}\\end{array}$ $\\ell$ denotes task loss, e.g. cross-entropy in classfication tasks. Consider a multi-block problem with $m$ tasks. This problem can be reformulated as a multi-block minimax bi-level optimization problem [24]: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{w},a,b}\\operatorname*{max}_{\\boldsymbol{\\alpha}}\\sum_{j=1}^{m}\\Phi_{j}\\big(\\mathbf{u}_{j}^{*}(\\mathbf{w}_{j}),a_{j},b_{j},\\alpha_{j}\\big)\\qquad\\mathit{s.t.}\\;\\mathbf{u}_{j}^{*}(\\mathbf{w}_{j})=\\arg\\operatorname*{min}_{\\mathbf{u}_{j}}g_{j}\\big(\\mathbf{u}_{j},\\mathbf{w}_{j}\\big),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "A.2  Robust Meta-learning ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Consider the formulation of Robust Meta-learning in Section 5.2: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\phi}F(\\phi,\\mathbf{w}^{*}):=\\frac{1}{k}\\sum_{i=n-k+1}^{n}g_{[i]}\\big(\\phi,w_{i}^{*}(\\phi)\\big)\\;\\;\\;\\;\\;s.t.\\;\\;w_{i}^{*}(\\phi)=\\arg\\operatorname*{min}_{w_{i}}g_{i}(\\phi,w_{i}),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $g_{[n]}\\big(\\phi,w_{n}^{*}(\\phi)\\big)\\,\\leq\\,g_{[n-1]}\\big(\\phi,w_{n-1}^{*}(\\phi)\\big)\\,\\leq\\,\\ldots\\,\\leq\\,g_{[1]}\\big(\\phi,w_{1}^{*}(\\phi)\\big)$ denotes task-specific losses. Wedefine $g_{i}^{*}(\\phi):=g_{i}\\bigl(\\phi,w_{i}^{*}(\\phi)\\bigr)$ for simplicity in later formulations. The summation of the bottom- $\\cdot\\mathbf{k}$ losses is equivalent to the sum of all task losses subtracted by the sum of the top-(n-k) losses: ", "page_idx": 13}, {"type": "equation", "text": "$$\nF(\\phi,\\mathbf{w}^{*})=\\frac{1}{k}\\bigg(\\sum_{i=1}^{n}g_{i}^{*}(\\phi)-\\sum_{i=1}^{n-k}g_{[i]}^{*}(\\phi)\\bigg).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "With the Lemma 1 in [42], we have: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n-k}g_{[i]}^{*}(\\phi)=\\operatorname*{min}_{\\gamma}\\Big\\{(n-k)\\gamma+\\sum_{i=1}^{n}[g_{i}^{*}(\\phi)-\\gamma]_{+}\\Big\\}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Now we can convert the original upper-level problem to: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\phi}\\operatorname*{max}_{\\gamma}\\hat{F}(\\phi,\\mathbf{w}^{*},\\gamma):=\\frac{1}{k}\\sum_{i=1}^{n}\\big\\{g_{i}^{*}(\\phi)-[g_{i}^{*}(\\phi)-\\gamma]_{+}-\\frac{n-k}{n}\\gamma\\big\\}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The problem of robust meta-learning is then formulated as: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\phi}\\operatorname*{max}_{\\gamma}\\hat{F}(\\phi,\\mathbf{w}^{*},\\gamma)=\\frac{1}{k}\\sum_{i=1}^{n}\\Bigl\\{f_{i}(\\phi,w_{i}^{*},\\gamma):=g_{i}^{*}(\\phi)-[g_{i}^{*}(\\phi)-\\gamma]_{+}-\\frac{n-k}{n}\\gamma\\Bigr\\}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\phi$ is the parameter of meta-model, and $\\mathbf{w}\\;=\\;[w_{i},...,w_{n}]^{T}$ is the vector of task specific parameters. ", "page_idx": 14}, {"type": "text", "text": "Inspired by [10], we introduce a smoothed version of the upper-level loss function by incorporating Gaussian noise into the indicator function for alignment with the assumption of our MemCS algorithm: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\tilde{F}(\\phi,\\mathbf{w}^{*},\\gamma)=\\frac{1}{k}\\sum_{i=1}^{n}\\Big\\{f_{i}(\\phi,w_{i}^{*},\\gamma):=g_{i}^{*}(\\phi)-[g_{i}^{*}(\\phi)-\\gamma+\\epsilon Z]_{+}-\\frac{n-k}{n}\\gamma\\Big\\},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\epsilon>0$ is the smoothing parameter, and $Z\\sim\\mathcal{N}(0,1)$ is standard normal random variable. ", "page_idx": 14}, {"type": "text", "text": "B  Implementation Details and Extra Experimental Results ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1  Datasets Description ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Deep AUC Maximization. We assess our methodology using four datasets. The first dataset, CIFAR100 [29], serves primarily for classification endeavors. Within the context of the multi-block deep AUC maximization challenge, we treat each of the 100 categories as an individual block, with samples belonging to a specific category considered positive for that block. This dataset comprises 60, 000 images, each measuring $32\\times32$ pixels, divided into 50, 000 training and 10, 000 testing images. The CelebA [40] dataset encompasses 202,599 facial images, each annotated with a diverse set of attributes from 40 different features. The CheXpert [26] dataset includes 224,316 chest radiograph images from 65, 240 patients, marked for 14 distinct observations. Adhering to the methodology proposed in [24], we employ a simplified version of CheXpert with a reduced image resolution and omit the Fracture observation due to insufficient positive samples. Lastly, the OGBGMolPCBA [25] dataset is employed to predict molecular properties, representing each molecule as a graph with atoms as nodes and chemical bonds as edges, featuring 437, 929 such graphs annotated across 128 properties. ", "page_idx": 14}, {"type": "text", "text": "Robust Meta-learning. Our experiments are conducted over two popular datasets for few-shot learning: Mini-ImageNet [53] and Tiered-ImageNet [47]. Both datasets are subsets of the ILSVRC12 dataset. Mini-ImageNet comprises 100 classes, each containing 600 images with dimensions of $84\\times84$ pixels. The 100 classes are distributed among training, validation, and testing sets with a ratio of 64:16:20, respectively. In contrast, Tiered-ImageNet is a more extensive and challenging dataset, featuring 779,165 images annotated across 608 classes. These classes are further organized into 34 categories, with 20 categories designated for training, 6 for validation, and 8 for testing. ", "page_idx": 14}, {"type": "text", "text": "B.2  Implementation Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Deep AUC Maximization. For the CIFAR100 and the CelebA datasets, we use the ResNet18 architecture. For the large-scale CheXpert dataset, we opt for the DenseNet121 model pre-trained on ImageNet. When dealing with the OGBG-MolPCBA graph dataset, the Graph Isomorphism Network (GIN) is used for training. All experimental runs are performed using a single NVIDIA RTX 6000 GPU. Regarding hyperparameters, we set the total training epoch to 2000 for the CIFAR100 and 100 for the OGBG-MolPCBA datasets, adjust it to 40 for CelebA, and reduce it to 6 for CheXpert. The learning rate for the optimal approximator $\\mathbf{v}$ is uniformly set to $\\eta_{\\mathbf{v}}=0.1$ across all experiments, with $\\eta_{\\mathbf{w}}=\\eta_{\\mathbf{u}}=\\eta_{\\mathbf{v}}/\\lambda$ to maintain gradient magnitude consistency between u and v. This consistency is vital due to the influence of the $\\lambda$ parameter in the Lagrangian function on the update process for u. ", "page_idx": 14}, {"type": "image", "img_path": "GZoAUVSkaw/tmp/e3e8579c4df0ec24acc61e6a4ddf4e0a8e5d195b039acb2110b90da4f509e310.jpg", "img_caption": ["Figure Al: Test AUC score along with training epochs on the CIFAR10o (left) and the CelebA (right) datasets. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Table A1: Comparison of average iteration time and total training time of our method and AUCCT[24] on small scale dataset (CelebA) and large scale dataset(CheXpert). ", "page_idx": 15}, {"type": "image", "img_path": "GZoAUVSkaw/tmp/209da99eb567d503764a94cc64f23db25825bceebc68d842f2310a2f6e396009.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "GZoAUVSkaw/tmp/1aa3bfad70e69ff5f179da87e242b5930252280ece804eb3c10531a88f6cdc0e.jpg", "table_caption": ["Table A2: Comparison between FOSL and MemCS on robust meta-learning task. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Learning rate decay is applied to CelebA starting at the 30th epoch and to CheXpert at the 4th epoch, whereas no decay strategy is applied for CIFAR100 and OGBG-MolPCBA. ", "page_idx": 15}, {"type": "text", "text": "Robust Meta-learning. We adopt the Adam optimizer to update the meta-model in the context of MAML. For the hyper-parameters of MAML, we set the learning rate of meta-model update as $\\eta_{m e t a}=0.02$ , and set the learning rate of fast adaptation as $\\eta_{a d a p t}=0.02$ , with an adaptation step of 15. To be consistent with the DAM experiments, we configure the learning rate of the optimal approximator as $\\eta_{\\mathbf{v}}=\\eta_{a d a p t}=0.02$ , and the learning rate of w and meta-model parameter $\\phi$ as $\\eta_{\\phi}=\\eta_{\\mathbf{w}}=\\eta_{\\mathbf{v}}/\\lambda$ in the implementation. In practice, setting $\\lambda$ to 3 results in favorable performance. All experiments are conducted on a single NVIDIA RTX 6000 GPU using a widely used lightweight model featuring 4 convolutional layers (CNN4). ", "page_idx": 15}, {"type": "text", "text": "B.3 Extra Results on Deep AUC Maximization ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "This section presents the visualization of statistics throughout the training process and compares the computational costs with our method and AUC-CT [24]. To better grasp the training dynamics, we charted the test AUC scores across various training epochs for both the CIFAR100 and CelebA datasets, as shown in Figure A1. The findings demonstrate that our method not only exhibits enhanced generalization capabilities but also greater stability. ", "page_idx": 15}, {"type": "text", "text": "Moreover, to assess efficiency across varying dataset scales, we examined the average iteration times and total training time of our FOSL algorithm and AUC-CT [24] on different-sized datasets $\\mathrm{32\\times32}$ in CIFAR100 vs. $224\\times224$ in CheXpert) as detailed in Table A1. Note that the training time largely depends on the implementation and hyperparameters, such as the number of sampled tasks and batch sizes per task, suggesting that computational costs can be adjusted by modifying these hyperparameters according to the dataset. The result in Table A1 shows the ability to control training costs for datasets of various scales, which is indicated by the small gap between the total training time on CelebA and CheXpert datasets. Additionally, our method demonstrated a faster training pace compared to AUC-CT [24] under the same training settings. Note that the implementation of AUC-CT [24] avoided the calculation of second-order matrices so that the computational cost is more controllable with an increased dataset scale. ", "page_idx": 15}, {"type": "text", "text": "B.4  Comparison between FOSL and MemCS ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we compare our two proposed methods within the same experimental setting, i.e. robust meta-learning. To make it compatible with our FOSL algorithm, we slightly adjusted our training setting so that the number of training tasks in the dataset is known (20oo0 tasks) while maintaining the same testing procedures as those used with the MemCS algorithm. The results, including test accuracy, memory cost, and computational cost, are detailed in Table A2. The result shows that using FOSL in a robust meta-learning setting can introduce a greatly increased memory cost, which is especially significant for a small model. However, the single-loop nature of FOSL can drastically shorten the average iteration time during training. This makes the FOSL algorithm a potentially advantageous choice in scenarios involving larger base models and fewer blocks. ", "page_idx": 15}, {"type": "text", "text": "C Notations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The original problem we solve here is ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\underset{x\\in\\mathbb{R}^{d_{x}}}{\\mathrm{min}}\\underset{y\\in\\mathbb{R}^{d_{y}}}{\\mathrm{max}}F\\big(x,y,\\mathbf z^{*}(x)\\big):=\\frac{1}{n}\\sum_{i=1}^{n}f_{i}\\big(x,y,z_{i}^{*}(x)\\big)}\\\\ &{\\displaystyle=\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}_{\\xi}\\big[f_{i}\\big(x,y,z_{i}^{*}(x);\\xi_{i}\\big)\\big]}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Moreover, we define $\\begin{array}{r}{z_{\\lambda,i}^{*}(x)=\\arg\\operatorname*{min}_{z}\\mathcal{L}_{i}(x,y^{*}(x),z,v)}\\end{array}$ and $y^{*}(x)=\\arg\\operatorname*{max}_{y}F{\\big(}x,y,\\mathbf{z}^{*}(x){\\big)}$ For the convenience of proof, we also define ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\Phi(x)=F{\\big(}x,y^{*}(x),\\mathbf{z}^{*}(x){\\big)}={\\frac{1}{n}}\\sum_{i=1}^{n}f_{i}{\\big(}x,y^{*}(x),z_{i}^{*}(x){\\big)}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For the notational convenience of FOSL, we define the estimators of client set $I_{t}$ as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle h_{x}^{t}:=\\frac{1}{|I_{t}|}\\sum_{i\\in I_{t}}\\left[h_{x,i}^{t}:=\\nabla_{x}\\mathcal{L}_{i}(x_{t},y_{t},z_{i,t},v_{i,t};\\xi_{x,i}^{t})\\right],}\\\\ {\\displaystyle h_{y}^{t}:=\\frac{1}{|I_{t}|}\\sum_{i\\in I_{t}}\\left[h_{y,i}^{t}:=\\nabla_{y}f_{i}(x_{t},y_{t},v_{i,t};\\xi_{y,i}^{t})\\right],}\\\\ {\\displaystyle h_{z}^{t}:=\\frac{1}{|I_{t}|}\\sum_{i\\in I_{t}}\\left[h_{y,i}^{t}:=\\nabla_{z}\\mathcal{L}_{i}(x_{t},y_{t},z_{i,t};\\xi_{z,i}^{t})\\right],}\\\\ {\\displaystyle h_{v}^{t}:=\\frac{1}{|I_{t}|}\\sum_{i\\in I_{t}}\\left[h_{v,i}^{t}:=\\nabla_{z}g_{i}(x_{t},v_{i,t};\\xi_{v,i}^{t})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Since we sample tasks without replacement and our estimators are unbiased, we have the expectations ofestimators as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\widetilde{h}_{x}^{t}:=\\mathbb{E}[h_{x}^{t}]=\\frac{1}{n}\\sum_{i=1}^{n}\\Big[\\widetilde{h}_{x,i}^{t}:=\\nabla_{x}\\mathcal{L}_{i}(x_{t},y_{t},z_{i,t},v_{i,t})\\Big],}\\\\ {\\displaystyle\\widetilde{h}_{y}^{t}:=\\mathbb{E}[h_{y}^{t}]=\\frac{1}{n}\\sum_{i=1}^{n}\\Big[\\widetilde{h}_{y,i}^{t}:=\\nabla_{y}f(x_{t},y_{t},v_{i,t})\\Big],}\\\\ {\\displaystyle\\widetilde{h}_{z}^{t}:=\\mathbb{E}[h_{z}^{t}]=\\frac{1}{n}\\sum_{i=1}^{n}\\Big[\\widetilde{h}_{z,i}^{t}:=\\nabla_{z}\\mathcal{L}_{i}(x_{t},y_{t},z_{i,t},v_{i,t})\\Big],}\\\\ {\\displaystyle\\widetilde{h}_{v}^{t}:=\\mathbb{E}[h_{v}^{t}]=\\frac{1}{n}\\sum_{i=1}^{n}\\Big[\\widetilde{h}_{v,i}^{t}:=\\nabla_{z}g(x_{t},v_{i,t})\\Big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We also define the optimal Lagrangian estimator of $x$ and its gradients as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}^{*}(x):=\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\mathcal{L}_{i}\\big(x,y^{*}(x),z_{\\lambda,i}^{*}(x),z_{i}^{*}(x)\\big),}\\\\ &{\\mathcal{H}^{*}(x):=\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\Big[\\mathcal{H}_{i}^{*}(x):=\\nabla_{x}\\mathcal{L}\\big(x,y^{*}(x),z_{\\lambda,i}^{*}(x),z_{i}^{*}(x)\\big)\\Big]}\\\\ &{\\qquad\\qquad=\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\Big[\\nabla_{x}f_{i}\\big(x,y^{*}(x),z_{\\lambda,i}^{*}(x)\\big)+\\lambda\\Big(\\nabla_{x}g_{i}\\big(x,z_{\\lambda,i}^{*}(x)\\big)-\\nabla_{x}g_{i}\\big(x,z_{i}^{*}(x)\\big)\\Big)\\Big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "D  Proofs of Preliminary Lemmas ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "D.1   Some basic properties ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Lemma D.1. UnderAssumptions 4.3, 44, for $\\begin{array}{r}{\\forall\\lambda\\geq\\frac{2L_{f,1}}{\\mu_{g}}}\\end{array}$ both $\\mathcal{L}_{i}(x,y,z,v)$ and $\\mathcal{L}(x,y,z,v)$ are $\\Big(\\frac{\\lambda\\mu_{g}}{2}\\Big)$ -strongly convex in $z$ and $L_{\\lambda,1}$ -smooth in $\\left(x,y,z\\right)$ where $L_{\\lambda,1}:=3\\lambda L_{g,1}$ ", "page_idx": 17}, {"type": "text", "text": "Proof. Since $\\begin{array}{r}{\\lambda\\geq\\frac{2L_{f,1}}{\\mu_{g}}\\geq\\frac{2L_{f,1}}{L_{g,1}}}\\end{array}$ we have that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\nabla_{z z}^{2}\\mathcal{L}_{i}(x,y,z,v)|\\|=\\|\\nabla_{z z}^{2}f_{i}(x,y,z)+\\lambda\\nabla_{z z}^{2}g_{i}(x,z)\\|\\ge\\||\\lambda\\nabla_{z z}^{2}g_{i}(x,z)|-\\|\\nabla_{z z}^{2}f_{i}(x,y,z)\\|\\ge\\frac{\\lambda\\mu_{g}}{2},}\\\\ &{|\\nabla_{z z}^{2}\\mathcal{L}(x,y,z,v)|\\|=\\|\\nabla_{z z}^{2}F(x,y,z)+\\lambda\\nabla_{z z}^{2}G(x,z)\\|\\ge\\|\\lambda\\nabla_{z z}^{2}G(x,z)\\|-\\|\\nabla_{z z}^{2}F(x,y,z)\\|\\ge\\frac{\\lambda\\mu_{g}}{2};}\\\\ &{|\\nabla^{2}\\mathcal{L}_{i}(x,y,z,v)|\\|=\\|\\nabla^{2}f_{i}(x,y,z)+\\lambda\\nabla^{2}g_{i}(x,z)-\\lambda\\nabla^{2}g_{i}(x,v)\\|\\le L_{f_{1}}+2\\lambda L_{g,1}\\le3\\lambda L_{g,1}=:L_{\\lambda,1},}\\\\ &{|\\nabla^{2}\\mathcal{L}(x,y,z,v)|\\|=\\|\\nabla^{2}F(x,y,z)+\\lambda\\nabla^{2}G(x,z)-\\lambda\\nabla^{2}G(x,v)\\|\\le L_{f_{1}}+2\\lambda L_{g,1}\\le3\\lambda L_{g,1}=:L_{\\lambda,1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then the proof is complete. ", "page_idx": 17}, {"type": "text", "text": "Lemma D.2. Under Assumptions 4.3, 4.4, for $\\begin{array}{r}{\\lambda\\;\\ge\\;\\operatorname*{max}\\left\\{\\frac{2L_{f,1}}{\\mu_{g}},(1+\\frac{L_{g,1}}{\\mu_{g}})\\frac{L_{f,1}^{2}}{3\\mu_{f}L_{g,1}}\\right\\}}\\end{array}$ g) 3\u03bcfLlg,, we have $\\begin{array}{r}{\\|\\nabla z_{i}^{*}(x)\\|\\leq\\frac{L_{g,1}}{\\mu_{g}},\\,\\|\\nabla z_{\\lambda,i}^{*}(x)\\|\\leq\\frac{12L_{g,1}}{\\mu_{g}}\\,a n d\\,\\|\\nabla y^{*}(x)\\|\\leq\\Big(1+\\frac{L_{g,1}}{\\mu_{g}}\\Big)\\frac{L_{f,1}}{\\mu_{f}}.}\\end{array}$ ", "page_idx": 17}, {"type": "text", "text": "Proof. Recallthat we define $z_{i}^{*}(x):=\\arg\\operatorname*{min}_{z}{g_{i}(x,z)}$ and $z_{\\lambda,i}^{*}(x):=\\arg\\operatorname*{min}_{z}\\mathcal{L}_{i}(x,y^{*}(x),z,v)$ Then we have $\\nabla_{z}g_{i}\\bigl(x,z_{i}^{*}(x)\\bigr)\\;=\\;{\\bf0}$ and $\\nabla_{z}\\mathcal{L}_{i}\\big(x,y^{*}(x),z_{\\lambda,i}^{*}(x),v\\big)=\\mathbf{0}$ .Via implicit function theorem, we obtain ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{x z}^{2}g_{i}\\big(x,z_{i}^{*}(x)\\big)+\\big(\\nabla z_{i}^{*}(x)\\big)^{T}\\nabla_{z z}^{2}g_{i}\\big(x,z_{i}^{*}(x)\\big)=\\mathbf{0},}\\\\ &{\\nabla_{x z}^{2}\\mathcal{L}_{i}\\big(x,y^{*}(x),z_{\\lambda,i}^{*}(x),v\\big)+\\big(\\nabla y^{*}(x)\\big)^{T}\\nabla_{y z}^{2}\\mathcal{L}_{i}\\big(x,y^{*}(x),z_{\\lambda,i}^{*}(x),v\\big)}\\\\ &{\\quad+\\big(\\nabla z_{\\lambda,i}^{*}(x)\\big)^{T}\\nabla_{z z}^{2}\\mathcal{L}_{i}\\big(x,y^{*}(x),z_{\\lambda,i}^{*}(x),v\\big)=\\mathbf{0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "To measure that Lipschitz continuity of $z_{i}^{*}(x)$ and $z_{\\lambda,i}^{*}(x)$ w.r.t. $\\mathbf{X}$ , we take spectral norm of $\\nabla z_{i}^{*}(x)$ and $\\nabla z_{\\lambda,i}^{*}(x)$ as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla z_{i}^{*}(x)\\|=\\!\\|-\\nabla_{x z}^{2}g_{i}\\big(x,z_{i}^{*}(x)\\big)\\big[\\nabla_{z z}^{2}g_{i}\\big(x,z_{i}^{*}(x)\\big)\\big]^{-1}\\big\\|\\overset{(a)}{\\leq}\\frac{L_{g,1}}{\\mu_{g}},}\\\\ &{\\|\\nabla z_{\\lambda,i}^{*}(x)\\|=\\!\\big\\|-\\nabla_{x z}^{2}\\mathcal{L}_{i}\\big(x,y^{*}(x),z_{\\lambda,i}^{*}(x),v\\big)\\big[\\nabla_{z z}^{2}\\mathcal{L}_{i}\\big(x,y^{*}(x),z_{\\lambda,i}^{*}(x),v\\big)\\big]^{-1}}\\\\ &{\\qquad\\qquad\\qquad-\\left(\\nabla y^{*}(x)\\right)^{T}\\!\\nabla_{y z}^{2}\\mathcal{L}_{i}\\big(x,y,z_{\\lambda,i}^{*}(x),v\\big)\\big[\\nabla_{z z}^{2}\\mathcal{L}_{i}\\big(x,y^{*}(x),z_{\\lambda,i}^{*}(x),v\\big)\\big]^{-1}\\big\\|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where (a) uses Assumption 4.4. Similarly, for $y^{*}(x)$ ,we have $\\nabla_{y}F\\big(x,y^{*}(x),\\mathbf{z}^{*}(x)\\big)\\,=\\,\\mathbf{0}$ Via implicit function theorem, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\left[\\nabla_{x y}^{2}f_{i}\\big(x,y^{*}(x),z_{i}^{*}(x)\\big)+\\left(\\nabla y^{*}(x)\\right)^{T}\\nabla_{y y}^{2}f_{i}\\big(x,y^{*}(x),z_{i}^{*}(x)\\big)\\right.}\\\\ {\\displaystyle\\left.+\\left(\\nabla z_{i}^{*}(x)\\right)^{T}\\nabla_{z y}^{2}f_{i}\\big(x,y^{*}(x),z_{i}^{*}(x)\\big)\\right]=0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which indicates ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\|\\nabla y^{*}(x_{t})\\|\\leq\\biggr\\|\\frac{1}{n}\\sum_{i=1}^{n}\\left[\\nabla_{x y}^{2}f_{i}(x,y^{*}(x),z_{i}^{*}(x))+\\left(\\nabla z_{i}^{*}(x)\\right)^{T}\\nabla_{y z}^{2}f_{i}(x,y^{*}(x),z_{i}^{*}(x))\\right]\\biggr\\|}}\\\\ &{}&{\\cdot\\left\\|\\left[\\nabla_{y y}^{2}F(x,y^{*}(x),\\mathbf{z}^{*}(x))\\right]^{-1}\\right\\|}\\\\ &{}&{\\leq\\biggr(\\frac{1}{n}\\sum_{i=1}^{n}\\left\\|\\left[\\nabla_{x y}^{2}f_{i}(x,y^{*}(x),z_{i}^{*}(x))+\\left(\\nabla z_{i}^{*}(x)\\right)^{T}\\nabla_{y z}^{2}f_{i}(x,y^{*}(x),z_{i}^{*}(x))\\right]\\right\\|\\biggr)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\cdot\\left\\|\\left[\\nabla_{y y}^{2}F\\big(x,y^{*}(x),\\mathbf{z}^{*}(x)\\big)\\right]^{-1}\\right\\|}\\\\ &{\\stackrel{(a)}{\\leq}\\biggl(1+\\frac{L_{g,1}}{\\mu_{g}}\\biggr)\\frac{L_{f,1}}{\\mu_{f}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where (a) uses Assumption 4.3 and Assumption 4.4. Back to the second equation in eq. (8), with $\\begin{array}{r}{\\|\\nabla y^{*}(x_{t})\\|\\le\\big(1+\\frac{{L_{g,1}}}{\\mu_{g}}\\big)\\frac{{L_{f,1}}}{\\mu_{f}}}\\end{array}$ \uff09\uff0cwehave ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla z_{\\lambda,i}^{*}(x)\\|\\leq\\!\\big\\|\\nabla_{x z}^{2}\\mathcal{L}_{i}\\big(x,y^{*}(x),z_{\\lambda,i}^{*}(x),v\\big)+\\big(\\nabla y^{*}(x)\\big)^{T}\\nabla_{y z}^{2}\\mathcal{L}_{i}\\big(x,y,z_{\\lambda,i}^{*}(x),v\\big)\\big\\|}\\\\ &{\\quad\\quad\\quad\\cdot\\big\\|\\big[\\nabla_{z z}^{2}\\mathcal{L}_{i}\\big(x,y^{*}(x),z_{\\lambda,i}^{*}(x),v\\big)\\big]^{-1}\\big\\|}\\\\ &{\\quad\\quad\\quad\\overset{(a)}{\\leq}\\!\\bigg[3\\lambda L_{g,1}+\\Big(1+\\displaystyle\\frac{L_{g,1}}{\\mu_{g}}\\Big)\\displaystyle\\frac{L_{f,1}^{2}}{\\lambda\\mu_{g}}\\bigg]\\displaystyle\\frac{2}{\\lambda\\mu_{g}}}\\\\ &{\\quad\\quad\\quad\\overset{(b)}{\\leq}\\displaystyle\\frac{12L_{g,1}}{\\mu_{g}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where (a) uses Lemma D.1 and (b) uses $\\begin{array}{r}{\\lambda\\ge(1+\\frac{L_{g,1}}{\\mu_{g}})\\frac{L_{f,1}^{2}}{3\\mu_{f}L_{g,1}}}\\end{array}$ . Then the proof is complete. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "Lemma D.3. Under Assumptions 4.3, 4.4, the optimal solutions $z_{i}^{*}(x),\\,z_{\\lambda,i}^{*}(x)$ and $y_{i}^{*}(x)$ are $L_{*,z}.$ \uff0c $L_{*,z_{\\lambda}}$ and $L_{*,y}$ -smooth respectively, where we define $L_{*,z},\\,L_{*,z_{\\lambda}}$ and $L_{*,y}$ as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L_{*,z}:=\\frac{L_{g,2}}{\\mu_{g}}\\bigg(1+\\frac{L_{g,1}}{\\mu_{g}}\\bigg)^{2},\\quad L_{*,z_{\\lambda}}:=\\bigg(1+\\Big(1+\\frac{L_{g,1}}{\\mu_{g}}\\Big)\\frac{L_{f,1}}{\\mu_{f}}+\\frac{12L_{g,1}}{\\mu_{g}}\\bigg),}\\\\ &{L_{*,y}:=\\bigg(1+\\frac{L_{g,1}}{\\mu_{g}}+\\Big(1+\\frac{L_{g,1}}{\\mu_{g}}\\Big)\\frac{L_{f,1}}{\\mu_{f}}\\bigg)^{2}\\frac{L_{f,2}}{\\mu_{f}}+\\bigg(1+\\frac{L_{g,1}}{\\mu_{g}}\\bigg)^{2}\\frac{L_{f,1}L_{g,2}}{\\mu_{f}\\mu_{g}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\begin{array}{r l r l r l}&{\\omega r}&{a n y}&{i}&{\\in}&{\\{1,...,n\\},}&{w h e r e}&{w e}&{a s s u m e}&{\\lambda}&{\\geq}&{\\left\\{2L_{f,1}/\\mu_{g},(1\\mathrm{~+~}\\frac{L_{g,1}}{\\mu_{g}})\\frac{L_{f,1}^{2}}{3\\mu_{f}L_{g,1}},(1\\mathrm{~+~}\\frac{L_{g,1}}{\\mu_{g}})\\frac{L_{f,1}^{2}}{3\\mu_{f}L_{g,1}},(1\\mathrm{~+~}\\frac{L_{g,1}}{\\mu_{g}})\\frac{L_{f,1}^{2}}{3\\mu_{f}L_{g,1}},(1\\mathrm{~-~}\\frac{L_{g,1}}{\\mu_{g}})\\frac{L_{f,1}^{2}}{3\\mu_{f}L_{g,1}}\\right\\},}\\end{array}}\\\\ &{\\begin{array}{r l}&{\\frac{\\gamma_{g,1}}{\\mu_{g}})\\frac{L_{f,1}L_{f,2}}{3\\mu_{f}L_{g,1}},\\frac{L_{f,1}L_{*,y}}{6L_{g,1}}\\Bigl(1+(1+\\frac{L_{g,1}}{\\mu_{g}})\\frac{L_{f,1}}{\\mu_{f}}+\\frac{12L_{g,1}}{\\mu_{g}}\\Bigr)^{-1},\\bigl((1+\\frac{L_{g,1}}{\\mu_{g}})\\frac{L_{f,1}}{\\mu_{f}}+1\\bigr)\\frac{L_{f,1}}{L_{g,1}}\\Bigr\\}.}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. Since $z_{i}^{*}(x)=\\arg\\operatorname*{min}_{z}g_{i}(x,z)$ , we have $\\nabla_{z}g_{i}\\big(x,z^{*}(x)\\big)=\\mathbf{0}$ , which indicates that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\nabla_{x z}^{2}g_{i}\\big(x,z^{*}(x)\\big)+\\nabla z^{*}(x)\\nabla_{z z}^{2}g_{i}\\big(x,z^{*}(x)\\big)=\\mathbf{0}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "For any $x_{1},x_{2}\\in\\mathbb{R}^{d_{x}}$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla_{z}^{2}(x_{1})-\\nabla_{z}^{2}(z_{2})\\|}\\\\ &{=\\left\\|\\nabla_{x}^{2}\\partial_{z}\\left(x_{1},z_{1}^{*}(x_{2})\\right)\\right\\|\\nabla_{x}^{2}\\partial_{y}\\left(x_{1},z_{1}^{*}(x_{1})\\right)\\right\\|^{-1}-\\nabla_{x}^{2}\\partial_{z}\\left(x_{2},z_{1}^{*}(x_{2})\\right)\\left\\|\\nabla_{x}^{2}\\partial_{y}\\left(x_{2},z_{1}^{*}(x_{2})\\right)\\right\\|^{-1}\\Big\\|}\\\\ &{\\leq\\Big\\|\\nabla_{x}^{2}\\partial_{z}\\left(x_{1},z_{1}^{*}(x_{1})\\right)-\\nabla_{x}^{2}\\partial_{z}\\left(x_{2},z_{1}^{*}(x_{2})\\right)\\Big\\|\\nabla_{x}^{2}\\partial_{y}\\left(x_{1},z_{1}^{*}(x_{1})\\right)^{-1}\\Big\\|}\\\\ &{\\qquad+\\left\\|\\nabla_{x}^{2}\\partial_{y}\\left(x_{2},z_{1}^{*}(x_{2})\\right)\\left(\\left[\\nabla_{x}^{2}\\partial_{y}\\left(x_{1},z_{1}^{*}(x_{1})\\right)\\right]^{-1}-\\left[\\nabla_{x}^{2}\\partial_{y}\\left(x_{2},z_{1}^{*}(x_{2})\\right)\\right]^{-1}\\right\\|}\\\\ &{\\leq\\left\\|\\nabla_{x}^{2}\\partial_{y}\\left(x_{1},z_{1}^{*}(x_{1})\\right)-\\nabla_{x}^{2}\\partial_{y}\\left(z_{2},z_{1}^{*}(x_{2})\\right)\\right\\|\\cdot\\left\\|\\nabla_{x}^{2}\\partial_{z}\\left(x_{1},z_{1}^{*}(x_{1})\\right)\\right\\|^{-1}\\Big\\|}\\\\ &{\\qquad+\\left\\|\\nabla_{x}^{2}\\partial_{y}\\left(x_{2},z_{1}^{*}(x_{2})\\right)\\right\\|\\cdot\\left\\|\\nabla_{x}^{2}\\partial_{z}\\left(x_{1},z_{1}^{*}(x_{1})\\right)\\right\\|^{-1}-\\left[\\nabla_{x}^{2}\\partial\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where (a) uses Assumption 4.3, 4.4 and $(A^{-1}-B^{-1})\\,=\\,A^{-1}(B\\,-\\,A)B^{-1}$ ; (b) follows from Lemma D.2. Next, plug $x=x_{1}$ and $x=x_{2}$ into eq. (9) and differentiate these two equations, then weget ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bigl(\\nabla y^{*}(x_{1})\\bigr)^{T}\\nabla_{y y}^{2}F\\bigl(x_{1},y^{*}(x_{1}),z_{i}^{*}(x_{1})\\bigr)-\\bigl(\\nabla y^{*}(x_{2})\\bigr)^{T}\\nabla_{y y}^{2}F\\bigl(x_{2},y^{*}(x_{2}),z_{i}^{*}(x_{2})\\bigr)}\\\\ &{=\\bigl(\\nabla y^{*}(x_{1})-\\nabla y^{*}(x_{2})\\bigr)^{T}\\nabla_{y y}^{2}F\\bigl(x_{1},y^{*}(x_{1}),z_{i}^{*}(x_{1})\\bigr)}\\\\ &{\\quad+\\bigl(\\nabla y^{*}(x_{2})\\bigr)^{T}\\Bigl(\\nabla_{y y}^{2}F\\bigl(x_{1},y^{*}(x_{1}),z_{i}^{*}(x_{1})\\bigr)-\\nabla_{y y}^{2}F\\bigl(x_{2},y^{*}(x_{2}),z_{i}^{*}(x_{2})\\bigr)\\Bigr),}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and by using eq. (9), we get ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\bigl[\\bigl(\\nabla y^{*}(x_{1})\\bigr)^{T}\\nabla_{y y}^{2}f_{i}\\bigl(x_{1},y^{*}(x_{1}),z_{i}^{*}(x_{1})\\bigr)-\\bigl(\\nabla y^{*}(x_{2})\\bigr)^{T}\\nabla_{y y}^{2}f_{i}\\bigl(x_{2},y^{*}(x_{2}),z_{i}^{*}(x_{2})\\bigr)\\bigr]}\\\\ &{\\quad=\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\Big[\\nabla_{x y}^{2}f_{i}\\bigl(x_{1},y^{*}(x_{1}),z_{i}^{*}(x_{1})\\bigr)-\\nabla_{x y}^{2}f_{i}\\bigl(x_{2},y^{*}(x_{2}),z_{i}^{*}(x_{2})\\bigr)}\\\\ &{\\qquad\\qquad+\\bigl(\\nabla z_{i}^{*}(x_{1})-\\nabla z_{i}^{*}(x_{2})\\bigr)^{T}\\nabla_{z y}^{2}f_{i}\\bigl(x_{1},y^{*}(x_{1}),z_{i}^{*}(x_{1})\\bigr)}\\\\ &{\\qquad\\qquad+\\bigl(\\nabla z_{i}^{*}(x_{1})\\bigr)^{T}\\Bigl(\\nabla_{z y}^{2}f_{i}\\bigl(x_{1},y^{*}(x_{1}),z_{i}^{*}(x_{1})\\bigr)-\\nabla_{z y}^{2}f_{i}\\bigl(x_{1},y^{*}(x_{1}),z_{i}^{*}(x_{1})\\bigr)\\Bigr)\\Bigr].}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "By combining eq. (11), eq. (12) and taking norm, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}_{\\sigma}^{\\dagger}(x)-\\mathbb{P}_{\\sigma}^{\\dagger}(x)\\Big|}\\\\ &{\\quad\\le\\operatorname*{lim}\\mathbb{P}_{\\sigma}^{\\dagger}\\Big(\\sigma_{x}\\Big(x_{t},\\eta_{t}^{*}(x_{t})\\Big)\\Big)^{-1}\\Big|}\\\\ &{\\quad\\le\\bigg(1\\prod_{s=0}^{\\frac{1}{\\eta_{t}}}\\mathbb{P}_{\\sigma}^{\\dagger}\\Big(x_{t},\\eta_{t}^{*}(x_{t}),x_{t}^{*}(x_{t})\\Big)-\\eta_{x}^{*}\\Big)\\Big(\\sigma_{x}^{\\top}\\sigma_{y}^{\\top}(x_{t}),z_{t}^{*}(x_{t})\\Big)\\bigg|}\\\\ &{\\quad\\quad+\\operatorname*{lim}\\frac{1}{\\eta_{t}}\\sum_{k=0}^{\\infty}\\mathbb{P}_{\\sigma}^{\\dagger}\\Big(\\sigma_{x}^{\\top}\\sigma_{y}^{\\top}(x_{t})-\\mathbb{P}_{\\sigma}^{\\top}\\Big)\\mathbb{P}_{\\sigma}^{\\dagger}\\Big(x_{t},y_{t}^{*}(x_{t})\\Big)\\mathbb{I}\\bigg|}\\\\ &{\\quad\\quad+\\operatorname*{lim}\\frac{1}{\\eta_{t}}\\sum_{k=0}^{\\infty}\\mathbb{P}_{\\sigma}^{\\dagger}\\Big(x_{t},\\eta_{t}^{*}(x_{t})\\Big)^{\\top}\\Big(\\sigma_{x}^{\\top}\\sigma_{x}\\Big(x_{t},y_{t}^{*}(x_{t})\\Big)-\\eta_{x}^{*}\\sigma_{y}^{\\top}\\Big(x_{t},y_{t}^{*}(x_{t}),x_{t}^{*}(x_{t})\\Big)\\Big)\\bigg|}\\\\ &{\\quad\\quad+\\operatorname*{lim}\\frac{1}{\\eta_{t}}\\sum_{k=0}^{\\infty}\\mathbb{P}_{\\sigma}^{\\dagger}\\Big(x_{t}^{*}\\Big)\\mathbb{P}_{\\sigma}^{\\dagger}\\Big(x_{t},y_{t}^{*}(x_{t}),x_{t}^{*}(x_{t})\\Big)-\\mathbb{P}_{\\sigma}^{\\top}\\Big(x_{t},y_{t}^{*}(x_{t}),x_{t}^{*}(x_{t})\\Big)\\Big|}\\\\ &{\\quad\\le\\operatorname*{lim}\\frac{1}{\\eta_{\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where (a) uses Assumption 4.4 and Lemma D.2; (b) follows from Assumption 4.4, Lemma D.2, and eq. (10). Similarly to eq. (10), from eq. (7), if we simplify the notation as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A_{1}=\\nabla_{x z}^{2}\\mathcal{L}_{i}\\big(x_{1},y^{*}(x_{1}),z_{\\lambda,i}^{*}(x_{1}),v_{1}\\big)+\\big(\\nabla y^{*}(x_{1})\\big)^{T}\\nabla_{y z}^{2}\\mathcal{L}_{i}\\big(x_{1},y^{*}(x_{1}),z_{\\lambda,i}^{*}(x),v_{1}\\big),}\\\\ &{B_{1}=\\nabla_{z z}^{2}\\mathcal{L}_{i}\\big(x_{1},y^{*}(x_{1}),z_{\\lambda,i}^{*}(x_{1}),v_{1}\\big)}\\\\ &{A_{2}=\\nabla_{x z}^{2}\\mathcal{L}_{i}\\big(x_{2},y^{*}(x_{2}),z_{\\lambda,i}^{*}(x_{2}),v_{2}\\big)+\\big(\\nabla y^{*}(x_{2})\\big)^{T}\\nabla_{y z}^{2}\\mathcal{L}_{i}\\big(x_{2},y^{*}(x_{1}),z_{\\lambda,i}^{*}(x),v_{2}\\big)}\\\\ &{B_{2}=\\nabla_{z z}^{2}\\mathcal{L}_{i}\\big(x_{2},y^{*}(x_{2}),z_{\\lambda,i}^{*}(x_{2}),v_{2}\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "then we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\|\\nabla z_{\\lambda,i}^{*}(x_{1})-\\nabla z_{\\lambda,i}^{*}(x_{2})\\|=\\|A_{1}B_{1}^{-1}-A_{2}B_{2}^{-1}\\|\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq\\parallel(A_{1}-A_{2})B_{1}^{-1}\\parallel+\\parallel A_{2}(B_{1}^{-1}-B_{2}^{-1})\\parallel}\\\\ &{\\leq\\parallel A_{1}-A_{2}\\parallel\\cdot\\parallel B_{1}^{-1}\\parallel+\\parallel A_{2}\\parallel\\cdot\\parallel B_{1}^{-1}-B_{2}^{-1}\\parallel}\\\\ &{\\leq\\parallel A_{1}-A_{2}\\parallel\\cdot\\parallel B_{1}^{-1}\\parallel+\\parallel A_{2}\\parallel\\cdot\\parallel B_{1}^{-1}\\parallel\\cdot\\parallel B_{2}^{-1}\\parallel\\cdot\\parallel B_{1}-B_{2}\\parallel.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For the first term in eq. (13), via Lemma D.1, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\begin{array}{r l}&{\\frac{u^{*}}{s}\\cdot\\sum_{i=1}^{s}-\\relax_{x2}\\|+\\|y^{*}(x_{1})-y^{*}(x_{2})\\|+\\|z_{\\bar{s},i}^{*}(x_{1})-z_{\\bar{s},i}^{*}(x_{2})\\|)}\\\\ &{\\quad+L_{f,1}\\|\\nabla y^{*}(x_{1})-\\nabla y^{*}(x_{2})\\|}\\\\ &{\\quad+L_{f,2}\\|\\nabla y^{*}(x_{2})\\|\\cdot\\left(\\|x_{1}-x_{2}\\|+\\|y^{*}(x_{1})-y^{*}(x_{2})\\|+\\|z_{\\bar{s},i}^{*}(x_{1})-z_{\\bar{s},i}^{*}(x_{2})\\|\\right)}\\\\ &{\\overset{(a)}{\\leq}\\biggl(3\\lambda L_{g,1}+\\Big(1+\\displaystyle\\frac{L_{g,1}}{\\mu_{g}}\\Big)\\displaystyle\\frac{L_{f,1}L_{f,2}}{\\mu_{f}}\\biggr)(\\|x_{1}-x_{2}\\|+\\|y^{*}(x_{1})-y^{*}(x_{2})\\|+\\|z_{\\bar{s},i}^{*}(x_{1})-z_{\\bar{s},i}^{*}(x_{2})\\|)}\\\\ &{\\quad+L_{f,1}\\|\\nabla y^{*}(x_{1})-\\nabla y^{*}(x_{2})\\|}\\\\ &{\\overset{(b)}{\\leq}6\\lambda L_{g,1}\\biggl(1+\\Big(1+\\displaystyle\\frac{L_{g,1}}{\\mu_{g}}\\Big)\\displaystyle\\frac{L_{f,1}}{\\mu_{f}}+\\displaystyle\\frac{12L_{g,1}}{\\mu_{g}}\\biggr)\\|x_{1}-x_{2}\\|+L_{f,1}L_{s,y}\\|x_{1}-x_{2}\\|}\\\\ &{\\overset{(c)}{\\leq}9\\lambda L_{g,1}\\biggl(1+\\Big(1+\\displaystyle\\frac{L_{g,1}}{\\mu_{g}}\\Big)\\displaystyle\\frac{L_{f,1}}{\\mu_{f}}+\\displaystyle\\frac{12L_{g,1}}{\\mu_{g}}\\biggr)\\|x_{1}-x_{2}\\|}\\end{array}}&{\\begin{array}{r l}&{(1+\\displaystyle\\frac{s}{\\lambda})}\\\\ &{\\quad+1\\,.}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "$\\begin{array}{r}{\\lambda\\ge(1+\\frac{L_{g,1}}{\\mu_{g}})\\frac{L_{f,1}L_{f,2}}{3\\mu_{f}L_{g,1}}}\\end{array}$ uses $\\begin{array}{r}{\\lambda\\geq\\frac{L_{f,1}L_{*,y}}{6L_{g,1}}\\big[1+(1+\\frac{L_{g,1}}{\\mu_{g}})\\frac{L_{f,1}}{\\mu_{f}}+\\frac{12L_{g,1}}{\\mu_{g}}\\big]^{-1}}\\end{array}$ By using Lema D.1 we have $\\begin{array}{r}{\\|B_{1}^{-1}\\|\\leq\\frac{2}{\\lambda\\mu_{g}}}\\end{array}$ $\\begin{array}{r}{\\|B_{2}^{-1}\\|\\leq\\frac{2}{\\lambda\\mu_{g}}}\\end{array}$ \uff0c $\\|B_{1}-B_{2}\\|\\le3\\lambda L_{g,1}\\|x_{1}-\\|$ and ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|A_{2}\\|=\\!\\big\\|\\nabla_{x z}^{2}\\mathcal{L}_{i}\\big(x_{2},y^{*}(x_{2}),z_{\\lambda,i}^{*}(x_{2}),v_{2}\\big)+\\big(\\nabla y^{*}(x_{2})\\big)^{T}\\nabla_{y z}^{2}\\mathcal{L}_{i}\\big(x_{2},y^{*}(x_{1}),z_{\\lambda,i}^{*}(x),v_{2}\\big)\\big\\|}\\\\ &{\\qquad\\leq\\!\\big\\|\\nabla_{x z}^{2}\\mathcal{L}_{i}\\big(x_{2},y^{*}(x_{2}),z_{\\lambda,i}^{*}(x_{2}),v_{2}\\big)\\big\\|+\\|\\nabla y^{*}(x_{2})\\|\\cdot\\big\\|\\nabla_{y z}^{2}f_{i}\\big(x_{2},y^{*}(x_{1}),z_{\\lambda,i}^{*}(x)\\big)\\big\\|\\,}\\\\ &{\\qquad\\overset{(a)}{\\leq}\\!\\big(L_{f,1}+\\lambda L_{g,1}\\big)+\\Big(1+\\frac{L_{g,1}}{\\mu_{g}}\\Big)\\frac{L_{f,1}^{2}}{\\mu_{f}}\\overset{(b)}{\\leq}2\\lambda L_{g,1},\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "hee aAa; $\\begin{array}{r}{\\lambda\\geq\\left(\\left(1+\\frac{L_{g,1}}{\\mu_{g}}\\right)\\frac{L_{f,1}}{\\mu_{f}}+1\\right)\\frac{L_{f,1}}{L_{g,1}}}\\end{array}$ We also have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\|B_{1}-B_{2}\\|=\\!3\\lambda L_{g,1}\\big(\\|x_{1}-x_{2}\\|+\\|y^{*}(x_{1})-y^{*}(x_{2})\\|+\\|z_{\\lambda,i}^{*}(x_{1})-z_{\\lambda,i}^{*}(x_{2})\\|\\big)}\\\\ {\\overset{(a)}{\\leq}\\!3\\lambda L_{g,1}\\bigg(\\!1+\\Big(1+\\frac{L_{g,1}}{\\mu_{g}}\\Big)\\frac{L_{f,1}}{\\mu_{f}}+\\frac{12L_{g,1}}{\\mu_{g}}\\bigg)\\|x_{1}-x_{2}\\|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where (a) uses Lemma D.2. Combining eq. (14), eq. (15), eq. (16) with the results in Lemma D.1, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla z_{\\lambda,i}^{*}(x_{1})-\\nabla z_{\\lambda,i}^{*}(x_{2})\\|\\leq\\|A_{1}-A_{2}\\|\\cdot\\|B_{1}^{-1}\\|+\\|A_{2}\\|\\cdot\\|B_{1}^{-1}\\|\\cdot\\|B_{2}^{-1}\\|\\cdot\\|B_{1}-B_{2}\\|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\bigg(\\frac{18L_{g,1}}{\\mu_{g}}+\\frac{24L_{g,1}^{2}}{\\mu_{g}^{2}}\\bigg)\\bigg(1+\\Big(1+\\frac{L_{g,1}}{\\mu_{g}}\\Big)\\frac{L_{f,1}}{\\mu_{f}}+\\frac{12L_{g,1}}{\\mu_{g}}\\bigg)\\|x_{1}-x_{2}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Thus, the proof is complete. ", "page_idx": 20}, {"type": "text", "text": "D.2 Gap of Lower-level Optimal Points ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Lemma D.4. Under Assumptions 4.3, 4.4, for any given x and , the gap between the optimal solutions of the lower-level problem $\\dot{z}_{i}^{*}(x)$ and the surrogateminimax problem $z_{\\lambda,i}^{*}(x)$ canbeboundedas ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\ \\|\\boldsymbol{z}_{\\lambda,i}^{*}(\\boldsymbol{x})-\\boldsymbol{z}_{i}^{*}(\\boldsymbol{x})\\|\\leq\\displaystyle\\frac{L_{f,0}}{\\mu_{g}\\lambda},}\\\\ &{\\|\\nabla{z}_{\\lambda,i}^{*}(\\boldsymbol{x})-\\nabla{z}_{i}^{*}(\\boldsymbol{x})\\|\\leq\\displaystyle\\frac{1}{\\lambda}\\cdot\\left[\\frac{1}{\\mu_{g}}\\biggl(\\frac{L_{f,0}L_{g,2}}{\\mu_{g}}+L_{f,1}\\biggl(1+\\bigl(1+\\frac{L_{g,1}}{\\mu_{g}}\\bigr)\\frac{L_{f,1}}{\\mu_{f}}\\biggr)\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\ \\displaystyle\\frac{6L_{g,1}}{\\mu_{g}^{2}}\\Bigl(1+\\bigl(1+\\frac{L_{g,1}}{\\mu_{g}}\\bigr)\\Bigr)\\cdot\\Bigl(L_{f,1}+\\frac{L_{f,0}L_{g,2}}{\\mu_{g}}\\Bigr)\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "for any $i\\in\\{1,...,n\\}$ where we assume $\\begin{array}{r}{\\lambda\\geq\\operatorname*{max}\\left\\{\\frac{2L_{f,1}}{\\mu_{g}},(1+\\frac{L_{g,1}}{\\mu_{g}})\\frac{L_{f,1}^{2}}{3\\mu_{f}L_{g,1}}\\right\\}}\\end{array}$ ", "page_idx": 20}, {"type": "text", "text": "Proof. For each block, we can have that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\|z_{\\lambda,i}^{*}(x)-z_{i}^{*}(x)\\|\\overset{(a)}{\\leq}\\cfrac{1}{\\mu_{g}}\\|\\nabla_{z}g_{i}\\big(x,z_{\\lambda,i}^{*}(x)\\big)-\\nabla_{z}g_{i}\\big(x,z_{i}^{*}(x)\\big)\\|}\\\\ &{}&{\\stackrel{(b)}{\\leq}\\cfrac{1}{\\mu_{g}\\lambda}\\|\\nabla_{z}f_{i}(x,y^{*}(x),z_{\\lambda,i}^{*}(x))\\|\\overset{(c)}{\\leq}\\cfrac{L_{f,0}}{\\mu_{g}\\lambda},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where (a) uses Assumption 4.3; (b) follows from the definition of $z_{i}^{*}(x)$ and $z_{\\lambda,i}^{*}(x)$ ; (c) uses Assumption 4.4. For the second part, since $\\nabla_{z}g_{i}\\left(x,z_{i}^{*}(x)\\right)=0$ \uff0c $\\nabla_{z}\\mathcal{L}_{i}\\big(x,y^{*}(x),z_{\\lambda,i}^{*}(x)\\big)=0.$ we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\nabla_{x z}^{2}g_{i}\\big(x,z_{i}^{*}(x)\\big)+\\nabla z_{i}^{*}(x)\\nabla_{z z}^{2}g_{i}\\big(x,z_{i}^{*}(x)\\big)=0,}}\\\\ {{\\nabla_{x z}^{2}\\mathcal{L}_{i}\\big(x,y,z_{\\lambda,i}^{*}(x)\\big)+\\nabla y^{*}(x)\\nabla_{y z}^{2}\\mathcal{L}_{i}\\big(x,y,z_{\\lambda,i}^{*}(x)\\big)+\\nabla z_{\\lambda,i}^{*}(x)\\nabla_{z z}^{2}\\mathcal{L}_{i}\\big(x,y,z_{\\lambda,i}^{*}(x)\\big)=0,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which indicates that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla z_{i}^{*}(x)=-\\nabla_{x z}^{2}g_{i}\\big(x,z_{i}^{*}(x)\\big)\\left[\\nabla_{z z}^{2}g_{i}\\big(x,z_{i}^{*}(x)\\big)\\right]^{-1},}\\\\ &{\\7z_{\\lambda,i}^{*}(x)=-\\left[\\nabla_{x z}^{2}\\mathcal{L}_{i}\\big(x,y^{*}(x),z_{\\lambda,i}^{*}(x)\\big)+\\nabla y^{*}(x)\\nabla_{y z}^{2}\\mathcal{L}_{i}\\big(x,y,z_{\\lambda,i}^{*}(x)\\big)\\right]\\left[\\nabla_{z z}^{2}\\mathcal{L}_{i}\\big(x,y^{*}(x),z_{\\lambda,i}^{*}(x)\\big)\\right]^{-1}}\\\\ &{\\qquad\\qquad=-\\frac{\\left[\\nabla_{x z}^{2}\\mathcal{L}_{i}\\big(x,y^{*}(x),z_{\\lambda,i}^{*}(x)\\big)+\\nabla y^{*}(x)\\nabla_{y z}^{2}\\mathcal{L}_{i}\\big(x,y,z_{\\lambda,i}^{*}(x)\\big)\\right]}{\\lambda}\\left[\\frac{\\nabla_{z z}^{2}\\mathcal{L}_{i}\\big(x,y^{*}(x),z_{\\lambda,i}^{*}(x)\\big)}{\\lambda}\\right]^{-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then the gap can be displayed as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left|\\nabla_{\\theta}\\cdot\\widehat{\\mathbf z}_{j}(a)-\\nabla_{\\theta}^{\\star}(a)\\right|\\leq\\frac{\\theta}{2}\\left|\\exp\\left(\\frac{1}{\\theta}\\right)-\\frac{\\theta}{2}\\sum_{k\\in\\{a,b\\}}^{{\\mathsf{T}}_{n}}\\sum_{\\ell=1}^{{M}_{\\mathsf{E}}_{j}}\\left|\\nabla_{\\theta}\\widehat{\\mathbf z}_{j}(a,b,c,b,c,b)\\right|\\right.}\\\\ {\\quad}&{+\\frac{{\\theta}^{2}}{2}\\left|\\exp\\left(\\theta\\cdot\\widehat{\\mathbf z}_{j}(a)\\right)\\right|^{-1}\\right|}\\\\ {\\quad}&{\\quad+\\left|\\nabla_{\\theta}^{\\star}\\widehat{\\mathbf z}_{j}(a,b,c,b)\\right|\\leq\\frac{1}{\\theta}\\sum_{k\\in\\{a,b\\}}^{\\mathsf{T}_{n}}\\sum_{\\ell=1}^{M}\\theta\\frac{1}{\\theta}\\sum_{\\ell=1}^{{M}_{\\mathsf{E}}_{j}}\\bigg|\\theta\\right|}\\\\ {\\quad}&{\\quad+\\frac{{\\theta}^{2}}{2}\\sum_{k\\in\\{a,b\\}}^{\\mathsf{T}_{n}}\\sum_{\\ell=1}^{{M}_{\\mathsf{E}}_{j}}\\bigg|\\nabla_{\\theta}\\widehat{\\mathbf z}_{j}(a,b,c,b,c,a)\\bigg|\\theta\\bigg|}\\\\ {\\quad}&{\\quad-\\frac{{\\theta}^{2}}{2}\\left|\\exp\\left(\\theta\\cdot\\widehat{\\mathbf z}_{j}(a)\\right)\\right|^{-1}\\bigg|-\\left[\\frac{{\\theta}^{2}}{\\theta}\\sum_{k\\in\\{a,b\\}}^{\\mathsf{T}_{n}}\\sum_{\\ell=1}^{{M}_{\\mathsf{E}}_{j}}\\bigg|\\theta\\right]}\\\\ {\\quad}&{\\quad\\bigg|\\frac{\\theta}{2}\\bigg|\\int_{{\\theta}\\in\\mathbb{R}^{2}_{0}}^{\\mathsf{T}_{n}}\\bigg|\\theta\\cdot\\widehat{\\mathbf z}_{j}(a,b,c,b)\\bigg|\\leq\\frac{{\\theta}^{2}}{2}\\bigg|\\exp\\left(\\theta\\cdot\\widehat{\\mathbf z}_{j}(a,b,c,b)\\right)\\bigg|}\\\\ {\\quad}&{\\quad+\\frac{{\\theta}^{2}}{2}\\sum_{k\\in\\{a,b\\}}^{\\mathsf{T}_{n}}\\sum_{\\ell=1}^{M}\\theta\\frac{1}{\\theta}\\sum_{\\ell=1}^{{M}_{ \n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where (a) can be satisfied because ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\nabla_{z z}^{2}g_{i}\\left(x,z_{i}^{*}(x)\\right)\\right|^{-1}-\\left[\\frac{\\nabla_{z z}^{2}\\mathcal{L}_{i}\\left(x,y^{*}(x),z_{\\lambda,i}^{*}(x)\\right)}{\\lambda}\\right]^{-1}\\right|}\\\\ &{=\\left\\|\\left[\\nabla_{z z}^{2}g_{i}\\left(x,z_{i}^{*}(x)\\right)\\right]^{-1}\\right\\|\\cdot\\left\\|\\frac{\\nabla_{z z}^{2}\\mathcal{L}_{i}\\left(x,y^{*}(x),z_{\\lambda,i}^{*}(x)\\right)}{\\lambda}-\\nabla_{z z}^{2}g_{i}\\left(x,z_{i}^{*}(x)\\right)\\right\\|}\\\\ &{\\quad\\cdot\\left\\|\\left[\\frac{\\nabla_{z z}^{2}\\mathcal{L}_{i}\\left(x,y^{*}(x),z_{\\lambda,i}^{*}(x)\\right)}{\\lambda}\\right]^{-1}\\right\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\stackrel{(a.1)}{\\leq}\\frac{2}{\\mu_{g}^{2}}\\left(\\left\\|\\frac{\\nabla_{z z}^{2}f_{i}\\left(x,y^{*}(x),z_{\\lambda,i}^{*}(x)\\right)}{\\lambda}\\right\\|+\\left\\|\\nabla_{z z}^{2}g_{i}\\left(x,z_{\\lambda,i}^{*}(x)\\right)-\\nabla_{z z}^{2}g_{i}\\left(x,z_{i}^{*}(x)\\right)\\right\\|\\right)}\\\\ &{\\leq\\frac{2}{\\mu_{g}^{2}}\\Big(\\displaystyle\\frac{L_{f,1}}{\\lambda}+L_{g,2}\\|z_{\\lambda,i}^{*}(x)-z_{i}^{*}(x)\\|\\Big)}\\\\ &{\\leq\\displaystyle\\frac{2}{\\mu_{g}^{2}}\\Big(L_{f,1}+\\frac{L_{f,0}L_{g,2}}{\\mu_{g}}\\Big)\\frac{1}{\\lambda},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where (a.1) uses Assumption 4.3 and Lemma D.1. Then, the proof is complete. ", "page_idx": 22}, {"type": "text", "text": "Lemma D.5. Under Assumptions 4.3, 4.4, the gap between $\\nabla\\Phi(x)$ and $\\mathcal{H}^{*}(x)$ can be bounded as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left\\|\\nabla\\Phi(x)-\\mathcal{H}^{*}(x)\\right\\|^{2}\\leq\\frac{C_{g a p}}{\\lambda^{2}}\\;,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\begin{array}{r l}{C_{g a p}\\;:=\\;3\\Big(1\\,+\\,\\frac{L_{f,1}^{2}}{\\mu_{g}^{2}}\\Big)L_{f,1}^{2}\\Big(\\frac{L_{f,0}}{\\mu_{g}}\\Big)^{2}\\,+\\,\\Big(1\\,+\\,\\frac{L_{g,1}^{2}}{\\mu_{g}^{2}}\\Big)\\frac{3L_{g,1}^{2}}{2}\\Big(\\frac{L_{f,0}}{\\mu_{g}}\\Big)^{4}}\\end{array}$ and we assume $\\lambda~\\geq$ max $\\begin{array}{r l r}&{}&{\\Big\\{\\frac{2L_{f,1}}{\\mu_{g}},\\big(1+\\frac{L_{g,1}}{\\mu_{g}}\\big)\\frac{L_{f,1}^{2}}{3\\mu_{f}L_{g,1}}\\Big\\}}\\end{array}$ ", "page_idx": 22}, {"type": "text", "text": "Proof. By the definitions of $\\nabla F\\big(x,y^{*}(x),\\mathbf{z}^{*}(x)\\big)$ and $\\mathcal{H}^{*}(x)$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\nabla F\\big(x,y^{*}(x),\\mathbf{z}^{*}(x)\\big)-\\mathcal{H}^{*}(x)\\right\\|^{2}}\\\\ &{\\quad=\\bigg\\|\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\nabla f_{i}\\big(x,y^{*}(x),z_{i}^{*}(x)\\big)-\\nabla\\mathcal{L}_{i}\\big(x,y^{*}(x),z_{\\lambda,i}^{*}(x),z_{i}^{*}(x)\\big)\\bigg\\|^{2}}\\\\ &{\\quad\\le\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\big\\|\\nabla f_{i}\\big(x,y^{*}(x),z_{i}^{*}(x)\\big)-\\nabla\\mathcal{L}_{i}\\big(x,y^{*}(x),z_{\\lambda,i}^{*}(x),z_{i}^{*}(x)\\big)\\big\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "For any $i\\in\\{1,...,n\\}$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\{Q(x,y^{*}(t),\\xi,\\xi(x))-\\mathbb{E}\\{Q(x,y^{*}(t),\\xi,\\xi(x),\\xi(y))\\}^{2}\\Big\\}}\\\\ &{=\\mathbb{E}\\{Q(x,y^{*}(t),\\xi,\\xi(x))-\\mathbb{E}\\{Q(x,y^{*}(t),\\xi,\\xi(x))\\}^{2}\\Big\\}}\\\\ &{\\quad-\\mathbb{E}\\{Q(x,y^{*}(t),\\xi,\\xi(x))-\\mathbb{E}\\{Q(x,y^{*}(t),\\xi,\\xi(x))\\}^{2}\\Big\\}}\\\\ &{\\le\\mathbb{E}\\{Q(x,y^{*}(t),\\xi,\\xi(x))-\\mathbb{E}\\{Q(x,y^{*}(t),\\xi,\\xi(x))\\}^{2}\\Big\\}}\\\\ &{\\le\\mathbb{E}\\{Q(x,y^{*}(t),\\xi,\\xi(x))\\}^{2}\\Big\\}\\\\ &{\\quad+\\mathbb{E}\\{Q(x,y^{*}(t),\\xi,\\xi(x))\\}^{2}\\Big\\}\\Big|\\eta_{t}\\le\\mathbb{E}\\{Q(x,y^{*}(t),\\xi,\\xi(x))\\}^{2}\\Big\\}\\\\ &{\\quad+\\mathbb{E}\\{Q(x,y^{*}(t),\\xi,\\xi(x))\\}^{2}\\Big\\}\\Big|\\eta_{t}\\le\\mathbb{E}\\{Q(x,y^{*}(t),\\xi,\\xi(x))\\}^{2}\\mathbb{E}\\{Q(x,y^{*}(t),\\xi,\\xi(x))\\}^{2}\\Big\\}\\\\ &{\\quad+\\mathbb{E}\\{Q(x,y^{*}(t),\\xi,\\xi(x))\\}^{2}\\mathbb{E}\\{Q(x,y^{*}(t),\\xi,\\xi(x))\\}^{2}\\mathbb{E}\\{Q(x,y^{*}(t),\\xi,\\xi(x))\\}^{2}\\mathbb{E}\\{Q(x,y^{*}(t),\\xi,\\xi(x))\\}^{2}\\Big\\}}\\\\ &{\\le\\mathbb{E}\\{Q(x,y^{*}(t),\\xi,\\xi(x))\\}^{2}\\mathbb{E}\\{Q(x,y^{*}(t),\\xi,\\xi(x))\\}^{2}\\mathbb{E}\\{Q(x,y^{*}(t),\\xi,\\xi(x))\\}^{2}}\\\\ &{\\quad+\\mathbb{E}\\{Q(x,y^{*}(t),\\xi,\\xi(x))\\}^{2}\\mathbb{E}\\{Q(x,y^{*}(t),\\xi,\\xi(x)) \n$$)L,1l2,(x)-(x)1\u00b2+6x\u00b2(1+ ()()(", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\n\\leq\\bigg[3\\Big(1+\\frac{L_{f,1}^{2}}{\\mu_{g}^{2}}\\Big)L_{f,1}^{2}\\Big(\\frac{L_{f,0}}{\\mu_{g}}\\Big)^{2}+\\Big(1+\\frac{L_{g,1}^{2}}{\\mu_{g}^{2}}\\Big)\\frac{3L_{g,1}^{2}}{2}\\Big(\\frac{L_{f,0}}{\\mu_{g}}\\Big)^{4}\\bigg]\\frac{1}{\\lambda^{2}},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where (a) follows from Assumption 4.3, 4.4 and eq. (7); (b) follows from Assumption 4.3, 4.4 and Lemma 1 in [44]; (c) uses Lemma D.4. The proof is finished by substituting eq. (18) into eq. (17). $\\sqsupset$ ", "page_idx": 23}, {"type": "text", "text": "Lemma D.6.Under Assumptions 4.3, 4.4, the gradient of Lagrangian function with optimal solutions $\\mathcal{H}^{*}(x)$ .is $L_{*,1}$ -Lipschiz continuous in $x$ where we defne $\\begin{array}{r}{L_{*,1}:=\\left(1\\!+\\!\\frac{12L_{g,1}}{\\mu_{g}}\\!+\\!\\left(1\\!+\\!\\frac{L_{g,1}}{\\mu_{g}}\\right)\\!\\frac{L_{f,1}}{\\mu_{f}}\\right)\\!L_{f,1}+}\\end{array}$ $\\begin{array}{r l}&{\\begin{array}{r l}&{\\frac{\\langle1+\\frac{L_{g,1}}{\\mu_{g}}\\rangle}{\\langle1+\\frac{L_{g,2}}{\\mu_{g}}\\rangle}\\frac{L_{f,0}L_{g,2}}{\\mu_{g}}+L_{g,2}\\biggl[\\frac{1}{\\mu_{g}}\\biggl(\\frac{L_{f,0}L_{g,2}}{\\mu_{g}}+L_{f,1}\\bigl(1+\\bigl(1+\\frac{L_{g,1}}{\\mu_{g}}\\bigr)\\frac{L_{f,1}}{\\mu_{f}}\\bigr)\\biggr)+\\frac{6L_{g,1}^{-}}{\\mu_{g}^{2}}\\biggl(1+\\bigl(1+\\frac{L_{g,1}}{\\mu_{g}}\\bigr)\\biggr)\\biggr/\\biggl(L_{f,1}+\\frac{L_{g,1}}{\\mu_{g}}\\biggr)}\\\\ &{\\frac{\\dot{\\mathcal{L}}_{f,0}L_{g,2}}{\\mu_{g}}\\biggr)\\biggr]\\;a n d w e\\;a s s u m e\\;\\lambda\\geq\\Bigl\\{2L_{f,1}/\\mu_{g},\\bigl(1+\\frac{L_{g,1}}{\\mu_{g}}\\bigr)\\frac{L_{f,1}^{2}}{3\\mu_{f}L_{g,1}},\\bigl(1+\\frac{L_{g,1}}{\\mu_{g}}\\bigr)\\frac{L_{f,1}L_{f,2}}{3\\mu_{f}L_{g,1}},\\frac{L_{f,1}L_{e,1}}{6L_{g,1}}\\Bigl(1+\\bigl(1+\\frac{L_{g,1}}{\\mu_{g}}\\bigr)\\frac{L_{g,1}}{\\mu_{g}}\\bigr)\\biggr\\}}\\\\ &{\\frac{\\dot{\\mathcal{L}}_{f,1}}{\\mu_{g}}\\frac{L_{f,1}}{\\mu_{f}}+\\frac{12L_{g,1}}{\\mu_{g}}\\Bigr)^{-1},\\bigl(\\bigl(1+\\frac{L_{g,1}}{\\mu_{g}}\\bigr)\\frac{L_{f,1}}{\\mu_{f}}+1\\bigr)\\frac{L_{f,1}}{L_{g,1}}\\biggr\\}.}\\end{array}}\\end{array}$ ", "page_idx": 23}, {"type": "text", "text": "Proof. Recall that in eq. (6), ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathcal{H}^{*}(x)=\\frac{1}{n}\\sum_{i=1}^{n}\\Big[\\nabla_{x}f_{i}\\big(x,y^{*}(x),z_{\\lambda,i}^{*}(x)\\big)+\\lambda\\Big(\\nabla_{x}g_{i}\\big(x,z_{\\lambda,i}^{*}(x)\\big)-\\nabla_{x}g_{i}\\big(x,z_{i}^{*}(x)\\big)\\Big)\\Big].\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Then we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\nabla\\mathcal{H}^{*}(x)\\overset{(a)}{=}\\cfrac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\nabla_{x x}^{2}f_{i}(x,y^{*}(x),z_{\\lambda,i}^{*}(x))+\\left(\\nabla y^{*}(x)\\right)^{T}\\nabla_{y x}^{2}f_{i}\\left(x,y^{*}(x),z_{\\lambda,i}^{*}(x)\\right)}&{{}\\quad}\\\\ {+\\left(\\nabla z_{\\lambda,i}^{*}(x)\\right)^{T}\\nabla_{z x}^{2}f_{i}(x,y^{*}(x),z_{\\lambda,i}^{*}(x))+\\lambda\\big(\\nabla_{x x}^{2}g_{i}(x,z_{\\lambda,i}^{*}(x)-\\nabla_{x x}^{2}g_{i}(x,z_{i}^{*}(x))\\big)}&{{}\\quad}\\\\ {+\\,\\lambda\\Big(\\big(\\nabla z_{\\lambda,i}^{*}(x)\\big)^{T}\\nabla_{z x}^{2}g_{i}\\big(x,z_{\\lambda,i}^{*}(x)\\big)-\\big(\\nabla z_{i}^{*}(x)\\big)^{T}\\nabla_{z x}^{2}g_{i}\\big(x,z_{i}^{*}(x)\\big)\\Big).}&{{}\\quad\\qquad(1\\leq x\\leq n)}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "By taking norm, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|\\nabla V^{\\prime}(x)\\|\\leq\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\big[\\nabla_{x}^{2}f_{i}(x,y^{*}(x);z_{i}^{*},\\alpha_{i}^{*}(x))\\big]+\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\|\\nabla\\nu^{\\prime}(x^{\\prime})\\|\\big[\\nabla_{x^{\\prime}}^{2}f_{i}(x,y^{*}(x);z_{i}^{*},\\alpha_{i}^{*}(x))\\big]}\\\\ &{+\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\|\\nabla_{x}^{2}f_{i}(x)\\|\\nabla_{y^{\\prime}}^{2}f_{i}(x,y^{*}(x);z_{i}^{*},\\alpha_{i}^{*}(x))}\\\\ &{+\\frac{3}{n}\\displaystyle\\sum_{i=1}^{n}\\Big[\\big[\\nabla_{x}^{2}g_{i}(x,y^{*}(x);z_{i}^{*},\\alpha_{i}^{*}(x))}\\\\ &{+\\log\\big[\\big[\\nabla_{x}^{2}\\hat{z}_{i}(x,y^{*}(x);z_{i}^{*},\\alpha_{i}^{*}(x))\\big]\\big]}\\\\ &{\\qquad+\\left.\\|\\nabla_{x}\\zeta(x)\\|\\cdot\\big[\\nabla_{x^{\\prime}}^{2}g_{i}(x,z_{i}^{*},\\alpha_{i}^{*}(x))-\\nabla_{x}^{2}g_{i}(x,z_{i}^{*}(x))\\big]\\right]}\\\\ &{\\qquad+\\|\\nabla_{x}\\zeta(x)-\\nabla_{x^{\\prime}}^{2}(x)\\|\\cdot\\big[\\nabla_{x^{\\prime}}^{2}g_{i}(x,z_{i}^{*},\\alpha_{i}^{*}(x))\\big]\\|}\\\\ {\\|\\tilde{\\leq}\\bigg(1+\\frac{12L_{\\mathcal{D},1}}{\\beta\\mu}+\\big(1+\\frac{L_{\\mathcal{D},1}}{\\beta\\mu}\\big)\\frac{L_{\\mathcal{D},1}}{\\beta\\mu}\\bigg)\\|_{\\mathcal{E}_{1}}+\\lambda\\Big(1+\\frac{L_{\\mathcal{D},1}}{\\beta\\mu}\\big)\\|_{\\mathcal{E}_{2}}f_{i}(x)-\\hat{\\varepsilon}(x)\\|}\\\\ &{+\\lambda\\ln\\frac{1}{\\beta\\mu}\\|\\nabla_ \n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where (a) uses Assumption 4.4 and Lemma D.2; (b) follows from Lemma D.4 Then, the proof is complete. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "E Proofs of Theorem 4.10 and and Corollary 4.11 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "E.1  Descent in Objective Function ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Lemma E.1. Under Assumptions 4.3, 4.4, 4.5 and Lemma D.6, for $L_{*,1}$ -smooth $\\mathcal L^{*}(x)$ ,theconsecutive iterates of Algorithm I satisfy: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathbb{E}\\big[\\mathcal{L}^{*}(x_{t+1})-\\mathcal{L}^{*}(x_{t})\\big]}\\\\ {\\le-\\displaystyle\\frac{\\eta_{x}}{2}\\mathbb{E}\\|\\mathcal{H}^{*}(x_{t})\\|^{2}-\\displaystyle\\frac{\\eta_{x}}{2}\\mathbb{E}\\|\\widetilde{h}_{x}^{t}\\|^{2}+\\displaystyle\\frac{\\eta_{x}^{2}L_{*,1}}{2}\\mathbb{E}\\|h_{x}^{t}\\|^{2}+\\displaystyle\\frac{3\\eta_{x}L_{f,1}^{2}}{2}\\mathbb{E}\\big\\|y_{t}-y^{*}(x_{t})\\big\\|^{2}}\\\\ {+\\displaystyle\\frac{3\\eta_{x}L_{\\lambda,1}^{2}}{2}\\mathbb{E}\\bigg[\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\big\\|z_{i,t}-z_{\\lambda,i}^{*}(x_{t})\\big\\|^{2}+\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\big\\|v_{i,t}-z_{i}^{*}(x_{t})\\big\\|^{2}\\bigg]}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "forall $t\\in\\{0,1,...,T-1\\}$ ,where we assume $\\begin{array}{r}{\\lambda\\ge\\frac{2L_{f,1}}{\\mu_{g}}}\\end{array}$ ", "page_idx": 24}, {"type": "text", "text": "Proof. Recall the definitions of $\\mathcal L^{*}(x)$ and $\\mathcal{H}^{*}(x)$ in eq. (6). By using the smoothness of $\\mathcal{L}^{*}\\left(x_{t}\\right)$ in Lemma D.6, we have that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\mathcal{L}^{*}(x_{t+1})]}\\\\ &{\\le\\mathbb{E}[\\mathcal{L}^{*}(x_{t})]+\\mathbb{E}\\langle\\mathcal{H}^{*}(x_{t}),x_{t+1}-x_{t}\\rangle+\\frac{L_{\\alpha,1}}{2}\\mathbb{E}\\|x_{t+1}-x_{t}\\|^{2}}\\\\ &{=\\mathbb{E}[\\mathcal{L}^{*}(x_{t})]-\\eta_{x}\\mathbb{E}\\langle\\mathcal{H}^{*}(x_{t}),h_{x}^{t}\\rangle+\\frac{\\eta_{x}^{2}L_{\\alpha,1}}{2}\\mathbb{E}\\|h_{x}^{t}\\|^{2}}\\\\ &{=\\mathbb{E}[\\mathcal{L}^{*}(x_{t})]-\\eta_{x}\\langle\\mathcal{H}^{*}(x_{t}),\\widehat{h}_{x}^{t}\\rangle+\\frac{\\eta_{x}^{2}L_{\\alpha,1}}{2}\\mathbb{E}\\|h_{x}^{t}\\|^{2}}\\\\ &{=\\mathbb{E}[\\mathcal{L}^{*}(x_{t})]-\\frac{\\eta_{x}}{2}\\mathbb{E}\\|\\mathcal{H}^{*}(x_{t})\\|^{2}-\\frac{\\eta_{x}}{2}\\mathbb{E}\\|\\widehat{h}_{x}^{t}\\|^{2}+\\frac{\\eta_{x}}{2}\\mathbb{E}\\|\\mathcal{H}^{*}(x_{t})-\\widehat{h}_{x}^{t}\\|^{2}+\\frac{\\eta_{x}^{2}L_{\\alpha,1}}{2}\\mathbb{E}\\|h_{x}^{t}\\|^{2}}\\\\ &{\\overset{(a)}{\\le}\\mathbb{E}[\\mathcal{L}^{*}(x_{t})]-\\frac{\\eta_{x}}{2}\\mathbb{E}\\|\\mathcal{H}^{*}(x_{t})\\|^{2}-\\frac{\\eta_{x}}{2}\\mathbb{E}\\|\\widehat{h}_{x}^{t}\\|^{2}+\\frac{\\eta_{x}^{2}L_{\\alpha,1}}{2}\\mathbb{E}\\|h_{x}^{t}\\|^{2}+\\frac{3\\eta_{x}L_{\\alpha,1}^{2}}{2}\\mathbb{E}\\|y_{t}-y^{*}(x_{t})\\|}\\\\ &{\\quad+\\frac{3\\eta_{x}L_{\\alpha,1}^{2}}{2}\\mathbb{E}\\Big[\\frac{1}{n},\\frac{\\eta^{3}} \n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where (a) follows from ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\|{\\mathcal{H}}^{*}(x_{t})-\\tilde{h}_{x}^{t}\\|^{2}}\\\\ &{\\quad=\\mathbb{E}\\bigg\\|\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\nabla_{x}{\\mathcal{L}}_{i}\\big(x_{t},y^{*}(x_{t}),z_{\\lambda,i}^{*}(x_{t}),z_{i}^{*}(x_{t})\\big)-\\nabla_{x}{\\mathcal{L}}_{i}\\big(x_{t},y_{t},z_{i,t},v_{i,t}\\big)\\bigg\\|^{2}}\\\\ &{\\quad\\overset{(a,1)}{\\leq}\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}\\big\\|\\nabla_{x}{\\mathcal{L}}_{i}\\big(x_{t},y^{*}(x_{t}),z_{\\lambda,i}^{*}(x_{t}),z_{i}^{*}(x_{t})\\big)-\\nabla_{x}{\\mathcal{L}}_{i}\\big(x_{t},y_{t},z_{i,t},v_{i,t}\\big)\\big\\|^{2}}\\\\ &{\\quad\\overset{(a,2)}{\\leq}3L_{f,1}^{2}\\mathbb{E}\\big\\|y_{t}-y^{*}(x_{t})\\big\\|^{2}+3L_{\\lambda,1}^{2}\\mathbb{E}\\bigg[\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\big\\|z_{i,t}-z_{\\lambda,i}^{*}(x_{t})\\big\\|^{2}+\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\big\\|v_{i,t}-z_{i}^{*}(x_{t})\\big\\|^{2}\\bigg],}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and (a.1) uses Jensen inequality; (a.2) follows from Assumption 4.4 and Lemma D.1. Then, the proof is complete. ", "page_idx": 24}, {"type": "text", "text": "E.2 Bounds of Estimators ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Lemma E.2. Under Assumptions 4.3, 4.4, 4.5, 4.6, the estimators of $v_{i}$ $z_{i}$ y and  canbe bounded as ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\|h_{v,i}^{t}\\|^{2}\\leq2L_{g,1}^{2}\\mathbb{E}\\|v_{i,t}-z_{i}^{*}(x_{t})\\|^{2}+2\\sigma_{g,}^{2},}\\\\ &{\\mathbb{E}\\|h_{z,i}^{t}\\|^{2}\\leq4(L_{f,1}^{2}+\\lambda^{2}L_{g,1}^{2})\\Big(\\mathbb{E}\\|y_{t}-y^{*}(x_{t})\\|^{2}+\\mathbb{E}\\|z_{i,t}-z_{\\lambda,i}^{*}(x_{t})\\|^{2}\\Big)+4(\\sigma_{f}^{2}+\\lambda^{2}\\sigma_{g}^{2}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{\\ E}\\|h_{y}^{t}\\|^{2}\\leq\\displaystyle\\frac{\\sigma_{f}^{2}}{|I_{t}|}+\\frac{(n-|I_{t}|)\\sigma_{t h}^{2}}{(n-1)|I_{t}|}+\\Big(1+\\displaystyle\\frac{\\beta_{t h}^{2}}{|I_{t}|}\\Big)L_{f,1}^{2}\\Big(\\mathbb{E}\\|y_{t}-y^{*}(x_{t})\\|^{2}+\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}\\|v_{i,t}-z_{i}^{*}(x_{t})\\|^{2}\\Big),}\\\\ &{\\mathbb{E}\\|h_{x}^{t}\\|^{2}\\leq\\mathbb{E}\\|\\widetilde{h}_{x}^{t}\\|^{2}+\\displaystyle\\frac{3(n-|I_{t}|)}{(n-1)|I_{t}|}(L_{f,0}^{2}+2\\lambda^{2}L_{g,0}^{2})+\\displaystyle\\frac{3}{|I_{t}|}(\\sigma_{f}^{2}+2\\lambda^{2}\\sigma_{g}^{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. By using the definition of $z_{i}^{*}(x_{t})$ , we can have that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\|h_{v,i}^{t}\\|^{2}\\leq2\\mathbb{E}\\|\\nabla_{z}g_{i}(x_{t},v_{i,t};\\xi_{i,t}^{v})-\\nabla_{z}g_{i}(x_{t},v_{i,t})\\|^{2}+2\\mathbb{E}\\|\\nabla_{z}g_{i}(x_{t},v_{i,t})-\\nabla_{z}g_{i}(x_{t},z_{i}^{*}(x_{t}))\\|^{2}}\\\\ &{\\qquad\\qquad\\leq2L_{g,1}^{2}\\mathbb{E}\\|v_{i,t}-z_{i}^{*}(x_{t})\\|^{2}+2\\sigma_{g}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Similarly, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\|h_{z,i}^{t}\\|^{2}\\leq2\\mathbb{E}\\|\\nabla_{z}\\mathcal{L}_{i}(x_{t},y_{t},z_{i,t},v_{i,t})-\\nabla_{z}\\mathcal{L}_{i}(x_{t},y_{t},z_{i,t},v_{i,t};\\xi_{i,t}^{z})\\|^{2}}\\\\ &{\\qquad\\qquad+2\\mathbb{E}\\|\\nabla_{z}\\mathcal{L}_{i}(x_{t},y_{t},z_{i,t},v_{i,t})-\\nabla_{z}\\mathcal{L}_{i}(x_{t},y^{*}(x_{t}),z_{\\lambda,i}^{*}(x_{t}),v_{i,t})\\|^{2}}\\\\ &{\\qquad\\qquad\\leq4(L_{f,1}^{2}+\\lambda^{2}L_{g,1}^{2})\\Big(\\mathbb{E}\\|z_{i,t}-z_{\\lambda,i}^{*}(x_{t})\\|^{2}+\\mathbb{E}\\|y_{t}-y^{*}(x_{t})\\|^{2}\\Big)+4(\\sigma_{f}^{2}+\\lambda^{2}\\sigma_{g}^{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Next, for the estimator of $x$ ,we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\|h_{x}^{t}\\|^{2}=\\!\\mathbb{E}\\bigg\\|\\frac{1}{|I_{t}|}\\sum_{i\\in I_{t}}\\nabla_{x}\\mathcal{L}_{i}(x_{t},y_{t},z_{i,t},v_{i,t};\\xi_{i,t}^{x})\\bigg\\|^{2}}\\\\ &{\\qquad\\overset{(a)}{=}\\mathbb{E}\\bigg\\|\\frac{1}{|I_{t}|}\\sum_{i\\in I_{t}}\\bigg[\\nabla_{x}\\mathcal{L}_{i}(x_{t},y_{t},z_{i,t},v_{i,t};\\xi_{i,t}^{x})-\\nabla_{x}\\mathcal{L}_{i}(x_{t},y_{t},z_{i,t},v_{i,t})\\bigg]\\bigg\\|^{2}}\\\\ &{\\qquad\\qquad+\\mathbb{E}\\bigg\\|\\frac{1}{|I_{t}|}\\sum_{i\\in I_{t}}\\!\\nabla_{x}\\mathcal{L}_{i}(x_{t},y_{t},z_{i,t},v_{i,t})\\bigg\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where (a) uses unbiased estimation in Assumption 4.5. For the first part of eq. (20), since tasks are selected without replacement, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\bigg\\|\\frac{1}{|I_{t}|}\\sum_{i\\in I_{t}}\\left[\\nabla_{x}\\mathcal{L}_{i}\\big(x_{t},y_{t},z_{i,t},v_{i,t};\\xi_{i,t}^{x}\\big)-\\nabla_{x}\\mathcal{L}_{i}\\big(x_{t},y_{t},z_{i,t},v_{i,t}\\big)\\right]\\bigg\\|^{2}}\\\\ &{\\quad\\quad\\overset{(a)}{=}\\displaystyle\\frac{1}{|I_{t}|^{2}}\\sum_{i\\in I_{t}}\\mathbb{E}\\bigg\\|\\nabla_{x}\\mathcal{L}_{i}\\big(x_{t},y_{t},z_{i,t},v_{i,t};\\xi_{i,t}^{x}\\big)-\\nabla_{x}\\mathcal{L}_{i}\\big(x_{t},y_{t},z_{i,t},v_{i,t}\\big)\\bigg\\|^{2}}\\\\ &{\\quad\\quad\\leq\\displaystyle\\frac{3}{|I_{t}|}\\big(\\sigma_{f}^{2}+2\\lambda^{2}\\sigma_{g}^{2}\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where (a) uses the unbiased estimation assumption in Assumption 4.5. For the second part of eq. (20), wehave ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{E}\\bigg\\|\\frac{1}{|I_{t}|}\\sum_{i\\in I_{t}}\\nabla_{x}\\mathcal{L}_{i}(x_{t},y_{t},z_{i,t},v_{i,t})\\bigg\\|^{2}}}\\\\ &{\\overset{(a)}{=}\\frac{n}{|I_{t}|(n-1)}\\mathbb{E}\\bigg\\|\\frac{1}{n}\\sum_{i=1}^{n}\\nabla_{x}\\mathcal{L}_{i}(x_{t},y_{t},z_{i,t},v_{i,t})\\bigg\\|^{2}+\\frac{n-|I_{t}|}{(n-1)|I_{t}|}\\cdot\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}\\bigg\\|\\nabla_{x}\\mathcal{L}_{i}(x_{t},y_{t},z_{i,t},v_{i,t})\\bigg\\|}\\\\ &{\\overset{(b)}{\\leq}\\mathbb{E}\\|\\widetilde{h}_{x}^{t}\\|^{2}+\\frac{3(n-|I_{t}|)}{(n-1)|I_{t}|}(L_{f,0}^{2}+2\\lambda^{2}L_{g,0}^{2})}&{\\quad{(22)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where (a) used the Lemma A.1 in [32]; (b) uses Assumption 4.4. By combining eq. (22) with eq. (21), the fourth inequality is proved. Last, for the estimator of $y$ wehave ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}\\|h_{y}^{t}\\|^{2}=\\!\\mathbb{E}\\bigg\\|\\frac{1}{|I_{t}|}\\sum_{i\\in I_{t}}\\nabla_{y}f_{i}\\big(x_{t},y_{t},v_{i,t};\\xi_{i,t}^{y}\\big)\\bigg\\|^{2}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad=\\operatorname{lim}_{{\\mathbf{x}}}\\left\\{\\|A_{1,\\cdot}\\|_{{\\mathbf{G}}^{\\infty}}\\leq\\operatorname{lims}_{\\theta\\leq1}w_{\\theta\\leq1},~~~~x_{\\theta\\leq1}=\\operatorname{lim}_{{\\mathbf{x}}}\\left\\{\\operatorname{\\|}A_{2,\\cdot}\\|_{{\\mathbf{G}}^{\\infty}}<\\operatorname{lims}_{\\theta\\leq1}w_{\\theta\\leq1}\\right\\}}\\\\ &{\\frac{3}{2}\\operatorname{lims}_{\\theta\\leq1}\\operatorname{lims}_{\\theta\\leq1}\\operatorname{lims}_{\\theta\\leq1}(\\zeta_{\\theta}(x_{1},y_{\\theta+1}),\\ldots,\\zeta_{\\theta}(x_{1},y_{\\theta+1}))^{2}}\\\\ &{\\quad\\quad+\\frac{l(\\zeta_{1}-1)}{l(\\zeta_{1}-1)}\\mathbb{E}_{\\theta\\leq1}^{1}\\sum_{w_{\\theta}\\in\\mathcal{K}_{\\theta}(\\cdot),\\ldots,\\theta\\leq1}^{\\infty}\\left\\{\\frac{a}{1}+\\frac{l(\\zeta_{1}-1)}{l(\\zeta_{1}-1)}\\prod_{s=1}^{\\theta}\\mathbb{E}\\left\\{\\mathbf{v}_{\\theta}(\\cdot,y_{1},y_{\\theta+1})\\right\\}\\right\\}}\\\\ &{\\frac{3}{2}\\frac{\\mathcal{G}_{\\theta}^{2}}{\\prod_{s}}\\frac{\\mathcal{G}_{\\theta}^{2}}{\\mathcal{N}}+\\frac{(l(\\zeta_{1}-1))\\theta}{l(\\zeta_{1}-1)}\\frac{3}{l(\\zeta_{1}-1)}\\frac{3}{l(\\zeta_{1}-1)}\\frac{3}{l(\\zeta_{1}-1)}\\frac{3}{l(\\zeta_{1}-1)}\\frac{3}{l(\\zeta_{1}-1)}\\frac{3}{l(\\zeta_{1}-1)}\\frac{3}{l(\\zeta_{1}-1)}\\bigg\\}}\\\\ &{\\frac{\\mathcal{G}_{\\theta}^{2}}{\\mathcal{N}}\\frac{\\mathcal{G}_{\\theta}^{2}}{\\mathcal{N}}+\\frac{(l(\\zeta_{1}-1))\\theta}{l(\\zeta_{1}-1)}\\frac{3}{l(\\zeta_{1}-1)}\\frac{3}{l(\\zeta_{1}-1)}\\frac{3}{l(\\zeta_{1}-1)}\\frac{3}{l(\\zeta_{1}-1 \n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where (a) uses Assumption 4.5; (b) follows from the Lemma A.1 in [32]; (c) uses Assumption 4.5, 4.6:(d folowsfomdefinition $\\begin{array}{r}{y^{\\ast}(x)=\\arg\\operatorname*{max}_{y}\\frac{1}{n}\\sum_{i}^{n}f_{i}\\big(x,y,z_{i}^{\\ast}\\bar{(x)}\\big)}\\end{array}$ eusesAssumpion4.4 and (f) uses $\\beta_{t h}\\geq1$ . Then, the proof is complete. ", "page_idx": 26}, {"type": "text", "text": "E.3Descent in Approximation Errors ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Lemma E.3. Under Assumptions 4.3, 4.4, 4.5, 4.6, there exists $\\delta_{v,1},\\delta_{z,1},\\delta_{y,1}$ suchthat theiterates of $v_{i,t},\\,z_{i,t}$ and $y_{t}$ inAlgorithm $^{\\,I}$ satisfy ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{n(n-1)}\\displaystyle\\sum_{i=1}^{n}\\big[\\mathrm{Eq}_{i}[\\mathrm{e}_{i+1}-\\mathrm{e}_{i}(\\boldsymbol{\\pi}_{i})]^{1}-\\mathbb{E}\\big[\\mathrm{Eq}_{i}-\\boldsymbol{z}_{i}^{*}(\\boldsymbol{\\pi}_{i})\\big]^{1}\\big]}\\\\ &{\\qquad\\le(-\\eta_{n-1}\\delta_{i})\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}\\frac{\\big[\\mathrm{Eq}_{i}[\\mathrm{e}_{i}-\\mathrm{e}_{i}(\\boldsymbol{\\pi}_{i})]^{1}\\big]^{2}}{n\\sqrt{\\pi}}\\big[\\mathrm{Eq}_{i}-\\mathrm{e}_{i}^{-2\\alpha}[\\alpha_{1}]\\big]^{1}}\\\\ &{\\qquad+\\frac{4}{n(n-1)^{2}}\\mathbb{E}\\big[\\mathrm{Eq}_{i}[\\mathrm{e}_{i}^{-1}+\\mathrm{e}_{i}^{-\\alpha}[\\alpha_{1}]^{2}+\\frac{L_{2}}{n}]\\big]\\mathbb{E}\\big[\\mathrm{Eq}_{i}[\\mathrm{e}_{i}^{-1}}\\\\ &{\\qquad\\widehat{\\mathrm{Eq}}_{i}[\\mathrm{Eq}_{i+1}-\\mathrm{e}_{i}(\\boldsymbol{\\pi}_{i})]^{2}-\\mathbb{E}\\big[\\mathrm{Eq}_{i}-\\mathrm{e}_{i}(\\boldsymbol{\\pi}_{i})]^{1}\\big]}\\\\ &{\\qquad=\\displaystyle\\sum_{i=1}^{n}\\big[\\mathrm{Eq}_{i}[\\mathrm{Eq}_{i+1}-\\mathrm{e}_{i}(\\boldsymbol{\\pi}_{i})]^{1}-\\mathbb{E}\\big[\\mathrm{Eq}_{i}[\\mathrm{e}_{i}-\\mathrm{e}_{i}(\\boldsymbol{\\pi}_{i})]^{2}\\big]^{1}\\big]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}\\big[\\mathrm{Eq}_{i}[\\mathrm{e}_{i}-\\mathrm{e}_{i}(\\boldsymbol{\\pi}_{i})]^{1}-\\mathbb{E}\\big[\\mathrm{Eq}_{i}[\\mathrm{e}_{i}]^{2}[\\mathrm{Eq}_{i+1}-\\mathrm{e}_{i}(\\boldsymbol{\\pi}_{i})]^{2}\\big]\\mathbb{E}[\\mathrm{Eq}_{i}[\\mathrm{e}_{i}^{-1}+\\mathrm{e} \n$$", "text_format": "latex", "page_idx": 26}, {"type": "equation", "text": "$$\n+\\,\\eta_{x}^{2}\\bigg(\\frac{L_{*,y}}{2}+\\Big(1+\\frac{L_{g,1}}{\\mu_{g}}\\Big)^{2}\\frac{L_{f,1}^{2}}{\\mu_{f}^{2}}\\bigg)\\mathbb{E}\\big\\|h_{x}^{t}\\big\\|^{2},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f o r\\;a l l\\;t\\in\\{0,...,T-1\\},\\;w h e r e\\;w e\\;d e f i n e\\;\\delta_{v}:=\\delta_{v,1}+\\frac{3\\eta_{x}^{2}L_{*,z}}{2}(L_{f,0}^{2}+2\\lambda^{2}L_{g,0}^{2}),\\;\\delta_{z}:=\\delta_{v,1}+}\\\\ &{\\frac{\\eta_{x}^{2}L_{*,z}+2\\lambda^{2}L_{g,0}^{2}),\\;\\delta_{y}:=\\delta_{y,1}+\\frac{3\\eta_{x}^{2}L_{*,y}}{2}(L_{f,0}^{2}+2\\lambda^{2}L_{g,0}^{2})\\,a n d w e\\;a s s u m e\\;\\lambda\\geq\\left\\{2L_{f,1}/\\mu_{g,}\\left(1+\\frac{L_{g,1}}{\\mu_{g,1}}\\right)L_{g,1}\\right\\},}\\\\ &{\\frac{L_{g,1}}{\\mu_{g}}\\frac{L_{f,1}}{3\\mu_{f}L_{g,1}},\\;(1+\\frac{L_{g,1}}{\\mu_{g}})\\frac{L_{f,1}L_{f,2}}{3\\mu_{f}L_{g,1}},\\frac{L_{f,1}L_{*,y}}{6L_{g,1}}\\big(1+\\big(1+\\frac{L_{g,1}}{\\mu_{g}}\\big)\\frac{L_{f,1}}{\\mu_{f}}+\\frac{12L_{g,1}}{\\mu_{g}}\\big)^{-1},\\big((1+\\frac{L_{g,1}}{\\mu_{g}})\\frac{L_{f,1}}{\\mu_{f}}+1\\big)\\frac{L_{f,1}}{L_{g,1}}\\big\\},}\\\\ &{v\\leq\\frac{\\mu_{g}}{2L_{g,1}},\\;\\eta_{z}\\leq\\frac{\\mu_{g}}{32L_{g,1}^{2}\\lambda}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. For the iterations of the lower-level problem, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\|v_{i,t+1}-z_{i}^{*}(x_{t+1})\\|^{2}=\\!\\!\\mathbb{E}\\|v_{i,t+1}-z_{i}^{*}(x_{t})\\|^{2}+\\mathbb{E}\\|z_{i}^{*}(x_{t})-z_{i}^{*}(x_{t+1})\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\quad+\\ 2\\mathbb{E}\\bigl\\langle v_{i,t+1}-z_{i}^{*}(x_{t}),z_{i}^{*}(x_{t})-z_{i}^{*}(x_{t+1})\\bigr\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "For the first term of eq. (23), we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\|v_{i,t+1}-z_{i}^{*}(x_{t})\\|^{2}}\\\\ &{=\\mathbb{E}\\|v_{i,t}-z_{i}^{*}(x_{t})-\\eta_{v}h_{v,i}^{t}\\|^{2}}\\\\ &{=\\mathbb{E}\\|v_{i,t}-z_{i}^{*}(x_{t})\\|^{2}+\\eta_{v}^{2}\\mathbb{E}\\|h_{v,i}^{t}\\|^{2}-2\\eta_{v}\\mathbb{E}\\langle v_{i,t}-z_{i}^{*}(x_{t}),h_{v,i}^{t}\\rangle}\\\\ &{=\\mathbb{E}\\|v_{i,t}-z_{i}^{*}(x_{t})\\|^{2}+\\eta_{v}^{2}\\mathbb{E}\\|h_{v,i}^{t}\\|^{2}-2\\eta_{v}\\mathbb{E}\\langle v_{i,t}-z_{i}^{*}(x_{t}),\\nabla_{z}g_{i}(x_{t},v_{i,t})-\\nabla_{z}g_{i}(x_{t},z_{i}^{*}(x_{t}))\\rangle}\\\\ &{\\overset{(a)}{\\leq}(1-2\\eta_{v}\\mu_{g})\\mathbb{E}\\|v_{i,t}-z_{i}^{*}(x_{t})\\|^{2}+\\eta_{v}^{2}\\mathbb{E}\\|h_{v,i}^{t}\\|^{2}}\\\\ &{\\overset{(b)}{\\leq}(1-2\\eta_{v}\\mu_{g}+2\\eta_{v}^{2}L_{g,1}^{2})\\mathbb{E}\\|v_{i,t}-z_{i}^{*}(x_{t})\\|^{2}+2\\eta_{v}^{2}\\sigma_{g}^{2}}\\\\ &{\\overset{(c)}{\\leq}(1-\\eta_{v}\\mu_{g})\\mathbb{E}\\|v_{i,t}-z_{i}^{*}(x_{t})\\|^{2}+2\\eta_{v}^{2}\\sigma_{g}^{2},}&{\\scriptscriptstyle{(2)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where (a) follows from Assumption 4.3; (b) uses Lemma E.2; (c) results from $\\begin{array}{r}{\\eta_{v}\\leq\\frac{\\mu_{g}}{2L_{g,1}^{2}}}\\end{array}$ .For the second term of eq. (23), we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}\\|z_{i}^{*}(x_{t})-z_{i}^{*}(x_{t+1})\\|^{2}\\leq\\frac{L_{g,1}^{2}}{\\mu_{g}^{2}}\\mathbb{E}\\|x_{t}-x_{t+1}\\|^{2}=\\frac{\\eta_{x}^{2}L_{g,1}^{2}}{\\mu_{g}^{2}}\\mathbb{E}\\|h_{x}^{t}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "For the last term of eq. (23), we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\big[\\langle v_{i,t+1}-\\hat{z}_{i}^{*}(\\tau),z_{i}^{*}(\\tau),\\tau_{i}^{*}(\\tau_{+1})\\rangle}\\\\ &{=-2\\mathbb{E}\\big[\\langle v_{i,t+1}-\\hat{z}_{i}^{*}(\\tau_{+}),\\nabla z_{i}^{*}(\\tau_{+})\\rangle\\big(\\tau_{+}-z_{i}\\big)\\big)}\\\\ &{\\quad-2\\mathbb{E}\\big(\\langle v_{i,t+1}-z_{i}^{*}(\\tau_{+}),z_{i}^{*}(\\tau_{+})\\rangle-z_{i}^{*}(\\tau_{+})-\\nabla_{z}^{*}(\\tau_{+})(\\alpha_{1}-\\kappa_{1})\\big)}\\\\ &{=2\\mathbb{E}\\big(\\langle v_{i,t+1}-\\hat{z}_{i}^{*}(\\tau),\\nabla z_{i}^{*}(\\tau_{+})\\rangle\\big\\|\\tilde{\\rho}_{i,t}\\big)}\\\\ &{\\quad-2\\mathbb{E}\\big(\\langle v_{i,t+1}-z_{i}^{*}(\\tau_{+}),z_{i}^{*}(\\tau_{+})\\big\\|\\nabla_{z}\\tilde{\\rho}_{i,t}\\big)}\\\\ &{\\leq2\\mathbb{E}\\big[\\|v_{i,t}-z_{i}^{*}(\\tau_{+})\\|^{2}\\,\\underbrace{\\mathbb{E}\\big[\\nabla z_{i}^{*}(\\tau_{+})\\|\\tilde{\\rho}_{i,t}^{*}\\big]}_{\\leq0}-\\hat{\\tau}_{i}^{*}(\\tau_{+})-\\nabla_{z}^{*}(\\tau_{+})(\\alpha_{1}-\\kappa_{1})\\big)}\\\\ &{\\leq2\\mathbb{E}\\big[\\|v_{i,t+1}-z_{i}^{*}(\\tau_{+})\\|\\cdot\\mathbb{E}\\big[\\nabla z_{i}^{*}(\\tau_{+})\\|\\tilde{\\rho}_{i,t}^{*}\\big]}\\\\ &{\\quad+2\\mathbb{E}\\big[\\|v_{i,t+1}-z_{i}^{*}(\\tau)\\|\\cdot\\mathbb{E}\\big[\\nabla z_{i}^{*}(\\tau_{+})\\|\\tilde{\\rho}_{i,t}^{*}\\big]+\\mathbb{E}\\big[\\|v_{i,t+1}-z_{i}^{*}(\\tau_{+})\\|\\cdot\\tilde{L}_{n,t}\\|\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "$\\begin{array}{r}{\\delta_{v}:=\\delta_{v,1}+\\frac{3\\eta_{x}^{2}L_{*,z}}{2}(L_{f,0}^{2}+2\\lambda^{2}L_{g,0}^{2})}\\end{array}$ By plugging eq. (24), eq. (25), eq. (26) into eq. (23), we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}\\big\\|v_{i,t+1}-z_{i}^{*}(x_{t+1})\\big\\|^{2}-\\mathbb{E}\\big\\|v_{i,t}-z_{i}^{*}(x_{t})\\big\\|^{2}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "equation", "text": "$$\n\\boldsymbol{\\leq}(-\\eta_{v}\\mu_{g}+\\delta_{v})\\mathbb{E}\\big\\|\\boldsymbol{v}_{i,t}-\\boldsymbol{z}_{i}^{*}(\\boldsymbol{x}_{t})\\big\\|^{2}+2\\eta_{v}^{2}(1+\\delta_{v})\\sigma_{g}^{2}+\\frac{\\eta_{x}^{2}L_{g,1}^{2}}{\\delta_{v,1}\\mu_{g}^{2}}\\mathbb{E}\\|\\widetilde{h}_{x}^{t}\\|^{2}+\\eta_{x}^{2}\\bigg(\\frac{L_{g,1}^{2}}{\\mu_{g}^{2}}+\\frac{L_{*,z}}{2}\\bigg)\\mathbb{E}\\|h_{x}^{t}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Then we get ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\frac{1}{n}\\sum_{i=1}^{n}\\Big[\\mathbb{E}\\big\\|v_{i,t+1}-z_{i}^{*}(x_{t+1})\\big\\|^{2}-\\mathbb{E}\\big\\|v_{i,t}-z_{i}^{*}(x_{t})\\big\\|^{2}\\Big]}}\\\\ &{\\quad\\quad\\le\\big(-\\eta_{v}\\mu_{g}+\\delta_{v}\\big)\\cdot\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}\\big\\|v_{i,t}-z_{i}^{*}(x_{t})\\big\\|^{2}+2\\eta_{v}^{2}(1+\\delta_{v})\\sigma_{g}^{2}}\\\\ &{\\quad\\quad\\quad+\\frac{\\eta_{x}^{2}L_{g,1}^{2}}{\\delta_{v,1}\\mu_{g}^{2}}\\mathbb{E}\\|\\widetilde h_{x}^{t}\\|^{2}+\\eta_{x}^{2}\\bigg(\\frac{L_{g,1}^{2}}{\\mu_{g}^{2}}+\\frac{L_{*,z}}{2}\\bigg)\\mathbb{E}\\|h_{x}^{t}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Thus, the first inequality in the lemma is proved. Similarly, we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\|z_{i,t+1}-z_{\\lambda,i}^{*}(x_{t+1})\\|^{2}=\\mathbb{E}\\|z_{i,t+1}-z_{\\lambda,i}^{*}(x_{t})\\|^{2}+\\mathbb{E}\\|z_{\\lambda,i}^{*}(x_{t})-z_{\\lambda,i}^{*}(x_{t+1})\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\ 2\\mathbb{E}\\bigl\\langle z_{i,t+1}-z_{\\lambda,i}^{*}(x_{t}),z_{\\lambda,i}^{*}(x_{t})-z_{\\lambda,i}^{*}(x_{t+1})\\bigr\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We can bound the first term in eq. (27) similarly with eq. (24) as ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\{\\left|x_{k+1}-x_{k}^{*}(x_{k})\\right|\\}^{2}}\\\\ &{\\leq\\mathbb{E}\\{\\left|x_{k+1}-x_{k}^{*}(x_{k})\\right|\\}^{2}+\\eta_{k}^{2}\\mathbb{E}\\{\\left|x_{k}^{*}(x_{k})\\right|^{2}-2\\eta_{k}^{*}\\mathbb{E}\\{x_{k}^{*}(x_{k}),x_{k}^{*}(x_{k})\\}}\\\\ &{\\leq\\mathbb{E}\\{\\left|x_{k}^{*}(x_{k})\\right|\\}^{2}+\\eta_{k}^{2}\\mathbb{E}\\{\\left|x_{k}^{*}(x_{k})\\right|^{2}\\}}\\\\ &{\\quad-2\\eta_{k}\\mathbb{E}\\{\\left|x_{k}^{*}(x_{k})\\right|\\}^{2}+\\eta_{k}^{2}\\mathbb{E}\\{\\left|x_{k}^{*}(x_{k})\\right|^{2}\\}}\\\\ &{\\qquad-2\\eta_{k}\\mathbb{E}\\{\\left|x_{k}^{*}(x_{k})\\right|\\}\\sqrt{\\eta_{k}(x_{k})}\\sqrt{\\eta_{k}(x_{k})}\\sqrt{\\eta_{k}(x_{k})}\\sqrt{\\eta_{k}(x_{k})}\\sqrt{\\eta_{k}(x_{k})}\\sqrt{\\eta_{k}(x_{k})}}\\\\ &{\\qquad-2\\eta_{k}\\mathbb{E}\\{\\left|x_{k}^{*}(x_{k})\\right|\\}\\sqrt{\\eta_{k}(x_{k})}\\sqrt{\\eta_{k}(x_{k})}\\sqrt{\\eta_{k}(x_{k})}\\sqrt{\\eta_{k}(x_{k})}\\sqrt{\\eta_{k}(x_{k})}\\sqrt{\\eta_{k}(x_{k})}\\sqrt{\\eta_{k}(x_{k})}}\\\\ &{=\\mathbb{E}\\{\\left|x_{k}^{*}(x_{k})\\right|\\}^{2}+\\eta_{k}^{2}\\mathbb{E}\\{\\left|x_{k}^{*}(x_{k})\\right|\\}^{2}+\\eta_{k}^{2}\\mathbb{E}\\{\\left|x_{k}^{*}(x_{k})\\right|\\}\\sqrt{\\eta_{k}(x_{k})}\\sqrt{\\eta_{k}(x_{k})}\\sqrt{\\eta_{k}(x_{k})}}\\\\ &{\\qquad-2\\eta_{k}\\mathbb{E}\\{x_{k}^{*}(x_\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where (a) uses Lemma E.2 and $\\begin{array}{r}{\\lambda\\ge\\operatorname*{max}\\left\\{\\frac{L_{f,1}}{L_{g,1}},\\frac{\\sigma_{f}}{\\sigma_{g}}\\right\\}}\\end{array}$ (L,};(b) uses nz\u5165 \u2264 $\\begin{array}{r}{\\eta_{z}\\lambda\\leq\\frac{\\mu_{g}}{32L_{g,1}^{2}}}\\end{array}$ ; we bound the second term in eq. (27) as ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{E}\\|z_{\\lambda,i}^{*}(x_{t})-z_{\\lambda,i}^{*}(x_{t+1})\\|^{2}\\leq\\frac{144L_{g,1}^{2}}{\\mu_{g}^{2}}\\mathbb{E}\\|x_{t}-x_{t+1}\\|^{2}=\\frac{144\\eta_{x}^{2}L_{g,1}^{2}}{\\mu_{g}^{2}}\\mathbb{E}\\|h_{x}^{t}\\|^{2};\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "and we bound the last term in eq. (27) similarly with eq. (26) as ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2\\mathbb{E}\\big\\langle z_{i,t+1}-z_{\\lambda,i}^{*}(x_{t}),z_{\\lambda,i}^{*}(x_{t})-z_{\\lambda,i}^{*}(x_{t+1})\\big\\rangle}\\\\ &{\\quad\\le\\!\\delta_{z}\\mathbb{E}\\big\\|z_{i,t+1}-z_{\\lambda,i}^{*}(x_{t})\\big\\|^{2}+\\displaystyle\\frac{144\\eta_{x}^{2}L_{g,1}^{2}}{\\delta_{z,1}\\mu_{g}^{2}}\\mathbb{E}\\big\\|\\widetilde{h}_{x}^{t}\\big\\|^{2}+\\frac{\\eta_{x}^{2}L_{*,z_{\\lambda}}}{2}\\mathbb{E}\\|h_{x}^{t}\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\begin{array}{r}{\\delta_{z}:=\\delta_{z,1}+\\frac{3\\eta_{x}^{2}L_{*,z_{\\lambda}}}{2}(L_{f,0}^{2}+2\\lambda^{2}L_{g,0}^{2})}\\end{array}$ we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{E}\\big\\|z_{i,t+1}-z_{\\lambda,i}^{*}(x_{t+1})\\big\\|^{2}-\\mathbb{E}\\big\\|z_{i,t}-z_{\\lambda,i}^{*}(x_{t})\\big\\|^{2}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\le\\Big(-\\frac{\\eta_{z}\\lambda\\mu_{g}}{4}+\\delta_{z}\\Big)\\mathbb{E}\\|z_{i,t}-z_{\\lambda,i}^{*}(x_{t})\\|^{2}+(1+\\delta_{z})\\Big(\\frac{2\\eta_{z}L_{f,1}^{2}}{\\lambda\\mu_{g}}+8\\eta_{z}^{2}\\lambda^{2}L_{g,1}^{2}\\Big)\\mathbb{E}\\|y_{t}-y^{*}(x_{t})\\|^{2}}}\\\\ {{\\quad+\\frac{144\\eta_{x}^{2}L_{g,1}^{2}}{\\delta_{z,1}\\mu_{g}^{2}}\\mathbb{E}\\|\\tilde{h}_{x}^{t}\\|^{2}+\\Big(\\frac{144\\eta_{x}^{2}L_{g,1}^{2}}{\\mu_{g}^{2}}+\\frac{\\eta_{x}^{2}L_{*,z_{\\lambda}}}{2}\\Big)\\mathbb{E}\\|h_{x}^{t}\\|^{2}+8(1+\\delta_{z})\\eta_{z}^{2}\\lambda^{2}\\sigma_{g}^{2},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "After telescoping, we obtain ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\frac{1}{n}\\sum_{i=1}^{n}\\Bigl[\\mathbb{E}\\bigl\\|z_{i,t+1}-z_{\\lambda,i}^{*}(\\boldsymbol{x}_{t+1})\\bigr\\|^{2}-\\mathbb{E}\\bigl\\|z_{i,t}-z_{\\lambda,i}^{*}(\\boldsymbol{x}_{t})\\bigr\\|^{2}\\Bigr]}}\\\\ &{\\leq\\Bigl(-\\,\\frac{\\eta_{z}\\lambda\\mu_{g}}{4}+\\delta_{z}\\Bigr)\\,\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}\\bigl\\|z_{i,t}-z_{\\lambda,i}^{*}(\\boldsymbol{x}_{t})\\bigr\\|^{2}+(1+\\delta_{z})\\Bigl(\\frac{2\\eta_{z}L_{f,1}^{2}}{\\lambda\\mu_{g}}+8\\eta_{z}^{2}\\lambda^{2}L_{g,1}^{2}\\Bigr)\\mathbb{E}\\bigl\\|y_{t}-y^{*}(\\boldsymbol{x}_{t})\\bigr\\|^{2}}\\\\ &{\\qquad+\\,\\frac{144\\eta_{x}^{2}L_{g,1}^{2}}{\\delta_{z,1}\\mu_{g}^{2}}\\mathbb{E}\\bigl\\|\\tilde{h}_{x}^{t}\\bigr\\|^{2}+\\Bigl(\\frac{144\\eta_{x}^{2}L_{g,1}^{2}}{\\mu_{g}^{2}}+\\frac{\\eta_{x}^{2}L_{*,z_{\\lambda}}}{2}\\Bigr)\\mathbb{E}\\|h_{x}^{t}\\|^{2}+8(1+\\delta_{z})\\eta_{z}^{2}\\lambda^{2}\\sigma_{g}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Then the second inequality in the lemma is proved. Last, for $y_{t}$ and $\\boldsymbol{y}^{*}(\\boldsymbol{x}_{t})$ ,wehave ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\|y_{t+1}-y^{*}(x_{t+1})\\|^{2}=\\mathbb{E}\\|y_{t+1}-y^{*}(x_{t})\\|^{2}+\\mathbb{E}\\|y^{*}(x_{t})-y^{*}(x_{t+1})\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\ 2\\mathbb{E}\\bigl\\langle y_{t+1}-y^{*}(x_{t}),y^{*}(x_{t})-y^{*}(x_{t+1})\\bigr\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "We can bound the first term in eq. (31) as ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\{Q_{1}(\\lambda_{R},\\cdot\\log(\\lambda_{R}))\\}}\\\\ &{=\\mathbb{E}\\{Q_{2}(\\lambda_{R},\\cdot\\log(r))\\}^{(1)}}\\\\ &{\\quad-\\mathbb{E}\\{Q_{3}(\\lambda_{R},\\cdot\\log(r))\\}^{(2)}+\\frac{r}{r}\\mathbb{E}\\{Q_{3}(\\lambda_{R},\\cdot)^{2}+2\\lambda_{R}\\}\\{Q_{3}(\\lambda_{R},\\cdot)^{2}}\\\\ &{\\quad+2\\lambda_{R}\\}\\\\ &{\\mathbb{E}\\{Q_{2}(\\lambda_{R},\\cdot)^{2}+2\\lambda_{R}\\}+\\mathbb{E}\\{Q_{2}(\\lambda_{R},\\cdot)^{2}+2\\lambda_{R}\\}}\\\\ &{\\quad+2\\lambda_{R}\\}\\\\ &{\\quad+2\\lambda_{R}\\}\\Bigg[\\lambda_{R}-\\mathbb{E}\\{Q_{3}(\\lambda_{R},\\cdot)\\}\\sum_{l=1}^{r}\\mathbb{E}\\{Q_{2}(\\lambda_{R},\\cdot)\\}-\\mathbb{E}\\{Q_{2}(\\lambda_{R},\\cdot)\\}^{(1)}\\Bigg]}\\\\ &{\\quad+2\\lambda_{R}\\mathbb{E}\\Bigg(\\lambda_{R}-\\mathbb{E}\\{Q_{3}(\\lambda_{R},\\cdot)\\}\\Bigg)}\\\\ &{\\stackrel{(a)}{\\leq}\\mathbb{E}\\{Q_{3}(\\lambda_{R},\\cdot)^{2}+4\\lambda_{R}\\}\\Bigg[\\lambda_{R}^{2}\\mathbb{E}\\{Q_{2}(\\lambda_{R},\\cdot)\\}+\\mathbb{E}\\{Q_{3}(\\lambda_{R},\\cdot)\\}-\\mathbb{E}\\{Q_{2}(\\lambda_{R},\\cdot)\\}}\\\\ &{\\quad+\\mathbb{E}\\{Q_{2}(\\lambda_{R},\\cdot)\\}\\Bigg]}\\\\ &{\\quad+\\mathbb{E}\\{Q_{3}(\\lambda_{R},\\cdot)^{2}-5\\lambda_{R}\\}+\\mathbb{E}\\{Q_{3}(\\lambda_{R},\\cdot)\\}+\\frac{r}{r}\\mathbb{E}\\{Q_{2}(\\lambda_{R},\\cdot)\\}-\\mathbb{E}\\{Q_{3}(\\lambda_{R},\\cdot)\\}}\\\\ &{\\quad-2\\lambda_{R}\\}{Q_{3}(\\lambda_{R},\\cdot)^{2}+4\\lambda_{R}\\}}\\\\ &{\\geq\\mathbb{E}\\{Q_{2}(\\lambda_{R},\\cdot)\\}\\mathbb{E}\\{Q_{3}(\\lambda_{R}, \n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where (a) uses the definition of $\\boldsymbol{y}^{*}(\\boldsymbol{x}_{t})$ and eq. (5); (b) uses strong concavity of $f_{i}$ in $y$ (c) follows from definition of $y^{\\ast}(x_{t})$ and Assumption 4.4; (d) uses Lemma E.2. We can bound the second term in eq. (31) as ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{E}\\|y^{*}(x_{t})-y^{*}(x_{t+1})\\|^{2}\\overset{(a)}{\\leq}\\Big(1+\\frac{L_{g,1}}{\\mu_{g}}\\Big)^{2}\\frac{L_{f,1}^{2}}{\\mu_{f}^{2}}\\mathbb{E}\\|x_{t}-x_{t+1}\\|^{2}=\\eta_{x}^{2}\\Big(1+\\frac{L_{g,1}}{\\mu_{g}}\\Big)^{2}\\frac{L_{f,1}^{2}}{\\mu_{f}^{2}}\\mathbb{E}\\|h_{x}^{t}\\|^{2},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where (a) follows from Lemma D.2. Also, we can get the bound of the last term as ", "page_idx": 29}, {"type": "equation", "text": "$$\n2\\mathbb{E}\\big\\langle y_{t+1}-y^{*}(x_{t}),y^{*}(x_{t})-y^{*}(x_{t+1})\\big\\rangle\n$$", "text_format": "latex", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=-2\\mathcal{E}\\big(\\eta_{+1}-\\gamma^{*}(x_{1}),\\nabla^{*}(x_{1}\\alpha)(x_{1}-x_{2})\\big)}\\\\ &{-2\\mathcal{E}\\big(\\eta_{+1}-\\gamma^{*}(x_{1}),\\nabla^{*}(x_{1}\\alpha)(\\cdot\\eta_{+1})-y_{1}^{*}(x_{1})(x_{1}+x_{2})\\big)}\\\\ &{\\ge2\\mathcal{E}\\big[\\eta_{+1}-\\gamma^{*}(x_{1}),\\nabla^{*}(x_{1}\\alpha)(\\cdot\\eta_{+1})\\big]-\\eta_{+1}^{*}\\int\\eta_{+1}^{*}\\int\\eta_{+1}^{*}\\big(x_{1}-x_{2}\\big)\\Big\\}}\\\\ &{+2\\mathcal{E}\\big(\\eta_{+1}-x_{2}\\big)\\Big[\\eta_{+1}\\Big]\\int\\eta_{+1}^{*}\\int\\eta_{+1}^{*}\\int\\eta_{+1}^{*}\\int\\eta_{+1}^{*}\\int\\eta_{+1}^{*}\\big(x_{1}+x_{2}\\big)\\Big\\}}\\\\ &{\\overset{(a,b_{0})}{\\le}\\Big\\delta\\Big\\{\\eta_{+1}\\Big\\}\\Big[\\eta_{+1}-y^{*}(x_{1})\\Big]+\\frac{eta_{+1}^{*}}{\\delta\\eta_{+1}^{*}}\\Big(1+\\frac{I_{0}}{I_{0}\\mu}\\Big)^{2}\\frac{I_{0}^{2}\\int_{x_{1}}^{x}y_{1}}{I_{0}^{2}}\\Big[\\eta_{+1}^{*}\\Big]^{2}}\\\\ &{+\\frac{2}{\\delta\\eta_{+1}}\\Big[\\eta_{+1}-\\gamma^{*}(x_{1})\\Big]\\int-\\xi_{0}\\Big]\\eta_{+1}^{*}\\int\\eta_{+1}^{*}\\int\\eta_{+1}^{*}\\int\\eta_{+1}^{*}\\int\\eta_{+1}^{*}\\Big[\\Big]}\\\\ &{\\overset{(c)}{\\le}\\frac{\\eta_{+1}}{\\delta\\eta_{+1}}\\Big[\\eta_{+1}-\\gamma^{*}(x_{1})\\Big]^{2}+\\frac{\\eta_{-1}^{*}}{\\delta\\eta_{+1}^{*}}\\Big(1 \n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where (a) uses Lemma D.2 and Lemma D.3; (b) use Lemma D.3 and Lemma 1 in [44]; (c) follows from Assumption 4.4; (d) defines $\\begin{array}{r}{\\delta_{y}\\,=\\,\\delta_{y,1}+\\frac{3\\eta_{x}^{2}L_{*,y}}{2}(L_{f,0}^{2}+2\\lambda^{2}L_{g,0}^{2})}\\end{array}$ eq. (33), eq. (34) into eq. (34), we get ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\|y_{t+1}-y^{*}(x_{t+1})\\|^{2}-\\mathbb{E}\\|y_{t}-y^{*}(x_{t})\\|^{2}}\\\\ &{\\qquad\\leq\\bigg[-\\eta_{y}\\mu_{f}+\\eta_{y}^{2}(1+\\delta_{y})\\bigg(1+\\frac{\\beta_{t}^{2}}{\\|I_{t}\\|}\\bigg)L_{f,1}^{2}+\\delta_{y}\\bigg]\\mathbb{E}\\|y_{t}-y^{*}(x_{t})\\|^{2}}\\\\ &{\\qquad\\quad+\\eta_{y}^{2}(1+\\delta_{y})\\bigg(\\frac{\\sigma_{f}^{2}}{\\|I_{t}\\|}+\\frac{(n-\\vert I_{t}\\vert)\\sigma_{t h}^{2}}{(n-1)\\vert I_{t}\\vert}\\bigg)+\\frac{\\eta_{x}^{2}}{\\delta_{y,1}}\\bigg(1+\\frac{L_{g,1}}{\\mu_{g}}\\bigg)^{2}\\frac{L_{f,1}^{2}}{\\mu_{f}^{2}}\\mathbb{E}\\|\\tilde{h}_{x}^{t}\\|^{2}}\\\\ &{\\qquad\\quad+\\bigg(\\frac{\\eta_{y}L_{f,1}^{2}}{\\mu_{f}}+\\eta_{y}^{2}\\bigg(1+\\frac{\\beta_{t h}^{2}}{\\vert I_{t}\\vert}\\bigg)L_{f,1}^{2}\\bigg)(1+\\delta_{y})\\cdot\\frac{1}{n}\\frac{\\sqrt{n}}{i-1}\\mathbb{E}\\|v_{i,t}-z_{i}^{*}(x_{t})\\|^{2}}\\\\ &{\\qquad\\quad+\\eta_{x}^{2}\\bigg(\\frac{L_{*,y}}{2}+\\Big(1+\\frac{L_{g,1}}{\\mu_{g}}\\Big)^{2}\\frac{L_{f,1}^{2}}{\\mu_{f}^{2}}\\bigg)\\mathbb{E}\\|h_{x}^{t}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Then the last inequality is proved. Thus, the proof is complete. ", "page_idx": 30}, {"type": "text", "text": "E.4Descent in the Lyapunov Function and Proof of Theorem 4.10 ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "We define the Lyapunov function as ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\Psi_{t}:=\\!\\!\\mathscr{L}^{*}(x_{t})+K_{y}\\mathbb{E}\\|y_{t}-y^{*}(x_{t})\\|^{2}+K_{z}\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}\\|z_{i,t}-z_{\\lambda,i}^{*}(x_{t})\\|^{2}}\\\\ {\\quad\\qquad+\\,K_{v}\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}\\|v_{i,t}-z_{i}^{*}(x_{t})\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where the coefficients are given by ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{K_{y}=\\displaystyle\\frac{\\eta_{x}}{\\eta_{y}}\\cdot\\frac{2}{\\mu_{f}}\\Big(\\frac{3L_{f,1}^{2}}{2}+\\frac{216L_{g,1}^{2}L_{f,1}^{2}}{\\mu_{g}^{2}}+\\frac{864L_{g,1}^{2}}{\\mu_{g}}\\Big),~~K_{z}=\\displaystyle\\frac{\\eta_{x}\\lambda^{2}}{\\eta_{z}\\lambda}\\cdot\\frac{54L_{g,1}^{2}}{\\mu_{g}},~~K_{v}=\\displaystyle\\frac{\\eta_{x}\\lambda^{2}}{\\eta_{v}}\\cdot\\frac{54L_{g,1}^{2}}{\\mu_{g}};}}\\\\ {{\\delta_{y,1}=\\displaystyle\\frac{\\eta_{y}\\mu_{f}}{8},~~~\\delta_{z,1}=\\displaystyle\\frac{\\eta_{z}\\lambda\\mu_{g}}{8},~~~\\delta_{v,1}=\\displaystyle\\frac{\\eta_{v}\\mu_{g}}{4}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "For convenience, we define the following constants: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{C_{1}:=\\operatorname*{max}\\bigg\\{\\frac{L_{*,1}}{2},\\frac{54L_{g,1}^{2}}{\\mu_{g}}\\bigg(\\frac{L_{g,1}^{2}}{\\mu_{g}^{2}}+\\frac{L_{*,z}}{2}\\bigg),\\frac{54L_{g,1}^{2}}{\\mu_{g}}\\bigg(\\frac{144L_{g,1}^{2}}{\\mu_{g}^{2}}+\\frac{L_{*,z_{\\lambda}}}{2}\\bigg),}\\\\ &{\\qquad\\qquad\\quad\\frac{2}{\\mu_{f}}\\bigg(\\frac{3L_{f,1}^{2}}{2}+\\frac{216L_{g,1}^{2}L_{f,1}^{2}}{\\mu_{g}^{2}}+\\frac{864L_{g,1}^{2}}{\\mu_{g}}\\bigg)\\bigg(\\frac{L_{*,y}}{2}+\\big(1+\\frac{L_{g,1}}{\\mu_{g}}\\big)^{2}\\frac{L_{f,1}^{2}}{\\mu_{f}}\\bigg)\\bigg\\},}\\\\ &{C_{2}:=\\operatorname*{max}\\bigg\\{\\frac{62208L_{g,1}^{4}}{\\mu_{g}^{4}},\\frac{16}{\\mu_{f}^{2}}\\bigg(\\frac{3L_{f,1}^{2}}{2}+\\frac{216L_{g,1}^{2}L_{f,1}^{2}}{\\mu_{g}^{2}}+\\frac{864L_{g,1}^{2}}{\\mu_{g}}\\bigg)\\big(1+\\frac{L_{g,1}}{\\mu_{g}}\\big)^{2}\\frac{L_{f,1}^{2}}{\\mu_{f}^{2}}\\bigg\\}}\\\\ &{C_{3}:=\\operatorname*{max}\\bigg\\{\\frac{864L_{g,1}^{2}\\sigma_{g}^{2}}{\\mu_{g}},\\frac{4}{\\mu_{f}}\\bigg(\\frac{3L_{f,1}^{2}}{2}+\\frac{216L_{g,1}^{2}L_{f,1}^{2}}{\\mu_{g}^{2}}+\\frac{864L_{g,1}^{2}}{\\mu_{g}}\\bigg)\\big(\\sigma_{f}^{2}+\\sigma_{t h}^{2}\\big)\\bigg\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We also constrain the conditions as below: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\lambda\\geq\\bigg\\{\\frac{2L_{f,1}}{\\mu_{g}},(1+\\frac{L_{g,1}}{\\mu_{g}})\\frac{L_{f,1}^{2}}{3\\mu_{f}L_{g,1}},(1+\\frac{L_{g,1}}{\\mu_{g}})\\frac{L_{f,1}L_{f,2}}{3\\mu_{f}L_{g,1}},\\frac{L_{f,1}L_{s,y}}{6L_{g,1}}(1+(1+\\frac{L_{g,1}}{\\mu_{g}})\\frac{L_{f,1}}{\\mu_{f}}+\\frac{12L_{g,1}}{\\mu_{g}})^{-1}\\bigg\\}}\\\\ &{\\qquad((1+\\frac{L_{g,1}}{\\mu_{g}})\\frac{L_{f,1}}{\\mu_{f}}+1)\\frac{L_{f,1}}{L_{g,1}}\\frac{L_{f,2}}{L_{g,0}},\\frac{L_{f,2}}{\\mu_{g}},\\frac{16L_{f,1}^{2}}{2\\mu_{f}^{2}L_{g,1}^{2}}\\bigg(\\frac{3L_{f,1}^{2}}{2}+\\frac{216L_{g,1}^{2}L_{f,1}^{2}}{\\mu_{g}^{2}}+\\frac{864L_{g,1}^{2}}{\\mu_{g}}\\bigg)\\bigg\\},}\\\\ &{\\eta_{z}\\leq\\frac{1}{16C_{1}},\\quad\\eta_{y}\\leq\\operatorname*{min}\\bigg\\{\\frac{\\mu_{f}}{8(1+\\beta_{h}^{2})L_{f,1}^{2}},\\frac{1}{(1+\\beta_{h}^{2})M_{f}}\\bigg\\},\\quad\\eta_{z}\\lambda\\leq\\operatorname*{min}\\bigg\\{\\frac{\\mu_{g}}{64L_{g,1}^{2}},\\frac{4}{\\mu_{g}}\\bigg\\},}\\\\ &{\\eta_{e}\\leq\\operatorname*{min}\\bigg\\{\\frac{\\mu_{g}}{8L_{g,1}^{2}},\\frac{2}{\\mu_{g}}\\bigg\\},\\quad\\eta_{z}\\lambda^{3}\\leq\\frac{\\mu_{g}}{L_{g,1}^{2}},\\quad\\frac{\\eta_{z}^{3}}{\\eta_{g}^{3}}\\leq\\frac{1}{12C_{2}},\\quad\\frac{\\eta_{z}^{2}}{\\eta_{g}^{\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Plugging Lemma E.1, Lemma E.3 into eq. (35) and using eq. (36), we have the descent in the Lyapunovfunction as ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\Psi_{t+1}-\\Psi_{t}\\leq-\\,\\frac{\\eta_{x}}{2}\\|\\mathcal{H}^{*}(x_{t})\\|^{2}+\\frac{\\eta_{x}^{2}\\lambda^{2}}{|I_{t}|}\\bigg(1+\\frac{\\eta_{x}}{\\eta_{y}}+\\frac{\\eta_{x}\\lambda^{2}}{(\\eta_{z}\\lambda)}+\\frac{\\eta_{x}\\lambda^{2}}{\\eta_{v}}\\bigg)C_{1}\\cdot9(L_{g,0}^{2}+\\sigma_{g}^{2})}\\\\ {+\\left(\\eta_{x}\\eta_{y}+\\eta_{x}(\\eta_{z}\\lambda)\\lambda^{2}+\\eta_{x}\\eta_{v}\\lambda^{2}\\right)\\!C_{3}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Proof. To simplify the problem, we assume $|I_{t}|=P$ for $t=0,...,T-1$ . By taking summation of eq. (39) from $t=0$ to $T-1$ ,weget ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\frac1T\\sum_{t=0}^{T-1}\\frac{\\eta_{x}}{2}\\mathbb{E}\\|\\mathcal{H}^{*}(x_{t})\\|^{2}\\leq\\frac{1}{T}(\\Psi_{0}-\\Psi_{T})+\\frac{\\eta_{x}^{2}\\lambda^{2}}{P}\\bigg(1+\\frac{\\eta_{x}}{\\eta_{y}}+\\frac{\\eta_{x}\\lambda^{2}}{(\\eta_{z}\\lambda)}+\\frac{\\eta_{x}\\lambda^{2}}{\\eta_{v}}\\bigg)C_{2}}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad+\\;\\big(\\eta_{x}\\eta_{y}+\\eta_{x}(\\eta_{z}\\lambda)\\lambda^{2}+\\eta_{x}\\eta_{v}\\lambda^{2}\\big)C_{3}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "By using Lemma D.5 and eq. (40), we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{T}\\sum_{t=0}^{T-1}\\mathbb{E}\\|\\nabla\\Phi(x_{t})\\|^{2}\\leq\\displaystyle\\frac{2}{T}\\sum_{t=0}^{T-1}\\mathbb{E}\\|\\nabla\\Phi(x_{t})-\\mathcal{H}^{*}(x_{t})\\|^{2}+\\displaystyle\\frac{2}{T}\\sum_{t=0}^{T-1}\\mathbb{E}\\|\\mathcal{H}^{*}(x_{t})\\|^{2}}\\\\ &{\\overset{(a)}{\\leq}\\displaystyle\\frac{2C_{g a p}}{\\lambda^{2}}+\\frac{4(\\Psi_{0}-\\Psi_{T})}{T\\eta_{x}}+\\frac{4\\eta_{x}\\lambda^{2}}{P}\\bigg(1+\\frac{\\eta_{x}}{\\eta_{y}}+\\frac{\\eta_{x}\\lambda^{2}}{(\\eta_{z}\\lambda)}+\\frac{\\eta_{x}\\lambda^{2}}{\\eta_{v}}\\bigg)C_{2}}\\\\ &{\\quad\\quad+4\\big(\\eta_{y}+(\\eta_{z}\\lambda)\\lambda^{2}+\\eta_{v}\\lambda^{2}\\big)C_{3}}\\\\ &{\\overset{(b)}{\\leq}\\mathcal{O}(T^{-\\frac{2}{\\7}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where (a) uses eq. (40); (b) takes $\\eta_{x}=\\mathcal{O}(T^{-\\frac{5}{7}})$ $\\begin{array}{r}{\\frac{5}{7}),\\eta_{y}=\\mathcal{O}(T^{-\\frac{2}{7}}),\\eta_{z}=\\mathcal{O}(T^{-\\frac{5}{7}}),\\eta_{v}=\\mathcal{O}(T^{-\\frac{4}{7}})}\\end{array}$ \uff0c $\\lambda={\\mathcal{O}}(T^{{\\frac{1}{7}}})$ , which satisfies eq. (38). Thus, Theorem 4.10 is proved. \u53e3 ", "page_idx": 31}, {"type": "text", "text": "E.5 Proof of Corollary 4.11 ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Proof. From eq. (41), to achieve $\\epsilon_{}$ -accurate stationary point of the objective function $\\Phi(x)$ in definition 4.2, we let $\\begin{array}{r}{\\frac{1}{T}\\sum_{t=0}^{T-1}\\mathbb{E}\\|\\nabla\\Phi(x_{t})\\|^{2}=\\mathcal{O}(T^{-\\frac{2}{7}})\\leq\\epsilon.}\\end{array}$ As a result, we can se that the epochs number we need is $T=\\mathcal{O}(\\epsilon^{-\\frac{7}{2}})$ . The total sample complexity is $P T=O(P\\epsilon^{-\\frac{7}{2}})$ . Then, we finish the proof. \u53e3 ", "page_idx": 32}, {"type": "text", "text": "F Proofs of Theorem 4.12 and Corollary 4.13 ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "F.1 Descent in Objective Function ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Lemma F.1. Under Assumptions 4.3, 4.4, 4.5 and Lemma D.6, for $L_{*,1}$ -smooth $\\mathcal{L}^{*}(x)$ theconsecutive iterates of Algorithm 2 satisfy: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\Xi\\big[\\mathcal{L}^{*}(x_{t+1})-\\mathcal{L}^{*}(x_{t})\\big]}\\\\ {\\displaystyle\\leq-\\,\\frac{\\eta_{x}}{2}\\mathbb{E}\\|\\mathcal{H}^{*}(x_{t})\\|^{2}-\\frac{\\eta_{x}}{2}\\mathbb{E}\\|\\tilde{h}_{x}^{t}\\|^{2}+\\frac{\\eta_{x}^{2}L_{*,1}}{2}\\mathbb{E}\\bigg\\|\\frac{1}{|I_{t}|}\\sum_{i\\in I_{t}}\\nabla_{x}\\mathcal{L}_{i}(x_{t},y_{t},z_{i,t}^{K},v_{i,t}^{K})\\bigg\\|^{2}}\\\\ {\\displaystyle\\quad+\\,\\frac{3\\eta_{x}L_{f,1}^{2}}{2}\\mathbb{E}\\big\\|y_{t}-y^{*}(x_{t})\\big\\|^{2}+\\frac{3\\eta_{x}L_{\\lambda,1}^{2}}{2}\\mathbb{E}\\bigg[\\frac{1}{n}\\sum_{i=1}^{n}\\big\\|z_{i,t}^{K}-z_{\\lambda,i}^{*}(x_{t})\\big\\|^{2}+\\frac{1}{n}\\sum_{i=1}^{n}\\big\\|v_{i,t}^{K}-z_{i}^{*}(x_{t})\\big\\|^{2}\\bigg]}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "for all $t\\in\\{0,1,...,T-1\\}$ , where we assume $\\begin{array}{r}{\\lambda\\ge\\frac{2L_{f,1}}{\\mu_{g}}}\\end{array}$ ", "page_idx": 32}, {"type": "text", "text": "Proof. Recall the definitions of $\\mathcal L^{*}(x)$ and $\\mathcal{H}^{*}(x)$ in eq. (6). By using the smoothness of $\\mathcal{L}^{*}\\left(x_{t}\\right)$ in Lemma D.6, we have that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\big[\\mathcal{L}^{*}(\\pi_{t+1})\\Big]\\leq\\mathbb{E}\\big[\\mathcal{L}^{*}(\\pi_{t})\\big]+\\mathbb{E}\\big[\\mathcal{H}^{*}(\\pi_{t}),\\pi_{t+1}-x_{t}\\big]+\\frac{L_{\\infty,1}}{2}\\mathbb{E}\\|\\pi_{t+1}-x_{t}\\|^{2}}\\\\ &{\\qquad+\\mathbb{E}\\big[\\mathcal{L}^{*}(\\pi_{t})\\big]-\\eta_{*}\\mathbb{E}\\big[\\mathcal{H}^{*}(\\pi_{t}),\\widetilde{M}_{\\pi}^{*}\\big]+\\frac{\\eta_{*}^{2}L_{\\infty,1}}{2}\\mathbb{E}\\bigg\\|\\frac{L_{\\infty}^{2}\\big\\|\\nabla_{\\pi}\\int_{\\mathbb{T}}\\mathbb{C}_{t}\\big[\\mathcal{X}_{\\pi},\\pi_{t},\\widetilde{\\pi}_{t,\\pi}^{\\kappa}\\big]\\big\\|^{2}}{\\|\\pi\\|_{t+1}^{2}}}\\\\ &{\\qquad=\\mathbb{E}\\big[\\mathcal{L}^{*}(\\pi_{t})\\big]-\\frac{\\eta_{*}}{2}\\mathbb{E}\\|\\mathcal{H}^{*}(\\pi_{t})\\|^{2}-\\frac{\\eta_{*}}{2}\\mathbb{E}\\|\\widetilde{M}_{\\pi}^{*}\\|^{2}+\\frac{\\eta_{*}}{2}\\mathbb{E}\\|\\mathcal{H}^{*}(\\pi_{t})-\\widetilde{h}_{\\pi}^{*}\\|^{2}}\\\\ &{\\qquad+\\frac{\\eta_{*}^{2}L_{\\infty,1}}{2}\\mathbb{E}\\bigg\\|\\frac{1}{H_{t}}\\bigg\\|\\sum_{\\ell=\\pi_{t}}\\mathbb{C}_{t}\\big(x_{t},y_{t,\\pi},\\widetilde{\\pi}_{t,\\pi}^{\\kappa}\\big)\\bigg\\|^{2}}\\\\ &{\\qquad\\stackrel{(a)}{\\leq}\\mathbb{E}\\big[\\mathcal{L}^{*}(\\pi_{t})\\big]-\\frac{\\eta_{*}}{2}\\mathbb{E}\\|\\mathcal{H}^{*}(\\pi_{t})\\|^{2}-\\frac{\\eta_{*}}{2}\\mathbb{E}\\|\\widetilde{h}_{\\pi}^{*}\\|^{2}}\\\\ &{\\qquad+\\frac{\\eta_{*}^{2}L_{\\infty,1}}{2}\\mathbb{E}\\bigg\\|\\frac{1}{H_{t}}\\\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where (a) uses ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\|\\mathcal{H}^{*}(x_{t})-\\widetilde{h}_{x}^{t}\\|^{2}}\\\\ &{\\quad=\\mathbb{E}\\bigg\\|\\displaystyle\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\nabla_{x}\\mathcal{L}_{i}\\big(x_{t},y^{*}(x_{t}),z_{\\lambda,i}^{*}(x_{t}),z_{i}^{*}(x_{t})\\big)-\\nabla_{x}\\mathcal{L}_{i}\\big(x_{t},y_{t},z_{i,t}^{K},v_{i,t}^{K}\\big)\\bigg\\|^{2}}\\\\ &{\\quad\\overset{(a,1)}{\\leq}\\displaystyle\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}\\big\\|\\nabla_{x}\\mathcal{L}_{i}\\big(x_{t},y^{*}(x_{t}),z_{\\lambda,i}^{*}(x_{t}),z_{i}^{*}(x_{t})\\big)-\\nabla_{x}\\mathcal{L}_{i}\\big(x_{t},y_{t},z_{i,t}^{K},v_{i,t}^{K}\\big)\\big\\|^{2}}\\\\ &{\\quad\\overset{(a,2)}{\\leq}3L_{f,1}^{2}\\mathbb{E}\\big\\|y_{t}-y^{*}(x_{t})\\big\\|^{2}+3L_{\\lambda,1}^{2}\\mathbb{E}\\bigg[\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\big\\|z_{i,t}^{K}-z_{\\lambda,i}^{*}(x_{t})\\big\\|^{2}+\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\big\\|v_{i,t}^{K}-z_{i}^{*}(x_{t})\\big\\|^{2}\\bigg],}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "and (a.1) uses Jensen inequality; (a.2) follows from Lemma D.1. Then, the proof is complete. ", "page_idx": 32}, {"type": "text", "text": "F.2Bounds of Estimators ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Lemma F.2. Under Assumptions 4.3, 4.4, 4.5, we have the bounds of the estimators of $y_{t}$ and $x_{t}$ as ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\bigg\\|\\displaystyle\\frac{1}{|I_{t}|}\\displaystyle\\sum_{i\\in I_{t}}\\nabla_{y}f_{i}(x_{t},y_{t},v_{i,t}^{K})\\bigg\\|^{2}\\leq\\Big(1+\\displaystyle\\frac{\\beta_{t h}^{2}}{|I_{t}|}\\Big)L_{f,1}^{2}\\Big(\\mathbb{E}\\|y_{t}-y^{*}(x_{t})\\|^{2}+\\displaystyle\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}\\|v_{i,t}^{K}-z_{i}^{*}(x_{t})\\|^{2}\\Big)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\displaystyle\\frac{(n-|I_{t}|)\\sigma_{t h}^{2}}{(n-1)|I_{t}|},}\\\\ &{\\mathbb{E}\\bigg\\|\\displaystyle\\frac{1}{|I_{t}|}\\displaystyle\\sum_{i\\in I_{t}}\\nabla_{x}\\mathcal{L}_{i}(x_{t},y_{t},z_{i,t}^{K},v_{i,t}^{K})\\bigg\\|^{2}\\leq\\mathbb{E}\\|\\widetilde h_{x}^{t}\\|^{2}+\\displaystyle\\frac{3(n-|I_{t}|)}{(n-1)|I_{t}|}(L_{f,0}^{2}+2\\lambda^{2}L_{g,0}^{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "for any $i\\in\\{1,...,n\\}$ and $t\\in\\{0,1,...,T-1\\}$ ", "page_idx": 33}, {"type": "text", "text": "Proof. For the estimator of $y$ ,we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\bigg\\|\\frac{1}{H_{1}}\\bigg\\|\\sum_{i\\in I_{k}}\\nabla_{x}f_{i}(x_{i},y_{i},v_{i,i}^{K})\\bigg\\|^{2}}\\\\ &{\\quad\\overset{(a)}{=}\\frac{\\eta\\left([I_{k}[i-1]]\\right)}{[I_{k}]\\left([n-1\\right)^{\\frac{1}{\\alpha}}]}\\bigg\\|\\frac{1}{n}\\sum_{i^{\\prime}}\\nabla_{y^{\\prime}}f_{i}(x_{i},y_{i},v_{i,i}^{K})\\bigg\\|^{2}+\\frac{n-|I_{1}|}{(n-1)\\alpha}\\bigg\\|\\frac{1}{n}\\sum_{i^{\\prime}}\\frac{1}{n+1}\\bigg\\|\\nabla_{y^{\\prime}}f_{i}(x_{i},y_{i},v_{i,i^{\\prime}}^{K})\\bigg\\|^{2}}\\\\ &{\\quad\\overset{(b)}{=}\\frac{([I_{k}[i]])^{\\alpha}}{\\frac{1}{\\alpha}}+\\frac{n\\left([I_{k}[i-1]]+1\\beta_{0}^{2}(n-1/L)\\right)}{[I_{k}]\\left([n-1\\right)^{\\frac{1}{\\alpha}}]}\\bigg\\|\\frac{1}{n}\\sum_{i^{\\prime}}\\nabla_{y^{\\prime}}f_{i}(x_{i},y_{i},v_{i,i^{\\prime}}^{K})\\bigg\\|^{2}}\\\\ &{\\quad\\overset{(c)}{\\leq}\\frac{([I_{k}[i]])^{\\alpha}}{(n-1)[I_{k}]}}\\\\ &{\\qquad+\\frac{n\\left([I_{k}[i-1]\\right)+\\beta_{0}^{2}(n-1/L)\\right)}{[I_{k}]\\left([I_{k}-1\\right)^{\\frac{1}{\\alpha}}]}\\bigg\\|\\frac{1}{n}\\sum_{i^{\\prime}}\\nabla_{y^{\\prime}}f_{i}(x_{i},y_{i^{\\prime}}^{K})-\\nabla_{y^{\\prime}}f_{i}(x_{i},y_{i^{\\prime}}^{\\kappa}(x_{i}),z_{i^{\\prime}}^{\\kappa}(x_{i}))\\bigg\\|^{2}}\\\\ &{\\quad\\overset{(b)}{=}\\frac{([I_{k}[i]])^{\\alpha}}{[I_{k}]\\left([n-1\\right)^{\\frac{1}{\\alpha}}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where (a) follows from the Lemma A.1 in [32]; (b) uses Assumption 4.5, 4.6; (c) follows from definition $\\begin{array}{r}{y^{\\ast}(x)=\\arg\\operatorname*{max}_{y}\\frac{1}{n}\\sum_{i}^{n}f_{i}\\big(x,y,z_{i}^{\\ast}(x)\\big)}\\end{array}$ (d) uses Assumption 4.4 and (e) uses $\\beta_{t h}\\geq1$ Next, for the estimator of $y$ wehave ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\bigg\\|\\frac{1}{|I_{t}|}\\displaystyle\\sum_{i\\in I_{t}}\\nabla_{x}\\mathcal{L}_{i}(x_{t},y_{t},z_{i,t}^{K},v_{i,t}^{K})\\bigg\\|^{2}\\overset{(a)}{=}\\cfrac{n(|I_{t}|-1)}{|I_{t}|(n-1)}\\mathbb{E}\\bigg\\|\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\nabla_{x}\\mathcal{L}_{i}(x_{t},y_{t},z_{i,t}^{K},v_{i,t}^{K})\\bigg\\|^{2}}&{}\\\\ {+\\quad\\displaystyle\\frac{n-|I_{t}|}{(n-1)|I_{t}|}\\cdot\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}\\bigg\\|\\nabla_{x}\\mathcal{L}_{i}(x_{t},y_{t},z_{i,t}^{K},v_{i,t}^{K})\\bigg\\|^{2}}&{}\\\\ {\\leq\\mathbb{E}\\big\\|\\widetilde{h}_{x}^{t}\\big\\|^{2}+\\frac{3(n-|I_{t}|)}{(n-1)|I_{t}|}(L_{f,0}^{2}+2\\lambda^{2}L_{g,0}^{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Then, the proof is complete. ", "page_idx": 33}, {"type": "text", "text": "F.3 Bounds of Sub-loop Errors ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Lemma F3. Under Assumptions 4.3, 4.4, for $\\forall\\delta\\in\\mathbb{R}^{+}$ ,weassumethat $\\|z_{i}^{*}(x_{t})\\|\\leq B$ forsome $B<\\infty$ Thenwe have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{max}\\left\\{\\mathbb{E}\\|v_{i,t}^{K}-z_{i}^{*}(x_{t})\\|^{2},\\mathbb{E}\\|z_{i,t}^{K}-z_{\\lambda,i}^{*}(x_{t})\\|^{2}\\right\\}\\le\\epsilon_{s u b}}\\\\ &{K\\ge\\operatorname*{max}\\bigg\\{\\frac{1}{\\eta_{t}^{v}\\mu_{g}}\\log\\frac{2\\big(\\|v_{i,t}^{0}\\|^{2}+B^{2}\\big)}{\\epsilon_{s u b}},\\ \\frac{1}{\\eta_{t}^{\\frac{1}{c}}L_{f,1}}\\log\\frac{3\\big(\\|z_{i,t}^{0}\\|^{2}+\\frac{L_{f,0}^{2}}{4L_{f,1}^{2}}+B^{2}\\big)}{\\epsilon_{s u b}}\\bigg\\},\\,\\mathfrak{u}}\\\\ &{(0,\\frac{1}{4\\lambda L_{g,1}}),\\,\\lambda\\ge\\frac{L_{f,1}}{\\mu_{g}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "when , where n e (0, 2,), n E ", "page_idx": 33}, {"type": "text", "text": "Proof rom Lemma D1, we have that $\\mathcal{L}_{i}$ $\\frac{\\lambda\\mu_{g}}{2}$ -stongly convex in $z$ and we have that $\\mathcal{L}_{i}$ .s $2\\lambda L_{g,1}$ -Lipschitz continue in $z$ when $\\begin{array}{r}{\\lambda\\,\\ge\\,\\frac{L_{f,1}}{\\mu_{g}}}\\end{array}$ ; also, from Assumption 4.3 4.4, we have that gi is $\\mu_{g}$ -strongly convex in $v$ and $L_{g,1}$ -smooth in $v$ . According to Theorem 3.6 in [11], by taking $\\begin{array}{r}{0<\\bar{\\eta}_{t}^{v}<\\frac{1}{2L_{g,1}}}\\end{array}$ and $\\begin{array}{r}{0<\\eta_{t}^{z}<\\frac{1}{4\\lambda L_{g,1}}}\\end{array}$ we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\|v_{i,t}^{K}-z_{i}^{*}(x_{t})\\|^{2}\\leq(1-\\eta_{t}^{v}\\mu_{g})^{K}\\mathbb{E}\\|v_{i,t}^{0}-z_{i}^{*}(x_{t})\\|^{2},}\\\\ &{\\mathbb{E}\\|z_{i,t}^{K}-z_{\\lambda,i}^{*}(x_{t})\\|^{2}\\leq(1-\\displaystyle\\frac{\\eta_{t}^{z}\\lambda\\mu_{g}}{2})^{K}\\mathbb{E}\\|z_{i,t}^{0}-z_{\\lambda,i}^{*}(x_{t})\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "To make sure $\\mathbb{E}\\|v_{i,t}^{K}-z_{i}^{*}(x_{t})\\|^{2}\\leq\\epsilon_{s u b}$ and $\\mathbb{E}\\|z_{i,t}^{K}-z_{\\lambda,i}^{*}(x_{t})\\|^{2}\\leq\\epsilon_{s u b}$ for some $\\epsilon_{s u b}\\ge0$ , we let ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(1-\\eta_{t}^{v}\\mu_{g})^{K}\\mathbb{E}\\|v_{i,t}^{0}-z_{i}^{*}(x_{t})\\|^{2}\\leq2(1-\\eta_{t}^{v}\\mu_{g})^{K}\\big(\\|v_{i,t}^{0}\\|^{2}+B^{2}\\big)\\leq\\epsilon_{s u b},}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "and ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(1-\\frac{\\eta_{t}^{2}\\lambda\\mu_{g}}{2})K\\mathbb{g}_{\\mathbb{H}}\\|z_{i,t}^{0}-z_{\\lambda,i}^{*}(x_{t})\\|^{2}}\\\\ &{\\qquad\\qquad\\leq3(1-\\frac{\\eta_{t}^{2}\\lambda\\mu_{g}}{2})^{K}\\big(\\|z_{i,t}^{0}\\|^{2}+\\mathbb{E}\\|z_{i}^{*}(x_{t})-z_{\\lambda,i}^{*}(x_{t})\\|^{2}+\\mathbb{E}\\|z_{i}^{*}(x_{t})\\|^{2}\\big)}\\\\ &{\\qquad\\qquad\\overset{(a)}{\\leq}3(1-\\frac{\\eta_{t}^{2}\\lambda\\mu_{g}}{2})^{K}\\bigg(\\|z_{i,t}^{0}\\|^{2}+\\frac{L_{f,0}^{2}}{\\mu_{g}^{2}\\lambda^{2}}+B^{2}\\bigg)}\\\\ &{\\qquad\\qquad\\overset{(b)}{\\leq}3(1-\\frac{\\eta_{t}^{2}\\lambda\\mu_{g}}{2})^{K}\\bigg(\\|z_{i,t}^{0}\\|^{2}+\\frac{L_{f,0}^{2}}{4L_{f,1}^{2}}+B^{2}\\bigg)}\\\\ &{\\qquad\\qquad<\\epsilon_{s u b},}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where (a) uses Lemma D.4; (b) take $\\begin{array}{r}{\\lambda\\geq\\frac{2L_{f,1}}{\\mu_{g}}}\\end{array}$ 2Lf1. Both can be achieved by taking ", "page_idx": 34}, {"type": "equation", "text": "$$\nK\\geq\\operatorname*{max}\\Bigg\\{\\frac{1}{\\eta_{t}^{v}\\mu_{g}}\\log\\frac{2\\big(\\|v_{i,t}^{0}\\|^{2}+B^{2}\\big)}{\\epsilon_{s u b}},\\,\\frac{1}{\\eta_{t}^{z}L_{f,1}}\\log\\frac{3\\big(\\|z_{i,t}^{0}\\|^{2}+\\frac{L_{f,0}^{2}}{4L_{f,1}^{2}}+B^{2}\\big)}{\\epsilon_{s u b}}\\Bigg\\}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Then the proof is complete. ", "page_idx": 34}, {"type": "text", "text": "Lemma F4. Under the Assumptions 4.3, 4.4, 4.5, the iterates of $y_{t}$ updates according to Algorithm 2 satisfy ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\|y_{t+1}-y^{*}(x_{t+1})\\|^{2}-\\mathbb{E}\\|y_{t}-y^{*}(x_{t})\\|^{2}}\\\\ &{\\le\\bigg[-\\eta_{y}\\mu_{f}+\\eta_{y}^{2}(1+\\delta_{y})\\bigg(1+\\displaystyle\\frac{\\beta_{t}^{2}\\lambda}{|I_{t}|}\\bigg)L_{f,1}^{2}+\\delta_{y}\\bigg]\\mathbb{E}\\|y_{t}-y^{*}(x_{t})\\|^{2}+\\eta_{y}^{2}(1+\\delta_{y})\\displaystyle\\frac{(n-|I_{t}|)\\sigma_{t h}^{2}}{(n-1)|I_{t}|}}\\\\ &{\\quad+\\bigg(\\displaystyle\\frac{\\eta_{y}L_{f,1}^{2}}{\\mu_{f}}+\\eta_{y}^{2}\\Big(1+\\displaystyle\\frac{\\beta_{t}^{2}\\lambda}{|I_{t}|}\\Big)L_{f,1}^{2}\\bigg)(1+\\delta_{y})\\cdot\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}\\|v_{i,t}-z_{i}^{*}(x_{t})\\|^{2}}\\\\ &{\\quad+\\displaystyle\\frac{\\eta_{x}^{2}}{\\delta_{y,1}}\\bigg(1+\\displaystyle\\frac{L_{g,1}}{\\mu_{g}}\\bigg)^{2}\\displaystyle\\frac{L_{f,1}^{2}}{\\mu_{f}^{2}}\\mathbb{E}\\|\\tilde{h}_{x}^{t}\\|^{2}}\\\\ &{\\quad+\\eta_{x}^{2}\\bigg(\\displaystyle\\frac{L_{*,y}}{2}+\\Big(1+\\displaystyle\\frac{L_{g,1}}{\\mu_{g}}\\Big)^{2}\\displaystyle\\frac{L_{f,1}^{2}}{\\mu_{f}^{2}}\\bigg)\\mathbb{E}\\bigg\\|\\displaystyle\\frac{1}{|I_{t}|}\\displaystyle\\sum_{i\\neq l}\\nabla_{x}\\mathcal{L}_{i}(x_{t},y_{t},z_{i,t}^{K},v_{i,t}^{K})\\bigg\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "for any $t~~\\in~~\\{0,...,T\\,-\\,1\\}$ where we assume $\\begin{array}{r l r}{\\lambda}&{{}\\ge}&{\\{2L_{f,1}/\\mu_{g},(1\\,+\\,\\frac{L_{g,1}}{\\mu_{g}})\\frac{L_{f,1}^{2}}{3\\mu_{f}L_{g,1}},(1\\,+\\,}\\end{array}$\uff09\uff0c(1+(1+)+\uff09,(1+\uff09+1\uff09)}", "page_idx": 34}, {"type": "text", "text": "Proof. For $y$ and $y^{*}(x)$ , we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\|y_{t+1}-y^{*}(x_{t+1})\\|^{2}=\\mathbb{E}\\|y_{t+1}-y^{*}(x_{t})\\|^{2}+\\mathbb{E}\\|y^{*}(x_{t})-y^{*}(x_{t+1})\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\ 2\\mathbb{E}\\bigl\\langle y_{t+1}-y^{*}(x_{t}),y^{*}(x_{t})-y^{*}(x_{t+1})\\bigr\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We can bound the first term in eq. (43) as ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathrm{sup.~}\\forall\\{\\mu_{1}=1\\}^{\\mathcal{E}}}}\\\\ &{=\\Xi\\Big\\{\\frac{1}{N}+\\frac{1}{N}\\sum_{i=1}^{N}\\mathrm{e}^{-\\mathrm{i}\\phi_{i,j}}[\\phi_{i,j}(\\phi_{i,j})\\phi_{j-1}^{\\mathcal{E}}-\\phi_{i,j}(\\phi_{i,j})]^{\\frac{1}{2}}}\\\\ &{=\\Xi\\Big\\{\\frac{1}{N}-\\mathrm{pr}_{1}^{(1)}\\frac{1}{N}+\\frac{1}{N}\\sum_{i=1}^{N}\\mathrm{e}^{-\\mathrm{i}\\phi_{i,j}}[\\phi_{i,j}(\\phi_{i,j-1},\\phi_{j})]^{\\frac{1}{2}}}\\\\ &{\\qquad+2\\eta\\mathrm{sup}_{i}\\biggl\\{(-\\eta_{1}-\\phi_{i,j})\\phi_{j-1}^{\\mathcal{E}}[\\phi_{i,j}(\\phi_{i,j-1},\\phi_{j})]\\biggr\\}}\\\\ &{\\leq\\mathrm{pr}_{1}\\{N>\\mathrm{pr}_{1}^{(1)}-\\mathrm{pr}_{1}^{(1)}\\frac{1}{N}\\sum_{i=1}^{N}\\mathrm{e}^{-\\mathrm{i}\\phi_{i,j}}[\\phi_{i,j-1},\\phi_{j}]^{\\frac{1}{2}}\\biggr\\}}\\\\ &{\\qquad+2\\eta\\mathrm{sup}_{i}\\biggl\\{(-\\eta_{1}-\\phi_{i,j-1})\\phi_{j-1}^{\\mathcal{E}}\\,\\mathrm{e}^{-\\mathrm{i}\\phi_{i,j-1}}[\\phi_{i,j-1},\\phi_{j}]\\biggr\\}}\\\\ &{\\qquad+2\\eta\\mathrm{sup}_{i}\\biggl\\{(-\\eta_{1}-\\phi_{i,j-1})\\phi_{j-1}^{\\mathcal{E}}\\,\\mathrm{e}^{-\\mathrm{i}\\phi_{i,j-1}}[\\phi_{i,j-1},\\phi_{j-1}(\\phi_{i,j-1})]^{\\frac{1}{2}}\\biggr\\}}\\\\ &{\\qquad+2\\eta\\mathrm{sup}_{i}\\biggl\\{(-\\eta_{1}-\\phi_{i,j})\\phi_{j-1}^{\\mathcal{E}}\\,\\mathrm{e}^{-\\mathrm{i}\\phi_{i,j-1}}[\\phi_{i,j-1},\\phi_{j-1}^{\\mathcal{E}}]\\biggr\\}}\\\\ & \n$$$$\n\\begin{array}{r l}&{\\mathbb{E}\\{\\left|\\mathbf{J}_{t}^{T}\\mathbf{h}-\\mathbf{J}^{t}\\in\\mathbb{Z}\\right\\}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! \n$$", "text_format": "latex", "page_idx": 35}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where (a) uses the definition of $\\boldsymbol{y}^{*}(\\boldsymbol{x}_{t})$ and eq. (5); (b) uses strong concavity of $f_{i}$ in $y$ (c) follos from definition of $y^{\\ast}(x_{t})$ and Assumption 4.4; (d) uses Lemma F.2. We can bound the second term in eq. (43) as ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\|y^{*}(x_{t})-y^{*}(x_{t+1})\\|^{2}\\overset{(a)}{\\leq}\\Big(1+\\frac{L_{g,1}}{\\mu_{g}}\\Big)^{2}\\frac{L_{f,1}^{2}}{\\mu_{f}^{2}}\\mathbb{E}\\|x_{t}-x_{t+1}\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\eta_{x}^{2}\\Big(1+\\frac{L_{g,1}}{\\mu_{g}}\\Big)^{2}\\frac{L_{f,1}^{2}}{\\mu_{f}^{2}}\\mathbb{E}\\bigg\\|\\frac{1}{|I_{t}|}\\sum_{i\\in I_{t}}\\nabla_{y}f_{i}\\big(x_{t},y_{t},v_{i,t}^{K}\\big)\\bigg\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where (a) follows from Lemma D.2. Also, we can get the bound of the last term as ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2\\mathbb{E}\\big<y_{t+1}-y^{*}(x_{t}),y^{*}(x_{t})-y^{*}(x_{t+1})\\big>}\\\\ &{\\qquad=-\\,2\\mathbb{E}\\big<y_{t+1}-y^{*}(x_{t}),\\nabla y^{*}(x_{t})(x_{t+1}-x_{t})\\big>}\\\\ &{\\qquad\\quad-\\,2\\mathbb{E}\\big<y_{t+1}-y^{*}(x_{t}),y^{*}(x_{t+1})-y^{*}(x_{t})-\\nabla y^{*}(x_{t})(x_{t+1}-x_{t})\\big>}\\\\ &{\\qquad=2\\mathbb{E}\\big\\|y_{t+1}-y^{*}(x_{t})\\big\\|\\cdot\\big\\|\\eta_{x}\\nabla y^{*}(x_{t})\\widetilde{h}_{x}^{t}\\big\\|}\\\\ &{\\qquad+\\,2\\mathbb{E}\\big\\|y_{t+1}-y^{*}(x_{t})\\big\\|\\cdot\\big\\|y^{*}(x_{t+1})-y^{*}(x_{t})-\\nabla y^{*}(x_{t})(x_{t+1}-x_{t})\\big\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\overline{{u}}_{\\delta_{3}}|\\sum_{\\|\\ell\\|+1}-y^{\\star}(\\boldsymbol{x}_{\\ell})||^{2}+\\frac{\\eta^{2}}{\\delta_{3}}\\left(1+\\frac{L_{\\ell,1}}{\\beta_{\\ell}}\\right)^{2}\\frac{L_{\\ell,1}^{2}}{\\mu_{\\ell}^{2}}\\|\\overline{{h}}_{\\ell}^{\\ell}\\|^{2}}\\\\ &{\\quad+\\mathbb{E}\\|\\{u_{1}+y^{\\star}(\\boldsymbol{x}_{\\ell})\\}\\|\\cdot\\boldsymbol{L}_{\\ell,1}\\|\\boldsymbol{x}_{\\ell+1}-\\boldsymbol{x}_{\\ell}\\|^{2}}\\\\ &{\\leq\\delta_{1}\\Im\\|y_{1}+y^{\\star}(\\boldsymbol{x}_{\\ell})\\|^{2}+\\frac{\\eta^{2}}{\\delta_{3}}\\Big(1+\\frac{L_{\\ell,1}}{\\beta_{\\ell}}\\Big)^{2}\\frac{L_{\\ell,1}^{2}}{\\mu_{\\ell}^{2}}\\mathbb{E}\\|\\widetilde{h}_{\\ell}^{\\ell}\\|^{2}}\\\\ &{\\quad+\\frac{L_{\\mathcal{S},\\mathbb{E}}}{2}\\mathbb{E}\\|\\|u_{1}-y^{\\star}(\\boldsymbol{x}_{\\ell})\\|^{2}\\cdot\\left\\|y_{1}+\\dots\\right\\|^{2}+\\frac{L_{\\mathcal{S},\\mathbb{E}}}{2}\\mathbb{E}\\|\\boldsymbol{x}_{\\ell+1}-\\boldsymbol{x}_{\\ell}\\|^{2}}\\\\ &{\\overset{(a)}{\\leq}\\left(\\delta_{1}\\delta_{1}+\\frac{3\\eta^{2}L_{\\mathcal{S},\\ell}}{2}(L_{\\ell}^{2}+2\\lambda_{2}^{2}L_{\\mathcal{S},\\ell}^{2})\\right)\\mathbb{E}\\|u_{1}-y^{\\star}(\\boldsymbol{x}_{\\ell})\\|^{2}}\\\\ &{\\qquad+\\frac{\\eta^{2}}{\\delta_{3}}\\bigg(1+\\frac{L_{\\ell,1}}{\\beta_{\\ell}}\\Big)^{2}\\frac{L_{\\ell,1}^{2}}{\\mu_{\\ell}^{2}}\\mathbb{E}\\|\\widetilde{h}_{\\ell}^{\\ell}\\|^{2}+\\frac{\\eta^{2}L_{\\mathcal{S},\\mathbb{E}}}{2}\\bigg\\|\\frac{1}{\\mu_{\\ell}}\\sum_{\\ell}C_\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where (a) uses Lemma D.2, D.3 and Lemma 1 in [44]; (b) follows from Assumption 4.4; (c) defines $\\begin{array}{r}{\\delta_{y}=\\delta_{y,1}+\\frac{3\\eta_{x}^{2}L_{*,y}}{2}(L_{f,0}^{2}+2\\lambda^{2}L_{g,0}^{2})}\\end{array}$ ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\|y_{t+1}-y^{*}(x_{t+1})\\|^{2}-\\mathbb{E}\\|y_{t}-y^{*}(x_{t})\\|^{2}}\\\\ &{\\qquad\\leq\\bigg[-\\eta_{y}\\mu_{f}+\\eta_{y}^{2}(1+\\delta_{y})\\Big(1+\\frac{\\beta_{t}^{2}\\lambda_{t}}{|I_{t}|}\\Big)L_{f,1}^{2}+\\delta_{y}\\bigg]\\mathbb{E}\\|y_{t}-y^{*}(x_{t})\\|^{2}+\\eta_{y}^{2}(1+\\delta_{y})\\frac{(n-|I_{t}|)\\sigma_{t}^{2}}{(n-1)|I_{t}|}}\\\\ &{\\qquad\\qquad+\\left(\\frac{\\eta_{y}L_{f,1}^{2}}{\\mu_{f}}+\\eta_{y}^{2}\\Big(1+\\frac{\\beta_{t}^{2}\\lambda_{t}}{|I_{t}|}\\Big)L_{f,1}^{2}\\right)(1+\\delta_{y})\\cdot\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}\\|v_{i,t}-z_{i}^{*}(x_{t})\\|^{2}}\\\\ &{\\qquad+\\frac{\\eta_{x}^{2}}{\\delta_{y,1}}\\bigg(1+\\frac{L_{g,1}}{\\mu_{g}}\\bigg)^{2}\\frac{L_{f,1}^{2}}{\\mu_{f}^{2}}\\mathbb{E}\\|\\tilde{h}_{x}\\|^{2}}\\\\ &{\\qquad\\qquad+\\eta_{x}^{2}\\bigg(\\frac{L_{s,y}}{2}+\\Big(1+\\frac{L_{g,1}}{\\mu_{g}}\\Big)^{2}\\frac{L_{f,1}^{2}}{\\mu_{f}^{2}}\\bigg)\\mathbb{E}\\bigg\\|\\frac{1}{|I_{t}|}\\sum_{i=1}^{n}\\nabla_{x}\\mathcal{L}_{i}(x_{t},y_{t},z_{i,t}^{K},v_{i,t}^{K})\\bigg\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Then the proof is complete. ", "page_idx": 36}, {"type": "text", "text": "F.4  Descent in the Lyapunov Function and Proof of Theorem 4.12 ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "We define the Lyapunov function as ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\Psi(x):=\\mathcal{L}^{*}(x)+K_{y}\\mathbb{E}\\|y_{t}-y^{*}(x_{t})\\|^{2},\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where the coefficient is given by $\\begin{array}{r}{K_{y}=\\frac{3\\eta_{x}L_{f,1}^{2}}{\\eta_{y}\\mu_{f}}}\\end{array}$ 3mL1. We also constrain the conditions as below: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\delta_{y,1}=\\frac{\\eta_{y}\\mu_{f}}{4},\\quad\\eta_{x}\\leq\\frac{1}{3L_{*,1}},\\quad\\eta_{y}\\leq\\operatorname*{min}\\bigg\\{\\frac{1}{(1+\\beta_{t h}^{2})\\mu_{f}},\\frac{\\mu_{f}}{8(1+\\beta_{t h}^{2})L_{g,1}^{2}}\\bigg\\},}\\\\ &{\\frac{\\eta_{x}^{2}}{\\eta_{y}}\\leq\\operatorname*{min}\\bigg\\{\\frac{\\mu_{g}}{12L_{*,y}(L_{f,0}^{2}+2\\lambda^{2}L_{g,0}^{2})},\\quad\\frac{\\mu_{f}}{18L_{f,1}^{2}}\\bigg(\\frac{L_{*,y}}{2}+\\bigg(1+\\frac{6L_{g,1}}{\\mu_{g}}\\bigg)^{2}\\frac{L_{f,1}^{2}}{\\mu_{f}^{2}}\\bigg)^{-1}\\bigg\\},}\\\\ &{\\frac{\\eta_{x}}{\\eta_{y}}\\leq\\frac{\\mu_{f}^{2}}{6\\sqrt{2}L_{f,1}^{2}}\\bigg(1+\\frac{6L_{g,1}}{\\mu_{g}}\\bigg)^{-1},}\\\\ &{\\lambda\\geq\\operatorname*{max}\\bigg\\{2L_{f,1}/\\mu_{g},(1+\\frac{L_{g,1}}{\\mu_{g}})\\frac{L_{f,1}^{2}}{3\\mu_{f}L_{g,1}},(1+\\frac{L_{g,1}}{\\mu_{g}})\\frac{L_{f,1}L_{f,2}}{3\\mu_{f}L_{g,1}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "equation", "text": "$$\n\\frac{L_{f,1}L_{*,y}}{6L_{g,1}}\\big(1+\\big(1+\\frac{L_{g,1}}{\\mu_{g}}\\big)\\frac{L_{f,1}}{\\mu_{f}}+\\frac{12L_{g,1}}{\\mu_{g}}\\big)^{-1},\\Big(\\big(1+\\frac{L_{g,1}}{\\mu_{g}}\\big)\\frac{L_{f,1}}{\\mu_{f}}+1\\Big)\\frac{L_{f,1}}{L_{g,1}}\\Bigg\\}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Plugging Lemma F.1, Lemma E.3 into eq. (47), we have the descent in the Lyapunov function as $\\Psi_{t+1}-\\Psi_{t}$ ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{S}-\\frac{\\alpha}{2}\\mathbb{E}\\mathbb{E}\\left(\\left(\\rho_{t}^{1}\\right)^{2}-\\nu\\right)\\Bigg[\\;-\\frac{N_{t}\\rho_{t}}{N_{t}\\rho_{t}^{1}}\\frac{\\partial^{2}}{\\partial\\nu}\\Bigg(1-\\frac{N_{t}\\rho_{t}}{N_{t}\\rho_{t}^{1}},~\\frac{\\mu}{N_{t}}\\Bigg(1\\!-\\!\\frac{\\nu}{\\rho_{t}}\\frac{\\mu}{N_{t}}\\Bigg)\\frac{\\mu}{N_{t}\\rho_{t}^{2}}\\Bigg)\\mathbb{E}\\Bigg]_{\\rho_{t}}\\mathbb{E}[\\rho_{t}^{3}]}\\\\ &{+\\mathbb{E}\\Bigg[\\frac{\\mu}{N_{t}\\rho_{t}^{2}}\\frac{\\mu}{\\mu}+N_{t}\\frac{\\mu}{N_{t}\\rho_{t}^{3}}+(1\\!-\\!\\frac{\\nu}{\\rho_{t}}\\frac{\\mu}{N_{t}\\rho_{t}^{3}})\\frac{\\mu}{N_{t}\\rho_{t}^{2}}\\Bigg]\\Bigg[\\nu\\Bigg]_{\\rho_{t}}\\mathbb{E}[\\rho_{t}^{1}\\!-\\!\\frac{\\nu}{\\rho_{t}}\\frac{\\mu}{N_{t}\\rho_{t}^{3}}\\Bigg]\\Bigg[\\;}\\\\ &{+2\\nu\\mu\\frac{\\mu}{N_{t}\\rho_{t}}\\frac{\\mu}{\\mu}(1\\!-\\!\\frac{\\nu}{\\rho_{t}}\\!)\\!+\\!\\frac{3}{2}N_{t}\\frac{\\mu}{N_{t}\\rho_{t}^{2}}\\Bigg]\\Bigg[\\frac{\\mu}{N_{t}\\rho_{t}^{3}}\\!-\\!\\frac{\\mu}{\\mu}(\\mu)\\!-\\!\\frac{\\nu}{\\mu}(\\mu)\\Bigg]+\\frac{1}{\\eta_{\\mathrm{ret}}}\\Bigg[\\nu\\Bigg]\\Bigg[\\frac{\\mu}{N_{t}\\rho_{t}}}\\\\ &{+2\\nu\\mathbb{E}\\Bigg(\\frac{\\mu}{N_{t}\\rho_{t}}\\frac{\\mu}{N_{t}\\rho_{t}^{2}}\\!+\\!\\frac{\\mu}{N_{t}\\rho_{t}^{3}}\\!\\Bigg)\\Bigg]\\Bigg[\\mu\\Bigg]}\\\\ &{-\\frac{\\alpha}{2}\\mathbb{E}[\\nu\\Bigg(\\rho_{t}^{1}\\!-\\!\\frac{\\nu}{\\rho_{t}}\\frac{N_{t}\\rho_{t}}{N_\n$$12L,1 ", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where (a) uses Lemma F.2; (b) uses eq. (48) and takes ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{K\\geq\\operatorname*{max}\\Big\\{\\frac{1}{\\eta_{t}^{v}\\mu_{g}}\\log\\frac{2\\mathbb{E}\\|v_{i,t}^{0}-z_{i}^{*}(x_{t})\\|^{2}}{\\epsilon_{s u b}},\\frac{2}{\\eta_{t}^{z}\\lambda\\mu_{g}}\\log\\frac{2\\mathbb{E}\\|z_{i,t}^{0}-z_{\\lambda,i}^{*}(x_{t})\\|^{2}}{\\epsilon_{s u b}}\\Big\\};}\\\\ &{\\varsigma_{4}:=\\frac{3(L_{f,0}^{2}+2\\lambda^{2}L_{g,0}^{2})}{\\lambda^{2}}\\cdot\\operatorname*{max}\\Big\\{\\frac{L_{*,1}}{2},\\frac{3L_{f,1}^{2}}{\\mu_{f}}\\Big(\\frac{L_{*,y}}{2}+\\big(1+\\frac{L_{g,1}}{\\mu_{g}}\\big)^{2}\\frac{L_{f,1}^{2}}{\\mu_{f}^{2}}\\Big)\\Big\\}\\mathrm{~and~plu}}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "$K_{y}$ \uff0c", "page_idx": 37}, {"type": "text", "text": "F.5Proof of Theorem 4.12 ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Proof. For partial block participation, we take the summation of eq. (49) from $t=0$ to $T-1$ .Then wehave ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac1T\\sum_{t=0}^{T-1}\\frac{\\eta_{x}}{2}\\mathbb{E}\\|\\mathcal{H}^{*}(x_{t})\\|^{2}\\le\\!\\Psi(x_{0})-\\Psi(x_{T})+\\eta_{x}^{2}\\Big(1+\\frac{\\eta_{x}}{\\eta_{y}}\\Big)C_{4}\\frac{(n-P)\\lambda^{2}}{(n-1)P}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad+\\ \\eta_{x}\\eta_{y}\\frac{n-P}{(n-1)P}\\frac{6L_{f,1}^{2}\\sigma_{t h}^{2}}{\\mu_{f}}+\\eta_{x}\\bigg(3L_{\\lambda,1}^{2}+\\frac{12L_{f,1}^{4}}{\\mu_{f}^{2}}\\bigg)\\epsilon_{s u b}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "For partial block participation, By using Lemma D.5, we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=0}^{T-1}\\mathbb{E}\\|\\nabla\\Phi(x_{t})\\|^{2}\\leq\\frac{2}{T}\\sum_{t=0}^{T-1}\\mathbb{E}\\|\\nabla\\Phi(x_{t})-\\mathcal{H}^{*}(x_{t})\\|^{2}+\\frac{2}{T}\\sum_{t=0}^{T-1}\\mathbb{E}\\|\\mathcal{H}^{*}(x_{t})\\|^{2}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overset{(a)}{\\leq}\\frac{2C_{\\omega\\nu}}{\\lambda^{2}}+\\frac{2\\big(\\Psi(x_{0})-\\Psi(x_{T})\\big)}{T_{T_{s}}}+4\\eta_{x}\\Big(1+\\frac{\\eta_{x}}{\\eta_{y}}\\Big)C_{4}\\frac{(n-P)\\lambda^{2}}{(n-1)P}}\\\\ &{\\quad+\\eta_{y}\\frac{n-P}{(n-1)P}\\frac{24L_{f,1}^{2}\\sigma_{\\hat{t}}^{2}}{\\mu_{f}}+4\\Big(3L_{\\lambda_{1}}^{2}+\\frac{12L_{f,1}^{2}}{\\mu_{f}^{2}}\\Big)\\epsilon_{\\mathrm{sho}}}\\\\ &{\\overset{(b)}{\\leq}\\frac{2\\big(\\Psi(x_{0})-\\Psi(x_{0})-\\Psi(x_{T})\\big)}{\\lambda^{2}}+\\frac{4\\eta_{x}\\lambda^{2}}{P}\\big(1+\\frac{\\eta_{x}}{\\eta_{y}}\\big)C_{4}+\\frac{\\eta_{y}}{P}\\frac{24L_{f,1}^{2}\\sigma_{\\hat{t}}^{2}}{\\mu_{f}}}\\\\ &{\\quad+4\\bigg(3L_{\\lambda_{1}}^{2}+\\frac{12L_{f,1}^{2}}{\\mu_{f}}\\Big)\\epsilon_{\\mathrm{sho}}}\\\\ &{\\leq\\frac{2C_{\\omega\\nu}}{\\lambda^{2}}+\\frac{2\\big(\\Psi(x_{0})-\\Psi(x_{T})\\big)}{T_{T_{s}}}+\\frac{4\\eta_{x}\\lambda^{2}}{P}\\big(1+\\frac{\\eta_{x}}{\\eta_{y}}\\big)C_{4}+\\frac{\\eta_{y}}{P}\\frac{24L_{f,1}^{2}\\sigma_{\\hat{t}}^{2}}{\\mu_{f}}}\\\\ &{\\quad+4\\bigg(9\\lambda^{2}L_{g,1}^{2}+\\frac{12L_{f,1}^{2}}{\\mu_{f}^{2}}\\big)\\epsilon_{\\mathrm{sho}}}\\\\ &{\\overset{(c)}{\\leq}\\mathcal{O}(P^{-1}\\!\\!-\\!\\!1;\\!\\lambda_{f}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where (a) follows from eq. (50); (b) follows from $1\\,\\leq\\,P\\,<\\,n$ ; (c) takes $\\eta_{x}\\;=\\;\\mathcal{O}(P^{\\frac{1}{5}}T^{-\\frac{2}{3}})$ \uff0c $\\eta_{y}=\\mathcal{O}(P^{-\\frac{1}{5}}T^{-\\frac{1}{2}}),\\lambda=\\bar{\\mathcal{O}}(P^{\\frac{1}{10}}T^{\\frac{1}{6}}),\\epsilon_{s u b}=\\mathcal{O}(P^{-\\frac{2}{5}}T^{-\\frac{2}{3}})$ ", "page_idx": 38}, {"type": "text", "text": "Next, for full block participation $(n=P$ ), we have the descent in the Lyapunov function for full block participation as ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\Psi(x_{t+1})-\\Psi(x_{t})\\leq-\\frac{\\eta_{x}}{2}\\mathbb{E}\\|\\mathcal{H}^{*}(x_{t})\\|^{2}+\\eta_{x}\\bigg(3L_{\\lambda,1}^{2}+\\frac{12L_{f,1}^{4}}{\\mu_{f}^{2}}\\bigg)\\epsilon_{s u b}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Taking summation of eq. (52) from $t=0$ to $T-1$ ,we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=0}^{T-1}\\frac{\\eta_{x}}{2}\\mathbb{E}\\|\\mathcal{H}^{*}(x_{t})\\|^{2}\\leq\\!\\Psi(x_{0})-\\Psi(x_{T})+\\eta_{x}\\bigg(3L_{\\lambda,1}^{2}+\\frac{12L_{f,1}^{4}}{\\mu_{f}^{2}}\\bigg)\\epsilon_{s u b}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "By using Lemma D.5 and eq. (53), we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{T}\\sum_{t=0}^{T-1}\\mathbb{E}\\|\\nabla\\Phi(x_{t})\\|^{2}\\leq\\displaystyle\\frac{2}{T}\\sum_{t=0}^{T-1}\\mathbb{E}\\|\\nabla\\Phi(x_{t})-\\mathcal{H}^{*}(x_{t})\\|^{2}+\\displaystyle\\frac{2}{T}\\sum_{t=0}^{T-1}\\mathbb{E}\\|\\mathcal{H}^{*}(x_{t})\\|^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\displaystyle\\leq\\frac{2C_{g a p}}{\\lambda^{2}}+\\frac{4\\big(\\Psi(x_{0})-\\Psi(x_{T})\\big)}{T\\eta_{x}}+12\\bigg(L_{\\lambda,1}^{2}+\\frac{4L_{f,1}^{4}}{\\mu_{f}^{2}}\\bigg)\\epsilon_{s u b}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\displaystyle\\leq\\frac{2C_{g a p}}{\\lambda^{2}}+\\frac{4\\big(\\Psi(x_{0})-\\Psi(x_{T})\\big)}{T\\eta_{x}}+12\\bigg(9\\lambda^{2}L_{g,1}^{2}+\\frac{4L_{f,1}^{4}}{\\mu_{f}^{2}}\\bigg)\\epsilon_{s u b}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\mathcal{O}(T^{-1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "By taking $\\eta_{x}=\\mathcal{O}(1),\\eta_{y}=\\mathcal{O}(1),\\lambda=\\mathcal{O}(T^{\\frac{1}{2}}),\\epsilon_{s u b}=\\mathcal{O}(T^{-2})$ . Then Theorem 4.12 is proved. ", "page_idx": 38}, {"type": "text", "text": "F.6Proof of Corollary 4.13 ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Proof. For tasks participate in updates partially, by eq. (51), we can find the $\\epsilon_{\\mathrm{:}}$ -stationary point in definition 4.2 once we take $\\bar{T}\\;=\\;\\bar{\\mathcal{O}}(P^{-\\frac{3}{5}}\\dot{\\epsilon}^{-3})$ . Note that we set the error of sub-loop as $\\epsilon_{s u b}\\ =\\ {\\mathcal O}(P^{-\\frac{2}{5}}T^{-\\frac{2}{3}})\\ =\\ {\\mathcal O}(\\epsilon^{2})$ .According to Lemma F.3, once we take $\\eta_{v}~=~\\mathcal{O}(1)$ and $\\eta_{z}\\,=\\,\\mathcal{O}(P^{-\\frac{1}{10}}T^{-\\frac{1}{6}})$ , we have the iteration number of sub-loop as $\\begin{array}{r}{K\\,=\\,\\mathcal{O}\\big(\\log(\\frac{1}{\\epsilon})\\big)}\\end{array}$ . Thus, we have the total sample complexity $\\begin{array}{r}{P K T=\\mathcal{O}(P^{\\frac{2}{5}}\\epsilon^{-3}\\log(\\frac{1}{\\epsilon}))=\\widetilde{\\mathcal{O}}(P^{\\frac{2}{5}}\\epsilon^{-3})}\\end{array}$ ", "page_idx": 38}, {"type": "text", "text": "Similarly, by eq. (54), we can find the $\\epsilon_{}$ -stationary point in definition 4.2 once we take $T=\\mathcal{O}(\\epsilon^{-1})$ Note that we set the eror of sub-loop as $\\epsilon_{s u b}=\\bar{\\mathcal{O}}(T^{-2})=\\mathcal{O}(\\epsilon^{2})$ Once we take $\\eta_{v}=\\mathcal{O}(1)$ and $\\eta_{z}=\\mathcal{O}(T^{-\\frac{1}{2}})$ , we have the iteration number of sub-loop as $\\begin{array}{r}{K=\\mathcal O\\big(\\log(\\frac{1}{\\epsilon})\\big)}\\end{array}$ . Thus, we have the total sample complexity $\\begin{array}{r}{n K T=\\mathcal{O}(n\\epsilon^{-1}\\log(\\frac{1}{\\epsilon}))=\\widetilde{\\mathcal{O}}(n\\epsilon^{-1})}\\end{array}$ \u53e3 ", "page_idx": 38}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 39}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 39}, {"type": "text", "text": "\u00b7 You should answer [Yes] , [No] , or [NA] .   \n\u00b7 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u00b7 Please provide a short (1-2 sentence) justification right after your answer (even for NA). ", "page_idx": 39}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 39}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 39}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 39}, {"type": "text", "text": "\u00b7 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u00b7 Keep the checklist subsection headings, questions/answers and guidelines below. \u00b7 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 39}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Justification: The abstract and introduction in this paper accurately reflect the paper's contributions and scope. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and refect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 39}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: The limitation of previous works is discussed in the Introduction part. Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should refect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should refect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 40}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Justification: We provide complete assumption in the main text and full proof in the appendix. Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 40}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Justification: We provide detailed setting of the experiments in the appendix. Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. ", "page_idx": 40}, {"type": "text", "text": "\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b)  If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. () If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 41}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Justification: We provide the code as the supplementary material ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 41}, {"type": "text", "text": "\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 42}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: Data and experimental settings are provided in the experiment part in the main text. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 42}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: Error bars are provided in the experiments in Table 1. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 42}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce theexperiments? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: The computational resources are described in the implementation detail part in the appendix. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. ", "page_idx": 42}, {"type": "text", "text": "\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 43}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: The authors have reviewed the Code of Ethics and make sure the research conformswithit. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 43}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 43}, {"type": "text", "text": "Justification: This paper focuses on the theoretical analysis of multi-block minimax bilevel optimization problem, which does not have a social impact. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 43}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 44}, {"type": "text", "text": "Justification: The datasets used in this paper contain widely used real-world data; This research does not use pre-trained LLM or other generative models. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safetyfilters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 44}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 44}, {"type": "text", "text": "Justification: We have introduced all datasets with proper reference in the experiments part. Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 44}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 44}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose asset isused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 44}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "page_idx": 44}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 45}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 45}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 45}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 45}]