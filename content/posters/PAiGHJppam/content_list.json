[{"type": "text", "text": "Functionally Constrained Algorithm Solves Convex Simple Bilevel Problems ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Huaqing Zhang\\* 1,2 Lesi Chen\\* 1,2 Jing $\\mathbf{Xu^{1}}$ Jingzhao Zhang1,2,3 ", "page_idx": 0}, {"type": "text", "text": "1IIIS, Tsinghua University 2Shanghai Qizhi Institute 3Shanghai AI Lab ", "page_idx": 0}, {"type": "text", "text": "{zhanghq22, chenlc23, xujing21}@mails.tsinghua.edu.cn jingzhaoz@mail.tsinghua.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "This paper studies simple bilevel problems, where a convex upper-level function is minimized over the optimal solutions of a convex lower-level problem. We first show the fundamental difficulty of simple bilevel problems, that the approximate optimal value of such problems is not obtainable by first-order zero-respecting algorithms. Then we follow recent works to pursue the weak approximate solutions. For this goal, we propose novel near-optimal methods for smooth and nonsmooth problems by reformulating them into functionally constrained problems. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "This work focuses on the following optimization problem: ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{x}\\in\\mathbb{R}^{n}}\\;f(\\mathbf{x})\\quad\\mathrm{s.t.}\\quad\\mathbf{x}\\in\\mathcal{X}_{g}^{*}\\triangleq\\arg\\operatorname*{min}_{\\mathbf{z}\\in\\mathcal{Z}}g(\\mathbf{z}),\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "where $f,g$ are convex and continuous functions and ${\\mathcal{Z}}\\subseteq\\mathbb{R}^{n}$ is a compact convex set. Such a problem is often referred to as \u201csimple bilevel optimization\u201d in the literature, as the upper-level objective function $f$ is minimized over the solution set of a lower-level problem. It captures a hierarchical structure and thus has many applications in machine learning, including lexicographic optimization [12, 14] and lifelong learning [12, 17]. Understanding the structure of simple bilevel optimization and designing efficient algorithms for it is vital and has gained massive attention in recent years [1, 5, 6, 10\u201313, 18, 22\u201326]. ", "page_idx": 0}, {"type": "text", "text": "To solve the problem, one may observe that Problem (1) is equivalent to the convex optimization problem $\\mathrm{min}_{\\mathbf{x}\\in\\mathcal{X}_{g}^{*}}\\,f(\\mathbf{x})$ with implicitly defined convex domain $\\mathcal{X}_{g}^{*}$ . Hence, it is natural to try to design first-order methods to find $\\hat{\\mathbf{x}}\\in\\mathbb{R}^{n}$ such that ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|f(\\hat{\\mathbf{x}})-f^{*}|\\leq\\epsilon_{f},\\quad g(\\hat{\\mathbf{x}})-g^{*}\\leq\\epsilon_{g},}\\end{array}\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "where $f^{*}$ is the optimal value of Problem (1) and $g^{*}$ is the optimal value of the lower-level problem. We highlight the asymmetry in $f$ and $g$ here. $f^{*}$ is the minimal in a constrained set, and hence it is possible that $f(\\mathbf{x})<f^{*}$ for some $\\mathbf{x}\\in\\mathbb{R}^{n}$ . On the other hand, $g^{*}$ is globally minimal and hence $g^{*}\\leq g(\\mathbf{x})$ for any $\\mathbf{x}\\in\\mathbb{R}^{n}$ . Such asymmetry is natural as the role of $f,g$ are inherently asymmetrical for bilevel problems. We call such $\\hat{\\bf x}$ a $(\\epsilon_{f},\\epsilon_{g})$ -absolute optimal solution. ", "page_idx": 0}, {"type": "text", "text": "When $\\mathcal{X}_{g}^{*}$ is explicitly given, finding such a solution is easy as we can apply methods for constrained optimization problems such as the projected gradient method and Frank-Wolfe method [4, Section 3]. Yet, somewhat surprisingly, our first contribution in this paper (Theorem 4.1 and 4.2) shows that it is generally intractable for any zero-respecting first-order method to find absolute optimal solutions for Problem (1). Our negative result shows the fundamental difficulty of simple bilevel problems compared to classical constrained optimization problems. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "As a compromise, most approaches developed for simple bilevel optimization in the literature aim to find a solution $\\hat{\\mathbf{x}}\\in\\mathbb{R}^{n}$ such that ", "page_idx": 1}, {"type": "equation", "text": "$$\nf(\\hat{\\mathbf{x}})-f^{*}\\leq\\epsilon_{f},\\quad g(\\hat{\\mathbf{x}})-g^{*}\\leq\\epsilon_{g},\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "which we call a $(\\epsilon_{f},\\epsilon_{g})$ -weak optimal solution. Much progress has been achieved towards this goal [10, 17, 18, 22\u201324]. We note all the above algorithms fall in the class of zero-respecting algorithms (Assumption 3.4) and hence cannot obtain absolute optimal solutions. ", "page_idx": 1}, {"type": "text", "text": "Our second contribution pushes this boundary by proposing near-optimal lower and upper bounds. We study two settings: (a) $f$ is $C_{f}$ -Lipschitz and $g$ is $C_{g}$ -Lipschitz, (b) $f$ is $L_{f}$ -smooth and $g$ is $L_{g}$ -smooth. We can extend the worst-case functions for single-level optimization to Problem (1) to show lower bounds of ", "page_idx": 1}, {"type": "text", "text": "Given our constructed lower bounds, we further improve known upper bounds by reducing the task of finding $(\\epsilon_{f},\\epsilon_{g})$ -weak optimal solutions to minimizing the functionally constrained problem: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{x}\\in\\mathcal{Z}}f(\\mathbf{x}),\\quad\\mathrm{s.t.}\\quad\\tilde{g}(\\mathbf{x})\\triangleq g(\\mathbf{x})-\\hat{g}^{*}\\leq0,\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Then we further leverage the reformulation by Nesterov [19, Section 2.3.4] which relates the optimal value of Problem (4) to the minimal root of the following auxiliary function, where a discrete minimax problem defines the function value: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\psi^{*}(t)=\\operatorname*{min}_{\\mathcal{Z}}\\left\\{\\psi(t,\\mathbf{x})\\triangleq\\operatorname*{max}\\left\\{f(\\mathbf{x})-t,\\tilde{g}(\\mathbf{x})\\right\\}\\right\\}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Based on this reformulation, we introduce a novel method FC-BiO (Functionally Constrained Bilevel Optimizer). FC-BiO is a double-loop algorithm. It adopts a bisection procedure on $t$ in the outer loop and applies gradient-based methods to solve the sub-problem (5). Our algorithms achieve the following upper bounds: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{\\mathcal{O}}\\left(\\operatorname*{max}\\left\\{C_{f}^{2}/\\epsilon_{f}^{2},C_{g}^{2}/\\epsilon_{g}^{2}\\right\\}\\right)\\,\\mathrm{for\\,the~setup~(a)};}\\\\ &{\\tilde{\\mathcal{O}}\\left(\\operatorname*{max}\\left\\{\\sqrt{L_{f}/\\epsilon_{f}},\\sqrt{L_{g}/\\epsilon_{g}}\\right\\}\\right)\\,\\mathrm{for\\,the~setup~(b)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\tilde{\\mathcal{O}}$ hides logarithmic terms. Both complexity upper bounds match the corresponding lower bounds up to logarithmic factors. In words, we summarize our contributions as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We prove the intractability for any zero-respecting first-order methods to find a $(\\epsilon_{f},\\epsilon_{g})$ - absolute optimal solution of simple bilevel problems.   \n\u2022 We propose a novel method FC-BiO that has near-optimal rates for finding $(\\epsilon_{f},\\epsilon_{g})$ -weak optimal solutions of both nonsmooth and smooth problems. ", "page_idx": 1}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In the literature, various methods [1, 3, 6, 10\u201313, 17, 18, 22\u201326] have been proposed to achieve a $(\\epsilon_{f},\\epsilon_{g})$ -weak optimal solution defined as Equation (3) . Below, we review the existing methods with non-asymptotic convergence. For ease of presentation, we state the results for $\\epsilon_{f}=\\epsilon_{g}=\\epsilon$ . ", "page_idx": 1}, {"type": "text", "text": "Prior results on Lipschitz problems Kaushik and Yousefian [13] proposed the averaging iteratively regularized gradient method (a-IRG) for convex optimization with variational inequality constraints, of which Problem (1) is a special case. a-IRG achieves the rate of ${\\mathcal O}(1/\\epsilon^{4})$ . Shen et al. [24] proposed a method for solving Problem (1) with $\\mathcal{O}(1/\\epsilon^{3})$ complexity based on the online learning framework. When $f$ is Lipschitz and $g$ is smooth, Merchav and Sabach [18] proposed a gradient-based algorithm with $\\mathcal{O}(1/\\epsilon^{1/(1-\\alpha)})$ complexity for any $\\alpha\\in(0.5,1)$ . However, none of these methods can achieve the optimal rate of $\\mathcal{O}(\\epsilon^{-\\dot{2}})$ . ", "page_idx": 2}, {"type": "text", "text": "Prior results on smooth problems Samadi et al. [23] proposed the regularized accelerated proximal method (R-APM) with a complexity of $\\mathcal{O}(1/\\epsilon)$ . Un\u221ader the additional weak sharp minima condition on $g$ , the complexity of R-APM improves to $\\mathcal{O}\\left(1/\\sqrt{\\epsilon}\\right)$ . However, this condition is often too strong and does not hold for many problems. Chen et al. [10] extended the result of [23] to the more general $\\alpha$ -H\u00f6lderian error bound condition. However, their method achieves the optimal rate only when $\\alpha=1$ , which reduces to the weak sharp minima condition. Jiang et al. [12] developed a conditional gradient type algorithm (CG-BiO) with a complexity of $\\mathcal{O}\\left(1/\\epsilon\\right)$ , which approximates $\\chi_{g_{.}}^{*}$ similar to the cutting plane approach. Later on, Cao et al. [6] pro\u221aposed an accelerated algorithm with a similar cutting plane approach to achieve the rate of $\\mathcal{O}(\\Bar{1}/\\Bar{\\epsilon})$ under the weak sharp minima condition. Recently, Wang et al. [27] reduced Problem (1) to finding the smallest $c$ such that the optimal value of the following parametric problem is $g^{\\ast}\\colon\\operatorname*{min}_{\\mathbf{x}\\in\\mathbb{R}^{n}}{g(\\mathbf{x})}$ , s.t. $f(\\mathbf{x})\\leq c$ . They adopted a bisection method to find such a $c$ . To solve this parametric problem, Accelerated Proximal Gradient method is applied on $g$ with projection operator onto the sublevel set $\\mathcal{F}_{c}=\\left\\{\\mathbf{x}\\mid f(\\mathbf{x})\\leq c\\right\\}$ , which we call sublevel set oracles, which we call sublevel set oracles. This leads to an upper bound of $\\tilde{\\mathcal{O}}(1/\\sqrt{\\epsilon})$ Such an oracle is obtainable for norm-like functions such as $\\begin{array}{r}{f(\\mathbf{x})=\\frac{1}{2}\\|\\mathbf{x}\\|^{2}}\\end{array}$ . However, projection onto sublevel sets may be hard for more general functions, such as MSE loss or l\u221aogistic loss. Compared with prior works [6, 10, 23, 27], our proposed methods achieve the $\\tilde{\\mathcal{O}}(1/\\sqrt{\\epsilon})$ rate under standard assumptions, without relying on the weak sharp minima condition or the sublevel set oracles. ", "page_idx": 2}, {"type": "text", "text": "Comparison with Nesterov\u2019s methods for functionally constrained problems Based on similar reformulation, Nesterov [19] hasve proposed algorithms for functionally constrained problems, of which Problem (4) is a special case: one for smooth problems in Section 2.3.5, and one for Lipschitz problems in Section 3.3.4. However, the first algorithm relies on the strong convexity of $f$ and $\\tilde{g}$ , which does not hold in our setups. Moreover, both algorithms require quadratic program (QP) sub-solvers, which are inefficient in large-scale problems: the first algorithm uses it to iteratively move $t$ to the root of $\\psi^{*}(t)$ , while the second algorithm uses it to project onto the sublevel set of a piecewise linear approximation of $\\psi(t,\\bf{x})$ . In contrast, our proposed algorithms have the following advantages: (a) require no strong convexity assumption; (b) require no QP solvers that lead to additional computational cost; (c) have a unified framework for both smooth and Lipschitz problems. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "For any $\\mathbf{x}\\in\\mathbb{R}^{n}$ , let $\\mathbf{x}_{[j]}$ represent the $j$ -th coordinate of $\\mathbf{x}$ for $j=1,\\cdot\\cdot\\cdot,n$ . We use $\\mathrm{supp}(\\mathbf{x}):=\\{j\\in$ $[d]\\;:\\;\\mathbf{x}_{[j]}\\neq0\\}$ to denote the support of $\\mathbf{x}$ . The Euclidean ball centered at $\\mathbf{x}$ with radius $R$ is denoted as ${\\mathcal{B}}(\\mathbf{x},R)\\triangleq\\{\\mathbf{y}\\ |\\ \\|\\mathbf{y}-\\mathbf{x}\\|_{2}\\leq R\\}$ . For any closed convex set ${\\mathcal{C}}\\subseteq\\mathbb{R}^{n}$ , the Euclidean projection of $\\mathbf{x}$ onto $\\mathcal{C}$ is denoted by $\\begin{array}{r}{\\Pi_{\\mathcal{C}}(\\mathbf{x})\\triangleq\\arg\\operatorname*{min}_{\\mathbf{y}\\in\\mathcal{C}}\\|\\mathbf{y}-\\mathbf{x}\\|_{2}}\\end{array}$ . We say a function $h$ is $C$ -Lipschitz in domain $\\mathcal{Z}$ if $\\|h(\\mathbf{x})-h(\\mathbf{y})\\|_{2}\\leq C\\|\\mathbf{x}-\\mathbf{y}\\|_{2}$ for all $\\mathbf{x},\\mathbf{y}\\in{\\mathcal{Z}}$ . We say a differentiable real-valued function $h$ is $L$ -smooth if it has $L$ -Lipschitz continuous gradients. ", "page_idx": 2}, {"type": "text", "text": "We now state the assumptions required in our theoretical results. ", "page_idx": 2}, {"type": "text", "text": "Assumption 3.1. Consider Problem $(I)$ . We assume that ", "page_idx": 2}, {"type": "text", "text": "1. Functions $f$ and $g:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ are convex and continuous.   \n2. The feasible set $\\mathcal{Z}$ is convex and compact with radius $\\begin{array}{r}{D=\\operatorname*{sup}_{\\mathbf{x},\\mathbf{y}\\in\\mathcal{Z}}||\\mathbf{x}-\\mathbf{y}||_{2}}\\end{array}$ . ", "page_idx": 2}, {"type": "text", "text": "The compactness assumption ensures that the subprocesses adopted in our method have a unified upper complexity bound (see Section 5.3). We note that other works involving bisection procedures, such as Wang et al. [27], may also need to address this issue to derive an explicit dependence on the distance term (although they did not state it formally in their paper). For unconstrained problems, if we know that the initial distance $\\lVert\\mathbf{x}^{*}-\\mathbf{x}_{0}\\rVert_{2}$ is upper bounded by $R$ , we can simply take $\\mathcal{Z}=B(\\mathbf{x}_{0},R)$ . ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "We use Assumption 3.1 throughout this paper, but distinguish the following two different settings. ", "page_idx": 3}, {"type": "text", "text": "Assumption 3.2. Consider Problem $(I)$ . We assume that $f$ and $g$ are $L_{f}$ -smooth and $L_{g}$ -smooth respectively. We call such problems $(L_{f},L_{g})$ -smooth problems. ", "page_idx": 3}, {"type": "text", "text": "Assumption 3.3. Consider Problem $(I)$ . We assume that $f$ and $g$ are $C_{f}$ -Lipschitz and $C_{g}$ -Lipschitz in $\\mathcal{Z}$ respectively. We call such problems $(C_{f},C_{g})$ -Lipschitz problems. ", "page_idx": 3}, {"type": "text", "text": "To study the complexity of solving Problem (1), we make the following assumption on the algorithms. ", "page_idx": 3}, {"type": "text", "text": "Assumption 3.4 (zero-respecting algorithm class). An iterative method $\\boldsymbol{\\mathcal{A}}$ can access the objective functions $f$ and $g$ only through a first-order black-box oracle, which takes a test point $\\hat{\\bf x}$ as the input and returns $\\partial f({\\hat{\\mathbf{x}}}),\\partial g({\\hat{\\mathbf{x}}})$ , where $\\partial f({\\hat{\\mathbf{x}}}),\\partial g({\\hat{\\mathbf{x}}})$ are arbitrary subgradients of the objective functions at \u02c6x. $\\boldsymbol{\\mathcal{A}}$ generates a sequence of test points $\\{\\mathbf{x}_{k}\\}_{k=0}^{K}$ with ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{supp}(\\mathbf{x}_{t+1})\\subseteq\\mathrm{supp}(\\mathbf{x}_{0})\\cup\\left(\\bigcup_{0\\leq s\\leq t}\\operatorname{supp}\\left(\\partial f(\\mathbf{x}_{s})\\right)\\cup\\mathrm{supp}(\\partial g(\\mathbf{x}_{s}))\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "This assumption generalizes the standard definition of zero-respecting algorithm for singlelevel minimization problems [7, 19]. Many existing methods that incorporate a gradient step in the update for Problem (1) clearly fall within this class of algorithms, including those proposed by [10, 17, 18, 22\u201324], since the gradient step ensures that $\\mathbf{x}_{k}\\in\\mathrm{~\\bf~x_{0}~}+$ $\\mathrm{Span}\\big\\{\\bar{\\partial}f(\\mathbf{\\bar{x}}_{0}),\\partial g(\\mathbf{\\bar{x}}_{0}),\\dots,\\partial f(\\mathbf{x}_{k-1}),\\partial g(\\mathbf{x}_{k-1})\\big\\}$ . In the appendix, we show that the proposed algorithm in this paper and the conditional gradient type methods [5, 12] also satisfy the condition (6) when the domain is a Euclidean ball centered at $\\mathbf{x}_{\\mathrm{0}}$ (see Proposition C.1 and Remark C.1). ", "page_idx": 3}, {"type": "text", "text": "The following concept of first-order zero-chain, introduced by Nesterov [19, Section 2.1.2], plays an essential role in proving lower bounds for zero-respecting algorithms. In our paper, we leverage the chain-like structure to show the intractability of finding absolute optimal solutions. ", "page_idx": 3}, {"type": "text", "text": "Definition 3.1 (first-order zero-chain). We call a differentiable function $h(\\mathbf{x}):\\mathbb{R}^{q}\\rightarrow\\mathbb{R}$ a first-order zero-chain if for any sequence $\\{{\\bf x}_{k}\\}_{k\\ge0}$ satisfying ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{supp}(\\mathbf{x}_{t+1})\\subseteq\\bigcup_{0\\leq s\\leq t}\\mathrm{supp}\\left(\\nabla h(\\mathbf{x}_{s})\\right),\\ k\\geq1;\\quad\\mathbf{x}_{0}=\\mathbf{0},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "it holds that $\\mathbf{x}_{k,[j]}=0,k+1\\leq j\\leq q$ . ", "page_idx": 3}, {"type": "text", "text": "Definition 3.1 defines differentiable zero-chain functions. We can similarly define non-differentiable zero-chain, by requiring $\\operatorname{supp}(\\mathbf{x}_{t+1})$ to be in $[\\cup_{0\\leq s\\leq t}\\operatorname{supp}\\left(\\partial h(\\mathbf{x}_{s})\\right)$ , where $\\partial h(\\mathbf{x})$ is a (possibly adversarial) subgradient of $h$ at $\\mathbf{x}$ . ", "page_idx": 3}, {"type": "text", "text": "4 Finding absolute optimal solutions is hard ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Faced with Problem (1), a natural initial response is to seek an approximate solution $\\hat{\\bf x}$ such that $f({\\hat{\\mathbf{x}}})$ is as close to $f^{*}$ as possible, under the premise that $g(\\hat{\\mathbf{x}})$ is close to $g^{\\ast}$ . Such a goal is captured by the concept of $(\\epsilon_{f},\\epsilon_{g})$ -absolute optimal solutions as defined in (2). However, it turns out that finding a $(\\epsilon_{f},\\epsilon_{g})$ -absolute optimal solution is intractable for any zero-respecting first-order methods in both smooth and Lipschitz problems as shown in the following theorems. ", "page_idx": 3}, {"type": "text", "text": "Theorem 4.1. For any first-order algorithm $\\boldsymbol{\\mathcal{A}}$ satisfying Assumption 3.4 that runs for $T$ iterations and any initial point $\\mathbf{x}_{\\mathrm{0}}$ , there exists a $(1,1)$ -smooth instance of Problem $(I)$ such that the optimal solution $\\mathbf{x}^{*}$ satisfies $\\|\\mathbf{x}_{0}-\\mathbf{x}^{*}\\|_{2}\\leq1$ and $\\begin{array}{r}{|\\mathbf{\\tilde{\\itf}}(\\mathbf{x}_{0})-\\mathbf{\\nabla}f^{*}|\\,\\geq\\,\\frac{1}{48}}\\end{array}$ . For the iterates $\\{{\\bf x}_{k}\\}_{k=0}^{T}$ generated by $\\mathcal{A}$ , the following holds: ", "page_idx": 3}, {"type": "equation", "text": "$$\nf(\\mathbf{x}_{k})=f(\\mathbf{x}_{0}),\\quad\\forall1\\leq k\\leq T.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Theorem 4.2. For any first-order algorithm $\\boldsymbol{\\mathcal{A}}$ satisfying Assumption 3.4 that runs for $T$ iterations and any initial point $\\mathbf{x}_{\\mathrm{0}}$ , there exists $a$ $(1,1)$ -Lipschitz instance of Problem $(I)$ and some adversarial subgradients $\\{\\partial f(\\mathbf{x}_{k}),\\partial g(\\mathbf{x}_{k})\\}_{k=0}^{T-1}$ such that the optimal solution $\\mathbf{x}^{*}$ satisfies $\\|\\mathbf{x}_{0}-\\mathbf{x}^{*}\\|_{2}\\leq1$ and $\\begin{array}{r}{|f(\\mathbf{x}_{0})-f^{*}|\\geq\\frac{1}{4}}\\end{array}$ . For the iterates $\\{{\\bf x}_{k}\\}_{k=0}^{T}$ generated by $\\mathcal{A}$ , the following holds ", "page_idx": 3}, {"type": "equation", "text": "$$\nf(\\mathbf{x}_{k})=f(\\mathbf{x}_{0}),\\quad\\forall1\\leq k\\leq T.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The proofs of Theorem 4.1 and Theorem 4.2 rely on the concept of worst-case convex zero-chain (Proposition A.1 and A.2). We show that for any first-order zero-respecting algorithm that runs for $T$ iterations, there exists a \u201chard instance\u201d such that $f(\\mathbf{x}_{t})$ remains unchanged from the initial value $f(\\mathbf{x}_{0})$ throughout the entire process. The complete proof is provided in Appendix A. ", "page_idx": 4}, {"type": "text", "text": "The constructions of the above hardness results are motivated by the work [9], which demonstrated that for general bilevel optimization problems of the form $\\begin{array}{r}{\\operatorname*{min}_{\\mathbf{x}\\in\\mathbb{R}^{n}},\\mathbf{y}\\!\\in\\!\\mathbb{R}^{m}\\mathbf{\\Omega}^{f}(\\mathbf{x},\\mathbf{y})}\\end{array}$ subject to $\\textbf{y\\in}$ arg $\\operatorname*{min}_{\\mathbf{z}\\in\\mathbb{R}^{m}}$ $g(\\mathbf{x},\\mathbf{z})$ , there exists a \"hard instance\" in which any zero-respecting algorithm always yields $\\mathbf{x}_{k}=\\mathbf{x}_{0}$ for all $1\\leq k\\leq T$ . Although our construction has a very similar high-level idea to [9], the $f$ and $g$ we construct are different from the functions in [9] since our desired conclusion is different. ", "page_idx": 4}, {"type": "text", "text": "Remark 4.1. Some previous works [6, 12, 23] provide guarantees for finding $(\\epsilon_{f},\\epsilon_{g})$ -absolute optimal solutions. However, these works assume an additional H\u00f6lderian error bound condition $[20]$ on g. Our near-optimal methods for finding weak optimal solutions, proposed in the next section, also work well under this additional assumption and achieve the best-known convergence rate for absolute suboptimality both in smooth and Lipschitz settings. See Appendix D for further discussions. ", "page_idx": 4}, {"type": "text", "text": "5 Near-optimal methods for weak optimal solutions ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Due to the intractability of obtaining $(\\epsilon_{f},\\epsilon_{g})$ -absolute optimal solutions of Problem (1), most existing works focus on developing first-order methods to find $(\\epsilon_{f},\\epsilon_{g})$ -weak optimal solutions as defined in (3). In this section, we establish the lower complexity bounds for finding weak optimal solutions and propose a new framework for simple bilevel problems named Functionally Constrained Bilevel Optimizer (FC-BiO) that achieves near-optimal convergence in both Lipschtitz and smooth settings. ", "page_idx": 4}, {"type": "text", "text": "5.1 Lower complexity bounds ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We first establish the lower complexity bounds for finding a $(\\epsilon_{f},\\epsilon_{g})$ -weak optimal solution of $(L_{f},L_{g})$ -smooth problems and $(C_{f},C_{g})$ -Lipschitz problems. The results follow directly from existing lower bounds for single-level optimization problems, as simple bilevel optimization is a more general framework. Although the proof is straightforward, we present the results because establishing a precise lower bound is essential for demonstrating that an algorithm is truly near-optimal. ", "page_idx": 4}, {"type": "text", "text": "Theorem 5.1. Given $L_{f},L_{g},D\\ >\\ 0$ . For any first-order algorithm $\\boldsymbol{\\mathcal{A}}$ satisfying Assumption 3.4 and any initial point $\\mathbf{x}_{\\mathrm{0}}$ , there exists a $\\left(L_{f},L_{g}\\right)$ -smooth instance of Problem (1) on the domain $\\mathcal{Z}\\,=\\,B(\\mathbf{x}_{0},D)$ such that the optimal solution $\\mathbf{x}^{*}$ is contained in $\\mathcal{Z}$ and $\\boldsymbol{\\mathcal{A}}$ needs at least $\\begin{array}{r}{\\Omega\\left(\\operatorname*{max}\\left\\{\\sqrt{\\frac{L_{f}}{\\epsilon_{f}}},\\sqrt{\\frac{L_{g}}{\\epsilon_{g}}}\\right\\}D\\right)}\\end{array}$ iterations to find a $(\\epsilon_{f},\\epsilon_{g})$ -weak optimal solution. ", "page_idx": 4}, {"type": "text", "text": "Theorem 5.2. Given $C_{f},C_{g},D\\ >\\ 0.$ . For any first-order algorithm $\\boldsymbol{\\mathcal{A}}$ satisfying Assumption 3.4 and any initial point $\\mathbf{x}_{\\mathrm{0}}$ , there exists a $\\left(C_{f},C_{g}\\right)$ -Lipschitz instance of Problem (1) on the domain $\\mathcal{Z}\\,=\\,B(\\mathbf{x}_{0},D)$ such that the optimal solution $\\mathbf{x}^{*}$ is contained in $\\mathcal{Z}$ and $\\boldsymbol{\\mathcal{A}}$ needs at least $\\Omega\\left(\\operatorname*{max}\\left\\{\\frac{C_{f}^{2}}{\\epsilon_{f}^{2}},\\frac{C_{g}^{2}}{\\epsilon_{g}^{2}}\\right\\}D^{2}\\right)$ iterations to find a $(\\epsilon_{f},\\epsilon_{g})$ -weak optimal solution. ", "page_idx": 4}, {"type": "text", "text": "5.2 Our proposed algorithms ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We now present a unified framework applicable to both smooth and Lipschitz problems. The proposed algorithms nearly match the lower complexity bounds in both settings. ", "page_idx": 4}, {"type": "text", "text": "Problem reformulation We apply two steps of reformulation. First, we relax Problem (1) to Problem (4), where the constraint $\\mathbf{x}\\in\\mathcal{X}_{g}^{*}$ is replaced by a relaxed functional constraint $\\tilde{g}(\\mathbf{x})$ \u225c $g(\\mathbf{x})-\\hat{g}^{*}\\leq0$ . Denoting ${\\hat{f}}^{*}$ as the optimal value of Problem (4), the following lemma holds: ", "page_idx": 4}, {"type": "text", "text": "Lemma 5.1. If $\\begin{array}{r}{g^{*}\\le\\hat{g}^{*}\\le g^{*}+\\frac{\\epsilon_{g}}{2}}\\end{array}$ and \u02c6x is a $\\left(\\epsilon_{f},\\epsilon_{g}/2\\right)$ -weak optimal solution to Problem (4), i.e.   \n$f(\\hat{\\mathbf{x}})\\leq\\hat{f}^{*}+\\epsilon_{f},\\tilde{g}(\\hat{\\mathbf{x}})\\leq\\epsilon_{g}/2$ , then x\u02c6 is a $(\\epsilon_{f},\\epsilon_{g})$ -weak optimal solution to Problem $(I)$ . ", "page_idx": 4}, {"type": "text", "text": "Therefore, it suffices to pursue an approximate solution of Problem (4). Second, Problem (4) is further reduced to the problem of finding the smallest root of the following auxiliary function: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\psi^{*}(t)=\\operatorname*{min}_{\\mathbf{x}\\in\\mathcal{Z}}\\left\\{\\psi(t,\\mathbf{x})\\triangleq\\operatorname*{max}\\left\\{f(\\mathbf{x})-t,\\tilde{g}(\\mathbf{x})\\right\\}\\right\\}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Algorithm 1 Functionally Constrained Bilevel Optimizer (FC-BiO) ", "page_idx": 5}, {"type": "text", "text": "Require: Problem parameters $\\mathbf{x}_{0},D$ , desired accuracy $\\epsilon$ , total number of iterations $T$ , initial bounds   \n$\\ell,u$ , and a subroutine for Problem (7) $\\mathcal{M}$ .   \n1: Set $\\begin{array}{r}{N=\\left\\lceil\\log_{2}\\frac{u-\\ell}{\\epsilon/2}\\right\\rceil}\\end{array}$ , $K=T/N$ . Set $\\bar{\\bf x}={\\bf x}_{0}$ .   \n2: for $k=0,\\cdot\\cdot\\cdot,N-1$ do   \n3: Set $\\textstyle t={\\frac{\\ell+u}{2}}$ .   \n4: Solve with the subroutine $(\\hat{\\mathbf{x}}_{(t)},\\hat{\\psi}^{*}(t))=\\mathcal{M}(\\bar{\\mathbf{x}},D,t,K)$ .   \n5: Set \u00afx = \u02c6x(t)   \n6: if $\\hat{\\psi}^{*}(t)>\\frac{\\epsilon}{2}$ then   \n7: Set $\\ell=t$ .   \n8: else   \n9: Set $u=t$ .   \n10: end if   \n11: end for   \n12: return $\\hat{\\mathbf{x}}=\\hat{\\mathbf{x}}_{(u)}$ as the approximate solution. ", "page_idx": 5}, {"type": "text", "text": "Such reformulation is introduced in Nesterov [19, Section 2.3] with the following characterization. ", "page_idx": 5}, {"type": "text", "text": "Lemma 5.2 (Nesterov [19, Lemma 2.3.4]). Let $\\hat{f}^{*}$ be the optimal value of Problem (4), and let $\\psi^{*}(t)$ be the auxiliary function as defined in (7). The following holds: ", "page_idx": 5}, {"type": "text", "text": "1. $\\psi^{*}(t)$ is continuous, decreasing, and Lipschitz with constant 1.   \n2. $\\hat{f}^{*}$ is exactly the smallest root of $\\psi^{*}(t)$ . ", "page_idx": 5}, {"type": "text", "text": "Bisection procedure Based on the preceding reformulation, we propose Algorithm 1 (FC-BiO), which uses a bisection procedure to estimate the smallest root of $\\psi^{*}(\\cdot)$ . For now, we assume that the desired accuracy on upper-level and lower-level problems is the same, (i.e. $\\epsilon_{f}=\\epsilon_{g}=\\epsilon_{,}$ ). Later we will show in Corollary 5.1 that we can handle the case when $\\epsilon_{f}\\neq\\epsilon_{g}$ by simply scaling the objectives. ", "page_idx": 5}, {"type": "text", "text": "Algorithm 1 applies the bisection method within an initial interval $[\\ell,u]$ which contains the smallest root, ${\\hat{f}}^{*}$ , for $\\begin{array}{r}{\\bar{N}=\\left\\lceil\\log_{2}\\frac{u-\\ell}{\\epsilon/2}\\right\\rceil}\\end{array}$ iterations. Similar to [27], the initial interval can be obtained by applying single-level first-order methods. The lower bound $\\ell$ is obtained by solving the global minimum of the upper-level objective $f$ , while $u=f(\\hat{\\mathbf{x}}_{g})$ serves as a valid upper bound, where $\\hat{\\mathbf{x}}_{g}$ is an approximate solution to the lower-level problem. Further details can be found in Appendix B.1. In each iteration, we set $\\textstyle t={\\frac{\\ell+u}{2}}$ . To approximate the function value of $\\psi^{*}(t)$ , we apply a first-order algorithm $\\mathcal{M}$ to solve the discrete minimax problem (7). For the Lipschitz setting, we let $\\mathcal{M}$ be the Subgradient Method (SGM, Algorithm 2) [4, Section 3.1]. For the smooth setting, we let $\\mathcal{M}$ be the generalized accelerated gradient method (generalized AGM, Algorithm 3) [19, Algorithm 2.3.12]. These methods guarantee to find an approximate solution $\\hat{\\mathbf{x}}_{(t)}$ of Problem (7) such that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\psi^{*}(t)\\leq\\hat{\\psi}^{*}(t)\\triangleq\\psi(t,\\hat{\\mathbf{x}}_{(t)})\\leq\\psi^{*}(t)+\\frac{\\epsilon}{2}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "If $\\hat{\\psi}^{*}(t)>\\epsilon/2$ , we update $\\ell=t$ . Conversely, if $\\hat{\\psi}^{\\ast}(t)\\leq\\epsilon/2$ , we set $u=t$ . For the initial point of $\\mathcal{M}$ , we exploit a warm-start strategy (see more details in Appendix B.2). After completing $N$ iterations, we return $\\hat{\\mathbf{x}}\\,=\\,\\hat{\\mathbf{x}}_{(u)}$ as the output. As shown in Lemma 5.3, $\\hat{\\bf x}$ is guaranteed to be a $(\\epsilon,\\epsilon)$ -weak optimal solution to Problem (1). ", "page_idx": 5}, {"type": "text", "text": "We remark that since we can only solve an approximate value of $\\psi^{*}(t)$ , the upper bound $u$ might fall below ${\\hat{f}}^{*}$ during the bisection process. But this is acceptable since we are only in pursuit of a weak optimal solution instead of an absolute optimal solution. ", "page_idx": 5}, {"type": "text", "text": "Lemma 5.3. If $\\hat{g}^{*}$ satisfies $g^{*}\\leq\\hat{g}^{*}\\leq g^{*}+\\frac{\\epsilon}{2}$ and (8) holds for every $t$ in the process of Algorithm 1, then the approximate solution $\\hat{\\bf x}$ returned by Algorithm $^{\\,l}$ is a $(\\epsilon,\\epsilon/2)$ -weak optimal solution to Problem (4), and therefore a $(\\epsilon,\\epsilon)$ -weak optimal solution to Problem $(I)$ by Lemma 5.1. ", "page_idx": 5}, {"type": "text", "text": "According to this lemma, ${\\mathcal O}(\\log(1/\\epsilon))$ iterations of the outer loops are sufficient to find a $(\\epsilon,\\epsilon)$ -weak optimal solution. Next, we will discuss the process and complexity of the subroutines in detail. ", "page_idx": 5}, {"type": "text", "text": "Algorithm 2 Solve Problem (7) with SGM $({\\bf x}_{0},D,t,K)$ ", "page_idx": 6}, {"type": "text", "text": "Require: Problem parameters $\\mathbf{x}_{0},D,t$ , total number of iterations $K$ .   \n1: Set $\\eta=D/(C\\sqrt{K})$ , where $C=\\operatorname*{max}\\{C_{f},C_{g}\\}$ .   \n2: for $k=0,\\cdot\\cdot\\cdot\\,,K-1$ do   \n3: Obtain a subgradient $\\mathbf{s}\\in\\partial_{\\mathbf{x}}\\psi(t,\\mathbf{x})$ by Proposition 5.1.   \n4: Update $\\mathbf{x}_{k+1}=\\Pi_{\\mathcal{Z}}(\\mathbf{x}_{k}-\\eta\\mathbf{s})$ .   \n5: end for   \n6: return \u02c6x(t) = K1 $\\begin{array}{r}{\\hat{\\mathbf{x}}_{(t)}=\\frac{1}{K}\\sum_{k=0}^{K-1}\\mathbf{x}_{k}}\\end{array}$ as the approximate solution and $\\hat{\\psi}^{*}(t)=\\operatorname*{max}\\{f\\bigl(\\hat{\\mathbf{x}}_{(t)}\\bigr)-t,\\tilde{g}\\bigl(\\hat{\\mathbf{x}}_{(t)}\\bigr)\\}$ as the approximate value. ", "page_idx": 6}, {"type": "text", "text": "5.3 Subroutines and total complexity ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To proceed with the bisection process, we need to invoke a subroutine $\\mathcal{M}$ to approximate the function value of $\\begin{array}{r}{\\psi^{*}(t)=\\operatorname*{min}_{\\mathbf{x}\\in\\mathcal{Z}}\\psi(\\bar{t},\\mathbf{x})}\\end{array}$ in each outer iteration, where $\\psi(t,\\mathbf{x})=\\operatorname*{max}\\{f(\\mathbf{x})-t,\\tilde{g}(\\mathbf{x})\\}$ . This reduces to a discrete minimax optimization problem (Problem (7)) for a given $t$ . Below we demonstrate the subroutines to solve this problem in Lipschitz and smooth settings. ", "page_idx": 6}, {"type": "text", "text": "Lipschitz setting When $f$ and $g$ are convex and $C_{f}$ and $C_{g}$ -Lipschitz respectively, it holds that $\\psi(t,\\bf{x})$ is also convex and Lipschitz with constant ma $\\operatorname{x}\\{C_{f},{\\bar{C}}_{g}\\}$ . In this case, setting $\\mathcal{M}$ to be the Subgradient Method (SGM) [4, Section 3.1] applied on $\\psi(t,\\cdot)$ (Algorithm 2) directly achieves the optimal convergence rate. To implement the SGM method, the subgradient of $\\psi(t,\\bf{x})$ needs to be computed as given in the following proposition: ", "page_idx": 6}, {"type": "text", "text": "Proposition 5.1 (Nesterov [19, Lemma 3.1.13]). Consider $\\psi(t,\\mathbf{x})=\\operatorname*{max}\\{f(\\mathbf{x})-t,\\tilde{g}(\\mathbf{x})\\}$ where $f$ and $g$ are convex functions. For given $t$ . We have ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\partial_{\\mathbf{x}}\\psi(t,\\mathbf{x})=\\left\\{\\partial f(\\mathbf{x}),\\begin{array}{l l}&{f(\\mathbf{x})-t>\\tilde{g}(\\mathbf{x});}\\\\ &{f(\\mathbf{x})-t<\\tilde{g}(\\mathbf{x});}\\\\ {\\operatorname{Conv}\\left\\{\\partial f(\\mathbf{x}),\\partial\\tilde{g}(\\mathbf{x})\\right\\},}&{f(\\mathbf{x})-t=\\tilde{g}(\\mathbf{x}).}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Algorithm 2 has the following convergence guarantee: ", "page_idx": 6}, {"type": "text", "text": "Lemma 5.4 (Bubeck et al. [4, Theorem 3.2]). Suppose Assumption 3.1 and 3.3 hold. When $K\\ge$ $\\frac{4D^{2}C^{2}}{\\epsilon^{2}}$ , the approximate value $\\hat{\\psi}^{*}(t)$ produced by Algorithm 2 satisfies $\\psi^{*}(t)\\leq\\hat{\\psi}^{*}(t)\\leq\\psi^{*}(t)+\\frac{\\epsilon}{2}$ , where $C=\\operatorname*{max}\\{C_{f},C_{g}\\}$ . ", "page_idx": 6}, {"type": "text", "text": "We refer to Algorithm 1 with SGM subroutine (Algorithm 2) as $_\\mathrm{FC-BiO^{Lip}}$ . Combining with Lemma 5.3, we obtain the following total iteration complexity of FC-BiOLip: ", "page_idx": 6}, {"type": "text", "text": "Theorem 5.3 (Lipschitz setting). Suppose Assumption 3.1 and 3.3 hold and $\\epsilon_{f}=\\epsilon_{g}=\\epsilon$ . When ", "page_idx": 6}, {"type": "equation", "text": "$$\nT\\ge\\left\\lceil\\log_{2}\\frac{u-\\ell}{\\epsilon/2}\\right\\rceil\\frac{4\\operatorname*{max}\\{C_{f}^{2},C_{g}^{2}\\}}{\\epsilon^{2}}D^{2},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "the approximate solution $\\hat{\\bf x}$ produced by $F C{-}B i O^{L i p}$ is a $(\\epsilon,\\epsilon)$ -weak optimal solution to Problem $(I)$ . ", "page_idx": 6}, {"type": "text", "text": "Smooth setting The optimal first-order method for optimizing smooth objective functions is the celebrated Accelerated Gradient Method (AGM) [19, Section 2.2] proposed by Nesterov. In contrast to the Lipschitz setting, AGM cannot be applied to $\\psi(t,\\mathbf{x})=\\operatorname*{max}\\!\\left\\{\\bar{f}(\\mathbf{x})-\\bar{t},g(\\mathbf{x})\\right\\}$ directly when $f$ and $g$ are convex and smooth, as the smoothness condition no longer holds for $\\psi(t,\\cdot)$ . However, Nesterov [19, Section 2.3] showed that by simply replacing the gradient step in standard AGM with the following gradient mapping (Nesterov [19, Definition 2.3.2]) ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{x}_{k+1}=\\underset{\\mathbf{x}\\in\\mathcal{Z}}{\\arg\\operatorname*{min}}\\left\\{\\bar{\\psi}(t,\\mathbf{x};\\mathbf{y}_{k})\\triangleq\\operatorname*{max}\\left\\{f(\\mathbf{y}_{k})+\\langle\\nabla f(\\mathbf{y}_{k}),\\mathbf{x}-\\mathbf{y}_{k}\\rangle+\\frac{L}{2}\\|\\mathbf{x}-\\mathbf{y}_{k}\\|_{2}^{2}-t,\\right.}\\\\ {\\left.\\tilde{g}(\\mathbf{y}_{k})+\\langle\\nabla\\tilde{g}(\\mathbf{y}_{k}),\\mathbf{x}-\\mathbf{y}_{k}\\rangle+\\frac{L}{2}\\|\\mathbf{x}-\\mathbf{y}_{k}\\|_{2}^{2}\\right\\}\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Algorithm 3 Solve Problem (7) with Generalized AGM $({\\bf x}_{0},D,t,K)$ [19, Algorithm 2.3.12] ", "page_idx": 7}, {"type": "text", "text": "Require: Problem parameters $\\mathbf{x}_{0},D,t$ , total number of iterations $K$ ", "page_idx": 7}, {"type": "text", "text": "1: Set y0 = x0, \u03b10 = $\\begin{array}{r}{\\alpha_{0}^{\\star}=\\frac{1}{2}}\\end{array}$   \n2: for $k=0,\\cdot\\cdot\\cdot K-\\bar{1}$ do   \n3: Compute $\\mathbf{x}_{k+1}$ as the solution to (9) by Proposition 5.2.   \n4: Compute $\\alpha_{k+1}$ from the equation $\\alpha_{k+1}^{2}=(1-\\alpha_{k+1})\\alpha_{k}^{2}$ .   \n5: Set $\\begin{array}{r}{\\beta_{k}=\\frac{\\alpha_{k}(1-\\alpha_{k})}{\\alpha_{k}^{2}+\\alpha_{k+1}}}\\end{array}$ \u03b1\u03b1k2(+1\u03b1\u2212k\u03b1+k1) , yk+1 = xk+1 + \u03b2k(xk+1 \u2212xk). ", "page_idx": 7}, {"type": "text", "text": "6: end for ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "7: return $\\hat{\\mathbf{x}}_{(t)}=\\mathbf{x}_{K}$ as the approximate solution and $\\hat{\\psi}^{*}(t)=\\operatorname*{max}\\{f(\\mathbf{x}_{K})-t,\\tilde{g}(\\mathbf{x}_{K})\\}$ as the approximate value. ", "page_idx": 7}, {"type": "text", "text": "the optimal rate of $O(\\sqrt{L/\\epsilon})$ can be achieved for Problem (7). Here $\\{\\mathbf{x}_{k}\\},\\{\\mathbf{y}_{k}\\}$ are the test point sequences and $L=\\operatorname*{max}\\{L_{f},L_{g}\\}$ . Solving $\\mathbf{x}_{k+1}$ for general discrete minimax problems (where the maximum is taken over potentially more than two objective functions, as studied in Nesterov [19, Section 2.2]), reduces to a quadratic programming (QP) problem and may not be efficiently solvable. However, we demonstrate that in our problem setup, $\\mathbf{x}_{k+1}$ can be expressed in the form of a projection onto the feasible set $\\mathcal{Z}$ , or onto the intersection of $\\mathcal{Z}$ and a hyperplane. ", "page_idx": 7}, {"type": "text", "text": "Proposition 5.2. Define the descent step candidates ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{x}_{1}=\\Pi_{\\mathcal{Z}}\\left(\\mathbf{y}_{k}-\\frac{1}{L}\\nabla f(\\mathbf{y}_{k})\\right),\\quad\\mathbf{x}_{2}=\\Pi_{\\mathcal{Z}}\\left(\\mathbf{y}_{k}-\\frac{1}{L}\\nabla\\tilde{g}(\\mathbf{y}_{k})\\right),}\\\\ &{\\mathbf{x}_{3}=\\Pi_{\\mathcal{Z}\\cap\\mathcal{H}}\\left(\\mathbf{y}_{k}-\\frac{1}{L}\\nabla f(\\mathbf{y}_{k})\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\mathcal{H}\\subset\\mathbb{R}^{n}$ is a hyperplane defined by ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{H}=\\{\\mathbf{x}\\mid f(\\mathbf{y}_{k})-\\tilde{g}(\\mathbf{y}_{k})+\\langle\\nabla f(\\mathbf{y}_{k})-\\nabla\\tilde{g}(\\mathbf{y}_{k}),\\mathbf{x}-\\mathbf{y}_{k}\\rangle-t=0\\}.}\\\\ &{\\mathit{i o n\\,t o\\,(9)\\,i s\\,}\\mathbf{x}_{k+1}=\\arg\\operatorname*{min}_{\\{\\mathbf{x}_{i}|i\\in\\{1,2,3\\}\\}}\\bar{\\psi}(t,\\mathbf{x}_{i};\\mathbf{y}_{k}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Then the solut ", "page_idx": 7}, {"type": "text", "text": "We present the generalized AGM subroutine (Algorithm 3) and its convergence rate below. ", "page_idx": 7}, {"type": "text", "text": "Lemma 5.5 (Nesterov [19, Theorem 2.3.5]). Suppose Assumption 3.1 and 3.2 hold.When $K\\_$ $D\\sqrt{\\frac{12L}{\\epsilon}}$ , the approximate value $\\hat{\\psi}^{*}(t)$ produced by Algorithm 3 satisfies $\\psi^{*}(t)\\leq\\hat{\\psi}^{*}(t)\\leq\\psi^{*}(t)+\\frac{\\epsilon}{2}$ , where $L=\\operatorname*{max}\\{L_{f},L_{g}\\}$ . ", "page_idx": 7}, {"type": "text", "text": "We refer to Algorithm 1 with generalized AGM subroutine (Algorithm 3) as FC-BiOsm, whose total iteration complexity is given by the following theorem: ", "page_idx": 7}, {"type": "text", "text": "Theorem 5.4 (Smooth setting). Suppose Assumption 3.1 and 3.2 hold and $\\epsilon_{f}=\\epsilon_{g}=\\epsilon$ . When ", "page_idx": 7}, {"type": "equation", "text": "$$\nT\\geq\\left\\lceil\\log_{2}\\frac{u-\\ell}{\\epsilon/2}\\right\\rceil\\sqrt{\\frac{12\\operatorname*{max}\\{L_{f},L_{g}\\}}{\\epsilon}}D,\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "the approximate solution \u02c6x produced by $F C{-}B i O^{s m}$ is $a\\left(\\epsilon,\\epsilon\\right)$ -weak optimal solution to Problem $(I)$ . ", "page_idx": 7}, {"type": "text", "text": "For more general cases when the desired accuracy for the upper-level and lower-level problems is different (i.e. $\\epsilon_{f}\\neq\\epsilon_{g}^{\\ }$ ), we can simply scale the objective functions before applying FC-BiOLip or $_\\mathrm{FC-BiO^{sm}}$ , resulting in the following guarantee: ", "page_idx": 7}, {"type": "text", "text": "Corollary 5.1. Suppose Assumption 3.1 holds. By scaling $\\begin{array}{r}{\\tilde{g}^{\\circ}=\\frac{\\epsilon_{f}}{\\epsilon_{g}}\\tilde{g}}\\end{array}$ and applying $F C{-}B i O^{L i p}$ or $F C{-}B i O^{s m}$ on functions $f$ and $\\tilde{g}^{\\circ}$ , a $(\\epsilon_{f},\\epsilon_{g})$ -weak optimal solution to Problem $(I)$ is obtained within the complexity of $\\tilde{\\mathcal{O}}\\left(\\operatorname*{max}\\left\\{\\frac{C_{f}^{2}}{\\epsilon_{f}^{2}},\\frac{C_{g}^{2}}{\\epsilon_{g}^{2}}\\right\\}D^{2}\\right)$ under Assumption 3.3 and $\\begin{array}{r}{\\tilde{\\mathcal{O}}\\left(\\operatorname*{max}\\left\\{\\sqrt{\\frac{L_{f}}{\\epsilon_{f}}},\\sqrt{\\frac{L_{g}}{\\epsilon_{g}}}\\right\\}D\\right)}\\end{array}$ under Assumption 3.2. ", "page_idx": 7}, {"type": "text", "text": "Our proposed algorithms are near-optimal in both Lipschitz and smooth settings as the convergence results in Corollary 5.1 match the lower bounds established in Theorem 5.1 and Theorem 5.2. ", "page_idx": 7}, {"type": "image", "img_path": "PAiGHJppam/tmp/8ee9e39b72c88a1426195e8523c96b9e9b203c0892d3f9746d9b4406e47f3654.jpg", "img_caption": ["Figure 1: The performance of Algorithm 1 compared with other methods in Problem (11). "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "PAiGHJppam/tmp/1c48e963e9dbbe2d777b0ba3c98ff16a52288aae6fe39589a0f981ec5f99b6db.jpg", "img_caption": ["Figure 2: The performance of Algorithm 1 compared with other methods in Problem (12) "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "6 Numerical experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we evaluate our proposed methods on two different bilevel problems with smooth objectives. We compare the performance of $_\\mathrm{FC-BiO^{sm}}$ with existing methods, including a-IRG [13], Bi-SG [18], CG-BiO[12], AGM-BiO [6], PB-APG[10], and Bisec-BiO[27]. The following problems are also Lipschitz on a compact set $\\mathcal{Z}$ , so we implement $_\\mathtt{F C-B i O^{L i p}}$ as well. The initialization time of FC-BiOsm, $_\\mathtt{F C-B i O^{L i p}}$ , CG-BiO, and Bisec-BiO is taken into account and is plotted in the figures. Our implementations of CG-BiO and a-IRG are based on the codes from [12], which is available online1. ", "page_idx": 8}, {"type": "text", "text": "6.1 Minimum norm solution ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "As in [27], we consider the following simple bilevel problem: ", "page_idx": 8}, {"type": "equation", "text": "$$\nf(\\mathbf{x})=\\frac{1}{2}\\|\\mathbf{x}\\|_{2}^{2},\\quad g(\\mathbf{x})=\\frac{1}{2}\\|A\\mathbf{x}-b\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "We set the feasible set $\\mathcal{Z}\\,=\\,\\mathcal{B}(\\mathbf{0},2)$ . We use the Wikipedia Math Essential dataset [21], which contains 1068 instances with 730 attributes. We uniformly sample 400 instances and denote the feature matrix and outcome vector by $A$ and b respectively. We choose the same random initial point $\\mathbf{x}_{\\mathrm{0}}$ for all methods. We set $\\epsilon_{f}=\\epsilon_{g}=10^{-6}$ . For this problem, we can explicitly solve $\\mathbf{x}^{*}$ and $f^{*}$ to measure the convergence. Figure 1 shows the superior performance of our method compared to existing methods in both upper-level and lower-level. The only exception is Bisec-BiO [27], which shows a comparable performance to our method. This also aligns well with the theory as these two methods have the same convergence rates. We remark that the output of our method satisfies that $f({\\hat{\\mathbf{x}}})<f^{*}$ . Thus although $|f(\\bar{\\hat{\\mathbf{x}}})-f^{*}|>\\epsilon_{f}$ , indeed a $(\\epsilon_{f},\\epsilon_{g})$ -weak optimal solution is solved by $_\\mathrm{FC-BiO^{sm}}$ . See more experiment details in Appendix E.1. ", "page_idx": 8}, {"type": "text", "text": "6.2 Over-parameterized logistic regression ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We examine simple bilevel problems where the lower-level and upper-level objectives correspond to the training loss and validation loss respectively. Here we address the logistic regression problem ", "page_idx": 8}, {"type": "text", "text": "using the \u201crcv1.binary\u201d dataset from \u201cLIBSVM\u201d [8, 15], which contains 20, 242 instances with 47, 236 features. We uniformly sample $m=5000$ instances as the training dataset $(A^{t r},\\mathbf{b}^{t r})$ , and $m$ instances as the validation dataset $(A^{v a l},\\mathbf{b}^{v a l})$ . We consider the bilevel problem with: ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\displaystyle{\\bf\\it\\Delta}f({\\bf x})=\\frac{1}{m}\\sum_{i=1}^{m}\\log(1+\\exp(-({\\cal A}_{i}^{v a l})^{\\top}{\\bf x}{\\bf b}_{i}^{v a l}))},}}\\\\ {{\\displaystyle{\\bf\\Delta}g({\\bf x})=\\frac{1}{m}\\sum_{i=1}^{m}\\log(1+\\exp(-({\\cal A}_{i}^{t r})^{\\top}{\\bf x}{\\bf b}_{i}^{t r}))}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "We set the feasible set $\\mathcal{Z}=B(\\mathbf{0},300)$ . We set the initial point $\\mathbf{x}_{0}\\,=\\,{\\mathbf{0}}$ for all methods. We set $\\epsilon_{f}=\\epsilon_{g}=10^{-3}$ . Since projecting to the sublevel set of $f(\\mathbf{x})$ is not practical, Bisec-BiO [27] does not apply to this problem, thus we only consider other methods. To the best of our knowledge, no existing solver could obtain the exact value of $f^{*}$ and $g^{*}$ of Problem (12). Thus we only plot function value $\\bar{f}({\\bf x}_{k})$ and $g(\\mathbf{x}_{k})$ , instead of suboptimality. As shown in Figure 2, our method converges faster than other algorithms in both upper-level and lower-level. More details are provided in Appendix E.2. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion and future work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper provides a comprehensive study of convex simple bilevel problems. We show that find a $(\\epsilon_{f},\\epsilon_{g})$ -absolute optimal solution for such problems is intractable for any zero-respecting first-order algorithm, thus justifying the notion of weak optimal solution considered by existing works. We then propose a novel framework FC-BiO for finding a $(\\epsilon_{f},\\epsilon_{g})$ -weak optimal solution. Our proposed methods achieve the near-optmal rates of $\\tilde{\\mathcal{O}}\\left(\\operatorname*{max}\\{1/\\epsilon_{f}^{2},1/\\epsilon_{g}^{2}\\}\\right)$ and $\\tilde{\\mathcal{O}}\\left(\\operatorname*{max}\\{1/\\sqrt{\\epsilon_{f}},1/\\sqrt{\\epsilon_{g}}\\}\\right)$ for Lipschitz and smooth problems respectively. ", "page_idx": 9}, {"type": "text", "text": "We discuss some limitations unaddressed in this work. First, our method introduces an additional logarithmic factor compared to the lower bounds. We hope future works can further close this gap between upper and lower bounds. Second, our methods cannot be directly applied to stochastic problems [5]. Establishing lower complexity bounds and developing optimal methods for stochastic simple bilevel problems remain an open question for future research. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We would like to express our sincere gratitude to the anonymous reviewers and the area chair for their invaluable feedback and insightful suggestions. In particular, we thank the area chair for suggesting simplifying the proofs of Theorem 5.1 and Theorem 5.2. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Mostafa Amini and Farzad Yousefian. An iterative regularized incremental projected subgradient method for a class of bilevel optimization problems. In 2019 American Control Conference (ACC), pages 4069\u20134074. IEEE, 2019.   \n[2] Michael Arbel and Julien Mairal. Amortized implicit differentiation for stochastic bilevel optimization. In ICLR, 2022.   \n[3] Amir Beck and Shoham Sabach. A first order method for finding minimal norm-like solutions of convex optimization problems. Mathematical Programming, 147(1):25\u201346, 2014.   \n[4] S\u00e9bastien Bubeck et al. Convex optimization: Algorithms and complexity. Foundations and Trends\u00ae in Machine Learning, 8(3-4):231\u2013357, 2015.   \n[5] Jincheng Cao, Ruichen Jiang, Nazanin Abolfazli, Erfan Yazdandoost Hamedani, and Aryan Mokhtari. Projection-free methods for stochastic simple bilevel optimization with convex lower-level problem. In NeurIPS, 2023.   \n[6] Jincheng Cao, Ruichen Jiang, Erfan Yazdandoost Hamedani, and Aryan Mokhtari. An accelerated gradient method for simple bilevel optimization with convex lower-level problem. arXiv preprint arXiv:2402.08097, 2024.   \n[7] Yair Carmon, John C. Duchi, Oliver Hinder, and Aaron Sidford. Lower bounds for finding stationary points i. Mathematical Programming, 184(1):71\u2013120, 2020.   \n[8] Chih-Chung Chang and Chih-Jen Lin. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2:27:1\u201327:27, 2011. Software available at http://www.csie.ntu.edu.tw/\\~cjlin/libsvm.   \n[9] Lesi Chen, Jing Xu, and Jingzhao Zhang. On finding small hyper-gradients in bilevel optimization: Hardness results and improved analysis. In COLT, 2024.   \n[10] Pengyu Chen, Xu Shi, Rujun Jiang, and Jiulin Wang. Penalty-based methods for simple bilevel optimization under Holderian Error Bounds. arXiv preprint arXiv:2402.02155, 2024.   \n[11] Elias S. Helou and Lucas EA Sim\u00f5es. \u03f5-subgradient algorithms for bilevel convex optimization. Inverse problems, 33(5):055020, 2017.   \n[12] Ruichen Jiang, Nazanin Abolfazli, Aryan Mokhtari, and Erfan Yazdandoost Hamedani. A conditional gradient-based method for simple bilevel optimization with convex lower-level problem. In AISTATS, 2023.   \n[13] Harshal D. Kaushik and Farzad Yousefian. A method with convergence rates for optimization problems with variational inequality constraints. SIAM Journal on Optimization, 31(3):2171\u2013 2198, 2021.   \n[14] Matthias Kissel, Martin Gottwald, and Klaus Diepold. Neural network training with safe regularization in the null space of batch activations. In Artificial Neural Networks and Machine Learning, pages 217\u2013228. Springer, 2020.   \n[15] David D Lewis, Yiming Yang, Tony Russell-Rose, and Fan Li. Rcv1: A new benchmark collection for text categorization research. Journal of machine learning research, 5(Apr): 361\u2013397, 2004.   \n[16] Hongzhou Lin, Julien Mairal, and Zaid Harchaoui. Catalyst acceleration for first-order convex optimization: from theory to practice. JMLR, 18(212):1\u201354, 2018.   \n[17] Yura Malitsky. Chambolle-pock and Tseng\u2019s methods: relationship and extension to the bilevel optimization. arXiv preprint arXiv:1706.02602, 2017.   \n[18] Roey Merchav and Shoham Sabach. Convex bi-level optimization problems with nonsmooth outer objective function. SIAM Journal on Optimization, 33(4):3114\u20133142, 2023.   \n[19] Yurii Nesterov. Lectures on convex optimization, volume 137. Springer, 2018.   \n[20] Jong-Shi Pang. Error bounds in mathematical programming. Mathematical Programming, 79 (1):299\u2013332, 1997.   \n[21] Benedek Rozemberczki, Paul Scherer, Yixuan He, George Panagopoulos, Alexander Riedel, Maria Astefanoaei, Oliver Kiss, Ferenc Beres, Guzman Lopez, Nicolas Collignon, et al. Pytorch geometric temporal: Spatiotemporal signal processing with neural machine learning models. In Proceedings of the 30th ACM international conference on information & knowledge management, pages 4564\u20134573, 2021.   \n[22] Shoham Sabach and Shimrit Shtern. A first order method for solving convex bilevel optimization problems. SIAM Journal on Optimization, 27(2):640\u2013660, 2017.   \n[23] Sepideh Samadi, Daniel Burbano, and Farzad Yousefian. Achieving optimal complexity guarantees for a class of bilevel convex optimization problems. arXiv preprint arXiv:2310.12247, 2023.   \n[24] Lingqing Shen, Nam Ho-Nguyen, and Fatma K\u0131l\u0131n\u00e7-Karzan. An online convex optimizationbased framework for convex bilevel optimization. Mathematical Programming, 198(2):1519\u2013 1582, 2023.   \n[25] Mikhail Solodov. An explicit descent method for bilevel convex optimization. Journal of Convex Analysis, 14(2):227, 2007.   \n[26] Mikhail V. Solodov. A bundle method for a class of bilevel nonsmooth convex minimization problems. SIAM Journal on Optimization, 18(1):242\u2013259, 2007.   \n[27] Jiulin Wang, Xu Shi, and Rujun Jiang. Near-optimal convex simple bilevel optimization with a bisection method. In AISTATS, 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Proofs for Section 4 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1 Constructions of first-order zero-chains ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We first give the constructions of the zero-chains that are used in our negative results. These zerochains are the worst-case functions that Nesterov [19] used to prove the lower complexity bounds for first-order methods in single-level optimization. ", "page_idx": 12}, {"type": "text", "text": "Proposition A.1 (Paraphrased from Section 2.1.2 Nesterov [19]). Consider the family of functions $h_{q,L,R}(\\mathbf{x}):\\mathbb{R}^{q}\\rightarrow\\mathbb{R}$ : ", "page_idx": 12}, {"type": "equation", "text": "$$\nh_{q,L,R}(\\mathbf{x})=\\frac{L}{4}\\left(\\frac{1}{2}\\left(\\mathbf{x}_{[1]}^{2}+\\sum_{i=1}^{q-1}(\\mathbf{x}_{[i]}-\\mathbf{x}_{[i+1]})^{2}+\\mathbf{x}_{[q]}^{2}\\right)-R\\mathbf{x}_{[1]}\\right)\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "The following properties hold for any $h_{q,L,R}(\\mathbf{x})$ with $q\\in\\mathbb{N}^{+}$ and $L,R>0$ : ", "page_idx": 12}, {"type": "text", "text": "1. It is a convex and $L$ -smooth function.   \n2. It has the following unique minimizer: $\\begin{array}{r}{\\mathbf{x}_{[j]}^{*}=R\\left(1-\\frac{j}{q+1}\\right)}\\end{array}$ with norm $\\|\\mathbf{x}^{*}\\|_{2}\\leq R\\sqrt{q}$ .   \n3. It is a differentiable first-order zero-chain as defined in Definition 3.1. ", "page_idx": 12}, {"type": "text", "text": "Remark A.1. In Section 2.1.2 of Nesterov $I I9I$ , it is shown that $h_{q,L,1}$ is a convex and $L$ -smooth zero-chain. Here we define $h_{q,L,R}(\\mathbf{x}):=R^{2}h_{q,L,1}(\\frac{\\mathbf{x}}{R})$ . Then $\\nabla^{2}\\bar{h_{q,L,R}}(\\mathbf{x})=\\nabla^{2}h_{q,L,1}(\\frac{\\mathbf{x}}{R})$ , and $h_{q,L,R}(\\mathbf{x})$ is also a convex and $L$ -smooth zero-chain. Similarly, in Proposition A.2, we define $r_{q,C,R}(\\mathbf{x}):=R r_{q,C,1}(\\frac{\\mathbf{x}}{R})$ . Nesterov $I I9J$ showed $r_{q,C,1}$ is a $C$ -Lipschitz zero-chain, which implies that $r_{q,C,R}$ is also a $C$ -Lipschitz zero-chain, since $\\begin{array}{r}{\\dot{\\nabla}r_{q,C,R}(\\mathbf{x})=\\nabla r_{q,C,R}(\\frac{\\mathbf{x}}{R})}\\end{array}$ . ", "page_idx": 12}, {"type": "text", "text": "Proposition A.2 (Paraphrased from Section 3.2.1 Nesterov [19]). Consider the family of functions $r_{q,L,R}(\\mathbf{x}):\\mathbb{R}^{q}\\rightarrow\\mathbb{R}$ : ", "page_idx": 12}, {"type": "equation", "text": "$$\nr_{q,C,R}(\\mathbf{x})=\\frac{C\\sqrt{q}}{1+\\sqrt{q}}\\operatorname*{max}_{1\\leq j\\leq q}\\mathbf{x}_{[j]}+\\frac{C}{2R(1+\\sqrt{q})}\\|\\mathbf{x}\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "The following properties hold for any $r_{q,L,R}(\\mathbf{x})$ with $q\\in\\mathbb{N}^{+}$ and $C,R>0$ : ", "page_idx": 12}, {"type": "text", "text": "1. It is a convex function and is $C$ -Lipschitz in the Euclidean ball $B(\\mathbf{0},R)$ .   \n2. It has a unique minimizer $\\mathbf{x}^{*}=-\\frac{R}{\\sqrt{q}}\\mathbf{1}$ .   \n3. It is a non-differentiable first-order zero-chain. ", "page_idx": 12}, {"type": "text", "text": "Remark A.2. The non-differentiable first-order zero-chain presented in Proposition A.2 is slightly different from the one given in Nesterov $I I^{Q}J.$ . $r_{q,C,R}(\\cdot)$ is Lipschitz in $B(\\mathbf{0},R)$ , while the zero-chain presented in Nesterov [19] is Lipschitz in $B(\\mathbf{x}^{*},R)$ , where $\\mathbf{x}^{*}$ is the minimizer of the zero-chain. ", "page_idx": 12}, {"type": "text", "text": "A.2 Proofs of Theorem 4.1 and Theorem 4.2 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Proof of Theorem 4.1. Consider any first-order algorithm $\\boldsymbol{\\mathcal{A}}$ satisfying Assumption 3.4 that runs for $T$ iterations. Without loss of generality, we assume the initial point of $\\boldsymbol{\\mathcal{A}}$ is $\\mathbf{x}_{0}=\\mathbf{0}$ . Otherwise, we can translate the following construction to $f(\\mathbf{x}-\\mathbf{x}_{0}),g(\\mathbf{x}-\\mathbf{x}_{0})$ . Let $q=2T$ and ", "page_idx": 12}, {"type": "equation", "text": "$$\nf(\\mathbf{x})=\\frac{1}{2}\\sum_{j=T+1}^{q}\\mathbf{x}_{[j]}^{2},\\quad g(\\mathbf{x})=h_{q,1,1/\\sqrt{q}}(\\mathbf{x}),\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $h_{q,1,1/\\sqrt{q}}(\\cdot)$ follows Proposition A.1. It is clear from the construction that both $f(\\cdot),g(\\cdot)$ are convex and 1-smooth. Furthermore, the optimal solution to Problem (1) defined by such $f,g$ is the unique minimizer of $g\\mathbf{(x)}$ , given by $\\begin{array}{r}{\\mathbf{x}_{[j]}^{*}=\\frac{1}{\\sqrt{q}}(1-\\frac{j}{q+1})}\\end{array}$ . We prove by induction on $k$ that the test points $\\{{\\bf x}_{k}\\}_{k=0}^{T}$ generated by $\\boldsymbol{\\mathcal{A}}$ satisfies $\\mathbf{x}_{k,[j]}=0$ for $0\\leq k\\leq T$ , $T+1\\leq j\\leq2T$ : Suppose for some $k\\leq T$ , $\\mathbf{x}_{i,[j]}=0$ holds for $0\\leq i\\leq k-1,T+1\\leq j\\leq2T$ , we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\nabla f(\\mathbf{x}_{i})=0,\\quad0\\leq i\\leq k-1.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "The zero-respecting assumption of $\\boldsymbol{\\mathcal{A}}$ (Assumption 3.4) leads to ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{x}_{k}\\in\\bigcup_{0\\leq i\\leq k-1}\\operatorname{supp}(\\nabla f(\\mathbf{x}_{i}))\\cup\\operatorname{supp}(\\nabla g(\\mathbf{x}_{i}))}\\\\ &{\\quad\\quad=\\bigcup_{0\\leq i\\leq k-1}\\operatorname{supp}(\\nabla g(\\mathbf{x}_{i})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Since $g\\mathbf{(x)}$ is a first-order zero-chain, we conclude that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{x}_{k,[j]}=0,\\quad k+1\\leq j\\leq2T.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Consequently, $f(\\mathbf{x}_{k})$ remains zero for all $0\\le k\\le K$ . However, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f^{*}=\\displaystyle\\frac{1}{2}\\sum_{j=T+1}^{2T}\\mathbf{x}_{[j]}^{*2}}\\\\ &{\\quad=\\displaystyle\\frac{1}{4T}\\sum_{j=T+1}^{2T}\\left(1-\\frac{j}{2T+1}\\right)^{2}}\\\\ &{\\quad=\\displaystyle\\frac{1}{24}\\cdot\\frac{T+1}{2T+1}}\\\\ &{\\quad\\geq\\displaystyle\\frac{1}{48}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Thus ", "page_idx": 13}, {"type": "equation", "text": "$$\n|f(\\mathbf{x}_{k})-f^{*}|\\geq{\\frac{1}{48}},\\quad1\\leq k\\leq T.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof of Theorem 4.2. Consider any first-order algorithm $\\boldsymbol{\\mathcal{A}}$ satisfying Assumption 3.4 that runs for $T$ iterations. We similarly assume that the initial point of $\\boldsymbol{\\mathcal{A}}$ is $\\mathbf{x}_{0}=0$ . Let $q=2T$ and ", "page_idx": 13}, {"type": "equation", "text": "$$\nf(\\mathbf{x})=\\frac{1}{2}\\sum_{j=T+1}^{2T}\\mathbf{x}_{[j]}^{2},\\;g(\\mathbf{x})=r_{2T,1,1}(\\mathbf{x}),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $r_{2T,1,1}(\\cdot)$ follows the construction in Proposition A.2. Both $g(\\cdot)$ and $f(\\cdot)$ are convex and 1- Lipschitz in $B(\\mathbf{0},1)$ , and the unique minimizer $\\begin{array}{r}{\\mathbf{x}^{*}=-\\frac{1}{\\sqrt{q}}\\mathbf{1}}\\end{array}$ of $g$ is the optimal solution to Problem (1) defined by such $f$ and $g$ , with norm $\\lVert\\mathbf{x}^{*}\\rVert_{2}=1$ . Similar to the proof of Theorem 4.1, we prove by induction on $k$ that there exist some adversarial subgradients $\\{\\partial\\bar{g}(\\mathbf{x}_{0}),\\cdot\\cdot\\cdot,\\partial g(\\mathbf{x}_{k-1})\\}$ such that the test points $\\{{\\bf x}_{k}\\}_{k=0}^{T}$ generated by $\\boldsymbol{\\mathcal{A}}$ satisfies $\\mathbf{x}_{k,[j]}=0$ for $0\\leq k\\leq T,T+1\\leq j\\leq2T$ : Suppose for some $k\\leq T$ , $\\mathbf{x}_{i,[j]}=0$ holds for $0\\leq i\\leq k-1,T+1\\leq j\\leq2T$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\nabla f(\\mathbf{x}_{i})=0,\\quad0\\leq i\\leq k-1.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The zero-respecting assumption of $\\boldsymbol{\\mathcal{A}}$ (Assumption 3.4) leads to ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{x}_{k}\\in\\bigcup_{0\\leq i\\leq k-1}\\operatorname{supp}(\\nabla f(\\mathbf{x}_{i}))\\cup\\operatorname{supp}(\\partial g(\\mathbf{x}_{i}))}\\\\ &{\\quad\\quad=\\bigcup_{0\\leq i\\leq k-1}\\operatorname{supp}(\\partial g(\\mathbf{x}_{i})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\left\\{\\partial g(\\ensuremath{\\mathbf{x}}_{0}),\\cdot\\cdot\\cdot\\mathbf{\\nabla},\\partial g(\\ensuremath{\\mathbf{x}}_{k-1})\\right\\}$ are the subgradients returned by a black-box first-order oracle. Since $g\\mathbf{(x)}$ is a first-order zero-chain, we conclude that there exists some adversarial subgradients $\\left\\{\\partial g(\\ensuremath{\\mathbf{x}}_{0}),\\cdot\\cdot\\cdot\\mathbf{\\nabla},\\partial g(\\ensuremath{\\mathbf{x}}_{k-1})\\right\\}$ such that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{x}_{k,[j]}=0,\\quad k+1\\leq j\\leq2T.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Consequently, $f(\\mathbf{x}_{k})$ remains zero for all $0\\le k\\le K$ . However, $\\begin{array}{r}{f^{*}=f(\\mathbf{x}^{*})=\\frac{1}{4}}\\end{array}$ . Thus ", "page_idx": 13}, {"type": "equation", "text": "$$\n|f(\\mathbf{x}_{k})-f^{*}|\\geq{\\frac{1}{4}},\\quad1\\leq k\\leq T.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "B Algorithm details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Determining $\\hat{g}^{*}$ and the initial interval ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "To apply the two-step reformulation, we need to solve an approximate value of the lower-level problem $\\hat{g}^{*}$ such that $g^{*}\\leq\\hat{g}^{*}\\leq g^{*}+\\frac{\\epsilon}{2}$ . Furthermore, to conduct the bisection method of FC-BiO, it is necessary to first determine an initial interval $[\\ell,u]$ that contains the minimal root of $\\psi^{*}(t)$ , which is exactly $\\hat{f}^{*}$ (Lemma 5.2). The following procedure is inspired by Wang et al. [27]: First, we apply optimal first-order methods for single-level optimization problems \u2013 subgradient method (SGM) [4, Section 3.1] for Lipschitz functions, and accelerated gradient method (AGM) [19, Section 2.2] for smooth functions \u2013 to the lower-level objective $g$ to obtain an approximate minimum point $\\hat{\\mathbf{x}}_{g}\\in\\mathbb{R}^{n}$ such that $g^{*}\\leq g(\\hat{\\mathbf{x}}_{g})\\leq g^{*}+\\frac{\\epsilon}{2}$ . We set $\\hat{g}^{*}=g(\\hat{\\mathbf{x}}_{g})$ and define the relaxed constraint function $\\tilde{g}(\\mathbf{x})$ as in (4). We set $\\bar{u}=f(\\hat{\\mathbf{x}}_{g})$ , then ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\psi^{*}(u)\\leq\\psi(u,\\hat{\\mathbf{x}}_{g})=\\operatorname*{max}\\{f(\\hat{\\mathbf{x}}_{g})-u,\\tilde{g}(\\hat{\\mathbf{x}}_{g})\\}=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Thus $u$ is indeed an upper-bound of the minimal root of $\\psi^{*}(\\cdot)$ given that $\\psi^{*}(\\cdot)$ is decreasing. ", "page_idx": 14}, {"type": "text", "text": "To derive a lower bound of the minimal root ${\\hat{f}}^{*}$ , we can also apply the optimal single-level minimization methods to the upper-level objective $f$ to find an approximate global minimum point $\\hat{\\mathbf{x}}_{f}\\in\\mathbb{R}^{n}$ such that $p^{*}\\leq\\hat{p}^{*}\\triangleq f(\\hat{\\mathbf{x}}_{f})\\leq p^{*}+\\frac{\\epsilon}{2}$ , where $p^{*}$ is the minimum value of $f$ over the whole space $\\mathbb{R}^{n}$ (i.e., $p^{*}\\triangleq\\operatorname*{min}_{\\mathbf{x}\\in\\mathbb{R}^{n}}\\,f(\\mathbf{x}))$ . Then $\\ell=\\hat{p}^{*}-\\frac{\\epsilon}{2}$ is a valid lower bound of ${\\hat{f}}^{*}$ since ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\ell=\\hat{p}^{*}-\\frac{\\epsilon}{2}\\leq p^{*}\\leq\\hat{f}^{*}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Nevertheless, any lower bound for $\\hat{f}^{*}$ is acceptable, such as 0 when the upper-level objective $f$ is non-negative. ", "page_idx": 14}, {"type": "text", "text": "The complexity of applying the optimal single-level minimization methods to $f$ and $g$ separately is $\\begin{array}{r}{\\mathcal{O}\\left(\\frac{C_{f}^{2}}{\\epsilon^{2}}D^{2}+\\frac{C_{g}^{2}}{\\epsilon^{2}}D^{2}\\right)}\\end{array}$ (SGM for Lipschitz problems) or $\\mathcal{O}\\left(\\sqrt{\\frac{L_{f}}{\\epsilon}}D+\\sqrt{\\frac{L_{g}}{\\epsilon}}D\\right)$ (AGM for smooth problems), which does not increase the total complexity established in Theorem 5.3 and 5.4. ", "page_idx": 14}, {"type": "text", "text": "B.2 Warm-start strategy ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "For the initial point of the subroutine $\\mathcal{M}$ (SGM in $_\\mathrm{FC-BiO^{Lip}}$ or generalized AGM in $_\\mathrm{FC-BiO^{sm}}$ ), we exploit the warm-start strategy, which uses the last point in the previous round (denoted by $\\bar{\\bf x}$ in Algorithm 1) as the initial point of the current round. Intuitively, the subproblem (i.e. minimizing $\\operatorname*{mix}\\{f(\\mathbf{x})-t,g(\\mathbf{x})\\})$ does not change significantly in each round since the parameter $t$ does not change too much as $\\ell$ and $u$ are getting closer. Thus the approximate solution of the last round is supposed to be close to the optimal solution of the current round. Nevertheless, any initial point in $B(\\mathbf{x}_{0},D)$ does not change the complexity upper bound. Such a strategy is widely used in two-level optimization methods [2, 9, 16]. ", "page_idx": 14}, {"type": "text", "text": "Similarly, we can use the approximate minimum point $\\hat{\\mathbf{x}}_{g}$ of $g$ (as described in Appendix B.1) instead of $\\mathbf{x}_{\\mathrm{0}}$ as the initial point of the first round of the subroutine. ", "page_idx": 14}, {"type": "text", "text": "C Proofs for Section 5 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "C.1 Proofs of Theorem 5.1 and Theorem 5.2 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Recall the lower complexity bounds for single-level convex optimization problems: ", "page_idx": 14}, {"type": "text", "text": "Lemma C.1 (Nesterov [19, Theorem 2.1.7]). Given $L>0,D>0$ . For any first-order algorithm $\\boldsymbol{\\mathcal{A}}$ satisfying Assumption 3.4 and any initial point $\\mathbf{x}_{\\mathrm{0}}$ , there exists a convex and $L$ -smooth function $f$ such that the minimizer $\\mathbf{x}^{*}$ satisfies ${\\|\\mathbf{x}^{*}-\\mathbf{x}_{0}\\|}_{2}\\le D$ and ", "page_idx": 14}, {"type": "equation", "text": "$$\nf(\\mathbf{x}_{T})-f(\\mathbf{x}^{*})\\geq\\frac{3L D^{2}}{32(T+1)^{2}},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Lemma C.2 (Nesterov [19, Theorem 3.2.1]). Given $C>0,D>0.$ . For any first-order algorithm $\\boldsymbol{\\mathcal{A}}$ satisfying Assumption 3.4 and any initial point $\\mathbf{x}_{\\mathrm{0}}$ , there exists a convex function $f$ that is $C$ -Lipschitz in $B(\\mathbf{x}_{0},D)$ , such that the minimizer $\\mathbf{x}^{*}$ satisfies ${\\|\\mathbf{x}^{*}-\\mathbf{x}_{0}\\|}_{2}\\le D$ and ", "page_idx": 15}, {"type": "equation", "text": "$$\nf(\\mathbf{x}_{T})-f(\\mathbf{x}^{*})\\geq\\frac{C D}{2(1+\\sqrt{T})},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The \u201chard functions\u201d $f(\\cdot)$ in the previous lemmas are exactly the zero-chain $h_{2T+1,L,D}(\\cdot)$ and $r_{T,C,D}(\\cdot)$ as constructed in Proposition A.1 and Proposition A.2, respectively. ", "page_idx": 15}, {"type": "text", "text": "Proof of Theorem 5.1. Consider any first-order algorithm $\\boldsymbol{\\mathcal{A}}$ satisfying Assumption 3.4 that runs for $T$ iterations. Without loss of generality, we assume the initial point of $\\boldsymbol{\\mathcal{A}}$ is $\\mathbf{x}_{0}=\\mathbf{0}$ . Otherwise, we can translate the following construction to $f(\\mathbf{x}-\\mathbf{x}_{0}),g(\\mathbf{x}-\\bar{\\mathbf{x_{0}}})$ . ", "page_idx": 15}, {"type": "text", "text": "If $\\begin{array}{r}{\\frac{L_{f}}{\\epsilon_{f}}\\ge\\frac{L_{g}}{\\epsilon_{g}}}\\end{array}$ , we set the upper-level and lower-level objective $f,g:\\mathbb{R}^{2T+1}\\rightarrow\\mathbb{R}$ be: ", "page_idx": 15}, {"type": "equation", "text": "$$\nf({\\bf x})=h_{2T+1,L_{f},D}({\\bf x}),\\quad g({\\bf x})=0,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $h_{2T+1,L_{f},D}(\\cdot)$ is defined in Proposition A.1. Then the zero-respecting assumption on $\\boldsymbol{\\mathcal{A}}$ implies that for any $k\\geq1$ , we have $\\begin{array}{r}{\\operatorname{supp}(\\mathbf{x}_{k+1})\\subseteq\\bigcup_{0\\leq s\\leq k-1}\\operatorname{supp}\\left(\\nabla f(\\mathbf{x}_{s})\\right)}\\end{array}$ . From Lemma C.1, it holds that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\epsilon_{f}=f({\\bf x}_{T})-f^{*}\\geq\\frac{3L_{f}D^{2}}{32(T+1)^{2}},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which implies that any zero-respecting first-order method needs at least ", "page_idx": 15}, {"type": "equation", "text": "$$\nT=\\Omega\\left(\\sqrt{\\frac{L_{f}}{\\epsilon_{f}}}D\\right)=\\Omega\\left(\\operatorname*{max}\\left\\{\\sqrt{\\frac{L_{f}}{\\epsilon_{f}}},\\sqrt{\\frac{L_{g}}{\\epsilon_{g}}}\\right\\}D\\right)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "iterations to solve a $(\\epsilon_{f},\\epsilon_{g})$ -weak optimal solution. ", "page_idx": 15}, {"type": "text", "text": "If $\\begin{array}{r}{\\frac{L_{f}}{\\epsilon_{f}}\\leq\\frac{L_{g}}{\\epsilon_{g}}}\\end{array}$ , we set the upper-level and lower-level objective $f,g:\\mathbb{R}^{2T+1}\\rightarrow\\mathbb{R}$ be: ", "page_idx": 15}, {"type": "equation", "text": "$$\nf({\\bf x})=0,\\quad g({\\bf x})=h_{2T+1,L_{g},D}({\\bf x}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By similar arguments, any zero-respecting first-order method needs at least ", "page_idx": 15}, {"type": "equation", "text": "$$\nT=\\Omega\\left(\\sqrt{\\frac{L_{g}}{\\epsilon_{g}}}D\\right)=\\Omega\\left(\\operatorname*{max}\\left\\{\\sqrt{\\frac{L_{f}}{\\epsilon_{f}}},\\sqrt{\\frac{L_{g}}{\\epsilon_{g}}}\\right\\}D\\right)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "iterations to solve a $(\\epsilon_{f},\\epsilon_{g})$ -weak optimal solution. ", "page_idx": 15}, {"type": "text", "text": "Proof of Theorem 5.2. Consider any first-order algorithm $\\boldsymbol{\\mathcal{A}}$ satisfying Assumption 3.4 that runs for $T$ iterations. Without loss of generality, we assume the initial point of $\\boldsymbol{\\mathcal{A}}$ is $\\mathbf{x}_{0}=\\mathbf{0}$ . Otherwise, we can translate the following construction to $f(\\mathbf{x}-\\mathbf{x}_{0}),g(\\mathbf{x}-\\bar{\\mathbf{x_{0}}})$ . ", "page_idx": 15}, {"type": "text", "text": "If $\\begin{array}{r}{\\frac{C_{f}}{\\epsilon_{f}}\\geq\\frac{C_{g}}{\\epsilon_{g}}}\\end{array}$ , we set the upper-level and lower-level objective $f,g:\\mathbb{R}^{T}\\rightarrow\\mathbb{R}$ be: ", "page_idx": 15}, {"type": "equation", "text": "$$\nf(\\mathbf{x})=r_{T,C_{f},D}(\\mathbf{x}),\\quad g(\\mathbf{x})=0,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $r_{T,C_{f},D}(\\cdot)$ is defined in Proposition A.2. Then the zero-respecting assumption on $\\boldsymbol{\\mathcal{A}}$ implies that for any $k\\geq1$ , we have $\\begin{array}{r}{\\mathrm{supp}(\\mathbf{x}_{k})\\subseteq\\bigcup_{0\\leq s\\leq k-1}\\operatorname{supp}\\left(\\partial f(\\mathbf{x}_{s})\\right)}\\end{array}$ . From Lemma C.2, it holds that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\epsilon_{f}=f(\\mathbf{x}_{T})-f^{*}\\geq\\frac{C_{f}D}{2(1+\\sqrt{T})},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which implies that any zero-respecting first-order method needs at least ", "page_idx": 15}, {"type": "equation", "text": "$$\nT=\\Omega\\left(\\frac{C_{f}^{2}}{\\epsilon_{f}^{2}}D^{2}\\right)=\\Omega\\left(\\operatorname*{max}\\left\\{\\frac{C_{f}^{2}}{\\epsilon_{f}^{2}},\\frac{C_{g}^{2}}{\\epsilon_{g}^{2}}\\right\\}D^{2}\\right)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "iterations to solve a $(\\epsilon_{f},\\epsilon_{g})$ -weak optimal solution. ", "page_idx": 16}, {"type": "text", "text": "If $\\begin{array}{r}{\\frac{C_{f}}{\\epsilon_{f}}\\leq\\frac{C_{g}}{\\epsilon_{g}}}\\end{array}$ , we set the upper-level and lower-level objective $f,g:\\mathbb{R}^{T}\\rightarrow\\mathbb{R}$ be: ", "page_idx": 16}, {"type": "equation", "text": "$$\nf(\\mathbf{x})=0,\\quad g(\\mathbf{x})=r_{T,C_{g},D}(\\mathbf{x}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "By similar arguments, any zero-respecting first-order method needs at least ", "page_idx": 16}, {"type": "equation", "text": "$$\nT=\\Omega\\left(\\frac{C_{g}^{2}}{\\epsilon_{g}^{2}}D^{2}\\right)=\\Omega\\left(\\operatorname*{max}\\left\\{\\frac{C_{f}^{2}}{\\epsilon_{f}^{2}},\\frac{C_{g}^{2}}{\\epsilon_{g}^{2}}\\right\\}D^{2}\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "iterations to solve a $(\\epsilon_{f},\\epsilon_{g})$ -weak optimal solution. ", "page_idx": 16}, {"type": "text", "text": "C.2 Proofs in Section 5.2 and 5.3 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof of Lemma 5.1. Any feasible solution to Problem (1) is also a feasible solution to Problem (4), thus ${\\hat{f}}^{*}\\leq f^{*}$ . For any $\\mathbf{x}$ that satisfies ", "page_idx": 16}, {"type": "equation", "text": "$$\nf(\\mathbf{x})\\leq\\hat{f}^{*}+\\epsilon_{f},\\tilde{g}(\\mathbf{x})\\leq\\frac{\\epsilon_{g}}{2},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{f(\\mathbf{x})\\leq\\hat{f}^{*}+\\epsilon_{f}\\leq f^{*}+\\epsilon_{f},}}\\\\ {{g(\\mathbf{x})=\\tilde{g}(\\mathbf{x})+\\hat{g}^{*}\\leq\\displaystyle\\frac{\\epsilon_{g}}{2}+g^{*}+\\displaystyle\\frac{\\epsilon_{g}}{2}=g^{*}+\\epsilon_{g}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Thus $\\mathbf{x}$ is indeed a $(\\epsilon_{f},\\epsilon_{g})$ -weak optimal solution to Problem (1). ", "page_idx": 16}, {"type": "text", "text": "Proof of Lemma 5.3. We first show that $\\ell$ is always a lower bound of $\\hat{f}^{*}$ . The initial lower bound satisfies $\\ell\\,\\leq\\,\\hat{f}^{*}$ . Furthermore, if (8) holds during the bisection process, we always have that $\\begin{array}{r}{\\psi^{*}(\\ell)\\geq\\hat{\\psi}^{*}(\\ell)-\\frac{\\epsilon}{2}>0}\\end{array}$ . Given that $\\psi^{*}(\\cdot)$ is decreasing, and considering that ${\\hat{f}}^{*}$ is the minimal root of $\\psi^{*}$ (Lemma 5.2), it follows that $\\ell<\\hat{f}^{*}$ . ", "page_idx": 16}, {"type": "text", "text": "As for the upper bound $u$ , the initial upper bound satisfies ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\hat{\\psi}^{*}(u)=\\psi(u,\\hat{\\mathbf{x}}_{g})=\\operatorname*{max}\\{f(\\hat{\\mathbf{x}}_{g})-u,\\tilde{g}(\\hat{\\mathbf{x}}_{g})\\}=\\operatorname*{max}\\{0,0\\}\\leq\\frac{\\epsilon}{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "And during the bisection process, we also have that $\\begin{array}{r}{\\hat{\\psi}^{*}(u)=\\operatorname*{max}\\{f(\\hat{\\mathbf{x}}_{(u)})-u,\\tilde{g}(\\hat{\\mathbf{x}}_{(u)})\\}\\leq\\frac{\\epsilon}{2}}\\end{array}$ ", "page_idx": 16}, {"type": "text", "text": "After $\\begin{array}{r}{N=\\left\\lceil\\log_{2}\\frac{u-\\ell}{\\epsilon/2}\\right\\rceil}\\end{array}$ bisection iterations, it holds that ", "page_idx": 16}, {"type": "equation", "text": "$$\nu-\\ell\\leq\\frac{\\epsilon}{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Consider the output of Algorithm $1\\hat{\\textbf{x}}\\hat{\\mathbf{x}}=\\hat{\\mathbf{x}}_{(u)}$ . Combining previous inequalities, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{f(\\hat{\\mathbf{x}})\\leq u+\\displaystyle\\frac{\\epsilon}{2}\\leq\\ell+\\epsilon\\leq\\hat{f}^{*}+\\epsilon,}\\\\ {\\tilde{g}(\\hat{\\mathbf{x}})\\leq\\displaystyle\\frac{\\epsilon}{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Some of the lemmas in this section are adapted from existing results [4, 19], but not exactly the same.   \nWe also provide a proof for these lemmas (Lemma 5.2, 5.4, 5.5). ", "page_idx": 16}, {"type": "text", "text": "Proof for Lemma 5.2. It\u2019s clear the $\\psi^{*}(t)$ is continuous and decreasing. For any $t\\in\\mathbb R$ and $\\Delta>0$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\psi^{*}(t+\\Delta)=\\operatorname*{min}_{\\mathbf{x}\\in\\mathcal{Z}}\\{\\operatorname*{max}\\{f(\\mathbf{x})-t,\\tilde{g}(\\mathbf{x})+\\Delta\\}\\}-\\Delta\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\geq\\underset{\\mathbf{x}\\in\\mathcal{Z}}{\\operatorname*{min}}\\lbrace\\operatorname*{max}\\lbrace f(\\mathbf{x})-t,\\tilde{g}(\\mathbf{x})\\rbrace\\rbrace-\\Delta}\\\\ &{=\\psi^{*}(t)-\\Delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Thus $\\psi^{*}(t)$ is 1-Lipschitz. ", "page_idx": 17}, {"type": "text", "text": "Let $\\hat{\\mathbf{x}}^{*}$ be the optimal solution to Problem (4), then $\\hat{f}^{*}=f(\\hat{\\mathbf{x}}^{*})$ . For any $t\\geq\\hat{f}^{*}$ , it holds that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\psi^{*}(t)\\leq\\psi^{*}(\\hat{f}^{*})\\leq\\psi(\\hat{f}^{*},\\hat{\\mathbf{x}}^{*})=\\operatorname*{max}\\{f(\\hat{\\mathbf{x}}^{*})-\\hat{f}^{*},\\tilde{g}(\\hat{\\mathbf{x}}^{*})\\}\\leq0.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Suppose that $t<\\hat{f}^{*}$ and $\\psi^{*}(t)\\leq0$ , then there exists a $\\hat{\\mathbf{x}}\\in\\mathcal{Z}$ such that $\\tilde{g}(\\hat{\\mathbf{x}})\\leq0,f(\\hat{\\mathbf{x}})-t\\leq0$ . Then $\\hat{\\bf x}$ is a feasible solution to Problem (4), with $f(\\hat{\\mathbf{x}})\\leq t<\\hat{f}^{*}$ , contradiction to the fact that $\\hat{f}^{*}$ is the optimal value to Problem (4). Thus for any $t<\\hat{f}^{*}$ , it holds that $\\psi^{*}(t)>0$ . ", "page_idx": 17}, {"type": "text", "text": "Thus ${\\hat{f}}^{*}$ is the smallest root of $\\psi^{*}(t)$ . ", "page_idx": 17}, {"type": "text", "text": "Proof for Lemma 5.4. Theorem 3.2 in Bubeck et al. [4] states that applying subgradient method to any $C$ -Lipschitz convex function $h$ on a compact set $Q$ with diameter $D$ gives ", "page_idx": 17}, {"type": "equation", "text": "$$\nh\\left(\\frac{1}{K}\\sum_{i=0}^{K-1}\\mathbf{x}_{i}\\right)-h^{*}\\leq\\frac{D C}{\\sqrt{K}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Here $h(\\mathbf{x})=\\psi(t,\\mathbf{x})=\\operatorname*{max}(f(\\mathbf{x})-t,\\tilde{g}(\\mathbf{x}))$ , $C=\\operatorname*{max}(C_{f},C_{g})$ . Then when $K\\geq{\\frac{4D^{2}C^{2}}{\\epsilon^{2}}}$ , it holds that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\hat{\\psi}^{*}(t)=\\psi\\left(t,\\frac{1}{K}\\sum_{i=0}^{K-1}\\mathbf{x}_{i}\\right)\\leq\\psi^{*}(t)+\\frac{\\epsilon}{2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof for Lemma 5.5. Theorem 2.3.5 in Nesterov [19] originally states that applying genearlizedgradient method to any $\\mu$ -strongly convex and $L$ -smooth convex function $h$ gives ", "page_idx": 17}, {"type": "equation", "text": "$$\nh(\\mathbf{x}_{K})-h^{*}\\leq\\frac{4L}{(\\gamma_{0}-\\mu)(K+1)^{2}}(f(\\mathbf{x}_{0})-f^{*}+\\frac{\\gamma_{0}}{2}\\|\\mathbf{x}_{0}-\\mathbf{x}^{*}\\|_{2}^{2}),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where \u03b10(1\u03b1\u22120\u03b1L0\u2212\u00b5). Here h(x) = \u03c8(t, x) = max(f(x) \u2212t, g\u02dc(x)), L = max(Lf, Lg), \u00b5 = 0, $\\begin{array}{r}{\\alpha_{0}=\\frac{1}{2},\\gamma_{0}=\\frac{L}{2}}\\end{array}$ . ", "page_idx": 17}, {"type": "text", "text": "Then ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\psi(t,\\mathbf{x}_{K})-\\psi^{*}\\leq\\frac{4L}{\\frac{1}{2}L(K+1)^{2}}\\left(\\frac{L}{2}\\|\\mathbf{x}_{0}-\\mathbf{x}^{*}\\|_{2}^{2}+\\frac{L}{4}\\|\\mathbf{x}_{0}-\\mathbf{x}^{*}\\|_{2}^{2}\\right)\\leq\\frac{6L D^{2}}{K^{2}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then when $K\\geq\\sqrt{\\frac{12L}{\\epsilon}}D$ 12L D, it holds that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\hat{\\psi}^{*}(t)=\\psi(t,{\\bf x}_{K})\\le\\psi^{*}(t)+\\frac{\\epsilon}{2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "To prove Proposition 5.2, we first present a lemma: ", "page_idx": 17}, {"type": "text", "text": "Lemma C.3. For any strictly convex and continuous functions $f$ ${\\mathfrak{f}},\\,g:\\mathbb{R}^{n}\\to\\mathbb{R}_{}$ , we define $\\varphi:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ by $\\varphi(\\mathbf x)=\\operatorname*{max}\\{f(\\mathbf x),g(\\mathbf x)\\}$ . Let $\\mathbf{x}^{*}$ be the unique minimizer of $\\varphi(\\cdot)$ on a convex and compact set $\\mathcal{Z}\\subset\\mathbb{R}^{n}$ , i.e. $\\begin{array}{r}{\\mathbf{x}^{*}=\\arg\\operatorname*{min}_{\\mathbf{x}\\in\\mathcal{Z}}\\varphi(\\mathbf{x})}\\end{array}$ , and let $\\mathbf{x}_{f}^{*}$ , $\\mathbf{x}_{g}^{*}$ be the unique minimizer of $f(\\cdot)$ and $g(\\cdot)$ on $\\mathcal{Z}$ respectively. $I f\\mathbf{x}^{*}\\neq\\mathbf{x}_{f}^{*}$ and $\\mathbf{x}^{*}\\neq\\mathbf{x}_{g}^{*}$ , then $f(\\bar{\\mathbf{x}^{*}})=g(\\mathbf{x}^{*})$ . ", "page_idx": 17}, {"type": "text", "text": "Proof. Below, we show that $f(\\mathbf{x}^{*})\\;\\neq\\;g(\\mathbf{x}^{*})$ leads to contradiction. Without loss of generality, assume $f(\\mathbf{x}^{*})>g(\\mathbf{x}^{*})$ . Due to the continuity of $f$ and $g$ , there exists some $\\delta>0$ such that for any $0<\\theta<\\delta$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\nf(\\mathbf{x}^{*}+\\theta(\\mathbf{x}_{f}^{*}-\\mathbf{x}^{*}))>g(\\mathbf{x}^{*}+\\theta(\\mathbf{x}_{f}^{*}-\\mathbf{x}^{*})).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Furthermore, for any $\\theta\\in(0,1)$ , we have that $\\mathbf{x}^{*}+\\theta(\\mathbf{x}_{f}^{*}-\\mathbf{x}^{*})\\in\\mathcal{Z}$ and ", "page_idx": 18}, {"type": "equation", "text": "$$\nf(\\mathbf{x}^{*}+\\theta(\\mathbf{x}_{f}^{*}-\\mathbf{x}^{*}))<\\theta f(\\mathbf{x}_{f}^{*})+(1-\\theta)f(\\mathbf{x}^{*})<f(\\mathbf{x}^{*}),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "since $f$ is strictly convex. ", "page_idx": 18}, {"type": "text", "text": "Then for any $\\theta\\in(0,\\operatorname*{min}(1,\\delta))$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\varphi(\\mathbf{x}^{*}+\\theta(\\mathbf{x}_{f}^{*}-\\mathbf{x}^{*}))=f(\\mathbf{x}^{*}+\\theta(\\mathbf{x}_{f}^{*}-\\mathbf{x}^{*}))<f(\\mathbf{x}^{*})=\\varphi(\\mathbf{x}^{*}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "That contradicts the fact that $\\mathbf{x}^{*}$ is the minimizer of $\\varphi(\\cdot)$ . Thus $f(\\mathbf{x}^{*})=g(\\mathbf{x}^{*})$ . ", "page_idx": 18}, {"type": "text", "text": "The previous lemma demonstrates that the minimizer of $\\operatorname*{max}_{\\mathbf{x}\\in\\mathcal{Z}}\\{f(\\mathbf{x}),g(\\mathbf{x})\\}$ falls into one of three categories: the minimizer of $f$ , the minimizer of $g$ , or the case where the function values of $f$ and $g$ are the same. ${\\bf x}_{1},{\\bf x}_{2},{\\bf x}_{3}$ in Proposition 5.2 corresponds to the three cases respectively. ", "page_idx": 18}, {"type": "text", "text": "Proof for Proposition 5.2. Define ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\bar{f}(t,\\mathbf{x};\\mathbf{y}_{k})\\triangleq f(\\mathbf{y}_{k})+\\langle\\nabla f(\\mathbf{y}_{k}),\\mathbf{x}-\\mathbf{y}_{k}\\rangle+\\frac{L}{2}\\|\\mathbf{x}-\\mathbf{y}_{k}\\|_{2}^{2}-t,}\\\\ {\\displaystyle\\bar{g}(\\mathbf{x};\\mathbf{y}_{k})\\triangleq\\tilde{g}(\\mathbf{y}_{k})+\\langle\\nabla g(\\mathbf{y}_{k}),\\mathbf{x}-\\mathbf{y}_{k}\\rangle+\\frac{L}{2}\\|\\mathbf{x}-\\mathbf{y}_{k}\\|_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which can be written as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bar{f}(t,\\mathbf{x};\\mathbf{y}_{k})=f(\\mathbf{y}_{k})-t-\\frac{1}{2L}\\|\\nabla f(\\mathbf{y}_{k})\\|^{2}+\\frac{L}{2}\\left\\|\\mathbf{x}-\\mathbf{y}_{k}+\\frac{1}{L}\\nabla f(\\mathbf{y}_{k})\\right\\|^{2},}\\\\ {\\bar{g}(\\mathbf{x};\\mathbf{y}_{k})=\\tilde{g}(\\mathbf{y}_{k})-\\frac{1}{2L}\\|\\nabla g(\\mathbf{y}_{k})\\|^{2}+\\frac{L}{2}\\left\\|\\mathbf{x}-\\mathbf{y}_{k}+\\frac{1}{L}\\nabla g(\\mathbf{y}_{k})\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Thus ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbf{x}_{1}=\\underset{\\mathbf{x}\\in\\mathcal{Z}}{\\arg\\operatorname*{min}}\\left\\|\\mathbf{x}-\\mathbf{y}_{k}+\\frac{1}{L}\\nabla f(\\mathbf{y}_{k})\\right\\|=\\underset{\\mathbf{x}\\in\\mathcal{Z}}{\\arg\\operatorname*{min}}\\,\\bar{f}(t,\\mathbf{x};\\mathbf{y}_{k}),}\\\\ {\\displaystyle\\mathbf{x}_{2}=\\underset{\\mathbf{x}\\in\\mathcal{Z}}{\\arg\\operatorname*{min}}\\left\\|\\mathbf{x}-\\mathbf{y}_{k}+\\frac{1}{L}\\nabla g(\\mathbf{y}_{k})\\right\\|=\\underset{\\mathbf{x}\\in\\mathcal{Z}}{\\arg\\operatorname*{min}}\\,\\bar{g}(t,\\mathbf{x};\\mathbf{y}_{k}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Assume the minimizer $\\mathbf{x}_{k+1}$ of $\\bar{\\psi}(t,\\mathbf{x};\\mathbf{y}_{k})$ satisfies $\\mathbf{x}_{k+1}\\neq\\mathbf{x}_{1}$ and $\\mathbf{x}_{k+1}\\neq\\mathbf{x}_{2}$ , then Lemma C.3 implies that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{x}_{k+1}\\in\\{\\mathbf{x}\\mid\\bar{f}(t,\\mathbf{x};\\mathbf{y}_{k})=\\bar{g}(\\mathbf{x};\\mathbf{y}_{k})\\}}\\\\ &{\\qquad=\\{\\mathbf{x}\\mid f(\\mathbf{y}_{k})-\\tilde{g}(\\mathbf{y}_{k})+\\langle\\nabla f(\\mathbf{y}_{k})-\\nabla\\tilde{g}(\\mathbf{y}_{k}),\\mathbf{x}-\\mathbf{y}_{k}\\rangle-t=0\\}}\\\\ &{\\qquad=\\mathcal{H}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Given that $\\bar{f}(t,{\\bf x};{\\bf y}_{k})=\\bar{g}({\\bf x};{\\bf y}_{k})$ in the subset $\\mathcal{Z}\\cap\\mathcal{H}$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\bf x}_{k+1}=\\arg\\operatorname*{min}_{{\\boldsymbol{\\psi}}}\\bar{\\psi}(t,{\\bf x};{\\bf y}_{k})}\\ ~}\\\\ {{\\displaystyle~~~~~~=\\arg\\operatorname*{min}_{{\\boldsymbol{\\psi}}}\\bar{f}(t,{\\bf x};{\\bf y}_{k})}\\ ~}\\\\ {{\\displaystyle~~~~~~~~~\\times\\mathcal{Z}\\cap\\mathcal{A}}\\ ~~}\\\\ {{\\displaystyle~~~~~=\\arg\\operatorname*{min}_{{\\bf x}\\in\\mathcal{Z}\\cap\\mathcal{H}}\\left\\|{\\bf x}-{\\bf y}_{k}+\\frac{1}{L}\\nabla f({\\bf y}_{k})\\right\\|}}\\\\ {{\\displaystyle~~~~~~~~=\\mathbf{x}_{3}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Thus we conclude that $\\mathbf{x}_{k+1}$ can only be one of ${\\bf x}_{1},{\\bf x}_{2},{\\bf x}_{3}$ . ", "page_idx": 18}, {"type": "text", "text": "Proof for Theorem 5.3. Combining Lemma 5.3 and Lemma 5.4, we directly get the result. ", "page_idx": 18}, {"type": "text", "text": "Proof for Theorem 5.4. Combining Lemma 5.3 and Lemma 5.5, we directly get the result. ", "page_idx": 18}, {"type": "text", "text": "Proof for Corollary 5.1. A $(\\epsilon_{f},\\epsilon_{g})$ -weak optimal solution to Problem (4) is equivalent to a $(\\epsilon_{f},\\epsilon_{f})$ - weak optimal solution to ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{x}\\in\\mathbb{R}^{n}}f(\\mathbf{x}),\\quad\\mathrm{s.t.}\\quad\\tilde{g}^{\\circ}(\\mathbf{x})=\\frac{\\epsilon_{f}}{\\epsilon_{g}}\\tilde{g}(\\mathbf{x})\\leq0.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "When $f$ and $g$ are $C_{f},C_{g}$ -Lipschitz respectively, $\\tilde{g}^{\\circ}$ is $\\frac{\\epsilon_{f}}{\\epsilon_{g}}C_{g}$ -Lipschitz. According to Theorem 5.3, $_\\mathrm{FC-BiO^{Lip}}$ finds a $(\\epsilon_{f},\\epsilon_{f})$ -weak optimal solution of Problem (15) in ", "page_idx": 19}, {"type": "equation", "text": "$$\nT=\\tilde{\\mathcal{O}}\\left(\\frac{\\operatorname*{max}\\{C_{f}^{2},\\frac{\\epsilon_{f}^{2}}{\\epsilon_{g}^{2}}C_{g}^{2}\\}}{\\epsilon_{f}^{2}}D^{2}\\right)=\\tilde{\\mathcal{O}}\\left(\\operatorname*{max}\\left\\{\\frac{C_{f}^{2}}{\\epsilon_{f}^{2}},\\frac{C_{g}^{2}}{\\epsilon_{g}^{2}}\\right\\}D^{2}\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "iterations. Similarly, when $f$ and $g$ are $L_{f},L_{g}$ -smooth respectively, $\\tilde{g}^{\\circ}$ is $\\frac{\\epsilon_{f}}{\\epsilon_{g}}L_{g}$ -smooth. According to Theorem 5.3, FC-BiOsm finds a $(\\epsilon_{f},\\epsilon_{f})$ -weak optimal solution of Problem (15) in ", "page_idx": 19}, {"type": "equation", "text": "$$\nT=\\tilde{\\mathcal{O}}\\left(\\sqrt{\\frac{\\operatorname*{max}\\{L_{f},\\frac{\\epsilon_{f}}{\\epsilon_{g}}L_{g}\\}}{\\epsilon_{f}}D}\\right)=\\tilde{\\mathcal{O}}\\left(\\operatorname*{max}\\left\\{\\sqrt{\\frac{L_{f}}{\\epsilon_{f}}},\\sqrt{\\frac{L_{g}}{\\epsilon_{g}}}\\right\\}D\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "iterations. ", "page_idx": 19}, {"type": "text", "text": "Finally, we prove the claim at the end of Section 5 that our proposed algorithms are zero-respecting algorithms. ", "page_idx": 19}, {"type": "text", "text": "Proposition C.1. Both FC-BiO ${\\mathfrak{N}}^{{\\_{}}{\\dot{\\imath}}{\\_{}}}$ and FC-BiOsm on the domain $B(\\mathbf{x}_{0},D)$ satisfy Assumption 3.4. ", "page_idx": 19}, {"type": "text", "text": "Proof. Without loss of generality, we assume $\\mathbf{x}_{\\mathrm{0}}=\\mathbf{0}$ . Note that the warm-start strategy preserves the zero-respecting property in Assumption 3.4. Therefore, it suffices to prove that the subroutines (Algorithm 2 and Algorithm 3) satisfy Assumption 3.4. ", "page_idx": 19}, {"type": "text", "text": "For Algorithm 2, the subgradient $\\mathbf{s}\\in\\partial_{\\mathbf{x}}\\psi(t,\\mathbf{x})$ lies in $\\operatorname{Span}\\{\\partial f(\\mathbf{x}),\\partial g(\\mathbf{x})\\}$ according to Proposition 5.1. And the projection onto $B(\\mathbf{0},D)$ does not disrupt the zero-respecting property. Thus, Algorithm 2 satisfies Assumption 3.4. ", "page_idx": 19}, {"type": "text", "text": "For Algorithm 3, we only need to prove that the gradient mapping $\\mathbf{x}_{k+1}$ satisfies ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{x}_{k+1}\\in\\operatorname{Span}\\{\\nabla f(\\mathbf{x}_{0}),\\nabla g(\\mathbf{x}_{0}),\\cdots,\\nabla f(\\mathbf{x}_{k}),\\nabla g(\\mathbf{x}_{k})\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We denote $S_{k}=\\mathrm{Span}\\{\\nabla f(\\mathbf{x}_{0}),\\nabla g(\\mathbf{x}_{0}),\\cdot\\cdot\\cdot\\mathbf{\\nabla},\\nabla f(\\mathbf{x}_{k}),\\nabla g(\\mathbf{x}_{k})\\}$ . According to Proposition 5.2, it suffices to prove that the three descent step candidates $\\mathbf{x}_{1},\\mathbf{x}_{2}$ and $\\mathbf{x}_{3}$ are in $S_{k}$ . It is clear that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbf{y}_{k}-\\frac{1}{L}\\nabla f(\\mathbf{y}_{k}),\\mathbf{y}_{k}-\\frac{1}{L}\\nabla\\tilde{g}(\\mathbf{y}_{k})\\in S_{k}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then after projection onto $B(\\mathbf{0},D),\\mathbf{x}_{1},\\mathbf{x}_{2}$ are still in $S_{k}$ . ", "page_idx": 19}, {"type": "text", "text": "The case for $\\mathbf{x}_{3}$ is slightly more complicated. Let $\\begin{array}{r}{\\mathbf z=\\mathbf y_{k}-\\frac{1}{L}\\nabla f(\\mathbf y_{k})}\\end{array}$ , $\\mathbf{w}=\\nabla f(\\mathbf{y}_{k})-\\nabla\\tilde{g}(\\mathbf{y}_{k})$ , and $b=f(\\mathbf{y}_{k})-\\tilde{g}(\\mathbf{y}_{k})-\\langle\\mathbf{w},\\mathbf{y}_{k}\\rangle-t$ . Then $\\mathcal{\\ H}=\\{\\mathbf{x}\\mid\\mathbf{w}^{T}\\mathbf{x}+b=0\\}$ . The point $\\mathbf{x}_{3}$ is the solution to the following convex optimization problem: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{\\mathbf{x}\\in\\mathbb{R}^{n}}{\\mathrm{min}}}&{\\|\\mathbf{x}-\\mathbf{z}\\|^{2}}\\\\ {\\mathrm{s.t.}}&{\\|\\mathbf{x}\\|^{2}\\leq D^{2}}\\\\ &{\\mathbf{w}^{T}\\mathbf{x}+b=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The Lagrangian for this problem is: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\mathbf{x},\\lambda,\\mu)=\\|\\mathbf{x}-\\mathbf{z}\\|^{2}+\\lambda(\\|\\mathbf{x}\\|^{2}-D^{2})+\\mu(\\mathbf{w}^{\\top}\\mathbf{x}-b)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\lambda\\geq0$ and $\\mu$ are dual variables. The KKT conditions give: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\nabla_{\\mathbf{x}}\\mathcal{L}(\\mathbf{x},\\lambda,\\mu)|_{\\mathbf{x}=\\mathbf{x}_{3}}=2(\\mathbf{x}_{3}-\\mathbf{z})+2\\lambda\\mathbf{x}_{3}+\\mu\\mathbf{w}=0.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Thus ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbf{x}_{3}=\\frac{2\\mathbf{z}-\\mu\\mathbf{w}}{2(1+\\lambda)},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "implying that $\\mathbf{x}_{3}$ is the linear combination of ${\\bf z}$ and w. Since $\\mathbf{z},\\mathbf{w}\\in S_{k}$ , it follows that ${\\bf x}_{3}\\in S_{k}$ . ", "page_idx": 19}, {"type": "text", "text": "Remark C.1. By similar analysis, we can show that the conditional gradient type methods for simple bilevel problems $[5,\\,l2]$ on domain $B(\\mathbf{x}_{0},D)$ also fall into the zero-respecting function class. At each iteration $k$ , their methods relies on the following linear program as an oracle. ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbf{s}_{k}=\\arg\\operatorname*{min}_{\\mathbf{s}\\in\\mathcal{X}_{k}}\\langle\\nabla f(\\mathbf{x}_{k},\\mathbf{s}\\rangle,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where X $\\dot{\\mathbf{\\xi}}_{k}=\\{\\mathbf{s}\\in\\mathcal{Z}:\\langle\\nabla g(\\mathbf{x}_{k}),\\mathbf{s}-\\mathbf{x}_{k}\\rangle\\leq g(\\mathbf{x}_{0})-g(\\mathbf{x}_{k})$ }. According to our proof of Proposition C.1, it suffices to prove that $\\mathbf{s}_{k}$ is a linear combination of $\\nabla f({\\bf x}_{K})$ and $\\nabla g(\\mathbf{x}_{k})$ then the remaining proofs are the same. Similarly, this can be seen by the KKT condition when $\\mathcal{Z}=B(\\mathbf{0},D)$ , which is ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\nabla f(\\mathbf{x}_{k})+2\\lambda\\mathbf{x}+\\mu\\nabla g(\\mathbf{x}_{k})=0,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\lambda\\geq0$ and $\\mu\\geq0$ are dual variables. ", "page_idx": 20}, {"type": "text", "text": "D Finding absolute optimal solutions under additional assumptions ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In Section 4, we showed that, in general, it is intractable for any zero-respecting first-order methods to find an absolute optimal solution of Problem (1). However, it is possible to establish a lower bound for $f(\\mathbf{x}_{k})-f^{*}$ under additional assumptions. H\u00f6lderian error bound [20] is a well-studied regularity condition in the optimization literature and is utilized by previous works to establish the convergence rate of finding absolute optimal solutions [6, 12, 23]. Prior to our work, the bestknown result is established by Cao et al. [6], whose method achieves a $(\\epsilon_{f},\\epsilon_{g})$ -absolute optimal solution for smooth problems in $\\tilde{\\mathcal{O}}\\left(\\operatorname*{max}\\left\\{1/\\epsilon_{f}^{\\frac{2\\alpha-1}{2}},1/\\epsilon_{g}^{\\frac{2\\alpha-1}{2\\alpha}}\\right\\}\\right)$ iterations. Below we will show that our proposed methods also work well with such an additional assumption and achieve superior convergence rates with regard to absolute suboptimality. ", "page_idx": 20}, {"type": "text", "text": "Assumption D.1. The lower-level objective $g$ satisfies the H\u00f6lderian error bound condition for some $\\alpha\\geq1$ and $\\beta>0,$ , i.e. ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\cfrac{\\beta}{\\alpha}\\mathrm{dist}(\\mathbf x,\\boldsymbol{\\mathcal X}_{g}^{*})^{\\alpha}\\leq g(\\mathbf x)-g^{*},\\quad\\forall\\mathbf x\\in\\mathbb R^{n},}\\\\ {\\mathrm{dist}(\\mathbf x,\\boldsymbol{\\mathcal X}_{g}^{*})\\triangleq\\operatorname*{inf}_{\\mathbf y\\in\\boldsymbol{\\mathcal X}_{g}^{*}}\\|\\mathbf x-\\mathbf y\\|\\,f o r\\,a r b i t r a r y\\,n o r m\\ \\|\\cdot\\|.\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Intuitively, when the lower-level suboptimality $g(\\hat{\\mathbf{x}})\\mathrm{~-~}g^{*}$ is small, $\\hat{\\bf x}$ should be close to $\\mathcal{X}_{g}^{*}$ if H\u00f6lderian error bound condition holds for $g$ . Then we can lower bound $f(\\hat{\\mathbf{x}})-f^{*}$ by the convexity of $f$ . Jiang et al. [12] formalizes the idea in the following proposition: ", "page_idx": 20}, {"type": "text", "text": "Proposition D.1 (Jiang et al. [12, Proposition 4.1]). Assume that $f$ is convex and $g$ satisfies Assumption D.1. Define $\\begin{array}{r}{M=\\operatorname*{max}_{\\mathbf{x}\\in\\mathcal{X}_{g}^{*}}\\{||\\bar{\\nabla}f(\\mathbf{x})||_{*}\\}}\\end{array}$ where $\\|\\cdot\\|_{*}$ is the dual norm of $\\|\\cdot\\|$ . Then it holds that ", "page_idx": 20}, {"type": "equation", "text": "$$\nf(\\hat{\\mathbf{x}})-f^{*}\\ge-M\\left(\\frac{\\alpha(g(\\hat{\\mathbf{x}})-g^{*})}{\\beta}\\right)^{\\frac{1}{\\alpha}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "for any $\\hat{\\mathbf{x}}\\in\\mathbb{R}^{n}$ . ", "page_idx": 20}, {"type": "text", "text": "This proposition shows that when $\\begin{array}{r}{g(\\hat{\\mathbf{x}})\\!-\\!g^{*}\\le\\left(\\frac{1}{M}\\epsilon_{f}\\right)^{\\alpha}\\frac{\\beta}{\\alpha}}\\end{array}$ , it holds that $f(\\hat{\\mathbf{x}})\\!-\\!f^{*}\\ge-\\epsilon_{f}$ . Combining with Corollary 5.1, we obtain: ", "page_idx": 20}, {"type": "text", "text": "Corollary D.1. Suppose Assumption 3.2 or Assumption 3.3 hold and $g$ satisfies Assumption $D.I.~~F C{\\dot{-}}B i O^{L i p}$ $F C{-}B i O^{s m}$ d aan $(\\epsilon_{f},\\epsilon_{g})$ tfhore  cLiopmspclheitxzi tya nodf $\\begin{array}{r}{\\tilde{\\mathcal{O}}\\left(\\operatorname*{max}\\left\\{\\frac{C_{f}^{2}}{\\epsilon_{f}^{2}},\\frac{C_{g}^{2}}{\\epsilon_{g}^{2}},\\frac{C_{g}^{2}M^{2\\alpha}\\alpha^{2}}{\\beta^{2}\\epsilon_{f}^{2\\alpha}}\\right\\}D^{2}\\right)}\\end{array}$ $\\begin{array}{r}{\\tilde{\\mathcal{O}}\\left(\\operatorname*{max}\\left\\{\\sqrt{\\frac{L_{f}}{\\epsilon_{f}}},\\sqrt{\\frac{L_{g}}{\\epsilon_{g}}},\\sqrt{\\frac{L_{g}M^{\\alpha}\\alpha}{\\beta\\epsilon_{f}^{\\alpha}}}\\right\\}D\\right)}\\end{array}$ smooth problems, respectively. ", "page_idx": 20}, {"type": "text", "text": "To our knowledge, this is the first result that establishes a convergence rate concerning absolute suboptimality for Lipschitz problems. In a smooth setting, our result of $\\tilde{O}\\left(\\operatorname*{max}\\left\\{1/\\epsilon_{f}^{\\frac{\\alpha}{2}},\\bar{1}/\\epsilon_{g}^{\\frac{1}{2}}\\right\\}\\right)$ i also superior to the convergence rate reported by Cao et al. [6] in both upper-level and lower-level. ", "page_idx": 20}, {"type": "text", "text": "E Experiment details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we provide more details of numerical experiments in Section 6. All experiments are implemented using MATLAB R2022b on a PC running Windows 11 with a 12th Gen Intel(R) Core(TM) i7-12700H CPU $(2.30\\,\\mathrm{GHz})$ and 16GB RAM. ", "page_idx": 20}, {"type": "text", "text": "E.1 Problem (11) ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "This problem is a $\\left(L_{f},L_{g}\\right)$ -smooth problem with $L_{f}=1,L_{g}=\\lambda_{\\operatorname*{max}}(A^{T}A)$ . ", "page_idx": 21}, {"type": "text", "text": "Experiment setting The original Wikipedia Math Essential dataset [21] contains 1068 instances with 730 attributes. Following the setting of Jiang et al. [12] and Cao et al. [6], we randomly choose one of the columns as the outcome vector and let the rest be the new feature matrix. We uniformly sample 400 instances to make the lower-level regression problem over-parameterized. In this case, the upper-level problem is actually equivalent to ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{x}\\in\\mathcal{Z}}\\;\\frac{1}{2}\\|\\mathbf{x}\\|_{2}^{2}\\quad\\mathrm{s.t.}\\quad A\\mathbf{x}=\\mathbf{b}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The optimal solution $\\mathbf{x}^{*}$ for this problem can be explicitly solved via the Lagrange multiplier method: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left(\\begin{array}{c c}{{A}}&{{O}}\\\\ {{I}}&{{A^{T}}}\\end{array}\\right)\\left(\\mathbf{x}^{*}\\right)=\\left(\\mathbf{0}\\right),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\nu$ is the Lagrange multiplier. Then we use $\\begin{array}{r}{f^{*}=\\frac{1}{2}\\|\\mathbf{x}^{*}\\|_{2}^{2},g^{*}=0}\\end{array}$ as the benchmark. ", "page_idx": 21}, {"type": "text", "text": "Implementation details To be fair, all algorithms start from a random point $\\mathbf{x}_{\\mathrm{0}}$ of unit length as the initial point. For our Algorithm 1, we take a slightly different implementation, that instead of setting the maximum number of iterations of the inner subroutine to be $T^{\\prime}=T/N$ , we preset $T^{\\prime}=800\\bar{0}$ . If current $\\mathbf{x}_{k}$ already satisfies $\\begin{array}{r}{\\psi(t,\\mathbf{x}_{k})\\le\\frac{\\epsilon}{2}}\\end{array}$ , then terminate the inner subroutine directly. We adopt the warm-start strategy as described in Appendix B.2. We set $L=0$ since $f(\\mathbf{x})$ is nonnegative. For $_\\mathrm{FC-BiO^{Lip}}$ , we set $\\eta\\stackrel{=}{=}3\\times10^{-4}$ . For AGM-BiO, we set $\\gamma=1$ as in [6, Theorem 4.1]. For PB-APG, we set $\\gamma=10^{5}$ . For Bi-SG, we set $\\alpha\\,=\\,0.75$ and $\\begin{array}{r}{c=\\frac{1}{L_{f}}}\\end{array}$ . For a-IRG, we set $\\eta_{0}\\,=\\,10^{-3}$ and $\\gamma_{0}=10^{-3}$ . For CG-BiO, we set $\\gamma_{k}=0.5/(k+1)$ . For Bisec-BiO, we set the maximum number of iterations of the internal APG process to be $T^{\\prime}=10000$ . For $_\\mathrm{FC-BiO^{sm}}$ , $_\\mathtt{F C-B i O^{L i p}}$ and Bisec-BiO, solving $\\hat{g}^{*}$ takes 15000 iterations; for CG-BiO, solving $\\hat{g}^{*}$ takes 10000 iterations. The results of such pretreatments are also plotted in Figure 1. ", "page_idx": 21}, {"type": "text", "text": "E.2 Problem (12) ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "This problem is a $\\left(L_{f},L_{g}\\right)$ -smooth problem with ", "page_idx": 21}, {"type": "equation", "text": "$$\nL_{f}=\\frac{1}{4m}\\lambda_{\\mathrm{max}}((A^{v a l})^{T}A^{v a l}),\\quad L_{g}=\\frac{1}{4m}\\lambda_{\\mathrm{max}}((A^{t r})^{T}A^{t r}).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Experiment setting In this experiment, a sample of 5000 instances is taken from the \u201crcv1.binary\u201d dataset [8, 15] as the training set $(A^{t r},\\mathbf{b}^{t r})$ ; another 5000 instances is sampled as the validation set $(A^{v a l},\\mathbf{b}^{v a l})$ . Each label $\\mathbf{b}_{i}^{t r}$ (or $\\mathbf{b}_{i}^{v a l}$ ) is either $+1$ or $-1$ . ", "page_idx": 21}, {"type": "text", "text": "Implementation details In this experiment, we set the initial point $\\mathbf{x}_{0}=0$ for all methods. The implementation of our Algorithm 1 is similar to that in the first experiment. We set the maximum number of iterations of the subroutine to be $T^{\\prime}=500$ and $T^{\\prime}=1000$ for $_\\mathrm{FC-BiO^{sm}}$ and $_\\mathtt{F C-B i O^{L i p}}$ respectively. For FC-BiOLip, we set \u03b7 = 2. For AGM-BiO, we set \u03b3 = 1/( 2LLfg $\\begin{array}{r}{\\gamma=1/(\\frac{2L_{g}}{L_{f}}T^{\\frac{2}{3}}+2)}\\end{array}$ T23 + 2) as in [6, Theorem 4.4]. For PB-APG, we set $\\gamma=10^{4}$ . For Bi-SG, we set $\\alpha=0.75$ and $\\begin{array}{r}{c=\\frac{1}{L_{f}}}\\end{array}$ . For a-IRG, we set $\\eta_{0}=10^{3}$ and $\\gamma_{0}=0.1$ . For CG-BiO, we set $\\begin{array}{r}{\\gamma_{k}=\\frac{2}{k+2}}\\end{array}$ . For $_\\mathrm{FC-BiO^{sm}}$ , solving $\\hat{g}^{*}$ takes 1000 iterations; for $_\\mathrm{FC-BiO^{Lip}}$ and CG-BiO, solving $\\hat{g}^{*}$ takes 1500 iterations. As in the first experiment, the results of such pretreatments are also plotted in Figure 2. ", "page_idx": 21}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The abstract and introduction outline our results accurately. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: We discuss the limitations and future directions in the last section. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We state all the assumptions we need in Section 3. We provide all proofs in the appendix. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We present all experiment details in Section 6 and Appendix E. We also include the codes to reproduce our results in the supplementary materials. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 23}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We present all experiment details in Section 6 and Appendix E. We also include the codes to reproduce our results in the supplementary materials. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We present all experiment details in Section 6. We also include the codes to reproduce our results in the supplementary materials. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 24}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [No] ", "page_idx": 24}, {"type": "text", "text": "Justification: We compare the training dynamics of the different algorithms in one graph on the same datasets with the same initialization. All the algorithms are deterministic. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Our experiments can be conducted on CPU with a single worker. (Detailed information is provided in Appendix E.) We use the training time as the x-axis in the figures. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 25}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: This research follows the NeurIPS Code of Ethics in all aspects. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 25}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: This paper focuses on optimization theory, which has no direct societal impacts. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The methods proposed by this paper (near-optimal first-order methods for simple bilevel optimization problems) do not pose such risks of misuse. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 26}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper does not use existing assets. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 26}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 27}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 27}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}]