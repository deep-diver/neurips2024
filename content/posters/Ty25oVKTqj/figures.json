[{"figure_path": "Ty25oVKTqj/figures/figures_1_1.jpg", "caption": "Figure 1: Comparison of surface normals (top) and RGB renderings (bottom) on \"garden spheres\" [44]. While the state-of-the-art methods Ref-NeRF [44], ENVIDR [22], and Neuralangelo [21] struggle to reconstruct reflective elements or fine geometric details, our method accurately models both, leading to high-quality mesh reconstructions of all parts of the scene. Best viewed when zoomed in.", "description": "This figure compares the performance of UniSDF against three state-of-the-art 3D reconstruction methods (Ref-NeRF, ENVIDR, and Neuralangelo) on a scene containing reflective spheres.  The top row shows surface normals, highlighting the accuracy of UniSDF in capturing fine geometric details and reflective surfaces. The bottom row shows RGB renderings, further demonstrating UniSDF's ability to reconstruct high-quality meshes, even in challenging reflective areas where other methods fail.", "section": "1 Introduction"}, {"figure_path": "Ty25oVKTqj/figures/figures_3_1.jpg", "caption": "Figure 2: Pipeline of UniSDF. We combine the camera view radiance field and reflected view radiance field in 3D. Given a position x, we extract iNGP features y and input them to an MLP f that estimates a signed distance value d used to compute the NeRF density. We parametrize the camera view and reflected view radiance fields with two different MLPs fcam and fref respectively. Finally, we learn a continuous weight field that is used to compute the final color as a weighted composite W of the radiance fields colors Ccam and Cref after volume rendering, Eq. 8.", "description": "This figure illustrates the architecture of UniSDF, which combines camera and reflected view radiance fields to reconstruct scenes with reflections.  It shows how input position x is processed through an Instant Neural Graphics Primitives (INGP) feature extractor to obtain features y. These features are then fed into a Multilayer Perceptron (MLP) f to estimate a signed distance function (SDF) value d.  The SDF value is used to calculate density for volume rendering. The camera view (d) and reflected view (\u03c9r) directions, along with the normal (n) and bottleneck features (b), are used in separate MLPs (fcam and fref) to produce the respective radiance field colors (Ccam and Cref). Finally, a weight MLP (fw) computes a weight (W) that blends these colors to produce the final rendered color (C).", "section": "3 Method"}, {"figure_path": "Ty25oVKTqj/figures/figures_4_1.jpg", "caption": "Figure 3: Visualization of the color of reflected view radiance field, color of camera view radiance field, learned weight W, composed color and surface normal on \u201csedan\u201d and \u201cgarden spheres\u201d scenes [44]. Our method assigns high weight (red color) for reflective surfaces, e.g., window and hood of sedan, spheres, without any supervision.", "description": "This figure visualizes the effectiveness of the proposed method (UniSDF) in handling reflections. It shows the color of the reflected view radiance field, the color of the camera view radiance field, the learned weight, the composed color, and the surface normals for two scenes containing reflective surfaces. The learned weight, represented by a heatmap, highlights reflective areas (e.g., the window and hood of the sedan, the spheres) with high values (red), demonstrating that the model successfully identifies and emphasizes reflections without requiring any explicit supervision.", "section": "3.2 UniSDF"}, {"figure_path": "Ty25oVKTqj/figures/figures_7_1.jpg", "caption": "Figure 4: Qualitative comparison with BakedSDF [51] on \u201cbicycle\u201d and \u201cofficebonsai\u201d scenes of Mip-NeRF 360 dataset [3]. BakedSDF produces hole structures in many regions (highlighted with dotted orange boxes) and less details of fine structures (highlighted with red boxes), while our method reconstructs more complete surfaces and better details. Best viewed when zoomed in.", "description": "This figure compares the results of the proposed UniSDF method and the BakedSDF method on two scenes from the Mip-NeRF 360 dataset.  The comparison highlights that BakedSDF produces incomplete reconstructions with missing geometry and detail, whereas UniSDF generates more complete and detailed 3D models.", "section": "4 Experiments"}, {"figure_path": "Ty25oVKTqj/figures/figures_8_1.jpg", "caption": "Figure 5: Qualitative comparison of surface normals with two baselines, RefV and CamV on \"sedan\" and \"toycar\" scenes [44]. Best viewed when zoomed in.", "description": "This figure compares the surface normal visualization of the proposed UniSDF method against two baselines: RefV (using only reflected view radiance fields) and CamV (using only camera view radiance fields).  The comparison is shown for the \"sedan\" and \"toycar\" scenes from the Ref-NeRF dataset [44]. The visualization highlights the differences in surface normal reconstruction accuracy between the methods, particularly in regions with reflections. UniSDF shows more accurate and detailed surface normal estimates than RefV and CamV.", "section": "4.2 Evaluation Results"}, {"figure_path": "Ty25oVKTqj/figures/figures_8_2.jpg", "caption": "Figure 6: Visualization of our meshes. Best viewed when zoomed in.", "description": "This figure shows the 3D mesh reconstructions generated by UniSDF for six different objects from the Shiny Blender dataset and the Mip-NeRF 360 dataset. The objects include a helmet, coffee cup and saucer, teapot, ball, bicycle, and kitchen Lego scene.  The meshes demonstrate UniSDF's ability to reconstruct fine geometric details and reflective surfaces with high fidelity.", "section": "4.2 Evaluation Results"}, {"figure_path": "Ty25oVKTqj/figures/figures_8_3.jpg", "caption": "Figure 7: Ablation study of our method. Best viewed when zoomed in.", "description": "This figure presents an ablation study comparing the performance of the proposed UniSDF method with and without two key components: coarse-to-fine training and learned composition of radiance fields.  The images showcase reconstructions of scenes from the Shiny Blender and Mip-NeRF 360 datasets, highlighting the visual impact of each component on the final result. The absence of coarse-to-fine training leads to noticeable artifacts, while omitting the learned composition results in less accurate and less detailed reconstructions, particularly for reflective surfaces.", "section": "4.3 Ablation Study"}, {"figure_path": "Ty25oVKTqj/figures/figures_14_1.jpg", "caption": "Figure 8: Final image rendering and normal of original BakedSDF [51] on \"garden spheres\" scene [44]. The training is not stable leading to degraded results (see text).", "description": "This figure shows the results of applying the BakedSDF method to the \"garden spheres\" scene.  The left image shows the rendered image, which has artifacts and inaccuracies. The right image displays the surface normals, further highlighting the instability of the reconstruction.  The caption notes that the training process was unstable, resulting in the poor quality of the results. This instability emphasizes a limitation of the BakedSDF method.", "section": "C BakedSDF"}, {"figure_path": "Ty25oVKTqj/figures/figures_15_1.jpg", "caption": "Figure 9: Qualitative comparison with state-of-the-art methods [21, 51, 4] on Shiny Blender [44], Mip-NeRF 360 [3] and Ref-NeRF real [44] datasets. PSNR values for each image patch are inset. Best viewed when zoomed in.", "description": "This figure presents a qualitative comparison of the proposed UniSDF method against several state-of-the-art methods for novel view synthesis on four different datasets (Shiny Blender, Mip-NeRF 360, Ref-NeRF real).  The figure shows rendered images from each method for each scene, with the peak signal-to-noise ratio (PSNR) displayed for each rendered image.  The results demonstrate UniSDF's ability to generate high-quality images comparable to or better than existing methods, particularly for complex scenes with reflections.", "section": "4.2 Evaluation Results"}, {"figure_path": "Ty25oVKTqj/figures/figures_16_1.jpg", "caption": "Figure 3: Visualization of the color of reflected view radiance field, color of camera view radiance field, learned weight W, composed color and surface normal on \u201csedan\u201d and \u201cgarden spheres\u201d scenes [44]. Our method assigns high weight (red color) for reflective surfaces, e.g., window and hood of sedan, spheres, without any supervision.", "description": "This figure visualizes the results of the UniSDF model on two scenes containing reflective surfaces.  It displays the color from both the reflected and camera view radiance fields, the learned weight assigned to combine them, the resulting composed color, and the surface normals. The key observation is the high learned weight (shown in red) assigned to reflective areas like the car's hood and windows, and the garden spheres, demonstrating the model's ability to identify and prioritize reflective information without explicit supervision.", "section": "3.2 UniSDF"}, {"figure_path": "Ty25oVKTqj/figures/figures_16_2.jpg", "caption": "Figure 11: Qualitative comparison of surface normal on \u201cteapot\u201d and \u201cball\u201d scenes [44]. Best viewed when zoomed in.", "description": "This figure compares the surface normal visualization of the \"teapot\" and \"ball\" objects from the Shiny Blender dataset [44] generated by Geo-NeuS, Neuralangelo, Ref-NeuS, and the proposed UniSDF method.  It highlights the superior accuracy and detail preservation of UniSDF in reconstructing complex shapes compared to the other methods.", "section": "4.2 Evaluation Results"}, {"figure_path": "Ty25oVKTqj/figures/figures_18_1.jpg", "caption": "Figure 12: Comparison with two baselines, CamV and RefV, on scan 37 of DTU [1] (CD is Chamfer distance error). CamV reconstructs more noisy surface on the red handle with reflections (highlighted with red box and zoomed in), while RefV generates holes on the shiny objects and even the brick without any reflections. Best viewed when zoomed in.", "description": "This figure compares the 3D reconstruction results of three different methods (CamV, RefV, and UniSDF) on a scene containing both reflective and non-reflective objects.  It highlights the ability of UniSDF to accurately reconstruct both types of surfaces and to avoid artifacts present in the other two methods.", "section": "4.2 Evaluation Results"}, {"figure_path": "Ty25oVKTqj/figures/figures_19_1.jpg", "caption": "Figure 3: Visualization of the color of reflected view radiance field, color of camera view radiance field, learned weight W, composed color and surface normal on \u201csedan\u201d and \u201cgarden spheres\u201d scenes [44]. Our method assigns high weight (red color) for reflective surfaces, e.g., window and hood of sedan, spheres, without any supervision.", "description": "This figure visualizes the results of the UniSDF method on two scenes containing reflective surfaces.  It shows four image channels for each scene: (1) the color of the reflected view radiance field; (2) the color of the camera view radiance field; (3) the learned weight (W) indicating the blend between reflected and camera views, with red showing higher weights for reflected components; and (4) the final composite color.  Surface normals are also shown. The key observation is the high weight assigned to reflective areas (windows and hoods) without explicit supervision, demonstrating the method's ability to selectively use the appropriate radiance field based on surface properties.", "section": "3.2 UniSDF"}, {"figure_path": "Ty25oVKTqj/figures/figures_19_2.jpg", "caption": "Figure 14: Ablation study of normals on \u201csedan\u201d and \u201cgarden spheres\u201d scene [44]. Best viewed when zoomed in.", "description": "This ablation study compares the results of using predicted normals versus ground truth normals for computing reflected view direction and loss function. The figure visually demonstrates that using ground truth normals leads to better reconstruction of reflective surfaces, particularly evident in the improved smoothness and detail of the reconstructed surfaces compared to using predicted normals.", "section": "4.3 Ablation Study"}]