[{"figure_path": "Ty25oVKTqj/tables/tables_6_1.jpg", "caption": "Table 1: Quantitative results of Chamfer Distance (C.D.) on DTU [1]. Red, orange and yellow indicate the first, second and third best methods. \u2020: Factored-NeuS [12] does not provide result for scan 69. Its result is the average error of the other 14 scenes.", "description": "This table presents a comparison of Chamfer Distance (CD) results on the DTU dataset, a benchmark for 3D reconstruction.  It compares the proposed UniSDF method against several state-of-the-art techniques, including NeuS, Neural Warp, Geo-NeuS, Neuralangelo, NERO, Ref-NeuS, and Factored-NeuS. Lower CD values indicate better performance. The table highlights the top three performing methods for each metric.", "section": "4 Experiments"}, {"figure_path": "Ty25oVKTqj/tables/tables_6_2.jpg", "caption": "Table 2: Quantitative results on Shiny Blender [44], Mip-NeRF 360 dataset [3] and Ref-NeRF real dataset [44]. 'Mean' represents the average rendering metrics on all datasets. Red, orange, and yellow indicate the first, second, and third best methods for each metric. *: We follow Ref-NeuS [14] and evaluate accuracy of mesh on four scenes (car, helmet, toaster, coffee). See supp. mat. for details.", "description": "This table presents a quantitative comparison of the proposed UniSDF method against several state-of-the-art neural implicit representations for 3D reconstruction.  The metrics used are PSNR, SSIM, LPIPS, and MAE for evaluating rendering quality, along with mesh accuracy for specific scenes. The comparison is performed across three datasets: Shiny Blender, Mip-NeRF 360, and Ref-NeRF real, representing different levels of complexity and scene types.  The \"Mean\" column provides an average performance across all datasets.", "section": "4 Experiments"}, {"figure_path": "Ty25oVKTqj/tables/tables_7_1.jpg", "caption": "Table 3: Quantitative comparison with two custom baselines. Best results are in bold. *: RefV fails on scan 110 of DTU [1], the reported chamfer distance (C.D.) is the average of other 14 scenes.", "description": "This table compares the performance of the proposed UniSDF method against two custom baselines: CamV (using only the camera view radiance field) and RefV (using only the reflected view radiance field).  The comparison is done across three datasets: DTU, Mip-NeRF 360, and Ref-NeRF real.  Metrics include Chamfer Distance (C.D.), Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index (SSIM), and Learned Perceptual Image Patch Similarity (LPIPS). The results show that UniSDF consistently outperforms both baselines, demonstrating the effectiveness of combining camera and reflected view radiance fields.", "section": "4.2 Evaluation Results"}, {"figure_path": "Ty25oVKTqj/tables/tables_15_1.jpg", "caption": "Table 1: Quantitative results of Chamfer Distance (C.D.) on DTU [1]. Red, orange and yellow indicate the first, second and third best methods. \u2020: Factored-NeuS [12] does not provide result for scan 69. Its result is the average error of the other 14 scenes.", "description": "This table presents a quantitative comparison of different methods for 3D reconstruction on the DTU dataset, specifically measuring the Chamfer distance.  It shows the performance of various methods on 15 different scans, highlighting the top three performers for each scan.  Note that one method, Factored-NeuS, lacks results for one scan and uses an average instead.", "section": "4 Experiments"}, {"figure_path": "Ty25oVKTqj/tables/tables_17_1.jpg", "caption": "Table 5: Quantitative results of individual scenes on Shiny Blender [44]. BakedSDF [51] fails on \"car\" and \"teapot\" scenes without producing reasonable geometry. Thus we do not report its MAE metric on these scenes. *: We follow Ref-NeuS [14] and evaluate accuracy of mesh on four scenes (car, helmet, toaster, coffee). Red, orange, and yellow indicate the first, second, and third best performing algorithms for each scene.", "description": "This table presents a quantitative comparison of different methods on the Shiny Blender dataset.  It shows the PSNR, SSIM, LPIPS, MAE, and mesh accuracy (Acc) for six scenes.  Note that BakedSDF failed to produce reasonable geometry for the 'car' and 'teapot' scenes, so those results are missing.", "section": "4.2 Evaluation Results"}, {"figure_path": "Ty25oVKTqj/tables/tables_18_1.jpg", "caption": "Table 1: Quantitative results of Chamfer Distance (C.D.) on DTU [1]. Red, orange and yellow indicate the first, second and third best methods. \u2020: Factored-NeuS [12] does not provide result for scan 69. Its result is the average error of the other 14 scenes.", "description": "This table presents a quantitative comparison of Chamfer Distance (CD) on the DTU dataset, a common benchmark for 3D reconstruction.  It compares the performance of the proposed UniSDF method against several state-of-the-art techniques.  Lower CD values indicate better performance, reflecting higher accuracy in reconstructing the 3D shapes.  The table highlights the top three performing methods for each metric.", "section": "4 Experiments"}, {"figure_path": "Ty25oVKTqj/tables/tables_18_2.jpg", "caption": "Table 2: Quantitative results on Shiny Blender [44], Mip-NeRF 360 dataset [3] and Ref-NeRF real dataset [44]. 'Mean' represents the average rendering metrics on all datasets. Red, orange, and yellow indicate the first, second, and third best methods for each metric. *: We follow Ref-NeuS [14] and evaluate accuracy of mesh on four scenes (car, helmet, toaster, coffee). See supp. mat. for details.", "description": "This table presents a quantitative comparison of the proposed UniSDF method against several state-of-the-art techniques across three distinct datasets: Shiny Blender, Mip-NeRF 360, and Ref-NeRF real.  The evaluation metrics include PSNR, SSIM, LPIPS, and mesh accuracy (Acc).  The datasets represent varying levels of scene complexity and the presence of reflective surfaces, providing a comprehensive assessment of UniSDF's performance in different scenarios.", "section": "4 Experiments"}, {"figure_path": "Ty25oVKTqj/tables/tables_19_1.jpg", "caption": "Table 8: Ablation study of normals on the Ref-NeRF real dataset [44].", "description": "This table presents the ablation study results of using predicted normals versus ground truth normals for computing reflected view direction and loss function.  The results show that using ground truth normals yields better performance in terms of PSNR, SSIM, and LPIPS metrics on the Ref-NeRF real dataset.", "section": "4.3 Ablation Study"}]