{"references": [{"fullname_first_author": "Edward J Hu", "paper_title": "LoRA: Low-rank adaptation of large language models", "publication_date": "2022", "reason": "This paper introduces LoRA, a parameter-efficient fine-tuning method crucial to Panacea's design for adapting LLMs to diverse preferences."}, {"fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022", "reason": "This paper details RLHF, a key technique used in Panacea for aligning LLMs with human preferences, and is foundational to the alignment work."}, {"fullname_first_author": "Kaisa Miettinen", "paper_title": "Nonlinear multiobjective optimization", "publication_date": "1999", "reason": "This book provides fundamental theoretical background for understanding multi-objective optimization problems that Panacea addresses."}, {"fullname_first_author": "Kalyanmoy Deb", "paper_title": "A fast and elitist multiobjective genetic algorithm: Nsga-ii", "publication_date": "2002", "reason": "This paper introduces NSGA-II, a widely used multi-objective optimization algorithm, providing a relevant baseline for comparison with Panacea."}, {"fullname_first_author": "Rafael Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "publication_date": "2023", "reason": "This paper presents DPO, an alternative LLM alignment method, allowing comparison and contrast with Panacea's approach."}]}