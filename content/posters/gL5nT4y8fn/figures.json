[{"figure_path": "gL5nT4y8fn/figures/figures_1_1.jpg", "caption": "Figure 1: Comparison of the predominant single-objective alignment and our multi-dimensional alignment. For the two responses to a prompt, labelers agree on the preferable one in each preference dimension, but conflict when assigning a synthesized scalar label denoting which is \"better\". This arises due to the inherently different preference weights held by labelers, a common case in reality. Performing single-objective optimization on the potentially conflicting scalar-label dataset (left) could lead to a dominated solution and misalignment. By contrast, our method, Panacea, leverages multi-dimensional preference optimization (right) on the consistent multi-dimensional dataset and learns the entire Pareto front (PF), thereby aligning with diverse and complex human preferences.", "description": "This figure compares single-objective and multi-dimensional approaches to LLM alignment.  Single-objective methods use a single scalar label to represent multiple dimensions of human preference, leading to inconsistencies and potential misalignment.  The multi-dimensional method (Panacea) uses multiple dimensions and aims for Pareto optimality, resulting in a diverse set of aligned responses. The visualizations show how single-objective methods can result in suboptimal solutions while Panacea finds Pareto optimal solutions.", "section": "1 Introduction"}, {"figure_path": "gL5nT4y8fn/figures/figures_3_1.jpg", "caption": "Figure 2: Panacea embeds the preference vector into singular values of each SVD-LoRA layer and scales it with learnable factors to match the magnitudes. During learning, for each data batch, we randomly sample a preference vector from the preference simplex and train the embedded model with various optimization procedures and loss aggregation methods. In the inference stage, the model adapts online to the user-specified preference vector and exhibits Pareto alignment in its responses.", "description": "This figure illustrates the Panacea model's architecture and workflow.  During training, the model learns to embed preference vectors into its singular values using SVD-LoRA.  Preference vectors are sampled from a simplex, and various optimization techniques are applied.  During inference, a user-specified preference vector is used, and the model adapts to provide a Pareto-optimal response, effectively aligning with diverse human preferences.", "section": "4 Panacea: Pareto Alignment via Preference Adaptation"}, {"figure_path": "gL5nT4y8fn/figures/figures_6_1.jpg", "caption": "Figure 3: Algorithm performance on HH. Baseline methods (RS and DPS) require training a separate model for each preference dimension/vector, whereas Panacea learns a single adaptable model. Left: Panacea is significantly better than RS and even outperforms DPS, showing its superiority in learning PF while being more efficient. Middle: on Llama2-ft across different seeds, Panacea again consistently outperforms RS, and its fronts exhibit smooth convex shapes that correspond with theory. Right: with DPO, Panacea using both LS and Tche aggregation learns better fronts than RS.", "description": "This figure compares the performance of Panacea, RS, and DPS on the helpful-harmless dilemma using three different settings.  The left panel shows that Panacea achieves a significantly better Pareto front than both RS and DPS, demonstrating its efficiency in learning the entire Pareto front with a single model. The middle panel demonstrates the robustness of Panacea across different random seeds, consistently outperforming RS and exhibiting smooth convex Pareto fronts. The right panel shows that when using DPO, Panacea with either LS or Tchebycheff aggregation learns superior Pareto fronts than RS.", "section": "5.1 Mastering Dual Dimensions: Addressing the Helpful-Harmless Dilemma"}, {"figure_path": "gL5nT4y8fn/figures/figures_7_1.jpg", "caption": "Figure 1: Comparison of the predominant single-objective alignment and our multi-dimensional alignment. For the two responses to a prompt, labelers agree on the preferable one in each preference dimension, but conflict when assigning a synthesized scalar label denoting which is \"better\". This arises due to the inherently different preference weights held by labelers, a common case in reality. Performing single-objective optimization on the potentially conflicting scalar-label dataset (left) could lead to a dominated solution and misalignment. By contrast, our method, Panacea, leverages multi-dimensional preference optimization (right) on the consistent multi-dimensional dataset and learns the entire Pareto front (PF), thereby aligning with diverse and complex human preferences.", "description": "This figure compares single-objective and multi-dimensional alignment methods for LLMs.  It illustrates how single-objective methods, using scalar human preference labels, can lead to inconsistencies and misalignment due to differing preference weights among labelers. In contrast, Panacea utilizes multi-dimensional preferences to achieve Pareto optimality, ensuring alignment with diverse preferences.", "section": "1 Introduction"}, {"figure_path": "gL5nT4y8fn/figures/figures_7_2.jpg", "caption": "Figure 5: Learned fronts of Panacea (red) and RS (blue) on HHC problem with Llama2-ft, RLHF, and LS aggregation. Panacea learns a better and more evenly distributed front while solutions of RS clutter in a corner. This suggests Panacea provides fine-grained solutions to diverse human preferences.", "description": "This figure compares the Pareto fronts learned by Panacea and RS on a three-objective optimization problem (helpful, harmless, concise). Panacea's front is smoother, more evenly distributed, and shows better performance than RS's, which is clustered in a corner.  This demonstrates Panacea's ability to learn a more comprehensive and better-distributed Pareto front.", "section": "5 Experiments"}, {"figure_path": "gL5nT4y8fn/figures/figures_8_1.jpg", "caption": "Figure 2: Panacea embeds the preference vector into singular values of each SVD-LoRA layer and scales it with learnable factors to match the magnitudes. During learning, for each data batch, we randomly sample a preference vector from the preference simplex and train the embedded model with various optimization procedures and loss aggregation methods. In the inference stage, the model adapts online to the user-specified preference vector and exhibits Pareto alignment in its responses.", "description": "This figure illustrates the Panacea model's architecture and workflow.  It shows how the preference vector is incorporated into the singular values of SVD-LoRA layers, allowing for online adaptation to diverse preferences. The learning process involves randomly sampling preference vectors and training the model using various optimization methods.  Inference involves directly injecting a user-specified preference vector to obtain a Pareto-optimal response.", "section": "4 Panacea: Pareto Alignment via Preference Adaptation"}, {"figure_path": "gL5nT4y8fn/figures/figures_21_1.jpg", "caption": "Figure 2: Panacea embeds the preference vector into singular values of each SVD-LoRA layer and scales it with learnable factors to match the magnitudes. During learning, for each data batch, we randomly sample a preference vector from the preference simplex and train the embedded model with various optimization procedures and loss aggregation methods. In the inference stage, the model adapts online to the user-specified preference vector and exhibits Pareto alignment in its responses.", "description": "This figure illustrates the architecture of Panacea, highlighting how it integrates user preferences into the model's parameters.  It shows the process during both training (randomly sampling preference vectors and training) and inference (injecting the user's preference vector to generate a Pareto-optimal response). The use of SVD-LoRA and the learnable scaling factor are key aspects illustrated in the diagram.", "section": "4 Panacea: Pareto Alignment via Preference Adaptation"}, {"figure_path": "gL5nT4y8fn/figures/figures_23_1.jpg", "caption": "Figure 1: Comparison of the predominant single-objective alignment and our multi-dimensional alignment. For the two responses to a prompt, labelers agree on the preferable one in each preference dimension, but conflict when assigning a synthesized scalar label denoting which is \"better\". This arises due to the inherently different preference weights held by labelers, a common case in reality. Performing single-objective optimization on the potentially conflicting scalar-label dataset (left) could lead to a dominated solution and misalignment. By contrast, our method, Panacea, leverages multi-dimensional preference optimization (right) on the consistent multi-dimensional dataset and learns the entire Pareto front (PF), thereby aligning with diverse and complex human preferences.", "description": "This figure compares single-objective and multi-dimensional approaches to LLM alignment.  Single-objective methods use scalar labels, which oversimplify human preferences and can lead to misalignment. The proposed Panacea method uses multi-dimensional preferences to optimize for Pareto optimality, resulting in a model that aligns with a diverse range of preferences.", "section": "1 Introduction"}, {"figure_path": "gL5nT4y8fn/figures/figures_24_1.jpg", "caption": "Figure 1: Comparison of the predominant single-objective alignment and our multi-dimensional alignment. For the two responses to a prompt, labelers agree on the preferable one in each preference dimension, but conflict when assigning a synthesized scalar label denoting which is \"better\". This arises due to the inherently different preference weights held by labelers, a common case in reality. Performing single-objective optimization on the potentially conflicting scalar-label dataset (left) could lead to a dominated solution and misalignment. By contrast, our method, Panacea, leverages multi-dimensional preference optimization (right) on the consistent multi-dimensional dataset and learns the entire Pareto front (PF), thereby aligning with diverse and complex human preferences.", "description": "This figure compares single-objective and multi-dimensional approaches to LLM alignment.  Single-objective methods use scalar labels, which can be inconsistent due to varying labeler preferences.  This leads to suboptimal models.  Panacea, a multi-dimensional approach, uses separate labels for each dimension of preference and learns the Pareto front of optimal solutions, leading to better alignment across diverse preferences.", "section": "1 Introduction"}, {"figure_path": "gL5nT4y8fn/figures/figures_25_1.jpg", "caption": "Figure 1: Comparison of the predominant single-objective alignment and our multi-dimensional alignment. For the two responses to a prompt, labelers agree on the preferable one in each preference dimension, but conflict when assigning a synthesized scalar label denoting which is \"better\". This arises due to the inherently different preference weights held by labelers, a common case in reality. Performing single-objective optimization on the potentially conflicting scalar-label dataset (left) could lead to a dominated solution and misalignment. By contrast, our method, Panacea, leverages multi-dimensional preference optimization (right) on the consistent multi-dimensional dataset and learns the entire Pareto front (PF), thereby aligning with diverse and complex human preferences.", "description": "This figure compares single-objective and multi-dimensional approaches to LLM alignment.  It highlights the problem of conflicting human preferences when using scalar labels in single-objective optimization, leading to suboptimal and misaligned models.  Panacea, the proposed multi-dimensional approach, addresses this issue by aligning with a diverse range of preferences and learning the complete Pareto front, resulting in better alignment.", "section": "1 Introduction"}, {"figure_path": "gL5nT4y8fn/figures/figures_25_2.jpg", "caption": "Figure 1: Comparison of the predominant single-objective alignment and our multi-dimensional alignment. For the two responses to a prompt, labelers agree on the preferable one in each preference dimension, but conflict when assigning a synthesized scalar label denoting which is \"better\". This arises due to the inherently different preference weights held by labelers, a common case in reality. Performing single-objective optimization on the potentially conflicting scalar-label dataset (left) could lead to a dominated solution and misalignment. By contrast, our method, Panacea, leverages multi-dimensional preference optimization (right) on the consistent multi-dimensional dataset and learns the entire Pareto front (PF), thereby aligning with diverse and complex human preferences.", "description": "This figure compares single-objective and multi-dimensional alignment methods for LLMs.  Single-objective methods use scalar labels, leading to inconsistencies and misalignment due to differing human preference weights.  The proposed method, Panacea, uses multi-dimensional preferences for better alignment and optimizes for the entire Pareto front, representing all possible optimal solutions.", "section": "1 Introduction"}, {"figure_path": "gL5nT4y8fn/figures/figures_26_1.jpg", "caption": "Figure 1: Comparison of the predominant single-objective alignment and our multi-dimensional alignment. For the two responses to a prompt, labelers agree on the preferable one in each preference dimension, but conflict when assigning a synthesized scalar label denoting which is \"better\". This arises due to the inherently different preference weights held by labelers, a common case in reality. Performing single-objective optimization on the potentially conflicting scalar-label dataset (left) could lead to a dominated solution and misalignment. By contrast, our method, Panacea, leverages multi-dimensional preference optimization (right) on the consistent multi-dimensional dataset and learns the entire Pareto front (PF), thereby aligning with diverse and complex human preferences.", "description": "This figure compares single-objective and multi-dimensional alignment methods for LLMs.  Single-objective methods use scalar labels, which can be inconsistent due to varying labeler preferences, leading to misalignment.  The proposed Panacea method uses multi-dimensional preference optimization, resolving these inconsistencies and recovering the entire Pareto front of optimal solutions.", "section": "1 Introduction"}]