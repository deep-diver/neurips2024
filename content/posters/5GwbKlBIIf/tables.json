[{"figure_path": "5GwbKlBIIf/tables/tables_2_1.jpg", "caption": "Table 1: Comparison between the convergence of digital and analog training algorithms: K represents the number of iterations, \u03c3\u00b2 is the variance of stochastic gradients, Wax/\u03c4\u00b2 and Pmax/\u03c4\u00b2 measure the saturation degree, and SK measures the non-ideality of analog devices (c.f. Theorem 2). Asymptotic error refers to the error that does not vanish with K.", "description": "This table compares the convergence rates and asymptotic errors of four different training algorithms: Digital SGD, Analog SGD, Tiki-Taka, and a lower bound.  It shows how the asymptotic error is affected by factors like the variance of stochastic gradients and device non-ideality, highlighting Tiki-Taka's ability to eliminate this error.", "section": "1.1 Main results"}, {"figure_path": "5GwbKlBIIf/tables/tables_9_1.jpg", "caption": "Table 1: Comparison between the convergence of digital and analog training algorithms: K represents the number of iterations, \u03c3\u00b2 is the variance of stochastic gradients, Wax/\u03c4\u00b2 and Pmax/\u03c4\u00b2 measure the saturation degree, and SK measures the non-ideality of analog devices (c.f. Theorem 2). Asymptotic error refers to the error that does not vanish with K.", "description": "This table compares the convergence rate and asymptotic error of three different training algorithms: digital SGD, analog SGD, and Tiki-Taka.  It shows how the asymptotic error relates to algorithm parameters like learning rate and noise variance, and to device characteristics.", "section": "1.1 Main results"}]