[{"type": "text", "text": "Learning the Infinitesimal Generator of Stochastic Diffusion Processes ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Vladimir R. Kostic CSML, Istituto Italiano di Tecnologia University of Novi Sad vladimir.kostic@iit.it ", "page_idx": 0}, {"type": "text", "text": "Karim Lounici CMAP-Ecole Polytechnique karim.lounici@polytechnique.edu ", "page_idx": 0}, {"type": "text", "text": "H\u00e9l\u00e8ne Halconruy SAMOVAR, T\u00e9l\u00e9com Sud-Paris MODAL\u2019X, Universit\u00e9 Paris Nanterre helene.halconruy@telecom-sudparis.eu ", "page_idx": 0}, {"type": "text", "text": "Timoth\u00e9e Devergne CSML & ATSIM, Istituto Italiano di Tecnologia timothee.devergne@iit.it ", "page_idx": 0}, {"type": "text", "text": "Massimiliano Pontil   \nCSML, Istituto Italiano di Tecnologia   \nAI Centre, University College London massimiliano.pontil@iit.it ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We address data-driven learning of the infinitesimal generator of stochastic diffusion processes, essential for understanding numerical simulations of natural and physical systems. The unbounded nature of the generator poses significant challenges, rendering conventional analysis techniques for Hilbert-Schmidt operators ineffective. To overcome this, we introduce a novel framework based on the energy functional for these stochastic processes. Our approach integrates physical priors through an energy-based risk metric in both full and partial knowledge settings. We evaluate the statistical performance of a reduced-rank estimator in reproducing kernel Hilbert spaces (RKHS) in the partial knowledge setting. Notably, our approach provides learning bounds independent of the state space dimension and ensures non-spurious spectral estimation. Additionally, we elucidate how the distortion between the intrinsic energy-induced metric of the stochastic diffusion and the RKHS metric used for generator estimation impacts the spectral learning bounds. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Continuous-time processes are often modeled using ordinary differential equations (ODEs), assuming deterministic dynamics. However, real systems in science and engineering that are modeled by ODEs are subject to unfeasible-to-model influences, necessitating the extension of deterministic models through stochastic differential equations (SDEs), see [40, 42] and references therein. SDEs are advantageous for modeling inherently random phenomena. For instance, in finance, they specify the stochastic process governing asset behavior, a crucial step in constructing pricing models for financial derivatives [41]. Another compelling application arises in atomistic simulations, where SDEs are used to model the evolution of atomic systems subjected to thermal fluctuations through the Boltzmann distribution [37]. ", "page_idx": 0}, {"type": "text", "text": "A diverse range of SDEs can be represented as $d X_{t}=a(X_{t})d t+b(X_{t})d W_{t}$ , where $X_{0}=x$ . Here, W denotes a (possibly multi-dimensional) Brownian motion, and the functions $a$ and $b$ are commonly known as the drift and diffusion coefficients, respectively. Determining these coefficients from one or more trajectories, whether discretized or continuous, has been a key pursuit in \"diffusion statistics\" since the 1980s, as seen in works like [23, 30, 31]. However, uncovering the drift and diffusion coefficients alone does not reveal all the intrinsic properties of a complex system, such as the metastable states of Langevin dynamics in atomistic simulations [see e.g. 49, and references therein]. Consequently, there has been a shift and growing interest in the Infinitesimal Generator (IG) of an SDE, as its spectral decomposition offers a more in-depth understanding of system dynamics and behavior, thus providing a comprehensive picture beyond the mere identification of coefficients. ", "page_idx": 0}, {"type": "table", "img_path": "H7SaaqfCUi/tmp/859453cef7ef1fcbe484309d3587346fa5a6a2655798734fbee0766f98068aba.jpg", "table_caption": [], "table_footnote": ["Table 1: Comparison to previous kernel-based works on generator learning. Sample size is $n$ , state-space dimension is $d$ , $\\gamma$ is the regularization parameter of KKR and RRR and $r$ is RRR rank parameter. Our learning bounds are derived in Theorem 2 where the parameters $\\alpha,\\beta,\\tau$ quantify the intrinsic difficulty of the problem and impact of kernel choice on learning IG. "], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Recovering the spectral decomposition of the IG can theoretically be achieved by exploiting the wellstudied Transfer Operators (TO) [see 27, and references therein]. TOs represent the average evolution of state functions (observables) over time and, being linear and amenable to spectral decomposition under certain conditions, they offer a valuable means to interpret and analyze nonlinear systems. However, they require evenly sampled data at a high rate, which may be impractical. Additionally, TO approaches are purely data-driven, complicating the incorporation of partial or full knowledge of an SDE into the learning process. Thus, there is growing interest in learning the IG directly from data, as it can handle uneven sampling and integrate SDE knowledge. The challenge lies in the IG being an unbounded operator, unlike TO which are often well-approximated by Hilbert-Schmidt operators with comprehensive statistical theory [28]. Unfortunately, the existing statistical theory collapses when applied to unbounded operators, prompting the need to completely rethink the problem ", "page_idx": 1}, {"type": "text", "text": "Related work Extensive research has explored learning dynamical systems from data [see the monographs 7, 32, and references therein]. Analytical models for dynamics are often unavailable, motivating the need for data-driven methods. Two prominent approaches have emerged: deep learning-based methods [5, 12, 36], effective for learning complex data representations but lacking statistical analysis, and kernel methods [2, 6, 10, 20, 22, 25, 27, 28, 50], offering solid statistical guarantees for the estimation of the TO but requiring careful selection of the kernel function. A related question, tackling the challenging problem of learning invariant subspaces of the TO, has recently led to the development of several methodologies [22, 34, 38, 46, 52, 51], some of which are based on deep canonical correlation analysis [3, 29]. In comparison, there have been significantly fewer works on learning the IG and only in very specific settings. In [55] a deep learning approach is developed for Langevin diffusion, while [24] presents an extended dynamical mode decomposition method for learning the generator and clarifies its connection to Galerkin\u2019s approximation. However, neither of these two works provides any learning guarantees. In this respect the only previous work we are aware of are [1], revising the Galerkin method for Laplacian diffusion, [19], presenting a kernel approach for general diffusion SDE with full knowledge of drift and diffusion coefficients, and [43] addressing Langevin diffusion. As highlighted in Table 1, the bounds and analysis in these works are either restricted to specific SDEs or are incomplete. Notably, none of these works proposed an adequate framework to handle the unboundedness of the IG, resulting in an incomplete theoretical analysis and suboptimal rates, sometimes explicitly depending on state space dimension. Moreover, the estimators proposed in these works are prone to spurious eigenvalues and do not offer guarantees on the accurate estimation of eigenvalues and eigenfunctions. ", "page_idx": 1}, {"type": "text", "text": "Contributions In summary, our main contributions are: 1) Proposing a fundamentally new idea to estimate the spectrum of self-adjoint generators of stable It\u00f4 SDE from a single trajectory. In contrast to all existing works, we exploit the geometry of the process via a novel energy (risk) functional; 2) In a certain sense we \u201cfight fire (resolvent) with fire (generator)\u201d to derive a new efficient learning method that is able to infer the best approximation of the resolvent of IG on the RKHS, independently of the time-sampling; 3) We prove the first IG spectral estimation finite sample bounds using (imperfect) partial knowledge, which notably, overcome the curse of dimensionality present in classical numerical methods; 4) Each important aspect of our learning method, especially in relation to the most relevant existing works, is empirically demonstrated to complement our theoretical analysis. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2 Background and the problem ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Our drive to estimate the eigenvalues of the infinitesimal generator for an SDE like (1) stems from its crucial role in characterizing dynamics in physical systems. This operator\u2019s closed form (3), relies on the drift $a$ and diffusion coefficient $b$ , where we have partial knowledge: $b$ is assumed to be known, but $a$ is not. To compensate for the lack of prior knowledge about $a$ , we introduce the system\u2019s energy as an additional known quantity. Below, we detail the mathematical concepts framing the problem (generator, spectrum, energy) exemplified through the Langevin and Cox-Ingersoll-Ross processes. For detailed list of notation used throughout the paper we refer Table 7 in the appendix. ", "page_idx": 2}, {"type": "text", "text": "Transfer operator and infinitesimal generator A variety of physical, biological, and financial systems evolve through stochastic processes $X\\,=\\,(X_{t})_{t\\in\\mathbb{R}^{+}}$ , where $X_{t}\\,\\in\\,\\dot{\\boldsymbol{x}^{\\prime}}\\subset\\,\\mathbb{R}^{d}$ denotes the system\u2019s state at time $t$ . A commonly employed model for such dynamics is captured by stochastic differential equations (SDEs) of the form ", "page_idx": 2}, {"type": "equation", "text": "$$\nd X_{t}=a(X_{t})d t+b(X_{t})d W_{t}\\quad{\\mathrm{and}}\\quad X_{0}=x,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $x\\in\\mathscr{X}$ , $W=(W_{t}^{1},\\dots,W_{t}^{p})_{t\\in\\mathbb{R}_{+}}$ is a $\\mathbb{R}^{p}$ -dimensional $(p\\in\\mathbb{N})$ standard Brownian motion, the drift $a:\\mathcal{X}\\to\\mathbb{R}^{d}$ , and the diffusion $b:\\mathcal{X}\\to\\mathbb{R}^{d\\times p}$ are assumed to be globally Lipschitz and sub-linear, so that the SDE (1) admits an unique solution $X=\\left(X_{t}\\right)_{\\geq0}$ with values in $\\left(\\mathcal{X},\\mathcal{B}(\\mathcal{X})\\right)$ . Processes akin to equations like (1) are diverse, spanning models like Langevin and Cox-IngersollRoss processes (see examples below), with broad applications in science and engineering. The process $X$ is a continuous-time Markov process with almost surely continuous sample paths whose dynamics is described by a family of probability densities $(p_{t})_{t\\in\\mathbb{R}_{+}}$ and transfer operators $(A_{t})_{t\\in\\mathbb{R}_{+}}$ such that for all $t\\in\\mathbb{R}_{+}$ , $E\\in B(\\vec{x})$ , $x\\in\\mathscr{X}$ and measurable function $f:\\mathcal{X}\\rightarrow\\mathbb{R}$ , ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbb{P}(X_{t}\\in E|X_{0}=x)=\\int_{E}p_{t}(x,y)d y\\;\\mathrm{and}\\;\\;A_{t}f=\\int_{\\mathcal{X}}f(y)p_{t}(\\cdot,y)d y=\\mathbb{E}\\big[f(X_{t})\\,\\big|\\,X_{0}=\\cdot\\big].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Evaluating $A_{t}f$ at $x$ yields the expectation of $f$ starting from $x$ and evolving until time $t$ , making the transfer operator crucial for understanding $X$ dynamics. The family $(A_{t})_{t\\in\\mathbb{R}_{+}}$ satisfies the fundamental semigroup equation $A_{t+s}=A_{t}\\circ A_{s}$ , for $s,t\\in\\ensuremath{\\mathbb{R}}_{+}$ . Here, we focus on the transfer operator\u2019s effect on the set $\\bar{\\mathcal{L}}_{\\pi}^{2}(\\mathcal{X})$ , a choice driven by the existence of an invariant measure $\\pi$ for $A_{t}$ on $(\\mathcal{X},\\mathcal{B}(\\mathcal{X}))$ which satisfies $A_{t}^{*}\\pi=\\pi$ for all $t\\in\\mathbb{R}_{+}$ . Then, the process $X$ is characterized by the infinitesimal generator $L$ defined for every $f\\in\\mathcal{L}_{\\pi}^{2}(\\mathcal{X})$ such that the limit $\\begin{array}{r}{L f=\\operatorname*{lim}_{t\\rightarrow0^{+}}(A_{t}f-\\dot{f})/t}\\end{array}$ exists in $\\mathcal{L}_{\\pi}^{2}(\\bar{\\mathcal{X}})$ . The operator $L$ is closed on its domain $\\mathrm{dom}(L)$ which is equal to the Sobolev space ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{W}_{\\pi}^{1,2}(\\mathcal{X})=\\{f\\in\\mathcal{L}_{\\pi}^{2}(\\mathcal{X})\\ |\\ ||f||_{\\mathcal{W}}^{2}=||f||_{\\mathcal{L}_{\\pi}^{2}}+||\\nabla f||_{\\mathcal{L}_{\\pi}^{2}}<\\infty\\}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "For SDE dynamics of the form (1), we can prove (see A.2 for details) that $L$ is the second-order differential operator given, for any $f\\in\\mathcal{L}_{\\pi}^{2}(\\dot{\\mathcal{X}})$ , $x\\in\\mathscr{X}$ , by ", "page_idx": 2}, {"type": "equation", "text": "$$\nL f(x)=\\nabla f(x)^{\\top}a(x)+\\frac{1}{2}\\mathrm{Tr}\\big[b(x)^{\\top}(\\nabla^{2}f(x))b(x)\\big],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\nabla^{2}f=(\\partial_{i j}^{2}f)_{i\\in[d],j\\in[p]}$ denotes the Hessian matrix of $f$ . ", "page_idx": 2}, {"type": "text", "text": "Spectral decomposition Knowing only the drift $a$ and diffusion $b$ is not enough to compute (2) or to understand quantitative aspects of dynamical phase transitions, such as time scales and metastable states. The eigenvalues and eigenvectors of the generator are crucial for capturing these effects. To address the possible unbounded nature of $L$ , one can turn to an auxiliary operator, the resolvent, which, under certain conditions, shares the same eigenfunctions as $L$ and becomes compact. When it exists and is continuous for some $\\mu\\in\\mathbb{C}$ , the operator $L_{\\mu}=(\\mu I-L)^{-1}$ is the resolvent of $L$ and the corresponding resolvent set is defined by ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\rho(L)=\\left\\{\\mu\\in\\mathbb{C}\\,\\vert\\,(\\mu I-L)\\,{\\mathrm{is~bijective~and}}\\,L_{\\mu}\\,{\\mathrm{is~continuous}}\\right\\}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "We assume $L$ has a compact resolvent, meaning $\\rho(L)\\neq\\emptyset$ and there exists $\\mu_{0}\\,\\in\\,\\rho(L)$ such that $(\\mu_{0}I-L)^{-1}$ is compact. Under this assumption, and given that $(L,\\mathrm{dom}(L))$ is self-adjoint, we can prove the spectral decomposition of the generator (see A.1 for details) as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\nL=\\sum_{i\\in\\mathbb{N}}\\lambda_{i}\\;f_{i}\\otimes f_{i}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $(\\lambda_{i})i\\in\\mathbb{N}$ are the eigenvalues of $L$ , and the corresponding eigenfunctions $f_{i}\\in\\mathcal{L}_{\\pi}^{2}(\\mathcal{X})$ , forming an orthonormal basis $(f_{i})_{i\\in\\mathbb{N}}$ , are also eigenfunctions of the transfer operator $A_{t}$ . ", "page_idx": 3}, {"type": "text", "text": "Dirichlet forms and energy To handle the initial lack of knowledge about the drift $a$ , we assume to have access to another quantity, called the energy, defined as $\\textstyle{\\mathfrak{E}}(f)^{-}{\\left|\\operatorname*{lim}_{t\\to0}\\int_{\\mathcal{X}}(f(f-A_{t}f))/t d\\pi\\right.}$ for all functions $f\\in\\mathcal{L}_{\\pi}^{2}(\\mathcal{X})$ for which this limit exists, defining in the way the domain $\\operatorname{dom}({\\mathfrak{E}})$ . The associated Dirichlet form is the bilinear form defined by polarization for any $f,g\\in\\mathrm{dom}(\\mathfrak{E})$ by ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathfrak{E}(f,g)=-\\int_{X}f(L g)d\\pi=\\int_{X}(-L f)g d\\pi.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "For every $f\\ \\in\\ \\mathrm{dom}(\\mathfrak{E})$ , we have $\\mathfrak{E}(f)~=~\\mathfrak{E}(f,f)$ . As for every $i\\;\\in\\;\\mathbb{N},\\;0\\;\\leq\\;\\mathfrak{E}(f_{i},f_{i})\\;=$ $\\begin{array}{r}{-\\int_{\\mathcal{X}}f_{i}(\\lambda_{i}f_{i})d\\pi=-\\lambda_{i}}\\end{array}$ , we check that the eigenvalues of $L$ are negative. To relate $L$ to Dirichlet form, we assume there exists a Dirichlet operator $B=s^{\\top}\\nabla$ where $s=[s_{1}\\,|\\,.\\,.\\,.\\,|\\,s_{p}]\\,:\\,x\\in\\mathcal{X}\\,\\mapsto$ $\\boldsymbol{s}(\\boldsymbol{x})\\!=\\![s_{1}(\\boldsymbol{x})\\,|\\,.\\,.\\,.\\,|\\,s_{p}(\\boldsymbol{x})]\\!\\in\\!\\mathbb{R}^{d\\times p}$ is a smooth function s.t. $L f=s(s^{\\top}\\nabla f)=s(B f)$ and so that ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\int_{X}(-L f)g d\\pi=\\int_{X}(s(x)s(x)^{\\top}\\nabla f(x))^{\\top}\\nabla g(x)\\pi(d x)=\\int_{X}(B f(x))^{\\top}(B g(x))\\pi(d x).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We get that for any $f\\in\\mathrm{dom}(\\mathfrak{E})$ ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathfrak{E}(f)=\\int_{\\chi}\\Vert s^{\\top}\\nabla f\\Vert^{2}d\\pi=\\mathbb{E}_{x\\sim\\pi}[\\Vert B f(x)\\Vert^{2}],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "which is reminiscent of the expected value of the kinetic energy in quantum mechanics [17]. ", "page_idx": 3}, {"type": "text", "text": "In the following, while we discuss in detail the examples of Overdamped Langevin and the CoxIngersoll-Ross processes, we briefly mention other ones that have a Dirichlet form: the Wright-Fisher diffusion (in dimension one), which can be defined in the context of population genetics and can be adapted to model interest rates, see [16], the geometric Brownian motion which models the price process of a financial asset, the multi-dimensional Brownian motion $(a{=}0)$ that corresponds to the heat equation, the transport processes associated with advection-diffusion equation, see [33], and the process associated with Poisson\u2019s equation in electrostatics, see [26]. ", "page_idx": 3}, {"type": "text", "text": "Example 1 (Langevin). Let $k_{b},T\\in\\mathbb{R}_{+}^{*}$ . The overdamped Langevin equation driven by $a$ potential $V:\\ensuremath{\\mathbb{R}^{d}}\\to\\ensuremath{\\mathbb{R}}$ is given by $\\begin{array}{r}{d X_{t}=-\\nabla V(X_{t})d t+\\sqrt{2(k_{b}T)}d W_{t}}\\end{array}$ and $X_{0}\\,=\\,x$ , where $k_{b}$ and $T$ respectively represent the coefficient of friction and the temperature of the system. Its infinitesimal generator $L$ is defined by $\\boldsymbol{L}\\boldsymbol{f}=-\\nabla V^{\\top}\\nabla\\boldsymbol{f}+(k_{b}T)\\Delta\\boldsymbol{f},$ , for $f\\in\\mathcal{W}_{\\pi}^{1,2}(\\mathcal{X})$ . Since $\\textstyle\\int(-{\\bar{L}}f)g\\,d\\pi=$ $\\begin{array}{r}{-\\int\\Big[\\nabla\\Big((k_{b}T)\\nabla f(x)\\frac{e^{-(k_{b}T)^{-1}V(x)}}{Z}\\Big)\\Big]g(x)d x\\,=\\,(k_{b}T)\\int\\nabla f^{\\top}\\nabla g\\,d\\pi\\,=\\,\\int f(-L g)\\,d\\pi.}\\end{array}$ , generator $L$ is self-adjoint and associated to a gradient Dirichlet form with $s(x)=(k_{b}T)^{1/2}(\\delta_{i j})_{i\\in[d],j\\in[p]}$ . ", "page_idx": 3}, {"type": "text", "text": "Example 2 (Cox-Ingersoll-Ross process). Let $d=1$ , $a,b\\in\\mathbb{R},$ , $\\sigma\\in\\mathbb{R}_{+}^{*}$ . The Cox-Ingersoll-Ross process is solution of the SDE $d X_{t}=(a+b X_{t})d t+\\sigma{\\sqrt{X_{t}}}d W_{t}$ and $X_{0}=x$ . Its infinitesimal generator $L$ is defined for $f\\in\\mathcal{L}_{\\pi}^{2}(\\mathcal{X})$ by $\\begin{array}{r}{L f=(a+b x)\\nabla f+\\frac{\\sigma^{2}x}{2}\\Delta f}\\end{array}$ . By integration by parts, we can check that the generator $L$ satisfies $\\begin{array}{r}{\\int(-L f)g\\,d\\pi=\\int\\frac{\\sigma^{2}x}{2}f^{\\prime}(x)g^{\\prime}(x)\\,\\pi(d x)=\\int f(-L g)\\,d\\pi,}\\end{array}$ , and it is associated to a gradient Dirichlet form with $s(x)=\\sigma\\sqrt{x}/\\sqrt{2}$ . ", "page_idx": 3}, {"type": "text", "text": "Learning in reproducing kernel Hilbert spaces (RKHSs) Throughout the paper we let $\\mathcal{H}$ be an RKHS and let $k:\\mathcal{X}\\times\\mathcal{X}\\to\\mathbb{R}$ be the associated kernel function. We let $\\phi:\\mathcal{X}\\to\\mathcal{H}$ be a feature map [45] such that $k(x,x^{\\prime})=\\langle\\phi(x),\\phi(x^{\\prime})\\rangle$ for all $x,x^{\\prime}\\in\\mathcal{X}$ . We consider RKHSs satisfying $\\mathcal{H}\\subset\\mathcal{L}_{\\pi}^{2}(\\bar{\\mathcal{X}})$ [45, Chapter 4.3], so that one can approximate $L:\\mathcal{L}_{\\pi}^{2}(\\mathcal{X})\\to\\mathcal{L}_{\\pi}^{2}(\\mathcal{X})$ with an operator $G:{\\mathcal{H}}\\rightarrow{\\mathcal{H}}$ . Notice that despite $\\mathcal{H}\\subset\\mathcal{L}_{\\pi}^{2}(\\mathcal{X})$ , the two spaces have different metric structures, that is for all $f,g\\in\\mathcal{H}$ , one in general has $\\ddot{\\langle f,g\\rangle}_{\\mathcal{H}}\\neq\\langle f,g\\rangle_{\\mathcal{L}_{\\pi}^{2}}$ . In order to handle this ambiguity, we introduce the injection operator $S_{\\pi}:{\\mathcal{H}}\\rightarrow{\\mathcal{L}}_{\\pi}^{2}({\\mathcal{X}})$ such that for all $f\\in\\mathcal H$ , the object $S_{\\pi}f$ is the element of $\\mathcal{L}_{\\pi}^{2}(\\vec{\\mathcal{X}})$ which is pointwise equal to $f\\in\\mathcal H$ , but endowed with the appropriate ${\\mathcal{L}}_{\\pi}^{2}$ norm. ", "page_idx": 3}, {"type": "text", "text": "Then, the infinitesimal generator restricted to $\\mathcal{H}$ is simply $L S_{\\pi}$ which can be estimated by $S_{\\pi}G$ for some $G\\in{\\mathrm{HS}}\\left({\\mathcal{H}}\\right)$ . This approach is based on the embedding $\\ell\\phi\\colon\\mathcal{X}\\rightarrow\\mathcal{H}$ of the generator in the RKHS that can be defined for kernels $k\\in\\mathcal{C}^{2}(\\mathcal{X}\\times\\mathcal{X})$ whenever one knows drift and diffusion coefficients, see $\\mathbf{B}$ , so that the reproducing property $\\langle\\ell\\phi(x),h\\rangle_{\\mathcal{H}}=[L S_{\\pi}h](x)$ holds. Based on this observation, [19] developed empirical estimators of $L S_{\\pi}$ that essentially minimize the risk $\\lVert L S_{\\pi}\\,-\\,S_{\\pi}G\\rVert_{\\mathrm{HS}(\\mathcal{H},\\mathcal{L}_{\\pi}^{2})}^{2}\\,=\\,\\dot{\\mathbb{E}}_{x\\sim\\pi}\\lVert\\ell\\dot{\\phi}(x)\\,-\\,G^{*}\\phi(x)\\rVert_{\\mathcal{H}}^{2}$ . In scenarios where drift and diffusion coefficients are not known, then $\\ell\\phi$ becomes non-computable. However if the process has the Dirichlet form (6), one can still empirically estimate the Galerkin projection $(S_{\\pi}^{*}S_{\\pi}^{\\bigstar})^{\\dagger}S_{\\pi}^{*}L S_{\\pi}$ onto $\\mathcal{H}$ , as considered in [1], which in fact minimizes the same risk. Yet this approach is problematic due to the unbounded nature of the generator and the associated estimators typically suffer from a large number of spurious eigenvalues around zero, making the estimation of physically most relevant eigenfunctions unreliable even for self-adjoint generators. Conversely, classical numerical methods can compute the leading part of a spectrum without spuriousness issues, suggesting that data-driven approaches should achieve similar reliability. In this paper, we address this problem by designing a novel notion of risk, leading to principled estimators designed to surmount these challenges. ", "page_idx": 4}, {"type": "text", "text": "3 Novel statistical learning framework ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we tackle the challenges in developing suitable generator estimators highlighted earlier. To this end, we introduce a risk metric for resolvent estimation that can be efficiently minimized empirically, leading to good spectral estimation. Since $L$ and $(\\mu I-L)^{-1}$ share the same eigenfunctions, the main idea is to learn the (compact) resolvent, which can be effectively approximated by finite-rank operators, instead of learning the generator directly. However, this approach is challenging due to the lack of closed analytical forms for the action of the resolvent. ", "page_idx": 4}, {"type": "text", "text": "First, given $\\mu\\,>\\,0$ , in order to approximate the action of the resolvent on the RKHS by some operator $G\\colon{\\mathcal{H}}\\ \\to\\ {\\mathcal{H}}$ , we introduce its embedding $\\chi_{\\mu}\\colon\\mathcal X\\ \\to\\ \\mathcal H$ via the reproducing property $\\langle\\chi_{\\mu}(x),h\\rangle_{\\varkappa}\\,=\\,[(\\mu I-L)^{-1}S_{\\pi}h](x)$ , formally given by $\\begin{array}{r}{\\chi_{\\mu}(x)\\,=\\,\\int_{0}^{\\infty}\\mathbb{E}[\\phi(X_{t})e^{-\\mu t}\\,|\\,X_{0}\\,=\\,x]d t,}\\end{array}$ see $\\mathbf{B}$ for details. Using this notation, we aim to estimate $[(\\mu I-L)^{-1}S_{\\pi}h](x)\\approx[G h](x),\\,h\\in$ $h\\in\\mathcal H$ , i.e. the objective is to estimate $\\chi_{\\mu}(x)\\approx G^{*}\\phi(x)\\;\\pi\\mathrm{-}\\mathrm{a.e}$ . ", "page_idx": 4}, {"type": "text", "text": "An obvious metric for the risk would be the mean square error (MSE) w.r.t. distribution $\\pi$ of the data. However, this becomes intractable since in general $\\chi_{\\mu}$ is not computable in closed form when either full or partial knowledge of the process is at hand. ", "page_idx": 4}, {"type": "text", "text": "To mitigate this issue, we introduce a different ambient space in which we study the resolvent ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{W}_{\\pi}^{\\mu}(\\mathcal{X})=\\{f\\in\\mathrm{dom}(L)\\mid\\|f\\|_{\\mathcal{W}_{\\pi}^{\\mu}(\\mathcal{X})}^{2}\\equiv\\mathfrak{E}_{\\mu}[f]=\\langle f,(\\mu I\\!-\\!L)f\\rangle_{\\mathcal{L}_{\\pi}^{2}}<\\infty\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the norm now balances the energy of an observable $f\\colon\\mathcal{X}\\rightarrow\\mathbb{R}$ w.r.t. the invariant distribution $\\|f\\|_{\\mathcal{L}_{\\pi}^{2}}^{2}$ and its energy w.r.t. the transient dynamics $-\\langle f,L f\\rangle_{\\mathcal{L}_{\\pi}^{2}}$ . Indeed ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathfrak{E}_{\\mu}[f]=\\mathbb{E}_{x\\sim\\pi}[\\mu|f(x)|^{2}-f(x)[L f](x)]=\\mathbb{E}_{x\\sim\\pi}[\\mu\\,|f(x)|^{2}+\\|s(x)^{\\top}\\nabla f(x)\\|^{2}],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the last equality holds for Dirichlet gradient form (6), in which case $\\mathcal{W}_{\\pi}^{\\mu}(\\mathcal{X})$ is simply a weighted Sobolev space. Importantly, this energy functional can be empirically estimated from data sampled from $\\pi$ , whenever full knowledge, that is drift and diffusion coefficients of the SDE (1), or partial knowledge, i.e. the diffusion coefficient and Dirichlet operator $B$ in (6), is at hand. With that in mind, instead of the standard MSE, we introduce the energy-based risk functional as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{G:\\,\\mathcal{H}\\to\\mathcal{H}}\\mathcal{R}(G)=\\mathfrak{E}_{\\mu}[\\|\\chi_{\\mu}(\\cdot)-G^{*}\\phi(\\cdot)\\|_{\\mathcal{H}}].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Denoting by $Z_{\\mu}\\colon{\\mathcal{H}}\\to\\mathcal{W}_{\\pi}^{\\mu}(X)$ the canonical injection, (8) can be equivalently written as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{R}(G)=\\Vert(\\mu I-L)^{-1}Z_{\\mu}-Z_{\\mu}G\\Vert_{\\mathrm{HS}(\\mathcal{H},\\mathcal{W})}^{2}=\\Vert(\\mu I-L)^{-1/2}S_{\\pi}-(\\mu I-L)^{1/2}S_{\\pi}G\\Vert_{\\mathrm{HS}(\\mathcal{H},\\mathscr{L}_{\\pi}^{2})}^{2},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where we abbreviated ${\\mathcal W}~=~{\\mathcal W}_{\\pi}^{\\mu}(X)$ and used $Z_{\\mu}^{*}\\;\\;=\\;\\;S_{\\pi}^{*}(\\mu I\\!-\\!L)$ , recalling that HilbertSchmidt and spectral norms for operators $A\\colon{\\mathcal{H}}\\to\\mathcal{W}$ , are $\\|A\\|_{\\mathrm{HS}(\\mathcal{H},\\mathcal{W})}{=}\\sqrt{\\mathrm{tr}(A^{*}(\\mu I{-}L)A)}$ and $\\lVert A\\rVert_{\\mathcal{H}\\to\\mathcal{W}}{=}\\sqrt{\\lambda_{1}(A^{*}(\\mu I{-}L)A)}\\ge\\mu^{-1/2}\\lVert A\\rVert_{\\mathcal{H}\\to\\mathcal{L}_{\\pi}^{2}}.$ ", "page_idx": 4}, {"type": "text", "text": "Therefore, (9) implies that the regularized energy norm, while dominating the classical ${\\mathcal{L}}_{\\pi}^{2}$ one, exerts a balancing effect on the estimation of the resolvent. This leads us to the first general result regarding the well-posedness of this framework. ", "page_idx": 4}, {"type": "text", "text": "Proposition 1. Given $\\mu>0,$ , let $\\mathcal{H}\\subseteq\\mathcal{W}_{\\pi}^{\\mu}(\\mathcal{X})$ be the RKHS associated to kernel $k\\in\\mathcal{C}^{2}(\\mathcal{X}\\times\\mathcal{X})$ such that $Z_{\\mu}~\\in~\\mathrm{HS}\\left({\\mathcal H},{\\mathcal W}_{\\pi}^{\\mu}(X)\\right)$ , and let $P_{\\mathcal{H}}$ be the orthogonal projector onto the closure of $\\operatorname{Im}(Z_{\\mu})\\subseteq{\\dot{\\mathcal{W}}}_{\\pi}^{\\mu}({\\mathcal{X}})$ . Then for every $\\varepsilon>0$ there exists a finite rank operator $G\\colon{\\mathcal{H}}\\!\\to{\\mathcal{H}}$ such that $\\mathcal{R}(G)\\leq\\|(I-P_{\\mathcal{H}})(\\mu I-L)^{-1}Z_{\\mu}\\|_{\\mathrm{HS}(\\mathcal{H},\\mathcal{W})}^{2}+\\varepsilon.$ . Consequently, when $k$ is universal, $\\mathcal{R}(G)\\leq\\varepsilon$ . ", "page_idx": 5}, {"type": "text", "text": "The previous proposition reveals that whenever the hypothetical domain is dense in the true domain and the injection operator is Hilbert-Schmidt, there is no irreducible risk and one can find arbitrarily good finite rank approximations of the generator\u2019s resolvent. Note that $Z_{\\mu}\\in\\mathrm{HS}\\left({\\mathcal{H}},{\\mathcal{W}}_{\\pi}^{\\mu}(X)\\right)$ is equivalent to $Z_{\\mu}^{*}Z_{\\mu}\\!=\\!S_{\\pi}^{*}(\\mu I\\!-\\!L)S_{\\pi}$ being a trace class operator, which is assured for our Examples 1 and 2, see the discussion in E. ", "page_idx": 5}, {"type": "text", "text": "Now, to address how minimization of the risk impacts the estimation of the spectral decomposition, let us define the operator norm error and the metric distortion functional, respectively, as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{E}(G)=\\Vert(\\mu I-L)^{-1}Z_{\\mu}-Z_{\\mu}G\\Vert_{\\mathcal{U}\\to\\mathcal{W}},\\,G\\in\\mathrm{HS}\\left(\\mathcal{H}\\right),\\,\\mathrm{~and~}\\,\\eta(h)=\\Vert h\\Vert_{\\mathcal{H}}/\\Vert h\\Vert_{\\mathcal{W}},\\,h\\in\\mathcal{H}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Proposition 2. Let $\\begin{array}{r}{\\widehat{G}=\\sum_{i\\in[r]}(\\mu-\\widehat{\\lambda}_{i})^{-1}\\,\\widehat{h}_{i}\\otimes\\widehat{g}_{i}}\\end{array}$ be the spectral decomposition of $\\widehat{G}\\colon\\mathcal{H}\\rightarrow\\mathcal{H}$ , where $\\widehat{\\lambda}_{i}\\geq\\widehat{\\lambda}_{i+1}$ and let $\\widehat{f}_{i}=S_{\\pi}\\widehat{h}_{i}\\,/\\,\\Vert S_{\\pi}\\widehat{h}_{i}\\Vert_{\\mathcal{L}_{\\pi}^{2}},f o$ r $i\\in[r]$ . Then for every $\\mu>0$ and $i\\in[r]$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{|\\lambda_{i}-\\widehat\\lambda_{i}|}{|\\mu-\\lambda_{i}||\\mu-\\widehat\\lambda_{i}|}\\leq\\mathcal{E}(\\widehat{G})\\eta(\\widehat{h}_{i})\\quad\\mathrm{~}a n d\\quad\\|\\widehat{f}_{i}-f_{i}\\|_{\\mathcal{L}_{\\pi}^{2}}^{2}\\leq\\frac{2\\,\\mathcal{E}(\\widehat{G})\\eta(\\widehat{h}_{i})}{\\mu\\,[\\mathrm{gap}_{i}-\\mathcal{E}(\\widehat{G})\\eta(\\widehat{h}_{i})]_{+}},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where ${\\mathrm{gap}}_{i}$ is the difference between $i$ -th and $(i+1)$ -th eigenvalue of $(\\mu I-L)^{-1}$ . ", "page_idx": 5}, {"type": "text", "text": "Note that the estimation of the eigenfunctions is first obtained in the norm with respect to the energy space, and then transformed to the ${\\mathcal{L}}_{\\pi}^{2}$ -norm, as $\\|f\\|_{\\mathcal{W}}\\geq\\sqrt{\\mu}\\|f\\|_{\\mathcal{L}_{\\pi}^{2}}$ , $f\\in W_{\\mu,\\gamma}$ . Therefore, by controlling the operator norm error and metric distortion (refer to $\\mathrm{C}$ ), we can guarantee accurate spectral estimation. Consequently, this allows us to approximately solve the SDE (1) starting from an initial condition ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[h(X_{t})\\,|\\,X_{0}\\!=\\!x]\\!=\\![e^{L t}S_{\\pi}h](x)\\approx\\sum_{i\\in[r]}e^{\\widehat\\lambda_{i}t}\\,\\langle\\widehat g_{i},h\\rangle_{\\mathcal{H}}\\widehat{h}_{i}(x).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "4 Empirical risk minimization ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section we address empirical risk minimization (ERM), deriving two main estimators. The first one minimizes empirical risk with Tikhonov regularization, while the second introduces additional regularization in the form of rank constraints. ", "page_idx": 5}, {"type": "text", "text": "To present the estimator, we denote the covariance operator w.r.t. ${\\mathcal{L}}_{\\pi}^{2}$ by $C=S_{\\pi}^{*}S_{\\pi}=\\mathbb{E}_{x\\sim\\pi}[\\phi(x)\\otimes$ $\\phi(\\bar{x})]$ , the cross-covariance operator $T\\,=\\,S_{\\pi}^{*}L S_{\\pi}$ , capturing correlations between input and the outputs of the generator, and the covariance operator $W_{\\mu}=Z_{\\mu}^{\\ast}Z_{\\mu}=S_{\\pi}^{\\ast}(\\mu I\\!-\\!L)S_{\\pi}$ w.r.t. energy space $\\mathcal{W}$ . All operators can be estimated from data, depending on the available prior knowledge. In this work we focus on the case when Dirichlet gradient form is known, i.e. we can define the embedding of the Dirichlet operator $B=s^{\\top}\\nabla\\colon\\check{\\mathcal L}_{\\pi}^{2}(\\mathcal X)\\rightarrow[\\mathcal L_{\\pi}^{2}(\\mathcal X)]^{p}$ into RKHS $d\\phi\\colon\\mathcal{X}\\to\\mathcal{H}^{p}$ via the reproducing property as $\\langle d\\phi(x),h\\rangle_{\\mathcal{H}}=[B S_{\\pi}h](x)=s(x)^{\\top}D h(x)\\in\\mathbb{R}^{p}$ , $h\\in\\mathcal H$ . More precisely, we have that $d\\phi({\\boldsymbol{x}})$ is a $p$ -dimensional vector with components $d_{k}\\phi(x)$ , where $d_{k}\\phi\\colon\\mathcal{X}\\rightarrow$ $\\mathcal{H}$ is given via reproducing property $\\langle d_{k}\\phi(x),h\\rangle_{\\mathcal{H}}=s_{k}(x)^{\\top}D h\\bar{(x)},k\\in[p]$ . Hence, in this case we have that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{T=-\\mathbb{E}_{x\\sim\\pi}[d\\phi(x)\\otimes d\\phi(x)]=-\\sum_{k\\in[p]}\\mathbb{E}_{x\\sim\\pi}[d_{k}\\phi(x)\\otimes d_{k}\\phi(x)].}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Moreover, defining $w\\phi\\colon\\mathcal{X}\\to\\mathcal{H}^{p+1}$ by $w\\phi(x)=[{\\sqrt{\\mu}}\\phi(x),d_{1}\\phi(x),\\ldots d_{p}\\phi(x)]^{\\top}\\in\\mathbb{R}^{p+1}$ , we get ", "page_idx": 5}, {"type": "equation", "text": "$$\nW_{\\mu}=\\mathbb{E}_{x\\sim\\pi}[\\mu\\phi(x)\\otimes\\phi(x)+d\\phi(x)\\otimes d\\phi(x)]=\\mathbb{E}_{x\\sim\\pi}[w\\phi(x)\\otimes w\\phi(x)].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Regularized risk Let us first introduce the regularized risk defined for some $\\gamma>0$ by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\gamma}(G)=\\Vert(\\mu I-L)^{-1/2}S_{\\pi}-(\\mu I-L)^{1/2}S_{\\pi}G\\Vert_{\\mathrm{HS}(\\mathcal{H},\\mathcal{L}_{\\pi}^{2})}^{2}+\\gamma\\mu\\Vert G\\Vert_{\\mathrm{HS}(\\mathcal{H})}^{2},\\quad G\\in\\mathrm{HS}\\left(\\mathcal{H}\\right),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "which, after some algebra, can be written as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{R}_{\\gamma}(G)\\,=\\,\\mathrm{tr}\\left[G^{*}(\\mu C{-}T{+}\\mu\\gamma I)G{-}2C G{+}S_{\\pi}^{*}(\\mu I-L)^{-1}S_{\\pi}\\right]}\\\\ &{\\qquad\\quad\\,=\\,\\underbrace{\\|W_{\\mu,\\gamma}^{-1/2}C-W_{\\mu,\\gamma}^{1/2}G\\|_{\\mathrm{HS}(\\mathcal H)}^{2}}_{\\mathrm{~}}+\\underbrace{\\mathfrak{E}_{\\mu}[\\chi_{\\mu}(\\cdot)]-\\|W_{\\mu,\\gamma}^{-1/2}C\\|_{\\mathrm{HS}(\\mathcal H)}^{2}}_{\\mathrm{~}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $W_{\\mu,\\gamma}=W_{\\mu}+\\mu\\gamma I$ is the regularized covariance w.r.t. $\\mathcal{W}$ . ", "page_idx": 6}, {"type": "text", "text": "Hence, assuming the access to the dataset $D_{n}=(x_{i})_{i\\in[n]}$ made of i.i.d. samples from the invariant distribution $\\pi$ , replacing the regularized energy ${\\mathfrak{E}}_{\\mu}$ with its empirical estimate ${\\widehat{\\mathfrak{E}}}_{\\mu}$ leads to the regularized empirical risk functional expressed as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\widehat{\\mathcal{R}}_{\\gamma}(G)=\\Vert\\widehat{W}_{\\mu,\\gamma}^{-1/2}\\widehat{C}-\\widehat{W}_{\\mu,\\gamma}^{1/2}G\\Vert_{\\mathrm{HS}(\\mathcal{H})}^{2}+\\widehat{\\mathfrak{C}}_{\\mu}[\\chi_{\\mu}(\\cdot)]-\\Vert\\widehat{W}_{\\mu,\\gamma}^{-1/2}\\widehat{C}\\Vert_{\\mathrm{HS}(\\mathcal{H})}^{2},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $W_{\\mu,\\gamma}$ and $C$ are estimated by their empirical counterparts $W_{\\mu,\\gamma}$ and $C$ , respectively, via (13). Therefore, our regularized empirical risk minimization approach reduces to ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{G}\\lvert|\\widehat{W}_{\\mu,\\gamma}^{-1/2}\\widehat{C}-\\widehat{W}_{\\mu,\\gamma}^{1/2}G\\rvert|_{\\mathrm{HS}(\\mathcal{H})}^{2},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "and we analyze two different estimators, the first one $\\widehat{G}_{\\mu,\\gamma}$ is obtained by minimizing regularized empirical risk (17) over all $G\\in{\\mathrm{HS}}\\left({\\mathcal{H}}\\right)$ , and, hence, the name Kernel Ridge Regression (KRR) of the generators resolvent. The second one $\\widehat{G}_{\\mu,\\gamma}^{r}$ minimizes (17) subject to the (hard) constraint that $G$ is at most of (a priori fixed) rank $r\\in\\mathbb N$ and, hence, is named Reduced Rank Regression (RRR) of the generator\u2019s resolvent. Notice that when $r=n$ , the two estimators coincide. After some algebra, one sees that both minimization problems have closed form solutions ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\widehat{G}_{\\mu,\\gamma}=\\widehat{W}_{\\mu,\\gamma}^{-1}\\widehat{C}\\qquad\\mathrm{~and~}\\qquad\\widehat{G}_{\\mu,\\gamma}^{r}=\\widehat{W}_{\\mu,\\gamma}^{-1/2}\\|\\widehat{W}_{\\mu,\\gamma}^{-1/2}\\widehat{C}\\|_{r},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $[\\![\\cdot]\\!]_{r}$ denotes the $r$ -truncated SVD of a compact operator. ", "page_idx": 6}, {"type": "text", "text": "To conclude this section, we show how to compute the eigenvalue decomposition of (18). To this end, we define the sampling operators ${\\widehat{S}}\\colon{\\mathcal{H}}\\to\\mathbb{R}^{n}$ and $\\widehat{Z}_{\\mu}\\colon\\bar{\\mathcal{H}}\\to\\mathbb{R}^{(1+p)n}$ by ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(\\widehat{S}h)_{i}=\\frac{1}{\\sqrt{n}}h(x_{i}),i\\in[n],\\quad\\mathrm{~and~}\\quad\\;(\\widehat{Z}_{\\mu}h)_{k n+i}=\\left\\{\\begin{array}{l l}{\\frac{\\sqrt{\\mu}}{\\sqrt{n}}\\,h(x_{i}),}&{k=0,i\\in[n],}\\\\ {\\frac{1}{\\sqrt{n}}s_{k}(x_{i})^{\\top}D h(x_{i}),}&{k\\in[p],i\\in[n].}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Further, let $\\mathbf{K}=\\ n^{-1}[k(x_{i},x_{j})]_{i,j\\in[n]}\\,\\in\\,\\mathbb{R}^{n\\times n}$ be kernel Gram matrix, and introduce the Gram matrices $\\mathbf{N}\\!\\in\\!\\mathbb{R}^{n\\times p n}$ and $\\mathbf{M}\\!\\in\\!\\mathbb{R}^{p n\\times p n}$ whose elements, for $k\\in[1\\!+\\!p],i,j\\in[n]$ are ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbf{M}_{(k-1)n+i,(\\ell-1)n+j}\\!=\\!n^{-1}\\langle d_{k}\\phi(x_{i}),d_{\\ell}\\phi(x_{j})\\rangle_{\\mathcal{H}}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We note that although we have introduced the above matrices via inner products in $\\mathcal{H}$ , they can be readily computed via the kernel and its gradients knowing the Dirichlet form, see $\\mathrm{D}$ . ", "page_idx": 6}, {"type": "text", "text": "Theorem 1. Given $\\mu>0$ and $\\gamma>0$ , let $\\mathbf{J}_{\\mu,\\gamma}\\!=\\!\\mathbf{K}\\!-\\!\\mathbf{N}(\\mathbf{M}\\!+\\!\\gamma\\mu I)^{-1}\\mathbf{N}^{\\top}\\!+\\!\\gamma I.$ . Let $(\\widehat{\\sigma}_{i}^{2},v_{i})_{i\\in[r]}$ be the leading eigenpairs of the following generalized eigenvalue problem ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mu^{-1}(\\mathbf{J}_{\\mu,\\gamma}-\\gamma I)\\mathbf{K}v_{i}=\\widehat\\sigma_{i}^{2}\\mathbf{J}_{\\mu,\\gamma}v_{i},\\quad v_{i}^{\\top}\\mathbf{K}v_{j}\\!=\\!\\delta_{i j},~i,j\\in[r].\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Denoting $\\mathbf{V}_{r}\\!=\\!\\left[v_{1}\\,\\big|.\\,.\\,.\\,\\right|v_{r}\\right]\\!\\in\\!\\mathbb{R}^{n\\times r}$ and $\\Sigma_{r}\\!=\\!\\mathrm{diag}(\\widehat{\\sigma}_{1},\\ldots,\\widehat{\\sigma}_{r})$ , $i f\\big(\\nu_{i},w_{i}^{\\ell},w_{i}^{r}\\big)_{i\\in[r]}$ are eigentriplets of matrix $\\mathbf{V}_{r}^{\\top}\\mathbf{V}_{r}\\Sigma_{r}^{2}\\in\\mathbb{R}^{r\\times r}$ , then the eigenvalue decomposition the RRR estimator $\\widehat{G}_{\\mu,\\gamma}^{r}=\\widehat{Z}_{\\mu}\\mathbf{U}_{r}\\mathbf{V}_{t}^{\\top}\\widehat{S}$ is given by $\\begin{array}{r}{\\widehat{G}_{\\mu,\\gamma}^{r}\\;=\\;\\sum_{i\\in[r]}(\\mu-\\widehat{\\lambda}_{i})^{-1}\\,\\widehat{h}_{i}\\,\\otimes\\,\\widehat{g}_{i}}\\end{array}$ , where $\\widehat{\\lambda}_{i}\\;=\\;\\mu-1/\\nu_{i}$ , $\\widehat{g}_{i}=\\nu_{i}^{-1/2}\\widehat{S}^{*}\\mathbf{V}_{r}w_{i}^{\\ell}$ $\\widehat{h}_{i}\\!=\\!\\widehat{Z}_{\\mu}^{*}\\mathbf{U}_{r}w_{i}^{r}$ for $\\mathbf{U}_{r}\\!=\\!(\\mu\\gamma)^{-1}[\\mu^{-1/2}I\\,|\\,-\\mathbf{N}(\\mathbf{M}\\!+\\!\\gamma\\mu I)^{-1}]^{\\top}\\big(\\mathbf{K}\\mathbf{V}_{r}\\!-\\!\\mu\\mathbf{V}_{r}\\Sigma_{r}^{2}\\big)\\!\\in\\!\\mathbb{R}^{(1\\!+\\!p)n\\times r}.$ ", "page_idx": 6}, {"type": "text", "text": "The main computational cost of our method, in view of (12), to solve SDE (1) lies in the implicit inversion of $\\mathbf{J}_{\\mu,\\gamma}$ when solving (20). When computed with direct solvers this inversion is of the order $\\mathcal{O}(n^{3}p^{3})$ , however leveraging on the fact that $\\mu\\mathbf{J}_{\\mu,\\gamma}$ is Schur\u2019s complement of the $(1\\!+\\!p)n\\!\\times\\!(1\\!+\\!p)n$ symmetric positive definite matrix and using classical iterative solvers, like Lanczos or the generalized Davidson method, when $r\\ll n$ this cost can significantly be reduced to $O(r\\,n^{2}p^{2})$ , c.f. [18]. ", "page_idx": 6}, {"type": "text", "text": "5 Spectral learning bounds ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Recalling Prop. 2, in order to obtain the bounds on eigenvalues and eigenfunctions of the generator, it suffices to analyze the learning rates for the operator norm error $\\mathcal{E}$ and metric distortion $\\eta$ . For this purpose, we analyze the operator norm error of empirical estimator $\\widehat{G}_{\\mu,\\gamma}^{r}$ using the decomposition ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}(\\widehat{G})\\leq\\|(\\mu I-L)^{-1}Z_{\\mu}-Z_{\\mu}G_{\\mu,\\gamma}\\|_{\\mathcal{H}\\to\\mathcal{W}}+\\|Z_{\\mu}(G_{\\mu,\\gamma}-G_{\\mu,\\gamma}^{r})\\|_{\\mathcal{H}\\to\\mathcal{W}}+\\|Z_{\\mu}(G_{\\mu,\\gamma}^{r}-\\widehat{G}_{\\mu,\\gamma}^{r})\\|_{\\mathcal{H}\\to\\mathcal{W}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $G_{\\mu,\\gamma}=W_{\\mu,\\gamma}^{-1}C$ is the minimizer of the full (i.e. without rank constraint) Tikhonov regularized risk and $G_{\\mu,\\gamma}^{r}=\\dot{W}_{\\mu,\\gamma}^{-1/2}\\[W_{\\mu,\\gamma}^{-1/2}C]_{r}$ is the population version of the empirical estimator $\\widehat{G}_{\\mu,\\gamma}^{r}$ . ", "page_idx": 7}, {"type": "text", "text": "Note that, while the last two terms in the above decomposition depend on the estimator, the first term depends only on the choice of $\\mathcal{H}$ and the regularity of $L$ w.r.t. $\\mathcal{H}$ . In this work we focus on the classical kernel-based learning where one chooses a universal kernel [45, Chapter 4] and controls the regularization bias with a regularity condition. For details see Rem. 2 of App. E.2. Let $\\mu>0$ be a prescribed parameter, we make following assumptions to quantify the difficulty of the learning problem: ", "page_idx": 7}, {"type": "text", "text": "(BK) Boundedness. There exists $c_{\\mathcal{W}}{>}0$ such that $\\operatorname*{ss}_{r\\sim\\pi}\\|w\\phi(x)\\|^{2}\\!\\le\\!c_{\\mathcal{W}}$ , i.e. $w\\phi\\!\\in\\!\\mathcal{L}_{\\pi}^{\\infty}(\\mathcal{X},\\mathcal{H}^{1\\!+\\!p})$ ;   \n(RC) Regularity. For some $\\alpha\\in(0,2]$ there exists $c_{\\alpha}>0$ such that $C^{2}\\preceq c_{\\alpha}^{2}W_{\\mu}^{1+\\alpha}$ ; ", "page_idx": 7}, {"type": "text", "text": "(SD) Spectral Decay. There exists $\\beta\\in(0,1]$ and $c_{\\beta}>0$ s.t. $\\lambda_{j}(W_{\\mu})\\le c_{\\beta}\\,j^{-1/\\beta}$ , for all $j\\in J$ . ", "page_idx": 7}, {"type": "text", "text": "The above assumptions, discussed in more details in App E.1, are in the spirit of state-of-the-art analysis of statistical learning theory of classical regression in RKHS spaces [14], recently extended to regression of transfer operators [35, 28]. In our setting, instead of relying on the injection into ${\\mathcal{L}}_{\\pi}^{2}$ as in the case of transfer operators, the relevant object is the injection $Z_{\\mu}$ into the energy space $\\mathcal{W}$ . ", "page_idx": 7}, {"type": "text", "text": "The first assumption $({\\bf B K})$ is the main limiting factor of our approach. Indeed, since $\\|w\\phi(x)\\|^{2}=$ $\\begin{array}{r}{\\mu\\|\\phi(x)\\|^{2}+\\sum_{k\\in[p]}\\|d_{k}\\phi(x)\\|^{2}}\\end{array}$ , apart from needing the kernel to be bounded, we also need the Dirichlet form embedding to be bounded. Essentially, this means that the Dirichlet coefficients are not growing too fast w.r.t. the kernel\u2019s gradients decay. Having this, we assure that $Z_{\\mu}\\in\\mathrm{HS}\\left({\\mathcal{H}},{\\mathcal{W}}_{\\pi}^{\\mu}({\\mathcal{X}})\\right)$ which implies the operator norm error can be controlled. ", "page_idx": 7}, {"type": "text", "text": "Another key difference between generator and transfer operator regression is that the covariance w.r.t. the domain of the operator becomes $W_{\\mu}=Z_{\\mu}^{*}Z_{\\mu}$ instead of $C=S_{\\pi}^{*}S_{\\pi}$ . On the other hand, the \"cross-covariance\" that captures now RKHS cross-correlations of the resolvent w.r.t domain is simply $Z_{\\mu}^{*}(\\mu I-L)^{-1}Z_{\\mu}=S_{\\pi}^{*}(\\mu I{-}L)(\\mu I-L)^{-1}S_{\\pi}=C$ . With this in mind, (RC) corresponds to the regularity condition in [28] and it quantifies the relationship between the hypothesis class (bounded operators in $\\mathcal{H}$ ) and the object of interest $(\\mu I-L)^{-1}$ . Indeed, if $L$ has eigenfunctions that belong to $\\alpha$ -interpolation space between $\\mathcal{H}$ and $\\mathcal{W}_{\\pi}^{\\mu}(\\mathcal{X})$ , (RC) holds true. In particular, if $f_{i}\\in\\mathcal{H}$ for all $i\\geq2$ (constant eigenfunction excluded), one has that $\\alpha\\geq1$ (c.f. Proposition 7 in App. E.1. ", "page_idx": 7}, {"type": "text", "text": "Finally, (SD) quantifies the \"size\" of the hypothetical domain $\\mathcal{H}$ within the true domain $\\mathcal{W}_{\\pi}^{\\mu}(\\mathcal{X})$ via the effective dimension $\\mathrm{tr}(W_{\\mu,\\gamma}^{-1}W_{\\mu})\\leq c_{\\beta}(\\mu\\gamma)^{-\\beta}$ , which, due to (BK), leads to another notion, known as the embedding property, quantifying the relationship between $\\mathcal{H}$ and essentially bounded functions in the domain of the operator ", "page_idx": 7}, {"type": "text", "text": "(KE) Kernel Embedding. There exists $\\tau\\in[\\beta,1]$ and such that ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{c_{\\tau}=\\exp\\operatorname*{sup}\\sum_{j\\in\\mathbb{N}}\\sigma_{j}^{2\\tau}[\\mu|z_{j}(x)|^{2}-z_{j}(x)[L z_{j}](x)]<+\\infty,}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Using the above assumptions, we prove generalization bound for the RRR estimator, notably addressing the general case when the prior knowledge of the Dirichlet coefficient is inexact, i.e. ", "page_idx": 7}, {"type": "text", "text": "(DF) Dirichlet form. For some $\\epsilon\\!\\in\\![0,1)$ there exists $s_{\\epsilon}$ : $\\mathbb{R}^{d}{\\rightarrow}\\mathbb{R}^{p}$ so that $(1\\!-\\!\\epsilon)s^{\\top}s{\\preceq}s_{\\epsilon}^{\\top}s_{\\epsilon}{\\preceq}(1{+}\\epsilon)s^{\\top}s$ holds $\\pi$ -a.e., where $s\\colon\\ensuremath{\\mathbb{R}}^{d}{\\to}\\ensuremath{\\mathbb{R}}^{p}$ is such that $\\scriptstyle{L=B^{*}B}$ for $B{=}s^{\\top}\\nabla\\colon\\mathcal{L}_{\\pi}^{2}(\\mathcal{X})\\rightarrow[\\mathcal{L}_{\\pi}^{2}(\\mathcal{X})]^{p}$ . ", "page_idx": 7}, {"type": "text", "text": "Recalling the form of IG in Equation 3, the Dirichlet form in $(\\mathbf{D}\\mathbf{F})$ of a self-adjoint generator exists whenever the positive definite diffusion part satisfie\u221as uniform ellipticity conditions and the drift term allows integration by parts, leading to $s(x){=}b(x)/\\sqrt{2}$ . Thus, to obtain the partial knowledge we need, it is enough to estimate the diffusion function $s_{\\epsilon}$ with the relative error bound in $(\\mathbf{D}\\mathbf{F})$ , see App. E.1. ", "page_idx": 7}, {"type": "text", "text": "Theorem 2. Let $(\\mathbf{D}\\mathbf{F})$ , (RC), (SD), and $(\\bf K E)$ hold for some $\\epsilon\\,\\in\\,[0,1)$ , $\\mathrm{~\\boldmath~\\cdot~}\\alpha\\,\\in\\,(0,2],\\;\\beta\\,\\in\\,(0,1]$ and $\\tau\\in[\\beta,1]$ , respectively, and let $\\operatorname{cl}(\\operatorname{Im}(S_{\\pi}))={\\mathcal{L}}_{\\pi}^{2}(\\chi)$ . Denoting $\\lambda_{k}^{\\star}=\\lambda_{k}\\big(S_{\\pi}^{\\ast}(\\mu I-L)^{-1}S_{\\pi}\\big)$ , $k\\,\\in\\,\\mathbb{N}$ , and given $\\delta\\in(0,1)$ and $r\\,\\in\\,[n]_{\\cdot}$ , if RRR estimator is built from the Dirichlet coefficients $s_{\\epsilon}\\colon\\mathbb{R}^{d}\\to,\\mathbb{R}^{p}$ and $\\gamma{\\asymp}n^{-{\\frac{1}{\\alpha+\\beta}}}$ and $^{\\iota}\\varepsilon_{n}^{\\star}\\!=\\!n^{-\\frac{\\alpha}{2(\\alpha+\\beta)}}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!w h e n\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\alpha\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\geq\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\geq\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\gamma\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\varepsilon_{n}^{\\star}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\$ (22) ", "page_idx": 7}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "then there exists a constant $c\\!>\\!0$ , depending only on $\\mathcal{H}$ and gap $\\lambda_{r}^{\\star}{-}\\lambda_{r+1}^{\\star}{>0}$ , such that for large enough $n\\geq r$ with probability at least $1-\\delta$ in the i.i.d. draw of $\\mathcal{D}_{n}$ from $\\pi$ it holds that ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{E}(\\widehat{G}_{\\mu,\\gamma}^{r})\\leq(\\widehat{\\sigma}_{r+1}\\,\\wedge\\,\\sqrt{\\lambda_{r+1}^{\\star}})+c\\,\\big(\\varepsilon_{n}^{\\star}\\,\\ln\\delta^{-1}+\\epsilon\\big).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Proof sketch. The regularization bias is bounded by $c_{\\alpha}\\,\\gamma^{\\alpha/2}$ by Prop. 9 of App. E.2, the rank reduction bias is upper bounded by $\\lambda_{r+1}\\big(S_{\\pi}^{*}(\\mu I-L)^{-1}S_{\\pi}\\big)$ , while in the exact knowledge case $(\\epsilon{=}0)$ ) the bounds on the variance terms critically rely on the well-known perturbation result for spectral projectors reported in Prop. 4, App. A. The latter is then chained to Pinelis-Sakhanenko\u2019s inequality and Minsker\u2019s inequality for self-adjoint HS-operators, Props. 12 and 13 in App. E.3.1, respectively. When the knowledge is not exact, that is $\\epsilon>0$ in $(\\mathbf{D}\\mathbf{F})$ , this relative bound implies that $\\epsilon\\widehat{W}_{\\mu}{\\preceq}\\widehat{W}_{\\mu}{-}\\widehat{W}_{\\mu}^{\\epsilon}{\\preceq}\\epsilon\\,\\widehat{W}_{\\mu}$ , where the empirical covariance with the inexact Dirichlet coefficient $s_{\\epsilon}$ is denoted by $\\widehat{W}_{\\mu}^{\\epsilon}$ . This allows one to control the additional approximation error in the analysis of variance, paying the price of additive term \u03f5. Combining the bias and variance terms, we obtain the balancing equations for the regularization parameter and then the next result follows. \u25a1 ", "page_idx": 8}, {"type": "text", "text": "First, note that the learning rate (23) implies the ${\\mathcal{L}}_{\\pi}^{2}$ -norm learning rate. Moreover, for $\\alpha\\geq\\tau$ , it matches information theoretic lower bounds for transfer operator learning upon replacing parameters $\\alpha$ , $\\beta$ and $\\tau$ related to the space $\\mathcal{W}$ with their ${\\mathcal{L}}_{\\pi}^{2}$ analogues [28], see App. E.6. This motivates the development of the first mini-max optimality for the IG learning, for which our results are an important first step. Next, remark that Theorem 2 guarantees the reliability of fully data-driven methods when the diffusion coefficients are not known but estimated. Furthermore, when $b$ is constant or linear (e.g. Overdamped Langevin and CIR), the classical estimation bounds coincide with the relative error bound of assumption (DF). ", "page_idx": 8}, {"type": "text", "text": "To conclude this section, we address the spectral learning bounds steaming from the Prop. 2. The main task to do so is to control the metric distortions, which we show how in App. E.5. In this context, an additional assumption $\\alpha\\geq1$ is needed, since otherwise the metric distortions can blow-up due to eigenfunctions being out of the RKHS space. Importantly, our analysis reveals that ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{|\\lambda_{i}-\\widehat\\lambda_{i}|}{|\\mu-\\lambda_{i}||\\mu-\\widehat\\lambda_{i}|}\\leq\\left(\\widehat{s}_{i}\\,\\wedge\\,2\\sqrt{\\lambda_{r+1}^{\\star}/\\lambda_{r}^{\\star}}\\right)+c\\left(\\varepsilon_{n}^{\\star}\\,\\ln\\delta^{-1}+\\epsilon\\right),\\;i\\in[r],}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\widehat{s}_{i}=\\widehat{\\sigma}_{i}\\,\\widehat{\\eta}_{i}$ is the empirical spectral bias that informs how good is the estimation of the particular eigenpair is (s ee Fig. 1 a)), ${\\widehat{\\sigma}}_{i}$ being given in (20) and $\\widehat{\\eta}_{i}=\\|\\widehat{h}_{i}\\|_{\\mathcal{H}}/\\|(\\widehat{W}_{\\mu}^{\\epsilon})^{1/2}\\widehat{h}_{i}\\|_{\\mathcal{H}}$ . Importantly, (24) reveals that our data-drive n  method for spectral deco  mposition of differential operator $L$ does not suffer from the curse of dimensionality as present in the classical numerical methods, see App. E.6. ", "page_idx": 8}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we showcase the key features of our method outlined in Table 1. We demonstrate that our approach: (1) avoids the spurious effects noted in other IG methods [19, 1], (2) is more effective than transfer operator methods [27], and (3) validates our bounds in a prediction task for a model with a non-constant diffusion term. Further details are available in Appendix F. ", "page_idx": 8}, {"type": "text", "text": "One dimensional four well potential We first investigate the overdamped Langevin dynamics in a potential that presents four different wells, two principal wells and then in each of them two smaller wells, given by $V(x)\\!=\\!4(x^{8}\\!+\\!0.8\\exp(-80(\\bar{x}^{2}))\\!+\\!0.2\\exp(-80(x\\!-\\!0.5)^{2})\\!+\\!0.5\\exp(-40(x\\!+\\!0.5)^{2}))$ . This leads to three relevant eigenpairs: the slowest mode corresponds to the transition between the two principal wells, while the others two capture transitions between the smaller wells. In Fig. 1 panel a), we show that the empirical bias $\\widehat{s}_{1}\\,=\\,\\widehat{\\sigma}_{1}\\,\\widehat{\\eta}_{1}$ allows us to choose the hyperparameters of the model, that is higher empirical bias coincides  with unreliable estimation of the operator\u2019s eigenfunction. In panel b) we observe how it varies w.r.t. land-scale (y-axis) and regularization $\\gamma$ (x-axis) hyperparameters showcasing the robustness of the model, see also Fig. 3 of App. F for hyperparameter $\\mu$ . Further, in panel c) we show the consistency of our model with the true Boltzmann distribution. Namely, we use our model to forecast the conditional probability density function (pdf) of the system. We perform the same procedure with prefect knowledge and imperfect diffusion coefficient estimated from data. We also report that if the same approach is used with the method described in [19, 43, 1], no dynamical quantity can be forecast due to the presence of numerous spurious eigenpairs, which prevent the system from relaxing towards the Boltzmann distribution. This issue is further illustrated in panel d) where we show how, contrary to KRR method of [19, 1], we avoid spuriousness in the estimation of eigenvalues. ", "page_idx": 8}, {"type": "text", "text": "CIR model Next, with the CIR model we show that our method is not limited to Langevin process with constant diffusion. For this process, the conditional expectation of the state $X_{t}$ is analytically known. We can thus compare the prediction of our model with respect to this expectation using root mean squared error (RMSE) and compute it for different number of samples to validate our bounds. Conditional expectation were computed on 100 different simulations at $t=\\ln(2)/a$ which corresponds to the half life of the mean reversion. Results are shown in panel e) of Fig. 1. ", "page_idx": 8}, {"type": "image", "img_path": "H7SaaqfCUi/tmp/1acbf9e9fa6d1053467a41c1911849d75b97b26f3a009081fdf8a5382e027cd9.jpg", "img_caption": ["Figure 1: a) Empirical biases $\\hat{s}_{1}=\\widehat\\sigma_{1}\\,\\widehat\\eta_{1}$ and estimation of the first (nontrivial) eigenfunction of the IG of a Langevin process under a four- well potential. The ground truth is shown in black, our method RRR is red and blue for two different kernel lengthscales. d) Estimation by our method (black) of the eigenvalues for the same process (red) compared to the methods in [19, 1], for which eigenvalue histogram in blue shows spuriousness. e) Prediction RMSE for the CIR model w.r.t. number of samples. f) Performance of our data-driven method and fitted CIR model on the real data of US mortgage rates. g) The second eigenfunction of a Langevin process under Muller brown potential (white level lines) with its estimation by RRR h) and Transfer Operator (TO) in e). Observe that TO fails to recover the metastable state. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "US mortgage rates We have trained our method on a real 30-year US mortgage rates dataset and contrasted it with the ftited CIR model using continuous ranked probability scores that are estimated from the forecasts obtained by of each of them, see panel f) of Fig. 1. Each model has been trained using data from January 2009 to December 2016. The initial condition was the last week of December 2016 and the predictions were made for the years 2017 and 2018. Since the dataset is real, we used the imperfect partial knowledge, that is, for our method, we estimated the diffusion coefficient only via a least squares calibration of a CIR model over the training set. This allows more flexibility on the drift term in our model. ", "page_idx": 9}, {"type": "text", "text": "Muller-Brown potential We next study Langevin dynamics under more challenging conditions: the Muller-Brown potential. Panels g)-h)-i) of Fig. 1 depict the second eigenfunction obtained by our method compared to the ground truth one, as well as the one found by the transfer operator approach, with the same number of samples. Notably, our physics informed approach outperforms transfer operator learning for this task. Note that with different lag times, we were able to recover this second eigenfunction. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We developed a novel energy-based framework for learning the Infinitesimal Generator of stochastic diffusion SDEs using kernel methods. Our approach integrates physical priors, achieves fast error rates, and provides the first spectral learning guarantees for generator learning. A limitation is its computational complexity, scaling as $n^{2}d^{2}$ . Future work will explore alternative methods to enhance computational efficiency and investigate a broader suite of SDEs beyond stochastic diffusion. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was partially funded by EU Project ELIAS under grant agreement No. 101120237, and by the European Union - NextGenerationEU initiative and the Italian National Recovery and Resilience Plan (PNRR) from the Ministry of University and Research (MUR), under Project PE0000013 CUP J53C22003010006 \"Future Artificial Intelligence Research (FAIR)\". We also extend our gratitude to the anonymous reviewers for their valuable feedback, which inspired us to enhance the quality and depth of our results. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] A Cabannes, V. and Bach, F. (2024). The Galerkin method beats graph-based approaches for spectral algorithms. In Proceedings of The 27th International Conference on Artificial Intelligence and Statistics, volume 238 of Proceedings of Machine Learning Research, pages 451\u2013459. PMLR.   \n[2] Alexander, R. and Giannakis, D. (2020). Operator-theoretic framework for forecasting nonlinear time series with kernel analog techniques. Physica D: Nonlinear Phenomena, 409:132520.   \n[3] Andrew, G., Arora, R., Bilmes, J., and Livescu, K. (2013). Deep canonical correlation analysis. In International conference on machine learning, pages 1247\u20131255. PMLR.   \n[4] Bakry, D., Gentil, I., and Ledoux, M. (2014). Analysis and Geometry of Markov Diffusion Operators. Springer.   \n[5] Bevanda, P., Beier, M., Kerz, S., Lederer, A., Sosnowski, S., and Hirche, S. (2021). KoopmanizingFlows: Diffeomorphically Learning Stable Koopman Operators. arXiv preprint arXiv.2112.04085.   \n[6] Bouvrie, J. and Hamzi, B. (2017). Kernel Methods for the Approximation of Nonlinear Systems. SIAM Journal on Control and Optimization, 55(4):2460\u20132492.   \n[7] Brunton, S. L., Budi\u0161i\u00b4c, M., Kaiser, E., and Kutz, J. N. (2022). Modern Koopman Theory for Dynamical Systems. SIAM Review, 64(2):229\u2013340.   \n[8] Caponnetto, A. and De Vito, E. (2007). Optimal rates for the regularized least-squares algorithm. Foundations of Computational Mathematics, 7(3):331\u2013368.   \n[9] Comte, F. and Genon-Catalot, V. (2020). Nonparametric drift estimation for iid paths of stochastic differential equations. The Annals of Statistics, 48(6):3336\u20133365.   \n[10] Das, S. and Giannakis, D. (2020). Koopman spectra in reproducing kernel Hilbert spaces. Applied and Computational Harmonic Analysis, 49(2):573\u2013607.   \n[11] Davis, C. and Kahan, W. M. (1970). The rotation of eigenvectors by a perturbation. iii. SIAM Journal on Numerical Analysis, 7(1):1\u201346.   \n[12] Fan, F., Yi, B., Rye, D., Shi, G., and Manchester, I. R. (2021). Learning Stable Koopman Embeddings. arXiv preprint arXiv.2110.06509.   \n[13] Fan, J. and Zhang, C.-H. (2003). A reexamination of diffusion estimators with applications to financial model validation. Journal of the American Statistical Association, 98(461):118\u2013134.   \n[14] Fischer, S. and Steinwart, I. (2020). Sobolev norm learning rates for regularized least-squares algorithms. Journal of Machine Learning Research, 21(205):1\u201338.   \n[15] Florens-Zmirou, D. (1989). Approximate discrete-time schemes for statistics of diffusion processes. Statistics: A Journal of Theoretical and Applied Statistics, 20(4):547\u2013557.   \n[16] Grothaus, M. and Sauerbrey, M. (2023). Dirichlet form analysis of the jacobi process. Stochastic Processes and their Applications, 157:81\u2013104.   \n[17] Haag, R. (2012). Local quantum physics: Fields, particles, algebras. Springer Science & Business Media.   \n[18] Hogben, L., editor (2006). Handbook of Linear Algebra. CRC Press, Boca Raton, FL, USA.   \n[19] Hou, B., Sanjari, S., Dahlin, N., Bose, S., and Vaidya, U. (2023). Sparse learning of dynamical systems in RKHS: An operator-theoretic approach. In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J., editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 13325\u201313352. PMLR.   \n[20] Inzerilli, P., Kostic, V., Lounici, K., Novelli, P., and Pontil, M. (2023). Consistent long-term forecasting of ergodic dynamical systems.   \n[21] Jacod, J. (2012). Discretization of Processes. Springer.   \n[22] Kawahara, Y. (2016). Dynamic Mode Decomposition with Reproducing Kernels for Koopman Spectral Analysis. In Lee, D., Sugiyama, M., Luxburg, U., Guyon, I., and Garnett, R., editors, Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc.   \n[23] Kessler, M., Lindner, A., and S\u00f8rensen, M. (2012). Statistical methods for stochastic differential equations. Monographs on Statistics and Applied Probability, 124:7\u201312.   \n[24] Klus, S., N\u00fcske, F., Peitz, S., Niemann, J.-H., Clementi, C., and Sch\u00fctte, C. (2020). Data-driven approximation of the koopman generator: Model reduction, system identification, and control. Physica D: Nonlinear Phenomena, 406:132416.   \n[25] Klus, S., Schuster, I., and Muandet, K. (2019). Eigendecompositions of transfer operators in reproducing kernel Hilbert spaces. Journal of Nonlinear Science, 30(1):283\u2013315.   \n[26] Kolluru, S. H. (2008). Preliminary Investigations of a Stochastic Method to Solve Electrostatic and Electrodynamic Problems. PhD thesis, PhD Thesis.   \n[27] Kostic, V., Novelli, P., Maurer, A., Ciliberto, C., Rosasco, L., and Pontil, M. (2022). Learning dynamical systems via Koopman operator regression in reproducing kernel hilbert spaces. In Advances in Neural Information Processing Systems.   \n[28] Kostic, V. R., Lounici, K., Novelli, P., and massimiliano pontil (2023a). Sharp spectral rates for koopman operator learning. In Thirty-seventh Conference on Neural Information Processing Systems.   \n[29] Kostic, V. R., Novelli, P., Grazzi, R., Lounici, K., et al. (2023b). Learning invariant representations of time-homogeneous stochastic dynamical systems. In The Twelfth International Conference on Learning Representations.   \n[30] Kutoyants, Y. A. (1984). Parameter estimation for stochastic processes. Berlin: Heldermann.   \n[31] Kutoyants, Y. A. (2013). Statistical inference for ergodic diffusion processes. Springer Science & Business Media.   \n[32] Kutz, J. N., Brunton, S. L., Brunton, B. W., and Proctor, J. L. (2016). Dynamic Mode Decomposition. Society for Industrial and Applied Mathematics.   \n[33] Lejay, A., Len\u00f4tre, L., and Pichot, G. (2019). Analytic expressions of the solutions of advectiondiffusion problems in one dimension with discontinuous coefficients. SIAM Journal on Applied Mathematics, 79(5):1712\u20131737.   \n[34] Li, Q., Dietrich, F., Bollt, E. M., and Kevrekidis, I. G. (2017). Extended dynamic mode decomposition with dictionary learning: A data-driven adaptive spectral decomposition of the koopman operator. Chaos: An Interdisciplinary Journal of Nonlinear Science, 27(10).   \n[35] Li, Z., Meunier, D., Mollenhauer, M., and Gretton, A. (2022). Optimal rates for regularized conditional mean embedding learning. In Advances in Neural Information Processing Systems.   \n[36] Lusch, B., Kutz, J. N., and Brunton, S. L. (2018). Deep learning for universal linear embeddings of nonlinear dynamics. Nature Communications, 9(1).   \n[37] Mackey, M. C. (2011). Time\u2019s Arrow: The origins of thermodynamic behavior. Courier Corporation.   \n[38] Mardt, A., Pasquali, L., Wu, H., and No\u00e9, F. (2018). VAMPnets for deep learning of molecular kinetics. Nature Communications, 9(1).   \n[39] Minsker, S. (2017). On some extensions of Bernstein\u2019s inequality for self-adjoint operators. Statistics & Probability Letters, 127:111\u2013119.   \n[40] Oksendal, B. (2013). Stochastic differential equations: an introduction with applications. Springer Science & Business Media.   \n[41] Pascucci, A. (2011). PDE and Martingale Methods in Option Pricing. Springer Milan.   \n[42] Pavliotis, G. A. (2014). Stochastic Processes and Applications. Springer New York.   \n[43] Pillaud-Vivien, L. and Bach, F. (2023). Kernelized diffusion maps. In The Thirty Sixth Annual Conference on Learning Theory, pages 5236\u20135259. PMLR.   \n[44] Reed, M. and Simon, B. (1980). I: Functional Analysis. Academic Press.   \n[45] Steinwart, I. and Christmann, A. (2008). Support Vector Machines. Springer New York.   \n[46] Tian, W. and Wu, H. (2021). Kernel embedding based variational approach for low-dimensional approximation of dynamical systems. Computational Methods in Applied Mathematics, 21(3):635\u2013 659.   \n[47] Trefethen, L. N. and Embree, M. (2020). Spectra and Pseudospectra: The Behavior of Nonnormal Matrices and Operators. Princeton University Press.   \n[48] Tropp, J. A. (2012). User-friendly tail bounds for sums of random matrices. Technical report.   \n[49] Tuckerman, M. E. (2023). Statistical mechanics: theory and molecular simulation. Oxford university press.   \n[50] Williams, M. O., , Rowley, C. W., and Kevrekidis, I. G. (2015). A kernel-based method for data-driven Koopman spectral analysis. Journal of Computational Dynamics, 2(2):247\u2013265.   \n[51] Wu, H. and No\u00e9, F. (2019). Variational Approach for Learning Markov Processes from Time Series Data. Journal of Nonlinear Science, 30(1):23\u201366.   \n[52] Yeung, E., Kundu, S., and Hodas, N. (2019). Learning deep neural network representations for koopman operators of nonlinear dynamical systems. In 2019 American Control Conference (ACC), pages 4832\u20134839. IEEE.   \n[53] Yosida, K. (2012). Functional analysis, volume 123. Springer Science & Business Media.   \n[54] Zabczyk, J. (2020). Mathematical Control Theory: An Introduction. Systems & Control: Foundations & Applications. Springer International Publishing.   \n[55] Zhang, W., Li, T., and Sch\u00fctte, C. (2022). Solving eigenvalue pdes of metastable diffusion processes using artificial neural networks. Journal of Computational Physics, 465:111377.   \n[56] Zhou, D.-X. (2008). Derivative reproducing properties for kernel methods in learning theory. Journal of computational and Applied Mathematics, 220(1-2):456\u2013463.   \n[57] Zwald, L. and Blanchard, G. (2005). On the Convergence of Eigenspaces in Kernel Principal Component Analysis. In Advances in Neural Information Processing Systems. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Supplementary Material ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "This appendix includes additional background on SDE and RKHS, proofs of the results omitted in the main body and information about the numerical experiments. ", "page_idx": 13}, {"type": "table", "img_path": "H7SaaqfCUi/tmp/988a5eb6288a1c947000eb2595388a6e556f86c82fd49e6482f52871cf27677c.jpg", "table_caption": ["Table 2: Summary of used notations. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "A Background ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Basics on operator theory for Markov processes. ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We provide here some basics on operator theory for Markov processes. Let $\\mathcal{X}\\subset\\mathbb{R}^{d}$ $(d\\in\\mathbb{N})$ and $(X_{t})_{t\\in\\mathbb{R}_{+}}$ be a $\\mathcal{X}$ -valued time-homogeneous Markov process defined on a filtered probability space $(\\Omega,\\mathcal{F},(\\mathcal{F}_{t})_{t\\in\\mathbb{R}_{+}},\\mathbb{P})$ where $\\mathcal{F}_{t}=\\sigma(\\bar{\\boldsymbol{X}}_{s},\\,s\\leq t)$ is the natural filtration of $(X_{t})_{t\\in\\mathbb{R}_{+}}$ . The dynamics of $X$ can be described through of a family of probability densities $(p_{t})_{t\\in\\mathbb{R}_{+}}$ such that for all $t\\in\\mathbb{R}_{+}$ , $E\\in B({\\mathcal{X}})$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{P}(X_{t}\\in E|X_{0}=x)=\\int_{E}p_{t}(x,y)d y.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Let $\\mathcal{G}$ be a set of real valued and measurable functions on $\\mathcal{X}$ . For any $t\\in\\mathbb{R}_{+}$ the transfer operator (TO) $A_{t}:\\mathcal{G}\\to\\mathcal{G}$ maps any measurable function $f\\in\\mathcal G$ to ", "page_idx": 14}, {"type": "equation", "text": "$$\n(A_{t}f)(x)=\\int_{X}f(y)p_{t}(x,y)d y.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "In theory of Markov processes, the family $(A_{t})_{t\\in\\mathbb{R}_{+}}$ is referred to as the Markov semigroup associated to the process $X$ . ", "page_idx": 14}, {"type": "text", "text": "Remark 1. A possible choice is $\\mathcal{G}=\\mathcal{L}^{\\infty}(\\mathcal{X})$ . Here, we are interested in another choice of $\\mathcal{G}$ related to the existence of an invariant measure for $A_{t}$ i.e., a $\\sigma$ -finite measure $\\pi$ on $(\\mathcal{X},\\mathcal{B}(\\mathcal{X}))$ such that $P_{t}^{*}\\pi\\,=\\,\\pi$ for any $t\\,\\in\\,\\mathbb{R}_{+}$ . In that case, we can choose $\\mathcal{G}=\\mathcal{L}_{\\pi}^{2}(\\mathcal{X})$ so that, for all $\\dot{f}\\,\\in\\,\\mathcal{L}_{\\pi}^{2}(\\mathcal{X})$ , $P_{t}f$ converges to $f$ in $\\mathcal{L}_{\\pi}^{2}(\\mathcal{X})$ as $t$ goes to 0. Note that $P_{0}f=f$ and $\\textstyle P_{\\infty}f={\\int f d\\mathcal{X}}$ for a suitable integrable function $f:\\mathcal{X}\\rightarrow\\mathbb{R}$ . ", "page_idx": 14}, {"type": "text", "text": "Within the existence of this invariant measure $\\pi$ , the process $X$ is then characterized by the infinitesimal generator (IG) $L:\\mathcal{L}_{\\pi}^{2}(\\mathcal{X})\\to\\mathcal{L}_{\\pi}^{2}(\\mathcal{X})$ of the family $(A_{t})_{t\\in\\mathbb{R}_{+}}$ defined by ", "page_idx": 14}, {"type": "equation", "text": "$$\nL=\\operatorname*{lim}_{t\\to0^{+}}\\frac{A_{t}-I}{t}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "In other words, $L$ characterizes the linear differential equation $\\partial_{t}A_{t}f\\,=\\,L A_{t}f$ satisfied by the transfer operator. The domain of $L$ denoted $\\mathrm{dom}(L)$ coincides with the the Sobolev space $\\mathcal{W}_{\\pi}^{1,\\bar{2}}(\\mathcal{X})$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{W}_{\\pi}^{1,2}(\\mathcal{X})=\\{f\\in\\mathcal{L}_{\\pi}^{2}(\\mathcal{X})\\ |\\ ||f||_{\\mathcal{W}}^{2}=||f||_{\\mathcal{L}_{\\pi}^{2}}+||\\nabla f||_{\\mathcal{L}_{\\pi}^{2}}<\\infty\\}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The spectrum of the IG can be difficult to capture due to the potential unboundedness of $L$ . To circumvent this problem, one can focus on an auxiliary operator, the resolvent, which, under certain conditions, shares the same eigenfunctions as $L$ and becomes compact. The following result can be found in Yosida\u2019s book ([53], Chap. IX) : For every $\\mu>0$ , the operator $(\\mu I-L)$ admits an inverse $L_{\\mu}=(\\mu I-L)^{-1}$ that is a continuous operator on $\\mathcal{X}$ and ", "page_idx": 14}, {"type": "equation", "text": "$$\n(\\mu I-L)^{-1}=\\int_{0}^{\\infty}e^{-\\mu t}A_{t}d t.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The operator $L_{\\mu}$ is the resolvent of $L$ and the corresponding resolvent set of $L$ is defined by ", "page_idx": 14}, {"type": "text", "text": "In fact, $\\rho(L)$ contains all real positive numbers and $(\\mu I-L)^{-1}$ is bounded. Here, we assume that $L$ has compact resolvent, i.e. $\\bar{\\rho}(L)\\neq\\emptyset$ and there exists $\\mu_{0}\\in\\rho(L)$ such that $L_{\\mu_{0}}$ is compact. Note that, through the resolvent identity, this implies the compactness of all resolvents $L_{\\mu}$ for $\\mu\\in\\rho(L)$ . Let then $\\mu\\in\\rho(L)$ . As $(L,\\mathrm{dom}(L))$ is assumed to be self-adjoint, so does $L_{\\mu}$ , so that $L_{\\mu}$ is both compact and self-adjoint. Then, its spectrum $\\operatorname{Sp}(L_{\\mu})=\\mathbb{C}\\setminus\\hat{\\rho(L_{\\mu})}$ is purely discrete and consists of isolated eigenvalues $(\\lambda_{i}^{\\mu})_{i\\in\\mathbb{N}}$ such that $|\\lambda_{i}^{\\mu}|\\to0$ associated with an orthonormal basis $(f_{i})_{i\\in\\mathbb{N}}$ (see [53], chapter XI). In other words, the spectral decomposition of the resolvent writes ", "page_idx": 14}, {"type": "equation", "text": "$$\nL_{\\mu}=\\sum_{i\\in\\mathbb{N}}\\lambda_{i}^{\\mu}\\,f_{i}\\otimes f_{i}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the functions $f_{i}\\in\\mathcal{L}_{\\pi}^{2}(\\mathcal{X})$ are also eigenfunctions of the operator $L$ we get ", "page_idx": 14}, {"type": "equation", "text": "$$\nL=\\sum_{i\\in\\mathbb{N}}\\lambda_{i}\\,f_{i}\\otimes f_{i}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Example 3 (Langevin). Let $(k_{b}T)\\in\\mathbb{R}.T h e$ overdamped Langevin equation driven by $a$ potential $V:\\ensuremath{\\mathbb{R}^{d}}\\to\\ensuremath{\\mathbb{R}}$ is given by $d X_{t}=-\\nabla V(X_{t})d t+\\sqrt{2(k_{b}T)}d W_{t}$ and $X_{0}=x$ . Its invariant measure of the solution process $X$ is the Boltzman distribution $\\pi(d x)=z^{-1}e^{-(k_{b}T)^{-1}V(x)}d x$ where $z$ denotes a normalizing constant. Its infinitesimal generator $L$ is defined by $\\boldsymbol{L}f=-\\boldsymbol{\\nabla}V^{\\top}\\boldsymbol{\\nabla}f+(k_{b}T)\\Delta f,$ , for $f\\in\\mathcal{W}_{\\pi}^{1,\\bar{2}}(\\mathcal{X})$ , and if $\\cdot\\nabla V\\in\\mathcal{L}_{\\pi}^{2}(\\mathcal{X})$ is positive and coercive, it has compact resolvent. Finally, $\\begin{array}{r}{\\int(-L f)g\\,d\\pi\\ =\\ -\\int\\Big[\\nabla\\Big((k_{b}T)\\nabla f(x)\\frac{e^{-(k_{b}T)^{-1}V(x)}}{Z}\\Big)\\Big]g(x)d x\\ =\\ (k_{b}T)\\int\\nabla f^{\\top}\\nabla g\\,d\\pi\\ =\\ }\\end{array}$ $\\textstyle\\int f(-L g)\\,d\\pi_{.}$ , generator $L$ is self-adjoint and associated to a gradient Dirichlet form with $\\bar{s}(x)=(k_{b}T)^{1/2}(\\delta_{i j})_{i\\in[d],j\\in[p]}$ . ", "page_idx": 15}, {"type": "text", "text": "Example 4 (Cox-Ingersoll-Ross process). Let $d=1$ \u221a, $a,b\\in\\mathbb{R},\\,\\sigma\\in\\mathbb{R}_{+}^{*}$ . The Cox-Ingersoll-Ross process is solution of the SDE $d X_{t}=(a+b X_{t})d t+\\sigma\\sqrt{X_{t}}d W_{t}$ and $X_{0}=x$ . Its invariant measure $\\pi$ is a Gamma distribution with shape parameter $a/\\sigma^{2}$ and rate parameter $b/\\sigma^{2}$ . Its infinitesimal generator $L$ is defined for $f\\in\\mathcal{L}_{\\pi}^{2}(\\mathcal{X})$ by $\\begin{array}{r}{L f=(a+b x)\\nabla f+\\frac{\\sigma^{2}x}{2}\\Delta f}\\end{array}$ . Note that by integration by parts, we can check that the generator $L$ satisfies $\\begin{array}{r}{\\int(-L f)g\\,d\\pi=\\int\\frac{\\sigma^{2}x}{2}f^{\\prime}(x)g^{\\prime}(x)\\,\\pi(d x)=\\int f(-L g)\\,d\\pi,}\\end{array}$ , and it is associated to a gradient Dirichlet form with $s(x)=\\sigma\\sqrt{x}/\\sqrt{2}$ . ", "page_idx": 15}, {"type": "text", "text": "A.2 Infinitesimal generator for diffusion processes ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "After defining the infinitesimal generator for Markov processes (see A.1), we provide its explicit form for solution processes of equations like(1). Given a smooth function $f\\in\\bar{\\mathcal{C}}^{2}(\\mathcal{X},\\mathbb{R})$ , It\u00f4\u2019s formula (see for instance [4], B, p.495) provides for $t\\in\\mathbb{R}_{+}$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(X_{t})-f(X_{0})=\\displaystyle\\int_{0}^{t}\\sum_{i=1}^{d}\\partial_{i}f(X_{s})d X_{s}^{i}+\\frac{1}{2}\\displaystyle\\int_{0}^{t}\\sum_{i,j=1}^{d}\\partial_{i j}^{2}f(X_{s})d\\langle X^{i},X^{j}\\rangle_{s}}\\\\ &{\\displaystyle\\qquad\\qquad=\\displaystyle\\int_{0}^{t}\\nabla f(X_{s})^{\\top}d X_{t}+\\frac{1}{2}\\displaystyle\\int_{0}^{t}\\mathrm{Tr}\\big[X_{s}^{\\top}(\\nabla^{2}f)(X_{s})X_{s}\\big]d s.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Recalling (1), we get ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{f(X_{t})=f(X_{0})+\\displaystyle\\int_{0}^{t}\\Big[\\nabla f(X_{s})^{\\top}a(X_{s})+\\frac{1}{2}\\mathrm{Tr}\\big[b(X_{s})^{\\top}(\\nabla^{2}f(X_{s}))b(X_{s})\\big]\\Big]d s}}\\\\ {{\\mathrm{}}}\\\\ {{\\mathrm{}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Provided $f$ and $b$ are smooth enough, the expectation of the last stochastic integral vanishes so that we get ", "page_idx": 15}, {"type": "text", "text": "$-[f(X_{t})|X_{0}=x]=f(x)+\\int_{0}^{t}\\mathbb{E}\\Big[\\nabla f(X_{s})^{\\top}a(X_{s})+\\frac{1}{2}\\mathrm{Tr}\\big[b(X_{s})^{\\top}(\\nabla^{2}f(X_{s}))b(X_{s})\\big]\\Big|X_{0}=x\\Big]d s$ Recalling that $L=\\operatorname*{lim}_{t\\to0^{+}}(A_{t}f-f)/t$ , we get for every $x\\in\\mathscr{X}$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\cal L}f(x)=\\operatorname*{lim}_{t\\rightarrow0}\\frac{\\mathbb{E}[f(X_{t})|X_{0}=x]-f(x)}{t}}\\ ~}\\\\ {{\\displaystyle~~~~~=\\operatorname*{lim}_{t\\rightarrow0}\\frac{1}{t}\\bigg[\\int_{0}^{t}\\mathbb{E}\\Big[\\nabla f(X_{s})^{\\top}a(X_{s})+\\frac{1}{2}\\mathrm{Tr}\\big[(X_{s})^{\\top}(\\nabla^{2}f(X_{s}))b(X_{s})\\big]\\Big]d s\\bigg|X_{0}=x\\bigg]}}\\\\ {{\\displaystyle~~~~~=\\nabla f(x)^{\\top}a(x)+\\frac{1}{2}\\mathrm{Tr}\\big[b(x)^{\\top}(\\nabla^{2}f(x))b(x)\\big],}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which provides the closed formula for the IG associated with the solution process of (1). ", "page_idx": 15}, {"type": "text", "text": "A.3 Spectral perturbation theory ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Recalling that for a bounded linear operator $A$ on some Hilbert space $\\mathcal{H}$ the resolvent set of the operator $A$ is defined as $\\rho(A)=\\left\\{\\lambda\\in\\mathbb{C}\\,|\\,A-\\lambda I\\right.$ is bijective and $(\\dot{A}-\\lambda I)^{-1}$ is continuous $\\}$ , and its spectrum $\\operatorname{Sp}(A)\\,=\\,\\mathbb{C}\\setminus\\{\\rho(A)\\}$ , let $\\lambda\\subseteq\\,{\\mathsf{S p}}(A)$ be isolated part of spectra, i.e. both $\\lambda$ and $\\mu=\\operatorname{Sp}(A)\\setminus\\lambda$ are closed in $\\operatorname{Sp}(A)$ . Than, the Riesz spectral projector $P_{\\lambda}\\colon\\mathcal{H}\\rightarrow\\mathcal{H}$ is defined by ", "page_idx": 15}, {"type": "equation", "text": "$$\nP_{\\lambda}=\\frac{1}{2\\pi}\\int_{\\Gamma}(z I-A)^{-1}d z,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\Gamma$ is any contour in the resolvent set ${\\mathrm{Res}}(A)$ with $\\lambda$ in its interior and separating $\\lambda$ from $\\mu$ . Indeed, we have that $P_{\\lambda}^{2}\\,=\\,P_{\\lambda}$ and $\\mathcal{H}=\\mathrm{Im}(P_{\\lambda})\\oplus\\mathrm{Ker}(P_{\\lambda})$ where $\\mathrm{Im}(P_{\\lambda})$ and $\\mathrm{Ker}(P_{\\lambda})$ are both invariant under $A$ and $\\operatorname{Sp}(A_{|_{\\mathrm{Im}(P_{\\lambda})}})\\,=\\,\\lambda$ , $\\operatorname{Sp}(A_{|_{\\operatorname{Ker}(P_{\\lambda})}})\\,=\\,\\mu$ . Moreover, $P_{\\lambda}+P_{\\mu}\\,=\\,I$ and $P_{\\lambda}P_{\\mu}=P_{\\mu}P_{\\lambda}=0.$ . ", "page_idx": 16}, {"type": "text", "text": "Finally if $A$ is compact operator, then the Riesz-Schauder theorem, see e.g. [44], assures that $\\mathrm{Sp}(T)$ is a discrete set having no limit points except possibly $\\lambda=0$ . Moreover, for any nonzero $\\lambda\\in\\operatorname{Sp}(T)$ , then $\\lambda$ is an eigenvalue (i.e. it belongs to the point spectrum) of finite multiplicity, and, hence, we can deduce the spectral decomposition in the form ", "page_idx": 16}, {"type": "equation", "text": "$$\nA=\\sum_{\\lambda\\in\\operatorname{Sp}(A)}\\lambda\\,P_{\\lambda},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where geometric multiplicity of $\\lambda$ , $r_{\\lambda}=\\mathrm{rank}(P_{\\lambda})$ , is bounded by the algebraic multiplicity of $\\lambda$ . If additionally $A$ is normal operator, i.e. $A A^{*}=A^{*}A$ , then $P_{\\lambda}=P_{\\lambda}^{*}$ is orthogonal projector for each $\\lambda\\in\\operatorname{Sp}(A)$ and $\\begin{array}{r}{P_{\\lambda}=\\sum_{i=1}^{r_{\\lambda}}\\psi_{i}\\otimes\\psi_{i}}\\end{array}$ , where $\\psi_{i}$ are normalized eigenfunctions of $A$ corresponding to $\\lambda$ and $r_{\\lambda}$ is both algebraic and geometric multiplicity of . ", "page_idx": 16}, {"type": "text", "text": "We conclude this section with well-known perturbation bounds for eigenfunctions and spectral projectors of self-adjoint compact operators. ", "page_idx": 16}, {"type": "text", "text": "Proposition 3 ([11]). Let $A$ be compact self-adjoint operator on a separable Hilbert space H.Given a pair $(\\widehat{\\lambda},\\widehat{f})\\;\\in\\;\\mathbb{C}\\,\\times\\,\\mathcal{H}$ such that $\\|{\\widehat{f}}\\|\\;=\\;1\\,$ , let $\\lambda$ be the eigenvalue of $A$ that is closest to $\\widehat{\\lambda}$ and let $f$ be its normalized eigenfunction. If $\\widehat{g}\\,=\\,\\operatorname*{min}\\{|\\widehat{\\lambda}-\\lambda|\\,|\\,\\lambda\\,\\in\\,\\mathsf{S p}(A)\\setminus\\{\\lambda\\}\\}\\,>\\,0$ , then $\\sin(\\sphericalangle(\\widehat{f},f))\\leq\\|A\\widehat{f}-\\widehat{\\lambda}\\widehat{f}\\|/\\widehat{g}.$ . ", "page_idx": 16}, {"type": "text", "text": "Proposition 4 ([57]). Let $A$ and $\\widehat{A}$ be two compact operators on a separable Hilbert space. For nonempty index set $J\\subset\\mathbb{N}$ let ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{gap}_{J}(A)=\\operatorname*{min}\\left\\{|\\lambda_{i}(A)-\\lambda_{j}(A)|\\,|\\,i\\in\\mathbb{N}\\setminus J,\\,j\\in J\\right\\}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "denote the spectral gap w.r.t $J$ and let $P_{J}$ and ${\\widehat{P}}_{J}$ be the corresponding spectral projectors of $A$ and $\\widehat{A}_{i}$ , respectively. If $A$ is self-adjoint and for some $\\|A-\\widehat{A}\\|<\\operatorname{gap}_{J}(A),$ , then ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\|P_{J}-\\widehat{P}_{J}\\|\\leq\\frac{\\|A-\\widehat{A}\\|}{\\mathrm{gap}_{J}(A)}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "B Representations in the RKHS ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In section 2, we have defined the infinitesimal generator of a diffusion process and specified its form when associated with Dirichlet forms. These operators act on $\\mathcal{L}_{\\pi}^{2}(\\mathcal{X})$ or specific subsets of it. To develop our learning procedure, we need to understand these operators\u2019 actions when embedding into the RKHS, and define their versions for feature maps. ", "page_idx": 16}, {"type": "text", "text": "IG and Dirichlet operator in RKHS. As a reminder, we consider $\\mathcal{H}$ be an RKHS and let $k:$ $\\mathcal{X}\\!\\times\\!\\mathcal{X}\\rightarrow\\mathbb{R}$ be the associated kernel function. The canonical feature map is denoted by $\\phi(x)=k(x,\\cdot)$ for $x\\in\\mathcal{X}\\;k(x,x^{\\prime})=\\langle\\phi(x),\\phi(x^{\\prime})\\rangle$ for all $x,x^{\\prime}\\in\\mathcal{X}$ . Assuming that $k$ is square-integrable with respect to the measure $\\pi$ , we define the injection operator $S_{\\pi}\\ :\\,\\mathcal{H}\\,\\hookrightarrow\\,\\mathcal{L}_{\\pi}^{\\bar{2}}(\\mathcal{X})$ and its adjoint $S_{\\pi}^{*}\\ ^{^{\\bullet}}\\colon{\\mathcal{L}}_{\\pi}^{2}(\\chi)\\to\\mathcal{H}$ by $\\begin{array}{r}{S_{\\pi}^{*}f\\,=\\,\\int_{\\mathcal{X}}f(x)\\phi(x)\\bar{\\pi}(d x)}\\end{array}$ . As a preliminary step, we need to define the reproducing partial derivatives in RKHS, which we introduce via Mercer kernels. ", "page_idx": 16}, {"type": "text", "text": "Definition 1 (Mercer kernel). A kernel function $k\\,:\\,\\mathcal{X}\\times\\mathcal{X}\\to\\mathbb{R}$ is called a Mercer kernel if it is a continuous and symmetric function such that for any finite set of points $\\{x_{1},\\ldots,x_{n}\\}\\subset{\\dot{x}}$ , the matrix $(k(x_{i},x_{j}))_{i,j=1}^{n}$ is positive semi-definite. ", "page_idx": 16}, {"type": "text", "text": "Several standard kernels satisfy the Mercer property with $s\\geq1$ , including the Gaussian kernel which we will consider subsequently. ", "page_idx": 16}, {"type": "text", "text": "For $s\\in{\\mathbb N}$ and $m\\in\\mathbb{N}$ , we define the index set $I_{s}^{m}=\\{\\alpha\\in\\mathbb{N}^{m}\\,:\\,|\\alpha|\\leq s\\}$ where $\\textstyle|\\alpha|=\\sum_{j=1}^{s}\\alpha_{j}$ , for $\\alpha=(\\alpha_{1},\\ldots,\\alpha_{m})\\in\\ensuremath{\\mathbb{N}}^{m}$ . For a function $f:\\mathbb{R}^{m}\\to\\mathbb{R}$ and $x=(x_{1},\\ldots,x_{m})\\in\\mathbb{R}^{m}$ , we denote its partial derivative $d^{\\alpha}f$ at point $x$ (if it exists) as ", "page_idx": 16}, {"type": "equation", "text": "$$\nD^{\\alpha}f(x)=\\prod_{j=1}^{m}D_{j}^{\\alpha_{j}}f(x)={\\frac{\\partial^{|\\alpha|}}{\\partial^{\\alpha_{1}}x_{1}\\cdot\\cdot\\cdot\\partial^{\\alpha_{m}}x_{m}}}f(x).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For a function $k\\in\\mathcal{C}^{2s}(\\mathcal{X}\\times\\mathcal{X})$ with $\\mathcal{X}\\subset\\mathbb{R}^{d}$ and $\\alpha\\in I_{s}^{d}$ , we define ", "page_idx": 17}, {"type": "equation", "text": "$$\nD^{\\alpha}k(x,y)=D^{(\\alpha,0)}k(x,y)={\\frac{\\partial^{|\\alpha|}}{\\partial^{\\alpha_{1}}x_{1}\\cdot\\cdot\\cdot\\partial^{\\alpha_{m}}x_{m}}}k(x,y).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and ", "page_idx": 17}, {"type": "equation", "text": "$$\nD^{(0,\\alpha)}k(x,y)=\\frac{\\partial^{|\\alpha|}}{\\partial^{\\alpha_{1}}y_{1}\\cdot\\cdot\\cdot\\partial^{\\alpha_{m}}y_{m}}k(x,y).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Theorem 3 (Theorem 1 in [56]). Let $s\\ \\in\\ \\mathbb{N},$ , ${\\mathcal{X}}\\ \\subseteq\\ \\mathbb{R}^{m}$ and $k$ be a Mercer kernel such that $k\\in\\mathcal{C}^{2s}(\\mathcal{X}\\times\\mathcal{X})$ with corresponding RKHS $\\mathcal{H}$ . Then the following hold: ", "page_idx": 17}, {"type": "text", "text": "i. For any $x\\in\\mathscr{X}$ , $\\alpha\\in I_{s}^{m}$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n(D^{\\alpha}k)_{x}(y)=D^{\\alpha}k(x,y)\\in\\mathcal{H}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "ii. A partial derivative reproducing property holds for $\\alpha\\in I_{s}^{m}$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n(D^{\\alpha}h)(x)=\\langle(D^{\\alpha}k)_{x},h\\rangle_{\\mathcal{H}},\\quad\\forall h\\in\\mathcal{H}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Theorem 3 allows us to introduce the first and second order operators $D$ and $D^{2}$ that act on any feature map $\\phi(x)$ as ", "page_idx": 17}, {"type": "equation", "text": "$$\nD\\phi(x)=((D^{e_{i}}k)_{x})_{i\\in[d]}\\quad{\\mathrm{and}}\\quad D^{2}\\phi(x)=((D^{e_{i}+e_{j}}k)_{x})_{i,j\\in[d]}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the $(D^{e i}k)_{x}$ and $(D^{e_{i}+e_{j}}k)_{x}$ can be defined via (32). Then, we define the operator $d$ that maps any feature map $\\phi(x)$ to $d\\phi(x)\\,=\\,s(x)^{\\top}D\\phi(x)$ . Denote $s^{\\top}=[\\bar{s}_{1}|\\,.\\,.\\,.\\,|\\bar{s}_{d}]:x\\in\\mathcal{X}\\mapsto s^{\\top}(x)=$ $[\\bar{s}_{1}(x)|\\dots|\\bar{s}_{d}(x)]\\in\\mathbb{R}^{p\\times d}$ . We have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{s(x)^{\\top}D h(x)=\\displaystyle\\sum_{i=1}^{d}\\bar{s}_{i}(x)\\partial_{i}h(x)=\\displaystyle\\sum_{i=1}^{d}\\bar{s}_{i}(x)\\langle D^{e_{i}}\\phi(x),h\\rangle_{\\mathcal{H}}=\\langle d\\phi(x),h\\rangle_{\\mathcal{H}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "so that we can define the embedding of the Dirichlet operator $B=s^{\\top}\\nabla\\colon\\mathcal{L}_{\\pi}^{2}(\\mathcal{X})\\rightarrow[\\mathcal{L}_{\\pi}^{2}(\\mathcal{X})]^{p}$ into RKHS $d\\phi\\colon\\mathcal{X}\\to\\mathcal{H}^{p}$ via the reproducing property as $\\langle d\\phi(x),h\\rangle_{\\mathcal{H}}=[B S_{\\pi}h](x)=s(x)^{\\top}D h(x)\\in$ $\\mathbb{R}^{p},\\;h\\;\\in\\;\\mathcal{H}$ . In fact, $d\\phi$ is a $p$ -dimensional vector with components $d_{k}\\phi\\colon\\mathcal{X}\\ \\rightarrow\\ \\mathcal{H}$ given via $\\langle d_{k}\\phi(x),h\\rangle_{\\mathcal{H}}=s_{k}(x)^{\\top}D h(x),k\\in[p].$ . Then, we can define ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{T=-\\mathbb{E}_{x\\sim\\pi}[d\\phi(x)\\otimes d\\phi(x)]=-\\sum_{k\\in[p]}\\mathbb{E}_{x\\sim\\pi}[d_{k}\\phi(x)\\otimes d_{k}\\phi(x)].}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which captures correlations betw\u221aeen input and the outputs of the generator in the RKHS. Defining $w\\phi\\colon\\mathcal{X}\\bar{\\to}\\;\\mathcal{H}^{p+1}$ by $w\\phi(x)=[\\sqrt{\\mu}\\phi(x),d_{1}\\phi(x),\\ldots\\hat{d_{p}}\\phi(x)]^{\\top}\\in\\bar{\\mathbb{R}}^{d+1}$ , we can consider ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{W_{\\mu}=Z_{\\mu}^{*}Z_{\\mu}=S_{\\pi}^{*}(\\mu I-L)S_{\\pi}}\\\\ &{\\qquad=\\mathbb{E}_{x\\sim\\pi}[\\mu\\phi(x)\\otimes\\phi(x)+d\\phi(x)\\otimes d\\phi(x)]=\\mathbb{E}_{x\\sim\\pi}[w\\phi(x)\\otimes w\\phi(x)]}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which corresponds to the RKHS covariance operator w.r.t. energy space $\\mathcal{W}$ . ", "page_idx": 17}, {"type": "text", "text": "Examples. One way to ensure that the essential assumption $((\\bf{K E}))$ ) holds is to show that $w\\phi$ fulfils the boundedness condition ((BK)), i.e. $w\\phi\\!\\in\\!\\mathcal{L}_{\\pi}^{\\infty}(\\mathcal{X},\\mathcal{H}^{\\mathrm{i}+p})$ . Let\u2019s show that the property holds true for the Damped Langevin (see Example 1) and the CIR process (see Example 2) if we consider the Radial Basis Function (RBF) kernel $k(x,y)=k_{x}(y)=\\exp(-\\kappa\\|x-y\\|^{2})$ where $\\kappa>0$ is a free parameter that sets the \u201cspread\u201d of the kernel. As a reminder, for every $x\\in\\mathscr{X}$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|w\\phi(x)\\|^{2}=\\mu\\|\\phi(x)\\|^{2}+\\sum_{k=1}^{p}\\|d_{k}\\phi(x)\\|^{2}\\quad\\mathrm{with}\\quad d_{k}=s_{k}(x)^{\\top}D\\phi(x).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Recalling that for the overdamped Langevin process $s(x)=(k_{b}T)^{1/2}(\\delta_{i j})_{i\\in[d],j\\in[p]},\\,s.$ $s s^{\\top}$ is diagonal so that $\\langle s(x)^{\\top}D k_{x}(\\cdot),s(x)^{\\top}D k_{x}(\\cdot)\\rangle=0$ for every $x\\in\\mathscr{X}$ . As $\\|\\phi(x)\\|^{2}=1$ , we get that for every $x\\in\\mathscr{X}$ , $\\|w\\phi(x)\\|^{2}\\leq\\mu=:c_{\\mathcal{H}}$ . ", "page_idx": 17}, {"type": "text", "text": "Consider now the CIR process. We have $d=p=1$ and $s(x)=\\sigma\\sqrt{x}/\\sqrt{2}$ for any $x\\in\\mathscr{X}$ . For the very same reasons, $\\langle s(x)^{\\top}D k_{x}(\\cdot),s(x)^{\\top}D k_{x}(\\cdot)\\rangle=0$ for every $x\\in\\mathscr{X}$ , so that $\\|w\\phi(x)\\|^{2}\\leq\\mu=:c_{\\mathcal{H}}$ . In both Langevin and CIR cases, we have $\\dot{w}\\phi\\!\\in\\!\\mathcal{L}_{\\pi}^{\\infty}(\\mathcal{X},\\mathcal{H}^{1\\!+\\!p})$ when considering an RBF kernel. ", "page_idx": 17}, {"type": "text", "text": "C Statistical learning framework ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "C.1 Spectral perturbation bounds ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section, we prove key perturbation result and discuss the properties of the metric distortion. We conclude this section with the approximation bound for arbitrary estimator $G\\in\\mathrm{B}_{r}(\\mathcal{H})$ that is the basis of the statistical bounds that follow. This result is a direct consequence of [27] and Davis-Khan spectral perturbation result for compact self-adjoint operators, [11]. ", "page_idx": 18}, {"type": "text", "text": "In the framework of Koopman operator learning [28], spectral bounds are expressed in terms of a distortion metric between the RKHS $\\mathcal{H}$ and $\\bar{\\mathcal{L}}_{\\pi}^{2}(\\mathcal{X})$ , corresponding to the cost incurred from observing the operator\u2019s action on the $\\mathcal{H}$ rather than on its domain $\\bar{\\mathcal{L}}_{\\pi}^{\\bar{2}}(\\mathcal{X})$ . Aligned with the risk definition (9), here we measure in a certain way the distortion between the $\\mathcal{H}$ and $\\mathcal{W}_{\\pi}^{\\mu}(\\mathcal{X})$ as given in definitions in (10). ", "page_idx": 18}, {"type": "text", "text": "Proposition 2. Let $\\begin{array}{r}{\\widehat{G}=\\sum_{i\\in[r]}(\\mu-\\widehat{\\lambda}_{i})^{-1}\\,\\widehat{h}_{i}\\otimes\\widehat{g}_{i}}\\end{array}$ be the spectral decomposition of $\\widehat{G}\\colon\\mathcal{H}\\rightarrow\\mathcal{H}$ , where $\\widehat{\\lambda}_{i}\\geq\\widehat{\\lambda}_{i+1}$ and let $\\widehat{f_{i}}={S_{\\pi}}\\widehat{h}_{i}\\,/\\,\\lVert S_{\\pi}\\widehat{h}_{i}\\rVert_{\\mathcal{L}_{\\pi}^{2}}$ , for $i\\in[r]$ . Then for every $\\mu>0$ and $i\\in[r]$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{|\\lambda_{i}-\\widehat\\lambda_{i}|}{|\\mu-\\lambda_{i}||\\mu-\\widehat\\lambda_{i}|}\\leq\\mathcal{E}(\\widehat{G})\\eta(\\widehat{h}_{i})\\quad\\mathrm{~}a n d\\quad\\|\\widehat{f}_{i}-f_{i}\\|_{\\mathcal{L}_{\\pi}^{2}}^{2}\\leq\\frac{2\\,\\mathcal{E}(\\widehat{G})\\eta(\\widehat{h}_{i})}{\\mu\\,[\\mathrm{gap}_{i}-\\mathcal{E}(\\widehat{G})\\eta(\\widehat{h}_{i})]_{+}},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where ${\\mathrm{gap}}_{i}$ is the difference between $i$ -th and $(i+1)$ - $^{t h}$ eigenvalue of $(\\mu I-L)^{-1}$ . ", "page_idx": 18}, {"type": "text", "text": "Proof. We first remark that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{((\\mu I-L)^{-1}-(\\mu-\\widehat\\lambda_{i})^{-1}I_{\\mathcal{L}_{\\pi}^{2}(\\mathcal{X})})^{-1}\\|_{\\mathcal{H}\\to\\mathcal{W}}^{-1}\\le\\|((\\mu I-L)^{-1}Z_{\\mu}-Z_{\\mu}\\widehat G)\\widehat h_{i}\\|_{\\mathcal{H}\\to\\mathcal{W}}/\\|Z_{\\mu}\\widehat h_{i}\\|\\le\\mathcal{E}(\\widehat G)\\eta}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then, from the first inequality, using that $(\\mu I-L)^{-1}$ is self-adjoint as operator $\\mathcal{W}_{\\pi}^{\\mu}(\\mathcal{X})\\rightarrow\\mathcal{W}_{\\pi}^{\\mu}(\\mathcal{X})$ , we obtain the first bound in (11). ", "page_idx": 18}, {"type": "text", "text": "So, observing that for every $(\\mu-\\lambda)^{-1}{\\in}\\,\\mathrm{Sp}((\\mu I-L)^{-1})\\backslash\\{(\\mu{-}\\lambda_{i})^{-1}\\},$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n|(\\mu-\\widehat\\lambda_{i})^{-1}-(\\mu-\\lambda)^{-1}|\\geq|(\\mu-\\lambda_{i})^{-1}-(\\mu-\\lambda)^{-1}|-|(\\mu-\\widehat\\lambda_{i})^{-1}-(\\mu-\\lambda_{i})^{-1}|\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "we conclude that $|(\\mu-\\widehat{\\lambda}_{i})^{-1}-(\\mu-\\lambda)^{-1}|\\geq|(\\mu-\\lambda_{i})^{-1}-(\\mu-\\lambda)^{-1}|-\\mathcal{E}(\\widehat{G})\\,\\eta(\\widehat{h}_{i}),\\mathscr{E}(\\mu-\\lambda)|>0.$ and ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sin\\{|(\\mu-\\widehat{\\lambda}_{i})^{-1}-(\\mu-\\lambda)^{-1}|\\,|\\,(\\mu-\\lambda)^{-1}\\in\\mathsf{S p}((\\mu I-L)^{-1})\\setminus\\{(\\mu-\\lambda_{i})^{-1}\\}\\}\\geq\\mathrm{gap}_{i}-\\mathcal{E}(\\widehat{G})\\,\\eta(\\widehat{h}_{i}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "So, applying Proposition 3, we obtain ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sin(\\triangle(\\widehat{f}_{i},f_{i}))\\leq\\frac{\\|(\\mu I-L)^{-1}\\widehat{f}_{i}-(\\mu-\\widehat{\\lambda}_{i})^{-1}\\widehat{f}_{i}\\|}{[\\mathrm{gap}_{i}-\\mathcal{E}(\\widehat{G})\\,\\eta(\\widehat{h}_{i})]_{+}}\\leq\\frac{\\|((\\mu I-L)^{-1}Z_{\\mu}-Z_{\\mu}\\widehat{G})\\widehat{h}_{i}\\|/\\|Z_{\\mu}\\widehat{h}_{i}\\|}{[\\mathrm{gap}_{i}-\\mathcal{E}(\\widehat{G})\\,\\eta(\\widehat{h}_{i})]_{+}}}\\\\ &{\\qquad\\qquad\\leq\\frac{\\mathcal{E}(\\widehat{G})\\,\\eta(\\widehat{h}_{i})}{[\\mathrm{gap}_{i}-\\mathcal{E}(\\widehat{G})\\,\\eta(\\widehat{h}_{i})]_{+}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Since, clearly $\\|\\widehat{f}_{i}-f_{i}\\|^{2}\\leq2(1-\\cos(\\sphericalangle(\\widehat{f}_{i},f_{i}))\\leq2\\sin(\\sphericalangle(\\widehat{f}_{i},f_{i}))$ , the proof of the second bound is completed. ", "page_idx": 18}, {"type": "text", "text": "Next, we adapt the [28, Proposition 1] to our setting as follows. ", "page_idx": 18}, {"type": "text", "text": "Proposition 5. Let $\\widehat{G}\\!\\in\\!\\mathrm{B}_{r}(\\mathcal{H})$ . For all $i\\in[r]$ the metric distortion of $\\widehat{h}_{i}$ w.r.t. energy space $\\mathcal{W}_{\\pi}^{\\mu}(\\mathcal{X})$ can be tightly boun ded as ", "page_idx": 18}, {"type": "equation", "text": "$$\n1\\,/\\sqrt{\\|W_{\\mu}\\|}\\,\\leq\\,\\eta(\\widehat{h}_{i})\\,\\leq\\,\\|\\widehat{G}\\|\\,/\\,\\sigma_{\\operatorname*{min}}^{+}(Z_{\\mu}\\widehat{G}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "D Empirical estimation ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The Reduced Rank Regression (RRR) estimator is the exact minimizer of (17) under fixed rank constraint. Specifically, RRR is the minimizer $\\widehat{G}_{\\mu,\\gamma}^{r}$ of $\\widehat{\\mathcal{R}}_{\\gamma}(G)$ within the set of bounded operators ", "page_idx": 18}, {"type": "text", "text": "$\\mathrm{HS}_{r}(\\mathcal{H})$ on $\\mathcal{H}$ that have rank at most $r$ . The regularization term $\\gamma\\|G\\|_{\\mathrm{HS}}^{2}$ is added to ensure stability. The closed form solution of the empirical RRR estimator is ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\widehat{G}_{\\mu,\\gamma}^{r}=\\widehat{H}_{\\gamma}^{-1/2}\\mathbb{[}\\widehat{H}_{\\gamma}^{1/2}C\\mathbb{]}_{r},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "while its population counterpart is given by $G_{\\mu,\\gamma}^{r}=W_{\\mu,\\gamma}^{-1/2}\\mathbb{I}W_{\\mu,\\gamma}^{1/2}C]\\mathbb{I}_{r}$ . ", "page_idx": 19}, {"type": "text", "text": "In order to prove Theorem 1, recall kernel matrices in (19) and define ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{F}_{\\mu}=\\left[\\!\\!\\begin{array}{c c}{\\mu\\mathrm{K}}&{\\sqrt{\\mu}\\mathrm{N}}\\\\ {\\sqrt{\\mu}\\mathrm{N}^{\\top}}&{\\mathrm{M}}\\end{array}\\!\\!\\right]\\quad\\mathrm{~and~}\\mathrm{F}_{\\mu,\\gamma}=\\left[\\!\\!\\begin{array}{c c}{\\mu\\mathrm{K}_{\\gamma}}&{\\sqrt{\\mu}\\mathrm{N}}\\\\ {\\sqrt{\\mu}\\mathrm{N}^{\\top}}&{\\mathrm{M}+\\gamma\\mu I}\\end{array}\\!\\!\\right].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Now we provide the explicit forms of the matrices $_\\mathrm{N}$ and $\\mathbf{M}$ in the case of Langevin (see Example 1) and CIR (see Example 2) processes, considering an RBF kernel $k(x,y)=k_{x}(y)\\bar{=}\\exp(-\\kappa\\|x\\bar{-}y\\|^{2})$ . As a reminder (see (36)), $\\mathbf{N}\\in\\mathbb{R}^{n\\times p n}$ and $\\mathbf{M}\\in\\mathbb{R}^{p n\\times p n}$ are Gram matrices whose elements, for $k\\in[1+p],i,j\\in[n]$ are given by ", "page_idx": 19}, {"type": "text", "text": "where $d_{k}\\phi(x_{j})=s_{k}(x_{j})^{\\top}D\\phi(x_{j})$ and $D\\phi(x_{j})=D k(x_{j},\\cdot)$ is defined by (31). For $k\\in[p],i,j\\in$ $[n]$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\langle\\phi(x_{i}),d_{k}\\phi(x_{j})\\rangle_{\\mathcal{H}}=\\langle k_{x_{i}},s_{k}(x_{j})^{\\top}D k_{x_{j}}\\rangle_{\\mathcal{H}}}&{}\\\\ {=\\Big\\langle k_{x_{i}},s_{k}(x_{j})^{\\top}\\Big(\\underbrace{\\operatorname*{lim}\\frac{k(\\cdot,x_{j}+h e t)-k(\\cdot,x_{j})}{h}}_{h}\\Big)_{l\\in[d]}\\Big\\rangle}&{}\\\\ {=\\displaystyle\\operatorname*{lim}_{h\\to0}\\Big\\langle k_{x_{i}},s_{k}(x_{j})^{\\top}\\Big(\\frac{k(\\cdot,x_{j}+h e t)-k(\\cdot,x_{j})}{h}\\Big)_{l\\in[d]}\\Big\\rangle}\\\\ {=s_{k}(x_{j})^{\\top}\\Big(\\underbrace{\\operatorname*{lim}\\frac{k(x_{i},x_{j}+h e t)-k(x_{i},x_{j})}{h}}_{h}\\Big)_{l\\in[d]}}&{}\\\\ {=s_{k}(x_{j})^{\\top}\\big(D^{(0,t)}k(x_{i},x_{j})\\big)_{l\\in[d]}}&{}\\\\ {=2\\gamma s_{k}(x_{j})^{\\top}\\big((x_{i}^{(l)}-x_{j}^{(l)})k(x_{i},x_{j})\\big)_{l\\in[d]}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where we have used the continuity of the inner product to get the third line and the reproducing property to obtain the following one. Similarly, for $k,\\ell\\in[p],j\\in[n]$ , we get ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{d_{k}\\phi(x_{i}),d_{\\ell}\\phi(x_{j})\\rangle_{\\mathcal{H}}\\!=\\!\\left\\langle s_{k}(x_{i})^{\\top}\\left(D^{(e_{\\ell},0)}k(x_{i},\\cdot)\\right)_{l^{\\prime}\\in[d]},s_{\\ell}(x_{j})^{\\top}\\!\\left(\\operatorname*{lim}\\frac{k(\\cdot,x_{j}+h e_{l})-k(\\cdot,x_{j})}{h}\\right)_{l\\in[d]}\\right.}}\\\\ &{\\left.=\\!\\operatorname*{lim}_{h\\to0}\\!\\left\\langle s_{k}(x_{i})^{\\top}\\left(D^{(e_{\\ell},0)}k(x_{i},\\cdot)\\right)_{l^{\\prime}\\in[d]},s_{\\ell}(x_{j})^{\\top}\\!\\left(\\frac{k(\\cdot,x_{j}+h e_{l})-k(\\cdot,x_{j})}{h}\\right)_{l\\in[d]}\\right.}\\\\ &{\\left.=\\!\\operatorname*{lim}_{h\\to0}\\!\\left\\langle\\left(D^{(e_{\\ell},0)}k(x_{i},\\cdot)\\right)_{l^{\\prime}\\in[d]},s_{k}(x_{j})s_{\\ell}(x_{j})^{\\top}\\!\\left(\\frac{k(\\cdot,x_{j}+h e_{l})-k(\\cdot,x_{j})}{h}\\right)_{l\\in[d]}\\right\\rangle}\\\\ &{=\\!s_{k}(x_{i})s_{\\ell}(x_{j})^{\\top}\\!\\left(\\operatorname*{lim}\\frac{D^{(e_{\\ell},\\cdot,0)}k(x_{i},x_{j}+h e_{l})-D^{(e_{\\ell},0)}k(x_{i},x_{j})}{h}\\right)_{l^{\\prime},l\\in[d]}}\\\\ &{=\\!s_{k}(x_{i})s_{\\ell}(x_{j})^{\\top}\\!\\left(\\operatorname*{lim}\\frac{D^{(e_{\\ell},t)}}{h-0}\\!\\right)\\!\\!\\right\\rangle_{l^{\\prime},l\\in[d]},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where we have used the partial derivative reproducing (32) property and where we define for ${l}^{\\prime}\\neq{l}$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{D}_{l^{\\prime},l}k(x_{i},x_{j})=\\underset{h\\to0}{\\operatorname*{lim}}\\frac{D^{(e_{l^{\\prime}},0)}k(x_{i},x_{j}+h e_{l})-D^{(e_{l^{\\prime}},0)}k(x_{i},x_{j})}{h}}\\\\ &{\\qquad\\qquad=-2\\gamma\\underset{h\\to0}{\\operatorname*{lim}}\\frac{(x_{i}^{(l^{\\prime})}-x_{j}^{(l^{\\prime})})k(x_{i},x_{j}+h e_{l})-(x_{i}^{(l^{\\prime})}-x_{j}^{(l^{\\prime})})k(x_{i},x_{j})}{h}}\\\\ &{\\qquad\\qquad=-4\\gamma^{2}(x_{i}^{(l^{\\prime})}-x_{j}^{(l^{\\prime})})(x_{i}^{(l)}-x_{j}^{(l)})k(x_{i},x_{j}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and for $l\\in[d]$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{D}_{l,l}k(x_{i},x_{j})=\\displaystyle\\operatorname*{lim}_{h\\to0}\\frac{D^{(e_{l},0)}k(x_{i},x_{j}+h e_{l})-D^{(e_{l},0)}k(x_{i},x_{j})}{h}}\\\\ &{\\qquad\\qquad=-2\\gamma\\displaystyle\\operatorname*{lim}_{h\\to0}\\frac{(x_{i}^{(l)}-x_{j}^{(l)}-h)k(x_{i},x_{j}+h e_{l})-(x_{i}^{(l)}-x_{j}^{(l)})k(x_{i},x_{j})}{h}}\\\\ &{\\qquad\\qquad=\\big[2\\gamma\\-4\\gamma^{2}(x_{i}^{(l)}-x_{j}^{(l)})^{2}\\big]k(x_{i},x_{j})=2\\gamma[1-2\\gamma(x_{i}^{(l)}-x_{j}^{(l)})^{2}]k(x_{i},x_{j}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For the overdamped Langevin (see Example 1), for $k\\ \\leq\\ d$ , $s_{k}(x_{i})\\;\\;=\\;\\;(k_{b}T)^{1/2}e_{k}$ and $s_{k}(x_{i})s_{\\ell}(x_{j})^{\\top}=\\bar{(k_{b}T)}I$ so that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathrm{N}_{i,(k-1)n+j}=n^{-1}\\langle\\phi(x_{i}),d_{k}\\phi(x_{j})\\rangle_{\\mathcal{H}}=2\\gamma(k_{b}T)^{1/2}n^{-1}(x_{i}^{(k)}-x_{j}^{(k)})k(x_{i},x_{j})\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and ", "page_idx": 20}, {"type": "equation", "text": "$$\n{\\bf M}_{(k-1)n+i,(\\ell-1)n+j}\\!=\\!n^{-1}\\langle d_{k}\\phi(x_{j}),d_{\\ell}\\phi(x_{j})\\rangle_{\\mathcal{H}}=(k_{b}T)n^{-1}(\\mathfrak{D}_{l^{\\prime},l}k(x_{j},x_{j}))_{l^{\\prime},l\\in[d]},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the elements of $\\mathfrak{D}$ are given \u221aby (\u221a37) and (38). For the CIR process (see Example 2) in dimension $d=1$ , we have $s(x)=\\sigma\\sqrt{x}/\\sqrt{2}$ . Then, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{N}_{i,(k-1)n+j}=n^{-1}\\langle\\phi(x_{i}),d_{k}\\phi(x_{j})\\rangle_{\\mathcal{H}}=\\sqrt{2}\\sigma\\gamma n^{-1}\\sqrt{x_{j}}(x_{i}-x_{j})\\exp(-\\gamma|x_{i}-x_{j}|^{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbf{M}_{(k-1)n+i,(\\ell-1)n+j}\\!=\\!n^{-1}\\langle d_{k}\\phi(x_{i}),d_{\\ell}\\phi(x_{j})\\rangle_{\\mathcal{H}}=\\sigma^{2}\\gamma n^{-1}\\sqrt{x_{i}}\\sqrt{x_{j}}\\exp(-\\gamma|x_{i}-x_{j}|^{2}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Based on the previous formulas, using Theorem 1, which we prove next, one can estimate generator\u2019s eigenpairs in practice. ", "page_idx": 20}, {"type": "text", "text": "Theorem 1. Given $\\mu>0$ and $\\gamma>0$ , let $\\mathbf{J}_{\\mu,\\gamma}\\!=\\!\\mathbf{K}\\!-\\!\\mathbf{N}(\\mathbf{M}\\!+\\!\\gamma\\mu I)^{-1}\\mathbf{N}^{\\top}\\!+\\!\\gamma I.$ . Let $(\\widehat{\\sigma}_{i}^{2},v_{i})_{i\\in[r]}$ be the leading eigenpairs of the following generalized eigenvalue problem ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mu^{-1}(\\mathbf{J}_{\\mu,\\gamma}-\\gamma I)\\mathbf{K}v_{i}=\\widehat{\\sigma}_{i}^{2}\\mathbf{J}_{\\mu,\\gamma}v_{i},\\quad v_{i}^{\\top}\\mathbf{K}v_{j}\\!=\\!\\delta_{i j},~i,j\\in[r].\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Denoting $\\mathbf{V}_{r}\\!=\\!\\left[v_{1}\\,\\big|.\\,.\\,.\\,\\right|v_{r}\\right]\\!\\in\\!\\mathbb{R}^{n\\times r}$ and $\\Sigma_{r}\\!=\\!\\mathrm{diag}(\\widehat{\\sigma}_{1},.\\ .\\ ,\\widehat{\\sigma}_{r})$ , $i f\\big(\\nu_{i},w_{i}^{\\ell},w_{i}^{r}\\big)_{i\\in[r]}$ are eigentriplets of matrix $\\mathbf{V}_{r}^{\\top}\\mathbf{V}_{r}\\Sigma_{r}^{2}\\in\\mathbb{R}^{r\\times r}$ , then the eigenvalue decomposition the RRR estimator $\\widehat{G}_{\\mu,\\gamma}^{r}=\\widehat{Z}_{\\mu}\\mathbf{U}_{r}\\mathbf{V}_{t}^{\\top}\\widehat{S}$ is given by $\\begin{array}{r}{\\widehat{G}_{\\mu,\\gamma}^{r}\\;=\\;\\sum_{i\\in[r]}(\\mu-\\widehat{\\lambda}_{i})^{-1}\\,\\widehat{h}_{i}\\,\\otimes\\,\\widehat{g}_{i}}\\end{array}$ , where $\\widehat{\\lambda}_{i}\\;=\\;\\mu-1/\\nu_{i}$ , i\u22121/2S\u2217Vrwi\u2113 a nd $\\widehat{h}_{i}\\!=\\!\\widehat{Z}_{\\mu}^{*}\\mathbf{U}_{r}w_{i}^{r}$ for $\\mathbf{U}_{r}\\!=\\!(\\mu\\gamma)^{-1}\\!\\left[\\mu^{-1/2}I\\right\\vert-\\!\\mathbf{N}(\\mathbf{M}\\!+\\!\\gamma\\mu I)^{-1}\\right]^{\\top}\\!\\left(\\mathbf{K}\\mathbf{V}_{r}\\!-\\!\\mu\\mathbf{V}_{r}\\Sigma_{r}^{2}\\right)\\!\\in\\!\\mathbb{R}^{(1\\!+\\!p)n\\times r},$ . ", "page_idx": 20}, {"type": "text", "text": "Proof. First, note that $\\mu\\,{\\bf{J}}_{\\mu,\\gamma}\\succ0$ is exactly Schurs\u2019s complement w.r.t. second diagonal block of $\\mathrm{F}_{\\mu,\\gamma}\\succ0$ , and that, due to block inversion lemma [18], we have that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{F}_{\\mu,\\gamma}^{-1}=\\left[{\\mu^{-1}}{\\mathbf{J}_{\\mu,\\gamma}}\\qquad\\qquad\\qquad\\qquad\\mu^{-1/2}\\mathbf{J}_{\\mu,\\gamma}^{-1}\\mathbf{N}(\\mathbf{M}+\\gamma\\mu I)^{-1}\\right].}\\\\ {\\qquad\\qquad\\qquad\\qquad\\mu^{-1/}(\\mathbf{M}+\\gamma\\mu I)^{-1}\\mathbf{N}^{\\top}\\mathbf{J}_{\\mu,\\gamma}^{-1}\\qquad\\qquad\\qquad\\mathbf{A}\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where A is some $n p\\times n p$ matrix. The first step in computing the RRR estimator lies in computing a truncating SVD of $\\widehat{W}_{\\mu,\\gamma}^{-1/2}\\widehat{C}$ , that is $\\widehat{C W}_{\\mu,\\gamma}^{-1}\\widehat{C}q_{i}=\\widehat{\\sigma}_{i}^{2}q_{i},{i}\\in[r]$ . Now, using the low-rank eigenvalue problem formulation [18], we have that $q_{i}=\\widehat{S}^{*}v_{i}$ and $\\widehat{S W}_{\\mu,\\gamma}^{-1}\\widehat{C}\\widehat{S}^{*}v_{i}=\\widehat{\\sigma}_{i}^{2}v_{i}$ . Now, recalling that $\\widehat{S}=[\\mu^{-1/2}\\,|\\,0]\\widehat{Z}_{\\mu}$ we obtain and that $\\widehat Z_{\\mu}(\\widehat Z_{\\mu}^{*}\\widehat Z_{\\mu}+\\mu\\gamma I)^{-1}{=}(\\widehat Z_{\\mu}\\widehat Z_{\\mu}^{*}+\\mu\\gamma I)^{-1}\\widehat Z_{\\mu}$ , we obtain ", "page_idx": 20}, {"type": "equation", "text": "$$\n[\\mu^{-1/2}I\\mid0]\\mathrm{F}_{\\mu,\\gamma}^{-1}\\mathrm{F}_{\\mu}[\\mu^{-1/2}I\\mid0]^{\\top}\\mathrm{K}v_{i}=\\widehat\\sigma_{i}^{2}v_{i},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which after some algebra, using (39) ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mu^{-1}(I-\\gamma{\\bf J}_{\\mu,\\gamma}^{-1}){\\bf K}v_{i}=\\widehat\\sigma_{i}^{2}v_{i},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "i.e. $\\mu^{-1}(I-\\gamma{\\bf J}_{\\mu,\\gamma}^{-1}){\\bf K}{\\bf V}_{r}={\\bf V}_{r}\\Sigma_{r}^{2}$ ", "page_idx": 20}, {"type": "text", "text": "By normalizing right singular value functions $q_{i}$ of $\\widehat{W}_{\\mu,\\gamma}^{-1/2}\\widehat{C}$ , that is by asking that $\\langle q_{i},q_{i}\\rangle_{\\mathcal{H}}\\,=$ $v_{i}^{\\top}\\mathbf{K}v_{i}=1$ , we obtain that $\\|\\widehat{W}_{\\mu,\\gamma}^{-1/2}\\widehat{C}\\|_{r}=\\widehat{W}_{\\mu,\\gamma}^{-1/2}\\widehat{C}Q_{r}Q_{r}^{*}$ , for $Q_{r}=\\left[q_{1}\\mid\\ldots\\mid q_{r}\\right]$ . In other words, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\hat{G}_{\\mu,\\gamma}=\\widehat{W}_{\\mu,\\gamma}^{-1}\\widehat{Z}_{\\mu}^{*}[\\mu^{-1/2}I\\mid0]^{\\top}\\widehat{S}\\widehat{S}^{*}\\mathbf{V}_{r}\\mathbf{V}_{r}^{\\top}\\widehat{S}=\\widehat{Z}_{\\mu}^{*}\\mathbf{F}_{\\mu,\\gamma}^{-1}[\\mu^{-1/2}I\\mid0]^{\\top}\\mathbf{K}\\mathbf{V}_{r}\\mathbf{V}_{r}^{\\top}\\widehat{S}=\\widehat{Z}_{\\mu}^{*}\\mathbf{U}_{r}\\mathbf{V}_{r}^{\\top}\\widehat{S},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where we used that $\\mathbf{J}_{\\mu,\\gamma}^{-1}\\mathbf{K}\\mathbf{V}_{r}=\\gamma^{-1}[\\mathbf{K}\\mathbf{V}_{r}-\\mu\\mathbf{V}_{r}\\boldsymbol{\\Sigma}_{r}^{2}]$ . Once with this form, we apply [27, Theorem 2] to obtain the result. \u53e3 ", "page_idx": 20}, {"type": "text", "text": "Finally, next result provides the reasoning for using empirical metric distortion of Theorem 23. ", "page_idx": 20}, {"type": "text", "text": "Proposition 6. Under the assumptions of Theorem $^{\\,l}$ , for every $i\\in[r]$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\widehat{\\eta_{i}}=\\frac{\\Vert\\widehat{h_{i}}\\Vert}{\\Vert\\widehat{Z_{\\mu}\\widehat{h_{i}}}\\Vert}=\\sqrt{\\frac{(w_{i}^{r})^{*}U_{r}^{\\top}\\mathrm{F}_{\\mu}U_{r}w_{i}^{r}}{\\Vert\\mathrm{F}_{\\mu}U_{r}w_{i}^{r}\\Vert^{2}}},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left|\\widehat{\\eta}_{i}-\\eta(\\widehat{h}_{i})\\right|\\leq\\left(\\eta(\\widehat{h}_{i})\\,\\wedge\\widehat{\\eta}_{i}\\right)\\,\\eta(\\widehat{h}_{i})\\,\\widehat{\\eta}_{i}\\,\\|\\widehat{W}_{\\mu}-W_{\\mu}\\|.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. First, note that (40) follows directly from Theorem 1. Next, since for every $i\\in[r]$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n(\\widehat{\\eta}_{i})^{-2}-(\\eta(\\widehat{h}_{i}))^{-2}=\\frac{\\langle\\widehat{h}_{i},(\\widehat{W}_{\\mu}-W_{\\mu})\\widehat{h}_{i})\\rangle}{\\|\\widehat{h}_{i}\\|^{2}}\\leq\\|\\widehat{W}_{\\mu}-W_{\\mu}\\|,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "we obtain ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\Big|\\widehat{\\eta_{i}}^{-1}-(\\eta(\\widehat{h}_{i}))^{-1}\\Big|\\leq\\frac{\\Big|\\widehat{\\eta_{i}}^{-2}-(\\eta(\\widehat{h}_{i}))^{-2}\\Big|}{(\\eta(\\widehat{h}_{i}))^{-1}\\ \\vee\\ \\widehat{\\eta_{i}}^{1}}\\leq\\Big(\\eta(\\widehat{h}_{i})\\ \\wedge\\widehat{\\eta_{i}}\\Big)\\,\\|\\widehat{W}_{\\mu}-W_{\\mu}\\|.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "E Learning bounds ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "E.1 Main assumptions ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Recalling the form of IG in Equation 3, the Dirichlet form in (DF) for a self-adjoint $L$ exists whenever the positive definite diffusion part satisfies uni\u221aform ellipticity conditions and the drift term allows integration by parts, leading to $s(x)=b(x)/\\sqrt{2}$ . Thus, to obtain the partial knowledge we need, it is enough to estimate the diffusion function $s_{\\epsilon}$ that satisfies the relative error bound in (DF). When the sample paths are observed continuously, such $s_{\\epsilon}$ can be directly identified from these observations, making it a non-statistical problem. For discrete realizations of the process, the diffusion coefficient can be estimated non-parametrically using various methods, such as pathwise estimation by computing the variance of the increments over small intervals [21], kernel-based methods [15], local polynomial regression [13]. Remark, however, that the estimation of the drift (needed for the full knowledge) is a much more demanding task. Different methods are reviewed in [31] and references therein. A more recent approach [9] drawing inspiration from particle systems, consists in constructing estimates from several i.i.d. paths of the solution process. ", "page_idx": 21}, {"type": "text", "text": "Next, observe that (BK) implies $Z_{\\mu}\\in\\mathrm{HS}\\left({\\mathcal{H}},{\\mathcal{W}}_{\\pi}^{\\mu}(X)\\right)$ , which according to the spectral theorem for positive self-adjoint operators, has an SVD, i.e. there exists at most countable positive sequence $\\bar{(\\sigma_{j})_{j\\in J}}$ , where $J=\\{1,2,\\ldots,\\}\\subseteq\\mathbb{N}$ , and ortho-normal systems $(z_{j})_{j\\in J}$ and $\\bar{(h_{j})_{j\\in J}}$ of $\\operatorname{cl}(\\operatorname{Im}(Z_{\\mu}))$ and $\\mathrm{Ker}(Z_{\\mu})^{\\perp}$ , respectively, such that $Z_{\\mu}h_{j}=\\sigma_{j}z_{j}$ and $Z_{\\mu}^{*}z_{j}=\\sigma_{j}h_{j}$ , $j\\in J$ . ", "page_idx": 21}, {"type": "text", "text": "Now, given $\\alpha\\geq0$ , let us define scaled injection operator $Z_{\\mu,\\alpha}\\colon\\mathcal{H}\\rightarrow\\mathcal{W}_{\\pi}^{\\mu}(\\mathcal{X})$ as ", "page_idx": 21}, {"type": "equation", "text": "$$\nZ_{\\mu,\\alpha}=\\sum_{j\\in J}\\sigma_{j}^{\\alpha}z_{j}\\otimes h_{j}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Clearly, we have that $Z_{\\mu}=Z_{\\mu,1}$ , while $\\mathrm{Im}\\,Z_{\\mu,0}=\\mathrm{cl}(\\mathrm{Im}(Z_{\\mu}))$ . Next, we equip $\\operatorname{Im}(Z_{\\mu,\\alpha})$ with a norm $\\Vert\\cdot\\Vert_{\\mathcal{H},\\alpha}$ to build an interpolation space ", "page_idx": 21}, {"type": "equation", "text": "$$\n[\\mathcal{H}]_{\\alpha}=\\left\\{f\\in\\operatorname{Im}(Z_{\\mu,\\alpha})\\mid\\|f\\|_{\\mathcal{H},\\alpha}^{2}=\\sum_{j\\in J}\\sigma_{j}^{-2\\alpha}\\langle f,z_{j}\\rangle_{\\mathcal{W}}^{2}<\\infty\\right\\},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "noting that the inner product in $\\mathcal{W}_{\\pi}^{\\mu}(\\mathcal{X})$ is given by bilinear energy functional ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\langle f,g\\rangle_{\\mathcal{W}}=\\mu\\langle f,g\\rangle_{\\mathcal{L}_{\\pi}^{2}}-\\langle f,L g\\rangle_{\\mathcal{L}_{\\pi}^{2}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We remark that for $\\alpha=1$ the space $[\\mathcal{H}]_{\\alpha}$ is just an RKHS $\\mathcal{H}$ seen as a subspace of $\\mathcal{W}_{\\pi}^{\\mu}(\\mathcal{X})$ . Moreover, we have the following injections ", "page_idx": 21}, {"type": "equation", "text": "$$\n[\\mathcal{H}]_{\\alpha_{1}}\\hookrightarrow[\\mathcal{H}]_{1}\\hookrightarrow[\\mathcal{H}]_{\\alpha_{2}}\\hookrightarrow[\\mathcal{H}]_{0}=\\mathcal{W}_{\\pi}^{\\mu}(\\mathcal{X}),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\alpha_{1}\\geq1\\geq\\alpha_{2}\\geq0$ . ", "page_idx": 22}, {"type": "text", "text": "In addition, from (BK) we also have that RKHS $\\mathcal{H}$ can be embedded into ", "page_idx": 22}, {"type": "equation", "text": "$$\nW_{\\pi}^{\\mu,\\infty}(\\mathcal{X})=\\{f\\in\\mathcal{W}_{\\pi}^{\\mu}(\\mathcal{X})\\,|\\,\\|f\\|_{W_{\\pi}^{\\mu,\\infty}}=\\operatorname{ess}_{x\\sim\\pi}[|f(x)|^{2}-f(x)[L f](x)]<\\infty\\}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "that is, for some $\\tau\\in(0,1]$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n[{\\mathcal{H}}]_{1}\\hookrightarrow[{\\mathcal{H}}]_{\\tau}\\hookrightarrow W_{\\pi}^{\\mu,\\infty}(X)\\hookrightarrow{\\mathcal{W}}_{\\pi}^{\\mu}(X).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Now, according to [14], if $Z_{\\mu,\\tau,\\infty}\\colon[\\mathcal{H}]_{\\tau}\\hookrightarrow L_{\\pi}^{\\infty}(\\mathcal{X})$ denotes the injection operator, its boundedness implies the polynomial decay of the singular values of $Z_{\\mu}$ , i.e. $\\sigma_{j}^{2}(Z_{\\mu})\\lesssim j^{-1/\\tau}$ , $j\\in J$ , and the condition $(\\bf K E)$ is assured. ", "page_idx": 22}, {"type": "text", "text": "Assumption (SD) allows one to quantify the effective dimension of $\\mathcal{H}$ in ambient space $\\mathcal{W}_{\\pi}^{\\mu}(\\mathcal{X})$ , while the kernel embedding property $(\\bf K E)$ allows one to estimate the norms of whitened feature maps, in our generator setting vector-valued since they define rank $\\left(1+p\\right)$ operators on $\\mathcal{H}$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\xi(x):=W_{\\mu,\\gamma}^{-1/2}w\\phi(x)\\in\\mathcal{H}^{1+p}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "This object plays a key role in deriving the learning rates for regression problems, [29] and the following result is bounding it. ", "page_idx": 22}, {"type": "text", "text": "Lemma 1. Let (KE) hold for some $\\tau\\in[\\beta,1]$ and $c_{\\tau}\\in(0,\\infty)$ . Then, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{x\\sim\\pi}\\|\\xi(x)\\|_{\\mathcal{H}^{1+p}}^{2}\\leq\\left\\{\\frac{c_{\\beta}^{\\beta}}{1-\\beta}(\\mu\\gamma)^{-\\beta}\\quad,\\beta<1,\\mathrm{~}a n d\\ \\|\\xi\\|_{\\infty}^{2}=\\displaystyle\\operatorname{ess}_{x\\sim\\pi}\\operatorname{sup}_{\\substack{x\\sim\\pi}}\\|\\xi(x)\\|_{\\mathcal{H}^{1+p}}^{2}\\leq c_{\\tau}(\\mu\\gamma)^{-\\tau}.\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. W.l.o.g. set $\\mu=1$ , observing that the only change in the proof is in scaling $\\gamma>0$ . We first observe that for every $j\\in J$ from definition of $w\\phi$ and fact that $\\bar{h}_{j}(x)=[Z_{\\mu}h_{j}](\\bar{x})$ $\\pi$ -a.e., it holds that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sum_{\\mathbf{\\mu}}\\langle w_{i}\\phi(x),h_{j}\\rangle^{2}\\!\\!=\\!\\!\\mu|h_{j}(x)|^{2}\\!\\!-\\!\\!h_{j}(x)[L Z_{\\mu}h_{j}](x)\\!=\\!\\!\\mu|[Z_{\\mu}h_{j}](x)|^{2}\\!-\\![Z_{\\mu}h_{j}](x)[L Z_{\\mu}h_{j}](x),\\;\\pi\\mathbf{\\cdota.e.},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "implying that $\\begin{array}{r}{\\sum_{i\\in[1+p]}\\left\\langle w_{i}\\phi(x),h_{j}\\right\\rangle^{2}\\leq\\sigma_{j}\\mu|z_{j}(x)|^{2}-z_{j}(x)[L z_{j}](x)}\\end{array}$ . So, for every $\\tau>0$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\xi(x)\\|_{\\mathcal{H}^{1+p}}^{2}\\!=\\!\\displaystyle\\sum_{j\\in J}\\displaystyle\\sum_{i\\in[1+p]}\\langle W_{\\mu,\\gamma}^{-1/2}w_{i}\\phi(x),h_{j}\\rangle^{2}=\\displaystyle\\sum_{j\\in J}\\displaystyle\\sum_{i\\in[1+p]}\\displaystyle\\frac{1}{\\sigma_{j}^{2}+\\gamma}\\langle w_{i}\\phi(x),h_{j}\\rangle^{2}}\\\\ &{\\qquad=\\displaystyle\\sum_{j\\in J}\\displaystyle\\sum_{i\\in[1+p]}\\displaystyle\\frac{\\sigma_{j}^{2(1-\\tau)}}{\\sigma_{j}^{2}+\\gamma}\\frac{\\langle w_{i}\\phi(x),h_{j}\\rangle^{2}}{\\sigma_{j}^{2}}\\sigma_{j}^{2\\tau}\\!\\!=\\!\\gamma^{-\\tau}\\displaystyle\\sum_{j\\in J}\\displaystyle\\sum_{i\\in[1+p]}\\displaystyle\\frac{(\\sigma_{j}^{2}\\gamma^{-1})^{1-\\tau}}{\\sigma_{j}^{2}\\gamma^{-1}+1}\\frac{\\langle w_{i}\\phi(x),h_{j}\\rangle^{2}}{\\sigma_{j}^{2}}\\sigma_{j}^{2\\tau}}\\\\ &{\\qquad\\le\\!\\gamma^{-\\tau}\\displaystyle\\sum_{j\\in J}\\displaystyle\\frac{\\mu|h_{j}(x)|^{2}-h_{j}(x)[L Z_{\\mu}h_{j}](x)}{\\sigma_{j}^{2}}\\sigma_{j}^{2\\tau}\\!\\!=\\!\\gamma^{-\\tau}\\displaystyle\\sum_{j\\in J}\\displaystyle(\\mu|z_{j}(x)|^{2}-z_{j}(x)[L z_{j}](x))\\sigma_{j}^{2\\tau},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and, due to (21), we obtain $\\|\\xi\\|_{\\infty}^{2}\\leq\\gamma^{-\\tau}c_{\\tau}$ . On the other hand, we also have that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathrm{tr}(\\mathbb{E}_{x\\sim\\pi}[\\xi(x)\\otimes\\xi(x)])=\\mathrm{tr}(W_{\\mu,\\gamma}^{-1/2}W_{\\mu}W_{\\mu,\\gamma}^{-1/2})=\\mathrm{tr}(W_{\\mu,\\gamma}^{-1}W_{\\mu}),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which is an effective dimension of the RKHS $\\mathcal{H}$ in $\\mathcal{W}_{\\pi}^{\\mu}(\\mathcal{X})$ . Therefore, following the proof of Fischer and Steinwart [14, Lemma 11] for classical covariances in ${\\mathcal{L}}_{\\pi}^{2}$ , we show that the bound on the effective dimension is ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathrm{tr}(W_{\\mu,\\gamma}^{-1}W_{\\mu})=\\sum_{j\\in{\\cal N}}\\frac{\\sigma_{j}^{2}}{\\sigma_{j}^{2}+\\gamma}\\le\\left\\{\\begin{array}{l l}{{\\frac{c_{\\beta}^{\\beta}}{1-\\beta}}\\gamma^{-\\beta}}&{,\\beta<1,}\\\\ {c_{\\tau}\\,\\gamma^{-1}}&{,\\beta=1.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "For the case $\\beta=1$ , it suffices to see that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\Vert z_{j}\\Vert_{\\mathcal{W}}=\\mathfrak{E}_{\\mu}[z_{j}]\\leq\\Vert z_{j}\\Vert_{W_{\\pi}^{\\mu,\\infty}}=\\displaystyle\\mathfrak{e s s\\,s u p}_{x\\sim\\pi}[|z_{j}(x)|^{2}-z_{j}(x)[L Z_{\\mu}z_{j}](x)],\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and, hence ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathrm{tr}(W_{\\mu,\\gamma}^{-1}W_{\\mu}){\\le}\\gamma^{-1}\\sum_{j\\in N}\\sigma_{j}^{2}\\Vert z_{j}\\Vert_{\\mathcal{W}}^{2}{=}\\gamma^{-1}\\sum_{j\\in N}\\sigma_{j}^{2}\\mathfrak{E}_{\\mu}[z_{j}]{\\le}\\gamma^{-1}c_{\\tau}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "For $\\beta<1$ we can apply the same classical reasoning as in the proof of Proposition 3 of [8]. ", "page_idx": 22}, {"type": "text", "text": "Proof. Note first that the resolvent $(\\mu I-L)^{-1}$ admits same eigenfunctions as the generator $L$ , meaning that $\\operatorname{Im}((\\mu I-L)^{-1}Z_{\\mu})\\,\\subseteq\\,{\\dot{\\operatorname{cl}}}(\\operatorname{Im}({\\dot{Z}}_{\\mu,\\alpha}))$ . But according to [54, Theorem 2.2], this last condition is equivalent to Condition (RC). \u53e3 ", "page_idx": 23}, {"type": "text", "text": "Finally we prove here Proposition 8 ", "page_idx": 23}, {"type": "text", "text": "Proposition 8. Given $\\mu>0$ , let $\\mathcal{H}\\subseteq\\mathcal{W}_{\\pi}^{\\mu}(\\mathcal{X})$ be the RKHS associated to kernel $k\\in\\mathcal{C}^{2}(\\mathcal{X}\\times\\mathcal{X})$ such that $Z_{\\mu}~\\in~\\mathrm{HS}\\left({\\mathcal H},{\\mathcal W}_{\\pi}^{\\mu}(X)\\right)$ , and let $P_{\\mathcal{H}}$ be the orthogonal projector onto the closure of $\\operatorname{Im}(Z_{\\mu})\\subseteq{\\dot{\\mathcal{W}}}_{\\pi}^{\\mu}({\\mathcal{X}})$ . Then for every $\\varepsilon>0$ there exists a finite rank operator $G\\colon{\\mathcal{H}}\\!\\to{\\mathcal{H}}$ such that $\\mathcal{R}(G)\\leq\\|(I-P_{\\mathcal{H}})(\\mu I-L)^{-1}Z_{\\mu}\\|_{\\mathrm{HS}(\\mathcal{H},\\mathcal{W})}^{2}+\\varepsilon.$ . Consequently, when $k$ is universal, $\\mathcal{R}(G)\\leq\\varepsilon$ . ", "page_idx": 23}, {"type": "text", "text": "Proof. Recall first that since $Z_{\\mu}\\in\\mathrm{HS}\\left({\\mathcal{H}},{\\mathcal{W}}_{\\pi}^{\\mu}(X)\\right)$ , according to the spectral theorem for positive self-adjoint operators, $Z_{\\mu}$ admits an SVD. Its form is provided in (42) taking $\\alpha=1$ . ", "page_idx": 23}, {"type": "text", "text": "Now, recalling that $[\\![\\cdot]\\!]_{r}$ denotes the $r$ -truncated SVD, i.e. $\\begin{array}{r}{\\|Z_{\\mu}\\|_{r}=\\sum_{j\\in[r]}\\sigma_{j}z_{j}\\otimes h_{j}}\\end{array}$ , since $\\|Z_{\\mu}-$ $\\begin{array}{r}{\\|Z_{\\mu}\\|_{r}\\|_{\\mathrm{HS}}^{2}\\,=\\,\\sum_{j>r}\\sigma_{j}^{2}}\\end{array}$ , for every $\\delta\\,>\\,0$ there exists $r\\,\\in\\,\\mathbb{N}$ such that $\\|Z_{\\mu}-\\|Z_{\\mu}\\|_{r}\\|_{\\mathrm{HS}}\\,<\\,\\mu\\delta/3$ . Consequently since all the eigenvalues of $L$ are non-positive, $\\|(\\mu I-L)^{-1}(Z_{\\mu}\\mathrm{~-~}\\[Z_{\\mu}\\]_{r})\\|_{\\mathrm{HS}}\\;\\leq$ $\\|Z_{\\mu}-\\|Z_{\\mu}\\|_{r}\\|_{\\mathrm{HS}}/\\mu\\leq\\delta/3$ . Next since $\\operatorname{Im}(P_{\\mathcal{H}}(\\mu I-L)^{-1}Z_{\\mu})\\subseteq\\operatorname{cl}(\\operatorname{Im}(Z_{\\mu}))$ , for any $j\\in[r]$ , there exists $g_{j}\\,\\in\\,{\\mathcal{H}}$ s.t. $\\begin{array}{r}{\\|P_{\\mathcal{H}}({\\mu}I-L)^{-1}z_{j}-Z_{\\mu}g_{j}\\|\\,\\le\\,\\frac{\\delta}{3r}}\\end{array}$ , and, denoting $\\begin{array}{r}{B_{r}:=\\sum_{j\\in[r]}\\sigma_{j}g_{j}\\otimes h_{j}}\\end{array}$ we conclude $\\|P_{\\mathcal{H}}(\\mu I-L)^{-1}\\|Z_{\\mu}\\|_{r}-Z_{\\mu}B_{r}\\|_{\\mathrm{HS}}\\leq\\delta/3.$ . Finally we recall that the set of non-defective matrices is dense in the space of matrices [47], implying that the set of non-defective rank- $^r$ linear operators is dense in the space of rank- $^r$ linear operators on a Hilbert space. Therefore, there exists a non-defective $G\\in\\mathrm{B}_{r}(\\mathcal{H})$ such that $\\lVert G-B_{r}\\rVert_{\\mathrm{HS}}<\\delta/(3\\sigma_{1}(Z_{\\mu}))$ . So, we conclude ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|(\\mu I-L)^{-1}Z_{\\mu}-Z_{\\mu}G\\|_{\\mathrm{HS}}}\\\\ &{\\qquad\\quad\\leq\\|(I-P_{\\mathcal H})(\\mu I-L)^{-1}Z_{\\mu}\\|_{\\mathrm{HS}}+\\|(\\mu I-L)^{-1}Z_{\\mu}-\\|(\\mu I-L)^{-1}Z_{\\mu}\\|_{r}\\|_{\\mathrm{HS}}}\\\\ &{\\qquad\\qquad\\qquad+\\|[(\\mu I-L)^{-1}Z_{\\mu}]_{r}-Z_{\\mu}B_{r}\\|_{\\mathrm{HS}}+\\|Z_{\\mu}(G-B_{r})\\|_{\\mathrm{HS}}}\\\\ &{\\qquad\\qquad\\quad\\leq\\|(I-P_{\\mathcal H})(\\mu I-L)^{-1}Z_{\\mu}\\|_{\\mathrm{HS}}+\\delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Example 5. These three conditions depend on the process $X$ (through its generator $L$ ) as well as the chosen RKHS. They are satisfied, for example, by choosing for $k$ as a Gaussian kernel. Indeed, the sub-linearity conditions on A and $B$ required to ensure the existence and uniqueness of the solution of the process of (1), also ensure that $A$ and $B$ are sufficiently \u2019nice\u2019 to fulfli, notably, condition ((BK)). ", "page_idx": 23}, {"type": "text", "text": "E.2 Bounding the Bias ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Recalling the error decomposition and passing to the $\\mathcal{H}$ and ${\\mathcal{L}}_{\\pi}^{2}$ -norms, we have that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}(\\widehat{G})\\leq\\underbrace{\\Vert(\\mu I-L)^{-1}Z_{\\mu}-Z_{\\mu}G_{\\mu,\\gamma}\\Vert_{\\mathcal{H}\\to\\mathcal{W}}}_{\\mathrm{regularization~bias}}+\\underbrace{\\Vert W_{\\mu}^{1/2}(G_{\\mu,\\gamma}-G_{\\mu,\\gamma}^{r})\\Vert}_{\\mathrm{rank~reduction~bias}}+\\underbrace{\\Vert W_{\\mu}^{1/2}(G_{\\mu,\\gamma}^{r}-\\widehat{G}_{\\mu,\\gamma}^{r})\\Vert}_{\\mathrm{estimator's~variance}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and continue to prove the bound of the first term. Note that, while this proof technique is standard for operator learning [29, 35], we present it here for the sake of completeness. ", "page_idx": 23}, {"type": "text", "text": "Proposition 9. Let $G_{\\mu,\\gamma}\\,=\\,W_{\\mu,\\gamma}^{-1}C$ for $\\gamma>0$ , and $P_{\\mathcal{H}}\\colon{\\mathcal{W}}_{\\pi}^{\\mu}({\\mathcal{X}})\\to{\\mathcal{W}}_{\\pi}^{\\mu}({\\mathcal{X}})$ be the orthogonal projector onto $\\operatorname{cl}(\\operatorname{Im}(Z_{\\mu}))$ . If the assumptions (BK), (SD) and (RC) hold, then $\\|G_{\\mu,\\gamma}\\|\\leq c_{\\alpha}c_{\\mathcal{W}}^{(\\alpha-1)/2}$ for $\\alpha\\in[1,2]$ , $\\begin{array}{r}{\\|G_{\\mu,\\gamma}\\|\\stackrel{{}}{\\leq}c_{\\alpha}\\,(\\mu\\gamma)^{(\\alpha-1)/2}}\\end{array}$ for $\\alpha\\in(0,1]$ , and ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|(\\mu I-L)^{-1}Z_{\\mu}-Z_{\\mu}G_{\\mu,\\gamma}\\|_{\\mathcal H\\to\\mathcal W}\\leq c_{\\alpha}\\left(\\mu\\gamma\\right)^{\\frac{\\alpha}{2}}+\\|(I-P_{\\mathcal H})(\\mu I-L)^{-1}Z_{\\mu}\\|_{\\mathcal H\\to\\mathcal W}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. Recalling that $\\begin{array}{r}{P_{\\mathcal{H}}:=\\sum_{j\\in J}z_{j}\\otimes z_{j}}\\end{array}$ , start by denoting the orthogonal projectors on the subspace of $k$ leading left singular functions of $Z_{\\mu}$ as $\\begin{array}{r}{P_{k}:=\\sum_{j\\in[k]}z_{j}\\otimes z_{j}}\\end{array}$ , respectively. Next, observe ", "page_idx": 23}, {"type": "text", "text": "that $C{=}Z_{\\mu}^{*}(\\mu I{-}L)^{-1}Z_{\\mu}$ and $Z_{\\mu}^{*}W_{\\mu,\\gamma}^{-1}{=}Z_{\\mu}^{*}(Z_{\\mu}^{*}Z_{\\mu}+\\mu\\gamma I)^{-1}{=}(Z_{\\mu}Z_{\\mu}^{*}+\\mu\\gamma I)^{-1}Z_{\\mu}$ . Therefore, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\mu I\\!-\\!L)^{-1}Z_{\\mu}\\!-\\!Z_{\\mu}G_{\\mu,\\gamma}\\!=\\!(I\\!-\\!Z_{\\mu}W_{\\mu,\\gamma}^{-1}Z_{\\mu}^{*})(\\mu I\\!-\\!L)^{-1}Z_{\\mu}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\!=\\!(I\\!-\\!(Z_{\\mu}Z_{\\mu}^{*}\\!+\\!\\mu\\gamma I)^{-1}Z_{\\mu}Z_{\\mu}^{*})(\\mu I\\!-\\!L)^{-1}Z_{\\mu}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\quad\\!=\\!\\mu\\gamma(Z_{\\mu}Z_{\\mu}^{*}\\!+\\!\\mu\\gamma I)^{-1}(\\mu I\\!-\\!L)^{-1}Z_{\\mu}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and, hence ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\mu I-L)^{-1}Z_{\\mu}-Z_{\\mu}G_{\\mu,\\gamma}\\!=\\left(\\displaystyle\\sum_{j\\in J}\\frac{\\mu\\gamma}{\\sigma_{j}^{2}+\\mu\\gamma}z_{j}\\otimes z_{j}\\right)(\\mu I-L)^{-1}Z_{\\mu}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\left(\\displaystyle\\sum_{j\\in J}\\frac{\\mu\\gamma}{(\\sigma_{j}^{2}+\\mu\\gamma)\\sigma_{j}}z_{j}\\otimes(Z_{\\mu}h_{j})\\right)(\\mu I-L)^{-1}Z_{\\mu}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\left(\\displaystyle\\sum_{j\\in J}\\frac{\\mu\\gamma}{(\\sigma_{j}^{2}+\\mu\\gamma)\\sigma_{j}}z_{j}\\otimes h_{j}\\right)C.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Therefore, for every $\\in J,\\|P_{k}((\\mu I\\!-\\!L)^{-1}Z_{\\mu}-Z_{\\mu}G_{\\mu,\\gamma})\\|_{\\mathcal{H}\\to\\mathcal{W}}^{2}$ becomes ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left\\|\\left(\\sum_{j\\in[k]}\\frac{\\mu\\gamma}{(\\sigma_{j}^{2}\\!+\\!\\mu\\gamma)\\sigma_{j}}z_{j}\\otimes h_{j}\\right)C^{2}\\left(\\sum_{j\\in[k]}\\frac{\\gamma}{(\\sigma_{j}^{2}\\!+\\!\\mu\\gamma)\\sigma_{j}}h_{j}\\otimes z_{j}\\right)\\right\\|_{\\mathcal{H}\\to\\mathcal{W}},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "which, due to (RC), implies that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\|P_{k}((\\mu I-L)^{-1}Z_{\\mu}-Z_{\\mu}G_{\\mu,\\gamma})\\|_{\\mathcal{H}\\to\\mathcal{W}}\\{c_{\\alpha}\\left\\|\\sum_{j\\in[k]}\\frac{\\mu\\gamma\\,\\sigma_{j}^{\\alpha}}{\\sigma_{j}^{2}+\\mu\\gamma}z_{j}\\otimes z_{j}\\right\\|_{\\mathcal{W}\\to\\mathcal{W}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "On the other hand, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{j\\in[k]}\\frac{\\mu\\gamma\\,\\sigma_{j}^{\\alpha}}{\\sigma_{j}^{2}+\\mu\\gamma}z_{j}\\otimes z_{j}\\!=\\!\\gamma^{\\frac{\\alpha}{2}}\\sum_{j\\in[k]}\\frac{(\\sigma_{j}^{2}(\\mu\\gamma)^{-1})^{\\frac{\\alpha}{2}}}{\\sigma_{j}^{2}(\\mu\\gamma)^{-1}+1}z_{j}\\otimes z_{j}\\preceq(\\mu\\gamma)^{\\frac{\\alpha}{2}}\\sum_{j\\in[k]}z_{j}\\otimes z_{j},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the inequality holds due to $x^{s}\\leq x+1$ for all $x\\geq0$ and $s\\in[0,1]$ . Since the norm of the projector equals one, we get $\\|P_{k}((\\mu I{-}L)^{-1}Z_{\\mu}-Z_{\\mu}G_{\\mu,\\gamma})\\|\\leq c_{\\alpha}(\\mu\\gamma)^{\\frac{\\alpha}{2}}$ . ", "page_idx": 24}, {"type": "text", "text": "Next, observe that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\|(P_{\\mathcal{H}}-P_{k})((\\mu I-L)^{-1}Z_{\\mu}-Z_{\\mu}G_{\\mu,\\gamma})\\|_{\\mathcal{H}\\to\\mathcal{W}}^{2}=\\left\\|\\sum_{j\\in\\mathcal{J}\\backslash[k]}\\frac{\\mu^{2}\\gamma^{2}}{(\\sigma_{j}^{2}+\\mu\\gamma)^{2}}(Z_{\\mu}^{*}z_{j})\\otimes(Z_{\\mu}^{*}z_{j})\\right\\|_{\\mathcal{H}\\to\\mathcal{H}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "which is bounded by ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{j\\in J\\backslash[k]}\\frac{\\mu^{2}\\gamma^{2}}{(\\sigma_{j}^{2}+\\mu\\gamma)^{2}}\\|Z_{\\mu}^{*}z_{j}\\|^{2}\\leq\\sum_{j\\in J\\backslash[k]}\\frac{\\mu^{2}\\gamma^{2}\\,\\sigma_{j}^{2\\alpha}}{(\\sigma_{j}^{2}+\\mu\\gamma)^{2}}\\leq\\sum_{j\\in J\\backslash[k]}\\sigma_{j}^{2\\alpha}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Using triangular inequality, for every $k\\in J$ we have that $\\|P_{\\mathcal{H}}((\\mu I\\!-\\!L)^{-1}Z_{\\mu}\\!-\\!Z_{\\mu}G_{\\mu,\\gamma})\\|_{\\mathcal{H}\\to\\mathcal{W}}$ is bounded by ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\|P_{k}((\\mu I-L)^{-1}Z_{\\mu}-Z_{\\mu}G_{\\mu,\\gamma})\\|_{\\mathcal{H}\\to\\mathcal{W}}+\\|(P_{\\mathcal{H}}-P_{k})((\\mu I-L)^{-1}Z_{\\mu}-Z_{\\mu}G_{\\mu,\\gamma})\\|_{\\mathcal{H}\\to\\mathcal{W}}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and, therefore, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\|P_{\\mathcal H}((\\mu I\\!-\\!L)^{-1}Z_{\\mu}\\!-\\!Z_{\\mu}G_{\\mu,\\gamma})\\|_{\\mathcal H\\to W}\\leq c_{\\alpha}(\\mu\\gamma)^{\\frac\\alpha2}+\\sum_{j\\in J\\backslash[k]}(\\sigma_{j}^{2\\beta})^{\\frac\\alpha\\beta},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and, hence, letting $k\\,\\rightarrow\\,\\infty$ we obtain $\\|P_{\\mathcal{H}}(\\mu I\\!-\\!L)^{-1}Z_{\\mu}-Z_{\\mu}G_{\\mu,\\gamma}\\|\\,\\leq\\,c_{\\alpha}(\\mu\\gamma)^{\\frac{\\alpha}{2}}$ . Hence, (47) follows from triangular inequality. ", "page_idx": 25}, {"type": "text", "text": "To estimate the $\\|G_{\\mu,\\gamma}\\|$ , note that (RC) implies $\\|G_{\\mu,\\gamma}\\|\\,\\leq\\,c_{\\alpha}\\,\\|W_{\\mu,\\gamma}^{-1}W_{\\mu}^{\\frac{1+\\alpha}{2}}\\|$ and considering two cases. First, if (RC) holds for some $\\alpha\\in[1,2]$ , then, clearly $\\|G_{\\mu,\\gamma}\\|\\leq c_{\\alpha}c_{\\mathcal{W}}^{(\\alpha-1)/2}$ c(W\u03b1\u22121)/2. On the other hand, if $\\alpha\\in(0,1]$ , then ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\frac{\\sigma_{j}^{1+\\alpha}}{\\sigma_{j}^{2}+\\mu\\gamma}=(\\mu\\gamma)^{-1}\\frac{\\left(\\sigma_{j}^{2}(\\mu\\gamma)^{-1}\\right)^{\\frac{1+\\alpha}{2}}}{\\sigma_{j}^{2}\\left(\\mu\\gamma\\right)^{-1}+1}\\leq(\\mu\\gamma)^{\\frac{\\alpha-1}{2}},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and, thus, $\\|G_{\\mu,\\gamma}\\|\\leq c_{\\alpha}\\,\\gamma^{(\\alpha-1)/2}$ . ", "page_idx": 25}, {"type": "text", "text": "Remark 2. Inequality (47) says that the regularization bias is comprised of a term depending on the choice of $\\gamma_{;}$ , and on a term depending on the \u201calignment\u201d between $\\mathcal{H}$ and $\\mathrm{Im}((\\mu I-L)^{-1}\\bar{Z}_{\\mu})$ . The term $\\|(I-P_{\\mathcal{H}})(\\mu I-L)^{-1}Z_{\\mu}\\|$ can be set to zero by two different approaches. One is choose $a$ kernel which in some way minimizes $\\|(I-P_{\\mathcal{H}})(\\mu I-L)^{-1}Z_{\\mu}\\|$ . Another is to choose a universal kernel $I^{45}$ , Chapter 4], for which $\\operatorname{Im}((\\mu I-L)^{-1}S_{\\pi})\\subseteq\\operatorname{cl}(\\operatorname{Im}(S_{\\pi}))$ . While we here develop theory for universal kernels, deep learning approaches that leverage on on our approach can be developed, which is the direction to pursue in future. ", "page_idx": 25}, {"type": "text", "text": "In order to proceed with bounding the bias due to rank reduction for both considered estimators, we first provide auxiliary result. ", "page_idx": 25}, {"type": "text", "text": "Proposition 10. Let $B:=W_{\\mu,\\gamma}^{-1/2}C_{;}$ , let (RC) hold for some $\\alpha\\in(0,2]$ , and for $j\\in\\mathbb N$ denote ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\lambda_{j}^{\\star}=\\sigma_{j}^{2}((\\mu I{-}L)^{-1}Z_{\\mu})=\\lambda_{j}(S_{\\pi}^{\\ast}(\\mu I-L)^{-1}S_{\\pi})\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Then for every $j\\in\\mathbb N$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\lambda_{j}^{\\star}-c_{\\alpha}^{2}\\,c_{\\mathcal{W}}^{\\alpha/2}\\,(\\mu\\gamma)^{\\alpha/2}\\leq\\sigma_{j}^{2}(B)\\leq\\lambda_{j}^{\\star}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. Start by observing that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{z}\\left[(\\mu I{-}L)^{-1}Z_{\\mu}\\right]^{\\ast}Z_{\\mu}W_{\\mu,\\gamma}^{-1}Z_{\\mu}^{\\ast}(\\mu I{-}L)^{-1}Z_{\\mu}}\\\\ &{\\mathfrak{z}=\\displaystyle[(\\mu I{-}L)^{-1}Z_{\\mu}]^{\\ast}(\\mu I{-}L)^{-1}Z_{\\mu}-\\mu\\gamma[(\\mu I{-}L)^{-1}Z_{\\mu}]^{\\ast}(Z_{\\mu}Z_{\\mu}^{\\ast}+\\mu\\gamma I)^{-1}(\\mu I{-}L)^{-1}Z_{\\mu},}\\\\ &{\\mathrm{at,~using~}[(\\mu I{-}L)^{-1}Z_{\\mu}]^{\\ast}{=}S_{\\pi}\\mathrm{~and~}[(\\mu I{-}L)^{-1}Z_{\\mu}]^{\\ast}(\\mu I{-}L)^{-1}Z_{\\mu}{=}S_{\\pi}^{\\ast}(\\mu I-L)^{-1}S_{\\pi},}\\\\ &{\\qquad S_{\\pi}^{\\ast}(\\mu I-L)^{-1}S_{\\pi}-\\displaystyle\\sum_{j\\in J}\\frac{\\mu\\gamma}{\\sigma_{j}^{2}+\\mu\\gamma}(S_{\\pi}^{\\ast}z_{j})\\otimes(S_{\\pi}^{\\ast}z_{j})=B^{\\ast}B\\preceq Z_{\\mu}^{\\ast}Z_{\\mu}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Next, similarly to the above, for every $k\\in J$ , we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\displaystyle\\sum_{j\\in[k]}\\frac{\\mu\\gamma}{\\sigma_{j}^{2}+\\mu\\gamma}(S_{\\pi}^{*}z_{j})\\otimes(S_{\\pi}^{*}z_{j})\\right|\\right|\\le\\!c_{\\alpha}^{2}\\left\\|\\displaystyle\\sum_{j\\in[k]}\\frac{\\sigma_{j}^{2\\alpha}}{\\sigma_{j}^{2}(\\mu\\gamma)^{-1}+1}z_{j}\\otimes z_{j}\\right\\|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\!c_{\\alpha}^{2}\\left\\|\\displaystyle\\sum_{j\\in[k]}\\!\\frac{(\\sigma_{j}^{2}(\\mu\\gamma)^{-1})^{\\alpha/2}\\sigma_{j}^{\\alpha}(\\mu\\gamma)^{\\alpha/2}}{\\sigma_{j}^{2}(\\mu\\gamma)^{-1}+1}z_{j}\\otimes z_{j}\\right\\|\\le\\!c_{\\alpha}^{2}\\gamma^{\\alpha/2}\\|W_{\\mu}\\|^{\\alpha/2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left\\|\\sum_{j\\in J\\backslash[k]}\\frac{\\gamma}{\\sigma_{j}^{2}+\\mu\\gamma}(S_{\\pi}^{*}z_{j})\\otimes(S_{\\pi}^{*}z_{j})\\right\\|\\leq c_{\\alpha}^{2}\\sum_{j\\in J\\backslash[k]}\\frac{\\gamma}{\\sigma_{j}^{2}+\\mu\\gamma}\\sigma_{j}^{2\\alpha}\\leq c_{\\alpha}^{2}\\sum_{j\\in J\\backslash[k]}(\\sigma_{j}^{2\\beta})^{\\alpha/\\beta}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "So, as before, letting $k\\rightarrow\\infty$ we get the result. ", "page_idx": 25}, {"type": "text", "text": "As a consequence, we obtain the bound for the rank reduction bias of the RRR method. ", "page_idx": 25}, {"type": "text", "text": "Proposition 11 (RRR). Let (RC) hold for some $\\alpha\\,\\in\\,(0,2]$ . Then the bias of $G_{\\mu,\\gamma}^{r}$ due to rank reduction, recalling (48) is bounded as ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sqrt{\\lambda_{r+1}^{\\star}}-c_{\\alpha}\\,c_{\\mathcal{W}}^{\\alpha/4}(\\mu\\gamma)^{\\alpha/4}-2\\,c_{\\alpha}\\,(\\mu\\gamma)^{(1\\wedge\\alpha)/2}\\leq\\|Z_{\\mu}(G_{\\mu,\\gamma}-G_{\\mu,\\gamma}^{r})\\|\\leq\\sqrt{\\lambda_{r+1}^{\\star}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. Observe that ", "page_idx": 26}, {"type": "equation", "text": "$$\nZ_{\\mu}(G_{\\mu,\\gamma}-G_{\\mu,\\gamma}^{r})\\|\\leq\\|W_{\\mu,\\gamma}^{1/2}(G_{\\mu,\\gamma}-G_{\\mu,\\gamma}^{r})\\|=\\|B-\\|B\\|_{r}\\|=\\sigma_{r+1}(B)\\leq\\sigma_{r+1}((\\mu I-L)^{-1}Z_{\\mu})\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "while ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|Z_{\\mu}(G_{\\mu,\\gamma}-G_{\\mu,\\gamma}^{r})\\|\\geq\\|W_{\\mu,\\gamma}^{1/2}(G_{\\mu,\\gamma}-G_{\\mu,\\gamma}^{r})\\|-(\\mu\\gamma)^{1/2}\\|G_{\\mu,\\gamma}-G_{\\mu,\\gamma}^{r}\\|}\\\\ &{\\qquad\\qquad\\qquad\\geq\\sigma_{r+1}((\\mu I{-}L)^{-1}Z_{\\mu})-c_{\\alpha}\\|W_{\\mu}\\|^{\\alpha/4}(\\mu\\gamma)^{\\alpha/4}-2c_{\\alpha}(\\mu\\gamma)^{(1\\wedge\\alpha)/2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "E.3 Bounding the Variance ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "E.3.1 Concentration Inequalities ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "All the statistical bounds we present will relay on two versions of Bernstein inequality. The first one is Pinelis and Sakhanenko inequality for random variables in a separable Hilbert space, see [8, Proposition 2]. ", "page_idx": 26}, {"type": "text", "text": "Proposition 12. Let $A_{i}$ , $i\\;\\in\\;[n]$ be i.i.d copies of a random variable $A$ in a separable Hilbert space with norm $\\lVert\\cdot\\rVert$ . If there exist constants $\\Lambda\\ >\\ 0$ and $\\sigma~>~0$ such that for every $m~\\geq~2$ $\\begin{array}{r}{\\dot{\\mathbb{E}}\\|A\\|^{m}\\leq\\frac{1}{2}m!\\Lambda^{m-2}\\sigma^{2}}\\end{array}$ , then with probability at least $1-\\delta$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left\\|\\frac{1}{n}\\sum_{i\\in[n]}A_{i}-\\mathbb{E}A\\right\\|\\leq\\frac{4\\sqrt{2}}{\\sqrt{n}}\\log\\frac{2}{\\delta}\\sqrt{\\sigma^{2}+\\frac{\\Lambda^{2}}{n}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "On the other hand, we recall that in [39], a dimension-free version of the non-commutative Bernstein inequality for finite-dimensional symmetric matrices is proposed (see also Theorem 7.3.1 in [48] for an easier to read and slightly improved version) as well as an extension to self-adjoint Hilbert-Schmidt operators on a separable Hilbert spaces. ", "page_idx": 26}, {"type": "text", "text": "Proposition 13. Let $A_{i}$ , $i\\in[n]$ be i.i.d copies of a Hilbert-Schmidt operator $A$ on the separable Hilbert space. Let $\\left\\|A\\right\\|\\leq c$ almost surely, $\\mathbb{E}A=0$ and let $\\mathbb{E}[A^{2}]\\preceq V$ for some trace class operator $V$ . Then with probability at least $1-\\delta$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left\\|\\frac{1}{n}\\sum_{i\\in[n]}A_{i}\\right\\|\\leq\\frac{2c}{3n}\\mathcal{L}_{A}(\\delta)+\\sqrt{\\frac{2\\|V\\|}{n}\\mathcal{L}_{A}(\\delta)},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathcal{L}_{A}(\\delta)=\\log\\frac{4}{\\delta}+\\log\\frac{\\mathrm{tr}(V)}{\\|V\\|}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proposition 14. Given $\\delta>0$ , with probability in the i.i.d. draw of $(x_{i})_{i=1}^{n}$ from $\\pi$ , it holds that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\{\\|\\widehat{W}_{\\mu}-W_{\\mu}\\|\\leq\\varepsilon_{n}(\\delta)\\}\\geq1-\\delta,}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\varepsilon_{n}(\\delta)=\\frac{2c_{\\mathcal{W}}}{3n}\\mathcal{L}(\\delta)+\\sqrt{\\frac{2\\|W_{\\mu}\\|}{n}\\mathcal{L}(\\delta)}\\quad a n d\\quad\\mathcal{L}(\\delta)=\\log\\frac{4\\operatorname{tr}(W_{\\mu})}{\\delta\\left\\|W_{\\mu}\\right\\|}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. Proof follows directly from Proposition 13 applied to rank- $\\left(1\\!+\\!p\\right)$ operators $w\\phi(x_{i})\\otimes w\\phi(x_{i})$ using the fact that $W_{\\mu}=\\mathbb{E}[\\stackrel{}{w}\\phi(x_{i})\\otimes\\stackrel{\\cdot}{w}\\phi(x_{i})]$ . ", "page_idx": 26}, {"type": "text", "text": "Proposition 15. Let (KE) hold for $\\tau\\in[\\beta,1]$ . Given $\\delta>0$ , with probability in the i.i.d. draw of $(x_{i})_{i=1}^{n}$ from $\\pi$ , it holds that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left\\{\\|W_{\\mu,\\gamma}^{-1/2}(\\widehat{W}_{\\mu}-W_{\\mu})W_{\\mu,\\gamma}^{-1/2}\\|\\leq\\varepsilon_{n}^{1}(\\gamma,\\delta)\\right\\}\\geq1-\\delta,\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\varepsilon_{n}^{1}(\\gamma,\\delta)=\\frac{2c_{\\tau}\\mu^{-\\tau}}{3n\\gamma^{\\tau}}\\mathcal L^{1}(\\gamma,\\delta)+\\sqrt{\\frac{2\\,c_{\\tau}\\mu^{-\\tau}}{n\\,\\gamma^{\\tau}}\\mathcal L^{1}(\\gamma,\\delta)},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathcal{L}^{1}(\\gamma,\\delta)=\\ln\\frac{4}{\\delta}+\\ln\\frac{\\mathrm{tr}(W_{\\mu,\\gamma}^{-1}W_{\\mu})}{\\lVert W_{\\mu,\\gamma}^{-1}W_{\\mu}\\rVert}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Moreover, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left\\{\\|W_{\\mu,\\gamma}^{1/2}\\widehat{W}_{\\mu,\\gamma}^{-1}W_{\\mu,\\gamma}^{1/2}\\|\\leq\\frac{1}{1-\\varepsilon_{n}^{1}(\\gamma,\\delta)}\\right\\}\\geq1-\\delta.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. The idea is to apply Proposition 13 for operator $\\xi(x)\\otimes\\xi(x)$ , where $\\xi(x)$ is defined in (43). Due to (1), we have that $\\|\\dot{A}\\|\\leq\\|\\xi\\|_{\\infty}^{2}\\leq(\\mu\\gamma)^{-\\bar{\\tau}}c_{\\tau}$ . On the other hand, we have that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{x\\sim\\pi}[\\xi(x)\\otimes\\xi(x)]^{2}\\preceq\\|\\xi\\|_{\\infty}^{2}\\mathbb{E}_{x\\sim\\pi}[\\xi(x)\\otimes\\xi(x)]=\\|\\xi\\|_{\\infty}^{2}W_{\\mu,\\gamma}^{-1/2}W_{\\mu}W_{\\mu,\\gamma}^{-1/2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and, hence (54) follows. To complete the proof, observe that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\lVert I_{\\mathcal{H}}-W_{\\mu,\\gamma}^{-1/2}\\widehat{W}_{\\mu,\\gamma}W_{\\mu,\\gamma}^{-1/2}\\rVert=\\lVert W_{\\mu,\\gamma}^{-1/2}(W_{\\mu}-\\widehat{W}_{\\mu})W_{\\mu,\\gamma}^{-1/2}\\rVert\\leq\\varepsilon_{n}^{1}(\\gamma,\\delta),\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and, hence for $\\varepsilon_{n}^{1}(\\gamma,\\delta)$ smaller than one we obtain ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\|W_{\\mu,\\gamma}^{1/2}\\widehat{W}_{\\mu,\\gamma}^{-1}W_{\\mu,\\gamma}^{1/2}\\|=\\|(W_{\\mu,\\gamma}^{-1/2}\\widehat{W}_{\\mu,\\gamma}W_{\\mu,\\gamma}^{-1/2})^{-1}\\|\\leq\\frac{1}{1-\\|I_{\\mathcal{H}}-W_{\\mu,\\gamma}^{-1/2}\\widehat{W}_{\\mu,\\gamma}W_{\\mu,\\gamma}^{-1/2}\\|}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proposition 16. Let (RC), (SD) and (KE) hold for some $\\alpha\\in(0,2]$ , $\\beta\\in(0,1]$ and $\\tau\\in[\\beta,1]$ . Given $\\delta>0$ , with probability in the i.i.d. draw of $(x_{i})_{i=1}^{n}$ from $\\pi$ , it holds ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left\\{\\|W_{\\mu,\\gamma}^{-1/2}(\\widehat{W}_{\\mu}-W_{\\mu})W_{\\mu,\\gamma}^{-1}C\\|_{\\mathrm{HS}}\\leq\\varepsilon_{n}^{2}(\\gamma,\\delta)\\right\\}\\geq1-\\delta,}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\varepsilon_{n}^{2}(\\gamma,\\delta)=4\\,\\sqrt{2\\,c_{\\alpha}\\,c_{\\tau}}\\ln\\frac{2}{\\delta}\\,\\sqrt{\\frac{c_{\\beta}\\mu^{-\\beta}}{n\\gamma^{\\beta}}+\\frac{c_{\\tau}\\mu^{-\\tau}}{n^{2}\\gamma^{\\tau}}}\\;\\left\\{\\begin{array}{l l}{(\\mu\\gamma)^{-(\\tau-\\alpha)/2}}&{,\\alpha\\leq\\tau,}\\\\ {c_{\\nu\\nu}^{(\\alpha-\\tau)/2}}&{,\\alpha\\geq\\tau.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. As before, w.l.o.g. set $\\mu\\:=\\:1\\:$ . First, recall that $\\mathrm{HS}\\left({\\mathcal{H}}\\right)$ equipped with $\\lVert\\cdot\\rVert_{\\mathrm{HS}}$ is separable Hilbert space. Hence, we will apply Proposition 12 for $\\bar{A}\\ \\stackrel{-}{=}\\ \\xi(x)\\,\\otimes\\,\\ddot{\\psi}(x)$ , where $\\psi(x)=C W_{\\mu,\\gamma}^{-1}\\dot{w}\\phi(x)\\in\\mathcal{H}^{1+p}$ . To that end, observe that $\\begin{array}{r}{\\mathbb{E}\\|A\\|_{\\mathrm{HS}}^{m}{=}\\mathbb{E}\\left[\\|\\xi(x)\\|_{\\mathcal{H}^{1+p}}^{m}\\,\\|\\psi(x)\\|_{\\mathcal{H}^{1+p}}^{m}\\right]}\\end{array}$ , and that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\Vert\\xi(x)\\Vert_{\\mathcal{H}^{1+p}}^{m}]\\leq\\frac{1}{2}m!\\left(\\gamma^{-\\tau/2}\\sqrt{c_{\\tau}}\\right)^{m-2}\\left(\\sqrt{\\mathrm{tr}(W_{\\mu,\\gamma}^{-1}W_{\\mu})}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Recalling Lemma 1, the task is to bound $\\begin{array}{r}{\\|\\psi(x)\\|_{\\mathcal{H}^{1+p}}^{2}\\!=\\!\\sum_{i\\in[1+p]}\\|C W_{\\mu,\\gamma}^{-1}w_{i}\\phi(x)\\|_{\\mathcal{H}}^{2}}\\end{array}$ . Using (RC), we have that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\psi(x)\\|_{\\mathcal{H}^{1+p}}^{2}\\leq c_{\\alpha}\\sum_{i\\in[1+p]}\\|W_{\\mu}^{(1+\\alpha)/2}W_{\\mu,\\gamma}^{-1}w_{i}\\phi(x)\\|_{\\mathcal{H}}^{2}{=}c_{\\alpha}\\sum_{j\\in J}\\sum_{i\\in[1+p]}\\langle W_{\\mu}^{(1+\\alpha)/2}W_{\\mu,\\gamma}^{-1}w_{i}\\phi(x),h_{j}\\rangle_{\\mathcal{H}}^{2}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "But, since ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\langle W_{\\mu}^{(1+\\alpha)/2}W_{\\mu,\\gamma}^{-1}w_{i}\\phi(x),h_{j}\\rangle_{\\mathcal{H}}=\\frac{\\sigma_{j}^{1+\\alpha}}{\\sigma_{j}^{2}+\\gamma}\\langle w_{i}\\phi(x),h_{j}\\rangle_{\\mathcal{H}},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "expanding as in proof of Lemma 1 and using (KE), we have that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\|\\psi(x)\\|_{\\mathcal{H}^{1+p}}^{2}\\!\\le\\!c_{\\alpha}\\sum_{j\\in J}\\sum_{i\\in[1+p]}\\left[\\frac{\\sigma_{j}^{(2+\\alpha-\\tau)}}{\\sigma_{j}^{2}+\\gamma}\\right]^{2}\\frac{\\langle w_{i}\\phi(x),h_{j}\\rangle^{2}}{\\sigma_{j}^{2}}\\sigma_{j}^{2\\tau}\\le\\left\\{\\!\\begin{array}{l l}{\\gamma^{-(\\tau-\\alpha)}}&{,\\alpha\\leq\\tau,}\\\\ {c_{\\nu}^{(\\alpha-\\tau)}}&{,\\alpha\\geq\\tau.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Therefore, we can set ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\Lambda^{2}=c_{\\alpha}c_{\\tau}^{2}\\left\\{\\begin{array}{l l}{\\gamma^{-(2\\tau-\\alpha)}}&{,\\alpha\\leq\\tau,}\\\\ {c_{\\mathcal{W}}^{(\\alpha-\\tau)}\\gamma^{-\\tau}}&{,\\alpha\\geq\\tau.}\\end{array}\\right.\\quad\\mathrm{and}\\quad\\sigma^{2}=c_{\\alpha}c_{\\tau}c_{\\beta}\\left\\{\\begin{array}{l l}{\\gamma^{-(\\beta+\\tau-\\alpha)}}&{,\\alpha\\leq\\tau,}\\\\ {c_{\\mathcal{W}}^{(\\alpha-\\tau)}\\gamma^{-\\beta}}&{,\\alpha\\geq\\tau.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "in Proposition 12 to obtain the bound ", "page_idx": 28}, {"type": "text", "text": "Proposition 17. Let $\\mathbf{\\mu}(\\mathbf{KE})$ hold for $\\tau\\in[\\beta,1]$ . Given $\\delta>0$ , with probability in the i.i.d. draw of $(x_{i})_{i=1}^{n}$ from $\\pi$ , it holds ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left\\{\\|W_{\\mu,\\gamma}^{-1/2}(\\widehat{C}-C)\\|_{\\mathrm{HS}}\\leq\\varepsilon_{n}^{3}(\\gamma,\\delta)\\right\\}\\geq1-\\delta,}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\varepsilon_{n}^{3}(\\gamma,\\delta)=\\frac{4\\,\\sqrt{2\\,c\\nu}}{\\mu}\\,\\ln\\frac{2}{\\delta}\\,\\sqrt{\\frac{c_{\\beta}\\mu^{-\\beta}}{n\\gamma^{\\beta}}+\\frac{c_{\\tau}\\mu^{-\\tau}}{n^{2}\\gamma^{\\tau}}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. First, note that since $\\|\\phi(x)\\|_{\\mathcal{H}}^{2}\\leq\\mu^{-1}\\|w\\phi(x)\\|_{\\mathcal{H}^{1+p}}^{2}$ and $C\\preceq\\mu^{-1}W_{\\mu}$ , $(\\bf K E)$ and (SD) for $W_{\\mu}$ imply analogous assumptions for $C$ . Hence, we can apply Proposition 13 from [29], which using the observation that $\\|W_{\\mu,\\gamma}^{-1/2}C_{\\gamma}^{1/2}\\|^{2}{=}\\|C_{\\gamma}^{1/2}(\\mu C_{\\gamma}{-}T)^{-1}C_{\\gamma}^{1/2}\\|{\\le}\\mu^{-1}\\|C_{\\gamma}C_{\\gamma}^{-1}\\|{=}\\mu^{-1}$ , completes the proof. \u53e3 ", "page_idx": 28}, {"type": "text", "text": "Next, we develop concentration bounds of some key quantities used to build RRR empirical estimator. ", "page_idx": 28}, {"type": "text", "text": "E.3.2 Variance and Norm of KRR Estimator ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Proposition 18. Let (RC), (SD) and (KE) hold for some $\\alpha\\in(0,2]$ , $\\beta\\in(0,1]$ and $\\tau\\in[\\beta,1]$ . Given $\\delta>\\stackrel{\\bar{}}{0}i f\\varepsilon_{n}^{1}(\\gamma,\\delta)<1$ , then with probability at least $1-\\delta$ in the i.i.d. draw of $(x_{i},y_{i})_{i=1}^{n}$ from $\\rho$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left\\{\\|W_{\\mu,\\gamma}^{1/2}(\\widehat{G}_{\\mu,\\gamma}-G_{\\mu,\\gamma})\\|\\leq\\frac{\\varepsilon_{n}^{2}(\\gamma,\\delta/3)+\\varepsilon_{n}^{3}(\\gamma,\\delta/3)}{1-\\varepsilon_{n}^{1}(\\gamma,\\delta/3)}\\right\\}\\geq1-\\delta.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. Note that $W_{\\mu,\\gamma}^{1/2}(\\widehat{G}_{\\mu,\\gamma}-G_{\\mu,\\gamma})=W_{\\mu,\\gamma}^{1/2}(\\widehat{W}_{\\mu,\\gamma}^{-1}\\widehat{C}-W_{\\mu,\\gamma}^{-1}C)$ , and, hence, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{W_{\\mu,\\gamma}^{1/2}(\\widehat{G}_{\\mu,\\gamma}-G_{\\mu,\\gamma})=W_{\\mu,\\gamma}^{1/2}\\widehat{W}_{\\mu,\\gamma}^{-1}(\\widehat{C}-\\widehat{W}_{\\mu,\\gamma}W_{\\mu,\\gamma}^{-1}C\\pm C)}\\\\ &{\\qquad\\qquad\\qquad\\qquad=W_{\\mu,\\gamma}^{1/2}\\widehat{W}_{\\mu,\\gamma}^{-1}W_{\\mu,\\gamma}^{1/2}\\left(W_{\\mu,\\gamma}^{-1/2}(\\widehat{C}-C)-W_{\\mu,\\gamma}^{-1/2}(\\widehat{W}_{\\mu}-W_{\\mu})W_{\\mu,\\gamma}^{-1}C\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Thus, taking the norm and using $\\|C_{\\gamma}^{-1}T\\|\\leq c_{\\alpha}\\,\\sigma_{1}^{\\alpha-1}(S_{\\pi})$ with the Propositions 16 and 15 we prove the first bound. ", "page_idx": 28}, {"type": "text", "text": "E.3.3 Variance of Singular Values ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In this section we prove concentration of singular values computed in Theorem 1, a necessary step to derive learining rates for RRR estimator. ", "page_idx": 28}, {"type": "text", "text": "Proposition 19. Let (RC), (SD) and (KE) hold for some $\\alpha\\in(0,2],$ , $\\beta\\in(0,1]$ and $\\tau\\in[\\beta,1]$ . Let $B=W_{\\mu,\\gamma}^{-1/2}C$ and $\\widehat{B}=\\widehat{W}_{\\mu,\\gamma}^{-1/2}\\widehat{C}$ . Given $\\delta>0$ if $\\varepsilon_{n}^{1}(\\gamma,\\delta/5)<1$ , then with probability at least $1-\\delta$ in the i.i.d. dr aw of $(x_{i})_{i=1}^{n}$ from $\\pi$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\sigma_{i}^{2}(\\widehat{B})\\!-\\!\\sigma_{i}^{2}(B)|\\!\\!\\le\\!\\!\\|\\widehat{B}^{\\ast}\\widehat{B}-B^{\\ast}B\\|\\!\\!\\le\\!\\varepsilon_{n}^{4}(\\gamma,\\delta/3),}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\varepsilon_{n}^{4}(\\gamma,\\delta/3)=(\\varepsilon_{n}^{2}(\\gamma,\\delta/3)+\\varepsilon_{n}^{3}(\\gamma,\\delta/3))\\Bigl(\\frac{1}{\\mu}+\\frac{\\varepsilon_{n}^{2}(\\gamma,\\delta/3)+\\varepsilon_{n}^{3}(\\gamma,\\delta/3)}{1-\\varepsilon_{n}^{1}(\\gamma,\\delta/3)}\\Bigr).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. We start from the Weyl\u2019s inequalities for the square of singular values ", "page_idx": 28}, {"type": "equation", "text": "$$\n|\\sigma_{i}^{2}(\\widehat{B})-\\sigma_{i}^{2}(B)|\\leq\\|\\widehat{B}^{*}\\widehat{B}-B^{*}B\\|,\\ i\\in[n].\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "But, since, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\hat{3}^{*}\\hat{B}-B^{*}B=\\hat{C}^{*}\\widehat{W}_{\\mu,\\gamma}^{-1}\\hat{C}-C^{*}W_{\\mu,\\gamma}^{-1}C=(\\widehat{C}-C)^{*}\\widehat{W}_{\\mu,\\gamma}^{-1}\\hat{C}+C^{*}W_{\\mu,\\gamma}^{-1}(\\hat{C}-C)+C^{*}(\\widehat{W}_{\\mu,\\gamma}^{-1}-W_{\\mu,\\gamma}^{-1})\\hat{C}\\quad,\n$$", "text_format": "latex", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{3}^{*}\\widehat{B}-B^{*}B=B^{*}M+M^{*}C_{\\gamma}^{1/2}\\widehat{G}_{\\mu,\\gamma}-B^{*}N\\widehat{G}_{\\mu,\\gamma}=B^{*}M+(M^{*}C_{\\gamma}^{1/2}-B^{*}N)(\\widehat{G}_{\\mu,\\gamma}\\pm G_{\\mu,\\gamma})}\\\\ &{\\qquad\\qquad\\qquad=B^{*}M+M^{*}B-B^{*}N G_{\\mu,\\gamma}+(M^{*}-B^{*}N W_{\\mu,\\gamma}^{-1/2})R}\\\\ &{\\qquad\\qquad\\qquad=(G_{\\mu,\\gamma})^{*}(\\widehat{C}-C)+(\\widehat{C}-C)G_{\\mu,\\gamma}-(G_{\\mu,\\gamma})^{*}(\\widehat{W}_{\\mu}-W_{\\mu})G_{\\mu,\\gamma}+(M^{*}+(G_{\\mu,\\gamma})^{*}N^{*})R.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Therefore, recalling that, due to (59), $R=W_{\\mu,\\gamma}^{1/2}\\widehat{W}_{\\mu,\\gamma}^{-1}W_{\\mu,\\gamma}^{1/2}(M-N G_{\\mu,\\gamma})$ , we conclude ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{B}^{*}\\widehat{B}-B^{*}B=(G_{\\mu,\\gamma})^{*}(\\widehat{C}-C)+(\\widehat{C}-C)G_{\\mu,\\gamma}-(G_{\\mu,\\gamma})^{*}(\\widehat{W}_{\\mu}-W_{\\mu})G_{\\mu,\\gamma}}\\\\ &{\\qquad\\qquad\\qquad+\\,(M-N G_{\\mu,\\gamma})^{*}W_{\\mu,\\gamma}^{1/2}\\widehat{W}_{\\mu,\\gamma}^{-1}W_{\\mu,\\gamma}^{1/2}(M-N G_{\\mu,\\gamma}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Next, observe that ", "page_idx": 29}, {"type": "text", "text": "\u2022 $\\|(\\widehat{C}-C)G_{\\mu,\\gamma}\\|\\leq\\|(\\widehat{C}-C)W_{\\mu,\\gamma}^{1/2}\\|\\|W_{\\mu,\\gamma}^{1/2}C\\|\\leq\\mu^{-1}\\|W_{\\mu,\\gamma}^{1/2}(\\widehat{C}-C)\\|$ is bounded by Proposi  \ntion 17   \n\u2022 $\\lVert M-N G_{\\mu,\\gamma}\\rVert_{-}^{}\\leq\\lVert W_{\\mu,\\gamma}^{-1/2}(\\widehat{C}-C)\\rVert+\\lVert W_{\\mu,\\gamma}^{-1/2}(\\widehat{W}_{\\mu}-W_{\\mu})W_{\\mu,\\gamma}^{-1}C\\rVert$ is bounded by Propositions 16 and 17,   \n$\\|G_{\\mu,\\gamma}^{*}(\\widehat{W}_{\\mu}-W_{\\mu})G_{\\mu,\\gamma}\\|\\leq\\mu^{-1}\\|W_{\\mu,\\gamma}^{-1/2}(\\widehat{W}_{\\mu}-W_{\\mu})W_{\\mu,\\gamma}^{-1}C\\|$ is bounded by Proposition 16. ", "page_idx": 29}, {"type": "text", "text": "Therefore, using additionally Proposition 15 result follows. ", "page_idx": 29}, {"type": "text", "text": "Remark that to bound singular values we can rely on the fact ", "page_idx": 29}, {"type": "equation", "text": "$$\n|\\sigma_{i}(\\widehat{B})-\\sigma_{i}(B)|=\\frac{|\\sigma_{i}^{2}(\\widehat{B})-\\sigma_{i}^{2}(B)|}{\\sigma_{i}(\\widehat{B})+\\sigma_{i}(B)}\\leq\\frac{|\\sigma_{i}^{2}(\\widehat{B})-\\sigma_{i}^{2}(B)|}{\\sigma_{i}(\\widehat{B})\\vee\\sigma_{i}(B)}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "E.3.4 Variance of RRR Estimator ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Recalling the notation $B\\;:=\\;W_{\\mu,\\gamma}^{-1/2}C$ and $\\widehat{B}\\ :=\\ \\widehat{W}_{\\mu,\\gamma}^{-1/2}\\widehat{C}$ , let denote $P_{r}$ and ${\\widehat{P}}_{r}$ denote the orthogonal projector onto the subspace of leading $r$ right singular vectors of $B$ and $\\widehat{B}$ , respectively. Then we have $[\\![B]\\!]_{r}=B P_{r}$ and $[\\hat{\\widehat{B}}]_{r}=\\widehat{B}\\widehat{P}_{r}$ , and, hence $G_{\\mu,\\gamma}^{r}=G_{\\mu,\\gamma}P_{r}$ and $\\widehat{G}_{\\mu,\\gamma}^{r}=\\widehat{G}_{\\mu,\\gamma}^{\\,^{-}}\\widehat{P}_{r}$ . ", "page_idx": 29}, {"type": "text", "text": "Proposition 20. Let (RC), (SD) and (KE) hold for some $\\alpha\\in(0,2]$ , $\\beta\\in(0,1]$ and $\\tau\\in[\\beta,1]$ . Given $\\delta>0$ and $\\gamma>0$ , if $\\varepsilon_{n}^{1}(\\gamma,\\delta)<1$ , then with probability at least $1-\\delta$ in the i.i.d. draw of $(x_{i})_{i=1}^{n}$ from $\\pi$ , ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\Vert Z_{\\mu}(G_{\\mu,\\gamma}^{r}-\\widehat{G}_{\\mu,\\gamma}^{r})\\Vert\\leq\\frac{\\varepsilon_{n}^{2}(\\gamma,\\delta/3)+\\varepsilon_{n}^{3}(\\gamma,\\delta/3)}{1-\\varepsilon_{n}^{1}(\\gamma,\\delta/3)}+\\frac{\\sigma_{1}(B)}{\\sigma_{r}^{2}(B)-\\sigma_{r+1}^{2}(B)}\\varepsilon_{n}^{4}(\\gamma,\\delta/3).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Proof. Start by observing that $\\|Z_{\\mu}(G_{\\mu,\\gamma}^{r}-\\widehat{G}_{\\mu,\\gamma}^{r})\\|\\leq\\|W_{\\mu,\\gamma}^{1/2}(G_{\\mu,\\gamma}^{r}-\\widehat{G}_{\\mu,\\gamma}^{r})\\|$ and ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{W_{\\mu,\\gamma}^{1/2}(G_{\\mu,\\gamma}^{r}-\\widehat{G}_{\\mu,\\gamma}^{r})=\\!(W_{\\mu,\\gamma}^{1/2}\\widehat{W}_{\\mu,\\gamma}^{-1}W_{\\mu,\\gamma}^{1/2})\\cdot}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\left(W_{\\mu,\\gamma}^{-1/2}(\\widehat{W}_{\\mu}-W_{\\mu})G_{\\mu,\\gamma}P_{r}+W_{\\mu,\\gamma}^{-1/2}(\\widehat{C}-C)\\widehat{P}_{r}+B(\\widehat{P}_{r}-P_{r})\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Using that the norm of orthogonal projector $\\widehat{P}$ is bounded by one and applying Propositions 15 and 16 together with Propositions 4 and 19 comp letes the proof. ", "page_idx": 29}, {"type": "text", "text": "E.4 Proof of Theorem 2 ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "First, observe that (DF) implies that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\epsilon\\widehat{W}_{\\mu}{\\preceq}\\widehat{W}_{\\mu}{-}\\widehat{W}_{\\mu}^{\\epsilon}{\\preceq}\\,\\epsilon\\,\\widehat{W}_{\\mu},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where the empirical covariance with the inexact Dirichlet coefficient $s_{\\epsilon}$ is denoted by $\\widehat{W}_{\\mu}^{\\epsilon}$ . Since RRR algorithm now uses $\\widehat{W}_{\\mu}^{\\epsilon}$ , when $\\epsilon>0$ the only change to our proof technique lies in the analysis of variance in section E. 3. In particular, we only need to adapt Propositions 15 and 16, which is straightforward. Indeed, we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|F(W_{\\mu}-\\widehat{W}_{\\mu}^{\\epsilon})G\\|=\\|F(W_{\\mu}\\pm\\widehat{W}_{\\mu}-\\widehat{W}_{\\mu}^{\\epsilon})F G\\|}\\\\ &{\\qquad\\qquad\\qquad\\leq\\|F(W_{\\mu}-\\widehat{W}_{\\mu})F G\\|+\\|F(\\widehat{W}_{\\mu}^{\\epsilon}-\\widehat{W}_{\\mu})F\\|\\|G\\|}\\\\ &{\\qquad\\qquad\\leq\\|F(W_{\\mu}-\\widehat{W}_{\\mu})F G\\|+\\epsilon\\|F\\widehat{W}_{\\mu}F\\|\\|G\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "for $F=W_{\\mu,\\gamma}^{-1/2}$ and $G$ being either $W_{\\mu,\\gamma}^{-1/2}C$ or $I$ . Thus, since $\\begin{array}{r}{\\|F\\widehat{W}_{\\mu}F\\|\\leq\\frac{1}{1-\\varepsilon^{1}(\\gamma,\\delta)}}\\end{array}$ and $\\|G\\|$ is either 1 or bounded by $\\sqrt{c\\varkappa/\\mu}$ , in conclusion the relative error $\\epsilon$ of imperfect knowledge simply appears as the additive term in the final guarantees. Thus, in what follows, we present that case $\\epsilon=0$ . ", "page_idx": 30}, {"type": "text", "text": "E.4.1 Operator Norm Error Bounds ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Summarising previous sections, in order to prove Theorem 2, we just need to analyse the bounds $\\varepsilon_{n}^{1}$ , $\\varepsilon_{n}^{2}$ and $\\varepsilon_{n}^{3}$ . Not that we fix the hyperparameter $\\mu>0$ , which affects the constants, but need to chose the decay rate of Tikhonov regularization parameter $\\gamma>0$ to obtain balancing of bias and variance in the generalization bounds. ", "page_idx": 30}, {"type": "text", "text": "Let us first assume regime $\\alpha\\geq\\tau$ , which covers \"well-specified learning\" when non-trivial eigenfunctions of $L$ are inside $\\mathcal{H}$ . Since $\\alpha\\geq\\tau$ and $\\beta\\leq\\tau$ , we have that for large enough $n$ one has ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\varepsilon_{n}^{1}(\\gamma,\\delta)\\lesssim\\frac{n^{-1/2}}{\\gamma^{\\tau/2}}\\ln\\delta^{-1}\\quad\\mathrm{~and~}\\quad\\varepsilon_{n}^{i}(\\gamma,\\delta)\\lesssim\\left(\\frac{n^{-1/2}}{\\gamma^{\\beta/2}}\\vee\\frac{n^{-1}}{\\gamma^{\\tau/2}}\\right)\\ln\\delta^{-1},\\ i=2,3,4.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "But, since the bias term is $\\lesssim\\gamma^{\\alpha/2}$ and the slow term from Proposition 20 is $1/\\sqrt{n\\gamma^{\\beta}}$ , we can set $\\gamma_{n}=n^{-\\frac{1}{\\alpha+\\beta}}$ and obtain ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\gamma_{n}^{\\alpha/2}=\\frac{n^{-1/2}}{\\gamma_{n}^{\\beta/2}}=n^{-\\frac{\\alpha}{2(\\alpha+\\beta)}},\\;\\;\\;\\;\\;\\mathrm{and}\\;\\;\\;\\;\\;\\frac{n^{-1/2}}{\\gamma_{n}^{\\tau/2}}=n^{-\\frac{\\alpha+\\beta-\\tau}{2(\\alpha+\\beta)}},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "which, due to $\\alpha\\geq\\tau$ , implies ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\to\\infty}\\varepsilon_{n}^{1}(\\gamma_{n},\\delta/3)=\\operatorname*{lim}_{n\\to\\infty}\\varepsilon_{n}^{2}(\\gamma_{n},\\delta/3)=\\operatorname*{lim}_{n\\to\\infty}\\varepsilon_{n}^{3}(\\gamma,\\delta/3)=\\operatorname*{lim}_{n\\to\\infty}\\varepsilon_{n}^{4}(\\gamma,\\delta/3)=0.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Therefore, for this choice of regularization parameter Equation (46) with Propositions 9, 11 and 20 assure that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathcal{E}(\\widehat{G}_{\\mu,\\gamma}^{r})-\\sigma_{r+1}(B)\\leq\\mathcal{E}(\\widehat{G}_{\\mu,\\gamma}^{r})-\\sqrt{\\lambda_{r+1}^{\\star}}\\lesssim n^{-\\frac{\\alpha}{2(\\alpha+\\beta)}}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Now, let us consider the more difficult to learn case $\\alpha<\\tau$ when eignefunctions of the generator have only weaker norms than the RKHS one. Then, for large enough $n$ bounds in (64) hold for $i=3$ , but ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\varepsilon_{n}^{2}(\\gamma,\\delta)\\vee\\varepsilon_{n}^{4}(\\gamma,\\delta)\\lesssim\\left(\\frac{n^{-1/2}}{\\gamma^{(\\beta+\\tau-\\alpha)/2}}\\vee\\frac{n^{-1}}{\\gamma^{(2\\tau-\\alpha)/2}}\\right)\\ln\\delta^{-1}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Then, by balancing the slow terms with bias $\\gamma^{\\alpha/2}$ , we set $\\gamma_{n}=n^{-\\frac{1}{\\tau+\\beta}}$ and obtain ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\gamma_{n}^{\\alpha/2}=\\frac{n^{-1/2}}{\\gamma_{n}^{(\\tau+\\beta-\\alpha)/2}}=n^{-\\frac{\\alpha}{2(\\tau+\\beta)}},\\quad\\mathrm{~and~}\\quad\\frac{n^{-1/2}}{\\gamma_{n}^{\\tau/2}}=n^{-\\frac{\\beta}{2(\\tau+\\beta)}},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "which, since $\\tau\\geq\\beta$ , implies ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\to\\infty}\\varepsilon_{n}^{1}(\\gamma_{n},\\delta/3)=\\operatorname*{lim}_{n\\to\\infty}\\varepsilon_{n}^{2}(\\gamma_{n},\\delta/3)=\\operatorname*{lim}_{n\\to\\infty}\\varepsilon_{n}^{3}(\\gamma,\\delta/3)=\\operatorname*{lim}_{n\\to\\infty}\\varepsilon_{n}^{4}(\\gamma,\\delta/3)=0,\n$$", "text_format": "latex", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathcal{E}(\\widehat{G}_{\\mu,\\gamma}^{r})-\\sigma_{r+1}(B)\\leq\\mathcal{E}(\\widehat{G}_{\\mu,\\gamma}^{r})-\\lambda_{r+1}^{\\star}\\lesssim n^{-\\frac{\\alpha}{2(\\tau+\\beta)}}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Therefore, denoting ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\gamma_{n}\\asymp\\left\\{\\b{n}^{-\\frac{1}{\\tau+\\beta}}\\b{}}&{,\\alpha\\leq\\tau,}\\\\ {\\b{n}^{-\\frac{1}{\\alpha+\\beta}}}&{,\\alpha\\geq\\tau,}\\end{array}\\right.\\quad\\mathrm{and}\\quad\\varepsilon_{n}^{\\star}=\\left\\{\\b{n}^{-\\frac{\\alpha}{2(\\tau+\\beta)}}\\b{}}&{,\\alpha\\leq\\tau,}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "as a consequence the operator norm error bound in Theorem 2 holds, and we have the following result on the estimation of singular values of $(\\mu I{-}L)^{-1}Z_{\\mu}$ , that is $\\lambda_{i}^{\\star}\\mathbf{s}$ by singular values of $\\widehat{B}$ , that is $\\widehat{\\sigma}_{i}\\mathbf{s}$ from Theorem 1. ", "page_idx": 31}, {"type": "text", "text": "Proposition 21. Let (RC), (SD) and (KE) hold for some $\\alpha\\in(0,2]$ and $\\beta\\in(0,1]$ and $\\tau\\in[\\beta,1]$ , and define (68). Then, there exists a constant $c>0$ such that for every given $\\delta\\in(0,1)$ , large enough $n>r$ and with probability at least $1-\\delta$ in the i.i.d. draw of $(x_{i})_{i=1}^{n}$ from $\\pi$ for all $i\\in[r]$ ", "page_idx": 31}, {"type": "equation", "text": "$$\n|\\widehat{\\sigma}_{i}^{2}-\\lambda_{i}^{\\star}|\\lesssim\\varepsilon_{n}^{\\star}\\ln\\delta^{-1}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Proof. The proof is direct consequence of Propositions 10 and 19 using (64)-(66). ", "page_idx": 31}, {"type": "text", "text": "E.5 Spectral Learning Rates ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Finally, we conclude the proof of Theorem 2 showing the concentration of eigenpairs. Recalling Proposition 2, we need to combine the operator norm bound and metric distortion. Since, as indicated in Proposition 9, the population KRR estimator can grow in the operator norm whenever the regularity condition is violated $\\alpha<1$ , leading to possibly unbounded metric distortions w.r.t. increasing sample size, we restrict to the case $\\alpha\\geq1$ . ", "page_idx": 31}, {"type": "text", "text": "First, combining (65)-(67) and Proposition 21, we have that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathcal{E}(\\widehat{G}_{\\mu,\\gamma}^{r})\\leq(\\widehat{\\sigma}_{r+1}\\wedge\\lambda_{r+1}^{\\star})+c\\,\\varepsilon_{n}^{\\star}\\ln\\delta^{-1}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "On the other hand, from Propositions 40 and 14, we have that with failure probability $\\delta$ ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(\\widehat{\\eta}_{i})^{2}-(\\eta(\\widehat{h}_{i}))^{-2}\\leq\\varepsilon_{n}(\\delta),}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "which, if we can prove that $\\widehat{\\eta_{i}}$ and $\\eta(\\widehat{h}_{i})$ are bounded, concludes the proof for empirical spectral biases. To that end, recall th at , c.f. Proposition 5, for RRR estimator we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\eta(\\widehat{h}_{i})\\leq\\frac{\\|\\widehat{G}_{\\mu,\\gamma}^{r}\\|}{\\sigma_{r}(Z_{\\mu}\\widehat{G}_{\\mu,\\gamma}^{r})}\\leq\\frac{\\|\\widehat{G}_{\\mu,\\gamma}\\|}{\\sigma_{r}((\\mu I-L)^{-1}Z_{\\mu})-\\mathcal{E}(\\widehat{G}_{\\mu,\\gamma}^{r})}\\leq\\frac{\\|G_{\\mu,\\gamma}\\|+\\|\\widehat{G}_{\\mu,\\gamma}-G_{\\mu,\\gamma}\\|}{\\sqrt{\\lambda_{r}^{\\star}}-\\mathcal{E}(\\widehat{G}_{\\mu,\\gamma}^{r})}}\\\\ &{\\qquad\\leq\\frac{\\|G_{\\mu,\\gamma}\\|+(\\mu\\gamma)^{-1/2}\\|W_{\\mu,\\gamma}^{1/2}(\\widehat{G}_{\\mu,\\gamma}-G_{\\mu,\\gamma})\\|}{\\sqrt{\\lambda_{r}^{\\star}}-\\mathcal{E}(\\widehat{G}_{\\mu,\\gamma}^{r})}\\lesssim\\frac{1+n^{-1/2}\\gamma^{-(\\beta+1)/2}}{\\sqrt{\\lambda_{r}^{\\star}}-n^{-1/2}\\gamma^{-\\beta/2}}=\\frac{1+n^{-\\frac{\\alpha-1}{\\alpha+\\beta}}}{\\sqrt{\\lambda_{r}^{\\star}}-n^{-\\frac{\\alpha}{\\alpha+\\beta}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where in the last inequality we have applied Propositions 9 and 18. Thus, using Proposition 40, the proof of Theorem 2 is concluded. ", "page_idx": 31}, {"type": "text", "text": "E.6 Discussion of the learning rates ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "We first discuss the learning rates reported in Table 1. Notice that although the papers we compare to employ a different risk, our comparison remains meaningful because our energy-based risk measure provides an upper bound on their risk measure. This ensures that any upper bound derived with our risk also applies to theirs. ", "page_idx": 31}, {"type": "text", "text": "\u2022 The learning bound for IG obtained is [1] covers only pure diffusion processes (Laplacian with constant weights). Their learning rate is non-parametric and depends on the state space dimension $d$ in a counter-intuitive way $O(n^{-\\frac{d}{2(d+1)}})$ in [1, Theorem 3], highlighting a potential limitation of their approach. In comparison, when we specify our RRR method with an RBF kernel (i.e. $\\beta=0$ ), we achieve a much faster parametric learning rate $O(n^{-1/2})$ . ", "page_idx": 31}, {"type": "text", "text": "\u2022 The recent work of [43] covers Langevin processes via a kernel approach, but they derive a sub-optimal learning bound for IG of order $O(n^{-1/4})$ in [43, Theorem 4.4]. For Langevin diffusions, our RRR method with an RBF kernel achieves a faster parametric rate $O(n^{-1/2})$ . Moreover, the computational complexity of their method is $\\bar{O}(n^{3}d^{3})$ , which limits its application in realistic molecular dynamics scenarios.   \n\u2022 As for [19, Theorem 7], although they considered general diffusions, they only derived a suboptimal bound for the variance component of their risk, with an explicit dependence on the dimension of the state space. ", "page_idx": 32}, {"type": "text", "text": "Additionally, note that the mentioned works lack learning guarantees for eigenfunctions and eigenvalues. Notably, their methods are prone to the spurious eigenvalue phenomenon, requiring expert manual review of each eigenpair to select plausible ones. ", "page_idx": 32}, {"type": "text", "text": "Next, we contrast our IG learning and well established TO learning. First, note that TO methods apply only to equally spaced data and the sampling frequency $1/\\Delta t$ must be high enough to distinguish all relevant time-scales. Otherwise, since TO eigenvalues are $e^{\\lambda_{i}\\Delta t}$ , small spectral gaps complicate learning (see [28, Thm. 3]). Conversely, our IG method, which uses gradient information, is timescale independent, handles irregularly spaced measurements, and does not rely on time discretizations. Indeed, recalling the risk functional in eq. (8), we see that the \u201clabel\u201d of the model $\\chi_{\\mu}(x)\\approx G^{*}\\phi(x)$ is the action of the resolvent. Since this \u201clabel\u201d is not computable, we \u201cfight fire (resolvent) with fire (generator)\u201d, i.e. we use the energy norm of eq. (9) to rewrite regularized problem (8). Crucially, this allows us to obtain estimators via energy covariance $W_{\\mu}$ in (14) that completely captures infinitesimal nature of the learning problem without needing time-lagged observations. This contrasts with TO methods, where choosing the time-lag $\\Delta t$ is the major bottleneck in real applications. Indeed, to obtain data from an invariant distribution $\\pi$ one uses the trajectory data after some burn-in time needed to ensure that ergodic mean approximates well $\\pi$ . Then, the problem is reduced to studying only the dependence as is done e.g. in [27] using standard tools of $\\beta$ -mixing and the method of blocks. This allows one to obtain non-parametric learning bounds for TO methods where the effective sample size suffers from the multiplicative effect of the time-lag to achieve approximate independence. So, TO methods \u201cwaste\u201d a lot of data negatively impacting statistical accuracy. Contrary to this, our method can be applied to data with larger time-lags (even irregularly spaced) so that effective sample size is close to the true one. All this results in better generalization, as shown in Fig. 1 g)-h)-i) where our IG estimator captures ground truth significantly better than TO for the same sample size, this generalization across all time scales incurs quadratic computational complexity w.r.t. state dimension and not statistical accuracy. Lastly, with our additional discussion on imperfect knowledge, our IG method can also be applied in a fully data-driven regime as TO methods. ", "page_idx": 32}, {"type": "text", "text": "To conclude this section, we contrast our method with classical numerical methods for the spectral decomposition of differential operators, such as finite-elements (FEM). For FEM, the approximation error is $|\\lambda_{k}-\\widehat{\\lambda}_{k}|\\,\\le\\,c h^{2p}\\lambda_{k}$ , where $p$ is the polynomial degree used to construct finite elements, $h$ is the mesh size, and $c$ depends on the eigenfunctions\u2019 smoothness. As the number of mesh elements grows exponentially with $d$ (i.e., $\\sim\\bar{h}^{-d}.$ ), reducing $h$ is the major bottleneck mitigated by computationally demanding adaptive higher order methods. On the other hand, our IG method that requires less or no knowledge has a quadratic impact of the $d$ only on the computational complexity. Indeed, sample complexity depends on the effective dimension of the equilibrium distribution on the domain that can be much lower than $d$ . Therefore, RRR for IG\u2019s resolvent doesn\u2019t suffer from the curse of dimensionality as the FEM does. ", "page_idx": 32}, {"type": "text", "text": "F Experiments ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "All the experiments were performed on a workstation with $125.6\\ \\mathrm{GiB}$ of memory and $\\mathrm{AMD}\\textcircled{\\mathrm{R}}$ Ryzen threadripper pro 3975wx 32-cores $\\times\\ 64$ processor, no graphics card was used. The version of python used is Python 3.9.18. The choice of the kernel in all experiments was Gaussian RBF with specified length-scales, and the hyperparameters were chosen via cross-validation. The code to reproduce the results of the experiments can be found in the following repository: https://github.com/DevergneTimothee/GenLearn_kernel ", "page_idx": 32}, {"type": "text", "text": "Four well potential For this experiment, we used an in-house code to simulate the system. The equations of motions were discretized using the Euler-Maruyama scheme with a timestep of $10^{-4}$ . ", "page_idx": 32}, {"type": "image", "img_path": "H7SaaqfCUi/tmp/78981d430c8401f48c136880366f6b7ac5c83607d69d5ee65053a7bb07dd894d.jpg", "img_caption": ["Figure 2: Results of the RRR given by our method for two different length scales (blue and red) compared with ground truth (black) for the Langevin dynamics driven by a four well one dimensional potential. "], "img_footnote": [], "page_idx": 33}, {"type": "image", "img_path": "H7SaaqfCUi/tmp/690eeebbf723bb8ae7609f5a772b2485b41aeb4f45f84977c7674a689239d4d9.jpg", "img_caption": ["Figure 3: Panels a)-c): Test of the model\u2019s robustness with respect to the hyperparameter $\\mu$ , tested for 30 different values between $10^{-3}$ and 5, compared to the ground truth result. Panel d): logarithm of the empirical bias as a function of the kernel length scale $\\sigma$ and the logarithm of regularization parameter $\\gamma$ . "], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "RRR was fitted using 1000 points, $\\mu\\,=\\,5$ and $\\gamma=10^{-5}$ . The length scales used were 0.05 and 0.5. This experiment was reproduced 100 times leading to very small change in the estimation of the eigenfunctions. In Figure 2 we report the result of one of them. The reduced rank regression was performed with a rank of 5. Further, in Figure 3 we show the robustness of eigenfunctions w.r.t. choice of shift hyper parameter $\\mu$ , as well as values of empirical bias for different values of length-scale and regularization hyperparameters. Concerning panel c) of Figure 1 where we show the consistency of our model with the true Boltzmann distribution. Namely, we use our model to forecast the conditional probability density function (pdf) of the system being in one bin, given it started at some point and after 100000 steps (so that it relaxes towards the equilibrium distribution). We bin the space into 50 bins and approximate the pdf via eq. (12), where $h$ is characteristic function of a bin. In order to achieve an accurate estimation, we use the knowledge that the leading eigenvalue is zero. Additionally, we perform the same procedure with imperfect diffusion coefficient estimated from data $\\mathit{\\Omega}_{k_{b}T}=0.45\\pm0.01$ instead of true value $0.5a.u.)$ ) by looking at the variance of increments over 10 steps. Note that the result (green dotted lines) is unchanged and compares well with the analytical Boltzmann distribution (black lines). Finally, we report that if the same approach is used with the method described in [19, 43], no dynamical quantity can be forecasted due to many spurious eigenpairs (see Figure 1a) which prevents the system from relaxing towards the Boltzmann distribution. ", "page_idx": 33}, {"type": "text", "text": "Muller Brown For this experiment, we used an in-house code to simulate the system. The equations of motions were discretized using the Euler-Maruyama scheme with a timestep of $10^{-3}$ and a temperature of 2 (arbitrary units). RRR was fitted using 2000 points, $\\mu=1$ and $\\gamma=10^{-5}$ . The length scale used was 0.6. The reduced rank regression was performed with a rank of 5 ", "page_idx": 33}, {"type": "text", "text": "CIR model For this experiment, we reproduced 100 times the simulations in order to obtain statistical uncertainties. We used a length scale of 0.5, $\\mu=1$ and $\\gamma=10^{-6}$ . The reduced rank regression was performed with a rank of 2. ", "page_idx": 33}, {"type": "image", "img_path": "H7SaaqfCUi/tmp/a1880911e1fd9bdc5f64e81aafe3ce4f392e9f010788b5b5e2b1e608d34e328b.jpg", "img_caption": ["Figure 4: Results of the RRR given by our method (second column) compared to ground truth (first column) and transfer operator RRR (last column). Points are colored according to the value of the eigenfunction "], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "US mortgage rates We have trained our method on a real 30-year US mortgage rates dataset and contrasted it with the ftited CIR model using continuous ranked probability scores that are estimated from the forecasts obtained by of each of them. Each model has been trained using data from January 2009 to December 2016. The initial condition was the last week of December 2016 and the predictions were made for the years 2017 and 2018. Since the dataset is real, we used the imperfect partial knowledge, that is, for our method, we estimated the diffusion coefficient only via a least squares calibration of a CIR model over the training set. This allows more flexibility on the drift term in our model. ", "page_idx": 34}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: we also provided a summary table comparing our results to key related works. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 35}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] , ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: see the conclusion section. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 35}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: assumptions are stated in Section 5 and proofs presented in the appendix. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 36}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: the experimental setting is reported in Section 6 and expanded in the appendix. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 36}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: See Appendix F. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 37}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: please see Appendix F. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 37}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] . ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Justification: whenever appropriate we have reported these. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 37}, {"type": "text", "text": "", "page_idx": 38}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: please see Appendix F. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 38}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: We enjoy research and we respect other people work. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 38}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: as for other theoretical papers no particular concerns are present. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to ", "page_idx": 38}, {"type": "text", "text": "generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 39}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 39}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: please see the previous answer. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 39}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: we use synthetic data generated by ourselves. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 39}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: see above. Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 40}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA]   \nJustification: not applicable. Guidelines:   \n\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 40}, {"type": "text", "text": "", "page_idx": 40}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] Justification: not applicable. ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 40}]