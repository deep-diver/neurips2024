{"references": [{"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper is foundational for the field, demonstrating the potential of LLMs as few-shot learners, a capability central to the current paper's evaluation methods."}, {"fullname_first_author": "Dan Hendrycks", "paper_title": "Measuring massive multitask language understanding", "publication_date": "2021-01-01", "reason": "This paper introduces a comprehensive benchmark for evaluating large language models across diverse tasks, addressing limitations of static benchmarks that are relevant to the dynamic evaluation proposed in the current work."}, {"fullname_first_author": "Karl Cobbe", "paper_title": "Training verifiers to solve math word problems", "publication_date": "2021-10-26", "reason": "This paper focuses on mathematical reasoning, a key domain in this paper's evaluation, and it introduces a novel approach for training LLMs which influenced the current paper's techniques."}, {"fullname_first_author": "Jason Wei", "paper_title": "Chain-of-thought prompting elicits reasoning in large language models", "publication_date": "2022-12-01", "reason": "This paper introduces chain-of-thought prompting, which is a technique used in the current paper to improve LLMs' reasoning abilities, making it an important methodological contribution to the field."}, {"fullname_first_author": "Kaijie Zhu", "paper_title": "DyVal: Graph-informed dynamic evaluation of large language models", "publication_date": "2023-09-26", "reason": "This paper is very closely related to the current paper, focusing on dynamic evaluation of LLMs and proposing a graph-based approach, and is directly compared against."}]}