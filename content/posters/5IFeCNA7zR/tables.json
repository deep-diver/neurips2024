[{"figure_path": "5IFeCNA7zR/tables/tables_2_1.jpg", "caption": "Table 1: Overview of the tasks and reasoning domains investigated, along with their corresponding graph components, complexity definitions, and illustrative examples.", "description": "This table provides a comprehensive overview of the four reasoning tasks (Math, Social, Spatial, and Symbolic Reasoning) used in the DARG evaluation framework.  For each task, it lists the specific dataset used, defines the structure of the reasoning graph (node and edge definitions), explains how complexity is measured for that task, and references a figure in the paper that illustrates an example of the reasoning graph for that task.  The table serves to clarify the different types of reasoning tasks and how their complexity was manipulated within the DARG framework.", "section": "1 Introduction"}, {"figure_path": "5IFeCNA7zR/tables/tables_17_1.jpg", "caption": "Table 2: Accuracy of 15 LLMs using CoT prompting on GSM8K when applying DARG on 3 complexity dimensions. Full results can be found in Figure 3, 4 and 5.", "description": "This table presents the overall accuracy of 15 Large Language Models (LLMs) on the GSM8K benchmark when using Chain-of-Thought (CoT) prompting.  The accuracy is measured across three different complexity dimensions manipulated by the DARG framework: numerical complexity, graph depth, and graph width.  Each complexity dimension has four levels, starting from the original complexity of the dataset.  The table shows how the accuracy of each LLM changes across these complexity levels for each of the dimensions.  For full results including the detailed breakdown of accuracy changes across each complexity level, it refers the reader to Figures 3, 4 and 5.", "section": "3.1 Mathematical Reasoning: GSM8K"}, {"figure_path": "5IFeCNA7zR/tables/tables_19_1.jpg", "caption": "Table 2: Accuracy of 15 LLMs using CoT prompting on GSM8K when applying DARG on 3 complexity dimensions. Full results can be found in Figure 3, 4 and 5.", "description": "This table presents the overall accuracy scores achieved by 15 different Large Language Models (LLMs) on the GSM8K benchmark when evaluated using Chain-of-Thought (CoT) prompting.  The models' performance is assessed under three different complexity dimensions manipulated by the DARG framework: numerical complexity, graph depth, and graph width. Each dimension is tested at four levels, including the original complexity and three increased complexity levels (+2, +4, +8 for numerical complexity, and +1, +2, +3, +4 for graph width and depth).  The full results for each of the three complexity dimensions are shown in Figures 3, 4, and 5 of the paper.", "section": "3.1 Mathematical Reasoning: GSM8K"}, {"figure_path": "5IFeCNA7zR/tables/tables_20_1.jpg", "caption": "Table 2: Accuracy of 15 LLMs using CoT prompting on GSM8K when applying DARG on 3 complexity dimensions. Full results can be found in Figure 3, 4 and 5.", "description": "This table shows the accuracy of 15 different Large Language Models (LLMs) on the GSM8K benchmark when using Chain-of-Thought (CoT) prompting.  The accuracy is measured across three different complexity dimensions manipulated by the DARG framework: numerical complexity, graph depth, and graph width. Each dimension has four levels of increasing complexity. The full results for each LLM and complexity level can be found in Figures 3, 4, and 5 of the paper.", "section": "3.1 Mathematical Reasoning: GSM8K"}, {"figure_path": "5IFeCNA7zR/tables/tables_21_1.jpg", "caption": "Table 2: Accuracy of 15 LLMs using CoT prompting on GSM8K when applying DARG on 3 complexity dimensions. Full results can be found in Figure 3, 4 and 5.", "description": "This table shows the accuracy of 15 different large language models (LLMs) on the GSM8K benchmark when evaluated using Chain-of-Thought (CoT) prompting and the DARG method.  DARG introduces controlled complexity variations in three dimensions: numerical complexity, graph depth, and graph width. The table presents the accuracy of each LLM on the original GSM8K dataset and at different complexity levels for each dimension.  Full results (including CIARR and error breakdowns) are shown in Figures 3, 4, and 5 of the paper.", "section": "3.1 Mathematical Reasoning: GSM8K"}, {"figure_path": "5IFeCNA7zR/tables/tables_23_1.jpg", "caption": "Table 2: Accuracy of 15 LLMs using CoT prompting on GSM8K when applying DARG on 3 complexity dimensions. Full results can be found in Figure 3, 4 and 5.", "description": "This table shows the accuracy of 15 different large language models (LLMs) on the GSM8K dataset when using Chain-of-Thought (CoT) prompting.  The models are evaluated under three different complexity dimensions modified by the DARG framework. The table presents the accuracy for the original GSM8K dataset and three levels of increased complexity for each dimension.  More detailed results for each LLM and complexity level can be found in Figures 3, 4, and 5 of the paper.", "section": "3.1 Mathematical Reasoning: GSM8K"}, {"figure_path": "5IFeCNA7zR/tables/tables_23_2.jpg", "caption": "Table 6: Success rates for reasoning graph extraction using different models.", "description": "This table presents the success rates achieved by four different language models in the task of extracting reasoning graphs from a dataset.  The models are GPT-4-Turbo, LLaMA 3.1-8B, LLaMA 3.1-70B, and LLaMA 3.1-405B.  The success rate represents the proportion of times the model successfully extracted a reasoning graph.  The table shows that GPT-4-Turbo has a high success rate (0.91), while the smaller LLaMA model (LLaMA 3.1-8B) has a success rate of 0. The larger LLaMA models have moderate success rates (0.83 and 0.85).", "section": "Additional Experiments to Apply DARG with Open-Source LLMs"}, {"figure_path": "5IFeCNA7zR/tables/tables_26_1.jpg", "caption": "Table 7: Success rates for graph-to-text decoding using different models, under single-run and iterative refinement conditions.", "description": "This table presents the success rates of graph-to-text decoding for four different language models.  The models are evaluated under two conditions: a single run, and a maximum of 5 iterative refinement steps. The table shows that the success rate increases significantly with the iterative refinement. GPT-4-Turbo shows the best results, while LLaMA 3.1-8B struggles to produce any successful decodings.", "section": "4 Related Work"}]