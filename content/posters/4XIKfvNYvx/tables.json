[{"figure_path": "4XIKfvNYvx/tables/tables_4_1.jpg", "caption": "Table 1: GSM8K results comparing Iterative Reasoning Preference Optimization (Iterative RPO) against other baselines that are based on the same base model and training data. We report the exact match accuracy from a single generation (using greedy decoding), as well as majority voting over 32 generations (through sampling with temperature 0.8).", "description": "This table presents the results of the GSM8K experiment, comparing the performance of Iterative RPO against several baselines.  The baselines include zero-shot and majority voting using 32 samples, standard DPO with different initializations, SFT on gold and chosen sequences, and STaR with different amounts of training data.  The table shows that Iterative RPO significantly outperforms all baselines in terms of exact match accuracy, with further gains achieved through majority voting.", "section": "3.1 Math Word Problems: GSM8K"}, {"figure_path": "4XIKfvNYvx/tables/tables_7_1.jpg", "caption": "Table 2: ARC and MATH results. We compare Iterative Reasoning Preference Optimization (Iterative RPO) against other baselines that are based on the same base model and training data.", "description": "This table presents the results of the Iterative Reasoning Preference Optimization (Iterative RPO) method and several baseline methods on two reasoning tasks: ARC-Challenge and MATH.  The Iterative RPO results are shown for each iteration of the training process, and for the majority voting of 32 samples.  Baseline methods include zero-shot Chain-of-Thought (CoT), Supervised Fine-Tuning (SFT), and Direct Preference Optimization (DPO), both initialized from the base Llama-2-70b-chat model and SFT initialized model. The table shows the test accuracy (%) achieved by each method on both datasets.", "section": "3.2 ARC-Challenge Task and 3.3 MATH Task"}]