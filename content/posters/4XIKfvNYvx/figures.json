[{"figure_path": "4XIKfvNYvx/figures/figures_1_1.jpg", "caption": "Figure 1: Iterative Reasoning Preference Optimization. Our iterative preference optimization method consists of two steps: (i) Chain-of-Thought & Answer Generation: training prompts are used to generate candidate reasoning steps and answers from model Mt, and then the answers are evaluated for correctness by a given reward model. (ii) Preference Optimization: preference pairs are selected from the generated data, which are used for training via a DPO+NLL objective, resulting in model Mt+1. This whole procedure is then iterated resulting in improved reasoning ability on the next iteration, until performance saturates.", "description": "This figure illustrates the iterative reasoning preference optimization method. It involves two main steps: 1) Chain-of-Thought & Answer Generation, where the model generates reasoning steps and answers for given prompts, and these answers are evaluated; and 2) Preference Optimization, where preference pairs are created and used to train the model via DPO+NLL, improving reasoning ability with each iteration.", "section": "2 Iterative Reasoning Preference Optimization"}, {"figure_path": "4XIKfvNYvx/figures/figures_4_1.jpg", "caption": "Figure 2: Effect of SFT training. (a) Although SFT training (solid green) is on chosen sequences (Dpairs, from iterative RPO iteration 1) only, the rejected sequence log probabilities (dotted green) also increase and are close to the chosen sequence probabilities. In contrast, our DPO+NLL training (blue) manages to decrease the rejected probabilities while increasing the chosen probabilities. This observation could potentially help explain why SFT-only performance lags significantly behind Iterative RPO Iteration 1 performance. (b) We show a similar plot but where SFT is trained on gold (dataset-provided) CoTs. Chosen and rejected sequence probabilities (which are from Dpairs) are still close to each other, but with a slightly bigger gap. Another observation is that the chosen sequence probabilities barely increase.", "description": "This figure compares the effect of Supervised Fine-Tuning (SFT) and the proposed DPO+NLL training methods on the log probabilities of chosen and rejected sequences in the Iterative Reasoning Preference Optimization process.  It shows that while SFT increases the probabilities of both chosen and rejected sequences, DPO+NLL training is more effective in improving the chosen sequences while decreasing the probability of rejected sequences, which contributes to better performance.", "section": "3.1 Math Word Problems: GSM8K"}, {"figure_path": "4XIKfvNYvx/figures/figures_4_2.jpg", "caption": "Figure 2: Effect of SFT training. (a) Although SFT training (solid green) is on chosen sequences (Dpairs, from iterative RPO iteration 1) only, the rejected sequence log probabilities (dotted green) also increase and are close to the chosen sequence probabilities. In contrast, our DPO+NLL training (blue) manages to decrease the rejected probabilities while increasing the chosen probabilities. This observation could potentially help explain why SFT-only performance lags significantly behind Iterative RPO Iteration 1 performance. (b) We show a similar plot but where SFT is trained on gold (dataset-provided) CoTs. Chosen and rejected sequence probabilities (which are from Dpairs) are still close to each other, but with a slightly bigger gap. Another observation is that the chosen sequence probabilities barely increase.", "description": "This figure shows the effect of supervised fine-tuning (SFT) training on the performance of the model in terms of log probabilities of chosen and rejected sequences.  It compares SFT training with the proposed DPO+NLL training method. The plots indicate that SFT training alone does not effectively distinguish between chosen and rejected sequences, while the DPO+NLL method achieves a better separation, leading to improved performance.", "section": "3.1 Math Word Problems: GSM8K"}, {"figure_path": "4XIKfvNYvx/figures/figures_5_1.jpg", "caption": "Figure 3: Effect of NLL loss term on DPO training for GSM8K. In our GSM8K experiments we observe the log probability of chosen sequences in standard DPO without NLL loss (solid orange) decreases over training steps, especially if the model is initialized from SFT training on chosen sequences (right). However, they increase over training steps when using DPO with NLL loss (solid blue). In all four settings, the margin between the two curves continues increasing. We find that DPO+NLL loss gives superior test accuracy in our experiments.", "description": "This figure shows the effect of adding a negative log-likelihood (NLL) term to the DPO loss function during training on the GSM8K dataset.  It compares the log probabilities of 'chosen' (correct) and 'rejected' (incorrect) sequences across training steps for both standard DPO and DPO+NLL. The results indicate that including the NLL term leads to a consistent increase in the log probabilities of chosen sequences while decreasing those of rejected sequences, resulting in improved test accuracy.", "section": "3.1 Math Word Problems: GSM8K"}, {"figure_path": "4XIKfvNYvx/figures/figures_5_2.jpg", "caption": "Figure 3: Effect of NLL loss term on DPO training for GSM8K. In our GSM8K experiments we observe the log probability of chosen sequences in standard DPO without NLL loss (solid orange) decreases over training steps, especially if the model is initialized from SFT training on chosen sequences (right). However, they increase over training steps when using DPO with NLL loss (solid blue). In all four settings, the margin between the two curves continues increasing. We find that DPO+NLL loss gives superior test accuracy in our experiments.", "description": "This figure compares the training performance of DPO with and without the NLL loss term on the GSM8K dataset. The plot shows the log probabilities of chosen and rejected sequences over training steps.  It demonstrates that adding the NLL loss improves the performance significantly as the log probabilities of chosen sequences increase, while the log probabilities of rejected sequences decrease.", "section": "3.1 Math Word Problems: GSM8K"}, {"figure_path": "4XIKfvNYvx/figures/figures_7_1.jpg", "caption": "Figure 4: Effect of NLL loss term on DPO training for ARC and MATH. The legend on the right plot is omitted due to space constraint, but it is the same as the legend in the left plot. Similar to GSM8K, in ARC-Challenge and MATH, we see that the log probabilities of chosen sequences barely increase over training steps when training with DPO. However, when training with DPO with NLL loss, the log probabilities increase over training steps.", "description": "This figure shows the effect of adding the negative log-likelihood (NLL) term to the DPO loss function during training on two datasets: ARC-Challenge and MATH. The plots display the log probabilities of chosen and rejected sequences over training steps for both DPO with and without the NLL term. The results indicate that including the NLL term leads to an increase in the log probabilities of chosen sequences during training for both datasets, suggesting that the NLL term is beneficial for improving the model's performance on reasoning tasks.", "section": "3.2 ARC-Challenge Task and 3.3 MATH Task"}]