[{"figure_path": "vwgWbCxeAQ/figures/figures_1_1.jpg", "caption": "Figure 1: (a) A motivating example of task misalignment, illustrating the cosine similarities between an image and various text descriptions in the embedding space of CLIP. (b) A motivating experiment on data misalignment, showing the accuracy trends for base and new classes across different training epochs on the DTD dataset.", "description": "Figure 1(a) shows the cosine similarity between an image and different text descriptions in the embedding space of CLIP. It illustrates the task misalignment issue in CLIP.  Figure 1(b) displays the accuracy trends of base and new classes during training epochs on the DTD dataset and shows the data misalignment in CLIP. The figure indicates that while the accuracy for base classes increases with more epochs, the accuracy for new classes decreases, illustrating the overfitting problem due to data misalignment.", "section": "3 Problem Formulation and Analysis"}, {"figure_path": "vwgWbCxeAQ/figures/figures_3_1.jpg", "caption": "Figure 2: SCMs. Solid and dashed circles indicate the observable and unobservable variables, respectively.", "description": "This figure shows three Structural Causal Models (SCMs) illustrating the causal relationships among variables during the pre-training and adaptation processes of CLIP.  (a) shows the full model with observed variables X (image space), Y (true labels), Gr (task-relevant generative factors), and Gi (task-irrelevant generative factors), and unobserved variable D (pre-training data). (b) simplifies the model by focusing on the observed variables and highlighting the confounding effect of task-irrelevant generative factors Gi on the relationship between X and Y (predicted labels). (c) illustrates the proposed solution using the front-door adjustment. Here, S (task-relevant semantics) acts as an intermediate variable between X and Y, mitigating the influence of Gi. ", "section": "3.2 Problem Analysis: Causal Perspective"}, {"figure_path": "vwgWbCxeAQ/figures/figures_4_1.jpg", "caption": "Figure 3: Framework of CDC.  t<sup>m</sup> denotes a single template, while p<sub>1</sub>, p<sub>2</sub>, ..., p<sub>d</sub> represent tokens in the template. Different colors indicate diverse templates.  \"fuse\" refers to the process of generating the final classification results from multiple template results as shown in Equation (8). The text encoder and the image encoder are frozen, and only the tokens in the prompt templates are learnable.", "description": "This figure illustrates the architecture of the Causality-Guided Semantic Decoupling and Classification (CDC) framework.  Multiple learnable text prompt templates (t<sup>1</sup>, t<sup>2</sup>, ..., t<sup>M</sup>) are used, each with learnable tokens (p<sub>1</sub>, p<sub>2</sub>, ..., p<sub>d</sub>), to decouple different semantic aspects of the input image.  A hand-crafted prompt (t<sup>0</sup>) is also included.  The image encoder and text encoder are frozen, and only the prompt tokens are trained.  Different image augmentations are applied to each template. The outputs from each template are then fused to produce a final classification result.  The losses used during training are shown:  L<sub>t-ce</sub> (trusted cross-entropy loss), L<sub>de</sub> (diversity loss), and L<sub>con</sub> (consistency loss).", "section": "4 Methodology"}, {"figure_path": "vwgWbCxeAQ/figures/figures_15_1.jpg", "caption": "Figure 1: (a) A motivating example of task misalignment, illustrating the cosine similarities between an image and various text descriptions in the embedding space of CLIP. (b) A motivating experiment on data misalignment, showing the accuracy trends for base and new classes across different training epochs on the DTD dataset.", "description": "This figure demonstrates the two types of misalignment in CLIP adaptation. (a) shows task misalignment: CLIP struggles to focus on specific task-relevant semantics within image-text pairs, hindering accurate classification. (b) shows data misalignment: overfitting to training data (base classes) leads to poor generalization on unseen data (new classes), as observed in the DTD dataset's accuracy trend across epochs.", "section": "1 Introduction"}, {"figure_path": "vwgWbCxeAQ/figures/figures_15_2.jpg", "caption": "Figure 1: (a) A motivating example of task misalignment, illustrating the cosine similarities between an image and various text descriptions in the embedding space of CLIP. (b) A motivating experiment on data misalignment, showing the accuracy trends for base and new classes across different training epochs on the DTD dataset.", "description": "This figure demonstrates two types of misalignment issues in CLIP model adaptation.  (a) shows \"task misalignment.\"  The cosine similarity between an image and its complete description is high, but similarity to individual components of that description is low, illustrating the challenge of selecting task-relevant features. (b) shows \"data misalignment.\" It plots accuracy over epochs for base (seen in training) and new (unseen) classes on the DTD dataset, revealing overfitting to base classes and reduced performance on new classes.", "section": "1 Introduction"}]