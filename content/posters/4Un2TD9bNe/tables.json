[{"figure_path": "4Un2TD9bNe/tables/tables_13_1.jpg", "caption": "Table 1: The 27 combinations of model states partitioning.", "description": "This table systematically lists all 27 possible combinations of partitioning strategies for model parameters (p), gradients (g), and optimizer states (os) across three levels of granularity: no partitioning (N), intra-group partitioning (I), and global partitioning (G).  Each combination is represented as Pp+g+os, where P, g, and os represent the partitioning strategies for parameters, gradients, and optimizer states, respectively.  A checkmark indicates effective combinations identified by the authors, while a cross indicates strategies eliminated based on the authors' analysis of the trade-off between memory and communication costs.", "section": "A.1.1 Effective Strategies of Partitioning"}, {"figure_path": "4Un2TD9bNe/tables/tables_13_2.jpg", "caption": "Table 2: The partitioned blocks of data held by GPU", "description": "This table shows how data blocks are distributed among GPUs based on different partitioning strategies: No partitioning, Intra-group partitioning, and Global partitioning.  For each strategy, it specifies the indices of the data blocks that reside on a given GPU within a group and across different groups.", "section": "A.1.2 Collective Communication Used in PaRO-DP"}, {"figure_path": "4Un2TD9bNe/tables/tables_14_1.jpg", "caption": "Table 3: The Collective Communication used in PaRO-DP from the perspective of a single GPU(GPUi,jo). The values (N/I/G) of Inputs/Outputs blocks refer to Table 2.", "description": "This table details the collective communication operations used for synchronization between blocks of different partitioning in the PaRO-DP framework.  It shows the input and output blocks, participation ranks (GPU indices), and a description of each operation (global all-gather, global reduce_scatter, etc.), indicating how they are optimized by PaRO-CC.", "section": "A.1.2 Collective Communication Used in PaRO-DP"}, {"figure_path": "4Un2TD9bNe/tables/tables_15_1.jpg", "caption": "Table 8: The Metric (1/T) of training throughput and GPU memory of LLaMA-7B. \u03a8' = \u03a8 and \u03a8' = \u03a8/16 mean the different ratios of trainable parameters to model parameters.", "description": "This table presents the results of training LLaMA-7B with different numbers of trainable parameters (full parameters, \u03a8'=\u03a8; partial parameters, \u03a8'=\u03a8/16).  It compares various strategies in terms of throughput (1/T, samples per second) and peak GPU memory usage (Mem(GB)).  The table allows for the comparison of different approaches under different memory constraints.", "section": "A.4.2 Results of the Experiments"}, {"figure_path": "4Un2TD9bNe/tables/tables_15_2.jpg", "caption": "Table 1: The 27 combinations of model states partitioning.", "description": "This table presents all possible combinations of partitioning strategies for model parameters (p), gradients (g), and optimizer states (os) across three levels of granularity: no partitioning (N), intra-group partitioning (I), and global partitioning (G).  Each combination is represented as Pp+g+os (e.g., NNN, IIG), indicating the partitioning level for each component.  The table highlights combinations identified as effective and those deemed ineffective, based on the analysis presented in the paper.  Ineffective strategies were eliminated based on an analysis of memory usage and communication costs, considering the tradeoffs between these two aspects. The effective strategies form the basis of the PaRO-DP approach.", "section": "A.1.1 Effective Strategies of Partitioning"}, {"figure_path": "4Un2TD9bNe/tables/tables_15_3.jpg", "caption": "Table 8: The Metric (1/T) of training throughput and GPU memory of LLaMA-7B. \u03a8' = \u03a8 and \u03a8' = \u03a8/16 mean the different ratios of trainable parameters to model parameters.", "description": "This table presents the training throughput (1/T) and GPU memory usage for the LLaMA-7B model under two different scenarios: full-parameter training (\u03a8' = \u03a8) and partial-parameter training (\u03a8' = \u03a8/16).  It compares various PaRO-DP strategies against existing methods like ZeRO and MiCS, showing the throughput and peak memory used by each strategy. The results highlight the effectiveness of PaRO-DP in improving training throughput while maintaining reasonable memory consumption.", "section": "4.3 Efficiency of PaRO-DP"}, {"figure_path": "4Un2TD9bNe/tables/tables_16_1.jpg", "caption": "Table 8: The Metric (1/T) of training throughput and GPU memory of LLaMA-7B. \u03a8' = \u03a8 and \u03a8' = \u03a8/16 mean the different ratios of trainable parameters to model parameters.", "description": "This table presents the results of training throughput (1/T) and GPU memory usage for the LLaMA-7B model under two different scenarios: full-parameter training (\u03a8' = \u03a8) and partial-parameter training (\u03a8' = \u03a8/16).  It compares various PaRO-DP strategies against existing methods such as ZeRO-2, ZeRO-3, MiCS, and FSDP-hz.  The table helps demonstrate the performance and memory efficiency improvements achieved with PaRO-DP, particularly in the partial-parameter training case, which is important for efficiency. Note that 1/T is a measure of training speed.", "section": "4.3 Efficiency of PaRO-DP"}, {"figure_path": "4Un2TD9bNe/tables/tables_17_1.jpg", "caption": "Table 1: The 27 combinations of model states partitioning.", "description": "This table presents all possible combinations of model states partitioning strategies (Parameter, Gradient, Optimizer state) across three levels of granularity (No partitioning, Intra-group partitioning, Global partitioning).  Each combination is evaluated for effectiveness, with a checkmark indicating effective strategies and a cross indicating ineffective strategies based on the insights from the paper.  This helps determine the optimal partitioning based on the specific needs of model size, memory usage and communication requirements.", "section": "A.1.1 Effective Strategies of Partitioning"}, {"figure_path": "4Un2TD9bNe/tables/tables_17_2.jpg", "caption": "Table 4: The calculation formula of tparam", "description": "This table presents the calculation formula for tparam (time cost of all-gather for parameters) under three different partitioning granularities: no partitioning (N), intra-group partitioning (I), and global partitioning (G).  It shows how the time cost changes depending on the number of GPUs in a group (m), the total number of GPUs (n), the bandwidth between GPUs within a group (B'), the bandwidth between groups (B), and the number of parameters (\u03a8).", "section": "3.1 PaRO-DP"}, {"figure_path": "4Un2TD9bNe/tables/tables_17_3.jpg", "caption": "Table 5: The calculation formula of tgradient", "description": "This table shows the calculation formula of the time cost (tgradient) for gradient synchronization during the backward pass in different model state partitioning strategies (No partitioning (N), Intra-group partitioning (I), and Global partitioning (G)). The formula considers the number of parameters (\u03a8'), the number of GPUs in a group (m), the total number of GPUs (n), inter-group bandwidth (B), and intra-group bandwidth (B').", "section": "3.1 PaRO-DP"}, {"figure_path": "4Un2TD9bNe/tables/tables_17_4.jpg", "caption": "Table 6: The calculation formula of tsyncg", "description": "This table details the calculation formulas for the time cost of synchronizing gradients (tsyncg) in various partitioning strategies for the optimizer states.  The formulas account for the number of parameters (\u03a8\u2019), the number of GPUs (n), the number of GPUs per group (m), the number of groups (g), inter-group bandwidth (B), and intra-group bandwidth (B\u2019).", "section": "3.1 PaRO-DP"}, {"figure_path": "4Un2TD9bNe/tables/tables_18_1.jpg", "caption": "Table 7: The calculation formula of tsyncp", "description": "This table presents the calculation formulas for the time cost (tsyncp) of synchronizing model parameters after they have been updated, considering different partitioning strategies (no partitioning (N), intra-group partitioning (I), and global partitioning (G)) for parameters (p) and optimizer states (os).  The formulas account for the number of trainable parameters (\u03a8'), the number of GPUs in a group (m), the total number of GPUs (n), the number of groups (g = n/m), intra-group bandwidth (B'), and global bandwidth (B). The time cost is 0 when neither parameters nor optimizer states are partitioned.", "section": "3.1.3 The Guideline for PaRO-DP Strategy Selection"}, {"figure_path": "4Un2TD9bNe/tables/tables_18_2.jpg", "caption": "Table 8: The Metric (1/T) of training throughput and GPU memory of LLaMA-7\u0392. \u03a8' = \u03a8 and \u03a8' = \u03a8/16 mean the different ratios of trainable parameters to model parameters.", "description": "This table presents the results of training throughput (1/T) and GPU memory usage for the LLaMA-7B model under two different scenarios: full-parameter training (\u03a8' = \u03a8) and partial-parameter training (\u03a8' = \u03a8/16).  For each scenario, it shows the performance of various PaRO-DP strategies along with some existing strategies like ZeRO-2 and ZeRO-3 for comparison. The results are intended to demonstrate the effectiveness of PaRO-DP strategies in optimizing the trade-off between training speed and memory consumption.", "section": "4.2 Experiment Settings"}, {"figure_path": "4Un2TD9bNe/tables/tables_18_3.jpg", "caption": "Table 8: The Metric (1/T) of training throughput and GPU memory of LLaMA-7B. \u03a8' = \u03a8 and \u03a8' = \u03a8/16 mean the different ratios of trainable parameters to model parameters.", "description": "This table presents the results of training throughput (1/T) and GPU memory usage for the LLaMA-7B model under two different scenarios: full-parameter training (\u03a8'=\u03a8) and partial-parameter training (\u03a8'=\u03a8/16).  It compares the performance of various PaRO-DP strategies against existing methods such as ZeRO-2, ZeRO-3, MiCS, and FSDP-hz. The 1/T values represent the training speed, while Mem(GB) shows the GPU memory consumption for each strategy.", "section": "4.2 Experiment Settings"}, {"figure_path": "4Un2TD9bNe/tables/tables_19_1.jpg", "caption": "Table 10: The Throughput of GPU to GPU communication", "description": "This table presents the measured inter- and intra-group GPU-to-GPU communication throughput in the experimental environment.  It shows the transfer size (in bytes) and duration (in milliseconds) for both intra-node and inter-node communication and calculates the throughput in Gigabits per second (Gbps). This highlights the significant performance difference between communication within a single node and communication between nodes.", "section": "A.4.2 Results of the Experiments"}, {"figure_path": "4Un2TD9bNe/tables/tables_19_2.jpg", "caption": "Table 11: Configuration and Result of Experiments when full-parameters training (\u03a8'=\u03a8).", "description": "This table presents a comparison of different training strategies (including PaRO-DP strategies and other state-of-the-art methods) in terms of throughput and peak memory usage when training large language models with full trainable parameters. It showcases the superior performance of several PaRO-DP strategies compared to existing approaches under the same experimental setup.", "section": "4.2 Experiment Settings"}, {"figure_path": "4Un2TD9bNe/tables/tables_19_3.jpg", "caption": "Table 12: Configuration and Result of Experiments when partial-parameters training (\u03a8' = \u03a8/16).", "description": "This table presents the results of experiments conducted on 7B and 65B LLaMA models under partial-parameters training conditions (\u03a8'=\u03a8/16). It compares various strategies including PaRO-DP, ZeRO++, and FSDP-hz, evaluating their throughput and peak memory usage.  The configurations used for each strategy are detailed, allowing for reproducibility. The table shows that PaRO-DP strategies generally achieve higher throughput while maintaining comparable or lower peak memory compared to other methods.", "section": "4.2 Experiment Settings"}, {"figure_path": "4Un2TD9bNe/tables/tables_20_1.jpg", "caption": "Table 13: Configuration and Result of Experiments when PEFT (\u03a8' = 3\u03a8/1000).", "description": "This table presents the results of experiments conducted using Parameter-Efficient Fine-Tuning (PEFT) with a ratio of trainable parameters to model parameters (\u03a8'/\u03a8) set to 3/1000. It compares the throughput and peak memory usage of different strategies (GGG (ZeRO-3), ING (PaRO), and ZERO++) in training the LLaMA-65B model, showcasing the performance improvement achieved by PaRO in the PEFT setting.", "section": "4.2 Experiment Settings"}, {"figure_path": "4Un2TD9bNe/tables/tables_20_2.jpg", "caption": "Table 14: Throughput Comparison of LLaMA-7B on 32 GPUs when full-parameters training (\u03a8\u2032 = \u03a8). The global batch size in one global step is fixed to 17280 or effective batch size per GPU is fixed to 540. Note: MBS = micro_batch_size, AS = accumulation_steps, EBS = effective_batch_size", "description": "This table compares the throughput of different strategies for training the LLaMA-7B model on 32 GPUs, when using a full-parameter training setup (\u03a8\u2032 = \u03a8). It shows the throughput (in samples/sec) for three different configurations of micro-batch size (MBS), accumulation steps (AS), and effective batch size (EBS). The table highlights the impact of dynamic effective batch size on training efficiency.", "section": "A.4.3 Maximum Throughput via Dynamic Effective Batch Size"}, {"figure_path": "4Un2TD9bNe/tables/tables_20_3.jpg", "caption": "Table 15: Throughput Comparison of LLaMA-65B on 64 GPUs when full-parameters training (\u03a8' = \u03a8). Note: MBS = micro_batch_size, AS = accumulation_steps, EBS = effective_batch_size", "description": "This table compares the throughput of different training strategies (IIG (PaRO) and GGG (ZeRO-3)) for the LLaMA-65B model on 64 GPUs, while varying the effective batch size. The results show that the IIG strategy of PaRO significantly outperforms ZeRO-3 under most conditions.", "section": "A.4.3 Maximum Throughput via Dynamic Effective Batch Size"}]