[{"type": "text", "text": "GDeR: Safeguarding Efficiency, Balancing, and Robustness via Prototypical Graph Pruning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Guibin Zhang\u22171,2, Haonan Dong\u22171, Yuchen Zhang2, Zhixun $\\mathbf{Li}^{3}$ , Dingshuo Chen4, Kai Wang5, Tianlong Chen6, Yuxuan Liang7, Dawei Cheng\u20201,2, Kun Wang\u20208 ", "page_idx": 0}, {"type": "text", "text": "Tongji Univerity, 2Shanghai AI Laboratory, 3CUHK, 4UCAS, 5NUS, 6UNC-Chapel Hill, 7HKUST (Guangzhou) 8NTU \u2217Equal Contribution, \u2020 Corresponding author dcheng@tongji.edu.cn, wk520529wjh@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Training high-quality deep models necessitates vast amounts of data, resulting in overwhelming computational and memory demands. Recently, data pruning, distillation, and coreset selection have been developed to streamline data volume by retaining, synthesizing, or selecting a small yet informative subset from the full set. Among these methods, data pruning incurs the least additional training cost and offers the most practical acceleration benefits. However, it is the most vulnerable, often suffering significant performance degradation with imbalanced or biased data schema, thus raising concerns about its accuracy and reliability in on-device deployment. Therefore, there is a looming need for a new data pruning paradigm that maintains the efficiency of previous practices while ensuring balance and robustness. Unlike the fields of computer vision and natural language processing, where mature solutions have been developed to address these issues, graph neural networks (GNNs) continue to struggle with increasingly large-scale, imbalanced, and noisy datasets, lacking a unified dataset pruning solution. To achieve this, we introduce a novel dynamic soft-pruning method, GDeR, designed to update the training \u201cbasket\u201d during the process using trainable prototypes. GDeR first constructs a well-modeled graph embedding hypersphere and then samples representative, balanced, and unbiased subsets from this embedding space, which achieves the goal we called Graph Training Debugging. Extensive experiments on five datasets across three GNN backbones, demonstrate that GDeR (I) achieves or surpasses the performance of the full dataset with $30\\%\\sim\\,50\\%$ fewer training samples, (II) attains up to a $2.81\\times$ lossless training speedup, and (III) outperforms state-of-the-art pruning methods in imbalanced training and noisy training scenarios by $0.3\\%\\sim4.3\\%$ and $3.6\\%\\sim7.8\\%$ , respectively. The source code is available at https://github.com/ins1stenc3/GDeR. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Data-centric AI, though continuously providing high-quality data for upcoming artificial general intelligence [1, 2, 3, 4], presents a significant hurdle for their on-device deployment during training and inference phases [5, 6, 7, 8]. To democratize existing state-of-the-art methods [9, 10, 9, 11, 12, 13, 14], considerable efforts are directed toward identifying unbiased and core data within training datasets and conducting troubleshooting to deepen our solid understanding of the intrinsic property of data shcema [15, 16, 17, 18]. To date, data pruning [19, 20, 21, 22, 23], distillation [24, 25, 26, 27, 28, 29, 30] and coreset selection [31, 32, 33, 16] aim to retain, synthesize or choose a small but informative dataset from original full set. While the sample size undergoes significant reshaping and reduction, methods like dataset distillation inevitably lead to additional training costs [23, 34, 35]. As a hardware-friendly candidate and accelerator for training and inference, data pruning serves as a promising candidate by mitigating the high computational burden. ", "page_idx": 0}, {"type": "text", "text": "Recent advancements, however, have demonstrated that the efficiency of data pruning may come at a cost\u2014utilizing only a portion of the data can potentially render the model more vulnerable to imbalance or malicious perturbation attacks [37], which are commonly seen in real-world applications. As illustrated in Figure 1, we evaluate the performance of the current state-of-the-art data pruning paradigm, InfoBatch [23], with a pruning ratio of $5\\bar{0}\\%$ , on MUTAG $\\left[38\\right]\\neg\\mathrm{GCN}$ [39] under both imbalance and biased scenarios. It can be observed that: $\\pmb{\\mathrm{\\Sigma}}$ InfoBatch exacerbates the imbalance of training samples during the training process; $\\pmb{\\varphi}$ InfoBatch efficiently saves training costs in noisefree scenarios, even surpassing the original full dataset performance by $0.7\\%$ . However, it encounters significant performance degradation $(5.7\\%\\sim6.\\bar{4}\\%\\downarrow)$ in biased scenarios. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we primarily focus on graph-level data pruning, aiming to enhance the model\u2019s ", "page_idx": 1}, {"type": "image", "img_path": "O97BzlN9Wh/tmp/18129c9af37eebee36c240d327e2bff827b6505348c1230d00325331fb1eacad.jpg", "img_caption": ["Figure 1: (a) We report the label distribution of the training set retained by InfoBatch at pruning ratios of $50\\bar{\\%}$ in the $\\{0,10\\dot{0},200,300\\}$ -th epochs. The gray, light blue and dark blue represent pruned, minority, and majority samples, respectively. (b) Performance comparison between InfoBatch and our GDeR when introducing outliers (following [36]) into $\\{0\\%,10\\%,20\\%\\}$ of the training set. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "robustness to data imbalance and noise while maintaining the efficiency inherited from traditional data pruning practices. This is because, unlike in computer vision (CV) and natural language processing (NLP) domains, where separate solutions already exist for addressing these issues [37, 23], graph learning models continue to grapple with increasingly large-scale, imbalanced, and biased datasets [40, 36, 41]. To this end, we first introduce a novel direction in the realm of graph training, termed Graph Training Debugging (GTD), to (dynamically) identify representative, robust, and unbiased subsets for accelerating the training process without compromising performance. ", "page_idx": 1}, {"type": "text", "text": "We achieve GTD goal by proposing a novel dynamical soft-pruning method, Graph De-Redundancy (GDeR), in which specifically designed to work efficiently and accurately on various GNN architectures. Concretely, GDeR draws inspiration from prototype learning [42, 43] practices, projecting training graph samples onto a hyperspherical embedding space. It utilizes a set of trainable prototypes to regularize the graph embedding distribution, essentially encouraging both inter-class separateness and intra-class compactness. Furthermore, on this well-regularized hypersphere, GDeR generates a sampling distribution that encourages the sampling of under-learned graphs, while excluding those with high outlier risk and belonging to majority clusters. Given a training budget (i.e., pruning ratio), GDeR dynamically maintains a sub-dataset at each epoch, efficiently combating the negative impact of imbalanced and noisy data on the model, simultaneously accelerating training significantly. ", "page_idx": 1}, {"type": "text", "text": "Broader Impact. In this paper, we present a novel training philosophy GDeR to achieve our defined GTD goal. GDeR dynamically prunes irrelevant graph samples, providing a more comprehensive insight and achieving a triple-win of efficiency, balancing, and robustness. This approach can contribute to a wide range of graph-related applications, accelerating model training while demonstrating great potential in scenarios such as adversarial attacks [44, 45, 45], imbalanced graph classification [40, 46], and unsupervised pre-training [47, 48, 49]. We believe GDeR can serve as a benchmark for future research in this area, attracting significant attention and inspiring further exploration into understanding sparsity in other domains such as LLMs. ", "page_idx": 1}, {"type": "text", "text": "Experimental Observation We validate the GDeR through a comprehensive series of graph-level tasks, across five datasets and three GNN backbones, showcasing that GDeR can: $\\pmb{\\mathrm{\\Sigma}}$ achieve lossless training performances with $30\\%\\sim50\\%$ fewer training samples, $\\pmb{\\varphi}$ achieve a $2.0\\times$ lossless speedup on OGBG-MOLHIV, and a $2.81\\times$ lossless speedup on pre-training ZINC. $\\pmb{\\otimes}$ mitigate imbalance issues by achieving a $0.3\\sim4.3\\%\\uparrow$ in F1-macro on MUTAG and DHFR datasets, $\\pmb{\\mathbb{\\otimes}}$ effectively help outlier-attacked GNNs improve accuracy by $3.5\\%\\sim10.2\\%$ through data pruning. ", "page_idx": 1}, {"type": "text", "text": "Limitations & Future Insight. GDeR, as a plug-in to graph training, not only improves efficiency but also ensures robustness and balance throughout the training. However, the applicability of its principles in fields such as CV remains unexplored, limiting the generalizability of data debugging. This represents a direction for future development in our work. ", "page_idx": 1}, {"type": "text", "text": "2 Technical Background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Notations Consider an undirected graph $\\mathcal{G}\\,=\\,(\\mathcal{V},\\mathcal{E})$ , where $\\nu$ represents the node set and $\\mathcal{E}$ signifies the edges. The feature matrix for the graph is designated as $\\mathbf{X}\\in\\mathbb{R}^{|\\mathcal{V}|\\times F}$ . Each node $v_{i}\\in\\mathcal{V}$ is associated with a feature vector of $F$ dimensions. The adjacency matrix $\\mathbf{A}\\in\\{0,1\\}^{N\\times N}$ represents the connectivity between nodes, where $\\mathbf{A}[i,j]=1$ suggests the presence of an edge $e_{i j}\\in\\mathcal{E}$ , and 0 indicates no edge. In graph-level training tasks, specifically for graph classification, given a set of $N$ graphs $\\{\\mathcal{G}\\}=\\{\\mathcal{G}_{1},\\bar{\\mathcal{G}_{2}},\\ldots,\\mathcal{G}_{N}\\}$ , where each graph $\\mathcal{G}_{i}\\doteq(\\nu^{i},\\bar{\\mathcal{L}}^{i})$ is as defined above, and their corresponding labels $\\mathbf{Y}\\in\\mathbb{R}^{N\\times C}$ with $C$ being the total number of classes, we aim to learn graph representations $\\mathbf{H}\\in\\mathbb{R}^{N\\times d^{\\prime}}$ with $\\mathbf{H}[i,:]$ for each $\\mathcal{G}_{i}\\in\\mathcal{G}$ that effectively predict $\\mathbf{Y}_{i}$ . ", "page_idx": 2}, {"type": "text", "text": "Graph Neural Networks (GNNs). GNNs [50, 51] have become pivotal for learning graph representations, achieving benchmark performances in various graph tasks at node-level [52], edge-level [53], and graph-level [54]. The success of GNN mainly stems from message-passing mechanism: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{h}_{i}^{(\\widetilde{l})}=\\mathbf{COMB}\\left(\\mathbf{h}_{i}^{(l-1)},\\mathtt{A G G R}\\{\\mathbf{h}_{j}^{(l-1)}:v_{j}\\in\\mathcal{N}(v_{i})\\}\\right)\\overset{\\sim}{,}\\overset{\\cdot}{0}\\le l\\overset{\\sim}{\\le}L.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Here, $L$ represents the number of GNN layers, where $\\mathbf{h}_{i}^{(0)}~=~\\mathbf{x}_{i}$ , and $\\mathbf{h}_{i}^{(l)}(1~\\le~l~\\le~L)$ denotes the node embedding of $v_{i}$ at the $l$ -th layer. ${\\mathcal N}(v_{i})$ denotes the 1-hop neighbors of $v_{i}$ , and AGGR(\u00b7) and $\\mathtt{C O M B(\\cdot)}$ are used for aggregating neighborhood information and combining ego/neighborrepresentations, respectively. Finally, a sum/mean pooling operation is commonly used for READOUT function to obtain the graph-level embedding. While promising, the increasing volume of graph samples [55, 19, 56] poses significant computational challenges for both training and pre-training of GNNs. Efficiently accelerating graph-level training remains an unresolved issue. ", "page_idx": 2}, {"type": "text", "text": "Data Pruning Current data pruning methods can be categorized as static or dynamic [23]. Static data pruning involves heuristic-based metrics or limited training to assess sample importance and perform pruning before formal training, like EL2N [18] and Influence-score [57]. On the other hand, dynamic data pruning dynamically selects different training samples during training [58, 23, 59], often achieving better results than static pruning. In the graph domain, attempts related to data pruning include edge-level sampling techniques like GraphSAGE [60] and GraphSAINT [61]. However, to the best of our knowledge, there is currently no method specially designed for graph-level data pruning, let alone one that can simultaneously improve the balance and robustness of GNNs. ", "page_idx": 2}, {"type": "text", "text": "Imbalance in GNNs Deep imbalanced learning has been one of the significant challenges in deep learning [62]. The current mainstream research can be broadly categorized into three approaches: (1) re-sampling [63, 64, 65], which balances the number of samples from different classes; (2) re-balancing [66, 67, 68], which adjusts the loss values for samples from different classes; and (3) post-hoc processing [69], which shifts the model logits based on label frequencies. In the domain of graph learning, most efforts to address the imbalance issue focus on node-level classification imbalances [70, 71, 72], yet solutions targeting graph-level imbalance are relatively limited. Despite a preliminary attempt [40], which requires complex up-sampling and regrouping operations, there is still a need for a straightforward yet effective solution to graph imbalance issue. ", "page_idx": 2}, {"type": "text", "text": "Robustness in GNNs As for robustness learning, many studies showcase graph classification is vulnerable to adversarial attacks [73, 74]. Given a set of training or test graphs, an attacker could perturb the graph structure [75] and/or node features to deceive a graph classifier into making incorrect predictions for the perturbed testing graph. Traditional empirical and certified defenses [76, 77, 78, 79] often involve complex designs and additional components. In this paper, we propose subtle adjustments during training, leveraging prototypes to enhance the robustness of graph training. ", "page_idx": 2}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Problem Formulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In the classic scenario of graph-level training (not limited to specific tasks like graph classification, regression, or pre-training), given a graph dataset $\\mathcal{D}=\\{z_{i}\\}_{i=1}^{|\\mathcal{D}|^{-}}=\\{(\\mathcal{G}_{i},\\mathbf{Y}_{i})\\}_{i=1}^{|\\mathcal{D}|^{-}}$ , a GNN encoder is employed to extract graph-level embeddings $\\mathbf{H}=\\{\\mathbf{h}_{i}\\}_{i=1}^{|\\mathcal{D}|}$ for each graph sample, which are then utilized for downstream tasks. The goal of GDeR is to find an oracle function that changes with time (epochs) and can determine the current most representative, balanced, and denoised core subset $\\mathbf{\\mathcal{X}}_{t}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{X}_{t}=\\mathcal{F}_{t-1}\\left(\\mathcal{D},\\{\\mathbf{h}_{i}^{(t-1)}\\}_{i=1}^{|\\mathcal{D}|}\\right),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathcal{F}_{t-1}$ is the selection function at the $(t-1)$ -th epoch. Given a preset sparsity ratio $s\\%$ , the subset\u2019s volume is fixed as $|\\mathcal{X}_{t}|=(1-s)\\%\\times|\\mathcal{D}|$ . ", "page_idx": 2}, {"type": "image", "img_path": "O97BzlN9Wh/tmp/f54b9b04d1c656ce1623c80f6d5d7341a0ea5f0ac0a6306bcc8579bdb33937d4.jpg", "img_caption": ["Figure 2: The overview of our proposed GDeR. GDeR comprises hypersphere projection, embedding space modeling, sampling distribution formatting, and the final dynamic sampling. We present the dynamic sample selection process of GDeR within one epoch. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "3.2 Overview of the Proposed Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "As shown in Figure 2, given an arbitrary GNN, GDeR selects a training sample set $\\textstyle{\\mathcal{X}}_{t}$ within a specified budget for each epoch. At the $t$ -th epoch, after the GNN $f_{\\theta}:\\mathbf{\\DeltaX}\\bar{\\rightarrow}\\mathbb{R}^{E}$ outputs graph embeddings $\\textbf{h}\\in\\mathbb{R}^{E}$ from the input graph $\\mathcal{G}_{i}$ with $\\mathbf{h}\\;=\\;f_{\\theta}(\\mathcal{G}_{i})$ , these are projected into a hyperspherical embedding space via a projector $g_{\\phi}:\\;\\mathbb{R}^{E}\\rightarrow\\mathbb{R}^{D}$ . GDeR allocates a set of $M$ trainable prototypes Pc = {pck}kK=1 for each class $c$ , with associated losses used to shape the embedding space, ensuring inter-class separation and intra-class compactness. In this regularized space, GDeR formulates a sampling distribution by focusing on samples unfamiliar to the model, excluding those from the majority prototype cluster and with high outlier risk, thereby providing a subset of samples $S_{t+1}$ for the next epoch. Through this balanced and robust dynamic pruning mechanism, GDeR achieves unbiased graph representations at a significantly lower training cost than the full dataset. ", "page_idx": 3}, {"type": "text", "text": "3.3 Projection onto Hyperspherical Embedding Space ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "At the $t$ -th epoch, GDeR maintains a subset $\\scriptstyle{\\mathcal{X}}_{t}$ with a given budget, where $s\\%=|\\mathcal{X}_{t}|/|\\mathcal{D}|$ is a constant, representing the dataset pruning ratio. Given the feature representations $\\mathbf{H}\\in\\mathbb{R}^{|\\mathcal{X}_{t}|\\times E}$ output by $f_{\\theta}$ , we first project these features into a hyperspherical embedding space, denoted as ${\\bf z}^{\\prime}=g_{\\phi}({\\bf h}),{\\bf z}=$ $\\mathbf{z}^{\\prime}/||\\mathbf{z}^{\\prime}||_{2}^{-}$ . This projection has been shown to be beneficial for compactly embedding samples of the same class [80, 81, 82]. The projected embeddings $\\mathbf{z}\\in\\mathbb{R}^{D}$ , which lie on the unit sphere $(||\\mathbf{z}||^{2}=1)$ ), can naturally be modeled using the von Mises-Fisher (vMF) distribution [80, 81]. Here, we first consider the graph classification scenario1, in which we allocate $K$ prototypes $\\mathbf{P}^{c}=\\{\\mathbf{p}_{k}^{c}\\}_{k=1}^{K}$ for each class $c$ ( $1\\leq c\\leq C)$ ). Following conventional practices in hyperspherical space modeling [83], we model a vMF distribution as the combination of a center prototype representation $\\mathbf{p}_{k}$ and the concentration parameter $\\kappa$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\np_{D}(\\mathbf{z};\\mathbf{p}_{k},\\kappa)=Z_{D}(\\kappa)\\exp\\left(\\kappa\\mathbf{p}_{k}^{\\top}\\mathbf{z}\\right),\\ Z_{D}(\\kappa)=\\frac{\\kappa^{D/2-1}}{(2\\pi)^{D/2}I_{D/2-1}(\\kappa)},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\kappa\\geq0$ denotes the tightness around the mean, $Z_{D}(\\kappa)$ represents a normalization factor [83], $\\exp\\left(\\kappa\\mathbf{p}_{k}^{\\top}\\mathbf{z}\\right)$ is called the angular distance and $I_{v}$ is the modified Bessel function of the first kind with order $v$ . In our multi-prototype settings, we model the probability density of a graph embedding $\\mathbf{z}_{i}$ in class $c$ as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\np(\\mathbf{z}_{i};\\mathbf{P}^{c},\\kappa)=\\sum_{k=1}^{K}Z_{D}(\\kappa)\\exp(\\kappa\\mathbf{p}_{k}^{c}{}^{\\top}\\mathbf{z}_{i}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Further, the embedding $\\mathbf{z}_{i}$ is assigned to class $c$ with the normalized probability as shown above: ", "page_idx": 4}, {"type": "equation", "text": "$$\np(y_{i}=c\\mid\\mathbf{z}_{i};\\;\\{\\mathbf{P}^{j},\\kappa\\}_{j=1}^{C})=\\frac{\\sum_{k=1}^{K}Z_{D}(\\kappa)\\exp(\\mathbf{p}_{k}^{c}{}^{\\top}\\mathbf{z}_{i}/\\tau)}{\\sum_{j=1}^{C}\\sum_{k^{\\prime}=1}^{K}Z_{D}(\\kappa^{\\prime})\\exp(\\mathbf{p}_{k^{\\prime}}^{j}{}^{\\top}\\mathbf{z}_{i}/\\tau)},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\tau$ is a temperature coefficient. Given that we have now allocated a corresponding class for each graph embedding, we aim to further encourage: $\\pmb{\\mathrm{\\Sigma}}$ allocation correctness, meaning that the allocation should be consistent with the ground truth label; $\\pmb{\\varphi}$ intra-class compactness, meaning that graph embeddings should be close to the appropriate prototypes belonging to their own class; and $\\pmb{\\otimes}$ inter-class separateness, meaning that graph embeddings should be distant from prototypes of other classes. To achieve $\\pmb{\\mathrm{\\Sigma}}$ and $\\pmb{\\varphi}$ , we have designed the compactness loss below: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{comp}}=-\\frac{1}{|\\mathcal{X}_{t}|}\\sum_{i=1}^{|\\mathcal{X}_{t}|}\\log\\frac{\\sum_{k=1}^{K}Z_{D}(\\boldsymbol{\\kappa})\\exp(\\mathbf{p}_{k}^{y_{i}\\top}\\mathbf{z}_{i}/\\tau)}{\\sum_{c=1}^{C}\\sum_{k^{\\prime}=1}^{K}Z_{D}(\\boldsymbol{\\kappa^{\\prime}})\\exp(\\mathbf{p}_{k^{\\prime}}^{y_{i}\\top}\\mathbf{z}_{i}/\\tau)},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $y_{i}$ represents the class index for $\\mathbf{z}_{i}$ . Equation (6) is the maximum likelihood estimation of $\\begin{array}{r}{\\operatorname*{max}_{\\theta,\\phi}\\Pi_{i=1}^{|\\mathcal{X}_{t}|}p(y_{i}=c|\\mathbf{z}_{i},\\{\\{\\mathbf{p}_{k}^{c},\\kappa\\}_{k=1}^{K}\\}_{j=1}^{C})}\\end{array}$ , which not only boosts the allocation correctness but also enforces graph embeddings to compactly surround the appropriate prototypes. Furthermore, to achieve $\\pmb{\\otimes}$ , namely encouraging inter-class separateness, we design the separation loss, optimizing large angular distances among different class prototypes: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{sepa}}=\\frac{1}{C}\\sum_{i=1}^{C}\\log\\frac{1}{C-1}\\sum_{j=1}^{C}\\mathbb{1}_{j\\neq i}\\sum_{k=1}^{K}\\exp({\\bf p}_{k}^{j}{\\bf z}_{i}/\\tau)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbb{1}(\\cdot)$ is an indicator function. Through the above regularization, we obtain $C$ prototype clusters $\\{\\chi_{c}\\}_{c=1}^{C}$ , each composed of $K$ prototype centers $\\{{\\bf p}_{k}\\}_{k=1}^{K}$ and surrounding sample sets $\\{\\mathbf{z}^{(C)}\\}$ . After modeling this hypersphere, we proceed with sample selection on the current subset $\\mathcal{D}^{(t)}$ . ", "page_idx": 4}, {"type": "text", "text": "3.4 Efficient, Balanced and Robust Graph Debugging ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Traditional dynamic dataset pruning methods typically rely on loss-based metrics to select informative subsets [58, 23], which, however, can make the model more vulnerable to imbalance and malicious perturbation (as discussed in Section 1). In this subsection, while selecting a representative subset $\\bar{\\mathcal{D}}^{(t)}$ , we also intend to further ensure it is balanced and noise-free. Our first step is to locate samples that are at risk of being outliers in the embedding space. We propose using a prototype-based Mahalanobis distance to estimate the outlier risk of each graph sample: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\omega_{\\mathrm{r}}(\\mathbf{z}_{i})=-\\operatorname*{min}_{c}\\left[-\\mathbb{1}_{y_{i}\\neq c}\\operatorname*{max}_{k}\\left[(\\mathbf{z}_{i}-\\mathbf{p}_{k}^{c})^{\\top}\\Sigma_{k}^{-1}(\\mathbf{z}_{i}-\\mathbf{p}_{k}^{c})\\right]\\right],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\Sigma_{k}\\in\\mathbb{R}^{K\\times K}$ is the sample covariance of all the prototypes in class $c$ . Equation (8) calculates the maximum distance of $\\mathbf{z}_{i}$ to all prototypes within its class, which serves as a robust outlier detection metric [84]. Furthermore, we intend to evaluate the effectiveness of each sample. Given that the distance of an embedding from its cluster center has been shown to be a good indicator of the model\u2019s familiarity with it [85], we compute the distance of each graph sample to its class-specific prototypes as a familiarity metric: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\omega_{\\mathrm{e}}(\\mathbf{z}_{i})=\\frac{\\sum_{k=1}^{K}\\mathrm{dist}(\\mathbf{p}_{k}^{y_{i}},\\mathbf{z}_{i})}{\\sum_{c=1}^{C}\\sum_{k^{\\prime}=1}^{K}\\mathbb{1}_{c\\neq y_{i}}\\mathrm{dist}(\\mathbf{p}_{k^{\\prime}}^{y_{i}},\\mathbf{z}_{i})},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "which suggests that if a graph sample is significantly closer to its own prototypes and farther from those of other classes, the model is more familiar with it. We implement the distance function using the angular distance in Equation (3). When considering the data balancing issue, we formulate the balancing score for each sample $\\mathbf{z}_{i}$ as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\omega_{\\mathrm{b}}(\\mathbf{z}_{i})=\\left|\\left\\{\\mathbf{z}_{i}|\\operatorname*{min}_{k}\\mathrm{dist}(\\phi_{\\mathbf{z}_{i}}(\\mathbf{p}_{k}),\\mathbf{z}_{i})\\right\\}\\right|/|\\{\\phi_{\\mathbf{z}_{i}}(\\boldsymbol{\\chi})\\}|,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\phi_{\\mathbf{z}_{i}}(\\mathbf{p}_{k})$ denotes the closest prototype to $\\mathbf{z}_{i}$ , and $\\phi_{\\mathbf{z}_{i}}(\\chi)$ denotes the prototype cluster that $\\mathbf{z}_{i}$ currently belongs to. Equation (10) evaluates whether the graph sample $\\mathbf{z}_{i}$ belongs to a minority from a prototype-cluster perspective. Finally, we assign sampling probabilities to all samples in $\\mathcal{D}^{(t)}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\omega(\\mathbf{z}_{i})=\\frac{\\omega_{\\mathrm{e}}^{\\sigma}(\\mathbf{z}_{i})}{(\\omega_{\\mathrm{r}}^{\\sigma}(\\mathbf{z}_{i})+\\epsilon)\\cdot(\\omega_{\\mathrm{b}}^{\\sigma}(\\mathbf{z}_{i})+\\epsilon)},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $(\\cdot)^{\\sigma}$ represents the Sigmoid transformation. Equation (11) is designed to sample with higher probability those samples that the model is less familiar with, have a lower outlier risk, and belong to a minority group. Now, at the $t$ -th epoch, we obtain the final sampling probability distribution $\\begin{array}{r l}{\\mathcal{P}^{(t)}(\\mathbf{z}):}&{{}\\int_{\\mathbf{z}\\in\\mathcal{X}_{t}}\\frac{\\omega(\\mathbf{z})}{\\int_{\\mathbf{z}}\\omega(\\mathbf{z})\\,d\\mathbf{z}}\\,d\\mathbf{z}}\\end{array}$ . Recall that we have $(1-s)\\%$ of samples pruned in the $t$ -th epoch, i.e., $\\tilde{\\mathcal{X}}_{t}=\\mathcal{D}\\setminus\\mathcal{X}_{t}$ . For $\\tilde{\\mathcal{X}}_{t}$ , we use the probability distribution $\\mathcal{P}^{(t-1)}(\\mathbf{z})$ from the $(t-1)$ -th epoch 2. Specifically, we formulate GDeR\u2019s coreset sampling function $\\mathcal{F}_{t}$ in Equation (2) as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{F}_{t}(\\mathcal{D},\\mathbf{H})=S\\left(\\mathcal{X}_{t},\\mathcal{P}^{(t)}(\\mathbf{z}),\\Psi(t)\\right)\\bigcup S\\left(\\tilde{\\mathcal{X}}_{t},\\mathcal{P}^{(t-1)}(\\mathbf{z}),\\tilde{\\Psi}(t)\\right),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathcal{F}_{t}$ outputs the selected samples $\\mathcal{X}_{t+1}$ for the next epoch\u2019s training, $S(\\mathcal{X},\\mathcal{P},N)$ is a sampling operator that samples $N$ samples from $\\mathcal{X}$ with probability distribution $\\mathcal{P}$ , and $\\Psi(t)\\,(\\tilde{\\Psi}(t))$ is the scheduler function (with implementation placed in Appendix B.5) that control the number of samples drawn from $\\mathcal X_{t}\\left(\\tilde{\\mathcal X_{t}}\\right)$ , respectively, subject to the given budget $\\Psi(t)+\\tilde{\\Psi}(t)=|\\mathcal{D}|\\times s\\%=|\\mathcal{X}_{t}|$ . ", "page_idx": 5}, {"type": "text", "text": "3.5 Optimization and Extension ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Optimization Aside from the original task-specific loss of GNN training denoted as $\\mathcal{L}_{\\mathrm{task}}$ , GDeR has additionally introduced $\\mathcal{L}_{\\mathrm{comp}}$ and $\\mathcal{L}_{\\mathrm{sepa}}$ . The overall training objective of $\\mathtt{G D e R}$ is formulated as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathtt{G D e R}}=\\mathcal{L}_{\\mathtt{t a s k}}+\\lambda_{1}\\cdot\\mathcal{L}_{\\mathtt{c o m p}}+\\lambda_{2}\\cdot\\mathcal{L}_{\\mathtt{s e p a}},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\lambda_{1}$ and $\\lambda_{2}$ are co-efficient adjusting the relative importance of two losses. We conclude the algorithm workflow table of $\\mathtt{G D e R}$ in Appendix C. ", "page_idx": 5}, {"type": "text", "text": "Extension Finally, we advocate that GDeR is not limited to graph classification but can also be seamlessly adapted to tasks such as graph regression and graph pre-training. The key distinction between these tasks and graph classification is that each graph sample does not have a ground truth class index, which makes ground truth class-based calculations, such as those in Equations (6) and (7), infeasible. One straightforward approach is to manually set $M$ virtual classes, using the class assigned by Equation (5) as the graph sample\u2019s current class. However, this may result in prototypes and hyperspherical embeddings that do not accurately reflect the underlying clustering distribution [86]. To address this, we leverage ProtNCELoss [43] as a self-supervised signal, providing a more reliable reflection of the data\u2019s structure. Detailed implementation can be found in Appendix D. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we conduct extensive experiments to answer the following research questions: (RQ1) Can GDeR effectively boost GNN efficiency (under both supervised and unsupervised settings)? (RQ2) Does GDeR genuinely accelerate the GNN training? (RQ3) Can GDeR help alleviate graph imbalance? (RQ4) Can GDeR aid in robust GNN training? ", "page_idx": 5}, {"type": "text", "text": "4.1 Experiment Setup ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Datasets and Backbones We test GDeR on two widely-used datasets, MUTAG [38] and DHFR [87]; two OGB large-scale datasets, OGBG-MOLHIV and OGBG-MOLPBCA [88]; one large-scale chemical compound dataset ZINC [89]. Following [40], we adopt a $25\\%/25\\%/50\\%$ train/validation/test random split for the MUTAG and DHFR under imbalanced scenarios and $80\\%/10\\%/10\\%$ under normal and biased scenarios, both reporting results across 20 data splits. For OGBG-MOLHIV and OGBGMOLPBCA, we use the official splits provided by [88]. For ZINC, we follow the splits specified in [90]. We choose three representative GNNs, including GCN [91], PNA [92] and GraphGPS [90]. Detailed dataset and backbone settings are in Appendices B.1 and B.2. ", "page_idx": 5}, {"type": "text", "text": "Parameter Configurations The hyperparameters in GDeR include the temperature coefficient $\\tau$ , prototype count $K$ , loss-specific coefficient $\\lambda_{1}$ and $\\lambda_{2}$ . Practically, we uniformly set $K=2$ , and tune the other three by grid searching: $\\tau\\in\\{1e{-}3,1e{-}4,1e{-}5\\}$ , $\\lambda\\in\\{1e{-}1,5e{-}1\\},\\lambda\\in\\{1e{-}1,1e{-}5\\}$ . Detailed ablation study on hyperparameters is placed in Section 4.5. ", "page_idx": 5}, {"type": "table", "img_path": "O97BzlN9Wh/tmp/e430803c7bd4962b38a80326c731aadc9633086450c93a0cc20a94c4f19cef23.jpg", "table_caption": ["Table 1: Performance comparison to state-of-the-art dataset pruning methods when remaining $\\{20\\%$ , $30\\%$ , $50\\%$ , $70\\%\\}$ of the full set. All methods are trained using PNA, and the reported metrics represent the average of five runs. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "4.2 GDeR makes GNN training way faster ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To answer RQ1 and RQ2, we comprehensively compare GDeR with fourteen widely-used static pruning methods and three dynamic pruning methods, as outlined in Table 1, with more detailed explanations in Appendix B.3. Following [23], we add hard random and soft random pruning as baselines for a more comprehensive comparison. Specifically, we set the dataset remaining ratio $(1-s)\\%\\in\\{20\\%,30\\%,5\\dot{0}\\%,70\\%$ . The performance results are shown in Tables 1, 2 and 7 and the efficiency comparisons are in Figure 3. Our observations (Obs.) are as summarized follows: ", "page_idx": 6}, {"type": "text", "text": "Obs.\u2776GDeR achieves maximum graph pruning with performance guarantees. As shown in Tables 1 and 2, GDeR consistently outperforms both static or dynamic baselines under various pruning ratios. For instance, on OGBGMOLHIV $+.$ PNA, GDeR experiences only a $0.5\\%$ performance decay even with $80\\%$ pruning, surpassing the current state-of-the-art method InfoBatch, which suffers a $1.7\\%$ decay. When pruning $50\\%$ and $30\\%$ of the data, GDeR even achieves performance improvements of $0.1\\%$ and $0.5\\%$ , respectively. ", "page_idx": 6}, {"type": "text", "text": "Obs.\u2777 The degree of redundancy varies across different datasets. We observe that OGBG-MOLPCBA is more sensitive to pruning than OGBG-MOLHIV, which suggests the degree of redundancy varies between datasets. For example, when pruning $80\\%$ of the data, GraphGPS on OGBG-MOLPCBA exhibits a performance decay ranging between $3.5\\%$ \u223c $13.9\\%$ , significantly higher than the $2.5\\%$ \u223c $11.5\\%$ decay observed on OGBG-MOLHIV. However, as the remaining ratio increases, GDeR quickly recovers and surpasses the full dataset performance by $0.2\\%$ at the $50\\%$ pruning level. ", "page_idx": 6}, {"type": "image", "img_path": "O97BzlN9Wh/tmp/9262d95d9901e0ef4d855d4ad04fcfaad802206fd224dcfd8f073ae39f44d463.jpg", "img_caption": ["Figure 3: The trade-off between per epoch time and ROC-AUC $(\\%)$ of data pruning methods. Specifically, we report the test performance when pruning methods achieve per epoch times of $\\mathbf{\\bar{\\{90\\%}}}$ , $\\bar{7}0\\%$ , $50\\%$ , $40\\%$ , $30\\%\\dot{\\mathrm{\\textmu}}$ of the full dataset training time. \"Vanilla\" denotes the original GNN backbone without any data pruning. "], "img_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "O97BzlN9Wh/tmp/0605838c72605263946227e89a15837884540fa80a3af544f1dd429994199e1e.jpg", "table_caption": ["Table 2: Performance comparison to state-of-the-art dataset pruning methods. All methods are trained using GraphGPS, and the reported metrics represent the average of five runs. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Obs. $\\pmb{\\otimes}$ GDeR can significantly accelerate GNN training. Figure 3 illustrates the per-epoch time and corresponding performance of each pruning method compared to full dataset training on OGBGMOLHIV $^+$ GraphGPS. It is evident that GDeR can achieve a $2.0\\times$ speedup without any performance loss (corresponding to $50\\%$ per-epoch time). Even with a significant $3.3\\times$ speedup, GDeR only experiences a moderate drop of $0.9\\%$ , which is superior to baselines including InfoBatch by a margin of $1.1\\%\\sim4.2\\%$ . Additionally, we observe from Table 7 that pretraining on ZINC with only $30\\%$ of the data leads to a $1.53\\%$ ROC-AUC improvement, with $2.81\\times$ training time acceleration. ", "page_idx": 7}, {"type": "text", "text": "4.3 GDeR Mitigates Graph Imbalance ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To answer RQ3, we tested GDeR in extremely imbalanced scenarios and compared its performance with other dynamic pruning methods. Following [40], we randomly set $25\\%/25\\%$ graphs as training/- validation sets and within each of them, we designate one class as the minority class and reduce the number of graphs for this class in the training set (while increasing the others) until the imbalance ratio reached 1:9, which creates an extremely imbalanced scenario. The reported metrics are the average of 50 different data splits to avoid bias from data splitting. We observe from Figure 4 that: ", "page_idx": 7}, {"type": "image", "img_path": "O97BzlN9Wh/tmp/2a9a46b511a35596ec784f08ed6071e744c9269f57bfedadac3aa71f5df1735f.jpg", "img_caption": ["Figure 4: Performance comparison of different pruning methods across various imbalance ratios. We utilize MUTAG and DHFR datasets with GCN, and reported the metrics when adjusting the imbalance ratios among {1:9, 3:7, 5:5, 7:3, 9:1}. \u201cNo Pruning\u201d denotes training GCN without dataset pruning. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Obs. \u2779GDeR can effectively mitigate imbalance issues. As observed in Figure 4, baseline pruning methods struggle to outperform \u201cno-pruning\u201d GCN, resulting in substantial losses in speedup efficacy. In contrast, GDeR offers a more meaningful pruning approach. For instance, on DHFR, pruning $50\\%$ of the data results in a $4.3\\%$ improvement in F1-Macro. This demonstrates that GDeR not only saves computational resources but also effectively mitigates data imbalance issues. ", "page_idx": 7}, {"type": "image", "img_path": "O97BzlN9Wh/tmp/466d39322d9d217ae231557d3a21e9bd6a9dc66fb9da0c4db5d73761d09205d1.jpg", "img_caption": ["Figure 5: (Left) We report the performance of several top-performing pruning methods when perturbation noise is added to $10\\%$ of the training set of MUTAG. The black dashed line represents the original GNN performance without pruning. (Right) We compare GDeR with DropEdge and GRAND under different noise settings, utilizing GDeR with pruning ratios of $10\\%$ and $30\\bar{\\%}$ . "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "We divide RQ4 into two sub-questions: (1) Is $\\mathtt{G D e R}$ more robust to outlier perturbation compared to previous data pruning methods? (2) Can GDeR compete with mainstream methods designed to enhance GNN robustness? In practice, following [36], we introduce perturbations to $k\\%$ of the graph samples in the training set by adding Gaussian noise to the node features of the selected graphs. We compare $\\mathtt{G D e R}$ against both data pruning baselines and GNN robustness enhancement baselines. The experimental results are presented in Figure 5, and we observe: ", "page_idx": 8}, {"type": "text", "text": "Obs. $\\tt{\\textcircled{\\div}G D e R}$ is a resource-saving GNN robustness booster. From Figure 5 $(L e f t)$ , we observe that GDeR effectively counters noise perturbation, outperforming the GNN under outlier attacks at both $30\\%$ and $50\\%$ pruning rates. Notably, InfoBatch, which performed competitively in RQ1, suffers a significant performance drop $(2.0\\%\\sim6.1\\%\\downarrow)$ ) in this biased training scenario, which is likely due to its loss magnitude-based sample selection mechanism, inadvertently amplifying the negative impact of high-loss outlier samples on the model. From Figure 5 (Right), we conclude that GDeR performs as well as or better than current robust GNN plugins, and it shows the most significant improvement in accuracy, with increases of $3.6\\%$ and $7.8\\%$ at noise ratios of $5\\%$ and $30\\%$ , respectively. ", "page_idx": 8}, {"type": "text", "text": "4.5 Ablation & Sensitivity Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Ablation Study To evaluate the effectiveness of the different modules in GDeR, we propose three variants: (1) GDeR w/o $\\omega_{e}$ , (2) GDeR w/o $\\omega_{r}$ , and (3) GDeR w/o $\\omega_{b}$ . GDeR w/o $\\omega_{e}$ represents removing $\\omega_{e}$ from Equation (11), with the other two variants defined similarly. We observe from Table 3 that $\\pmb{\\mathrm{\\Sigma}}$ removing any component leads to a performance drop for GDeR, while removing $\\omega_{b}$ in the imbalance scenario or $\\omega_{r}$ in the biased scenario results in the most significant impact; $\\pmb{\\varphi}$ GDeR w/o $\\omega_{e}$ consistently underperforms across all scenarios, indicating that selecting highly representative samples is fundamental to the success of dynamic pruning methods. ", "page_idx": 8}, {"type": "table", "img_path": "O97BzlN9Wh/tmp/ae3c813574c9c833099b85a41679604c529dc3130456b8c4d67bcde128d2f395.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 3: Ablation study on GDeR and its three variants. \u201cImbalance\u201d refers to setting the imbalance ratio to be $\\{1:9\\}$ , and \u201cNoisy\u201d refers to adding $5\\%$ noise to the training set. All metrics are reported under $30\\%$ pruning ratio. ", "page_idx": 8}, {"type": "table", "img_path": "O97BzlN9Wh/tmp/11d7d45a739b1fc557923cc1d64b5b9ea441150d92b12eb9dde61c96a94276ac.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 4: Sensitivity analysis on $K$ . We report the ROCAUC $(\\%)$ and per-epoch time (s) on OGBG-MOLHIV $^+$ GraphGPS. ", "page_idx": 8}, {"type": "text", "text": "Sensitivity and Efficiency Analysis We investigate the impact of $K$ , on the performance and efficiency of GDeR. Specifically, we vary $K\\in\\{1,2,4\\}$ on OGBG-MOLHIV $^+$ GraphGPS and observe changes in performance and per-epoch time. We observe from Table 4 that $K\\,=\\,1$ leads to an under-learning of the hypersphere, resulting in consistently lower performance. While $K=4$ shows a marginal performance gain compared to $K=2$ , for efficiency considerations, we opt for $K=2$ across all experiments. Additionally, we observe that data pruning significantly saves per-epoch time, with $s=20$ resulting in per-epoch times being $40\\%\\sim60\\%$ of those achieved with $s=70$ . ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion & Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we propose the graph training debugging concept and explore soft dataset pruning in the graph learning area for the first time. Particularly, we present a prototype-guided soft pruning method, termed GDeR, which initially establishes a well-modeled graph embedding hypersphere and subsequently samples representative, balanced, and noise-free subsets from this embedding space, debugging and troubleshooting graph processing. In the future, we plan to extend this concept to the CV realm, aiming to expedite the process of image training and provide efficient insights for the development of high-quality visual large-scale models. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Dawei Cheng is supported by the National Natural Science Foundation of China (Grant No. 62102287). Yuxuan Liang is supported by the National Natural Science Foundation of China (No. 62402414), Guangzhou Municipal Science and Technology Project (No. 2023A03J0011), and Guangdong Provincial Key Lab of Integrated Communication, Sensing and Computation for Ubiquitous Internet of Things (No. 2023B1212010007). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Mohammad Motamedi, Nikolay Sakharnykh, and Tim Kaldewey. A data-centric approach for training deep neural networks with less data. arXiv preprint arXiv:2110.03613, 2021.   \n[2] Daochen Zha, Zaid Pervaiz Bhat, Kwei-Herng Lai, Fan Yang, Zhimeng Jiang, Shaochen Zhong, and Xia Hu. Data-centric artificial intelligence: A survey. arXiv preprint arXiv:2303.10158, 2023.   \n[3] Luciano Floridi and Massimo Chiriatti. Gpt-3: Its nature, scope, limits, and consequences. Minds and Machines, 30:681\u2013694, 2020.   \n[4] Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for\" mind\" exploration of large language model society. Advances in Neural Information Processing Systems, 36, 2024.   \n[5] Elias Frantar and Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned in one-shot. In International Conference on Machine Learning, pages 10323\u201310337. PMLR, 2023.   \n[6] Saleh Ashkboos, Maximilian L Croci, Marcelo Gennari do Nascimento, Torsten Hoefler, and James Hensman. Slicegpt: Compress large language models by deleting rows and columns. arXiv preprint arXiv:2401.15024, 2024.   \n[7] Mingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter. A simple and effective pruning approach for large language models. arXiv preprint arXiv:2306.11695, 2023.   \n[8] Victor Sanh, Thomas Wolf, and Alexander Rush. Movement pruning: Adaptive sparsity by fine-tuning. Advances in neural information processing systems, 33:20378\u201320389, 2020.   \n[9] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \n[10] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.   \n[11] Junke Wang, Dongdong Chen, Chong Luo, Xiyang Dai, Lu Yuan, Zuxuan Wu, and Yu-Gang Jiang. Chatvideo: A tracklet-centric multimodal and versatile video understanding system, 2023.   \n[12] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechu Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. Minigpt-v2: large language model as a unified interface for vision-language multi-task learning. arXiv preprint arXiv:2310.09478, 2023.   \n[13] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023.   \n[14] Sihan Chen, Handong Li, Qunbo Wang, Zijia Zhao, Mingzhen Sun, Xinxin Zhu, and Jing Liu. Vast: A vision-audio-subtitle-text omni-modality foundation model and dataset, 2023.   \n[15] Bojan Karla\u0161, David Dao, Matteo Interlandi, Bo Li, Sebastian Schelter, Wentao Wu, and Ce Zhang. Data debugging with shapley importance over end-to-end machine learning pipelines, 2022.   \n[16] Jae-hun Shim, Kyeongbo Kong, and Suk-Ju Kang. Core-set sampling for efficient neural architecture search. arXiv preprint arXiv:2107.06869, 2021.   \n[17] Murad Tukan, Alaa Maalouf, and Dan Feldman. Coresets for near-convex functions. Advances in Neural Information Processing Systems, 33:997\u20131009, 2020.   \n[18] Mansheej Paul, Surya Ganguli, and Gintare Karolina Dziugaite. Deep learning on a data diet: Finding important examples early in training. Advances in Neural Information Processing Systems, 34:20596\u201320607, 2021.   \n[19] Guibin Zhang, Kun Wang, Wei Huang, Yanwei Yue, Yang Wang, Roger Zimmermann, Aojun Zhou, Dawei Cheng, Jin Zeng, and Yuxuan Liang. Graph lottery ticket automated. In The Twelfth International Conference on Learning Representations, 2024.   \n[20] Kun Wang, Yuxuan Liang, Xinglin Li, Guohao Li, Bernard Ghanem, Roger Zimmermann, Huahui Yi, Yudong Zhang, Yang Wang, et al. Brave the wind and the waves: Discovering robust and generalizable graph lottery tickets. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023.   \n[21] Guibin Zhang, Yanwei Yue, Kun Wang, Junfeng Fang, Yongduo Sui, Kai Wang, Yuxuan Liang, Dawei Cheng, Shirui Pan, and Tianlong Chen. Two heads are better than one: Boosting graph sparse training via semantic and topological awareness, 2024.   \n[22] Ravi S Raju, Kyle Daruwalla, and Mikko Lipasti. Accelerating deep learning with dynamic data pruning. arXiv preprint arXiv:2111.12621, 2021.   \n[23] Ziheng Qin, Kai Wang, Zangwei Zheng, Jianyang Gu, Xiangyu Peng, Zhaopan Xu, Daquan Zhou, Lei Shang, Baigui Sun, Xuansong Xie, et al. Infobatch: Lossless training speed up by unbiased dynamic data pruning. arXiv preprint arXiv:2303.04947, 2023.   \n[24] Bo Zhao and Hakan Bilen. Dataset condensation with distribution matching. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 6514\u20136523, 2023.   \n[25] Kai Wang, Bo Zhao, Xiangyu Peng, Zheng Zhu, Shuo Yang, Shuo Wang, Guan Huang, Hakan Bilen, Xinchao Wang, and Yang You. Cafe: Learning to condense dataset by aligning features. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12196\u201312205, 2022.   \n[26] George Cazenavette, Tongzhou Wang, Antonio Torralba, Alexei A Efros, and Jun-Yan Zhu. Dataset distillation by matching training trajectories. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4750\u20134759, 2022.   \n[27] Timothy Nguyen, Roman Novak, Lechao Xiao, and Jaehoon Lee. Dataset distillation with infinitely wide convolutional networks. Advances in Neural Information Processing Systems, 34:5186\u20135198, 2021.   \n[28] Xinglin Li, Kun Wang, Hanhui Deng, Yuxuan Liang, and Di Wu. Attend who is weak: Enhancing graph condensation via cross-free adversarial training. arXiv preprint arXiv:2311.15772, 2023.   \n[29] Yuchen Zhang, Tianle Zhang, Kai Wang, Ziyao Guo, Yuxuan Liang, Xavier Bresson, Wei Jin, and Yang You. Navigating complexity: Toward lossless graph condensation via expanding window matching. In Forty-first International Conference on Machine Learning, 2024.   \n[30] Tianle Zhang, Yuchen Zhang, Kun Wang, Kai Wang, Beining Yang, Kaipeng Zhang, Wenqi Shao, Ping Liu, Joey Tianyi Zhou, and Yang You. Two trades is not baffled: Condense graph via crafting rational gradient matching. arXiv preprint arXiv:2402.04924, 2024.   \n[31] Sariel Har-Peled and Soham Mazumdar. On coresets for k-means and $\\mathbf{k}$ -median clustering. In Proceedings of the thirty-sixth annual ACM symposium on Theory of computing, pages 291\u2013300, 2004.   \n[32] Ke Chen. On coresets for k-median and k-means clustering in metric and euclidean spaces and their applications. SIAM Journal on Computing, 39(3):923\u2013947, 2009.   \n[33] Mariya Toneva, Alessandro Sordoni, Remi Tachet des Combes, Adam Trischler, Yoshua Bengio, and Geoffrey J Gordon. An empirical study of example forgetting during deep neural network learning. arXiv preprint arXiv:1812.05159, 2018.   \n[34] Bo Zhao, Konda Reddy Mopuri, and Hakan Bilen. Dataset condensation with gradient matching. arXiv preprint arXiv:2006.05929, 2020.   \n[35] Yao Lu, Xuguang Chen, Yuchen Zhang, Jianyang Gu, Tianle Zhang, Yifan Zhang, Xiaoniu Yang, Qi Xuan, Kai Wang, and Yang You. Can pre-trained models assist in dataset distillation? arXiv preprint arXiv:2310.03295, 2023.   \n[36] Zenan Li, Qitian Wu, Fan Nie, and Junchi Yan. Graphde: A generative framework for debiased learning and out-of-distribution detection on graphs. Advances in Neural Information Processing Systems, 35:30277\u201330290, 2022.   \n[37] Dongmin Park, Seola Choi, Doyoung Kim, Hwanjun Song, and Jae-Gil Lee. Robust data pruning under label noise via maximizing re-labeling accuracy. Advances in Neural Information Processing Systems, 36, 2024.   \n[38] Asim Kumar Debnath, Rosa L Lopez de Compadre, Gargi Debnath, Alan J Shusterman, and Corwin Hansch. Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. correlation with molecular orbital energies and hydrophobicity. J. Med. Chem., 34(2):786\u2013797, 1991.   \n[39] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks, 2016.   \n[40] Yu Wang, Yuying Zhao, Neil Shah, and Tyler Derr. Imbalanced graph classification via graph-of-graph neural networks. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management, pages 2067\u20132076, 2022.   \n[41] Zhixun Li, Yushun Dong, Qiang Liu, and Jeffrey Xu Yu. Rethinking fair graph neural networks from re-balancing. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 1736\u20131745, 2024.   \n[42] Sercan O Arik and Tomas Pfister. Protoattend: Attention-based prototypical learning. Journal of Machine Learning Research, 21(210):1\u201335, 2020.   \n[43] Junnan Li, Pan Zhou, Caiming Xiong, and Steven CH Hoi. Prototypical contrastive learning of unsupervised representations. arXiv preprint arXiv:2005.04966, 2020.   \n[44] Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alexei Efros, and Trevor Darrell. Cycada: Cycle-consistent adversarial domain adaptation. In International conference on machine learning, pages 1989\u20131998. Pmlr, 2018.   \n[45] Yi-Hsuan Tsai, Wei-Chih Hung, Samuel Schulter, Kihyuk Sohn, Ming-Hsuan Yang, and Manmohan Chandraker. Learning to adapt structured output space for semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7472\u20137481, 2018.   \n[46] Shirui Pan and Xingquan Zhu. Graph classification with imbalanced class distributions and noise. In IJCAI, pages 1586\u20131592, 2013.   \n[47] Zhenyu Hou, Xiao Liu, Yukuo Cen, Yuxiao Dong, Hongxia Yang, Chunjie Wang, and Jie Tang. Graphmae: Self-supervised masked graph autoencoders. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 594\u2013604, 2022.   \n[48] Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph contrastive learning with augmentations. Advances in neural information processing systems, 33:5812\u20135823, 2020.   \n[49] Yuning You, Tianlong Chen, Yang Shen, and Zhangyang Wang. Graph contrastive learning automated. In International Conference on Machine Learning, pages 12121\u201312132. PMLR, 2021.   \n[50] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. A comprehensive survey on graph neural networks. IEEE transactions on neural networks and learning systems, 32(1):4\u201324, 2020.   \n[51] Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li, and Maosong Sun. Graph neural networks: A review of methods and applications. AI open, 1:57\u201381, 2020.   \n[52] Shunxin Xiao, Shiping Wang, Yuanfei Dai, and Wenzhong Guo. Graph neural networks in node classification: survey and evaluation. Machine Vision and Applications, 33(1):4, 2022.   \n[53] Muhan Zhang and Yixin Chen. Link prediction based on graph neural networks. In Proceedings of NIPS, 2018.   \n[54] Chuang Liu, Yibing Zhan, Jia Wu, Chang Li, Bo Du, Wenbin Hu, Tongliang Liu, and Dacheng Tao. Graph pooling for graph neural networks: Progress, challenges, and opportunities. arXiv preprint arXiv:2204.07321, 2022.   \n[55] Kun Wang, Yuxuan Liang, Pengkun Wang, Xu Wang, Pengfei Gu, Junfeng Fang, and Yang Wang. Searching lottery tickets in graph neural networks: A dual perspective. In The Eleventh International Conference on Learning Representations, 2022.   \n[56] Dingshuo Chen, Yanqiao Zhu, Jieyu Zhang, Yuanqi Du, Zhixun Li, Qiang Liu, Shu Wu, and Liang Wang. Uncovering neural scaling laws in molecular representation learning. Advances in Neural Information Processing Systems, 36, 2024.   \n[57] Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 1885\u20131894. JMLR. org, 2017.   \n[58] Ravi S Raju, Kyle Daruwalla, and Mikko Lipasti. Accelerating deep learning with dynamic data pruning, 2021.   \n[59] Dingshuo Chen, Zhixun Li, Yuyan Ni, Guibin Zhang, Ding Wang, Qiang Liu, Shu Wu, Jeffrey Xu Yu, and Liang Wang. Beyond efficiency: Molecular data pruning for enhanced generalization. arXiv preprint arXiv:2409.01081, 2024.   \n[60] Yongduo Sui, Xiang Wang, Tianlong Chen, Xiangnan He, and Tat-Seng Chua. Inductive lottery ticket learning for graph neural networks. 2021.   \n[61] Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Viktor Prasanna. Graphsaint: Graph sampling based inductive learning method. arXiv preprint arXiv:1907.04931, 2019.   \n[62] Yifan Zhang, Bingyi Kang, Bryan Hooi, Shuicheng Yan, and Jiashi Feng. Deep long-tailed learning: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023.   \n[63] Haibo He and Edwardo A Garcia. Learning from imbalanced data. IEEE Transactions on knowledge and data engineering, 21(9):1263\u20131284, 2009.   \n[64] Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and W Philip Kegelmeyer. Smote: synthetic minority over-sampling technique. Journal of artificial intelligence research, 16:321\u2013 357, 2002.   \n[65] Bingyi Kang, Saining Xie, Marcus Rohrbach, Zhicheng Yan, Albert Gordo, Jiashi Feng, and Yannis Kalantidis. Decoupling representation and classifier for long-tailed recognition. arXiv preprint arXiv:1910.09217, 2019.   \n[66] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision, pages 2980\u20132988, 2017.   \n[67] Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced loss based on effective number of samples. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9268\u20139277, 2019.   \n[68] Jingru Tan, Changbao Wang, Buyu Li, Quanquan Li, Wanli Ouyang, Changqing Yin, and Junjie Yan. Equalization loss for long-tailed object recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11662\u201311671, 2020.   \n[69] Aditya Krishna Menon, Sadeep Jayasumana, Ankit Singh Rawat, Himanshu Jain, Andreas Veit, and Sanjiv Kumar. Long-tail learning via logit adjustment. arXiv preprint arXiv:2007.07314, 2020.   \n[70] Min Shi, Yufei Tang, Xingquan Zhu, David Wilson, and Jianxun Liu. Multi-class imbalanced graph convolutional network learning. In IJCAI, 2020.   \n[71] Tianxiang Zhao, Xiang Zhang, and Suhang Wang. Graphsmote: Imbalanced node classification on graphs with graph neural networks. In WSDM, 2021.   \n[72] Liang Qu, Huaisheng Zhu, Ruiqi Zheng, Yuhui Shi, and Hongzhi Yin. Imgagn: Imbalanced network embedding via generative adversarial graph networks. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pages 1390\u20131398, 2021.   \n[73] Jiaqi Ma, Shuangrui Ding, and Qiaozhu Mei. Towards more practical adversarial attacks on graph neural networks. Advances in neural information processing systems, 33:4756\u20134766, 2020.   \n[74] Dingyuan Zhu, Ziwei Zhang, Peng Cui, and Wenwu Zhu. Robust graph convolutional networks against adversarial attacks. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, pages 1399\u20131407, 2019.   \n[75] Zhixun Li, Xin Sun, Yifan Luo, Yanqiao Zhu, Dingshuo Chen, Yingtao Luo, Xiangxin Zhou, Qiang Liu, Shu Wu, Liang Wang, et al. Gslb: The graph structure learning benchmark. Advances in Neural Information Processing Systems, 36, 2024.   \n[76] Yizheng Chen, Yacin Nadji, Athanasios Kountouras, Fabian Monrose, Roberto Perdisci, Manos Antonakakis, and Nikolaos Vasiloglou. Practical attacks against graph-based clustering. In Proceedings of the 2017 ACM SIGSAC conference on computer and communications security, pages 1125\u20131142, 2017.   \n[77] Binghui Wang, Meng Pang, and Yun Dong. Turning strengths into weaknesses: A certified robustness inspired attack framework against graph neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16394\u201316403, 2023.   \n[78] Han Yang, Binghui Wang, Jinyuan Jia, et al. Graphguard: Provably robust graph classification against adversarial attacks. In The Twelfth International Conference on Learning Representations, 2023.   \n[79] Zaixi Zhang, Jinyuan Jia, Binghui Wang, and Neil Zhenqiang Gong. Backdoor attacks to graph neural networks. In Proceedings of the 26th ACM Symposium on Access Control Models and Technologies, pages 15\u201326, 2021.   \n[80] Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In Proceedings of ICML, 2020.   \n[81] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. Advances in neural information processing systems, 33:18661\u201318673, 2020.   \n[82] Haodong Lu, Dong Gong, Shuo Wang, Jason Xue, Lina Yao, and Kristen Moore. Learning with mixture of prototypes for out-of-distribution detection. arXiv preprint arXiv:2402.02653, 2024.   \n[83] Xuefeng Du, Gabriel Gozum, Yifei Ming, and Yixuan Li. Siren: Shaping representations for detecting out-of-distribution objects. Advances in Neural Information Processing Systems, 35:20434\u201320449, 2022.   \n[84] Vikash Sehwag, Mung Chiang, and Prateek Mittal. Ssd: A unified framework for selfsupervised outlier detection. arXiv preprint arXiv:2103.12051, 2021.   \n[85] Amro Abbas, Evgenia Rusak, Kushal Tirumala, Wieland Brendel, Kamalika Chaudhuri, and Ari S Morcos. Effective pruning of web-scale datasets based on complexity of concept clusters. arXiv preprint arXiv:2401.04578, 2024.   \n[86] Hichem Frigui and Olfa Nasraoui. Unsupervised learning of prototypes and attribute weights. Pattern recognition, 37(3):567\u2013581, 2004.   \n[87] Jeffrey J Sutherland, Lee A O\u2019brien, and Donald F Weaver. Spline-fitting with a genetic algorithm: A method for developing classification structure- activity relationships. J Chem Inform Comput Sci, 43(6), 2003.   \n[88] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. arXiv preprint arXiv:2005.00687, 2020.   \n[89] John J Irwin, Teague Sterling, Michael M Mysinger, Erin S Bolstad, and Ryan G Coleman. ZINC: A free tool to discover chemistry for biology. Journal of Chemical Information and Modeling, 52(7):1757\u20131768, 2012.   \n[90] Ladislav Ramp\u00e1\u0161ek, Michael Galkin, Vijay Prakash Dwivedi, Anh Tuan Luu, Guy Wolf, and Dominique Beaini. Recipe for a general, powerful, scalable graph transformer. Advances in Neural Information Processing Systems, 35:14501\u201314515, 2022.   \n[91] Thomas N. Kipf and Max Welling. Semi-Supervised Classification with Graph Convolutional Networks. In Proceedings of the 5th International Conference on Learning Representations, 2017.   \n[92] Gabriele Corso, Luca Cavalleri, Dominique Beaini, Pietro Li\u00f2, and Petar Velic\u02c7kovic\u00b4. Principal neighbourhood aggregation for graph nets. Advances in Neural Information Processing Systems, 33:13260\u201313271, 2020.   \n[93] Sharat Agarwal, Himanshu Arora, Saket Anand, and Chetan Arora. Contextual diversity for active learning. In ECCV, pages 137\u2013153. Springer, 2020.   \n[94] Max Welling. Herding dynamical weights to learn. In ICMLg, pages 1121\u20131128, 2009.   \n[95] Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set approach. In ICLR, 2018.   \n[96] Cody Coleman, Christopher Yeh, Stephen Mussmann, Baharan Mirzasoleiman, Peter Bailis, Percy Liang, Jure Leskovec, and Matei Zaharia. Selection via proxy: Efficient data selection for deep learning. In ICLR, 2019.   \n[97] Melanie Ducoffe and Frederic Precioso. Adversarial active learning for deep networks: a margin based approach. arXiv preprint arXiv:1802.09841, 2018.   \n[98] Baharan Mirzasoleiman, Jeff Bilmes, and Jure Leskovec. Coresets for data-efficient training of machine learning models. In ICML. PMLR, 2020. [99] Krishnateja Killamsetty, Durga Sivasubramanian, Ganesh Ramakrishnan, and Rishabh Iyer. Glister: Generalization based data subset selection for efficient and robust learning. In Proceedings of the AAAI Conference on Artificial Intelligence, 2021.   \n[100] Shuo Yang, Zeke Xie, Hanyu Peng, Min Xu, Mingming Sun, and Ping Li. Dataset pruning: Reducing training data by examining generalization influence. In The Eleventh International Conference on Learning Representations, 2023.   \n[101] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020.   \n[102] Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, and Jure Leskovec. Strategies for pre-training graph neural networks. arXiv preprint arXiv:1905.12265, 2019.   \n[103] Michael Zhu and Suyog Gupta. To prune, or not to prune: exploring the efficacy of pruning for model compression. arXiv preprint arXiv:1710.01878, 2017.   \n[104] Teague Sterling and John J Irwin. Zinc 15\u2013ligand discovery for everyone. Journal of Chemical Information and Modeling, 55(11):2324\u20132337, 2015.   \n[105] Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S Pappu, Karl Leswing, and Vijay Pande. MoleculeNet: A benchmark for molecular machine learning. Chemical Science, 9(2):513\u2013530, 2018.   \n[106] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-parametric instance discrimination. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3733\u20133742, 2018.   \n[107] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.   \n[108] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of CVPR, 2020.   \n[109] Ming Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, and Yaliang Li. Simple and deep graph convolutional networks. In International conference on machine learning, pages 1725\u20131735. PMLR, 2020.   \n[110] Joshua David Robinson, Ching-Yao Chuang, Suvrit Sra, and Stefanie Jegelka. Contrastive learning with hard negative samples. In International Conference on Learning Representations, 2021.   \n[111] Mahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bojanowski, Armand Joulin, Nicolas Ballas, and Michael Rabbat. Semi-supervised learning of visual features by non-parametrically predicting view assignments with support samples. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8443\u20138452, 2021.   \n[112] Feng Wang and Huaping Liu. Understanding the behaviour of contrastive loss. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2495\u20132504, 2021.   \n[113] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsupervised learning of visual features. In Proceedings of the European conference on computer vision (ECCV), pages 132\u2013149, 2018.   \n[114] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. Advances in neural information processing systems, 30, 2017. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "[115] Junnan Li, Pan Zhou, Caiming Xiong, and Steven Hoi. Prototypical contrastive learning of unsupervised representations. In International Conference on Learning Representations, 2021. ", "page_idx": 16}, {"type": "text", "text": "Appendices ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "A Notations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We conclude the commonly used notations throughout the manuscript in Table 5. ", "page_idx": 16}, {"type": "text", "text": "Table 5: The notations that are commonly used in the manuscript. ", "page_idx": 16}, {"type": "table", "img_path": "O97BzlN9Wh/tmp/17d8fcdddd27517dab430c5d11ff4b575b477470593a7117df2c53106d769d57.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "B Experimental Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "B.1 Dataset Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The graph dataset details are summarized in Table 6. ", "page_idx": 16}, {"type": "table", "img_path": "O97BzlN9Wh/tmp/868175f1720987b9831f09e9adf0668f4defebeb03795418737398603e73cbd5.jpg", "table_caption": ["Table 6: Graph datasets statistics. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "B.2 Backbone Settings ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We choose three representative GNNs, including one classic message-passing network GCN [91], one classical graph classification backbone PNA [92] and a graph transformer backbone GraphGPS [90]. For GCN, we simply set layer_num $\\mathrm{~\\it~\\Omega~}=\\mathrm{~\\it~3~}$ and hidden_dim $\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad128$ . For PNA, we set layer_num $\\mathrm{~\\ensuremath~{~\\mu~=~4~}~}$ , and hidden_dim $\\mathrm{~\\mu~=~64~}$ , edge_dim $\\mathit{\\Theta}=\\ 16$ . The rest configurations are the same as provided by [92] (https://github.com/lukecavabarrett/pna/blob/master/ models/pytorch_geometric/example.py). For GraphGPS, we uniformly set hidden_dim $=$ $\\mathtt{j4},\\mathtt{p e\\_d i m}=8$ and utilize random walk encoding, performer [101], and GINE [102] as the positional encoding, attention module and the local convolutional module, respectively. The rest configurations are the same as provided by the PyTorch library (https://github.com/pyg-team/pytorch_ geometric/blob/master/examples/graph_gps.py). All the experiments are conducted on NVIDIA Tesla V100 (32GB GPU), using PyTorch and PyTorch Geometric framework. ", "page_idx": 17}, {"type": "text", "text": "B.3 Pruning Baselines ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "As for static pruning methods, we first introduce Hard Random, which conducts a random sample selection before training. Influence [57] and EL2N [18] are two classical static pruning methods that prune samples based on Influence-score and EL2N-score, respectively. DP [100] conducts pruning with consideration of generalization. Following the methodology of [23], we introduce a total of 13 static data pruning methods. These methods select a core set of data via predefined score functions or heuristic knowledge. Additionally, we introduce three dynamic pruning methods, including $\\epsilon$ -greedy [58], UCB [58], and InfoBatch [23]. Following [58, 23], we also introduce the dynamic pruning baseline, termed Soft Random, which conducts random selection in each epoch. ", "page_idx": 17}, {"type": "text", "text": "B.4 Metrics ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "For MUTAG and DHFR, the metrics used vary across different scenarios. In the normal (Section 4.2) and biased (Section 4.4) scenarios, we use accuracy. However, in the imbalanced scenario (Section 4.3), accuracy does not faithfully reflect the performance for the minority group. Following previous works in imbalanced classification [71], we choose to use F1-macro, which computes the accuracy independently for each class and then takes the average, treating different classes equally. For OGBG-MOLHIV and OGBG-MOLPCBA, we use ROC-AUC and Average Precision (AP), following [88]. ", "page_idx": 17}, {"type": "text", "text": "B.5 Scheduler Function ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "$\\Psi(t)$ and $\\tilde{\\Psi}(t)$ are scheduler functions that determine the proportions of samples in $\\boldsymbol{\\chi}_{t+1}$ originating from $\\textstyle{\\mathcal{X}}_{t}$ and $\\tilde{\\mathcal{X}}_{t}$ , respectively. For simplicity, we adopt the Inverse Power function [103]: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\Psi(t)=|\\mathcal{X}_{t}|\\cdot\\varsigma\\left(1-\\frac{t}{T}\\right)^{\\kappa},\\:\\:\\tilde{\\Psi}(t)=|\\mathcal{X}_{t}|-\\Psi(t)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\varsigma$ denotes the initial ratio and $\\kappa$ is the decay factor controlling the rate at which the ratio decreases over intervals. In practice, we uniformly set $\\varsigma=0.7$ and $\\kappa=2$ . ", "page_idx": 17}, {"type": "text", "text": "C Algorithm Workflow ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The algorithm framework is presented in Algo. 1. ", "page_idx": 17}, {"type": "text", "text": "D Extension of GDeR ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we will explain how to extend GDeR beyond traditional graph classification tasks to more complex scenarios like graph regression and graph pre-training. As noted in Section 3.5, tasks such as graph pre-training do not have ground truth class indices, making direct application of GDeR, which relies on true class labels in Equations (6) and (7), infeasible. ", "page_idx": 17}, {"type": "text", "text": "AD usrtirnagi gphrtoftoortwypaer da lalpocpartoiaocn,h  iwse t uo sae stshieg np $C$ bvaibritluitayl  dcilsatsrisbeus,t ieoanc phr owviitdh $K$ bpyr Eotqoutaytpioens $\\mathbf{P}^{c}=\\{\\mathbf{p}_{k}^{c}\\}_{k=1}^{K}$ ", "page_idx": 17}, {"type": "text", "text": "Input :Graph datasets D = {zi}i=1 = {(Gi, Yi)}i=1, the number of epochs $T$ , GNN encoder $f_{\\theta}$ , feature projector $g_{\\phi}$ , ", "page_idx": 18}, {"type": "text", "text": "Initialized M prototypes {p(kC)}iM=1 for class C   \nfor epoch $t\\gets1$ to do $/*$ Extract graph-level embedding and Projection \\*/ $\\chi_{t}\\gets$ current training set. for sample index $i\\gets1$ to $|\\mathcal{X}_{t}|$ do Compute graph embedding $\\mathbf{h}_{i}\\leftarrow g_{\\theta}(\\mathcal{G}_{i})$ . Project graph embedding onto hypersphere by $\\mathbf{z}_{i}=\\mathbf{z}^{\\prime}/\\vert\\vert\\mathbf{z}^{\\prime}\\vert\\vert_{2},\\mathbf{z}^{\\prime}=g_{\\theta}(\\mathbf{h}_{i})$ . end Determine which prototype cluster $\\chi_{c}$ each graph sample $\\mathbf{z}_{i}$ corresponds to; \u25b7 Eq. 5 $/*$ Formatting sampling distribution \\*/ Calculate the outlier score $\\omega_{\\mathrm{r}}({\\bf z}_{i})$ by prototype-based Mahalanobis distance; \u25b7 Eq. 8 Calculate the familiarity score $\\omega_{\\mathrm{e}}({\\bf z}_{i})$ based on prorotype-sample distance; \u25b7 Eq. 9 Calculate the balancing distribution $\\omega_{\\mathrm{b}}$ based on cluster volume; \u25b7 Eq. 10 Formulate sampling distribution \u03c9(zi) = (\u03c9r\u03c3 (zi)+\u03c9\u03f5e) \u00b7((zi\u03c9)\u03c3 (zi)+\u03f5); \u25b7 Eq. 11 $/*$ Dataset sampling \\*/ Initialize the sample set for the $(t+1)$ -th epoch $\\mathcal{X}_{t+1}\\gets\\emptyset$ $/*$ Sample from currently remained set \\*/ $\\mathcal{X}_{t+1}\\gets\\mathcal{X}_{t+1}+S\\left(\\mathcal{X}_{t},\\mathcal{P}^{(t-1)}(\\mathbf{z}),\\Psi(t)\\right)$ $/*$ Sample from currently pruned set \\*/ $\\mathcal{X}_{t+1}\\gets\\mathcal{X}_{t+1}+S\\left(\\tilde{\\mathcal{X}}_{t},\\mathcal{P}^{(t-1)}(\\mathbf{z}),\\tilde{\\Psi}(t)\\right)$ /\\* Standard GNN training \\*/ Compute loss $\\mathcal{L}_{\\mathtt{G D e R}}=\\mathcal{L}_{\\mathtt{t a s k}}+\\lambda_{1}\\cdot\\mathcal{L}_{\\mathtt{c o m p}}+\\lambda_{2}\\cdot\\mathcal{L}_{\\mathtt{s e p a}}$ ; \u25b7 Eq. 13 Backpropagate to update the GNN model $f_{\\theta}$ , projector $g_{\\phi}$ , and prototypes.   \nend ", "page_idx": 18}, {"type": "text", "text": "", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "the class of each graph sample: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{y}_{i}=\\operatorname*{argmax}_{c}p(y_{i}=c|\\mathbf{z}_{i},\\{\\mathbf{P}^{j},\\kappa\\}_{j=1}^{C}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We then substitute $\\tilde{y}_{i}$ for $y_{i}$ in Equations (6) and (7), essentially emphasizing that the sample $\\mathbf{z}_{i}$ should cluster tightly around its assigned prototype cluster $\\chi_{\\tilde{y}_{i}}$ and remain distant from other clusters. However, this approach is prone to error accumulation: if a sample is initially misclassified, $\\mathcal{L}_{\\mathrm{comp}}$ and $\\mathcal{L}_{\\mathrm{sepa}}$ will erroneously encourage it to continue moving in the wrong direction. To address this issue, we draw inspiration from previous practices in prototypical contrastive learning and leverage the prototypical contrastive loss [43]: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{contra}}=\\sum_{i=1}^{|\\mathcal{X}_{t}|}-\\biggl(\\frac{1}{C}\\sum_{c=1}^{C}\\log\\frac{\\sum_{k=1}^{K}\\exp(\\mathbf{z}_{i}\\cdot\\mathbf{p}_{s}^{c}/\\phi_{s}^{c})}{\\sum_{j=0}^{r}\\sum_{k=1}^{K}\\exp(\\mathbf{z}_{i}\\cdot\\mathbf{p}_{j}^{c}/\\phi_{j}^{m})}\\biggr),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\phi$ calculates the concentration level of the feature distribution around a prototype as defined in [43]. Equation (16) has been shown to learn cluster distributions with high mutual information with ground truth labels in unsupervised settings. It encourages samples to migrate between clusters by measuring a concentration-weighted contrastive signal, rather than accumulating current errors. Thus, the overall objective of GDeR becomes: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathtt{G D e R}}^{\\prime}=\\mathcal{L}_{\\mathtt{t a s k}}+\\lambda_{1}\\cdot\\mathcal{L}_{\\mathrm{comp}}+\\lambda_{2}\\cdot\\mathcal{L}_{\\mathrm{sepa}}+\\lambda_{3}\\cdot\\mathcal{L}_{\\mathrm{contra}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We applied this setting when extending GDeR to pre-training with GraphMAE on the ZINC dataset, with the experimental results in Table 7. ", "page_idx": 18}, {"type": "text", "text": "Table 7: Graph pre-training performance of GDeR on GraphMAE $[47]\\!\\!+\\!\\!Z\\mathrm{INC}15$ [104]. Following [47], the model is first pre-trained in 2 million unlabeled molecules sampled from the ZINC15, and then finetuned in 3 classification benchmark datasets contained in MoleculeNet [105]. ", "page_idx": 19}, {"type": "table", "img_path": "O97BzlN9Wh/tmp/5548feb46d0b02747252ae7f3b52cd35de63ce2dced4759cf559046cae85bac9.jpg", "table_caption": [], "table_footnote": ["The original pre-training time is $\\overline{{4.8\\,\\mathrm{h}}}$ . "], "page_idx": 19}, {"type": "table", "img_path": "O97BzlN9Wh/tmp/063c7113a29b7b72a9c15632e6b6898265bf8f0df332cd56fa4e897deab91a24.jpg", "table_caption": ["Table 8: Performance comparison to state-of-the-art dataset pruning methods. All methods are trained using PNA, and the reported metrics represent the average of twenty random runs and different dataset splits. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "E Additional Experimental Results ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We place additional results about MUTAG and DHFR in Table 8, and the GraphMAE $+.$ ZINC pre-training results in Table 7. ", "page_idx": 19}, {"type": "text", "text": "F Supplementary Related Work ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Constrastive Learning and Prototypical learning Contrastive representation learning methods consider each sample as a unique class, aligning multiple views of the same input while distancing other samples. This significantly improves the discriminative power of the learned representations, allowing these methods to excel in learning robust feature representations across unsupervised [106, 107, 108, 109, 110], semi-supervised [111], and supervised settings [81]. The foundational properties and effectiveness of contrastive loss within hyperspherical space have been extensively studied [80, 112]. Other approaches focus on learning feature representations by modeling the relationships between samples and cluster centroids [113] or prototypes [114]. Building on contrastive learning, [115] incorporates prototypical learning, adding a contrastive mechanism between samples and prototypes obtained through offline clustering. PALM [82] utilizes prorotypical learning for out-of-distribution (OOD) identification, which automatically identifies and dynamically updates prototypes, and assigns each sample to a subset of prototypes via reciprocal neighbor soft assignment weights. However, all these methods are not conducive to a more lightweight training burden, and our method is the first attempt at leveraging prototype learning for soft data pruning. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: In this paper, we introduce a novel soft pruning strategy and we claim the contributions and scope in the abstract and introduction sections (See Abstract and Introduction Section). ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: In this work, we systematically discuss the limitations of our research and outline directions for future work (See Introduction Section). ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: This paper does not include experimental results related to theoretical aspects. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We provide the code necessary for replicating the studies described in this paper via an anonymous link, and we detail the experimental setup for the replication in the article itself (See Appendix). ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 22}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: For the datasets disclosed in the article, we have provided information regarding their sources and origins (See Appendix). ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 23}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: we have specified all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results (See Appendix B). ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 23}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: In this paper, we have reported the standard deviation of the experiments (See Experiments). ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 23}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 24}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: In this paper, we provide detailed information about the experimental resources, including GPU configurations used in our studies (See Appendix). ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The study presented in this paper conforms to the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 24}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We have provided the societal impacts of the work (See Introduction). Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 24}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 25}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: This paper does not address issues related to this aspect. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 25}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: All creators and original owners of the assets used in our paper, such as code, data, and models, have been properly credited. We have explicitly mentioned the licenses and terms of use for each asset and have ensured full compliance with these terms throughout our research. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The research presented in this paper is not concerned with new assets. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 26}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: This paper does not involve experiments or research related to human subjects. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 26}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: This paper does not address potential risks incurred by study participants. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 26}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}]