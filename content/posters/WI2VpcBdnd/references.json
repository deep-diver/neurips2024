{"references": [{"fullname_first_author": "Tongzhou Wang", "paper_title": "Dataset distillation", "publication_date": "2018-11-10", "reason": "This paper introduces the concept of dataset distillation, the core topic of the current paper."}, {"fullname_first_author": "Timothy Nguyen", "paper_title": "Dataset distillation with infinitely wide convolutional networks", "publication_date": "2021-12-01", "reason": "This paper is a significant prior work that uses KRR and deep neural networks for dataset distillation, providing a strong foundation for the current paper's work."}, {"fullname_first_author": "Zachary Izzo", "paper_title": "A theoretical study of dataset distillation", "publication_date": "2023-12-01", "reason": "This paper provides a theoretical analysis of dataset distillation, which is directly compared and extended by the current paper."}, {"fullname_first_author": "Alaa Maalouf", "paper_title": "On the size and approximation error of distilled sets", "publication_date": "2023-05-14", "reason": "This paper provides a theoretical analysis of the size of distilled datasets and their approximation error, which is directly relevant to the current paper's focus on efficient dataset distillation."}, {"fullname_first_author": "Noel Loo", "paper_title": "Efficient dataset distillation with convexified implicit gradients", "publication_date": "2023-07-01", "reason": "This paper proposes a practical algorithm for dataset distillation, which is compared against in the current paper's experimental evaluation."}]}