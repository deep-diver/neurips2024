[{"type": "text", "text": "TextGraphBART: Unifying Graph and Text with Structure Token ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 We propose a novel encoding method called \u201cStructure Token\u201d to unify the pro  \n2 cessing and generation of both graphs and texts with a single transformer-based   \n3 model. This method allows graphs with text labels to be generated by a series   \n4 of tokens, enabling both graph and text data to be handled interchangeably. By   \n5 utilizing structure tokens, our model learns a unified representation, enhancing   \n6 the ability to process diverse data without requiring extra modules or models.   \n7 Additionally, the model can be trained like most transformer models with simply   \n8 cross-entropy loss. To demonstrate the effectiveness of our method, we introduce a   \n9 pre-training scheme inspired by mBART but adapted to leverage structure tokens.   \n10 Our model, named TextGraphBART, uses the same architecture as normal Trans  \n11 former Encoder-Decoder models with small modifications on the input and output   \n12 to accommodate structure tokens. The evaluations show that this approach achieves   \n13 comparable results against baseline models of similar sizes on both text-to-graph   \n14 and graph-to-text generation tasks, without needing specialized loss functions or   \n15 sampling techniques. These findings suggest that our approach can effectively   \n16 bridge the gap between textual and structural data representations, and the design   \n17 of encoding method could offer a new direction for future improvement. ", "page_idx": 0}, {"type": "text", "text": "18 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "19 Transformer layers have been proven to work well in several domains beyond text, like audio, image,   \n20 and even multi-modal data. Some research has also shown that with careful design, transformer layers   \n21 can extract features from graph data[30, 16]. Graph is a common data structure for representing   \n22 concepts and relationships. In this work, we focus on a specific type, named text graph, where   \n23 the concepts and relationships are expressed as texts, such as knowledge graphs and parsing trees.   \n24 Learning vector representations and generating new text graphs are two essential aspects of text   \n25 graphs in machine learning. Since texts can be viewed as a chain of words or characters, the text   \n26 graph then becomes a nested graph. The complexity of handling such a nested graph leads to two   \nmajor approaches for generating text graphs.   \n28 The first strand is the multi-stage approach which generates concepts, relationships, and texts in   \n29 different steps [22, 12]. The process usually involves multiple models that generate different parts   \n30 of the text graph. For example, Grapher [22] uses the T5 [24] pre-trained model to generate all the   \n31 concepts and then uses a relation extraction model to predict the relationships between every two   \n32 concepts. This approach requires the models to generate a complete graph despite the edge sparsity   \n33 of the target text graph. Therefore, the model includes a special \u201cno-relation\" relationship and turns   \n34 every graph into a complete graph thus requiring extra predictions. Since the relation extraction is ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "image", "img_path": "JVtwC9RzlI/tmp/1c6b3a9b501ab1e43b52f15a48aab252fccc03a40fac38b38ba5ff90d5be70b3.jpg", "img_caption": ["(a) Model with structure token "], "img_footnote": [], "page_idx": 1}, {"type": "image", "img_path": "JVtwC9RzlI/tmp/4665bcea77c968f084f28ea21b6cd2495c1426def33946232abafc48d9475177.jpg", "img_caption": ["(b) Autoregressive decoding "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Overview of the proposed structure token approach. (1a) The model takes the input text graph (left) and the partially generated sub-graph (right) and then generates a new structure token. Each structure token contains a (sub-)word token with the locational information of that word token in the text graph. (1b) An example of autoregressive decoding with structure token. The procedure is mostly the same as normal text decoding with Transformer model. ", "page_idx": 1}, {"type": "text", "text": "35 done on every two concepts, the model does not consider multi-hop relations. Moreover, the model   \n36 cannot handle the case where two concepts have more than one relation. The second method is the   \n37 graph linearization approach that fuses the hierarchy in text graph into chain of tokens [1, 10]. This   \n38 approach treats the text graph as a special text sequence and enables the direct adoption of Language   \n39 Model (LM) for text graph generation. The idea can also be applied to learning vector representations   \n40 of text graphs. For example, BT5 [1] convert the text graph into sequence of (subject, relation,   \n41 object) triples and train T5 to translate between sentence and sequence of triples. Since sequence   \n42 generation with LM is done in an autoregressive manner, the generation is conditioned on the   \n43 already generated triples. This behavior allows the model to handle multi-hop relations and the   \n44 multi-relational case. Meanwhile, the model does not suffer from generating complete graphs because   \n45 the model can learn to terminate when the generated triples match the target text graph. However,   \n46 using sequence of triples also introduces extra complexity to the LM. Since the format requires   \n47 matching the concepts in triples to reconstruct the text graph, there will be duplications of subject   \n48 or object in the sequence. Thus, the model generates duplicated texts and cannot handle the case   \n49 where two concepts are represented by the same text but refer to different things. Also, it relies on   \n50 the model to implicitly learn the connection between two triples with duplication. Furthermore, LM   \n51 is neither permutation invariance nor equivariance, which means the prediction alters if the generated   \n52 triples are being shuffled.   \n53 The goal of this work is to design a new approach that preserves some of the advantages while   \n54 avoiding the drawbacks of previous approaches. This sets a few desired properties of the new   \n55 approach. First, the method should be suitable for both representation and generation. It should also   \n56 consider the cases that cannot be handled by multi-stage and graph linearization approach. Second,   \n57 the model should be permutation equivariance and perform generation in an autoregressive manner.   \n58 Last, the method should avoid extra computation, such as the duplication of concepts. To achieve   \n59 the desired characteristics, we propose the structure token approach, as illustrated in 1. Our method   \n60 employs a concept we call \u201cStructure Token\" which losslessly encodes the text graph into a set of   \n61 tokens. The token contains a word and a few identifiers for the precise location of that (sub-)word   \n62 in the text graph. Our model incorporates an unmodified Transformer Encoder-Decoder model [26]   \n63 with structure token embeddings and a structure predictor for predicting new structure tokens. The   \n64 text graph is generated autoregressively like regular text generation and the generated structure tokens   \n65 form a subgraph. Once the generation stops, we can decode the set of structure tokens into the   \n66 target text graph. A notable difference between our approach and previous approaches is that our   \n67 Transformer model operates on sets instead of sequences and we view text graphs as nested graphs.   \n68 To our knowledge, our structure token approach is the first method that can autoregressively generate   \n69 sub-graphs with multi-token labels without modifying transformers.   \n70 We validate our structure token approach on text-to-graph (T2G) generation and graph-to-text (G2T)   \n71 generation tasks. By treating sentence as a special text graph without any edges, the generation   \n72 tasks become a text-graph-to-text-graph translation problem. Therefore, we present TextGraphBART,   \n73 a Transformer Encoder-Decoder model pre-trained on text-graph-to-text-graph translation with   \n74 our structure token approach. The model is evaluated on two publicly available parallel datasets,   \n75 EventNarrative [8] and WebNLG (2020) [5], and achieves comparable results using fewer parameters   \n76 and pre-training data. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "77 2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "78 Recent works for text graph generation primarily focus on reusing pre-trained LMs. BT5 [1] applies   \n79 the graph linearization approach with T5 [24] pre-trained model. ReGen [10] further proposes a   \n80 Reinforcement Learning (RL) objective to improve the performance. On the other hand, Grapher [22]   \n81 uses T5 as an entity extraction model and jointly trains another relation extraction model. It provides   \n82 two types of implementation: Grapher (Text) and Grapher (Query). The Grapher (Text), which is the   \n83 state-of-the-art method on WebNLG (2020) [5] dataset, generates entities as a flat sequence, while   \n84 Grapher (Query) feeds a set of query embeddings to T5 decoder and apply another model to fully   \n85 decode the outputs to entities. Other works target on the problem of lacking paired datasets for T2G   \n86 generation. CycleGT [12] apply the cycle-training framework on a G2T model and a multi-stage   \n87 T2G model. INFINITY [29] apply the cycle-training framework on a single T5 model with graph   \n88 linearization approach.   \n89 Most of the works for learning vector representations only focus on G2T generation. GAP [9]   \n90 proposes a graph-aware attention that first uses the pooling operator to get the features of each text   \n91 label and then applies a modified attention operator on those features. KGPT [6] provides two types   \n92 of encoder: graph encoder and sequence encoder. The graph encoder is based on graph attention   \n93 network [27] that also operates on the pooled text features. On the other hand, the sequence encoder   \n94 infuses the structure information into the text tokens and feeds them into a transformer model. This   \n95 approach resembles our structure token approach upon learning vector representations. However,   \n96 they convert the graph into a dictionary-like format which suffers from a similar duplication problem   \n97 as graph linearization approach.   \n98 Our method is essentially derived from the design of TokenGT [16], which also converts graphs   \n99 into sets of tokens containing labels and identifiers. However, their idea does not directly fit in our   \n100 scenario of text graph for two reasons. First, a single graph element (node or edge) in TokenGT needs   \n101 to be representable by a single token. On the contrary, it would require multiple tokens for an element   \n102 of text graph because the label is a multi-token text. Second, TokenGT only focuses on representing   \n103 the graph, while we are interested in graph generation as well. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "104 3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "105 3.1 Structure Token ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "106 The proposed structure token is a data representation that can losslessly encode all data in a text   \n107 graph as a set of tokens. Given a text graph $\\mathcal{G}=(\\mathcal{N},\\mathcal{A})$ containing a node set $\\mathcal{N}$ and an arc set $\\boldsymbol{\\mathcal{A}}$ .   \n108 Each arc is a triple of a head node, an edge, and a tail node. Each graph element (node and edge)   \n109 is a unique text label S identifiable with an integer ID. This allows different nodes or edges to have   \n110 the same text label. The full formal definitions can be found in Appendix A.1. In order to convert   \n111 text graph to structure tokens, we express the node set and arc set into one unified structure of graph   \n112 elements. Each graph element will be represented by multiple structure tokens. A structure token   \n113 consists of seven parts: 1. Label: The (sub-)word token of a graph element. 2. Type: A binary   \n114 indicator specifying whether this graph element is a node or an edge. 3. Token ID: An unique ID for   \n115 this token. 4. Previous ID: The token ID of previous token. 5. Segment ID: An unique ID for the   \n116 graph element. 6. Head ID: The segment ID of the head node. 7. Tail ID: The segment ID of the   \n117 tail node. If the token is part of a node, the head ID and tail ID will just be the segment ID of itself.   \n118 With these information, we are able to differentiate between structure tokens that are from different   \n119 parts of the graph. The text graph is converted into a set of structure tokens. Since the IDs can point   \n120 to a graph element directly, there is no need for duplications like graph linearization approach. We   \n121 provide the formal definition of structure token in the Appendix A.2. A real example of text graph   \n122 and corresponding structure tokens can be found in Figure 2a. The idea of type, head ID, and tail ID   \n123 are inherited from TokenGT [16], which uses identifiers to indicate the connections. We modify their   \n124 definition and introduce extra identifiers for our text components. The token ID and previous ID are   \n125 text-level identifiers. The text order is determined by the token ID and previous ID for reconstructing   \n126 the text label. On the other hand, the segment ID, head ID, and tail ID are graph-level identifiers. For   \n127 tokens of a specific graph element, the graph-level identifiers of each token will be the same.   \n128 Furthermore, We add an extra \u201cdomain token\u201d to the structure tokens of a text graph to indicate the   \n129 domain of the graph, like the special language token used in multi-lingual translation [15, 21]. With   \n130 the domain token, we can specify what kind of data the text graph is holding. For example, since text   \n131 is treated as a text graph without any edges, we use a \u201c[text]\u201d domain token to indicate that this text   \n132 graph represents a text. Besides, we use the domain token as the first token of every text label, so the   \n133 previous ID of the first token of all labels are pointing to the domain token. ", "page_idx": 2}, {"type": "image", "img_path": "JVtwC9RzlI/tmp/72146b20b761acf90105570fd675df0cb2b1b513a7a318264345b5b9f561475f.jpg", "img_caption": ["(a) Example of structure tokens. "], "img_footnote": [], "page_idx": 3}, {"type": "image", "img_path": "JVtwC9RzlI/tmp/e502bbeb1aa4b9f6f3e9a8f11ad79950d1eb89ec1f96c4e13c3d5fa1ceb11ca2.jpg", "img_caption": ["Figure 2: Structure token and embedding. (2a) Each column is a structure token and each token has a unique token ID. The IDs can be used to locate the (sub-)word in the text graph. (2b) Both OneHot and Orth convert the ID or word into a vector and those vectors would be concatenated together to form the embeddings of the structure token ", "(b) Converting structure token to embedding. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "134 3.2 Structure Embedding ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "135 The structure tokens are transformed into fixed-size high-dimensional vector representations. Each   \n136 part of the structure token is converted into a vector and concatenated together. Then the vectorized   \n137 result will be fed to a trainable projection layer for getting the token embedding, as illustrated in   \n138 Figure 2b. Label and type are converted with one-hot encoding, denoted as OneHot. On the other   \n139 hand, the IDs need to be handled differently. In order to preserve the graph structure in the tokens   \n140 with the Transformer model, each ID needs to be converted into orthonormal vectors as proved by   \n141 TokenGT [16]. We loose the requirement of orthonormality and use a set of orthonormal-like vectors.   \n142 The dot product value of two different orthonormal-like vector is close to zero or less than some   \n143 thresholds. These vectors of identifiers enable the attention operation in the Transformer model to be   \n144 able to aggregate corresponding information through dot product. Each ID would be converted into an   \n145 orthonormal-like vector through a transform function Orth. We use this transform function to convert   \n146 the graph-level identifiers directly. On the other hand, we add the vectors of the text-level identifiers   \n147 together, which allows the attention to aggregate information from neighbor tokens like the absolute   \n148 position embedding [26]. Details and definitions can be found in Appendix A.3. Unlike position   \n149 embedding which depends on the location of the tokens in a sequence, the text-level identifiers   \n150 directly point to the neighbor token no matter their location in the sequence. Meanwhile, sequence   \n151 orders are defined by IDs and the IDs can be randomly assigned. Therefore, applying any permutation   \n152 of the input embeddings is equivalent to applying the same permutation on the output hidden states. ", "page_idx": 3}, {"type": "text", "text": "153 3.3 Generation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "154 Text Generation After converting the structure tokens into embeddings, those embeddings are   \n155 fed into the unmodified Transformer Encoder-Decoder model. Conceptually, our model generates a   \n156 structure token at each step which contains seven objects. However, we do not really need to generate   \n157 seven objects at every step. The token ID is unique for every token and we can randomly pick any ID   \n158 sequence beforehand. Notably, the structure token representation is a set, while the autoregressive   \n159 generation manner makes the generated tokens resemble a sequence. Although the design of structure   \n160 tokens enables the possibility of non-monotonic order of text generation, we slightly restrict the   \n161 generation order of the structure tokens from the same graph element to be ordered and contiguous.   \n162 With this restriction, we do not need to predict the token ID and previous ID. We can use the same   \n163 generation scheme of other text generation Transformer model that simply generates the next text   \n164 token until we are done with this element. Meanwhile, since the graph-level identifiers are the same   \n165 within a graph element, we only need to predict the graph-level identifiers for the first token of labels.   \n166 The generation can be further simplified for a single sentence since the graph-level identifiers are   \n167 merely the token ID of the first token. Thus, text generation with our structure token approach is   \n168 almost the same as other Transformer-based text generation models.   \n169 Text Graph Generation For text graph generation, the same methodology applies. We use a   \n170 structure predictor for predicting the identifiers. The graph-level identifiers are the same within a   \n171 graph element. The prediction of graph-level identifiers can be done only one time per graph element.   \n172 Moreover, the type and segment ID can also be omitted because we can tell the values once we get   \n173 the head ID and tail ID. As a result, our structure predictor only needs to predict the head ID and tail   \n174 ID. For predicting the IDs, we employ a single causal Transformer layer (a layer of the Transformer   \n175 decoder), as illustrated in Figure 3. The causal Transformer layer takes the output of the Transformer   \n176 model plus the transformed segment ID to produce a hidden vector. The hidden vector will be fed   \n177 into two projection layers to get a prediction of the head ID and tail ID. To get the ID, we multiply   \n178 the final hidden vectors with a list of our orthonormal-like vectors, and perform softmax on the   \n179 multiplication result to get the predictions. With this setup, we can apply the same teacher forcing   \n180 technique as other Transformer decoders, so the training process is also parallelized. ", "page_idx": 3}, {"type": "image", "img_path": "JVtwC9RzlI/tmp/a353713105c87da7824b469b03695268767530d33acd24df26440af7ede6c62c.jpg", "img_caption": ["Figure 3: The Structure Predictor predicts Figure 4: Illustration of our pre-training graph-level identifier of the $k$ -th token by tak- scheme. The input data is corrupted with ing the hidden state of previously generated masks or subgraph sampling on the encoder tokens plus the segment ID of the next tokens. side, and the decoder would need to generate The output is then multiplied with all possible the uncorrupted data based on a domain totoken IDs. ken. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "181 3.4 Pre-Training ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "182 We introduce a pre-training method for our model based on the mBART pre-trained model for   \n183 multilingual text generation [21]. The pre-training method contains two types of training objectives:   \n184 the self-supervised objective and translation-like objective, as illustrated in Figure 4. The translation  \n185 like objective forces the model to generate tokens depending on the domain token. The self-supervised   \n186 objective allows us to utilize more datasets without paired data (e.g. plain text datasets or sample   \n187 subgraphs from a large graph database). By using both kinds of objectives, the effective training data   \n188 are doubled. Meanwhile, the model is encouraged to learn a more unified representation. With these   \n189 objectives, we could utilize many different datasets to improve our model. ", "page_idx": 4}, {"type": "table", "img_path": "JVtwC9RzlI/tmp/7f380d0358253e68b99b87ffe4041e1a477ec0e8695a49461d2811b4d656a4c3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "190 4 Experiments and Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "191 Datasets We use four parallel datasets containing both text and text graph for our experiments,   \n192 as presented in Table 1. The model is pre-trained on TEKGEN [2] and GenWiki [14], and then   \n193 we fine-tune the pre-trained model on EventNarrative [8] and WebNLG (2020) [5] for evaluating   \n194 our model on G2T and T2G generation, respectively. The datasets are automatically generated by   \n195 aligning texts with existing databases, except WebNLG (2020). TEKGEN is a large-scale dataset   \n196 curated by aligning a subset of the Wikipedia text with Wikidata [28]. GenWiki is another large-scale   \n197 dataset built on Wikipedia text. The text graphs are collected from DBpedia [3]. EventNarrative is an   \n198 event-centric dataset that contains text graphs from the EventKG [11] and Wikidata. The text is also   \n199 a subset of Wikipedia text. WebNLG (2020) is crowd-sourced dataset crafted by human annotators.   \n200 The text graphs are collected from DBpedia, while the texts are manually written by annotators. It   \n201 contains 16 categories in the training set and 19 categories (3 extra categories) in the test set. We   \n202 use the official data split for all the datasets. As for the metrics, we used BLEU [23], METEOR [4],   \n203 and BERTScore [31] to evaluate the G2T performance, and we use the official evaluation script of   \n204 WebNLG (2020) to evaluate the T2G performance.   \n205 Setups Our model is trained in two phases: pre-training and fine-tuning. We initialize our model   \n206 6 from scratch and perform the pre-training method. For pre-training, We use the RAdam optimizer   \n207 [20] with a learning rate of 0.0001. The model is updated with an effective batch size of 256 and   \n208 being trained for 5 epochs on a single A100 40GB GPU. All fine-tuning experiments are done on a   \n209 single RTX 3090 24GB GPU. For fine-tuning on EventNarrative, we use the Lion optimizer [7] with   \n210 a learning rate of 0.00001. The model is updated with an effective batch size of 128 and trained for   \n211 20 epochs. For fine-tuning on WebNLG (2020), we use the Adam optimizer [17] with a learning rate   \n212 of 0.0001. The model is updated with an effective batch size of 128 and trained for 100 epochs.   \n213 We use an overall hidden size of 512 for our model. The unmodified Transformer encoder and decoder   \n214 both have 6 layers. Each attention has 16 heads, and we use a hidden size of 32 for self attentions and   \n215 64 for cross attentions. The feed-forward layer in Transformer has an input and output hidden size of   \n216 512, and the intermediate hidden size is 2048. We use these numbers for the structure predictor as   \n217 well. For the activation functions, we use the GELU activation function [13] for Transformers and   \n218 hyperbolic tangent function for the projection layers of structure predictor. During pre-training, we   \n219 apply the dropout [25] on the attention weights and the residual connections with a dropout rate of   \n220 0.1. The model weights are randomly initialized with a mean of 0 and a standard deviation of 0.02.   \n221 For data processing, we use the same subword tokenizer as T5 which uses the Unigram tokenization   \n222 method [24, 18]. The tokenizer has a vocabulary of 32100 text tokens, which contain 32000 subword   \n223 text tokens and 100 reserved special tokens. We use the reserved special tokens for our domain tokens.   \n224 Each dataset is assigned with a corresponding domain token for the graph data, while all text data   \n225 from different datasets share the same text domain token. The samples in each dataset are truncated   \n226 with a maximum length of 128 or 256 text tokens depending on the training stage. A random unique   \n227 ID sequence is determined for each sample at every epoch. During the pre-training, we randomly   \n228 assign a unique ID sequence with a maximum value of 512. For the encoder input, we randomly drop   \n229 $15\\%$ of graph elements or tokens depending on the domain.   \n230 G2T Results We compared our model with T5 [24], BART [19], and GAP [9]. Both T5 and   \n231 BART are Transformer Encoder-Decoder models pre-trained on text data and fine-tuned with graph   \n232 linearization [8], while GAP modifies the encoder of Transformer Encoder-Decoder model with   \n233 graph-aware modules for extracting graph features [9]. It is noteworthy that all these models use a   \n234 similar Transformer decoder. The main difference among TextGraphBART and these models is the   \n235 way we represent and handle the text graph input.   \n236 The result is shown in Table 2. In comparison to T5 and BART, our structure token method achieves   \n237 better score with fewer parameters than graph linearization approach. Meanwhile, our model is   \n238 comparable with GAP without modifying the Transformer model. As a conclusion, our structure   \n239 token representations enabled the Transformer model to capture better features from the text graph   \n240 than the graph linearization approach.   \n241 T2G Results We compare our model with CycleGT [12, 5], BT5 [1], and Grapher [22]. BT5   \n242 is T5 pre-trained and fine-tuned with graph linearization. On the other hand, both CycleGT and   \n243 Grapher adopt the multi-stage approach. The CycleGT is a well-known multi-stage approach for   \n244 text-to-graph generation using cycle training [12], while Grapher performs supervised learning with   \n245 a special loss function [22]. Meanwhile, we use the officially released source code of Grapher1to   \n246 train a Grapher-small (Text) which has a similar model size (95M) with our model (75M). Both   \n247 Grapher-small and our TextGraphBART are trained for 100 epochs with the same learning rate and   \n248 effective batch size.   \n249 The result is shown in Table 3. In comparison to CycleGT and Grapher (Query), our simple generation   \n250 method with structure tokens outperforms models with special training methods. Although our model   \n251 does not directly match the performance of the large models like BT5 or Grapher (Text), our model is   \n252 comparable with Grapher-small that has similar model size. Furthermore, we analyze the result by   \n253 measuring the performance on each category of the WebNLG (2020) test set comparing to Grapher  \n254 small. The result is shown in Table 4. Even though Grapher-small is based on the T5-small pre-trained   \n255 model, which is trained on an extremely large dataset of 750 GB: the Colossal Clean Crawled Corpus   \n256 (C4) [24], we can see that our model performs slightly better than Grapher-small on unseen categories   \n257 (0 samples in training set). In conclusion, our structure token approach can achieve comparable   \n258 performance on text-to-graph generation under similar model size without using special training   \n259 methods or loss functions.   \n260 Ablation Study To investigate the performance contribution of the components of structure tokens,   \n261 we conducted the ablation study on our structure embedding by fine-tuning our model with the   \n262 removal of some parts of the embeddings. The model is trained on the WebNLG (2020) with the   \n263 same setup. The results are shown in Table 5. In all ablations, the model performance was attenuated   \n264 as expected. First, the ablation of the token ID and previous ID removes the text order information in   \n265 the text labels hence the degeneration of performance. Similarly, the head ID and tail ID provide the   \n266 connectivities of the graph. Removal of this embedding decreases the performance, indicating the   \n267 importance of the connectivities. On the other hand, the ablation of type and segment ID are not as   \n268 detrimental as others because the type and segment ID may be inferred from other IDs. Thus, our   \n269 model is still able to perform albeit less performant. In conclusion, the ablation study showed that all   \n270 of our structure embedding is important for good model performance. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "table", "img_path": "JVtwC9RzlI/tmp/66abd29ecd2847710fc1f2bbd238e46a3c306d86f227b337ead6f0bc39b9a632.jpg", "table_caption": ["Table 4: Performance of our model on each category of WebNLG (2020) test set comparing to Grapher-small. The \u2217denotes the unseen categories. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "table", "img_path": "JVtwC9RzlI/tmp/9401d579d4c09cdb552080545b67b73bfeda8d8931d75e41c53e4f3e3ac63f78.jpg", "table_caption": ["Table 5: Ablation results of our structure embedding on WebNLG (2020) test set. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "271 5 Discussion ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "272 The primary objective of this work is to demonstrate the effectiveness of the proposed structure token   \n273 approach. Due to resource constraints, there are numerous aspects remain unexplored.   \n274 Scaling Up We believe that TextGraphBART has the potential to achieve better results when scaled   \n275 up. The backbone of our model is simply the transformer model, like most of the other baselines.   \n276 Meanwhile, we observed a clear improvement on other baselines when scaling up (e.g. comparing   \n277 T5-Base and T5-Large for G2T in 2 and Grapher-small and Grapher for T2G in 3). Therefore, we   \n278 anticipate that transformers on T2G and G2T follow the scaling laws.   \n279 Model Architecture While we use The Transformer Encoder-Decoder model in our experiments,   \n280 there are no strict restrictions on the model architecture as long as there are dot product attention   \n281 operations. The same approach can be applied to encoder or decoder-only model.   \n282 Data and Objective for Pre-Training In the experiments, we only use parallel datasets for pre  \n283 training. Since our pre-training scheme contains a T2T path, it\u2019s possible to pre-train the model   \n284 merely with plain-text datasets. On the other hand, we can also incorporate programming language   \n285 datasets and use syntax parsers to generate the Abstract Syntax Tree (AST) as the text graph for   \n286 pre-training. However, it is unclear how much the self-supervised objective and translation-like   \n287 objective affect the downstream performances. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "288 6 Conclusions ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "289 We present a novel approach to the problem of text graph generation leveraging the strength of   \n290 Transformer models. Our exploration has led to an effective method for structured data representation   \n291 and generation via structure tokens. In the structure token, we use several identifiers to indicate the   \n292 connectivities of the graphs and the order of the texts. Then an embedding method for structure   \n293 token is proposed, allowing the Transformer model to utilize the structural information. We show   \n294 that the structure token approach can be used to represent and generate both texts and text graphs.   \n295 The experiment results demonstrated the effectiveness of our method with less data and parameters.   \n296 Meanwhile, the ablation study further confirmed the importance of various elements of the structure   \n297 tokens. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "298 References ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "299 [1] O. Agarwal, M. Kale, H. Ge, S. Shakeri, and R. Al-Rfou. Machine translation aided bilin  \n300 gual data-to-text generation and semantic parsing. In Proceedings of the 3rd International   \n301 Workshop on Natural Language Generation from the Semantic Web (WebNLG+), pages 125\u2013   \n302 130, Dublin, Ireland (Virtual), 12 2020. Association for Computational Linguistics. URL   \n303 https://aclanthology.org/2020.webnlg-1.13.   \n304 [2] O. Agarwal, H. Ge, S. Shakeri, and R. Al-Rfou. Knowledge graph based synthetic cor  \n305 pus generation for knowledge-enhanced language model pre-training. In Proceedings of   \n306 the 2021 Conference of the North American Chapter of the Association for Computational   \n307 Linguistics: Human Language Technologies, pages 3554\u20133565, Online, June 2021. As  \n308 sociation for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.278. URL   \n309 https://aclanthology.org/2021.naacl-main.278.   \n310 [3] S. Auer, C. Bizer, G. Kobilarov, J. Lehmann, R. Cyganiak, and Z. Ives. Dbpedia: A nucleus for   \n311 a web of open data. In Proceedings of the 6th International The Semantic Web and 2nd Asian   \n312 Conference on Asian Semantic Web Conference, ISWC\u201907/ASWC\u201907, page 722\u2013735, Berlin,   \n313 Heidelberg, 2007. Springer-Verlag. ISBN 3540762973.   \n314 [4] S. Banerjee and A. Lavie. METEOR: An automatic metric for MT evaluation with improved   \n315 correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and   \n316 Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 65\u201372,   \n317 Ann Arbor, Michigan, June 2005. Association for Computational Linguistics. URL https:   \n318 //aclanthology.org/W05-0909.   \n319 [5] T. Castro Ferreira, C. Gardent, N. Ilinykh, C. van der Lee, S. Mille, D. Moussallem, and   \n320 A. Shimorina. The 2020 bilingual, bi-directional WebNLG+ shared task: Overview and   \n321 evaluation results (WebNLG $^{\\cdot+}$ 2020). In Proceedings of the 3rd International Workshop on   \n322 Natural Language Generation from the Semantic Web (WebNLG+), pages 55\u201376, Dublin, Ireland   \n323 (Virtual), 12 2020. Association for Computational Linguistics. URL https://aclanthology.   \n324 org/2020.webnlg-1.7.   \n325 [6] W. Chen, Y. Su, X. Yan, and W. Y. Wang. KGPT: Knowledge-grounded pre-training for   \n326 data-to-text generation. In Proceedings of the 2020 Conference on Empirical Methods in   \n327 Natural Language Processing (EMNLP), pages 8635\u20138648, Online, Nov. 2020. Association   \n328 for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.697. URL https://   \n329 aclanthology.org/2020.emnlp-main.697.   \n330 [7] X. Chen, C. Liang, D. Huang, E. Real, K. Wang, Y. Liu, H. Pham, X. Dong, T. Luong,   \n331 C.-J. Hsieh, Y. Lu, and Q. V. Le. Symbolic discovery of optimization algorithms. ArXiv,   \n332 abs/2302.06675, 2023.   \n333 [8] A. Colas, A. Sadeghian, Y. Wang, and D. Z. Wang. Eventnarrative: A large  \n334 scale event-centric dataset for knowledge graph-to-text generation. In J. Van  \n335 schoren and S. Yeung, editors, Proceedings of the Neural Information Process  \n336 ing Systems Track on Datasets and Benchmarks, volume 1. Curran, 2021. URL   \n337 https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/   \n338 2021/file/a3f390d88e4c41f2747bfa2f1b5f87db-Paper-round1.pdf.   \n339 [9] A. Colas, M. Alvandipour, and D. Z. Wang. Gap: A graph-aware language model framework for   \n340 knowledge graph-to-text generation. In International Conference on Computational Linguistics,   \n341 2022.   \n342 [10] P. Dognin, I. Padhi, I. Melnyk, and P. Das. ReGen: Reinforcement learning for text and   \n343 knowledge base generation using pretrained language models. In Proceedings of the 2021 Con  \n344 ference on Empirical Methods in Natural Language Processing, pages 1084\u20131099, Online and   \n345 Punta Cana, Dominican Republic, Nov. 2021. Association for Computational Linguistics. doi:   \n346 10.18653/v1/2021.emnlp-main.83. URL https://aclanthology.org/2021.emnlp-main.   \n347 83.   \n348 [11] S. Gottschalk and E. Demidova. Eventkg: A multilingual event-centric temporal knowledge   \n349 graph. In Extended Semantic Web Conference, 2018.   \n350 [12] Q. Guo, Z. Jin, X. Qiu, W. Zhang, D. Wipf, and Z. Zhang. CycleGT: Unsupervised graph-to  \n351 text and text-to-graph generation via cycle training. In Proceedings of the 3rd International   \n352 Workshop on Natural Language Generation from the Semantic Web $^{\\prime}W e b N L G+_{,}$ ), pages 77\u2013   \n353 88, Dublin, Ireland (Virtual), 12 2020. Association for Computational Linguistics. URL   \n354 https://aclanthology.org/2020.webnlg-1.8.   \n355 [13] D. Hendrycks and K. Gimpel. Gaussian error linear units (gelus). arXiv: Learning, 2016.   \n356 [14] Z. Jin, Q. Guo, X. Qiu, and Z. Zhang. GenWiki: A dataset of 1.3 million content-sharing text and   \n357 graphs for unsupervised graph-to-text generation. In Proceedings of the 28th International Con  \n358 ference on Computational Linguistics, pages 2398\u20132409, Barcelona, Spain (Online), Dec. 2020.   \n359 International Committee on Computational Linguistics. doi: 10.18653/v1/2020.coling-main.217.   \n360 URL https://aclanthology.org/2020.coling-main.217.   \n361 [15] M. Johnson, M. Schuster, Q. V. Le, M. Krikun, Y. Wu, Z. Chen, N. Thorat, F. B. Vi\u00e9gas,   \n362 M. Wattenberg, G. S. Corrado, M. Hughes, and J. Dean. Google\u2019s multilingual neural ma  \n363 chine translation system: Enabling zero-shot translation. Transactions of the Association for   \n364 Computational Linguistics, 5:339\u2013351, 2016.   \n365 [16] J. Kim, D. Nguyen, S. Min, S. Cho, M. Lee, H. Lee, and S. Hong. Pure transformers are   \n366 powerful graph learners. Advances in Neural Information Processing Systems, 35:14582\u201314595,   \n367 2022.   \n368 [17] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980,   \n369 2014.   \n370 [18] T. Kudo. Subword regularization: Improving neural network translation models with multi  \n371 ple subword candidates. In Proceedings of the 56th Annual Meeting of the Association for   \n372 Computational Linguistics (Volume 1: Long Papers), pages 66\u201375, Melbourne, Australia,   \n373 July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1007. URL   \n374 https://aclanthology.org/P18-1007.   \n375 [19] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov, and   \n376 L. Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural language   \n377 generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting   \n378 of the Association for Computational Linguistics, pages 7871\u20137880, Online, July 2020.   \n379 Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.703. URL   \n380 https://aclanthology.org/2020.acl-main.703.   \n381 [20] L. Liu, H. Jiang, P. He, W. Chen, X. Liu, J. Gao, and J. Han. On the variance of the adaptive   \n382 learning rate and beyond. In 8th International Conference on Learning Representations,   \n383 ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https:   \n384 //openreview.net/forum?id $\\cdot$ rkgz2aEKDr.   \n385 [21] Y. Liu, J. Gu, N. Goyal, X. Li, S. Edunov, M. Ghazvininejad, M. Lewis, and L. Zettlemoyer.   \n386 Multilingual denoising pre-training for neural machine translation. Transactions of the Asso  \n387 ciation for Computational Linguistics, 8:726\u2013742, 2020. doi: 10.1162/tacl_a_00343. URL   \n388 https://aclanthology.org/2020.tacl-1.47.   \n389 [22] I. Melnyk, P. Dognin, and P. Das. Knowledge graph generation from text. In Findings   \n390 of the Association for Computational Linguistics: EMNLP 2022, pages 1610\u20131622, Abu   \n391 Dhabi, United Arab Emirates, Dec. 2022. Association for Computational Linguistics. URL   \n392 https://aclanthology.org/2022.findings-emnlp.116.   \n393 [23] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. Bleu: a method for automatic evalua  \n394 tion of machine translation. In Proceedings of the 40th Annual Meeting of the Associa  \n395 tion for Computational Linguistics, pages 311\u2013318, Philadelphia, Pennsylvania, USA, July   \n396 2002. Association for Computational Linguistics. doi: 10.3115/1073083.1073135. URL   \n397 https://aclanthology.org/P02-1040.   \n398 [24] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J.   \n399 Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal   \n400 of Machine Learning Research, 21(140):1\u201367, 2020. URL http://jmlr.org/papers/v21/   \n401 20-074.html.   \n402 [25] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: A simple   \n403 way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15   \n404 (56):1929\u20131958, 2014. URL http://jmlr.org/papers/v15/srivastava14a.html.   \n405 [26] A. Vaswani, N. M. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and   \n406 I. Polosukhin. Attention is all you need. In NIPS, 2017.   \n407 [27] P. Veli\u02c7ckovi\u00b4c, G. Cucurull, A. Casanova, A. Romero, P. Li\u00f2, and Y. Bengio. Graph attention   \n408 networks. 6th International Conference on Learning Representations, 2017.   \n409 [28] D. Vrande\u02c7ci\u00b4c and M. Kr\u00f6tzsch. Wikidata: A free collaborative knowledgebase. Commun.   \n410 ACM, 57(10):78\u201385, sep 2014. ISSN 0001-0782. doi: 10.1145/2629489. URL https:   \n411 //doi.org/10.1145/2629489.   \n412 [29] Y. Xu, S. Sheng, J. Qi, L. Fu, Z. Lin, X. Wang, and C. Zhou. Unsupervised graph-text mutual   \n413 conversion with a unified pretrained language model. In Proceedings of the 61st Annual   \n414 Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages   \n415 5130\u20135144, Toronto, Canada, July 2023. Association for Computational Linguistics. doi:   \n416 10.18653/v1/2023.acl-long.281. URL https://aclanthology.org/2023.acl-long.281.   \n417 [30] C. Ying, T. Cai, S. Luo, S. Zheng, G. Ke, D. He, Y. Shen, and T.-Y. Liu. Do transformers really   \n418 perform bad for graph representation? In Neural Information Processing Systems, 2021.   \n419 [31] T. Zhang, V. Kishore, F. Wu, K. Q. Weinberger, and Y. Artzi. Bertscore: Evaluating text   \n420 generation with BERT. In 8th International Conference on Learning Representations, ICLR   \n421 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://   \n422 openreview.net/forum?id $\\equiv$ SkeHuCVFDr. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "423 A Formal Definitions ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "424 A.1 Preliminaries ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "425 Let $\\tau$ be the set of all possible text tokens. A text label is defined as a set of pairs containing the text   \n426 token and positions. We generalize this definition by replacing the position with a contiguous sequence   \n427 of unique ID. Given an infinite unique ID sequence: $\\mathcal{T}=(\\mathrm{id}_{i})_{i=0}^{\\infty}$ where $\\mathrm{id}_{i}\\in\\mathbb{Z}^{+}\\wedge\\mathrm{id}_{i}^{-}\\neq\\mathrm{id}_{j}$ if $i\\neq j$ .   \n428 Each $\\mathrm{id}_{i}$ is a positive integer. We also define $\\mathrm{id}(i)=\\mathrm{id}_{i}$ for simplicity. By picking a corresponding   \n429 ID sequence, we can use any positive integer sequence as the positions. Then a text label $\\mathbf{S}$ of   \n430 length $l$ is a set of token-ID pairs defined as: $\\mathbf{S}\\bar{=}\\left\\{(t_{i},\\mathrm{id}_{j+i})\\ \\middle\\vert\\ 1\\leq i\\leq l,t_{i}\\in{\\mathcal{T}}\\right\\}\\subseteq{\\mathcal{T}}\\times{\\mathbb{Z}}^{+}$ .   \n431 We can conditionally specify the start point $j\\in\\mathbb N$ with $\\mathbf{S}^{j}$ . Let $\\boldsymbol{S}$ be the set of all possible text   \n432 labels and $S^{j}$ for a specific start point. With the ID sequence $\\mathcal{T}$ , the positions $(\\hat{p)}_{p=1}^{l}$ can be   \n433 replaced with $(\\mathrm{id}_{j+k})_{k=1}^{l}$ . Then we can union text labels without missing information by picking   \n434 non-overlapped ID sequences, which is a desired property for the attention operation. A text   \n435 graph $\\mathcal{G}\\;=\\;(\\mathcal{N},\\mathcal{A})$ is composed of a node set $\\mathcal{N}$ with $q$ nodes and an arc set $\\boldsymbol{\\mathcal{A}}$ with $r$ arcs.   \n436 The node set $\\mathcal{N}$ is a set of node labels paired with unique IDs of the nodes, defined as: ${\\mathcal{N}}=$   \n437 $\\{(N_{i},n_{i})\\mid1\\leq i\\leq q,n_{i}\\in\\mathbb{Z}^{+},N_{i}\\in{\\bar{\\mathcal{S}}}\\}\\subseteq\\mathcal{S}\\times\\mathbb{Z}^{+}$ where $N_{i}$ is the node label and $n_{i}$ is the   \n438 corresponding node ID. Similarly, an edge set $\\mathcal{E}$ is a set of edge labels paired with unique IDs,   \n439 defined as: $\\bar{\\mathcal{E}^{=}}\\left\\{(E_{i},e_{i})~|~1\\leq\\bar{i}\\leq r,e_{i}^{-}\\in\\mathbb{Z}^{+},E_{i}\\in\\mathcal{S}\\right\\}\\subseteq\\bar{\\mathcal{S}^{\\times}}\\mathbb{Z}^{-}$ + where $E_{i}$ is the edge label   \n440 and $e_{i}$ is the edge ID. Notably, the ID used in $\\mathcal{N}$ and $\\mathcal{E}$ are disjoint. Then the arc set is defined as:   \n441 ${\\cal A}=\\{({\\bf N}_{i}^{h},{\\bf E}_{i},{\\bf\\bar{N}}_{i}^{t})\\mid1\\leq i\\leq r,{\\bf N}_{i}^{h},{\\bf N}_{i}^{t}\\in\\mathcal{N},{\\bf E}_{i}\\in\\mathcal{E}\\}\\subseteq\\tilde{\\mathcal{N}^{\\times}}\\,\\mathcal{E}\\times\\mathcal{N}$ where $\\mathbf{E}_{i}$ is the edge and   \n442 $\\mathbf{N}_{i}^{h}$ , $\\mathbf{N}_{i}^{t}$ is the head node and tail node, respectively. ", "page_idx": 10}, {"type": "text", "text": "443 A.2 Structure Tokens ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "444 With the setup in A.1, the formal definition of the structure token representation is defined as a set   \n445 of septuples (tuples with 7 elements). Given a text graph $\\mathcal{G}=(\\mathcal{N},\\dot{\\mathcal{A}})$ and its components: node   \n446 $\\mathbf{N}_{i}=(N_{i},n_{i})\\in\\mathcal{N}$ , $1\\leq i\\leq|\\mathcal{N}|$ , arc $\\mathbf{A}_{j}=((N_{j}^{h},n_{j}^{h}),(E_{j},e_{j}),(N_{j}^{t},n_{j}^{t}))\\in A$ , $1\\leq j\\leq|A|$ ,   \n447 edge $\\mathbf{E}_{j}\\in\\mathcal{E}=\\{(E_{j},e_{j})\\mid1\\leq j\\leq|A|\\}$ , and the length of the text labels $l_{i}^{N}=\\left|N_{i}\\right|$ , $l_{j}^{E}=\\vert E_{j}\\vert$ ,   \n448 we define two sequences: ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal L^{N}=\\left(\\mathbf L_{k}^{N}\\right)_{k=1}^{|\\mathcal N|}\\mathrm{where}\\,\\mathbf L_{k}^{N}=\\left\\{0,\\qquad\\qquad\\qquad\\mathrm{if}\\;k=1\\right.}\\\\ &{\\qquad\\qquad\\qquad\\mathrm{if}\\;k\\neq1}\\\\ &{\\mathcal L^{E}=\\left(\\mathbf L_{k}^{E}\\right)_{k=1}^{|\\mathcal A|}\\mathrm{where}\\,\\mathbf L_{k}^{E}=\\left\\{\\sum_{k=1}^{|\\mathcal N|}l_{k}^{N},\\quad\\mathrm{if}\\;k=1\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "449 We assign $n_{i}=\\mathrm{id}(\\mathbf{L}_{i}^{N}+1)$ , $e_{j}=\\mathrm{id}(\\mathbf{L}_{j}^{E}+1)$ and specify the start point of each text label such that   \n450 $N_{i}\\in S^{\\mathbf{L}_{i}^{N}},E_{j}\\in S^{\\mathbf{L}_{j}^{E}}$ . By doing so, we can get the node (or edge) $\\mathrm{ID}$ from the token IDs of its text   \n451 label. For each node $\\mathbf{N}_{i}$ and edge $\\mathbf{{E}}_{j}$ , we define the corresponding structure token representation   \n452 $X_{i}^{N}$ and $X_{j}^{E}$ as: ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{X_{i}^{N}=\\{(t_{k},1,u i d_{k},u i d_{k-1},n_{i},n_{i},n_{i})\\mid}\\\\ &{~~~~~~~~~~~~1\\leq k\\leq l_{i}^{N},(t_{k},u i d_{k})\\in N_{i},u i d_{0}=\\mathrm{id}_{0}\\}}\\\\ &{X_{j}^{E}=\\{(t_{k},0,u i d_{k},u i d_{k-1},e_{j},n_{j}^{h},n_{j}^{t})\\mid}\\\\ &{~~~~~~~~~~~~1\\leq k\\leq l_{j}^{E},(t_{k},u i d_{k})\\in E_{j},u i d_{0}=\\mathrm{id}_{0}\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "453 Then the corresponding structure token representation $\\mathcal{G}^{\\prime}$ of the text graph $\\mathcal{G}$ is defined as: ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{G}^{\\prime}=\\bigcup_{k=1}^{|\\mathcal{N}|}X_{k}^{N}\\cup\\bigcup_{k=1}^{|\\mathcal{A}|}X_{k}^{E}\\cup X_{D}}\\\\ &{\\quad\\subseteq\\mathcal{T}\\times\\{0,1\\}\\times\\mathbb{Z}^{+}\\times\\mathbb{Z}^{+}\\times\\mathbb{Z}^{+}\\times\\mathbb{Z}^{+}\\times\\mathbb{Z}^{+}\\times\\mathbb{Z}^{+}}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "454 where $t_{D}\\,\\in\\,\\mathcal{T},\\,\\mathrm{id}_{0}$ is the $\\mathrm{ID}$ of domain token and $X_{D}\\,=\\,\\{(t_{D},1,\\mathrm{{id}_{0},\\mathrm{{id}_{0},\\mathrm{{id}_{0},\\mathrm{{id}_{0},\\mathrm{{id}_{0})}}}}\\}}$ is the   \n455 domain token. Each septuple $X\\,\\in\\,\\mathcal{G}^{\\prime}$ is a structure token containing the label, type, token ID,   \n456 previous ID, segment $\\mathrm{ID}$ , head ID, and tail ID. With this definition, we can represent every possible   \n457 token ID assignment by specifying the unique ID sequence $\\mathcal{T}$ . On the other hand, since the graph   \n458 element can be randomly permuted, every possible ordering is also representable with our set $\\mathcal{G}^{\\prime}$ by   \n459 picking the corresponding unique ID sequence. ", "page_idx": 11}, {"type": "text", "text": "460 A.3 Structure Embedding ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "461 To convert the structure tokens into embeddings, we use 3 kinds of transform functions. For label   \n462 and type, we use the one-hot encoding, denoted as $O n e H o t_{n}\\colon\\mathbb{A}\\to\\mathbb{E}_{n}$ where $\\mathbb{A}$ is a set with $n$   \n463 elements and $\\mathbb{E}_{n}$ is the standard basis of $\\mathbb{R}^{n}$ . On the other hand, each ID would first be converted   \n464 into a $d$ -dimensional orthonormal-like vector with a function ${\\cal O}r t h_{d}$ . To get the orthonormal-like   \n465 vectors, we modify and normalize the sinusoidal position encoding of Transformer [26] with different   \n466 frequencies. The $d_{\\cdot}$ -dimensional sinusoidal position encoding $\\mathrm{PE}_{d}$ at position $i$ is defined as: ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{PE}_{d}(i)=\\displaystyle\\prod_{k=1}^{d/2}p e(i,k)}&{{}\\in\\mathbb{R}^{d}}\\\\ {p e(i,k)=\\sin(\\frac{i}{10000^{k/d}})\\|\\cos(\\frac{i}{10000^{k/d}})}&{{}\\in\\mathbb{R}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "467 where $\\parallel$ denotes the vector concatenation. We generalize the definition of $\\mathrm{PE}_{d}$ with a frequency   \n468 function $f$ : ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{PE}_{d}^{*}\\{f\\}(i)=\\displaystyle\\prod_{k=1}^{d/2}p e^{*}\\{f\\}(i,k)}&{{}\\in\\mathbb{R}^{d}}\\\\ {p e^{*}\\{f\\}(i,k)=\\sin(i*f(k))\\|\\cos(i*f(k))}&{{}\\in\\mathbb{R}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "469 Then by taking $f^{\\prime}(k)=10000^{-k/d}$ , the original $\\mathrm{PE}_{d}$ can be defined with $\\mathrm{PE}_{d}=\\mathrm{PE}_{d}^{*}\\{f^{\\prime}\\}$ . We   \n470 use this generalized position encoding to define the function $O r t h_{d}\\colon\\mathbb{Z}^{+}\\to\\mathbb{R}^{d}$ of the IDs for our ", "page_idx": 11}, {"type": "image", "img_path": "JVtwC9RzlI/tmp/ebabe87a117fcc06aaa8671dd24bf77e6445a75a956404bdf2ffea1906bbf35b.jpg", "img_caption": ["Figure 5: Histogram of cos-similarities of 1024 vectors with different frequency functions. The orange line is our ${\\it O r t h}$ , while the blue line is the normalized sinusoidal position embedding. A dimensionality of 512 is used in this figure. "], "img_footnote": [], "page_idx": 12}, {"type": "text", "text": "471 orthonormal-like vectors as: ", "page_idx": 12}, {"type": "equation", "text": "$$\nO r t h_{d}=n o r m_{2}\\circ{\\mathrm{PE}}_{d}^{\\ast}\\{-\\log(k)\\}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "472 where $n o r m_{2}$ is the L2 normalization and $\\circ$ denote the function composition. We find that by picking   \n473 the frequency function $-\\log(k)$ , the generated vectors satisfied the desired properties. In Figure 5,   \n474 we generate 1024 vectors by applying ${\\it O r t h}$ on $1\\leq i\\leq1024$ and compute the cosine similarity of   \n475 every possible pair. We can see that the similarity values are mostly close to zero.   \n476 Once we can convert the IDs into orthonormal-like vectors, we use ${\\cal O}r t h_{d}$ directly as the transform   \n477 functions for the graph-level identifiers. For text-level identifiers, we add the vector of token ID and   \n478 previous ID together. Given two non-domain token $a$ and $b$ with token ID $\\mathrm{t}_{a}$ , $\\mathrm{t}_{b}$ and previous ID   \n479 $\\mathrm{p}_{a}$ , $\\mathrm{p}_{b}$ , we have $O r t h(\\mathrm{t}_{a})\\cdot O r t h(\\mathrm{t}_{b})\\approx0$ since token ID is unique and ${\\it O r t h}$ is orthonormal-like.   \n480 Meanwhile, we have $\\mathrm{p}_{a}\\neq\\mathrm{p}_{b}$ for most tokens. The dot product value of $a$ and $b$ become: ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(O r t h(\\mathbf{t}_{a})+O r t h(\\mathbf{p}_{a}))\\cdot(O r t h(\\mathbf{t}_{b})+O r t h(\\mathbf{p}_{b}))}\\\\ &{\\quad=O r t h(\\mathbf{t}_{a})\\cdot O r t h(\\mathbf{t}_{b})+O r t h(\\mathbf{t}_{a})\\cdot O r t h(\\mathbf{p}_{b})+O r t h(\\mathbf{p}_{a})\\cdot O r t h(\\mathbf{t}_{b})+O r t h(\\mathbf{p}_{a})\\cdot O r t h(\\mathbf{p}_{b})}\\\\ &{\\quad=O r t h(\\mathbf{t}_{a})\\cdot O r t h(\\mathbf{p}_{b})+O r t h(\\mathbf{p}_{a})\\cdot O r t h(\\mathbf{t}_{b})+O r t h(\\mathbf{p}_{a})\\cdot O r t h(\\mathbf{p}_{b})+\\varepsilon}\\\\ &{\\quad=\\left\\{O r t h(\\mathbf{t}_{a})\\cdot O r t h(\\mathbf{p}_{b})+\\varepsilon=1+\\varepsilon,\\quad\\quad\\mathrm{~if~}a\\mathrm{~is~the~previous~token~of~}b\\right.}\\\\ &{\\quad=\\left\\{O r t h(\\mathbf{p}_{a})\\cdot O r t h(\\mathbf{t}_{b})+\\varepsilon=1+\\varepsilon,\\quad\\quad\\mathrm{~if~}b\\mathrm{~is~the~previous~token~of~}a\\right.}\\\\ &{\\quad\\left.O r t h(\\mathbf{id}_{0})\\cdot O r t h(\\mathbf{id}_{0})+\\varepsilon=1+\\varepsilon,\\quad\\quad\\mathrm{~if~both~}a\\mathrm{~and~}b\\mathrm{~are~the~first~token~of~text~label~}}\\\\ &{\\quad\\quad\\in\\mathcal{E},}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "481 where $\\varepsilon$ is a small value around 0. The value will only be meaningful in the dot product if one token   \n482 is the previous token of the other. The case of first tokens is set to allow exchanging information with   \n483 the domain token. This can be suppressed by making $O r t h(\\mathrm{id}_{0})=\\mathbf{0}$ . ", "page_idx": 12}, {"type": "text", "text": "484 With these designs, we define our structure token vectorize function $V e c$ as: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V e c(X)=}\\\\ &{\\quad O n e H o t_{|\\mathcal{T}|}(\\pi_{1}(X))\\parallel O n e H o t_{2}(\\pi_{2}(X))\\parallel(O r t h_{n}(\\pi_{3}(X))+O r t h_{n}\\pi_{4}(X)))}\\\\ &{\\quad\\parallel O r t h_{n}(\\pi_{5}(X))\\parallel O r t h_{n}(\\pi_{6}(X))\\parallel O r t h_{n}(\\pi_{7}(X))\\in\\mathbb{R}^{4n+|\\mathcal{T}|+2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "485 where $\\pi_{i}(X)$ denote the $i$ -th element of the septuple $X$ . The vectorized result will be fed into a train  \n486 able projection layer $E m b$ : $\\mathbb{R}^{4n+|T|+2}\\to\\mathbb{R}^{d}$ to get the structure embedding with $d$ dimensions. ", "page_idx": 12}, {"type": "text", "text": "487 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "490 paper\u2019s contributions and scope?   \n491 Answer: [Yes]   \n492 Justification: The proposed methods and results are described in the Method Section (Section   \n493 3) and the Experiments and Results Section (Section 4).   \n494 Guidelines:   \n495 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n496 made in the paper.   \n497 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n498 contributions made in the paper and important assumptions and limitations. A No or   \n499 NA answer to this question will not be perceived well by the reviewers.   \n500 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n501 much the results can be expected to generalize to other settings.   \n502 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n503 are not attained by the paper. ", "page_idx": 13}, {"type": "text", "text": "504 2. Limitations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 13}, {"type": "text", "text": "Justification: The limitations are discussed in the Discussion Section (Section 5) ", "page_idx": 13}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper. \u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. 0 \u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. \u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. \u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. \u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers 4 will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 13}, {"type": "text", "text": "535 3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "536 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n537 a complete (and correct) proof? ", "page_idx": 13}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 13}, {"type": "text", "text": "Justification: This work does not introduce any new theoretical results Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 14}, {"type": "text", "text": "551 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 14}, {"type": "text", "text": "Justification: The experiment setups are described in the Experiments and Results Section (Section 4) and code is also provided. ", "page_idx": 14}, {"type": "text", "text": "Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 14}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 14}, {"type": "text", "text": "595 Justification: code is provided   \n596 Guidelines:   \n597 \u2022 The answer NA means that paper does not include experiments requiring code.   \n598 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n599 public/guides/CodeSubmissionPolicy) for more details.   \n600 \u2022 While we encourage the release of code and data, we understand that this might not be   \n601 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n602 including code, unless this is central to the contribution (e.g., for a new open-source   \n603 benchmark).   \n604 \u2022 The instructions should contain the exact command and environment needed to run to   \n605 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n606 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n607 \u2022 The authors should provide instructions on data access and preparation, including how   \n608 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n609 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n610 proposed method and baselines. If only a subset of experiments are reproducible, they   \n611 should state which ones are omitted from the script and why.   \n612 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n613 versions (if applicable).   \n614 \u2022 Providing as much information as possible in supplemental material (appended to the   \n615 paper) is recommended, but including URLs to data and code is permitted.   \n616 6. Experimental Setting/Details   \n617 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n618 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n619 results?   \n620 Answer: [Yes]   \n621 Justification: The experiment setups are described in the Experiments and Results Section   \n622 (Section 4)   \n623 Guidelines:   \n624 \u2022 The answer NA means that the paper does not include experiments.   \n625 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n626 that is necessary to appreciate the results and make sense of them.   \n627 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n628 material.   \n629 7. Experiment Statistical Significance   \n630 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n631 information about the statistical significance of the experiments?   \n632 Answer: [No]   \n633 Justification: Due to resource constraints, we are unable to perform significance tests.   \n634 Guidelines:   \n635 \u2022 The answer NA means that the paper does not include experiments.   \n636 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n637 dence intervals, or statistical significance tests, at least for the experiments that support   \n638 the main claims of the paper.   \n639 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n640 example, train/test split, initialization, random drawing of some parameter, or overall   \n641 run with given experimental conditions).   \n642 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n643 call to a library function, bootstrap, etc.)   \n644 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n645 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n646 of the mean.   \n647 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n648 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n649 of Normality of errors is not verified.   \n650 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n651 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n652 error rates).   \n653 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n654 they were calculated and reference the corresponding figures or tables in the text.   \n655 8. Experiments Compute Resources   \n656 Question: For each experiment, does the paper provide sufficient information on the com  \n657 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n658 the experiments?   \n659 Answer: [Yes]   \n660 Justification: The experiment setups are described in the Experiments and Results Section   \n661 (Section 4)   \n662 Guidelines:   \n663 \u2022 The answer NA means that the paper does not include experiments.   \n664 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n665 or cloud provider, including relevant memory and storage.   \n666 \u2022 The paper should provide the amount of compute required for each of the individual   \n667 experimental runs as well as estimate the total compute.   \n668 \u2022 The paper should disclose whether the full research project required more compute   \n669 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n670 didn\u2019t make it into the paper).   \n671 9. Code Of Ethics   \n672 Question: Does the research conducted in the paper conform, in every respect, with the   \n673 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n674 Answer: [Yes]   \n675 Justification: The content does not violate NeurIPS Code of Ethics   \n676 Guidelines:   \n677 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n678 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n679 deviation from the Code of Ethics.   \n680 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n681 eration due to laws or regulations in their jurisdiction).   \n682 10. Broader Impacts   \n683 Question: Does the paper discuss both potential positive societal impacts and negative   \n684 societal impacts of the work performed?   \n685 Answer: [NA]   \n686 Justification: The main result of this work is a new encoding method, which has no societal   \n687 impact   \n688 Guidelines:   \n689 \u2022 The answer NA means that there is no societal impact of the work performed.   \n690 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n691 impact or why the paper does not address societal impact.   \n692 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n693 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n694 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n695 groups), privacy considerations, and security considerations.   \n696 \u2022 The conference expects that many papers will be foundational research and not tied   \n697 to particular applications, let alone deployments. However, if there is a direct path to   \n698 any negative applications, the authors should point it out. For example, it is legitimate   \n699 to point out that an improvement in the quality of generative models could be used to   \n700 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n701 that a generic algorithm for optimizing neural networks could enable people to train   \n702 models that generate Deepfakes faster.   \n703 \u2022 The authors should consider possible harms that could arise when the technology is   \n704 being used as intended and functioning correctly, harms that could arise when the   \n705 technology is being used as intended but gives incorrect results, and harms following   \n706 from (intentional or unintentional) misuse of the technology.   \n707 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n708 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n709 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n710 feedback over time, improving the efficiency and accessibility of ML).   \n711 11. Safeguards   \n712 Question: Does the paper describe safeguards that have been put in place for responsible   \n713 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n714 image generators, or scraped datasets)?   \n715 Answer: [NA]   \n716 Justification: All datasets used are publicly available open datasets.   \n717 Guidelines:   \n718 \u2022 The answer NA means that the paper poses no such risks.   \n719 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n720 necessary safeguards to allow for controlled use of the model, for example by requiring   \n721 that users adhere to usage guidelines or restrictions to access the model or implementing   \n722 safety filters.   \n723 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n724 should describe how they avoided releasing unsafe images.   \n725 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n726 not require this, but we encourage authors to take this into account and make a best   \n727 faith effort.   \n728 12. Licenses for existing assets   \n729 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n730 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n731 properly respected?   \n732 Answer: [Yes]   \n733 Justification: All dataset and code used are publicly available and cited.   \n734 Guidelines:   \n735 \u2022 The answer NA means that the paper does not use existing assets.   \n736 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n737 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n738 URL.   \n739 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n740 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n741 service of that source should be provided.   \n742 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n743 package should be provided. For popular datasets, paperswithcode.com/datasets   \n744 has curated licenses for some datasets. Their licensing guide can help determine the   \n745 license of a dataset.   \n746 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n747 the derived asset (if it has changed) should be provided.   \n748 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n749 the asset\u2019s creators.   \n750 13. New Assets   \n751 Question: Are new assets introduced in the paper well documented and is the documentation   \n752 provided alongside the assets?   \n753 Answer: [NA]   \n754 Justification: This work does not release new assets   \n755 Guidelines:   \n756 \u2022 The answer NA means that the paper does not release new assets.   \n757 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n758 submissions via structured templates. This includes details about training, license,   \n759 limitations, etc.   \n760 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n761 asset is used.   \n762 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n763 create an anonymized URL or include an anonymized zip file.   \n764 14. Crowdsourcing and Research with Human Subjects   \n765 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n766 include the full text of instructions given to participants and screenshots, if applicable, as   \n767 well as details about compensation (if any)?   \n768 Answer: [NA]   \n769 Justification: This work does not involve crowdsourcing nor research with human subjects.   \n770 Guidelines:   \n771 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n772 human subjects.   \n773 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n774 tion of the paper involves human subjects, then as much detail as possible should be   \n775 included in the main paper.   \n776 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n777 or other labor should be paid at least the minimum wage in the country of the data   \n778 collector.   \n779 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n780 Subjects   \n781 Question: Does the paper describe potential risks incurred by study participants, whether   \n782 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n783 approvals (or an equivalent approval/review based on the requirements of your country or   \n784 institution) were obtained?   \n785 Answer: [NA]   \n786 Justification: This work does not involve crowdsourcing nor research with human subjects.   \n787 Guidelines:   \n788 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n789 human subjects.   \n790 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n791 may be required for any human subjects research. If you obtained IRB approval, you   \n792 should clearly state this in the paper.   \n793 \u2022 We recognize that the procedures for this may vary significantly between institutions   \n794 and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the   \n795 guidelines for their institution.   \n796 \u2022 For initial submissions, do not include any information that would break anonymity (if   \n797 applicable), such as the institution conducting the review. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}]