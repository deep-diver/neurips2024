[{"type": "text", "text": "Unified Generative and Discriminative Training for Multi-modal Large Language Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Wei Chow1 Juncheng $\\mathbf{Li^{1,\\dag}}$ Qifan $\\mathbf{M}^{1}$ Kaihang Pan1 Hao Fei2 Zhiqi Ge1 Shuai Yang1 Siliang Tang1,\u2020 Hanwang Zhang3 Qianru Sun4 ", "page_idx": 0}, {"type": "text", "text": "1Zhejiang University 2National University of Singapore 3Nanyang Technological University 4Singapore Management University {xieqiao, junchengli, yuqifan, kaihangpan}@zju.edu.cn {zhiqige, syang, siliang}@zju.edu.cn haofei37@nus.edu.sg, hanwangzhang@ntu.edu.sg, qianrusun@smu.edu.sg ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In recent times, Vision-Language Models (VLMs) have been trained under two predominant paradigms. Generative training has enabled Multimodal Large Language Models (MLLMs) to tackle various complex tasks, yet issues such as hallucinations and weak object discrimination persist. Discriminative training, exemplified by models like CLIP, excels in zero-shot image-text classification and retrieval, yet struggles with complex scenarios requiring fine-grained semantic differentiation. This paper addresses these challenges by proposing a unified approach that integrates the strengths of both paradigms. Considering interleaved image-text sequences as the general format of input samples, we introduce a structure-induced training strategy that imposes semantic relationships between input samples and the MLLM\u2019s hidden state. This approach enhances the MLLM\u2019s ability to capture global semantics and distinguish fine-grained semantics. By leveraging dynamic sequence alignment within the Dynamic Time Warping framework and integrating a novel kernel for fine-grained semantic differentiation, our method effectively balances generative and discriminative tasks. Extensive experiments demonstrate the effectiveness of our approach, achieving state-of-the-art results in multiple generative tasks, especially those requiring cognitive and discrimination abilities. Additionally, our method surpasses discriminative benchmarks in interleaved and fine-grained retrieval tasks. By employing a retrieval-augmented generation strategy, our approach further enhances performance in some generative tasks within one model, offering a promising direction for future research in vision-language modeling. The project repository is here. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In recent times, Vision-Language Models (VLMs) have been trained under two predominant paradigms: generative training and discriminative training. Generative Training has achieved remarkable success in enabling Multimodal Large Language Models (MLLMs) [1, 55, 86] to develop a wide range of powerful capabilities that can handle various complex tasks (e.g., open-world visual question-answering, image caption generation, etc.) within a single model. However, challenges such as hallucinations and weak image object discrimination abilities [7, 89] persist. Discriminative Training, exemplified by CLIP [73], exhibits remarkable representation capabilities for zero-shot image-text classification and retrieval. Nonetheless, it encounters difficulties in processing complex scenarios (i.e., , retrieving multi-modal documents with interleaved images and texts) [53, 54] and exhibits a limited ability to discern detailed semantic differences [79, 85]. ", "page_idx": 0}, {"type": "text", "text": "The disparity between these two paradigms has sparked recent studies aimed at imparting discriminative ability to generative pre-trained MLLMs. However, certain aspects of performance still pose limitations (e.g., singular discriminative tasks [89], weak discriminative task performance [40], weak generalization [59], etc.), while others entail compromising the model\u2019s original generative capabilities [8]. ", "page_idx": 1}, {"type": "text", "text": "Overall, the reason generative paradigms struggle with performing discriminative tasks like retrieval is due to overlooking two crucial abilities: ", "page_idx": 1}, {"type": "text", "text": "(i) Comprehensively capturing the global semantics. Recent studies have revealed that causal LLMs tend to exhibit a bias towards capturing global information from the input samples, often resulting in a tendency to overlook information located in the middle, especially for long sequences [15, 57]. As illustrated in Figure 1(a), we chose 500 samples from WebQA [10], where the task is to find and reason about the right image-text pair among five distractors to produce a yes or no answer. We conducted experiments using VILA [52], a MLLM with state-of-the-art interleaved image-text comprehension ability, alongside our model. When placing the relevant pair in different positions, the performance of MLLMs followed a \u2019U\u2019 shape, indicating a bias in capturing global semantic information. Consequently, MLLMs encounter difficulties in forming comprehensive representations that encompass global semantics for retrieval tasks. ", "page_idx": 1}, {"type": "text", "text": "(ii) Keenly differentiating the detailed semantics. Some research [47, 82] has found that the existing generative training framework cannot fully distinguish input semantics in certain contexts, causing MLLMs to struggle with tasks requiring fine-grained semantics [46, 98]. As depicted in Figure 1(b), we noticed that MLLMs face challenges in choosing the right description for two similar images in the MMVP-VLM benchmark [81]. This indicates that MLLMs struggle to effectively differentiate the detailed semantics of input samples, naturally leading to difficulties in forming effective queries for retrieval. ", "page_idx": 1}, {"type": "image", "img_path": "w67vRHZF13/tmp/d8baf0aa35d034fc90137f6ae880efbebdff487ad88b89510c1fd22f977c5f7e.jpg", "img_caption": ["Figure 1: (a) In WebQA [10], the accuracy roughly forms a \u201cU\u201d shape curve when the relevant image-text pair for a question appears at different positions. While our model also shows similar trends, it tends to be more stable overall. (b) The accuracy of various types of questions in MMVPVLM [81], it can be observed that our model\u2019s performance improves on such tasks after introducing the discriminative training. Details can be seen in Appendix E.3 "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "In this paper, we argue that the current separated paradigms possess the potential for achieving synergistic gains. We propose Sugar: Structure-induced approach to unify generative and discriminative paradigms (shown in Figure 2), leveraging discriminative training to acquire the two abilities above while harnessing the potential of generative training in complex discriminative tasks like image-text interleaved retrieval and fine-grained retrieval. Specifically, we explicitly impose the semantic relationships between different input samples as an induced structural constraint on the hidden state of MLLMs. We consider the interleaved image-text sequence as the general format of input samples, and then formulate the relationship between any two samples as a dynamic sequence alignment problem within the Dynamic Time Warping framework [67, 33]. In this way, we can explicitly modulate the hidden states of the MLLM by leveraging the semantic relationships between interleaved input sequences, thereby encouraging the MLLM to fully capture the global semantics of the inputs. ", "page_idx": 1}, {"type": "text", "text": "To further enhance the ability to differentiate fine-grained semantics, we integrate a novel kernel into the Dynamic Time Warping framework. Leveraging the strengths of various discriminative pre-trained models, it performs dynamic sequence alignment for diverse embeddings tailored to specific contexts, thus addressing the inherent limitations in fully utilizing input semantics. Through this explicit structure-induced constraint, our framework enables MLLMs to capture the global semantics and fine-grained details of the input multimodal sequence more effectively, thus bridging the gap between generative and discriminative training paradigms. ", "page_idx": 1}, {"type": "image", "img_path": "w67vRHZF13/tmp/6d328b3c2d7cec8f4d6b7856625756bfc0fe84019a7b69772f96c8e7fb8f95f8.jpg", "img_caption": ["Figure 2: Our structure-induced generative and discriminative training joint training strategy. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Our method effectively balances both discriminative and generative tasks, demonstrating synergistic benefits. (i) Large-scale generative pre-trained models possess semantic-rich hidden states [41, 91, 23], which facilitate discriminative tasks like retrieval. Moreover, harnessing the capabilities of MLLM is crucial for complex discriminative tasks, such as interleaved image-text retrieval and fine-grained retrieval. (ii) By integrating discriminative tasks, the model\u2019s effectiveness in generative tasks, particularly within tasks requiring cognitive and discrimination abilities, is enhanced, thereby mitigating certain occurrences of hallucinations. (iii) We can employ Sugar to realize retrievalaugmented generation [2], eliminating the need for an off-the-shelf retrieval module [75], thereby amplifying the performance of various generative tasks. The usage of off-the-shelf retrieval presents a challenge wherein the retriever\u2019s performance affects the generator\u2019s final output [62]. This necessitates independent optimization of both components, posing a dilemma in selecting optimal configurations. However, our approach circumvents such optimization challenges. ", "page_idx": 2}, {"type": "text", "text": "Through extensive experimentation, we have demonstrated the effectiveness of our approach. For generative tasks, Sugar establishes new state-of-the-art results on the tasks for complicated multimodal comprehension tasks (i.e., DEMON [47]), fine-grained semantic distinctions (i.e., VizWiz [28], MME [95]), object hallucinations detection (i.e., POPE [51]) (Section 4.2 and Section 4.3). For discriminative tasks, we achieved competitive results in image-text retrieval compared, and significantly surpassed CLIP in interleaved retrieval and fine-grained retrieval (Section 4.4). Furthermore, employing the retrieval-augmented generation (RAG) strategy led to further improvements in a series of generative tasks (Section 4.5). ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Multi-modal Large Language Models. Flamingo [3] and BLIP-2 [49] integrate LLMs with visual encoders, showcasing impressive zero-shot capabilities by aligning visual features with language representations. Building upon the advancements of LLaVA-1.5 [55], subsequent studies [103, 19, 94, 6, 42, 72, 95, 98, 45] propose fine-tuning MLLMs with multimodal instruction tuning data [102]. Recently, there has been a surge in research [52, 80, 22, 21, 48] dedicated to enhancing the capacity of MLLMs to process interleaved image-text inputs effectively. However, these models primarily focus on generative tasks, overlooking the importance of introducing discriminative constraints. In this paper, we propose a structure-induced joint training strategy for unifying generative and discriminative tasks, further enhancing the capabilities of MLLMs, especially those requiring cognitive and discriminative abilities. ", "page_idx": 2}, {"type": "text", "text": "Vision-Language Pre-training. Vision-Language Pre-training primarily come in two forms: singlestream and dual-stream. In single-stream models, the embeddings for the image and text modalities are concatenated and jointly encoded [39, 50], while in dual-stream models, they are encoded by separate modality-specific encoders with optional cross-modality fusion [73, 31, 5]. These models have shown effectiveness in tasks such as classification and retrieval. However, they face challenges including difficulty in processing complex composed sequences [53, 54] and limited ability to discern detailed semantic differences [81, 79]. Recent attempts to utilize generative MLLMs for discriminative tasks have faced limitations, such as singular discriminative tasks [89], weak discriminative task performance [40], poor generalization [59], and compromised generative capabilities [8]. ", "page_idx": 2}, {"type": "text", "text": "LLMs for Retrieval. Early models for retrieval primarily focused on word representations [16, 64, 74], with minimal generative capabilities. Some recent works have endeavored to fine-tune generative pre-trained LLMs to generate discriminative embeddings, albeit at the expense of compromising the model\u2019s original generative capabilities [44, 70, 65, 63, 24, 71]. GRIT [66] integrates generative and discriminative tasks in NLP and demonstrates mutual benefits between them. However, its training cost is prohibitively high compared to individual tasks. Moreover, due to its specialized attention mechanism, the model can only be trained from scratch. ", "page_idx": 2}, {"type": "text", "text": "Retrieval-Augmented Generation. Retrieval-Augmented Generation (RAG)[25, 69], which harnesses the advanced inference capabilities of LLMs along with external knowledge, has the potential to significantly mitigate issues related to long-tail entities and reduce the occurrence of hallucinatory responses [29, 36, 101, 77, 90, 92, 97]. Recently, there have also been related studies in the multimodal domain attempting to utilize retrieval augmentation [93, 96]. These methods typically require an additional retrieval module (e.g., CLIP), leading to component optimization challenges where the overall model performance is affected by the performance of the retrieval model, as well as concerns regarding the compatibility between the retrieval model and the MLLMs. Furthermore, retrieval modules like CLIP struggle to handle compositional or fine-grained scenarios, posing certain challenges for retrieval. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "As illustrated in Figure 3, we initially introduce the problem formulation and offer an overview of our structure-induced joint training strategy in Section 3.1. Subsequently, we delve into the specifics of dynamic sequence alignment algorithm in Section 3.2. Finally, we further introduce the Triple Kernel to aid in discriminating detailed semantics in Section 3.3. ", "page_idx": 3}, {"type": "image", "img_path": "w67vRHZF13/tmp/91443c95ca8d784c75c8d1c5fd15d061ea2bf0b04130af4893e4251db8314584.jpg", "img_caption": ["3.1 Problem Formulation and Architecture Overview "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 3: (a) Dynamic Sequence Alignment. Semantically matched slices are connected with a blue dashed line. The arrows indicate the direction of the ordered temporal alignment path. With these alignments, we can obtain the similarity between two interleaved inputs for training. (b) Sugar Framework. Sugar supports both multi-modal generation and retrieval simultaneously. ", "page_idx": 3}, {"type": "text", "text": "We view the interleaved image-text sequence as the general format for input samples, where images and textual data are alternately arranged. Typically, Multimodal Large Language Models (MLLMs) [55, 52, 11, 6] are tailored to generate text based on such input sequences, and it is conventionally optimized using self-regressive loss $\\mathcal{L}_{g}$ . A special scenario arises when the input comprises only one image and a question, prompting the MLLMs to generate an answer accordingly. ", "page_idx": 3}, {"type": "text", "text": "While intuitive, this optimization objective solely supervises text generation and lacks constraints on the hidden states of the entire interleaved sequence input. Additionally, the existing generative training framework struggles to fully distinguish input semantics in certain contexts, such as discerning fine-grained object details. Consequently, it fails to adequately capture the global information or distinguish detailed semantics of the input samples. ", "page_idx": 3}, {"type": "text", "text": "Hence, we introduce a structure-induced constraint $\\mathcal{L}_{d}$ (see in Figure 2), which explicitly imposes the semantic relationships between different input samples as an induced structural constraint on the hidden states of MLLMs, facilitating the model in capturing global semantics. We conceptualize the derivation of semantic relationships between input samples as a Dynamic Sequence Alignment problem [67]. Additionally, we straightforwardly select a token in the hidden state of the MLLM to encompass all preceding input information, eliminating the need for training any specialized tokens. ", "page_idx": 3}, {"type": "text", "text": "To further effectively distinguish detailed semantics, we integrate a novel kernel into the Dynamic Time Warping framework. Leveraging the strengths of various discriminative pre-trained models. Combined with this newly proposed loss with a hyperparameter $\\alpha$ , the training objective can be formulated as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\mathcal{L}_{g}+\\alpha\\mathcal{L}_{d}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "3.2 Dynamic Sequence Alignment ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We formulate the computation of relationships within input interleaved sequences as a dynamic sequence alignment problem, and solve it by global alignment kernel. For two interleaved image-text sequence, each consisting of $n$ and $m$ images/sentences in total respectively (which we\u2019ll refer to as slices later on). We encode and normalize each slice, resulting in two sequences ${\\bf x}=(x_{1},...,x_{n})$ and $\\mathbf{y}=(y_{1},\\dots,y_{m})$ all of which take values in a state space $\\mathcal{X}$ , that is two elements of $\\chi^{\\star}\\ {\\stackrel{\\mathrm{def}}{=}}\\ \\bigcup_{i=1}^{\\infty}\\chi^{i}$ . In our setting, $\\mathcal{X}$ is simply $\\mathbb{R}^{d}$ , $d$ refers to the feature dimension. We define the global alignment kernel as follows, and it has been proved to be positive-definite under mild conditions and may prove more robust to quantify the similarity of two sequences [73, 68]: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\nK(\\mathbf x,\\mathbf y)=\\sum_{\\pi\\in\\mathcal{A}(\\mathbf x,\\mathbf y)}\\prod_{i=1}^{|\\pi|}e^{-\\phi_{\\sigma}}\\in(0,1]\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Following the suggestion by [18], we let $\\begin{array}{r}{\\varphi_{\\sigma}=\\frac{1}{2\\sigma^{2}}\\varphi\\left(x_{\\pi_{1}(i)},y_{\\pi_{2}(i)}\\right)+\\log(2-e^{-\\frac{\\varphi\\left(x_{\\pi_{1}(i)},y_{\\pi_{2}(i)}\\right)}{2\\sigma^{2}}}),}\\end{array}$ $\\sigma$ is standard deviation, and it can be calculated by $\\begin{array}{r}{\\sigma=\\delta\\sqrt{\\frac{M+N}{2}}}\\end{array}$ for $x_{i},y_{i}$ in $\\mathbf{X},\\mathbf{y}$ . $\\delta$ is a fixed pre-defined hyperparameter and $\\varphi\\left(x_{\\pi_{1}\\left(i\\right)},y_{\\pi_{2}\\left(i\\right)}\\right)$ is the distance between slice $x_{\\pi_{1}(i)}$ and $y_{\\pi_{2}(i)}$ for an alignment (details for the definition of alignment can be seen in Appendix D.2). ", "page_idx": 4}, {"type": "text", "text": "Due to the causal attention mechanism, the token in hidden state of MLLM can encapsulate information from preceding tokens in the sequence. Therefore, we directly utilize the last token $d_{i}$ of a sequence from the MLLM\u2019s hidden state and map it to the $r_{i}$ using an MLP to represent the entire in-context sequence. During training, we obtain a set of $(r_{1},r_{2},\\ldots,r_{n})$ and their corresponding input sequence embedding set $\\left(\\mathbf{x}_{1},\\mathbf{x}_{2},\\ldots,\\mathbf{x}_{n}\\right)$ . It is noteworthy that $r_{i}$ and $r_{j}$ $\\mathbf{x}_{i}$ and $\\mathbf{X}_{j}$ ) may originate from the same sequence but occupy different positions, thus enabling our method to utilize samples more efficiently. ", "page_idx": 4}, {"type": "text", "text": "Leveraging the GAK, we can derive the similarity matrix of $(r_{1},r_{2},\\ldots,r_{n})$ and $\\left(\\mathbf{x}_{1},\\mathbf{x}_{2},\\ldots,\\mathbf{x}_{n}\\right)$ distinctively, denoted as ${\\mathcal{M}}^{r}$ , $\\mathcal{M}^{l}\\in\\mathbb{R}^{n\\times n}$ . For imposing the semantic relationships between different input samples as an induced structural constraint on the hidden state of MLLMs, we employ Mean Squared Error (MSE) loss aligned ${\\mathcal{M}}^{r}$ with the label matrix $\\mathcal{M}^{l}$ . This approach eliminates the need for pre-defined label (i.e., positive and negative candidates) during training, allowing seamless integration into the aforementioned training framework (for specific training templates, please refer to Appendix E.1). Thus, we have the discriminative loss $\\mathcal{L}_{d}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{d}=\\frac{1}{n}\\sum_{i=1}^{n}\\sum_{j=1}^{n}\\big(m_{i j}^{r}-m_{i j}^{l}\\big)^{2}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Additionally, when both $\\mathbf{X}$ and $\\mathbf{y}$ contain only one slice, the computed result of the formula is monotonically increasing with the directly calculated cosine similarity (proof can be seen in Appendix 3). Therefore, in such cases, we simplify the computation by directly using cosine similarity. If $r_{i}$ and $r_{j}$ comes from the same input interleaved sample, we manually set $m_{i j}^{l}=1$ . ", "page_idx": 4}, {"type": "text", "text": "3.3 Detailed Semantics Modeling ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To further effectively distinguish detailed semantics, we further propose the Triple Kernel (TK), a positive definite kernel compatible with the previous framework. The TK leverages representations from diverse pre-trained discriminative models across uni-modal and cross-modal settings, harnessing their respective strengths. The definition is as below: ", "page_idx": 4}, {"type": "text", "text": "For two slice $a,b\\in\\mathbb{R}^{d}$ , meets (i) $|a|=|b|=2\\,,d=d_{1}\\!+\\!d_{2},$ $a=\\operatorname{concat}(a_{1},a_{2}),b=\\operatorname{concat}(b_{1},b_{2})$ , $a_{1},b_{1}\\in\\mathbb{R}^{d_{1}},a_{2},b_{2}\\in\\mathbb{R}^{d_{2}}$ and $\\left|a_{1}\\right|=\\left|a_{2}\\right|=\\left|b_{1}\\right|=\\left|b_{2}\\right|=1$ ; or (ii) $|a|=|b|=1$ . We define tripe kernel as follows: ", "page_idx": 4}, {"type": "equation", "text": "$\\varphi(a,b)=\\left\\{\\begin{array}{c c}{||a_{1}-b_{1}||^{2}}&{|a|=|b|=2}\\\\ {||a_{2}-b_{2}||^{2}}&{|a|=|b|=2}\\\\ {||a-b||^{2}}&{}\\end{array}\\right.$ ", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We prove triple kernel $\\varphi$ is a conditionally positive-definite kernel defined on $\\mathcal{X}\\times\\mathcal{X}\\rightarrow\\mathbb{R}$ (Appendix 2), aligning with the kernel definition in [18], thereby possessing its properties. ", "page_idx": 4}, {"type": "text", "text": "In practice, we let the feature dimension $d=d_{1}+d_{2}$ . For images, we employ DINOv2-base [68] and CLIP ViT-L/14 [73] for encoding, then concatenate the embeddings after normalization. For sentences, we utilize BGE-base [87] and CLIP ViT-L/14, keeping the dimension unchanged. By utilizing the Triple Kernel, we can fully leverage the strengths of these three models, effectively distinguishing detailed semantics. ", "page_idx": 4}, {"type": "image", "img_path": "w67vRHZF13/tmp/48c3c2eb72754911091025cfef5ac2455339b96edda020b4ccf4414ce6ffcdc9.jpg", "img_caption": ["Retrieval at Different Place "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 4: Selected examples for various image-text tasks. The pink background indicates retrieval results, while the blue background indicates generated results. More examples are provided in the Appendix F.2. ", "page_idx": 5}, {"type": "table", "img_path": "w67vRHZF13/tmp/c90f6b43b15c0415356e29099991c2ae776d91f8711f5624e1a7286baf6e10be.jpg", "table_caption": [], "table_footnote": ["Table 1: Comparison with state-of-the-art methods on 11 visual-language benchmarks. We mark the best performance bold and the second-best underlined. Benchmark names are abbreviated due to space limits. VQA-v2 [27]; GQA [35]; VizWiz [28]; SQAI: ScienceQA-IMG [61]; VQAT: TextVQA [76]; POPE [51]; $\\mathbf{M}\\mathbf{M}\\mathbf{E}^{\\mathrm{P}}$ , MMEC: MME Perception, MME Cognition [95]; MMB: MMBench [58]; LLaVAWd: LLaVA-Bench(In-the-Wild)-Detail [56]; MM-Vet [99]. \u2217indicates the training images of the datasets are observed during training. "], "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To assess Sugar\u2019s generative ability, we conduct a comprehensive comparison with state-of-the-art models on 11 commonly used visual-language benchmarks in Section 4.2. Furthermore, we evaluate more complicated multimodal comprehension tasks on DEMON with 29 datasets in Section 4.3. For discriminative tasks, we compare performance across three different retrieval tasks: image-text retrieval, interleaved retrieval, and fine-grained retrieval in Section 4.4. Subsequently, we leverage Sugar\u2019s discriminative ability for retrieval-augmented generation compared with common used retrieve module in Section 4.5. Finally, we conduct ablation experiments to analyze the effectiveness of our method in Section 4.6. ", "page_idx": 6}, {"type": "text", "text": "4.1 Setup", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We apply our method to VILA [52], a recent state-of-the-art MLLM supporting interleaved input. We further fine-tune VILA using LoRA [30]. Details about the experiments setting, datasets and the instruction examples, please check in Appendix E. ", "page_idx": 6}, {"type": "text", "text": "4.2 Multimodal Comprehension on 11 Benchmarks ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We conduct a comprehensive comparison with state-of-the-art models on 11 commonly used benchmarks, as shown in Table 1. Compared to existing models, Sugar achieves remarkable improvements over the second-best performing model on tasks requiring fine-grained semantics (i.e., LLaVAWd [56], VizWiz [28], SQA [61] improve by $8\\%$ , $4.5\\%$ , $1.8\\%$ respectively) and benchmarks for detecting hallucinations (i.e., POPE [51]), while maintaining competitive results in other tasks. Notably, Sugar excels in discriminative tasks and still achieves 5 state-of-the-art results and 3 second-best results on 11 benchmarks for generative tasks, even outperforming some models larger than 7B. Our results demonstrate the benefits of incorporating the discriminative loss, aiding in fine-grained semantic tasks and reducing hallucinations. ", "page_idx": 6}, {"type": "text", "text": "4.3 Complicated Multimodal Comprehension on DEMON ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Table 2 demonstrates the superior performance of Sugar on the DEMON benchmark, which comprises 7 categories and a total of 29 sub-tasks. These tasks are considerably more complex than the previously used 11 common benchmarks. DEMON is tailored to evaluate the capacity of models and systems to understand demonstrative instructions that include multiple, interleaved, and multimodal contexts, presenting the essential information needed to complete a task. Sugar surpasses the previous stateof-the-art model on the DEMON benchmark, VPG-C [47], across 6 of 7 categories. For example, we achieve performance improvements of $36.1\\%$ in Text-Rich Images QA (TRQA) tasks and $17.2\\%$ in Visual Relation Inference (VRI) tasks, both of which require detailed semantics, compared to the second-best performing model. This underscores our advanced ability to associate interleaved ", "page_idx": 6}, {"type": "table", "img_path": "w67vRHZF13/tmp/04d570a2b920396416cb2512277324a1fe839de40d1294e4fe4d82d873341c5b.jpg", "table_caption": [], "table_footnote": ["Sugar Vicuna-7B 51.8 34.3 32.3 16.8 64.4 65.9 51.7 "], "page_idx": 7}, {"type": "table", "img_path": "w67vRHZF13/tmp/9f08c1e5ff2f66e4140260acf5995ca0f7dc8cdddaed0a1bc6dcf75f41783156.jpg", "table_caption": ["(b) VIST "], "table_footnote": ["Table 2: Comparision with state-of-the-art method on DEMON [47] benchmark. "], "page_idx": 7}, {"type": "text", "text": "Table 3: Retrieval results compared with previous models, reported by Recall $@k$ for (a)(b) and Accuracy $(\\%)$ for (c). (a) MSCOCO for image-text retrieval: FROMAGe(d) indicates the FROMAGe model pre-trained only with discriminative loss, and FROMAGe $(\\mathrm{g+d})$ indicates joint training with both discriminative and generative losses. (b) VIST for interleaved retrieval: \u2020 indicates retrieval over images not previously seen in the story sequence. $\\\"5c{+}4\\mathrm{i\"}$ is shorthand for 5 captions and 4 images, and $\"5c\"$ is shorthand for 5 captions. (c) Winoground for fine-grained retrieval. ", "page_idx": 7}, {"type": "text", "text": "text-image inputs for stronger in-context understanding, and Sugar\u2019s strong capability to capture global semantics in interleaved sequences, facilitated by joint training with discriminative loss. ", "page_idx": 7}, {"type": "text", "text": "4.4 Zero-shot Cross-modal Information Retrieval ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Image-text Retrieval. We evaluated the performance of Sugar on the widely adopted MSCOCO [38] dataset in the context of a standard image-text retrieval task. Sugar demonstrated comparable performance to FROMAGe [40] in $\\mathbf{R}@1$ and surpassed it in ${\\bf R}@5$ and ${\\bf R}\\@10$ , highlighting Sugar\u2019s superiority in normal retrieval tasks. ", "page_idx": 7}, {"type": "text", "text": "Interleaved Retrieval. To assess the proficiency of Sugar in processing multimodal contextual information, we evaluated its performance in retrieving relevant images conditioned on sequences of interleaved image-text inputs from the Visual Storytelling (VIST) dataset [32]. We conducted evaluations across several experimental configurations, following the same setup as FROMAGe [40] (see Appendix F.1). Our results show that Sugar outperforms FROMAGe in most settings, particularly achieving a $20.3\\%$ improvement in the $5c{+}4\\mathrm{i}$ configuration, significantly surpassing both CLIP and BLIP-2. This demonstrates that our method effectively leverages MLLMs\u2019 ability to handle complex interleaved sequence inputs, thereby achieving superior retrieval performance. ", "page_idx": 7}, {"type": "text", "text": "Fine-grained Retrieval. We tested fine-grained retrieval using the Winoground dataset [79], which evaluates the ability to perform vision-linguistic compositional reasoning. Surprisingly, Sugar outperformed all discriminative pre-trained models (both single-stream and dual-stream encoder architectures), achieving improvements of $5.3\\%$ , $77.1\\%$ , and $86.2\\%$ over the second-best model in the Text, Image, and Group dimensions, respectively. This demonstrates Sugar\u2019s strong capability to distinguish detailed semantics and performing compositional reasoning. ", "page_idx": 7}, {"type": "text", "text": "4.5 Retrieval-Augmented Generation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Due to Sugar\u2019s dual capabilities in both discrimination and generation, we can achieve retrieval augmentation without the need for an additional retrieval module. For performing retrieval-augmented generation (RAG), we selected two tasks, namely $\\mathrm{VizWiz}$ and $\\mathrm{SQA}^{\\hat{\\mathrm{I}}}$ , as they offer held-in data that were not seen during model training. We utilized a mixed set comprising the widely-used LLaVA-1.5 SFT subset and the held-in datasets of the two tasks as the knowledge base and employed different ", "page_idx": 7}, {"type": "table", "img_path": "w67vRHZF13/tmp/53667df1682f33cc49646644487b0071702b06203e134cfed0445acc844d7668.jpg", "table_caption": ["(a) Retrieval-Augmented Generation "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "w67vRHZF13/tmp/aca0d2bd09cdfa6ad7e962918a040dca15039420f92da53652042b687f010def.jpg", "table_caption": ["(b) Ablation Study "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 4: (a) Retrieval-Augmented Generation. (b) Ablation Study. For MSCOCO, we report the ${\\bf R}@5$ in text-to-image retrieval. For VIST, we report the ${\\bf R}@5$ of retrieving an image given 5 captions and 4 images. For Winoground, we report the Image score. For other tasks, we report Accuracy $(\\%)$ . ", "page_idx": 8}, {"type": "text", "text": "retrieval modules to retrieve relevant knowledge for the MLLM. The results are as follows: (i) We observed a drop in performance for LLaVA-1.5 with RAG in all tasks. This may be because LLaVA is designed solely for single-image input, without the ability to utilize in-context external knowledge. $(i i)$ Compared to VILA, Sugar\u2019s performance improved in both tasks, whereas VILA improved in $\\mathrm{SOA^{I}}$ but decreased in VizWiz. These findings suggest that Sugar\u2019s retrieved knowledge is more beneficial, while the knowledge retrieved by CLIP and BLIP-2 may hinder performance. ", "page_idx": 8}, {"type": "text", "text": "4.6 Ablations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Importance of Both Tasks. (1) w/o data $g$ : As shown in Table 4, when we reduce the amount of data for discriminative tasks (Row 2), there are performance drops of $1.5\\%$ in hallucination detection tasks (i.e., POPE) and $4.2\\%$ in interleaved multi-modal comprehension tasks (i.e., KGQA). (2) w/o $\\mathbf{data}_{d}$ : Similarly, reducing the data for generative tasks (Row 3), the performance on generative tasks declines with a $10.1\\%$ decrease in VIST, which requires global semantics capturing, and a $4.1\\%$ decrease in Winoground, which necessitates fine-grained semantic understanding. This indicates that generative and discriminative training can mutually benefit each other. ", "page_idx": 8}, {"type": "text", "text": "Effectiveness of Individual Components. (1) w/o GAK: When we exclude the Global Alignment Kernel (GAK) (Row 4) and resort to using the average similarity for the slices, a notable decrease in performance is observed across several interleaved image-text tasks (i.e., a $5.1\\%$ decrease in KQGA and a $28.3\\%$ decrease in VIST). This underscores the fundamental role of GAK in aiding Sugar to capture global semantics effectively. (2) w/o TK: Upon removal of the Triple Kernel (TK) (Row 5) and utilization of CLIP for encoding the input sequence instead, a dramatic performance decline is evident in Winoground, with a $43.5\\%$ decrease. This underscores the significant role of TK in facilitating the distinction of detailed semantics. (3) w/o AvgPool: When solely using the last token for retrieval, a general decline in performance is observed across discriminative tasks, with decreases of $19.1\\%$ for MSCOCO, $22.6\\%$ for VIST, and $13.2\\%$ for Winoground. This phenomenon may be attributed to the last token of an image often corresponding to a pooling token, containing relatively weaker semantic information. Utilizing all image tokens and performing AvgPooling tends to yield greater improvements in retrieval tasks. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Vision-Language Models (VLMs) have been trained using both generative and discriminative paradigms, each with distinct advantages and limitations. To bridge this gap, we introduce Structureinduced approach to unify generative and discriminative paradigms, which imposes semantic relationships between input samples, thereby enhancing the MLLM\u2019s ability to capture global semantics and distinguish fine-grained details. This approach effectively balances generative and discriminative tasks, yielding synergistic benefits. Extensive experiments demonstrate the effectiveness of our approach, achieving state-of-the-art results in multiple generative tasks, particularly those requiring cognitive and discrimination abilities, while also demonstrating competitive performance in discriminative tasks such as image-text retrieval and achieving state-of-the-art results in interleaved and fine-grained retrieval. Furthermore, employing a retrieval-augmented generation strategy within a single model leads to additional improvements, offering a promising direction for future research. ", "page_idx": 8}, {"type": "text", "text": "Acknowledgment. This work has been supported in part by the Key Research and Development Projects in Zhejiang Province (No. 2024C01106, 2024C01028), the NSFC (No. 62272411), the National Key Research and Development Project of China (2018AAA0101900), and Ant Group. ", "page_idx": 8}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 1   \n[2] Armen Aghajanyan, Bernie Huang, Candace Ross, Vladimir Karpukhin, Hu Xu, Naman Goyal, Dmytro Okhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis, et al. Cm3: A causal masked multimodal model of the internet. arXiv preprint arXiv:2201.07520, 2022. 3   \n[3] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in neural information processing systems, 35:23716\u201323736, 2022. 3   \n[4] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al. Openflamingo: An opensource framework for training large autoregressive vision-language models. arXiv preprint arXiv:2308.01390, 2023. 8   \n[5] Jinbin Bai, Tian Ye, Wei Chow, Enxin Song, Qing-Guo Chen, Xiangtai Li, Zhen Dong, Lei Zhu, and Shuicheng Yan. Meissonic: Revitalizing masked generative transformers for efficient high-resolution text-to-image synthesis. arXiv preprint arXiv:2410.08261, 2024. 3   \n[6] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint, 2023. 3, 4, 7, 19   \n[7] Zechen Bai, Pichao Wang, Tianjun Xiao, Tong He, Zongbo Han, Zheng Zhang, and Mike Zheng Shou. Hallucination of multimodal large language models: A survey. arXiv preprint arXiv:2404.18930, 2024. 1   \n[8] Oriol Barbany, Michael Huang, Xinliang Zhu, and Arnab Dhua. Leveraging large language models for multimodal search. arXiv preprint arXiv:2404.15790, 2024. 2, 3 [9] Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim. Coyo- $700\\mathrm{m}$ : Image-text pair dataset. Coyo-700m: Image-text pair dataset, 2022. 20   \n[10] Yinghsan Chang, Mridu Narang, Hisami Suzuki, Guihong Cao, Jianfeng Gao, and Yonatan Bisk. WebQA: Multihop and Multimodal QA. 2021. 2, 20, 30   \n[11] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. Minigptv2: Large language model as a unified interface for vision-language multi-task learning. arXiv:2310.09478, 2023. 4, 19   \n[12] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing multimodal llm\u2019s referential dialogue magic. arXiv preprint arXiv:2306.15195, 2023. 7   \n[13] Zhuo Chen, Jiaoyan Chen, Yuxia Geng, Jeff Z Pan, Zonggang Yuan, and Huajun Chen. Zeroshot visual question answering using knowledge graph. In The Semantic Web\u2013ISWC 2021: 20th International Semantic Web Conference, ISWC 2021, Virtual Event, October 24\u201328, 2021, Proceedings 20, pages 146\u2013162. Springer, 2021. 30   \n[14] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with $90\\%^{*}$ chatgpt quality, March 2023. 19   \n[15] Jo\u00e3o Coelho, Bruno Martins, Jo\u00e3o Magalh\u00e3es, Jamie Callan, and Chenyan Xiong. Dwell in the beginning: How language models embed long documents for dense retrieval. arXiv preprint arXiv:2404.04163, 2024. 2   \n[16] Alexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00efc Barrault, and Antoine Bordes. Supervised learning of universal sentence representations from natural language inference data. arXiv preprint arXiv:1705.02364, 2017. 3   \n[17] Marco Cuturi and Mathieu Blondel. Soft-dtw: a differentiable loss function for time-series. In International conference on machine learning, pages 894\u2013903. PMLR, 2017. 17   \n[18] Marco Cuturi, Jean-Philippe Vert, Oystein Birkenes, and Tomoko Matsui. A kernel for time series based on global alignments. In 2007 IEEE International Conference on Acoustics, Speech and Signal Processing-ICASSP\u201907, volume 2, pages II\u2013413. IEEE, 2007. 5, 17   \n[19] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale N Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. Advances in Neural Information Processing Systems, 36, 2024. 3, 7, 8   \n[20] Alex Fang, Albin Madappally Jose, Amit Jain, Ludwig Schmidt, Alexander Toshev, and Vaishaal Shankar. Data filtering networks. arXiv preprint arXiv:2309.17425, 2023. 22   \n[21] Hao Fei, Shengqiong Wu, Wei Ji, Hanwang Zhang, Meishan Zhang, Mong-Li Lee, and Wynne Hsu. Video-of-thought: Step-by-step video reasoning from perception to cognition. In Proceedings of the International Conference on Machine Learning, 2024. 3   \n[22] Hao Fei, Shengqiong Wu, Hanwang Zhang, Tat-Seng Chua, and Shuicheng Yan. Vitron: A unified pixel-level vision llm for understanding, generating, segmenting, editing. 2024. 3   \n[23] Hao Fei, Shengqiong Wu, Meishan Zhang, Min Zhang, Tat-Seng Chua, and Shuicheng Yan. Enhancing video-language representations with structural spatio-temporal alignment. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. 3   \n[24] Minghe Gao, Juncheng Li, Hao Fei, Wei Ji, Guoming Wang, Wenqiao Zhang, Siliang Tang, and Yueting Zhuang. De-fine: Decomposing and refining visual programs with auto-feedback. arXiv preprint arXiv:2311.12890, 2023. 3   \n[25] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. Retrieval-augmented generation for large language models: A survey. arXiv preprint arXiv:2312.10997, 2023. 3   \n[26] Zhiqi Ge, Hongzhe Huang, Mingze Zhou, Juncheng Li, Guoming Wang, Siliang Tang, and Yueting Zhuang. Worldgpt: Empowering llm as multimodal world model. arXiv preprint arXiv:2404.18202, 2024. 24   \n[27] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6904\u20136913, 2017. 7, 20   \n[28] Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P Bigham. Vizwiz grand challenge: Answering visual questions from blind people. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3608\u20133617, 2018. 3, 7, 20   \n[29] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented language model pre-training. In International conference on machine learning, pages 3929\u20133938. PMLR, 2020. 4   \n[30] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 7, 20   \n[31] Ronghang Hu and Amanpreet Singh. Unit: Multimodal multitask learning with a unified transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1439\u20131449, 2021. 3   \n[32] Ting-Hao Huang, Francis Ferraro, Nasrin Mostafazadeh, Ishan Misra, Aishwarya Agrawal, Jacob Devlin, Ross Girshick, Xiaodong He, Pushmeet Kohli, Dhruv Batra, et al. Visual storytelling. In Proceedings of the 2016 conference of the North American chapter of the association for computational linguistics: Human language technologies, pages 1233\u20131239, 2016. 8, 20   \n[33] Xiaohua Huang, Abhinav Dhall, Roland Goecke, Matti Pietikainen, and Guoying Zhao. A global alignment kernel based approach for group-level happiness intensity estimation. arXiv preprint arXiv:1809.03313, 2018. 2   \n[34] Xuanwen Huang, Wei Chow, Yang Wang, Ziwei Chai, Chunping Wang, Lei Chen, and Yang Yang. One graph model for cross-domain dynamic link prediction. arXiv preprint arXiv:2402.02168, 2024. 19   \n[35] Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 6700\u20136709, 2019. 7, 20   \n[36] Wei Ji, Renjie Liang, Zhedong Zheng, Wenqiao Zhang, Shengyu Zhang, Juncheng Li, Mengze Li, and Tat-seng Chua. Are binary annotations sufficient? video moment retrieval via hierarchical uncertainty-based active learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 23013\u201323022, 2023. 4   \n[37] Jeff Johnson, Matthijs Douze, and Herv\u00e9 J\u00e9gou. Billion-scale similarity search with gpus. IEEE Transactions on Big Data, 7(3):535\u2013547, 2019. 23   \n[38] Andrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descriptions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3128\u20133137, 2015. 8, 20, 23   \n[39] Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-and-language transformer without convolution or region supervision. In International conference on machine learning, pages 5583\u20135594. PMLR, 2021. 3   \n[40] Jing Yu Koh, Ruslan Salakhutdinov, and Daniel Fried. Grounding language models to images for multimodal generation. arXiv preprint arXiv:2301.13823, 2, 2023. 2, 3, 8, 23, 24   \n[41] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation via large language model. arXiv preprint arXiv:2308.00692, 2023. 3   \n[42] Hugo Lauren\u00e7on, Daniel van Strien, Stas Bekman, Leo Tronchon, Lucile Saulnier, Thomas Wang, Siddharth Karamcheti, Amanpreet Singh, Giada Pistilli, Yacine Jernite, et al. Introducing idefics: An open reproduction of state-of-the-art visual language model, 2023. URL https://huggingface. co/blog/idefics. Accessed, pages 09\u201318, 2023. 3, 7   \n[43] Hector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge. In Thirteenth international conference on the principles of knowledge representation and reasoning, 2012. 29   \n[44] Chaofan Li, Zheng Liu, Shitao Xiao, and Yingxia Shao. Making large language models a better foundation for dense retrieval. arXiv preprint arXiv:2312.15503, 2023. 3   \n[45] Juncheng Li, Minghe Gao, Longhui Wei, Siliang Tang, Wenqiao Zhang, Mengze Li, Wei Ji, Qi Tian, Tat-Seng Chua, and Yueting Zhuang. Gradient-regulated meta-prompt learning for generalizable vision-language models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2551\u20132562, 2023. 3   \n[46] Juncheng Li, Xin He, Longhui Wei, Long Qian, Linchao Zhu, Lingxi Xie, Yueting Zhuang, Qi Tian, and Siliang Tang. Fine-grained semantically aligned vision-language pre-training. Advances in neural information processing systems, 35:7290\u20137303, 2022. 2   \n[47] Juncheng Li, Kaihang Pan, Zhiqi Ge, Minghe Gao, Wei Ji, Wenqiao Zhang, Tat-Seng Chua, Siliang Tang, Hanwang Zhang, and Yueting Zhuang. Fine-tuning multimodal llms to follow zero-shot demonstrative instructions. In The Twelfth International Conference on Learning Representations, 2023. 2, 3, 7, 8, 20, 26 [48] Juncheng Li, Siliang Tang, Linchao Zhu, Wenqiao Zhang, Yi Yang, Tat-Seng Chua, Fei Wu, and Yueting Zhuang. Variational cross-graph reasoning and adaptive structured semantics learning for compositional temporal grounding. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(10):12601\u201312617, 2023. 3 [49] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models. In International conference on machine learning, pages 19730\u201319742. PMLR, 2023. 3, 7, 8 [50] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi. Align before fuse: Vision and language representation learning with momentum distillation. Advances in neural information processing systems, 34:9694\u20139705,   \n2021. 3 [51] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. arXiv preprint arXiv:2305.10355, 2023.   \n3, 7, 20 [52] Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models, 2023.   \n2, 3, 4, 7, 8, 19, 20, 22 [53] Weizhe Lin, Jinghong Chen, Jingbiao Mei, Alexandru Coca, and Bill Byrne. Fine-grained late-interaction multi-modal retrieval for retrieval augmented visual question answering. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. 1, 3 [54] Weizhe Lin, Jingbiao Mei, Jinghong Chen, and Bill Byrne. Preflmr: Scaling up fine-grained late-interaction multi-modal retrievers. arXiv preprint, (arXiv:2402.08327), 2024. 1, 3 [55] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744, 2023. 1, 3, 4, 7, 19, 20 [56] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. 7, 8, 20 [57] Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157\u2013173, 2024. 2 [58] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023. 7, 20 [59] Ziyu Liu, Zeyi Sun, Yuhang Zang, Wei Li, Pan Zhang, Xiaoyi Dong, Yuanjun Xiong, Dahua Lin, and Jiaqi Wang. Rar: Retrieving and ranking augmented mllms for visual recognition. arXiv preprint arXiv:2403.13805, 2024. 2, 3 [60] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 20 [61] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems,   \n35:2507\u20132521, 2022. 7, 20 [62] Yang Luo, Zangwei Zheng, Zirui Zhu, and Yang You. How does the textual information affect the retrieval of multimodal in-context learning? arXiv preprint arXiv:2404.12866, 2024. 3 [63] Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. Fine-tuning llama for multi-stage text retrieval. arXiv preprint arXiv:2310.08319, 2023. 3 [64] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. Advances in neural information processing systems, 26, 2013. 3   \n[65] Niklas Muennighoff. Sgpt: Gpt sentence embeddings for semantic search. arXiv preprint arXiv:2202.08904, 2022. 3   \n[66] Niklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet Singh, and Douwe Kiela. Generative representational instruction tuning. arXiv preprint arXiv:2402.09906, 2024. 3   \n[67] Meinard M\u00fcller. Dynamic time warping. Information retrieval for music and motion, pages 69\u201384, 2007. 2, 4, 17   \n[68] Maxime Oquab, Timoth\u00e9e Darcet, Th\u00e9o Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 5   \n[69] Kaihang Pan, Zhaoyu Fan, Juncheng Li, Qifan Yu, Hao Fei, Siliang Tang, Richang Hong, Hanwang Zhang, and Qianru Sun. Towards unified multimodal editing with enhanced knowledge collaboration. arXiv preprint arXiv:2409.19872, 2024. 3   \n[70] Kaihang Pan, Juncheng Li, Hongye Song, Hao Fei, Wei Ji, Shuo Zhang, Jun Lin, Xiaozhong Liu, and Siliang Tang. Controlretriever: Harnessing the power of instructions for controllable retrieval. arXiv preprint arXiv:2308.10025, 2023. 3   \n[71] Kaihang Pan, Juncheng Li, Wenjie Wang, Hao Fei, Hongye Song, Wei Ji, Jun Lin, Xiaozhong Liu, Tat-Seng Chua, and Siliang Tang. I3: I ntent-i ntrospective retrieval conditioned on i nstructions. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 1839\u20131849, 2024. 3   \n[72] Kaihang Pan, Siliang Tang, Juncheng Li, Zhaoyu Fan, Wei Chow, Shuicheng Yan, Tat-Seng Chua, Yueting Zhuang, and Hanwang Zhang. Auto-encoding morph-tokens for multimodal llm. arXiv preprint arXiv:2405.01926, 2024. 3   \n[73] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021. 1, 3, 5, 19, 22   \n[74] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bertnetworks. arXiv preprint arXiv:1908.10084, 2019. 3   \n[75] Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. Replug: Retrieval-augmented black-box language models. arXiv preprint arXiv:2301.12652, 2023. 3   \n[76] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8317\u20138326, 2019. 7, 20   \n[77] Krishna Srinivasan, Karthik Raman, Anupam Samanta, Lingrui Liao, Luca Bertelli, and Mike Bendersky. Quill: Query intent with large language models using retrieval augmentation and multi-stage distillation. arXiv preprint arXiv:2210.15718, 2022. 4   \n[78] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. Eva-clip: Improved training techniques for clip at scale. arXiv preprint arXiv:2303.15389, 2023. 22   \n[79] Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh, Adina Williams, Douwe Kiela, and Candace Ross. Winoground: Probing vision and language models for visio-linguistic compositionality. In CVPR, 2022. 1, 3, 8, 20, 24   \n[80] Changyao Tian, Xizhou Zhu, Yuwen Xiong, Weiyun Wang, Zhe Chen, Wenhai Wang, Yuntao Chen, Lewei Lu, Tong Lu, Jie Zhou, Hongsheng Li, Yu Qiao, and Jifeng Dai. Mm-interleaved: Interleaved image-text generative modeling via multi-modal feature synchronizer. arXiv preprint arXiv:2401.10208, 2024. 3   \n[81] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. arXiv preprint arXiv:2401.06209, 2024. 2, 3, 21   \n[82] Guangzhi Wang, Yixiao Ge, Xiaohan Ding, Mohan Kankanhalli, and Ying Shan. What makes for good visual tokenizers for large language models? arXiv preprint arXiv:2305.12223, 2023. 2   \n[83] Peng Wang, Qi Wu, Chunhua Shen, Anthony Dick, and Anton Van Den Hengel. Fvqa: Fact-based visual question answering. IEEE transactions on pattern analysis and machine intelligence, 40(10):2413\u20132427, 2017. 30   \n[84] Cong Wei, Yang Chen, Haonan Chen, Hexiang Hu, Ge Zhang, Jie Fu, Alan Ritter, and Wenhu Chen. Uniir: Training and benchmarking universal multimodal information retrievers. arXiv preprint arXiv:2311.17136, 2023. 23   \n[85] Shengqiong Wu, Hao Fei, Xiangtai Li, Jiayi Ji, Hanwang Zhang, Tat-Seng Chua, and Shuicheng Yan. Towards semantic equivalence of tokenization in multimodal llm. arXiv preprint arXiv:2406.05127, 2024. 1   \n[86] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any multimodal llm. In Proceedings of the International Conference on Machine Learning, 2024. 1   \n[87] Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff. C-pack: Packaged resources to advance general chinese embedding, 2023. 5   \n[88] Hu Xu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang, Russell Howes, Vasu Sharma, ShangWen Li, Gargi Ghosh, Luke Zettlemoyer, and Christoph Feichtenhofer. Demystifying clip data. arXiv preprint arXiv:2309.16671, 2023. 22   \n[89] Chenglin Yang, Siyuan Qiao, Yuan Cao, Yu Zhang, Tao Zhu, Alan Yuille, and Jiahui Yu. Ig captioner: Information gain captioners are strong zero-shot classifiers. arXiv preprint arXiv:2311.17072, 2023. 1, 2, 3   \n[90] Nan Yang, Tao Ge, Liang Wang, Binxing Jiao, Daxin Jiang, Linjun Yang, Rangan Majumder, and Furu Wei. Inference with reference: Lossless acceleration of large language models. arXiv preprint arXiv:2304.04487, 2023. 4   \n[91] Senqiao Yang, Tianyuan Qu, Xin Lai, Zhuotao Tian, Bohao Peng, Shu Liu, and Jiaya Jia. An improved baseline for reasoning segmentation with large language model. arXiv preprint arXiv:2312.17240, 2023. 3   \n[92] Zhuolin Yang, Wei Ping, Zihan Liu, Vijay Korthikanti, Weili Nie, De-An Huang, Linxi Fan, Zhiding Yu, Shiyi Lan, Bo Li, et al. Re-vilm: Retrieval-augmented visual language model for zero and few-shot image captioning. arXiv preprint arXiv:2302.04858, 2023. 4   \n[93] Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Rich James, Jure Leskovec, Percy Liang, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. Retrieval-augmented multimodal language modeling. arXiv preprint arXiv:2211.12561, 2022. 4   \n[94] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023. 3, 8   \n[95] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. A survey on multimodal large language models. arXiv preprint arXiv:2306.13549, 2023. 3, 7, 20   \n[96] Lili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller, Olga Golovneva, Tianlu Wang, Arun Babu, Binh Tang, Brian Karrer, Shelly Sheynin, et al. Scaling autoregressive multi-modal models: Pretraining and instruction tuning. arXiv preprint arXiv:2309.02591, 2(3), 2023. 4   \n[97] Qifan Yu, Juncheng Li, Longhui Wei, Liang Pang, Wentao Ye, Bosheng Qin, Siliang Tang, Qi Tian, and Yueting Zhuang. Hallucidoctor: Mitigating hallucinatory toxicity in visual instruction data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 4   \n[98] Qifan Yu, Juncheng Li, Yu Wu, Siliang Tang, Wei Ji, and Yueting Zhuang. Visually-prompted language model for fine-grained scene graph generation in an open world. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 21560\u201321571, 2023. 2, 3   \n[99] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. 7, 20   \n[100] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11975\u201311986, 2023. 22   \n[101] Wenqiao Zhang, Jiannan Guo, Mengze Li, Haochen Shi, Shengyu Zhang, Juncheng Li, Siliang Tang, and Yueting Zhuang. Boss: Bottom-up cross-modal semantic composition with hybrid counterfactual training for robust content-based image retrieval. arXiv preprint arXiv:2207.04211, 2022. 4   \n[102] Wenqiao Zhang, Tianwei Lin, Jiang Liu, Fangxun Shu, Haoyuan Li, Lei Zhang, He Wanggui, Hao Zhou, Zheqi Lv, Hao Jiang, et al. Hyperllava: Dynamic visual and language expert tuning for multimodal large language models. arXiv preprint arXiv:2403.13447, 2024. 3   \n[103] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. 3, 8   \n[104] Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex Fang, Youngjae Yu, Ludwig Schmidt, William Yang Wang, and Yejin Choi. Multimodal c4: An open, billion-scale corpus of images interleaved with text. Advances in Neural Information Processing Systems, 36, 2024. 20 ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "A Broader Impact ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The broader impact of Sugar, carries both potential benefits and risks upon deployment and release. Some considerations are unique due to their visual nature, while others mirror existing instructionfollowing Large Language Models (LLMs). Built upon Vicuna, CLIP, DINOv2, and BGE, Sugar inherits issues associated with LLMs and vision encoders. Below, we outline risks and mitigation strategies for its release. ", "page_idx": 16}, {"type": "text", "text": "Hallucination. Similar to other MLLMs, Sugar may generate outputs detached from facts or input data, posing concerns, especially in critical applications like medicine and the field related to security. ", "page_idx": 16}, {"type": "text", "text": "Biases. Bias from base models can transfer to Sugar, originating from both the vision encoder (CLIP) and the language decoder (Vicuna), potentially leading to biased outcomes or unfair representations. ", "page_idx": 16}, {"type": "text", "text": "Ethical Impacts. This study doesn\u2019t raise ethical concerns, as it doesn\u2019t involve subjective assessments or private data, only utilizing publicly available datasets. ", "page_idx": 16}, {"type": "text", "text": "Expected Societal Implications. A significant societal concern lies in potential misuse, such as fabricating unauthorized texts leading to misinformation, privacy breaches, and other damaging consequences. Strong ethical standards and ongoing surveillance are essential for mitigation. ", "page_idx": 16}, {"type": "text", "text": "These issues aren\u2019t unique to our method but are prevalent across different techniques for multiconcept customization. Despite the risks, we believe the benefits outweigh the potential harm, allowing ongoing investigation and improvement of the model while engaging the community in developing better mitigation strategies. Moreover, its release can foster new applications and research directions, contributing to the progress and responsible deployment of foundation models in visionlanguage tasks. ", "page_idx": 16}, {"type": "text", "text": "B Limitations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "$(i)$ Our method, while effective, may inherit limitations from the underlying models, such as hallucination in generating outputs detached from facts or input data and potential biases originating from the model we used. $(i i)$ The training data might inevitably contain mismatched image and text, which could adversely affect training. ", "page_idx": 16}, {"type": "text", "text": "C Mathematical Proof ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Theorem 1. The alignment kernel $K$ can be computed in quadratic complexity, namely in $O(m n d^{2})$ iterations. where $m,n$ denotes the length of two sequence and their hidden dimension all is $d$ , $m,n,d\\in\\mathbb{R}.$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. Given $\\mathbf{x}\\,=\\,(x_{1},\\ldots,x_{n})$ and $\\mathbf{y}\\,=\\,(y_{1},\\hdots,y_{m})$ two sequences of $\\varkappa^{\\star}$ , we set the doublesubscripted series $M_{i,j}$ as $M_{i,0}\\,=\\,0$ for $i=1,...,n$ , $M_{0,j}\\,=\\,0$ for $j\\,=\\,1,...,m$ , and $M_{0,0}\\,=\\,1$ . Computing recursively for $(i,j)\\in\\{1,...,n\\}\\times\\{1,...,m\\}$ the terms ", "page_idx": 16}, {"type": "equation", "text": "$$\nM_{i,j}=(M_{i,j-1}+M_{i-1,j-1}+M_{i-1,j})k(x_{i},y_{j})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "we obtain that $K(\\mathbf{x},\\mathbf{y})=M_{n,m}$ The result can be proved by recursion and is intuitively an equivalent of the Dynamic Time Warping(DTW) [67, 17] algorithm where the max-sum algebra is simply replaced by the sum-product one [18]. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "Theorem 2. triple kernel $\\varphi$ is a conditionally symmetric positive-definite kernel [18] defined on $\\mathcal{X}\\times\\mathcal{X}\\rightarrow\\mathbb{R}$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. (i) for two slice $a,b\\,\\in\\,\\mathbb{R}^{d}$ , meets $|a|\\,=\\,|b|\\,=\\,2$ , $d={d_{1}+d_{2}}$ , a = concat $\\!\\;(a_{1},a_{2}),b=$ concat $(b_{1},b_{2}),a_{1},b_{1}\\in\\mathbb{R}^{d_{1}},a_{2},b_{2}\\in\\mathbb{R}^{d_{2}}$ and $\\left|a_{1}\\right|=\\left|a_{2}\\right|=\\left|b_{1}\\right|=\\left|b_{2}\\right|=1$ : ", "page_idx": 16}, {"type": "text", "text": "let $a^{\\prime}=\\mathrm{concat}(a_{1}^{\\prime},a_{2}^{\\prime},a_{3}^{\\prime})$ , $a^{\\prime}=\\mathrm{concat}(b_{1}^{\\prime},b_{2}^{\\prime},b_{3}^{\\prime})$ : when $a^{\\prime}(b^{\\prime})$ is from text modal, we let $a\\prime=$ conca $\\langle a_{1},a_{2},\\mathbf{0}\\rangle(b^{\\prime}\\ =\\ \\mathrm{concat}(b_{1},b_{2},\\mathbf{0}))$ , and $a^{\\prime}\\ =\\ \\mathrm{concat}({a_{1},\\bf\\dot{0}},a_{2})(b^{\\prime}\\ =\\ \\mathrm{concat}(b_{1},\\bf0,\\it b_{2}))$ for image modal. $\\mathbf{0}\\in\\mathbb{R}^{d_{2}}$ . we can unify the Equation 4 first and second case in: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\varphi(a,b)=\\varphi(a^{\\prime},b^{\\prime})}\\\\ &{\\qquad=(|a_{2}^{\\prime}||b_{2}^{\\prime}|+|a_{3}^{\\prime}||b_{3}^{\\prime}|)||a_{1}^{\\prime}-b_{1}^{\\prime}||^{2}+(1-|a_{2}^{\\prime}||b_{2}^{\\prime}|)||a_{2}^{\\prime}-b_{2}^{\\prime}||^{2}}\\\\ &{\\qquad\\quad+\\left(1-|a_{3}^{\\prime}||b_{3}^{\\prime}|\\right)||a_{3}^{\\prime}-b_{3}^{\\prime}||^{2}}\\\\ &{\\qquad\\geq(|a_{2}^{\\prime}||b_{2}^{\\prime}|+|a_{3}^{\\prime}||b_{3}^{\\prime}|)||a_{1}^{\\prime}-b_{1}^{\\prime}||^{2}+0+0}\\\\ &{\\qquad\\geq0}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "As $(1-|a_{2}^{\\prime}||b_{2}^{\\prime}|)\\geq0$ and $(1-|a_{3}^{\\prime}||b_{3}^{\\prime}|)\\geq0$ . for any family $\\alpha_{1},\\alpha_{2},...\\alpha_{n}\\in\\mathcal{X}$ and $c_{1},c_{2},...,c_{n}\\in\\mathbb{R}$ , we have that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{i,j}c_{i}c_{j}\\varphi(x_{i},x_{j})=\\sum_{i,j}c_{i}c_{j}\\varphi(x_{i}^{\\prime},x_{j}^{\\prime})\\geq0\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "(ii) for two slice meets $|a|=|b|=1$ : for any family $\\alpha_{1},\\alpha_{2},...\\alpha_{n}\\in\\mathcal{X}$ and $c_{1},c_{2},...,c_{n}\\in\\mathbb{R}$ , we have that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{i,j}c_{i}c_{j}\\varphi(x_{i},x_{j})=\\sum_{i,j}c_{i}c_{j}||x_{i}-x_{j}||^{2}\\geq0\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Additionally, it\u2019s evident that for both (i) and (ii), $\\varphi(a,b)=\\varphi(b,a)$ . Therefore, triple kernel $\\varphi$ is a conditionally symmetric positive-definite kernel defined on $\\mathcal{X}\\times\\mathcal{X}\\rightarrow\\mathbb{R}$ \u53e3 ", "page_idx": 17}, {"type": "text", "text": "Theorem 3. when both $\\pmb{x}$ and $\\pmb{y}$ contain only one slice, GAK is monotonically increasing with the directly calculated cosine similarity. ", "page_idx": 17}, {"type": "text", "text": "Proof. let $\\mathbf{x}=(x),\\mathbf{y}=(y)$ , and cosine similarity of $x$ and $y$ is $c o s=c o s<x,y>$ . we can get ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sigma=\\delta\\sqrt{\\frac{M+N}{2}}=\\delta\\sqrt{\\frac{1+1}{2}}=\\delta\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and we have $\\varphi(a,b)=|a|^{2}+|b|^{2}-2c o s<a,b>=2(1-c o s<a,b>)$ , thus: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\varphi_{\\sigma}=\\frac{1}{2\\sigma^{2}}\\varphi\\left(x_{\\pi_{1}(i)},y_{\\pi_{2}(i)}\\right)+\\log\\left(2-e^{-\\frac{\\varphi\\left(x_{\\pi_{1}(i)},y_{\\pi_{2}(i)}\\right)}{2\\sigma^{2}}}\\right)}}\\\\ {{\\displaystyle\\qquad=\\frac{1}{2\\delta^{2}}\\varphi\\left(x_{\\pi_{1}(i)},y_{\\pi_{2}(i)}\\right)+\\log\\left(2-e^{-\\frac{\\varphi\\left(x_{\\pi_{1}(i)},y_{\\pi_{2}(i)}\\right)}{2\\delta^{2}}}\\right)}}\\\\ {{\\displaystyle\\qquad=\\frac{1-c o s<a,b>}{\\delta^{2}}+\\log\\left(2-e^{-\\frac{1-c o s<a,b>}{\\delta^{2}}}\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Letting $\\begin{array}{r}{t=\\frac{1-c o s<a,b>}{\\delta^{2}}}\\end{array}$ and substituting the result of $\\varphi_{\\sigma}=t+\\log(2-e^{-t})$ into Equation 2, we obtain: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{K(\\mathbf{x},\\mathbf{y})=\\displaystyle\\sum_{\\pi\\in A(\\mathbf{x},\\mathbf{y})}\\prod_{i=1}^{\\pi|}e^{-\\phi_{\\sigma}}}\\\\ &{\\phantom{x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x}}\\\\ &{\\phantom{x x x x x x x x x x x x x x x x x x x x x x x x x x x}=e^{-\\phi_{\\sigma}}}\\\\ &{\\phantom{x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x}=e^{-t+l o g(\\frac{1}{2-e^{-t}})}}\\\\ &{\\phantom{x x x x x x x x x x x x x x x}=e^{-t}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Letting $s=e^{-t}$ , we can further obtain: ", "page_idx": 17}, {"type": "equation", "text": "$$\nK(\\mathbf{x},\\mathbf{y})={\\frac{s}{2-s}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "As $\\cos\\langle a,b\\rangle\\in[-1,1]$ , $s$ strictly increases with $\\cos\\langle a,b\\rangle$ and $s\\in[e^{-\\frac{2}{\\delta^{2}}},1]$ when the hyperparameter $\\delta$ is fixed. Derivative of $K(\\mathbf x,\\mathbf y)$ can be obtained: ", "page_idx": 18}, {"type": "equation", "text": "$$\nK({\\bf x},{\\bf y})^{\\prime}=\\frac{2-s+s}{(2-s)^{2}}=\\frac{2}{(2-s)^{2}}>0\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Overall, when both $\\mathbf{X}$ and $\\mathbf{y}$ contain only one slice, GAK is monotonically increasing with the directly calculated cosine similarity. ", "page_idx": 18}, {"type": "text", "text": "D Method Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "D.1 Architecture Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We adopt the manifold multimodal model architecture [55, 52, 11, 6, 34], formulated as follows: ", "page_idx": 18}, {"type": "text", "text": "Visual Representation. We first process $x_{\\mathrm{img}}$ subject to a visual representation backbone $V_{\\omega}$ that outputs a sequence of features $p_{\\mathrm{img}}\\in\\mathbb{R}^{L\\times h_{\\mathrm{vision}}}$ where $p_{\\mathrm{img}}=V_{\\omega}(x_{\\mathrm{img}})$ . As an example, $p_{\\mathrm{img}}$ might be the patch features output by a Vision Transformer. ", "page_idx": 18}, {"type": "text", "text": "Vision-Language Projector. Next, we map $p_{\\mathrm{img}}$ to a sequence of embeddings $e_{\\mathrm{img}}\\in\\mathbb{R}^{L\\times h_{\\mathrm{text}}}$ via a learned projector $F_{\\psi}$ , where $e_{\\mathrm{img}}=F_{\\psi}(p_{\\mathrm{img}})$ . ", "page_idx": 18}, {"type": "text", "text": "Language Model. Finally, we concatenate the sequence $e_{\\mathrm{img}}$ with the text prompt embeddings $e_{\\mathrm{prompt}}=\\mathrm{embed}(u_{\\mathrm{prompt}})$ , passing the result to the language model. Generally, we have the interleaved image-text input $x_{\\mathrm{input}}$ by concatting all the $e_{\\mathrm{prompt}}$ and $e_{\\mathrm{img}}$ . The language model generates output text $u_{\\mathrm{gen}}=\\mathrm{LM}_{\\theta}(x_{\\mathrm{input}}^{\\phantom{\\dagger}})$ . ", "page_idx": 18}, {"type": "text", "text": "Retrieval Projector. For discriminative tasks, we select the token $d_{i}$ from MLLM\u2019s hidden state and map it to $r_{i}$ via a learned projector $F_{\\varphi}$ . ", "page_idx": 18}, {"type": "text", "text": "In Implementation, we utilize CLIP ViT-L/14 [73] as the visual encoder, and Vicuna 1.5 [14] as the language model. ", "page_idx": 18}, {"type": "text", "text": "D.2 Sequence Alignment ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "An alignment $\\pi$ of length $|\\pi|=p$ between two sequences $\\mathbf{X}$ and $\\mathbf{y}$ is a pair of increasing p-tuples $(\\pi_{1},\\pi_{2})$ such that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{1=\\pi_{1}(1)\\leq...\\leq\\pi_{1}(p)=n,}\\\\ {1=\\pi_{2}(1)\\leq...\\leq\\pi_{2}(p)=m,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We write $\\mathcal{A}(\\mathbf{x},\\mathbf{y})$ for the set of all possible alignments between $\\mathbf{X}$ and $\\mathbf{y}$ . Intuitively, an alignment $\\pi$ between $\\mathbf{X}$ and $\\mathbf{y}$ describes a way to associate each element of a sequence $\\mathbf{X}$ to one or possibly more elements in $\\mathbf{y}$ , and vice versa. Such alignments can be conveniently represented by paths in the $n\\times m$ grid displayed in the left of Figure 3. ", "page_idx": 18}, {"type": "text", "text": "with unitary increments and no simultaneous repetitions, that is $\\forall1\\leq i\\leq p-1.$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pi_{1}(i+1)\\leq\\pi_{1}(i)+1,\\quad\\pi_{2}(j+1)\\leq\\pi_{2}(j)+1,}\\\\ {(\\pi_{1}(i+1)-\\pi_{1}(i))+(\\pi_{2}(i+1)-\\pi_{2}(i))\\geq1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The score on a path is defined as: ", "page_idx": 18}, {"type": "equation", "text": "$$\nS(\\pi)=\\sum_{i=1}^{|\\pi|}\\varphi(x_{\\pi_{1}(i)},y_{\\pi_{2}(i)})\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "E Experimental Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "E.1 Datasets ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Training Data. Our vision-language task datasets are a subset of VILA [52], including MMC4 [104], COYO [9], LLaVA-1.5 SFT dataset [55]. ", "page_idx": 19}, {"type": "text", "text": "We use a prompt template formatted as (system-message is a system prompt from Vicuna, and the following messages all have the same meaning.): ", "page_idx": 19}, {"type": "text", "text": "{system-message}. USER: <image>\\n {question}. ASSISTANT: {answer}. ", "page_idx": 19}, {"type": "text", "text": "For interleaved vision-language datasets, the template is formatted as: ", "page_idx": 19}, {"type": "text", "text": "{system-message}. USER: {interleaving question}. ASSISTANT: {answer}. ", "page_idx": 19}, {"type": "text", "text": "Training Strategy. To jointly train the discriminative loss and generative loss, we calculate the loss as follows. Since the last token of an image is often a padding token, we take all 576 hidden state tokens before the LM head for images and apply average pooling to obtain a single token. For text, we directly take the last toke in the hidden state of MLLM. ", "page_idx": 19}, {"type": "text", "text": "During training, We calculate the discriminative loss using the last token from either the end of the text or the image in the MLLM\u2019s hidden state. Notably, in an interleaved input sequence with multiple texts or images, we randomly select multiple last tokens from the same sequence to more efficiently utilize the samples. ", "page_idx": 19}, {"type": "text", "text": "Evaluation Data. For generative tasks, we first evaluate on a wide range of question-answering tasks and some MLLM-oriented comprehension benchmarks, including VQA-v2 [27], GQA [35], VizWiz [28], ScienceQA-IMG [61], TextVQA [76], POPE [51], MME [95], MMBench [58], LLaVABench (In-the-Wild) [56], MM-Vet [99] and. The split of test sets and the evaluation metrics are aligned with those described in VILA[52] and LLaVA [55]. ", "page_idx": 19}, {"type": "text", "text": "To test the generative ability in interleaving tasks, we use DEMON [47], a comprehensive benchmark that demonstrative instruction following ability, including a wide variety of multi-modal datasets from different fields and scenarios. ", "page_idx": 19}, {"type": "text", "text": "For generation tasks, our evaluation encompasses MSCOCO [38] for image-text retrieval, Visual Storytelling (VIST) [32] for interleaved retrieval and Winoground [79] for fine-grained retrieval. ", "page_idx": 19}, {"type": "text", "text": "E.2 Training ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We train the parameters for both the LLM and the MLP for embedding the MLLM\u2019s hidden state, initializing from VILA [52]. To enhance efficiency for the LLM, we employ LoRA tuning [30] on the $W_{q}$ and $W_{v}$ matrices using low-rank adaptation. In our implementation, we set the rank $r=128$ and $\\alpha=256$ . We utilize the AdamW optimizer [60] in conjunction with a cosine learning rate scheduler. The hyperparameters for the AdamW optimizer are configured with a warm-up ratio of 0.03 and a maximum learning rate of $1e-4$ . Training is conducted on $8\\,\\mathbf{x}$ A800 GPUs for approximately 12 hours. ", "page_idx": 19}, {"type": "text", "text": "E.3 Introduction Experiment Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "(a) WebQA. The original WebQA contains two types of questions: \"Qcate\": \"text\" (open-ended questions) and \"Qcate\": \"YesNo\" (binary judgment questions). For ease of evaluation, we only used the second type. We selected 500 samples of the \"YesNo\" question type from WebQA [10], each containing one relevant image-text pair and five unrelated image-text pairs. Since the original data provides responses in declarative sentences, we modified the answers of these samples to be either \"yes\" or \"no\" by prompting with \"please answer the question in Yes or No.\" ", "page_idx": 19}, {"type": "text", "text": "We transformed this dataset into a question-answering format. Each question takes the following form (Due to display problems, we have performed line breaks, the same below.): ", "page_idx": 20}, {"type": "text", "text": "{system-message}. USER: {qustion}\\n{image-text pairs}.\\n please answer the question in Yes or No.\\n ASSISTANT: {answer}. ", "page_idx": 20}, {"type": "text", "text": "Here is a case for one sample in Figure 5: ", "page_idx": 20}, {"type": "image", "img_path": "w67vRHZF13/tmp/a02a58c09f3b6c4e10f5a72c631565e87a40edefe8555e699dd3fc5f8fb50112.jpg", "img_caption": ["Figure 5: A Case for WebQA. The index for the useful pair is three. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "In WebQA, the accuracy roughly forms a \"U\" shape curve when the relevant image-text pair for a question appears at different positions. While Sugar also shows similar trends, it tends to be more stable overall. Specific numerical results can be found in Table 5. ", "page_idx": 20}, {"type": "table", "img_path": "w67vRHZF13/tmp/aa20b8502e1cd13c41d94702449a1b367ef220baef83a8235ebe9da3f0a225fd.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Table 5: Specific accuracy $(\\%)$ values displayed on WebQA. The index indicates the position of the useful image-text pair, denoting which position it occupies in the sequence. ", "page_idx": 20}, {"type": "text", "text": "(b) MMVP-VLM Benchmark. MMVP-VLM [81] contains 30 carefully annotated images in each dimension of capability, with pairs of images being highly similar to each other (as indicated by their high similarity scores in CLIP). To evaluate the discriminative ability of generative models on these finely nuanced images, we transformed this dataset into a question-answering format. Each question takes the following form: ", "page_idx": 20}, {"type": "text", "text": "which choice meets the first image:\\n ", "page_idx": 20}, {"type": "text", "text": "A.{data[\"Statement\"]}\\nB.{data[\"Statement2\"]}\\n.please answer in A or B ", "page_idx": 20}, {"type": "text", "text": "ASSISTANT: {answer}. ", "page_idx": 20}, {"type": "text", "text": "Among them, both Statement 1 and Statement 2, as well as Image 1 and Image 2, are highly similar, with only subtle differences. Furthermore, there is a corresponding relationship between Image 1 and Statement 1, and between Image 2 and Statement 2. We employed a random seed to ensure that the correct answer is equally distributed between option A and option B. The specific values for the experiment are provided in Table 6. ", "page_idx": 20}, {"type": "table", "img_path": "w67vRHZF13/tmp/4047b7b7c641d7b457a9e36b98f8beaf1b196a884c1fbf75be7f39d12cef31fe.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "Table 6: Performance Comparison of VILA and Various CLIP-Based Models on Different Visual Patterns in MMVP-VLM Benchmark. For most of the visual patterns, all CLIP-based methods show struggle, as evident from the scores. Sugar achieves state-of-the-art performance on the majority of tasks, demonstrating its powerful discriminative ability. We use symbols for visual patterns due to space limit: $\\pmb{\\mathcal{o}}$ : Orientation and Direction, $\\mathfrak{a}$ : Presence of Specific Features, $\\Rsh$ : State and Condition, $\\uparrow\\frac{\\pi}{9}$ : Quantity and Count, $\\mathring{\\pmb{\\upgamma}}$ : Positional and Relational Context, $\\pmb{\\Bigg\\phi}$ : Color and Appearance, $\\pmb{\\upalpha}_{\\pmb{\\uptheta}}^{\\pmb{\\upalpha}}$ : Structural and Physical Characteristics,  : Texts, $\\clubsuit$ Viewpoint and Perspective. \u2020 indicates that we use question-answering as the test method, instead of dot product. ", "page_idx": 21}, {"type": "text", "text": "E.4 Retrieval-Augmented Generation. ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "For performing retrieval-augmented generation (RAG), we selected two tasks, namely VizWiz and $\\mathrm{SQ}\\bar{\\mathrm{A}}^{\\mathrm{I}}$ , as they provide held-in data not seen during model training. We did not use $\\mathrm{\\dot{V}Q A}^{\\mathrm{v}2}$ , GQA, and $\\mathrm{{VQA^{\\mathrm{{T}}}}}$ because their held-in data is a subset of the widely-used LLaVA-1.5 SFT. Benchmarks like POPE, MMB, and others lack held-in data. Therefore, we focused on VizWiz and $\\mathrm{SQA^{I}}$ for our experiments. We utilized a mixed set comprising the widely-used LLaVA-1.5 SFT subset and the held-in dataset of the two tasks as the knowledge base and employed different retrieval modules to retrieve relevant knowledge for the MLLM. Similar to common practice, we average the similarity scores for CLIP (We choose CLIP ViT- $\\cdot{\\mathrm{L}}/14@336{\\mathrm{px}})$ . For BLIP-2, we compute the similarity using its multimodal token\u2019s CLS token. Figure 7 shows some specific retrieval results on test data, demonstrating that Sugar can better integrate information from both images and text, retrieving more similar data as external knowledge. ", "page_idx": 21}, {"type": "image", "img_path": "w67vRHZF13/tmp/37a6cbb3e53ff6a234dfdda47941e0216c846c3134f43186a08f9bca0b2f3824.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure 6: Selected examples from do retrieval-augmented generation.Sugar can retrieve more useful knowledge compared with CLIP and BLIP-2. Inside the parentheses are the answers, note that the When retrieving, we will only retrieve the questions, not the answers, which are shown here for convenience only. ", "page_idx": 21}, {"type": "image", "img_path": "w67vRHZF13/tmp/8ae216383da24b7ba0028681c0b71d5fa1f9ac06b6ab7eef20683b641e4cddf5.jpg", "img_caption": ["Figure 7: Selected examples from do retrieval-augmented generation (continued for Figure 6). "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "F More Results ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "F.1 Details of Retrieval ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Image-text Retrieval. FROMAGe [40] was evaluated on the 5K validation set of MSCOCO 2017. Due to the split method confusion in FROMAGe, we report our image-text retrieval results on MSCOCO val2014\u2019s 5K val set following UniIR [84] and the Karpathy split [38]. What\u2019s more we then utilize FAISS [37], a powerful library for efficient similarity searches in dense vector spaces, to index and retrieve candidates. Therefore, the results may exhibit slight differences when compared under identical settings. The results in Table 3(a) are provided for reference only. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "Interleaved Retrieval. We conduct evaluations across several experimental configurations, following the same setup as FROMAGe [40]. The settings are as follows: ", "page_idx": 23}, {"type": "text", "text": "1. Retrieval of the last image given the descriptions of the preceding 5 images. This evaluates models\u2019 ability to condition on temporally dependent language. ", "page_idx": 23}, {"type": "text", "text": "2. Retrieval of the last image given the descriptions of the preceding 5 images and the 4 preceding images. This assesses models\u2019 capability to process interleaved image-and-text context. ", "page_idx": 23}, {"type": "text", "text": "Fine-grained Retrieval. Winoground [79] is designed to evaluate the ability of vision and language models to perform vision-linguistic compositional reasoning. The task involves matching two images with two captions, where both captions contain an identical set of words/morphemes arranged in different orders. This dataset, meticulously hand-curated by expert annotators, includes a rich set of fine-grained tags to facilitate detailed performance analysis. ", "page_idx": 23}, {"type": "text", "text": "F.2 Quality Results ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "To analyze Sugar\u2019s emergent behaviors and observed weaknesses, we present additional qualitative samples that were not included in the main paper due to space constraints. Please note that for brevity, we have omitted the system prompts and the line breaks after the images for all the quality examples. ", "page_idx": 23}, {"type": "text", "text": "We hope these additional results and observations showcase the potential of Sugar in various application areas. In future work, it is important to investigate these emergent behaviors more thoroughly and to understand the underlying mechanisms that enable Sugar to demonstrate such generalization abilities. This will pave the way towards building better MLLMs, including enhancing robustness, reducing biases, and improving the alignment and scope of the learned vision-language representations. ", "page_idx": 23}, {"type": "text", "text": "World Knowledge: We observe that Sugar can leverage the world knowledge [26] embedded within the LLM to enhance performance on multimodal tasks. For example, as shown in Figure 4, the model understands that during Halloween, people typically dress up in various ways to portray scary, funny, or creative characters, such as ghosts and skeletons. ", "page_idx": 23}, {"type": "text", "text": "Retrieval the Same Sequence at Different Place: One interesting emergent behavior of Sugar is its ability to retrieve sequences from different positions within the input interleaved sequence, demonstrating flexibility and high sample efficiency, as shown in Figure 4. Unlike CLIP, which requires encoding each sample separately, Sugar can encode sequences of varying lengths for the same multi-modal document in a single forward pass. ", "page_idx": 23}, {"type": "text", "text": "What\u2019s more, Sugar is capable of both retrieval and generation tasks. Below in Figure 8 are some examples from the VIST dataset. ", "page_idx": 23}, {"type": "image", "img_path": "w67vRHZF13/tmp/8967fa96ffd5f51c56ed74e6773c8996270e9cf3f6c83e95eb862ff05f1f3864.jpg", "img_caption": ["Figure 8: Selected examples for various image-text tasks. The pink background indicates retrieval results, while the blue background indicates generated results. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Fine-grained Image Discrimination: As shown in Figure 9, Sugar excels at accurately discerning subtle differences between images and identifying detailed objects and their attributes. VILA, on the other hand, tends to describe the content of the images without pinpointing the precise differences between them. In contrast, Sugar provides more concise and direct answers. ", "page_idx": 24}, {"type": "image", "img_path": "", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "w67vRHZF13/tmp/0ce39b10c0cf6e96ec123ca2f40fa2ddf65d550d16cd566520441c445c8f6ec8.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "The main difference between the two images is the perspective. In the first image, the view is from a higher angle, giving a broader view of the parking lot and the building. In contrast, the second image is taken from a lower angle, focusing on the entrance of the building. This difference in perspective allows for a more detailed view of the entrance and the surrounding area in the second image. ", "page_idx": 24}, {"type": "text", "text": "which one of following meets the image:\\n   \nA.a car facing back. $\\ln$   \nB.a car facing front. $\\backslash\\mathfrak{n}$   \nplease answer in A or B with the content of the choice. ", "page_idx": 24}, {"type": "text", "text": "which one of following meets the image: $\\ln$ A.A painting on the wall behind the bed. $\\backslash\\mathfrak{n}$ B.Empty on the wall behind the bed. $\\backslash\\mathfrak{n}$ please answer in A or B with the content of the choice. ", "page_idx": 24}, {"type": "text", "text": "which one of following meets the image: $\\backslash{\\bf n}$   \nA.points at $\\because0\\backslash{}^{\\epsilon}\\backslash\\mathbf{n}$   \nB.points at $\\because80\\backslash\"\\backslash{\\mathfrak{n}}$   \nplease answer in A or B with the content of the choice. ", "page_idx": 24}, {"type": "image", "img_path": "w67vRHZF13/tmp/32482aa9df386571ec9480b441fb6ce28dc90680eda925ddc8521816b88aae42.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Please give a editing Request to describe the transformation from the source image to the target image. ", "page_idx": 24}, {"type": "text", "text": "The image depicts a young girl with blonde hair, wearing a purple shirt, standing in front of a white house. The girl is smiling and looking directly at the camera. The image has been edited to remove the background, focusing solely on the girl. The background has been replaced with a solid color, which is not specified in the image description. The girl's position relative to the house has not been altered, and she is still standing in front of it. The image does not contain any text or additional objects. The girl's smile and direct gaze are the main focus of the image. ", "page_idx": 24}, {"type": "image", "img_path": "w67vRHZF13/tmp/7774d989ce3b24cea369f06dbc585d076a68c6ad6a0d01b50363694248c89610.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Change the girls face Zoom it in slightly. ", "page_idx": 24}, {"type": "text", "text": "Figure 9: Selected examples. Sugar excels at accurately discerning subtle differences between images and identifying detailed objects and their attributes. ", "page_idx": 24}, {"type": "text", "text": "Style Following: Sugar exhibits a certain degree of in-context style following capability. As shown in Figure 10, with the aid of external knowledge, Sugar partially adopts the style of retrieved results, resulting in more accurate and detailed answers compared to scenarios without retrieval augmentation. ", "page_idx": 25}, {"type": "image", "img_path": "", "img_caption": [], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "w67vRHZF13/tmp/11cf9d2929a4e4f02239fb955ccd2632930910f575c11f5ca18dd0f8befc501f.jpg", "img_caption": ["Figure 10: Selected examples from LLaVA-Bench(In-the-wild). Using external knowledge, Sugar partially follows the style of retrieved results, providing more accurate and detailed answers compared to not using retrieval augmentation. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Interleaved Comprehension: As demonstrated by results from DEMON [47], Sugar exhibits superior interleaved comprehension capabilities compared to VILA, particularly in tasks requiring fine-grained analysis and an understanding of global context. For instance, in the third example of Figure 11, VILA confuses character names, whereas Sugar maintains narrative coherence while adhering to the style of the preceding text. Similarly, in the third example of Figure 12, VILA provides an irrelevant response, while Sugar delivers a more contextually appropriate answer. Additionally, Figure 13 demonstrates Sugar\u2019s ability to effectively capture global information, identifying the relevant images and text within the sequence to provide accurate responses. ", "page_idx": 25}, {"type": "image", "img_path": "w67vRHZF13/tmp/b3a638a299d429b695a40fff887611ebfd8408e523b43a557e1d918378c5d5be.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Given two images and a sentence outlining their relation, your responsibility is to judge whether the sentence correctly describes the connection between them.\\n ", "page_idx": 26}, {"type": "image", "img_path": "w67vRHZF13/tmp/920191883fb83f03a0db27dcad4e92416aa6037e1750ea023322ca69a71eeaa7.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Relation: The left image shows at least one rear-facing hyena standing in front of two larger standing animals with horns. ", "page_idx": 26}, {"type": "text", "text": "True ", "page_idx": 26}, {"type": "text", "text": "False ", "page_idx": 26}, {"type": "image", "img_path": "w67vRHZF13/tmp/2291591e60da534e6e775fee6d16d320f7a1440f7a1857c7bf621ed8fca32900.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "8 ", "page_idx": 26}, {"type": "text", "text": "Having viewed the images, can you use the information presented to answer the following question $?\\mathfrak{n}$ ", "page_idx": 26}, {"type": "text", "text": "Image Caption #1: StadiumTrack Track inside Olympic Stadium for the Monster Spectacular . $\\mathfrak{m}$ ", "page_idx": 26}, {"type": "image", "img_path": "w67vRHZF13/tmp/ee258cac9a23ea921f72766d8c6a86c43dc3356f98fc2300e17cd3a41e38f41e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Image Caption #2: Stade Olympique Monteal A match of football of Montreal Impact at the Oympic Stadium of Montreal. $\\backslash{\\mathfrak{n}}$ ", "page_idx": 26}, {"type": "image", "img_path": "w67vRHZF13/tmp/0e3652481e2bb599d6cb2a6922054b5b785263778b729b6f439a385e623ebc56.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Question: Is the center of the Olympic Stadium of Montreal always the same? ", "page_idx": 26}, {"type": "text", "text": "The answer is yes. ", "page_idx": 26}, {"type": "text", "text": "No, the center of the Olympic Stadium of Montreal is not always the same. ", "page_idx": 26}, {"type": "image", "img_path": "w67vRHZF13/tmp/fbfebc4fb658d55eee034689b0ca64a547fe12c491bd748ff7c33bc32ec050cc.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Given the progression of the story with the first few images, can you write a fitting end considering the last image?\\n ", "page_idx": 26}, {"type": "image", "img_path": "w67vRHZF13/tmp/5ea698535d9c68a36fc64df99a5c6b21ab063be8202e0c3358ebe19a538a3fa5.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Caption#1:Wilma and Betty are sitting on a couch in the living room. Wilma speaks to Betty and cries into a handkerchief.\\n ", "page_idx": 26}, {"type": "image", "img_path": "w67vRHZF13/tmp/8e149dea6cc1ae44d17e2c11692b50010eae0b3ac7bb94c98b52dfb4c4babbc9.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Caption#2:Wilma and Betty are sitting on a couch in the living room. Wilma is crying and wiping her tears with a handkerchief while Betty speaks to her. \\n Caption#3:Wilma and Betty are sitting on the couch in the living room in front of the television set. Wilma is holding a handkerchief and talking to Betty. $\\backslash\\mathfrak{n}$ ", "page_idx": 26}, {"type": "image", "img_path": "w67vRHZF13/tmp/6bb6019c3af4bc11974e10bb6f71227a6b27297e45644d193dc6c1cca6dc9fa5.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "image", "img_path": "w67vRHZF13/tmp/cf247d80be5569d3ad43ac40721b56bd8c91605966ad043339e5d52be375b774.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Caption#4:Fred and Barney are in a car. Barney is looking at Fred and holding the steering wheel. Fred speaks, then  Barney speaks.\\n ", "page_idx": 26}, {"type": "image", "img_path": "w67vRHZF13/tmp/9746a7f1b822b842ef7556cec9240409315c33c2821bff5d3c343a939402bace.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Wilma and Betty are sitting on the couch in the living room in front of the television set. Wilma is holding a handkerchief and talking to Betty. ", "page_idx": 26}, {"type": "image", "img_path": "w67vRHZF13/tmp/287344072f5bec7ad054b475837898d5818b3802e70571661a42df543c11f263.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Fred and Barney are in a car. Barney is driving while Fred is talking to Barney while they look behind them. ", "page_idx": 26}, {"type": "image", "img_path": "w67vRHZF13/tmp/42ff75734a0ecc6ea8ac5559c923c82e1f50a91d454f3e05d466e83662f19681.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Figure 11: Selected examples for Interleaved Comprehension. ", "page_idx": 26}, {"type": "image", "img_path": "w67vRHZF13/tmp/bec39560d028e66b0c6420133a0c392cdd08a103d2e4c1f20faffcb7edf48f69.jpg", "img_caption": [], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Utilizing the information, including images, text, and tables that I provide, could you provide a correct answer to the ", "page_idx": 27}, {"type": "image", "img_path": "w67vRHZF13/tmp/c8825ffdd86624ddc59c6f6de0fc990ee5c76b61c17a06b3ac7a985d9903ea8a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Question: How many people are in the car in diablero? ", "page_idx": 27}, {"type": "text", "text": "There are five people in the car in the image. ", "page_idx": 27}, {"type": "text", "text": "There are four people in the car in the image. ", "page_idx": 27}, {"type": "text", "text": "8 ", "page_idx": 27}, {"type": "text", "text": "Provided with a series of diagrams from a textbook, your responsibility is to correctly answer the following question. $\\ln$ ", "page_idx": 27}, {"type": "image", "img_path": "w67vRHZF13/tmp/792c1c5ca377bb54ec116ef5b6212dd5ee08272947a7002ba016f304ea8ae058.jpg", "img_caption": [], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Question: How many layers of the atmosphere is identified in the diagram? ", "page_idx": 27}, {"type": "text", "text": "There are four layers of the atmosphere identified in the diagram. ", "page_idx": 27}, {"type": "text", "text": "There are five layers of the atmosphere identified in the diagram. ", "page_idx": 27}, {"type": "text", "text": "Then he would take a picture of the beach to put in the other window. ", "page_idx": 27}, {"type": "text", "text": "Figure 12: Selected examples for Interleaved Comprehension (continued for Figure 11). ", "page_idx": 27}, {"type": "image", "img_path": "w67vRHZF13/tmp/a1ce7f42f4419687b3b73e5986ce4854a459900cdb220de6466f31800295d5c4.jpg", "img_caption": ["Figure 13: Selected examples for Interleaved Comprehension (continued for Figure 12). "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "Sensitivity with Detailed Semantics: Sugar can address various examples inspired by the Winograd schema [43]. These examples consist of multiple sentences that differ only by a single word, leading to different resolutions of ambiguity. Sugar can accurately match images and text, demonstrating its sensitivity to even minor changes in input prompts. Figure 14 showcases some cases that align with the Winograd schema from Winoground. ", "page_idx": 28}, {"type": "image", "img_path": "w67vRHZF13/tmp/7c476d25e6a47c1a6505149938892c9df7bfd297aa69023ae7ede8a1ddb35ad4.jpg", "img_caption": ["Figure 14: Selected examples from Winoground. Sugar is Sensitivity with Detailed Semantics "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "G Retrieval for Knowledge-based VQA ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "In this section, we use FVQA [83] and WebQA [10], two knowledge-based VQA datasets, to verify Sugar\u2019s effectiveness of combining retrieval and comprehension abilities in a single model, thereby avoiding compatibility issues and suboptimal performance. ", "page_idx": 29}, {"type": "text", "text": "Historically, solving FVQA has relied on modeling the knowledge database using a Knowledge Graph [13]. For WebQA, each question is associated with 10-20 knowledge bases, but only one is relevant to the image and caption. FVQA knowledge is textual, whereas WebQA knowledge consists of both text and pictures. ", "page_idx": 29}, {"type": "text", "text": "Implement Details. In this experiment, we used CLIP ViT-L/14@336px, and both experiments report the ROUGE-L Score. ", "page_idx": 29}, {"type": "text", "text": "For FVQA, answers originate from two sources: directly from the image or from the knowledge base. To minimize interference, we only tested questions requiring the knowledge base. We used the following prompt for FVQA: \"Please answer the questions based on the pictures. If the reference information is useful, please use it. Otherwise, please ignore the reference information. Reference information: retrieved knowledge <image> question.\" The baseline without retrieval means we did not search for knowledge, but directly input the image and question for the model to answer. $+~C L I P$ image means using the image to retrieve knowledge, $+\\ C L I P$ text means using the text to retrieve knowledge, and $+~C L I P$ average means using the average annotations of both image and text to retrieve knowledge. For our model, sugar+rag indicates the average result obtained using both image and text to retrieve knowledge. ", "page_idx": 29}, {"type": "text", "text": "For WebQA, each question has 10-20 negative captions and images. Due to context length limitations in LLaVA and VILA, we could not input all the data, necessitating a retrieval model to extract relevant knowledge. Due to the large dataset size, we randomly selected 1000 samples. For WebQA, $+C L I P$ image means providing the positive image and using it to retrieve the most relevant text from the knowledge base, which is then used as input for the model to answer the question. Conversely, $+C L I P$ text uses the text to retrieve relevant images. For our model, sugar+rag indicates the result obtained using the average similarity score of the aforementioned methods. ", "page_idx": 29}, {"type": "table", "img_path": "w67vRHZF13/tmp/d1b7770490f620fe88693ae346043d925357d25e47854084be14cdca9369c1d4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "Results. Based on Table 7, we can observe that while MLLM can answer a small portion of FVQA questions using its internal knowledge, it still requires the support of a retriever for enhanced accuracy. However, the impact of retrieval strategies on the results is inconsistent. For instance, using text retrieval often outperforms image retrieval in FVQA, whereas in WebQA, image retrieval is more effective. Additionally, there are compatibility issues between retrieval strategies and models. For example, in WebQA, VILA is more sensitive to CLIP\u2019s retrieval strategy, with fluctuations 3.4 times greater than those of LLaVA-1.5. Our integrated retriever and generator model, however, does not require an additional retriever and avoids the aforementioned optimization and selection issues. ", "page_idx": 29}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: The main claims presented in the abstract and introduction provide an accurate representation of the paper\u2019s contributions and scope. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 30}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: We discuss the limitations of the work in Appendix B ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 30}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We give the proof in Appendix C. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 31}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification:We give experimental setup and implementation details in Section 4.1 and Appendix E. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 31}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: The codes will come soon and all the data is public to access. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 32}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: we have provided necessary implementation details of our method in Appendix E. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 32}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We followed the baseline settings on the evaluation benchmark. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We give the statements of experiments compute resources in Appendix E.2. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 33}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: The research conducted in the paper conforms, in every respect, to the NeurIPS Code of Ethics. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 33}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: We discuss the Broader Impacts in Appendix A. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 34}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We have already cited all the original paper that produced the code package or dataset. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 34}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 35}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 35}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 35}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 35}]