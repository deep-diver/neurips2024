[{"figure_path": "wDDvJzvvBR/tables/tables_4_1.jpg", "caption": "Table 1: Comparison of model capabilities and performance for retrieval of semantic captions from AudioCaps, and 3D sound localization for the REAL component TUT Sound Events 2018. ELSA is the only model that allows both open vocabulary language understanding and spatial localization, and performs comparably against the baselines for both tasks.", "description": "This table compares the performance of several models on two tasks: semantic caption retrieval (using the AudioCaps dataset) and 3D sound localization (using the REAL component of the TUT Sound Events 2018 dataset).  It highlights that ELSA, unlike other models, handles both open vocabulary language understanding and spatial localization, achieving competitive results on both tasks.", "section": "5 Experiments, Results, and Discussion"}, {"figure_path": "wDDvJzvvBR/tables/tables_6_1.jpg", "caption": "Table 1: Comparison of model capabilities and performance for retrieval of semantic captions from AudioCaps, and 3D sound localization for the REAL component TUT Sound Events 2018. ELSA is the only model that allows both open vocabulary language understanding and spatial localization, and performs comparably against the baselines for both tasks.", "description": "This table compares ELSA with other state-of-the-art models in terms of semantic and spatial capabilities.  It shows each model's ability to perform semantic caption retrieval (measured by mean Average Precision at 10, or mAP@10) on the AudioCaps dataset and its ability to perform 3D sound localization (measured by mean absolute error in degrees) on the REAL component of the TUT Sound Events 2018 dataset.  ELSA is highlighted as uniquely achieving both open vocabulary semantic understanding and spatial localization.", "section": "5 Experiments, Results, and Discussion"}, {"figure_path": "wDDvJzvvBR/tables/tables_6_2.jpg", "caption": "Table 2: Zero-shot classification accuracy using the cosine similarity between test set audio embeddings and templated probe caption embeddings. The template is \"A sound coming from <spatial attribute>\" and a value for <spatial attribute> is substituted into the template representing the desired class (e.g., \"near\" or \"far\" for distance). A classification is correct if the attribute in the closest test sample matches the attribute in the template. We cannot provide comparisons with baselines since this is a new task.", "description": "This table presents the zero-shot classification accuracy of ELSA on spatial attributes.  It uses cosine similarity between audio embeddings and templated captions (e.g., \"A sound coming from near\"). Accuracy is measured by comparing the spatial attribute of the closest test sample to the attribute in the template.  Since this is a novel task, no comparisons to baselines are provided.", "section": "5.2 Spatial Attributes Evaluation"}, {"figure_path": "wDDvJzvvBR/tables/tables_7_1.jpg", "caption": "Table 3: Semantic retrieval (R@1, R@5, and R@10) for CLAP and ELSA calculated over the original (non-spatial) versions of Clotho and AudioCaps. Although ELSA is trained using a mixture of non-spatial and spatial audio, it conserves the retrieval performance on non-spatial audio of LAION-AI CLAP, which was trained on only non-spatial data. For the training data, read C as Clotho, AC as AudioCaps, LA as LAION-Audio-630K and FS as Freesound. A superscript S denotes the spatially-augmented equivalent dataset. We use Freesound, a subset of LAION-Audio-630K due to its more permissive licensing. For a fair comparison, we train a version of CLAP locally with Clotho, Audiocaps and Freesound, which is not reported in the CLAP paper.", "description": "This table compares the performance of ELSA and LAION-CLAP on semantic retrieval tasks using the original (non-spatial) versions of the Clotho and AudioCaps datasets.  It shows the recall at ranks 1, 5, and 10 (R@1, R@5, R@10) for both text-to-audio and audio-to-text retrieval.  The table highlights that while ELSA is trained on both spatial and non-spatial data, its performance on non-spatial data is comparable to LAION-CLAP, which was trained only on non-spatial data.  Different training data combinations are explored for both models (Clotho, AudioCaps, Freesound, and their spatially augmented counterparts).", "section": "5.3 Semantics Evaluation"}, {"figure_path": "wDDvJzvvBR/tables/tables_9_1.jpg", "caption": "Table 4: Evaluation of Spatial Audio Caption Generation. Metrics were obtained from the Audio Captioning task of the DCASE Challenges by comparing the generated captions produced from spatial audio and the ground-truth captions from the test splits of Spatial-AudioCaps (S-AC) and Spatial-Clotho.", "description": "This table presents the results of evaluating the spatial audio caption generation model.  The evaluation used the Audio Captioning task from the DCASE Challenges.  It compared generated captions from spatial audio with ground truth captions. The evaluation is done on the test splits of Spatial-AudioCaps (S-AC) and Spatial-Clotho datasets.", "section": "5.5 Spatial Audio Caption Generation"}, {"figure_path": "wDDvJzvvBR/tables/tables_13_1.jpg", "caption": "Table A.T.1: Audio-caption dataset descriptions. The first three rows correspond to the original publicly available datasets, and the subsequent rows correspond to our spatially-augmented variants. For each spatially augmented dataset, there are at least two spatial augmentations per original sample in the train split.", "description": "This table presents a summary of the datasets used in the paper. It lists the dataset name, whether it contains spatial audio, the splits (training, validation, testing), the number of samples, the duration in hours, and a description of the captions.", "section": "A.1 Dataset statistics"}, {"figure_path": "wDDvJzvvBR/tables/tables_13_2.jpg", "caption": "Table 1: Comparison of model capabilities and performance for retrieval of semantic captions from AudioCaps, and 3D sound localization for the REAL component TUT Sound Events 2018. ELSA is the only model that allows both open vocabulary language understanding and spatial localization, and performs comparably against the baselines for both tasks.", "description": "This table compares the capabilities and performance of different models on two tasks: semantic caption retrieval using the AudioCaps dataset and 3D sound localization using the REAL component of the TUT Sound Events 2018 dataset.  It highlights that ELSA is unique in handling both open vocabulary language understanding and spatial localization, and it shows that ELSA achieves performance comparable to state-of-the-art models for both tasks.  The table includes the models, their semantic and spatial capabilities, and their performance metrics.", "section": "5 Experiments, Results, and Discussion"}, {"figure_path": "wDDvJzvvBR/tables/tables_13_3.jpg", "caption": "Table 1: Comparison of model capabilities and performance for retrieval of semantic captions from AudioCaps, and 3D sound localization for the REAL component TUT Sound Events 2018. ELSA is the only model that allows both open vocabulary language understanding and spatial localization, and performs comparably against the baselines for both tasks.", "description": "This table compares ELSA against other models on two tasks: semantic caption retrieval using the AudioCaps dataset and 3D sound localization using the REAL component of the TUT Sound Events 2018 dataset.  The table highlights ELSA's unique ability to handle both open vocabulary language understanding and spatial localization, unlike other models that excel at only one of these tasks.  ELSA's performance is shown to be competitive with state-of-the-art models in both areas.", "section": "5 Experiments, Results, and Discussion"}, {"figure_path": "wDDvJzvvBR/tables/tables_14_1.jpg", "caption": "Table 1: Comparison of model capabilities and performance for retrieval of semantic captions from AudioCaps, and 3D sound localization for the REAL component TUT Sound Events 2018. ELSA is the only model that allows both open vocabulary language understanding and spatial localization, and performs comparably against the baselines for both tasks.", "description": "This table compares ELSA's performance against other state-of-the-art models on two tasks: semantic caption retrieval (using the AudioCaps dataset) and 3D sound localization (using the REAL component of the TUT Sound Events 2018 dataset).  It highlights that ELSA uniquely combines open vocabulary language understanding with spatial localization capabilities, achieving competitive results on both tasks.", "section": "5 Experiments, Results, and Discussion"}, {"figure_path": "wDDvJzvvBR/tables/tables_16_1.jpg", "caption": "Table A.T.4: Comparison of semantic and spatial retrieval performance across data input ablations. mAP@10 refers to the text-to-audio mean average precision @ 10 and audio-to-text mean average precision @ 10. It is a summary metric capturing the model's semantic retrieval capabilities.", "description": "This table presents ablation studies on the ELSA model, comparing its performance with different configurations.  Specifically, it shows the impact of using static intensity vectors versus a learned encoder for spatial information, and the effect of including spatial regression losses in addition to the contrastive loss.  The results are evaluated using 3D localization error, distance error, and the mean average precision at 10 (mAP@10), a metric reflecting the model's semantic retrieval ability.", "section": "A.6 Ablations across model architectures"}, {"figure_path": "wDDvJzvvBR/tables/tables_16_2.jpg", "caption": "Table 3: Semantic retrieval (R@1,R@5,andR@10) for CLAP and ELSA calculated over the original (non-spatial) versions of Clotho and AudioCaps. Although ELSA is trained using a mixture of non-spatial and spatial audio, it conserves the retrieval performance on non-spatial audio of LAION-AI CLAP, which was trained on only non-spatial data. For the training data, read C as Clotho, AC as AudioCaps, LA as LAION-Audio-630K and FS as Freesound. A superscript S denotes the spatially-augmented equivalent dataset. We use Freesound, a subset of LAION-Audio-630K due to its more permissive licensing. For a fair comparison, we train a version of CLAP locally with Clotho, Audiocaps and Freesound, which is not reported in the CLAP paper.", "description": "This table compares the performance of ELSA and CLAP on semantic retrieval tasks using the original (non-spatial) versions of the Clotho and AudioCaps datasets.  It highlights that although ELSA is trained on both spatial and non-spatial data, its performance on non-spatial data is comparable to LAION-CLAP, which was trained only on non-spatial data.  The table also shows the training data used for each model and provides retrieval scores (R@1, R@5, R@10) for both text-to-audio and audio-to-text tasks.", "section": "5.3 Semantics Evaluation"}, {"figure_path": "wDDvJzvvBR/tables/tables_19_1.jpg", "caption": "Table A.T.6: Mean and standard deviation of absolute direction-of arrival errors (in radians) predicted by 2-layer MLP. Tables (a)-(e) show the Spatial Audiocaps and Spatial Clotho test sets errors by different dimensions. Table (f) shows the predictions of the test set of TUT Sounds 2018 by different semantic classes.", "description": "This table presents a detailed breakdown of the errors in direction-of-arrival predictions made by a two-layer Multilayer Perceptron (MLP) model.  The analysis is categorized by several factors to understand the sources of error. These factors include azimuth, elevation, distance, room floor area, reverberation time (T30), and semantic class from the TUT Sound Events 2018 dataset. For each category, the table provides the mean and standard deviation of the prediction errors, along with the number of samples used in the calculation. This level of detail helps assess the model's performance across different conditions and identify areas for potential improvement.", "section": "A.9 Fine-grained of direction-of-arrival error analysis"}, {"figure_path": "wDDvJzvvBR/tables/tables_19_2.jpg", "caption": "Table A.T.6: Mean and standard deviation of absolute direction-of arrival errors (in radians) predicted by 2-layer MLP. Tables (a)-(e) show the Spatial Audiocaps and Spatial Clotho test sets errors by different dimensions. Table (f) shows the predictions of the test set of TUT Sounds 2018 by different semantic classes.", "description": "This table presents a detailed breakdown of the errors in predicting the direction of arrival of sounds. It shows how these errors vary depending on different factors such as azimuth, elevation, distance, room size, reverberation time, and semantic class of the sound. The table is divided into six parts, each showing the mean and standard deviation of the errors for specific ranges of values for each of the factors.", "section": "A.9 Fine-grained of direction-of-arrival error analysis"}, {"figure_path": "wDDvJzvvBR/tables/tables_19_3.jpg", "caption": "Table A.T.6: Mean and standard deviation of absolute direction-of arrival errors (in radians) predicted by 2-layer MLP. Tables (a)-(e) show the Spatial Audiocaps and Spatial Clotho test sets errors by different dimensions. Table (f) shows the predictions of the test set of TUT Sounds 2018 by different semantic classes.", "description": "This table presents a detailed breakdown of the errors in predicting the direction of arrival of sounds, as determined by a two-layer Multilayer Perceptron (MLP) model.  The errors are analyzed across various factors: azimuth, elevation, distance, floor area of the room, reverberation time (T30), and semantic classes from the TUT Sound Events 2018 dataset. For each factor, the table shows the mean and standard deviation of the errors, along with the number of samples used in the analysis. This level of detail helps to understand the performance of the model under different conditions and identify potential areas for improvement.", "section": "A.9 Fine-grained of direction-of-arrival error analysis"}, {"figure_path": "wDDvJzvvBR/tables/tables_19_4.jpg", "caption": "Table A.T.6: Mean and standard deviation of absolute direction-of arrival errors (in radians) predicted by 2-layer MLP. Tables (a)-(e) show the Spatial Audiocaps and Spatial Clotho test sets errors by different dimensions. Table (f) shows the predictions of the test set of TUT Sounds 2018 by different semantic classes.", "description": "This table shows a detailed breakdown of the errors in direction-of-arrival prediction made by a two-layer Multi-Layer Perceptron (MLP) model.  It analyzes the errors across various factors: azimuth, elevation, distance, room floor area, reverberation time (T30), and semantic classes from the TUT Sound Events 2018 dataset. For each factor, it provides the mean and standard deviation of the errors, along with the number of samples in each bin.", "section": "A.9 Fine-grained of direction-of-arrival error analysis"}, {"figure_path": "wDDvJzvvBR/tables/tables_19_5.jpg", "caption": "Table A.T.6: Mean and standard deviation of absolute direction-of arrival errors (in radians) predicted by 2-layer MLP. Tables (a)-(e) show the Spatial Audiocaps and Spatial Clotho test sets errors by different dimensions. Table (f) shows the predictions of the test set of TUT Sounds 2018 by different semantic classes.", "description": "This table presents a detailed breakdown of the errors in direction-of-arrival predictions made by a two-layer multi-layer perceptron (MLP) model. It categorizes these errors based on several factors: azimuth, elevation, distance, room floor area, reverberation time (T30), and semantic classes from the TUT Sound Events 2018 dataset.  For each category, the table provides the mean and standard deviation of the errors, along with the number of samples used in the calculation.", "section": "A.9 Fine-grained of direction-of-arrival error analysis"}, {"figure_path": "wDDvJzvvBR/tables/tables_19_6.jpg", "caption": "Table 1: Comparison of model capabilities and performance for retrieval of semantic captions from AudioCaps, and 3D sound localization for the REAL component TUT Sound Events 2018. ELSA is the only model that allows both open vocabulary language understanding and spatial localization, and performs comparably against the baselines for both tasks.", "description": "This table compares ELSA against other state-of-the-art models in terms of semantic retrieval (using AudioCaps) and 3D sound localization (using TUT Sound Events 2018).  It highlights ELSA's unique ability to handle both open vocabulary language understanding and spatial localization, demonstrating competitive performance on both tasks.  The table clearly shows ELSA's advantages over models which only address semantic understanding or spatial awareness.", "section": "5 Experiments, Results, and Discussion"}, {"figure_path": "wDDvJzvvBR/tables/tables_19_7.jpg", "caption": "Table 2: Zero-shot classification accuracy using the cosine similarity between test set audio embeddings and templated probe caption embeddings. The template is \"A sound coming from <spatial attribute>\" and a value for <spatial attribute> is substituted into the template representing the desired class (e.g., \"near\" or \"far\" for distance). A classification is correct if the attribute in the closest test sample matches the attribute in the template. We cannot provide comparisons with baselines since this is a new task.", "description": "This table presents the zero-shot classification accuracy of ELSA on spatial attributes.  It uses cosine similarity between audio embeddings and captions templated with spatial attributes (e.g., \"near\", \"far\", \"left\", \"right\"). Accuracy is determined by comparing the closest test sample's attribute to the template attribute.  No baseline comparisons are provided as this is a novel task.", "section": "5.2 Spatial Attributes Evaluation"}, {"figure_path": "wDDvJzvvBR/tables/tables_22_1.jpg", "caption": "Table 3: Semantic retrieval (R@1, R@5, and R@10) for CLAP and ELSA calculated over the original (non-spatial) versions of Clotho and AudioCaps. Although ELSA is trained using a mixture of non-spatial and spatial audio, it conserves the retrieval performance on non-spatial audio of LAION-AI CLAP, which was trained on only non-spatial data. For the training data, read C as Clotho, AC as AudioCaps, LA as LAION-Audio-630K and FS as Freesound. A superscript S denotes the spatially-augmented equivalent dataset. We use Freesound, a subset of LAION-Audio-630K due to its more permissive licensing. For a fair comparison, we train a version of CLAP locally with Clotho, Audiocaps and Freesound, which is not reported in the CLAP paper.", "description": "This table compares the performance of ELSA and CLAP models on semantic retrieval tasks using the original (non-spatial) versions of Clotho and AudioCaps datasets.  It highlights that while ELSA was trained on both spatial and non-spatial data, it maintains competitive performance with CLAP, which was trained only on non-spatial data. The table specifies the training datasets used for both models (Clotho, AudioCaps, Freesound, and their spatially augmented counterparts) and provides Recall@1, Recall@5, and Recall@10 scores for both text-to-audio and audio-to-text retrieval tasks. A locally trained version of CLAP is also included for comparison.", "section": "5.3 Semantics Evaluation"}, {"figure_path": "wDDvJzvvBR/tables/tables_22_2.jpg", "caption": "Table 3: Semantic retrieval (R@1,R@5,andR@10) for CLAP and ELSA calculated over the original (non-spatial) versions of Clotho and AudioCaps. Although ELSA is trained using a mixture of non-spatial and spatial audio, it conserves the retrieval performance on non-spatial audio of LAION-AI CLAP, which was trained on only non-spatial data. For the training data, read C as Clotho, AC as AudioCaps, LA as LAION-Audio-630K and FS as Freesound. A superscript S denotes the spatially-augmented equivalent dataset. We use Freesound, a subset of LAION-Audio-630K due to its more permissive licensing. For a fair comparison, we train a version of CLAP locally with Clotho, Audiocaps and Freesound, which is not reported in the CLAP paper.", "description": "This table compares the performance of ELSA and CLAP on semantic retrieval tasks using the original (non-spatial) versions of Clotho and AudioCaps datasets.  It shows Recall@1, Recall@5, and Recall@10 for both audio-to-text and text-to-audio retrieval.  The key finding is that ELSA, despite being trained on a mixture of spatial and non-spatial audio, maintains comparable performance to LAION-CLAP on non-spatial audio retrieval tasks. The table also details the training data used for both models, highlighting the different datasets and their spatial augmentation.", "section": "5.3 Semantics Evaluation"}, {"figure_path": "wDDvJzvvBR/tables/tables_22_3.jpg", "caption": "Table A.T.10: Wasserstein distances of 512-dimensional ELSA embeddings, clustered by either (a) direction or (b) distance.", "description": "This table presents the Wasserstein distances between different clusters of ELSA embeddings in a 512-dimensional space.  The clusters are formed based on spatial attributes: direction (left, right, front, back) in part (a) and distance (near, far) in part (b). Lower Wasserstein distances indicate higher similarity between the clusters.  This helps to demonstrate how well the ELSA embeddings capture and separate spatial information.", "section": "A.13 Further analysis on embedding clusters"}, {"figure_path": "wDDvJzvvBR/tables/tables_22_4.jpg", "caption": "Table A.T.10: Wasserstein distances of 512-dimensional ELSA embeddings, clustered by either (a) direction or (b) distance.", "description": "This table presents the Wasserstein distances calculated between clusters of ELSA embeddings in a 512-dimensional space.  The clusters are formed based on either direction (left, right, front, back) or distance (near, far) attributes.  Lower Wasserstein distances indicate higher similarity between the clusters.  The table provides quantitative support for the qualitative observation from Figure 2 that ELSA embeddings capture spatial attributes effectively.", "section": "A.13 Further analysis on embedding clusters"}, {"figure_path": "wDDvJzvvBR/tables/tables_23_1.jpg", "caption": "Table A.T.11: Direction swapping of ELSA embeddings. See Appendix A.14 for a detailed explanation of how we swapped the embedding directions. \u03be is the number of test samples misclassified by our direction classifier, and subsequently excluded. N is the number of samples that were used for direction transposition. R@10 is the recall@10 computed over the corresponding non-spatial captions. \u03b8 is the classification accuracy of the transposed sample. \u0394R@10 is the change in recall@10 after performing the change of direction.", "description": "This table presents the results of an experiment where the spatial direction encoded in ELSA audio embeddings was swapped using text prototypes.  The original direction was removed, and a new direction was added. The table shows the number of samples, recall@10 (a measure of semantic retrieval), the accuracy of the direction classification after the swap, and the change in recall@10 after the swap for each original and new direction combination. This experiment demonstrates the ability to manipulate the spatial attributes of audio in the ELSA embedding space using text.", "section": "A.14 Swapping of Spatial Direction Experiments"}, {"figure_path": "wDDvJzvvBR/tables/tables_24_1.jpg", "caption": "Table A.T.12: Direction removal of ELSA embeddings. See Appendix A.14 for a detailed explanation of how we swapped the embedding directions. \u03be is the number of test samples misclassified by our direction classifier, and subsequently excluded. N is the number of samples that were used for direction transposition. R@10 is the recall@10 computed over the corresponding non-spatial captions. \u03b8 is the classification accuracy of the transposed sample. \u0394R@10 is the change in recall@10 after performing the change of direction.", "description": "This table presents ablation study results on swapping spatial direction of ELSA embeddings. It shows the number of samples misclassified by the direction classifier (\u03be), the number of samples used for transposition (N), the recall@10 score (R@10) before and after transposition, the classification accuracy (\u03b8) after transposition, and the change in recall@10 (\u0394R@10) after transposition, for each of the four original directions (left, front, right, and back).  The study removes the original direction but does not add a new direction. ", "section": "A.14 Swapping of Spatial Direction Experiments"}, {"figure_path": "wDDvJzvvBR/tables/tables_24_2.jpg", "caption": "Table 1: Comparison of model capabilities and performance for retrieval of semantic captions from AudioCaps, and 3D sound localization for the REAL component TUT Sound Events 2018. ELSA is the only model that allows both open vocabulary language understanding and spatial localization, and performs comparably against the baselines for both tasks.", "description": "This table compares the performance of several models on two tasks: semantic caption retrieval using the AudioCaps dataset and 3D sound localization using the REAL component of the TUT Sound Events 2018 dataset.  It highlights ELSA's unique ability to handle both open vocabulary language and spatial information, showing competitive results compared to models specialized in either semantic or spatial tasks.", "section": "5 Experiments, Results, and Discussion"}, {"figure_path": "wDDvJzvvBR/tables/tables_24_3.jpg", "caption": "Table 1: Comparison of model capabilities and performance for retrieval of semantic captions from AudioCaps, and 3D sound localization for the REAL component TUT Sound Events 2018. ELSA is the only model that allows both open vocabulary language understanding and spatial localization, and performs comparably against the baselines for both tasks.", "description": "This table compares the performance of different models on two tasks: semantic caption retrieval and 3D sound localization.  It shows that ELSA, the model introduced in the paper, performs comparably to state-of-the-art models on both tasks, despite being the only model to handle both open vocabulary language and spatial audio.  The table highlights ELSA's unique ability to combine semantic and spatial understanding.", "section": "5 Experiments, Results, and Discussion"}, {"figure_path": "wDDvJzvvBR/tables/tables_24_4.jpg", "caption": "Table 1: Comparison of model capabilities and performance for retrieval of semantic captions from AudioCaps, and 3D sound localization for the REAL component TUT Sound Events 2018. ELSA is the only model that allows both open vocabulary language understanding and spatial localization, and performs comparably against the baselines for both tasks.", "description": "This table compares the capabilities of different models in semantic caption retrieval and 3D sound localization.  It highlights that ELSA, unlike other models, handles both open vocabulary language understanding and spatial localization, achieving comparable performance to state-of-the-art models in both tasks.", "section": "Experiments, Results, and Discussion"}]