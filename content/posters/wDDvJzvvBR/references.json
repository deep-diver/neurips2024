{"references": [{"fullname_first_author": "Alec Radford", "paper_title": "Learning Transferable Visual Models from Natural Language Supervision", "publication_date": "2021-07-01", "reason": "This paper introduces CLIP, a foundational model for multimodal learning that inspired the architecture and training methodology of ELSA."}, {"fullname_first_author": "Benjamin Elizalde", "paper_title": "CLAP: Learning Audio Concepts from Natural Language Supervision", "publication_date": "2023-05-01", "reason": "This paper introduces CLAP, a state-of-the-art audio foundation model that ELSA builds upon, adding spatial awareness."}, {"fullname_first_author": "Christoph Schuhmann", "paper_title": "LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs", "publication_date": "2021-12-01", "reason": "This paper describes the LAION-400M dataset, a large-scale dataset of image-text pairs, which influenced the creation of the ELSA dataset."}, {"fullname_first_author": "Jort F Gemmeke", "paper_title": "Audio Set: An Ontology and Human-labeled Dataset for Audio Events", "publication_date": "2017-03-01", "reason": "This paper introduces AudioSet, a large-scale dataset of audio events that forms a part of the data used to train ELSA."}, {"fullname_first_author": "Miguel Sarabia", "paper_title": "Spatial LibriSpeech: An Augmented Dataset for Spatial Audio Learning", "publication_date": "2023-08-01", "reason": "This paper introduces Spatial LibriSpeech, a dataset of spatially augmented audio that ELSA uses for training its spatial audio branch."}]}