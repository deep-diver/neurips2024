[{"figure_path": "wDDvJzvvBR/figures/figures_2_1.jpg", "caption": "Figure 1: Our pipeline for learning spatial-audio representations aligned with natural language.", "description": "The figure shows the pipeline used for creating a dataset of spatial audio and corresponding natural language captions.  First, spatial audio is simulated using different room properties and source locations (a). Then, the original captions are augmented with this spatial information, and a large language model (LLM) rewrites the captions to better align with the spatial attributes of the audio (b). Finally, the augmented captions and audio are encoded and aligned using a contrastive learning objective (c).", "section": "3 Paired Spatial Audio and Text Datasets"}, {"figure_path": "wDDvJzvvBR/figures/figures_8_1.jpg", "caption": "Figure 2: UMAP projection of ELSA embeddings of the test splits of Spatial-Clotho and Spatial-AudioCaps. Filled markers are obtained from spatial audio, and hollow markers are obtained from spatial captions. The UMAP projection was fitted with the train splits of Spatial-Clotho and Spatial-Audio caps, and we made use of supervised dimension reduction to highlight the direction differences rather than the semantic differences in the embeddings.", "description": "This figure shows a UMAP projection of the ELSA embeddings from the test sets of Spatial-AudioCaps and Spatial-Clotho datasets.  The plot visually represents the embeddings in a 2D space, where each point corresponds to an embedding.  Filled markers indicate embeddings derived from spatial audio data, while hollow markers represent embeddings derived from spatial captions.  The UMAP projection itself is guided by the training set embeddings. The visualization highlights how the embeddings cluster according to the direction of the sound source (left, right, front, back). This demonstrates that the model effectively encodes spatial information in both audio and textual representations.", "section": "5.4 Interpreting the representation structure of ELSA"}, {"figure_path": "wDDvJzvvBR/figures/figures_17_1.jpg", "caption": "Figure 1: Our pipeline for learning spatial-audio representations aligned with natural language.", "description": "This figure illustrates the process of creating a dataset for training a spatially aware audio-language model.  It shows three stages: (a) Spatial audio pipeline that simulates various room properties and microphone placements, creating spatial audio; (b) Augmentation of original captions with simulated room information, then prompting a large language model (LLM) to rewrite captions to reflect the spatial information; (c) Encoding of the augmented captions and audio using encoders, aligning representations using contrastive learning (CLIP objective). This pipeline helps create training data that links spatial audio attributes with natural language descriptions, enabling the model to learn spatial awareness.", "section": "3 Paired Spatial Audio and Text Datasets"}, {"figure_path": "wDDvJzvvBR/figures/figures_17_2.jpg", "caption": "Figure A.F.2: Architecture diagram for Spatial Attributes Branch. Filled blocks include trainable parameters. The AddCoords2D block is described in [20].", "description": "This figure details the architecture of the spatial attributes branch of the audio encoder. It consists of two parallel branches processing active and reactive intensity vectors, each containing six convolutional blocks followed by a flatten layer, a dropout layer, and an ELU activation. The outputs of these two branches are then concatenated and fed into a three-layer multilayer perceptron (MLP) that projects them into a 44-dimensional embedding. This design allows the model to capture both the semantic and spatial attributes of the audio input.", "section": "A.8 Spatial attributes branch of audio encoder details"}, {"figure_path": "wDDvJzvvBR/figures/figures_18_1.jpg", "caption": "Figure 1: Our pipeline for learning spatial-audio representations aligned with natural language.", "description": "The figure illustrates the pipeline used to create a dataset for training ELSA, a model that learns to map between spatial audio and natural language descriptions.  It begins with generating simulated spatial audio using room parameters and microphone/source placement. This simulated audio is then paired with existing audio captions, which are then rephrased by a large language model (LLM) to incorporate the spatial characteristics of the simulated room. These augmented captions and audio are finally encoded and aligned using a contrastive learning method.", "section": "3 Paired Spatial Audio and Text Datasets"}, {"figure_path": "wDDvJzvvBR/figures/figures_20_1.jpg", "caption": "Figure 1: Our pipeline for learning spatial-audio representations aligned with natural language.", "description": "The figure illustrates the pipeline used for creating a dataset and training the ELSA model.  It begins by simulating spatial audio in various environments with specific parameters (room dimensions, materials, reverberation, source locations). These parameters are then incorporated into captions describing the audio.  The captions are then fed to a large language model (LLM) to rephrase them into more natural-sounding sentences that incorporate the spatial attributes. Finally, the spatially augmented captions and audio are encoded and aligned using a contrastive learning objective, resulting in a model that jointly learns semantic and spatial audio representations.", "section": "3 Paired Spatial Audio and Text Datasets"}, {"figure_path": "wDDvJzvvBR/figures/figures_21_1.jpg", "caption": "Figure 1: Our pipeline for learning spatial-audio representations aligned with natural language.", "description": "This figure illustrates the pipeline used to generate a dataset for training the ELSA model.  It begins with using simulated rooms with varying parameters (dimensions, materials, reverberation) and placing sound sources at different locations within those rooms.  The original captions for the audio are then augmented using the room properties, and a large language model (LLM) rewrites these captions to incorporate the spatial characteristics of the generated audio. Finally, these spatially augmented captions and audio are encoded and aligned using a contrastive learning approach.", "section": "3 Paired Spatial Audio and Text Datasets"}, {"figure_path": "wDDvJzvvBR/figures/figures_23_1.jpg", "caption": "Figure 2: UMAP projection of ELSA embeddings of the test splits of Spatial-Clotho and Spatial-AudioCaps. Filled markers are obtained from spatial audio, and hollow markers are obtained from spatial captions. The UMAP projection was fitted with the train splits of Spatial-Clotho and Spatial-Audio caps, and we made use of supervised dimension reduction to highlight the direction differences rather than the semantic differences in the embeddings.", "description": "This figure shows a UMAP projection of the ELSA embeddings from the test sets of Spatial-AudioCaps and Spatial-Clotho datasets.  The filled markers represent embeddings from spatial audio data, while the hollow markers represent embeddings from spatial captions.  The UMAP projection was created using supervised dimension reduction, which emphasizes the differences in directionality between the embeddings rather than their semantic content.  The plot visually demonstrates how ELSA's representation space clusters embeddings based on spatial direction. ", "section": "Interpreting the representation structure of ELSA"}, {"figure_path": "wDDvJzvvBR/figures/figures_25_1.jpg", "caption": "Figure A.F.5: Architecture diagram for spatial audio caption generation.", "description": "This figure shows the architecture of the spatial audio caption generation system. First, the first-order ambisonics (FOA) audio is fed into the ELSA audio branch, which outputs a 512-dimensional embedding. This embedding is then passed through a 2-layer multi-layer perceptron (MLP), resulting in a 768-dimensional embedding. This embedding is then concatenated with the text embedding, which is obtained by the GPT-2 model. Finally, the combined embedding is used for autoregressive decoding to generate the caption. ", "section": "A.15 Further details on Spatial Audio Caption Generation"}]