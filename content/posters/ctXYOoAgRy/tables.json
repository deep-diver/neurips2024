[{"figure_path": "ctXYOoAgRy/tables/tables_4_1.jpg", "caption": "Table 1: Multilingual performance on XLSum when deactivating language-specific neurons (\"Lang-Spec\") and an equivalent number of randomly selected neurons (\"Random\").", "description": "This table presents the results of a multilingual summarization experiment using the XLSum dataset.  It compares the performance of two large language models (Vicuna and Mistral) under different conditions: the original model, after deactivating randomly selected neurons, and after deactivating language-specific neurons. The performance is measured for several languages (French, Chinese, Spanish, Russian) and averaged.  The goal is to show that deactivating language-specific neurons significantly reduces performance more than deactivating random neurons, suggesting the importance of these neurons for multilingual capabilities.", "section": "2 Parallel Language-specific Neuron Detection (PLND)"}, {"figure_path": "ctXYOoAgRy/tables/tables_6_1.jpg", "caption": "Table 2: Results of the understanding task, where 'X' indicates that chosen neurons in the corresponding layer are deactivated, and '\u2714' signifies they are activated. A is defined as the difference between the reduction in performance in English, denoted as \u2206Eng, and the reduction in performance in non-English languages, denoted as An-Eng.", "description": "This table presents the results of the understanding task after deactivating different sets of neurons in various layers of the model.  It shows the performance in English and non-English languages after deactivating random neurons or language-specific neurons in the understanding, task-solving, and generation layers. The 'A' metric helps determine the relative impact on English vs. non-English performance after deactivation, indicating which neurons are more crucial for each language.", "section": "3.3 Verify the Understanding Stage in MWork"}, {"figure_path": "ctXYOoAgRy/tables/tables_7_1.jpg", "caption": "Table 2: Results of the understanding task, where 'X' indicates that chosen neurons in the corresponding layer are deactivated, and '\u2714' signifies they are activated. A is defined as the difference between the reduction in performance in English, denoted as \u2206Eng, and the reduction in performance in non-English languages, denoted as An-Eng.", "description": "This table presents the results of an experiment on the understanding task.  It shows the performance (in English and non-English languages) after deactivating different sets of neurons: randomly selected, language-specific in different layers (Understanding, Task-Solving, Generation), and the difference between the performance reduction in English and non-English.  The goal is to verify the function of the understanding layer in the proposed Multilingual Workflow (MWork).", "section": "3.3 Verify the Understanding Stage in MWork"}, {"figure_path": "ctXYOoAgRy/tables/tables_7_2.jpg", "caption": "Table 2: Results of the understanding task, where 'X' indicates that chosen neurons in the corresponding layer are deactivated, and '\u2714' signifies they are activated. A is defined as the difference between the reduction in performance in English, denoted as \u2206Eng, and the reduction in performance in non-English languages, denoted as An-Eng.", "description": "This table presents the results of an experiment on the understanding task.  It shows the performance (in English and non-English languages) after deactivating different sets of neurons in various layers of the model (understanding, task-solving, and generation layers). The 'A' metric helps to determine if the deactivation impacts English performance differently compared to non-English performance, highlighting the specificity of certain neurons for non-English languages.", "section": "3.3 Verify the Understanding Stage in MWork"}, {"figure_path": "ctXYOoAgRy/tables/tables_8_1.jpg", "caption": "Table 5: Results of the generation task. The highest performance reduction difference (A) is achieved by disabling all language-specific neurons in the generation layer.", "description": "This table shows the results of the generation task when deactivating different sets of neurons in various layers of the model. It compares the performance when deactivating language-specific neurons versus randomly selected neurons, and focuses specifically on the generation layer. The results are presented for English and non-English languages, showing the impact of deactivating language-specific neurons on multilingual capabilities. The '\u0394\u2191' column indicates the performance difference between English and non-English language performance changes after deactivation. ", "section": "3.6 Verify the Generation Structure in MWork"}, {"figure_path": "ctXYOoAgRy/tables/tables_8_2.jpg", "caption": "Table 6: Enhancement is achieved by fine-tuning Mistral-7b-v0.1 model utilizing 400 documents from each language correspondingly. The results are averaged across four tasks. Performance on English (\"En\") is obtained by averaging the results from four fine-tuned models.", "description": "This table shows the performance improvement achieved by fine-tuning the Mistral language model using language-specific neurons.  The model was fine-tuned with 400 documents per language, and the results are presented for five languages (English, Vietnamese, Thai, Arabic, and Swahili) across four tasks.  The \"Original\" row shows the performance before fine-tuning, \"Random\" shows performance after fine-tuning a random set of neurons, and \"Lang-Spec\" shows results after fine-tuning language-specific neurons. The improvements in low-resource languages are notable.", "section": "3.2 Verification Experiment Setup"}, {"figure_path": "ctXYOoAgRy/tables/tables_13_1.jpg", "caption": "Table 7: Corpus details across languages are tailored to encompass the majority of each language's vocabulary, where \u201ccorpus size\u201d indicates the number of contexts selected, \u201ccorpus vocab\u201d represents the vocabulary coverage within the selected contexts, \u201cvocab size\u201d refers to the number of vocabularies of that language.", "description": "This table shows the size of the corpus used for each of the six languages in the study (English, German, French, Chinese, Spanish, and Russian).  It indicates the number of documents (Corpus Size), the number of unique words within those documents (Corpus Vocab), and the total number of words in the vocabulary of that language (Vocab Size).  The corpus size is tailored to ensure substantial coverage of each language's vocabulary.", "section": "2 Parallel Language-specific Neuron Detection (PLND)"}, {"figure_path": "ctXYOoAgRy/tables/tables_14_1.jpg", "caption": "Table 8: Performance of deactivating language-specific neurons without overlapped between English.", "description": "This table presents the results of an experiment where language-specific neurons were deactivated in a multilingual language model. Specifically, it shows the impact of deactivating all language-specific neurons versus only those that do not overlap with English. The results are presented in terms of the model's performance on English and non-English languages, showing the effect of removing language-specific neurons on multilingual capabilities.", "section": "2.4 Analysis of Language-Specific Neurons"}, {"figure_path": "ctXYOoAgRy/tables/tables_14_2.jpg", "caption": "Table 8: Performance of deactivating language-specific neurons without overlapped between English.", "description": "This table presents the results of an experiment where language-specific neurons, excluding those shared with English, were deactivated. It compares the performance in English and non-English languages after this selective deactivation to the performance when all language-specific neurons are deactivated. The goal is to investigate the impact of removing the overlapping neurons and assess their role in multilingual understanding.", "section": "2.4 Analysis of Language-Specific Neurons"}, {"figure_path": "ctXYOoAgRy/tables/tables_15_1.jpg", "caption": "Table 1: Multilingual performance on XLSum when deactivating language-specific neurons (\"Lang-Spec\") and an equivalent number of randomly selected neurons (\"Random\").", "description": "This table presents the results of a multilingual summarization experiment on the XLSum dataset using two large language models (Vicuna and Mistral).  It shows the performance (average F1 score) of each model on four languages (French, Chinese, Spanish, and Russian) under two conditions: 1) Deactivating neurons identified as being specific to each language; 2) Deactivating the same number of randomly selected neurons. The comparison reveals the impact of language-specific neurons on multilingual summarization performance.", "section": "2 Parallel Language-specific Neuron Detection (PLND)"}, {"figure_path": "ctXYOoAgRy/tables/tables_15_2.jpg", "caption": "Table 1: Multilingual performance on XLSum when deactivating language-specific neurons (\"Lang-Spec\") and an equivalent number of randomly selected neurons (\"Random\").", "description": "This table presents the results of a multilingual summarization experiment on the XLSum dataset.  It compares the performance of two large language models (Vicuna and Mistral) when language-specific neurons (neurons consistently activated for specific languages) are deactivated, versus when an equivalent number of random neurons are deactivated.  The performance is measured across four high-resource languages (French, Chinese, Spanish, Russian) using the average F1 score, showing the impact of language-specific neurons on the model's ability to summarize text in different languages.  The metric shows that deactivating language-specific neurons causes a significantly larger drop in performance than deactivating random neurons, indicating the crucial role of language-specific neurons in handling multilingual tasks.", "section": "2 Parallel Language-specific Neuron Detection (PLND)"}, {"figure_path": "ctXYOoAgRy/tables/tables_15_3.jpg", "caption": "Table 13: XQuAD with Chinese on Mistral.", "description": "This table shows the results of the XQuAD experiment on Mistral model with Chinese language. It presents the accuracy (ACC) achieved under different settings of deactivating neurons (Du) in the understanding layer and deactivating neurons (Dg) in the generation layer with various numbers of layers (N1 and N2) involved.  The results are compared to the original performance of the model on English (En-Vanilla) and Chinese (Zh-Vanilla).  It helps in determining the optimal layer numbers (N1 and N2) for deactivation to maximize performance.", "section": "3.3 Verify the Understanding Stage in MWork"}, {"figure_path": "ctXYOoAgRy/tables/tables_15_4.jpg", "caption": "Table 1: Multilingual performance on XLSum when deactivating language-specific neurons (\"Lang-Spec\") and an equivalent number of randomly selected neurons (\"Random\")", "description": "This table presents the results of a multilingual summarization experiment using the XLSum dataset.  It shows the performance of two large language models (Vicuna and Mistral) when language-specific neurons (neurons consistently activated when processing a particular language) were deactivated, compared to when an equivalent number of randomly selected neurons were deactivated.  The performance is measured in terms of average scores across four languages (French, Chinese, Spanish, and Russian). The goal is to demonstrate the impact of language-specific neurons on the models' multilingual capabilities.", "section": "2 Parallel Language-specific Neuron Detection (PLND)"}, {"figure_path": "ctXYOoAgRy/tables/tables_16_1.jpg", "caption": "Table 1: Multilingual performance on XLSum when deactivating language-specific neurons (\"Lang-Spec\") and an equivalent number of randomly selected neurons (\"Random\").", "description": "This table presents the results of a multilingual summarization experiment on the XLSum dataset using two large language models (LLMs): Vicuna and Mistral.  The experiment involved selectively deactivating either language-specific neurons or a random set of neurons within the models.  The table shows the performance (likely measured as accuracy or ROUGE score) on summarization tasks across several languages (French, Chinese, Spanish, Russian) for each deactivation method.  It allows comparison of the impact of deactivating language-specific versus randomly selected neurons on multilingual performance.", "section": "2 Parallel Language-specific Neuron Detection (PLND)"}, {"figure_path": "ctXYOoAgRy/tables/tables_17_1.jpg", "caption": "Table 1: Multilingual performance on XLSum when deactivating language-specific neurons (\"Lang-Spec\") and an equivalent number of randomly selected neurons (\"Random\").", "description": "This table presents the results of a multilingual summarization experiment using the XLSum dataset.  It shows the performance of two language models (Vicuna and Mistral) when language-specific neurons are deactivated, compared to when a similar number of randomly selected neurons are deactivated.  The performance is measured across multiple languages (French, Chinese, Spanish, Russian) and averaged. The table helps demonstrate the impact of language-specific neurons on multilingual capabilities.", "section": "2 Parallel Language-specific Neuron Detection (PLND)"}, {"figure_path": "ctXYOoAgRy/tables/tables_17_2.jpg", "caption": "Table 1: Multilingual performance on XLSum when deactivating language-specific neurons (\"Lang-Spec\") and an equivalent number of randomly selected neurons (\"Random\")", "description": "This table shows the results of multilingual text summarization experiments using the XLSum dataset.  The performance of two large language models (Vicuna and Mistral) is evaluated under two conditions: deactivating language-specific neurons and deactivating a similar number of randomly selected neurons.  The metrics are presented for several high-resource languages (French, Chinese, Spanish, and Russian), showing the impact of deactivating language-specific neurons on the models' ability to perform summarization in various languages.  The average performance across all languages is also included.", "section": "2 Parallel Language-specific Neuron Detection (PLND)"}, {"figure_path": "ctXYOoAgRy/tables/tables_17_3.jpg", "caption": "Table 1: Multilingual performance on XLSum when deactivating language-specific neurons (\"Lang-Spec\") and an equivalent number of randomly selected neurons (\"Random\").", "description": "This table presents the results of a multilingual summarization experiment using the XLSum dataset.  It compares the performance of two large language models (Vicuna and Mistral) when language-specific neurons are deactivated versus when a comparable number of random neurons are deactivated. The goal is to assess the impact of language-specific neurons on the models' ability to summarize text in different languages.  The table shows the performance (presumably a metric like ROUGE score) for each language (French, Chinese, Spanish, Russian) and the average across these languages, broken down by whether language-specific or random neurons were deactivated. This helps determine the importance of language-specific neurons for multilingual capabilities.", "section": "2 Parallel Language-specific Neuron Detection (PLND)"}]