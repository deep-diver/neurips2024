[{"figure_path": "G99BSV9pt5/tables/tables_6_1.jpg", "caption": "Table 1: Models' performance on task generalization. R-CBMs generalize well in relational tasks. A indicates methods that cannot be applied due to the dataset structure. OOT indicates out-of-time training due to large domains.", "description": "This table compares the performance of various models, including R-CBMs and standard CBMs, on several relational tasks.  It shows that R-CBMs generalize well across different datasets and task types.  The table also indicates where certain models were not applicable (A) or took too long to train (OOT). The performance metrics used vary depending on the specific task (ROC-AUC, Accuracy, MRR).", "section": "5 Key Findings"}, {"figure_path": "G99BSV9pt5/tables/tables_6_2.jpg", "caption": "Table 2: MRR and Hits@N metrics on the test set of the WN18RR and FB15k-237dataset. The competitor results have been taken from Cheng et al. [2] or from the original datasets.", "description": "This table compares the performance of Relational Concept Bottleneck Models (R-CBMs) with other state-of-the-art methods on two knowledge graph datasets, WN18RR and FB15k-237.  The metrics used are Mean Reciprocal Rank (MRR) and Hits@N (the proportion of correct answers within the top N predictions). The table shows that R-CBMs achieve competitive performance compared to existing approaches.", "section": "4 Experiments"}, {"figure_path": "G99BSV9pt5/tables/tables_7_1.jpg", "caption": "Table 3: CBMs response to interventions. R-CBMs effectively respond to human interventions.", "description": "This table presents the results of experiments evaluating the effectiveness of different Concept Bottleneck Models (CBMs) in responding to human interventions.  The experiments involved introducing adversarial examples that caused concept encoders to mispredict concepts.  The table shows the performance (in terms of AUC) of different models before and after the interventions.  The results demonstrate that Relational Concept Bottleneck Models (R-CBMs) are more effective at responding to interventions than standard CBMs.", "section": "5.2 Interpretability"}, {"figure_path": "G99BSV9pt5/tables/tables_8_1.jpg", "caption": "Table 1: Models' performance on task generalization. R-CBMs generalize well in relational tasks. A indicates methods that cannot be applied due to the dataset structure. OOT indicates out-of-time training due to large domains.", "description": "This table presents the performance of various models on task generalization across different datasets.  It compares the performance of Relational Concept Bottleneck Models (R-CBMs) against standard CBMs, black-box relational models, and other relational methods. The datasets represent diverse relational tasks, including image classification, link prediction, and node classification. The table highlights the ability of R-CBMs to generalize well to relational tasks, even outperforming other methods in several instances.  The 'A' indicates that some methods were not applicable to certain datasets due to structural constraints, while 'OOT' denotes instances where training ran out of time due to the scale of the dataset.", "section": "5 Key Findings"}, {"figure_path": "G99BSV9pt5/tables/tables_8_2.jpg", "caption": "Table 5: Data efficiency (Citeseer dataset). Relational CBMs are more robust than an equivalent relational black-box when reducing the amount of supervised training nodes.", "description": "This table presents the results of an experiment evaluating the performance of relational CBMs and relational black-box models on the Citeseer dataset with varying amounts of supervised training data (100%, 75%, 50%, and 25%).  The goal was to assess the robustness of each model type under data scarcity conditions. The table shows that relational CBMs, particularly R-DCR, maintain relatively high accuracy even when the amount of supervised training data is significantly reduced. This contrasts with the relational black-box model, which exhibits a much sharper decline in performance as the amount of supervision decreases.", "section": "5.3 Low data regimes"}, {"figure_path": "G99BSV9pt5/tables/tables_14_1.jpg", "caption": "Table 1: Models' performance on task generalization. R-CBMs generalize well in relational tasks. A indicates methods that cannot be applied due to the dataset structure. OOT indicates out-of-time training due to large domains.", "description": "This table presents the performance comparison of various models (including R-CBMs and baselines) on several relational tasks (RPS, Hanoi, Cora, Citeseer, PubMed, Countries S1, Countries S2).  The metrics used for evaluation vary depending on the task type (ROC-AUC for RPS and Hanoi; accuracy for Cora, Citeseer, PubMed; MRR for Countries S1 and S2). The table highlights the superior generalization capabilities of R-CBMs compared to standard CBMs and other relational black-box methods, demonstrating their effectiveness even in scenarios with out-of-distribution data and limited training time.", "section": "5 Key Findings"}, {"figure_path": "G99BSV9pt5/tables/tables_14_2.jpg", "caption": "Table 7: Completeness scores of each concept-based model wrt the relational black-box baseline.", "description": "This table presents the completeness scores for various concept-based models compared to a relational black-box baseline.  Completeness score is a metric evaluating how well a model captures the underlying concepts. Higher scores indicate better concept coverage. The table shows the scores for different datasets (RPS, Hanoi, Cora, Citeseer, PubMed, Countries S1, Countries S2) and different models (CBM-Linear, CBM-Deep, DCR, R-CBM Linear, R-CBM Deep, R-DCR).  It helps to assess the effectiveness of different model architectures in representing and utilizing concepts for prediction tasks.", "section": "5 Key Findings"}]