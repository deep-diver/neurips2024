[{"type": "text", "text": "The Best of Both Worlds: On the Dilemma of Out-of-distribution Detection ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Qingyang Zhang1,\u2217 Qiuxuan Feng1, Joey Tianyi Zhou2, Yatao Bian3, Qinghua $\\mathbf{H}\\mathbf{u}^{\\mathbf{1}}$ , Changqing Zhang1\u2020 College of Intelligence and Computing, Tianjin University1 A\\*STAR2, Tencent AI Lab3 ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Out-of-distribution (OOD) detection is essential for model trustworthiness which aims to sensitively identify semantic OOD samples and robustly generalize for covariate-shifted OOD samples. However, we discover that the superior OOD detection performance of state-of-the-art methods is achieved by secretly sacrificing the OOD generalization ability. Specifically, the classification accuracy of these models could deteriorate dramatically when they encounter even minor noise. This phenomenon contradicts the goal of model trustworthiness and severely restricts their applicability in real-world scenarios. What is the hidden reason behind such a limitation? In this work, we theoretically demystify the \u201csensitive-robust\u201d dilemma that lies in many existing OOD detection methods. Consequently, a theory-inspired algorithm is induced to overcome such a dilemma. By decoupling the uncertainty learning objective from a Bayesian perspective, the confilct between OOD detection and OOD generalization is naturally harmonized and a dual-optimal performance could be expected. Empirical studies show that our method achieves superior performance on standard benchmarks. To our best knowledge, this work is the first principled OOD detection method that achieves state-of-the-art OOD detection performance without compromising OOD generalization ability. Our code is available at https://github.com/QingyangZhang/DUL. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Endowing machine learning models with out-of-distribution (OOD) detection and OOD generalization ability are both essential for their deployment in the open world [1, 2, 3]. We borrow an example of autonomous driving from [4] to demonstrate the motivation of these two tasks. Given a machine learning model trained on in-distribution (ID) data (top image in Fig. 1 (a)), OOD detection aims to sensitively perceive uncertainty arising upon outliers that do not belong to any known classes of training data [5] (bottom right image in Fig. 1 (a)). While OOD generalization expects machine learning models to be robust in the presence of unexpected noise or corruption, e.g., rainy or snowy weather (bottom left image in Fig. 1 (a)). In this paper, we reveal that many previous methods pursue OOD detection performance at a secret cost of sacrificing OOD generalization ability. To make things worse, we observe that some SOTA OOD detection methods may result in a catastrophic collapse in classification performance $\\mathord{\\sim}15\\%$ accuracy degradation) when encountering even slight noise. One pioneering work [4] makes a trade-off between OOD detection and OOD generalization, but the relationship between these two tasks is still largely unexplored. The learning objectives of these two tasks are seemingly conflicting at first glance. OOD detection encourages sensitive uncertainty awareness (highly uncertain prediction) on unseen data, while generalization expects the prediction to be confident and robust under unforeseeable distributional shifts. Previous work in OOD detection ", "page_idx": 0}, {"type": "image", "img_path": "B9FPPdNmyk/tmp/5fa03af4a10f02455e14f57c6d6cc7e69f8a983e52a81283c81595f87e49170d.jpg", "img_caption": ["(a)Three types of data arise in the open world "], "img_footnote": [], "page_idx": 1}, {"type": "image", "img_path": "B9FPPdNmyk/tmp/7c535c9aef434e2e1ffbc6742aa7aeedfa273ff3c9fdcd2eb70390c1898220dd.jpg", "img_caption": ["(b\uff09Dilemma of current SOTA methods "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: (a): Models trained on in-distribution (ID) data inevitably encounter distributional shifts during their deployment. OOD generalization expects the model to correctly classify covariate-shifted data that undergoes noise or corruption due to environmental issues. OOD detection aims to identify samples that do not belong to any known classes for trustworthiness consideration. (b): Limitations of current advanced OOD detection methods. We consider 8 representative OOD detection methods including the baseline method MSP [6] (without any OOD detection regularization), Entropy [7], EBM [8], Bayesian [9], SOTA OOD detection methods WOODS [10], POEM [11], recent advanced SCONE [4] which aims to seek for a good trade-off and the proposed DUL. All these methods exhibit a degraded generalization ability compared to baseline method MSP and lie in a trade-off area except our DUL. The goal of this paper is to understand and mitigate this phenomenon. ", "page_idx": 1}, {"type": "text", "text": "research area [4] characterizes the relationship between OOD detection and OOD generalization as a trade-off and thus striking for a balanced performance. However, this trade-off significantly limits the employment of current state-of-the-art OOD detection methods. Naturally, one might require the model to be aware of the OOD input for ensuring safety, but certainly does not expect to sacrifice the generalization ability, not to mention that the catastrophically collapsed classification performance under noise or corruption. ", "page_idx": 1}, {"type": "text", "text": "In this work, we first uncover the potential reason behind this limitation by characterizing the generalization error lower bound of previous OOD detection methods, which is referred to sensitiverobust dilemma. To overcome the dilemma, we devise a novel Decoupled Uncertainty Learning (DUL) framework for dual-optimal performance. The decoupled uncertainties are separately responsible for characterizing semantic OOD (detection) and covariate-shifted OOD (generalization). Thanks to the decoupled uncertainty learning objective, dual-optimal OOD detection and OOD generalization performance could be expected. Our emphasis lies on a particular category of OOD detection methods in the classification task, including max softmax probability (MSP) based model [6], energy-based model (EBM) [8] and Bayesian methods [9]. This selection offers two-fold advantages. First, MSP, EBM and Bayesian detectors encompass major OOD detection advances in classification task [5]. Second, numerous OOD detection works in diverse learning tasks (classification, object detection [12], time-series prediction and image segmentation) are all roughly related to classification [13]. The contributions of this paper are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 This paper reveals that existing SOTA OOD detection methods may suffer from catastrophic degradation in terms of OOD generalization. That is, their superior OOD detection ability is achieved by (secretly) sacrificing OOD generalization ability. We theoretically demystify the sensitive-robust dilemma in learning objectives as the main reason behind such a limitation.   \n\u2022 In contrast to previous works that characterize OOD detection and generalization as confilctive learning tasks and thus implying an inevitable trade-off, we propose a novel learning framework termed Decoupled Uncertainty Learning (DUL) to successfully break through the limitation beyond a simple trade-off. Our DUL substantially harmonizes the conflict between OOD detection and OOD generalization, which achieves the best OOD detection performance without sacrificing the OOD generalization ability.   \n\u2022 We conduct extensive experiments on standard benchmarks to validate our findings. Our DUL achieves dual-optimal OOD detection and OOD generalization performance. To our best knowledge, DUL is the first method that gains state-of-the-art OOD detection performance without sacrificing OOD generalization ability. ", "page_idx": 1}, {"type": "text", "text": "2 Related works ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "OOD detection aims to indicate whether the input arises from unknown classes that are not present in training data, which is essential for model trustworthiness. In the classification task, the majority of advanced OOD detection methods include MSP detectors which characterize samples with lower max softmax probability as OOD [6, 14, 15, 7, 16]. EBM detectors identify high energy samples as OOD and frequently establish better performance than MSP detectors [8, 11, 17, 4], and various other types OOD detection methods such as distance-based detectors [18], non-parametric KNN-based detectors [19] which also show promises. According to the training paradigm, OOD detection methods can be split into auxiliary OOD-free and auxiliary OOD-required methods. Auxiliary OOD-free methods directly use the model pre-trained on ID data only for OOD detection. Another line of methods assumes that some OOD data is accessible during training and incorporates auxiliary outlier datasets (collected from websites or other datasets) for further enhancing OOD detection performance. By exposing the model to some semantic OOD during training, auxiliary OOD-required methods frequently outperform auxiliary OOD-free methods on commonly-used benchmarks [20, 5]. ", "page_idx": 2}, {"type": "text", "text": "OOD generalization expects the model to be robust under unforeseeable noise or corruption [21, 22, 23, 24, 25]. Basically, OOD generalization expects invariant and confident prediction on OOD data. Examples include classic domain adaption (DA) methods which encourage the model\u2019s behavior to be invariant across different distributions [21, 26, 27]. Besides, test-time adaption (TTA) directly encourages confident predictions on OOD data by minimizing predictive entropy [28, 29, 30]. However, as we will show later, confident prediction and invariance are seemingly confilctive to OOD detection purpose and further imply an unavoidable trade-off. The most related work to our paper is SCONE [4], which strikes to keep a balance between OOD detection and generalization performance. We argue that such a trade-off is not necessary and the conflict can be elegantly eliminated. ", "page_idx": 2}, {"type": "text", "text": "Uncertainty estimation in Bayesian framework. In the Bayesian framework, predictive uncertainty can be regarded as an indicator of whether the input sample is prone to be OOD. Since OOD samples are unseen during training and thus should be of higher uncertainty than ID. The overall predictive uncertainty of a classification model can be decomposed into three factors according to their source, including data (aleatoric) uncertainty (AU), distributional uncertainty (DU), and model (epistemic) uncertainty (EU) [31, 9]. AU measures the natural complexity of the data (e.g., class overlap, label noise) and EU results from the difficulty of estimating the model parameters with finite training data. DU arises due to the mismatch between the distributions of test and training data. A line of classic measurement can be used to capture various types of uncertainty including entropy, mutual information, and differential entropy [9]. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We consider $K$ -class classification task with classifier $f:\\mathcal{X}\\rightarrow\\mathbb{R}^{K}$ parameterized by $\\theta$ , where $\\mathcal{X}$ is the input space and $\\mathcal{V}=\\{1,2,...,K\\}$ denotes the target space. The model output $f_{\\boldsymbol{\\theta}}(\\boldsymbol{x})$ is considered as logits. The $k$ -th element in logits is denoted as $f_{k}(x)$ indicates the confidence of predicting $x$ to class $k$ . The predicted distribution $F(x)$ is obtained by normalizing $f(x)$ with the softmax function. We first formalize all possible distributions that the model might encounter. ", "page_idx": 2}, {"type": "text", "text": "\u2022 In-distribution $P_{\\mathcal{X}\\mathcal{Y}}^{\\mathrm{ID}}$ which denotes the distribution of labeled training data. ", "page_idx": 2}, {"type": "text", "text": "\u2022 Covariate-shifted OOD $P_{X\\mathcal{X}}^{\\mathrm{COV}}$ which is relevant to OOD generalization. P COVis of the same label space with ID. However, its marginal distribution $P_{\\mathcal{X}}^{\\mathrm{COV}}$ encounters shifts due to unexpected noise or corruption. \u2022 Semantic OOD $P_{X\\mathrm{3^{\\prime}}}^{\\mathrm{SEM}}$ is the distribution of data that do not belong to any known class. Its label space has no overlap with the known ID label space, i.e., $y^{\\prime}\\cap y=\\emptyset$ . ", "page_idx": 2}, {"type": "text", "text": "In the following paper, we omit the subscript for simplicity. The goal of OOD detection is to build a detector $G:\\mathcal{X}\\rightarrow[\\mathrm{IN},\\mathrm{OUT}]$ to decide whether an input $x$ is semantic OOD data or not through a thresholding function $G$ deduced from classifier $f$ ", "page_idx": 2}, {"type": "equation", "text": "$$\nG_{\\gamma}(x)=\\left\\{\\!\\!\\begin{array}{l l}{{\\mathrm{IN}}}&{{g_{f}(x)\\leq\\gamma}}\\\\ {{\\mathrm{OUT}}}&{{g_{f}(x)>\\gamma}}\\end{array}\\!,\\right.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\gamma$ is the threshold. $g_{f}$ is an OOD scoring function deduced from $f$ , which is expected to assign a higher value to OOD than ID. For example, in MSP detectors, $g_{f}(x)=-\\mathrm{max}_{k}\\;F(x)$ where $\\bar{F(x)}$ is the predicted softmax probability (negative max softmax probability). In EBM detectors, $g_{f}$ is realized by the energy function $\\begin{array}{r}{E(x;f):=-\\log\\sum_{i=1}^{K}e^{f_{k}(x)}}\\end{array}$ and the semantic input of OOD should be of high energy [8]. Since it is difficult to fore see $P^{\\mathrm{SEM}}$ one will encounter, a board line of OOD detection works [7, 8, 9, 11, 17, 12, 32, 33] regularize the model on some auxiliary OOD data $P_{\\mathrm{train}}^{\\mathrm{SEM}}$ hdeuruirnisgt itcr ation ihnagn d(lee. gu.,n kdnatoaw fnr otemst -tthiem ew eOb OoDr $P_{\\mathrm{test}}^{\\mathrm{SEM}}$ d. aTtahsee tlse)a,r nainndg  eoxbpjeecctt itvhee i sm sohdoewl nc aans  fleolalronw usseful ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta}\\mathbb{E}_{(x,y)\\sim P^{\\mathrm{ID}}}[\\mathcal{L}_{\\mathrm{CE}}(f(x),y)]+\\lambda\\mathbb{E}_{\\tilde{x}\\sim P_{\\mathrm{train}}^{\\mathrm{SEM}}}[\\mathcal{L}_{\\mathrm{reg}}f(\\tilde{x})],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathcal{L}_{\\mathrm{CE}}$ is the standard cross entropy loss for the original classification task. ${\\mathcal L}_{\\mathrm{reg}}$ is the OOD detection regularization term depending on the detector used, which generally encourages a high uncertainty on $P_{\\mathrm{train}}^{\\mathrm{SEM}}$ . For example, ${\\mathcal{L}}_{\\mathrm{reg}}$ is set to cross entropy between $F(x)$ and the uniform distribution for MSP detector [7]. In EBM detectors [8], ${\\mathcal{L}}_{\\mathrm{reg}}$ is realized as a margin ranking loss to explicitly encourage a large energy gap between ID and semantic OOD. In this paper, we are interested in this setting for the following reasons: ", "page_idx": 3}, {"type": "text", "text": "1) In contrast to labeled data in supervised learning literature, auxiliary OOD data can be unlabeled and easy-to-collected in practice [11].   \n2) Most SOTA methods involve auxiliary outliers [5, 20] for superior performance.   \n3) Even under some strict assumptions that $P_{\\mathrm{train}}^{\\mathrm{SEM}}$ is unavailable, recent works utilize GAN [15], diffusion model [34] or sampling strategy [12] to generate \u201cvirtual\" outliers for training. ", "page_idx": 3}, {"type": "text", "text": "Thus we believe this setting is promising and the cost of auxiliary outliers is minor given the importance of ensuring model trustworthiness. At test-time, the model is evaluated in terms of ", "page_idx": 3}, {"type": "text", "text": "\u2022 ID accuracy (ID-Acc $\\uparrow$ ) which measures the model\u2019s performance on $P^{\\mathrm{ID}}$ , \u2022 OOD accuracy (OOD-Acc $\\uparrow$ ) measures the OOD generalization ability on $P^{\\mathrm{COV}}$ , \u2022 False positive rate at $95\\%$ true positive rate $(\\mathrm{FPR}95\\downarrow):=\\mathbb{E}_{x\\sim P_{\\mathrm{test}}^{\\mathrm{SEM}}}(\\mathbb{I}(G_{\\gamma}(x)=\\mathrm{IN}))$ measures the OOD detection ability, where $\\gamma$ is chosen when true positive rate (TPR) is $95\\%$ . I is the indicator function. In OOD detection, ID samples are considered as negative. ", "page_idx": 3}, {"type": "text", "text": "nbye  otvheer llaapbeple sd pcalcaes soefs palneds r ariensipnegc-ttiivmeley ,a uwxei lihaarvye $P_{\\mathrm{train}}^{\\mathrm{SEM}}$ .t $\\mathcal{Y}_{\\mathrm{test}}^{\\mathrm{SEM}}$ waisned, $\\upgamma_{\\mathrm{train}}^{\\mathrm{SEM}}$ P tSesEtM P tSraEinM $\\mathcal{V}_{\\mathrm{test}}^{\\mathrm{SEM}}\\cap\\mathcal{V}_{\\mathrm{train}}^{\\mathrm{SEM}}=\\emptyset$ OOD detection would be a trivial problem. ", "page_idx": 3}, {"type": "text", "text": "4 Sensitive-robust Dilemma of Out-of-distribution Detection ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we detail the limitation of current OOD detection methods: their OOD detection performance is achieved at the cost of generalization ability. This limitation implies the potential risk of SOTA OOD detection methods and underscores the urgent need for a better solution. Firstly, we re-examine representative OOD detection methods of six different types, including 1) baseline model MSP that is trained without any OOD detection regularization [6], 2) entropy-regularization (Entropy) that encourages high predictive entropy on OOD [7], which is devised for MSP detectors, 3) energy-regularization for EBM detectors that enforces the output with high energy score for OOD input [8], 4) Bayesian uncertainty learning that encourages high overall uncertainty on OOD [9], 5) state-of-the-art OOD detection methods WOODS [10] and POEM [11] 6) the most related SCONE [4] that seeks for a trade-off between OOD detection and generalization performance. ", "page_idx": 3}, {"type": "text", "text": "Limitation of current OOD detection methods. In Fig. 1 (b), we investigate current OOD detection methods in terms of OOD classification error and FPR95. The expected classifier should yield both low OOD classification error and FPR95. As it is observed, despite the superior OOD detection performance, all above methods significantly underperform the baseline MSP in terms of OOD generalization. By contrast, our method (DUL) successfully overcomes the limitation. ", "page_idx": 3}, {"type": "text", "text": "Theoretical justification. Toward understanding the limitation, we provide theoretical analysis for two types of most popular OOD detection methods, i.e., MSP and EBM detectors. Our analysis identifies the \u201csensitive-robust\u201d dilemma as the main reason behind such a limitation. The roadmap of our analysis is: (1) inspired by transfer learning theory, we first reveal that OOD detection regularization applied on semantic OOD may also affect the behavior of model on covariate-shifted OOD; (2) then we demonstrate why MSP detectors suffer from poor generalization by characterizing its generalization error bound; (3) we further identify that EBM methods [8] suffer from a similar drawback when incorporating with gradient-based optimization. First of all, we recap the definition of disparity discrepancy in transfer learning theory [35, 27]. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Definition 1 (Disparity with Total Variation Distance). Given two hypotheses $f^{\\prime},f\\ \\in\\ {\\mathcal{F}}$ and distribution $P$ , we define the Disparity with Total Variation Distance between them as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{disp}_{P}(f^{\\prime},f)=\\mathbb{E}_{P}[T V(F_{f}||F_{f^{\\prime}})],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $F_{f^{\\prime}},F_{f}$ are the class distributions predicted by $f^{\\prime},f$ respectively. $T V(\\cdot||\\cdot)$ is the total variation distance, i.e., $\\begin{array}{r}{T V(F_{f}||F_{f^{\\prime}})=\\frac{1}{2}\\sum_{k=1}^{K}||F_{f,k}-F_{f^{\\prime},k}||.}\\end{array}$ . ", "page_idx": 4}, {"type": "text", "text": "Definition 2 (Disparity Discrepancy with Total Variation Distance, DD with TVD). Given a hypothesis space $\\mathcal{F}$ and two distributions $P,Q$ , the Disparity Discrepancy with Total Variation Distance $^{D D}$ with TVD) is defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\nd_{\\mathcal{F}}(P,Q):=\\operatorname*{sup}_{f^{\\prime},f\\in\\mathcal{F}}(\\mathrm{disp}_{P}(f^{\\prime},f)-\\mathrm{disp}_{Q}(f^{\\prime},f)).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Disparity discrepancy (DD) measures the \"distance\" between two distributions $P,Q$ which considers the hypothesis space. DD is one of the most fundamental conceptions in transfer learning theory which constrains the behavior of hypothesis in $\\mathcal{F}$ should not be dissimilar substantially on different distributions $P$ and $Q$ . 3If the DD between semantic OOD and covariate-shifted OOD is limited, one can suppose that OOD detection regularization applied to semantic OOD samples will also influence the model\u2019s behavior on covariate-shifted OOD. Thus encouraging high uncertainty on semantic OOD may also result in highly uncertain prediction on covariate-shifted OOD, which is potentially harmful to generalization ability. We first formalize this intuition for MSP detectors. ", "page_idx": 4}, {"type": "text", "text": "Theorem 1 (Sensitive-robust dilemma). Let $\\mathcal{P}^{\\mathrm{COV}}$ , $P_{\\mathrm{test}}^{\\mathrm{SEM}}$ be the covariate-shifted OOD and semantic OOD distribution. $\\operatorname{GError}_{P^{\\operatorname{COv}}}(f)$ denotes standard cross entropy loss taking expectation on $P^{\\mathrm{COV}}$ , i.e., generalization error. Then we have ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underbrace{\\mathrm{GError}_{P}\\mathrm{cov}(f)}_{\\mathrm{C}}}&{\\ge C-\\sqrt{\\frac{1}{8\\kappa^{2}}}\\,\\mathbb{E}_{P_{\\mathrm{test}}^{\\mathrm{SEM}}}[\\mathrm{~\\boldmath~\\underbrace~{\\mathcal~{L}}_{\\mathrm{reg}}(f)~}_{=\\mathrm{~\\tiny~{~\\it~\\/~\\!~\\infty~}~}}-\\log K]^{\\frac{1}{2}}-\\frac{1}{2\\kappa}d_{\\mathcal{F}}(\\mathcal{P}^{\\mathrm{COV}},\\mathcal{P}_{\\mathrm{test}}^{\\mathrm{SEM}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where ${\\mathcal{L}}_{\\mathrm{reg}}$ is the OOD detection loss devised for MSP detectors defined in $I7J$ , i.e., cross-entropy between predicted distribution $F(x)$ and uniform distribution. $d_{\\mathcal{F}}(P^{\\mathrm{COV}},P_{\\mathrm{test}}^{\\mathrm{SEM}})$ is $_{D D}$ with TVD that measures the dissimilarity of covariate-shifted OOD and semantic OOD. $C$ and $\\kappa$ are both some constants depending on hypothesis space $\\mathcal{F}$ , $\\bar{P}^{\\mathrm{COV}}$ and $P_{\\mathrm{test}}^{\\mathrm{SEM}}$ ", "page_idx": 4}, {"type": "text", "text": "The proof is deferred in Appendix A. Theorem 1 demonstrates that for MSP detectors, the OOD detection objective confilcts with OOD generalization. The model\u2019s generalization error lower bound is negatively correlated with OOD detection loss that the model tries to minimize. Thus given a lpirmeidtiecdt $d_{\\mathcal{F}}$ , opnu .lo Iwt  iOs OwDo rdteht encotitoinn gl otshsa to ns $P_{\\mathrm{test}}^{\\mathrm{SEM}}$ iwnitlle raplrseot iantievvei ttahbelyo rreesmu lits i na phpilgihclayb luen cfeorrt aailnl $P^{\\mathrm{COV}}$   \nMSP-based OOD detectors no matter whether the model involve s P tSraEinM during training or not. Since the inherent motivation of OOD detection methods lies in minimizing the OOD detection loss in $P_{\\mathrm{test}}^{\\mathrm{SEM}}$ , regardless of the training strategies used.   \ndwWihshseiyn m $d_{\\mathcal{F}}(P^{\\mathrm{COV}},P_{\\mathrm{test}}^{\\mathrm{SEM}})$ g. rhIa.t  cstHeiecoamwlse? vt heIran,t   stTihhnisec oelr otewhmee  r s1be, $d_{\\mathcal{F}}(P^{\\mathrm{COV}},P_{\\mathrm{test}}^{\\mathrm{SEM}})$ m eamsl lec aaasnnud r bteersi  vatinhaeyl $P^{\\mathrm{COV}}$ $P_{\\mathrm{test}}^{\\mathrm{SEM}}$ $d_{\\mathcal{F}}(\\dot{P}^{\\mathrm{COV}},P_{\\mathrm{test}}^{\\mathrm{SEM}})$   \nsamples that do not belong to ID classes, one can suppose that semantic OOD samples are extremely   \ndiverse and some are of high similarity with ID and covariate-shifted OOD [37]. Detecting these   \n\u201crIeaDs-olinkaeb\u201dl e OtoO aDs ssuammep al elsi misi tiendh $d_{\\mathcal{F}}(P^{\\mathrm{{\\dot{C}}O V}},P_{\\mathrm{test}}^{\\mathrm{{SEM}}})$ .a llWeen pgreo ovfi dOe OmDor ed edtiescctuisosni o[n1s1 i, n1 t7h, e3 A8]p. pTenhduisx,  iDt .i2s. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "As presented before, the key limitation of MSP detectors is that they enforce high-entropy prediction on semantic OOD. We proceed to reveal that EBM detectors suffer from similar issues due to the natural property of gradient-based optimization. For EBM detectors, ${\\mathcal{L}}_{\\mathrm{reg}}$ is defined by [8] is ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{reg}}=\\mathbb{E}_{\\tilde{x}\\sim P_{\\mathrm{train}}^{\\mathrm{SEM}}}[\\operatorname*{max}(m_{\\mathrm{OUT}}-E(\\tilde{x}),0)]^{2}+\\mathbb{E}_{x\\sim P^{\\mathrm{ID}}}[\\operatorname*{max}(0,E(x)-m_{\\mathrm{IN}})]^{2},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "which constrains the energy score $\\begin{array}{r}{E(x;f):=-l o g\\sum_{k=1}^{K}e^{f_{k}(x)}}\\end{array}$ 1 efk(x) of ID sample x to be lower than that of OOD sample . $m_{\\mathrm{IN}},m_{\\mathrm{OUT}}$ are manually se lected margins. Although such regularization does not indicate high entropy prediction at first glance, unfortunately, we demonstrate that EBM detectors also tend to uncertain prediction when equipped with gradient-based optimization. Here we focus on Gradient Descent (GD) as a showcase. In each training epoch $t$ , model $f_{\\theta}$ is updated with GD as $\\theta_{t+1}=\\theta_{t}-\\eta\\nabla_{\\theta}\\mathcal{L}_{\\mathrm{reg}}$ , where $\\eta$ is the learning rate. For any sample $\\tilde{x}$ drawn from $\\mathcal{P}^{\\mathrm{SEM}}$ , the gradient $\\nabla_{\\theta}\\mathcal{L}_{\\mathrm{reg}}$ can be written as ", "page_idx": 5}, {"type": "equation", "text": "$$\n2\\sum_{k=1}^{K}\\nabla f_{k}(\\tilde{x})\\underbrace{[e^{f_{k}(\\tilde{x})}(\\sum_{k=1}^{K}e^{f_{k}(\\tilde{x})})^{-1}](m_{\\mathrm{OUT}}-E(\\tilde{x}))}_{\\mathrm{higher~for~larger~}f_{k}(\\tilde{x})},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $m_{\\mathrm{OUT}}-E(\\tilde{x})>0$ (otherwise the gradient is zero) and $f_{k}(\\tilde{x})$ is the predicted logits on the $k$ -th class. For sample $\\tilde{x}$ , when class $k$ has larger predicted logit, it contributes more to the overall gradient $\\nabla_{\\theta}\\mathcal{L}_{\\mathrm{reg}}$ and thus could obtain more optimization efforts during backpropagation. Eventually, one can infer that when an EBM detector is about to converge, it tends to high-entropy prediction on $P_{\\mathrm{train}}^{\\mathrm{SEM}}$ accordingly. Incorporating this into the established Theorem 1, this is likely to harm the generalization ability of $P^{\\mathrm{COV}}$ . Empirical evidence can support this supposition (see Table 14 in Appendix C). Therefore both MSP and EBM detectors face the \u201csensitive-robust\u201d dilemma. ", "page_idx": 5}, {"type": "text", "text": "5 Decoupled Uncertainty Learning ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We demonstrate how to handle the dilemma between OOD detection and generalization by decoupled uncertainty learning in the Bayesian framework. Unlike the most related work [4] which aims to seek a good trade-off, our method successfully gets out of the aforementioned sensitive-robust dilemma. ", "page_idx": 5}, {"type": "text", "text": "Uncertainty Estimation in Bayesian Framework. We first revisit the theoretical properties of different types of uncertainty in a Bayesian framework. Non-Bayesian classifiers consider the model\u2019s output $f(x)$ as logits, which is then normalized with softmax to directly model predictive categoricals $p(\\hat{y}|x)$ . While in Bayesian framework [9], $f(x)$ is considered as parameters of a Dirichlet distribution $p(\\mu|x)$ firstly, which is used to model the prior of predictive categoricals $p(\\hat{y}|x)$ by ", "page_idx": 5}, {"type": "equation", "text": "$$\np(\\mu|x)=\\mathrm{Dir}(\\mu|\\alpha)=\\frac{\\Gamma(\\alpha_{0})}{\\prod_{k=1}^{K}\\Gamma(\\alpha_{k})}\\prod_{k=1}^{K}\\mu_{k}^{\\alpha_{k}-1},\\;\\alpha=f(x),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\operatorname{Dir}(\\mu|\\alpha)$ is Dirichlet distribution and $_{\\alpha}$ is the concentration parameters of Dirichlet. The sum of all $\\alpha_{k}\\in\\alpha$ (noted as $\\alpha_{0}$ ) is so called the strength of the Dirichlet distribution, i.e., $\\alpha_{k}>0,\\alpha_{0}=$ $\\sum_{k}\\alpha_{k}$ . After obtaining prior $p(\\mu|x,\\theta)$ , the final predicted posterior $p(\\hat{y}|x)$ over class labels is given by calculating the mean of the Dirichlet prior ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\underbrace{p(\\hat{y}|x,\\theta)}_{p(\\hat{y}|x,\\theta)}~=\\int\\,\\overbrace{p(\\hat{y}|\\mu)}^{\\textstyle{\\mathsf{\\Pi}}}~~\\underbrace{p(\\mu|x,\\theta)}_{\\textstyle{\\mathsf{D}}}~~~d\\mu.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "From a Bayesian perspective, given deterministic parameters $\\theta$ , the overall uncertainty of final prediction $\\dot{p}(\\hat{y}|x)$ can be decomposed into two factors, including data (aleatoric) uncertainty (AU) and distributional uncertainty (DU). DU lies in $p(\\mu|x,\\theta)$ which is defined as uncertainty due to the mismatch between the distributions of test and train data. AU is described by $p(\\hat{y}|\\mu)$ which captures the natural complexity of the data (e.g., class-overlap) [9]. By definition, OOD detection is primarily associated with DU which is only a part of the overall uncertainty. While generalization is related to the overall uncertainty of $p(\\hat{y}|x)$ as we mentioned in related works (both AU and DU). One essential property of DU is that it can be high even if the expected categorical $p({\\hat{y}}|x,\\theta)$ expresses low overall uncertainty. Such a property is well suited to achieve OOD detection and generalization jointly since high DU no longer necessarily indicates high overall uncertainty. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Decoupled Uncertainty Learning. While the aforementioned Bayesian framework enjoys theoretical potentiality, its learning object [9] lacks consideration of OOD generalization. Similar to other OOD detection methods, it also directly enforces high overall uncertainty on OOD ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta}\\mathbb{E}_{\\mathcal{P}^{\\mathrm{ID}}}\\mathrm{KL}(p(y|x))||p(\\hat{y}|x))+\\mathbb{E}_{\\mathcal{P}_{\\mathrm{train}}^{\\mathrm{SEM}}}\\mathrm{KL}(p(\\mu|\\tilde{x}))||\\mathrm{Dir}(\\mu|\\alpha=\\mathbf{1})),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $p(y|x),p(\\hat{y}|x)$ are the ground-truth distribution and predicted distribution on ID. The model\u2019s prediction on OOD is enforced to be close to a rather flat Dirichlet distribution. It is worth noting that $\\operatorname{Dir}(\\mu|\\alpha=1)$ means all classes are equiprobable, and the entropy of the final prediction is maximized. As shown in Fig. 1 (b), the vanilla Bayesian method [9] also suffers from degraded OOD generalization performance. To this end, we propose Decoupled Uncertainty Learning (DUL), a novel OOD detection regularization method that explicitly encourages high DU on OOD samples without affecting the overall uncertainty. Similarly to previous OOD detection methods [8], our DUL is also devised in a finetune manner for effectiveness. Given a classifier $f_{\\theta_{0}}$ well pre-trained on $P^{\\mathrm{ID}}$ , the goal of DUL lies in enhancing its OOD detection performance without sacrificing any generalization ability. Specifically, we finetune the model by encouraging higher DU but non-increased overall uncertainty on $P_{\\mathrm{train}}^{\\mathrm{SEM}}$ . The learning objective of DUL is ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\theta}{\\operatorname*{min}}\\ \\underbrace{{\\mathbb{E}}_{(x,y)\\sim P^{\\mathrm{ID}}}[\\mathcal{L}_{\\mathrm{CE}}(f(x),y)]}_{\\mathrm{ID~classification}}+\\lambda\\underbrace{{\\mathbb{E}}_{\\tilde{x}\\sim P_{\\mathrm{train}}^{\\mathrm{SEM}}}||\\operatorname*{max}(0,(h_{0}+m_{\\mathrm{OUT}})-h)||_{\\tau}}_{\\mathrm{high~distributional~uncertainty~(detection)}}}\\\\ &{\\quad\\quad\\quad\\quad\\mathrm{s.t.}\\qquad\\qquad\\underbrace{H(p(\\hat{y}|\\tilde{x}))={H}(p_{0}(\\hat{y}|\\tilde{x}))}_{\\mathrm{H}(p_{0}(\\hat{y}|\\tilde{x}))}\\qquad\\qquad\\forall\\,\\tilde{x}\\sim\\,P_{\\mathrm{train}}^{\\mathrm{SEM}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $H(\\cdot)$ is the entropy. $p(\\hat{y}|\\tilde{x})$ and $p_{0}(\\hat{y}|\\tilde{x})$ are the predicted distribution on semantic OOD data $\\tilde{x}$ after and before finetuning. The first term is the original ID classification loss. The second term is OOD detection loss, which encourages high DU on outlier $\\tilde{x}$ . $m_{\\mathrm{OUT}}$ and $\\tau>0$ are hyperparameters. $h_{0},h$ are DU on $\\tilde{x}$ before and after finetuning. Here we measure DU with the differential entropy $\\begin{array}{r}{(h[p(\\mu|\\tilde{x},\\theta)]=-\\int_{S^{K-1}}p(\\mu|\\tilde{x})\\mathrm{ln}(p(\\mu|\\tilde{x}))d\\bar{\\mu},S}\\end{array}$ is a $K$ -simplex). We refer interested readers to the Appendix D.1 for mathematical details. The third term constraining on $H(p(\\hat{y}|\\tilde{x}))$ avoids increment of overall uncertainty during finetuning and thus the generalization ability can be retained. Considering the difficulty of constrained optimization, we convert Eq. 11 into an unconstrained form and get our final minimizing objective ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\underbrace{\\mathbb{E}_{P^{\\mathrm{ID}}}[\\mathcal{L}_{\\mathrm{CE}}(f(x),y)]}_{\\mathrm{ID~classification}}+\\mathbb{E}_{P_{\\mathrm{train}}^{\\mathrm{SEM}}}\\{\\lambda\\underbrace{|\\mathrm{max}(0,(h_{0}+m_{\\mathrm{OUT}})-h)||}_{\\mathrm{high~distributional~uncertainty}}+\\underbrace{\\gamma\\mathrm{KL}(p(\\hat{y}|\\widetilde{x})||p_{0}(\\hat{y}|\\widetilde{x}))}_{\\mathrm{unchanged~overall~uncertainty}}\\},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\gamma$ is hyperparameter. In contrast to previous Bayesian method [9], DUL only encourages high DU rather than overall uncertainty on OOD and explicitly discourages high entropy in the final prediction. The implementation details are in Appendix D.1. ", "page_idx": 6}, {"type": "text", "text": "6 Experiment ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We conduct experiments to validate our analysis and the superiority of DUL. The questions to be verified are Q1 Motivation. To what extent does OOD detection confilct with OOD generalization in previous methods? Q2 Effectiveness. Does DUL achieve better OOD detection and generalization performance compared to its counterparts? Q3 Interpretability. Does the proposed method well decouple uncertainty as expected? ", "page_idx": 6}, {"type": "text", "text": "6.1 Experimental Setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Our settings follow the common practice [8, 11, 20, 5] in OOD detection. Here we present a brief description and more details about datasets, metrics, and implementation are in Appendix B.1 and B.2. ", "page_idx": 6}, {"type": "text", "text": "Datasets. $\\mathbf{\\mu}_{\\mathrm{~O~IID~}}$ datasets $P^{\\mathrm{ID}}$ . We train the model on different ID datasets including CIFAR-10, CIFAR-100 and ImageNet-200 (a subset of ImageNet-1K [39] with 200 classes). $\\circ$ Auxiliary OOD datasets $P_{\\mathrm{train}}^{\\mathrm{SEM}}$ . In CIFAR experiments, we use ImageNet-RC as $P_{\\mathrm{train}}^{\\mathrm{SEM}}$ . ImageNet-RC is a downsampled variant of the original ImageNet-1K which is widely adopted in previous OOD detection works [8, 11, 17]. We also conduct experiments on the recent TIN-597 [20] as an alternative. When ImageNet-200 is ID, the remaining 800 classes termed ImageNet-800 are considered as $P_{\\mathrm{train}}^{\\mathrm{SEM}}$ . $\\circ$ OOD detection test sets $P_{\\mathrm{test}}^{\\mathrm{SEM}}$ are a suite of diverse datasets introduced by commonly uLsSeUd Nb-eRn,c hLmSaUrkN -[5C] .[ 4I3n]  CaInFd AiSR UexNp [e4ri4]m eans .u seW SheVnH $P^{\\bar{\\mathrm{ID}}}$ 0is],  IPmlaacgeesN3e6t5- 2[0401,] $P_{\\mathrm{test}}^{\\mathrm{SEM}}$ rceosn [s4is2t]s, $P_{\\mathrm{test}}^{\\mathrm{SEM}}$ ,r do thOeOrwDi sdee tOecOtiDo nd esteetctitinogns ,i st hae trrei vsihaol uplrdo bblee nmo.  \u25e6ovOerOlaDp pgeedn eclraasliszeas tiboetn wteesetn $P^{\\mathrm{ID}}$ $P^{\\mathrm{COV}}$ P tSraEinM and s the original ID test set corrupted with additive Gaussian noise of $\\mathcal{N}(0,5)$ , following [4]. Besides, we also conduct experiments on CIFAR10-C, CIFAR100-C and ImageNet-C which involve 15 diverse types of different noise or corruption (e.g., snow, rain, frost, fog...) in Appendix C. ", "page_idx": 6}, {"type": "table", "img_path": "B9FPPdNmyk/tmp/be67f0c94c0ffb02a97f7202c1ef44d04562302056332db3a0a009fa5a946394.jpg", "table_caption": ["Table 1: OOD detection and generalization performance comparison. Substantial $\\left(\\geq0.5\\right)$ improvement and degradation compared to the baseline MSP [6] (training without any OOD detection regularization) are highlighted in blue or red, respectively. The best and second-best results are in bold or underlined. DUL is the only method that achieves SOTA OOD detection performance (mostly the best or second best) without sacrificing generalization i.e., the value of the entire row is either blue or black. Full results with standard deviation and diverse types of corruption are in Appendix C. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Metrics. For OOD detection performance evaluation, we report the average FPR95, AUROC and AUPR to be consistent with [11]. OOD generalization ability is compared in terms of classification accuracy (OOD-Acc). Besides, we also report classification accuracy on ID test sets (ID-Acc). ", "page_idx": 8}, {"type": "text", "text": "Compared methods. We compare DUL with a board line of OOD detection methods, including auxiliary OOD required and auxiliary OOD free methods. \u25e6Auxiliary OOD-free methods do not require $P_{\\mathrm{train}}^{\\mathrm{SEM}}$ during training, including MSP [6], Maxlogits [49], pretrained EBM [8] and Mahalaobis [18]. \u25e6Auxiliary OOD-required methods explicitly regularize the model on $P_{\\mathrm{train}}^{\\mathrm{SEM}}$ , including entropy-regularization (Entropy) [7], finetuned EBM [8], DPN of Bayesian framework [9], POEM [11] and WOODS [10]. We also compare our DUL to recent advanced SCONE [4] which aims to keep a balance between OOD detection and generalization. ", "page_idx": 8}, {"type": "text", "text": "6.2 Experimental Results ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Dilemma between OOD detection and generalization (Q1). We validate the dilemma mentioned before in Fig. 1. As shown in Tab. 1, though many advanced methods establish superior OOD detection performance, their OOD generalization degrades a lot. For example, recent SOTA POEM achieves nearly perfect OOD detection performance on CIFAR10 when ImageNet-RC serves as $P_{\\mathrm{train}}^{\\mathrm{SEM}}$ with $3.32\\%$ false positive error rate (FPR95). However, its OOD-Acc drops a lot (about $10\\%$ ) compared to baseline MSP. This phenomenon is also observed in other advanced methods. To further detail this phenomenon, we reduce the weight of OOD detection regularization terms in Entropy and finetuned EBM and show the performance on both OOD detection and generalization. As shown in Table 3, when the regularization strength increases, OOD detection performance improves (lower FPR.), while the OOD generalization performance degrades (higher error rate). ", "page_idx": 8}, {"type": "text", "text": "OOD detection and generalization ability (Q2). As shown in Tab. 1, DUL establishes strong overall performance in terms of both OOD detection and generalization. We highlight a few essential observations: 1) Compared to auxiliary OOD free methods, DUL establishes substantial improvement due to additional regularization on auxiliary outliers. 2) Compared to auxiliary OOD required methods, our method achieves superior OOD detection performance without sacrificing generalization ability. Meanwhile, previous OOD detection methods commonly exhibit severely degraded classification accuracy, with many cases increasing by more than $10\\%$ error rate. 3) Comparison to the most related work SCONE [4]. Despite recent advanced SCONE simultaneously considering both two targets, we observe that it can be hard to find a good trade-off. In contrast, dual-optimal OOD detection and generalization performance is achieved by our DUL. Noted that DUL is the only method that achieves state-of-the-art detection performance (mostly the best or second best) without degraded generalization ability (no red values in the entire row). The sensitive-robust dilemma is no longer observed in our method. These observations justify our expectation of DUL. 4) Combining with existing methods. Besides, to further demonstrate the effectiveness of the proposed DUL, we also add the unchanged overall uncertainty term in Eq.12 to the original Entropy and finetuned EBM. The results in Table 2 show that DUL regularization can also benefit EBM. However, combining Entropy with our regularization can not improve the accuracy substantially. This is not surprising, since the target of Entropy (high entropy prediction) and our DUL (non-increased entropy) directly conflict according to Theorem 1. 5) Comparison to methods with an extra OOD detect branch. Different from aforementioned methods, a line of recent OOD detectors [50, 51, 52, 17] employ extra output branches aside from the classification logits (with a shared backbone for feature extraction). For these OOD detectors, our theoretical analysis is not directly applicable and further analysis from a feature learning perspective may be needed in future work. However, the proposed DUL is devised in a finetune manner. Compared to OOD detectors with extra output branches that requires re-training the classifier from scratch, DUL can be applied to any pre-trained model (e.g., from torchvision, huggingface), with modest computation overhead. ", "page_idx": 8}, {"type": "text", "text": "Visualization of estimated uncertainty (Q3). To evaluate the uncertainty estimation, we visualize the distribution of ID (CIFAR-10) and OOD (SVHN) samples in terms of uncertainty. As we can see in Fig. 2 (b), our DUL establishes a distinguishable (distributional) uncertainty gap between test-time ID and OOD data, which indicates a good sensitiveness for OOD detection. By contrast, the baseline method MSP (Fig.2 (a)) can not effectively discriminate ID and OOD. Besides, we visualize the predictive entropy (overall uncertainty) on covariate-shifted OOD (CIFAR-10 with Gaussian noise) in Fig. 2 (c), our DUL yields much lower entropy compared to other methods. Besides, we visualize the data uncertainty on semantic OOD test data (Textures) when CIFAR-10 is ID in Fig. 6.2. The investigated methods are 1) pretrained model training on ID dataset only, 2) finetuned model with OOD detection regularization (ablating the last term in Eq.12), and 3) finetuned model with the full DUL method described by Eq.12. As shown in Fig. 6.2, to keep the overall uncertainty and enlarge the distributional uncertainty (for OOD detection), the data uncertainty must be reduced. We use Eq.17 from [9] to calculate data uncertainty. The distributional uncertainty is shifted by subtracting that on ID dataset. These results meet our expectation. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "image", "img_path": "B9FPPdNmyk/tmp/3f2399076ba330f3c0b5fb760acb7b0ad3fa597db9c67e3e8b817bd5bcd3ce7c.jpg", "img_caption": ["Figure 2: Visualization of different types of uncertainty estimated by DUL. "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "B9FPPdNmyk/tmp/8b9178733686e7c7a92dddcdb56eca44cadfdfcd78f37f60a81869806815499c.jpg", "img_caption": ["Figure 3: Visualization of uncertainty on semantic OOD test dataset when CIFAR-10 is ID dataset. Without DUL (orange), all three types of uncertainty will increase altogether. In contrast, DUL (green) increases the DU but decreases the AU, which further lead to unchanged overall uncertainty. Table 2: Additional results when equip DUL Table 3: We tune the weight of OOD detection tEo BeMxi.s tiInDg  mdaettahsoedts  iis.e .,C IEFnAtrRo-p1y0 .and $P_{\\mathrm{train}}^{\\mathrm{SEM}}$ neids raengdu rlearpiozratt itohne  tFePrRm  (foOr OEDB dMe taesc tiwoenl l mase trEinct)r aonpdy ImageNet-RC. is the original CIFAR-10 error rate (Err, OOD generalization metric). The testset corrupted by Gaussian noise ${\\mathcal{N}}(0,5)$ . experimental settings are the same with Table 2. "], "img_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "B9FPPdNmyk/tmp/3d98ebf297c1ec55b0eede5e2d0845f13523af036589b46f246d3152ef165ddb.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "B9FPPdNmyk/tmp/a5a7aab60b808134504dde33e1824a9dc9998be3640ca910c6f3a041d4e73659.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper provides both theoretical and empirical analysis towards understanding the dilemma between OOD detection and generalization. We demonstrate that the superior OOD detection performance of current advances are achieved at the cost of generalization ability. The theory-inspired algorithm successfully removes the conflict between previous OOD detection and generalization methods. For SOTA OOD detection performance, our implementation assumes that auxiliary outliers are available during training. This limitation is noteworthy for our DUL as well as the most existing SOTA OOD detection methods. We argue that this added cost is minor and reasonable given the significance of ensuring model trustworthiness in open-environments. Reducing the dependency on auxiliary OOD data can be an interesting research direction for the future exploration. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by the National Natural Science Foundation of China (62376193 and 61925602). The authors thank NeurIPS anonymous peer reviewers for their helpful suggestions. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Chunjong Park, Anas Awadalla, Tadayoshi Kohno, and Shwetak Patel. Reliable and trustworthy machine learning for health using dataset shift detection. Advances in Neural Information Processing Systems, 34:3043\u20133056, 2021. [2] Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man\u00e9. Concrete problems in ai safety. arXiv preprint arXiv:1606.06565, 2016.   \n[3] Jiashuo Liu, Zheyan Shen, Yue He, Xingxuan Zhang, Renzhe Xu, Han Yu, and Peng Cui. Towards out-of-distribution generalization: A survey. arXiv preprint arXiv:2108.13624, 2021.   \n[4] Haoyue Bai, Gregory Canal, Xuefeng Du, Jeongyeol Kwon, Robert D Nowak, and Yixuan Li. Feed two birds with one scone: Exploiting wild data for both out-of-distribution generalization and detection. In International Conference on Machine Learning, 2023.   \n[5] Jingyang Zhang, Jingkang Yang, Pengyun Wang, Haoqi Wang, Yueqian Lin, Haoran Zhang, Yiyou Sun, Xuefeng Du, Kaiyang Zhou, Wayne Zhang, Yixuan Li, Ziwei Liu, Yiran Chen, and Hai Li. Openood v1.5: Enhanced benchmark for out-of-distribution detection. arXiv preprint arXiv:2306.09301, 2023.   \n[6] Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution examples in neural networks. International Conference on Learning Representations, 2017.   \n[7] Dan Hendrycks, Mantas Mazeika, and Thomas Dietterich. Deep anomaly detection with outlier exposure. International Conference on Learning Representations, 2019.   \n[8] Weitang Liu, Xiaoyun Wang, John Owens, and Yixuan Li. Energy-based out-of-distribution detection. Advances in neural information processing systems, 2020. [9] Andrey Malinin and Mark Gales. Predictive uncertainty estimation via prior networks. Advances in neural information processing systems, 2018.   \n[10] Julian Katz-Samuels, Julia B Nakhleh, Robert Nowak, and Yixuan Li. Training ood detectors in their natural habitats. In International Conference on Machine Learning, 2022.   \n[11] Yifei Ming, Ying Fan, and Yixuan Li. Poem: Out-of-distribution detection with posterior sampling. In International Conference on Machine Learning, 2022.   \n[12] Xuefeng Du, Zhaoning Wang, Mu Cai, and Yixuan Li. Vos: Learning what you don\u2019t know by virtual outlier synthesis. International Conference on Learning Representations, 2022.   \n[13] Jingkang Yang, Kaiyang Zhou, Yixuan Li, and Ziwei Liu. Generalized out-of-distribution detection: A survey. arXiv preprint arXiv:2110.11334, 2021.   \n[14] Shiyu Liang, Yixuan Li, and Rayadurgam Srikant. Enhancing the reliability of out-ofdistribution image detection in neural networks. International Conference on Learning Representations, 2018.   \n[15] Kimin Lee, Honglak Lee, Kibok Lee, and Jinwoo Shin. Training confidence-calibrated classifiers for detecting out-of-distribution samples. International Conference on Learning Representations, 2018.   \n[16] Dan Hendrycks, Mantas Mazeika, Saurav Kadavath, and Dawn Song. Using self-supervised learning can improve model robustness and uncertainty. Advances in neural information processing systems, 2019.   \n[17] Jiefeng Chen, Yixuan Li, Xi Wu, Yingyu Liang, and Somesh Jha. Atom: Robustifying out-ofdistribution detection using outlier mining. In Machine Learning and Knowledge Discovery in Databases, 2021.   \n[18] Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for detecting out-of-distribution samples and adversarial attacks. Advances in neural information processing systems, 2018.   \n[19] Yiyou Sun, Yifei Ming, Xiaojin Zhu, and Yixuan Li. Out-of-distribution detection with deep nearest neighbors. In International Conference on Machine Learning, pages 20827\u201320840. PMLR, 2022.   \n[20] Jingkang Yang, Pengyun Wang, Dejian Zou, Zitang Zhou, Kunyuan Ding, Wenxuan Peng, Haoqi Wang, Guangyao Chen, Bo Li, Yiyou Sun, et al. Openood: Benchmarking generalized out-ofdistribution detection. Advances in Neural Information Processing Systems, 35:32598\u201332611, 2022.   \n[21] Martin Arjovsky, L\u00e9on Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. arXiv preprint arXiv:1907.02893, 2019.   \n[22] Zhaolin Hu and L Jeff Hong. Kullback-leibler divergence constrained distributionally robust optimization. Available at Optimization Online, 2013.   \n[23] Paul Michel, Tatsunori Hashimoto, and Graham Neubig. Modeling the second player in distributionally robust optimization. International Conference on Learning Representations, 2021.   \n[24] Aman Sinha, Hongseok Namkoong, Riccardo Volpi, and John Duchi. Certifying some distributional robustness with principled adversarial training. International Conference on Learning Representations, 2018.   \n[25] Zongbo Han, Zhipeng Liang, Fan Yang, Liu Liu, Lanqing Li, Yatao Bian, Peilin Zhao, Bingzhe Wu, Changqing Zhang, and Jianhua Yao. Umix: Improving importance weighting for subpopulation shift via uncertainty-aware mixup. Advances in Neural Information Processing Systems, 2022.   \n[26] Han Zhao, Remi Tachet Des Combes, Kun Zhang, and Geoffrey Gordon. On learning invariant representations for domain adaptation. In International conference on machine learning, pages 7523\u20137532. PMLR, 2019.   \n[27] Yuchen Zhang, Tianle Liu, Mingsheng Long, and Michael Jordan. Bridging theory and algorithm for domain adaptation. In International Conference on Machine Learning, 2019.   \n[28] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. International Conference on Learning Representations, 2021.   \n[29] Marvin Zhang, Sergey Levine, and Chelsea Finn. Memo: Test time robustness via adaptation and augmentation. Advances in neural information processing systems, 35:38629\u201338642, 2022.   \n[30] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Zhiquan Wen, Yaofo Chen, Peilin Zhao, and Mingkui Tan. Towards stable test-time adaptation in dynamic wild world. arXiv preprint arXiv:2302.12400, 2023.   \n[31] Alex Kendall and Yarin Gal. What uncertainties do we need in bayesian deep learning for computer vision? Advances in neural information processing systems, 2017.   \n[32] Qizhou Wang, Zhen Fang, Yonggang Zhang, Feng Liu, Yixuan Li, and Bo Han. Learning to augment distributions for out-of-distribution detection. Advances in Neural Information Processing Systems, 36, 2024.   \n[33] Hyunjun Choi, Hawook Jeong, and Jin Young Choi. Balanced energy regularization loss for out-of-distribution detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15691\u201315700, 2023.   \n[34] Xuefeng Du, Yiyou Sun, Jerry Zhu, and Yixuan Li. Dream the impossible: Outlier imagination with diffusion models. Advances in Neural Information Processing Systems, 36, 2024.   \n[35] Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In International Conference on Machine Learning, 2015.   \n[36] Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation with multiple sources. Advances in neural information processing systems, 21, 2008.   \n[37] William Yang, Byron Zhang, and Olga Russakovsky. Imagenet-ood: Deciphering modern out-of-distribution detection algorithms. International Conference on Learning Representations, 2024.   \n[38] Yichen Bai, Zongbo Han, Changqing Zhang, Bing Cao, Xiaoheng Jiang, and Qinghua Hu. Idlike prompt learning for few-shot out-of-distribution detection. arXiv preprint arXiv:2311.15243, 2023.   \n[39] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115:211\u2013252, 2015.   \n[40] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Baolin Wu, Andrew Y Ng, et al. Reading digits in natural images with unsupervised feature learning. In NIPS workshop on deep learning and unsupervised feature learning. Granada, Spain, 2011.   \n[41] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 million image database for scene recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017.   \n[42] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In Proceedings of the IEEE conference on computer vision and pattern recognition, 2014.   \n[43] Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365, 2015.   \n[44] Junting Pan and Xavier Gir\u00f3-i Nieto. End-to-end convolutional network for saliency prediction. arXiv preprint arXiv:1507.01422, 2015.   \n[45] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig Adam, Pietro Perona, and Serge Belongie. The inaturalist species classification and detection dataset. In Proceedings of the IEEE conference on computer vision and pattern recognition, 2018.   \n[46] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. International journal of computer vision, 128(7):1956\u20131981, 2020.   \n[47] Julian Bitterwolf, Maximilian Mueller, and Matthias Hein. In or out? fixing imagenet out-ofdistribution detection evaluation. In International Conference on Machine Learning, 2023.   \n[48] Sagar Vaze, Kai Han, Andrea Vedaldi, and Andrew Zisserman. The semantic shift benchmark. In ICML 2022 Shift Happens Workshop, 2022.   \n[49] Dan Hendrycks, Steven Basart, Mantas Mazeika, Andy Zou, Joe Kwon, Mohammadreza Mostajabi, Jacob Steinhardt, and Dawn Song. Scaling out-of-distribution detection for realworld settings. International conference on machine learning, 2022.   \n[50] Wenjun Miao, Guansong Pang, Xiao Bai, Tianqi Li, and Jin Zheng. Out-of-distribution detection in long-tailed recognition with calibrated outlier class learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 4216\u20134224, 2024.   \n[51] Julian Bitterwolf, Alexander Meinke, Maximilian Augustin, and Matthias Hein. Breaking down out-of-distribution detection: Many methods based on ood training data estimate a combination of the same core quantities. In International Conference on Machine Learning, pages 2041\u20132074. PMLR, 2022.   \n[52] Kai Liu, Zhihang Fu, Chao Chen, Sheng Jin, Ze Chen, Mingyuan Tao, Rongxin Jiang, and Jieping Ye. Category-extensible out-of-distribution detection via hierarchical context descriptions. Advances in Neural Information Processing Systems, 36, 2024.   \n[53] Cl\u00e9ment L Canonne. A short note on an inequality between kl and tv. arXiv preprint arXiv:2202.07198, 2022.   \n[54] Haoqi Wang, Zhizhong Li, Litong Feng, and Wayne Zhang. Vim: Out-of-distribution with virtual-logit matching. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4921\u20134930, 2022.   \n[55] Sagar Vaze, Kai Han, Andrea Vedaldi, and Andrew Zisserman. Open-set recognition: A good closed-set classifier is all you need? International Conference on Learning Representations, 2022.   \n[56] Dan Hendrycks and Thomas G Dietterich. Benchmarking neural network robustness to common corruptions and surface variations. International Conference on Learning Representations, 2019.   \n[57] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. British Machine Vision Conference, 2016.   \n[58] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, 2016.   \n[59] Yarin Gal et al. Uncertainty in deep learning. 2016. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendices ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B Experimental Details 17 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Datasets details 17   \nB.2 Implementation details 18 ", "page_idx": 14}, {"type": "text", "text": "C Additional Results ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "C.1 Uncertainty estimation. 20   \nC.2 Time-consuming comparison. . . 20   \nC.3 Full results with standard deviation. 20   \nC.4 Results of different types of corruption. 21   \nC.5 OOD detection results on individual datasets. 21   \nC.6 Empirical evidence 24 ", "page_idx": 14}, {"type": "text", "text": "D Discussions ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "D.1 Math derivation 24   \nD.2 Discussion about Disparity Discrepancy 24   \nD.3 Social Impact . . 25 ", "page_idx": 14}, {"type": "text", "text": "A Proofs ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "First, we recap the definitions of Disparity with Total Variation Distance and Disparity Discrepancy. ", "page_idx": 14}, {"type": "text", "text": "Definition 3 (Disparity with Total Variation Distance). Given two hypotheses $f^{\\prime},f\\ \\in\\ {\\mathcal{F}}$ and distribution $P$ , we define the Disparity with Total Variation Distance between them as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{disp}_{P}(f^{\\prime},f)=\\mathbb{E}_{P}[T V(F_{f}||F_{f^{\\prime}})],\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $F_{f},F_{f^{\\prime}}$ are the class distributions predicted by $f^{\\prime},f$ respectively. $T V(\\cdot||\\cdot)$ is the total variation distance (TVD), i.e., $\\begin{array}{r}{T V(F_{f}||F_{f^{\\prime}})=\\frac{1}{2}\\sum_{k=1}^{K}||F_{f,k}-F_{f^{\\prime},k}||_{1}}\\end{array}$ . ", "page_idx": 14}, {"type": "text", "text": "Definition 4 (Disparity Discrepancy with Total Variation Distance, DD with TVD). Given a hypothesis space $\\mathcal{F}$ and two distributions $P,Q$ , the Disparity Discrepancy with Total Variation Distance $^{\\mathit{\\Gamma}}D D$ with TVD) is defined as ", "page_idx": 14}, {"type": "equation", "text": "$$\nd_{\\mathcal{F}}(P,Q):=\\operatorname*{sup}_{f^{\\prime},f\\in\\mathcal{F}}(\\mathrm{disp}_{P}(f^{\\prime},f)-\\mathrm{disp}_{Q}(f^{\\prime},f)).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Since TVD is a distance measurement of two distribution. It yields the triangle equality. That is, for any distribution $P_{\\mathcal{X}}$ support on $\\mathcal{X}$ and hypotheses $f^{1},f^{2}$ and $f^{3}\\in\\mathcal{F}$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{disp}_{P_{X}}(f^{1},f^{2})\\leq\\mathbb{E}_{x\\sim P_{X}}[T V(F_{f^{1}}(x)||F_{f^{3}}(x))]+\\mathbb{E}_{x\\sim P_{X}}[T V(F_{f^{2}}(x)||F_{f^{3}}(x))],}\\\\ {\\mathrm{disp}_{P_{X}}(f^{1},f^{2})\\geq\\mathbb{E}_{x\\sim P_{X}}[T V(F_{f^{1}}(x)||F_{f^{3}}(x))]-\\mathbb{E}_{x\\sim P_{X}}[T V(F_{f^{2}}(x)||F_{f^{3}}(x))].}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "To prove Theorem 1, we need the following lemmas. ", "page_idx": 14}, {"type": "text", "text": "Lemma 1. For any $f\\in\\mathcal F$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{E}_{P}\\mathrm{cov}T V(F_{f}||U)\\le\\mathbb{E}_{P_{\\mathrm{test}}^{\\mathrm{SEM}}}T V(F_{f}||U)+d_{\\mathcal{F}}(P^{\\mathrm{COV}},P_{\\mathrm{test}}^{\\mathrm{SEM}})+\\lambda\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\lambda$ is a constant independent of $f.~~U$ is the $K$ -classes uniform distribution. $P^{\\mathrm{COV}}$ is the covariate-shifted OOD distribution. $P_{\\mathrm{test}}^{\\mathrm{SEM}}$ is the semantic OOD distribution. ", "page_idx": 14}, {"type": "text", "text": "Proof. Let $f^{*}$ be the hypothesis which jointly minimizes the total variance distance between the predicted distribution $F_{f}$ with uniform distribution $U$ taking expectation on $P^{\\mathrm{COV}}$ and $P_{\\mathrm{test}}^{\\mathrm{SEM}}$ sEtM, which is to say ", "page_idx": 15}, {"type": "equation", "text": "$$\nf^{*}:=\\underset{f\\in\\mathcal{F}}{\\mathrm{argmin}}\\{\\mathbb{E}_{x\\sim P^{\\mathrm{COv}}}[T V(F_{f}(x)||U)]+\\mathbb{E}_{x\\sim P_{\\mathrm{test}}^{\\mathrm{sgau}}}[T V(F_{f}(x)||U)]\\}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Set $\\lambda=\\mathbb{E}_{x\\sim P^{\\mathrm{COV}}}[T V(F_{f^{*}}(x)||U)]+\\mathbb{E}_{x\\sim P_{\\mathrm{test}}^{\\mathrm{SEM}}}[T V(F_{f^{*}}(x)||U)],$ , then by the triangle equality we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\upxi_{P}\\mathrm{cosv}T V(F_{f}||U)\\leq\\mathrm{disp}_{P}\\mathrm{cov}\\left(f,f^{*}\\right)+\\mathbb{E}_{P}\\mathrm{cov}T V(F_{f^{*}}||U)}&{}\\\\ {\\leq\\mathbb{E}_{P_{\\mathrm{test}}^{\\mathrm{sgna}}}T V(F_{f}||U)-\\mathbb{E}_{P_{\\mathrm{test}}^{\\mathrm{sgna}}}T V(F_{f}||U)+\\mathrm{disp}_{P}\\mathrm{cov}\\left(f,f^{*}\\right)+\\mathbb{E}_{P}\\mathrm{cov}T V(F_{f^{*}}||U)}&{}\\\\ {\\leq\\mathbb{E}_{P_{\\mathrm{test}}^{\\mathrm{sgna}}}T V(F_{f}||U)+\\mathbb{E}_{P_{\\mathrm{test}}^{\\mathrm{sgna}}}T V(F_{f^{*}}||U)}&{}\\\\ {-\\mathrm{disp}_{P_{\\mathrm{test}}^{\\mathrm{sgna}}}(f,f^{*})+\\mathrm{disp}_{P}\\mathrm{cov}(f,f^{*})+\\mathbb{E}_{P}\\mathrm{cov}T V(F_{f^{*}}||U)}&{}\\\\ {\\leq\\mathbb{E}_{P_{\\mathrm{test}}^{\\mathrm{sgna}}}T V(F_{f}||U)+d_{f}(P_{\\mathrm{test}}^{\\mathrm{sEM}},P^{\\mathrm{COv}})+\\lambda.}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Ionnt $P_{\\mathrm{test}}^{\\mathrm{SEM}}$ y( is.pe.e, atkhien gp,r eLdeicmtemd ad i1s tdreibmutoinosnt $F_{f}$ si st hclaot sief  ttho eu cnliafsosrimf iedri $f$ riebxuptrieosns) ,h iitg hw iollv aelrsaoll  tuenncd etrot ahiingthy uncertain prediction on $P^{\\mathrm{COV}}$ given a limited $d_{\\mathcal{F}}(P_{\\mathrm{test}}^{\\mathrm{SEM}},P^{\\mathrm{COV}})$ . ", "page_idx": 15}, {"type": "text", "text": "Lemma 2. [Inequality between $K L$ and TV] For any $K$ -class distribution $P$ and $Q$ on $\\{1,\\cdot\\cdot\\cdot,K\\}$ and $\\kappa>0$ , the following inequality holds ", "page_idx": 15}, {"type": "equation", "text": "$$\n{\\frac{1}{2\\kappa}}K L(P||Q)+{\\frac{\\kappa}{4}}\\geq T V(P||Q).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. The proof can be found in [53] page 8. ", "page_idx": 15}, {"type": "text", "text": "Lemma 3. Denote the OOD detection loss used for MSP detectors as $\\mathcal{L}_{\\mathrm{reg}}$ , then we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}_{P_{\\mathrm{test}}^{\\mathrm{SEM}}}T V(F_{f}||U)\\le\\mathbb{E}_{P_{\\mathrm{test}}^{\\mathrm{SEM}}}\\sqrt{\\frac{1}{2}(\\mathcal{L}_{\\mathrm{reg}}(f)-\\log K)}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $T V(\\cdot||\\cdot)$ is total variance distance (TVD). $U$ denotes uniform distribution support on $y=$ $\\{1,2\\cdot\\cdot\\cdot K\\}$ . ${\\mathcal{L}}_{\\mathrm{reg}}$ is defined in $I7J$ is the cross-entropy between predicted distribution $F_{f}(x)$ and uniform distribution $U$ . ", "page_idx": 15}, {"type": "text", "text": "Lemma 2 means that minimizing the OOD detection loss will constrains the predicted distribution to be close to uniform distribution, which is a intuitive and straightforward result. ", "page_idx": 15}, {"type": "text", "text": "Proof. In $K$ -classes classification task, for any sample $\\tilde{x}$ drawn from $P_{\\mathcal{X}}^{\\mathrm{SEM}}$ EM, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n{\\mathcal{L}}_{\\mathrm{reg}}(f({\\tilde{x}}))=K L(U||F_{f})+H(U).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Applying Pinsker\u2019s Inequality, the following inequality holds ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{reg}}(f(\\tilde{x}))=K L(F_{f}||U)+H(U)\\geq2T V(F_{f}(\\tilde{x})||U)^{2}+H(U).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Noted that $H(U)=\\log K$ , we can re-write above inequality as ", "page_idx": 15}, {"type": "equation", "text": "$$\nT V(F_{f}(\\tilde{x})||U)\\leq\\sqrt{\\frac{1}{2}(\\mathcal{L}_{\\mathrm{reg}}(f(\\tilde{x}))-\\log K)}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then, by taking expectation on $P_{\\mathcal{X}}^{\\mathrm{SEM}}$ we can get the result. ", "page_idx": 15}, {"type": "text", "text": "Now we are ready to present the proof of Theorem 1. ", "page_idx": 15}, {"type": "text", "text": "Proof. By the definition, the generalization error can be written as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{GError}_{P_{x y}^{\\mathrm{COV}}}(f):=\\!\\!\\mathbb{E}_{(x,y)\\sim P_{x y}^{\\mathrm{COV}}}\\mathcal{L}_{\\mathrm{CE}}(f(x),y)}\\\\ &{\\qquad\\qquad\\qquad\\quad=\\!\\!\\mathbb{E}_{(x,y)\\sim P_{x y}^{\\mathrm{COv}}}[K L(P_{t r u e}||F_{f}(x))+H(P_{t r u e})]}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $P_{t r u e}$ is the target distribution given input $x$ (i.e., the true class distribution) and $\\mathcal{L}_{\\mathrm{CE}}(\\cdot)$ denotes the cross-entropy loss. ", "page_idx": 16}, {"type": "text", "text": "Applying Lemma 2, for any $x$ we have ", "page_idx": 16}, {"type": "equation", "text": "$$\nK L(P_{t r u e}||F_{f})\\geq\\frac{1}{2\\kappa}(T V(P_{t r u e}||F_{f}))+\\frac{\\kappa}{4}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "By the sub-additivity of TVD, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{K L(P_{t r u e}||F_{f}(x))\\geq\\cfrac{1}{2\\kappa}(T V(P_{t r u e}||F_{f}))+\\cfrac{\\kappa}{4}}\\\\ &{\\qquad\\qquad\\qquad\\geq\\cfrac{1}{2\\kappa}[T V(P_{t r u e}||U)-T V(F_{f}(x)||U)]+\\cfrac{\\kappa}{4}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Taking expectation on $P_{X\\mathcal{X}}^{\\mathrm{COV}}$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{GError}_{P_{x y}^{\\mathrm{cov}}}(f)=\\!\\!\\mathbb{E}_{(x,y)\\sim P_{x y}^{\\mathrm{cov}}}K L(P_{t r u e}||F_{f}(x))+H(P_{t r u e})}\\\\ &{\\qquad\\qquad\\qquad\\geq\\!\\frac{1}{2\\kappa}\\!\\mathbb{E}_{(x,y)\\sim P_{x y}^{\\mathrm{cov}}}[T V(P_{t r u e}||U)-T V(F_{f}(x)||U)]+\\frac{\\kappa}{4}+\\mathbb{E}_{P_{x y}^{\\mathrm{cov}}}H(P_{t r u e})}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Applying Lemma 1 and Lemma 2, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{GError}_{P_{G}^{\\mathrm{cov}}}(f)=\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\sum_{I,\\gamma_{S}^{\\mathrm{cov}}}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": ".i vFeinn atlhley ,f awcet  tgheatt $P_{X\\mathcal{Y}}^{\\mathrm{COV}}$ , $P_{t r u e}$ are both fixed, $H(P_{t r u e})$ and $T V(P_{t r u e}||U)$ are constants for each $x$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{GError}_{P_{x y}^{\\mathrm{cov}}}(f)\\geq C-\\frac{1}{2\\kappa}\\mathbb{E}_{P_{x}^{\\mathrm{sgM}}}\\sqrt{\\frac{1}{2}(\\mathcal{L}_{\\mathrm{reg}}(f)-\\log K)}-\\frac{1}{2\\kappa}d_{\\mathcal{F}}(P_{x}^{\\mathrm{COV}},P_{x}^{\\mathrm{SEM}}),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\begin{array}{r}{C=\\frac{1}{2\\kappa}(\\mathbb{E}_{P_{\\lambda}^{\\mathrm{COV}}}T V(P_{t r u e}||U)-\\lambda+8)+\\mathbb{E}_{P_{\\lambda\\mathcal{Y}}^{\\mathrm{COV}}}H(P_{t r u e})}\\end{array}$ is a constant. ", "page_idx": 16}, {"type": "text", "text": "B Experimental Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "B.1 Datasets details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "ID datasets $P^{\\mathrm{ID}}$ . ID datasets are chosen following common practice in OOD detection. We use CIFAR-10, CIFAR-100 and ImageNet-200 as $P^{\\mathrm{{\\tilde{ID}}}}$ . ImageNet-200 is a subset of the original ImageNet-1K introduced by [20, 5]. ", "page_idx": 16}, {"type": "text", "text": "Auxiliary OOD datasets $P_{\\mathrm{train}}^{\\mathrm{SEM}}$ . For CIFAR experiments, we use ImageNet-RC and TIN-597 as auxiliary datasets. ImageNet-RC is a down-sampled variant of the ImageNet-1K, which consists of 1000 classes and 1,281,167 images. We also conduct experiments on TIN-597 as an alternative for ImageNet-RC. TIN-597 is introduced by recent work [5]. The resolutions of ID and auxiliary samples are both $64\\times64$ . For ImageNet experiments, we use a subset of ImageNet-1K consisting of 200 classes as ID datasets. The remaining images belong to other 800 classes are utilized as auxiliary datasets. The resolutions of ID and auxiliary images are both $224\\times224$ . ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "OOD detection test datasets $P_{\\mathrm{test}}^{\\mathrm{SEM}}$ . In CIFAR experiments, following standard practice [8], we use SVHN [40], Textures [42], Places365 [41], iSUN [44], LSUN-C and LSUN-R [43] to evaluate the OOD detection performance. \u25e6The SVHN test set comprises 26,032 color images of house numbers. $\\circ$ Textures (Describable Textures Dataset, DTD) consists of 5,640 images depicting natural textures. \u25e6Places365 dataset consists scenic images of 365 different categories. Each class consists of 900 images. \u25e6The iSUN dataset is a subset of the SUN database with 8,925 images. \u25e6The Large-scale Scene Understanding dataset (LSUN) comprises a testing set with 10,000 images of 10 different scenes. LSUN offers two datasets, LSUN-C and LSUN-R. In LSUN-C, the original high-resolution images are randomly cropped into $32\\times32$ . Meanwhile, in LSUN-R, the images are resized to $32\\times32$ . In ImageNet experiments, we follow the settings of [5], where OpenImage-O [54], SSB-hard [55], Textures [42], iNaturalist [45] and NINCO [47] are selected as OOD detection test datasets. $\\circ$ OpenImage-O contains 17632 manually filtered images and is $7.8~\\times$ larger than the ImageNet dataset. $\\circ$ SSB-hard is selected from ImageNet-21K. It consists of 49K images and 980 categories. $\\circ$ Textures (Describable Textures Dataset, DTD) consists of 5,640 images depicting natural textures. $\\circ$ iNaturalist consists of 859000 images from over 5000 different species of plants and animals. $\\circ$ NINCO consists with a total of 5879 samples of 64 classes which are non-overlapped with ImageNet-1K. ", "page_idx": 17}, {"type": "text", "text": "OOD generalization test datasets $P^{\\mathrm{COV}}$ . Following previous work [4], we corrupt the original test data with Gaussian noise of zero mean and variance of 5 in the main paper. In appendix, we conduct additional experiments involving CIFAR10-C, CIFAR100-C and ImageNet-C [56] with 15 diverse types of noise. ", "page_idx": 17}, {"type": "text", "text": "B.2 Implementation details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "B.2.1 CIFAR experiments. ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We use WideResNet-40-10 [57] as the backbone network, which comprises 40 layers. The widen factor is set to 10. We use SGD optimizer to train all methods with dropout strategy. The dropout rate is 0.3. The momentum is set to 0.9 and weight decay is set to 0.0005. ", "page_idx": 17}, {"type": "text", "text": "Pretraining details. The pretrained model is obtained by training WideResNet-40-10 for 200 epochs with an initial learning rate of 0.1. We decay the learning rate by a factor of 0.2 at the 60-th, 120-th, and 160-th epochs. Batch size is set to 128. ", "page_idx": 17}, {"type": "text", "text": "Finetuning details. $\\circ$ For Entropy and EBM, we finetune the pretrained model for 20 epochs with an initial learning rate of 0.001, utilizing a cosine annealing strategy to adjust the learning rate. Following the official implementation, the weight of OOD detection regularization term is set to 0.5 and 0.1 for Entropy and EBM (finetune) respectively. The hyperparameters $m_{\\mathrm{ID}}$ and $m_{\\mathrm{OOD}}$ in EBM regularization learning are set to -25 and -7 respectively. The ID batch size is 128 and the OOD batch size is set to 256. $\\circ$ For SCONE, we finetune the pretrained model for 10 epochs with an initial learning rate of 0.0002, utilizing a cosine annealing strategy to adjust the learning rate. The batch size is 32, the OOD batch size is 64. The margin of the OOD detection boundary is set to 1. To be aligned with most previous works in OOD detection and generalization, we assume $P_{\\mathcal{X}}^{\\mathrm{COV}}$ is unavailable during finetuning. $\\circ$ For WOODS, we finetune the pretrained model for 10 epochs with an initial learning rate of 0.0002, utilizing a cosine annealing strategy to adjust the learning rate. The ID batch size is 32, the OOD batch size is 64. Other hyperparameters settings are consistent with SCONE. $\\circ$ For DUL, $\\alpha_{0}$ is set to 12. While finetuning on CIFAR10, the $m_{\\mathrm{ID}}$ and $m_{\\mathrm{OOD}}$ are set to 10 and 30 respectively. The weight $\\lambda,\\gamma$ are set to 0.3 and 2. We train for 20 epochs with an initial learning rate of 0.00005, utilizing a cosine annealing strategy to adjust the learning rate. While finetuning on CIFAR100/TIN-597, the $m_{\\mathrm{ID}}$ and $m_{\\mathrm{OOD}}$ are set to 10 and 30 respectively. The weights $\\lambda,\\gamma$ are set to 0.05 and 2 respectively. We finetune for 20 epochs with an initial learning rate of 0.00005, utilizing a cosine annealing strategy to adjust the learning rate. While finetuning on CIFAR100/ImageNet-RC, we set $h_{0}=0$ . The $m_{\\mathrm{ID}}$ and $m_{\\mathrm{OOD}}$ are set to $-430$ and -370 respectively. We train for 30 epochs with an initial learning rate of 0.0001, utilizing a cosine annealing strategy to adjust the learning rate. ", "page_idx": 17}, {"type": "text", "text": "The weights $\\lambda,\\gamma$ are set to 0.1 and 1 respectively. For CIFAR-100/ImageNet-RC, we set $\\tau=2$ and otherwise $\\tau=1$ . \u25e6For DUL\u2020, we use Thompson sampling strategy [11] for OOD informativeness mining. The sampling hyperparameters are consistent with that of POEM. ", "page_idx": 18}, {"type": "text", "text": "Training from scratch details. \u25e6For POEM, we train from scratch for 200 epochs with an initial learning rate of 0.1, and decay the learning rate by a factor of 0.2 at the 60-th, 120-th, and 160-th epochs following [57]. The ID and OOD batch size are set to 128 and 256 respectively. Following the official implementation, the pool of outliers consists of randomly selected 400,000 samples from auxiliary datasets, and only 50,000 samples (same size as the ID training set) are selected for training based on the boundary score. $\\circ$ For DPN, we train for 200 epochs with an initial learning rate of 0.1, and decay the learning rate by a factor of 0.2 at the 60-th, 120-th, and 160-th epochs. The Dirichlet parameters $\\alpha$ are calculated by performing ReLU plus one on the model\u2019s outputs, i.e., $\\alpha=\\operatorname{ReLU}(f(x))+1$ . $\\alpha_{0}$ is set to 15 and 12 respectively when training on CIFAR10 and CIFAR100. The auxiliary datasets are ImageNet and TIN-597. The ID and OOD batch size are set to 128 and 256 respectively. When training on CIFAR100/TIN-597, the OOD regularization weight $\\lambda$ is set to 0.05. In other cases, $\\lambda$ is set to 0.5. ", "page_idx": 18}, {"type": "text", "text": "B.2.2 ImageNet experiments. ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We use ResNet18 [58] as the backbone network. We use SGD optimizer to train all the models. The momentum is set to 0.9. ", "page_idx": 18}, {"type": "text", "text": "Pretraining details. The pretrained model is obtained by training ResNet18 for 100 epochs with an initial learning rate of 0.1, utilizing a cosine annealing strategy to adjust the learning rate. The weight decay is set to 0.0001. Batch size is set to 64. ", "page_idx": 18}, {"type": "text", "text": "Finetuning details. $\\circ$ For Energy regularized learning, we finetune the pretrained model for 10 epochs with an initial learning rate of 0.001, utilizing a cosine annealing strategy to adjust the learning rate. The weight decay is set to 0.0001. Following the official implementation, the weights of OOD detection regularization term are set to 0.1. Specifically, the $m_{\\mathrm{ID}}$ and $m_{\\mathrm{OOD}}$ in energy regularization method are set to -25 and -7 respectively. The ID batch size is 64 and the OOD batch size is set to 128. \u25e6For Entropy, we finetune the pretrained model for 10 epochs with an initial learning rate of 0.001, utilizing a cosine annealing strategy to adjust the learning rate. The weight decay is set to 0.0001. The ID and OOD batch size are set to 64 and 128 respectively. Following the official implementation, the weights of OOD detection regularization term are set to $0.5.\\circ$ For SCONE, we finetune the pretrained model for 10 epochs with an initial learning rate of 0.0002, utilizing a cosine annealing strategy to adjust the learning rate. The weight decay is set to 0.0005. The batch size is 32, the OOD batch size is 64. The margin of the OOD detection boundary is set to 1. To be aligned with most previous works in OOD detection and generalization, we assume $P_{\\mathcal{X}}^{\\mathrm{COV}}$ is unavailable during finetuning. \u25e6For WOODS, we finetune the pretrained model for 10 epochs with an initial learning rate of 0.0002, utilizing a cosine annealing strategy to adjust the learning rate. The batch size is 32, the OOD batch size is 64. Other hyperparameters of WOODS are consistent with SCONE. \u25e6For our DUL, the Dirichlet parameters $\\alpha$ are calculated by performing ReLU and exp operation on the model\u2019s outputs, i.e., $\\alpha=\\exp(\\mathrm{ReLU}(f(x)))$ . For numerical stability on large scale benchmark, we measure the distributional uncertainty by the strength of Dirichlet distribution. $\\lambda,\\gamma$ are set to 0.1 and 4 respectively. We set $\\tau=1$ in large-scale ImageNet experiments. ", "page_idx": 18}, {"type": "text", "text": "Training from scratch details. \u25e6For DPN, we train ResNet18 for 100 epochs with an initial learning rate of 0.1, utilizing a cosine annealing strategy to adjust the learning rate. The weight decay is set to 0.0001. Batch size is set to 64. The Dirichlet parameters $\\alpha$ are calculated by performing ReLU plus one on the model\u2019s outputs, i.e., $\\alpha=\\operatorname{ReLU}(f(x))+1$ . The ID classification loss is set to KL-divergence between predicted class distribution under Dirichlet prior and target distribution because of the inconvenience of directly setting $\\alpha_{0}$ . The target distribution is obtained by label smoothing strategy with parameter of 0.01 [9]. The weight of regularization term applied on OOD auxiliary samples is 1. ", "page_idx": 18}, {"type": "table", "img_path": "B9FPPdNmyk/tmp/2de33de291ac2eb3d4de926f91ec0d04feb98718933aad31351acc0eb5f85cfb.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "C Additional Results ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "C.1 Uncertainty estimation. ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We add Gaussian noise with zero mean and varying variance $\\epsilon$ on CIFAR-10 and investigate the estimated distributional uncertainty and overall uncertainty. Distributional uncertainty is measured by differential entropy. It clear that with DUL regularization, the prediction yields a low overall uncertainty and high distributional uncertainty on covariate-shifted data. We conduct experiments on CIFAR-10/ImageNet-RC and CIFAR-10/TIN-597, tabular results are shown in Tab. 4. ", "page_idx": 19}, {"type": "table", "img_path": "B9FPPdNmyk/tmp/595b7d495b9b45a8c921150f58e0d4c3a73ab780b099029372620fa53703e061.jpg", "table_caption": ["Table 4: Mean value of estimated uncertainty on CIFAR-10-C with varying severity of Gaussian noise with zero mean and variance of $\\epsilon$ . "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "C.2 Time-consuming comparison. ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We compare the time-cost of proposed DUL to other training-required OOD detection methods in Tab. 5. We run all the experiments on one single NVIDIA GeForce RTX-3090 GPU. Compared with other OOD detection methods in a finetune manner, DUL does not introduce noticeably extra cost of computation. ", "page_idx": 19}, {"type": "table", "img_path": "B9FPPdNmyk/tmp/e279d0e0f7a92240b07224341b452140a454c4bae7b4a863760d5c4e2a56cc1d.jpg", "table_caption": ["Table 5: Average execution times (s) per epoch of training required OOD detection methods. Compare to other OOD detection methods, DUL does not introduce noticeable computational cost. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "C.3 Full results with standard deviation. ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Full results with standard deviation are presented this section. In CIFAR experiments, we report the mean and standard deviation in 5 random runs. In ImageNet experiments, we report the mean and standard deviation in 3 random runs to be consist with [5]. CIFAR experimental results are shown in Tab. 6. Large-scale ImageNet results are shown in Tab. 7. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "Table 6: OOD detection and generalization performance comparison with standard variance. Marginal improvement and degradation $\\left(\\geq0.5\\right)$ compare to the baseline method MSP are highlighted in blue or red respectively. The best and second best results are in bold or underlined. DUL is the only method achieves state-of-art OOD detection performance (mostly the best or second best) without trade-offs on generalization i.e., the value of entire row is either blue or black. ", "page_idx": 20}, {"type": "table", "img_path": "B9FPPdNmyk/tmp/38379bb06644d6ddadb6399889a23dda34d253c18a1c4a3266185e1458a52831.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "C.4 Results of different types of corruption. ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We conduct additional experiments on CIFAR10-C, CIFAR100-C and ImageNet-C with 15 different types of corruption. The results validate that the proposed method can improve the overall performance under different types of corruption. ", "page_idx": 20}, {"type": "text", "text": "C.5 OOD detection results on individual datasets. ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We provide OOD detection results of DUL on each individual OOD detection test dataset in Tab. 8 and Tab. 9, based on the checkpoint with random seed 1. ", "page_idx": 20}, {"type": "text", "text": "Table 7: OOD detection and generalization performance comparison with standard variance. Substantially improvement and degradation $\\left(\\geq0.5\\right)$ compare to baseline method w.r.t. MSP are highlighted in blue or red respectively. The best and second best results are in bold or underlined. Similar with CIFAR experiments, DUL establishes strong OOD detection performance (always the best or second best) without degraded generalization i.e., the entire row is either blue or black. ", "page_idx": 21}, {"type": "table", "img_path": "B9FPPdNmyk/tmp/ea082a165262e068e50cb57f876af927f19b1b599d110e4ff87c50c57aeb61e7.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "B9FPPdNmyk/tmp/4d6df83b03004b967dd0fb0da7f7edca9611542ab610ca98035963803df51049.jpg", "table_caption": ["Table 8: OOD detection results of DUL on each individual OOD detection test dataset. "], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "B9FPPdNmyk/tmp/d2a3b753b2b4c49aa22dd9cc31798abcc04b5d0dc9ee3b5a2728274734b5cd1a.jpg", "table_caption": ["Table 9: OOD detection results of DUL on each OOD detection test dataset. "], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "B9FPPdNmyk/tmp/687e7dd3be90e1655ca25ca01550ae438ef285d16a3ee9035192ddbca205373a.jpg", "table_caption": ["Table 10: Classification error rate comparison on CIFAR10-C. ID dataset is CIFAR10. "], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "B9FPPdNmyk/tmp/fa5fce89585fbf3e49e243f855bad5315cc8029386e413f0cdc249b071583005.jpg", "table_caption": ["Table 11: Classification error rate comparison on CIFAR100-C. ID dataset is CIFAR100. "], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "B9FPPdNmyk/tmp/fc4ef0a7eb9ffc4f7cd206c73c71a8fa379dc1405d97846cd86e95a211a31df7.jpg", "table_caption": ["Table 12: Classification error rate comparison on ImageNet-C. Here we test compared methods on a subset of the original ImageNet-C consisting of 200 classes. ID dataset is ImageNet-200. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "Table 13: Comprehensive comparison involving 15 different types of corruption from commonly-used domain adaption benchmark [52]. Substantial $\\left(\\geq0.5\\right)$ improvement and degradation compared to the baseline MSP [6] are highlighted in blue or red respectively. DUL is the only method that achieves SOTA OOD detection performance without sacrificing generalization i.e., the value of the entire row is almost black or blue. The best or second best results are highlighted in bold or underlined. MD is the shorthand of Mahalanobis. ", "page_idx": 22}, {"type": "table", "img_path": "B9FPPdNmyk/tmp/5a24b8c2c76576c5e01e2f6d17b85e8fa1eb330ed78d588fa83891d35250bea9.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "C.6 Empirical evidence ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Here we provide empirical supports to our intuition about energy-based OOD detection regularization. We calculate the entropy of predicted distribution before and after finetuning with Energy regularization [8]. The results show can support our claim in Section 4. ", "page_idx": 23}, {"type": "text", "text": "Table 14: Predictive entropy of predicted distribution on covariate-shifted OOD dataset before and after finetuning with energy-based OOD detection regularization [8]. ", "page_idx": 23}, {"type": "table", "img_path": "B9FPPdNmyk/tmp/d0f27d608d04969a966d4a0448099f059e4283e5fbe3ed16bebeeab952862ee1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "D Discussions ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "D.1 Math derivation ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Differential entropy. The proposed DUL calculates differential entropy as OOD detection measurement. Here we detail how to calculate the differential entropy of a Dirichlet distribution. The following derivation of differential entropy is taken from [9]. The differential entropy of a Dirichlet parameterized by $\\alpha$ is calculated by ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle h[p(\\mu|x)]=-\\int_{S}p(\\mu|x)\\ln(p(\\mu|x))d\\mu}}\\\\ {{\\displaystyle\\qquad=\\sum_{k}^{K}\\ln\\Gamma(\\alpha_{k})-\\ln\\Gamma(\\alpha_{0})-\\sum_{k}^{K}(\\alpha_{k}-1)\\cdot(\\psi(\\alpha_{k})-\\psi(\\alpha_{0}))}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\alpha_{0}$ is the strength of Dirichlet, i.e., $\\begin{array}{r}{\\alpha_{0}=\\sum_{K}\\alpha_{k}.\\ \\alpha_{k}}\\end{array}$ denotes the $k$ -th element in $\\alpha.\\,\\Gamma$ is the Gamma function and $\\psi$ is the digamma function.  Here we provide a PyTorch implementation on how to calculate distribution uncertainty measured by differential entropy. ", "page_idx": 23}, {"type": "table", "img_path": "B9FPPdNmyk/tmp/f605c1cc948e8b6eb7dded834b05dca8465bc2540572f5a86c0056c332a8e23b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "We refer interested readers to [9] and Gal\u2019s PhD Thesis [59] for more detailed math derivations. ", "page_idx": 23}, {"type": "text", "text": "D.2 Discussion about Disparity Discrepancy ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In section 4, we claim that a limited disparity discrepancy between test-time semantic OOD and covariate-shifted OOD is practical. Here we provide some empirical evidence and discussion to support such a claim. ", "page_idx": 23}, {"type": "text", "text": "The key challenge of OOD detection lies in identifying ID-like semantic OOD. As mentioned in recent works [38], effectively distinguishing between the most challenging OOD samples that are much like in-distribution (ID) data is the core challenge of OOD detection. Recent works regularize models on ID-like OOD to enhance the OOD detection performance. Since the ID-like semantic OOD samples are more difficult to be detected and more informative. For example, NTOM [17] and ", "page_idx": 23}, {"type": "text", "text": "POEM [11] utilizes greedy and Thompson sampling strategies to find semantic OOD samples which are more closely to ID. [38] proposes to explicitly discover outliers near ID by prompt learning. ", "page_idx": 24}, {"type": "text", "text": "Semantic OOD and covariate OOD can be very similar in practice. As shown in Fig. 4. There exists many similar samples from semantic OOD and covariate OOD in large-scale commonly used benchmarks. We borrow some examples from recent works [47] to show case. ", "page_idx": 24}, {"type": "image", "img_path": "B9FPPdNmyk/tmp/5d993d1c744996767d4195b2ccd943cb8bee4ef8999a882091d40b0456d4a612.jpg", "img_caption": ["Figure 4: Semantic OOD samples can be very similar to ID. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "D.3 Social Impact ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "AI safety and trustworthiness are closely related to our work. This paper presents work to harmonize the confilcts between out-of-distribution detection methods and model generalization. The proposed method puts effort to enhance machine learning models for their safely deployment on out-ofdistribution data, avoiding both undesirable behavior and degraded performance in challenging high-stake tasks. However, due to the bias from data used by current OOD detection benchmark, e.g., large-scale ImageNet, the ones using the proposed method need to carefully consider the selection of auxiliary outliers for safety-critical applications. ", "page_idx": 24}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The abstract and introduction accurately reflect the paper\u2019s contributions and scope. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 25}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: In conclusion and Appendix. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 25}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper provide the full set of assumptions and a complete (and correct) proof in Appendix. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Section 6 and Appendix. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 26}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: See Appendix and supplemental material. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 27}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper provides above details in Section 6 and Appendix. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 27}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: Appendix C. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: In Appendix C. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 28}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The authors have carefully reviewed the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 28}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: In Appendix. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to ", "page_idx": 28}, {"type": "text", "text": "generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 29}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: The license and terms of use explicitly mentioned and properly respected. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 29}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 30}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 30}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 30}]