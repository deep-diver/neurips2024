[{"type": "text", "text": "Personalized Adapter for Large Meteorology Model on Devices: Towards Weather Foundation Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Shengchao Chen\u2666, Guodong Long\u2666, Jing Jiang\u2666, and Chengqi Zhang ", "page_idx": 0}, {"type": "text", "text": "\u2666Australian Artificial Intelligence Institute, University of Technology Sydney \u2660Department of Data Science and AI, The Hong Kong Polytechnic University shengchao.chen.uts@gmail.com, {guodong.long, jing.jiang}@uts.edu.au chengqi.zhang@polyu.edu.hk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "This paper demonstrates that pre-trained language models (PLMs) are strong foundation models for on-device meteorological variables modeling. We present LM-WEATHER, a generic approach to taming PLMs, that have learned massive sequential knowledge from the universe of natural language databases, to acquire an immediate capability to obtain highly customized models for heterogeneous meteorological data on devices while keeping high efficiency. Concretely, we introduce a lightweight personalized adapter into PLMs and endows it with weather pattern awareness. During communication between clients and the server, lowrank-based transmission is performed to effectively fuse the global knowledge among devices while maintaining high communication efficiency and ensuring privacy. Experiments on real-wold dataset show that LM-WEATHER outperforms the state-of-the-art results by a large margin across various tasks (e.g., forecasting and imputation at different scales). We provide extensive and in-depth analyses experiments, which verify that LM-WEATHER can (1) indeed leverage sequential knowledge from natural language to accurately handle meteorological sequence, (2) allows each devices obtain highly customized models under significant heterogeneity, and (3) generalize under data-limited and out-of-distribution (OOD) scenarios. Code available on https://github.com/shengchaochen82/LM-Weather. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Accurately modeling weather variation pattern from large amount of meteorological variables sequences is increasingly vital for providing efficient weather analysis support for disaster warning. Recently, the promise of learning to understand weather pattern from data via deep learning (DL) has led to an ongoing paradigm shift apart from the long-established physics-based methods [1, 2]. ", "page_idx": 0}, {"type": "text", "text": "Mining potential patterns from meteorological sequences that collected from different regions, including forecasting and imputation, is one of the most important problems in meteorology. Significant progress has been made by several latest time series approaches [1, 3, 4]. These approaches formulate meteorological variable modeling as an end-to-end spatio-temporal learning problem. This overlooks the reality that ground weather devices distributed globally gather vast amounts of data quickly. The sheer volume of data, coupled with limited network capacity, necessitates local processing on the devices, making centralised learning challenging [5]. On-device intelligence enables edge devices to compute independently, offering a primary solution to the problem. ", "page_idx": 0}, {"type": "text", "text": "Federated Learning (FL) [7] is a promising on-device intelligence implementation that collaboratively train a uniform model across devices without exchanging raw data. However, the model often underperform due to data heterogeneity among clients. Personalized FL (PFL) provides new insights for on-device intelligence that allows each device obtains customized models for providing personalized insights [8, 9]. Albeit PFL methods showing revolutionized capability in this field, we argue that the current advancements are not necessarily at their best in on-device meteorological variable modeling as three major obstacles remain and hinder further progress: ", "page_idx": 0}, {"type": "image", "img_path": "llTroju97T/tmp/249259253599d021868530793da3e9dc1ac98c0aff34e235ed1d7283b1408d3a.jpg", "img_caption": ["Figure 1: Framework Overview. (a) Schematic of LM-WEATHER, each client using personalized adapter to endow the PLM for local weather awareness, only low-rank matrices are transmitted to enhance efficiency during communication; (b) Brief structure of PLM on each client, detailed architecture can be found in Appendix; (c) Task Adapter Generation, the multivariate weather series input splits into two paths. The first path isolates the trend, seasonal, and residual elements, which each go through independent generator to produce specific adapters; (d) Architecture of the generator for each decomposed element; (e) Schematic diagram of Channel-Independent Patching [6]. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "(i) Challenge of Heterogeneity. Weather data\u2019s heterogeneity, unlike that of images or text, arises mainly from the unique characteristics of data collected by weather devices in various regions, such as tropical or arid areas. Furthermore, sensor malfunctions or extreme events can lead to collection disruptions or inconsistent missing data, which significantly increase the differences in data distribution across devices.   \n(ii) Underperformed Shallow Network Structures. The vast and varied data gathered by weather devices challenge simpler neural network models to generalize effectively. Furthermore, the frequent updates of weather data (hourly or by the minute) require neural models on devices to train and infer more often. This demand is hard to meet with deeper models that, while more performant, are also more resource-intensive.   \n(iii) Resource-constrained Weather Devices. From a computation perspective, weather devices cannot afford of training complex neural models from scratch, especially for foundation models [4]. From a communication perspective, transmitting complete model during the ", "page_idx": 1}, {"type": "text", "text": "aggregation phase in FL/PFL significantly increases communication overhead, which is impractical for real-time weather modeling. ", "page_idx": 2}, {"type": "text", "text": "Therefore, a compact foundational model (FM) is crucial for personalized on-device weather modeling. Yet, there\u2019s a gap in FMs for observational data. Models trained on large-scale simulation data struggle in practical applications because of notable differences in data formats and parameter scales [1, 4]. ", "page_idx": 2}, {"type": "text", "text": "Inspired by the impressive progress of large language models (LLMs) in natural language processing, recent literature in time series analysis research has also demonstrated that pre-trained LMs provide excellent performance over dedicated models for time series analysis with tuning [10] or reprogramming [11]. This comprehensive and thorough sequence knowledge from language models can be effortlessly transferred across domains without large-scale parameter tuning. Thus, an exciting research question naturally arises: ", "page_idx": 2}, {"type": "text", "text": "Question: Since PLMs are powerful sequence modelers, can we leverage PLMs as foundation models to achieve personalized on-device meteorological variable modeling? ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this paper, we show that pre-trained language models (PLMs) can as outstanding foundation models that tuned on each device with low cost can achieve personalized on-device weather pattern modeling. We propose LM-WEATHER, a generic approach to taming PLMs to understand heterogeneity ondevice weather data. As shown in Fig.1a, we conduct a local tuning on an uniform PLM (e.g., GPT2), where lightweight personalized adapters are implanted to endow PLMs with weather pattern awareness by decomposing weather sequence to implicit knowledge (e.g., seasonal, trend). During communication between client and server, fewer parameters are shared globally while locally retained adapters are enforced to resist heterogeneity and facilitate privacy-assured fusion of global knowledge. ", "page_idx": 2}, {"type": "text", "text": "We highlight our contributions and findings as follows: ", "page_idx": 2}, {"type": "text", "text": "\u2022 We introduce LM-WEATHER, a generic approach that transforms Pre-trained Language Models as the foundation model to customized on-device meteorological variable modeling via personalized adapter. LM-WEATHER yields preferable meteorological variable sequences modeling, while being parameter-, communication-, and data-efficient.   \n\u2022 We collect and compile four real-world versatile datasets for on-device meteorological variable modeling across regions. As opposed to simulated datasets such as ERA5 [12], our datasets are all real-time observations. These datasets based on real-world practice and challenging, provide a pioneer in the field of on-device meteorological variable modeling.   \n\u2022 Experiments show that LM-WEATHER advances the state-of-the-art methods by a large margin across various setting while keeping $3.7\\,\\%$ of parameters communication. LMWEATHER also demonstrates superior communication efficiency in the context of meteorological variable modeling, beating FL baselines tailored to reduce communication overhead.   \n\u2022 In particular, we find that LM-WEATHER can accurately handle structurally nondeterministic sequences (e.g., differences in time or variable dimensions across devices) thanks to the learned sequences knowledge from pre-trained LMs. We also find that LMWEATHER can indeed be spatio-temporal sequences sensitive, thereby better modeling the weather pattern specificity of those high distribution similarity.   \n\u2022 We find that LM-WEATHER can work well in data-limited environments across various few-shot settings. We further evaluate zero-shot generalizability of LM-WEATHER in modeling complex weather patterns of unseen data, including different group of datasets and other devices, and observe superb performance. ", "page_idx": 2}, {"type": "text", "text": "We highlight that the goal of this study is not to compete but instead to complement current on-device meteorological variables modeling framework. Today\u2019s climate foundation models are typically trained from scratch, utilizing exceptionally large datasets (nearly 100TB [4, 13]) and incurring substantial computational costs [1]. We hope that LM-WEATHER offers a cost-effective alternative for modeling meteorological variables on-device, thereby enabling accurate regional weather trend analysis. In addition, the dataset we complied can be the important resource to provide exploring chances for this field, facilitating future research. ", "page_idx": 2}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "2.1 On-device Meteorological Variable (Sequence) Modeling ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The on-device meteorological variable (sequence) modeling challenge involves predicting future sequences from past observations for forecasting or predicting missing values for imputation on each device. While traditional physics-based approach this as a complex problem of solving multilevel atmospheric equations [14], recent deep learning techniques have shown significant potential in uncovering patterns for better weather prediction [4, 2]. ", "page_idx": 3}, {"type": "text", "text": "Problem Formulation On-device meteorological variable modeling can be formulated as an endto-end sequence-to-sequence learning problem for each device without exchange raw data. Formally, a parameterized local model for $i$ -th device $\\mathcal{M}_{\\theta}^{i}$ is tasked with predicting the weather sequence, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{M}_{\\theta}^{i}:\\mathcal{X}_{i}\\rightarrow\\hat{\\mathcal{X}}_{i}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the $\\mathcal{X}_{i}\\in\\mathbb{R}^{L\\times C}$ and $\\hat{\\mathcal{X}}_{i}\\in\\mathbb{R}^{L^{\\prime}\\times C^{\\prime}}$ denote the input and output sequences on $i$ -device, $L$ and $L^{\\prime}$ is the input length and output length, $C$ and $C^{\\prime}$ is the number of input and output variable. Note that the $L^{\\prime}\\to L$ when performing imputation. The local learning objective on each device is to find the model parameter $\\theta$ that minimize the distance between $\\hat{\\mathcal{X}}_{i}$ and $\\mathcal{X}_{i}$ given sufficient weather sequence data. The overall optimization objective is based on FedAvg, ", "page_idx": 3}, {"type": "equation", "text": "$$\nF(\\theta)\\!:=\\operatorname arg\\operatorname*{min}\\sum_{i=1}^{N}\\frac{n_{i}}{n}F_{i}(\\theta_{i}|\\{D_{i}\\}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $n_{i}$ and $n$ is the number of samples held by the $i$ -th client and all clients1, respectively, $F(\\theta|\\{D\\})$ denotes the local objective function, $\\{D\\}$ is the local data. ", "page_idx": 3}, {"type": "text", "text": "2.2 Language Models in Time Series ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Language models (LMs) trained on large-scale sequence data have shown extraordinary advances and led to a significant paradigm shift in NLP, boosting machines in understanding human languages (BERT/MLM-style) and synthesizing human-like text (GPT/CLM-style [15]). Analogies between time series and human languages have long been noted [16]. Recent advancements in time series analysis have demonstrated the effectiveness of PLMs in modeling time series [17, 11]. Although some of those have shown that PLMs can beat time series-specific models in updating a minor fraction of parameters [18]. As such, it is exciting to expect cutting-edge techniques of language modeling can tackle weather variables sequence-related problems rather than considering train climate foundation models [4, 1] from scratch that are heavy and expensive, and are trained from simulated data. ", "page_idx": 3}, {"type": "text", "text": "3 Taming PLMs for On-device Meteorological Variable (Sequence) Modeling ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Overview We proposed a generic framework named LM-WEATHER that encouraging PLMs to yield accurate prediction while keeping high efficiency for each device. The architecture is illustrated in Fig. 1. To endow PLMs with weather pattern awareness, we introduce a lightweight personalized adapter into PLMs (e.g., GPT2 [15]) such that the emergent ability of sequence modeling that transferred from text into weather is activated. To achieve cross-domain knowledge transfer with minimal effort while maintaining the sequence modeling capabilities of PLMs as intact as possible, we introduce lightweight operations in it enables both clients and servers to achieve a good trade-off between performance and efficiency (e.g., computation and communication). ", "page_idx": 3}, {"type": "text", "text": "3.1 Local Training on Each Device ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Our LM-WEATHER refines PLMs for personalized weather sequence modeling on heterogeneous devices using a modular, plug-and-play architecture. Specifically, we introduce personalized adapter consists of (1) Task Adapter from latent weather knowledge and (2) Parameter Adapter that converts representation from the PLM into into weather forecasts. In addition, we employ lightweight operations in local training to boost computational efficiency. ", "page_idx": 3}, {"type": "text", "text": "Task Adapter. To provide PLMs with richer effective information to activate their sequence modeling capabilities in the target knowledge domain, similar to text-based prompts in language to LLMs in NLP, we constructed task adapters by decomposing the input weather sequences into multimodal latent statistical information, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{X}_{\\mathrm{Trend}}^{k}+\\mathcal{X}_{\\mathrm{Seasonal}}^{k}+\\mathcal{X}_{\\mathrm{Residual}}^{k}=\\mathsf{D e c o m p}(\\mathcal{X}^{k}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathcal{X}^{k}\\in\\mathbb{R}^{L\\times1}$ denote the $k$ -th variable in weather sequence $\\mathcal{X}\\in\\mathbb{R}^{L\\times C}$ , the trend component $\\chi_{\\mathrm{Trend}}$ and the seasonal component $\\chi_{\\mathrm{Seasonal}}$ captures the underlying long-term weather pattern and encapsulates the repeating short-term weather cycles, respectively. Furthermore, the residual component $\\chi_{\\mathrm{Residual}}$ represents the remainder of the sequence after the trend and seasonality have been extracted. Note that $\\chi_{\\mathrm{Trend}}$ , $\\mathcal{X}_{\\mathrm{Seasonal}}$ , and $\\chi_{\\mathrm{Residual}}$ have the same shape as $\\mathcal{X}$ . This decomposition explicitly enables the identification of unusual observation and shifts in seasonal patterns or trends. The $\\chi_{\\mathrm{Trend}}$ , XSeasonal, $\\chi_{\\mathrm{Residual}}$ are used to generate Task Adapter via an unified generator as Fig. 1c & Fig. 1d that consisting of Token Embedding, Position Embedding, and Temporal Embedding. Specially, we use one-dimensional convolution operation to map each each specific sample $\\chi^{\\breve{k}}$ while keeping raw shape to generate Token Adapter $P_{\\mathrm{TO}}$ . Additionally, we use a trainable lookup table to map each point\u2019s explicit position in the entire sequence, to generate Position Adapter $P_{\\mathrm{PO}}$ . Furthermore, we separately encode different time attributes such as minutes, hours, days, weeks, and months, via trainable parameters to dynamically model complex temporal shifts, to generate Temporal Adapter $P_{\\mathrm{TE}}$ . Finally, for each decomposition components, corresponding generated adapters can be obtained by aggregating Token Adapter $P_{\\mathrm{TO}}\\in\\mathbb{R}^{L\\,\\dot{\\times}C}$ , Position Adapter $\\breve{P}_{0}\\in\\mathbb{R}^{L\\times C}$ , and Temporal Adapter $\\check{P}_{\\mathrm{TE}}\\overset{\\mathcal{\\mathrm{~\\,~}}}{\\in}\\check{\\mathbb{R}^{L\\times\\check{C}}}$ as $P_{d}=\\dot{P}_{\\mathrm{TO}}^{d}+\\dot{P}_{\\mathrm{PO}}^{d}+P_{\\mathrm{TE}}^{d}$ , where $d\\in\\{$ Trend, Seasonal, Residual}, this means that we can obtain $P_{\\mathrm{Trend}}$ , $P_{\\mathrm{{Seasonal}}}$ , $P_{\\mathrm{Residual}}$ . Details about the generator in Appendix B.5. ", "page_idx": 4}, {"type": "text", "text": "Lightweight Operations. To enhance the PLMs\u2019 ability to represent complex inputs while reducing the computational burden to adapt to low-resource devices, we introduce lightweight operations, which includes channel-independent patching (CIP, Fig. 1e) [6] for input and efficient tuning of parameters for PLMs. Among them, CIP splits the multivariate sequence into separate univariate sequences, each processed by a single model with length $L_{p}$ . This approach outperforms the original method of mixing channels by treating the variables as independent. It enables the model to capture channel interactions indirectly through shared weights, leading to improved performance without directly modeling the complexity of multiple data channels. The total number of inputs patches is $\\begin{array}{r}{P=\\frac{(\\dot{T}-L_{p})}{S}{+2}}\\end{array}$ , where $S$ denotes the horizontal sliding stride. Given these patches $\\mathcal{X}_{P}^{i}\\in\\mathbb{R}^{P\\times L_{p}}$ , we use rearrange operation and a trainable FFN embed them as $\\hat{\\mathcal{X}}_{P}^{i}\\in\\mathbb{R}^{P\\times d_{m}}$ , where $d_{m}$ is dimensions created by the FFN. We also introduce a low-rank adaptation (LoRA) [19] inside PLMs aiming at language modeling for lightweight fine-tuning of attention layers to achieve cross-modal/-domain knowledge transfer from text sequences to weather sequences with minimal effort. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Parameter Adapter. To adapt PLM outputs for downstream weather sequence modeling, we introduceParameter Adapter, a simple FFN with a single linear layer positioned after the PLM. This adapter transforms the PLM\u2019s output to match the prediction horizon, formalized as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{\\mathcal{X}}=\\mathrm{FFN}(\\mathcal{M}_{\\theta}(\\mathrm{Concat}[\\hat{P}_{\\mathrm{Trend}},\\hat{P}_{\\mathrm{Seasonal}},\\hat{P}_{\\mathrm{Residual}},\\hat{\\mathcal{X}}])),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the $\\hat{P}_{\\mathrm{Trend}}$ , $\\hat{P}_{\\mathrm{Seasonal}}$ , $\\hat{P}_{\\mathrm{Residual}}$ , and $\\hat{\\mathcal X}$ are obtained from CIP based on $P_{\\mathrm{Trend}}$ , $P_{\\mathrm{{Seasonal}}}$ , $P_{\\mathrm{Residual}}$ , and $\\mathcal{X}$ . The key objectives are twofold: (1) to enrich the PLM\u2019s cross-modal representations by incorporating task-specific knowledge, and (2) to enhance the PLM\u2019s output accuracy while preserving its inherent knowledge through the integration of weather data for cross-domain knowledge transfer. ", "page_idx": 4}, {"type": "text", "text": "3.2 High-efficiency Communication Between Clients and Server ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To avoid data silos and counteract the performance disparities caused by data heterogeneity while ensuring efficient communication, we update personalized adapters locally and share low-rank parameters globally in each round. Specifically, the local PLM $\\mathcal{M}_{\\theta}$ can be formulated as below: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{M}_{\\theta}\\rightarrow\\mathcal{M}_{\\theta,t}(C o m m u n i c a t i o n)+\\mathcal{M}_{\\theta,f}(L o c a l l y)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathcal{M}_{\\theta,t}$ denotes the trainable parameter from the low-rank matrices of query and value in attention modules, $\\mathcal{M}_{\\theta,f}$ is the frozen parameter (mainly the PLM backbone) and other trainable ones (primarily for the personalized adapter). During client-server communication, only $\\mathcal{M}_{\\theta,t}$ is transmitted and averaged using FedAvg [7]. At the start of the next training round, the updated $\\mathcal{M}_{\\theta,t}$ is broadcast to clients for further updates. Privacy is further protected by sending minimal parameters. ", "page_idx": 4}, {"type": "text", "text": "4 Main Theorems ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Theorem 4.1 (Decomposition Rationality from Time Series). Given a weather series $\\mathcal{X}\\,=$ $\\mathcal{X}_{T r e n d,t}+\\mathcal{X}_{S e a s o n a l,t}+\\mathcal{X}_{R e s i d u a l,t},\\;t\\in[t_{1},t_{n}]$ . Let $\\pmb{E}=\\{e_{1},e_{2},...,e_{n}\\}$ denotes a set of orthogonal bases. Lets $E_{S e a s o n a l}\\subseteq E$ denote the subset of $\\pmb{E}$ on which $\\mathcal{X}_{S e a s o n a l,t}$ has non-zero eigenvalues and $E_{T r e n d}\\subseteq E$ denote the subset of $\\boldsymbol{E}$ on which $\\chi_{T r e n d,t}$ has non-zero eigenvalues. If $\\chi_{T r e n d,t}$ and XSeasonal,t are not orthogonal, i.e., $\\begin{array}{r}{\\sum_{i=1}^{n}\\lambda_{T r e n d,t}^{i}\\lambda_{S e a s o n a l,t}^{i}\\neq0,}\\end{array}$ tX Sieasonal,t \u0338= 0, then ETrend ESeasonal \u0338= 0, i.e., E can not disentangle the two signals  onto two disjoint set of bases. ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.2 (Exchange Low-Rank Matrices Ensures Privacy). Given a on-device weather modeling framework based on federated learning that gloabl optimization object is $F(\\theta)\\;=$ $\\begin{array}{r}{\\sum_{n}^{i=1}p_{i}f(\\{D_{i}\\};\\theta)}\\end{array}$ , where $f(x;{\\boldsymbol{\\theta}})$ is the loss function of i-th client, $\\{D_{i}\\}$ is dataset of $\\romannumeral1$ -th client, and $p_{i}$ and $\\theta$ denote the data distribution weight of client i and the model parameters, respectively. Given that the parameters $\\theta$ of the PLM $\\mathcal{M}_{\\theta}$ broadcasted by the server consist of two parts: a frozen part $\\mathcal{M}_{\\theta,f}$ and a trainable part $\\mathcal{M}_{\\theta,t}$ , interacting only the low-rank matrix parameters $\\mathcal{M}_{\\theta,l}\\subset\\mathcal{M}_{\\theta,t}$ is a subset of trainable part $\\mathcal{M}_{\\theta,t}$ during each round ensures privacy. ", "page_idx": 5}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we first present the real-world datasets that we have collected and compiled for ondevice meteorological variable modeling, and second, we evaluate LM-WEATHER on these datasets, which involves normal scenario, a data-limited few-shot scenario, and a zero-shot scenario with no training data (OOD). Please refer to Appendix for more detailed information about proposed datasets and additional results of all evaluations (e.g., full results, additional findings & experiments). ", "page_idx": 5}, {"type": "text", "text": "5.1 Datasets ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Despite the proliferation of reanalysis data aimed at building frameworks for global climate analysis, these datasets often struggle to model regional weather trend due to: (1) they depend on numerous simulations of atmospheric equations, introducing biases inconsistent with real observations, and (2) they face challenges in refining their scale to suit specific regional applications. Hence, we collected real observational data from various weather stations across different regions. We then organized this data into two series, each comprising two distinct datasets, to underscore the heterogeneity inherent in real-world settings. For detailed information on these datasets, please see the Appendix B.1. ", "page_idx": 5}, {"type": "text", "text": "On-device Weather Series 1# (ODW1). The dataset gathered from 15 ground weather stations across China, Japan, and South Korea, encompasses over 20 variables. It has been divided into two subsets: ODW1T has a heterogeneous time span, meaning the data collection start and end times vary by location. and ODW1V extends ODW1T by adding variability in the observed variables; while one variable remains constant at each station, the others vary. ", "page_idx": 5}, {"type": "text", "text": "On-device Weather Series 2# (ODW2). This dataset consists of data from 36 weather observation stations in the United States, Canada, and Israel, covering 5 different variables with a temporal resolution of 1 hour. Following the dataset setting of ODW1, the dataset was also subdivided into two different dataset, including ODW2T and ODW2V. ", "page_idx": 5}, {"type": "text", "text": "5.2 Setup", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Baseline. Since our framework is based on a language model, we compare with DL-based SOTA time series models, including Transformer-based methods: Transformer [20], Informer [3], Reformer [21], Pyraformer [22], iTransformer [23], and PatchTST [6], and recent competitive models: GPT4TS [17], DLinear [24] and LightTS [25]. Note that our setting is FL-based, so we place them in FL and rename them FL-(baseline) like FL-Transformer, etc., and all aggregation methods used in above models is FedAvg [7]. In addition, we report a variants of LM-WEATHER, LM-WEATHER-AVE that based on FedAvg without personalization. Detailed information are in Appendix B.2. ", "page_idx": 5}, {"type": "text", "text": "Basic Setup. We focus on on-device meteorological variable forecasting and imputation tasks. For forecasting, we create scenarios for predicting a single variable (multivariate-univariate) and for predicting all variables (multivariate-multivariate). The main text only includes multivariate-tomultivariate forecasting results due to page constraints. For multivariate-to-univariate forecasts, refer to the Appendix E. In imputation, we use sequence lengths of $\\{96,192,336,720\\}$ and apply three different masking probabilities $\\{25\\%,35\\%,50\\%\\}$ to represent missing data. The main manuscript shows imputation results for a $50\\%$ masking ratio. For more details on the setup, please refer to Appendix B.3. All our experiments are repeat five times and we report the averaged results. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "5.3 Main Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we evaluate LM-WEATHER and baseline methods on four on-device meteorological variable modeling datasets in general experiments to validate its effectiveness. ", "page_idx": 6}, {"type": "text", "text": "Setups & Results of Forecasting Tasks. Input length $L_{b}$ is fixed to 192, and we use four different prediction horizons $L_{f}~\\in~\\{96,192,336,720\\}$ . Evaluation metrics include mean absolute error (MAE) and root square mean error (RMSE). The brief results is shown in Tab. 1, where our LMWEATHER outperforms all baselines in most cases and significantly so to the majority of them. Particularly noteworthy is the comparison with GPT4TS that involves fine-tuning PLMs, where LM-WEATHER has an average ${\\bf9.8\\%}$ improvement over FL-GPT4TS (MAE reported), and even the variant LM-WEATHER-AVE has an average ${\\bf4\\%}$ improvement over FL-GPT4TS. In addition, LM-WEATHER shows significant average performance gains of $11.2\\%$ and $19\\%$ w.r.t. MAE relative to other SOTA such as FL-DLinear and FL-PatchTST. ", "page_idx": 6}, {"type": "text", "text": "Table 1: Results under on-device meteorological variable forecasting task (multivariate-tomultivariate). A lower value indicates better performance. Bold: the best, Underline: the second best. Complete results can be found at Appendix E due to page limitation. ", "page_idx": 6}, {"type": "table", "img_path": "llTroju97T/tmp/c5738a1f33cb11a8558ef2dc0896e20bdec0e212ffe21496631dad9be638c69a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Setups & Results of Imputation Tasks. Our brief results are in Tab. 2, where LM-WEATHER consistently surpasses all baselines, outperforming FL-GPT4TS by ${\\pmb5.7\\%}$ . LM-WEATHER remains competitive even when compared with the SOTA, FL-PatchTST, FL-LightTS, and FL-DLinear. ", "page_idx": 6}, {"type": "text", "text": "Table 2: Results under on-device meteorological variable imputation task, where random masking ratio is $50\\%$ . A lower value indicates better performance. Bold: the best, Underline: the second best. Complete results can be found at Appendix E due to page limitation. ", "page_idx": 6}, {"type": "table", "img_path": "llTroju97T/tmp/25148ad63cb54cc2102873d86889980ea255bee646dd0359529ba52ebf325784.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "5.4 Few-Shot Learning Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "PLMs have demonstrated remarkable few-shot learning capabilities [26]. In this subsection, we assess whether LM-WEATHER retains this ability in both forecasting and imputation tasks, based on FL for resource-constrained on-device weather modeling environments. ", "page_idx": 6}, {"type": "text", "text": "Setups and Results of Forecasting & Imputation. For both forecasting and imputation tasks, we evaluate the few-shot learning capability in scenarios using limited data, specifically, we use training ratios of $5\\%$ and $15\\%$ (Our full few-shot learning results (training ratio of $5\\%$ and $15\\%$ ) can be found at Appendix E.2). The brief $5\\%$ few-shot learning results on forecasting and imputation tasks are depicted in Tab. 3 and Tab. 4, respectively. LM-WEATHER remarkably excels over all baseline methods, and we attribute this to the successful cross-domain knowledge activation in our local dual fine-tuning for the PLM. In addition, our LM-WEATHER\u2019s communication mechanism also reduces the impact of data heterogeneity on performance, which is reflected in the fact that LM-WEATHER has an average $14.7\\%$ and $2\\mathbf{0}\\%$ improvement relative to LM-WEATHER-AVE, in the forecasting and imputation, respectively. In relation to recent SOTA methods such as FL-PatchTST, FL-LightTS, and FL-DLinear, our LM-WEATHER enhancements surpass $7\\mathbf{8\\%}$ , ${\\bf14.3\\%}$ , and $72.8\\%$ for forecasting, and $102.1\\%$ , $122.1\\%$ , and $96.35\\%$ for imputation. This means that heterogeneity poses challenge to baseline and they struggle to understand weather patterns with limited data. Moreover, it implies that LM-WEATHER can effectively achieve cross-domain knowledge transfer to PLMs. This benftis from the personalized adapter we integrated into the PLM, coupled with lightweight operations. ", "page_idx": 7}, {"type": "table", "img_path": "llTroju97T/tmp/94662c6b1fd8f2f35a993126064d410b3faa68ecd5549f31e7356cd626791804.jpg", "table_caption": ["Table 3: Few-Shot learning results on forecasting task ( ${5\\%}$ training data). A lower value indicates better performance. Bold: the best, Underline: the second best, \u2018-\u2018 denotes insufficient data. Complete results can be found at Appendix E.2. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 4: Few-Shot learning results on imputation task ( ${5\\%}$ training data), where random masking ratio is $50\\%$ . A lower value indicates better performance. Bold: the best, Underline: the second best, \u2018-\u2018 denotes insufficient data. Appendix E.2 shows our full results. ", "page_idx": 7}, {"type": "table", "img_path": "llTroju97T/tmp/324eb420caf70e0c3a66c6b1eada3d10f0cb8ee7bbf360cc72125d69ecece812.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "5.5 Zero-Shot Learning (Out of Distribution Modeling) Experiments ", "text_level": 1, "page_idx": 7}, {"type": "table", "img_path": "llTroju97T/tmp/6eb8ee705fe9cfbf4072eee514488cfdb398daee0e04541b21b402726cf77626.jpg", "table_caption": ["Table 5: Results on Zero-Shot Learning (ave. MAE on forecasting/imputation tasks report). Bold: the best, Underline: the second best, $\\Leftrightarrow$ : domain transferring between datasets. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Beyond few-shot learning, PLMs hold potential as effective zero-shot reasoners. We evaluate the zero-shot learning capabilities of LM-WEATHER within the framework of cross-domain adaption. Specifically, we examine how well a method performs on a dataset when it is optimized on another dataset, where the model has not encountered any data samples from the original dataset. We use forecasting/imputation protocol and evaluate on various cross-domain scenarios. Note that we choose LM-WEATHERAVE rather than LM-WEATHER for ", "page_idx": 7}, {"type": "text", "text": "comparison due to it can obtain an unified model for zero-shot experiments whereas LM-WEATHER is obtain multiple personalized models. The results are in Tab. 5. LM-WEATHER-AVE consistently outperforms the most competitive baselines by a large margin, over ${\\bf14.2\\,\\%}$ and ${\\bf14.2\\%}$ w.r.t the second-best in MAE reduction, in forecasting and imputation, respectively. We attribute this to our personalized adapter that we implant in PLMs being better at activating the PLM\u2019s knowledge transfer and domain-adaption capabilities in a resource-efficient manner when modeling weather variables. ", "page_idx": 8}, {"type": "text", "text": "5.6 Framework Analysis Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We demonstrate the effectiveness of LM-WEATHER through experiments focused on ablation studies, computational/communication comparison, and robustness evaluation. For detailed results and further analysis, please refer to the Appendix D and Appendix E. ", "page_idx": 8}, {"type": "text", "text": "Ablation Study. Follow the setting of main experiments, we report our brief ablation results in Tab. 6, please refer to Appendix E.3 for full results. The results indicate a notable drop in performance when we omit the weather decomposition components (LM-WEATHER-A/B/C/D). Additionally, keeping the decomposition term but removing the associated generator leads to a $14.5\\%$ average performance decline. This suggests that our personalized adapter effectively leverages the PLM\u2019s modeling of weather data. Conversely, when we alter the personalized approach by changing the shared low-rank matrix to other trainable parameters (LM-WEATHER-F), we observe a significant performance drop and increased communication costs. Furthermore, moving from LoRA to fully fine-tuning the attention parameters results in a slight performance gain but incurs over four times the parameter count and a massive increase in communication overhead, which is inefficient for us. These outcomes highlight the benefits of the personalized adapter. ", "page_idx": 8}, {"type": "text", "text": "Table 6: Ablation results on forecasting (multivariate to multivariate) and imputation $50\\%$ masking ratio, OWD1T dataset). A lower value indicates better performance. Bold: the best, Underline: the second best, $\\downarrow$ and $\\uparrow$ denote performance degradation and performance improvement, respectively. ", "page_idx": 8}, {"type": "table", "img_path": "llTroju97T/tmp/206a457854a3af01935a69c4a2c395be14433d01e66ade8b42eb248b6ac993dd.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Parameter Comparison. The results are shown in Tab. 7. LM-WEATHER ensures top while only communicate about $3.7\\%$ of the trainable parameters, compared to the baseline that communicates the full model parameters. When compared with competitive methods, FL-DLinear and FL-LightTS, LMWEATHER\u2019s communication overhead is just $35.9\\,\\%$ and $22.6\\%$ of theirs, respectively, highlighting LM-WEATHER\u2019s superior communication efficiency. ", "page_idx": 8}, {"type": "table", "img_path": "llTroju97T/tmp/2c46cb44af7cf4d23be6fc411e0d5766c68cfd086826cd4003e12e5bebf21613.jpg", "table_caption": ["Table 7: Experiment results on parameter comparison (ave. MAE/RMSE report), Bold: the best. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Communication Efficiency. To further validate the excellent communication efficiency of LMWEATHER, we introduce quantitative comparisons by including FL methods tailored to improve communication efficiency (FedKD [27], FedPAQ [28], FedBF [29], FedAP [29], PromptFL [30]) as baselines2. The results is shown in Table 8, which demonstrate our LM-WEATHER achieves a significant improvement in communication efficiency while maintaining excellent performance. Additionally, LM-WEATHER significantly outperforms baseline in terms of both communication efficiency and performance across different tasks. Even when compared to lightweight baselines (i.e., FL-LightTS/DLinear), LM-WEATHER continues to outperform them. This underscores LMWEATHER\u2019s superiority in both communication efficiency and performance. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Table 8: Comparison of LM-WEATHER and baseline that tailored to improve communication efficiency in terms of forecasting (multivariate-multivariate)/imputation ( $50\\%$ masking rate) performance as well as communication efficiency, with $\\times$ denotes the improvement in communication efficiency relative to the standard line (LM-WEATHER-Ave), MAE/RMSE report. Bold: the best. ", "page_idx": 9}, {"type": "table", "img_path": "llTroju97T/tmp/1f05581bf6f2616a1a9f4d780d311ced04a2429da692a34ebc37a3a98962defc.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Robustness to Number of Devices. To evaluate LM-WEATHER\u2019s robustness against device count variations, we assessed the percentage change in performance relative to the default device number. Our results (Tab. 9) reveals that LM-WEATHER maintains robustness across different device counts due to several factors: (1) Increasing device numbers during training typically yields slight performance improvements within a stable range, applicable in both regular and few-shot scenarios. (2) Additional devices can sometimes impair performance due to imbalances in data distribution, highlighting non-proportional gains. (3) Adding more devices increases communication overhead, which may not justify minor improvements, especially in resource-limited settings. These findings underscore LM-WEATHER\u2019s relative resilience to device count variations and its ability to strike an optimal balance between performance enhancement and communication overhead. ", "page_idx": 9}, {"type": "table", "img_path": "llTroju97T/tmp/84f525ca7be91fd0c56c163fba5a8c742c880faa7e5c664c4a09cd9a357ea2e1.jpg", "table_caption": ["Table 9: Results of LM-WEATHER under forecasting (multivariate-multivariate) and imputation $50\\%$ masking rate) at different device participation rates [0.1, 0.3, 0.5, 0.7, 0.9], $\\uparrow/\\downarrow$ implies an increase/decrease in performance relative to the original setting (0.1), MAE/RMSE report, where $15\\%$ represents the proportion of data on each client involved in training. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "6 Conclusion and Future Works ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper demonstrate that pre-trained language models (PLMs) are strong foundation models for personalized on-device meteorological variable modeling. We propose LM-WEATHER, a generic framework to taming PLMs to acquire highly customized models for heterogeneous meteorological data on devices while keeping high efficiency. Concretely, we introduce a lightweight personalize adapter into PLMs and endow it with weather pattern awareness. Experiments on real-world datasets demonstrate that LM-WEATHER outperforms the SOTA results by a large margin across various tasks. In addition, extensive analyses indicate that LM-WEATHER can (1) effectively achieve crossdomain knowledge transfers, (2) render device with highly customized model while keeping high efficiency, and (3) generalize under few-shot and zero-shot scenario. In future work, we plan to extend LM-WEATHER to multimodal weather data (text/image/time-series) and to finer scales. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Tung Nguyen, Johannes Brandstetter, Ashish Kapoor, Jayesh K Gupta, and Aditya Grover. Climax: A foundation model for weather and climate. arXiv preprint arXiv:2301.10343, 2023.   \n[2] Ryan Keisler. Forecasting global weather with graph neural networks. arXiv preprint arXiv:2202.07575, 2022.   \n[3] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In Proceedings of the AAAI conference on artificial intelligence, volume 35, pages 11106\u201311115, 2021.   \n[4] Kaifeng Bi, Lingxi Xie, Hengheng Zhang, Xin Chen, Xiaotao Gu, and Qi Tian. Accurate medium-range global weather forecasting with 3d neural networks. Nature, 619(7970):533\u2013538, 2023.   \n[5] Shengchao Chen, Guodong Long, Jing Jiang, Dikai Liu, and Chengqi Zhang. Foundation models for weather and climate data understanding: A comprehensive survey. arXiv preprint arXiv:2312.03014, 2023.   \n[6] Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. A time series is worth 64 words: Long-term forecasting with transformers. arXiv preprint arXiv:2211.14730, 2022.   \n[7] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In Artificial intelligence and statistics, pages 1273\u20131282. PMLR, 2017.   \n[8] Xiaoxiao Li, Meirui Jiang, Xiaofei Zhang, Michael Kamp, and Qi Dou. Fedbn: Federated learning on non-iid features via local batch normalization. arXiv preprint arXiv:2102.07623, 2021.   \n[9] Alysa Ziying Tan, Han Yu, Lizhen Cui, and Qiang Yang. Towards personalized federated learning. IEEE Transactions on Neural Networks and Learning Systems, 2022.   \n[10] Defu Cao, Furong Jia, Sercan O Arik, Tomas Pfister, Yixiang Zheng, Wen Ye, and Yan Liu. Tempo: Promptbased generative pre-trained transformer for time series forecasting. arXiv preprint arXiv:2310.04948, 2023.   \n[11] Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y Zhang, Xiaoming Shi, Pin-Yu Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, et al. Time-llm: Time series forecasting by reprogramming large language models. arXiv preprint arXiv:2310.01728, 2023.   \n[12] Hans Hersbach, Bill Bell, Paul Berrisford, Shoji Hirahara, Andr\u00e1s Hor\u00e1nyi, Joaqu\u00edn Mu\u00f1oz-Sabater, Julien Nicolas, Carole Peubey, Raluca Radu, Dinand Schepers, et al. The era5 global reanalysis. Quarterly Journal of the Royal Meteorological Society, 146(730):1999\u20132049, 2020.   \n[13] Stephan Rasp, Peter D Dueben, Sebastian Scher, Jonathan A Weyn, Soukayna Mouatadid, and Nils Thuerey. Weatherbench: a benchmark data set for data-driven weather forecasting. Journal of Advances in Modeling Earth Systems, 12(11):e2020MS002203, 2020.   \n[14] Peter Bauer, Alan Thorpe, and Gilbert Brunet. The quiet revolution of numerical weather prediction. Nature, 525(7567):47\u201355, 2015.   \n[15] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.   \n[16] Hao Xue and Flora D Salim. Promptcast: A new prompt-based learning paradigm for time series forecasting. IEEE Transactions on Knowledge and Data Engineering, 2023.   \n[17] Tian Zhou, Peisong Niu, Xue Wang, Liang Sun, and Rong Jin. One ftis all: Universal time series analysis by pretrained lm and specially designed adaptors. arXiv preprint arXiv:2311.14782, 2023.   \n[18] Ching Chang, Wen-Chih Peng, and Tien-Fu Chen. Llm4ts: Two-stage fine-tuning for time-series forecasting with pre-trained llms. arXiv preprint arXiv:2308.08469, 2023.   \n[19] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.   \n[20] Qingsong Wen, Tian Zhou, Chaoli Zhang, Weiqi Chen, Ziqing Ma, Junchi Yan, and Liang Sun. Transformers in time series: A survey. arXiv preprint arXiv:2202.07125, 2022.   \n[21] Nikita Kitaev, \u0141ukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. arXiv preprint arXiv:2001.04451, 2020.   \n[22] Shizhan Liu, Hang Yu, Cong Liao, Jianguo Li, Weiyao Lin, Alex X Liu, and Schahram Dustdar. Pyraformer: Low-complexity pyramidal attention for long-range time series modeling and forecasting. In International conference on learning representations, 2021.   \n[23] Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, and Mingsheng Long. itransformer: Inverted transformers are effective for time series forecasting. arXiv preprint arXiv:2310.06625, 2023.   \n[24] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers effective for time series forecasting? In Proceedings of the AAAI conference on artificial intelligence, volume 37, pages 11121\u201311128, 2023.   \n[25] David Campos, Miao Zhang, Bin Yang, Tung Kieu, Chenjuan Guo, and Christian S Jensen. Lightts: Lightweight time series classification with adaptive ensemble distillation. Proceedings of the ACM on Management of Data, 1(2):1\u201327, 2023.   \n[26] Xin Liu, Daniel McDuff, Geza Kovacs, Isaac Galatzer-Levy, Jacob Sunshine, Jiening Zhan, Ming-Zher Poh, Shun Liao, Paolo Di Achille, and Shwetak Patel. Large language models are few-shot health learners. arXiv preprint arXiv:2305.15525, 2023.   \n[27] Chuhan Wu, Fangzhao Wu, Lingjuan Lyu, Yongfeng Huang, and Xing Xie. Communication-efficient federated learning via knowledge distillation. Nature communications, 13(1):2032, 2022.   \n[28] Amirhossein Reisizadeh, Aryan Mokhtari, Hamed Hassani, Ali Jadbabaie, and Ramtin Pedarsani. Fedpaq: A communication-efficient federated learning method with periodic averaging and quantization. In International conference on artificial intelligence and statistics, pages 2021\u20132031. PMLR, 2020.   \n[29] Zhuo Zhang, Yuanhang Yang, Yong Dai, Qifan Wang, Yue Yu, Lizhen Qu, and Zenglin Xu. Fedpetuning: When federated learning meets the parameter-efficient tuning methods of pre-trained language models. In Annual Meeting of the Association of Computational Linguistics 2023, pages 9963\u20139977. Association for Computational Linguistics (ACL), 2023.   \n[30] Tao Guo, Song Guo, Junxiao Wang, Xueyang Tang, and Wenchao Xu. Promptf:l Let federated participants cooperatively learn prompts instead of models-federated learning in age of foundation model. IEEE Transactions on Mobile Computing, 2023.   \n[31] Shengchao Chen, Ting Shu, Huan Zhao, Guo Zhong, and Xunlai Chen. Tempee: Temporal-spatial parallel transformer for radar echo extrapolation beyond auto-regression. IEEE Transactions on Geoscience and Remote Sensing, 2023.   \n[32] Shengchao Chen, Ting Shu, Huan Zhao, Qilin Wan, Jincan Huang, and Cailing Li. Dynamic multiscale fusion generative adversarial network for radar image extrapolation. IEEE Transactions on Geoscience and Remote Sensing, 60:1\u201311, 2022.   \n[33] Shengchao Chen, Guodong Long, Tao Shen, Tianyi Zhou, and Jing Jiang. Spatial-temporal prompt learning for federated weather forecasting. arXiv preprint arXiv:2305.14244, 2023.   \n[34] Shengchao Chen, Guodong Long, Tao Shen, and Jing Jiang. Prompt federated learning for weather forecasting: Toward foundation models on meteorological data. arXiv preprint arXiv:2301.09152, 2023.   \n[35] Xin Man, Chenghong Zhang, Jin Feng, Changyu Li, and Jie Shao. W-mae: Pre-trained weather model with masked autoencoder for multi-variable weather forecasting. arXiv preprint arXiv:2304.08754, 2023.   \n[36] Remi Lam, Alvaro Sanchez-Gonzalez, Matthew Willson, Peter Wirnsberger, Meire Fortunato, Ferran Alet, Suman Ravuri, Timo Ewalds, Zach Eaton-Rosen, Weihua Hu, et al. Graphcast: Learning skillful medium-range global weather forecasting. arXiv preprint arXiv:2212.12794, 2022.   \n[37] Sojung An, Junha Lee, Jiyeon Jang, Inchae Na, Wooyeon Park, and Sujeong You. Self-supervised pre-training for precipitation post-processor. arXiv preprint arXiv:2310.20187, 2023.   \n[38] Filip Hanzely, Slavom\u00edr Hanzely, Samuel Horv\u00e1th, and Peter Richt\u00e1rik. Lower bounds and optimal algorithms for personalized federated learning. Advances in Neural Information Processing Systems, 33:2304\u20132315, 2020.   \n[39] Tian Li, Shengyuan Hu, Ahmad Beirami, and Virginia Smith. Ditto: Fair and robust federated learning through personalization. In International Conference on Machine Learning, pages 6357\u20136368. PMLR, 2021.   \n[40] Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated optimization in heterogeneous networks. Proceedings of Machine learning and systems, 2:429\u2013450, 2020.   \n[41] Liam Collins, Hamed Hassani, Aryan Mokhtari, and Sanjay Shakkottai. Exploiting shared representations for personalized federated learning. In International conference on machine learning, pages 2089\u20132099. PMLR, 2021.   \n[42] Michael Zhang, Karan Sapra, Sanja Fidler, Serena Yeung, and Jose M Alvarez. Personalized federated learning with first order model optimization. arXiv preprint arXiv:2012.08565, 2020.   \n[43] Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. Personalized federated learning with theoretical guarantees: A model-agnostic meta-learning approach. Advances in Neural Information Processing Systems, 33:3557\u20133568, 2020.   \n[44] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \n[45] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting. Advances in Neural Information Processing Systems, 34:22419\u201322430, 2021.   \n[46] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. Fedformer: Frequency enhanced decomposed transformer for long-term series forecasting. In International Conference on Machine Learning, pages 27268\u201327286. PMLR, 2022.   \n[47] Gerald Woo, Chenghao Liu, Doyen Sahoo, Akshat Kumar, and Steven Hoi. Etsformer: Exponential smoothing transformers for time-series forecasting. arXiv preprint arXiv:2202.01381, 2022.   \n[48] Tianping Zhang, Yizhuo Zhang, Wei Cao, Jiang Bian, Xiaohan Yi, Shun Zheng, and Jian Li. Less is more: Fast multivariate time series forecasting with light sampling-oriented mlp structures, 2022.   \n[49] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.   \n[50] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.   \n[51] Taesung Kim, Jinhee Kim, Yunwon Tae, Cheonbok Park, Jang-Ho Choi, and Jaegul Choo. Reversible instance normalization for accurate time-series forecasting against distribution shift. In International Conference on Learning Representations, 2021.   \n[52] Yue Tan, Guodong Long, Lu Liu, Tianyi Zhou, Qinghua Lu, Jing Jiang, and Chengqi Zhang. Fedproto: Federated prototype learning across heterogeneous clients. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 8432\u20138440, 2022.   \n[53] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.   \n[54] Ji\u02c7r\u00ed Matou\u0161ek. On variants of the johnson\u2013lindenstrauss lemma. Random Structures & Algorithms, 33(2):142\u2013156, 2008.   \n[55] Jun Luo and Shandong Wu. Adapt to adaptation: Learning personalization for cross-silo federated learning. In IJCAI: proceedings of the conference, volume 2022, page 2166. NIH Public Access, 2022.   \n[56] Manoj Ghuhan Arivazhagan, Vinay Aggarwal, Aaditya Kumar Singh, and Sunav Choudhary. Federated learning with personalization layers. arXiv preprint arXiv:1912.00818, 2019.   \n[57] Jianqing Zhang, Yang Hua, Hao Wang, Tao Song, Zhengui Xue, Ruhui Ma, and Haibing Guan. Fedala: Adaptive local aggregation for personalized federated learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 11237\u201311244, 2023.   \n[58] Yang Cao, Haolong Xiang, Hang Zhang, Ye Zhu, and Kai Ming Ting. Anomaly detection based on isolation mechanisms: A survey. arXiv preprint arXiv:2403.10802, 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "APPENDIX: Personalized Adapter for Large Meteorology Model on Devices: Towards Weather Foundation Models ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The appendix includes missing information from our main text, including: Appendix A More Related Work; Appendix B Experimental Details; Appendix C Theorems and Proof; Appendix D Additional Finding & Experiment & Discussion; Appendix E Full Experimental Results and Appendix F Additional Statements. ", "page_idx": 13}, {"type": "text", "text": "Appendix A More Related Work ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we will discuss in detail advances relevant to our work, which include weather variable modeling, personalized federated learning, universal time series learning, and large language models (LLMs) in time series. ", "page_idx": 13}, {"type": "text", "text": "From Meteorological Variable Modeling to Weather Forecasting. Weather conditions play a crucial role in sectors such as transportation, tourism, and agriculture. Meteorological factors, including temperature, humidity, and precipitation, provide essential support and historical insights that enable researchers to analyze weather trends. For decades, Numerical Weather Prediction (NWP) [14] has been the prevalent method, employing physical models to simulate and forecast atmospheric dynamics. However, the accuracy of NWP can be compromised by the uncertainty of initial conditions in differential equations [31, 32], particularly in complex atmospheric processes, and it requires significant computational resources [5, 33, 34]. ", "page_idx": 13}, {"type": "text", "text": "The recent exponential growth in weather data has prompted a shift from traditional physics-based methods to data-driven approaches using machine learning (ML) and deep learning (DL), which bypass physical constraints in meteorological variables [5]. DL strategies, with their deeper representational capabilities, generally surpass ML methods. Various deep network architectures have been employed to perform extensive weather modeling using large-scale reanalysis data [1, 4, 35, 36, 37]. Yet, these methods tend to focus on global weather patterns, often overlooking the specifics of regional weather variables, and thus fail to offer detailed regional analyses. Moreover, these models require extensive datasets and substantial computational resources\u2014for example, some need to train on 192 NVIDIA Tesla V100 GPUs for 16 days [4]. Additionally, prevailing models assume centralized data storage, which contrasts with the decentralized data collection from diverse ground weather stations. Our research addresses these challenges by focusing on regional meteorological variables in lowresource settings, aiming to provide reliable analytical support for weather pattern modeling and understanding. ", "page_idx": 13}, {"type": "text", "text": "Personalized Federated Learning. Federated learning (FL) [7] is a distributed learning paradigm that facilitates the collaborative training of models without exposing data from each participant. Personalized FL (PFL) aims to train a personalized model for each client. Existing PFLs are based on various techniques. Refs. [38, 39, 40] add a regularization term that beneftis decomposing the personalized model optimization from global model learning. Refs. [8, 41] share part of the model and keep personalized layers private to achieve personalization. Ref. [42] enables a more flexible personalization by adaptive weighted aggregation. Ref. [43] study PFL from a Model-Agnostic Meta-Learning where a meta-model is learned to generate the initialized local model for each client. This paper tackles on-device meteorological variable modeling from PFL perspective. ", "page_idx": 13}, {"type": "text", "text": "Universal Time Series Learning. On-device meteorological variable modeling addresses time series analysis of complex weather patterns on diverse, low-resource devices. We have expanded this to include taskspecific time series learning. Recent advancements have enhanced Transformer [44] for time series forecasting by integrating signal processing techniques such as patching [6], exponential smoothing [45], decomposition [24], and frequency analysis [46]. Among them, PatchTST [6] improves the accuracy of long-term forecasting compared to other Transformer models. ETSFormer [47] applies principles of power series smoothing within the Transformer framework to boost efficiency. Similarly, FEDformer [46] merges the Transformer with seasonal & trend decomposition, offering improved performance and efficiency. Autoformer [45] leverages sequence periodicity for better dependency discovery and representation, excelling in both efficiency and accuracy. ", "page_idx": 13}, {"type": "text", "text": "While these methods excel in efficiency and accuracy, they are typically tailored for narrow-range forecasting on select classical time series datasets. Real-world weather data, however, often displays more complex patterns and interconnected variable relationships. Furthermore, weather modeling extends beyond forecasting, rendering these methods less effective for weather sequences. To improve modeling for intricate weather sequences, models need the flexibility to adjust to complex distributions and various tasks with minimal training. The ideal weather models would capture weather patterns accurately, facilitating knowledge transfer, such as between regions. However, creating versatile weather models remains a challenging endeavor. Recent studies have started to examine the potential of large-scale climate models [1, 4], utilizing simulated datasets advances. Yet, their generalizability is hindered by data differences, complex architectures, and the vast number of model parameters. ", "page_idx": 13}, {"type": "text", "text": "LLMs in Time Series. Large language models (LLMs) have spurred advances in natural language processing (NLP). Although time series modeling hasn\u2019t seen similar leaps, the impressive capabilities of LLMs have led to their use in this field. In general, pre-trained LLMs are often fine-tuned or reprogrammed to model time series [18, 11, 10, 17]. Among them, PROMPTCAST [16] and HEALTHLEARNER [26] treat time series as \"text sequences,\" inputting them directly into LLMs and using prompts for forecasting. [17] dencodes time series as embeddings for LLM output, showing LLMs\u2019 strength in time series analysis. LLM4TS [18] uses a two-stage fine-tuning approach to adapt LLMs to time series data. TEMPO [10] breaks down time series features to leverage LLMs in prediction tasks, while TIME-LLM [11] fine-tunes LLMs with multimodal data, integrating relevant text prompts for efficient analysis. However, these approaches focus on centralized time series modeling and overlook the complexities of real-world distributed settings. Weather data, in particular, has unique challenges like heterogeneity from geographic factors and privacy concerns, making central training methods both risky and difficult. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Appendix B Experimental Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Datasets ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Despite the proliferation of reanalysis data aimed at building frameworks for global climate analysis, these datasets often struggle to model regional weather trend due to: (1) they depend on numerous simulations of atmospheric equations, introducing biases inconsistent with real observations, and (2) they face challenges in refining their scale to suit specific regional applications. Hence, we collected real observational data from various weather stations across different regions. We then organized this data into two series, each comprising two distinct datasets, to underscore the heterogeneity inherent in real-world settings. ", "page_idx": 14}, {"type": "text", "text": "On-device Weather Series 1# (ODW1). The dataset gathered from 15 ground weather stations across China, Japan, and South Korea, encompasses over 20 variables3. It has been divided into two subsets: ODW1T has a heterogeneous time span, meaning the data collection start and end times vary by location. and ODW1V extends ODW1T by adding variability in the observed variables; while one variable remains constant at each station, the others vary. The temporal resolution of the dataset is 1h. Details are presented in Tab. 10 and Tab. 11. ", "page_idx": 14}, {"type": "text", "text": "Table 10: Details about ODW1T dataset, where Start and End indicate the respective beginning and ending timestamps of data collected at a specific weather station, Samples denotes the count of weather sequence samples gathered at that station, and Variables refers to the weather variables included in the data from each station (For the full names of these variables, please refer to Tab. 14). ", "page_idx": 14}, {"type": "table", "img_path": "llTroju97T/tmp/49c2acf6071e45e90b393cc775a3518e7146153ecd9ab44568bc73d55c423dfa.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "On-device Weather Series 2# (ODW2). This dataset consists of data from 36 weather observation stations in the United States, Canada, and Israel, covering 5 different variables with a temporal resolution of 1 hour. Following the dataset setting of ODW1, the dataset was also subdivided into two different dataset, including ODW2T and ODW2V. Detailed information are presented in Tab. 12 and Tab. 13. ", "page_idx": 14}, {"type": "text", "text": "Remark. Four standard steps were performed during the collection and compilation of these dataset, as shown below: ", "page_idx": 14}, {"type": "text", "text": "[1] Collection of Raw Meteorological Data. Raw data collection represents the foundational and initial step in constructing our dataset. We procure open-source raw data from various national meteorological centers and data repositories, including the National Meteorological Science Data Center of China4, Korea Meteorological Administration5, Global Surface Meteorological Observations ", "page_idx": 14}, {"type": "text", "text": "Table 11: Details about ODW1V dataset, where Start and End indicate the respective beginning and ending timestamps of data collected at a specific weather station, Num. of Samples # denotes the count of weather sequence samples gathered at that station, and Fixed Variables refers to the shared variables among different weather stations, and Other Variables is the remain weather variables in each weather station (For the full names of these variables, please refer to Tab. 14). ", "page_idx": 15}, {"type": "table", "img_path": "llTroju97T/tmp/2d8ed32aeb7afa50b807816b105e093f95fa0e94a8f4a10d56ce9cfb45effe79.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Table 12: Details about ODW2T dataset, where Start and End indicate the respective beginning and ending timestamps of data collected at a specific weather station, Num. of Samples # denotes the count of weather sequence samples gathered at that station, and Variables refers to the weather variables included in the data from each station (For meaning of variables, please refer to Tab. 14). ", "page_idx": 15}, {"type": "table", "img_path": "llTroju97T/tmp/d4833d1b16daa5e7a445b8df5283a9c889d96a7615bd4503359cd0c9afd45da8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Table 13: Details about ODW2V dataset, where Start and End indicate the respective beginning and ending timestamps of data collected at a specific weather station, Num. of Samples # denotes the count of weather sequence samples gathered at that station, and Fixed Variables refers to the shared variables among different weather stations, and Other Variables is the remain weather variables in each weather station (For the full names of these variables, please refer to Tab. 14). ", "page_idx": 16}, {"type": "table", "img_path": "llTroju97T/tmp/2a8f8e11d5b3e84f19e7da31173229b399a27b0dd9f98da6168811058f554b5f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "from Kaggle 8. This process ensures that the collected weather data from these sources are consistent in terms of temporal resolution and variable dimensions. All raw data are open-source and can be freely utilized or modified. ", "page_idx": 16}, {"type": "text", "text": "[2] Selection of Critical Meteorological Variables. To support personalized on-device meteorological variable modeling and enhance regional weather forecasting reliability, we selected twenty representative meteorological variables. These variables, including temperature, barometric pressure, relative humidity, and precipitation, were chosen based on their significant impact on weather conditions. Detailed definitions, physical descriptions, and units of these selected variables are provided in Table 14. ", "page_idx": 16}, {"type": "text", "text": "[3] Ensuring Completion of Meteorological Time Series. In this step, we primarily focus on ensuring the completeness of weather time series data collected from ground weather stations. Incomplete weather time series can generate unreliable predictions, potentially leading to significant unforeseen losses. Most ground weather stations are susceptible to unpredictable events such as power outages and equipment damage, which may result in data gaps. To enhance dataset completeness, we meticulously examined the raw data for missing values across various timestamps and employed a linear interpolation strategy to fill these gaps. ", "page_idx": 16}, {"type": "text", "text": "[4] Handling of Outliers. Outliers are common in weather time series data. We distinguish between factual outliers, typically caused by extreme weather events (e.g., heavy rainfall, typhoons, thunderstorms), and non-factual outliers, often due to observational device anomalies or sensor malfunctions at weather stations. We identify significant deviations in a weather variable\u2014for instance, a sudden increase from an average rainfall of $2~\\mathrm{mm}$ to $200\\;\\mathrm{mm}$ \u2014as outliers. These are manually corrected; initially, the values are set to zero and then replaced using linear interpolation, reflecting the gradual nature of weather phenomena. ", "page_idx": 16}, {"type": "table", "img_path": "llTroju97T/tmp/70496fe4af5d98aaed657f626ec8b292539ebe318d8762fcbc2b67e5ae5e5faa.jpg", "table_caption": ["Table 14: Abbreviations, full names and corresponding units of the different variables in our proposed datasets. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "Visualisation. We hope to deepen the reader\u2019s understanding of the datasets we have collected and compiled by providing standard visualizations. Considering the overall size of the datasets and the large number of meteorological variables, we have provided visualisations of representative variables here for reference. The visualisation of OWD1 is shown in Fig. 2. Due to the number of devices involved in the OWD2 dataset, we have divided it into two consecutive images for presentation, as shown in Fig 3 and Fig. 4. ", "page_idx": 17}, {"type": "text", "text": "B.2 Baselines ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We compare with state-of-the-art time series analysis models and put them into Federated Learning environments, including Transformer-based methods like Transformer [44], Informer [3], Reformer [21], Pyraformer [22], iTransformer [23], and PatchTST [6], and recent competitive models including GPT4TS [17], DLinear [24] and LightTS [48], detailed information about baselines is below: ", "page_idx": 17}, {"type": "text", "text": "\u2022 Transformer. [44] This model uses a self-attention mechanism, popular for time series prediction tasks, to efficiently and accurately learn relationships within a sequence and contextual information.   \n\u2022 Informer. [3] An optimized Transformer-based model for long-range time series prediction. It uses ProbSparse self-attention for efficiency, processes long inputs effectively, and employs a fast prediction decoder.   \n\u2022 Reformer. [21] This model improves Transformer efficiency by using locality-sensitive hashing for attention and reversible residual layers. It offers better memory efficiency and speed for lengthy sequences without sacrificing performance.   \n\u2022 Pyraformer. [22] It features hierarchical pyramidal attention modules with binary trees to capture temporal dependencies across different ranges efficiently, both in time and memory complexity.   \n\u2022 iTransformer. [23] The iTransformer adds attention and feedforward networks to the inverse dimension. It embeds time points as variable tokens, using attention to capture multivariate correlations and feedforward networks for nonlinear representation of each token.   \n\u2022 PatchTST. [6] This method divides the time series into patches at the subseries level for input to the Transformer. Each channel holds a univariate time series, sharing the same embedding and Transformer weights across all series.   \n\u2022 DLinear. [24] DLinear integrates decomposition schemes from Autoformer and FEDformer with linear layers to model time series data tables. It effectively summarizes trend and seasonal components, enhancing performance on datasets rich in trends.   \n\u2022 LightTS. [48] A lightweight structure based on a simple MLP. It utilizes two downsampling strategies\u2014spaced and sequential sampling\u2014on the MLP structure, capitalizing on the fact that downsampled time series generally maintain most of their original information.   \n\u2022 GPT4TS. [17] This model is designed for time series analysis across various scenarios, achieved by fine-tuning a pre-trained language model, specifically GPT2, for the time series domain. It\u2019s important to note that for a fair comparison, our baseline setup differs from the original publication\u2019s configuration. Instead of using the first six layers of GPT2 as the backbone, we align with our approach and utilize only the first five layers. ", "page_idx": 17}, {"type": "image", "img_path": "llTroju97T/tmp/3fe122a17342d5ccc47d4d2403256866dd7ac4976c19d08437fd9ef26592a72b.jpg", "img_caption": ["Figure 2: Visualisation of partial variables in ODW1 dataset, where we have selected the first 1,000 time points for presentation. The data distribution from different ground weather stations exhibit significant heterogeneity, and even though the trends of some variables may be similar, there are serious differences in magnitudes. The selected variables are, from left to right, temperature, precipitation in 1-hour/12-hour, humidity, and wind direction. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "In addition, pre-trained language models (PLMs) are the key component of our LM-Weather, we use different PLMs as the backbone to demonstrate the PLM can as the strong weather foundation model for on-device weather modeling. We use GPT-2 as the default setting, and BERT [49], LLaMA [50] as the alternatives. ", "page_idx": 18}, {"type": "text", "text": "\u2022 BERT. [49] BERT, short for Bidirectional Encoder Representations from Transformers, is a deep learning model that uses the Transformer architecture. It understands the context of words by analyzing text in both directions. When used as a baseline for evaluation, we only employ the first 5 layers of the pre-trained BERT. ", "page_idx": 18}, {"type": "image", "img_path": "llTroju97T/tmp/50277e4a258f6fe8aa9ed757252ed3e9579b50a78876ee2e4c02bca750ddd89b.jpg", "img_caption": ["Figure 3: Visualisation of partial variables in ODW2 dataset, where we have selected the first 1,000 time points for presentation. The data distribution from different ground weather stations exhibit significant heterogeneity, and even though the trends of some variables may be similar, there are serious differences in magnitudes. The selected variables are, from left to right, humidity, precipitation, temperature, wind direction, and wind speed. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "\u2022 GPT-2. [15] Developed by OpenAI, GPT-2 is a language model that can generate coherent and diverse text based on a given prompt. In our research, we utilize the first 5 layers of the pre-trained GPT-2-base. \u2022 LLaMa. [50] LLaMa stands for Large Language Model Meta AI and is a series of cutting-edge language models with sizes ranging from 7B to 65B parameters. They offer top-notch performance with less computational power and resources. In our research, we utilize the first 4 layers of the 3B LLaMa model. ", "page_idx": 19}, {"type": "text", "text": "A brief description of these FL methods tailored to improve communication efficiency is as follows. ", "page_idx": 19}, {"type": "text", "text": "\u2022 FedKD: This parameter-efficient PFL method integrates knowledge distillation within a single client and employs a parameter aggregation strategy using Singular Value Decomposition (SVD). For the purposes of this section, which focuses solely on comparing communication efficiency, we incorporate only the SVD-based client-server communication strategies into LM-WEATHER as a baseline. ", "page_idx": 19}, {"type": "image", "img_path": "llTroju97T/tmp/8bbec4ca7670ca2f98c1aab75a39b00310316282ee501a969bb74b69aaf8db5c.jpg", "img_caption": ["Figure 4: (Figure 3 continued) Visualisation of partial variables in ODW2 dataset, where we have selected the first 1,000 time points for presentation. The selected variables are, from left to right, humidity, precipitation, temperature, wind direction, and wind speed. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "\u2022 FedPer: This PFL approach maintains a personalized layer while sharing the remaining base layers during communication. This enhances communication efficiency by transmitting only a portion of the parameters.   \n\u2022 FedBF: This fine-tuning method enhances parameter efficiency by sharing only the biases of the local model during global aggregation, thereby reducing communication overhead. To integrate this method into LM-WEATHER as a baseline, we adjusted all biases in LM-WEATHER to be unfrozen.   \n\u2022 FedAP: A parameter-efficient fine-tuning method in FL, which involves sharing only adapters during global aggregation.   \n\u2022 PromptFL: This parameter-efficient FL method enables participants to cooperatively train lightweight prompts without sharing the entire model, significantly accelerating both local training and global aggregation. In our experiments, we treat the adapter generated on clients as the prompt to facilitate the incorporation of this baseline. ", "page_idx": 20}, {"type": "text", "text": "B.3 Task Setups ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We evaluate our proposed LM-WEATHER using four distinct on-device weather modeling datasets, each with tailored settings for various tasks. The specific task settings for these datasets are detailed in Tab. 15. ", "page_idx": 20}, {"type": "text", "text": "Additionally, the specific tasks and scenarios for the on-device weather forecasting/imputation vary by dataset, as outlined in Tab. 16. ", "page_idx": 21}, {"type": "table", "img_path": "llTroju97T/tmp/7584257e60f3072711267ee421148bf4b5e707403022a4e0011192511f9eeeb0.jpg", "table_caption": ["Table 15: Task setup for different datasets during the evaluation. Note that for the imputation task there are actually no historical observations, but rather they are performed on a single long sequence. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "Table 16: Summary of framework evaluation scenarios for various datasets. Scenario 1/2/3/4 (in forecasting) refers to multivariate to univariate forecasting, where all historical variables are used to predict a single future variable. All represents multivariate to multivariate forecasting, meaning all variables predict all others. The symbol \"-\" indicates a non-existent scenario for that dataset. Scenario $\\mathbf{1/2/3}$ (in imputation) indicates different masking ratios for the original weather sequences. ", "page_idx": 21}, {"type": "table", "img_path": "llTroju97T/tmp/4aefa6e05afb5dcc6de638902fca601378a6b654fcf5eb382918081ada70e0ca.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "B.4 Implementation ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We mainly follow the experimental configurations across all baselines within a unified evaluation pipeline in https://github.com/thuml/Time-Series-Library for fair comparison. Specially, we use GPT-2-base as the default backbone model unless state otherwise. All our experiments are repeat five times and we report the averaged results. Our detailed model configurations are in Appendix B.8. All the algorithm implementations and designs in this study are based on Py torch and the algorithms are run on two RTX3090 GPUs 24GB. ", "page_idx": 21}, {"type": "text", "text": "B.5 Technical Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Reversible Normalization. In time series analysis, statistical properties like mean and variance often shift over time, indicating distributional changes in the data. To address this, we\u2019ve incorporated Reversible Normalization (RevIn) [51] into our LM-WEATHER. Specifically, we\u2019ve integrated RevIn into our Task Adapter Generation. This introduces two dynamic factors that adaptively normalize segments of the meteorological variable sequence $\\scriptscriptstyle\\mathcal{X}$ , or their decomposed components (Trend $\\chi_{\\mathrm{Trend}}$ , Seasonal $\\mathcal{X}_{\\mathrm{Seasonal}}$ , Residual $\\chi_{\\mathrm{Residual}})$ ), enhancing the accuracy of meteorological variable modeling. Specifically, for the trend component of $\\scriptscriptstyle\\mathcal{X}$ , i.e.,, $\\chi_{\\mathrm{Trend}}$ , its transformed value $\\chi_{\\mathrm{Trend}}^{\\prime}$ can be given by: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{X}_{\\mathrm{Trend}}^{\\prime}=\\gamma_{T}\\left(\\mathcal{X}_{\\mathrm{Trend}}-\\frac{\\mathbb{E}[\\mathcal{X}_{\\mathrm{Trend}}]}{\\sqrt{\\mathrm{Var}\\left[\\mathcal{X}_{\\mathrm{Trend}}\\right]}+\\epsilon_{T}}\\right)+\\beta_{T}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\mathbb{E}[\\mathcal{X}_{\\mathrm{Trend}}]$ and Var $[\\mathcal{X}_{\\mathrm{Trend}}]$ are the instance-specific mean and variance, respectively. $\\gamma_{T}$ and $\\beta_{T}$ are the trainable parameters for this component. This transformation is also applied to both the seasonal and residual components. ", "page_idx": 21}, {"type": "text", "text": "Pre-trained Language Model (PLM). In LM-WEATHER, we do not change the main architecture of the PLM, but use the parameter-efficient fine-tuning (PEFT) strategy to avoid large-scale parameter variations to ensure high efficiency on resource-constrained weather devices, and in this way, to achieve more reliable cross-domain knowledge transfer. Specifically, we introduce LoRA in the local PLM, which allows only $1.5\\%$ of the PLM parameters to be trained while the rest remain frozen, as shown in Fig. Note that LoRA is only applied to the query and value of each Attention in the PLM, and the resulting low-rank matrices are used for global sharing between the client and the server. ", "page_idx": 21}, {"type": "image", "img_path": "llTroju97T/tmp/b6ef3f6375f88203f6832fda1dad711a404ac071655b5dc2c07f23134553d76b.jpg", "img_caption": ["Figure 5: Schematic diagram of the PLM in LM-WEATHER, where we introduced LoRA to the PLM, to achieve more reliable cross-domain knowledge transfer while at the same time ensuring greater efficiency in adapting to low-resource weather devices. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Low-Rank Adaption (LoRA). To achieve more reliable cross-domain knowledge transfer (i.e., from natural language to complex weather sequences) while guaranteeing higher efficiency, we introduce LoRA [19], a parameter-efficient fine-tuning method for large language models, into PLM. Specifically, LoRA is applied to the Query and Value of each Attention layer by creating low-rank matrices for two pre-trained parameters $W_{q}$ and $W_{k}$ : ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathrm{QUERY}=W_{q}\\mathcal{X}+A_{q}B_{q},\\quad\\mathrm{VALUE}=W_{v}\\mathcal{X}+A_{v}B_{v},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\scriptscriptstyle\\mathcal{X}$ denote the latent representation from input weather sequences through PLM\u2019s word embedding layer, $A_{q}\\in\\mathbb{R}^{d\\times r}$ and $B_{q}\\in\\mathbb{R}^{r\\times\\hat{d}}$ are low-rank matrices created from $W_{q}\\in\\mathbb{R}^{d\\times d}$ , $A_{v}\\in\\mathbb{R}^{d\\times r}$ and $\\boldsymbol{B}_{v}\\in\\mathbb{R}^{\\dot{r}\\times d}$ are low-rank matrices created from $W_{v}\\in\\mathbb{R}^{d\\times d}$ , $d$ is the number of dimensions, $r$ is the rank, and $r\\ll d$ . It\u2019s important to note that only the low-rank matrices $A_{q},B_{q},A_{v},B_{v}$ are trainable; the others remain fixed during training. Initially, $A_{q}$ and $A_{v}$ are set with random Gaussian values, and $B_{q}$ and $B_{v}$ start as zero at the beginning of training. ", "page_idx": 22}, {"type": "text", "text": "Task Adapter Generation. The $\\chi_{\\mathrm{Trend}}$ , $\\mathcal{X}_{\\mathrm{Seasonal}}$ , $\\chi_{\\mathrm{Residual}}$ obtained from decomposition are used to generate Task Adapter via an unified generator as Fig. 1B that consisting of Token Embedding, Position Embedding, and Temporal Embedding. Specially, we use one-dimensional convolution operation to map each each specific sample $\\dot{\\mathcal{X}}^{k}\\in\\mathbb{R}^{T\\times1}$ while keeping raw shape to generate TOKEN ADAPTER $P_{\\mathrm{TO}}\\in\\mathbb{R}^{T\\times C}$ , as ", "page_idx": 22}, {"type": "equation", "text": "$$\nP_{\\mathrm{TO}}^{k}=\\operatorname{CONv1D}(\\chi^{k}),\\quad P_{\\mathrm{TO}}=\\operatorname{CoNv1D}(\\chi)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Additionally, we use a trainable lookup table to map each point\u2019s explicit position in the entire sequence, to generate POSITION ADAPTER $P_{\\mathrm{PO}}\\in\\dot{\\mathbb{R}}^{T\\times C}$ , as: ", "page_idx": 22}, {"type": "equation", "text": "$$\nP_{\\mathrm{PO}}=E(\\mathrm{INDEX}(\\mathcal{X})),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $E(\\cdot)$ is the trainable lookup table, and INDEX $(\\cdot)$ is a function that achieve the indices of each point\u2019s locations of weather sequence $\\scriptscriptstyle\\mathcal{X}$ . Furthermore, we separately encode different time attributes such as minutes, hours, days, weeks, and months, using trainable parameters to dynamically model complex temporal shifts, to generate TEMPORAL ADAPTER $P_{\\mathrm{TE}}$ , as ", "page_idx": 22}, {"type": "equation", "text": "$$\nP_{\\mathrm{TE}}=\\sum_{\\alpha\\in\\{\\mathrm{mins,hours,days,weeks,months}\\}}E_{\\alpha}(\\chi)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\alpha$ represents different temporal attributes, $\\pmb{E}_{\\alpha}$ denotes the trainable lookup table for each temporal attributes. Finally, for each decomposition components, corresponding generated adapters can be obtained by aggregating Token Adapter $P_{\\mathrm{TO}}\\stackrel{\\cdot}{\\in}\\mathbb{R}^{L\\times C}$ , Position Adapter $\\dot{P}_{0}\\in\\breve{\\mathbb{R}}^{\\breve{L}\\times C}$ , and Temporal Adapter $P_{\\mathrm{TE}}\\in$ $\\mathbb{R}^{\\check{L}\\times\\check{C}}$ as $P_{d}\\stackrel{\\smile}{=}P_{\\mathrm{TO}}^{d}+P_{\\mathrm{PO}}^{\\dot{d}}+P_{\\mathrm{TE}}^{d}$ , where $d\\in$ {Trend, Seasonal, Residual}, this means that we can obtain $P_{\\mathrm{Trend}}$ , $P_{\\mathrm{Seasonal}}$ , PResidual. ", "page_idx": 22}, {"type": "text", "text": "B.6 Theoretical Insights on Personalized Adapter ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "The effectiveness and superiority of our proposed Personalized Adapter have been demonstrated in sufficient ablation studies (please refer to Table 6 in the main text). Here, we will discuss theoretical insights that further supports the effectiveness of Personalized Adapter. Personalized Adapter can effectively capture potential pattern in meteorological variable time series, which comprises both the Task Adapter and the FFN-based Parameter Adapter. Specifically, our focus primarily revolves around the Task Adapter active in extracting representations, which including Token/Positional/Temporal Embedding for transforming meteorological variable time series. ", "page_idx": 23}, {"type": "text", "text": "Let the weather sequence be $\\mathbf{X}=(\\mathbf{x}_{1},\\mathbf{x}_{2},\\cdots\\,,\\mathbf{x}_{T})$ , where $\\mathbf x_{t}\\,\\in\\mathbb R^{d}$ Is the observed value with $d$ variables at $t$ moment. Let $\\scriptscriptstyle\\mathcal{X}$ represent the function space to which $\\mathbf{x}_{t}$ of a weather sequence belongs, and $\\mathcal{Z}$ denote the function space to which the implicit representation $\\mathbf{z}_{t}$ belongs. Token Embedding can be interpreted as a mapping $f_{\\theta}:\\mathcal{X}\\to\\mathcal{Z}$ . According to the Kolmogorov-Arnold representation theorem, for any continuous function $\\bar{f}\\in C([0,1]^{d})$ , there exist $2d+1$ continuous functions $\\phi_{q}\\;\\;\\in\\;C([0,1])$ and $\\psi_{q}\\in C([0,1])$ such: ", "page_idx": 23}, {"type": "equation", "text": "$$\nf(\\ensuremath{\\mathbf{x}})=\\sum_{q=0}^{2d}\\phi_{q}(\\sum_{p=1}^{d}\\psi_{q}(x_{p})),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "which means Token Embedding can construct a high-dimensional nonlinear mapping from multiple onedimensional functions, capturing complex patterns within weather sequences. Positional Embedding introduces a vector $\\mathbf{p}_{t}$ for each step, enabling the model to differentiate between observations at different time steps. For any two steps $t_{1}$ and $t_{2}$ , their position vectors $\\mathbf{p}_{t_{1}}$ and $\\mathbf{p}_{t_{2}}$ satisfy: ", "page_idx": 23}, {"type": "equation", "text": "$$\n||\\mathbf{p}_{t_{1}}-\\mathbf{p}_{t_{2}}||_{2}=\\sqrt{2k(1-\\cos\\frac{2\\pi(t_{1}-t_{2})}{10000^{1/k}})}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The growing distance between $t_{1}$ and $t_{2}$ with an increasing time gap mirrors the relative positioning of time steps, aiding the model in grasping temporal dependencies. Additionally, the sine-cosine function\u2019s periodicity resonates with weather data\u2019s cyclical behavior, helping the model to learn from these recurrent patterns. ", "page_idx": 23}, {"type": "text", "text": "Finally, consider the role of Temporal Embedding from the view of matrix decomposition. Suppose the temporal matrix $\\mathbf{T}$ has a rank of $r$ , it can be decomposed as $\\begin{array}{r}{\\mathbf{z}_{t}\\otimes\\mathbf{T}\\,=\\,\\sum_{i=1}^{r}(\\mathbf{z}_{t}\\otimes\\dot{\\mathbf{u}}_{i})\\mathbf{v}_{i}^{T}}\\end{array}$ . Temporal Embedding transforms the original sequence by scaling and rotating it to re present different interaction patterns. Using singular value decomposition, the top $r$ singular vectors distill the core structure of the time-based matrix. This allows Temporal Embedding to intuitively learn a compact representation of weather sequences, highlighting the primary interactions between variables. In optimizing Personalized Adapter within LM-WEATHER, the focus lies solely on Personalized Adapter and the attention layer influenced by LoRA during local updates. As Personalized Adapter undergoes solely local updates while sharing low-rank matrices globally, akin to layer-wise optimization in PLM, the efficacy of its optimization process can be theoretically substantiated by the theoretical analysis provided in [52]. ", "page_idx": 23}, {"type": "text", "text": "B.7 Evaluation Metrics ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "For evaluation metrics, as [34], we utilize the mean absolute error (MAE) and root mean square error (RMSE) for both forecasting and imputation. The calculation of these metrics are as follows: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathrm{MAE}=\\frac{1}{T}\\sum_{i=1}^{T}|\\boldsymbol{Y}_{i}-\\hat{\\boldsymbol{Y}}_{i}|,\\qquad\\qquad\\mathrm{RMSE}=\\sqrt{\\frac{1}{T}\\sum_{i=1}^{T}(\\boldsymbol{Y}_{i}-\\hat{\\boldsymbol{Y}}_{i})^{2}},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $T$ denotes the number of data points (i.e., prediction horizon in our cases), $\\mathbf{Y}_{i}$ and $\\hat{Y_{i}}$ are the $i$ -th ground truth and prediction where $i\\in\\{1,...,T\\}$ . ", "page_idx": 23}, {"type": "text", "text": "B.8 Model Configurations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "The configurations of our LM-WEATHER for different tasks and datasets are summarized in Tab. 17. We consistently use the AdamW [53] optimizer in all experiments. ", "page_idx": 23}, {"type": "text", "text": "Appendix C Theorems and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Theorem C.1 (Decomposition Rationality from Time Series). Given a weather time series $\\mathcal{X}=\\mathcal{X}_{T r e n d,t}+$ $\\mathcal{X}_{S e a s o n a l,t}\\,+\\,\\mathcal{X}_{R e s i d u a l,t.}$ , $t~\\in~[t_{1},t_{n}]$ . Let $\\pmb{E}~=~\\{e_{1},e_{2},...,e_{n}\\}$ denotes a set of orthogonal bases. Lets $E_{S e a s o n a l}\\ \\subseteq\\ E$ denote the subset of $\\pmb{E}$ on which $\\mathcal{X}_{S e a s o n a l,t}$ has non-zero eigenvalues and $\\pmb{{\\cal E}}_{T r e n d}\\ \\subseteq\\ \\pmb{{\\cal E}}$ denote the subset of $\\pmb{E}$ on which $\\chi_{T r e n d,t}$ has non-zero eigenvalues. If $\\chi_{T r e n d,t}$ and $\\mathcal{X}_{S e a s o n a l,t}$ are not orthogonal, i.e., $\\begin{array}{r}{\\sum_{i=1}^{n}\\mathcal{X}_{T r e n d,t}^{i}\\mathcal{X}_{S e a s o n a l,t}^{i}\\neq0,}\\end{array}$ , then $E_{T r e n d}\\bigcap E_{S e a s o n a l}\\not=0,$ , i.e., $\\pmb{E}$ can not disentangle the two signals onto two  disjoint set of bases. ", "page_idx": 23}, {"type": "table", "img_path": "llTroju97T/tmp/490854567e9e4aeec36743d250a2cf9282706d1164c1e80dca76004b13a07e1a.jpg", "table_caption": ["Table 17: An overview of the experimental configuration for LM-WEATHER. LR is the initial learning rate, (FS) denotes the few-shot learning setting. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "Proof. We decompose $\\mathcal{X}_{\\mathrm{Seasonal},t}$ and $\\chi_{\\mathrm{Trend},t}$ onto $\\pmb{E}$ and acquire that $\\mathcal{X}_{\\mathrm{Seasonal},t}\\ =\\ \\sum a_{i}e_{i}$ and $\\mathcal{X}_{\\mathrm{Trend},t}\\ =$ $\\sum b_{i}e_{i}$ . Then it is obvious that $e_{i}\\in\\mathcal{X}_{\\mathrm{Seasonal}}\\Leftrightarrow a_{i}\\neq0$ and $e_{i}\\in\\mathcal{X}_{\\mathrm{Trend}}\\Leftrightarrow b_{i}\\neq0$ .  Now, let us consider the inner product of $\\mathcal{X}_{\\mathrm{Seasonal},t}$ and $\\chi_{\\mathrm{Trend},t}$ : ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{i=1}^{n}\\chi_{\\mathrm{Trend},t}^{i}\\chi_{\\mathrm{Seasonal},t}^{i}=\\chi_{\\mathrm{Trend},t}\\chi_{\\mathrm{Seasonal},t}}\\\\ {\\displaystyle=(\\sum a_{i}e_{i})\\cdot(\\sum b_{i}e_{i})=\\sum_{i,j}a_{i}b_{j}e_{i}e_{j}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Note that $\\begin{array}{r}{\\sum_{i=1}^{n}\\mathcal{X}_{\\mathrm{Trend},t}^{i}\\mathcal{X}_{\\mathrm{Seasonal},t}^{i}=0}\\end{array}$ . Thus, there must be at least one $i$ such that $a_{i}\\neq0$ and $b_{i}\\neq0$ . Thus. $e_{i}\\in E_{\\mathrm{Seasonal}}$ and $e_{i}\\in E_{\\mathrm{Trend}}$ , in other words, . The theorem demonstrates that if $\\mathcal{X}_{\\mathrm{Trend},t}$ and $\\mathcal{X}_{\\mathrm{Seasonal},t}$ are not orthogonal, orthogonal bases that separate $\\chi_{\\mathrm{Trend},t}$ and $\\mathcal{X}_{\\mathrm{Seasonal},t}$ into two distinct sets cannot exist. Typically, periodic and non-periodic signals are not orthogonal because the periodic signal has a discrete spectrum, while the non-periodic signal has a continuous one, leading to potential overlaps at non-zero frequencies. Principal Component Analysis (PCA) seeks to find orthogonal bases in data, but it cannot split these two signals into separate bases. Citing Theorem 1 from [17], we understand that self-attentive mechanisms in pre-trained large models function similarly to PCA. Thus, without manual intervention, the self-attentive mechanism is unable to automatically divide a time series into trend and seasonal components. \u53e3 ", "page_idx": 24}, {"type": "text", "text": "Theorem C.2 (Exchange Low-Rank Matrices Ensures Privacy: Parameter Interaction Perspective). Given a on-device weather modeling framework based on federated learning that global optimization object $\\begin{array}{r}{\\pmb{F}(\\theta)=\\sum_{n}^{i=1}p_{i}\\,f(\\{D_{i}\\};\\theta)}\\end{array}$ , where $f(x;{\\boldsymbol{\\theta}})$ is the loss function of i-th client, $\\{D_{i}\\}$ is dataset of $i$ -th client, and $p_{i}$ and $\\theta$ denote the data distribution weight of client i and the model parameters, respectively. Given that the parameters $\\theta$ of the $P L M\\,\\mathcal{M}_{\\theta}$ broadcasted by the server consist of two parts: a frozen part $\\mathcal{M}_{\\theta,f}$ and $a$ trainable part $\\mathcal{M}_{\\theta,t}$ , interacting only the low-rank matrix parameters $\\mathcal{M}_{\\theta,l}\\subset\\mathcal{M}_{\\theta,t}$ is a subset of trainable part $\\mathcal{M}_{\\theta,t}$ during each round ensures privacy. ", "page_idx": 24}, {"type": "text", "text": "Proof. We assume that $f(x;{\\boldsymbol{\\theta}})$ is a convex function with respect to $\\theta$ , i.e., for any $\\theta_{1}$ and $\\theta_{2}$ and $\\lambda\\in[0,1]$ , we have ", "page_idx": 24}, {"type": "equation", "text": "$$\nf(x;\\lambda\\theta_{1}+(1-\\lambda)\\theta_{2})\\leq\\lambda f(x;\\theta_{1})+(1-\\lambda)f(x;\\theta_{2}).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Since only low-rank matrices parameter $\\mathcal{M}_{\\theta,l}$ parameterized by $\\theta_{l}$ is exchanged, we can convert $\\theta$ to $\\theta^{\\prime}=[\\theta_{l}^{\\prime},\\theta_{o}]$ , where $\\theta_{l}^{\\prime}$ is the embedding parameter after the server update. Since we only update on $\\theta_{l},\\theta_{o}$ remains unchanged. Thus, data privacy can be ensured, as $\\theta_{o}$ contains parameters that reveal user-specific information. Furthermore, the low-rank matrices applied to the PLM $\\mathcal{M}_{\\theta}$ using LoRA are initialized with a random Gaussian distribution and all-zero values, respectively, before training. This global information sharing approach also helps to enhance privacy. \u53e3 ", "page_idx": 24}, {"type": "text", "text": "Theorem C.3 (Exchange Low-Rank Matrices Ensures Privacy: Model Indistinguishability Perspective). Our LM-Weather can hide key features of local model when sharing low-rank parameters, even external attacker gains access to a shared low-rank update, it is difficult to reconstruct or differentiate between the original models of different participants. Support client i and client $j$ get two different local model due to updated on heterogeneity weather time series, as $\\mathcal{M}_{i}$ and $\\mathcal{M}_{j}$ parametered by $\\theta_{i}$ and $\\theta_{j},L(\\mathcal{M})$ denotes the low-rank matrix generated by model $\\mathcal{M}$ , where $B\\in\\mathbb{R}^{d\\times r}$ and $A\\in\\mathbb{R}^{r\\times d}$ , and $r\\ll d,$ and define Adv as the adversary. Conditions exist such that for any polynomial-time attacker $A d v,|P r[A d v(L(\\mathcal{M}_{i}))=1]-P r[A d v(L(\\mathcal{M}_{j}))=1]|\\leq\\epsilon$ holds, where \u03f5 is a small positive number, which implies that even if the attacker acquires a shared low-rank update, it is difficult to reconstruct or distinguish between the original models of the different participants. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "Proof. The goal of LoRA is to find the low-rank approximation: $m i n||\\theta-\\theta_{0}\\,-\\,B A||_{F}$ , where $\\theta_{0}$ is the initial weight, $||\\cdot||_{F}$ is the Frobenius paradigm. Consider two models $\\mathcal{M}_{i}$ and $\\mathcal{M}_{j}$ , whose weights differ by $\\Delta\\theta=\\theta_{i}-\\theta_{j}$ . The corresponding LoRA matrix is: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{L_{i}=B_{i}A_{i}\\approx\\Delta\\theta_{i}=\\theta_{i}-\\theta_{0},}\\\\ {L_{j}=B_{j}A_{j}\\approx\\Delta\\theta_{j}=\\theta_{j}-\\theta_{0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "According to matrix approximation theory, for the best approximation with rank $r$ , the upper bound on the error is: ", "page_idx": 25}, {"type": "equation", "text": "$$\n||\\Delta\\theta_{i}-L_{i}||F\\leq\\sigma_{r+1}(\\Delta\\theta_{i})\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $\\sigma_{r+1}(\\Delta\\theta_{i})$ is the $_{\\mathrm{r}+1}$ -st singular value of $\\Delta\\theta_{i}$ . By Johnson-Lindenstrauss Lemma [54], for any $\\epsilon>0$ , there exists a mapping $f:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{k}$ , where $k=O(l o g(n)/\\epsilon^{2})$ , such that for any $x,y\\in\\mathbb{R}^{d}$ . In our case, LoRA can be regarded as such a degenerate mapping. Assume $\\lVert\\Delta\\dot{{\\boldsymbol{\\theta}}}_{i}-\\Delta{\\boldsymbol{\\theta}}_{j}\\rVert_{F}\\leq\\delta$ and according to the trigonometric inequality: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|L_{i}-L_{j}\\|_{F}\\leq\\|L_{i}-\\Delta\\theta_{i}\\|_{F}+\\|\\Delta\\theta_{i}-\\Delta\\theta_{j}\\|_{F}+\\|\\Delta\\theta_{j}-L_{j}\\|F\\leq\\sigma_{r+1}(\\Delta\\theta_{i})+\\delta+\\sigma_{r+1}(\\Delta\\theta_{j}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Let $\\varepsilon^{\\prime}=\\sigma_{r+1}(\\Delta\\theta_{i})+\\sigma_{r+1}(\\Delta\\theta_{j})$ , we get: ", "page_idx": 25}, {"type": "equation", "text": "$$\n||L_{i}-L_{j}||_{F}\\leq\\delta+\\varepsilon^{\\prime}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "For any polynomial-time attacker Adv,its ability to distinguish between $L_{i}$ and $L_{j}$ is restricted to the difference in their Frobenius paradigms. We can define a function $f$ such that: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|P r[\\mathrm{Adv}(L(\\mathcal{M}_{i}))=1]-P r[\\mathrm{Adv}(L(\\mathcal{M}_{j}))=1]|\\leq f(||L_{i}-L_{j}||_{F})}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $f$ is a monotonically increasing function that represents the attacker\u2019s capability with respect to the matrix difference. Combining Eq. 16 and Eq. 17: ", "page_idx": 25}, {"type": "equation", "text": "$$\n|P r[\\mathtt{A d v}(L(\\mathcal{M}_{i}))=1]-P r[\\mathtt{A d v}(L(\\mathcal{M}_{j}))=1]|\\le\\epsilon\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "when $\\delta$ and $\\varepsilon^{\\prime}$ are small enough, we can make sure that the right-hand side is smaller than the intended $\\varepsilon$ . This means that an attacker cannot reverse-engineer personalised local parameters and data to ensure privacy through the low-rank matrix of communication across clients. \u53e3 ", "page_idx": 25}, {"type": "text", "text": "Appendix D Additional Finding & Experiment & Discussion ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In this section, we explore and discuss potential research findings and questions for our LM-WEATHER via conducting additional experiments. These potential research questions are as follows: ", "page_idx": 25}, {"type": "text", "text": "\u2022 RQ1. How does LM-WEATHER compare to Personalized Federated Learning (PFL) baselines in terms of trade-offs in personalization and global model performance?   \n\u2022 RQ2. How does LM-WEATHER perform compared to centralized and local-only training modes?   \n\u2022 RQ3. How does the pre-trained language model contribute in LM-WEATHER?   \n\u2022 RQ4. What is the resource utilization and training $\\&$ inference cost of LM-WEATHER?   \n\u2022 RQ5. Can LM-WEATHER be used for other tasks? ", "page_idx": 25}, {"type": "text", "text": "D.1 Trade-offs in Personalization and Global Model Performance (RQ1) ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Our LM-WEATHER builds on the assumption that the foundation model already exists, treating pre-trained language models (PLMs) as such and broadcasting it to each client to achieve local updates. Our aim is to employ device information-specific (e.g., geographic/atmospheric patterns) adapter, to promote the local PLM in achieving cross-domain knowledge transfer from language to meteorological sequences. This approach yields highly customized models for individual devices while achieving global knowledge to avoid data silo, thereby supporting diverse analyses of heterogeneous weather data. Alternative PFL methods do not match the efficiency and flexibility of our personalized adapter in this context, making them less suitable. By incorporating PFL baselines (Per-FedAvg [43], APPLE [55], FedPer [56], and FedALA [57]), we provide quantitative results that substantiate our claims, experiment setting is consistent with the manuscript on ODW1T. ", "page_idx": 25}, {"type": "text", "text": "\u2022 Per-FedAvg: Allowing for personalized model updates for each client by adding client-specific parameters to the global model and optimizing them during FL training.   \n\u2022 APPLE: Tackling statistical heterogeneity in FL by automatically capturing information required by clients from global models using adaptive local aggregation methods.   \n\u2022 FedPer: Dividing the model into a base layer and a personalization layer, only the base layer is uploaded during aggregation while keeping the personalization layer to combat statistical heterogeneity.   \n\u2022 FedALA: Tackling statistical heterogeneity in FL by automatically capturing information required by clients from global models using adaptive local aggregation methods. ", "page_idx": 26}, {"type": "text", "text": "Table 18: Comparison on personalized performance between our LM-WEATHER and PFL baselines under forecasting (multivariate-multivariate) and imputation $50\\%$ masking rate), where Avg. denotes the average performance of four periods [96, 192, 336, 720], Bold means the best. ", "page_idx": 26}, {"type": "table", "img_path": "llTroju97T/tmp/f5dc3eead398333cad64dbcf01952153a4b3e37b2ccb438a22b9af40cf7953eb.jpg", "table_caption": [], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "Personalized Performance Comparison. The performance quantification of our LM-WEATHER and PFL baselines under personalized performance for different tasks and scenarios is shown in Table 18. Our LM-WEATHER outperform other PFL baselines across different tasks (forecasting/imputation) and scenarios (regular/few-shot learning) by a wide margin. This supports our finding that in the scenario of on-device weather variable modeling, PFL methods is not appropriate. ", "page_idx": 26}, {"type": "text", "text": "Global Model Performance Comparison. The comparison on global model performance across client between our LM-WEATHER and PFL baselines are shown in Table 18. Our LM-WEATHER outperforms PFL baselines in terms of global model performance, as demonstrated by the fact that its global model performs more stable across client with heterogeneous data. ", "page_idx": 26}, {"type": "table", "img_path": "llTroju97T/tmp/2861f90a3dd43a172f08b7461cba968c347476cec7154760e3b3aab218cbfaeb.jpg", "table_caption": ["Table 19: Comparison on global model performance across client between LM-Weather and PFL baselines on multivariate-multivariate forecasting tasks (OWD1T dataset, MAE/RMSE report), Red denotes the original LM-WEATHER\u2019s performance, and Bold means the best among global performance. "], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "Personalization and Global Model Performance Trade-offs. We consider the trade-off between personalization performance and global model performance for our LM-WEATHER and PFL baselines, the results are shown in Table 20. Compared with PFL baselines, our LM-WEATHER maintains the best trade-off between personalization performance and global model performance, i.e., the personalization performance does not significantly exceed the global model performance while the performance far exceeds PFL methods, which means that the LM-WEATHER can be flexibly applied to different practical scenarios, including personalised analysis of regional weather trends as well as comprehensive analysis of weather trends over large-scale regions. This means that LM-WEATHER can be flexibly applied to different practice scenarios, including the personalised analysis of regional weather trends and the comprehensive analysis of weather trends over large scale areas. ", "page_idx": 26}, {"type": "text", "text": "Table 20: Comparison of LM-Weather between personalized performance and global model performance, results are obtained on the multivariate-multivariate forecasting task on OWD1T (MAE/RMSE report), Bold means the best, $\\uparrow$ represents the improvement (gap) in the personalization performance of the method relative to the global model performance. ", "page_idx": 27}, {"type": "table", "img_path": "llTroju97T/tmp/cce7713822438dc5f93acb329b6c82d451de3f0c5e0773c3953d81c6c8859add.jpg", "table_caption": [], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "Performance and Adapter Updating Trade-offs. Furthermore, we investigated the effect of varying the number of local update rounds in adapters across clients on the performance of LM-WEATHER regrading personalization. The results are presented in Table 21. We observed that increasing the local update rounds from the default five to fifteen leads to smoother and enhanced personalization performance across heterogeneous clients. However, this increase in local update rounds also incurs additional computational and communication costs, which, in our assessment, do not justify the modest performance improvements. ", "page_idx": 27}, {"type": "table", "img_path": "llTroju97T/tmp/12e549a7c9bdef81fd0e89c87db4543d912296f05903fa93287b97f3c2e749f2.jpg", "table_caption": ["Table 21: Performance of each client under the multivariate-multivariate forecasting task on ODWT1 with different adapter local update epoch (MAE/RMSE report), where $E=5/10/\\mathrm{i}5$ represent the 5, 10, and 15 local training rounds, respectively. "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "D.2 Centralised and Local-only Training (RQ2) ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "The ordinary centralised training strategy (all data were aggregated into a single server) exhibits learning efficiency that an ordinary distributed learning strategy. The ultimate goal of FL is to achieve performance close to that of centralised training and to ensure privacy across data sources. Table 22 illustrates that our LM-WEATHER achieves comparable effectiveness to Non-FL (centralised) training, with only a $2.04\\%$ disparity. Compared to LM-WEATHER-Local, which lacks interaction between devices, LM-WEATHER performs better due to overcoming data silos. ", "page_idx": 27}, {"type": "text", "text": "D.3 Contributions of Pre-trained Language Model in LM-WEATHER (RQ3) ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Our LM-WEATHER significantly outperforms time series-specific models trained from scratch under centralised setup. Centralised training aims to acquire an excellent pre-trained model, where PLMs possess inherent advantages due to their prior sequence modeling capabilities. Moreover, various parameter-efficient fine-tuning (PEFT) strategies enable PLMs to adapt to new domain knowledge cost-effectively. FL-based aggregation facilitates a stable on-device fine-tuning process, with LM-Weather enabling highly customized on-device fine-tuning of PLMs with greater efficiency. This highlights the substantial contribution of PLMs in this task. ", "page_idx": 27}, {"type": "text", "text": "Table 22: Comparison of LM-WEATHER\u2019s multivariate-multivariate performance in the FL and the Non-FL (centralised) setups, LM-WEATHER-Local is the setting in which LM-WEATHER is trained locally at each device without communication, and disparity is the difference in performance relative to Non-FL. ", "page_idx": 28}, {"type": "table", "img_path": "llTroju97T/tmp/2a16f6f4032858f4fed26f046e4c68f163048cd67afca5b6b76924836056cabe.jpg", "table_caption": [], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "Table 23: Comparison between fine-tuning PLM with Adapter (LM-WEATHER) and training from scratch using non-PLM architecture (Pyraformer, Reformer, PatchTST, DLinear, and LightTS) on multivariate-multivariate forecasting tasks (MAE/RMSE report), Bold means the best. ", "page_idx": 28}, {"type": "table", "img_path": "llTroju97T/tmp/3ef65a00b2211eb3b6d6872147de23e0bec39e48fc4901979b1c60c4e2a050eb.jpg", "table_caption": [], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "D.4 No Free Lunch in Performance Improvement (RQ4) ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "The remarkable capabilities of cutting-edge DL models across various domains and tasks, such as LLMs, and VLMs, can be attributed to their extensive parameters and training on large datasets. Currently, a perfect balance among performance, model size, and cost does not exist. Despite numerous studies focusing on reducing training and inference costs while maintaining superior performance, there is no one-size-fits-all solution. This is also true for our LM-WEATHER, which demonstrates exceptional performance across diverse tasks and scales on real-world datasets with significant heterogeneity, significantly outperforming comparable DL methods. In this context, we analyze the costs associated with training and inference for LM-WEATHER and its baselines, exploring and discussing the trade-offs between cost-effectiveness and performance in practical applications. ", "page_idx": 28}, {"type": "text", "text": "Table 24: Comparison of training/inference costs based on ODW1T with $N=192$ under multivariatemultivariate forecasting tasks (MAE/RMSE report), where Bold denotes the best, \u2018Comm.\u2018 and \u2018Perf.\u2018 denote communication and performance, respectively. ", "page_idx": 28}, {"type": "table", "img_path": "llTroju97T/tmp/4173935a95e80154109c81bd30af6f74b8904c3f9ea95fdc1fb7b99bd18c0f94.jpg", "table_caption": [], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "The quantification and comparison of computational costs against LM-WEATHER and baseline are shown in Table 24. We discuss this results from two perspectives as follow. ", "page_idx": 28}, {"type": "text", "text": "Communication and Performance. LM-WEATHER outperforms baseline in these two key metrics, which is critical for practical meteorological variable modeling and analysis, a bandwidth-sensitive and high accuracy demanding application. ", "page_idx": 29}, {"type": "text", "text": "Trade-offs between Resource Consumption and Performance. In terms of training and inference time and memory usage, LM-WEATHER is less efficient than lightweight baselines such as FL-DLinear, LightTS, and iTransformer, due to its use of a Pretrained Language Model (PLM) as a backbone. Although LM-WEATHER demands more resources, the trade-off is justified by its cost-effective performance gains. Its slightly increased memory requirements for training and inference are manageable on most devices. In the context of weather analysis, where precision is critical, prioritizing performance improvements over minimal resource consumption is essential. Additionally, LM-WEATHER capitalizes on the knowledge-rich PLM and requires only minimal, low-cost fine-tuning on devices to achieve superior performance. This strategy not only enhances performance but also reduces the frequency of future model updates, thereby lowering long-term costs compared to developing a baseline model from scratch. ", "page_idx": 29}, {"type": "table", "img_path": "llTroju97T/tmp/312a4ff2404405259d51141145415cde8f5b87177f1c96969fad2f5986beedaf.jpg", "table_caption": ["Table 25: Comparison between LM-Weather and baseline in terms of model size on the device and performance of forecasting (multivariate-to-multivariate), and imputation ( $50\\%$ masking rate) on ODW1T (MAE/RMSE report), where Bold and Underline denote the best and the second best. "], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "In addition, we further provide a comparison of model sizes and performance between LM-WEATHER and baseline, as shown in Table 25. The difference in model size between our LM-WEATHER and baseline can be deemed acceptable for the following reasons. ", "page_idx": 29}, {"type": "text", "text": "Trade-offs between Performance and Size. While LM-WEATHER may not be as compact in terms of model size or resource efficiency as lightweight baselines, it offers significant advantages in various analysis tasks. Its high performance is particularly valuable in practical applications. Moreover, with a model size of 304.19 M, LM-WEATHER is still accessible for devices with limited resources. This contrasts sharply with many large foundation models, which typically comprise several hundred million parameters. The trade-off between performance and size is justified, especially considering the critical nature of accurate weather data analysis. ", "page_idx": 29}, {"type": "text", "text": "Efficient Parameter Update and Communication. LM-WEATHER implements efficient on-device fine-tuning of the pretrained language model. Unlike baselines that require training from scratch, LM-WEATHER only needs fine-tuning of a relatively small number of parameters $(10.38\\;\\mathrm{M})$ on each device, with minimal device-to-server communication overhead $(0.38\\;\\mathrm{M})$ . This approach facilitates highly personalized cross-domain knowledge transfer, significantly reducing the ongoing costs associated with processing the ever-changing streams of weather data. These aspects highlight the pragmatic considerations that have shaped the design of LM-WEATHER. The model\u2019s capabilities to deliver exceptional performance, combined with its efficient parameter tuning and communication strategies, offer a cost-effective solution for advanced weather data analysis in resource-constrained environments. ", "page_idx": 29}, {"type": "text", "text": "D.5 Additional Tasks for Potential Applications (RQ5) ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Given datasets we proposed in this paper focus on forecasting and imputation tasks, we broaden its scope briefly to explore its potential application by integrating anomaly weather detection tasks. This involves relabeling the dataset to identify intervals with anomalous meteorological variables as instances of abnormal weather processes. Specifically, we label original datasets via Isolation Forecast [58], the main process as follows: (1) We set the cut length to 100, using this metric to segment each channel (variable) and construct several random trees that collectively form a forest. (2) The \u201cisolation\u201c degree of each data point is quantified by the average path length from the root node to the terminal node. (3) Data points with shorter path lengths are more easily isolated and thus more likely to be outliers. (4) We establish a threshold based on the average path length; data points falling below this threshold are classified as anomalies. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "Evaluation Metrics. We used Precision (P), Recall (R), and F1-Score (F1) to simply quantify the performance of LM-WEATHER and baselines on the weather anomaly detection task, these can be formulated as: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathrm{\\calP}=\\frac{\\mathrm{T}\\mathrm{\\calP}}{\\mathrm{T}\\mathrm{\\calP}+\\mathrm{F}\\mathrm{\\calP}},\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\mathrm{\\cal{R}}=\\frac{\\mathrm{T}\\mathrm{\\calP}}{\\mathrm{T}\\mathrm{\\calP}+\\mathrm{F}\\mathrm{\\calN}},\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\mathrm{F}1=2\\times\\frac{\\mathrm{\\cal{P}}\\times\\mathrm{\\cal{R}}}{\\mathrm{\\calP}+\\mathrm{\\cal{R}}},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where TP (True Positives), FP (False Positives), and FN (False Negatives) represent the number of samples correctly labeled as anomalous, the number of samples incorrectly labeled as anomalous, and the number of samples that were not labeled as anomalous by the model but were actually anomalous, respectively. ", "page_idx": 30}, {"type": "text", "text": "Experiments and Results. We set the input time series length to 100, and other settings (e.g., baselines, hyper-parameters, local updating steps and federated communication rounds, etc.) are consistent with those in the main text, and we conduct experiments on OWD1T and OWD2T to briefly show the results. The performance quantification of our proposed LM-WEATHER and baseline on weather anomaly detection tasks is shown in Table 26 (ODW1T results) and Table 27 (ODW2T results).The findings underscore LM-WEATHER\u2019s robust applicability and its Moreover, LM-WEATHER\u2019s superior performance over baselines in both regular and fewshot tasks reaffirms its effectiveness and overall superiority. Moreover, LM-WEATHER\u2019s superior performance over baselines in both normal and few-shot tasks reaffirms its effectiveness and overall superiority. ", "page_idx": 30}, {"type": "text", "text": "Table 26: Results of LM-WEATHER and baseline for weather anomaly detection tasks on ODW1T, including regular and few-shot scenarios, where $5\\%$ means that $5\\%$ of the data is used in training, Bold and Underline denote the best and the second best. ", "page_idx": 30}, {"type": "table", "img_path": "llTroju97T/tmp/8a265d332a35a0ad0dc7290d78692419d35ef7599aada0c2478d1dc7096e0834.jpg", "table_caption": [], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "Table 27: Results of LM-WEATHER and baseline for weather anomaly detection tasks on ODW2T, including regular and few-shot scenarios, where $5\\%$ means that $5\\%$ of the data is used in training, Bold and Underline denote the best and the second best. ", "page_idx": 30}, {"type": "table", "img_path": "llTroju97T/tmp/2471cc03c6e5971dfe22aaa309abb68ea79c1839e5cb546e4b8415d728c7ac34.jpg", "table_caption": [], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "Appendix E Full Experiment Results ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "In this section, we provide the full experimental results not included in the main manuscript. This includes the main experiments (Appendix E.1), few-shot learning experiments (Appendix E.2), and ablation studies (Appendix E.3), as well as extra analysis of our framework, covering hyperparameter sensitivity (Appendix E.4) and its performance with different PLMs. ", "page_idx": 31}, {"type": "text", "text": "E.1 Full Main Results ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "In this section, we show detailed and full experimental results including: ", "page_idx": 31}, {"type": "text", "text": "\u2022 Forecasting (Tab. 28) and imputation (Tab. 29) across different scenes and settings on the ODW1T dataset.   \n\u2022 Forecasting (Tab. 30) and imputation (Tab. 31) across different scenes and settings on the ODW1V dataset.   \n\u2022 Forecasting (Tab. 32) and imputation (Tab. 33) across different scenes and settings on the ODW2T dataset.   \n\u2022 Forecasting (Tab. 34) and imputation (Tab. 35) across different scenarios and settings on the ODW2V dataset. ", "page_idx": 31}, {"type": "text", "text": "Note that we only show the comparison between the proposed LM-WEATHER and the time series-specific baseline in the full experimental results. Our LM-WEATHER outperforms specialized time-series analysis models on on-device weather datasets across various environments. Unlike these models, our method doesn\u2019t require training from scratch but only minor adjustments to a small number of parameters. This validates the effectiveness and superiority of our proposed framework in on-device weather modeling practice. ", "page_idx": 31}, {"type": "text", "text": "Table 28: Comparison of the performance of LM-WEATHER and baselines on the ODW1T under forecasting tasks. Bold: the best, Underline: the second best. ", "page_idx": 31}, {"type": "table", "img_path": "llTroju97T/tmp/4054b3549af09476f6b11c9c2111d58b440b454e044204052bc0cebe0117bae1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 31}, {"type": "table", "img_path": "llTroju97T/tmp/0b4616c07ab7204a1bcabb82ca1f221c0395bd47d5c30b1d4232e43c7ee6f109.jpg", "table_caption": ["Table 29: Comparison of the performance of the proposed method and the baseline method on the ODW1T under the imputation task, where bold indicates the optimal results and underline indicates the sub-optimal results. "], "table_footnote": [], "page_idx": 31}, {"type": "table", "img_path": "llTroju97T/tmp/b1b5f15622ba847e4210b61e41fedead6de4958c3903962acda4bbcb85ed67c9.jpg", "table_caption": ["Table 30: Comparison of the performance of LM-WEATHER and baselines on the ODW1V under the long-term forecasting task. Bold: the best, Underline: the second best. "], "table_footnote": [], "page_idx": 32}, {"type": "table", "img_path": "llTroju97T/tmp/357c1666641003ca3b83bbe5c298d996937dac27b91c5c04f7bee8a88d913773.jpg", "table_caption": ["Table 31: Comparison of the performance of LM-WEATHER and baselines on the ODW1V dataset under the imputation task. Bold: the best, Underline: the second best. "], "table_footnote": [], "page_idx": 32}, {"type": "table", "img_path": "llTroju97T/tmp/c374cf41755a6f4484620e7e65dc67371975e5ae689e910356a7ad4acd0a6713.jpg", "table_caption": ["Table 32: Comparison of the performance of LM-WEATHER and baselines on the ODW2T under the long-term forecasting task. Bold: the best, Underline: the second best. "], "table_footnote": [], "page_idx": 32}, {"type": "table", "img_path": "llTroju97T/tmp/ae5607ca5d61213f2081c0aab5f65d9e847e8e787905f650fd68f60ce276f511.jpg", "table_caption": ["Table 33: Comparison of the performance of the proposed method and the baseline method on the ODW2T under the imputation task. Bold: the best, Underline: the second best. "], "table_footnote": [], "page_idx": 32}, {"type": "text", "text": "E.2 Full Few-Shot Learning Experiments ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "In this section, we show detailed and full few-shot learning experimental results including: ", "page_idx": 32}, {"type": "table", "img_path": "llTroju97T/tmp/534bfdd6dab838c72fc32775007d374883c7be19bd0751d5082b2b7ad629eadc.jpg", "table_caption": ["Table 34: Comparison of the performance of LM-WEATHER and the baseline method on the ODW2V under the forecasting task. Bold: the best, Underline: the second best. "], "table_footnote": [], "page_idx": 33}, {"type": "table", "img_path": "llTroju97T/tmp/ddf00efa354cb36af248ad94bda161afd39ae30a0f86ff773399ffb7d40a5137.jpg", "table_caption": ["Table 35: Comparison of the performance of LM-WEATHER and the baseline method on the ODW2V under the imputation task. Bold: the best, Underline: the second best. "], "table_footnote": [], "page_idx": 33}, {"type": "text", "text": "\u2022 Forecasting (Table. 36 for $5\\%$ training data, Table. 37 for $15\\%$ training data) and imputation (Table. 38 for $5\\%$ training data, Table. 39 for $15\\%$ training data) across different scenes and settings on the ODW1T dataset. ", "page_idx": 33}, {"type": "text", "text": "\u2022 Forecasting (Table. 40 for $5\\%$ training data, Table. 41 for $15\\%$ training data) and imputation (Table. 42 for $5\\%$ training data, Table. 43 for $15\\%$ training data) across different scenes and settings on the ODW1V dataset.   \n\u2022 Forecasting (Table. 44 for $5\\%$ training data, Table. 45 for $15\\%$ training data) and imputation (Table. 46 for $5\\%$ training data, Table. 47 for $15\\%$ training data) across different scenes and settings on the ODW2T dataset.   \n\u2022 Forecasting (Table. 48 for $5\\%$ training data, Table. 49 for $15\\%$ training data) and imputation (Table. 50 for $5\\%$ training data, Table. 51 for $15\\%$ training data) across different scenarios and settings on the ODW2V dataset. ", "page_idx": 33}, {"type": "table", "img_path": "llTroju97T/tmp/e09206711d381c67624c453662bfd89143306c061c037a2d0f732d6ddcf33e4a.jpg", "table_caption": ["Table 36: Comparison of the performance of LM-WEATHER with the baseline method on the ODW1T dataset under the long-term forecasting task in a scenario where the proportion of training data is set to be $5\\%$ in the few-shot learning. Bold: the best, Underline: the second best, \u201c-\u201c denotes insufficient training data. "], "table_footnote": [], "page_idx": 33}, {"type": "text", "text": "Experimental results indicate that our LM-WEATHER significantly outperforms the baseline in resourceconstrained situations, such as few-shot learning environments with limited training data. This suggests that LM-WEATHER effectively leverages PLMs for sequential data modeling and achieves commendable performance without requiring extensive data for training. ", "page_idx": 34}, {"type": "text", "text": "Table 37: Comparison of the performance of LM-WEATHER with baselines on the ODW1T under the forecasting task in a scenario where the proportion of training data is set to be $15\\%$ in the few-shot learning. Bold: the best, Underline: the second best, \u201c-\u201c denotes insufficient training data. ", "page_idx": 34}, {"type": "table", "img_path": "llTroju97T/tmp/601cb2ed9bee02228e607c714c04d38272f5ffbcc0ee2d0bb85c51a39dd5a8c4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 34}, {"type": "table", "img_path": "llTroju97T/tmp/c8a99eb9e4045881925bef74c80f85111de9fc81dec5770beed7ab91a265027e.jpg", "table_caption": ["Table 38: Comparison of the performance of LM-WEATHER with the baseline on the ODW1T under the imputation task in a scenario where the proportion of training data is set to be $5\\%$ in the few-shot learning. Bold: the best, Underline: the second best, \u201c-\u201c denotes insufficient training data. "], "table_footnote": [], "page_idx": 34}, {"type": "table", "img_path": "llTroju97T/tmp/88a90c079a4935798ce2ff513ad9dcf3ed014307137faee551fa47e58f3b2a98.jpg", "table_caption": ["Table 39: Comparison of the performance of the LM-WEATHER with the baseline on the ODW1T under the imputation task in a scenario where the proportion of training data is set to be $15\\%$ in the few-shot learning. Bold: the best, Underline: the second best, \u201c-\u201c denotes insufficient training data. "], "table_footnote": [], "page_idx": 34}, {"type": "table", "img_path": "llTroju97T/tmp/8204b075718a07165e887a49559122d2ca5b62856a061cde19d4ab9bb1881d09.jpg", "table_caption": ["Table 40: Comparison of the performance of LM-WEATHER with the baseline method on the ODW1V under the long-term forecasting task in a scenario where the proportion of training data is set to be $5\\%$ in the few-shot learning. Bold: the best, Underline: the second best, \u201c-\u201c denotes insufficient training data. "], "table_footnote": [], "page_idx": 35}, {"type": "table", "img_path": "llTroju97T/tmp/297413344e7e7e360ba52add91c963986bcfa6ab32a73a28d5d110fbb6558e9d.jpg", "table_caption": ["Table 41: Comparison of the performance of LM-WEATHER with baselines on the ODW1V under the forecasting task in a scenario where the proportion of training data is set to be $15\\%$ in the few-shot learning. Bold: the best, Underline: the second best, \u201c-\u201c denotes insufficient training data. "], "table_footnote": [], "page_idx": 35}, {"type": "table", "img_path": "llTroju97T/tmp/3b9d659a9393ae5970f2b144a3437caec8acc7425755db9a352c88027cdf5541.jpg", "table_caption": ["Table 42: Comparison of the performance of LM-WEATHER with baselines on the ODW1V under the imputation task in a scenario where the proportion of training data is set to be $5\\%$ in the few-shot learning. Bold: the best, Underline: the second best, \u201c-\u201c denotes insufficient training data. "], "table_footnote": [], "page_idx": 35}, {"type": "text", "text": "Table 43: Comparison of the performance of LM-WEATHER with baselines on the ODW1V under the imputation task in a scenario where the proportion of training data is set to be $15\\%$ in the few-shot learning. Bold: the best, Underline: the second best. ", "page_idx": 35}, {"type": "table", "img_path": "llTroju97T/tmp/6f6469cefc0e8dd91ee23c110c4db31622e48ba61458157937a28a82c118694a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 35}, {"type": "table", "img_path": "llTroju97T/tmp/faa25a644abd8e53242df12c183c730d646c8f13157bb3bbd2c14345cdae00fd.jpg", "table_caption": ["Table 44: Comparison of the performance of LM-WEATHER with baselines on the ODW2T under the forecasting task in a scenario where the proportion of training data is set to be $5\\%$ in the few-shot learning. Bold: the best, Underline: the second best, \u201c-\u201c denotes insufficient training data. "], "table_footnote": [], "page_idx": 36}, {"type": "table", "img_path": "llTroju97T/tmp/709ae0fde14d7a5b1aef5989930c1baf1a67ba8399f458fececc98171d8c5e01.jpg", "table_caption": ["Table 45: Comparison of the performance of LM-WEATHER with baselines on the ODW2T under the forecasting task in a scenario where the proportion of training data is set to be $15\\%$ in the few-shot learning. Bold: the best, Underline: the second best, \u201c-\u201c denotes insufficient training data. "], "table_footnote": [], "page_idx": 36}, {"type": "table", "img_path": "llTroju97T/tmp/30b6beb81202f11a2dcb21592f749fe9fe2f60ed66e400ca5a66215b2d2e336f.jpg", "table_caption": ["Table 46: Comparison of the performance of LM-WEATHER with the baseline on the ODW2T under the imputation task in a scenario where the proportion of training data is set to be $5\\%$ in the few-shot learning. Bold: the best, Underline: the second best, \u201c-\u201c denotes insufficient training data. "], "table_footnote": [], "page_idx": 36}, {"type": "table", "img_path": "llTroju97T/tmp/f6c16d95517e89d98ce6b189ac1092e331dc3ce3441f41bd7836c9824442965b.jpg", "table_caption": ["Table 47: Comparison of the performance of LM-WEATHER with the baselines on the ODW2T under the imputation task in a scenario where the proportion of training data is set to be $15\\%$ in the few-shot learning. Bold: the best, Underline: the second best, \u201c-\u201c denotes insufficient training data. "], "table_footnote": [], "page_idx": 36}, {"type": "table", "img_path": "llTroju97T/tmp/6bef9e4d2db5382f05f2abacd5bc8f900a56c1f3f598f3452d72ee6a3396eae0.jpg", "table_caption": ["Table 48: Comparison of the performance of LM-WEATHER with the baseline on the ODW2V under forecasting tasks in a scenario where the proportion of training data is set to be $5\\%$ in the few-shot learning. Bold: the best, Underline: the second best, \u201c-\u201c denotes insufficient training data. "], "table_footnote": [], "page_idx": 37}, {"type": "table", "img_path": "llTroju97T/tmp/022c9115cf2aa9c092f849bd007c68b1aea53cb0216d7a806664780b909db2b0.jpg", "table_caption": ["Table 49: Comparison of the performance of LM-WEATHER with baselines on the ODW2V under the long-term forecasting task in a scenario where proportion of training data is set to be $15\\%$ in the few-shot learning. Bold: the best, Underline: the second best, \u201c-\u201c denotes insufficient training data. "], "table_footnote": [], "page_idx": 37}, {"type": "table", "img_path": "llTroju97T/tmp/e996c19725c542b29119f217d7202eb23e4583e976d5a65a9b7aff1b3c89be19.jpg", "table_caption": ["Table 50: Comparison of the performance of LM-WEATHER with baselines on the ODW2V under the imputation task in a scenario where the proportion of training data is set to be $5\\%$ in the few-shot learning. Bold: the best, Underline: the second best, \u201c-\u201c denotes insufficient training data. "], "table_footnote": [], "page_idx": 37}, {"type": "table", "img_path": "llTroju97T/tmp/666ec0a393a87c28cba06dc960a1de1a7a8cca7383c6f02c929c6eab8e011028.jpg", "table_caption": ["Table 51: Comparison of the performance of LM-WEATHER with baselines on the ODW2V under the imputation task in a scenario where the proportion of training data is set to be $15\\%$ in the few-shot learning. Bold: the best, Underline: the second best, \u201c-\u201c denotes insufficient training data. "], "table_footnote": [], "page_idx": 37}, {"type": "text", "text": "E.3 Full Ablation Experiments ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "In this subsection, we show the results of the complete ablation experiment, both in the forecasting (Table. 52) and in imputation (Table. 53). ", "page_idx": 38}, {"type": "text", "text": "Table 52: Ablation experimental (forecasting) results for both the model composition level and the personalization mechanism level are included, where $\\uparrow$ represent the degree of performance increase relative to the original LM-WEATHER, $\\downarrow$ represent the degree of performance degradation, and the Comm. Param# represents the number of parameters transferred between client and server communication for the different variants. Bold: the best, Underline: the second best. ", "page_idx": 38}, {"type": "table", "img_path": "llTroju97T/tmp/9e4246249bf22880f428bdb478b7fc525353299eb415e1b38adfec511e74ac36.jpg", "table_caption": [], "table_footnote": [], "page_idx": 38}, {"type": "text", "text": "Table 53: Ablation experimental results (imputation) for both the model composition level and the personalization mechanism level are included, where $\\uparrow$ represent the degree of performance increase relative to the original LM-WEATHER, $\\downarrow$ represent the degree of performance degradation, and the Comm. Param# represents the number of parameters transferred between client and server communication for the different variants. Bold: the best, Underline: the second best. ", "page_idx": 38}, {"type": "table", "img_path": "llTroju97T/tmp/2c70915ae91ec86b0b806509299aa22ff40d892298186b1284c0c34316ac7bc6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 38}, {"type": "text", "text": "E.4 Hyper-parameter Sensitivity ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "The impacts of rank on performance are detailed in Table. 54. As the rank goes up, there\u2019s a consistent improvement, reaching its best at $r=8$ . However, when $r=12$ , there\u2019s a drop in performance. This happens because a higher rank means the local model has more trainable parameters, which can improve performance empirically. While a higher rank can cause increased communication cost and introduce more uncertainty. ", "page_idx": 38}, {"type": "text", "text": "E.5 Pre-trained Language Model Variants ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "We compare three representative PLM backbones with varying capacities, the result is shown in Table. 55. Under the proposed LM-WEATHER framework, it\u2019s evident that various PLM backbones maintain strong sequence modeling capabilities. Moreover, the lightweight personalized adapter in LM-WEATHER enhance the PLM\u2019s ", "page_idx": 38}, {"type": "text", "text": "Table 54: Results on parameter impact study, where Length refers to the length of weather sequences (that is, predicted horizons in forecasting and input sequence length in imputation). Avg. represents the average value of predicted horizons, encompassing $\\{96,192,336,720\\}$ . ", "page_idx": 39}, {"type": "table", "img_path": "llTroju97T/tmp/b636c73b8aa8c599a588cf7f7132608b0f86ceb03e794a75e62517d800a561ae.jpg", "table_caption": [], "table_footnote": [], "page_idx": 39}, {"type": "table", "img_path": "llTroju97T/tmp/ed7a6690ea1c8409715c2df42c2d1a2bf4dcfbcdfdefdd1152e83a35f384629c.jpg", "table_caption": ["Table 55: Performance statistics for the proposed LM-WEATHER with various PLM backbones are presented, recording only the average performance across all lengths for different datasets (namely, 96/192/336/720 prediction horizons). For the imputation task, results are documented solely for a random masking probability of $50\\%$ . Bold: the best, Underline: the second best. "], "table_footnote": [], "page_idx": 39}, {"type": "text", "text": "ability to transfer knowledge from natural language sequences to complex weather sequences. This further validates the superiority and versatility of our LM-WEATHER. ", "page_idx": 39}, {"type": "text", "text": "Appendix F Additional Statements ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "F.1 Impact Statements ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "We highlight that the goal of this study to proposed LM-WEATHER is not to compete but instead to complement current on-device meteorological variable modeling framework. Today\u2019s climate foundation models are typically trained from scratch, utilizing exceptionally large datasets (nearly 100TB) and incurring substantial computational costs. We hope that LM-WEATHER offers a cost-effective alternative for modeling meteorological variables on-device, thereby enabling accurate regional weather trend analysis. In addition, the dataset we complied can be the important resource to provide exploring chances for this field, facilitating future research. ", "page_idx": 39}, {"type": "text", "text": "This research seeks to make on-device meteorological variable modeling more efficient and adaptable. By using a PLM as a foundation model instead of training large foundation models from scratch, it eliminates the need for large-scale real weather data and extensive computational resources. Additionally, it supports a variety of devices, enabling everything from advanced smartphones to basic IoT sensors to perform meteorological variable modeling. The method is also designed to be stable in environments with limited data and those outside of typical distribution ranges, providing credible analytical support for further weather trend analyses. ", "page_idx": 39}, {"type": "text", "text": "F.2 Limitations ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Although our LM-WEATHER significantly outperforms models trained from scratch for time series analysis across various tasks and scenarios with minimal parameter tweaks, it still faces two primary limitations: ", "page_idx": 40}, {"type": "text", "text": "\u2022 Limited Dataset Scale: Due to constraints on computational resources and operational costs, we evaluated the performance of LM-WEATHER using the real-world datasets that did not approach the scale of tens of terabytes often required for training large-scale meteorological models. This limitation does not affect LM-WEATHER to be extended as a general framework for regional weather trend analysis. This framework supports the analysis of on-device meteorological variables and can be further developed and adapted for additional applications. ", "page_idx": 40}, {"type": "text", "text": "\u2022 Dependence on PLMs\u2019 Quality and Performance: Although LM-WEATHER leverages PLMs to achieve high efficiency and customization on heterogeneous devices, this dependency means that the quality and the performance of LM-WEATHER are intrinsically tied to the underlying PLMs. Should there be inherent limitations or biases within the PLMs, these could translate to the meteorological modeling performance. Conversely, if conditions allow the use of a more powerful LLM, LMWEATHER\u2019s performance can be significantly improved. This might give the community more opportunities to explore the future road-map. ", "page_idx": 40}, {"type": "text", "text": "F.3 Future Works ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "In future work, we aim to broaden the use of LM-WEATHER across more on-device variable modeling applications. We also plan to incorporate additional types of data, including satellite and radar imagery, as well as textual weather descriptions, to advance towards a more generalized approach to on-device meteorological variable modeling. ", "page_idx": 40}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "1. Claims ", "page_idx": 41}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: The main claims made in the main abstract and introduction has accurately reflected the paper\u2019s contributions and scope. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 41}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Justification: This paper has discussed the limitations of the work. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 41}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: This paper provide the full set of assumptions and a complete proof. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 41}, {"type": "text", "text": "", "page_idx": 42}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: This paper has fully disclosed all the information needed to reproduce the main experimental results. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 42}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Justification: This paper has provided access to the code, but due to the size of the dataset, additional releases are required. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 42}, {"type": "text", "text": "", "page_idx": 43}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: This paper has specified all the training and test details. Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.   \n\u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 43}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 43}, {"type": "text", "text": "Answer: [No] ", "page_idx": 43}, {"type": "text", "text": "Justification: The experiments in this paper were conducted under five different random seeds and averaged to from the final reprot. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 43}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: This paper has provided sufficient information on the computer resource. Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 43}, {"type": "text", "text": "", "page_idx": 44}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 44}, {"type": "text", "text": "Justification: This research is conducted in the paper conform, in every respect, with the NeurIPS code of Ethics. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 44}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 44}, {"type": "text", "text": "Justification: This paper has discussed both potential positive societal impacts and negative societal impacts of the work performed. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 44}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Justification: This paper has described the relevant safegurads adopted. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. \u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. ", "page_idx": 44}, {"type": "text", "text": "\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. \u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 45}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: This paper has noted the original owners of the assets used. Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 45}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Justification: This paper introduces new datasets and has been described in detail. Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 45}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 45}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 45}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 46}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 46}]