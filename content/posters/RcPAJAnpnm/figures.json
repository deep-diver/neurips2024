[{"figure_path": "RcPAJAnpnm/figures/figures_2_1.jpg", "caption": "Figure 1: The scenario demonstrating how IsCiL enhances continual imitation learning efficiency through retrievable skills: (i) Prototype-based skill incremental learning: despite the failure of \u03c4\u2081, skills are incrementally learned from the available demonstrations. In later stages, missing skills for \u03c4\u2081 are retrieved from other tasks, achieving the resolution of \u03c4\u2081 and illustrating the reversibility and efficiency of retrievable skills. (ii) Task-wise selective adaptation: IsCiL effectively retrieves relevant learned skills, facilitating rapid task adaptation.", "description": "This figure illustrates the IsCiL framework's two main components: prototype-based skill incremental learning and task-wise selective adaptation.  The left panel shows how IsCiL incrementally learns skills from incomplete demonstrations across multiple stages, even recovering from initial failures by retrieving relevant skills from other tasks later. The right panel demonstrates how IsCiL efficiently adapts to a completely new unseen task (\u03c4u) by selectively retrieving and utilizing previously learned skills.", "section": "3 Approaches"}, {"figure_path": "RcPAJAnpnm/figures/figures_5_1.jpg", "caption": "Figure 3: CiL scenarios including Complete, Semi-Complete, and Incomplete, categorized by sample utilization difficulty, based on the completeness of the demonstration for task performance: In Complete, each of the 20 CiL stages incrementally introduces new tasks featuring objects not encountered in the pre-training stage, along with full, comprehensive demonstrations for each task. In Semi-Complete, the first 10 stages are repeated twice, with tasks presented alongside incomplete demonstrations, where specific sub-goals are missing from the trajectories. In Incomplete, the same sequence of tasks from the Complete scenario is used, but all stages feature incomplete demonstrations, requiring the system to handle tasks with missing sub-goal trajectories.", "description": "This figure illustrates three different scenarios for continual imitation learning (CiL): Complete, Semi-complete, and Incomplete.  These scenarios differ in the completeness of the demonstrations provided for each task, ranging from complete demonstrations in the 'Complete' scenario to partially complete demonstrations in the 'Semi-complete' and 'Incomplete' scenarios. The 'Incomplete' scenario presents the most challenging condition, where significant parts of the demonstration are missing for many of the tasks. This difference in demonstration completeness is used to measure the impact of incomplete information on the algorithm's sample efficiency and task adaptation performance.", "section": "4.1 Environments and data streams"}, {"figure_path": "RcPAJAnpnm/figures/figures_8_1.jpg", "caption": "Figure 4: Comparison w.r.t. the number of rehearsals: The horizontal axis represents the amount of stored rehearsal data at each stage, while the vertical axis indicates goal-conditioned success rates (GC).", "description": "This figure compares the performance of IsCiL and an experience replay (ER) baseline across three different scenarios in the Evolving Kitchen environment. The x-axis shows the number of rehearsals per stage, while the y-axis shows the goal-conditioned success rate (GC).  The results indicate that IsCiL consistently outperforms ER across all scenarios and rehearsal amounts, showcasing its sample efficiency. In the Complete scenario, IsCiL maintains high performance even without rehearsals, while ER shows a decline in performance as the number of rehearsals increases. In the Semi and Incomplete scenarios, the advantage of IsCiL is even more pronounced.", "section": "4.6 Analysis"}, {"figure_path": "RcPAJAnpnm/figures/figures_8_2.jpg", "caption": "Figure 5: Comparison w.r.t. training resources: In all baselines, the plain bar graph represents FWT, while the bar graph with hatch marks represents AUC. The vertical axis indicates goal-conditioned success rates (GC).", "description": "This figure compares the performance of IsCiL and several baselines across different training resource levels (1%, 10%, 20%, 50%).  The performance metrics used are Forward Transfer (FWT) and Area Under the Curve (AUC), both measuring goal-conditioned success rates (GC).  The results show the computational efficiency of IsCiL, maintaining robust performance even with limited training data.", "section": "4.3 Overall performance : sample efficiency"}, {"figure_path": "RcPAJAnpnm/figures/figures_12_1.jpg", "caption": "Figure 6: Example of a multi-stage Meta-World environment in our continual imitation learning scenarios.", "description": "This figure shows a multi-stage Meta-World environment used in the continual imitation learning experiments.  The left side depicts the pre-training stage, where the robot arm interacts with a simplified set of objects.  The right side shows the continual imitation learning stages, where the environment complexity increases with the addition of new objects and tasks. This illustrates the non-stationary nature of the continual learning problem addressed in the paper.", "section": "A Environment and Data Stream Details"}, {"figure_path": "RcPAJAnpnm/figures/figures_18_1.jpg", "caption": "Figure 7: Evolving Kitchen-complete and Evolving Kitchen-incomplete training curves represent the cumulative task success rate up to a given stage. The goal conditioned success rate(GC) is scaled such that achieving success in all tasks by the final stage is represented as 100%. This result corresponds to the data presented in Table 1.", "description": "This figure shows the training curves for the \"Evolving Kitchen-complete\" and \"Evolving Kitchen-incomplete\" scenarios. The y-axis represents the cumulative task success rate (GC), scaled to 100% for complete success in all tasks at the final stage. The x-axis shows the training stage. The curves illustrate how the task success rates change over the training stages for various continual learning methods (IsCiL, TAIL-\u03c4, TAIL-g, L2M-g, L2M, Seq, EWC), allowing for comparison of their performance.", "section": "4.1 Environments and data streams"}, {"figure_path": "RcPAJAnpnm/figures/figures_19_1.jpg", "caption": "Figure 8: Visualization of Skill Retriever on the LIBERO-goal Scenario. Left: T-SNE visualization of the state space of the existing dataset for each stage. Middle: Visualization of the stages where skills retrieved by the Skill Retriever, after all CiL stages of learning. Right: Map showing the stages of each dataset and the retrieved skills. This demonstrates that the Skill Retriever can find skills capable of handling the given state, even in the LIBERO scenario. Additionally, in task-specific parts (a), it accurately retrieves the skills, and in parts showing similar behaviors (b), it shares skills.", "description": "This figure visualizes the performance of the Skill Retriever component of the IsCiL framework.  The left panel shows a t-SNE visualization of the state space for each stage of the LIBERO-goal dataset, highlighting the distribution of skills over time. The middle panel displays the skills retrieved by the Skill Retriever, demonstrating its ability to identify relevant skills across stages. Finally, the right panel shows a heatmap illustrating the accuracy of skill retrieval for different stages, revealing that the retriever successfully identifies and shares skills across various stages and tasks in the complex LIBERO environment.", "section": "C.3 Scalability"}]