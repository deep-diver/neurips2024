[{"type": "text", "text": "Incremental Learning of Retrievable Skills For Efficient Continual Task Adaptation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Daehee Lee\u2660,\u2662, Minjong Yoo\u2660, Woo Kyung $\\mathbf{Kim^{*}}$ , Wonje Choi\u2660, Honguk Woo\u2660 ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "\u2660Sungkyunkwan University \u2662Carnegie Mellon University {dulgi7245, mjyoo2, kwk2696, wjchoi1995, hwoo}@skku.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Continual Imitation Learning (CiL) involves extracting and accumulating task knowledge from demonstrations across multiple stages and tasks to achieve a multi-task policy. With recent advancements in foundation models, there has been a growing interest in adapter-based CiL approaches, where adapters are established parameter-efficiently for tasks newly demonstrated. While these approaches isolate parameters for specific tasks and tend to mitigate catastrophic forgetting, they limit knowledge sharing among different demonstrations. We introduce IsCiL, an adapter-based CiL framework that addresses this limitation of knowledge sharing by incrementally learning shareable skills from different demonstrations, thus enabling sample-efficient task adaptation using the skills particularly in non-stationary CiL environments. In IsCiL, demonstrations are mapped into the state embedding space, where proper skills can be retrieved upon input states through prototype-based memory. These retrievable skills are incrementally learned on their corresponding adapters. Our CiL experiments with complex tasks in Franka-Kitchen and MetaWorld demonstrate robust performance of IsCiL in both task adaptation and sampleefficiency. We also show a simple extension of IsCiL for task unlearning scenarios. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Lifelong agents such as home robots are required to continually adapt to new tasks in sequential decision-making situations by leveraging knowledge from past experiences. However, many realworld domains pose substantial challenges for these lifelong agents; the complexity and ever-changing nature of these tasks make it difficult for agents to constantly adapt, leading to difficulties in retaining knowledge and maintaining operational efficiency [1]. For instance, a home robot agent, operating within a single household, needs to continuously adapt, learning specific tasks in various areas such as cooking assistance in the kitchen or cleaning in the bathroom. At the same time, it is crucial that the agent not only retains but also improves its proficiency in the tasks it has previously learned, ensuring that it maintains consistent efficiency throughout the home. ", "page_idx": 0}, {"type": "text", "text": "For these lifelong agents, Continual Imitation Learning (CiL) has been explored, in which an agent progressively learns a series of tasks by leveraging expert demonstrations over time to achieve a multi-task policy. Yet, CiL often encounters practical challenges: (1) the high costs and inefficiencies associated with comprehensive expert demonstrations [2] that are required for imitation, (2) frequently shifting tasks in dynamic, non-stationary environments, and (3) privacy concerns [3] related to learning from expert demonstrations. In this context, CiL faces significant issues in terms of cost, adaptability, and privacy, complicating its implementation in real-world scenarios. ", "page_idx": 0}, {"type": "text", "text": "To address these challenges, our work focuses on incorporation of skill learning and fine-tuning in CiL, leveraging recent advancements in foundation models [4, 5]. These have been increased interests in continual task adaptation based on multiple adapters learned on a foundation model [6, 7]. The adapterbased learning approach allows for parameter isolation for individual tasks, thus enabling to mitigate catastrophic forgetting of previously learned knowledge in CiL. Motivated by this use of adapters, we develop IsCiL, a new adapter-based CiL framework that addresses the practical challenges of CiL aforementioned, by incrementally learning shareable skills from different demonstrations through multiple adapters. IsCiL facilitates sample-efficient task adaptation using the skills particularly in non-stationary CiL environments. ", "page_idx": 1}, {"type": "text", "text": "Specifically, in the IsCiL framework, a prototype-based skill incremental learning method is employed with a two-level hierarchy including smaller, more manageable adapters: skill retriever and skill decoder. The skill retriever is responsible for composing skills to complete given goal-reaching tasks. It utilizes skill prototypes, which are representative embeddings of skills, to retrieve the appropriate skill for input. The knowledge of each skill is contained within the adapter, which can modify its associated base model output. The skill decoder is responsible for producing short-horizon actions for state-skill pairs. ", "page_idx": 1}, {"type": "text", "text": "We evaluate IsCiL and several adapter-based continual learning baselines across scenario variations based on complex, long-horizon tasks in the Franka-Kitchen and Meta-World environments to assess sample efficiency, task adaptation, and privacy considerations. The baselines include adapter-based continual adaptation techniques as well as conventional continual imitation learning methods. Our results demonstrate that IsCiL achieves robust performance without requiring comprehensive expert demonstrations. This flexibility allows IsCiL to continually and efficiently adapt to varying sequences in different environments by leveraging any available expert data to learn useful skills, with tasks composed of diverse instructions and demonstrations. ", "page_idx": 1}, {"type": "text", "text": "In summary, the IsCiL framework enhances sample efficiency and task adaptation, effectively bridging the gap between adapter-based CiL approaches and the knowledge sharing across demonstrations. Comprehensive experiments demonstrate that IsCiL outperforms other adapter-based continual learning approaches in various CiL scenarios. ", "page_idx": 1}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Continual imitation learning. To tackle the problem of catastrophic forgetting in continual learning, numerous studies have employed rehearsal techniques [8, 9, 10, 11], which involve replaying past experiences to maintain performance on previously learned tasks. Another approach involves utilizing additional model parameters to progressively extend the model architecture [12, 13, 14, 15, 16]. These methods adapt the model\u2019s structure over time to accommodate new tasks. However, rehearsal techniques exhibit high variability in forgetting depending on the replay ratio and often demand substantial training to incorporate new knowledge [17]. Progressive models, on the other hand, require stage identification during evaluation and often overlook unseen tasks [13]. In this work, we propose a CiL framework that enables effective learning and expansion without requiring rehearsal and stage identification, leveraging pre-trained goal-based model knowledge. ", "page_idx": 1}, {"type": "text", "text": "Continual task adaptation with pre-trained models. Several recent works use pre-trained models, accumulating knowledge continually through additional Parameter Efficient Tuning (PET) modules such as adapters [18, 17, 19, 20, 21, 6, 22]. These methods enhance the flexibility and scalability of continual learning systems. However, they suffer from inaccurate matching between adapter selection and trained knowledge, leading to a misalignment between the knowledge learned during training and the knowledge used during evaluation [17, 20], which hinders overall performance. In the realm of sequential decision making, some studies have explored adapting pre-trained models. In [6], the state space of tasks is fully partitioned, restricting its applicability in more integrated environments. Meanwhile, [7] relies on comprehensive demonstrations for learning, which may be impractical in real-world scenarios. Our study aims to enhance task adaptation efficiency by using incrementally generalized skills with accurate matching on state space. ", "page_idx": 1}, {"type": "text", "text": "Skill adaptation. Reinforcement learning research has enhanced fast adaptation through skill exploration [23] and skill priors [24], focusing on improving sample efficiency with offline datasets. Despite these advancements, adapting fixed skill decoders to new environments remains challenging. To overcome these limitations, skill-based few-shot imitation learning methods have been developed ", "page_idx": 1}, {"type": "image", "img_path": "RcPAJAnpnm/tmp/91c0576da8c3cf7fe321a8a457a3f5a9ab3fac626ed45b231446a156c363bc13.jpg", "img_caption": ["(i) Prototype-based skill incremental learning "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 1: The scenario demonstrating how IsCiL enhances continual imitation learning efficiency through retrievable skills: (i) Prototype-based skill incremental learning: despite the failure of $\\tau_{1}$ , skills are incrementally learned from the available demonstrations. In later stages, missing skills for $\\tau_{1}$ are retrieved from other tasks, achieving the resolution of $\\tau_{1}$ and illustrating the reversibility and efficiency of retrievable skills. (ii) Task-wise selective adaptation: IsCiL effectively retrieves relevant learned skills, facilitating rapid task adaptation. ", "page_idx": 2}, {"type": "text", "text": "[25, 26]. However, these methods require extensive past data and struggle with scalability and generalization. Even skill-based approaches used in continual imitation learning [8] still require rehearsal data to mitigate knowledge loss and face difficulties addressing privacy issues through unlearning. Our IsCiL employs parameter-efficient skill adapters to prevent catastrophic forgetting and maintain efficiency, providing a scalable solution for unlearning. ", "page_idx": 2}, {"type": "text", "text": "3 Approaches ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Our work addresses three key challenges of CiL: (1) data inefficiency, (2) non-stationarity, and (3) privacy concerns, by adopting retrievable skills in the CiL context. Specifically, our IsCiL framework not only enhances data-efficient continual task evaluation in a non-stationary environment but also supports unlearning as a task adaptation strategy, thereby mitigating privacy concerns. ", "page_idx": 2}, {"type": "text", "text": "3.1 Problem formulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In CiL scenarios, we consider a data stream of task datasets $\\{\\mathcal{D}_{i}\\}_{i=1}^{p}$ , where $\\mathcal{D}_{i}$ contains an expert demonstration $\\mathcal{D}_{i}=\\{d_{i}^{1},...,d_{i}^{N}\\}$ for its associated task $\\tau_{i}$ . To effectively represent complex longhorizon tasks, each task $\\tau_{i}$ is comprised of sub-goal list, $\\tau\\,=\\,\\{g_{i}^{1},...,g_{i}^{M}\\}$ . Each task dataset is sampled in a finite-horizon markov decision process $(S,{\\mathcal{A}},{\\mathcal{P}},{\\mathcal{R}},\\mu_{0},H)$ , where $\\boldsymbol{S}$ is a state space, $\\boldsymbol{\\mathcal{A}}$ is a action space, $\\mathcal{P}$ is a transition probability, $\\mathcal{R}$ is a reward function, $\\mu_{0}$ is an initial state distribution, and $H$ is an environment horizon. ", "page_idx": 2}, {"type": "text", "text": "For demonstration $d\\,=\\,\\{(s_{t},a_{t})\\}_{t=1}^{H}$ , a state $s_{t}\\ \\in\\ S$ represents a tuple $\\left(o_{t},g_{t}\\right)$ consisting of an observation $o_{t}$ and a sub-goal $g_{t}$ . In our work, we represent sub-goals through language and use language-based goal embeddings for $g_{t}$ to achieve language-conditioned policies. Then, the objective of IsCiL is to obtain a multi-task policy $\\pi^{*}$ , by which the performance on the tasks in the data stream can be comparable to that of respective expert policies. This is formulated as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\pi^{*}=\\operatorname*{argmin}_{\\pi}\\left[\\mathbb{E}_{i}\\left[\\sum_{\\tau\\in T_{i}}{\\mathrm{KL}}(\\pi(\\cdot|s)\\|{\\tilde{\\pi}}_{\\tau}(\\cdot|s))\\right]\\right]\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\tilde{\\pi}_{\\tau}$ represents an expert policy for $\\tau$ and $\\mathcal{T}_{i}$ denotes a set of evaluation tasks at stage $i$ . In this context, the evaluation tasks continuously vary across different stages. ", "page_idx": 2}, {"type": "text", "text": "3.2 Overall architecture ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To effectively handle complicated CiL scenarios, we present the IsCiL framework which involves (i) prototype-based skill incremental learning and (ii) task-wise selective adaptation. ", "page_idx": 3}, {"type": "text", "text": "As illustrated in Figure 1, in (i) the prototype-based skill incremental learning, we use a two-level hierarchy structure with a skill retriever $\\pi_{R}$ composing the skills for each sub-goal, and a skill decoder $\\pi_{D}$ producing short-horizon actions based on state-skill pairs. For this two-level policy hierarchy, we employ a skill prototype-based approach, in which skill prototypes capture the sequential patterns of actions and associated environmental states, as observed from expert demonstrations. These prototypes serve as a reference for skills learned from a multi-stage data stream. Using these skill prototypes, we can effectively translate task-specific instructions or demonstrations into a series of appropriate skills. ", "page_idx": 3}, {"type": "text", "text": "Through this prototype-based skill retrieval method, the policy flexibly uses skills that are shareable among tasks, potentially learned in the past or future, for policy evaluation. This enables the CiL agent to effectively learn diverse tasks and rapidly adapt to variations, while incrementally accumulating skill knowledge from a multi-stage data stream. Furthermore, to facilitate sample-efficient learning and enhance stability in CiL, we employ parameter-efficient adapters that are continually fine-tuned on a base model. Each skill knowledge is encapsulated within a dedicated adapter and incorporated into the skill decoder $\\pi_{D}$ to infer expert actions. ", "page_idx": 3}, {"type": "text", "text": "In (ii) the task-wise selective adaptation, we devise efficient task adaptation procedures in the policy hierarchy to adapt to specific tasks using incrementally learned skills. This enables the CiL agent to not only facilitate adaptation to shifts in task distribution (e.g., due to non-stationary environment conditions) but also support task unlearning upon explicit user request (e.g., due to privacy concerns). ", "page_idx": 3}, {"type": "text", "text": "Suppose that the smart home environment undergoes an upgrade with the installation of new smart lighting systems throughout the house. In this case, task-wise selective adaptation can be used for rapid adaptation by removing outdated control routines associated with the previous systems. ", "page_idx": 3}, {"type": "text", "text": "3.3 Prototype-based skill incremental learning ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "State encoder and prototype-based skill retriever. To facilitate skill retrieval from demonstrations, we encode observation and goal pairs $\\left(o_{t},g_{t}\\right)$ into state embeddings $s_{t}$ using a function $f:(o_{t},g_{t})\\mapsto$ $s_{t}$ . We implement $f$ as a fixed function to ensure consistent retrieval results for learning efficiency, mitigating the negative effects of input distribution shifts. ", "page_idx": 3}, {"type": "text", "text": "To effectively handle the multi-modality of the state distribution in non-stationary environments, we employ a skill retriever $\\pi_{R}$ . For this, we use multifaceted skill prototypes $\\chi_{z}\\in\\mathcal{X}$ , where $\\mathcal{X}$ is the set of learned skill prototypes. These prototypes capture the sequential patterns of expert demonstrations associated with specific goal-reaching tasks. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\theta_{z}=\\pi_{R}(s_{t};\\mathcal{X})=h\\left(\\operatorname*{argmax}_{\\chi_{z}\\in\\mathcal{X}}\\!S(\\chi_{z},s_{t})\\right),\\mathrm{~where~}\\,S(\\chi_{z},s_{t})=\\operatorname*{max}_{b\\in\\chi_{z}\\in\\sin(b,s_{t})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here, $h:\\chi_{z}\\mapsto\\theta_{z}$ denotes a one-to-one function that maps each skill prototype $\\chi_{z}$ to its dedicated adapter parameters $\\theta_{z}$ , while the similarity function $S$ is defined as the maximum similarity between state $s$ and bases $b\\in\\chi_{z}$ . Each $\\chi_{z}$ consists of multiple bases (e.g., 20 bases), and each basis $b$ is a representative vector containing its corresponding centroid, shaped identically to the state $s_{t}$ . ", "page_idx": 3}, {"type": "text", "text": "Adapter conditioned skill decoder. To effectively use the knowledge of the pre-trained base model without forgetting, even in a non-stationary changing environment, the skill decoder is conditioned based on parameters. The skill decoder policy $\\pi_{D}\\big(\\hat{a}_{t}\\big|o_{t},g_{t};\\theta_{\\mathrm{pre}},\\theta_{z}\\big)$ operates with the skill adapter parameters $\\theta_{z}$ and the pre-trained base model $\\theta_{\\mathrm{pre}}$ , using the Low-Rank Adaptation [27]. ", "page_idx": 3}, {"type": "text", "text": "Skill incremental learning. To incrementally learn new retrievable skills, we update the skill prototype and adapter pair $(\\chi_{z^{*}},\\theta_{z^{*}})$ for a novel skill $z^{*}$ . The skill prototype $\\chi_{z^{*}}$ is created by dividing a dataset of a single skill into several clusters based on similarity. From each cluster, a representative value is extracted to serve as the basis $b$ , representing $z^{*}$ . We use the KMeans algorithm [28] to determine these bases, ensuring that the number of bases $|\\chi_{z}|$ adequately captures the diversity within the dataset of the novel skill. This multifaceted set of bases allows the skill prototype to capture an accurate multi-modal distribution of the skill represented in the state space, enabling effective retrieval as described in Eq. 2. In our experiment, $z^{*}$ is created for each sub-goal $g$ in the given dataset $\\mathcal{D}_{i}$ for each stage $i$ . ", "page_idx": 3}, {"type": "table", "img_path": "RcPAJAnpnm/tmp/c7d4cd6aa28bc68e1baecec05b84c14b2f2609d002cada410550cd931ce5af71.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 2: Overview of the IsCiL framework: (a) The prototype-based skill retriever sequentially utilizes a state encoder $f$ , multifaceted skill prototypes $\\mathcal{X}$ , and a skill adapter mapping function $h$ to identify the skill adapter $\\theta_{z}$ . (b) Skill incremental learning involves the initialization and updating of the skill prototype $\\chi_{z^{*}}$ and its corresponding adapter $\\theta_{z^{*}}$ . ", "page_idx": 4}, {"type": "text", "text": "The learning of the skill adapter is divided into two phases: initialization and update. During the initialization phase, $\\theta_{z^{*}}$ is initialized using existing skill adapters. Predictions with the existing skill dataset and skill prototypes $\\chi_{z}\\in\\mathcal{X}$ are used to identify the most frequently selected skill. Average scores are computed for each skill prototype using the dataset involved in training $z^{*}$ , as defined in Eq. 2. The skill with the highest average score, denoted as $\\bar{z}$ , is selected. Consequently, $\\boldsymbol{\\theta}_{z^{\\ast}}$ is initialized by $\\theta_{\\bar{z}}$ . Then, the initialized adapter is updated through the following imitation loss. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}(o_{t},g_{t},a_{t};\\theta_{z})=\\|a-\\pi_{D}(\\hat{a}_{t}\\mid o_{t},g_{t};\\theta_{\\mathrm{pre}},\\theta_{z})\\|,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The novel skill $z^{*}$ is incorporated into the learned prototypes $\\mathcal{X}\\leftarrow\\mathcal{X}\\cup\\chi_{z^{*}}$ , and the novel prototype and adapter pair $(\\chi_{z^{*}},\\theta_{z^{*}})$ updates the function $h$ for pair mapping. Figure 2 presents an overview of this methodology, along with the algorithm for incremental learning is detailed in Appendix B.1. ", "page_idx": 4}, {"type": "text", "text": "3.4 Task-wise selective adaptation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Task evaluation. Given the pre-trained model $\\theta_{\\mathrm{pre}}$ and learned skill prototypes $\\mathcal{X}$ , for given inputs $(o_{t},g_{t})$ from the environment, IsCiL performs the following evaluation process. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{a}_{t}\\sim\\pi_{D}(\\hat{a}_{t}\\ |\\ o_{t},g_{t};\\theta_{\\mathrm{pre}},\\theta_{z}),\\ \\mathrm{where}\\ \\ \\theta_{z}=\\pi_{R}(o_{t},g_{t};\\mathcal{X})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The evaluation process adapts to novel tasks and sub-goal sequences from the environment by modifying the goal $g_{t}$ . This adjustment enables the inference of appropriate current actions, in a manner of similar to handling learned tasks. For example, a kitchen robot tailored to a specific user\u2019s kitchen setup can continuously and instantly adapt to changes in recipes without additional training. ", "page_idx": 4}, {"type": "text", "text": "Task unlearning. To ensure privacy protection for incrementally learned skills, our architecture allows for task unlearning by removing task-specific skill prototypes and adapters. In IsCiL, the separation of skill adapters for each task facilitates easy tagging of task information on each skill. When an unlearning request is given with a task identifier $\\tau$ , the corresponding skill prototypes and adapters are removed. This approach ensures exceptionally efficient and effective unlearning, aligning with the strong unlearning strategies in continual learning discussed in [3]. ", "page_idx": 4}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "4.1 Environments and data streams ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To investigate the sample efficiency and adaptation performance, we construct complex CiL scenarios using diverse long-horizon tasks [29, 30, 31]. We then analyze the sample efficiency across different stages and tasks with three types of scenarios: Complete, Semi-complete, and Incomplete, depending on how the samples are utilized and shared. Each scenario consists of a pre-training stage followed by 20 CiL stages. Figure 3 illustrates these scenarios. ", "page_idx": 4}, {"type": "text", "text": "Evolving Kitchen. Evolving Kitchen is a data stream based on long-horizon tasks in the FrankaKitchen environment [29, 30]. Each task requires sequentially achieving four out of seven sub-goals. The scenario consists of a pre-training stage in the environment with only four objects: kettle, bottom burner, top burner, and light switch, followed by continual adaptation to tasks involving seven objects. ", "page_idx": 4}, {"type": "image", "img_path": "RcPAJAnpnm/tmp/a256a562d241e63e3f2b8791d35f619d1aca90d2a5bb911bd8ddd13104ebe3cc.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 3: CiL scenarios including Complete, Semi-Complete, and Incomplete, categorized by sample utilization difficulty, based on the completeness of the demonstration for task performance: In Complete, each of the $20~\\mathrm{CiL}$ stages incrementally introduces new tasks featuring objects not encountered in the pre-training stage, along with full, comprehensive demonstrations for each task. In Semi-Complete, the first 10 stages are repeated twice, with tasks presented alongside incomplete demonstrations, where specific sub-goals are missing from the trajectories. In Incomplete, the same sequence of tasks from the Complete scenario is used, but all stages feature incomplete demonstrations, requiring the system to handle tasks with missing sub-goal trajectories. ", "page_idx": 5}, {"type": "text", "text": "Evolving World. Evolving World is a data stream based on the Meta-World environment [31] with long-horizon tasks, similar to [32, 33, 34]. Each task requires sequentially achieving four out of eight sub-goals. The scenario consists of a pre-training stage in the environment with only four objects, followed by continual adaptation to an entire environment with all eight Meta-World objects. More detailed configurations are provided in Appendix A. ", "page_idx": 5}, {"type": "text", "text": "4.2 Baselines and metrics ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Baselines. We implement continual imitation learning and continual adaptation methods for sequential decision-making problems, which do not use rehearsal. First, we consider continual learning algorithms which involve full-model updates (Seq, EWC [35]). We also implement several continual adaptation approaches that utilize pre-trained models with adapters (L2M [6], TAIL [7]). L2M learns a key and adapter pair to modulate the pre-trained model, where the key is a retrievable state embedding similar to our prototypes. TAIL, unlike L2M, incrementally constructs task identifiers and corresponding adapters to modulate the pre-trained model with new task data without forgetting previous tasks. Each method is categorized based on the values used for adapter retrieval: a version that uses no additional identifiers, sub-goal identifiers (denoted as $-g$ ), and whole sub-goal sequences as single identifiers (denoted as $\\bullet\\tau$ ). Additionally, we include a Multi-task learning approach as an oracle baseline, which retains all incoming data at each stage and utilizes it for training in subsequent stages. For all baselines, we use the same pre-trained goal-conditioned policy and a diffusion model [36, 37] as the base policy architecture. A detailed description of the baselines and their hyperparameters are provided in Appendix B.2. ", "page_idx": 5}, {"type": "text", "text": "Metrics. We use three metrics to report CiL performance: Forward Transfer (FWT), Backward Transfer (BWT), and Area Under Curve (AUC) [38, 7]. In our long-horizon tasks, these metrics rely on goal-conditioned success rates (GC), which measure the ratio of successfully completed sub-goals to the total sub-goals within each task [39]. ", "page_idx": 5}, {"type": "text", "text": "\u2022 FWT (Forward Transfer): This evaluates the ability to learn tasks using previously learned knowledge. It is measured by the performance of a task when it occurs.   \n\u2022 BWT (Backward Transfer): This evaluates the impact of each learning stage on the performance of tasks learned in previous stages. It measures the change in task performance from past stages observed in the current stage.   \n\u2022 AUC (Area Under Curve): This represents the overall continual imitation learning performance in a scenario. It measures the average performance of tasks learned in the current stage over the remaining stages of the scenario. ", "page_idx": 5}, {"type": "text", "text": "For all metrics, higher values indicate better performance, with details provided in Appendix B.3. ", "page_idx": 5}, {"type": "text", "text": "Table 1: Overall performance on CiL scenarios of Evolving Kitchen and Evolving World: The rows represent baselines, categorized into sequential adaptation and adapter-based approaches, and oracle, respectively. The columns represent continual learning scenarios, where each scenario has 20 stages. Each scenario in the environment is categorized into Complete, Semi-complete, and Incomplete. The highest performance is highlighted in bold and the second highest performance is underlined. ", "page_idx": 6}, {"type": "table", "img_path": "RcPAJAnpnm/tmp/db7b47749f67af0d10b7caba3f76e0918e49c44f01da43cab68df2d97e21abe4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "4.3 Overall performance : sample efficiency ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Table 1 shows the CiL performance on Evolving Kitchen and Evolving World across three different scenarios (Complete, Semi, Incomplete). We compare the performance achieved by our framework IsCiL and other baselines (L2M, TAIL) with different conditioning values $(g,\\tau)$ for adapter retrieval. IsCiL consistently demonstrates superior performance in AUC across all scenarios, achieving between $84.5\\%$ and $97.2\\%$ of the oracle baseline (Multi-task learning). TAIL- $\\tau$ shows the most competitive performance in the Complete CiL scenario across both environments. However, due to its isolated adapter for learning and evaluation, it fails to effectively utilize samples across stages. ", "page_idx": 6}, {"type": "text", "text": "L2M and L2M- $^g$ exhibit relatively lower and less stable AUC in the Evolving Kitchen scenario. Conversely, in Evolving World-Semi, they surpass TAIL- $\\tau$ in AUC. This demonstrates that they are capable of sharing different skills across stages. Despite this, they still struggle with accurately retrieving the correct skill or suffer from performance degradation due to knowledge overwriting. Unlike them, IsCiL effectively mitigates overwriting by maintaining distinct skill representations across stages. Both L2M- $g$ and TAIL- $^g$ , which aim to leverage sub-goal labels for CiL, struggle to maintain performance due to skill distribution shifts, leading to catastrophic forgetting of skills for sub-goals. These challenges reveal that relying solely on sub-goal labels may not be sufficient to sustain and share skills effectively across different stages and tasks. ", "page_idx": 6}, {"type": "text", "text": "Both Seq-FT and Seq-LoRA struggle with forgetting. This is evident in the Complete scenario, where Seq-FT achieves the highest FWT but shows the lowest BWT, leading to a decline in overall performance. EWC exhibits consistently lower performance, as the regularization used to preserve past knowledge significantly hinders learning on current tasks, leading to severe degradation in long-horizon tasks. Although EWC shows higher BWT compared to other sequential tuning baselines, its low FWT limits overall effectiveness. ", "page_idx": 6}, {"type": "table", "img_path": "RcPAJAnpnm/tmp/90feadc931ab881d992dd0619aa6e059d7d469446715c556d65f8dde2bed3f97.jpg", "table_caption": ["Table 2: Task adaptation performance with unseen tasks: This is based on the existing Evolving World-Complete and Evolving Kitchen-Complete. In Evolving World, four novel tasks are introduced every four stages, while in Evolving Kitchen, two novel tasks are introduced every five stages. Metrics with the suffix -A denote performance based solely on adaptation tasks, while other metrics report performance across all tasks. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "RcPAJAnpnm/tmp/812982fbb24f933ad50e9f9e266a93b63a74e5e37158165c4e84f8d512eb1bb4.jpg", "table_caption": ["Table 3: Overall performance with task unlearning as task adaptation: Additional stages for unlearning tasks that were learned during other stages are included for tests. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.4 Task adaptation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Table 2 shows the unseen task adaptation ability of IsCiL, where only the sub-goal sequence of novel task is provided without demonstrations. This scenario extends the existing Complete CiL scenarios by periodically introducing novel tasks. Metrics labeled with the suffix -A indicate results from adaptation tasks, whereas the other metrics reflect performance on all tasks. For this scenario, we exclude TAIL- $\\tau$ from comparison, as it lacks the ability to adapt to novel tasks. ", "page_idx": 7}, {"type": "text", "text": "IsCiL demonstrates superior task performance in both scenarios, which contributes to greater efficiency in task adaptation. Moreover, in Evolving Kitchen, IsCiL not only demonstrates task adaptation ability by achieving the highest FWT-A, but also significantly enhances its initial performance, raising FWT-A from 52.1 to an AUC-A of 72.8. TAIL- $g$ shows comparable performance in FWT for the Evolving Kitchen. However, it struggles with catastrophic forgetting, leading to a $-34.9$ negative BWT when faced with significant distribution shifts in sub-goal demonstrations. In Evolving World, L2M, which actively learns to share skills during training, outperforms TAIL- $g$ . L2M is the only baseline achieving performance improvement on unseen tasks through CiL. ", "page_idx": 7}, {"type": "text", "text": "4.5 Task unlearning as adaptation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Table 3 measures CiL performance in scenario with task-level unlearning. For comparison, we use an adapter-based approach with parameter isolation-based continual learning private unlearning (CLPU) [3], extending TAIL to TAIL- $\\tau$ CLPU and IsCiL without skill adapter initialization. Similar to IsCiL, CLPU learns tasks in isolated models tagged with specific task identifiers and handles unlearning requests by removing the corresponding model parameters of the target task. Both TAIL- $\\tau$ CLPU and IsCiL ensure output distribution equality between the unlearned model and the model trained with the retained dataset. Thus, their CiL performance remains largely unaffected by unlearning. ", "page_idx": 7}, {"type": "text", "text": "Although IsCiL exhibits a slight performance degradation of $1.8\\%\\sim5.2\\%$ after unlearning, as reported in Table 1, it still demonstrates robustness by achieving a $115\\%$ higher AUC compared to TAIL- $\\tau$ CLPU in incomplete scenarios. ", "page_idx": 7}, {"type": "text", "text": "4.6 Analysis ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Rehearsal comparison. Figure 4 compares the sample efficiency to retain learned knowledge between IsCiL and a rehearsal-based continual imitation learning approach, Experience Replay (ER) [40]. For ER, we adjust the number of stored samples per learning stage, while IsCiL does not store rehearsals for training. IsCiL achieves the highest AUC in all environments and is the only approach where AUC surpasses FWT. ER shows comparable FWT in Complete, but as the number of stored samples increases, FWT decreases, indicating that more rehearsals actually reduce training sample efficiency. In Semi and Incomplete, using 250 rehearsals (approximately $5\\%$ of the stage dataset) yields FWT comparable to IsCiL but rarely improves AUC. ", "page_idx": 7}, {"type": "image", "img_path": "RcPAJAnpnm/tmp/ae19f64ed05e449f2b703acba901bdfac1c63c9401cd2ffc4bf66e8bf02d665e.jpg", "img_caption": ["Figure 4: Comparison w.r.t. the number of rehearsals: The horizontal axis represents the amount of stored rehearsal data at each stage, while the vertical axis indicates goal-conditioned success rates (GC). "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "RcPAJAnpnm/tmp/57cd75d857e19085d12b882c5101ae6e58332eeba8859783e0476bf1a77c7796.jpg", "img_caption": ["Figure 5: Comparison w.r.t. training resources: In all baselines, the plain bar graph represents FWT, while the bar graph with hatch marks represents AUC. The vertical axis indicates goal-conditioned success rates (GC). "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Limited training resource. Figure 5 shows the computational efficiency of IsCiL in resourceconstrained training settings, as discussed in [38]. In this experiment, the training resources is limited to $1\\%$ to $50\\%$ of those used in Table 1. IsCiL and TAIL- $\\tau$ show robust performance for varied training resources. TAIL- $^{\\prime}g$ shows higher FWT, as it trains the same sub-goal data on the same adapter, which excels in learning new tasks, but it fails to retain that knowledge. However, using skill data from different stages to update the same adapter makes it vulnerable to skill distribution shifts in CiL; this ends up with significant AUC degradation. ", "page_idx": 8}, {"type": "text", "text": "4.7 Ablation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Table 4 investigates the impact of the number of prototype bases on CiL performance, showing that increasing the number of bases improves both AUC and result stability, particularly around $K{=}10$ . Results are reported based on units ( $g$ and $\\tau$ ) used to construct new skill prototypes and the corresponding number of bases. IsCiL with a single base fails to effectively learn task knowledge, achieving similar performance to L2M in Table 1, due to insufficient representation of the skill distribution. Additionally, the IsCiL framework maintains positive BWT scores, ", "page_idx": 8}, {"type": "table", "img_path": "RcPAJAnpnm/tmp/a9e6a097e2377f0985880857c8dbc72a25873dc1869116338e17354bc6737fde.jpg", "table_caption": ["Table 4: Ablation on IsCiL skill prototype. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "demonstrating its ability to leverage future samples to enhance past performance. IsCiL with $\\tau$ , which constructs new skills based on entire task trajectories, required more bases in proportion to the increase in the number of transitions involved in constructing the skill trajectory to maintain stability. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this study, we presented the IsCiL framework to address key challenges in continual imitation learning (CiL). Our approach incorporates adapter-based skill learning, leveraging multifaceted skill prototypes and an adapter pool to effectively capture the distribution of skills for continual task adaptation. IsCiL specifies enhanced sample efficiency and robust task adaptation, effectively bridging the gap between adapter-based CiL approaches and the need for knowledge sharing across staged demonstrations. Comprehensive experiments demonstrate that IsCiL consistently outperforms other adapter-based continual learning approaches in various CiL scenarios. ", "page_idx": 9}, {"type": "text", "text": "Limitations. Like other adapter-based CiL approaches, IsCiL requires extra computation for evaluation, which can create overhead, especially in resource-constrained environments. It also depends on sub-goal sequences for training and evaluation, adding complexity and resource demands. Another limitation is determining the appropriate size of the adapter parameters, which depends on the performance of the pre-trained base model and the degree of task shift, making optimal adaptation challenging. Moreover, balancing the stability of the embedding function with the prototype size remains an area that requires further refinement to achieve optimal performance. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) RS-2022-II220043 (2022-0- 00043), Adaptive Personality for Intelligent Agents, RS-2022-II221045 (2022-0-01045), Self-directed multi-modal Intelligence for solving unknown, open domain problems, RS-2019-II190421, Artificial Intelligence Graduate School Program (Sungkyunkwan University), the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. RS-2023-00213118), BK21 FOUR Project (S-2024-0580-000) and by Samsung Electronics. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Byeonghwi Kim, Minhyuk Seo, and Jonghyun Choi. Online continual learning for interactive instruction following agents. In The Twelfth International Conference on Learning Representations (ICLR), 2024.   \n[2] Suneel Belkhale, Yuchen Cui, and Dorsa Sadigh. Data quality in imitation learning. In Advances in neural information processing systems (NeurIPS), 2023.   \n[3] Bo Liu, Qiang Liu, and Peter Stone. Continual learning and private unlearning. In Conference on Lifelong Learning Agents (CoLLAs), 2022.   \n[4] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. Advances in neural information processing systems (NeurIPS), 2021.   \n[5] Kuang-Huei Lee, Ofir Nachum, Mengjiao Sherry Yang, Lisa Lee, Daniel Freeman, Sergio Guadarrama, Ian Fischer, Winnie Xu, Eric Jang, Henryk Michalewski, et al. Multi-game decision transformers. Advances in neural information processing systems (NeurIPS), 2022.   \n[6] Thomas Schmied, Markus Hofmarcher, Fabian Paischer, Razvan Pascanu, and Sepp Hochreiter. Learning to modulate pre-trained models in rl. Advances in neural information processing systems (NeurIPS), 36, 2024.   \n[7] Zuxin Liu, Jesse Zhang, Kavosh Asadi, Yao Liu, Ding Zhao, Shoham Sabach, and Rasool Fakoor. TAIL: Task-specific adapters for imitation learning with large pretrained models. In The Twelfth International Conference on Learning Representations (ICLR), 2024.   \n[8] Weikang Wan, Yifeng Zhu, Rutav Shah, and Yuke Zhu. Lotus: Continual imitation learning for robot manipulation through unsupervised skill discovery. In 2024 IEEE International Conference on Robotics and Automation (ICRA), 2024.   \n[9] Philemon Schopf, Sayantan Auddy, Jakob Hollenstein, and Antonio Rodriguez-Sanchez. Hypernetworkppo for continual reinforcement learning. In Deep RL Workshop at NeurIPS, 2022.   \n[10] Chongkai Gao, Haichuan Gao, Shangqi Guo, Tianren Zhang, and Feng Chen. Cril: Continual robot imitation learning via generative and prediction model. In 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2021.   \n[11] Dibya Ghosh, Jad Rahme, Aviral Kumar, Amy Zhang, Ryan P Adams, and Sergey Levine. Why generalization in rl is difficult: Epistemic pomdps and implicit partial observability. Advances in neural information processing systems (NeurIPS), 2021.   \n[12] Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv preprint arXiv:1606.04671, 2016.   \n[13] Arun Mallya and Svetlana Lazebnik. Packnet: Adding multiple tasks to a single network by iterative pruning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.   \n[14] Xilai Li, Yingbo Zhou, Tianfu Wu, Richard Socher, and Caiming Xiong. Learn to grow: A continual structure learning framework for overcoming catastrophic forgetting. In International Conference on Machine Learning (ICML), 2019.   \n[15] Dushyant Rao, Francesco Visin, Andrei Rusu, Razvan Pascanu, Yee Whye Teh, and Raia Hadsell. Continual unsupervised representation learning. Advances in neural information processing systems (NeurIPS), 2019.   \n[16] Tiantian Zhang, Zichuan Lin, Yuxing Wang, Deheng Ye, Qiang Fu, Wei Yang, Xueqian Wang, Bin Liang, Bo Yuan, and Xiu Li. Dynamics-adaptive continual reinforcement learning via progressive contextualization. IEEE Transactions on Neural Networks and Learning Systems, 2023.   \n[17] Zifeng Wang, Zizhao Zhang, Sayna Ebrahimi, Ruoxi Sun, Han Zhang, Chen-Yu Lee, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer Dy, et al. Dualprompt: Complementary prompting for rehearsal-free continual learning. European Conference on Computer Vision (ECCV), 2022.   \n[18] Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruoxi Sun, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer Dy, and Tomas Pfister. Learning to prompt for continual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022.   \n[19] Qiankun Gao, Chen Zhao, Yifan Sun, Teng Xi, Gang Zhang, Bernard Ghanem, and Jian Zhang. A unified continual learning framework with general parameter-efficient tuning. International Conference on Computer Vision (ICCV), 2023.   \n[20] James Seale Smith, Leonid Karlinsky, Vyshnavi Gutta, Paola Cascante-Bonilla, Donghyun Kim, Assaf Arbelle, Rameswar Panda, Rogerio Feris, and Zsolt Kira. Coda-prompt: Continual decomposed attentionbased prompting for rehearsal-free continual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023.   \n[21] Wei-Cheng Huang, Chun-Fu Chen, and Hsiang Hsu. OVOR: Oneprompt with virtual outlier regularization for rehearsal-free class-incremental learning. In The Twelfth International Conference on Learning Representations (ICLR), 2024.   \n[22] Martin Wistuba, Prabhu Teja Sivaprasad, Lukas Balles, and Giovanni Zappella. Continual learning with low rank adaptation. In NeurIPS 2023 Workshop on Distribution Shifts (DistShifts), 2023.   \n[23] Seohong Park, Kimin Lee, Youngwoon Lee, and Pieter Abbeel. Controllability-aware unsupervised skill discovery. In Proceedings of the 40th International Conference on Machine Learning (ICML), 2023.   \n[24] Karl Pertsch, Youngwoon Lee, and Joseph J. Lim. Accelerating reinforcement learning with learned skill priors. In Conference on robot learning (CoRL), 2020.   \n[25] Kourosh Hakhamaneshi, Ruihan Zhao, Albert Zhan, Pieter Abbeel, and Michael Laskin. Hierarchical few-shot imitation with skill transition models. In International Conference on Learning Representations (ICLR), 2022.   \n[26] Soroush Nasiriany, Tian Gao, Ajay Mandlekar, and Yuke Zhu. Learning and retrieval from prior data for skill-based imitation learning. In Conference on robot learning (CoRL), 2022.   \n[27] Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations (ICLR), 2022.   \n[28] S. Lloyd. Least squares quantization in pcm. IEEE Transactions on Information Theory, 1982.   \n[29] Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020.   \n[30] Abhishek Gupta, Vikash Kumar, Corey Lynch, Sergey Levine, and Karol Hausman. Relay policy learning: Solving long-horizon tasks via imitation and reinforcement learning. arXiv preprint arXiv:1910.11956, 2019.   \n[31] Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. In Conference on robot learning (CoRL), 2020.   \n[32] Suraj Nair, Eric Mitchell, Kevin Chen, Silvio Savarese, Chelsea Finn, et al. Learning language-conditioned robot behavior from offline data and crowd-sourced annotation. In Conference on robot learning (CoRL), 2022.   \n[33] Divyansh Garg, Skanda Vaidyanath, Kuno Kim, Jiaming Song, and Stefano Ermon. Lisa: Learning interpretable skill abstractions from language. Advances in neural information processing systems (NeurIPS), 2022.   \n[34] Sangwoo Shin, Daehee Lee, Minjong Yoo, Woo Kyung Kim, and Honguk Woo. One-shot imitation in a non-stationary environment via multi-modal skill. In International Conference on Machine Learning (ICML), 2023.   \n[35] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 2017.   \n[36] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in neural information processing systems (NeurIPS), 2020.   \n[37] Zhendong Wang, Jonathan J Hunt, and Mingyuan Zhou. Diffusion policies as an expressive policy class for offilne reinforcement learning. In International Conference on Learning Representations (ICLR) 11, 2023.   \n[38] Bo Liu, Yifeng Zhu, Chongkai Gao, Yihao Feng, qiang liu, Yuke Zhu, and Peter Stone. LIBERO: Benchmarking knowledge transfer for lifelong robot learning. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023.   \n[39] Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. Alfred: A benchmark for interpreting grounded instructions for everyday tasks. In Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition (CVPR), 2020.   \n[40] Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan, Puneet K Dokania, Philip HS Torr, and Marc\u2019Aurelio Ranzato. On tiny episodic memories in continual learning. arXiv preprint arXiv:1902.10486, 2019.   \n[41] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 2017. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Environment and Data Stream Details ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1 Franka Kitchen ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We conduct experiments on the Franka kitchen environment [29, 30]. Each Franka kitchen task comprises of 4 sub-goals, from total pool of 7: microwave, kettle, bottom burner, top burner, light switch, slide cabinet, and hinge cabinet. Observation is a 60-dimensional vector, which is a combination of the positions and velocities of 7-DoF robot arm and interacting objects. We express sub-goal information using language embedding. The target sub-goal of the current state is acquired by a pre-defined environmental reward and task, and a sub-goal sequence to solve. We use 24 tasks in the \u2019mixed\u2019 dataset from the D4RL [29]. In the pre-training stage, we train the model only on tasks comprised of following four sub-goals: kettle, bottom burner, top burner, light switch. ", "page_idx": 12}, {"type": "image", "img_path": "RcPAJAnpnm/tmp/ff634f2b49e90225b95275345ae6e2bc6028707550fc0e258717b7f9e0a66936.jpg", "img_caption": [], "img_footnote": [], "page_idx": 12}, {"type": "text", "text": "Figure 6: Example of a multi-stage Meta-World environment in our continual imitation learning scenarios. ", "page_idx": 12}, {"type": "text", "text": "A.2 Multi-stage Meta World ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We conduct experiments on the multi-stage variation of the Meta-World environment [31, 34]. Each Meta-World task comprises of 4 sub-goals from total pool of 8: puck, box, handle, drawer, lever, button, door, and stick. The environments are divided into different scenarios based on which 4 out of 8 objects are placed on the table. For each environment, tasks are defined according to the sequence in which the 4 sub-goals must be achieved. Observation is a 140-dimensional vector, which contains the positions and velocities of the 4-DoF robot arm and all interacting objects in the environment. In this environment, sub-goal information is also expressed using language embedding. The expert dataset is collected using a heuristic expert policy provided by Meta-World [31]. In the pre-training stage of Meta World, we train the model on 24 tasks on a environment consisting of four objects: a puck, a drawer, a button, and a door. ", "page_idx": 12}, {"type": "text", "text": "A.3 Data Stream ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Evolving Kitchen Tables 5 and 6 display the detailed configurations of the Evolving Kitchen in our CiL scenario. Each task involves sequentially solving its respective sub-goals. The underlined sub-goals (e.g., kettle) are those missing in the Semi Complete and Incomplete scenarios. ", "page_idx": 12}, {"type": "text", "text": "Evolving World Tables 7 and 8 display the detailed configurations of our Evolving World CiL scenario. Similarly, the Evolving World is also presented in the same way as the Evolving Kitchen. ", "page_idx": 12}, {"type": "text", "text": "Unseen Task Adaptation Tables 9 and 10 show the detailed configurations of unseen tasks for our Evolving Kitchen-Complete Unseen and Evolving World-Complete Unseen. In the Evolving Kitchen, 2 new tasks appear every 5 stages. In the Evolving World, 4 new tasks appear every 4 stages. Each new task includes only the sequence of sub-goals that must be completed in order, without any demonstrations. ", "page_idx": 12}, {"type": "text", "text": "Table 5: Evolving Kitchen-Complete & Incom- Table 6: Evolving Kitchen-Semi data stream task plete data stream task configuration configuration ", "page_idx": 13}, {"type": "table", "img_path": "RcPAJAnpnm/tmp/8420243e4bf2501abb92ae0ce55354badf8df1d6d864ccaef05784f5171bae2a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "Table 8: Evolving World-Semi data stream task configuration ", "page_idx": 13}, {"type": "table", "img_path": "RcPAJAnpnm/tmp/ea69b0504b9342e6c34c0bef1831274ef426d8bfd5473f0bc72ca0481bb0ae80.jpg", "table_caption": ["Table 7: Evolving World-Complete & Incomplete data stream task configuration "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "Unlearning Scenario In the Unlearning Scenario, 1 learned task is unlearned every 5 learning stages.   \nIn the Evolving Kitchen Unlearning scenario, $\\tau_{4},\\,\\tau_{8}$ , $\\tau_{13}$ , and $\\tau_{17}$ are sequentially unlearned. ", "page_idx": 13}, {"type": "text", "text": "B Experiment Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "B.1 IsCiL Implementation ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "IsCiL consists of two modules: a skill retriever, $\\pi_{R}$ , and a skill decoder, $\\pi_{D}$ . The skill retriever $\\pi_{R}$ includes three components: a state encoder $f$ , skill prototypes $\\mathcal{X}$ , and a skill adapter mapping function $h$ . Each skill prototype $\\chi_{z}$ in $\\mathcal{X}$ is composed of 20 bases $b$ . To modulate skill decoder $\\pi_{D}$ , we use Low Rank Adaptation(LoRA) [27]. In our experiment, we used 4-rank LoRA adapters for skill adapter. IsCiL training and evaluation process follows : ", "page_idx": 13}, {"type": "text", "text": "Table 10: Evolving World-Complete Unseen task configuration ", "page_idx": 14}, {"type": "table", "img_path": "RcPAJAnpnm/tmp/119cfe5506747ac301a3fb1efb07def507b2f6635d440c31d8f829e155a947e3.jpg", "table_caption": ["Table 9: Evolving Kitchen-Complete Unseen task configuration "], "table_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "RcPAJAnpnm/tmp/595bc4f293e2ee1db501652521b0c4380f323aa839165c68e22e10d5a5ded362.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "Algorithm 1 IsCiL Skill Incremental Learning ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "1: State encoding function $f$ , Skill retriever $\\pi_{R}$   \n2: Skill decoder $\\pi_{D}$ , Pre-trained parameter $\\theta_{\\mathrm{pre}}$   \n3: Skill adapter mapping function $h$   \n4: for each stage $i$ in CiL Stages do   \n5: for each sub-goal $g$ in task dataset $D_{i}$ do   \n6: $D_{i}^{g}\\gets\\{(o,g^{\\prime})\\in D_{i}$ | $g^{\\prime}=g\\}$ // filter transitions related to the current sub-goal $g$   \n7: $S_{i}^{\\bar{g}}\\;\\leftarrow\\;\\overline{{{\\{f(o_{t},g_{t})\\;\\;|\\;\\;(o_{t},g_{t})\\;\\;\\in\\;D_{i}^{g}\\}}}}\\;,$ // encode states from the filtered dataset into state embeddings   \n8: $\\begin{array}{r}{\\mathcal{X}^{g}\\leftarrow\\{\\mathrm{argmax}_{\\chi_{z}\\in\\mathcal{X}}\\,S(\\chi_{z},s_{t})\\mid s_{t}\\in S_{i}^{g}\\}\\,/\\!,}\\end{array}$ retrieve the most relevant skill prototypes for each state $s_{t}$   \n9: $\\chi_{\\bar{z}}\\leftarrow\\mathbf{Mode}(\\mathcal{X}^{g})\\,//$ select the most frequently retrieved skill prototype from the set   \n10: $\\theta_{z^{*}}\\gets h(\\chi_{\\bar{z}})\\,/,$ / map the selected skill prototype $\\chi_{\\bar{z}}$ to its skill adapter via $h$   \n11: Update $\\theta_{z^{*}}$ using Eq. (3) $//$ update the skill adapter based on task-specific learning   \n12: $\\mathcal{X}\\leftarrow\\mathcal{X}\\cup\\chi_{z^{*}}\\mathrm{~//~}$ append the new skill prototype to the skill set for future retrieval   \n13: Update the mapping function $h$ to map $\\chi_{z^{*}}$ to the updated adapter $\\theta_{z^{\\ast}}$ // update $h$ with the new skill adapter end for ", "page_idx": 14}, {"type": "text", "text": "14: 15: end for ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Algorithm 2 IsCiL Evaluation ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "1: State encoding function $f$ , Skill retriever $\\pi_{R}$   \n2: Skill decoder $\\pi_{D}$ , Pre-trained parameter $\\theta_{\\mathrm{pre}}$   \n3: while not done do   \n4: $s_{t}=f(o_{t},g_{t})\\,/$ / encode state   \n5: $\\theta_{z}=\\pi_{R}(s_{t})\\,,$ // retrieve skill   \n6: $\\hat{a}_{t}\\sim\\pi_{D}\\big(o_{t},g_{t};\\theta_{\\mathrm{pre}},\\theta_{z}\\big)\\,,$ // decode the skill   \n7: end while ", "page_idx": 14}, {"type": "text", "text": "B.2 Baselines ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Seq-FT & Seq-LoRA Sequential Fine Tuning(Seq-FT) is a method that updates the entire model sequentially. The variation, Seq-LoRA, is used to determine how effectively the fixed pre-trained model can utilize its knowledge. Due to poor performance at very low ranks, Seq-LoRA was trained with a 64-rank adapter in our environment. ", "page_idx": 15}, {"type": "text", "text": "EWC[41] Elastic Weight Consolidation (EWC) regularizes the weight update by using the Fisher information matrix for each network parameter. For our experiment, we adopted the online version of EWC, which updates the Fisher information at each stage by exponential moving average, following the methods in [38, 7].We use the hyperparameter alpha, set to 10, to determine the regularization strength. For updating the online Fisher information matrix $\\bar{F}_{i}$ , we use the Fisher information matrix calculated at the current stage and apply the following formula for regularization: $\\bar{F}_{i}=\\gamma F_{i-1}+$ $(1-\\gamma)F_{i}$ , where $\\gamma$ is set to 0.9. ", "page_idx": 15}, {"type": "text", "text": "L2M[6] L2M is an adapter-based continual learning method consisting of keys and their corresponding adapters. When an input is provided, L2M uses a similarity function to search for the key corresponding to that input. Input is converted to a query and utilized to search for a key, where key is a vector with the same shape as the query. Each key is then updated to maximize its similarity to the data point associated with it. Finally, the data is used to update the adapter that corresponds to the key value found through that data. This method maximizes the diversity of key usage frequency by adjusting the similarity between keys and input values during the training phase for learning new tasks [18, 6]. In our implementation, we use the normalized state value as the input query to find the key in L2M. For L2M- $^g$ , we use the normalized embedding of the state concatenated with the given conditioned sub-goal information directly as a query. Our adapter pool consists of 100 adapters, each being a 4-rank LoRA adapter. ", "page_idx": 15}, {"type": "text", "text": "TAIL[7] TAIL directly assigns an adapter to the given task using the task\u2019s identifier. We directly map the given identifier to the corresponding adapter. TAIL- $g$ uses a 4-rank adapter, while TAIL- $\\tau$ uses a 16-rank adapter. ", "page_idx": 15}, {"type": "text", "text": "Multi-task At each stage, the model learns from the given data and stores all data for the next stage. The data stored in the buffer is mixed with the data from each stage in a 1:1 ratio for training the model. ", "page_idx": 15}, {"type": "text", "text": "ER[40] Experience Replay (ER), similarly, retains knowledge by storing a subset of the current stage\u2019s data for the next stage. The data stored in the buffer is mixed with the data from each stage in a 1:1 ratio for training the model. ", "page_idx": 15}, {"type": "text", "text": "CLPU[3] Continual Learning Private Unlearning (CLPU) [3] is a method for managing continual learning and unlearning. In a continual learning scenario, tasks that require maintenance are trained using existing models, while data that may require unlearning is trained on independent model parameters, tagged with when and through which task each model was trained. When an unlearning request for a specific task or training stage is received, the corresponding model parameters are completely removed to eliminate the influence of the target unlearning task from the model. CLPU provides highly efficient and powerful unlearning performance with a single delete operation for tasks learned in a continual learning scenario. In our experiment, we integrate the unlearning approach CLPU with TAIL- $\\tau$ , which conducts training through task information-based searches, to handle unlearning requests in continual imitation learning as a comparative method. ", "page_idx": 15}, {"type": "text", "text": "B.3 Metric ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We report 3 metrics for CiL performance for tasks: Forward Transfer(FWT), Backward Transfer(BWT), Area Under Curve(AUC) [38, 7]. In multi-stage environment task, we report performance using the goal-conditioned success rates (GC), which evaluate the average success rate of successfully completed sub-goals out of $N$ sub-goals in the task. ", "page_idx": 15}, {"type": "text", "text": "\u2022 FWT: $\\begin{array}{r}{\\mathrm{FWT}_{\\tau}=\\frac{1}{|I_{\\tau}|}\\sum_{i\\in I_{\\tau}}C_{\\tau,i}}\\end{array}$ where $\\tau$ is task and $C_{\\tau,i}$ represents the GC score of task $\\tau$ at stage $i$ . and $I_{\\tau}$ is set of stage indices where task $\\tau$ is trained in the CiL scenario. \u2022 BWT: $\\begin{array}{r}{{\\mathrm{BWT}_{\\tau}}=\\frac{1}{|I_{\\tau}|}\\sum_{i\\in I_{\\tau}}\\left(\\frac{1}{p-i-1}\\sum_{j=i+1}^{p}(C_{\\tau,j}-C_{\\tau,i})\\right)}\\end{array}$ , where $p$ is the final stage at which task $\\tau$ is available. In the case where $p=i$ BWT is $0$ . ", "page_idx": 15}, {"type": "text", "text": "\u2022 AUC: $\\begin{array}{r}{\\mathrm{AUC}_{\\tau}\\,=\\,\\frac{1}{|I_{\\tau}|}\\sum_{i\\in I_{\\tau}}\\left(\\frac{1}{p-i}\\sum_{j=i}^{p}C_{\\tau,j}\\right)}\\end{array}$ , represent the the overall performance of continual learning, internally including FWT and BWT. In the case where $p=i$ , $\\mathrm{AUC}_{\\tau}$ is $\\mathrm{FWT}_{\\tau}$ . ", "page_idx": 16}, {"type": "text", "text": "The final reported metric is the average across all tasks $\\tau\\in\\mathcal{T}$ . For all metrics, higher values indicate better performance. ", "page_idx": 16}, {"type": "text", "text": "B.4 Scenario training details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Pre-trained Base Model and Stage Settings Table 11 shows the hyperparameters and the architecture of the model we used as the base model for all baselines. Table 12 shows the common hyperparameters used to train the model for each stage in our experiments. ", "page_idx": 16}, {"type": "table", "img_path": "RcPAJAnpnm/tmp/263368f1d6913e8486cd1a3ac5227e392fc68fab6439f135b17873ec1a62e869.jpg", "table_caption": ["Table 11: Pre-trained model configure "], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "RcPAJAnpnm/tmp/0b041e8c9d522242b00b97ef45b4decd2b30d4a6b9d49ddef6ef34496fcdeafc.jpg", "table_caption": ["Table 12: Continual imitation learning default hyperparameters "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Pre-trained model performance Table 13 shows the learning performance of the pre-trained model and its adaptation performance for tasks learned in scenarios without any prior training. In Evolving World, where new objects are added and the environment changes significantly, the pre-trained model failed to successfully complete any sub-goals of tasks. ", "page_idx": 16}, {"type": "table", "img_path": "RcPAJAnpnm/tmp/c639f1243a324af3ddbc7f66eee9c4821f7e52ea13b4464224bc4c51ea41fccd.jpg", "table_caption": ["Table 13: Pre-trained model performance "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "B.5 Compute Resources ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Computing machine Our experimental platform is powered by an AMD 5975wx CPU and $2\\mathbf{x}$ RTX 4090 GPUs. The operating system used is Ubuntu 22.04.4 LTS, with Nvidia driver version 535.171.04 and CUDA version 12.2. ", "page_idx": 16}, {"type": "text", "text": "Software Detail We utilized jax 0.4.24, jaxlib 0.4.19, and flax 0.8.2 for our implementation. ", "page_idx": 16}, {"type": "text", "text": "Training time In the context of the Evolving Kitchen, each scenario involves training with three different seeds. The training duration averages 2 minutes per stage, with each stage consisting of 5000 epochs. Each scenario comprises 20 such stages, culminating in a total training time of 2 hours for a single experiment. ", "page_idx": 16}, {"type": "text", "text": "C Additional Experiments ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "C.1 Main experiment extension ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Training curve. Figure 7 shows training curves of Evolving-kitchen Complete and Incomplete on Table 1. The curves provide a clear illustration of the performance progression of IsCiL and baseline methods, making changes in key metrics over the course of training easily observable. ", "page_idx": 16}, {"type": "text", "text": "Skill adapter rank. Table 14 shows the results of the ablation study on the performance of CiL based on the rank of the skill adapter. Overall, the 1-rank adapter in Evolving Kitchen demonstrates sufficient, or even superior, adaptation performance. However, in Evolving World, the 1-rank adapter leads to lower overall performance, indicating that some skills cannot be fully learned with a 1-rank adapter, resulting in a decline in performance. ", "page_idx": 17}, {"type": "text", "text": "Skill decoder pre-trained model quality. Table 15 shows the results of the ablation study on performance changes based on the quality of the pre-trained model (skill decoder). The quality of the pre-trained model varies with the number of objects included in the tasks used to pre-train the model. A decrease in the quality of the pre-trained model leads to a performance drop in both TAIL- $\\tau$ and IsCiL, as the number of objects is reduced from 4 to 1. ", "page_idx": 17}, {"type": "text", "text": "Scenario task sequence variation. Table 16 shows the results of the task sequence variation analysis. We report the average performance for four different task sequences in Evolving Kitchen-Complete. The performance of all tasks at the final stage is not significantly affected. Since TAIL- $\\tau$ learns independently for each task ID, there was no performance change with different sequences, and IsCiL also showed similar performance, indicating that task sequence variation had minimal impact on overall outcomes. ", "page_idx": 17}, {"type": "text", "text": "Computational efficiency. In our framework, skill retrieval and adaptation occur at each time step. Despite this continuous process, the impact on inference time and computational demands is minimal. Through our implementation on JAX, we observed that factors like compile optimization had a more significant effect on performance than model size. As a result, IsCiL demonstrates fast evaluation times, with retrieval and adaptation processes taking 3.6ms and $3.0\\mathrm{ms}$ , respectively, which ensures that IsCiL remains highly efficient during inference. ", "page_idx": 17}, {"type": "text", "text": "Additionally, the memory overhead required for the adapted model is minimal, with the skill adaptation adding only $0.37\\%$ to $1.48\\%$ additional parameters compared to the pre-trained model, depending on the LoRA rank (1 to 4). For skill retrieval, the parameter size of each skill prototype is relatively small, accounting for approximately $0.3\\%$ of the total model size. Furthermore, the inclusion of adapters in the skill decoder only increases the FLOPs by $3.13\\%$ of the pre-trained model, demonstrating that the retrieval and adaptation processes are computationally efficient and have a negligible impact on resource consumption. ", "page_idx": 17}, {"type": "table", "img_path": "RcPAJAnpnm/tmp/7db9979a734c7b7f38c1aa737e4e057f0682a2ce8a1439a1d791f26647957aa3.jpg", "table_caption": ["Table 14: Ablation study on the skill adapter rank in Evolving Kitchen-Complete and Evolving World-Complete. "], "table_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "RcPAJAnpnm/tmp/1a9e6e3feadbeca34d260b8d7807e6a871c3fdeb147688ba1396a2f5b2774d7f.jpg", "img_caption": ["Cumulative task success rate (Evolving Kitchen \u2013 complete) ", "Figure 7: Evolving Kitchen-complete and Evolving Kitchen-incomplete training curves represent the cumulative task success rate up to a given stage. The goal conditioned success rate(GC) is scaled such that achieving success in all tasks by the final stage is represented as $100\\%$ . This result corresponds to the data presented in Table 1. "], "img_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "RcPAJAnpnm/tmp/6b2d39ac7c4309b595283de5c98aeed580e4374c4835573a352cbb438a4e5591.jpg", "table_caption": ["Table 15: Ablation study on the quality of the skill decoder pre-trained model in Evolving KitchenComplete and Incomplete. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Table 16: Analysis of task sequence variation in the CiL scenario of Evolving Kitchen-Complete. ", "page_idx": 18}, {"type": "table", "img_path": "RcPAJAnpnm/tmp/32be1e342ab354b09815fdfc506db7ab608a20bb828a7866f2c496c01ee79293.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "C.3 Scalability ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "LIBERO. Figure 8 provides a comprehensive visualization of the Skill Retriever in the LIBERO-goal scenario. The visualization highlights how the retriever successfully identifies and shares skills across different stages of CiL. This demonstrates its adaptability in handling varied states and tasks, showing its potential effectiveness even in complex LIBERO environments. ", "page_idx": 19}, {"type": "image", "img_path": "RcPAJAnpnm/tmp/e08c93125d06da99e75378d4ffb8618a3efb30c4282fa3feecc60ac85014edc3.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 8: Visualization of Skill Retriever on the LIBERO-goal Scenario. Left: T-SNE visualization of the state space of the existing dataset for each stage. Middle: Visualization of the stages where skills retrieved by the Skill Retriever, after all CiL stages of learning. Right: Map showing the stages of each dataset and the retrieved skills. This demonstrates that the Skill Retriever can find skills capable of handling the given state, even in the LIBERO scenario. Additionally, in task-specific parts (a), it accurately retrieves the skills, and in parts showing similar behaviors (b), it shares skills. ", "page_idx": 19}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and precede the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 20}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 20}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 20}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 20}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 20}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 20}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 20}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: The experimental results explain the mentioned addressed problems in abstract. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 20}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: In the limitations section, the paper discusses the limitations of our methodology and the potential trade-offs. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 21}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The methodology describes the necessary components, and the evaluation process and details are documented in the appendix. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 21}, {"type": "text", "text": "\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 22}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: Yes, we provide the codes for supplementary material. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 22}, {"type": "text", "text": "\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 23}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We report these information in Appendix B.4 ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 23}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Our paper reports error bars and statistical significance information in Table 1, 2, and 3 Figure 4 and 5. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 23}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We report these information in Appendix B.3. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. ", "page_idx": 23}, {"type": "text", "text": "\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We conforms to the NeurIPS Code of Ethics in every respect. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 24}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: [NA] Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 24}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 24}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 25}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We have cited the papers for the datasets and assets used. [29, 30, 31] ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 25}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 25}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] Justification: [NA] ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 26}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 26}]