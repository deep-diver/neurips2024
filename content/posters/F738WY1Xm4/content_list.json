[{"type": "text", "text": "Deep linear networks for regression are implicitly regularized towards flat minima ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Pierre Marion L\u00e9na\u00efc Chizat Institute of Mathematics Institute of Mathematics EPFL EPFL Station Z, CH-1015 Lausanne, Switzerland Station Z, CH-1015 Lausanne, Switzerland pierre.marion@epfl.ch lenaic.chizat@epfl.ch ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The largest eigenvalue of the Hessian, or sharpness, of neural networks is a key quantity to understand their optimization dynamics. In this paper, we study the sharpness of deep linear networks for univariate regression. Minimizers can have arbitrarily large sharpness, but not an arbitrarily small one. Indeed, we show a lower bound on the sharpness of minimizers, which grows linearly with depth. We then study the properties of the minimizer found by gradient flow, which is the limit of gradient descent with vanishing learning rate. We show an implicit regularization towards flat minima: the sharpness of the minimizer is no more than a constant times the lower bound. The constant depends on the condition number of the data covariance matrix, but not on width or depth. This result is proven both for a small-scale initialization and a residual initialization. Results of independent interest are shown in both cases. For small-scale initialization, we show that the learned weight matrices are approximately rank-one and that their singular vectors align. For residual initialization, convergence of the gradient flow for a Gaussian initialization of the residual network is proven. Numerical experiments illustrate our results and connect them to gradient descent with non-vanishing learning rate. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Neural networks have intricate optimization dynamics due to the non-convexity of their objective. A key quantity to understand these dynamics is the largest eigenvalue of the Hessian or sharpness $S(\\mathcal{W})$ (see Section 3 for a formal definition), in particular because of its connection with the choice of learning rate $\\eta$ . Classical theory from convex optimization indicates that the sharpness should remain lower than $2/\\eta$ to avoid divergence (Nesterov, 2018). The relevance of this point of view for deep learning has recently been questioned since neural networks have been shown to often operate at the edge of stability (Cohen et al., 2021), where the sharpness oscillates around $2/\\eta$ , while the loss still steadily decreases, albeit non-monotonically. Damian et al. (2023) explained the stability of gradient descent slightly above the $2/\\eta$ threshold to be a general phenomenon for non-quadratic objectives, where the third-order derivatives of the loss induce a self-stabilization effect. They also show that, under appropriate assumptions, gradient descent on a risk $R^{L}$ implicitly solves the constrained minimization problem ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathcal{W}}R^{L}(\\mathcal{W})\\quad\\mathrm{such\\,\\,that}\\quad S(\\mathcal{W})\\leqslant\\frac{2}{\\eta}\\,.\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "Trainability of neural networks initialized with a sharpness larger than $2/\\eta$ is also studied by Lewkowycz et al. (2020), which describes a transient catapult regime, which lasts until the sharpness goes below $2/\\eta$ . These results beg the question of quantifying the largest learning rate that enables successful training of neural networks. For classification with linearly separable data and logistic loss, ", "page_idx": 0}, {"type": "text", "text": "Wu et al. (2024) show that gradient descent converges for any learning rate. In this work, we address the case of deep linear networks for regression. As illustrated by Figure 1a, the picture then differs from the classification case: when the learning rate exceeds some critical value, the network fails to learn. We further remark that this critical value does not seem to be related to the initial scale: when the learning rate is under the critical value, learning is successful for a wide range of initial scales. In Figure 1b, we see that the large initialization scales correspond to initial sharpnesses well over the $2/\\eta$ threshold, confirming that training is possible while initializing beyond the $2/\\eta$ threshold. ", "page_idx": 1}, {"type": "image", "img_path": "F738WY1Xm4/tmp/d928f52bbb395cf62f460f3d7e119b1f5344b45187f0494912f315e4bb8977f9.jpg", "img_caption": ["(a) Squared distance of the trained network to the em- (b) Sharpness at initialization and after training, for pirical risk minimizer, for various learning rates and various learning rates and initialization scales. For a initialization scales. Training succeeds when the learn- given learning rate $\\eta$ , the dashed lines represent the $2/\\eta$ ing rate is lower than a critical value independent of the threshold. The dotted black lines represent the lower initialization scale. and upper bounds given in Theorem 1 and Corollary 2. "], "img_footnote": [], "page_idx": 1}, {"type": "image", "img_path": "F738WY1Xm4/tmp/25c559e301a4d9d8717b57db4f0b7fb70f3c2de569fedb1f42c9b28db1bcd241.jpg", "img_caption": ["(c) Evolution during training of the squared distance to (d) Evolution during training of the squared distance to the empirical risk minimizer and of sharpness, for $\\eta=$ the empirical risk minimizer and of sharpness, for $\\eta=$ 0.02 and an initialization scale of 0.35. The network 0.1 and an initialization scale of 0.35. The network does not enter edge of stability. enters edge of stability. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Training a deep linear network on a univariate regression task with quadratic loss. The weight matrices are initialized as Gaussian random variables, whose standard deviation is the $\\mathbf{X}$ -axis of plots 1a and 1b. Experimental details are given in Appendix C. ", "page_idx": 1}, {"type": "text", "text": "To understand where the critical value for the learning rate comes from, we characterize the sharpness of minimizers of the empirical risk. We show that there exist minimizers with arbitrarily large sharpness, but not with arbitrarily small sharpness. Indeed, the sharpness of any minimizer grows linearly with depth, as made precise next. ", "page_idx": 1}, {"type": "text", "text": "Theorem 1. Let $X\\in\\mathbb{R}^{n\\times d}$ be a design matrix and $y\\in\\mathbb{R}^{n}$ a target. Then the minimal sharpness $S_{\\mathrm{min}}$ of any linear network $x\\mapsto W_{L}\\,.\\,.\\,.\\,W_{1}x$ of depth $L$ that implements the optimal linear regressor $w^{\\star}\\in\\mathbb{R}^{d}$ satisfies ", "page_idx": 1}, {"type": "equation", "text": "$$\n2\\|w^{\\star}\\|_{2}^{2-\\frac{2}{L}}L a\\leqslant S_{\\operatorname*{min}}\\leqslant2\\|w^{\\star}\\|_{2}^{2-\\frac{2}{L}}\\sqrt{(2L-1)\\Lambda^{2}+(L-1)^{2}a^{2}}\\,,\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\Lambda$ is the largest eigenvalue of the empirical covariance matrix $\\begin{array}{r}{\\hat{\\Sigma}\\,:=\\,\\frac{1}{n}X^{\\top}X}\\end{array}$ , and $a:=$ $(\\boldsymbol{w}^{\\star}/\\|\\boldsymbol{w}^{\\star}\\|)^{\\top}\\hat{\\Sigma}(\\boldsymbol{w}^{\\star}/\\|\\boldsymbol{w}^{\\star}\\|)$ . ", "page_idx": 1}, {"type": "text", "text": "We note that this bound is similar to a result of Mulayoff and Michaeli (2020), although we alleviate their assumption of data whiteness $\\hat{\\Sigma}=I_{.}$ ). In particular, we do not assume that data covariance ", "page_idx": 1}, {"type": "text", "text": "matrix is full rank. Furthermore, our proof technique differs, since we do not require tensor algebra, and instead exhibit a direction in which the second derivative of the loss is large. ", "page_idx": 2}, {"type": "text", "text": "This result shows that it is not possible to find a minimizer of the empirical risk in regions of low sharpness. Combined with (1), this suggests an interpretation of the critical value for the learning rate: gradient descent should not be able to converge to a global minimizer as soon as ", "page_idx": 2}, {"type": "equation", "text": "$$\nS_{\\mathrm{min}}>\\frac{2}{\\eta}\\quad\\Leftrightarrow\\quad\\eta>\\frac{2}{S_{\\mathrm{min}}}\\simeq(\\|w^{\\star}\\|_{2}^{2-\\frac{2}{L}}L a)^{-1}\\,.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "This is confirmed experimentally by Figure 2, which shows that the critical value of the learning rate matches our theoretical prediction. We note that this gives a quantitative answer to the observations of Lewkowycz et al. (2020), which shows the existence of a maximal architecture-dependent learning rate beyond which training fails. The dependence of learning rate on depth (namely, constant over depth) also matches other papers that study scaling of neural networks (Chizat and Netrapalli, 2024; Yang et al., 2024). ", "page_idx": 2}, {"type": "image", "img_path": "F738WY1Xm4/tmp/0809fa8b710acd5e9c4bb940612de697a524ed3522c9a4caaf2c2e5cd8bd1064.jpg", "img_caption": ["Figure 2: Squared distance of the trained network to the empirical risk minimizer, for various learning rates and depth. For each depth, learning succeeds if the learning rate is below a threshold, which corresponds to the theoretical valueSm2in $\\begin{array}{r}{\\frac{2}{S_{\\mathrm{min}}}\\simeq(\\|w^{\\star}\\|_{2}^{2-\\frac{2}{L}}L a)^{-1}}\\end{array}$ of Theorem 1 (dashed vertical line). "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "To deepen our understanding of the training dynamics, we aim at quantifying the sharpness of the minimizer found by gradient descent (when it succeeds): we know that it has to be larger than $S_{\\mathrm{min}}$ , but is it close to $S_{\\mathrm{min}}$ or is it much larger? Inspecting Figure 1b, we see that the answer empirically depends on the interplay between initialization scale and learning rate. For small initialization scale, the sharpness after training is equal to a value relatively close to $S_{\\mathrm{min}}$ and independent of the learning rate. As the initialization scale increases, the sharpness of the trained network also increases, and plateaus at the value $2/\\eta$ . The plateauing for large initialization scales can be explained by the edge of stability analysis (1), which upper bounds the sharpness of the minimizer by $2/\\eta$ (see Figure 1d). However, this gives no insight on the value of the sharpness when the learning rate is sufficiently small so that the network does not enter the edge of stability regime (see Figure 1c). ", "page_idx": 2}, {"type": "text", "text": "In this paper, we study the limiting case for vanishing $\\eta$ , i.e., training with gradient flow. As our main finding, we bound the sharpness of the minimizer found by gradient flow in the case of overdetermined regression, meaning that the sample size $n$ is larger that the data dimension $d$ and the data empirical covariance matrix is nonsingular. In this case, we prove that the ratio between the sharpness after training and $S_{\\mathrm{min}}$ is less than a constant depending mainly on the condition number of the empirical covariance matrix $\\hat{\\Sigma}$ . In particular, the ratio does not depend on the width or depth of the network. This shows an implicit regularization towards flat minima. Note that the phenomenon we exhibit is different from the well-studied implicit regularization towards flat minima caused by stochasticity in SGD (Keskar et al., 2017; Smith and Le, 2018; Blanc et al., 2020; Damian et al., 2021; Li et al., 2022; Liu et al., 2023). In the present study, the dynamics are purely deterministic, and the low sharpness is due to the fact that the weight matrices found by gradient flow have (approximately) the same norm across layers, and that this norm is (approximately) the smallest possible one so that the network can minimize the risk. ", "page_idx": 2}, {"type": "text", "text": "Link with generalization. Flatter minima have been found to generalize better (Hochreiter and Schmidhuber, 1997; Jastrz\u02dbebski et al., 2017; Keskar et al., 2017; Jiang et al., 2020), although the picture is subtle (Dinh et al., 2017; Neyshabur et al., 2017; Andriushchenko et al., 2023). However, in this paper, we focus on the link of sharpness with (non-convex) optimization dynamics, rather than with generalization abilities. Indeed, our implicit regularization result holds in the overdetermined setting, where all minimizers of the empirical risk implement the same function thus have the same generalization error, although they differ in parameter space and in particular have different sharpnesses. We leave to future work extensions to more complex settings where our approach may link sharpness and generalization, beginning with deep linear networks for underdetermined regression. We refer to Appendix C for more comments on the link with generalization. ", "page_idx": 3}, {"type": "text", "text": "We investigate two initialization schemes, quite different in nature: small-scale initialization and residual initialization. Let us explain both settings, by presenting our approach, contributions of independent interest, and related works. ", "page_idx": 3}, {"type": "text", "text": "Small-scale initialization. In this setting, we consider an initialization of the weight matrices $W_{k}$ with i.i.d. Gaussian entries of small variance. Initialization scale is known to play a key role in training of neural networks: small-scale initialization corresponds to the \u201cfeature learning\u201d regime where the weights change significantly during training, by opposition to the \u201clazy\u201d regime (see, e.g., Chizat et al., 2019). We show convergence of the empirical risk to zero, then characterize the structure of the minimizer found by gradient flow, a novel result of interest independently of its connection with sharpness. At convergence, the weight matrices are close to being rank-one, in the sense that all their singular values but the largest one are small. Furthermore, the first left singular vector of any weight matrix aligns with the first right singular vector of the next weight matrix. From this specific structure, we deduce our bound on the sharpness of the trained network. The bound is illustrated on Figure 1b, where our lower and upper theoretical bounds on the sharpness are plotted as dotted black lines. We observe that the sharpness after training, when starting from a small-scale initialization, is indeed situated between the black lines. ", "page_idx": 3}, {"type": "text", "text": "The result and proof extend the study by Ji and Telgarsky (2020) for classification, although the parameters do not diverge to infinity contrarily to the classification case, thus requiring a finer control of the distance to the rank-one aligned solution. In regression, implicit regularization towards lowrank structure in parameter space was also studied by Saxe et al. (2014); Lampinen and Ganguli (2019); Gidel et al. (2019); Saxe et al. (2019); Varre et al. (2023) for two-layer neural networks and in Timor et al. (2023) for deep ReLU networks. This latter paper assumes convergence of the optimization algorithm and show that a solution with minimal $\\ell_{2}$ -norm has to be low-rank. In our linear setting, we instead show convergence. As detailed below, we impose mild requirements on the structure on the initialization beyond its scale; they are satisfied for instance by initializing one weight matrix to zero and the others with i.i.d. Gaussian entries. In particular, we do not require the so-called \u201czero-balanced initialization\u201d as is common in the literature on deep linear networks (see, e.g., Arora et al., 2018; Advani et al., 2020; Li et al., 2021) or a deficient-margin initialization as in Arora et al. (2019a). Finally, the limit when initialization scale tends to zero has been described for deep linear networks in Jacot et al. (2021) for multivariate regression. It consists in a saddle-to-saddle dynamics, where the rank of the weight matrices increases after each saddle. The present study considers instead a non-asymptotic setting where the initialization scale is small but nonzero, and shows convergence to a rank-one limit because univariate and not multivariate regression is considered. ", "page_idx": 3}, {"type": "text", "text": "We note that sharpness at initialization can be made arbitrarily small, since it is controlled by the initialization scale, while sharpness after training scales as $\\Theta(L)$ . This therefore showcases an example of sharpening during training (although we make no statement on monotonicity). ", "page_idx": 3}, {"type": "text", "text": "Residual initialization. Architectures of deep neural networks used in practice often present residual connections, which stabilize training (He et al., 2015). A simple non-linear residual architecture writes $h_{k+1}=h_{k}+\\sigma(N_{k}h_{k})$ . Removing the non-linearity, we get $h_{k+1}=(I+N_{k})h_{k}$ , which prompts us to consider deep linear networks with square weight matrices $W_{k}\\,\\in\\,\\mathbb{R}^{d\\times d}$ that are initialized as ", "page_idx": 3}, {"type": "equation", "text": "$$\nW_{k}(0)=I+{\\frac{s}{\\sqrt{L d}}}N_{k}\\,,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the $N_{k}\\in\\mathbb{R}^{d\\times d}$ are filled with i.i.d. standard Gaussian entries \u221aand $s\\geqslant0$ is a hyperparameter tuning the initialization scale. The scaling of the residual branch in $1/\\sqrt{d}$ is common and corresponds for instance to the so-called Glorot and He initializations respectively from Glorot and Bengio (2010) and He et al. (2015). It ensures that the variance of the residual branch is independ\u221aent of the width $d$ . Similarly, as studied in Arpit et al. (2019); Marion et al. (2022), the scaling in $1/\\sqrt{L}$ is the right one so that the initialization noise neither blows up nor decays when $L\\rightarrow\\infty$ . Note that, in practice, this scaling factor is often replaced by batch normal\u221aization (Ioffe and Szegedy, 2015), which has been shown empirically to have a similar effect to $1/\\sqrt{L}$ scaling (De and Smith, 2020). ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "In this setting which we refer to as residual initialization, we show global convergence of the empirical risk. To our knowledge, it is the first time that convergence is proven for a standard Gaussian initialization of the residual network outside the large width $d\\geqslant n$ regime. Previous works considered either an identity initialization (Bartlett et al., 2018; Arora et al., 2019a; Zou et al., 2020) or a smooth initialization such that $\\|W_{k+1}(0)-W_{k}(0)\\|_{F}=\\mathcal{O}(1/L)$ (Sander et al., 2022; Marion et al., 2024). The extension to standard Gaussian initialization leverages sharp bounds for the singular values of the product of $W_{k}(0)$ . Our main assumption, in alignment with the literature, is that the risk at initialization should not be larger than a constant (depending on $\\hat{\\Sigma}$ and $s$ ). ", "page_idx": 4}, {"type": "text", "text": "We then show that the weights after training can be written ", "page_idx": 4}, {"type": "equation", "text": "$$\nW_{k}(\\infty)=I+{\\frac{s}{\\sqrt{L d}}}N_{k}+{\\frac{1}{L}}\\theta_{k}\\,,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the Frobenius norm of the $\\theta_{k}$ is bounded by a constant (depending only on $s$ ). This structure finally enables us to bound the sharpness of the trained network. Remark that, to connect this analysis with our discussion of sharpness in univariate regression, we add to the residual network a final fixed projection vector $p\\in\\mathbb{R}^{d}$ , so that our neural network writes $x\\mapsto p^{\\top}W_{L}\\;.\\;.\\;.\\;W_{1}x$ , but the proof of convergence also holds without this projection. ", "page_idx": 4}, {"type": "text", "text": "Experimentally, we give in Appendix C plots in the residual case that are qualitatively similar to Figure 1. The main difference is that the initial sharpness is less sensitive to the initialization scale $s$ . ", "page_idx": 4}, {"type": "text", "text": "Organization of the paper. Section 2 details our setting and notations. Section 3 studies the sharpness of minimizers of the empirical risk and proves Theorem 1. Dynamics of gradient flow starting from small-scale initialization and residual initialization are respectively presented in Sections 4 and 5. The Appendix contains proofs, additional plots, experimental details, and related works. ", "page_idx": 4}, {"type": "text", "text": "2 Setting ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Model. We consider linear networks of depth $L$ from $\\mathbb{R}^{d}$ to $\\mathbb{R}$ , which are linear maps ", "page_idx": 4}, {"type": "equation", "text": "$$\nx\\mapsto p^{\\top}W_{L}\\;.\\;.\\;.\\;W_{1}x\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "parameterized by weight matrices $W_{1},\\dots,W_{L}$ , where $W_{k}\\in\\mathbb{R}^{d_{k}\\times d_{k-1}}$ , $d_{0}=d$ and $p\\in\\mathbb{R}^{d_{L}}$ is a fixed vector. This definition includes both fully-connected networks by setting $d_{L}=1$ and $p=1$ , and residual networks by setting $d_{1}=\\cdot\\cdot\\cdot=d_{L}=d$ , the $W_{k}$ close to the identity, and $p$ to some fixed (potentially random) vector in $\\mathbb{R}^{d}$ . We let $\\mathcal{W}=(W_{1},\\ldots,W_{L})$ and $w_{\\mathrm{prod}}\\,=\\,W_{1}^{\\top}\\,.\\,.\\,.\\,W_{L}^{\\top}p$ Given $X\\in\\mathbb{R}^{n\\times d}$ a design matrix and $y\\in\\mathbb{R}^{n}$ a target, we consider the empirical risk for regression ", "page_idx": 4}, {"type": "equation", "text": "$$\nR^{L}(\\mathcal{W}):=\\frac{1}{n}\\|y-X W_{1}^{\\top}\\ldots W_{L}^{\\top}p\\|_{2}^{2}=\\frac{1}{n}\\|y-X w_{\\mathrm{prod}}\\|_{2}^{2}=:R^{1}(w_{\\mathrm{prod}})\\,.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The notations $R^{L}(\\mathcal{W})$ and $R^{1}(w_{\\mathrm{prod}})$ may seem redundant, but are actually practical to define gradients of the risk both with respect to a single matrix $W_{k}$ and to the product $w_{\\mathrm{prod}}$ . Let $\\hat{\\Sigma}:=$ $\\scriptstyle{\\frac{1}{n}}X^{\\top}X$ the empirical covariance matrix, and $\\lambda$ and $\\Lambda$ respectively its smallest nonzero and largest eigenvalue. For now, we do not assume that $\\hat{\\Sigma}$ is full rank, so there is more than one solution to the regression problem $\\mathrm{min}_{w_{\\mathrm{prod}}\\in\\mathbb{R}^{d}}\\,R^{1}(w_{\\mathrm{prod}})$ . We denote by $w^{\\star}\\in\\mathbb{R}^{d}$ the smallest norm solution, and we let $R_{\\mathrm{min}}=R^{1}(w^{\\star})$ be the minimum of $R^{1}$ (and $R^{L}$ ). In all the following, we assume that $w^{\\star}\\neq0$ . Note that, due to the overparameterization induced by the neural network, there exists an infinity of parameterizations of the mapping $x\\mapsto w^{\\star\\top}x$ . ", "page_idx": 4}, {"type": "text", "text": "Gradient flow. We consider that the neural network is trained by gradient flow on the empirical risk $R^{L}$ , that is, the parameters evolve according to the ordinary differential equation ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{d W_{k}}{d t}=-\\frac{\\partial R^{L}}{\\partial W_{k}}\\,.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "An application of the chain rule gives ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\nabla_{k}R^{L}(\\mathcal{W}):=\\frac{\\partial R^{L}}{\\partial W_{k}}=W_{k+1}^{\\top}\\dots W_{L}^{\\top}p\\nabla R^{1}(w_{\\mathrm{prod}})^{\\top}W_{1}^{\\top}\\dots W_{k-1}^{\\top}\\,,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "with ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\nabla R^{1}(w_{\\mathrm{prod}})=-\\frac{2}{n}X^{\\top}(y-X w_{\\mathrm{prod}})=-\\frac{2}{n}X^{\\top}X(w^{\\star}-w_{\\mathrm{prod}})\\,.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the second equality is a consequence of $\\nabla R^{1}(w^{\\star})=0$ . ", "page_idx": 5}, {"type": "text", "text": "Notations. For $k\\in\\{1,\\ldots L\\}$ , we denote respectively by $\\sigma_{k},\\,u_{k}$ , and $v_{k}$ the first singular value (which equals the $\\ell_{2}$ operator norm), the first left singular vector and the first right singular vector of $W_{k}$ . The $\\ell_{2}$ operator norm of a matrix $M$ is denoted by $\\|M\\|_{2}$ and its Frobenius norm by $\\|M\\|_{F}$ . Its smallest singular value is denoted by $\\sigma_{\\mathrm{min}}(M)$ . For a vector $v$ , we let $\\|\\boldsymbol{v}\\|_{2}$ its Euclidean norm. Finally, for quantities that depend on the gradient flow time $t$ , we omit for concision their explicit dependence on $t$ when it is dispensable. ", "page_idx": 5}, {"type": "text", "text": "3 Estimates of the minimal sharpness of minimizers ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To define the sharpness of the model, we let $\\begin{array}{r}{D=\\sum_{k=1}^{L}d_{k}d_{k-1}}\\end{array}$ and identify the space of parameters with $\\mathbb{R}^{D}$ , which amounts to stacking all the entries of the weight matrices in a large $D$ -dimensional vector. Then the norm of the parameters seen as a $D$ -dimensional vector can be related to the Frobenius norm of the matrices by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\|\\mathcal{W}\\|_{2}^{2}=\\sum_{k=1}^{L}\\|W_{k}\\|_{F}^{2}\\,.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This allows us to define the Hessian of the risk $H:\\mathbb{R}^{D}\\rightarrow\\mathbb{R}^{D\\times D}$ , and we denote by $S(\\mathcal{W})$ its largest eigenvalue for some parameters $\\mathcal{W}$ , or sharpness. We note that there exists alternative definitions of the sharpness, but this one is the most relevant to study optimization dynamics. Our results are specific to this definition. The following result gives estimates on the minimal sharpness of minimizers of the empirical risk (and is a strictly stronger statement than Theorem 1). ", "page_idx": 5}, {"type": "text", "text": "Theorem 2. Let $S_{\\mathrm{min}}=\\operatorname*{inf}_{\\mathcal{W}\\in\\mathrm{arg}\\operatorname*{min}R^{L}(\\mathcal{W})}S(\\mathcal{W})$ and $a:=(\\boldsymbol{w}^{\\star}/\\|\\boldsymbol{w}^{\\star}\\|)^{\\top}\\hat{\\Sigma}(\\boldsymbol{w}^{\\star}/\\|\\boldsymbol{w}^{\\star}\\|)$ . We have ", "page_idx": 5}, {"type": "equation", "text": "$$\nS_{\\operatorname*{min}}\\geqslant2a\\|w^{\\star}\\|_{2}^{2-\\frac1L}\\|p\\|^{\\frac1L}\\sum_{k=1}^{L}\\frac{1}{\\|W_{k}\\|_{F}}\\,,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "and ", "page_idx": 5}, {"type": "equation", "text": "$$\n2\\|w^{\\star}\\|_{2}^{2-\\frac{2}{L}}\\|p\\|^{\\frac{2}{L}}L a\\leqslant S_{\\operatorname*{min}}\\leqslant2\\|w^{\\star}\\|_{2}^{2-\\frac{2}{L}}\\|p\\|^{\\frac{2}{L}}\\sqrt{(2L-1)\\Lambda^{2}+(L-1)^{2}a^{2}}\\,.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The proof of the result relies on the following variational characterization of the sharpness as the direction of the highest change of the gradient ", "page_idx": 5}, {"type": "equation", "text": "$$\nS(\\mathcal{W})^{2}=\\operatorname*{lim}_{\\xi\\rightarrow0}\\operatorname*{sup}_{\\|W_{k}-\\tilde{W}_{k}\\|_{F}\\leqslant\\xi}\\frac{\\sum_{k=1}^{L}\\|\\nabla_{k}R^{L}(\\mathcal{W})-\\nabla_{k}R^{L}(\\tilde{\\mathcal{W}})\\|_{F}^{2}}{\\sum_{k=1}^{L}\\|W_{k}-\\tilde{W}_{k}\\|_{F}^{2}}\\,.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Lower bounds are proven by considering well-chosen directions $\\tilde{W}_{k}$ , for instance $\\tilde{W}_{k}=(1\\!+\\!\\xi\\beta_{k})W_{k}$ for the first lower bound. The upper bound is proven by constructing a specific minimizer and bounding its sharpness. The first lower bound shows that the sharpness of minimizers can be arbitrarily high if one of the matrices has a low-enough norm. More precisely, take any minimizer $\\mathcal{W}=(W_{1},\\cdot\\cdot\\cdot W_{L})$ and consider $\\mathcal{W}^{C}=(C W_{1},W_{2}/\\bar{C},W_{3},\\ldots,W_{L}\\bar{)}$ , for some $C>0$ . Then $\\mathcal{W}^{C}$ is still a minimizer, and ", "page_idx": 5}, {"type": "equation", "text": "$$\nS(\\mathcal{W}^{C})\\geqslant\\frac{2a\\|w^{\\star}\\|_{2}^{2-\\frac{1}{L}}\\|p\\|^{\\frac{1}{L}}}{\\|W_{2}/C\\|_{F}}=\\frac{2a\\|w^{\\star}\\|_{2}^{2-\\frac{1}{L}}\\|p\\|^{\\frac{1}{L}}C}{\\|W_{2}\\|_{F}}\\xrightarrow{C\\rightarrow\\infty}\\infty.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The fact that a reparameterization of the network can lead to arbitrarily high sharpness is consistent with a similar result by Dinh et al. (2017) for two-layer ReLU networks. ", "page_idx": 5}, {"type": "text", "text": "Note that the first lower bound is arbitrarily small for minimizers such that the norms $\\|W_{k}\\|_{F}$ are large. On the contrary, the second lower bound is uniform and asymptotically matches the upper bound when $L\\rightarrow\\infty$ : we have $S_{\\mathrm{min}}\\sim2\\|w^{\\star}\\|_{2}^{2}L a$ . ", "page_idx": 6}, {"type": "text", "text": "As already noted by Mulayoff and Michaeli (2020), the intuition behind the linear scaling of the bound with depth can be seen from a one-dimensional example: take $\\begin{array}{r}{f(x_{1},\\ldots,x_{L})=\\prod_{k=1}^{L}x_{i}}\\end{array}$ . Then an easy computation shows that the sharpness of $f$ at $(1,\\ldots,1)$ is equal to $L-1$ .  This showcases a simple example where the output of $f$ is constant with $L$ while its sharpness grows linearly with $L$ . ", "page_idx": 6}, {"type": "text", "text": "4 Analysis of gradient flow from small initialization ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we characterize the structure of the minimizer found by gradient flow starting from a small-scale initialization. The proof is inspired by the one of Ji and Telgarsky (2020) for linearlyseparable classification, with a finer analysis due to the finiteness of minimizers in our setting. ", "page_idx": 6}, {"type": "text", "text": "We consider the model (3) with $d_{L}=1$ and $p=1$ . Denoting by $R_{0}$ the empirical risk when the weight matrices are equal to zero, we can state our assumption on the initialization. ", "page_idx": 6}, {"type": "text", "text": "$\\left(A_{1}\\right)$ The initialization satisfies that $R^{L}(\\mathcal{W}(0))\\leqslant R_{0}$ and $\\nabla R^{L}({\\mathcal{W}}(0))\\neq0.$ ", "page_idx": 6}, {"type": "text", "text": "It is satisfied for instance if one of the weight matrices $W_{k}$ is equal to zero at initialization while the others have i.i.d. Gaussian entries, so that $\\mathbf{\\check{R}}^{L}(\\mathcal{W}(0))=R_{0}$ and $\\nabla_{k}R^{L}({\\mathcal{W}}(0))\\neq0$ (almost surely). ", "page_idx": 6}, {"type": "text", "text": "Linear networks trained by gradient flow possess the following remarkable property (Arora et al., 2018) that shall be useful in the remainder. ", "page_idx": 6}, {"type": "text", "text": "Lemma 1. For any time $t\\geqslant0$ and any $k\\in\\{1,\\ldots,L-1\\}$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\nW_{k+1}^{\\top}(t)W_{k+1}(t)-W_{k+1}^{\\top}(0)W_{k+1}(0)=W_{k}(t)W_{k}^{\\top}(t)-W_{k}(0)W_{k}^{\\top}(0)\\,.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Define now ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\varepsilon:=3\\operatorname*{max}_{1\\leqslant k\\leqslant L}\\|W_{k}(0)\\|_{F}^{2}+2\\sum_{k=1}^{L-1}\\|W_{k}(0)W_{k}^{\\top}(0)-W_{k+1}^{\\top}(0)W_{k+1}(0)\\|_{2}\\,.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Note that $\\varepsilon$ only depends on the initialization, and can be made arbitrarily small by scaling down the initialization. The following key lemma connects throughout training three key quantities to $\\varepsilon$ . ", "page_idx": 6}, {"type": "text", "text": "Lemma 2. The parameters following gradient flow satisfy for any $t\\geqslant0$ that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{\\bullet\\;f o r\\;k\\in\\{1,\\dots,L\\},}&&{\\|W_{k}(t)\\|_{F}^{2}-\\|W_{k}(t)\\|_{2}^{2}\\leqslant\\varepsilon\\,,}\\\\ &{\\bullet\\;f o r\\;j,k\\in\\{1,\\dots,L\\},}&&{|\\sigma_{k}^{2}(t)-\\sigma_{j}^{2}(t)|\\leqslant\\varepsilon\\,,}\\\\ &{\\bullet\\;f o r\\;k\\in\\{1,\\dots,L-1\\},}&&{\\langle v_{k+1}(t),u_{k}(t)\\rangle^{2}\\geqslant1-\\frac{\\varepsilon}{\\sigma_{k+1}^{2}(t)}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The first identity of the Lemma bounds the sum of the squared singular values of $W_{k}(t)$ , except the largest one. In other words, it quantifies how close $W_{k}(t)$ is to the rank-one approximation given by the first term in its singular value decomposition. The second statement bounds the distance between the spectral norms of any two weight matrices. The last bound quantifies the alignment between the first left singular vector of $W_{k}(t)$ and the first right singular vector of $W_{k+1}(t)$ . In particular, if $\\varepsilon$ is small and $\\bar{\\sigma_{k+1}^{2}}(t)$ is of order 1, then $v_{k+1}(t)$ and $u_{k}(t)$ are nearly aligned. ", "page_idx": 6}, {"type": "text", "text": "We next use this Lemma to show that the neural network satisfies a Polyak-\u0141ojasiewicz (PL) condition, which is one of the main tools to study non-convex optimization dynamics (Rebjock and Boumal, 2023). A well-known result, recalled in Appendix A for completeness, shows that this implies exponential convergence of the gradient flow to a minimizer ${\\mathcal{W}}^{\\mathrm{SI}}$ of the empirical risk. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3. Under Assumption $\\left(A_{1}\\right)$ , the network satisfies the $P L$ condition for $t\\geqslant1$ , in the sense that there exists some $\\mu>0$ such that, for $t\\geqslant1$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{L}\\left\\|\\nabla_{k}R^{L}(\\mathcal{W}(t))\\right\\|_{F}^{2}\\geqslant\\mu(R^{L}(\\mathcal{W}(t))-R_{\\operatorname*{min}})\\,.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The proof leverages the structure of the gradient of the risk with respect to the first weight matrix, which relies on the linearity of the neural network and the fact that we consider a univariate output. More precisely, recall that, by (5), ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\nabla_{1}R^{L}(\\mathcal{W}(t))=\\underbrace{(W_{L}(t)\\ldots W_{2}(t))^{\\top}}_{d_{1}\\times1}\\underbrace{\\nabla R^{1}(w_{\\mathrm{prod}}(t))^{\\top}}_{1\\times d_{0}}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Therefore the Frobenius norm of the gradient decomposes as the product of two vector norms ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\nabla_{1}R^{L}(\\mathcal{W}(t))\\right\\|_{F}^{2}=\\|W_{L}(t)\\ldots W_{2}(t)\\|_{2}^{2}\\|\\nabla R^{1}(w_{\\mathrm{prod}}(t))\\|_{2}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\geqslant4\\lambda\\|W_{L}(t)\\ldots W_{2}(t)\\|_{2}^{2}(R^{L}(\\mathcal{W}(t))-R_{\\operatorname*{min}})\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where the lower bound unfolds from a straightforward computation. The delicate step is to lower bound $\\lVert W_{L}(t)\\cdot..\\cdot W_{2}(t)\\rVert_{2}$ , which we approach by distinguishing depending on the magnitude of $\\sigma_{1}(t)=\\|W_{1}(t)\\|_{2}$ . If $\\sigma_{1}(t)$ is large, we use Lemma 2 to deduce both that all $\\sigma_{k}(t)$ are large and then that the first singular vectors of successive weight matrices are aligned. This implies that the product of weight matrices has a large norm. To analyze the case where $\\sigma_{1}(t)$ is small, we use Assumption $\\left(A_{1}\\right)$ to bound away $R^{L}(\\dot{\\mathcal{W}}(t))$ from $R_{0}$ for $t\\geqslant1$ , and therefore $w_{\\mathrm{prod}}(t)$ from 0. The fact that $w_{\\mathrm{prod}}(t)$ cannot be too close to 0 while $\\sigma_{1}(t)$ is small implies that $\\lVert W_{L}(t)\\ldots W_{2}(t)\\rVert_{2}$ is large. All in all, this allows us to lower bound $\\lVert W_{L}(t)\\ldots W_{2}(t)\\rVert_{2}$ , and the PL condition follows. ", "page_idx": 7}, {"type": "text", "text": "To characterize the weights at the end of the training, we make the following assumption. ", "page_idx": 7}, {"type": "text", "text": "The first statement ensures unicity of the minimizer $w^{\\star}$ of $R^{1}$ , and thus, given that the risk goes to 0, we have $w_{\\mathrm{prod}}\\rightarrow w^{\\star}$ . The last condition means that the initialization has to be scaled down as the depth increases, so that $\\varepsilon=\\mathcal{O}(1/L^{2})$ . Intermediates conditions are technical. We can then show the following corollary. ", "page_idx": 7}, {"type": "text", "text": "Corollary 1. Under Assumptions $(A_{1}){-}(A_{2})$ , there exists $T\\ \\geqslant\\ 1$ , such that, for all $t\\geqslant T$ and $k\\in\\{1,\\ldots,L\\}$ , ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\left(\\frac{\\|w^{\\star}\\|_{2}}{2}\\right)^{1/L}\\leqslant\\sigma_{k}(t)\\leqslant\\left(2\\|w^{\\star}\\|_{2}\\right)^{1/L}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Together with Lemma 2, this result gives a precise description of the structure of the weights at the end of the gradient flow trajectory. Up to the small factor $\\varepsilon$ , the weights are rank-one matrices, with equal norms and aligned singular vectors. Since the product of weights aligns with $w^{\\star}$ , this means that the first right singular vector of $W_{1}$ has to align with $w^{\\star}$ , and then the weight matrices align with their neighbors in order to propagate the signal in the network. ", "page_idx": 7}, {"type": "text", "text": "Combining this specific structure of the weights with the variational characterization (7) of the sharpness and the explicit formulas (5)\u2013(6) for the gradients, we derive the following upper bound on the sharpness of the found minimizer. ", "page_idx": 7}, {"type": "text", "text": "Corollary 2. Under Assumptions $(A_{1})\u2013(A_{2})$ , the following bounds on the sharpness of the minimizer ${\\mathcal{W}}^{\\mathrm{{\\scriptsize{SI}}}}$ hold: ", "page_idx": 7}, {"type": "equation", "text": "$$\n1\\leqslant\\frac{S(\\mathcal{W}^{\\mathrm{SI}})}{S_{\\operatorname*{min}}}\\leqslant4\\frac{\\Lambda}{\\lambda}\\,.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "This result shows that the sharpness of the minimizer is close to $S_{\\mathrm{min}}$ in the sense that their ratio is bounded by a constant times the condition number of $\\hat{\\Sigma}$ . For example, in the case of white data $\\hat{\\Sigma}=I)$ ), we obtain that $S(\\mathcal{W}^{\\mathrm{SI}})\\leqslant4S_{\\mathrm{min}}$ . ", "page_idx": 7}, {"type": "text", "text": "5 Analysis of gradient flow from residual initialization ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We now study the case of residual initialization. We consider a linear network of the form (3) with ", "page_idx": 7}, {"type": "equation", "text": "$$\nW_{k}(t)=I+\\frac{s}{\\sqrt{L d}}N_{k}+\\frac{1}{L}\\theta_{k}(t)\\,,\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where each matrix is a $d\\!\\times d$ matrix and the $N_{k}$ are fliled with i.i.d. Gaussian entries ${\\mathcal{N}}(0,1)$ . We refer to Section 1 for a discussion of the scaling factor in front of $N_{k}$ . Following the standard initialization of residual networks, we assume that the $\\theta_{k}$ are initialized to zero. Note that the scaling factor $1/L$ in front of the $\\theta_{k}$ has no impact on the dynamics; it is convenient for exposition and computations, since we show that, with this scaling factor, the $\\theta_{k}$ are of order $O(1)$ after training. ", "page_idx": 8}, {"type": "text", "text": "Before stating the main result of this section on the convergence of the gradient flow, recall that $p\\in\\mathbb{R}^{d}$ is a fixed vector appended at the end of the residual network to project its output back in $\\mathbb{R}$ . ", "page_idx": 8}, {"type": "text", "text": "Theorem 4. There exist $C_{1},\\dots,C_{5}>0$ depending only on s such that, $i f\\,L\\geqslant C_{1}$ and $d\\geqslant C_{2}$ , then, with probability at least ", "page_idx": 8}, {"type": "equation", "text": "$$\n1-16\\exp(-C_{3}d)\\,,\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "if ", "page_idx": 8}, {"type": "equation", "text": "$$\nR^{L}(\\mathcal{W}(0))-R_{\\mathrm{min}}\\leqslant\\frac{C_{4}\\lambda^{2}\\|p\\|_{2}^{2}}{\\Lambda}\\,,\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "the gradient flow dynamics (4) converge to a global minimizer $\\mathcal{W}^{\\mathrm{RI}}$ of the risk. Furthermore, the minimizer $\\mathcal{W}^{\\mathrm{RI}}$ satisfies ", "page_idx": 8}, {"type": "equation", "text": "$$\nW_{k}^{\\mathrm{RI}}=I+\\frac{s}{\\sqrt{L d}}N_{k}+\\frac{1}{L}\\theta_{k}^{\\mathrm{RI}}\\quad\\mathrm{with}\\quad\\|\\theta_{k}^{\\mathrm{RI}}\\|_{F}\\leqslant C_{5}\\,,\\quad1\\leqslant k\\leqslant L\\,.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "To our knowledge, Theorem 4 is the first result showing convergence of gradient flow for standard Gaussian initialization of residual networks without assuming overparameterization. The main requirement is that the loss at initialization be not too large, as is standard in the literature analyzing gradient flow for deep linear residual networks (Bartlett et al., 2018; Arora et al., 2019a; Zou et al., 2020; Sander et al., 2022). Note that our bound on the loss at initialization does not depend on the width $d$ , depth $L$ , or sample size $n$ . We emphasize that the same proof holds for multivariate regression, in the absence of the projection vector $p$ . We focus here on univariate regression to connect the result with the analysis of sharpness for univariate regression in Section 3. Details on adaptation to multivariate regression are given in Appendix B.7. Finally, the precise dependence of $C_{1}$ to $C_{5}$ on $s$ can be found in the proof. ", "page_idx": 8}, {"type": "text", "text": "The proof is a refinement of the analysis for identity initialization of residual networks (Zou et al., 2020; Sander et al., 2022). From the expression of the gradients (5), we can show that ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{4\\Lambda\\|p\\|_{2}^{2}\\|\\Pi_{L:k+1}\\|_{2}^{2}\\|\\Pi_{k-1:1}\\|_{2}^{2}(R(\\mathcal{W})-R_{\\operatorname*{min}})}&{}&\\\\ {\\quad}&{\\geqslant\\|\\nabla_{k}R^{L}(\\mathcal{W})\\|_{F}^{2}\\geqslant4\\lambda\\|p\\|_{2}^{2}\\sigma_{\\operatorname*{min}}^{2}(\\Pi_{L:k+1})\\sigma_{\\operatorname*{min}}^{2}(\\Pi_{k-1:1})(R(\\mathcal{W})-R_{\\operatorname*{min}})\\,,}&\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "with $\\Pi_{L:k}:=W_{L}\\ldots W_{k}$ and $\\Pi_{k:1}:=W_{k}\\ldots W_{1}$ . Letting ", "page_idx": 8}, {"type": "equation", "text": "$$\nt^{*}=\\operatorname*{inf}\\left\\{t\\in\\mathbb{R}_{+},\\exists k\\in\\{1,\\dots,L\\},\\|\\theta_{k}(t)\\|_{F}>C_{5}\\right\\},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "the crucial step is to lower bound $\\sigma_{\\mathrm{min}}^{2}(\\Pi_{L:k+1})$ and $\\sigma_{\\mathrm{min}}^{2}(\\Pi_{k-1:1})$ uniformly for $t\\in[0,t^{\\star}]$ , in order to get a $\\mathrm{PL}$ condition valid for $t\\in[0,t^{\\star}]$ . Then, the condition on the loss at initialization is used to prove that $t^{\\star}=\\infty$ , thereby the PL condition holds for all $t\\geqslant0$ . We deduce both convergence and the bound on the norm of $\\theta_{k}^{\\mathrm{RI}}$ . The lower bound on $\\sigma_{\\mathrm{min}}^{2}(\\Pi_{L:k+1})$ and $\\sigma_{\\mathrm{min}}^{2}(\\Pi_{k-1:1})$ is straightforward in the case of an identity initialization. In the case of Gaussian initialization, the proof is more intricate, and leverages the following high probability bounds on the singular values of residual networks. ", "page_idx": 8}, {"type": "text", "text": "Lemma 3. There exist $C_{1},\\ldots,C_{4}>0$ depending only on s such that, if ", "page_idx": 8}, {"type": "equation", "text": "$$\nL\\geqslant C_{1}\\,,\\quad d\\geqslant C_{2}\\,,\\quad u\\in[C_{3},C_{4}L^{1/4}]\\,,\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "then, with probability at least ", "page_idx": 8}, {"type": "equation", "text": "$$\n1-8\\exp\\Big(-\\frac{d u^{2}}{32s^{2}}\\Big)\\,,\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "it holds for all $\\theta$ such that $\\begin{array}{r}{\\operatorname*{max}_{1\\leqslant k\\leqslant L}\\|\\theta_{k}\\|_{2}\\leqslant\\frac{1}{64}\\exp(-2s^{2}-4u)}\\end{array}$ and all $k\\in\\{1,\\ldots,L\\}$ that ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\left\\|\\left(I+\\frac{s}{\\sqrt{L d}}N_{k}+\\frac{1}{L}\\theta_{k}\\right)\\ldots\\left(I+\\frac{s}{\\sqrt{L d}}N_{1}+\\frac{1}{L}\\theta_{1}\\right)\\right\\|_{2}\\leqslant4\\exp\\left(\\frac{s^{2}}{2}+u\\right),\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "and ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\sigma_{\\mathrm{min}}\\Bigl(\\Bigl(I+\\frac{s}{\\sqrt{L d}}N_{k}+\\frac{1}{L}\\theta_{k}\\Bigr)\\dots\\Bigl(I+\\frac{s}{\\sqrt{L d}}N_{1}+\\frac{1}{L}\\theta_{1}\\Bigr)\\Bigr)\\geqslant\\frac{1}{4}\\exp\\Big(-\\frac{2s^{2}}{d}-u\\Big)\\,.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "The proof of this result goes in three steps. We first study the evolution of the norm of the activations across the layers of the residual network when $\\theta\\,=\\,0$ , and prove a high-probability bound by leveraging concentration inequalities for Gaussian and $\\chi^{2}$ distributions. Then an $\\varepsilon$ -net argument allows to bound the singular values. Finally, the extension to $\\theta$ in a ball around 0 is done via a perturbation analysis. The proof technique is related to previous works (Marion et al., 2022; Zhang et al., 2022), but provides a crisper and sounder bound. More precisely, Marion et al. (2022) show a bound on the norm of the activations with a probability of failure that decays polynomially with the width $d$ , which is not sufficient to apply the $\\varepsilon$ -net argument that requires an exponentially decreasing probability of failure. As for Zhang et al. (2022), they provide a similar bound with the purpose of showing convergence of wide residual networks, however with a less sharp probability of failure that increases polynomially with depth.1 Finally, as previously, the dependence of $C_{1}$ to $C_{4}$ on $s$ can be found in the proof. ", "page_idx": 9}, {"type": "text", "text": "The characterization (8) of the minimizer in Theorem 4 allows to bound its sharpness, as made precise by the following corollary. It holds under the same assumptions and high-probability bound as the conclusion of Theorem 4. ", "page_idx": 9}, {"type": "text", "text": "Corollary 3. Under the assumptions of Theorem 4, and if the data covariance matrix $\\hat{\\Sigma}$ is full rank, there exists $C\\,>\\,0$ depending only on s such that the following bounds on the sharpness of the minimizer $\\mathcal{W}^{\\mathrm{RI}}$ hold: ", "page_idx": 9}, {"type": "equation", "text": "$$\n1\\leqslant\\frac{S(\\mathcal{W}^{\\mathrm{RI}})}{S_{\\operatorname*{min}}}\\leqslant C\\frac{\\Lambda}{\\lambda}\\,.\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "As for Corollary 2, the proof relies on the fact that the norms of the weight matrices are close to each other and to the smallest possible norm to minimize the risk. This result shows again an implicit regularization towards a low-sharpness minimizer. Experimental illustration connecting the result with gradient descent with non-vanishing learning rate is provided in Appendix C. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper studies dynamics of gradient flow for deep linear networks on a regression task. For small-scale initialization, we prove that the learned weight matrices are approximately rank-one and that their singular vectors align. For residual initialization, convergence of the gradient flow for a Gaussian initialization is proven. In both cases, we obtain that the sharpness of the solution found by gradient flow is close to the smallest sharpness among all minimizers. Interesting next steps include studying the dynamics at any initialization scale, for non-vanishing learning rates, as well as extension to non-linear networks. We refer to Appendix C for additional comments and preliminary experimental results regarding possible extensions. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "P.M. acknowledges support of Google through a Google PhD Fellowship. Authors thank Matus Telgarsky for insightful discussions, and Elo\u00efse Berthier, Guillaume Dalle, Benjamin Dupuis, as well as anonymous NeurIPS reviewers, for thoughtful proofreading and remarks. An improvement to a proof was also made possible by a tweet of Gabriel Peyr\u00e9. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "M. S. Advani, A. M. Saxe, and H. Sompolinsky. High-dimensional dynamics of generalization error in neural networks. Neural Networks, 132:428\u2013446, 2020. ", "page_idx": 9}, {"type": "text", "text": "A. Agarwala, F. Pedregosa, and J. Pennington. Second-order regression models exhibit progressive sharpening to the edge of stability. In A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 169\u2013195. PMLR, 23\u201329 Jul 2023. ", "page_idx": 9}, {"type": "text", "text": "M. Andriushchenko, F. Croce, M. M\u00fcller, M. Hein, and N. Flammarion. A modern look at the relationship between sharpness and generalization. In A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 840\u2013902. PMLR, 23\u201329 Jul 2023.   \nS. Arora, N. Cohen, and E. Hazan. On the optimization of deep networks: Implicit acceleration by overparameterization. In J. Dy and A. Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 244\u2013253. PMLR, 10\u201315 Jul 2018.   \nS. Arora, N. Cohen, N. Golowich, and W. Hu. A convergence analysis of gradient descent for deep linear neural networks. In International Conference on Learning Representations, 2019a.   \nS. Arora, N. Cohen, W. Hu, and Y. Luo. Implicit regularization in deep matrix factorization. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019b.   \nD. Arpit, V. Campos, and Y. Bengio. How to initialize your network? Robust initialization for WeightNorm & ResNets. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d\u2019Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32, pages 10902\u201310911. Curran Associates, Inc., 2019.   \nP. Bartlett, D. Helmbold, and P. Long. Gradient descent with identity initialization efficiently learns positive definite linear transformations by deep residual networks. In J. Dy and A. Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 521\u2013530. PMLR, 10\u201315 Jul 2018.   \nG. Blanc, N. Gupta, G. Valiant, and P. Valiant. Implicit regularization for deep neural networks driven by an ornstein-uhlenbeck like process. In J. Abernethy and S. Agarwal, editors, Proceedings of Thirty Third Conference on Learning Theory, volume 125 of Proceedings of Machine Learning Research, pages 483\u2013513. PMLR, 09\u201312 Jul 2020.   \nJ. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke, J. VanderPlas, S. Wanderman-Milne, and Q. Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax.   \nL. Chizat and P. Netrapalli. The feature speed formula: a flexible approach to scale hyper-parameters of deep neural networks. In A. Globerson, L. Mackey, A. Fan, C. Zhang, D. Belgrave, J. Tomczak, and U. Paquet, editors, Advances in Neural Information Processing Systems, volume 37. Curran Associates, Inc., 2024.   \nL. Chizat, E. Oyallon, and F. Bach. On lazy training in differentiable programming. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.   \nJ. Cohen, S. Kaur, Y. Li, J. Z. Kolter, and A. Talwalkar. Gradient descent on neural networks typically occurs at the edge of stability. In International Conference on Learning Representations, 2021.   \nA. Damian, T. Ma, and J. D. Lee. Label noise SGD provably prefers flat global minimizers. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 27449\u201327461. Curran Associates, Inc., 2021.   \nA. Damian, E. Nichani, and J. D. Lee. Self-stabilization: The implicit bias of gradient descent at the edge of stability. In The Eleventh International Conference on Learning Representations, 2023.   \nS. De and S. Smith. Batch normalization biases residual blocks towards the identity function in deep networks. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 19964\u201319975. Curran Associates, Inc., 2020.   \nL. Dinh, R. Pascanu, S. Bengio, and Y. Bengio. Sharp minima can generalize for deep nets. In D. Precup and Y. W. Teh, editors, Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 1019\u20131028. PMLR, 06\u201311 Aug 2017.   \nM. Ghosh. Exponential tail bounds for chisquared random variables. Journal of Statistical Theory and Practice, 15(2):35, 2021.   \nG. Gidel, F. Bach, and S. Lacoste-Julien. Implicit regularization of discrete gradient dynamics in linear neural networks. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.   \nX. Glorot and Y. Bengio. Understanding the difficulty of training deep feedforward neural networks. In Y. Teh and M. Titterington, editors, Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, volume 9 of Proceedings of Machine Learning Research, pages 249\u2013256. PMLR, 2010.   \nS. Gunasekar, B. E. Woodworth, S. Bhojanapalli, B. Neyshabur, and N. Srebro. Implicit regularization in matrix factorization. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.   \nK. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification. In 2015 IEEE International Conference on Computer Vision (ICCV), pages 1026\u20131034. IEEE Computer Society, 2015.   \nD. Hendrycks and K. Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016.   \nS. Hochreiter and J. Schmidhuber. Flat minima. Neural Computation, 9(1):1\u201342, 1997.   \nS. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In F. Bach and D. Blei, editors, Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pages 448\u2013456. PMLR, 2015.   \nA. Jacot, F. Ged, B. \u00b8Sim\u00b8sek, C. Hongler, and F. Gabriel. Saddle-to-saddle dynamics in deep linear networks: Small initialization training, symmetry, and sparsity. arXiv preprint arXiv:2106.15933, 2021.   \nS. Jastrze\u02dbbski, Z. Kenton, D. Arpit, N. Ballas, A. Fischer, Y. Bengio, and A. Storkey. Three factors influencing minima in SGD. arXiv preprint arXiv:1711.04623, 2017.   \nZ. Ji and M. Telgarsky. Directional convergence and alignment in deep learning. In Advances in Neural Information Processing Systems, volume 33, pages 17176\u201317186. Curran Associates, Inc., 2020.   \nY. Jiang, B. Neyshabur, H. Mobahi, D. Krishnan, and S. Bengio. Fantastic generalization measures and where to find them. In International Conference on Learning Representations, 2020.   \nN. S. Keskar, D. Mudigere, J. Nocedal, M. Smelyanskiy, and P. T. P. Tang. On large-batch training for deep learning: Generalization gap and sharp minima. In International Conference on Learning Representations, 2017.   \nA. K. Lampinen and S. Ganguli. An analytic theory of generalization dynamics and transfer learning in deep linear networks. In International Conference on Learning Representations, 2019.   \nA. Lewkowycz, Y. Bahri, E. Dyer, J. Sohl-Dickstein, and G. Gur-Ari. The large learning rate phase of deep learning: the catapult mechanism. arXiv preprint arXiv:2003.02218, 2020.   \nZ. Li, Y. Luo, and K. Lyu. Towards resolving the implicit bias of gradient descent for matrix factorization: Greedy low-rank learning. In International Conference on Learning Representations, 2021.   \nZ. Li, T. Wang, and S. Arora. What happens after SGD reaches zero loss? \u2013a mathematical framework. In International Conference on Learning Representations, 2022.   \nH. Liu, S. M. Xie, Z. Li, and T. Ma. Same pre-training loss, better downstream: Implicit bias matters for language models. In A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 22188\u201322214. PMLR, 23\u201329 Jul 2023.   \nL. E. MacDonald, J. Valmadre, and S. Lucey. On progressive sharpening, flat minima and generalisation. arXiv preprint arXiv:2305.14683, 2023.   \nP. Marion, A. Fermanian, G. Biau, and J.-P. Vert. Scaling ResNets in the large-depth regime. arXiv preprint arXiv:2206.06929, 2022.   \nP. Marion, Y.-H. Wu, M. E. Sander, and G. Biau. Implicit regularization of deep residual networks towards neural ODEs. In The Twelfth International Conference on Learning Representations, 2024.   \nM. Michel Petrovitch. Sur une mani\u00e8re d\u2019\u00e9tendre le th\u00e9or\u00e8me de la moyenne aux \u00e9quations diff\u00e9rentielles du premier ordre. Mathematische Annalen, 54(3):417\u2013436, 1901.   \nR. Mulayoff and T. Michaeli. Unique properties of flat minima in deep networks. In H. D. III and A. Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 7108\u20137118. PMLR, 13\u201318 Jul 2020.   \nY. Nesterov. Lectures on Convex Optimization. Springer Optimization and Its Applications. Springer Cham, 2nd edition, 2018.   \nB. Neyshabur, S. Bhojanapalli, D. Mcallester, and N. Srebro. Exploring generalization in deep learning. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.   \nQ. Rebjock and N. Boumal. Fast convergence to non-isolated minima: four equivalent conditions for $\\mathrm{C^{2}}$ functions. arXiv preprint arXiv:2303.00096, 2023.   \nM. E. Sander, P. Ablin, and G. Peyr\u00e9. Do residual neural networks discretize neural ordinary differential equations? In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 35, pages 36520\u201336532. Curran Associates, Inc., 2022.   \nA. Saxe, J. McClelland, and S. Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. In International Conference on Learning Representations, 2014.   \nA. M. Saxe, J. L. McClelland, and S. Ganguli. A mathematical theory of semantic development in deep neural networks. Proceedings of the National Academy of Sciences, 116(23):11537\u201311546, 2019.   \nS. L. Smith and Q. V. Le. A bayesian perspective on generalization and stochastic gradient descent. In International Conference on Learning Representations, 2018.   \nN. Timor, G. Vardi, and O. Shamir. Implicit regularization towards rank minimization in ReLU networks. In S. Agrawal and F. Orabona, editors, Proceedings of The 34th International Conference on Algorithmic Learning Theory, volume 201 of Proceedings of Machine Learning Research, pages 1429\u20131459. PMLR, 20 Feb\u201323 Feb 2023.   \nA. V. Varre, M.-L. Vladarean, L. Pillaud-Vivien, and N. Flammarion. On the spectral bias of two-layer linear networks. In A. Oh, T. Neumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 64380\u201364414. Curran Associates, Inc., 2023.   \nR. Vershynin. High-Dimensional Probability: An Introduction with Applications in Data Science. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2018.   \nZ. Wang, Z. Li, and J. Li. Analyzing sharpness along GD trajectory: Progressive sharpening and edge of stability. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 9983\u20139994. Curran Associates, Inc., 2022.   \nJ. Wu, P. L. Bartlett, M. Telgarsky, and B. Yu. Large stepsize gradient descent for logistic loss: Non-monotonicity of the loss improves optimization efficiency. arXiv preprint arXiv:2402.15926, 2024.   \nG. Yang, D. Yu, C. Zhu, and S. Hayou. Tensor programs VI: Feature learning in infinite depth neural networks. In The Twelfth International Conference on Learning Representations, 2024.   \nC. Yun, S. Krishnan, and H. Mobahi. A unifying view on implicit bias in training linear neural networks. In International Conference on Learning Representations, 2021.   \nH. Zhang, D. Yu, M. Yi, W. Chen, and T.-Y. Liu. Stabilize deep ResNet with a sharp scaling factor $\\tau$ . Machine Learning, 111(9):3359\u20133392, 2022.   \nD. Zou, P. M. Long, and Q. Gu. On the global convergence of training deep linear ResNets. In International Conference on Learning Representations, 2020. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Organization of the Appendix. Appendix A presents some useful preliminary lemmas. The proofs of the results of the main paper are presented in Appendix B, while additional plots, discussion, and experimental details are given in Appendix C. Finally, Appendix D discusses some additional related work. ", "page_idx": 14}, {"type": "text", "text": "A Technical lemmas ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Lemma 4. For $\\alpha>0$ and $x\\in[0,1/2]$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n(1-x)^{\\alpha}\\geqslant1-2\\alpha x\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "For $\\alpha>0$ and $x>0$ such that $\\alpha x\\leqslant1$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n(1+x)^{\\alpha}\\leqslant1+2\\alpha x\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. Regarding the first inequality of the Lemma, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n(1-x)^{\\alpha}=\\exp(\\alpha\\log(1-x))\\geqslant\\exp(\\alpha(-2x))\\geqslant1-2\\alpha x\\,,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the first inequality holds for $x\\in[0,{^1\\mathord{/}}2]$ . The second inequality of the Lemma is proven by ", "page_idx": 14}, {"type": "equation", "text": "$$\n(1+x)^{\\alpha}=\\exp(\\alpha\\log(1+x))\\leqslant\\exp(\\alpha x)\\leqslant1+2\\alpha x\\,,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the second inequality holds when $\\alpha x\\leqslant1$ . ", "page_idx": 14}, {"type": "text", "text": "Lemma 5. There exists an absolute constant $C>0$ such that, for $L\\geqslant C$ and $x\\geqslant1$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\nL\\exp(-\\sqrt{L}x)\\leqslant4\\exp(-x)\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. For any $x\\in\\mathbb R$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\nL\\exp(-\\sqrt{L}x)\\leqslant4\\exp(-x)\\Leftrightarrow\\exp((\\sqrt{L}-1)x)\\geqslant\\frac{L}{4}\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Then, for $L\\geqslant1$ and $x\\geqslant1$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\exp((\\sqrt{L}-1)x)\\geqslant1+(\\sqrt{L}-1)x+\\frac{1}{2}(\\sqrt{L}-1)^{2}x^{2}}\\\\ {\\displaystyle=1+(\\sqrt{L}-1)x+\\frac{1}{2}(L+1-2\\sqrt{L})x^{2}}\\\\ {\\displaystyle\\geqslant(\\sqrt{L}-1)x+\\frac{L}{4}+\\frac{1}{2}(\\frac{L}{2}+1-2\\sqrt{L})x^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "For $L$ large enough, $\\begin{array}{r}{\\frac{L}{2}+1-2\\sqrt{L}\\geqslant0.}\\end{array}$ . Thus, since $x\\geqslant1$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\exp((\\sqrt{L}-1)x)\\geqslant(\\sqrt{L}-1)x+\\cfrac{L}{4}+\\cfrac{1}{2}(\\cfrac{L}{2}+1-2\\sqrt{L})x}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\cfrac{L}{4}+(\\cfrac{L}{4}-\\cfrac{1}{2})x}\\\\ &{\\qquad\\qquad\\qquad\\geqslant\\cfrac{L}{4}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the last inequality holds for $L$ large enough. This concludes the proof. ", "page_idx": 14}, {"type": "text", "text": "Lemma 6. Let $h\\in\\mathbb{R}^{d}$ , $N\\in\\mathbb{R}^{d\\times d}$ with i.i.d. standard Gaussian entries, and ", "page_idx": 14}, {"type": "equation", "text": "$$\nY_{1}=\\frac{\\|N h\\|_{2}^{2}}{\\|h\\|_{2}^{2}}\\,,\\quad Y_{2}=\\frac{h^{\\top}N h}{\\|h\\|_{2}^{2}}\\,,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Then ", "page_idx": 14}, {"type": "equation", "text": "$$\nY_{1}\\sim\\chi^{2}(d)\\quad\\mathrm{and}\\quad Y_{2}\\sim\\mathcal{N}(0,1)\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. We have, for $i\\in\\{1,\\ldots,d\\}$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n(N h)_{i}=\\sum_{j=1}^{d}N_{i j}h_{j}\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By independence of the $(N_{i j})_{1\\leqslant j\\leqslant d}$ , we deduce that $(N h)_{i}$ follows a $\\mathcal{N}(0,\\|h\\|^{2})$ distribution. Furthermore, by independence of the rows of $N$ , the $(N h)_{i}$ are independent. Thus ", "page_idx": 15}, {"type": "equation", "text": "$$\nY_{1}=\\frac{1}{\\|h\\|^{2}}\\sum_{i=1}^{d}(N h)_{i}^{2}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "follows a $\\chi^{2}(d)$ distribution. Moving on to $Y_{2}$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\nY_{2}=\\frac{1}{\\|h\\|^{2}}\\sum_{i,j=1}^{d}N_{i j}h_{i}h_{j}\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Thus $Y_{2}$ follows a centered Gaussian distribution, and by independence of the $(N_{i j})_{1\\leqslant i,j\\leqslant d}$ , its variance is equal to ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{1}{\\|h\\|^{4}}\\sum_{i,j=1}^{d}h_{i}^{2}h_{j}^{2}=1\\,,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which concludes the proof. ", "page_idx": 15}, {"type": "text", "text": "The next lemma shows that the PL condition implies exponential convergence of the gradient flow. It is a well-known fact (see, e.g., Rebjock and Boumal, 2023, for an overview of similar conditions), proved here for completeness. ", "page_idx": 15}, {"type": "text", "text": "Lemma 7. Let $f:\\mathbb{R}^{D}\\to\\mathbb{R}$ be a differentiable function lower bounded by $f_{\\operatorname*{min}}\\in\\mathbb{R}$ , and consider the gradient flow dynamics ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{d x}{d t}=-\\nabla f(x(t))\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "If $f$ satisfies the Polyak-\u0141ojasiewicz inequality for $t\\geqslant0$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\|\\nabla f(x(t))\\|_{2}^{2}\\geqslant\\mu(f(x(t))-f_{\\operatorname*{min}})\\,,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "then $x(t)$ converges to a global minimizer $x_{\\infty}$ , and, for $t\\geqslant0$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\nf(x(t))-f_{\\operatorname*{min}}\\leqslant(f(x(0))-f_{\\operatorname*{min}})e^{-\\mu t}\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. By the chain rule, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{d}{d t}f(x(t))=\\left\\langle\\nabla f(x(t)),\\frac{d x}{d t}\\right\\rangle=-\\|\\nabla f(x(t))\\|_{2}^{2}\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Plugging in the Polyak-\u0141ojasiewicz inequality, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{d}{d t}f(x(t))\\leqslant-\\mu(f(x(t))-f_{\\operatorname*{min}})\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Thus ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{d}{d t}(f(x(t))-f_{\\operatorname*{min}})\\leqslant-\\mu(f(x(t))-f_{\\operatorname*{min}})\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "To solve this differential inequality, one can for instance use the comparison theorem (Michel Petrovitch, 1901), which states that $f(x(t))\\,-\\,f_{\\mathrm{min}}$ is smaller that the solution $g$ of the initial value problem ", "page_idx": 15}, {"type": "equation", "text": "$$\ng(0)=f(x(0))-f_{\\operatorname*{min}}\\,,\\quad\\frac{d}{d t}g(t)=-\\mu g(t)\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This shows that ", "page_idx": 15}, {"type": "equation", "text": "$$\nf(x(t))-f_{\\operatorname*{min}}\\leqslant(f(x(0))-f_{\\operatorname*{min}})e^{-\\mu t}\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "To show convergence of the iterates, let $\\begin{array}{r}{F(t)=\\sqrt{\\mu}\\int_{0}^{t}\\|\\nabla f(x(s))\\|d s+2\\sqrt{f(x(t))-f_{\\mathrm{min}}}}\\end{array}$ . We have by the Polyak-\u0141ojasiewicz inequality ", "page_idx": 16}, {"type": "equation", "text": "$$\nF^{\\prime}(t)=\\|\\nabla f(x(t))\\|\\Big(\\sqrt{\\mu}-\\frac{\\|\\nabla f(x(t))\\|}{f(x(t))-f_{\\operatorname*{min}}}\\Big)\\leqslant0\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then ", "page_idx": 16}, {"type": "equation", "text": "$$\n0\\leqslant\\int_{0}^{t}\\Big\\|\\frac{d x}{d s}\\Big\\|d s=\\int_{0}^{t}\\|\\nabla f(x(s))\\|d s\\leqslant\\frac{F(t)}{\\sqrt{\\mu}}\\leqslant\\frac{F(0)}{\\sqrt{\\mu}}\\,,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "showing that $x(t)$ converges. ", "page_idx": 16}, {"type": "text", "text": "Lemma 8. With the notation introduced in Section 2, the following identities holds ", "page_idx": 16}, {"type": "equation", "text": "$$\nR^{L}(\\mathcal{W})=R_{\\mathrm{min}}+\\frac{1}{n}\\|X(w^{\\star}-w_{\\mathrm{prod}})\\|_{2}^{2}\\,,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and ", "page_idx": 16}, {"type": "equation", "text": "$$\n4\\lambda(R^{L}(\\mathcal{W})-R_{\\operatorname*{min}})\\leqslant\\|\\nabla R^{1}(w_{\\mathrm{prod}})\\|_{2}^{2}\\leqslant4\\Lambda(R^{L}(\\mathcal{W})-R_{\\operatorname*{min}})\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. We have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle R^{L}(\\mathcal{W})=\\frac{1}{n}\\|y-X w_{\\mathrm{prod}}\\|_{2}^{2}}}\\\\ {{\\displaystyle=\\frac{1}{n}\\|y-X w^{\\star}+X(w^{\\star}-w_{\\mathrm{prod}})\\|_{2}^{2}}}\\\\ {{\\displaystyle=\\frac{1}{n}\\|y-X w^{\\star}\\|_{2}+\\frac{1}{n}\\|X(w^{\\star}-w_{\\mathrm{prod}})\\|_{2}^{2}+\\frac{2}{n}\\langle y-X w^{\\star},X(w^{\\star}-w_{\\mathrm{prod}})\\rangle}}\\\\ {{\\displaystyle=R_{\\mathrm{min}}+\\frac{1}{n}\\|X(w^{\\star}-w_{\\mathrm{prod}})\\|_{2}^{2}+\\frac{2}{n}\\langle X^{\\top}(y-X w^{\\star}),w^{\\star}-w_{\\mathrm{prod}}\\rangle\\,,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the scalar product is equal to zero because $\\begin{array}{r}{\\nabla R^{1}(w^{\\star})=-\\frac{2}{n}X^{\\top}(y-X w^{\\star})=0}\\end{array}$ . This gives the first identity of the Lemma. Next, denoting $\\pi(w_{\\mathrm{prod}})$ the projection of $w_{\\mathrm{prod}}$ on the orthogonal subspace to the kernel of $X$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{R^{L}(\\mathscr{W})-R_{\\operatorname*{min}}=\\frac{1}{n}\\|X(w^{\\star}-\\pi(w_{\\mathrm{prod}}))\\|_{2}^{2}}}\\\\ &{=\\frac{1}{n}(w^{\\star}-\\pi(w_{\\mathrm{prod}}))^{\\top}X^{\\top}X(w^{\\star}-\\pi(w_{\\mathrm{prod}}))}\\\\ &{\\leqslant\\frac{1}{n}\\|w^{\\star}-\\pi(w_{\\mathrm{prod}})\\|_{2}\\|X^{\\top}X(w^{\\star}-\\pi(w_{\\mathrm{prod}}))\\|_{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "By the formula (6) for the gradient of $R^{1}$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla R^{1}(w_{\\mathrm{prod}})\\|_{2}=\\displaystyle\\frac{2}{n}\\|X^{\\top}X(w^{\\star}-w_{\\mathrm{prod}})\\|_{2}}\\\\ &{\\qquad\\qquad\\qquad=\\displaystyle\\frac{2}{n}\\|X^{\\top}X(w^{\\star}-\\pi(w_{\\mathrm{prod}}))\\|_{2}\\geqslant2\\lambda\\|w^{\\star}-\\pi(w_{\\mathrm{prod}})\\|_{2}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the last lower bound holds because both $w^{\\star}$ and $\\pi(w_{\\mathrm{prod}})$ are in the orthogonal to the kernel of $X$ . Plugging in the formula above, we obtain that ", "page_idx": 16}, {"type": "equation", "text": "$$\nR^{L}(\\mathcal{W})-R_{\\mathrm{min}}\\leqslant\\frac{1}{2\\lambda}\\|\\nabla R^{1}(w_{\\mathrm{prod}})\\|_{2}\\cdot\\frac{1}{2}\\|\\nabla R^{1}(w_{\\mathrm{prod}})\\|_{2}=\\frac{1}{4\\lambda}\\|\\nabla R^{1}(w_{\\mathrm{prod}})\\|_{2}^{2}\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Finally, to obtain the upper bound on the gradient, note that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla R^{1}(w_{\\mathrm{prod}})\\|_{2}^{2}=\\cfrac{4}{n^{2}}\\|X^{\\top}X(w^{\\star}-w_{\\mathrm{prod}})\\|_{2}^{2}}\\\\ &{\\qquad\\qquad\\qquad=\\cfrac{4}{n^{2}}(w^{\\star}-w_{\\mathrm{prod}})^{\\top}X^{\\top}X X^{\\top}X(w^{\\star}-w_{\\mathrm{prod}})}\\\\ &{\\qquad\\qquad\\qquad\\leqslant\\cfrac{4\\Lambda}{n}(w^{\\star}-w_{\\mathrm{prod}})^{\\top}X^{\\top}X(w^{\\star}-w_{\\mathrm{prod}})}\\\\ &{\\qquad\\qquad=4\\Lambda(R^{L}(\\mathcal{W})-R_{\\operatorname*{min}})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Lemma 9. Take $W_{1},\\dots W_{L}\\in\\mathbb{R}^{d\\times d}$ such that, for all $k\\in\\{1,\\ldots,L\\}$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|W_{k}\\dots W_{1}\\|_{2}\\leqslant M\\,,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sigma_{\\mathrm{min}}(W_{k}\\,.\\,.\\,.\\,W_{1})\\geqslant m\\,,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $M\\geqslant1$ and $m\\in(0,1)$ . Then, for all $\\theta$ such that $\\begin{array}{r}{\\operatorname*{max}_{1\\leqslant k\\leqslant L}\\|\\theta_{k}\\|_{2}\\leqslant\\frac{m^{2}}{4M^{2}}}\\end{array}$ , letting $\\tilde{W}_{k}=$ $\\begin{array}{r}{W_{k}+\\frac{\\theta_{k}}{L}}\\end{array}$ , we have ", "page_idx": 17}, {"type": "text", "text": "and ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|\\tilde{W}_{k}\\dots\\tilde{W}_{1}\\|_{2}\\leq2M\\,,}\\\\ {\\displaystyle\\sigma_{\\operatorname*{min}}\\big(\\tilde{W}_{k}\\dots\\tilde{W}_{1}\\big)\\geqslant\\frac{m}{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. First note that the assumptions imply that, for any $1\\leqslant j<k\\leqslant L$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|W_{k}\\ldots W_{j+1}\\|_{2}\\leqslant{\\frac{\\|W_{k}\\ldots W_{j+1}W_{j}\\ldots W_{1}\\|_{2}}{\\sigma_{\\operatorname*{min}}(W_{j}\\ldots W_{1})}}={\\frac{\\|W_{k}\\ldots W_{1}\\|_{2}}{\\sigma_{\\operatorname*{min}}(W_{j}\\ldots W_{1})}}\\leqslant{\\frac{M}{m}}\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "It shall come in handy to extend this formula to the case where $j=k$ , where we define the empty matrix product to be equal to the identity matrix, which has an operator norm of $\\textstyle1\\leqslant{\\frac{M}{m}}$ . ", "page_idx": 17}, {"type": "text", "text": "Now, take any $\\theta$ as in the Lemma and any $h_{0}\\in\\mathbb{R}^{d}$ . Let, for $k\\geqslant0$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{h_{k}=W_{k}\\ldots W_{1}h_{0}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\tilde{h}_{k}=\\tilde{W}_{k}\\,.\\,.\\,.\\,\\tilde{W}_{1}h_{0}\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then ", "page_idx": 17}, {"type": "equation", "text": "$$\nh_{k}=W_{k}h_{k-1}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\tilde{h}_{k}=\\Big(W_{k}+\\frac{\\theta_{k}}{L}\\Big)\\tilde{h}_{k-1}\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Thus ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\tilde{h}_{k}-h_{k}=\\frac{\\theta_{k}}{L}\\tilde{h}_{k-1}+W_{k}(\\tilde{h}_{k-1}-h_{k-1})\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Since $\\tilde{h}_{0}-h_{0}=0$ , we get by recurrence that, for $k\\geqslant1$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\tilde{h}_{k}-h_{k}=\\sum_{j=1}^{k}W_{k}\\,.\\,.\\,.\\,W_{j+1}\\frac{\\theta_{j}}{L}\\tilde{h}_{j-1}\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "From there, let us prove by recurrence that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|\\tilde{h}_{k}\\|_{2}\\leqslant2M\\|\\tilde{h}_{0}\\|_{2}\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This equation holds for $k=0$ since $M\\geqslant1$ . Next, assume that it holds up to a certain rank $k-1$ , and let us prove it at rank $k$ . From (10), we get that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|\\tilde{h}_{k}-h_{k}\\|_{2}\\leqslant\\frac{1}{L}\\sum_{j=1}^{k}\\|W_{k}\\ldots W_{j+1}\\|_{2}\\|\\theta_{j}\\|_{2}\\|\\tilde{h}_{j-1}\\|_{2}\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Since $M\\geqslant1$ and $m\\,<\\,1$ , the bound on $\\lVert{\\boldsymbol{\\theta}}_{j}\\rVert_{2}$ from the assumptions of the Lemma implies in particular that $\\begin{array}{r}{\\|\\theta_{j}\\|_{2}\\leqslant\\frac{m}{2M}}\\end{array}$ . Utilizing this, as well as (9) and the recurrence hypothesis (11) up to rank , we get ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|\\tilde{h}_{k}-h_{k}\\|_{2}\\leqslant\\frac{1}{L}\\sum_{j=1}^{k}\\frac{M}{m}\\cdot\\frac{m}{2M}\\cdot2M\\|\\tilde{h}_{0}\\|_{2}\\leqslant M\\|\\tilde{h}_{0}\\|_{2}\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\tilde{h}_{k}\\|_{2}\\leqslant\\|h_{k}\\|_{2}+\\|\\tilde{h}_{k}-h_{k}\\|_{2}\\leqslant\\|W_{k}\\ldots W_{1}\\|_{2}\\|h_{0}\\|_{2}+M\\|\\tilde{h}_{0}\\|_{2}\\leqslant2M\\|\\tilde{h}_{0}\\|_{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This concludes the proof of the recurrence hypothesis at rank $k$ . We therefore get that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\lVert\\tilde{W}_{k}\\cdot\\cdot\\tilde{W}_{1}\\rVert_{2}\\leqslant2M\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "To prove the lower bound on the smallest singular value of $\\tilde{W}_{k}\\ldots\\tilde{W}_{1}$ , observe that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|\\tilde{h}_{k}\\|_{2}\\geqslant\\|h_{k}\\|_{2}-\\|\\tilde{h}_{k}-h_{k}\\|_{2}}\\\\ {\\displaystyle\\geqslant\\sigma_{\\operatorname*{min}}(W_{k}\\ldots W_{1})\\|h_{0}\\|_{2}-\\displaystyle\\frac{1}{L}\\sum_{j=1}^{k}\\|W_{k}\\ldots W_{j+1}\\|_{2}\\|\\theta_{j}\\|_{2}\\|\\tilde{h}_{j-1}\\|_{2}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "by (12). Finally, by (9), the bound on $\\lVert{\\boldsymbol{\\theta}}_{j}\\rVert_{2}$ from the assumptions, and the upper bound we just proved on $\\|\\tilde{h}_{j-1}\\|_{2}$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|\\tilde{h}_{k}\\|_{2}\\geqslant m\\|h_{0}\\|_{2}-\\frac{1}{L}\\sum_{j=1}^{k}\\frac{M}{m}\\cdot\\frac{m^{2}}{4M^{2}}\\cdot2M\\|h_{0}\\|_{2}\\geqslant\\frac{m}{2}\\|h_{0}\\|_{2}\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This concludes the proof. ", "page_idx": 18}, {"type": "text", "text": "B Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "B.1 Proof of Theorem 2 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "For a twice continuously differentiable function $f:\\mathbb{R}^{D}\\to\\mathbb{R}$ , the largest eigenvalue $S$ of its Hessian $H(f):\\mathbb{R}^{D}\\rightarrow\\mathbb{R}^{D\\times D}$ at some $x\\in\\mathbb{R}^{D}$ admits the variational characterization ", "page_idx": 18}, {"type": "equation", "text": "$$\nS(x)=\\operatorname*{lim}_{\\xi\\rightarrow0}\\operatorname*{sup}_{\\|x-\\tilde{x}\\|\\leqslant\\xi}\\frac{\\|\\nabla f(x)-\\nabla f(\\tilde{x})\\|_{2}}{\\|x-\\tilde{x}\\|_{2}}\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "In our case, the parameters are a set of matrices and the formula above translates into ", "page_idx": 18}, {"type": "equation", "text": "$$\nS(\\mathcal{W})^{2}=\\operatorname*{lim}_{\\xi\\rightarrow0}\\operatorname*{sup}_{\\|W_{k}-\\tilde{W}_{k}\\|_{F}\\leqslant\\xi}\\frac{\\sum_{k=1}^{L}\\|\\nabla_{k}R^{L}(\\mathcal{W})-\\nabla_{k}R^{L}(\\tilde{\\mathcal{W}})\\|_{F}^{2}}{\\sum_{k=1}^{L}\\|W_{k}-\\tilde{W}_{k}\\|_{F}^{2}}\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We now take $\\mathcal{W}$ to be an arbitrary minimizer of $R^{L}$ . To obtain the lower bounds, consider for $\\xi\\leqslant1$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\tilde{W}_{k}(\\xi)=W_{k}+\\xi M_{k}\\,,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the $M_{k}\\in\\mathbb{R}^{d_{k}\\times d_{k-1}}$ are parameters that will be chosen later (depending on the $W_{k}$ but not on $\\xi$ ). Then ", "page_idx": 18}, {"type": "equation", "text": "$$\nS(\\mathcal{W})^{2}\\geqslant\\operatorname*{lim}_{\\xi\\rightarrow0}\\frac{\\sum_{k=1}^{L}\\|\\nabla_{k}R^{L}(\\mathcal{W})-\\nabla_{k}R^{L}(\\tilde{\\mathcal{W}}(\\xi))\\|_{F}^{2}}{\\xi^{2}\\sum_{k=1}^{L}\\|M_{k}\\|_{F}^{2}}\\,,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "To alleviate notations, we drop the dependence of $\\tilde{W}_{k}$ on $\\xi$ . Recall that, for any parameters $\\mathcal{W}^{0}$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\nabla_{k}R^{L}(\\mathcal{W}^{0})=W_{k+1}^{0\\top}\\dots W_{L}^{0\\top}p\\nabla R^{1}(w_{\\mathrm{prod}}^{0})^{\\top}W_{1}^{0\\top}\\dots W_{k-1}^{0\\top}\\,,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "with ", "page_idx": 18}, {"type": "equation", "text": "$$\nw_{\\mathrm{prod}}^{0}=W_{1}^{0\\top}\\cdot\\cdot\\cdot W_{L}^{0\\top}p\\quad\\mathrm{and}\\quad\\nabla R^{1}(w_{\\mathrm{prod}}^{0})=-\\frac{2}{n}X^{\\top}X(w^{\\star}-w_{\\mathrm{prod}}^{0})\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "For minimizers of the empirical risk, $\\nabla_{k}R^{L}({\\mathcal{W}})=0$ , so ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\Delta_{k}:=\\nabla_{k}R^{L}(\\mathcal{W})-\\nabla_{k}R^{L}(\\tilde{\\mathcal{W}})=-\\tilde{W}_{k+1}^{\\top}\\dots\\tilde{W}_{L}^{\\top}p\\nabla R^{1}(\\tilde{w}_{\\mathrm{prod}})^{\\top}\\tilde{W}_{1}^{\\top}\\dots\\tilde{W}_{k-1}^{\\top}\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Minimizers of the empirical risk also satisfy that $X^{\\top}X w_{\\mathrm{prod}}\\,=\\,X^{\\top}X w^{\\star}$ . Thus, by adding and subtracting differences, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla R^{1}(\\tilde{w}_{\\mathrm{prod}})=\\cfrac{2}{n}X^{\\top}X(\\tilde{w}_{\\mathrm{prod}}-w_{\\mathrm{prod}})}\\\\ &{\\qquad\\qquad\\qquad=\\cfrac{2}{n}X^{\\top}X\\Big(\\displaystyle\\sum_{k=1}^{L}\\tilde{W}_{1}^{\\top}\\cdot\\cdot\\cdot\\tilde{W}_{k-1}^{\\top}(\\tilde{W}_{k}^{\\top}-W_{k}^{\\top})W_{k+1}^{\\top}\\cdot\\cdot\\cdot W_{L}^{\\top}p\\Big)}\\\\ &{\\qquad\\qquad\\qquad=\\cfrac{2}{n}\\xi X^{\\top}X\\Big(\\displaystyle\\sum_{k=1}^{L}W_{1}^{\\top}\\cdot\\cdot\\cdot W_{k-1}^{\\top}M_{k}^{\\top}W_{k+1}^{\\top}\\cdot\\cdot\\cdot W_{L}^{\\top}p\\Big)+\\mathcal{O}(\\xi^{2})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Here, and in the remainder of this proof, the notation $\\scriptscriptstyle\\mathcal{O}$ is taken with respect to the limit when $\\xi\\,\\rightarrow\\,0$ , everything else being fixed. In particular, inspecting the expression above, we see that $\\nabla R(\\tilde{w}_{\\mathrm{prod}})=\\mathcal{O}(\\xi)$ , and therefore, going back to (15), that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Delta_{k}=-W_{k+1}^{\\top}\\cdot\\cdot\\cdot W_{L}^{\\top}p\\nabla R^{1}(\\tilde{w}_{\\mathrm{prod}})^{\\top}W_{1}^{\\top}\\cdot\\cdot\\cdot W_{k-1}^{\\top}+\\mathcal{O}(\\xi^{2})=:\\bar{\\Delta}_{k}+\\mathcal{O}(\\xi^{2})\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "By the inequality of arithmetic and geometric means, and by subadditivity of the operator norm, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{k=1}^{L}\\|\\Delta_{k}\\|_{F}^{2}=\\sum_{k=1}^{L}\\|\\bar{\\Delta}_{k}\\|_{F}^{2}+\\mathcal{O}(\\xi^{3})}}\\\\ &{}&{\\gg\\displaystyle\\sum_{k=1}^{L}\\|\\bar{\\Delta}_{k}\\|_{2}^{2}+\\mathcal{O}(\\xi^{3})}\\\\ &{}&{\\gg L\\Big(\\displaystyle\\prod_{k=1}^{L}\\|\\bar{\\Delta}_{k}\\|_{2}\\Big)^{2/L}+\\mathcal{O}(\\xi^{3})}\\\\ &{}&{\\gg L\\big(\\|\\bar{\\Delta}_{L}\\cdot\\cdot\\bar{\\Delta}_{1}\\|_{2}\\big)^{2/L}+\\mathcal{O}(\\xi^{3})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "By definition of $\\bar{\\Delta}_{k}$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{\\Delta}_{L}\\cdot\\cdot\\cdot\\bar{\\Delta}_{1}=(-1)^{L}p\\nabla R^{1}(\\tilde{w}_{\\mathrm{prod}})^{\\top}\\underbrace{w_{\\mathrm{prod}}\\nabla R^{1}(\\tilde{w}_{\\mathrm{prod}})^{\\top}\\cdot\\cdot\\cdot w_{\\mathrm{prod}}\\nabla R^{1}(\\tilde{w}_{\\mathrm{prod}})^{\\top}}_{L-1\\mathrm{~times}}}\\\\ &{\\qquad\\qquad=(-1)^{L}p(\\nabla R^{1}(\\tilde{w}_{\\mathrm{prod}})^{\\top}w_{\\mathrm{prod}})^{L-1}\\nabla R^{1}(\\tilde{w}_{\\mathrm{prod}})^{\\top}}\\\\ &{\\qquad\\quad=(-1)^{L}p(\\nabla R^{1}(\\tilde{w}_{\\mathrm{prod}})^{\\top}w^{\\star})^{L-1}\\nabla R^{1}(\\tilde{w}_{\\mathrm{prod}})^{\\top}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the last identity comes from the formulas $\\begin{array}{r l r}{\\nabla{R}^{1}(\\tilde{w}_{\\mathrm{prod}})}&{=}&{\\frac{2}{n}{X}^{\\top}{X}(\\tilde{w}_{\\mathrm{prod}}\\;-\\;{w}_{\\mathrm{prod}})}\\end{array}$ and $X^{\\top}X w_{\\mathrm{prod}}=X^{\\top}X w^{\\star}$ . Thus ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\bar{\\Delta}_{L}\\cdot\\cdot\\cdot\\bar{\\Delta}_{1}\\|_{2}=(\\nabla R^{1}(\\tilde{w}_{\\mathrm{prod}})^{\\top}w^{\\star})^{L-1}\\|p\\|_{2}\\|\\nabla R^{1}(\\tilde{w}_{\\mathrm{prod}})\\|_{2}}\\\\ &{\\qquad\\qquad\\quad\\geqslant(\\nabla R^{1}(\\tilde{w}_{\\mathrm{prod}})^{\\top}w^{\\star})^{L-1}\\|p\\|_{2}\\frac{\\nabla R^{1}(\\tilde{w}_{\\mathrm{prod}})^{\\top}w^{\\star}}{\\|w^{\\star}\\|_{2}}}\\\\ &{\\qquad\\qquad=\\frac{(\\nabla R^{1}(\\tilde{w}_{\\mathrm{prod}})^{\\top}w^{\\star})^{L}\\|p\\|_{2}}{\\|w^{\\star}\\|_{2}}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and, by (18), ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{L}\\|\\Delta_{k}\\|_{F}^{2}\\geqslant L\\frac{\\bigl(\\nabla R^{1}(\\tilde{w}_{\\mathrm{prod}})^{\\top}w^{\\star})^{2}\\|p\\|_{2}^{2/L}}{\\|w^{\\star}\\|_{2}^{2/L}}+\\mathcal{O}(\\xi^{3})\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Coming back to (16), we have ", "page_idx": 19}, {"type": "equation", "text": "$$\nw^{\\star\\top}\\nabla R^{1}(\\tilde{w}_{\\mathrm{prod}})=\\xi\\sum_{k=1}^{L}w^{\\star\\top}\\frac{2}{n}X^{\\top}X W_{1}^{\\top}\\dots W_{k-1}^{\\top}M_{k}^{\\top}W_{k+1}^{\\top}\\dots W_{L}^{\\top}p+{\\mathcal O}(\\xi^{2})\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "At this point, the computations diverge for the two lower bounds we want to prove. For the first inequality, we take $M_{k}=\\beta_{k}W_{k}$ , where the $\\beta_{k}$ are free parameters to be optimized. We have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{w^{*\\top}\\nabla R^{1}(\\bar{w}_{\\mathrm{prod}})=\\xi\\displaystyle\\sum_{k=1}^{L}\\beta_{k}w^{*\\top}\\frac{2}{n}X^{\\top}X W_{1}^{\\top}\\cdot\\cdot\\cdot W_{k-1}^{\\top}W_{k}^{\\top}W_{k+1}^{\\top}\\cdot\\cdot\\cdot W_{L}^{\\top}p+\\mathcal O(\\xi^{2})}\\\\ &{\\phantom{=}\\displaystyle\\xi\\displaystyle\\sum_{k=1}^{L}\\beta_{k}w^{*\\top}\\frac{2}{n}X^{\\top}X w_{\\mathrm{pod}}+\\mathcal O(\\xi^{2})}\\\\ &{\\phantom{=}\\displaystyle\\xi\\displaystyle\\sum_{k=1}^{L}\\beta_{k}w^{*\\top}\\frac{2}{n}X^{\\top}X w^{*}+\\mathcal O(\\xi^{2})}\\\\ &{\\phantom{=}\\displaystyle=2\\xi a\\|w^{*}\\|_{\\displaystyle\\hat{\\xi}=\\hat{\\xi}}^{2}\\beta_{k}+\\mathcal O(\\xi^{2})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore, by (19), ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{L}\\|\\Delta_{k}\\|_{F}^{2}\\geqslant4L\\xi^{2}a^{2}\\|w^{\\star}\\|_{2}^{4-\\frac2L}\\|p\\|_{2}^{\\frac2L}\\biggl(\\sum_{k=1}^{L}\\beta_{k}\\biggr)^{2}+\\mathcal{O}(\\xi^{3})\\,.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Furthermore, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{L}\\|M_{k}\\|_{F}^{2}=\\sum_{k=1}^{L}\\beta_{k}^{2}\\|W_{k}\\|_{F}^{2}\\,.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "By (14), we get ", "page_idx": 20}, {"type": "equation", "text": "$$\nS(\\mathcal{W})^{2}\\geqslant4L a^{2}\\|w^{\\star}\\|_{2}^{4-\\frac{2}{L}}\\|p\\|_{2}^{\\frac{2}{L}}\\frac{\\Big(\\sum_{k=1}^{L}\\beta_{k}\\Big)^{2}}{\\sum_{k=1}^{L}\\beta_{k}^{2}\\|W_{k}\\|_{F}^{2}}\\,.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The first lower bound unfolds by taking $\\beta_{k}=1/\\lVert W_{k}\\rVert_{F}$ . ", "page_idx": 20}, {"type": "text", "text": "Moving on to the second lower-bound, we now take ", "page_idx": 20}, {"type": "equation", "text": "$$\nM_{k}=\\beta_{k}\\frac{u_{k}v_{k}^{\\top}}{\\|u_{k}\\|_{2}\\|v_{k}\\|_{2}}\\,,\\quad u_{k}=W_{k-1}\\dots W_{1}\\frac{2}{n}X^{\\top}X w^{\\star}\\,,\\quad v_{k}=W_{k+1}^{\\top}\\dots W_{L}^{\\top}\\,p\\,,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where again the $\\beta_{k}$ are free parameters to be optimized. We therefore have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{w^{\\star\\top}\\nabla R^{1}(\\tilde{w}_{\\mathrm{prod}})=\\displaystyle\\sum_{k=1}^{L}w^{\\star\\top}\\frac{2}{n}X^{\\top}X W_{1}^{\\top}\\ldots W_{k-1}^{\\top}M_{k}^{\\top}\\underbrace{W_{k+1}^{\\top}\\ldots W_{L}^{\\top}p}_{=v_{k}}+\\mathcal{O}(\\xi^{2})}\\\\ &{=\\xi\\displaystyle\\sum_{k=1}^{L}\\beta_{k}\\|u_{k}\\|_{2}\\|v_{k}\\|_{2}+\\mathcal{O}(\\xi^{2})}\\\\ &{=\\xi\\displaystyle\\sum_{k=1}^{L}\\beta_{k}\\|W_{k+1}^{\\top}\\ldots W_{L}^{\\top}p\\|_{2}\\|w^{\\star\\top}\\frac{2}{n}X^{\\top}X W_{1}^{\\top}\\ldots W_{k-1}^{\\top}\\|_{2}+\\mathcal{O}(\\xi^{2})}\\\\ &{\\geqslant\\xi\\displaystyle\\sum_{k=1}^{L}\\beta_{k}\\|W_{k+1}^{\\top}\\ldots W_{L}^{\\top}p w^{\\star\\top}\\|_{n}^{2}X^{\\top}X W_{1}^{\\top}\\ldots W_{k-1}^{\\top}\\|_{2}+\\mathcal{O}(\\xi^{2})\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the last line unfolds from subadditivity of the operator norm. We let $\\begin{array}{r l}{A_{k}}&{{}=}\\end{array}$ $W_{k+1}^{\\top}\\cdot\\cdot\\cdot W_{L}^{\\top}p w^{\\star\\top}\\frac{2}{n}X^{\\top}X W_{1}^{\\top}\\cdot\\cdot\\cdot W_{k-1}^{\\top}$ , and choose $\\beta_{k}=\\|\\bar{A}_{k}\\|_{2}$ . Then ", "page_idx": 20}, {"type": "equation", "text": "$$\nw_{\\mathrm{prod}}^{\\top}\\nabla R^{1}(\\tilde{w}_{\\mathrm{prod}})\\geqslant\\xi\\sum_{k=1}^{L}\\|A_{k}\\|_{2}^{2}+\\mathcal{O}(\\xi^{2})\\,.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Coming back to (19), we get ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{L}\\|\\Delta_{k}\\|_{F}^{2}\\geqslant\\frac{L\\xi^{2}\\|p\\|_{2}^{2/L}}{\\|w^{\\star}\\|_{2}^{2/L}}\\Big(\\sum_{k=1}^{L}\\|A_{k}\\|_{2}^{2}\\Big)^{2}+\\mathcal{O}(\\xi^{3})\\,.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Furthermore, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{L}\\|W_{k}-\\tilde{W}_{k}\\|_{F}^{2}=\\xi^{2}\\sum_{k=1}^{L}\\beta_{k}^{2}\\frac{\\|u_{k}v_{k}^{\\top}\\|_{F}^{2}}{\\|u_{k}\\|_{2}^{2}\\|v_{k}\\|_{2}^{2}}=\\xi^{2}\\sum_{k=1}^{L}\\beta_{k}^{2}=\\xi^{2}\\sum_{k=1}^{L}\\|A_{k}\\|_{2}^{2}\\,.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Therefore, we obtain, by (14), ", "page_idx": 20}, {"type": "equation", "text": "$$\nS(\\mathcal{W})^{2}\\geqslant\\frac{L\\|p\\|_{2}^{2/L}}{\\|w^{\\star}\\|_{2}^{2/L}}\\sum_{k=1}^{L}\\|A_{k}\\|_{2}^{2}\\,.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We can lower bound the sum similarly to (18): ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{L}\\|A_{k}\\|_{2}^{2}\\geqslant L\\bigl(\\|A_{L}\\cdot\\cdot\\cdot A_{1}\\|_{2}\\bigr)^{2/L}\\,,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{A_{L}\\cdot\\cdot\\cdot A_{1}=p w^{\\star\\top}\\frac{2}{n}X^{\\top}X\\underbrace{w_{\\mathrm{prod}}w^{\\star\\top}\\frac{2}{n}X^{\\top}X\\cdot\\cdot\\cdot w_{\\mathrm{prod}}w^{\\star\\top}\\frac{2}{n}X^{\\top}X}_{L-1\\mathrm{~times}}}}\\\\ {{=p\\Big(w^{\\star\\top}\\frac{2}{n}X^{\\top}X w^{\\star}\\Big)^{L-1}w^{\\star\\top}\\frac{2}{n}X^{\\top}X\\,.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Thus, using the Cauchy-Schwarz inequality, we get ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|A_{L}\\cdot\\cdot\\cdot A_{1}\\|_{2}=\\left(w^{\\star\\top}\\frac{2}{n}X^{\\top}X w^{\\star}\\right)^{L-1}\\|p w^{\\star\\top}\\frac{2}{n}X^{\\top}X\\|_{2}}\\\\ &{\\qquad\\qquad=\\left(w^{\\star\\top}\\frac{2}{n}X^{\\top}X w^{\\star}\\right)^{L-1}\\|p\\|_{2}\\|w^{\\star\\top}\\frac{2}{n}X^{\\top}X\\|_{2}}\\\\ &{\\qquad\\qquad\\geqslant\\frac{\\|p\\|_{2}}{\\|w^{\\star}\\|_{2}}\\Big(w^{\\star\\top}\\frac{2}{n}X^{\\top}X w^{\\star}\\Big)^{L}}\\\\ &{\\qquad\\qquad=2^{L}a^{L}\\|p\\|_{2}\\|w^{\\star}\\|_{2}^{2L-1}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Therefore ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{L}\\|A_{k}\\|_{2}^{2}\\geqslant4L a^{2}\\|w^{\\star}\\|_{2}^{4-\\frac{2}{L}}\\|p\\|_{2}^{\\frac{2}{L}}\\,.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We finally obtain ", "page_idx": 21}, {"type": "equation", "text": "$$\nS(\\mathcal W)^{2}\\geqslant4L^{2}a^{2}\\|w^{\\star}\\|_{2}^{4-\\frac4L}\\|p\\|_{2}^{\\frac4L}\\,,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which gives the second lower bound. ", "page_idx": 21}, {"type": "text", "text": "To obtain the upper bound on $S_{\\mathrm{min}}$ , we construct explicitly a minimizer, and upper bound its sharpness. More precisely, let the $W_{k}$ be rank-one matrices, such that $\\lVert W_{k}\\rVert_{2}=\\lVert w^{\\star}\\rVert^{\\bar{1}/L}/\\lVert p\\rVert^{1/L}$ , successive matrices have aligned first singular vectors, the first right singular vector of $W_{1}$ is aligned with $w^{\\star}$ , and the first left singular vector of $W_{L}$ is aligned with $p$ . We then have ", "page_idx": 21}, {"type": "equation", "text": "$$\np^{\\top}W_{L}\\ldots W_{1}=w^{\\star}\\,,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "meaning that the network minimizes the loss. We now upper bound its sharpness using (13), where we recall that the matrix appearing in the numerator is denoted by $\\Delta_{k}$ and satisfies $\\Delta_{k}=\\bar{\\Delta}_{k}\\!+\\!\\mathcal{O}(\\xi^{2})$ , where $\\bar{\\Delta}_{k}$ is given by (17). Contrarily to the proof of the lower bounds where we exhibited a specific direction $\\Tilde{W}$ , we here seek an upper bound valid for all $\\Tilde{W}$ . Recall that, by (16), we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\nabla R^{1}({\\tilde{w}}_{\\mathrm{prod}})=\\frac{2}{n}{\\boldsymbol{X}}^{\\top}{\\boldsymbol{X}}\\sum_{k=1}^{L}W_{1}^{\\top}\\,.\\,.\\,W_{k-1}^{\\top}({\\tilde{W}}_{k}-W_{k})^{\\top}W_{k+1}^{\\top}\\,.\\,.\\,W_{L}^{\\top}p+{\\mathcal{O}}(\\xi^{2})\\,.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "By subadditivity of the operator norm, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|\\nabla R^{1}(\\tilde{w}_{\\mathrm{prod}})\\|_{2}\\leqslant2\\Lambda\\sum_{k=2}^{L}\\|W_{1}\\|_{2}\\ldots\\|W_{k-1}\\|_{2}\\|\\tilde{W}_{k}-W_{k}\\|_{2}\\|W_{k+1}\\|_{2}\\ldots\\|W_{L}\\|_{2}\\|p\\|_{2}+\\mathcal{O}(\\xi^{2})}\\\\ {\\displaystyle=2\\Lambda\\|w^{\\star}\\|_{2}^{(L-1)/L}\\|p\\|_{2}^{1/L}\\sum_{k=1}^{L}\\|\\tilde{W}_{k}-W_{k}\\|_{2}+\\mathcal{O}(\\xi^{2})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Moving on to bounding the squared Frobenius norm of $\\Delta_{k}$ , we observe that $\\bar{\\Delta}_{k}$ decomposes as a rank-one matrix. We split cases for $k=1$ and $k>1$ . First, for $k=1$ , we have, by subadditivity of the operator norm and by (21), ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\Delta_{1}\\|_{F}=\\|W_{2}^{\\top}\\cdot\\cdot\\cdot W_{L}^{\\top}p\\|_{2}\\|\\nabla R^{1}(\\tilde{w}_{\\mathrm{prod}})\\|_{2}+\\mathcal{O}(\\xi^{2})}\\\\ &{\\qquad\\leqslant\\|W_{2}\\|_{2}\\ldots\\|W_{L}\\|_{2}\\|p\\|_{2}\\|\\nabla R^{1}(\\tilde{w}_{\\mathrm{prod}})\\|_{2}+\\mathcal{O}(\\xi^{2})}\\\\ &{\\qquad\\leqslant2\\Lambda\\|w^{\\star}\\|_{2}^{2(L-1)/L}\\|p\\|_{2}^{2/L}\\displaystyle\\sum_{k=1}^{L}\\|\\tilde{W}_{k}-W_{k}\\|_{2}+\\mathcal{O}(\\xi^{2})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\Delta_{1}\\|_{F}^{2}\\leqslant4\\Lambda^{2}\\|w^{\\star}\\|_{2}^{4(L-1)/L}\\|p\\|_{2}^{4/L}\\Big(\\displaystyle\\sum_{k=1}^{L}\\|\\tilde{W}_{k}-W_{k}\\|_{2}\\Big)^{2}+\\mathcal{O}(\\xi^{3})}\\\\ &{\\qquad\\leqslant4L\\Lambda^{2}\\|w^{\\star}\\|_{2}^{4(L-1)/L}\\|p\\|_{2}^{4/L}\\displaystyle\\sum_{k=1}^{L}\\|\\tilde{W}_{k}-W_{k}\\|_{2}^{2}+\\mathcal{O}(\\xi^{3})}\\\\ &{\\qquad\\leqslant4L\\Lambda^{2}\\|w^{\\star}\\|_{2}^{4(L-1)/L}\\|p\\|_{2}^{4/L}\\displaystyle\\sum_{k=1}^{L}\\|\\tilde{W}_{k}-W_{k}\\|_{F}^{2}+\\mathcal{O}(\\xi^{3})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "For $k>1$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\Delta_{k}\\|_{F}=\\|W_{k+1}^{\\top}\\ldots W_{L}^{\\top}p\\|_{2}\\|\\nabla R^{1}(\\tilde{w}_{\\mathrm{prod}})^{\\top}W_{1}^{\\top}\\ldots W_{k-1}^{\\top}\\|_{2}+\\mathcal{O}(\\xi^{2})}\\\\ &{\\qquad\\leqslant\\|W_{k+1}\\|_{2}\\ldots\\|W_{L}\\|_{2}\\|p\\|_{2}\\|\\nabla R^{1}(\\tilde{w}_{\\mathrm{prod}})^{\\top}W_{1}^{\\top}\\|_{2}\\|W_{2}\\|_{2}\\ldots\\|W_{k-1}\\|_{2}+\\mathcal{O}(\\xi^{2})}\\\\ &{\\qquad=\\|w^{\\star}\\|_{2}^{(L-2)/L}\\|p\\|_{2}^{2/L}\\|\\nabla R^{1}(\\tilde{w}_{\\mathrm{prod}})^{\\top}W_{1}^{\\top}\\|_{2}+\\mathcal{O}(\\xi^{2})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Let us now bound $\\|\\nabla R^{1}(\\tilde{w}_{\\mathrm{prod}})^{\\top}W_{1}^{\\top}\\|_{2}$ . By (20), separating the first term, we have ", "page_idx": 22}, {"type": "text", "text": "\u2225\u2207R1( w\u02dcprod)\u22a4W 1\u22a4 \u22252 ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Vert\\nabla^{\\mathcal{K}}u^{\\mathcal{K}}(v)\\Vert^{\\mathcal{K}_{1}}\\Vert^{2}}\\\\ &{\\leqslant\\left\\Vert\\int_{0}^{T}W_{L}\\cdot\\mathcal{K}_{2}(\\bar{W}_{1}-W_{1})\\frac{2}{n}X^{\\top}X W_{1}^{\\top}\\right\\Vert_{2}}\\\\ &{\\qquad+\\displaystyle\\sum_{k=2}^{L}\\left\\Vert p^{\\top}W_{L}\\dots W_{k+1}(\\bar{W}_{k}-W_{k})W_{k-1}\\dots W_{1}\\frac{2}{n}X^{\\top}X W_{1}^{\\top}\\right\\Vert_{2}+\\mathcal{O}(\\xi^{2})}\\\\ &{\\leqslant2\\Lambda\\left\\Vert p\\right\\Vert\\left\\Vert W_{L}\\right\\Vert_{2}\\ldots\\cdot\\left\\Vert W_{2}\\right\\Vert_{2}\\left\\Vert\\bar{W}_{1}-W_{1}\\right\\Vert_{2}\\left\\Vert W_{1}\\right\\Vert_{2}}\\\\ &{+\\displaystyle\\sum_{k=2}^{L}\\left\\Vert p\\right\\Vert\\left\\Vert W_{L}\\right\\Vert_{2}\\ldots\\cdot\\left\\Vert W_{k+1}\\right\\Vert_{2}\\left\\Vert\\bar{W}_{k}-W_{k}\\right\\Vert_{2}\\left\\Vert W_{k-1}\\right\\Vert_{2}\\ldots\\cdot\\left\\Vert W_{2}\\right\\Vert_{2}\\left\\Vert W_{1}\\frac{2}{n}X^{\\top}X W_{1}^{\\top}\\right\\Vert_{2}+\\mathcal{O}(\\xi^{2})}\\\\ &{=2\\Lambda\\left\\Vert w^{*}\\right\\Vert_{2}\\left\\Vert\\bar{W}_{1}-W_{1}\\right\\Vert_{2}}\\\\ &{\\qquad+\\displaystyle\\sum_{k=2}^{L}\\left\\Vert w^{*}\\right\\Vert_{2}\\left\\Vert\\bar{W}_{1}-W_{1}\\right\\Vert_{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Finally, recall that $W_{1}$ is rank-one and its first right singular vector is aligned with $w^{\\star}$ . A short computation therefore shows that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left\\|W_{1}\\frac{2}{n}X^{\\top}X W_{1}^{\\top}\\right\\|_{2}=2a\\|W_{1}\\|^{2}=2a\\|u^{\\star}\\|_{2}^{2/L}\\|p\\|_{2}^{-2/L}\\,.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Thus ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|\\nabla R^{1}(\\tilde{w}_{\\mathrm{prod}})^{\\top}W_{1}^{\\top}\\|_{2}\\leqslant2\\|w^{\\star}\\|_{2}\\Big(\\Lambda\\|\\tilde{W}_{1}-W_{1}\\|_{2}+a\\sum_{k=2}^{L}\\|\\tilde{W}_{k}-W_{k}\\|_{2}\\Big)+\\mathcal{O}(\\xi^{2})\\,.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Then, coming back to (22), for $k>1$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|\\Delta_{k}\\|_{F}\\leqslant2\\|w^{\\star}\\|_{2}^{2(L-1)/L}\\|p\\|_{2}^{2/L}\\Big(\\Lambda\\|\\tilde{W}_{1}-W_{1}\\|_{2}+a\\sum_{k=2}^{L}\\|\\tilde{W}_{k}-W_{k}\\|_{2}\\Big)+\\mathcal{O}(\\xi^{2})\\,.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Thus ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\Delta_{k}\\|_{F}^{2}\\leqslant4\\|w^{\\star}\\|_{2}^{4(L-1)/L}\\|p\\|_{2}^{4/L}\\Big(\\Lambda\\|\\tilde{W}_{1}-W_{1}\\|_{2}+a\\displaystyle\\sum_{k=2}^{L}\\|\\tilde{W}_{k}-W_{k}\\|_{2}\\Big)^{2}+\\mathcal{O}(\\xi^{3})}\\\\ &{\\qquad\\leqslant4\\|w^{\\star}\\|_{2}^{4(L-1)/L}\\|p\\|_{2}^{4/L}(\\Lambda^{2}+(L-1)a^{2})\\displaystyle\\sum_{k=1}^{L}\\|\\tilde{W}_{k}-W_{k}\\|_{2}^{2}+\\mathcal{O}(\\xi^{3})}\\\\ &{\\qquad\\leqslant4\\|w^{\\star}\\|_{2}^{4(L-1)/L}\\|p\\|_{2}^{4/L}(\\Lambda^{2}+(L-1)a^{2})\\displaystyle\\sum_{k=1}^{L}\\|\\tilde{W}_{k}-W_{k}\\|_{F}^{2}+\\mathcal{O}(\\xi^{3})\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the second inequality holds by the Cauchy-Schwarz inequality. Thus, by (13), putting together the bounds on $\\|\\Delta_{k}\\|_{F}^{2}$ for $k=1$ and $k>1$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{S(\\mathcal{W})^{2}=\\underset{\\xi\\rightarrow0}{\\mathrm{lim}}\\,\\underset{\\|W_{k}-\\tilde{W}_{k}\\|_{F}\\leqslant\\xi}{\\operatorname*{sup}}\\frac{\\sum_{k=1}^{L}\\|\\Delta_{k}\\|_{F}^{2}}{\\sum_{k=1}^{L}\\|W_{k}-\\tilde{W}_{k}\\|_{F}^{2}}}\\\\ &{\\quad\\quad\\leqslant\\underset{\\xi\\rightarrow0}{\\mathrm{lim}}\\,4\\|w^{\\star}\\|_{2}^{4(L-1)/L}\\|p\\|_{2}^{4/L}(L\\Lambda^{2}+(L-1)(\\Lambda^{2}+(L-1)a^{2}))+\\mathcal{O}(\\xi)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Therefore ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{S(\\mathcal{W})\\leqslant2\\|w^{\\star}\\|_{2}^{2-\\frac{2}{L}}\\|p\\|_{2}^{\\frac{2}{L}}\\sqrt{(2L-1)\\Lambda^{2}+(L-1)^{2}a^{2}}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "which concludes the proof. ", "page_idx": 23}, {"type": "text", "text": "B.2 Proof of Lemma 1 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "This identity can be shown by noting that the identity is trivially true for $t=0$ , then differentiating on both sides with respect to time, and using (5). We refer, e.g., to Arora et al. (2018) for a detailed proof. ", "page_idx": 23}, {"type": "text", "text": "B.3 Proof of Lemma 2 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Before proving the three statements of the lemma in order, we let ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\bar{\\varepsilon}=\\operatorname*{max}_{1\\leqslant k\\leqslant L}\\|W_{k}(0)\\|_{F}^{2}+\\sum_{k=1}^{L-1}\\|W_{k+1}^{\\top}(0)W_{k+1}(0)-W_{k}(0)W_{k}^{\\top}(0)\\|_{2}\\,.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Note that $\\bar{\\varepsilon}\\leqslant\\varepsilon$ . ", "page_idx": 23}, {"type": "text", "text": "First statement. The claim is true for $k=L$ since $W_{L}$ is a (row) vector. For $k\\in\\{1,\\ldots,L-1\\}$ , taking the 2-norm in Lemma 1, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|W_{k+1}^{\\top}W_{k+1}\\|_{2}=\\|W_{k}W_{k}^{\\top}+W_{k+1}^{\\top}(0)W_{k+1}(0)-W_{k}(0)W_{k}^{\\top}(0)\\|_{2}}\\\\ &{\\qquad\\qquad\\qquad\\leqslant\\|W_{k}W_{k}^{\\top}\\|_{2}+\\|W_{k+1}^{\\top}(0)W_{k+1}(0)-W_{k}(0)W_{k}^{\\top}(0)\\|_{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Thus, using $\\|A^{\\top}A\\|_{2}=\\|A A^{\\top}\\|_{2}=\\|A\\|_{2}^{2}$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|W_{k+1}\\|_{2}^{2}-\\|W_{k+1}^{\\top}(0)W_{k+1}(0)-W_{k}(0)W_{k}^{\\top}(0)\\|_{2}\\leqslant\\|W_{k}\\|_{2}^{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We now take the trace in Lemma 1 to obtain ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\|W_{k+1}\\|_{F}^{2}-\\|W_{k+1}(0)\\|_{F}^{2}=\\|W_{k}\\|_{F}^{2}-\\|W_{k}(0)\\|_{F}^{2}\\,.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Combining with the inequality above, we have that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|W_{k}\\|_{F}^{2}-\\|W_{k}\\|_{2}^{2}\\leqslant\\|W_{k+1}\\|_{F}^{2}-\\|W_{k+1}\\|_{2}^{2}+\\|W_{k}(0)\\|_{F}^{2}-\\|W_{k+1}(0)\\|_{F}^{2}\\quad}\\\\ {\\qquad\\qquad+\\,\\|W_{k+1}^{\\top}(0)W_{k+1}(0)-W_{k}(0)W_{k}^{\\top}(0)\\|_{2}\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Summing from $k$ to $L-1$ and telescoping, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|W_{k}\\|_{F}^{2}-\\|W_{k}\\|_{2}^{2}\\leqslant\\|W_{L}\\|_{F}^{2}-\\|W_{L}\\|_{2}^{2}+\\|W_{k}(0)\\|_{F}^{2}-\\|W_{L}(0)\\|_{F}^{2}}\\\\ {+\\displaystyle\\sum_{k^{\\prime}=k}^{L-1}\\|W_{k^{\\prime}+1}^{\\top}(0)W_{k^{\\prime}+1}(0)-W_{k^{\\prime}}(0)W_{k^{\\prime}}^{\\top}(0)\\|_{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The first two terms compensate since $W_{L}$ is a vector, and the remainder of the terms is less than $\\bar{\\varepsilon}\\leqslant\\varepsilon$ by definition. This gives the first statement of the Lemma. ", "page_idx": 23}, {"type": "text", "text": "Second statement. Assume without loss of generality that $j>k$ . By recurrence over (23), we obtain that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sigma_{j}^{2}\\leqslant\\sigma_{k}^{2}+\\sum_{k^{\\prime}=k}^{j-1}\\|W_{k^{\\prime}+1}^{\\top}(0)W_{k^{\\prime}+1}(0)-W_{k^{\\prime}}(0)W_{k^{\\prime}}^{\\top}(0)\\|_{2}\\leqslant\\sigma_{k}^{2}+\\bar{\\varepsilon}\\,.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The reverse inequality can be shown similarly: by considering again Lemma 1 and taking the 2-norm, we get ", "page_idx": 24}, {"type": "text", "text": "Thus ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{W_{k}W_{k}^{\\top}\\|_{2}\\leqslant\\|W_{k+1}^{\\top}W_{k+1}\\|_{2}+\\|W_{k+1}^{\\top}(0)W_{k+1}(0)-W_{k}(0)W_{k}^{\\top}(0)\\|_{2}}\\\\ &{\\quad\\|W_{k}\\|_{2}^{2}\\leqslant\\|W_{k+1}\\|_{2}^{2}+\\|W_{k+1}^{\\top}(0)W_{k+1}(0)-W_{k}(0)W_{k}^{\\top}(0)\\|_{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "As previously, we get by recurrence that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sigma_{k}^{2}\\leqslant\\sigma_{j}^{2}+\\bar{\\varepsilon}\\,.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Combined with the reverse bound above, this gives the second statement of the lemma. ", "page_idx": 24}, {"type": "text", "text": "Third statement. Let us lower and upper bound $u_{k}^{\\top}W_{k+1}^{\\top}W_{k+1}u_{k}$ . We have on the one hand, by Lemma 1, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{u_{k}^{\\top}W_{k+1}^{\\top}W_{k+1}u_{k}=u_{k}^{\\top}W_{k}W_{k}^{\\top}u_{k}-u_{k}^{\\top}W_{k}(0)W_{k}^{\\top}(0)u_{k}+u_{k}^{\\top}W_{k+1}^{\\top}(0)W_{k+1}(0)u_{k}}\\\\ &{\\qquad\\qquad\\qquad\\geqslant u_{k}^{\\top}W_{k}W_{k}^{\\top}u_{k}-u_{k}^{\\top}W_{k}(0)W_{k}^{\\top}(0)u_{k}}\\\\ &{\\qquad\\qquad\\qquad\\geqslant\\sigma_{k}^{2}-\\|W_{k}(0)\\|_{2}^{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "On the other hand, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{u_{k}^{\\top}W_{k+1}^{\\top}W_{k+1}u_{k}=u_{k}^{\\top}\\big(W_{k+1}^{\\top}W_{k+1}-v_{k+1}\\sigma_{k+1}^{2}v_{k+1}^{\\top}\\big)u_{k}+u_{k}^{\\top}v_{k+1}\\sigma_{k+1}^{2}v_{k+1}^{\\top}u_{k}}\\\\ &{\\qquad\\qquad\\qquad\\leqslant\\|W_{k+1}^{\\top}W_{k+1}-v_{k+1}\\sigma_{k+1}^{2}v_{k+1}^{\\top}\\|_{2}+\\langle v_{k+1},u_{k}\\rangle^{2}\\sigma_{k+1}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The 2-norm above is equal to the second largest eigenvalue of $W_{k+1}^{\\top}W_{k+1}$ , which is the square of the second largest singular value of $W_{k+1}$ . In particular, it is lower than $\\lVert W_{k+1}\\rVert_{F}^{2}-\\lVert W_{k+1}\\rVert_{2}^{2}$ which is the sum of the squared singular values of $W_{k+1}$ except the largest one. We obtain ", "page_idx": 24}, {"type": "equation", "text": "$$\nu_{k}^{\\top}W_{k+1}^{\\top}W_{k+1}u_{k}\\leqslant\\|W_{k+1}\\|_{F}^{2}-\\|W_{k+1}\\|_{2}^{2}+\\langle v_{k+1},u_{k}\\rangle^{2}\\sigma_{k+1}^{2}\\leqslant\\bar{\\varepsilon}+\\langle v_{k+1},u_{k}\\rangle^{2}\\sigma_{k+1}^{2}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "by the first statement of the Lemma. Combining the lower and upper bound of $u_{k}^{\\top}W_{k+1}^{\\top}W_{k+1}u_{k}$ , we get ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\bar{\\varepsilon}+\\langle v_{k+1},u_{k}\\rangle^{2}\\sigma_{k+1}^{2}\\geqslant\\sigma_{k}^{2}-\\|W_{k}(0)\\|_{2}^{2}\\,.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Thus ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\langle v_{k+1},u_{k}\\rangle^{2}\\geqslant\\frac{\\sigma_{k}^{2}-\\|W_{k}(0)\\|_{2}^{2}-\\bar{\\varepsilon}}{\\sigma_{k+1}^{2}}\\geqslant\\frac{\\sigma_{k+1}^{2}-\\bar{\\varepsilon}-\\|W_{k}(0)\\|_{2}^{2}-\\bar{\\varepsilon}}{\\sigma_{k+1}^{2}}\\,,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "by the second statement of the Lemma. We finally obtain ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\langle v_{k+1},u_{k}\\rangle^{2}\\geqslant1-\\frac{2\\bar{\\varepsilon}+\\|W_{k}(0)\\|_{2}^{2}}{\\sigma_{k+1}^{2}}\\geqslant1-\\frac{\\varepsilon}{\\sigma_{k+1}^{2}}\\,,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "by definition of $\\varepsilon$ and $\\bar{\\varepsilon}$ . ", "page_idx": 24}, {"type": "text", "text": "B.4 Proof of Theorem 3 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We lower bound the first term in the sum of the left-hand side. Recall that, by (5), we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\nabla_{1}R^{L}(W(t))=\\underbrace{(W_{L}(t)\\ldots W_{2}(t))^{\\top}}_{d_{1}\\times1}\\underbrace{\\nabla R^{1}(w_{\\mathrm{prod}}(t))^{\\top}}_{1\\times d_{0}},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "thus ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left\\|\\nabla_{1}R^{L}(W(t))\\right\\|_{F}^{2}=\\left\\|W_{L}(t)\\ldots W_{2}(t)\\right\\|_{2}^{2}\\left\\|\\nabla R^{1}(w_{\\mathrm{prod}}(t))\\right\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We show that $\\lVert W_{L}(t)\\cdot.\\cdot W_{2}(t)\\rVert_{2}$ is large by distingui\u221ashing between two cases depending on the magnitude of $\\sigma_{1}(t)=\\|W_{1}(t)\\|_{2}$ . To this aim, let $C>\\sqrt{2\\varepsilon}L$ . ", "page_idx": 24}, {"type": "text", "text": "Large spectral norm. We first consider the case where ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\sigma_{1}>C>\\sqrt{2\\varepsilon}L\\,.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "By Lemma 2, for $k\\in\\{1,\\ldots,L-1\\}$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\sigma_{k}^{2}\\geqslant\\sigma_{1}^{2}-\\varepsilon>\\varepsilon\\,,\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the second inequality unfolds from (25). Then, again by Lemma 2, for $k\\in\\{1,\\ldots,L-1\\}$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\langle v_{k+1},u_{k}\\rangle^{2}\\geqslant1-\\frac{\\varepsilon}{\\sigma_{k+1}^{2}}>0\\,.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "It is always possible (without loss of generality) to choose the orientation of the $u_{k}$ and $v_{k}$ such that $\\left\\langle v_{k+1},u_{k}\\right\\rangle\\geqslant0$ for any $k\\in\\{1,\\ldots,L-1\\}$ . Making this choice, the equation above implies that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|v_{k+1}-u_{k}\\|^{2}=2-2\\langle v_{k+1},u_{k}\\rangle}\\\\ {\\displaystyle\\leqslant2\\bigg(1-\\sqrt{1-\\frac{\\varepsilon}{\\sigma_{k+1}^{2}}}\\bigg)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "For $x\\in[0,1]$ , ${\\sqrt{1-x}}\\geqslant1-x$ , and thus ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\|v_{k+1}-u_{k}\\|_{2}^{2}\\leqslant\\frac{2\\varepsilon}{\\sigma_{k+1}^{2}}\\,.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Let us show that this implies a lower bound on $\\lVert W_{L}\\ldots W_{2}\\rVert_{2}$ . To do so, let us denote recursively ", "page_idx": 25}, {"type": "equation", "text": "$$\nx_{1}=v_{2},\\quad x_{k+1}=W_{k+1}x_{k}\\quad{\\mathrm{for}}\\quad k\\in\\left\\{1,\\ldots,L-1\\right\\}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We then have $x_{L}=W_{L}\\,.\\,.\\,.\\,W_{2}x_{1}$ , thus ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\|W_{L}\\ldots W_{2}\\|_{2}\\geqslant{\\frac{\\|x_{L}\\|_{2}}{\\|x_{1}\\|_{2}}}=\\|x_{L}\\|_{2}\\,.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Our goal is thus to lower bound $\\|{\\boldsymbol{x}}_{L}\\|_{2}$ , which entails a lower bound on $\\lVert W_{L}\\,.\\,.\\,W_{2}\\rVert_{2}$ . To this aim, first note that, for any $k\\in\\{1,\\ldots,L-1\\}$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\langle x_{k+1},u_{k+1}\\rangle=\\langle W_{k+1}x_{k},u_{k+1}\\rangle}}\\\\ &{=\\sigma_{k+1}\\langle x_{k},v_{k+1}\\rangle}\\\\ &{=\\sigma_{k+1}\\langle x_{k},u_{k}+v_{k+1}-u_{k}\\rangle}\\\\ &{\\geqslant\\sigma_{k+1}\\langle x_{k},u_{k}\\rangle-\\sigma_{k+1}\\|x_{k}\\|_{2}\\|v_{k+1}-u_{k}\\|_{2}}\\\\ &{\\geqslant\\sigma_{k+1}\\langle x_{k},u_{k}\\rangle-\\sqrt{2\\varepsilon}\\|x_{k}\\|_{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the last equation stems from (26). Denote $\\begin{array}{r}{\\alpha_{k}\\,=\\,\\langle\\frac{x_{k}}{\\|x_{k}\\|_{2}},u_{k}\\rangle}\\end{array}$ . Then the previous equation shows that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\alpha_{k+1}\\geqslant\\frac{\\|x_{k}\\|_{2}}{\\|x_{k+1}\\|_{2}}(\\sigma_{k+1}\\alpha_{k}-\\sqrt{2\\varepsilon})=\\frac{\\|x_{k}\\|_{2}\\sigma_{k+1}}{\\|x_{k+1}\\|_{2}}\\Big(\\alpha_{k}-\\frac{\\sqrt{2\\varepsilon}}{\\sigma_{k+1}}\\Big)\\,.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Further note that $\\|x_{k+1}\\|_{2}\\leqslant\\sigma_{k+1}\\|x_{k}\\|_{2}$ , thus ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\alpha_{k+1}\\geqslant\\alpha_{k}-\\frac{\\sqrt{2\\varepsilon}}{\\sigma_{k+1}}\\,.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "By recurrence, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\alpha_{k}\\geqslant\\alpha_{2}-\\sqrt{2\\varepsilon}\\sum_{k^{\\prime}=2}^{k-1}\\frac{1}{\\sigma_{k+1}}\\geqslant\\alpha_{2}-\\sqrt{2\\varepsilon}\\sum_{k^{\\prime}=2}^{k-1}\\frac{1}{\\sqrt{\\sigma_{1}^{2}-\\varepsilon}}=\\alpha_{2}-\\frac{\\sqrt{2\\varepsilon}(k-2)}{\\sqrt{\\sigma_{1}^{2}-\\varepsilon}}\\,.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Coming back to (27), we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|x_{k+1}\\|_{2}\\geqslant\\langle x_{k+1},u_{k+1}\\rangle\\geqslant\\|x_{k}\\|_{2}(\\sigma_{k+1}\\alpha_{k}-\\sqrt{2\\varepsilon})\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "thus by recurrence, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\|x_{L}\\|_{2}\\geqslant\\|x_{2}\\|_{2}\\prod_{k=2}^{L-1}(\\sigma_{k+1}\\alpha_{k}-\\sqrt{2\\varepsilon})\\geqslant\\|x_{2}\\|_{2}\\prod_{k=2}^{L-1}\\left(\\sqrt{\\sigma_{1}^{2}-\\varepsilon}\\Big(\\alpha_{2}-\\displaystyle\\frac{\\sqrt{2\\varepsilon}(k-2)}{\\sqrt{\\sigma_{1}^{2}-\\varepsilon}}\\Big)-\\sqrt{2\\varepsilon}\\right)}\\\\ {\\displaystyle\\geqslant\\|x_{2}\\|_{2}\\prod_{k=2}^{L-1}\\left(\\sqrt{\\sigma_{1}^{2}-\\varepsilon}\\alpha_{2}-\\sqrt{2\\varepsilon}k\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Finally, by definition, $\\begin{array}{r}{\\alpha_{2}=\\langle\\frac{x_{2}}{\\|x_{2}\\|_{2}},u_{2}\\rangle}\\end{array}$ . Since $x_{2}=W_{2}x_{1}=W_{2}v_{2}=\\sigma_{2}u_{2}$ , we obtain that $\\alpha_{2}=1$ , and thus ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|W_{L}\\ldots W_{2}\\|_{2}\\geqslant\\|x_{L}\\|_{2}}\\\\ &{\\qquad\\qquad\\geqslant\\sigma_{2}\\prod_{k=2}^{L-1}\\Big(\\sqrt{\\sigma_{1}^{2}-\\varepsilon}-\\sqrt{2\\varepsilon}k\\Big)}\\\\ &{\\qquad\\qquad\\geqslant\\sqrt{\\sigma_{1}^{2}-\\varepsilon}\\Big(\\sqrt{\\sigma_{1}^{2}-\\varepsilon}-\\sqrt{2\\varepsilon}(L-1)\\Big)^{L-2}}\\\\ &{\\qquad\\qquad\\geqslant\\Big(\\sqrt{\\sigma_{1}^{2}}-\\sqrt{\\varepsilon}-\\sqrt{2\\varepsilon}(L-1)\\Big)^{L-1}}\\\\ &{\\qquad\\qquad\\geqslant\\big(\\sigma_{1}-\\sqrt{2\\varepsilon}L\\big)^{L-1}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We finally get ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\lVert W_{L}\\dots W_{2}\\rVert_{2}\\geqslant\\left(C-\\sqrt{2\\varepsilon}L\\right)^{L-1},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "which is a positive quantity by definition of $C$ . ", "page_idx": 26}, {"type": "text", "text": "Small spectral norm. We now inspect the case where (25) is not satisfied, that is, $\\sigma_{1}(t)\\leqslant C$ . First note that, by the formula (6) for the gradient of $R^{1}$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\|\\nabla R^{1}(w_{\\mathrm{prod}})\\|_{2}\\leqslant2\\Lambda\\|w^{\\star}-w_{\\mathrm{prod}}\\|_{2}\\leqslant2\\Lambda(\\|w^{\\star}\\|_{2}+\\|w_{\\mathrm{prod}}\\|_{2})\\,.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Thus, for $\\lVert w_{\\mathrm{prod}}\\rVert_{2}\\leqslant\\lVert w^{\\star}\\rVert_{2}$ , $w_{\\mathrm{prod}}\\mapsto R^{1}(w_{\\mathrm{prod}})$ is $4\\Lambda\\Vert w^{\\star}\\Vert_{2}$ -Lipschitz. Let us use this property to lower bound $\\|w_{\\mathrm{prod}}(t)\\|_{2}$ by a constant independent of $t$ , for $t\\geqslant1$ . Either we have $\\|w_{\\mathrm{prod}}(t)\\|_{2}\\geqslant$ $\\|\\boldsymbol{w}^{\\star}\\|_{2}$ , or $\\|w_{\\mathrm{prod}}\\|_{2}\\leqslant\\|w^{\\star}\\|_{2}$ , but then, by the Lipschitzness property, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|R^{1}(w_{\\mathrm{prod}}(t))-R_{0}|\\leqslant4\\Lambda\\|w^{\\star}\\|_{2}\\|w_{\\mathrm{prod}}(t)-0\\|_{2}=4\\Lambda\\|w^{\\star}\\|_{2}\\|w_{\\mathrm{prod}}(t)\\|_{2}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where we recall that $R_{0}$ is the risk associated to the null parameters. Furthermore, for $t\\geqslant1$ , ", "page_idx": 26}, {"type": "text", "text": "(The risk is decreasing along the gradient flow) ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R^{1}(w_{\\mathrm{prod}}(t))=R^{L}(\\mathcal{W}(t))}\\\\ &{\\leqslant R^{L}(\\mathcal{W}(1))}\\\\ &{\\leqslant R^{L}(\\mathcal{W}(0))}\\\\ &{\\leqslant R_{0}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Thus, for $t\\geqslant1$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\n|R^{1}(w_{\\mathrm{prod}}(t))-R_{0}|>R_{0}-R^{L}(\\mathcal{W}(1))>0\\,.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "To summarize, we proved that, for $t\\geqslant1$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\|w_{\\mathrm{prod}}(t)\\|_{2}\\geqslant\\operatorname*{min}\\Big(\\frac{R_{0}-R^{L}(\\mathcal{W}(1))}{4\\Lambda\\|w^{\\star}\\|_{2}},\\|w^{\\star}\\|_{2}\\Big)>0\\,,\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where we recall that $\\|\\boldsymbol{w}^{\\star}\\|_{2}>0$ by assumption (see Section 2). Furthermore, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\|w_{\\mathrm{prod}}(t)\\|_{2}\\leqslant\\|W_{L}(t)\\ldots W_{2}(t)\\|_{2}\\|W_{1}(t)\\|_{2}=\\|W_{L}(t)\\ldots W_{2}(t)\\|_{2}\\sigma_{1}(t)\\,.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Then, for $t\\geqslant1$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|W_{L}(t)\\ldots W_{2}(t)\\|_{2}\\geqslant\\frac{1}{\\sigma_{1}(t)}\\operatorname*{min}\\left(\\frac{R_{0}-R^{L}(\\mathcal{W}(1))}{4\\Lambda\\|w^{\\star}\\|_{2}},\\|w^{\\star}\\|_{2}\\right)}\\\\ {\\geqslant\\frac{1}{C}\\operatorname*{min}\\left(\\frac{R_{0}-R^{L}(\\mathcal{W}(1))}{4\\Lambda\\|w^{\\star}\\|_{2}},\\|w^{\\star}\\|_{2}\\right).\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Conclusion. Combining (28) and (29), we obtain that, for $t\\geqslant1$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\|W_{L}(t)\\cdot\\cdot\\cdot W_{2}(t)\\|_{2}\\geqslant\\operatorname*{min}\\left(\\big(C-\\sqrt{2\\varepsilon}L\\big)^{L-1},\\frac{1}{C}\\operatorname*{min}\\left(\\frac{R_{0}-R^{L}(\\mathcal{W}(1))}{4\\Lambda\\|w^{\\star}\\|_{2}},\\|w^{\\star}\\|_{2}\\right)\\right)=:\\sqrt{\\mu_{1}}\\,,\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $\\mu_{1}>0$ by the proof above. Then, for $t\\geqslant1$ , by (24), ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\left\\|\\nabla_{1}R^{L}(W(t))\\right\\|_{F}^{2}\\geqslant\\mu_{1}\\|\\nabla R^{1}(w_{\\mathrm{prod}}(t))\\|_{2}^{2}\\,.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "By Lemma 8, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\|\\nabla R^{1}(w_{\\mathrm{prod}}(t))\\|_{2}^{2}\\geqslant4\\lambda(R^{L}(\\mathcal{W}(t))-R_{\\mathrm{min}})\\,.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Thus, taking $\\mu=4\\mu_{1}\\lambda>0$ , for $t\\geqslant1$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{L}\\left\\|\\nabla_{k}R^{L}(\\mathcal{W}(t))\\right\\|_{F}^{2}\\geqslant\\left\\|\\nabla_{1}R^{L}(\\mathcal{W}(t))\\right\\|_{F}^{2}\\geqslant\\mu(R^{L}(\\mathcal{W}(t))-R_{\\operatorname*{min}})\\,.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "B.5 Proof of Corollary 1 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "We first show that Assumption $\\left(A_{2}\\right)$ implies a number of estimates that are useful in the following. Since $\\|\\boldsymbol{w}^{\\star}\\|_{2}\\geqslant1$ , we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\left(\\frac{\\|w^{\\star}\\|_{2}}{2}\\right)^{1/L}\\geqslant\\left(\\frac{1}{2}\\right)^{1/L}\\geqslant\\frac{1}{2}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "thus ", "page_idx": 27}, {"type": "equation", "text": "$$\n8L\\varepsilon\\leqslant8L\\sqrt{\\varepsilon}\\leqslant\\frac{1}{4}\\leqslant\\Big(\\frac{\\|w^{\\star}\\|_{2}}{2}\\Big)^{1/L}\\leqslant\\big(2\\|w^{\\star}\\|_{2}\\big)^{1/L}\\,.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Moreover, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\left(\\frac{\\|w^{\\star}\\|_{2}}{2}\\right)^{2/L}\\geqslant\\frac{1}{4}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "so we also have ", "page_idx": 27}, {"type": "equation", "text": "$$\n8L\\varepsilon\\leqslant\\Big(\\frac{\\|w^{\\star}\\|_{2}}{2}\\Big)^{2/L}\\,.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Let us now prove the Corollary. We first note that Theorem 3 implies exponential convergence of the empirical risk to its minimum by Lemma 7. This also implies the (exponential) convergence of $w_{\\mathrm{prod}}$ to $w^{\\star}$ , since the covariance matrix $X^{\\top}X$ is full rank, so $w^{\\star}$ is the unique minimizer of $R^{1}$ , and, by Lemma 8, ", "page_idx": 27}, {"type": "equation", "text": "$$\nR^{L}(\\mathcal{W})-R_{\\mathrm{min}}=\\frac{1}{n}\\|X(w^{\\star}-w_{\\mathrm{prod}})\\|_{2}^{2}\\geqslant\\lambda\\|w^{\\star}-w_{\\mathrm{prod}}\\|_{2}^{2}\\,,\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Furthermore, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\|w_{\\mathrm{prod}}\\|_{2}=\\|W_{1}^{\\top}\\cdot\\cdot\\cdot W_{L}^{\\top}\\|_{2}\\leqslant\\|W_{1}\\|_{2}\\ldots\\|W_{L}\\|_{2}\\leqslant\\left(\\operatorname*{max}_{k=1}^{L}\\sigma_{k}\\right)^{L}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Let us show that, for $t$ large enough, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{k=1}\\sigma_{k}\\geqslant\\Big(\\frac{\\|w^{\\star}\\|}{2}\\Big)^{1/L}+\\varepsilon\\,.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "If it were not the case, then ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|w_{\\mathrm{prod}}\\|_{2}\\leqslant\\Big(\\displaystyle\\operatorname*{max}_{k=1}^{L}\\sigma_{k}\\Big)^{L}}\\\\ &{\\qquad\\quad\\leqslant\\Big(\\Big(\\displaystyle\\frac{\\|w^{*}\\|_{2}}{2}\\Big)^{1/L}+\\varepsilon\\Big)^{L}}\\\\ &{\\qquad\\quad=\\displaystyle\\frac{\\|w^{*}\\|_{2}}{2}\\Big(1+\\varepsilon\\Big(\\displaystyle\\frac{2}{\\|w^{*}\\|_{2}}\\Big)^{1/L}\\Big)^{L}}\\\\ &{\\qquad\\leqslant\\displaystyle\\frac{\\|w^{*}\\|_{2}}{2}\\Big(1+2L\\varepsilon\\Big(\\displaystyle\\frac{2}{\\|w^{*}\\|_{2}}\\Big)^{1/L}\\Big)\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the last inequality holds by Lemma 4 since ", "page_idx": 28}, {"type": "equation", "text": "$$\nL\\varepsilon\\Big(\\frac{2}{\\|w^{\\star}\\|_{2}}\\Big)^{1/L}\\leqslant1\\quad\\Leftrightarrow\\quad L\\varepsilon\\leqslant\\Big(\\frac{\\|w^{\\star}\\|_{2}}{2}\\Big)^{1/L}\\,,\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "which holds by (31). Then, we obtain ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\|\\boldsymbol{w}_{\\mathrm{prod}}\\|_{2}\\leqslant\\frac{3\\|\\boldsymbol{w}^{\\star}\\|_{2}}{4}\\,,\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "since ", "page_idx": 28}, {"type": "equation", "text": "$$\n1+2L\\varepsilon\\Big(\\frac{2}{\\|w^{\\star}\\|_{2}}\\Big)^{1/L}\\leqslant\\frac{3}{2}\\quad\\Leftrightarrow\\quad4L\\varepsilon\\leqslant\\Big(\\frac{\\|w^{\\star}\\|_{2}}{2}\\Big)^{1/L}\\,,\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "which also holds by (31). The inequality $\\begin{array}{r}{\\|w_{\\mathrm{prod}}\\|_{2}\\leqslant\\frac{3\\|w^{\\star}\\|_{2}}{4}}\\end{array}$ contradicts the fact that $w_{\\mathrm{prod}}$ converges to $w^{\\star}$ , thus proving (33). Then, we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{k=1}\\sigma_{k}^{2}\\geqslant\\Big(\\frac{\\|w^{\\star}\\|_{2}}{2}\\Big)^{2/L}+2\\varepsilon\\Big(\\frac{\\|w^{\\star}\\|_{2}}{2}\\Big)^{1/L}\\geqslant\\Big(\\frac{\\|w^{\\star}\\|_{2}}{2}\\Big)^{2/L}+\\varepsilon\\,,\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where the last inequality holds by (30). Furthermore, by Lemma 2, for all $k\\in\\{1,\\ldots,L\\}$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\sigma_{k}^{2}\\geqslant\\operatorname*{max}_{j=1}^{L}\\sigma_{j}^{2}-\\varepsilon\\,.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "This brings the first inequality of the Corollary. We now show the second inequality of the Corollary. First note that, by Lemma 2, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\|W_{k}-\\sigma_{k}u_{k}v_{k}^{\\top}\\|_{F}^{2}=\\|W_{k}\\|_{F}^{2}-\\|W_{k}\\|_{2}^{2}\\leqslant\\varepsilon\\,,\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "since both quantities are equal to the sum of the squared singular values of $W_{k}$ except the largest one. Then, adding and substracting, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\vert\\vert W_{1}^{T}-\\mathcal{N}_{1}^{T}-\\sigma_{1},\\cdots,\\sigma_{t+1}\\vert\\vert^{n},\\rangle\\phantom{\\vert\\vert\\mu_{1}\\rangle}}&{:=\\vert0,}\\\\ &{\\le\\displaystyle{\\sum_{i=1}^{k}\\vert W_{1}^{T}-\\underbrace{N_{1}^{i}(W_{1}^{T})}_{\\le1}\\vert0,\\rangle}}\\\\ &{\\le\\displaystyle{\\sum_{i=1}^{k}\\vert W_{1}^{T}-\\vert W_{1}^{T}\\vert\\vert0,\\rangle}}\\\\ &{\\le\\displaystyle{\\sum_{i=1}^{k}\\vert W_{1}\\vert\\vert\\vert\\frac{\\partial}{\\partial}\\rangle}\\phantom{\\vert\\mu_{1}\\rangle},\\phantom{\\vert\\mu_{1}\\rangle}}\\\\ &{\\le\\displaystyle{\\sum_{i=1}^{k}\\vert W_{1}\\vert\\vert\\frac{\\partial}{\\partial}\\rangle}\\phantom{\\vert\\mu_{1}\\rangle},\\phantom{\\vert\\mu_{1}\\rangle}}\\\\ &{\\le\\displaystyle{\\sum_{i=1}^{k}\\vert\\vert\\vert W_{1}^{T}-\\vert\\vert W_{1}^{T}\\rangle\\vert\\vert\\vert\\mu_{1}\\rangle}\\cdot\\vert\\vert\\sigma_{1+1},\\dots,\\sigma_{t}\\vert}\\\\ &{\\le\\displaystyle{\\sum_{i=1}^{k}\\vert\\vert\\frac{\\partial}{\\partial}\\rangle}\\phantom{\\vert\\mu_{1}\\rangle},}\\\\ &{\\le\\displaystyle{\\sum_{i=1}^{k}\\vert\\vert\\frac{\\partial}{\\partial}\\rangle}\\phantom{\\vert\\mu_{1}\\rangle},}\\\\ &{\\le\\displaystyle{\\sum_{i=1}^{k}\\vert\\vert\\frac{\\partial}{\\partial}\\rangle}\\phantom{\\vert\\mu_{1}\\rangle},}\\\\ &{:=\\displaystyle{\\mathcal{N}_{1}^{T}\\vert\\vert\\frac{\\partial}{\\partial}\\rangle}\\phantom{\\vert\\mu_{1}\\rangle}\\frac{\\phantom{\\vert\\mu_{1}\\rangle}}{\\vert\\mu_{1}\\rangle}}\\\\ &{\\leqslant\\displaystyle{\\mathcal{N}_{1}^{T}\\vert\\vert\\frac{\\partial}{\\partial}\\rangle}\\phantom{\\vert\\mu_{1}\\rangle},}\\\\ &{\\leqslant\\displaystyle{\\mathcal{N}_{2}^{T}\\vert\\vert\\frac{\\partial}{\\partial}\\rangle}\\phantom{\\vert\\mu_{1}\\rangle},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "by the first inequality of the Corollary. Moreover, using again the first inequality of the Corollary and Lemma 2, we have that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\langle v_{k+1},u_{k}\\rangle^{2}\\geqslant1-\\frac{\\varepsilon}{\\sigma_{k+1}^{2}}\\geqslant1-\\frac{\\varepsilon}{\\left(\\frac{\\|w^{\\star}\\|_{2}}{2}\\right)^{2/L}}\\,.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Since $u_{L}=1$ , we deduce that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Vert\\sigma_{1}\\ldots\\sigma_{L}v_{1}u_{1}^{\\top}\\ldots v_{L}u_{L}^{\\top}\\Vert_{2}=\\displaystyle\\prod_{k=1}^{L}\\sigma_{k}\\Vert v_{1}\\Vert_{2}\\displaystyle\\prod_{k=1}^{L-1}u_{k}^{\\top}v_{k+1}}&{}\\\\ {\\geqslant\\displaystyle\\prod_{k=1}^{L}\\sigma_{k}\\left(1-\\frac{\\varepsilon}{\\left(\\frac{\\left\\Vert v^{*}\\right\\Vert_{2}}{2}\\right)^{2/L}}\\right)^{\\frac{L-1}{2}}}&{}\\\\ &{\\geqslant\\displaystyle\\prod_{k=1}^{L}\\sigma_{k}\\left(1-\\frac{\\left(L-1\\right)\\varepsilon}{\\left(\\frac{\\left\\Vert v^{*}\\right\\Vert_{2}}{2}\\right)^{2/L}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where in the last step we used Lemma 4 which is valid since ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\frac{\\varepsilon}{\\left(\\frac{\\|w^{\\star}\\|_{2}}{2}\\right)^{2/L}}\\leqslant\\frac{1}{2}\\leqslant\\varepsilon\\leqslant\\frac{1}{2}\\Big(\\frac{\\|w^{\\star}\\|_{2}}{2}\\Big)^{2/L}\\,,\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "which holds by (32). By the triangular inequality, we now have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\prod_{k=1}^{L}\\sigma_{k}\\left(1-\\frac{(L-1)\\varepsilon}{\\left(\\frac{\\|w^{*}\\|_{\\cdot}}{2}\\right)^{2/L}}\\right)\\leqslant\\|\\sigma_{1}\\ldots\\sigma_{L}v_{1}u_{1}^{\\top}\\ldots v_{L}u_{L}^{\\top}\\|_{2}}&{}\\\\ {\\displaystyle\\leqslant\\|W_{1}^{\\top}\\ldots W_{L}^{\\top}\\|_{2}+\\|W_{1}^{\\top}\\ldots W_{L}^{\\top}-\\sigma_{1}\\ldots\\sigma_{L}v_{1}u_{1}^{\\top}\\ldots v_{L}u_{L}^{\\top}\\|_{2}}&{}\\\\ {\\displaystyle\\leqslant\\|W_{1}^{\\top}\\ldots W_{L}^{\\top}\\|_{2}+\\frac{L\\sqrt{\\varepsilon}}{\\left(\\frac{\\|w^{*}\\|_{2}}{2}\\right)^{1/L}}\\displaystyle\\prod_{k=1}^{L}\\sigma_{k}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Thus ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\prod_{k=1}^{L}\\sigma_{k}\\left(1-\\frac{(L-1)\\varepsilon}{\\left(\\frac{\\|w^{\\star}\\|_{2}}{2}\\right)^{2/L}}-\\frac{L\\sqrt{\\varepsilon}}{\\left(\\frac{\\|w^{\\star}\\|_{2}}{2}\\right)^{1/L}}\\right)\\leqslant\\|W_{1}^{\\top}\\cdot\\cdot\\cdot W_{L}^{\\top}\\|_{2}\\,.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Using (31) and (32), ", "page_idx": 29}, {"type": "equation", "text": "$$\n1-\\frac{(L-1)\\varepsilon}{\\left(\\frac{\\|w^{\\star}\\|_{2}}{2}\\right)^{2/L}}-\\frac{L\\sqrt{\\varepsilon}}{\\left(\\frac{\\|w^{\\star}\\|_{2}}{2}\\right)^{1/L}}\\geqslant1-\\frac18-\\frac18=\\frac34\\,.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Moreover, the product of the singular values can be lower-bounded by the smallest one to the power $L$ , so ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\frac{3(\\operatorname*{min}{\\sigma_{k}})^{L}}{4}\\leqslant\\|W_{1}^{\\top}\\cdot\\cdot\\cdot W_{L}^{\\top}\\|_{2}\\,.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Let us show that, for $t$ large enough, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\operatorname*{min}\\sigma_{k}\\leqslant(2\\|w^{\\star}\\|_{2})^{1/L}-\\varepsilon\\,.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "If it were not the case, we would have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\|W_{1}^{\\top}\\ldots W_{L}^{\\top}\\|_{2}\\geqslant\\frac{3\\left((2\\|w^{\\star}\\|_{2})^{1/L}-\\varepsilon\\right)^{L}}{4}}}\\\\ &{}&{\\quad=\\frac{3}{2}\\|w^{\\star}\\|_{2}\\Bigl(1-\\frac{\\varepsilon}{(2\\|w^{\\star}\\|_{2})^{1/L}}\\Bigr)^{L}}\\\\ &{}&{\\quad\\geqslant\\frac{3}{2}\\|w^{\\star}\\|_{2}\\Bigl(1-\\frac{2L\\varepsilon}{(2\\|w^{\\star}\\|_{2})^{1/L}}\\Bigr)}\\\\ &{}&{\\quad\\geqslant\\frac{9}{8}\\|w^{\\star}\\|_{2}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where the second inequality holds by Lemma 4 since ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\frac{\\varepsilon}{(2\\|w^{\\star}\\|_{2})^{1/L}}\\leqslant\\frac{1}{2}\\,,\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "by (31), and the third since ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\frac{2L\\varepsilon}{(2\\|w^{\\star}\\|_{2})^{1/L}}\\leqslant\\frac14\\quad\\Leftrightarrow\\quad L\\varepsilon\\leqslant\\frac{(2\\|w^{\\star}\\|_{2})^{1/L}}8\\,,\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "also by (31). Since $9/8>1$ , the inequality (34) is a contradiction with the fact that $\\|W_{1}^{\\top}\\dots W_{L}^{\\top}\\|_{2}=$ $\\lVert\\boldsymbol{w}_{\\mathrm{prod}}\\rVert_{2}\\to\\lVert\\boldsymbol{w}^{\\star}\\rVert_{2}$ , showing that, for $t$ large enough, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\operatorname*{min}\\sigma_{k}\\leqslant(2\\|w^{\\star}\\|_{2})^{1/L}-\\varepsilon\\,.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Thus ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\operatorname*{min}\\sigma_{k}^{2}\\leqslant(2\\|w^{\\star}\\|_{2})^{2/L}+\\varepsilon^{2}-2\\varepsilon(2\\|w^{\\star}\\|_{2})^{1/L}\\,.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "By Lemma 2, for all $k\\in\\{1,\\ldots,L\\}$ , ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sigma_{k}^{2}\\leqslant\\operatorname*{min}\\sigma_{j}^{2}+\\varepsilon}\\\\ &{\\quad\\leqslant(2\\|w^{\\star}\\|_{2})^{2/L}+\\varepsilon^{2}-2\\varepsilon(2\\|w^{\\star}\\|_{2})^{1/L}+\\varepsilon}\\\\ &{\\quad\\leqslant(2\\|w^{\\star}\\|_{2})^{2/L}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "since ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\varepsilon^{2}-2\\varepsilon(2\\|w^{\\star}\\|_{2})^{1/L}+\\varepsilon\\leqslant0\\quad\\Leftrightarrow\\quad\\varepsilon\\leqslant2(2\\|w^{\\star}\\|_{2})^{1/L}-1\\,,\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "which holds true by Assumption $\\left(A_{2}\\right)$ since ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\varepsilon\\leqslant1\\leqslant2(2\\|w^{\\star}\\|_{2})^{1/L}-1\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "since $\\|\\boldsymbol{w}^{\\star}\\|_{2}\\geqslant1$ . ", "page_idx": 30}, {"type": "text", "text": "B.6 Proof of Corollary 2 ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "The lower bound unfolds by definition of $S_{\\mathrm{min}}$ since ${\\mathcal{W}}^{\\mathrm{SI}}$ is a minimizer of the empirical risk. To obtain the upper bound, we proceed similarly to the proof of Theorem 2. For simplicity, we denote $\\mathcal{W}=\\mathcal{W}^{\\mathrm{SI}}$ in the remainder of the proof. We have, as in the proof of Theorem 2, ", "page_idx": 30}, {"type": "equation", "text": "$$\nS(\\mathcal{W})^{2}=\\operatorname*{lim}_{\\xi\\rightarrow0}\\operatorname*{sup}_{\\|W_{k}-\\tilde{W}_{k}\\|_{F}\\leqslant\\xi}\\frac{\\sum_{k=1}^{L}\\|\\nabla_{k}R^{L}(\\mathcal{W})-\\nabla_{k}R^{L}(\\tilde{\\mathcal{W}})\\|_{F}^{2}}{\\sum_{k=1}^{L}\\|W_{k}-\\tilde{W}_{k}\\|_{F}^{2}}\\,,\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "with ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta_{k}:=\\nabla_{k}R^{L}(\\mathcal{W})-\\nabla_{k}R^{L}(\\tilde{\\mathcal{W}})}\\\\ &{\\quad=-W_{k+1}^{\\top}\\ldots W_{L}^{\\top}\\nabla R^{1}(\\tilde{w}_{\\mathrm{prod}})^{\\top}W_{1}^{\\top}\\ldots W_{k-1}^{\\top}+\\mathcal{O}(\\xi^{2})\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "and ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\nabla R^{1}(\\tilde{w}_{\\mathrm{prod}})=\\frac{2}{n}X^{\\top}X\\Big(\\sum_{k=1}^{L}W_{1}^{\\top}\\,.\\,.\\,W_{k-1}^{\\top}(\\tilde{W}_{k}^{\\top}-W_{k}^{\\top})W_{k+1}^{\\top}\\,.\\,.\\,W_{L}^{\\top}\\Big)+{\\mathcal O}(\\xi^{2})\\,.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We recall that, as in the proof of Theorem 2, the notation $\\scriptscriptstyle\\mathcal{O}$ is taken with respect to the limit when $\\xi\\rightarrow0$ , everything else being fixed. By subadditivity of the operator norm and by Corollary 1, we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\|\\nabla R^{1}(\\tilde{w}_{\\mathrm{prod}})\\|_{2}\\leqslant2\\Lambda\\sum_{k=1}^{L}\\|W_{1}\\|_{2}\\ldots\\|W_{k-1}\\|_{2}\\|\\tilde{W}_{k}-W_{k}\\|_{2}\\|W_{k+1}\\|_{2}\\ldots\\|W_{L}\\|_{2}+\\mathcal{O}(\\xi^{2})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leqslant2\\Lambda(2\\|w^{\\star}\\|_{2})^{(L-1)/L}\\displaystyle\\sum_{k=1}^{L}\\|W_{k}-\\tilde{W}_{k}\\|_{2}+\\mathcal{O}(\\xi^{2})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Moving on to bounding the Frobenius norm of $\\Delta_{k}$ , we observe that the dominating term of this matrix decomposes as a rank-one matrix. Thus, again by subadditivity of the operator norm, equation (36) and Corollary 1, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\Delta_{k}\\|_{F}=\\|W_{k+1}^{\\top}\\ldots W_{L}^{\\top}\\|_{2}\\|\\nabla R^{1}(\\tilde{w}_{\\mathrm{prod}})^{\\top}W_{1}^{\\top}\\ldots W_{k-1}^{\\top}\\|_{2}+\\mathcal{O}(\\xi^{2})}\\\\ &{\\qquad\\quad\\leqslant\\|W_{k+1}\\|_{2}\\ldots\\|W_{L}\\|_{2}\\|\\nabla R^{1}(\\tilde{w}_{\\mathrm{prod}})\\|_{2}\\|W_{1}\\|_{2}\\ldots\\|W_{k-1}\\|_{2}+\\mathcal{O}(\\xi^{2})}\\\\ &{\\qquad\\quad\\leqslant2\\Lambda(2\\|w^{\\star}\\|_{2})^{2(L-1)/L}\\displaystyle\\sum_{k=1}^{L}\\|W_{k}-\\tilde{W}_{k}\\|_{2}+\\mathcal{O}(\\xi^{2})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Then ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\|\\Delta_{k}\\|_{F}^{2}\\leqslant4\\Lambda^{2}(2\\|w^{\\star}\\|_{2})^{4(L-1)/L}\\Big(\\displaystyle\\sum_{k=1}^{L}\\|W_{k}-\\tilde{W}_{k}\\|_{2}\\Big)^{2}+\\mathcal{O}(\\xi^{3})}}\\\\ &{}&{\\leqslant4L\\Lambda^{2}(2\\|w^{\\star}\\|_{2})^{4(L-1)/L}\\displaystyle\\sum_{k=1}^{L}\\|W_{k}-\\tilde{W}_{k}\\|_{2}^{2}+\\mathcal{O}(\\xi^{3})}\\\\ &{}&{\\leqslant4L\\Lambda^{2}(2\\|w^{\\star}\\|_{2})^{4(L-1)/L}\\displaystyle\\sum_{k=1}^{L}\\|W_{k}-\\tilde{W}_{k}\\|_{F}^{2}+\\mathcal{O}(\\xi^{3})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Thus, by (35), ", "page_idx": 31}, {"type": "equation", "text": "$$\nS(\\mathcal{W})^{2}\\leqslant\\operatorname*{lim}_{\\xi\\rightarrow0}4L^{2}\\Lambda^{2}(2\\|w^{\\star}\\|_{2})^{4(L-1)/L}+\\mathcal{O}(\\xi)\\leqslant2^{6}L^{2}\\Lambda^{2}\\|w^{\\star}\\|_{2}^{4-\\frac{4}{L}}\\,.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Therefore ", "page_idx": 31}, {"type": "equation", "text": "$$\nS(\\mathcal{W})\\leqslant8L\\Lambda\\|w^{\\star}\\|_{2}^{2-\\frac{2}{L}}\\,,\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "which concludes the proof by the second lower bound on $S_{\\mathrm{min}}$ of Theorem 2, where here $\\|p\\|=1$ and $a\\geqslant\\lambda>0$ . ", "page_idx": 31}, {"type": "text", "text": "B.7 Proof of Theorem 4 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "To alleviate notations, we omit in this proof to write the explicit dependence of parameters on time. Starting from (5), since the gradient decomposes as a rank-one matrix, we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla_{k}R^{L}(\\mathcal{W})\\|_{F}^{2}=\\|\\boldsymbol{W}_{k+1}^{\\top}\\cdot\\cdot\\cdot\\boldsymbol{W}_{L}^{\\top}p\\|_{2}^{2}\\|\\nabla R^{1}(\\boldsymbol{w}_{\\mathrm{prod}})^{\\top}\\boldsymbol{W}_{1}^{\\top}\\cdot\\cdot\\cdot\\boldsymbol{W}_{k-1}^{\\top}\\|_{2}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\geqslant\\sigma_{\\operatorname*{min}}^{2}(\\boldsymbol{W}_{k+1}^{\\top}\\cdot\\cdot\\cdot\\boldsymbol{W}_{L}^{\\top})\\|p\\|_{2}^{2}\\sigma_{\\operatorname*{min}}^{2}(\\boldsymbol{W}_{1}^{\\top}\\cdot\\cdot\\cdot\\boldsymbol{W}_{k-1}^{\\top})\\|\\nabla R^{1}(\\boldsymbol{w}_{\\mathrm{prod}})\\|_{2}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "By Lemma 8, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla R^{1}(w_{\\mathrm{prod}})\\|_{2}^{2}\\geqslant4\\lambda(R^{L}(\\mathcal{W})-R_{\\mathrm{min}})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Recall the notation introduced in Section 5 ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\Pi_{L:k}:=W_{L}\\;.\\;.\\;.\\;W_{k}\\quad\\mathrm{and}\\quad\\Pi_{k:1}:=W_{k}\\;.\\;.\\;.\\;W_{1}\\;.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Then ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla_{k}R^{L}(\\mathcal{W})\\|_{F}^{2}\\geqslant4\\lambda\\|p\\|_{2}^{2}\\sigma_{\\operatorname*{min}}^{2}(\\Pi_{L:k+1})\\sigma_{\\operatorname*{min}}^{2}(\\Pi_{k-1:1})\\big(R^{L}(\\mathcal{W})-R_{\\operatorname*{min}}\\big)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We now let $u>0$ (whose value will be specified next), and ", "page_idx": 31}, {"type": "equation", "text": "$$\nt^{*}=\\operatorname*{inf}\\left\\{t\\in\\mathbb{R}_{+},\\exists k\\in\\{1,\\dots,L\\},\\|\\theta_{k}\\|_{F}>\\frac{1}{64}\\exp(-2s^{2}-4u)\\right\\}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Let us lower bound the minimum singular values of $\\Pi_{k}$ : and $\\Pi_{:k}$ uniformly for $t\\in[0,t^{\\star}]$ by Lemma 3. By definition of $t^{\\star}$ , for $t\\leqslant t^{\\star}$ and for any $\\begin{array}{r}{k\\in\\{1,\\ldots,L\\},\\,\\|\\theta_{k}\\|_{2}\\leqslant\\frac{1}{64}\\overset{\\cdot}{\\exp}(-2s^{2}-4\\overset{\\cdot}{u})}\\end{array}$ . Therefore, renaming ${\\tilde{C}}_{1}$ to ${\\tilde{C}}_{4}$ the constants of Lemma 3, and taking $u=\\tilde{C}_{3}$ , we get that, if ", "page_idx": 31}, {"type": "equation", "text": "$$\nL\\geqslant\\operatorname*{max}\\left(\\tilde{C}_{1},\\Big(\\frac{\\tilde{C}_{3}}{\\tilde{C}_{4}}\\Big)^{4}\\right),\\quad d\\geqslant\\tilde{C}_{2}\\,,\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "then, with probability at least ", "page_idx": 31}, {"type": "equation", "text": "$$\n1-8\\exp\\Big(-\\frac{d\\tilde{C}_{3}^{2}}{32s^{2}}\\Big)\\,,\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "it holds for $t\\leqslant t^{\\star}$ and for all $k\\in\\{1,\\ldots,L\\}$ that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\|\\Pi_{k:1}\\|_{2}\\leqslant4\\exp\\Big(\\frac{s^{2}}{2}+\\tilde{C}_{3}\\Big)\\,,\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "and ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\sigma_{\\mathrm{min}}(\\Pi_{k:1})\\geqslant\\frac{1}{4}\\exp\\Big(-\\frac{2s^{2}}{d}-\\tilde{C}_{3}\\Big)\\,.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "By symmetry, the same statement holds for $\\Pi_{L;k}$ instead of $\\Pi_{k:1}$ with the same probability. By the union bound, the event ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\cal E}_{1}:=\\displaystyle\\bigcap_{1\\leqslant k\\leqslant L}\\left\\{\\|\\Pi_{k:1}\\|_{2},\\|\\Pi_{L:k}\\|_{2}\\leqslant4\\exp\\Big(\\displaystyle\\frac{s^{2}}{2}+\\tilde{C}_{3}\\Big)\\right.}}\\\\ {{\\mathrm{~and~}\\sigma_{\\operatorname*{min}}(\\Pi_{k:1}),\\sigma_{\\operatorname*{min}}(\\Pi_{L:k})\\geqslant\\displaystyle\\frac{1}{4}\\exp\\Big(-\\displaystyle\\frac{2s^{2}}{d}-\\tilde{C}_{3}\\Big)\\right\\}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "holds with probability at least ", "page_idx": 32}, {"type": "equation", "text": "$$\n1-16\\exp\\Big(-\\frac{d\\tilde{C}_{3}^{2}}{32s^{2}}\\Big)\\,.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Let now ", "page_idx": 32}, {"type": "equation", "text": "$$\nE_{2}:=\\left\\{R^{L}(\\mathcal{W}(0))-R_{\\operatorname*{min}}\\leqslant\\frac{C_{4}\\lambda^{2}\\|p\\|_{2}^{2}}{\\Lambda}\\right\\}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "and $E_{3}$ the event that the gradient flow dynamics converge to a global minimizer $\\mathcal{W}^{\\mathrm{RI}}$ of the risk satisfying the statement (8). We show next that, if $E_{1}$ and $E_{2}$ hold, then $E_{3}$ must hold, which shall conclude the proof of the Theorem with ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{C_{1}=\\operatorname*{max}\\left(\\tilde{C}_{1},\\Big(\\displaystyle\\frac{\\tilde{C}_{3}}{\\tilde{C}_{4}}\\Big)^{4}\\right),\\quad C_{2}=\\tilde{C}_{2}\\,,\\quad C_{3}=\\displaystyle\\frac{\\tilde{C}_{3}^{2}}{32s^{2}}\\,,}\\\\ &{C_{4}=2^{-36}\\exp\\Big(-4s^{2}-\\displaystyle\\frac{16s^{2}}{\\tilde{C}_{2}}-20\\tilde{C}_{3}\\Big)\\,,\\quad C_{5}=\\displaystyle\\frac{1}{64}\\exp(-2s^{2}-4\\tilde{C}_{3})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Under $E_{1}$ , we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\sigma_{\\mathrm{min}}^{2}\\bigl(\\Pi_{L:k+1}\\bigr)\\sigma_{\\mathrm{min}}^{2}\\bigl(\\Pi_{k-1:1}\\bigr)\\geqslant\\frac{1}{2^{8}}\\exp\\Big(-\\frac{8s^{2}}{d}-4\\tilde{C}_{3}\\Big)\\,,\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "thus by (37) ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\|\\nabla_{k}R^{L}(\\mathcal{W})\\|_{F}^{2}\\geqslant\\frac{1}{2^{6}}\\exp\\Big(-\\frac{8s^{2}}{d}-4\\tilde{C}_{3}\\Big)\\lambda\\|p\\|_{2}^{2}\\big(R^{L}(\\mathcal{W})-R_{\\operatorname*{min}}\\big)\\,.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Therefore we get the $\\mathrm{PL}$ condition, for $t\\leqslant t^{\\star}$ , ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{L}\\left\\|\\nabla_{k}R^{L}(\\mathcal{W})\\right\\|_{F}^{2}\\geqslant\\mu(R^{L}(\\mathcal{W})-R_{\\operatorname*{min}})\\,,\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "with ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mu:=\\frac{1}{2^{6}}\\exp\\Big(-\\frac{8s^{2}}{d}-4\\tilde{C}_{3}\\Big)\\lambda\\|p\\|^{2}L\\,.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "By Lemma 7, this implies that, for $t\\leqslant t^{\\star}$ , ", "page_idx": 32}, {"type": "equation", "text": "$$\nR^{L}(\\mathcal{W})-R_{\\operatorname*{min}}\\leqslant(R^{L}(\\mathcal{W}(0))-R_{\\operatorname*{min}})e^{-\\mu t}\\,.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Let us now show that $t^{\\star}=\\infty$ . We have, since $\\theta(0)=0$ and by definition of the gradient flow, ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\|\\theta_{k}\\|_{F}=\\|\\theta_{k}-\\theta_{k}(0)\\|_{F}\\leqslant L\\int_{0}^{t}\\|\\nabla_{k}R^{L}(\\mathcal{W}(\\tau))\\|_{F}d\\tau\\,.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "We now upper bound the gradient as follows: starting again from (5), ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla_{k}R^{L}(\\mathcal{W})\\|_{F}=\\|W_{k+1}^{\\top}\\ldots W_{L}^{\\top}p\\|_{2}\\|\\nabla R^{1}(w_{\\mathrm{prod}})^{\\top}W_{1}^{\\top}\\ldots W_{k-1}^{\\top}\\|_{2}}\\\\ &{\\qquad\\qquad\\qquad\\leqslant\\|W_{k+1}^{\\top}\\ldots W_{L}^{\\top}\\|_{2}\\|p\\|_{2}\\|W_{1}^{\\top}\\ldots W_{k-1}^{\\top}\\|_{2}\\|\\nabla R^{1}(w_{\\mathrm{prod}})\\|_{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "By Lemma 8, we get ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla_{k}R^{L}(\\mathcal{W})\\|_{F}\\leqslant2\\sqrt{\\Lambda}\\|W_{k+1}^{\\top}\\dots W_{L}^{\\top}\\|_{2}\\|p\\|_{2}\\|W_{1}^{\\top}\\dots W_{k-1}^{\\top}\\|_{2}\\sqrt{R^{L}(\\mathcal{W})-R_{\\operatorname*{min}}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "By (39) and (41), for $t\\leqslant t^{\\star}$ , ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla_{k}R^{L}(\\mathcal{W})\\|_{F}\\leqslant16\\exp(s^{2}+2\\tilde{C}_{3})\\sqrt{\\Lambda}\\|p\\|_{2}\\sqrt{R^{L}(\\mathcal{W}(0))-R_{\\operatorname*{min}}}e^{-\\frac{\\mu}{2}t}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Plugging this into (42), we get, for $t\\leqslant t^{\\star}$ , ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\theta_{k}\\|_{F}\\leqslant16\\exp(s^{2}+2\\tilde{C}_{3})\\sqrt{\\Lambda}\\|p\\|_{2}\\sqrt{R^{L}(\\mathcal{W}(0))-R_{\\operatorname*{min}}}L\\int_{0}^{t}e^{-\\frac{\\mu}{2}\\tau}d\\tau}\\\\ &{\\qquad\\leqslant\\frac{32\\exp(s^{2}+2\\tilde{C}_{3})\\sqrt{\\Lambda}\\|p\\|_{2}\\sqrt{R^{L}(\\mathcal{W}(0))-R_{\\operatorname*{min}}}L}{\\mu}}\\\\ &{\\qquad=\\frac{2^{11}\\exp(s^{2}+\\frac{8s^{2}}{d}+6\\tilde{C}_{3})\\sqrt{\\Lambda}\\sqrt{R^{L}(\\mathcal{W}(0))-R_{\\operatorname*{min}}}}{\\lambda\\|p\\|_{2}}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where the last equality comes from the definition of $\\mu$ . By $E_{2}$ and by definition (40) of $C_{4}$ , for $t\\leqslant t^{\\star}$ , ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\Vert\\theta_{k}\\Vert_{F}\\leqslant2^{11}\\exp(s^{2}+\\frac{8s^{2}}{d}+6\\tilde{C}_{3})\\sqrt{C_{3}}\\leqslant\\frac{1}{128}\\exp(-2s^{2}-4\\tilde{C}_{3})\\,.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "If we had $t^{\\star}<\\infty$ , we would have, by definition of $t^{\\star}$ , $\\begin{array}{r}{\\|\\theta_{k}(t^{\\star})\\|_{F}\\geqslant\\frac{1}{64}\\exp(-2s^{2}-4\\tilde{C}_{3})}\\end{array}$ . This contradicts the equation above, showing that $t^{\\star}=\\infty$ . By (41), this implies convergence of the risk to its minimum. We also see by (44) that $\\mathbf{\\bar{V}}_{k}R^{L}(\\mathcal{W})$ is integrable, so $\\mathcal{W}$ has a limit as $t$ goes to infinity. This limit $\\mathcal{W}^{\\mathrm{RI}}$ is a minimizer of the risk and satisfies the condition (8) by definition of $t^{\\star}=\\infty$ . ", "page_idx": 33}, {"type": "text", "text": "Extension to multivariate regression. We emphasize that a very similar proof holds in the case of multivariate regression, where the neural network is defined as the linear map from $\\mathbb{R}^{d}$ to $\\mathbb{R}^{d}$ ", "page_idx": 33}, {"type": "equation", "text": "$$\nx\\mapsto W_{L}\\,.\\,.\\,.\\,W_{1}x\\,,\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and we now aim at minimizing the mean squared error loss ", "page_idx": 33}, {"type": "equation", "text": "$$\nR^{L}(\\mathcal{W})=\\frac{1}{n}\\|Y-W_{L}\\ldots W_{1}X^{\\top}\\|_{2}^{2}\\,,\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $X,Y\\in\\mathbb{R}^{n\\times d}$ . In this case, as shown in Zou et al. (2020); Sander et al. (2022), the following bounds on the gradient hold: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\|\\nabla_{k}R^{L}(\\mathcal{W})\\|_{F}^{2}\\geqslant4\\lambda\\sigma_{\\operatorname*{min}}^{2}(\\Pi_{L:k+1})\\sigma_{\\operatorname*{min}}^{2}(\\Pi_{k-1:1})\\big(R(\\mathcal{W})-R_{\\operatorname*{min}}\\big)\\,,\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla_{k}R^{L}(\\mathcal{W})\\|_{F}^{2}\\leqslant4\\Lambda\\|\\Pi_{L:k+1}\\|_{2}^{2}\\|\\Pi_{k-1:1}\\|_{2}^{2}(R(\\mathcal{W})-R_{\\operatorname*{min}})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Comparing with (37) and (43), the only difference is the absence of $\\|\\boldsymbol{p}\\|_{2}$ here. From there, the same computations as above hold (taking $\\|\\bar{p}\\|_{2}=1)$ ), and give the following result. ", "page_idx": 33}, {"type": "text", "text": "Theorem 5. There exist $C_{1},\\dots,C_{5}>0$ depending only on s such that, if $:L\\geqslant C_{1}$ and $d\\geqslant C_{2}$ , then, with probability at least ", "page_idx": 33}, {"type": "equation", "text": "$$\n1-16\\exp(-C_{3}d)\\,,\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "if ", "page_idx": 33}, {"type": "equation", "text": "$$\nR^{L}(\\mathcal{W}(0))-R_{\\mathrm{min}}\\leqslant\\frac{C_{4}\\lambda^{2}\\|p\\|_{2}^{2}}{\\Lambda}\\,,\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "the gradient flow dynamics (4) converge to a global minimizer $\\mathcal{W}^{\\mathrm{RI}}$ of the risk. Furthermore, the minimizer $\\mathcal{W}^{\\mathrm{RI}}$ satisfies ", "page_idx": 33}, {"type": "equation", "text": "$$\nW_{k}^{\\mathrm{RI}}=I+\\frac{s}{\\sqrt{L d}}N_{k}+\\frac{1}{L}\\theta_{k}^{\\mathrm{RI}}\\quad\\mathrm{with}\\quad\\|\\theta_{k}^{\\mathrm{RI}}\\|_{F}\\leqslant C_{5}\\,,\\quad1\\leqslant k\\leqslant L\\,.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "B.8 Proof of Lemma 3 ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "We begin by proving the result when $\\theta=0$ , then explain how to extend to any $\\theta$ in a ball around 0. Recall that $\\Pi_{k:1}$ denotes the product of weight matrices up to the $k$ -th. Let ", "page_idx": 34}, {"type": "equation", "text": "$$\nM_{s,u}=2\\exp\\left(\\frac{s^{2}}{2}+u\\right),\\quad m_{s,u}=\\frac{1}{2}\\exp\\Big(-\\frac{2s^{2}}{d}-u\\Big)\\,,\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "and denote by $A$ the event that for all $k$ , $\\|\\Pi_{k:1}\\|_{2}\\leqslant M_{s,u}$ and $B$ the event that for all $k$ , $\\sigma_{\\operatorname*{min}}(\\Pi_{k:1})\\geqslant$ $m_{s,u}$ . We begin by bounding the probability of $\\bar{A}$ , then bound the probability of $\\bar{B}\\cap A$ . ", "page_idx": 34}, {"type": "text", "text": "Useful identities. To this aim, we first introduce some notations and derive some useful identities. We let $\\mathcal{F}_{k}$ the filtration generated by the random variables $N_{1},\\ldots,N_{k}$ . For some $h_{0}\\in\\mathbb{R}^{d}$ , we let ", "page_idx": 34}, {"type": "equation", "text": "$$\nh_{k}=\\Pi_{k:1}h_{0}=\\left(I+{\\frac{s}{\\sqrt{L d}}}N_{k}\\right)\\ldots\\left(I+{\\frac{s}{\\sqrt{L d}}}N_{1}\\right)h_{0}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "In particular, $h_{k+1}$ and $h_{k}$ are related through ", "page_idx": 34}, {"type": "equation", "text": "$$\nh_{k+1}=\\Big(I+\\frac{s}{\\sqrt{L d}}N_{k+1}\\Big)h_{k}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Taking the squared norm, we get ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\|h_{k+1}\\|_{2}^{2}=\\|h_{k}\\|_{2}^{2}+\\frac{s^{2}}{L d}\\|N_{k+1}h_{k}\\|_{2}^{2}+\\frac{2s}{\\sqrt{L d}}h_{k}^{\\top}N_{k+1}h_{k}\\,.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Dividing by $\\|h_{k}\\|_{2}^{2}$ and taking the logarithm leads to ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\ln(\\|h_{k+1}\\|_{2}^{2})=\\ln(\\|h_{k}\\|_{2}^{2})+\\ln\\left(1+\\frac{s^{2}\\|N_{k+1}h_{k}\\|_{2}^{2}}{L d\\|h_{k}\\|_{2}^{2}}+\\frac{2s h_{k}^{\\top}N_{k+1}h_{k}}{\\sqrt{L d}\\|h_{k}\\|_{2}^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Let ", "page_idx": 34}, {"type": "equation", "text": "$$\nY_{k,1}=\\frac{s^{2}\\|N_{k+1}h_{k}\\|_{2}^{2}}{L d\\|h_{k}\\|_{2}^{2}}\\,,\\quad Y_{k,2}=\\frac{2s h_{k}^{\\top}N_{k+1}h_{k}}{\\sqrt{L d}\\|h_{k}\\|_{2}^{2}}\\,,\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "and $Y_{k}=Y_{k,1}+Y_{k,2}$ . Then, by Lemma 6, ", "page_idx": 34}, {"type": "equation", "text": "$$\nY_{k,1}|\\mathcal{F}_{k}\\sim\\frac{s^{2}}{L d}\\chi^{2}(d)\\quad\\mathrm{and}\\quad Y_{k,2}|\\mathcal{F}_{k}\\sim\\mathcal{N}\\Big(0,\\frac{4s^{2}}{L d}\\Big)\\,.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "This shows in particular that $Y_{k,1}$ and $Y_{k,2}$ are independent of $N_{1},\\ldots,N_{k}$ , and depend only on $N_{k+1}$ Then, letting ", "page_idx": 34}, {"type": "equation", "text": "$$\nS_{k,1}=\\sum_{j=0}^{k-1}Y_{j,1}\\quad{\\mathrm{and}}\\quad S_{k,2}=\\sum_{j=0}^{k-1}Y_{j,2}\\,,\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "both are sums of i.i.d. random variables. In particular, ", "page_idx": 34}, {"type": "equation", "text": "$$\nS_{L,1}\\sim\\frac{s^{2}}{L d}\\chi^{2}(L d)\\quad\\mathrm{and}\\quad S_{L,2}\\sim\\mathcal{N}\\Bigl(0,\\frac{4s^{2}}{d}\\Bigr)\\,.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Bound on $\\mathbb{P}(\\bar{A})$ . We first bound the deviation of the norm of $h_{k}=\\Pi_{k:1}h_{0}$ for any fixed $h_{0}\\in\\mathbb{R}^{d}$ , then conclude on the operator norm of $\\Pi_{k:1}$ by an $\\varepsilon$ -net argument. For any (fixed) $h_{0}\\,\\in\\,\\mathbb{R}^{d}$ and $u>0$ , we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\bigg(\\underset{1\\leqslant k\\leqslant L}{\\operatorname*{max}}\\,\\frac{\\|h_{k}\\|_{2}^{2}}{\\|h_{\\ell}\\|_{2}^{2}}\\ge\\exp(s^{2}+2u)\\bigg)}\\\\ &{\\qquad\\qquad=\\mathbb{P}\\bigg(\\underset{1\\leqslant k\\leqslant L}{\\operatorname*{max}}\\,\\ln(\\|h_{k}\\|_{2}^{2})-\\ln(\\|h_{0}\\|_{2}^{2})\\geqslant s^{2}+2u\\bigg)}\\\\ &{\\qquad\\qquad=\\mathbb{P}\\bigg(\\underset{1\\leqslant k\\leqslant L}{\\operatorname*{max}}\\sum_{j=0}^{k-1}\\ln\\Big(1+\\frac{s^{2}\\|N_{j+1}h_{j}\\|_{2}^{2}}{L d\\|h_{j}\\|_{2}^{2}}+\\frac{2s h_{j}^{\\top}N_{j+1}h_{j}}{\\sqrt{L d}\\|h_{j}\\|_{2}^{2}}\\Big)\\geqslant s^{2}+2u\\bigg)}\\\\ &{\\qquad\\qquad\\leqslant\\mathbb{P}\\bigg(\\underset{1\\leqslant k\\leqslant L}{\\operatorname*{max}}\\sum_{j=0}^{k-1}\\frac{s^{2}\\|N_{j+1}h_{j}\\|_{2}^{2}}{L d\\|h_{j}\\|_{2}^{2}}+\\frac{2s h_{j}^{\\top}N_{j+1}h_{j}}{\\sqrt{L d}\\|h_{j}\\|_{2}^{2}}\\geqslant s^{2}+2u\\bigg)\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "by using $\\ln(1+x)\\leqslant x$ . Thus ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb P\\bigg(\\displaystyle\\operatorname*{max}_{1\\leqslant k\\leqslant L}\\frac{\\|h_{k}\\|_{2}^{2}}{\\|h_{0}\\|_{2}^{2}}\\geqslant\\exp(s^{2}+2u)\\bigg)\\leqslant\\mathbb P\\bigg(\\displaystyle\\operatorname*{max}_{1\\leqslant k\\leqslant L}S_{k,1}+S_{k,2}\\geqslant s^{2}+2u\\bigg)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leqslant\\mathbb P\\bigg(\\displaystyle\\operatorname*{max}_{1\\leqslant k\\leqslant L}S_{k,1}\\geqslant s^{2}+u\\bigg)+\\mathbb P\\bigg(\\displaystyle\\operatorname*{max}_{1\\leqslant k\\leqslant L}S_{k,2}\\geqslant u\\bigg)}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "by the union bound. We now study the two deviation probabilities separately. Beginning by the first one, recall that $\\ensuremath{\\frac{L d}{s^{2}}}S_{L,1}$ follows a $\\chi^{2}(L d)$ distribution. Chi-squared random variables are subexponential, and more precisely satisfy the following property (Ghosh, 2021): if $X\\sim\\chi^{2}(c)$ and $u>0$ , then ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbb{P}(X\\geqslant c+u)\\leqslant\\exp\\Big(-\\frac{u^{2}}{4(c+u)}\\Big)\\,.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Since the $S_{k,1}$ are increasing, we have, for $u>0$ , ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb P\\Big(\\displaystyle\\operatorname*{max}_{1\\leqslant k\\leqslant L}S_{k,1}\\geqslant s^{2}+u\\Big)=\\mathbb P(S_{L,1}\\geqslant s^{2}+u)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\mathbb P\\Big(\\displaystyle\\frac{L d}{s^{2}}S_{L,1}\\geqslant L d+\\displaystyle\\frac{L d u}{s^{2}}\\Big)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leqslant\\exp\\Big(-\\displaystyle\\frac{L^{2}d^{2}u^{2}}{4s^{4}(L d+\\frac{L d u}{s^{2}})}\\Big)=\\exp\\Big(-\\displaystyle\\frac{L d u^{2}}{4s^{4}+u s^{2}}\\Big)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Moving on to $S_{k,2}$ , we have, for $u,\\lambda>0$ , ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Big(\\operatorname*{max}_{1\\leqslant k\\leqslant L}S_{k,2}\\geqslant u\\Big)=\\mathbb{P}\\Big(\\operatorname*{max}_{1\\leqslant k\\leqslant L}\\exp(\\lambda S_{k,2})\\geqslant\\exp(\\lambda u)\\Big)\\,.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Furthermore, $\\exp(\\lambda S_{k,2})$ is a sub-martingale, since $S_{k,2}$ is a martingale and by Jensen\u2019s inequality: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}(\\exp(\\lambda S_{k+1,2})|\\mathcal{F}_{k})\\geqslant\\exp(\\lambda\\mathbb{E}(S_{k+1,2}|\\mathcal{F}_{k}))=\\exp(\\lambda S_{k,2})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Thus, by Doob\u2019s martingale inequality, ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Big(\\operatorname*{max}_{1\\leqslant k\\leqslant L}S_{k,2}\\geqslant u\\Big)\\leqslant\\mathbb{E}\\big(\\exp(\\lambda S_{L,2})\\big)\\exp(-\\lambda u)\\,.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Furthermore, since $\\begin{array}{r}{S_{L,2}\\sim\\mathcal{N}(0,\\frac{4s^{2}}{d})}\\end{array}$ , ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbb{E}(\\exp(\\lambda S_{L,2}))=\\exp\\left(\\frac{4s^{2}\\lambda^{2}}{d}\\right).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Therefore, ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Big(\\operatorname*{max}_{1\\leqslant k\\leqslant L}S_{k,2}\\geqslant u\\Big)\\leqslant\\exp\\Big(-\\lambda u+\\frac{4s^{2}\\lambda^{2}}{d}\\Big)\\,.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "This quantity is minimal for $\\begin{array}{r}{\\lambda=\\frac{d u}{8s^{2}}}\\end{array}$ . We get, for all $u>0$ , ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Big(\\operatorname*{max}_{1\\leqslant k\\leqslant L}S_{k,2}\\geqslant u\\Big)\\leqslant\\exp\\Big(-\\frac{d u^{2}}{16s^{2}}\\Big)\\,.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Therefore, for any $u>0$ , ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Big(\\operatorname*{max}_{1\\leqslant k\\leqslant L}\\frac{\\|h_{k}\\|_{2}^{2}}{\\|h_{0}\\|_{2}^{2}}\\geqslant\\exp\\bigl(s^{2}+2u\\bigr)\\Big)\\leqslant\\exp\\Big(-\\frac{d u^{2}}{16s^{2}}\\Big)+\\exp\\Big(-\\frac{L d u^{2}}{4s^{4}+u s^{2}}\\Big)\\,.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "To conclude on the operator norm of $\\Pi_{k:1}$ , consider $\\Sigma$ a $1/2$ -net of the unit sphere of $\\mathbb{R}^{d}$ . By Vershynin (2018, Corollary 4.2.13), it is possible to take such a net of cardinality $5^{d}$ . Let us show that, for any $u\\in\\mathbb R$ , ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Big(\\operatorname*{max}_{1\\leqslant k\\leqslant L}\\|\\Pi_{k:1}\\|_{2}\\geqslant2\\exp\\Big(\\frac{s^{2}}{2}+u\\Big)\\Big)\\leqslant\\mathbb{P}\\Big(\\bigcup_{h_{0}\\in\\Sigma}\\operatorname*{max}_{1\\leqslant k\\leqslant L}\\|\\Pi_{k:1}h_{0}\\|_{2}\\geqslant\\exp\\Big(\\frac{s^{2}}{2}+u\\Big)\\Big)\\,.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Indeed, assume that there exists $k$ such that $\\begin{array}{r}{\\|\\Pi_{k:1}\\|_{2}\\geqslant2\\exp(\\frac{s^{2}}{2}+u)}\\end{array}$ . By definition of the operator norm, there exists $x$ on the unit sphere of $\\mathbb{R}^{d}$ such that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\|\\Pi_{k:1}x\\|=\\|\\Pi_{k:1}\\|_{2}\\,.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "But then, by definition of $\\Sigma$ , there exists $h_{0}\\in\\Sigma$ such that $\\|x-h_{0}\\|_{2}\\leqslant1/2$ . Then ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\|\\Pi_{k:1}(x-h_{0})\\|_{2}\\leqslant\\frac{1}{2}\\|\\Pi_{k:1}\\|_{2}\\,.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Therefore, by the triangular inequality, ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\Vert\\Pi_{k:1}h_{0}\\Vert_{2}\\geqslant\\Vert\\Pi_{k:1}x\\Vert_{2}-\\Vert\\Pi_{k:1}(x-h_{0})\\Vert_{2}\\geqslant\\frac{1}{2}\\Vert\\Pi_{k:1}\\Vert_{2}\\,.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Then ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\|\\Pi_{k:1}h_{0}\\|_{2}\\geqslant\\frac{1}{2}\\|\\Pi_{k:1}\\|_{2}\\geqslant\\exp\\left(\\frac{s^{2}}{2}+u\\right),\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "which proves (50). By the union bound, we conclude that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}(\\bar{A})=\\mathbb{P}\\Big(\\displaystyle\\operatorname*{max}_{1\\leqslant k\\leqslant L}\\|\\Pi_{k:1}\\|_{2}\\geqslant2\\exp\\Big(\\displaystyle\\frac{s^{2}}{2}+u\\Big)\\Big)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leqslant\\mathbb{P}\\Big(\\displaystyle\\bigcup_{h_{0}\\in\\Sigma}\\displaystyle\\operatorname*{max}_{1\\leqslant k\\leqslant L}\\|\\Pi_{k:1}h_{0}\\|_{2}\\geqslant\\exp\\Big(\\displaystyle\\frac{s^{2}}{2}+u\\Big)\\Big)}\\\\ &{\\qquad\\qquad\\qquad\\leqslant|\\Sigma|\\mathbb{P}\\Big(\\displaystyle\\operatorname*{max}_{1\\leqslant k\\leqslant L}\\|\\Pi_{k:1}h_{0}\\|_{2}\\geqslant\\exp\\Big(\\displaystyle\\frac{s^{2}}{2}+u\\Big)\\Big)\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where now $h_{0}$ denotes any unit-norm fixed vector in $\\mathbb{R}^{d}$ . Thus, by (49), ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}(\\bar{A})\\leqslant5^{d}\\mathbb{P}\\Big(\\underset{1\\leqslant k\\leqslant L}{\\operatorname*{max}}\\ \\|\\Pi_{k:1}h_{0}\\|_{2}^{2}\\geqslant\\exp(s^{2}+2u)\\Big)}\\\\ &{\\qquad\\leqslant5^{d}\\Big(\\exp\\Big(-\\frac{d u^{2}}{16s^{2}}\\Big)+\\exp\\Big(-\\frac{L d u^{2}}{4s^{4}+u s^{2}}\\Big)\\Big)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "This upper bound will be simplified in the conclusion of the proof. ", "page_idx": 36}, {"type": "text", "text": "Bound on $\\mathbb{P}(\\bar{B}\\cap A)$ . We now move on to proving the lower-bound on $\\sigma_{\\mathrm{min}}(\\Pi_{k:1})$ . We again use an $\\varepsilon$ -net argument, as follows. Let $\\Sigma$ be an $\\varepsilon$ -net for $\\begin{array}{r}{\\varepsilon=\\frac{m_{s,u}}{M_{s,u}}}\\end{array}$ ms,u. Then ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Big(A\\cap\\Big\\{\\operatorname*{min}_{1\\leqslant k\\leqslant L}\\sigma_{\\mathrm{min}}(\\Pi_{k:1})\\leqslant m_{s,u}\\Big\\}\\Big)\\leqslant\\mathbb{P}\\Big(\\bigcup_{h_{0}\\in\\Sigma}\\operatorname*{min}_{1\\leqslant k\\leqslant L}\\|\\Pi_{k:1}h_{0}\\|_{2}\\leqslant2m_{s,u}\\Big)\\,.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Indeed, assume that $A$ holds, that is $\\|\\Pi_{k:1}\\|_{2}\\leqslant M_{s,u}.$ , and that there exists $k$ such that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\sigma_{\\mathrm{min}}(\\Pi_{k:1})\\leqslant m_{s,u}\\,.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "By definition of the singular value, there exists $x$ on the unit sphere of $\\mathbb{R}^{d}$ such that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\vert\\vert\\Pi_{k:1}x\\vert\\vert_{2}=\\sigma_{\\operatorname*{min}}(\\Pi_{k:1})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "But then, by definition of $\\Sigma$ , there exists $h_{0}\\in\\Sigma$ such that $\\|x-h_{0}\\|_{2}\\leqslant\\varepsilon$ . Then ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\|\\Pi_{k:1}(x-h_{0})\\|_{2}\\leqslant\\varepsilon\\|\\Pi_{k:1}\\|_{2}\\,.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Under $A$ , the right-hand side is smaller than $\\varepsilon M_{s,u}$ . Therefore, by the triangular inequality, ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\Pi_{k:1}h_{0}\\|_{2}\\leqslant\\|\\Pi_{k:1}x\\|_{2}+\\|\\Pi_{k:1}(h_{0}-x)\\|_{2}\\leqslant m_{s,u}+\\varepsilon M_{s,u}=2m_{s,u}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "By the union bound, we conclude that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{P}\\Big(A\\cap\\Big\\{\\underset{1\\leqslant k\\leqslant L}{\\operatorname*{min}}\\sigma_{\\operatorname*{min}}(\\Pi_{k:1})\\leqslant m_{s,u}\\Big\\}\\Big)\\leqslant\\mathbb{P}\\Big(A\\cap\\Big\\{\\bigcup_{h_{0}\\in\\Sigma}\\underset{1\\leqslant k\\leqslant L}{\\operatorname*{min}}\\|\\Pi_{k:1}h_{0}\\|_{2}\\leqslant2m_{s,u}\\Big\\}\\Big)}&{}\\\\ {\\leqslant\\mathbb{P}\\Big(\\bigcup_{h_{0}\\in\\Sigma}\\underset{1\\leqslant k\\leqslant L}{\\operatorname*{min}}\\|\\Pi_{k:1}h_{0}\\|_{2}\\leqslant2m_{s,u}\\Big)}&{}\\\\ {\\leqslant|\\Sigma|\\mathbb{P}\\Big(\\underset{1\\leqslant k\\leqslant L}{\\operatorname*{min}}\\|\\Pi_{k:1}h_{0}\\|_{2}\\leqslant2m_{s,u}\\Big)}&{}\\\\ {=\\Big(\\frac{2M_{s,u}}{m_{s,u}}+1\\Big)^{d}\\mathbb{P}\\Big(\\underset{1\\leqslant k\\leqslant L}{\\operatorname*{min}}\\|\\Pi_{k:1}h_{0}\\|_{2}\\leqslant2m_{s,u}\\Big)\\,,}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where the cardinality of the net is given by Vershynin (2018, Corollary 4.2.13). We now take a fixed $h_{0}\\in\\mathbb{R}^{d}$ , and compute ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Big(\\operatorname*{min}_{1\\leqslant k\\leqslant L}\\frac{\\|h_{k}\\|_{2}}{\\|h_{0}\\|_{2}}\\leqslant2m_{s,u}\\Big)=\\mathbb{P}\\Big(\\operatorname*{min}_{1\\leqslant k\\leqslant L}\\frac{\\|h_{k}\\|_{2}^{2}}{\\|h_{0}\\|_{2}^{2}}\\leqslant4m_{s,u}^{2}\\Big)\\,.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Denote by $E$ the event ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\bigcap_{1\\leqslant k\\leqslant L}\\left\\{Y_{k,2}\\geqslant-{\\frac{1}{2}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "We have, by (45), ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\Big(\\underset{1\\leqslant k\\leqslant L}{\\operatorname*{min}}\\frac{\\|h_{k}\\|_{2}^{2}}{\\|h_{0}\\|_{2}^{2}}\\leqslant4m_{s,u}^{2}\\Big)=\\mathbb{P}\\Big(\\underset{1\\leqslant k\\leqslant L}{\\operatorname*{min}}\\ln(\\|h_{k}\\|_{2}^{2})-\\ln(\\|h_{0}\\|_{2}^{2})\\leqslant\\ln(4m_{s,u}^{2})\\Big)}\\\\ &{\\leqslant\\mathbb{P}\\Big(\\underset{1\\leqslant k\\leqslant L}{\\operatorname*{min}}\\underset{j=0}{\\sum}\\ln(1+Y_{j,2})\\leqslant\\ln(4m_{s,u}^{2})\\Big)}\\\\ &{\\leqslant\\mathbb{P}\\Big(\\Big\\{\\underset{1\\leqslant k\\leqslant L}{\\operatorname*{min}}\\sum_{j=0}^{k-1}\\ln(1+Y_{j,2})\\leqslant\\ln(4m_{s,u}^{2})\\Big\\}\\cap E\\Big)+\\mathbb{P}(\\bar{E})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Using the inequality $\\ln(1+x)\\geqslant x-x^{2}$ for $x\\geqslant-{1}/{2}$ , we obtain ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Big(\\frac{\\|h_{L}\\|_{2}^{2}}{\\|h_{0}\\|_{2}^{2}}\\leqslant4m_{s,u}^{2}\\Big)\\leqslant\\mathbb{P}\\Big(\\Big\\{\\operatorname*{min}_{1\\leqslant k\\leqslant L}\\sum_{j=0}^{k-1}(Y_{j,2}-Y_{j,2}^{2})\\leqslant\\ln(4m_{s,u}^{2})\\Big\\}\\cap E\\Big)+\\mathbb{P}(\\bar{E})\\,.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Thus, by the union bound, ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Big(\\operatorname*{min}_{1\\leqslant k\\leqslant L}\\frac{\\|h_{L}\\|_{2}^{2}}{\\|h_{0}\\|_{2}^{2}}\\leqslant4m_{s,u}^{2}\\Big)\\leqslant\\mathbb{P}\\Big(\\operatorname*{min}_{1\\leqslant k\\leqslant L}\\sum_{j=0}^{k-1}(Y_{j,2}-Y_{j,2}^{2})\\leqslant\\ln(4m_{s,u}^{2})\\Big)+\\sum_{k=0}^{L-1}\\mathbb{P}\\Big(Y_{k,2}<-\\frac{1}{2}\\Big)\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "We handle both terms separately. Beginning by the second term, we have, for $k\\in\\{1,\\ldots L\\}$ , ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Big(Y_{k,2}<-\\frac{1}{2}\\Big)\\leqslant\\exp\\Big(-\\frac{L d}{32s^{2}}\\Big)\\,,\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where we used (46) and the tail bound $\\mathbb{P}(N\\geqslant u)\\leqslant e^{-u^{2}/2}$ if $N\\sim\\mathcal{N}(0,1)$ . Moving on to the first term, we have ", "page_idx": 37}, {"type": "equation", "text": "$$\nP_{1}=\\mathbb{P}\\Big(\\operatorname*{min}_{1\\leqslant k\\leqslant L}S_{k,2}-S_{k,3}\\leqslant\\ln(4m_{s,u}^{2})\\Big)\\,,\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where we let $\\begin{array}{r}{S_{k,3}=\\sum_{j=0}^{k-1}Y_{j,2}^{2}}\\end{array}$ . By definition of $m_{s,u}$ , we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\ln(4m_{s,u}^{2})=-{\\frac{4s^{2}}{d}}-2u\\,.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Thus we can split the probability into two parts by the union bound: ", "page_idx": 37}, {"type": "equation", "text": "$$\nP_{1}\\leqslant\\mathbb{P}\\Big(\\operatorname*{min}_{1\\leqslant k\\leqslant L}S_{k,2}\\leqslant-u\\Big)+\\mathbb{P}\\Big(\\operatorname*{max}_{1\\leqslant k\\leqslant L}S_{k,3}\\geqslant\\frac{4s^{2}}{d}+u\\Big)\\,.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Let us bound each probability separately. We have, for $u>0$ , ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Big(\\operatorname*{min}_{1\\leqslant k\\leqslant L}S_{k,2}\\leqslant-u\\Big)=\\mathbb{P}\\Big(\\operatorname*{max}_{1\\leqslant k\\leqslant L}-S_{k,2}\\geqslant u\\Big)=\\mathbb{P}\\Big(\\operatorname*{max}_{1\\leqslant k\\leqslant L}S_{k,2}\\geqslant u\\Big)\\leqslant\\exp\\Big(-\\frac{d u^{2}}{16s^{2}}\\Big)\\,,\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "by symmetry of $S_{k,2}$ and by (48). Moving on to $S_{k,3}$ , we have that ", "page_idx": 37}, {"type": "equation", "text": "$$\nY_{j,2}^{2}|\\mathcal{F}_{j}\\sim\\frac{4s^{2}}{L d}\\mathcal{N}(0,1)^{2}=\\frac{4s^{2}}{L d}\\chi^{2}(1)\\,,\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "thus ", "page_idx": 38}, {"type": "equation", "text": "$$\n{\\frac{L d}{4s^{2}}}S_{k,3}\\sim\\chi^{2}(L)\\,.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "By monotonicity of $S_{k,3}$ and by (47), ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\bigg(\\displaystyle\\operatorname*{max}_{1\\leqslant k\\leqslant L}S_{k,3}\\geqslant\\frac{4s^{2}}{d}+u\\bigg)=\\mathbb{P}\\bigg(S_{L,3}\\geqslant\\frac{4s^{2}}{d}+u\\bigg)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\mathbb{P}\\bigg(\\displaystyle\\frac{L d}{4s^{2}}S_{L,3}\\geqslant L+\\displaystyle\\frac{L d u}{4s^{2}}\\bigg)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leqslant\\exp\\bigg(-\\displaystyle\\frac{L^{2}d^{2}u^{2}}{64s^{4}(L+\\frac{L d u}{4s^{2}})}\\bigg)=\\exp\\bigg(-\\displaystyle\\frac{L d^{2}u^{2}}{16(4s^{2}+d u)}\\bigg)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Putting everything together, we proved that ", "page_idx": 38}, {"type": "text", "text": "with ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\cap A\\!\\!\\!\\!\\slash\\leqslant\\Big(\\displaystyle\\frac{2M_{s,u}}{m_{s,u}}+1\\Big)^{d}(P_{1}+P_{2})=\\exp\\Big(d\\ln\\Big(\\displaystyle\\frac{2M_{s,u}}{m_{s,u}}+1\\Big)\\Big)(P_{1}+P_{2})\\,,}\\\\ {P_{1}\\leqslant\\exp\\Big(-\\displaystyle\\frac{d u^{2}}{16s^{2}}\\Big)+\\exp\\Big(-\\displaystyle\\frac{L d^{2}u^{2}}{16(4s^{2}+d u)}\\Big)\\qquad\\qquad\\qquad\\quad}\\\\ {P_{2}\\leqslant L\\exp\\Big(-\\displaystyle\\frac{L d}{32s^{2}}\\Big)\\,.\\qquad\\qquad\\qquad\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "and ", "page_idx": 38}, {"type": "text", "text": "Bounding the probability of failure. Putting together the two main bounds we showed, we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}(\\bar{A}\\cup\\bar{B})=\\mathbb{P}(\\bar{A})+\\mathbb{P}(\\bar{B}\\cup A)}\\\\ &{\\leqslant5^{d}\\Big(\\exp\\Big(-\\frac{d u^{2}}{16s^{2}}\\Big)+\\exp\\Big(-\\frac{L d u^{2}}{4s^{4}+u s^{2}}\\Big)\\Big)}\\\\ &{\\,+\\exp\\Big(d\\ln\\Big(\\frac{2M_{s,u}}{m_{s,u}}+1\\Big)\\Big)\\Big(\\exp\\Big(-\\frac{d u^{2}}{16s^{2}}\\Big)+\\exp\\Big(-\\frac{L d^{2}u^{2}}{16(4s^{2}+d u)}\\Big)+L\\exp\\Big(-\\frac{L d}{32s^{2}}\\Big)\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "This expression is valid for all $u,s>0$ and $L,d\\geqslant1$ . We now simplify the expression of our upper bound by algebraic computations using the assumptions of the Theorem. To somewhat alleviate the technicality of the computations, we stop at this point tracking some of the explicit constants and let $C$ denote a positive absolute constant that might vary from equality to equality. We show next that the conditions ", "page_idx": 38}, {"type": "equation", "text": "$$\nL\\geqslant C,\\quad d\\geqslant C,\\quad u\\geqslant C\\operatorname*{max}(s^{2},s^{-2/3}),\\quad u\\leqslant C L^{1/4}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "imply that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\bar{A}\\cup\\bar{B})\\leqslant8\\exp\\Big(-\\frac{d u^{2}}{32s^{2}}\\Big)\\,.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "This shall conclude the proof of the Lemma with ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r}{C_{1}=C,\\quad C_{2}=C,\\quad C_{3}=C\\operatorname*{max}(s^{2},s^{-2/3}),\\quad C_{4}=C\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "First note that the conditions (51) imply that ", "page_idx": 38}, {"type": "equation", "text": "$$\nu\\geqslant C s\\quad{\\mathrm{and}}\\quad u\\geqslant C\\,.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "We study the terms of the bound on $\\mathbb{P}(\\bar{A}\\cup\\bar{B})$ one by one. First, we have by (52) that ", "page_idx": 38}, {"type": "equation", "text": "$$\n5^{d}\\exp\\Big(-\\frac{d u^{2}}{16s^{2}}\\Big)=\\exp\\Big(d\\ln(5)-\\frac{d u^{2}}{16s^{2}}\\Big)\\leqslant\\exp\\Big(-\\frac{d u^{2}}{32s^{2}}\\Big)\\,.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Next, ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{5^{d}\\exp\\bigg(-\\frac{L d u^{2}}{4s^{4}+u s^{2}}\\bigg)\\leqslant5^{d}\\exp\\bigg(-\\frac{L d u^{2}}{8u s^{2}}\\bigg)}}\\\\ &{}&{=5^{d}\\exp\\bigg(-\\frac{L d u}{8s^{2}}\\bigg)}\\\\ &{}&{=\\exp\\bigg(d\\ln(5)-\\frac{L d u}{8s^{2}}\\bigg)}\\\\ &{}&{\\leqslant\\exp\\bigg(-\\frac{L d u}{16s^{2}}\\bigg)\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where the first inequality uses $u\\geqslant s^{2}$ by (51), and the last one uses that $\\begin{array}{r}{\\frac{L u}{s^{2}}\\geqslant C}\\end{array}$ . This is true since $L\\geqslant C$ and $u\\geqslant C s^{2}$ by (51). Then we can bound this term by ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\exp\\left(-\\;\\frac{d u^{2}}{32s^{2}}\\right),\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "since $u\\leqslant C L$ which is implied by the assumption $u\\leqslant C L^{1/4}$ in (51). Next, ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\ln\\left(\\frac{2M_{s,u}}{m_{s,u}}+1\\right)=\\ln\\left(8\\exp\\left(\\frac{s^{2}}{2}+\\frac{2s^{2}}{d}+2u\\right)+1\\right)}}&{}&\\\\ &{}&{\\leqslant\\ln\\left(9\\exp\\left(\\frac{s^{2}}{2}+\\frac{2s^{2}}{d}+2u\\right)\\right),\\quad}&\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where we used that the exponential of a positive term is greater than 1. Then ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\ln\\Big(\\frac{2M_{s,u}}{m_{s,u}}+1\\Big)\\leqslant\\ln(9)+\\frac{s^{2}}{2}+\\frac{2s^{2}}{d}+2u\\leqslant3+3u\\,,\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "since $u\\geqslant s^{2}$ , $d\\geqslant4$ by (51). Thus we can bound the three remaining terms appearing in the bound of $\\mathbb{P}(\\bar{A}\\cup\\bar{B})$ , as follows: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\exp\\left(d\\ln\\left(\\frac{2M_{s,u}}{m_{s,u}}+1\\right)\\right)\\exp\\Big(-\\frac{d u^{2}}{16s^{2}}\\Big)\\leqslant\\exp\\left(3d+3d u-\\frac{d u^{2}}{16s^{2}}\\right)\\leqslant\\exp\\Big(-\\frac{d u^{2}}{32s^{2}}\\Big)\\,,\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "since ", "page_idx": 39}, {"type": "equation", "text": "$$\nu^{2}\\geqslant C s^{2}(1+u)\\,.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "This is the case by (52), which implies that ", "page_idx": 39}, {"type": "equation", "text": "$$\nu^{2}={\\frac{1}{2}}u^{2}+{\\frac{1}{2}}u u\\geqslant{\\frac{C^{2}}{2}}s^{2}+{\\frac{C}{2}}s^{2}u={\\frac{C^{2}+C}{2}}s^{2}(1+u)\\,.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Next, by (53), ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\exp\\left(d\\ln\\left(\\frac{2M_{s,u}}{m_{s,u}}+1\\right)\\right)\\exp\\Big(-\\frac{L d^{2}u^{2}}{16\\left(4s^{2}+d u\\right)}\\Big)\\leqslant\\exp\\left(3d+3d u-\\frac{L d^{2}u^{2}}{16\\left(4s^{2}+d u\\right)}\\right)}\\\\ &{}&{\\leqslant\\exp\\left(3d+3d u-\\frac{L d^{2}u^{2}}{32d u}\\right),\\quad\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "since $d u\\geqslant u\\geqslant C s^{2}$ by (51). Thus ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mathrm{xp}\\left(d\\ln\\Big(\\frac{2M_{s,u}}{m_{s,u}}+1\\Big)\\right)\\mathrm{exp}\\Big(-\\frac{L d^{2}u^{2}}{16(4s^{2}+d u)}\\Big)\\leqslant\\mathrm{exp}\\left(3d+3d u-\\frac{L d u}{32}\\right)\\leqslant\\mathrm{exp}\\,\\Big(-\\frac{L d u}{64}\\Big)\\,,\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where the second inequality uses that $L\\geqslant C$ by (51), and $L u\\geqslant C$ since $L\\geqslant C$ and $u\\geqslant C$ by (51) and (52). We can also bound this term by ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\exp\\left(-\\;\\frac{d u^{2}}{32s^{2}}\\right),\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "since $L\\geqslant C u/s^{2}$ , using first that $L\\geqslant C u^{4}$ then that $u\\geqslant C s^{-2/3}$ , by (51). Regarding the last term of the bound of $\\mathbb{P}(\\bar{A}\\cup\\bar{B})$ , we have by (53) that ", "page_idx": 39}, {"type": "equation", "text": "$$\nL\\exp\\left(d\\ln\\left(\\frac{2M_{s,u}}{m_{s,u}}+1\\right)\\right)\\exp\\Big(-\\frac{L d}{32s^{2}}\\Big)\\leqslant L\\exp\\left(3d+3d u-\\frac{L d}{32s^{2}}\\right)\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Let us show that this implies that ", "page_idx": 39}, {"type": "equation", "text": "$$\nL\\exp\\left(d\\ln\\Big(\\frac{2M_{s,u}}{m_{s,u}}+1\\Big)\\right)\\exp\\Big(-\\frac{L d}{32s^{2}}\\Big)\\leqslant L\\exp\\Big(-\\frac{\\sqrt{L}d u^{2}}{32s^{2}}\\Big)\\,.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "This statement is true because the three terms appearing inside the exponential of the right-hand side of (54) can be bounded by $\\frac{C{\\sqrt{L}}d u^{2}}{s^{2}}$ . More precisely, we have ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\frac{L d}{32s^{2}}\\geqslant\\frac{C\\sqrt{L}d u^{2}}{s^{2}}\\Leftrightarrow\\sqrt{L}\\geqslant C u^{2}\\,,\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "which holds by (51); ", "page_idx": 40}, {"type": "equation", "text": "$$\n3d u\\leqslant\\frac{C\\sqrt{L}d u^{2}}{s^{2}}\\Leftrightarrow C s^{2}\\leqslant\\sqrt{L}u\\,,\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "which is implied by $L\\geqslant C$ and $u\\geqslant C s^{2}$ by (51); and ", "page_idx": 40}, {"type": "equation", "text": "$$\n3d\\leqslant\\frac{C\\sqrt{L}d u^{2}}{s^{2}}\\Leftrightarrow C s^{2}\\leqslant\\sqrt{L}u^{2}\\,,\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "which is implied by $L\\geqslant C$ and $u\\geqslant C s$ by (51) and (52). Finally, we use Lemma 5 to bound the right-hand side of (55) by ", "page_idx": 40}, {"type": "equation", "text": "$$\n4\\exp\\left(-\\,\\frac{d u^{2}}{32s^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "This is possible for $L\\geqslant C$ and $\\begin{array}{r}{\\frac{d u^{2}}{32s^{2}}\\geqslant1}\\end{array}$ , which is implied by $d\\geqslant C$ and $u\\geqslant C s$ , by (51) and (52). Collecting everything, we bounded $\\mathbb{P}(\\bar{A}\\cup\\bar{B})$ by ", "page_idx": 40}, {"type": "equation", "text": "$$\n8\\exp\\left(-\\,\\frac{d u^{2}}{32s^{2}}\\right),\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "which concludes the proof when $\\theta=0$ . ", "page_idx": 40}, {"type": "text", "text": "Summary when $\\theta\\ =\\ 0$ . In summary, we proved so far the following result: there exist $C_{1},\\ldots,C_{4}>0$ depending only on $s$ such that, if ", "page_idx": 40}, {"type": "equation", "text": "$$\nL\\geqslant C_{1}\\,,\\quad d\\geqslant C_{2}\\,,\\quad u\\in[C_{3},C_{4}L^{1/4}]\\,,\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "then, with probability at least ", "page_idx": 40}, {"type": "equation", "text": "$$\n1-8\\exp\\Big(-\\frac{d u^{2}}{32s^{2}}\\Big)\\,,\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "it holds for all $k\\in\\{1,\\ldots,L\\}$ that ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\left\\|\\left(I+\\frac{s}{\\sqrt{L d}}N_{k}\\right)\\ldots\\left(I+\\frac{s}{\\sqrt{L d}}N_{1}\\right)\\right\\|_{2}\\leqslant m_{s,u}\\,,\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "and ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\sigma_{\\mathrm{min}}\\Bigl(\\Bigl(I+\\frac{s}{\\sqrt{L d}}N_{k}\\Bigr)\\cdot\\cdot\\cdot\\Bigl(I+\\frac{s}{\\sqrt{L d}}N_{1}\\Bigr)\\Bigr)\\geqslant M_{s,u}\\,.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Conclusion for arbitrary $\\theta$ . The conclusion is a direct application of Lemma 9, with $W_{k}\\,=$ $I+\\frac{s}{\\sqrt{L d}}N_{k}$ . The size of the admissible ball for $\\theta$ is ", "page_idx": 40}, {"type": "equation", "text": "$$\n{\\frac{m_{s,u}^{2}}{4M_{s,u}^{2}}}={\\frac{1}{64}}\\exp{\\Big(}-s^{2}-{\\frac{4s^{2}}{d}}-4u{\\Big)}\\geqslant{\\frac{1}{64}}\\exp(-2s^{2}-4u)\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "for $d\\geqslant4$ , which concludes the proof. ", "page_idx": 40}, {"type": "text", "text": "Comparison with the bounds of Zhang et al. (2022). Theorem 1 of Zhang et al. (2022) gives an upper bound on the singular values of residual networks at initialization, and their Theorem 2 gives a lower bound on the norm of the activations. Comparing with our results, we note two important differences. First, their probability of failure grows quadratically with the depth, whereas ours is independent of depth. This is achieved by a more precise martingale argument making use of Doob\u2019s martingale inequality. Second, their lower bound incorrectly assumes that $\\chi^{2}$ random variables are sub-Gaussian (see equation (21) of their paper), while in fact they are only sub-exponential (Ghosh, 2021). Finally, their upper bound holds for the product ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\left(I+{\\frac{s}{\\sqrt{L d}}}N_{k}+{\\frac{1}{L}}\\theta_{k}\\right)\\ldots\\left(I+{\\frac{s}{\\sqrt{L d}}}N_{j}+{\\frac{1}{L}}\\theta_{j}\\right)\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "for any $1\\leqslant j\\leqslant k\\leqslant L$ , which could seem stronger than our result stated for $j=1$ . In fact, both statements are equivalent, because it is possible to deduce the statement for any $j$ by combining the upper bound and the lower bound for $j=1$ . The precise argument is given in the beginning of the proof of our Lemma 9. ", "page_idx": 40}, {"type": "text", "text": "B.9 Proof of Corollary 3 ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "The beginning of the proof is very similar to the one of Corollary 2. Denoting $\\mathcal{W}:=\\mathcal{W}^{\\mathrm{RI}}$ , we have ", "page_idx": 41}, {"type": "equation", "text": "$$\nS(\\mathcal{W})^{2}=\\operatorname*{lim}_{\\xi\\rightarrow0}\\operatorname*{sup}_{\\|W_{k}-\\tilde{W}_{k}\\|_{F}\\leqslant\\xi}\\frac{\\sum_{k=1}^{L}\\|\\nabla_{k}R^{L}(\\mathcal{W})-\\nabla_{k}R^{L}(\\tilde{\\mathcal{W}})\\|_{F}^{2}}{\\sum_{k=1}^{L}\\|W_{k}-\\tilde{W}_{k}\\|_{F}^{2}}\\,,\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "with ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta_{k}:=\\nabla_{k}R^{L}(\\mathcal{W})-\\nabla_{k}R^{L}(\\tilde{\\mathcal{W}})}\\\\ &{\\quad=-W_{k+1}^{\\top}\\,.\\,.\\,.\\,W_{L}^{\\top}p\\nabla R^{1}(\\tilde{w}_{\\mathrm{prod}})^{\\top}W_{1}^{\\top}\\,.\\,.\\,.\\,W_{k-1}^{\\top}+\\mathcal{O}(\\xi^{2})\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "and ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\nabla R^{1}(\\tilde{w}_{\\mathrm{prod}})=\\frac{2}{n}X^{\\top}X\\Big(\\sum_{k=1}^{L}W_{1}^{\\top}\\,\\ldots\\,W_{k-1}^{\\top}(\\tilde{W}_{k}^{\\top}-W_{k}^{\\top})W_{k+1}^{\\top}\\,\\ldots\\,W_{L}^{\\top}p\\Big)+{\\mathcal O}(\\xi^{2})\\,.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "At this point, the proofs diverge. We have, by subadditivity of the operator norm, ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\|\\nabla R^{1}(\\tilde{w}_{\\mathrm{prod}})\\|_{2}\\leqslant2\\Lambda\\sum_{k=1}^{L}\\|W_{k-1}\\dots W_{1}\\|_{2}\\|\\tilde{W}_{k}-W_{k}\\|_{2}\\|W_{L}\\dots W_{k+1}\\|_{2}\\|p\\|_{2}+\\mathcal{O}(\\xi^{2})\\,.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Let us now briefly recall the outline of the proof of Theorem 4, which will be useful in bounding the quantity above. The proof shows the existence of ${\\tilde{C}}_{3}$ depending only on $s$ such that, with high probability (which is exactly the probability in the statement of the Theorem), we have for all $t\\geqslant0$ and $k\\in\\{1,\\ldots,L\\}$ that ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\|W_{k-1}(t)\\ldots W_{1}(t)\\|_{2}\\leqslant4\\exp\\left(\\frac{s^{2}}{2}+\\tilde{C}_{3}\\right)\\quad\\mathrm{and}\\quad\\|W_{L}(t)\\ldots W_{k+1}(t)\\|_{2}\\leqslant4\\exp\\left(\\frac{s^{2}}{2}+\\tilde{C}_{3}\\right),\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "as well as ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\sigma_{\\mathrm{min}}(W_{k}(t)\\ldots W_{1}(t))\\geqslant\\frac{1}{4}\\exp\\Big(-\\frac{2s^{2}}{d}-\\tilde{C}_{3}\\Big)\\,.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Under this high-probability event, the proof of Theorem 4 shows convergence of the gradient flow to $\\mathcal{W}=\\mathcal{W}^{\\mathrm{RI}}$ . In particular, this means that $\\mathcal{W}$ also verifies the bounds on the operator norm of the matrix products. We therefore obtain ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla R^{1}(\\tilde{w}_{\\mathrm{prod}})\\|_{2}\\leqslant2\\Lambda\\displaystyle\\sum_{k=1}^{L}4\\exp\\Big(\\frac{s^{2}}{2}+\\tilde{C}_{3}\\Big)\\|\\tilde{W}_{k}-W_{k}\\|_{2}4\\exp\\Big(\\frac{s^{2}}{2}+\\tilde{C}_{3}\\Big)\\|p\\|_{2}+\\mathcal{O}(\\xi^{2})}\\\\ &{\\qquad\\qquad\\qquad\\qquad=32\\exp(s^{2}+2\\tilde{C}_{3})\\Lambda\\|p\\|_{2}\\displaystyle\\sum_{k=1}^{L}\\|\\tilde{W}_{k}-W_{k}\\|_{2}+\\mathcal{O}(\\xi^{2})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Moving on to bounding the Frobenius norm of $\\Delta_{k}$ , we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\Delta_{k}\\|_{F}=\\|W_{k+1}^{\\top}\\ldots W_{L}^{\\top}p\\|_{2}\\|\\nabla R^{1}(\\tilde{w}_{\\mathrm{prod}})^{\\top}W_{1}^{\\top}\\ldots W_{k-1}^{\\top}\\|_{2}+\\mathcal{O}(\\xi^{2})}\\\\ &{\\qquad\\quad\\leqslant\\|W_{L}\\ldots W_{k+1}\\|_{2}\\|p\\|_{2}\\|\\nabla R^{1}(\\tilde{w}_{\\mathrm{prod}})\\|_{2}\\|W_{k-1}\\ldots W_{1}\\|_{2}+\\mathcal{O}(\\xi^{2})}\\\\ &{\\qquad\\quad\\leqslant2^{9}\\exp(2s^{2}+4\\tilde{C}_{3})\\Lambda\\|p\\|_{2}^{2}\\displaystyle\\sum_{k=1}^{L}\\|W_{k}-\\tilde{W}_{k}\\|_{2}+\\mathcal{O}(\\xi^{2})\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "by bounding the three norms by the expressions given above. Then ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\|\\Delta_{k}\\|_{F}^{2}\\leqslant2^{18}\\exp(4s^{2}+8\\tilde{C}_{3})\\Lambda^{2}\\|p\\|_{2}^{4}\\Big(\\displaystyle\\sum_{k=1}^{L}\\|W_{k}-\\tilde{W}_{k}\\|_{2}\\Big)^{2}+\\mathcal{O}(\\xi^{3})}\\\\ &{}&{\\quad\\quad\\leqslant2^{18}\\exp(4s^{2}+8\\tilde{C}_{3})L\\Lambda^{2}\\|p\\|_{2}^{4}\\displaystyle\\sum_{k=1}^{L}\\|W_{k}-\\tilde{W}_{k}\\|_{2}^{2}+\\mathcal{O}(\\xi^{3})\\,.}\\\\ &{}&{\\quad\\quad\\leqslant2^{18}\\exp(4s^{2}+8\\tilde{C}_{3})L\\Lambda^{2}\\|p\\|_{2}^{4}\\displaystyle\\sum_{k=1}^{L}\\|W_{k}-\\tilde{W}_{k}\\|_{F}^{2}+\\mathcal{O}(\\xi^{3})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Thus, by (56), ", "page_idx": 42}, {"type": "equation", "text": "$$\nS(\\mathcal{W})^{2}\\leqslant\\operatorname*{lim}_{\\xi\\rightarrow0}2^{18}\\exp(4s^{2}+8\\tilde{C}_{3})L^{2}\\Lambda^{2}\\|p\\|_{2}^{4}+\\mathcal{O}(\\xi)\\,.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Therefore ", "page_idx": 42}, {"type": "equation", "text": "$$\nS(\\mathcal{W})\\leqslant2^{9}\\exp(2s^{2}+4\\tilde{C}_{3})L\\Lambda\\|p\\|_{2}^{2}\\,.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "To conclude, we need to upper bound $\\|\\boldsymbol{p}\\|_{2}$ by a constant times $\\|\\boldsymbol{w}^{\\star}\\|_{2}$ . To this aim, we leverage the bound from the assumptions of Theorem 4 on the risk at initialization, to show that $\\|\\boldsymbol{p}\\|_{2}$ cannot be too far away from $\\|\\boldsymbol{w^{\\star}}\\|_{2}$ . More precisely, by Lemma 8, since the covariance matrix $X^{\\top}X$ is full rank, ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R^{L}(\\mathcal{W}(0))-R_{\\mathrm{min}}=\\displaystyle\\frac{1}{n}\\|X(w_{\\mathrm{prod}}(0)-w^{\\star})\\|_{2}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\frac{1}{n}(w_{\\mathrm{prod}}(0)-w^{\\star})^{\\top}X^{\\top}X(w_{\\mathrm{prod}}(0)-w^{\\star})}\\\\ &{\\qquad\\qquad\\qquad\\geqslant\\lambda\\|w_{\\mathrm{prod}}(0)-w^{\\star}\\|_{2}^{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Thus ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\sqrt{R^{L}(\\mathcal{W}(0))-R_{\\mathrm{min}}}\\geqslant\\sqrt{\\lambda}\\|w_{\\mathrm{prod}}(0)-w^{\\star}\\|_{2}\\,.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Then, by the triangular inequality, ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|w^{\\star}\\|_{2}\\geqslant\\|w_{\\mathrm{prod}}(0)\\|_{2}-\\|w_{\\mathrm{prod}}(0)-w^{\\star}\\|_{2}}\\\\ &{\\qquad\\geqslant\\|W_{1}^{\\top}(0)\\ldots W_{L}^{\\top}(0)p\\|_{2}-\\sqrt{\\frac{R^{L}(\\mathcal{W}(0))-R_{\\operatorname*{min}}}{\\lambda}}}\\\\ &{\\qquad\\geqslant\\sigma_{\\operatorname*{min}}(W_{L}(0)\\ldots W_{1}(0))\\|p\\|_{2}-\\sqrt{\\frac{R^{L}(\\mathcal{W}(0))-R_{\\operatorname*{min}}}{\\lambda}}}\\\\ &{\\qquad\\geqslant\\frac{1}{4}\\exp\\Big(-\\frac{2s^{2}}{d}-\\tilde{C}_{3}\\Big)\\|p\\|_{2}-\\sqrt{C_{3}}\\sqrt{\\frac{\\lambda}{\\Lambda}}\\|p\\|_{2}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "by (57) and by the assumption of Theorem 4 on the risk at initialization. We now note that the value of $C_{3}$ is given by (40) as ", "page_idx": 42}, {"type": "equation", "text": "$$\nC_{3}=2^{-36}\\exp\\Big(-4s^{2}-\\frac{16s^{2}}{\\tilde{C}_{2}}-20\\tilde{C}_{3}\\Big)\\,,\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where $\\tilde{C}_{2}\\leqslant d$ by (38). Thus ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\sqrt{C_{3}}\\sqrt{\\frac{\\lambda}{\\Lambda}}\\leqslant\\sqrt{C_{3}}=2^{-18}\\exp(-2s^{2}-\\frac{8s^{2}}{\\tilde{C}_{2}}-10\\tilde{C}_{3})<\\frac{1}{4}\\exp\\Big(-\\frac{2s^{2}}{d}-\\tilde{C}_{3}\\Big)\\,.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Denoting ", "page_idx": 42}, {"type": "equation", "text": "$$\nC^{\\prime}=\\frac{1}{4}\\exp\\Big(-\\frac{2s^{2}}{d}-\\tilde{C}_{3}\\Big)-\\sqrt{C_{3}}\\,\\in(0,1)\\,,\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "we therefore obtain that $\\|w^{\\star}\\|_{2}\\geqslant C^{\\prime}\\|p\\|_{2}$ . Therefore, by (58), ", "page_idx": 42}, {"type": "equation", "text": "$$\nS(\\mathcal{W})\\leqslant2^{9}\\exp(2s^{2}+4\\tilde{C}_{3})(C^{\\prime})^{-2+\\frac{2}{L}}L\\Lambda\\|w^{\\star}\\|_{2}^{2-\\frac{2}{L}}\\|p\\|_{2}^{\\frac{2}{L}}\\,.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Thus, by the second lower bound on $S_{\\mathrm{min}}$ from Theorem 2, and since $a\\geqslant\\lambda>0$ , ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\frac{S(\\mathcal{W})}{S_{\\mathrm{min}}}\\leqslant2^{8}\\exp(2s^{2}+4\\tilde{C}_{3})(C^{\\prime})^{-2+\\frac{2}{L}}\\frac{\\Lambda}{\\lambda}\\,,\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "which concludes the proof by setting ", "page_idx": 42}, {"type": "equation", "text": "$$\nC:=2^{8}\\exp(2s^{2}+4\\tilde{C}_{3})(C^{\\prime})^{-2}\\geqslant2^{8}\\exp(2s^{2}+4\\tilde{C}_{3})(C^{\\prime})^{-2+\\frac{2}{L}}\\,.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "C Experimental details, additional plots, and additional comments ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Our code is available at https://github.com/PierreMarion23/implicit-reg-sharpness. Our framework for experiments is JAX (Bradbury et al., 2018). The experiments take around 3 hours to run on a laptop CPU. ", "page_idx": 42}, {"type": "text", "text": "Setup. We take $n\\,=\\,50$ , $d\\,=\\,5$ , $L\\,=\\,10$ . The design matrix $X$ is sampled from an isotropic Gaussian distribution. The target $y$ is computed in two steps. First, we compute $y_{0}=X w_{\\mathrm{true}}+\\zeta$ , where $w_{\\mathrm{true}}$ and $\\zeta$ are standard Gaussian vectors. Then, we compute $w_{0}^{\\star}$ as the optimal regressor of $y_{0}$ on $X$ . Finally, we let $y=y_{0}/\\lVert w_{0}^{\\star}\\rVert$ and $w^{\\star}=w_{0}^{\\star}/\\vert\\vert w_{0}^{\\star}\\vert\\vert$ . This simplifies the expressions of our bounds by having $w^{\\star}$ of unit norm. All Gaussian random variables are independent. ", "page_idx": 43}, {"type": "table", "img_path": "F738WY1Xm4/tmp/f449f37c100c52f1c2afec4d586268df0aedfbe9388446d0fe18387a1bc339a4.jpg", "table_caption": ["Details of Figure 1. We consider a Gaussian initialization of the weight matrices, where the scale of the initialization ( $\\bf\\Tilde{x}$ -axis of some the graphs) is the standard deviation of the entries. All weight matrices are $d\\times d$ , except the last one which is $1\\times d$ . The square distance to the optimal regressor corresponds to $\\lVert\\boldsymbol{w}_{\\mathrm{prod}}-\\bar{\\boldsymbol{w}}^{\\star}\\rVert_{2}^{2}$ . The largest eigenvalue of the Hessian is computed by a power iteration method, stopped after 20 iterations. In Figures 1a and 1b, the $95\\%$ confidence intervals are plotted. The number of gradient steps and number of independent repetitions depend on the learning rate, and are given below. "], "table_footnote": [], "page_idx": 43}, {"type": "text", "text": "For large values of the initialization scale, it may happen that the gradient descent diverges. Figure 3 shows the probability of divergence depending on the initialization scale and the learning rate. ", "page_idx": 43}, {"type": "image", "img_path": "F738WY1Xm4/tmp/615bc7406c66d93802ea79e0f94ee67f196fa04f9bf39336884e0e74fc1bea13.jpg", "img_caption": ["Figure 3: Probability of divergence of gradient descent for a Gaussian initialization of the weight matrices, depending on the initialization scale and the learning rate. "], "img_footnote": [], "page_idx": 43}, {"type": "text", "text": "When the probability of divergence is equal to one, no point is reported in Figure 1. When it is strictly between 0 and 1, the confidence intervals are computed over non-diverging runs. ", "page_idx": 43}, {"type": "text", "text": "Figures 1c and 1d show one randomly-chosen run each. The plots are subsampled 5 times for readability, due to the oscillations in Figure 1d. ", "page_idx": 43}, {"type": "text", "text": "Residual initialization. We now consider the case of a residual initialization as in Section 5. Results are given in Figure 4. The scale of the initialization now corresponds to the hyperparameter $s$ in (2). The projection vector $p\\in\\mathbb{R}^{d}$ is a random isotropic vector of unit norm, which does not change during training. For each learning rate, we use 4, 000 steps of gradient descent, and perform 20 independent repetitions. The plots are similar to the case of Gaussian initialization, apart from the fact that the sharpness at initialization is better conditioned. ", "page_idx": 43}, {"type": "text", "text": "As previously, for large values of the initialization scale, it may happen that the gradient descent diverges. Figure 5 shows the probability of divergence depending on the initialization scale and the learning rate. ", "page_idx": 43}, {"type": "text", "text": "Details of Figure 2. The setup is the same as for the residual initialization. For each learning rate, we use 1, 000 steps of gradient descent, and perform 50 independent repetitions. We take $s=0.25$ . ", "page_idx": 43}, {"type": "image", "img_path": "F738WY1Xm4/tmp/57d1b52b313ed27b8115566f06c41bee9ce52d50f6c696e7c430711de1378a81.jpg", "img_caption": ["(c) Evolution during training of the squared distance to (d) Evolution during training of the squared distance the empirical risk minimizer and of sharpness, for $\\eta=$ to the empirical risk minimizer and of sharpness, for 0.02 and an initialization scale of 0.5. The network $\\eta=0.1$ and an initialization scale of 0.5. The network does not enter edge of stability. enters edge of stability. "], "img_footnote": [], "page_idx": 44}, {"type": "text", "text": "Figure 4: Training a deep linear network on a univariate regression task with quadratic loss. The initialization is a residual initialization as in Section 5. ", "page_idx": 44}, {"type": "image", "img_path": "F738WY1Xm4/tmp/d6c8ee8b5ac8b5e39ac2610f82fe74f71c3e8a494d012636337ca5cc6cf08a4a.jpg", "img_caption": ["Figure 5: Probability of divergence of gradient descent for a residual initialization of the weight matrices, depending on the initialization scale and the learning rate. "], "img_footnote": [], "page_idx": 44}, {"type": "text", "text": "Underdetermined regression and link to generalization. Although this is not our original motivation, we note that a simple change to our setting allows to make appear the connection between sharpness and generalization. To this aim, we consider the underdetermined case, where the number of data is lower than the dimension (while keeping the rest of the setup identical). Figure 6 shows in this case a correlation between generalization and sharpness. This suggests that the tools developed in the paper could be used in this case to understand the generalization performance of deep (linear) networks, and we leave this analysis for future work. We also qualitatively observe in Figure 6 a similar connection between learning rate, initialization scale and sharpness as in the case of full-rank data (Figure 1b). We take here $n=15,d=20,L=5$ . The number of gradient steps and number of independent repetitions depend on the learning rate, and are given below. Other technical details are as Figure 1. ", "page_idx": 44}, {"type": "text", "text": "", "page_idx": 45}, {"type": "table", "img_path": "F738WY1Xm4/tmp/72267a5ebe5c63a5064fe4c66558dfe2b977b07cc307d2332bf580cbd4a8a4f5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 45}, {"type": "image", "img_path": "F738WY1Xm4/tmp/3f7fcf7cf3ffb6fb535e5cd3e28c149127bf208beb782419a5334edf4816235e.jpg", "img_caption": ["(a) Sharpness at initialization and after training, for (b) Link between generalization and sharpness. Each various learning rates and initialization scales. We qual- dot corresponds to one realization of the experiment itatively observe a similar connection between learning (where the randomness comes from the random initialrate, initialization scale and sharpness as in the case of ization of the neural network). The plot is shown in a full rank data matrix (Figure 1b of the paper). log-log scale. The line corresponds to the linear regression of the $\\log_{10}$ of the generalization gap on the $\\log_{10}$ of the sharpness after training (slope $=\\!0.42\\pm0.01$ , intercept $=-0.96\\pm0.03)$ . ", "Comments on the connection between gradient flow and gradient descent. In this paper, we show that gradient flow from a small-scale initialization is driven towards low-sharpness regions. This should imply that gradient flow and gradient descent up to a reasonably large learning rate should follow the same trajectory when starting from small-scale initialization, because they do not "], "img_footnote": [], "page_idx": 45}, {"type": "text", "text": "Figure 6: Experiment with a deep linear network and a degenerate data covariance matrix, where the number of data $n$ is less than the dimension $d$ . ", "page_idx": 45}, {"type": "text", "text": "Non-linear MLPs. As a first attempt to extend our results to non-linear networks, we consider the case of non-linear MLPs. The non-linearity is GELU (Hendrycks and Gimpel, 2016), a smooth version of ReLU, which we chose because smoothness is necessary in order to compute the sharpness. We qualitatively observe in Figure 7 a similar connection between learning rate, initialization scale and sharpness as for deep linear neural networks (Figure 1b). For large initialization, the sharpness after training plateaus at $2/\\eta$ , as in the linear case. For small initialization, the sharpness after training is less that $2/\\eta$ , and is close to the bounds in the linear case (dotted black lines). For this experiment, we consider noiseless data, meaning that $y_{0}=X w_{\\mathrm{true}}$ (see paragraph \u201cSetup\u201d above for notations). We perform 20 independent repetitions of each experiment. The number of gradient steps depends on the learning rate, and is given below. Other details are as for Figure 1. Finally, we also performed the same experiment in the case of noisy data $y_{0}=X w_{\\mathrm{true}}+\\zeta$ (no plot reported). We observed that the sharpness of the network reaches $2/\\eta$ , for every learning rate and initialization scale reported in Figure 7. We suspect that this is because the network (over)fits the noise in the data, resulting in a high sharpness. ", "page_idx": 45}, {"type": "table", "img_path": "F738WY1Xm4/tmp/daf71c941aa5277b23a2869c1719f9060b3dbbd792d619ca2c0ffffc2a22d7a2.jpg", "table_caption": [], "table_footnote": [], "page_idx": 45}, {"type": "image", "img_path": "F738WY1Xm4/tmp/66778ca80c1497019cda6469390d601f9d40991cf1234a6c3a1d6024677d8b9e.jpg", "img_caption": ["Figure 7: For a non-linear MLP, sharpness at initialization and after training, for various learning rates and initialization scales. For a given learning rate $\\eta$ , the dashed lines represent the $2/\\eta$ threshold. The dotted black lines represent the lower and upper bounds given in Theorem 1 and Corollary 2 of the paper. "], "img_footnote": [], "page_idx": 46}, {"type": "text", "text": "go in regions of high sharpness where the difference between gradient flow and gradient descent would become significant. This intuition is supported by Figure 1b where we see that, for small initializations, the sharpness after training is independent of the learning rate. We leave further investigation of these questions for future work. ", "page_idx": 46}, {"type": "text", "text": "D Additional related work ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Progressive sharpening. Cohen et al. (2021) show that the edge of stability phase is typically preceded by a phase of progressive sharpening, where the sharpness steadily increases until reaching the value of $2/\\eta$ . Our setting of small-scale initialization presents an example of such a progressive sharpening (although we make no statement on the monotonicity of the increase in sharpness). Other works have proposed analyses of progressive sharpening. Wang et al. (2022) suggest that progressive sharpening is driven by the increase in norm of the output layer. MacDonald et al. (2023) assume from empirical evidence a link between sharpness and the magnitude of the input-output Jacobian, and show that the latter has to be large for the loss to decrease. Finally, Agarwala et al. (2023) propose and analyze a simplified model with quadratic dependence in its parameters, which exhibits a progressive sharpening phenomenon. ", "page_idx": 46}, {"type": "text", "text": "Connection with deep matrix factorization. Regression with deep linear networks can be seen as an instance of a matrix factorization problem. There is a well-established literature studying the implicit regularization of gradient descent for this class of problem (see, e.g., Gunasekar et al., 2017; Arora et al., 2019b; Li et al., 2021; Yun et al., 2021). However, this line of work study under-determined settings where there are an infinite number of factorizations reaching zero empirical risk, and study the implicit regularization in function space. On the contrary, we consider an overdetermined setting where there is a single optimal regressor, and study the regularization in parameter space. ", "page_idx": 46}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: The claims regard the theoretical analysis of sharpness of deep linear networks.   \nThe corresponding results are given in Sections 3, 4, and 5, and proven in Appendix B. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 47}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Justification: The paper presents work of theoretical nature, and explains clearly the setting and assumptions (e.g., linear activation function) in which the results are applicable. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 47}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: The assumptions are stated in the main paper, and all proofs are given in Appendix B. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 48}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: The experimental details are given in Appendix C. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 48}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 49}, {"type": "text", "text": "Justification: The code is given in the supplemental material. Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 49}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 49}, {"type": "text", "text": "Justification: The experimental details are given in Appendix C. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 49}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 49}, {"type": "text", "text": "Justification: All plots have error bars (except the plots showing the trajectory of gradient descent along one run, for illustration). ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 49}, {"type": "text", "text": "", "page_idx": 50}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 50}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 50}, {"type": "text", "text": "Justification: The experiments are small-scale and run on a laptop CPU. The runtime is given in Appendix C. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 50}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 50}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 50}, {"type": "text", "text": "Justification: The authors have reviewed the NeurIPS Code of Ethics, and attest that this paper conforms to the Code of Ethics. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 50}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 50}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 50}, {"type": "text", "text": "Justification: This is a foundational research paper, not tied to particular applications, let alone deployments. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 50}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 51}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 51}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 51}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 51}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 51}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Justification: We cite the papers for code we are using in Appendix C. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 51}, {"type": "text", "text": "", "page_idx": 52}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 52}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 52}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 52}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 52}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 52}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 52}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 52}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 52}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 52}]