[{"figure_path": "F738WY1Xm4/tables/tables_43_1.jpg", "caption": "Details of Figure 1. We consider a Gaussian initialization of the weight matrices, where the scale of the initialization (x-axis of some the graphs) is the standard deviation of the entries. All weight matrices are d \u00d7 d, except the last one which is 1 \u00d7 d. The square distance to the optimal regressor corresponds to ||Wprod - w*||2. The largest eigenvalue of the Hessian is computed by a power iteration method, stopped after 20 iterations. In Figures 1a and 1b, the 95% confidence intervals are plotted. The number of gradient steps and number of independent repetitions depend on the learning rate, and are given below.", "description": "The table shows the number of gradient steps and the number of repetitions performed for different learning rates in the experiments for Figure 1 of the paper.  The setup uses a Gaussian initialization of weight matrices where the x-axis in the plots represents the standard deviation of the entries.  The table clarifies the experimental parameters used in the experiments generating the results shown in Figure 1.", "section": "C Experimental details, additional plots, and additional comments"}, {"figure_path": "F738WY1Xm4/tables/tables_45_1.jpg", "caption": "Details of Figure 1. We consider a Gaussian initialization of the weight matrices, where the scale of the initialization (x-axis of some the graphs) is the standard deviation of the entries. All weight matrices are d \u00d7 d, except the last one which is 1 \u00d7 d. The square distance to the optimal regressor corresponds to ||Wprod - w*||2. The largest eigenvalue of the Hessian is computed by a power iteration method, stopped after 20 iterations. In Figures 1a and 1b, the 95% confidence intervals are plotted. The number of gradient steps and number of independent repetitions depend on the learning rate, and are given below.", "description": "This table details the experimental setup used to generate Figure 1 in the paper.  It specifies the hyperparameters used (learning rate, number of steps, number of repetitions) for different learning rates. It also explains how the key metrics (squared distance to optimal regressor and sharpness) were computed.", "section": "C Experimental details, additional plots, and additional comments"}, {"figure_path": "F738WY1Xm4/tables/tables_45_2.jpg", "caption": "Table 1: Learning rate, number of steps, and number of repetitions for Figures 1a and 1b.", "description": "This table shows the hyperparameters used in the experiments presented in Figures 1a and 1b of the paper.  It lists the learning rate used for gradient descent, the corresponding number of gradient descent steps performed, and the number of independent repetitions of the experiment for each learning rate.  These hyperparameters were chosen to explore the impact of learning rate on training success and convergence, particularly in relation to the critical learning rate beyond which the training fails to converge (as described in Figure 1a).", "section": "C Experimental details, additional plots, and additional comments"}]