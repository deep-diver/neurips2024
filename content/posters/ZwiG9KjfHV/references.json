{"references": [{"fullname_first_author": "T. Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper introduces the concept of few-shot learning in large language models, a significant advancement that is fundamental to the current state-of-the-art in LLMs."}, {"fullname_first_author": "T. Dettmers", "paper_title": "LLM.int8(): 8-bit matrix multiplication for transformers at scale", "publication_date": "2022-12-01", "reason": "This paper presents a method for efficient 8-bit quantization of LLMs, a key technique for reducing the computational and memory requirements of LLMs, directly relevant to the current work."}, {"fullname_first_author": "E. Frantar", "paper_title": "GPTQ: Accurate post-training quantization for generative pre-trained transformers", "publication_date": "2022-10-17", "reason": "This paper introduces GPTQ, a highly effective post-training quantization method for LLMs, which is used as a baseline and compared against in this paper."}, {"fullname_first_author": "Z. Liu", "paper_title": "LLM-QAT: Data-free quantization aware training for large language models", "publication_date": "2024-07-01", "reason": "This paper introduces LLM-QAT, another significant quantization method that is compared to the proposed method in this paper, offering a direct comparison of techniques."}, {"fullname_first_author": "W. Shao", "paper_title": "OmniQuant: Omnidirectionally calibrated quantization for large language models", "publication_date": "2024-05-01", "reason": "This paper presents OmniQuant, a state-of-the-art quantization technique that serves as a strong baseline for comparison, highlighting the advancements in the field."}]}