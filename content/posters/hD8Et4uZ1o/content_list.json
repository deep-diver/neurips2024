[{"type": "text", "text": "An Equivalence Between Static and Dynamic Regret Minimization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Francesco Orabona KAUST ", "page_idx": 0}, {"type": "text", "text": "Andrew Jacobsen\\* Universita degli Studi di Milano Politecnico di Milano contact@andrew-jacobsen.com francesco@orabona.com ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We study the problem of dynamic regret minimization in online convex optimization, in which the objective is to minimize the difference between the cumulative loss of an algorithm and that of an arbitrary sequence of comparators. While the literature on this topic is very rich, a unifying framework for the analysis and design of these algorithms is still missing. In this paper we show that for linear losses, dynamic regret minimization is equivalent to static regret minimization in an extended decision space. Using this simple observation, we show that there is a frontier of lower bounds trading off penalties due to the variance of the losses and penalties due to variability of the comparator sequence, and provide a framework for achieving any of the guarantees along this frontier. As a result, we also prove for the first time that adapting to the squared path-length of an arbitrary sequence of comparators to achieve regret $\\begin{array}{r}{R_{T}(\\boldsymbol{u}_{1},\\ldots,\\boldsymbol{u}_{T})\\leq\\mathcal{O}(\\sqrt{T\\sum_{t}\\|\\boldsymbol{u}_{t}-\\boldsymbol{u}_{t+1}\\|^{2}})}\\end{array}$ is impossible. However, using our framework we introduce an alternative notion of variability based on a locallysmoothed comparator sequence $\\bar{\\b u}_{1},\\dots,\\bar{\\b u}_{T}$ , and provide an algorithm guaranteeing dynamic regret of the form $\\bar{R}_{T}(\\pmb{u}_{1},\\dots,\\pmb{u}_{T})\\leq\\tilde{\\mathcal{O}}(\\sqrt{T\\sum_{i}\\|\\bar{\\pmb{u}}_{i}-\\bar{\\pmb{u}}_{i+1}\\|^{2}}).$ while still matching in the worst case the usual path-length dependencies up to polylogarithmic terms. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "This paper introduces new techniques for Online Convex Optimization (OCO), a framework for designing and analyzing algorithms which learn on-the-fly from a stream of data [14, 51, 5, 31, 6]. Formally, consider $T$ rounds of interaction between the learner and their environment. In each round, the learner chooses $w_{t}\\in\\mathcal{W}$ from a convex feasible set $\\mathcal{W}\\subseteq\\mathbb{R}^{d}$ , the environment reveals a $G$ -Lipschitzconvex loss function $\\ell_{t}:\\mathcal{W}\\to\\mathbb{R}$ , and the learner incurs a loss of $\\ell_{t}(\\pmb{w}_{t})$ . The classic objective in this setting is to minimize the learner's regret relative to any fixed benchmark $\\pmb{u}\\in\\mathcal{W}$ ", "page_idx": 0}, {"type": "equation", "text": "$$\nR_{T}(\\pmb{u}):=\\sum_{t=1}^{T}(\\ell_{t}(\\pmb{w}_{t})-\\ell_{t}(\\pmb{u}))\\;.\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "In this paper, we study the more general problem of minimizing the learner's regret relative to any sequence of benchmarks $\\pmb{u}_{1},\\dots,\\pmb{u}_{T}\\in\\mathcal{W}$ [17,18]: ", "page_idx": 0}, {"type": "equation", "text": "$$\nR_{T}(\\pmb{u}_{1},\\dots,\\pmb{u}_{T}):=\\sum_{t=1}^{T}(\\ell_{t}(\\pmb{w}_{t})-\\ell_{t}(\\pmb{u}_{t}))\\;.\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "This objective is typically referred to as dynamic regret, to distinguish it from the special case where the comparator sequence is fixed $\\pmb{u}_{1}=\\dots=\\pmb{u}_{T}$ (referred to as static regret). We focus in particular on the special case of Online Linear Optimization (OLO), in which $\\bar{\\ell}_{t}(\\pmb{w})\\,=\\,\\langle\\pmb{g}_{t},\\pmb{w}\\rangle$ for some $\\pmb{g}_{t}\\in\\mathbb{R}^{d}$ . Note that OCO problems can always be reduced to OLO via the well-known inequality $\\ell_{t}(\\pmb{w}_{t})-\\ell_{t}(\\pmb{u})\\leq\\langle\\pmb{g}_{t},\\pmb{w}_{t}-\\pmb{u}\\rangle$ for $\\pmb{g}_{t}\\in\\partial\\ell_{t}(\\pmb{w}_{t})$ , where $\\partial\\ell_{t}({\\pmb w}_{t})$ is the subdifferential set of $\\ell_{t}$ at $\\pmb{w}_{t}$ [see, e.g., 36], so throughout this paper we will focus on the OLO setting. ", "page_idx": 1}, {"type": "text", "text": "Intuitively, if the sequence of comparators $\\pmb{u}_{1},\\dots,\\pmb{u}_{T}$ varies too much, it should be impossible to achieve low dynamic regret. On the other hand, we know it is possible to achieve sublinear regret if the sequence of comparators is constant, i.e., $\\pmb{u}_{1}=\\dots=\\pmb{u}_{T}$ , because this is simply the static case. Hence, we need a way to quantify the complexity, or variability, of the comparator sequence. The most commonly used notion of complexity in this regard is the path-length of the comparator sequence [17, 18], defined as ", "page_idx": 1}, {"type": "equation", "text": "$$\nP_{T}^{\\parallel\\cdot\\parallel}:=\\sum_{t=2}^{T}\\left\\|\\pmb{u}_{t}-\\pmb{u}_{t-1}\\right\\|\\,.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "ItispoiblhwtatOdieDetadyf $\\mathcal{O}((D+P_{T}^{\\parallel\\cdot\\parallel})G\\sqrt{T})$ in bounded domains, where $D$ is an upper bound on the diameter of the feasible set and $G$ is the Lipschitz constant of the losses [51]. This bound was improved to $\\mathcal{O}(\\sqrt{D P_{T}^{\\parallel\\cdot\\parallel}}G\\sqrt{T})$ and shown to be minimax optimal by Zhang et al. [46]. ", "page_idx": 1}, {"type": "text", "text": "Notice  that the  path-length  bounds  scale  with  a  rather  pessimistic constant  of $\\begin{array}{r l}{D}&{{}=}\\end{array}$ $\\operatorname*{sup}_{w,w^{\\prime}\\in\\mathcal{W}}\\|w-\\bar{w}^{\\prime}\\|$ . A better bound would instead scale with the squared path-length: ", "page_idx": 1}, {"type": "equation", "text": "$$\nP_{T}^{\\parallel\\cdot\\parallel^{2}}:=\\sum_{t=1}^{T-1}\\|\\pmb{u}_{t}-\\pmb{u}_{t-1}\\|^{2},\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "which can be significantly smaller\u00b2 than the penalty in the bound above: $P_{T}^{\\parallel\\cdot\\parallel^{2}}\\leq D P_{T}^{\\parallel\\cdot\\parallel}$ However, guarantees scaling with PI-I- are not well understood in general compared with the more common Pl bouds, andaveonlyben btainedby restricting thecomparatr squn t $\\begin{array}{r}{\\pmb{u}_{t}=\\mathrm{argmin}_{\\pmb{w}\\in\\mathcal{W}}~\\ell_{t}(\\pmb{w})}\\end{array}$ or under additional assumptions such as strong-convexity [44, 45, 7]. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we focus on the challenging case that the domain is unbounded, where recent works have achieved thedyamiereret $\\begin{array}{r}{\\widetilde{\\mathcal{O}}(\\sqrt{\\operatorname*{max}_{t,t^{\\prime}}\\|\\boldsymbol{u}_{t}-\\boldsymbol{u}_{t^{\\prime}}\\|\\,P_{T}^{\\parallel\\cdot\\parallel}T})}\\end{array}$ inthe wors cas 20 25, 21.47] Of particular interest, Jacobsen and Cutkosky [20], Zhang et al. [47] achieve bounds of the form ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r}{R_{T}(\\boldsymbol{u}_{1},\\dots,\\boldsymbol{u}_{T})\\leq\\tilde{\\mathcal{O}}\\left(\\sqrt{P_{T}^{\\|\\cdot\\|}\\sum_{t=1}^{T}\\|\\boldsymbol{g}_{t}\\|^{2}\\left\\|\\boldsymbol{u}_{t}-\\bar{\\boldsymbol{u}}\\right\\|}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "which avoids the pessimistic multiplicative penalty of $\\operatorname*{max}_{t,t^{\\prime}}\\|\\boldsymbol{u}_{t}-\\boldsymbol{u}_{t^{\\prime}}\\|$ , but results in a coupling between the gradient and variability penalties. It is unclear if it is possible to obtain a guarantee which cleanly separates the variability and variance penalties, to achieve dynamic regret scaling as $\\begin{array}{r}{R_{T}(\\pmb{u}_{1},\\dots,\\pmb{u}_{T})\\leq\\mathcal{O}\\left(\\sqrt{P_{T}^{\\|\\cdot\\|^{2}}\\sum_{t=1}^{T}\\|\\pmb{g}_{t}\\|^{2}}\\right).}\\end{array}$ In fct, i is no clar in genera how to reason about potential trade-offs that may result from adapting to different measures of variability. ", "page_idx": 1}, {"type": "text", "text": "Contributions. In this paper, we show how to reformulate the dynamic regret miniminization problem as an equivalent static regret problem (Section 2). This equivalence allows us to use results for the static regret setting to prove both upper and lower bounds for dynamic regret. ", "page_idx": 1}, {"type": "text", "text": "In our first application of this equivalence, we show that the ideal guarantee scaling the with squared path-length $\\begin{array}{r}{\\mathcal{O}\\big(\\sqrt{P_{T}^{\\parallel\\cdot\\parallel^{2}}\\sum_{t=1}^{T}{\\|g_{t}\\|^{2}}}\\big)}\\end{array}$ is not possible in general (Section 3). We do this by proving a novel lower bound showing that there is a fundamental trade-off between the penalties incurred due to comparator variability and penalties incurred due to loss variance, leading to a new frontier of dynamic regret lower bounds. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Our second application is to provide a framework for achieving any of the variance/variability tradeoffs along the lower bound frontier, up to polylogarithmic terms (Section 4). Our framework allows us to develop dynamic regret algorithms by simply choosing suitable dual-norm pairs $(\\lVert\\cdot\\rVert,\\lVert\\cdot\\rVert_{*})$ in the static regret problem. Along with our matching lower bound, this framework provides a concrete way to reason about different measures of comparator variability and the trade-offs they entail, and to design algorithms achieving those trade-offs. ", "page_idx": 2}, {"type": "text", "text": "While our lower bound demonstrates that the ideal squared path-length guarantee cannot be achieved, using our framework we show that it is possible to achieve an alternative guarantee that scales with ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\bar{P}^{\\|\\cdot\\|^{2}}(\\pmb{u}_{1},\\dots,\\pmb{u}_{T})\\approx\\sum_{i}\\left\\|\\bar{\\pmb{u}}_{i}^{(\\tau)}-\\bar{\\pmb{u}}_{i+1}^{(\\tau)}\\right\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "wherer $\\bar{\\pmb u}_{i}^{(\\tau)}$ isa localaverage o the comparatorseqguence at timescale of $\\tau$ (seSecto 4.1). Similar to $P_{T}^{\\parallel\\cdot\\parallel^{2}}$ , this variability measure maintains the property that it matches the worst-case guarantees based on path-length up to plylogarithmi terms, .e,. $\\begin{array}{r}{\\bar{P}_{T}^{\\parallel\\cdot\\parallel^{2}}\\leq\\widetilde{\\mathcal{O}}(\\operatorname*{max}_{t,t^{\\prime}}\\|\\boldsymbol{u}_{t}-\\boldsymbol{u}_{t^{\\prime}}\\|\\,P_{T}^{\\parallel\\cdot\\parallel})}\\end{array}$ .These are the first guarantees for general OCO that fully decouple the variance and variability penalties for dynamic regret without explicitly incurring pessimistic $\\operatorname*{max}_{t,t^{\\prime}}$ $\\left\\|\\boldsymbol{u}_{t}-\\boldsymbol{u}_{t^{\\prime}}\\right\\|$ penalties. ", "page_idx": 2}, {"type": "text", "text": "Related Work. Our approach is inspired by the Haar OLR algorithm of Zhang et al. [47]. In that work, dynamic regret is approached by interpreting the comparator sequence as a high-dimensional \"signal\" which is decomposed into a frequency domain representation using a dictionary of features. Then, for each feature vector in the dictionary, a 1-dimensional parameter-free [32, 28] algorithm is used to learn how well that feature correlates with the losses. This allows one to compete with an arbitrary comparator sequence, so long as it can be represented in terms of the chosen dictionary of features. We take a similar but slightly more general approach. Our framework also represents the comparator sequence as a high-dimensional signal, but we instead use this signal to define an equivalent static regret problem, a perspective that allows us design algorithms for dynamic regret by simply choosing suitable dual-norm pairs. ", "page_idx": 2}, {"type": "text", "text": "Other prior works in the general OCO setting have also studied various alternative forms of variability such as the temporal variability $\\begin{array}{r}{\\sum_{t=1}^{T-1}\\operatorname*{sup}_{\\pmb{w}\\in\\mathcal{W}}|\\ell_{t}(\\pmb{w})-\\ell_{t+1}(\\pmb{w})|}\\end{array}$ [3,22, 4] or deviation of the comparator from a given dynamical model $\\begin{array}{r}{\\sum_{t=1}^{T-1}\\|\\pmb{u}_{t}-\\Phi_{t}(\\pmb{u}_{t-1})\\|}\\end{array}$ [16]. Alternative variance penaltis have also be studied i the dynamic seting, such as the salloss penalies $\\textstyle\\sum_{t=1}^{T}\\ell_{t}(\\mathbf{\\boldsymbol{u}}_{t})$ or gradient variation penalties $\\begin{array}{r}{\\sum_{t=1}^{T}\\operatorname*{sup}_{\\pmb{w}\\in\\mathcal{W}}\\|\\nabla\\ell_{t}(\\pmb{w})-\\nabla\\ell_{t+1}(\\pmb{w})\\|}\\end{array}$ [15, 49, 21, 50]. It is also possible to achieve a smaller regret with stronger assumptions on the losses [2]. It is important to note however that almost all prior works, with the exception of Jacobsen and Cutkosky [20], Luo et al. [25] and Zhang et al. [47], study dynamic regret only in the easier bounded domain setting. ", "page_idx": 2}, {"type": "text", "text": "There is also an often ignored connection between measures of comparator variability and the function classes studied in non-parametric regression. In particular, considering the case that the losses are $\\ell_{t}(x)=(x-u_{t})^{2}$ , then the sequence of comparators $u_{1},\\dotsc,u_{T}$ with bounded path length $C_{T}$ and bounded squared path length $\\(\\Bar{C_{T}^{\\prime}})^{2}$ corresponds to the sequence with discrete total variation bounded by $C_{T}$ and the discrete Sobolev class with bound $C_{T}^{\\prime}$ , respectively. In this setting, the minimax rates are known [24, 35] and Koolen et al. [24] obtain the minimax regret for the Sobolev classes, while Baby and Wang [1] for both classes with slightly stronger assumptions. However, these results are not directly related to this paper because we consider linear losses. ", "page_idx": 2}, {"type": "text", "text": "Notations. We will use the following definitions and notations. The elements of a matrix $\\textbf{\\em A\\in}$ $\\mathbb{R}^{n\\times m}$ are denoted by $A_{i j}$ for $i=1,\\dots,n$ and $j=1,\\dots,m$ . Similarly, the elements of a vector $\\pmb{u}\\in\\mathbb{R}^{d}$ are $u_{i}$ for $i=1,\\ldots,d$ . The Kronecker product of matrices $A\\in\\mathbb{R}^{m\\times n}$ and $B\\in\\mathbb{R}^{p\\times q}$ is the block matrix defined by ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\pmb{A}\\otimes\\pmb{B}:=\\left(\\!\\!\\begin{array}{c c c}{\\!A_{1,1}\\pmb{B}\\,\\mathrm{~.~.~}\\,\\!}&{\\!A_{1,n}\\pmb{B}}\\\\ {\\!\\!\\therefore}&{\\!\\!\\cdot\\!}&{\\!\\!\\cdot\\!}\\\\ {\\!\\!A_{m,1}\\pmb{B}\\,\\mathrm{~.~.~}\\,\\!}&{\\!\\!A_{m,n}\\pmb{B}}\\end{array}\\!\\!\\right)\\,.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Algorithm 1: Dynamic-to-Static Reduction   \nInput Domain $\\mathcal{W}\\subseteq\\mathbb{R}^{d}$ , Online Learning Algorithm $\\boldsymbol{\\mathcal{A}}$ withdomain $\\mathcal{W}^{T}$   \nfor $t=1:T$ do Get $\\widetilde{\\pmb{w}}_{t}=(\\pmb{w}_{t}^{(1)},\\dots,\\pmb{w}_{t}^{(T)})\\in\\mathcal{W}^{T}$ from $\\boldsymbol{\\mathcal{A}}$ Play $\\pmb{w}_{t}=\\pmb{w}_{t}^{(t)}\\in\\mathcal{W}$ and observe $\\pmb{g}_{t}\\in\\partial\\ell_{t}(\\pmb{w}_{t})$ Pass $\\widetilde{\\pmb{g}}_{t}=\\mathbf{e}_{t}\\otimes\\pmb{g}_{t}=(\\mathbf{0}_{d}^{\\top},\\dots,\\mathbf{0}_{d}^{\\top}$ $\\boldsymbol{\\mathfrak{g}}_{t}^{\\top}\\,,\\mathbf{0}_{d}^{\\top},\\dots)^{\\top}\\in\\mathcal{W}^{T}$ 10 $\\boldsymbol{\\mathcal{A}}$ indices $i\\!\\in\\![d(t\\!-\\!1)\\!+\\!1,d t]$   \nend ", "page_idx": 3}, {"type": "text", "text": "We let ${\\bf e}_{t}$ denote the $t^{\\mathrm{th}}$ standard basis vector of $\\mathbb{R}^{T}$ and $\\scriptstyle{I_{d}}$ is the $d\\times d$ identity matrix. For a square matrix $\\pmb{A}$ , Diag $(A)$ is the diagonal matrix that contains the elements of the diagonal of $\\pmb{A}$ . For a positive definite matrix $_M$ , we define the weighted norm $\\|\\pmb{x}\\|_{M}:=\\sqrt{\\langle\\pmb{x},M\\pmb{x}\\rangle}$ . For a matrix $A\\in\\mathbb{R}^{m\\times n}$ we denote its Frobenius norm by $\\begin{array}{r}{\\|\\boldsymbol{A}\\|_{F}:=\\sqrt{\\sum_{i=1}^{m}\\sum_{j=1}^{n}A_{i,j}^{2}}}\\end{array}$ The vec operatoris the mapping defined by stacking the columns of a matrix $\\pmb{A}$ in a vector. We will denote by $\\|A\\|_{p,p}$ the entry-wise $p$ -norm of $\\pmb{A}$ i.e., $\\|A\\|_{p,p}:=\\|\\mathrm{vec}(A)\\|_{p}$ ", "page_idx": 3}, {"type": "text", "text": "2  A dynamic-to-static reduction ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we present a general reduction from dynamic regret to static regret. The key idea is to embed the comparator sequence in a high dimensional space $\\bar{\\mathcal{W}}^{T}$ ,where $T$ is the number of rounds, so that competing with a fixed comparator $\\widetilde{\\pmb{u}}\\in\\mathcal{W}^{T}$ in this high-dimensional space is equivalent to competing with a sequence of comparators in the original space $\\mathcal{W}$ . In this way, we can reduce the problem of minimizing the dynamic regret to the one of minimizing the static regret. ", "page_idx": 3}, {"type": "text", "text": "Our reduction is shown in Algorithm 1. We simply embed the linear losses $\\scriptstyle\\mathbf{\\textit{g}}_{t}$ in a high-dimensional space by setting ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widetilde{\\pmb{g}}_{t}=\\mathbf{e}_{t}\\otimes\\pmb{g}_{t}=(\\mathbf{0}_{d}^{\\top},\\dots,\\mathbf{0}_{d}^{\\top},\\underbrace{\\pmb{g}_{t}^{\\top}}_{\\mathrm{Indices}},\\mathbf{0}_{d}^{\\top},\\dots,\\mathbf{0}_{d}^{\\top})^{\\top},}\\\\ {\\mathrm{Indices}\\in[d(t{-}1){+}1{,}d t]}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathbf{e}_{t}\\in\\mathbb{R}^{T}$ is the $t^{\\mathrm{th}}$ standard basis vector of $\\mathbb{R}^{T}$ and $\\mathbf{0}_{d}\\in\\mathbb{R}^{d}$ denotes the vector of zeros. We pass these losses to the online learning algorithm $\\boldsymbol{\\mathcal{A}}$ , which predicts with a vector $\\widetilde{\\pmb{w}}_{t}\\in\\mathcal{W}^{T}$ . Finally, we set $\\pmb{w}_{t}\\in\\mathbb{R}^{d}$ equal to the $t^{\\mathrm{th}}$ \"component\u201d? of $\\widetilde{\\pmb{w}}_{t}$ , and play $\\pmb{w}_{t}$ ", "page_idx": 3}, {"type": "text", "text": "We show that the dynamic regret of the resulting algorithm will be equal to the static regret of the algorithm $\\boldsymbol{\\mathcal{A}}$ . In particular, for any sequence $\\vec{\\pmb u}\\,=\\,({\\pmb u}_{1},...\\,{\\pmb u}_{T})$ in $\\mathcal{W}$ we will denote the concatenation of $\\vec{\\pmb{u}}$ into a single vector in WT as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widetilde{\\pmb u}=\\sum_{t=1}^{T}\\mathbf{e}_{t}\\otimes\\pmb u_{t}=(\\pmb u_{1}^{\\top},\\dots,\\pmb u_{T}^{\\top})^{\\top}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Then, the following proposition shows that the dynamic regret of Algorithm $1\\ w.r t$ anysequence $\\vec{\\pmb u}=(\\pmb u_{1},\\dots,\\pmb u_{T})$ is equal to the static regret of $\\mathcal{A}\\,w.r.t\\,\\widetilde{\\pmb{u}}$ ", "page_idx": 3}, {"type": "text", "text": "Proposition 1. Let $\\mathcal{W}\\subseteq\\mathbb{R}^{d}$ and let $\\boldsymbol{\\mathcal{A}}$ be an online learning algorithm with domain $\\mathcal{W}^{T}$ . Then, for any sequence $\\vec{u}=(\\pmb{u}_{1},\\dots,\\pmb{u}_{T})\\in\\mathcal{W}^{T}$ ,Algorithm $^{\\,l}$ guarantees ", "page_idx": 3}, {"type": "equation", "text": "$$\nR_{T}(\\vec{u})=\\sum_{t=1}^{T}\\left\\langle g_{t},w_{t}-u_{t}\\right\\rangle=\\sum_{t=1}^{T}\\left\\langle\\widetilde{g}_{t},\\widetilde{w}_{t}-\\widetilde{u}\\right\\rangle=:R_{T}^{\\mathrm{Seq}}(\\widetilde{u})\\;.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Proof. The proof is immediate from Equations (2) and (3). In fact, observe that the cumulative loss of the comparator sequence is precisely ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\left\\langle g_{t},u_{t}\\right\\rangle=\\left\\langle\\binom{g_{1}}{g_{T}},\\binom{u_{1}}{u_{T}}\\right\\rangle=\\left\\langle\\sum_{t=1}^{T}\\mathbf{e}_{t}\\otimes g_{t},\\sum_{t=1}^{T}\\mathbf{e}_{t}\\otimes u_{t}\\right\\rangle=\\left\\langle\\sum_{t=1}^{T}\\widetilde{g}_{t},\\widetilde{u}\\right\\rangle\\;.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We get a similar relationship for the algorithm's cumulative loss. Hence, we have $R_{T}(\\vec{u})\\;=\\;$ $\\begin{array}{r}{\\sum_{t=1}^{T}\\left\\langle g_{t},w_{t}-u_{t}\\right\\rangle=\\sum_{t=1}^{T}\\left\\langle\\widetilde{\\pmb{g}}_{t},\\widetilde{\\pmb{w}}_{t}-\\widetilde{\\pmb{u}}\\right\\rangle=R_{T}^{\\mathrm{Seq}}(\\widetilde{\\pmb{u}})}\\end{array}$ \u53e3 ", "page_idx": 3}, {"type": "text", "text": "Remark 1. It is important to note that the regret equivalence holds in the context of linear losses. However, our reduction can still be leveraged for arbitrary convex losses by frst applying the standard reductiontoOLOo: $\\begin{array}{r}{\\sum_{t=1}^{T}\\ell_{t}(\\pmb{w}_{t})-\\ell_{t}(\\pmb{u}_{t})\\leq\\sum_{t=1}^{T}\\left\\langle\\pmb{g}_{t},\\pmb{w}_{t}-\\pmb{u}_{t}\\right\\rangle=R_{T}^{\\mathrm{Seq}}(\\widetilde{\\pmb{u}}),}\\end{array}$ $\\pmb{g}_{t}\\in\\partial\\ell_{t}(\\pmb{w}_{t})$ ", "page_idx": 4}, {"type": "text", "text": "While our reduction is exceptionally simple, its utility should not be understated. Proposition 1 is a regret equivalence \u2014\u2014 we lose nothing by taking this perspective, yet it allows us to immediately apply all the usual techniques and approaches from the static regret setting. For instance, given any dual norm pair $(\\lVert\\cdot\\rVert,\\lVert\\cdot\\rVert_{*})$ , it is well-understood how to develop algorithms which adapt simultaneously to the comparator norm $\\lVert\\widetilde{\\boldsymbol{u}}\\rVert$ and to the gradient variance $\\textstyle\\sum_{t=1}^{T}\\|\\widetilde{\\pmb{g}}_{t}\\|_{*}^{2}$ to guarantee ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{R_{T}(\\pmb{u}_{1},\\dots,\\pmb{u}_{T})=R_{T}^{\\mathrm{Seq}}(\\widetilde{\\pmb{u}})\\leq\\widetilde{\\mathcal{O}}\\left(\\lVert\\widetilde{\\pmb{u}}\\rVert\\,\\sqrt{\\sum_{t=1}^{T}\\left\\lVert\\widetilde{\\pmb{g}}_{t}\\right\\rVert_{*}^{2}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Such algorithms are commonly referred to as \u201cparameter-free\", or \u201ccomparator adaptive\", because they achieve this adaptation by completely removing the parameter that depends on the unknown comparator $\\widetilde{\\pmb{u}}$ [e.g., 26, 27, 32, 9, 12, 20, 8, 21, 48]. In this way, we have effectively reduced the problem of minimizing dynamic regret to the problem of selecting a dual-norm pair $(\\lVert\\cdot\\rVert,\\lVert\\cdot\\rVert_{*})$ that meaningfully measures the \u201cdiffculty\u201d of the sequence in $\\widetilde{\\pmb{u}}$ and the losses $\\widetilde{\\pmb{g}}_{t}$ . In particular, $(\\left\\lVert\\cdot\\right\\rVert,\\left\\lVert\\cdot\\right\\rVert_{*})$ should be chosen with the following considerations in mind: ", "page_idx": 4}, {"type": "text", "text": "1. $\\lVert\\widetilde{\\boldsymbol{u}}\\rVert$ should produce a meaningful measure of variability of the comparator sequence $\\pmb{u}_{1},\\dots,\\pmb{u}_{T}$ . For instance, we will show in Proposition 2 that the squared path-length arises from a particular weighted norm applied to $\\widetilde{\\pmb{u}}$   \n2. $\\|\\widetilde{\\boldsymbol{g}}_{t}\\|_{*}$ should not \u201cblow up\". Ideally $\\|\\widetilde{\\pmb{g}}_{t}\\|_{*}$ should match the magnitude of the true losses $\\scriptstyle g_{t}$ up to polylog factors.   \n3. $(\\left\\lVert\\cdot\\right\\rVert,\\left\\lVert\\cdot\\right\\rVert_{*})$ should be chosen with computational considerations in mind. For instance, to apply an FTRL-based algorithm to the losses $\\widetilde{\\pmb{g}}_{t}\\in\\mathbb{R}^{d T}$ , efficient implementation will typically require $\\left\\|\\cdot\\right\\|_{*}$ to have sparse subgradients. In general, an ideal dual-norm pair should facilitate updating only ${\\mathcal{O}}(\\log T)$ variables at a time, so as to match the ${\\mathcal{O}}(d\\log T)$ per-step computation enjoyed by existing dynamic regret algorithms. We will see one such example in Section 4.1. ", "page_idx": 4}, {"type": "text", "text": "In the next section, we show that there is in fact a fundamental trade-off between the penalties induced by the dual-norm pair $(\\left\\lVert\\cdot\\right\\rVert,\\left\\lVert\\cdot\\right\\rVert_{*})$ , creating a tension between the first two considerations. ", "page_idx": 4}, {"type": "text", "text": "3  Lower bounds for unconstrained dynamic regret ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In the static regret setting, there is a well-known trade-off between the way in which we measure the complexity of the comparator $\\textbf{\\em u}$ and the way in which we measure the complexity of the linear losses $\\scriptstyle\\mathbf{\\textit{g}}_{t}$ . For example, in Online Mirror Descent [29, 42] one can get a regret guarantee that depends on the maximum diameter of the feasible set with respect to a norm $\\Vert\\cdot\\Vert$ , while the linear losses are measured using the dual norm $\\|\\cdot\\|_{*}$ . The equivalence in Proposition 1 suggests that a similar tension exists for the dynamic regret. ", "page_idx": 4}, {"type": "text", "text": "Given the structure of our reduction, it makes sense to focus on the weighted norms $\\lVert\\cdot\\rVert_{M}$ and $\\lVert\\cdot\\rVert_{M^{-1}}$ ,where $_M$ is a symmetric positive definite matrix. In particular, the next theorem shows that there is a fundamental trade-off between a variability penalty $\\lVert\\widetilde{\\boldsymbol{u}}\\rVert_{M}$ and a variance penalty $G^{2}\\operatorname{Tr}(M^{-1})$ related to the losses. The proof is provided in Appendix A.1 and it is based on a lower bound to the tail of Rademacher chaos of order 2. ", "page_idx": 4}, {"type": "text", "text": "Theorem 1. Let the number of rounds $T\\geq T_{0}$ where $T_{0}$ is a universal constant. Let $\\boldsymbol{\\mathcal{A}}$ be an online learning algorithm, and suppose $\\boldsymbol{\\mathcal{A}}$ guarantees $R_{T}(0)\\leq G\\epsilon_{T}$ for any sequence of linear losses $g_{1},\\dotsc,g_{T}\\in\\mathbb{R}$ satisfying $|g_{t}|\\leq G$ Let $\\boldsymbol{M}^{-1}\\in\\mathbb{R}^{T\\times T}$ be any symmetric positive definite matrix, denote $\\widetilde{M}^{-1}:=M^{-1}-D i a g\\left(M^{-1}\\right)$ and $V_{T}:=\\mathrm{Tr}(M^{-1})+\\|\\widetilde{M}^{-1}\\|_{F}$ Suppose that $\\begin{array}{r}{\\|\\widetilde{M}^{-1}\\|_{F}^{2}\\;\\ge\\;\\frac{T}{2}\\operatorname*{max}_{i}\\sum_{j}(\\widetilde{M}_{i j}^{-1})^{2}}\\end{array}$ . Then,for any $P$ satisying $\\begin{array}{r}{T_{0}\\,\\le\\,\\log_{2}\\frac{\\sqrt{P V_{T}}}{2\\epsilon_{T}}\\,\\le\\,T,}\\end{array}$ there is $a$ sequence of losses $g_{1},\\ldots,g_{T}\\in\\mathbb{R},$ and $\\widetilde{\\pmb{u}}=(u_{1},\\dots,u_{T})^{\\top}\\in\\mathbb{R}^{T}$ satisfying $\\|\\widetilde{\\boldsymbol{u}}\\|_{M}=\\sqrt{P}$ such that we have ", "page_idx": 5}, {"type": "equation", "text": "$$\nR_{T}(u_{1},\\dots,u_{T})\\ge\\Omega\\left(G\\epsilon_{T}+G\\sqrt{P\\left[\\mathrm{Tr}(M^{-1})+\\left\\lVert\\widetilde{\\boldsymbol{M}}^{-1}\\right\\rVert_{F}\\log_{2}^{\\frac{1}{2}}\\frac{\\sqrt{P V_{T}}}{2\\epsilon_{T}}\\right]}\\right)\\ .\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Let us first briefly discuss the conditions on $_M$ .First, note that the restriction that $_M$ be positive definite and symmetric simply specifies that $\\lVert\\cdot\\rVert_{M}$ defines a valid norm. The condition on $\\begin{array}{r}{\\|\\widetilde{\\boldsymbol{M}}^{-1}\\|_{F}=\\|\\boldsymbol{M}^{-1}-\\mathrm{Diag}\\left(\\boldsymbol{M}^{-1}\\right)\\|_{F}}\\end{array}$ is less straight forward tointerpret,b t entiallystat that the total \u201cvariance'\" of M  is at least as much as that of any of its columns. on a technical level this assumption leads to the restriction that $P$ satisfies $\\log_{2}\\left(\\sqrt{P V_{T}}/2\\epsilon_{T}\\right)\\leq T$ . This is a natural restriction which encodes the fact that if $P$ is too large relative to $T$ (i.e., when $\\log_{2}(\\sqrt{P V}/2\\epsilon_{T})\\geq T)$ \uff0c one can ensure \u201clow\" regret by simply playing $w_{t}=\\mathbf{0}$ on every round: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{R_{T}}(\\vec{u})=-\\sum_{t=1}^{T}\\left<\\pmb{g}_{t},\\pmb{u}_{t}\\right>\\leq\\operatorname*{max}_{t}\\left\\|\\pmb{u}_{t}\\right\\|G\\,T\\leq G\\operatorname*{max}_{t}\\left\\|\\pmb{u}_{t}\\right\\|\\log_{2}\\left(\\sqrt{P V_{T}}/2\\epsilon\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "and hence the only lower bounds in such settings are trivial ones and it suffices to consider only $P$ satisfying $\\log\\left(\\dot{\\sqrt{P V_{T}}}/2\\epsilon\\right)\\leq T$ . We will see in Proposition 2 that the matrix that produces the squared path-length satisfies this condition, and it can be seen that symmetric matrices with equal column sums (as is the case in Proposition 3) satisfy this condition as well. ", "page_idx": 5}, {"type": "text", "text": "The result of Theorem 1 shows that there is a frontier of lower bounds which trade off penalties related to variability of the comparator sequence and penalties related to the variance of the subgradients. That is, one can not guarantee a small variability penalty in all situations without also accepting a large subgradient variance penalty. The next proposition shows that i) the squared path-length can be represented by a particular choice of the weighted norm $\\lVert\\widetilde{\\boldsymbol{u}}\\rVert_{M}$ , and i) the fundamental tension between $\\lVert\\widetilde{\\boldsymbol{u}}\\rVert_{M}$ and its corresponding variance penalty $\\mathrm{Tr}(M^{-1})$ prevents any algorithm from ataining the ideal ariability dependence of $\\begin{array}{r}{\\|\\widetilde{\\boldsymbol{u}}\\|_{M}\\,=\\,\\left(\\sqrt{\\textstyle\\sum_{t=1}^{T-1}\\|\\boldsymbol{u}_{t}-\\boldsymbol{u}_{t+1}\\|^{2}}\\right)}\\end{array}$ . In fact, the corresponding variance penalty is $G^{2}\\operatorname{Tr}(M^{-1})={\\mathcal{O}}(G^{2}T^{2})$ , resulting in a vacuous guarantee. Proof of the proposition can be found in Appendix A.2. ", "page_idx": 5}, {"type": "text", "text": "Proposition 2. (Adapting to Squared Path-length Requires Superlinear Regret) Define the finitedifferenceoperator $\\bar{\\Sigma}\\in\\bar{\\mathbb{R}}^{T}$ asthematrixwithentries ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\Sigma_{i j}={\\left\\{\\begin{array}{l l}{1}&{i f\\,i=j}\\\\ {-1}&{i f\\,i=j-1}\\\\ {0}&{o t h e r w i s e}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Let $\\mathbf{S}=\\pmb{\\Sigma}^{\\top}\\pmb{\\Sigma}$ and $M=\\mathbf{S}\\otimes I_{d}$ Then, $_M$ satisfies the assumptions of Theorem 1 and ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\|\\widetilde{\\boldsymbol{u}}\\|_{M}^{2}=\\|\\boldsymbol{u}_{T}\\|_{2}^{2}+\\sum_{t=1}^{T-1}\\left\\|\\boldsymbol{u}_{t}-\\boldsymbol{u}_{t+1}\\right\\|_{2}^{2}\\qquad\\boldsymbol{a}\\boldsymbol{n}d\\qquad\\mathrm{Tr}\\left(\\boldsymbol{M}^{-1}\\right)=\\frac{T(T+1)}{2}\\;.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Proposition 2 shows that adapting to the squared path-length of an arbitrary comparator sequence necessarily requires incurring a linear penalty, so adapting to the squared path-length is impossible without facing a vacuous guarantee. However, we will show in Section 4.1 that it is possible to adapt to a measure of variability which is similar in spirit to the squared path-length, yet only incurs a $\\operatorname{Tr}(M^{-1})={\\mathcal{O}}(\\log T)$ variancepenalty. ", "page_idx": 5}, {"type": "text", "text": "Remark 2. The matrix $_M$ in Proposition 2 uniquely exposes the the squared path-length up to the biasterm $\\left\\Vert u_{T}\\right\\Vert^{2}$ :Suchabias termmust appearbecause inthe static regret setting,wherein $\\pmb{u}_{1}=$ $\\mathbf{\\Omega}\\cdot\\mathbf{\\Omega}=\\pmb{u}_{T}=\\pmb{u},$ the variability measure $\\left\\|\\cdot\\right\\|_{M}$ must still reduce to a dependence on $\\lVert u\\rVert$ otherwise theguaranteewouldviolateexisting $\\widetilde\\Omega(\\|u\\|\\sqrt{T})$ lower bounds for static regret. More generally, we show in Appendix A.3 that any other choice of bias would similarly lead to $\\operatorname{Tr}\\left(M^{-1}\\right)\\geq\\Omega(T^{2})$ so Proposition 2 along with our lower bound in Theorem 1 are suffcient to show that adapting to squared path-length requires accepting a vacuous guarantee. ", "page_idx": 5}, {"type": "image", "img_path": "hD8Et4uZ1o/tmp/cc0c47960cd6fcbfa800f2ece7aa0b0ac126e7d06df28d016ea395855657da28.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "4   Dynamic regret for unconstrained OLO via weighted norms ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "So far, we've seen that there exists a frontier of lower bounds trading off a variability penalty, measured by $\\lVert\\widetilde{\\boldsymbol{u}}\\rVert_{M}$ , and a loss variance penalty, measured by $\\mathrm{Tr}(M^{-1})$ , and that the tension between these two quantities makes it impossible to adapt to the squared path-length of the comparator sequence without accepting a vacuous regret guarantee. A natural next question is whether there are choices Oof $_M$ which lead to a more favorable trade-off of these two quantities. In this section, we provide a simple framework for achieving lower bounds along the frontier described by Theorem 1, and an instance which successfully achieves an improved variance/variability trade-off. The guarantees on the lower bound frontier can be achieved using any parameter-free algorithm along with the 1-dimensional reduction of Cutkosky and Orabona [9] to extend the algorithm to dual-norm pair $\\left(\\left\\Vert\\cdot\\right\\Vert_{M},\\left\\Vert\\cdot\\right\\Vert_{M^{-1}}\\right)$ . The generic procedure is summarized in Algorithm 2 for convenience. ", "page_idx": 6}, {"type": "text", "text": "Theorem 2. Let $\\mathbf{S}\\in\\mathbb{R}^{T\\times T}$ be a symmetric positive definite matrix, ${\\cal M}\\,=\\,{\\bf S}\\,\\otimes\\,{\\cal I}_{d},$ and $\\epsilon\\,>\\,0$ There is an algorithm $\\boldsymbol{\\mathcal{A}}$ such that for any $\\pmb{g}_{1},\\bar{\\dots},\\pmb{g}_{T}\\in\\bar{\\mathbb{R}}^{d}$ satisfying $\\|\\pmb{\\mathscr{g}}_{t}\\|_{2}\\leq G$ for all $t$ and any sequence $\\vec{\\b u}=\\left(\\pmb u_{1},\\dots,\\pmb u_{T}\\right)\\in\\mathbb{R}^{d T}$ , the dynamic regret is bounded as ", "page_idx": 6}, {"type": "equation", "text": "$$\nR_{T}(\\vec{u})\\leq\\mathcal{O}\\left(\\mathfrak{G}\\epsilon+\\Vert\\widetilde{u}\\Vert_{M}\\left[\\sqrt{V_{T}\\log\\left(\\frac{\\Vert\\widetilde{u}\\Vert_{M}\\sqrt{V_{T}}}{\\mathfrak{G}\\epsilon}+1\\right)}\\vee\\mathfrak{G}\\log\\left(\\frac{\\Vert\\widetilde{u}\\Vert_{M}\\sqrt{V_{T}}}{\\epsilon\\mathfrak{G}}\\right)\\right]\\right),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where V =(- and 6 = GI-0, ", "page_idx": 6}, {"type": "text", "text": "For the proof, we will need the following technical lemma. ", "page_idx": 6}, {"type": "text", "text": "Lemma 1. Let $\\mathbf{S}\\,\\in\\,\\mathbb{R}^{T\\times T}$ be a symmetric positive definite matrix and let ${\\cal M}\\,=\\,{\\bf S}\\otimes{\\cal I}_{d}$ .For $t=1,\\dots,T,$ let $\\pmb{g}_{t}\\in\\mathbb{R}^{d}$ and let $\\widetilde{\\pmb{g}}_{t}=\\mathbf{e}_{t}\\otimes\\pmb{g}_{t}$ . Then, we have $\\lVert\\widetilde{\\pmb{g}}_{t}\\rVert_{M}^{2}=\\left\\lVert\\pmb{g}_{t}\\right\\rVert_{2}^{2}S_{t t}$ ", "page_idx": 6}, {"type": "text", "text": "Proof. Using the mixed-product property $(A\\otimes B)(C\\otimes D)=A C\\otimes B D$ and the transpose property $(A\\otimes B)^{\\top}=A^{\\top}\\otimes B^{\\top}$ of the Kronecker product, we have that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\widetilde{g}_{t},M\\widetilde{g}_{t}\\rangle=\\langle\\mathbf{e}_{t}\\otimes g_{t},[\\mathbf{S}\\otimes I_{d}]\\,\\mathbf{e}_{t}\\otimes g_{t}\\rangle=\\langle\\mathbf{e}_{t}\\otimes g_{t},\\mathbf{S}\\mathbf{e}_{t}\\otimes g_{t}\\rangle=(\\mathbf{e}_{t}^{\\top}\\otimes g_{t}^{\\top})(\\mathbf{S}\\mathbf{e}_{t}\\otimes g_{t})}\\\\ &{\\qquad\\qquad=\\mathbf{e}_{t}^{\\top}\\mathbf{S}\\mathbf{e}_{t}\\otimes g_{t}^{\\top}g_{t}=S_{t t}\\,\\Vert g_{t}\\Vert^{2}~.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "ProfofThrAiPrsitwha $\\begin{array}{r}{R_{T}(\\vec{u})=\\sum_{t=1}^{T}\\langle\\widetilde{\\pmb{g}}_{t},\\widetilde{\\pmb{w}}_{t}-\\widetilde{\\pmb{u}}\\rangle=R_{T}^{\\mathrm{Seq}}(\\widetilde{\\pmb{u}})}\\end{array}$ Since $_M$ is symmetric and positive definite, $\\left(\\lVert\\cdot\\rVert_{M}\\,,\\lVert\\cdot\\rVert_{M^{-1}}\\right)$ is a valid dual-norm pair. By Lemma 1, we have $\\|\\widetilde{\\pmb{g}}_{t}\\|_{M^{-1}}^{2}=\\|\\pmb{g}_{t}\\|_{2}^{2}\\,S_{t t}^{-1}\\le G^{2}\\,\\big\\|\\mathbf{S}^{-1}\\big\\|_{\\infty,\\infty}:=\\mathfrak{G}^{2}$ Hence, let $\\boldsymbol{\\mathcal{A}}$ be any algorithm which guarantees a parameter-free regret w.rt. $\\left(\\left\\lVert\\cdot\\right\\rVert,\\left\\lVert\\cdot\\right\\rVert_{*}\\right)$ on losses satisfying $\\|\\widetilde{\\pmb{g}}_{t}\\|_{M^{-1}}\\leq\\mathfrak{G}$ . Note that any parameter-free algorithm can be extended to handle arbitrary dual-norm pairs by leveraging the one-dimensional reduction of Cutkosky and Orabona [9, Section 3], that reduces the OLO problem to a unconstrained 1d problem plus an OLO problem in the unitary ball defined by the primal norm. For instance, applying Jacobsen and Cutkosky [20, Algorithm 1] with the one-dimensional reduction one can easily show (see details in Appendix B.1) ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "equation", "text": "$$\nR_{T}(\\vec{u})\\leq\\mathcal{O}\\left(\\mathfrak{G}\\epsilon+\\left\\Vert\\widetilde{u}\\right\\Vert_{M}\\left[\\sqrt{V_{T}\\log\\left(\\frac{\\left\\Vert\\widetilde{u}\\right\\Vert_{M}\\sqrt{V_{T}}\\Lambda_{T}}{\\mathfrak{G}\\epsilon}+1\\right)}\\vee\\mathfrak{G}\\log\\left(\\frac{\\left\\Vert\\widetilde{u}\\right\\Vert_{M}\\sqrt{V_{T}}\\Lambda_{T}}{\\epsilon\\mathfrak{G}}\\right)\\right]\\right),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Note in particular that by Lemma 1, we have $\\begin{array}{r}{\\sum_{t=1}^{T}\\left\\Vert\\widetilde{\\pmb{g}}_{t}\\right\\Vert_{M^{-1}}^{2}=\\sum_{t=1}^{T}S_{t t}^{-1}\\left\\Vert\\pmb{g}_{t}\\right\\Vert^{2}\\leq G\\sum_{t=1}^{T}S_{t t}^{-1}=}\\end{array}$ $G\\,\\mathrm{Tr}({\\bf S}^{-1})$ , so this bound matches the lower bound from Section 3, up to polylogarithmic terms.3 Thus, any valid choice of $_M$ will be on the lower bound frontier of Section 3. ", "page_idx": 7}, {"type": "text", "text": "4.1   Trading-off Variance and Variability ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Leveraging the algorithm characterized by Theorem 2, we now show that it is indeed possible to chose $_M$ such that $\\sum_{t=1}^{T}\\|\\widetilde{\\pmb{g}}_{t}\\|_{M^{-1}}^{2}$ $\\begin{array}{r}{\\mathcal{O}(\\log\\left(T\\right)\\sum_{t=1}^{T}\\left\\|g_{t}\\right\\|^{2})}\\end{array}$ , in exchange for a variablity penalty which is still similar in spirit to the squared path-length. ", "page_idx": 7}, {"type": "text", "text": "Inspired by the Haar OLR algorithm of [47], we apply Theorem 2 using $\\mathbf{S}=\\mathbf{H}_{n}\\mathbf{H}_{n}^{\\top}$ ,where $\\mathbf{H}_{n}$ is the unnormalized Haar basis matrix of order $n=\\lceil\\bar{\\log_{2}{T}}\\rceil$ . The Haar wavelet transform and its basis matrix are common tools in the signal processing literature; we recall the basic definitions and facts for convenience in Appendix B.2. With this choice, we have the following bounds on $\\lVert\\widetilde{\\boldsymbol{u}}\\rVert_{M}$ and $\\|\\widetilde{\\pmb{g}}_{t}\\|_{M^{-1}}^{2}$ . The proof can be found in Appendix B.3. ", "page_idx": 7}, {"type": "text", "text": "2 $n=\\log_{2}T$ $\\mathbf{H}_{n}$ $n$ $\\tau\\,\\in\\,\\left\\{2^{i}:i=0,\\dots,\\log_{2}T\\right\\}$ $N_{\\tau}=T/\\tau$ $\\mathcal{T}_{1}^{(\\tau)},\\dots,\\mathcal{T}_{N_{\\tau}}^{(\\tau)}$ IN) be a partition of [T] into interasoflength $\\tau$ Defne the everage coparatorin intenadl $\\mathcal{T}_{i}^{(\\tau)}$ to be $\\begin{array}{r}{\\bar{\\pmb u}_{i}^{(\\tau)}=\\frac{1}{\\tau}\\sum_{t\\in\\mathcal{Z}_{i}^{(\\tau)}}{\\pmb u}_{t},}\\end{array}$ and define the squared path-length at time-scale $\\tau<T$ to be ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\bar{P}(\\vec{u},\\tau):=\\sum_{i=1}^{N_{\\tau}/2}\\left\\lVert\\bar{\\pmb u}_{2i-1}^{(\\tau)}-\\bar{\\pmb u}_{2i}^{(\\tau)}\\right\\rVert_{2}^{2},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "and $\\bar{P}(\\vec{u},T)=\\left\\|\\bar{\\pmb{u}}_{1}^{(T)}\\right\\|_{2}^{2}=\\left\\|\\bar{\\pmb{u}}\\right\\|_{2}^{2}$ Then, seting ${\\bf S}=[{\\bf H}_{n}{\\bf H}_{n}^{\\top}]^{-1}$ and $M=\\mathbf{S}\\otimes I_{d}$ we have ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\|{\\widetilde u}\\|_{M}^{2}\\leq\\|{\\bar{u}}\\|_{2}^{2}+\\displaystyle\\frac{1}{4}\\sum_{i=0}^{\\log_{2}(T)}{\\bar{P}}({\\vec{u}},2^{i})\\leq\\|{\\bar{u}}\\|_{2}^{2}+\\displaystyle\\frac{1}{4}\\log\\left(T\\right)\\operatorname*{max}_{\\tau}{\\bar{P}}({\\vec{u}},\\tau),}}\\\\ {{\\displaystyle\\|{\\widetilde g}_{t}\\|_{M^{-1}}^{2}=\\|g_{t}\\|_{2}^{2}\\left(1+\\log T\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Summarizing, by applying Algorithm 1 with ${\\bf S}=[{\\bf H}_{n}{\\bf H}_{n}^{\\top}]^{-1}$ we ensure regret ", "page_idx": 7}, {"type": "equation", "text": "$$\nR_{T}(\\vec{u})\\leq\\widetilde{\\mathcal{O}}\\left(\\sqrt{\\left(\\|\\bar{u}\\|_{2}^{2}+\\operatorname*{max}_{\\tau}\\sum_{i=1}^{N_{\\tau}/2}\\left\\|\\bar{u}_{2i+1}^{(\\tau)}-\\bar{u}_{2i}^{(\\tau)}\\right\\|_{2}^{2}\\right)\\sum_{t=1}^{T}\\|g_{t}\\|_{2}^{2}}\\right)\\ .\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "This is the first fully decoupled guarantee for general dynamic regret which incurs no pessimistic multiplicative penalties of the form $\\operatorname*{max}_{t,t^{\\prime}}\\|\\boldsymbol{u}_{t}-\\boldsymbol{u}_{t^{\\prime}}\\|$ . That is, the terms depending on the comparators and the terms depending on the gradients appear in separate sums. Moreover, observe that ", "page_idx": 7}, {"type": "text", "text": "this measure of variability can immediately be related to the more standard (first-order/non-squared) path-length using the local averaging lemma of Zhang et al. [47] (Lemma D.7). We have ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\widetilde{u}\\|_{M}^{2}\\leq\\|\\bar{u}\\|_{2}^{2}+\\frac{\\log_{2}T}{4}\\operatorname*{max}_{\\tau}\\sum_{i=1}^{N_{\\tau}/2}\\left\\|\\bar{u}_{2i-1}^{(\\tau)}-\\bar{u}_{2^{i}}^{(\\tau)}\\right\\|_{2}^{2}\\leq\\widetilde{\\mathcal{O}}\\left(\\bar{D}^{2}+\\operatorname*{max}_{\\tau}\\bar{D}\\sum_{i=1}^{N_{\\tau}/2}\\left\\|\\bar{u}_{2i-1}^{(\\tau)}-\\bar{u}_{2^{i}}^{(\\tau)}\\right\\|_{2}\\right)}\\\\ {\\displaystyle\\leq\\widetilde{\\mathcal{O}}\\left(\\bar{D}^{2}+\\bar{D}\\sum_{t=1}^{T-1}\\left\\|u_{t}-u_{t+1}\\right\\|_{2}\\right)\\leq\\widetilde{\\mathcal{O}}\\left(\\bar{D}^{2}+\\bar{D}P_{T}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\begin{array}{r}{\\bar{D}=\\operatorname*{max}_{\\tau,i}\\left\\|\\bar{\\boldsymbol{u}}_{i}^{(\\tau)}-\\bar{\\boldsymbol{u}}_{i+1}^{(\\tau)}\\right\\|\\le\\operatorname*{max}_{i,j}\\|\\boldsymbol{u}_{i}-\\boldsymbol{u}_{j}\\|}\\end{array}$ Thus applyingAgorim 1 wihl. norm pair $\\left(\\left\\Vert\\cdot\\right\\Vert_{\\mathbf{H}_{n}^{-}}\\right\\Vert_{\\mathbf{H}_{n}^{\\top}},\\left\\Vert\\cdot\\right\\Vert_{\\mathbf{H}_{n}\\mathbf{H}_{n}^{\\top}}\\right)$ still guarantees worst-caseregret ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{R_{T}(\\vec{u})\\leq\\widetilde{\\mathcal{O}}\\left(\\lVert\\widetilde{u}\\rVert_{\\mathbf{H}_{n}^{-\\top}\\mathbf{H}_{n}^{-1}}\\sqrt{\\sum_{t=1}^{T}\\lVert\\widetilde{g}_{t}\\rVert_{\\mathbf{H}_{n}\\mathbf{H}_{n}^{\\top}}^{2}}\\right)\\leq\\widetilde{\\mathcal{O}}\\left(\\sqrt{\\left(\\lVert\\bar{u}\\rVert_{2}^{2}+\\bar{D}P_{T}\\right)\\sum_{t=1}^{T}\\lVert g_{t}\\rVert_{2}^{2}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "which matches the guarantees of prior works, up to polylogarithmic terms. ", "page_idx": 8}, {"type": "text", "text": "Importantly, with $M\\,=\\,{\\pmb H}_{n}^{-\\top}{\\pmb H}_{n}^{-1}\\otimes{\\pmb I}_{d}$ the dual-norm pair $\\left(\\lVert\\cdot\\rVert_{M}\\,,\\lVert\\cdot\\rVert_{M^{-1}}\\right)$ leads to updates that can be implemented efficiently, in requiring only $O(\\log T)$ variables to be updated on each round. This is because the Haar basis matrices are locally supported \u2014\u2014 the columns of ${\\cal{H}}_{n}\\,=$ $\\left(\\pmb{h}^{(1)}\\quad.\\dots\\quad\\pmb{h}^{(T)}\\right)\\in\\mathbb{R}^{T\\times T}$ form an orthogonal basis with the property that for any $t$ $[\\pmb{h}^{(i)}]_{t}\\neq0$ for only $1+\\log_{2}T$ indices $i$ (see Proposition 5). Hence, $(\\pmb{H}^{\\top}\\otimes\\pmb{I}_{d})\\widetilde{\\pmb{g}}_{t}=(\\pmb{H}^{\\top}\\otimes\\pmb{I}_{d})(\\mathbf{e}_{t}\\otimes\\pmb{g}_{t})=$ $(\\boldsymbol{H}^{\\top}\\mathbf{e}_{t})\\otimes\\mathbf{g}_{t}$ , is a block vector with only $1+\\log_{2}T$ active blocks, requiring that we update only $O(d\\log T)$ indices to maintain each of the variables needed to implement Algorithm 2. We provide the full details of this computation in Appendix B.4, which we summarize below in Proposition 4. ", "page_idx": 8}, {"type": "text", "text": "Proposition 4. The algorithm characterized by applying Theorem 2 with $S=[H_{n}H_{n}^{\\top}]^{-1}$ canbe implementedwith ${\\mathcal{O}}\\left(d\\log T\\right)$ per-round computation. ", "page_idx": 8}, {"type": "text", "text": "5  Recovering Variance-Variability Coupling Guarantees ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Our main focus throughout the paper has been on designing algorithms that achieve a regret bounds of the form $R_{T}(\\vec{u})\\,\\leq\\,O\\left(\\sqrt{f(u_{1},\\dots,u_{T})V(g_{1},\\dots,g_{T})}\\right)$ for some functions $f$ and $V$ ,which cleanly separates the penalties associated with difficult loss sequences from the penalties associated with difficult comparator sequences. However, the first works to achieve unconstrained dynamic regret guarantees uncovered guarantees of a slightly different form, containing a gradient-comparator correlationpenalty: ", "page_idx": 8}, {"type": "equation", "text": "$$\nR_{T}(\\vec{u})\\leq\\widetilde{O}\\left(\\sqrt{\\sum_{t=1}^{T-1}\\|u_{t}-u_{t+1}\\|\\sum_{t=1}^{T}\\|g_{t}\\|^{2}\\,\\|u_{t}-\\bar{u}\\|}\\right),\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "for some reference point $\\bar{\\pmb u}$ [20, 47]. Guarantees of this form allow some degree of coupling between the variability and variance penalties. This can be appealing in certain situations. For instance, guarantees of the form above have the appealing property that the variance penalty completely disappears on any rounds where the comparator $\\pmb{u}_{t}$ matches the reference point $\\bar{\\pmb u}$ . This can be a very powerful property when one has $a$ priori access to a benchmark model (represented by $\\bar{\\pmb u}$ which can be expected to predict well on average, so that we accumulate the variance penalties only when facing atypical/unexpected conditions. ", "page_idx": 8}, {"type": "text", "text": "The prior works achieving a coupling guarantee do so using rather mysterious means. For instance, the guarantee of Jacobsen and Cutkosky [20] achieves the coupling guarantee almost by coincidence, as it appears in response to a composite regularizer they add to the update to cancel out certain unstable terms in the analysis, and the analysis of Zhang et al. [47] recovers a guarantee of a similar form using a rather dificult analysis of the frequency-domain representation of $\\widetilde{\\pmb{u}}$ after projecting onto the Haar basis vectors. So far there is no unifying explanation of the principles leading to these sorts of guarantees. ", "page_idx": 8}, {"type": "text", "text": "Our equivalence in Proposition 1 instead shows that guarantees of the form Equation (4) can instead be understood through the lens of reward-regret duality, a standard tool used to design algorithms in the static regret setting. The reward-regret duality states that in order to guarantee regret of the form $R_{T}(\\pmb{u})\\leq f(\\pmb{u})$ for all $\\pmb{u}\\in\\mathcal{W}$ , it suffices to design an algorithm that guarantees $\\begin{array}{r}{-\\sum_{t=1}^{T}\\left\\langle g_{t},\\boldsymbol{w}_{t}\\right\\rangle\\geq f^{*}(-\\sum_{t=1}^{T}g_{t})}\\end{array}$ $g_{1},\\dots,g_{T}$ ", "page_idx": 9}, {"type": "text", "text": "Theorem 3. Let WealthT $\\begin{array}{r}{\\mathrel{\\mathop:}=\\,-\\sum_{t=1}^{T}\\langle\\widetilde{\\pmb{g}}_{t},\\widetilde{\\pmb{w}}_{t}\\rangle}\\end{array}$ denote the \u201cwealth\" of an algorithm $\\boldsymbol{\\mathcal{A}}$ and let $(f,f^{*})$ be a Fenchel conjugate pair Then $\\boldsymbol{\\mathcal{A}}$ guarantees WealthT $\\mathbf{\\Psi}:\\;\\geq\\;f_{T}^{*}\\big(-\\sum_{t=1}^{T}\\widetilde{\\pmb g}_{t}\\big)$ for any sequence $\\widetilde{\\pmb g}_{1},\\dots,\\widetilde{\\pmb g}_{T}$ if and only if $R_{T}(\\vec{\\pmb{u}})\\,\\leq\\,f_{T}(\\widetilde{\\pmb{u}})$ for any sequence $\\vec{\\pmb u}\\,=\\,({\\pmb u}_{1},\\dots,{\\pmb u}_{T})$ in $\\mathcal{W}$ where $\\widetilde{\\pmb u}=(\\pmb u_{1}^{\\top},\\dots,\\pmb u_{T}^{\\top})^{\\top}$ is the concatenation of the sequence $\\vec{\\pmb{u}}$ into a vector. ", "page_idx": 9}, {"type": "text", "text": "So, suppose we would like to design an algorithm that guarantees for any sequence $\\vec{\\pmb u}=(\\pmb u_{1},\\dots,\\pmb u_{T})$ and any $\\vec{g}=(g_{1},\\dots,g_{T})$ regret of the form ", "page_idx": 9}, {"type": "equation", "text": "$$\nR_{T}(\\vec{u})\\leq\\sqrt{f_{T}(\\widetilde{\\mathbf{u}})V_{T}(\\widetilde{\\mathbf{u}})},\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "for some $f_{T}(\\widetilde{\\boldsymbol{u}})$ and $V_{T}(\\widetilde{\\mathbf{u}})=V_{T}(\\widetilde{\\mathbf{u}};\\vec{g})$ Then, since $\\begin{array}{r}{\\sqrt{a b}=\\operatorname*{min}_{\\eta\\geq0}\\frac{a}{2\\eta}+\\frac{\\eta}{2}b}\\end{array}$ any such algorithm must have $\\begin{array}{r}{R_{T}(\\vec{u})\\leq\\frac{f_{T}(\\widetilde{\\mathbf{u}})}{2\\eta}\\!+\\!\\frac{\\eta}{2}V_{T}(\\widetilde{\\mathbf{u}})}\\end{array}$ forevery $\\eta\\geq0$ So, viaProposition and the thereward-regret duality of Theorem 3, we have that the desired guarantee is equivalent to guaranteeing for all $\\eta\\geq0$ a wealthlower bound of ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\boldsymbol{\\mathrm{Vealth}}_{t}=-\\sum_{t=1}^{T}\\langle\\widetilde{g}_{t},\\widetilde{w}_{t}\\rangle\\ge\\left[\\frac{f_{T}(\\cdot)}{2\\eta}+\\frac{\\eta}{2}V_{T}(\\cdot)\\right]^{*}\\left(-\\widetilde{g}_{1:T}\\right)=\\frac{f_{T}^{*}\\big(-2\\eta\\widetilde{g}_{1:T}\\big)}{2\\eta}\\sqcup2\\eta V_{T}^{*}\\left(\\frac{\\widetilde{g}_{1:T}}{2\\eta}\\right),\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "where $f_{T}^{*}$ and $V_{T}^{*}$ are the Fenchel conjugates of $f_{T}$ and $V_{T}$ respectively, and $(f_{1}\\sqcup f_{2})$ denotes the infimal convolution [34, 19] of $f_{1}$ and $f_{2}$ ", "page_idx": 9}, {"type": "equation", "text": "$$\n(f_{1}\\boxed{f_{2}})(z)=\\operatorname*{inf}\\left\\{f_{1}(y)+f_{2}(z-y)\\right\\}.\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "Thus, the variance/variability coupling guarantees observed in Equation (4) can be interpreted as achieving wealth lower-bounds for potential functions involving infimal convolution. ", "page_idx": 9}, {"type": "text", "text": "The above discussion provides a general characterization of variance/variability coupling guarantes, though it is admittedly less clear how difficult it is to design algorithms from this perspective due to the rather complicated potential function that appears. Nonetheless, we believe that this provides a valuable perspective and insight that could be of general interest. An important direction for future work is to develop useful tools for working with potential functions of this form. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we have shown a way to reduce the problem of dynamic regret minimization to the static one. We proved a novel frontier of lower bounds showing a fundamental trade-off between penalties on the comparators and penalties on the variance of the gradients. In particular, we have shown thatit is no psible to achieve a guarante tha cale witht $\\sqrt{\\sum_{t=1}^{T-1}\\left\\Vert u_{t}-u_{t+1}\\right\\Vert^{2}}$ without incurring a variance penalty of $\\mathcal{O}(G T)$ . We developed a simple framework for achieving guarantees along the lower bound frontier, and used it to develop the first algorithm making a non-trivial variance/variability decoupling guarantee against arbitrary comparator sequences. Our framework is simple but powerful, allowing one to fully utilize the rich literature of static regret algorithms for Online learning. ", "page_idx": 9}, {"type": "text", "text": "We conclude by noting some directions for future work. There is a lot of exciting potential to explore different measures of variability induced by different choices of the matrix $_M$ ,aswell asgoing beyond weighted norms. As mentioned in Section 5, developing a useful toolset for potential functions involving infimal convolution is an important next-step for developing and understanding guarantees with a coupled variance/variability penalty, such as Equation (4). Also, our lower bound in Section 3 illustrates the variance-variability trade-off, but achieving the correct logarithmic dependencies proved to be very challenging \u2014\u2014 many of the standard tools for proving lower bounds in unconstrained settings revolve around anti-concentration results that do not readily extend to arbitrary weighted norms and higher-dimensions. We look forward to exciting development in these future directions. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We thank Yu-Xiang Wang for the discussion on the function classes studied in non-parametric regression theory. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1]  Dheeraj Baby and Yu-Xiang Wang. Online forecasting of total-variation-bounded sequences In Advances in Neural Information Processing Systems, volume 32, 2019. [2]  Dheeraj Baby and Yu-Xiang Wang. Optimal dynamic regret in exp-concave online learning. In Conference on Learning Theory, pages 359-409. PMLR, 2021. [3]  Omar Besbes, Yonatan Gur, and Assaf Zeevi. Non-stationary stochastic optimization. Operations Research, 63(5):1227-1244, 2015. doi: 10.1287/opre.2015.1408.   \n[4]  Nicolo Campolongo and Francesco Orabona. A closer look at temporal variability in dynamic online learning, 2021. [5]  Nicolo Cesa-Bianchi and Gabor Lugosi. Prediction, learning, and games. Cambridge University Press, 2006. [6]  Nicolo Cesa-Bianchi and Francesco Orabona. Online learning algorithms. Annual Review of Statistics and Its Application, 8:165-190, 2021. [7]  Ting-Jui Chang and Shahin Shahrampour. On online optimization: Dynamic regret analysis of strongly convex and smooth problems. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 6966-6973, 2021. [8]  Keyi Chen, Ashok Cutkosky, and Francesco Orabona. Implicit parameter-free online learning with truncated linear models. In International Conference on Algorithmic Learning Theory, pages 148-175. PMLR, 2022. [9]  Ashok Cutkosky and Francesco Orabona. Black-box reductions for parameter-free online learning in banach spaces. In Sebastien Bubeck, Vianney Perchet, and Philippe Rigollet, editors, Proceedings of the 31st Conference On Learning Theory, volume 75 of Proceedings of Machine Learning Research, pages 1493-1529. PMLR, 06-09 Jul 2018.   \n[10]  Irit Dinur, Ehud Friedgut, Guy Kindler, and Ryan O'Donnell. On the Fourier tails of bounded functions over the discrete cube. In Proceedings of the thirty-eighth annual ACM symposium on Theory of computing, pages 437-446, 2006.   \n[11]  Bogdan J Falkowski. Generalized haar spectral representations and their applications. Nanyang Technological University. Singapore, 1998.   \n[12]  Dylan J. Foster, Alexander Rakhlin, and Karthik Sridharan. Online learning: Suffcient statistics and the burkholder method. In Sebastien Bubeck, Vianney Perchet, and Philippe Rigollet, editors, Proceedings of the 31st Conference On Learning Theory, volume 75 of Proceedings of Machine Learning Research, pages 3028-3064. PMLR, 06-09 Jul 2018.   \n[13]  Gene H Golub and Charles F Van Loan. Matrix computations. JHU press, 2013.   \n[14]  Geoffrey J. Gordon. Regret bounds for prediction problems. In Proc. of the twelfth annual conference on Computational learning theory (COLT), pages 29-40, 1999.   \n[15]  Andras Gyorgy and Csaba Szepesvari. Shifting regret, mirror descent, and matices. In Maria Florina Balcan and Kilian Q. Weinberger, editors, Proceedings of The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research, pages 2943-2951, New York, New York, USA, 20-22 Jun 2016. PMLR.   \n[16]  Eric C. Hall and Rebecca M. Willett. Online optimization in dynamic environments, 2016.   \n[17]  Mark Herbster and Manfred K Warmuth. Tracking the best regressor. In Proceedings of the eleventh annual conference on Computational learning theory, pages 24-31, 1998.   \n[18]  Mark Herbster and Manfred K Warmuth. Tracking the best linear predictor. Journal of Machine Learning Research, 1(281-309):10-1162, 2001.   \n[19]  Jean-Baptiste Hiriart-Uruty and Claude Lemarechal. Fundamentals of convex analysis. Springer Science & Business Media, 2004.   \n[20] Andrew Jacobsen and Ashok Cutkosky. Parameter-free mirror descent. In Po-Ling Loh and Maxim Raginsky,editors, Proceedings of Thirty Fifth Conference on Learning Theory,volume 178 of Proceedings of Machine Learning Research, pages 4160-4211. PMLR, 02-05 Jul 2022.   \n[21]  Andrew Jacobsen and Ashok Cutkosky. Unconstrained online learning with unbounded losses. In International Conference on Machine Learning (ICML). PMLR, 2023.   \n[22]  Ali Jadbabaie, Alexander Rakhlin, Shahin Shahrampour, and Karthik Sridharan. Online Optimization : Competing with Dynamic Comparators. In Guy Lebanon and S. V. N. Vishwanathan, editors, Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics, volue 38 of Procedings of Machine Learning Research, pages 398-406, SanDieg, California, USA, 09-12 May 2015. PMLR.   \n[23]  Charles Royal Johnson. Positive definite matrices. The American Mathematical Monthly, 77(3): 259-264, 1970.   \n[24] Wouter M Koolen, Alan Malek, Peter L Bartlett, and Yasin Abbasi-Yadkori. Minimax time series prediction. In Advances in Neural Information Processing Systems, volume 28, 2015.   \n[25] Haipeng Luo, Mengxiao Zhang, Peng Zhao, and Zhi-Hua Zhou. Corralling a larger band of bandits: A case study on switching regret for linear bandits. In Po-Ling Loh and Maxim Raginsky, editors, Proceedings of Thirty Fifth Conference on Learning Theory, volume 178 of Proceedings of Machine Learning Research, pages 3635-3684. PMLR, 02-05 Jul 2022.   \n[26]  Brendan Mcmahan and Matthew Streeter. No-regret algorithms for unconstrained online convex optimization. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems, volume 25. Curran Associates, Inc., 2012.   \n[27]  H. Brendan McMahan and Francesco Orabona. Unconstrained online linear learning in Hilbert spaces: Minimax algorithms and normal approximations. In Maria Florina Balcan, Vitaly Feldman, andCsaba zepesvari ditorsProcedings ofThe 27thConfeencenLeing Theory,volume 35of Proceedings of Machine Learning Research, pages 1020-1039,Barcelona, Spain, 13-15 Jun 2014. PMLR.   \n[28]  Zakaria Mhammedi and Wouter M. Koolen. Lipschitz and comparator-norm adaptivity in online learning. In Conference on Learning Theory, pages 2858-2887. PMLR, 2020.   \n[29]  Arkadij Semenovic Nemirovskij and David Borisovich Yudin. Problem complexity and method efficiency in optimization. Wiley, New York, NY, USA, 1983.   \n[30] Ryan O'Donnelland Yu Zhao. Polynomial bounds for decoupling, with applications. arXiv preprint arXiv:1512.01603, 2015.   \n[31]  Francesco Orabona. A modern introduction to online learning. arXiv preprint arXiv:1912.13213, 2019. Version 6.   \n[32] Francesco Orabona and David Pal. Coin betting and parameter-free online learning. In Procedings of the 30th International Conference onNeural InformationProcessingSystems, NIPS' 16, page 577-585, Red Hook, NY, USA, 2016. Curran Associates Inc.   \n[33]  Francesco Orabona and David Pal. Scale-free online learning. Theoretical Computer Science, 716:50 - 69, 2018. ISSN 0304-3975. doi: https://doi.org/10.1016/j.tcs.2017.11.021. Special Issue on ALT 2015. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "[34]  R. Tyrrell Rockafellar. Convex Analysis. Princeton University Press, 1970. ", "page_idx": 12}, {"type": "text", "text": "[35]  Veeranjaneyulu Sadhanala, Yu-Xiang Wang, and Ryan J Tibshirani. Total variation classes beyond 1d: Minimax rates, and the limitations of linear smoothers. In Advances in Neural Information Processing Systems, volume 29, 2016.   \n[36]  Shai Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends in Machine Learning, 4(2), 2011.   \n[37]  Radomir S. Stankovic and Bogdan J. Falkowski. The Haar wavelet transform: its status and achievements. Computers & Electrical Engineering, 29(1):25-44, 2003. ISSN 0045-7906.   \n[38]  Willi-Hans Steeb and Tan Kiat Shi. Matrix calculus and Kronecker product with applications and $C{+{+}}$ programs. World Scientific, 1997.   \n[39]  Josef Stoer, Roland Bulirsch, R Bartels, Walter Gautschi, and Christoph Witzgall. Introduction to numerical analysis, volume 2. Springer, 1980.   \n[40]  Mathew Streeter and H Brendan McMahan. Less regret via online conditioning. arXiv preprint arXiv:1002.4862, 2010.   \n[41]  David F Walnut. An introduction to wavelet analysis. Springer Science & Business Media, 2013.   \n[42]  Manfred K Warmuth and Arun K Jagota. Continuous and discrete-time nonlinear gradient descent: Relative loss bounds and convergence. In Electronic proceedings of the 5th International Symposium on Artijficial Intelligence and Mathematics, 1997.   \n[43] Henry Wolkowicz and George PH Styan. Bounds for eigenvalues using traces. Linear algebra and its applications, 29:471-506, 1980.   \n[44]  Tianbao Yang, Lijun Zhang, Rong Jin, and Jinfeng Yi. Tracking slowly moving clairvoyant: Optimal dynamic regret of online learning with true and noisy gradient. In Maria Florina Balcan and Kilian Q.Weinberger, editors, Proceedings of The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research, pages 449-457, New York, New York, USA, 2016. PMLR.   \n[45] Lijun Zhang, Tianbao Yang, Jinfeng Yi, Rong Jin, and Zhi-Hua Zhou. Improved dynamic regret for non-degenerate functions. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.   \n[46] Lijun Zhang, Shiyin Lu, and Zhi-Hua Zhou. Adaptive online learning in dynamic environments. In Proceedingsofthe32nd InternationalConferenceonNeural InformationProcessingSystems, pages 1330-1340, 2018.   \n[47]  Zhiyu Zhang, Ashok Cutkosky, and Yannis Paschalidis. Unconstrained dynamic regret via sparse coding. Advances in Neural Information Processing Systems, 36, 2024.   \n[48] Zhiyu Zhang, Heng Yang, Ashok Cutkosky, and Ioannis C Paschalidis. Improving adaptive online learning usingrefned discretization.In InternationalConference onAlgorithmicLearning Theory, pages 1208-1233. PMLR, 2024.   \n[49]  Peng Zhao, Yan-Feng Xie, Lijun Zhang, and Zhi-Hua Zhou. Effcient methods for non-stationary online learning. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 11573-11585. Curran Associates, Inc., 2022.   \n[50] Peng Zhao, Yu-Jie Zhang, Lijun Zhang, and Zhi-Hua Zhou. Adaptivity and non-stationarity: Problem-dependent dynamic regret for online convex optimization. Journal of Machine Learning Research, 25(98):1-52, 2024.   \n[51]  Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. Im Proceedings of the 20th international conference on machine learning (icml-03), pages 928-936, 2003. ", "page_idx": 12}, {"type": "text", "text": "A  Proofs for Section 3 (Lower bounds for unconstrained dynamic regret) ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we provide proof of our main lower bound result from Section 3. We first introduce a technical tool from the literature on decoupling theory and a key lemma (Lemma 2). Proof of our main result is in Appendix A.1. ", "page_idx": 13}, {"type": "text", "text": "Consider a function $f:[-1,1]^{d}\\to\\mathbb{R}$ , defined as ", "page_idx": 13}, {"type": "equation", "text": "$$\nf(\\pmb{x})=\\sum_{i,j}A_{i,j}x_{i}x_{j}\\ ,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Define $\\pmb{A}$ the matrix with elements $A_{i,j}$ . In this section we will use the following notations for quantities related to a polynomial induced by the quadratic form ${\\pmb x}\\mapsto\\langle{\\pmb x},A{\\pmb x}\\rangle$ (see page 6 of O'Donnell and Zhao [30]) ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathrm{Var}[f]=\\sum_{i,j}A_{i,j}^{2}=\\|A\\|_{F}^{2},}\\\\ {\\displaystyle\\mathrm{Inf}_{i}[f]=\\sum_{j=1}^{d}(A_{i,j}^{2}+A_{j,i}^{2})\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "One of the key difficulties in deriving the lower bound is that squared weighted norms ${\\pmb x}\\mapsto\\langle{\\pmb x},A{\\pmb x}\\rangle$ introduce dependencies between the coordinates of $\\textbf{\\em x}$ , which breaks the usual lower bound arguments which rely on anti-concentration of independent Rademacher random variables. Instead, we must leverage an anti-concentration result that holds for polynomials of random variables. ", "page_idx": 13}, {"type": "text", "text": "Theorem 4 (Theorem 3 of Dinur et al. [10]). There is a universal constant $C$ such that the following holds. Suppose $G:\\left\\{\\pm1\\right\\}^{d}\\rightarrow\\mathbb{R}$ is a polynomial of degree at most 2 and assume $\\mathrm{Var}[g]=1$ Let $t\\geq1$ and suppose that $\\operatorname{Inf}_{i}[g]\\leq C^{-2}t^{-2}$ for all $i\\in[d]$ .Then ", "page_idx": 13}, {"type": "equation", "text": "$$\n{\\mathbb{P}}\\left\\{|g(x)|\\ge t\\right\\}\\ge\\exp\\left(-C^{2}t^{2}4\\log2\\right)~.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Using this anti-concentration result, the following key lemma provides a general lower bound on the wealth obtainable by any algorithm, subject to the weighting imposed by a matrix $\\pmb{A}$ ", "page_idx": 13}, {"type": "text", "text": "Lemma 2. Let $\\boldsymbol{\\mathcal{A}}$ be an online learning algorithm, and suppose $\\boldsymbol{\\mathcal{A}}$ guarantees $R_{T}(0)\\leq G\\epsilon_{T}$ for any sequence of linear losses $g_{1},\\dotsc,g_{T}\\in\\mathbb{R}$ satisfying $|g_{t}|\\leq G$ Let $\\b{A}\\in\\mathbb{R}^{T\\times T}$ be any symmetric positive definite matrix, and let $B=A-D i a g\\left(A\\right)$ .Then, there is a universal constant $C>0$ such that for any 1 \u2264q\u2264 C2maxi= B\u00b2 , there is a sequence of losses $g_{1},\\dotsc,g_{T}\\in\\mathbb{R}$ such that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\left\\|\\binom{g_{1}}{\\vdots}\\right\\|_{A}^{2}\\geq G^{2}\\left[\\mathrm{Tr}(A)+q\\left\\|A-D i a g\\left({\\pmb A}\\right)\\right\\|_{F}\\right]\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "and ", "page_idx": 13}, {"type": "equation", "text": "$$\nR_{T}(0)\\geq G\\epsilon_{T}\\left[1-2^{4C^{2}q^{2}}\\right]\\ .\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof. Let $Y_{1},\\ldots,Y_{T}$ be independent Rademacher random variables and set $g_{t}\\,=\\,G\\,Y_{t}$ , so that $\\begin{array}{r}{\\mathbb{E}\\left[R_{T}(0)\\right]=\\mathbb{E}\\left[\\sum_{t=1}^{T}g_{t}w_{t}\\right]=0}\\end{array}$ q tioning on any event $\\mathcal{E}$ with $\\mathbb{P}\\left\\{\\mathcal{E}\\right\\}>0$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{0=\\mathbb{E}\\left[R_{T}(0)\\right]}\\\\ &{\\quad=\\mathbb{E}\\left[R_{T}(0)\\Big|\\mathcal{E}\\right]\\mathbb{P}\\left\\{\\mathcal{E}\\right\\}+\\mathbb{E}\\left[R_{T}(0)\\Big|\\mathcal{E}^{c}\\right]\\mathbb{P}\\left\\{\\mathcal{E}^{c}\\right\\}}\\\\ &{\\quad\\le\\mathbb{E}\\left[R_{T}(0)|\\mathcal{E}\\right]\\mathbb{P}\\left\\{\\mathcal{E}\\right\\}+G\\,\\epsilon_{T}\\left(1-\\mathbb{P}\\left\\{\\mathcal{E}\\right\\}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where the last line uses the fact that $\\boldsymbol{\\mathcal{A}}$ guarantees $R_{T}(0)\\,\\leq\\,G\\epsilon_{T}$ for any $g_{1},\\dots,g_{T}$ satisfying $|g_{t}|\\leq G$ for all $t$ . Re-arranging, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[R_{T}(0)\\Big|\\mathscr{E}\\right]\\geq G\\epsilon_{T}\\left(1-\\frac{1}{\\mathbb{P}\\left\\{\\mathscr{E}\\right\\}}\\right)\\ .\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Next, let $\\widetilde{\\pmb{g}}_{t}=\\mathbf{e}_{t}\\otimes\\pmb{g}_{t}$ for all $t$ and consider the event ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{E}=\\left\\{\\left\\Vert\\sum_{t=1}^{T}\\widetilde{g}_{t}\\right\\Vert_{A}^{2}=\\left\\Vert(g_{1},\\ldots,g_{T})^{\\top}\\right\\Vert_{A}^{2}\\geq\\operatorname{Tr}(A)+q\\left\\Vert A-\\operatorname{Diag}{(A)}\\right\\Vert_{F}\\right\\}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "forsome $q>0$ . We proceed by lower bounding the probability of this event. Observe that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left\\|\\sum_{t=1}^{T}\\mathbf{\\widetilde{g}}_{t}\\right\\|_{A}^{2}=G^{2}\\sum_{i,j}Y_{i}Y_{j}A_{i j}=G^{2}\\left[\\mathrm{Tr}(A)+\\sum_{i,j\\neq i}Y_{i}Y_{j}A_{i j}\\right].\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Denote $B=A-\\operatorname{Diag}\\left(A\\right)$ and note that $\\begin{array}{r}{f(Y_{1},\\ldots,Y_{T})=\\sum_{i,j}Y_{i}Y_{j}B_{i j}}\\end{array}$ is a polynomial of degree at most 2 and variance $\\begin{array}{r}{\\mathrm{Var}[f]=\\sum_{i,j}B_{i j}^{2}=\\left\\|A-\\mathrm{Diag}\\left(A\\right)\\right\\|_{F}^{2}=\\left\\|B\\right\\|_{F}^{2}}\\end{array}$ . Moreover, since $\\pmb{A}$ is symmetric we have $\\begin{array}{r}{\\mathrm{Inf}_{i}[f]=\\sum_{j=1}^{T}B_{i j}^{2}+B_{j i}^{2}=2\\sum_{j=1}^{T}B_{i j}^{2}}\\end{array}$ for any $i$ It follows that if we let $\\begin{array}{r}{g(\\mathbf{Y})\\,=\\,\\frac{f(\\mathbf{Y})}{\\sqrt{\\|\\boldsymbol{B}\\|_{F}^{2}}}\\,=\\,\\frac{f(\\mathbf{Y})}{\\|\\boldsymbol{B}\\|_{F}}}\\end{array}$ $g$ $\\mathrm{Var}[g]\\,=\\,1$ $i\\in[T]$ wehave $\\begin{array}{r}{\\operatorname{Inf}_{i}[g]=\\frac{2\\sum_{j=1}^{T}B_{i j}^{2}}{\\|\\boldsymbol{B}\\|_{F}^{2}}}\\end{array}$ $C$ such hat for any 1 \u2264\u03b1\u2264 $\\begin{array}{r}{1\\leq q\\leq\\frac{\\|B\\|_{F}}{C\\sqrt{2\\operatorname*{max}_{i}\\sum_{j=1}^{T}B_{i j}^{2}}}}\\end{array}$ it holds that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left\\{f(\\pmb{Y})\\geq q\\,\\|\\pmb{B}\\|_{F}\\right\\}=\\mathbb{P}\\left\\{g(\\pmb{Y})\\geq q\\right\\}\\geq\\exp\\left(-4C^{2}q^{2}\\log2\\right)=2^{-4C^{2}q^{2}}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Now observe that $\\mathbb{P}\\left\\{{\\mathcal{E}}\\right\\}=\\mathbb{P}\\left\\{f(\\pmb{Y})\\geq q\\,\\|\\pmb{B}\\|_{F}\\right\\}$ by construction, so Equation (5) can be bound as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[R_{T}(0)\\Big|\\mathcal{E}\\right]\\geq G\\epsilon_{T}\\left(1-\\frac{1}{\\mathbb{P}\\left\\{\\mathcal{E}\\right\\}}\\right)=G\\epsilon_{T}\\left(1-2^{4C^{2}q^{2}}\\right),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which implies the existence of a sequence $g_{1},\\dotsc,g_{T}\\in\\mathbb{R}$ such that $R_{T}(0)\\geq G\\epsilon_{T}\\left[1-2^{4C^{2}q^{2}}\\right]$ and ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left\\|\\sum_{t=1}^{T}\\widetilde{\\pmb{g}}_{t}\\right\\|_{A}^{2}\\geq G^{2}\\left[\\operatorname{Tr}(A)+q\\left\\|B\\right\\|_{F}\\right]=G^{2}\\left[\\operatorname{Tr}(A)+q\\left\\|A-\\operatorname{Diag}\\left(A\\right)\\right\\|_{F}\\right],\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "for any $\\begin{array}{r}{1\\leq q\\leq\\frac{\\|B\\|_{F}}{C\\sqrt{2\\operatorname*{max}_{i}\\sum_{j=1}^{T}B_{i j}^{2}}}}\\end{array}$ ", "page_idx": 14}, {"type": "text", "text": "A.1 Proof of Theorem 1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section we prove our main lower bound. ", "page_idx": 14}, {"type": "text", "text": "Theorem 1. Let the number of rounds $T\\geq T_{0}$ ,where $T_{0}$ is a universal constant. Let $\\boldsymbol{\\mathcal{A}}$ be an online learning algorithm, and suppose $\\boldsymbol{\\mathcal{A}}$ guarantees $R_{T}(0)\\,\\leq\\,G\\epsilon_{T}$ for any sequence of linear losses $g_{1},\\dotsc,g_{T}\\in\\mathbb{R}$ satisfying $|g_{t}|\\leq G$ Let $\\boldsymbol{M}^{-1}\\in\\mathbb{R}^{T\\times T}$ be any symmetric positive definite matrix, denote $\\widetilde{M}^{-1}:=M^{-1}-D i a g\\left(M^{-1}\\right)$ and $V_{T}:=\\mathrm{Tr}(M^{-1})+\\|\\widetilde{M}^{-1}\\|_{F}$ Suppose that M2 \u2265maxi (M1). Then, for any P satisfying To \u2264 log2 e PV < T, there is a sequence of losses $g_{1},\\dotsc,g_{T}\\in\\mathbb{R}.$ and $\\widetilde{\\pmb{u}}=(u_{1},\\dots,u_{T})^{\\top}\\in\\mathbb{R}^{T}$ satisfying $\\left\\|\\widetilde{\\boldsymbol{u}}\\right\\|_{M}=\\sqrt{P}$ such that we have ", "page_idx": 14}, {"type": "equation", "text": "$$\nR_{T}(u_{1},\\dots,u_{T})\\ge\\Omega\\left(G\\epsilon_{T}+G\\sqrt{P\\left[\\mathrm{Tr}(M^{-1})+\\left\\lVert\\widetilde{\\boldsymbol{M}}^{-1}\\right\\rVert_{F}\\log_{2}^{\\frac{1}{2}}\\frac{\\sqrt{P V_{T}}}{2\\epsilon_{T}}\\right]}\\right)\\ .\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. Denote $A=M^{-1}$ and $B=A-\\operatorname{Diag}\\left(A\\right)$ . By Lemma 2, there is a universal constant $C$ and a sequence $g_{1},\\dotsc,g_{T}\\in\\mathbb{R}$ such that for any $\\begin{array}{r}{1\\leq q\\leq\\frac{\\|B\\|_{F}}{C\\sqrt{2\\operatorname*{max}_{i}\\sum_{j=1}^{T}B_{i j}^{2}}}}\\end{array}$ , it holds that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left\\lVert\\sum_{t=1}^{T}\\widetilde{\\pmb{g}}_{t}\\right\\rVert_{A}^{2}\\ge G^{2}\\left[\\mathrm{Tr}(\\pmb{A})+q\\left\\lVert\\pmb{A}-\\mathrm{Diag}\\left(\\pmb{A}\\right)\\right\\rVert_{F}\\right]\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and ", "page_idx": 15}, {"type": "equation", "text": "$$\nR_{T}(0)\\geq G\\epsilon_{T}\\left[1-2^{4C^{2}q^{2}}\\right]\\ .\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Hence, choosing comparator sequence $u_{1},\\dotsc,u_{T}\\ \\in\\ \\mathbb{R}$ to satisfy and $\\widetilde{\\pmb u}\\;=\\;\\left(u_{1},\\ldots,u_{T}\\right)^{\\top}\\;=$ $-\\sqrt{P}\\frac{\\pmb{A}\\sum_{t=1}^{T}\\widetilde{\\pmb{g}}_{t}}{\\left\\|\\sum_{t=1}^{T}\\widetilde{\\pmb{g}}_{t}\\right\\|_{\\pmb{A}}}\\in\\mathbb{R}^{T}$ iw haeve $\\|\\widetilde{\\boldsymbol{u}}\\|_{A^{-1}}=\\|\\widetilde{\\boldsymbol{u}}\\|_{M}=\\sqrt{P}$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{R_{T}(u_{1},\\dots,u_{T})=R_{T}(0)-\\left\\langle\\sum_{t=1}^{T}\\widetilde{g}_{t},\\widetilde{u}\\right\\rangle}\\quad}&{}\\\\ &{=G\\sqrt{P}\\left\\|\\displaystyle\\sum_{t=1}^{T}\\widetilde{g}_{t}\\right\\|_{A}+R_{T}(0)}\\\\ &{\\ge G\\sqrt{P[\\mathrm{Tr}(A)+q\\left\\|A-\\mathrm{Diag}\\left(A\\right)\\right\\|_{F}]}+R_{T}(0)}\\\\ &{\\ge G\\epsilon_{T}+G\\sqrt{P\\left[\\mathrm{Tr}(A)+q\\left\\|A-\\mathrm{Diag}\\left(A\\right)\\right\\|_{F}\\right]}-G\\epsilon_{T}2^{4C^{2}q^{2}}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Now, for $P$ satisfying $\\begin{array}{r}{T_{0}:=4C^{2}\\leq\\log_{2}\\left(\\frac{\\sqrt{P\\left[\\operatorname{Tr}(A)+\\|B\\|_{F}\\right]}}{2\\epsilon_{T}}\\right)\\leq T}\\end{array}$ we may choose ", "page_idx": 15}, {"type": "equation", "text": "$$\nq=\\sqrt{\\frac{\\log_{2}\\left(\\frac{\\sqrt{P[\\mathrm{Tr}(\\boldsymbol{A})+\\lVert\\boldsymbol{B}\\rVert_{F}]}}{2\\epsilon_{T}}\\right)}{4C^{2}}}\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Indeed, observe that this choice satisfies $\\begin{array}{r}{1\\leq q\\leq\\frac{\\|B\\|_{F}}{C\\sqrt{2\\operatorname*{max}_{i}\\sum_{j=1}^{T}B_{i j}^{2}}}}\\end{array}$ as required: ", "page_idx": 15}, {"type": "equation", "text": "$$\n1\\leq q=\\sqrt{\\frac{\\log_{2}\\left(\\frac{\\sqrt{P[\\mathrm{Tr}(A)+\\|B\\|_{F}]}}{2\\epsilon_{T}}\\right)}{4C^{2}}}\\leq\\sqrt{\\frac{T}{4C^{2}}}\\leq\\frac{\\|B\\|_{F}}{C\\sqrt{2\\operatorname*{max}_{i}\\sum_{j=1}^{T}B_{i j}^{2}}},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the fnal inequality uses the assumption $\\begin{array}{r}{\\left\\|\\boldsymbol{B}\\right\\|_{F}^{2}/2\\operatorname*{max}_{i}\\sum_{i j}B_{i j}^{2}\\geq\\frac{T}{4}}\\end{array}$ Hence, we have that ", "page_idx": 15}, {"type": "equation", "text": "$$\nG\\epsilon_{T}2^{4C^{2}q^{2}}\\leq\\frac{G}{2}\\sqrt{P\\left[\\mathrm{Tr}(\\pmb{A})+\\|\\pmb{B}\\|_{F}\\right]}\\leq\\frac{G}{2}\\sqrt{P\\left[\\mathrm{Tr}(\\pmb{A})+q\\left\\|\\pmb{B}\\right\\|_{F}\\right]},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "so that the overall the regret can be lower-bounded as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R_{T}(u_{1},\\ldots,u_{T})\\geq G\\epsilon_{T}+\\displaystyle\\frac{1}{2}G\\sqrt{P\\left[\\mathrm{Tr}(A)+q\\left\\|{\\pmb B}\\right\\|_{F}\\right]}}\\\\ &{\\qquad\\qquad\\qquad=G\\epsilon_{T}+\\displaystyle\\frac{G}{2}\\sqrt{P\\left[\\mathrm{Tr}(A)+\\|{\\pmb B}\\|_{F}\\,\\frac{\\log^{\\frac{1}{2}}\\,\\left(\\sqrt{P[\\mathrm{Tr}(A)+\\|{\\pmb B}\\|_{F}]}/2\\epsilon_{T}\\right)}{\\sqrt{T_{0}}}\\right]}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "A.2  Proof of Proposition 2 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proposition 2. (Adapting to Squared Path-length Requires Superlinear Regret) Define the finitedifferenceoperator $\\bar{\\Sigma}\\in\\bar{\\mathbb{R}}^{T}$ asthematrixwithentries ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\Sigma_{i j}={\\left\\{\\begin{array}{l l}{1}&{i f\\,i=j}\\\\ {-1}&{i f\\,i=j-1}\\\\ {0}&{o t h e r w i s e}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Let $\\mathbf{S}=\\pmb{\\Sigma}^{\\top}\\pmb{\\Sigma}$ and $M=\\mathbf{S}\\otimes I_{d}$ .Then, $_M$ satisfes the assumptions of Theorem 1l and ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\|\\widetilde{\\boldsymbol{u}}\\|_{M}^{2}=\\|\\boldsymbol{u}_{T}\\|_{2}^{2}+\\sum_{t=1}^{T-1}\\left\\|\\boldsymbol{u}_{t}-\\boldsymbol{u}_{t+1}\\right\\|_{2}^{2}\\qquad\\boldsymbol{a}\\boldsymbol{n}d\\qquad\\mathrm{Tr}\\left(\\boldsymbol{M}^{-1}\\right)=\\frac{T(T+1)}{2}\\;.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof.  We  first  show  the  properties  that $\\begin{array}{r l r}{\\|\\widetilde{\\pmb{u}}\\|_{F}^{2}}&{=}&{\\|\\pmb{u}_{T}\\|_{2}^{2}\\;+\\;\\sum_{t=1}^{T-1}\\|\\pmb{u}_{t}-\\pmb{u}_{t+1}\\|_{2}^{2}}\\end{array}$ and $\\begin{array}{r}{\\mathrm{Tr}\\big(\\Sigma^{-1}\\Sigma^{-\\top}\\big)=\\sum_{t=1}^{T}\\Big[\\Sigma^{-1}\\Sigma^{-\\top}\\Big]_{t t}=\\sum_{t=1}^{T}T-t+1=\\frac{T(T+1)}{2}}\\end{array}$ and show that $_M$ saisiesthe conditions of Theorem 1 at the end. ", "page_idx": 16}, {"type": "text", "text": "Observe that ", "page_idx": 16}, {"type": "equation", "text": "$$\n(\\Sigma\\otimes I_{d})\\widetilde{\\boldsymbol{u}}=\\left(\\begin{array}{l l l l l}{I_{d}}&{-I_{d}}&{\\textbf{0}}&{\\textbf{0}}&{...}\\\\ {\\textbf{0}}&{I_{d}}&{-I_{d}}&{\\textbf{0}}&{...}\\\\ {\\bf{0}}&{\\textbf{0}}&{I_{d}}&{-I_{d}}&{...}\\\\ {\\vdots}&{}&{}&{\\ddots.}&{}\\\\ {\\bf{0}}&{\\textbf{0}}&{\\textbf{0}}&{...}&{I_{d}}\\end{array}\\right)\\left(\\begin{array}{l}{u_{1}}\\\\ {\\vdots}\\\\ {u_{T}}\\end{array}\\right)=\\left(\\begin{array}{l}{u_{1}-u_{2}}\\\\ {u_{2}-u_{3}}\\\\ {\\vdots}\\\\ {u_{T-1}-u_{T}}\\\\ {u_{T}}\\end{array}\\right),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and since $(\\Sigma^{\\top}\\otimes I_{d})(\\Sigma\\otimes I_{d})=(\\Sigma^{\\top}\\Sigma)\\otimes I_{d}=M$ we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\|\\widetilde{\\boldsymbol{u}}\\|_{M}^{2}=\\left\\langle\\widetilde{\\boldsymbol{u}},(\\boldsymbol{\\Sigma}^{\\top}\\otimes\\boldsymbol{I}_{d})(\\boldsymbol{\\Sigma}\\otimes\\boldsymbol{I}_{d})\\widetilde{\\boldsymbol{u}}\\right\\rangle=\\left\\langle(\\boldsymbol{\\Sigma}\\otimes\\boldsymbol{I}_{d})\\widetilde{\\boldsymbol{u}},(\\boldsymbol{\\Sigma}\\otimes\\boldsymbol{I}_{d})\\widetilde{\\boldsymbol{u}}\\right\\rangle}}\\\\ &{}&{=\\|\\boldsymbol{u}_{T}\\|_{2}^{2}+\\displaystyle\\sum_{t=1}^{T-1}\\|\\boldsymbol{u}_{t}-\\boldsymbol{u}_{t+1}\\|_{2}^{2}\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Using the inverse property of the Kronecker product, we also have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\boldsymbol{M}^{-1}=\\left[\\boldsymbol{\\Sigma}^{\\top}\\boldsymbol{\\Sigma}\\otimes\\boldsymbol{I}_{d}\\right]^{-1}=\\left[\\boldsymbol{\\Sigma}^{\\top}\\boldsymbol{\\Sigma}\\right]^{-1}\\otimes\\boldsymbol{I}_{d}=\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\Sigma}^{-\\top}\\otimes\\boldsymbol{I}_{d},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and by Lemma 8 we have that $\\Sigma^{-1}$ is the upper-triangular matrix of all 1's, that is, the matrix with entries ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\Sigma_{i j}^{-1}={\\binom{1}{0}}_{\\mathrm{~\\mathrm{~otherwise}}}^{\\mathrm{~-~}\\mathrm{~if~}i}\\,,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and likewise, $\\pmb{\\Sigma}^{-\\top}$ is a lower-triangular matrix of $1^{\\prime}s$ . In other words, for any $t$ we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left[\\Sigma^{-1}\\Sigma^{-\\top}\\right]_{t t}=\\sum_{i=1}^{T}\\Sigma_{t i}^{-1}\\Sigma_{i t}^{-\\top}=\\sum_{i\\leq t}\\Sigma_{t i}^{-1}=T-t+1\\;.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "So, summing over $t$ we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname{Tr}(\\Sigma^{-1}\\Sigma^{-\\top})=\\sum_{t=1}^{T}\\left[\\Sigma^{-1}\\Sigma^{-\\top}\\right]_{t t}=\\sum_{t=1}^{T}T-t+1={\\frac{T(T+1)}{2}}\\;.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Now we show that $_M$ satisfies the conditions of Theorem 1. $M\\,=\\,\\mathbf{S}\\otimes I_{d}\\,=\\,[\\pmb{\\Sigma}^{\\top}\\pmb{\\Sigma}]\\otimes I_{d}$ is clearly symmetric since it is the Kronecker product of two symmetric matrices. Observe that for any $\\mathbf{\\Delta}\\dot{\\mathbf{x}}\\neq\\mathbf{0}\\in\\mathbb{R}^{T}$ we have $\\ensuremath{\\Sigma}x\\neq0$ by positive definiteness of $\\Sigma$ (Lemma 8) and thus $\\langle\\pmb{x},\\mathbf{S}\\pmb{x}\\rangle=$ $\\langle\\Sigma\\pmb{x},\\Sigma\\pmb{x}\\rangle>0$ . Thus, $M=\\mathbf{S}\\otimes I_{d}$ is the Kronecker product of symmetric positive definite matrices, $_M$ is symmetric positive definite [see, e.g., 38, Chapter 2]. ", "page_idx": 16}, {"type": "text", "text": "Lastly, let $B=\\Sigma^{-1}\\Sigma^{-\\top}-\\mathrm{Diag}\\left(\\Sigma^{-1}\\Sigma^{-\\top}\\right)$ . We are to show that $\\begin{array}{r}{\\|B\\|_{F}\\,\\ge\\,\\frac{T}{2}\\sum_{j}B_{i j}^{2}}\\end{array}$ for any $i$ First observe that calculation of $[\\pmb{\\Sigma}^{-1}\\pmb{\\Sigma}^{-\\top}]_{t t}$ is generalized to ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left[\\Sigma^{-1}\\Sigma^{-\\top}\\right]_{i j}=\\sum_{k=1}^{T}\\Sigma_{i k}^{-1}\\Sigma_{k j}^{-\\top}=\\sum_{k=1}^{T}\\Sigma_{k i}^{-\\top}\\Sigma_{k j}^{-\\top}=\\sum_{k=1}^{j\\wedge i}1=T-\\operatorname*{max}\\left\\{j,i\\right\\}+1,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "for any $i,j$ , and likewise $B_{i j}=T-\\operatorname*{max}\\left\\{j,i\\right\\}+1$ for $i\\neq j$ and O otherwise, from which it is easily seen that $\\begin{array}{r}{\\operatorname*{max}_{i}\\sum_{j}B_{i j}^{2}=\\dot{\\sum}_{j}\\,B_{1j}^{2}}\\end{array}$ , so for any $i$ we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{j}B_{i j}^{2}\\leq\\sum_{j}B_{1j}^{2}=\\sum_{j=2}^{T}(T-j+1)^{2}=\\frac{1}{6}T(2T^{2}-3T+1)\\;.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "On the other hand, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|B\\|_{F}^{2}=\\sum_{i}\\sum_{j}B_{i j}^{2}=\\frac{1}{6}T^{2}(T^{2}-1)}\\\\ {\\displaystyle\\qquad=\\frac{T}{2}\\frac{T}{6}(2T^{2}-2)=\\frac{T}{2}\\frac{T}{6}(2T^{2}-3T+3T-2)\\geq\\frac{T}{2}\\frac{T}{6}(2T^{2}-3T+1)}\\\\ {\\displaystyle\\qquad\\geq\\frac{T}{2}\\sum_{j}B_{i j}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for any $i$ , where the last line applies the inequality in the previous display. ", "page_idx": 17}, {"type": "text", "text": "A.3 Sufficiency of Proposition 2 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Thechoiceof $_M$ in Proposition 2 uniquely exposes the squared path-length up to the constant offset term $\\left\\Vert u_{T}\\right\\Vert^{2}$ . In this section we demonstrate that the choice of offset term in Proposition 2 does not make any significant difference for the claim that adapting to the squared path-length requires incurring a $\\mathrm{{Tr}}(\\bar{M}^{-1})\\geq\\Omega(T^{2})$ penalty, and hence that Proposition 2 is sufficient to demonstrate that adapting to the squared path-length is not possible without incurring vacuous regret. ", "page_idx": 17}, {"type": "text", "text": "To expedite the discussion, we first introduce two technical lemmas, proven in Appendices A.3.1 and A.3.2 respectively. ", "page_idx": 17}, {"type": "text", "text": "Lemma 3. Let $\\pmb{v}\\in\\mathbb{R}^{T}$ be an arbitrary non-zero vector and let $B\\in\\mathbb{R}^{T\\times T}$ be a symmetric matrix witheigenvalues $0=\\lambda_{1}(B)<\\lambda_{2}(B)\\leq.\\ldots\\leq\\lambda_{T}(B)$ Then ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{Tr}((B+v\\pmb{v}^{\\top})^{-1})\\geq\\left\\|\\pmb{v}\\right\\|^{2}+\\sum_{t=2}^{T}\\frac{1}{\\lambda_{t}(\\pmb{B})}\\;.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Lemma 4. Let $\\boldsymbol{\\Sigma}\\,\\in\\,\\mathbb{R}^{T\\times T}$ denote the fnite-diffeencematrix defnd inProposition2 and lt $M=\\Sigma^{\\top}\\Sigma$ Then, for any $T>1$ wehave ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\lambda_{\\operatorname*{max}}(M^{-1})\\leq\\frac{9}{10}\\operatorname{Tr}(M^{-1}),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\lambda_{\\operatorname*{max}}(M^{-1})$ is the maximal eigenvalue of $M^{-1}$ ", "page_idx": 17}, {"type": "text", "text": "Now, Consider the 1-dimensional setting and note that for any positive definite $_M$ we can find a unique $\\Sigma$ such that $M=\\Sigma^{\\top}\\Sigma$ .Hence, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\widetilde{\\boldsymbol{u}}\\right\\|_{M}^{2}=\\left\\langle\\widetilde{\\boldsymbol{u}},M\\widetilde{\\boldsymbol{u}}\\right\\rangle=\\left\\langle\\Sigma\\widetilde{\\boldsymbol{u}},\\Sigma\\widetilde{\\boldsymbol{u}}\\right\\rangle,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "so without loss of generality we can focus on $\\Sigma$ for which ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\langle\\Sigma\\widetilde{\\boldsymbol{u}},\\Sigma\\widetilde{\\boldsymbol{u}}\\rangle=\\langle\\boldsymbol{v},\\widetilde{\\boldsymbol{u}}\\rangle^{2}+\\sum_{t=2}^{T}\\|u_{t}-u_{t-1}\\|^{2},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\pmb{v}\\neq\\mathbf{0}\\in\\mathbb{R}^{T}$ . 4 Note such a constant offset term is unavoidable: it is what captures the static regret guarantee in the case where $u_{1}=...=u_{T}=u$ . Proposition 2 considers $\\pmb{v}=(0,\\ldots,0,1)$ to get $\\begin{array}{r}{\\|\\widetilde{\\boldsymbol{u}}\\|_{M}^{2}=\\|\\boldsymbol{u}_{T}\\|^{2}+\\sum_{t=2}^{T}\\|\\boldsymbol{u}_{t}-\\boldsymbol{u}_{t-1}\\|^{2}}\\end{array}$ though belowwe wil show that any vector $\\pmb{v}$ would still lead to $\\operatorname{Tr}(M^{-1})=\\Omega(T^{2})$ ", "page_idx": 17}, {"type": "text", "text": "It is clear that the only way to construct expressions of the form above is via matrices $\\Sigma$ satisfying ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\Sigma\\widetilde{\\boldsymbol{u}}=c\\left(\\begin{array}{c}{u_{1}-u_{2}}\\\\ {u_{2}-u_{3}}\\\\ {\\vdots}\\\\ {u_{T-1}-u_{T}}\\\\ {\\langle\\boldsymbol{v},\\tilde{\\boldsymbol{u}}\\rangle}\\end{array}\\right),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $c\\in\\{-1,1\\}$ and the order of the rows indices of the vector can be permuted without loss of generality. In particular, the only matrices that can produce these expressions (again noting that the rows can be permuted without loss of generality) are of the form ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\Sigma=c\\left(\\begin{array}{c c c c c c c}{1}&{-1}&{0}&{0}&{\\dots}&{0}&{0}\\\\ {0}&{1}&{-1}&{0}&{\\dots}&{0}&{0}\\\\ {0}&{0}&{1}&{-1}&{\\dots}&{0}&{0}\\\\ {\\vdots}&&&{\\ddots}&&&\\\\ {0}&{0}&{0}&{0}&{\\dots}&{1}&{-1}\\\\ {v_{1}}&{v_{2}}&{v_{3}}&{v_{4}}&{\\dots}&{v_{T-1}}&{v_{T}}\\end{array}\\right)=:c\\left(\\begin{array}{c}{\\Delta}\\\\ {\\vdots}\\\\ {v^{\\top}}\\end{array}\\right),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "$M\\,=\\,\\Sigma^{\\top}\\Sigma\\,=\\,\\Delta^{\\top}\\Delta+v v^{\\top}$ .Moreover, $\\Delta^{\\top}\\Delta$ is a symmetric matrix with a unique zero eigenvalue (corresponding to vectors in the span of $\\mathbf{1}=(1,\\bar{\\ldots},1)\\in\\mathbb{R}^{T})$ , so applying Lemma 3, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathrm{Tr}(\\boldsymbol{M}^{-1})=\\mathrm{Tr}((\\Delta^{\\top}\\Delta+v\\boldsymbol{v}^{\\top})^{-1})\\geq\\left\\|\\boldsymbol{v}\\right\\|^{2}+\\sum_{t=2}^{T}\\frac{1}{\\lambda_{t}(\\Delta^{\\top}\\Delta)}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Now,define $\\pmb{v}_{0}\\,=\\,(0,\\ldots,0,1)\\,\\in\\,\\mathbb{R}^{T}$ and observe that $M_{0}\\,:=\\,\\pmb{\\Delta}^{\\top}\\pmb{\\Delta}+\\pmb{v}_{0}\\pmb{v}_{0}^{\\top}$ is precisely the matrix studied in Proposition 2. We have via the interlacing property of rank-1 updates to symmetric matrices that $\\lambda_{t}(\\Delta^{\\top}\\Delta)\\leq\\lambda_{t}(M_{0})$ [13, Theorem 8.1.8], so overall we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathrm{Tr}(\\boldsymbol{M}^{-1})\\geq\\sum_{t=2}^{T}\\frac{1}{\\lambda_{t}(M_{0})}=\\sum_{t=2}^{T}\\lambda_{t}(\\boldsymbol{M}_{0}^{-1})}\\\\ {\\displaystyle=\\mathrm{Tr}(\\boldsymbol{M}_{0}^{-1})-\\lambda_{\\operatorname*{max}}(\\boldsymbol{M}_{0}^{-1})}\\\\ {\\displaystyle\\geq\\mathrm{Tr}(\\boldsymbol{M}_{0}^{-1})-\\frac{9}{10}\\,\\mathrm{Tr}(\\boldsymbol{M}_{0}^{-1})}\\\\ {\\displaystyle=\\frac{1}{10}\\,\\mathrm{Tr}(\\boldsymbol{M}_{0}^{-1})=\\frac{1}{10}\\frac{T(T+1)}{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the last inequality applies Lemma 4 to bound $\\lambda_{\\operatorname*{max}}(M_{0}^{-1})$ and recalls $\\begin{array}{r}{\\mathrm{Tr}(M_{0}^{-1})=\\frac{T(T+1)}{2}}\\end{array}$ from Proposition 2. ", "page_idx": 18}, {"type": "text", "text": "Hence, the variance penalty will still be $\\Omega(T^{2})$ regardless of the choice of bias $\\left\\langle v,\\widetilde{u}\\right\\rangle^{2}$ in the variability measure. Combined with our lower bound in Theorem 1, it follows that adapting to the squared path-length necessarily implies a variance penalty of $\\operatorname{Tr}(M^{-1})\\,\\geq\\,\\Omega(T^{2})$ , leading to a vacuous regret upper bound. ", "page_idx": 18}, {"type": "text", "text": "A.3.1 Proof of Lemma 3 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Lemma 3. Let $\\pmb{v}\\in\\mathbb{R}^{T}$ be an arbitrary non-zero vector and let $B\\in\\mathbb{R}^{T\\times T}$ be a symmetric matrix with eigenvalues $0=\\lambda_{1}(B)<\\lambda_{2}(B)\\leq.\\ldots\\leq\\lambda_{T}(B)$ Then ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathrm{Tr}((B+v\\pmb{v}^{\\top})^{-1})\\geq\\left\\|\\pmb{v}\\right\\|^{2}+\\sum_{t=2}^{T}\\frac{1}{\\lambda_{t}(\\pmb{B})}\\;.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. Let $\\pmb{A}:=\\pmb{B}+\\pmb{v}\\pmb{v}^{\\top}$ . Since $_B$ is symmetric, we have via the interlacing property that there is an $a_{1},\\dots,a_{T}\\geq0$ such that $\\textstyle\\sum_{t=1}^{T}a_{t}=\\|\\pmb{v}\\|^{2}$ and $\\lambda_{t}(A)=\\lambda_{t}(B)+a_{i}$ [see, e.g., Theorem 8.1.8 in 13]. Hence, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Tr}((B+v v^{\\top})^{-1})=\\mathrm{Tr}(A^{-1})=\\displaystyle\\sum_{t=1}^{T}\\lambda_{t}(A^{-1})=\\displaystyle\\sum_{t=1}^{T}\\frac{1}{\\lambda_{t}(A)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\sum_{t=1}^{T}\\frac{1}{\\lambda_{t}(B)+a_{i}}\\geq\\operatorname*{min}_{\\scriptstyle\\underset{t=1}{a_{1},\\ldots,a_{T}\\geq0}}\\sum_{t=1}^{T}\\frac{1}{\\lambda_{t}(B)+a_{i}}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "To analyze the constrained optimization in the last line, let $\\alpha_{1},\\dotsc,\\alpha_{T}\\geq0,\\beta\\in\\mathbb{R}.$ and define the Lagrangian ", "page_idx": 19}, {"type": "equation", "text": "$$\nL(a_{1},\\ldots,a_{T},\\alpha_{1},\\ldots,\\alpha_{T},\\beta)=\\sum_{t=1}^{T}\\frac{1}{\\lambda_{t}(\\pmb{B})+a_{t}}-\\sum_{t=1}^{T}\\alpha_{t}a_{t}+\\beta\\left(\\sum_{t=1}^{T}a_{t}-\\|\\pmb{v}\\|^{2}\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For any $t$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{\\partial L}{\\partial a_{t}}=\\frac{-1}{(\\lambda_{t}({\\pmb B})+a_{t})^{2}}-\\alpha_{t}+\\beta=0\\iff a_{t}=\\frac{1}{\\sqrt{\\beta-\\alpha_{t}}}-\\lambda_{t}({\\pmb B})\\;.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Plugging this into the dual $\\begin{array}{r}{D(\\alpha_{1},\\ldots,\\alpha_{T},\\beta)=\\operatorname*{min}_{a_{1},\\ldots,a_{T}}L(a_{1},\\ldots,a_{T},\\alpha_{1},\\ldots,\\alpha_{T},\\beta)}\\end{array}$ we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{{\\displaystyle\\mathsf{\\xi}}}&{{}\\displaystyle\\sum_{t=1}^{T}\\sqrt{\\beta-\\alpha_{t}}-\\sum_{t=1}^{T}\\alpha_{t}\\left(\\frac{1}{\\sqrt{\\beta-\\alpha_{t}}}-\\lambda_{t}(B)\\right)+\\beta\\left(\\sum_{t=1}^{T}\\frac{1}{\\sqrt{\\beta-\\alpha_{t}}}-\\lambda_{t}(B)-\\frac{1}{\\sqrt{\\beta-\\alpha_{t}}}\\right)}\\\\ {{}}&{{}\\displaystyle=\\sum_{t=1}^{T}\\sqrt{\\beta-\\alpha_{t}}+\\sum_{t=1}^{T}(\\beta-\\alpha_{t})\\frac{1}{\\sqrt{\\beta-\\alpha_{t}}}+\\sum_{t=1}^{T}(\\alpha_{t}-\\beta)\\lambda_{t}(B)-\\beta\\left\\|{\\pmb{v}}\\right\\|^{2}}\\\\ {{}}&{{}={\\displaystyle2\\sum_{t=1}^{T}\\sqrt{\\beta-\\alpha_{t}}}+\\sum_{t=1}^{T}(\\alpha_{t}-\\beta)\\lambda_{t}(B)-\\beta\\left\\|{\\pmb{v}}\\right\\|^{2}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The derivatives of the dual $w.r.t~\\alpha_{t}$ are ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{\\partial D}{\\partial\\alpha_{t}}=\\frac{-1}{\\sqrt{\\beta-\\alpha_{t}}}+\\lambda_{t}(B)\\;.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Observe that for \u51651(B) = 0, we have \u00b1 $\\begin{array}{r}{\\frac{\\partial D}{\\partial\\alpha_{t}}\\,=\\,-\\frac{1}{\\sqrt{\\beta-\\alpha_{t}}}\\,\\leq\\,0}\\end{array}$ $D$ is decreasing in $\\alpha_{1}$ , so the dual is maximized when Q1 = 0. Using the relation a1 = $\\begin{array}{r}{a_{1}\\ =\\ \\frac{1}{\\sqrt{\\beta-\\alpha_{1}}}\\,-\\,\\lambda_{1}(B)}\\end{array}$ above we have $\\begin{array}{r}{a_{1}=\\frac{1}{\\sqrt{\\beta-\\alpha_{1}}}-\\lambda_{1}(B)=\\frac{1}{\\sqrt{\\beta}}}\\end{array}$ . Equating the other derivatives for $t>1$ to zero we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\frac{1}{\\sqrt{\\beta-\\alpha_{t}}}=\\lambda_{t}({\\pmb B})\\implies\\lambda_{t}({\\pmb B})+a_{t}=\\lambda_{t}({\\pmb B})}}\\\\ {{\\implies a_{t}=0~~~\\forall t>1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where we used the relationship at = V3-\u03b1t $\\begin{array}{r}{a_{t}=\\frac{1}{\\sqrt{\\beta-\\alpha_{t}}}-\\lambda_{t}(B)}\\end{array}$ from above. Finll, the optimal $\\beta$ is such that $\\begin{array}{r}{\\sum_{t=1}^{T}a_{t}=\\frac{1}{\\sqrt{\\beta}}=\\left\\|\\pmb{v}\\right\\|^{2}}\\end{array}$ sooveral wehaeve ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\operatorname*{min}_{\\pmb{n}_{1},\\ldots,\\pmb{n}_{T}\\geq0}\\displaystyle\\sum_{t=1}^{T}\\frac{1}{\\pmb{n}(\\pmb{B})+a_{i}}=\\frac{1}{\\sqrt{\\beta-\\alpha_{1}}}+\\sum_{t=2}^{T}\\frac{1}{\\lambda_{t}(\\pmb{B})+a_{i}}=\\frac{1}{\\sqrt{\\beta}}+\\sum_{t=2}^{T}\\frac{1}{\\lambda_{t}(\\pmb{B})}}\\\\ {\\displaystyle=\\|\\pmb{v}\\|^{2}+\\sum_{t=2}^{T}\\frac{1}{\\lambda_{t}(\\pmb{B})}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "A.3.2 Proof of Lemma 4 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Lemma 4. Let $\\boldsymbol{\\Sigma}\\,\\in\\,\\mathbb{R}^{T\\times T}$ denote the finite-difference matrix defined in Proposition 2 and let $M=\\Sigma^{\\top}\\Sigma$ Then, for any $T>1$ wehave ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\lambda_{\\operatorname*{max}}(M^{-1})\\leq\\frac{9}{10}\\operatorname{Tr}(M^{-1}),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\lambda_{\\operatorname*{max}}(M^{-1})$ is the maximal eigenvalue of $M^{-1}$ ", "page_idx": 19}, {"type": "text", "text": "Proof. The matrix $\\boldsymbol{M}^{-1}=\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\Sigma}^{-\\top}$ is symmetric and positive definite, hence has real eigenvalues. The eigenvalues of $M^{-1}$ can be bound in terms of its trace as follows (see, e.g., Theorem 2.1 Wolkowicz and Styan [43], provided for convenience in Theorem 5): ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\lambda_{\\operatorname*{max}}\\left(M^{-1}\\right)\\leq\\frac{\\operatorname{Tr}\\left(M^{-1}\\right)}{T}+\\sqrt{\\left(T-1\\right)\\left[\\frac{\\operatorname{Tr}\\left(M^{-\\top}M^{-1}\\right)}{T}-\\left(\\frac{\\operatorname{Tr}\\left(M^{-1}\\right)}{T}\\right)^{2}\\right]}\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Next, observe that by Lemma 8, matrix $\\Sigma^{-1}$ is an upper-triangular matrix of all 1's, so that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{[M^{-1}]_{i j}=\\left[\\Sigma^{-1}\\Sigma^{-\\top}\\right]_{i j}=\\displaystyle\\sum_{k\\in[t]}\\Sigma_{i k}^{-1}\\Sigma_{k j}^{-\\top}=\\displaystyle\\sum_{k\\in[T]}\\Sigma_{i k}^{-1}\\Sigma_{j k}^{-1}}\\\\ {=\\displaystyle\\sum_{k\\in[T]}\\mathbf{1}\\left\\{k\\geq i\\right\\}\\mathbf{1}\\left\\{k\\geq j\\right\\}=T-\\operatorname*{max}\\left\\{i,j\\right\\}+1,}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Hence, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname{Tr}\\left(M^{-1}\\right)=\\sum_{t=1}^{T}[M^{-1}]_{i i}=\\sum_{t=1}^{T}(T-t+1)=\\sum_{t=1}^{T}t={\\frac{T(T+1)}{2}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Moreover, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Tr}\\left(\\boldsymbol{M}^{-\\top}\\boldsymbol{M}^{-1}\\right)=\\displaystyle\\sum_{t=1}^{T}[\\boldsymbol{M}^{-\\top}\\boldsymbol{M}^{-1}]_{t t}=\\displaystyle\\sum_{t=1}^{T}\\sum_{k=1}^{t}\\boldsymbol{M}_{t k}^{-\\top}\\boldsymbol{M}_{k t}^{-1}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad=\\displaystyle\\sum_{t=1}^{T}\\sum_{k=1}^{t}(\\boldsymbol{M}_{k t}^{-1})^{2}=\\displaystyle\\sum_{t=1}^{T}\\sum_{k=1}^{t}(T-\\operatorname*{max}\\left\\{t,k\\right\\}+1)^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad=\\displaystyle\\frac{T(T+1)^{2}(T+2)}{12}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Thus, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left[\\cfrac{\\mathrm{Tr}\\left(M^{-\\top}M^{-1}\\right)}{T}-\\left(\\cfrac{\\mathrm{Tr}\\left(M^{-1}\\right)}{T}\\right)^{2}\\right]=\\cfrac{(T+1)^{2}(T+2)}{12}-\\cfrac{(T+1)^{2}}{4}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\cfrac{(T+1)^{2}}{4}\\left[\\cfrac{T+2}{3}-1\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\cfrac{(T+1)^{2}}{4}\\cfrac{T-1}{3}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Overall, $\\lambda_{\\operatorname*{max}}(M^{-1})$ is bounded by ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lambda_{\\operatorname*{max}}(M^{-1})\\le\\displaystyle\\frac{\\mathrm{Tr}\\left(M^{-1}\\right)}{T}+\\sqrt{(T-1)\\frac{(T+1)^{2}}{4}\\frac{T-1}{3}}}&{}\\\\ {\\displaystyle=\\frac{T+1}{2}+\\frac{(T+1)(T-1)}{2\\sqrt{3}}}\\\\ {\\displaystyle=\\frac{T(T+1)}{2\\sqrt{3}}+\\frac{T+1}{2}\\left[1-\\frac{1}{\\sqrt{3}}\\right]}&{}\\\\ {\\displaystyle\\le\\frac{T(T+1)}{2\\sqrt{3}}+\\frac{T(T+1)}{2}\\frac{1}{4}\\le\\frac{9}{10}\\frac{T(T+1)}{2}=\\frac{9}{10}\\,\\mathrm{Tr}(M^{-1}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the last line observes that $\\begin{array}{r}{1-\\frac{1}{\\sqrt{3}}\\leq\\frac{1}{2}\\leq\\frac{T}{4}}\\end{array}$ for $T\\geq2$ and the fact that $\\textstyle{\\frac{1}{\\sqrt{3}}}+{\\frac{1}{4}}\\approx0.83\\leq$ $\\frac{9}{10}$ \u53e3 ", "page_idx": 20}, {"type": "text", "text": "B   Proofs for Section 4 (Dynamic regret for unconstrained OLO via weighted norms) ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "B.1 Details on the 1-Dimensional Reduction ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, for completeness we provide the details of the 1-dimensional reduction of Cutkosky and Orabona [9], specialized to dual weighted-norm pairs $\\left(\\left\\Vert\\cdot\\right\\Vert_{M},\\left\\Vert\\cdot\\right\\Vert_{M^{-1}}\\right)$ as well as its regret guarantee. ", "page_idx": 20}, {"type": "text", "text": "For concreteness, we choose adaptive FTRL with AdaGrad-norm stepsizes [40] as the direction learner. For simplicity we use the scale-free version of [33], so that the direction learner's update is slightly simpler, not requiring prior knowledge of the Lipschitz constant $\\mathfrak{E}\\geq\\|\\widetilde{\\pmb{g}}_{t}\\|_{M^{-1}}$ ", "page_idx": 21}, {"type": "text", "text": "Using Cutkosky and Orabona [9, Theorem 2], we have that the regret of Algorithm 2 is equal to ", "page_idx": 21}, {"type": "equation", "text": "$$\nR_{T}(\\widetilde{\\pmb{u}})=R_{T}^{A}(\\|\\widetilde{\\pmb{u}}\\|_{M})+\\|\\pmb{u}\\|_{M}R_{T}^{\\mathrm{direction}}\\left(\\frac{\\widetilde{\\pmb{u}}}{\\|\\widetilde{\\pmb{u}}\\|_{M}}\\right),\\;\\forall\\widetilde{\\pmb{u}}\\in\\mathbb{R}^{d T},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Where $R_{T}^{A}$ is the regret of $\\boldsymbol{\\mathcal{A}}$ over a sequence of $G$ Lipschitz linear losses and $R_{T}^{\\mathrm{direction}}$ is the regret of (scale-free) adaptive FTRL with a feasible set equal to the unitary ball defined by $\\Vert\\cdot\\Vert_{M}$ ", "page_idx": 21}, {"type": "text", "text": "Choosing the algorithm $\\boldsymbol{\\mathcal{A}}$ to be [20, Algorithm 1], we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathfrak{L}_{T}^{A}(\\|\\tilde{u}\\|_{M})\\leq\\mathcal{O}\\left(\\mathfrak{G}_{\\epsilon}+\\|\\tilde{u}\\|_{M}\\left[\\sqrt{V_{T}\\log\\left(\\frac{\\|\\tilde{u}\\|_{M}\\sqrt{V_{T}}\\Lambda_{T}}{\\mathfrak{G}_{\\epsilon}}+1\\right)}\\vee\\mathfrak{G}\\log\\left(\\frac{\\|\\tilde{u}\\|_{M}\\sqrt{V_{T}}\\Lambda_{T}}{\\epsilon\\mathfrak{G}}\\right)\\right]\\right)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Where $\\begin{array}{r}{V_{T}=\\sum_{t=1}^{T}\\|\\widetilde{\\pmb{g}}_{t}\\|_{M^{-1}}^{2}}\\end{array}$ and $\\begin{array}{r}{\\Lambda_{T}=\\log^{2}(\\sum_{t=1}^{T}\\|\\widetilde{\\pmb{g}}_{t}\\|_{M^{-1}}^{2}/\\mathfrak{G}^{2})\\leq\\mathcal{O}(\\log^{2}T).}\\end{array}$ ", "page_idx": 21}, {"type": "text", "text": "Focusing now on the regret of the direction learner, define the distance generating function $\\psi(\\widetilde{\\pmb{x}})=$ $\\frac{1}{2}\\|\\widetilde{\\pmb x}^{2}\\|_{M}$ Using [31, Theorem 4.3], we have that $\\psi$ is 1-strongly convex $w.r.t\\parallel\\cdot\\parallel_{M}$ Hence, using the regret guarantee of Scale-free FTRL, i.e., Theorem 1 of Orabona and Pal [33], for any $\\widetilde{\\pmb{v}}\\in\\mathbb{R}^{d T}$ such that $\\|\\widetilde{\\pmb{v}}\\|_{M}\\leq1$ the regret of the direction learner is ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathfrak{L}_{T}^{\\mathrm{Birecion}}(\\widetilde v)\\le\\left[\\frac{1}{2}\\left\\lVert\\widetilde v\\right\\rVert_{M}^{2}+2.75\\right]\\sqrt{\\sum_{t=1}^{T}\\left\\lVert\\widetilde g_{t}\\right\\rVert_{M^{-1}}^{2}}+3.5\\operatorname*{max}_{t\\le T}\\left\\lVert\\widetilde g_{t}\\right\\rVert_{M^{-1}}\\le\\mathcal{O}\\left(\\sqrt{\\sum_{t=1}^{T}\\left\\lVert\\widetilde g_{t}\\right\\rVert_{M^{-1}}^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Applying this with  = Taum and combining with the previous two displays leads to the bound stated in the proof of Theorem 2. ", "page_idx": 21}, {"type": "text", "text": "B.2  The Haar Matrices and their Properties ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this section we provide some useful supporting lemmas related to the Haar matrices $H_{n}$ .Wefirst introduce the Haar basis vectors, which make up the columns of the matrix $H_{n}$ .Throughout this section we assume for simplicity that $T$ is a power of 2. ", "page_idx": 21}, {"type": "text", "text": "Definition 1. For any $\\tau\\in\\left\\{2^{i}:i=1:\\log_{2}(T)\\right\\}$ and $i\\in[T/\\tau]$ , the Haar basis vector at timescale $\\tau$ and location $i$ is the vector in $\\mathbb{R}^{T}$ with entries ", "page_idx": 21}, {"type": "equation", "text": "$$\n[\\pmb{h}_{i}^{(\\tau)}]_{t}=\\left\\{\\begin{array}{l l}{1}&{i f t\\in[\\frac{1}{2}\\tau(i-1)+1,\\frac{1}{2}\\tau i]}\\\\ {-1}&{i f t\\in[\\frac{1}{2}\\tau i+1,\\tau i]}\\\\ {0}&{o t h e r w i s e}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The Haar basis vectors are often arranged into the columns of a matrix as follows: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbf{H}_{n}=\\left(h_{0}\\quad h_{1}^{(T)}\\quad h_{1}^{(T/2)}\\quad h_{2}^{(T/2)}\\quad h_{1}^{(T/4)}\\quad h_{2}^{(T/4)}\\quad h_{3}^{(T/4)}\\quad h_{4}^{(T/4)}\\quad\\cdots\\quad h_{T/2}^{(2)}\\right),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\pmb{h}_{0}\\,=\\,(1,1,\\dots,1)^{\\top}\\,\\in\\,\\mathbb{R}^{T}$ . This matrix is referred to as the (unnormalized) Haar basis matrix of order $n=\\log_{2}(T)$ . It is well-known that $H_{n}$ has the following equivalent recursive form [38,11, 37]: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{H}_{0}=(1),}\\\\ &{\\mathbf{H}_{n}=\\Bigg(\\mathbf{H}_{n-1}\\otimes\\binom{1}{1}-\\mathbf{\\sigma}\\mathbf{I}_{2^{n-1}}\\otimes\\binom{1}{-1}\\Bigg)~.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "So, for instance, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbf{H}_{1}=\\left(\\mathbf{H}_{0}\\otimes{\\binom{1}{1}}\\right)\\quad I_{2^{0}}\\otimes{\\binom{1}{-1}}\\right)=\\left((1)\\otimes{\\binom{1}{1}}\\quad(1)\\otimes{\\binom{1}{-1}}\\right)=\\left(11\\quad1\\right),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbf H_{2}=\\left({\\binom{1}{1}}\\quad1_{1}\\right)\\otimes{\\binom{1}{1}}\\quad{\\binom{1}{0}}\\quad{\\binom{1}{1}}\\otimes{\\binom{1}{-1}}\\right)={\\binom{1}{1}}\\quad1\\quad1\\quad0\\quad1_{2}^{0}\\quad\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and so on. For our purposes, we will primaly work in terms of the matrices $H_{n}$ rather than the basis vectors $h_{i}^{(\\tau)}$ . The main utilityof defining the basis vectors $h_{i}^{(\\tau)}$ is that their definition eaily implies the following useful result, which states that the Haar basis vectors are sparsely supported $w.r t$ time. ", "page_idx": 22}, {"type": "text", "text": "Proposition 5. Let $n=\\log_{2}T$ and let $\\pmb{H}_{n}\\in\\mathbb{R}^{T\\times T}$ be the unnormalized Haar basis matrix of order $n$ Then for any $t\\in[T]$ , there at most $1+\\log T$ indices i for which $[H_{n}]_{t,i}\\neq0$ ", "page_idx": 22}, {"type": "text", "text": "The proof follows immediately from Definition 1 (i.e., any $t$ can fall into only one of the intervals covered at each of the $\\log_{2}(T)$ time-scales) and accounting for the additional column $h_{0}$ of all $\\mathrm{1\\,\\dot{s}}$ In what follows, we will also use the following well-known relationship between the vec operator and the Kronecker product (see, e.g., Steeb and Shi [38, Chapter 2.11]). ", "page_idx": 22}, {"type": "text", "text": "Proposition 6. Let A, $B$ and $_{C}$ be matrices of appropriate dimensions such that the product ABC exists.Then, $\\nu e c(A B C)=(C^{\\top}\\otimes A)\\nu e c(B)$ ", "page_idx": 22}, {"type": "text", "text": "The following three lemmas will be used to prove the guarantees of the algorithm characterized in Section 4.1 (Propositions 3 and 4). ", "page_idx": 22}, {"type": "text", "text": "Lemma 5. Let $n=\\log_{2}(T)$ $\\pmb{v}=(v_{1},\\dots,v_{T})^{\\top}\\in\\mathbb{R}^{T}$ , and let $\\mathbf{H}_{n}$ be the unnormalized Haar basis matrix of order $n$ .Then ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbf{H}_{n}^{T}\\boldsymbol{v}=\\left(\\mathbf{H}_{n-1}^{\\top}\\boldsymbol{v}_{+}\\right),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\pmb{v}_{+}=\\left(\\begin{array}{c}{v_{1}+v_{2}}\\\\ {v_{3}+v_{4}}\\\\ {\\vdots}\\\\ {v_{T-1}+v_{T}}\\end{array}\\right),\\quad\\pmb{v}_{-}=\\left(\\begin{array}{c}{v_{1}-v_{2}}\\\\ {v_{3}-v_{4}}\\\\ {\\vdots}\\\\ {v_{T-1}-v_{T}}\\end{array}\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. From Equation (7), we have that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{H}_{n}^{\\top}v=\\left(\\mathbf{H}_{n-1}\\otimes\\left(\\!\\!\\begin{array}{l l}{1}\\\\ {1}\\end{array}\\!\\!\\right)\\!\\!\\right)\\,\\,\\,\\,I_{2^{n-1}}\\otimes\\left(\\!\\!\\begin{array}{l}{1}\\\\ {-1}\\end{array}\\!\\!\\right)\\right)^{\\top}v=\\left(\\!\\!\\begin{array}{l l}{\\mathbf{H}_{n-1}^{\\top}\\otimes(1}&{1)}\\\\ {I_{2^{n-1}}\\otimes(1}&{-1)}\\end{array}\\!\\!\\right)v}\\\\ &{\\qquad=\\left(\\left[\\!\\!\\begin{array}{l}{\\mathbf{H}_{n-1}^{\\top}\\otimes(1}&{1)}\\end{array}\\!\\!\\!\\right]v\\right)\\,\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Moreover, leveraging Proposition 6 we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{H}_{n}^{\\top}v=\\left(\\begin{array}{l l l l}{\\mathrm{vec}\\left((11~\\textstyle1)\\left(v_{1}~v_{3}~\\cdots~\\cdot~v_{T-1}\\right)\\mathbf{H}_{n-1}\\right)}\\\\ {\\mathrm{vec}\\left((1~\\textstyle1)\\left(v_{2}~v_{4}~\\cdots~\\cdot~v_{T-1}\\right)\\mathbf{H}_{n-1}\\right)}\\\\ {\\mathrm{vec}\\left((1~\\textstyle-1)\\left(v_{2}~v_{3}~\\cdots~\\cdot~v_{T-1}\\right)\\mathbf{H}_{n-1}\\right)\\right)}\\\\ {\\mathrm{}}&{}&{}\\\\ {\\qquad=\\left(\\begin{array}{l l l l}{\\mathrm{vec}\\left(\\overline{{(v_{1}+v_{2}~v_{3}+v_{4}~\\cdots~\\cdot~v_{T-1}+v_{T})}}\\,\\mathbf{H}_{n-1}\\right)}\\\\ {\\mathrm{wc}\\left(\\underline{{(v_{1}-v_{2}~\\cdot~v_{3}-v_{4}~\\cdots~\\cdot~v_{T-1}-v_{T})}}\\,I_{2^{n-1}}\\right)}\\\\ {\\mathrm{wc}\\left(\\overline{{(v_{1}-v_{2}~\\cdot~v_{3}-v_{4}~\\cdots~\\cdot~v_{T-1}-v_{T})}}\\,I_{2^{n-1}}\\right)}\\\\ {\\mathrm{}}&{}&{=\\mathrm{vec}}\\end{array}\\right)}\\\\ &{=\\left(\\begin{array}{l}{\\mathbf{H}_{n-1}^{\\top}v_{+}}\\\\ {I_{2^{n-1}}v_{+}}\\end{array}\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Lemma 6. Let $\\mathbf{H}_{n}$ be the unnormalized Haar basis matrix of order $n$ Then, $\\mathbf{H}_{n}\\mathbf{H}_{n}^{\\top}$ satisfies ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{H}_{n}\\mathbf{H}_{n}^{\\top}=\\mathbf{H}_{n-1}\\mathbf{H}_{n-1}^{\\top}\\otimes\\left(\\!\\!\\begin{array}{c c}{1}&{1}\\\\ {1}&{1}\\end{array}\\!\\!\\right)+I_{2^{n-1}}\\otimes\\left(\\!\\!\\begin{array}{c c}{1}&{-1}\\\\ {-1}&{1}\\end{array}\\!\\!\\right)}\\\\ &{\\qquad\\qquad=\\left(\\!\\!\\begin{array}{c c}{\\mathbf{H}_{n-1}\\mathbf{H}_{n-1}^{\\top}+\\mathbf{1}_{2^{n-1}}}&{\\mathbf{0}_{2^{n-1}}}\\\\ {\\mathbf{0}_{2^{n-1}}}&{\\mathbf{H}_{n-1}\\mathbf{H}_{n-1}^{\\top}+\\mathbf{1}_{2^{n-1}},}\\end{array}\\!\\!\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where ${\\bf1}_{2^{n-1}}$ and $\\mathbf{0}_{2^{n-1}}$ are $2^{n-1}\\times2^{n-1}$ matrices of $^{\\,I}$ 's and $\\boldsymbol{O}$ 's respectively. ", "page_idx": 22}, {"type": "text", "text": "Proof. For brevity, let us denote $\\boldsymbol{B}_{n}=\\mathbf{H}_{n}\\mathbf{H}_{n}^{\\top}$ . The first equality follows from elementary properties of block matrices and the Kronecker product: using the recursive form of $\\mathbf{H}_{n}$ ,wehave ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbf{B}_{n}=\\mathbf{H}_{n}\\mathbf{H}_{n}^{\\top}}&{}\\\\ &{\\quad=\\left(\\mathbf{H}_{n-1}\\otimes\\left(\\begin{array}{l l}{1}&{I_{2^{n-1}}\\otimes\\left(\\begin{array}{l}{1}\\\\ {-1}\\end{array}\\right)}\\right)\\,\\left(\\begin{array}{l l}{\\mathbf{H}_{n-1}^{\\top}\\otimes\\left(1}&{1}\\end{array}\\right)\\right)\\right.}\\\\ &{\\quad=\\mathbf{H}_{n-1}\\otimes\\left(\\begin{array}{l}{1}\\\\ {1}\\end{array}\\right)\\,\\mathbf{H}_{n-1}^{\\top}\\otimes\\left(1}&{1\\right)+I_{2^{n-1}}\\otimes\\left(\\begin{array}{l}{1}\\\\ {-1}\\end{array}\\right)\\,I_{2^{n-1}}\\otimes\\left(1}&{-1\\right)}\\\\ &{\\quad=\\mathbf{H}_{n-1}\\otimes\\left(\\begin{array}{l l}{1}&{1}\\\\ {1}\\end{array}\\right)\\,\\mathbf{H}_{n-1}^{\\top}\\otimes\\left(\\begin{array}{l l}{1}&{1}\\end{array}\\right)+I_{2^{n-1}}\\otimes\\left(\\begin{array}{l}{1}\\\\ {-1}\\end{array}\\right)\\,\\mathbf{I}_{2^{n-1}}\\otimes\\left(\\begin{array}{l l}{1}&{-1}\\\\ {1}\\end{array}\\right)}\\\\ &{\\quad=\\mathbf{H}_{n-1}\\mathbf{H}_{n-1}^{\\top}\\otimes\\left(\\begin{array}{l l}{1}&{1}\\\\ {1}&{1}\\end{array}\\right)+I_{2^{n-1}}\\otimes\\left(\\begin{array}{l l}{1}\\\\ {-1}\\end{array}\\right)\\,\\left(\\begin{array}{l l}{1}&{-1}\\\\ {1}&{-1}\\end{array}\\right)}\\\\ &{\\quad=\\mathbf{H}_{n-1}\\mathbf{H}_{n-1}^{\\top}\\otimes\\left(\\begin{array}{l l}{1}&{1}\\\\ {1}&{1}\\end{array}\\right)+I_{2^{n-1}}\\otimes\\left(\\begin{array}{l l}{1}&{-1}\\\\ {-1}&{1}\\end{array}\\right)}\\\\ &{\\quad=B_{n-1}\\otimes\\left(\\begin{array}{l l}{1}&{1}\\\\ {1}&{1}\\end{array}\\right)+I_{2^{n-1}}\\otimes\\left(\\begin{array}{l l}{1}&{-1}\\\\ {-1}&{1}\\end{array}\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "To get the second expression, let us proceed by induction. We have $B_{0}=(1)$ and ", "page_idx": 23}, {"type": "equation", "text": "$$\nB_{1}=\\mathbf{H}_{1}\\mathbf{H}_{1}^{\\top}={\\left(\\begin{array}{l l}{1}&{1}\\\\ {1}&{-1}\\end{array}\\right)}\\left({\\begin{array}{l l}{1}&{1}\\\\ {1}&{-1}\\end{array}}\\right)^{\\top}={\\left(\\begin{array}{l l}{2}&{0}\\\\ {0}&{2}\\end{array}\\right)}=\\left({\\begin{array}{c c}{B_{0}+\\mathbf{1}_{1}}&{\\mathbf{0}_{1}}\\\\ {\\mathbf{0}_{1}}&{B_{0}+\\mathbf{1}_{1}}\\end{array}}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Next, let us assume that $B_{n}$ satisfies ", "page_idx": 23}, {"type": "equation", "text": "$$\nB_{n}=\\binom{B_{n-1}+{\\bf1}_{2^{n-1}}}{{\\bf0}_{2^{n-1}}}\\quad{\\bf0}_{2^{n-1}}\\quad\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Then, applying the recursive form Equation (8) for $B_{n+1}$ ,wehave ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{B_{n+1}=B_{n}\\otimes\\binom{1}{1}\\!+\\!1\\!\\!\\!}\\\\ &{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!B_{n-1}\\!\\!\\!+\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! \n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the last line observes that $\\mathbf{1}_{2^{n-1}}\\otimes\\left(1\\!\\!\\!\\begin{array}{c c}{{1}}&{{1}}\\\\ {{1}}&{{1}}\\end{array}\\right)=\\mathbf{1}_{2^{n}}$ and that after adding the two block matrices, the top left and bottom right blocks are both ", "page_idx": 23}, {"type": "equation", "text": "$$\n{\\cal B}_{n-1}\\otimes\\left(\\!\\!{1\\atop1}\\quad{1\\atop1}\\right)+I_{2^{n-1}}\\otimes\\left(\\!\\!{1\\atop-1}\\quad{-1\\atop1}\\right)+{\\bf1}_{2^{n}}={\\cal B}_{n}+{\\bf1}_{2^{n}},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "via Equation (8). Hence, the stated result follows by induction. ", "page_idx": 23}, {"type": "text", "text": "Now using this, we have the following bound on the norm of the high-dimensional surrogate losses. Lemma 7. Let $n\\,=\\,\\log_{2}(T)$ $\\mathbf{e}_{t}$ be the $t^{t h}$ standard basis vector of $\\mathbb{R}^{T}$ .and for $\\pmb{g}_{t}\\ \\in\\ \\mathbb{R}^{d}$ let $\\widetilde{\\pmb{g}}_{t}\\,=\\,\\mathbf{e}_{t}\\otimes\\pmb{g}_{t}\\,\\in\\,\\mathbb{R}^{d T}$ . Let $\\mathbf{H}_{n}$ be a Haar matrix of order $n$ and let ${\\boldsymbol{B}}=\\mathbf{H}_{n}\\otimes I_{d}$ be it's block extension to sequence in $\\mathbb{R}^{d}$ . Then, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left\\lVert\\widetilde{\\pmb{g}}_{t}\\right\\rVert_{B B^{\\top}}^{2}=(\\log T+1)\\left\\lVert\\pmb{g}_{t}\\right\\rVert_{2}^{2}\\;.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. Using Lemma 1, we have that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left\\|\\widetilde{\\pmb{g}}_{t}\\right\\|_{B B^{\\top}}^{2}=\\left[\\pmb{H}_{n}\\pmb{H}_{n}^{\\top}\\right]_{t t}\\left\\|\\pmb{g}_{t}\\right\\|^{2}\\;.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Moreover, using Equation (9) it can easily be seen that the diagonal entries of $H_{n}H_{n}^{\\top}$ are $\\log_{2}T+1$ sowehave ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left\\|\\widetilde{\\pmb{g}}_{t}\\right\\|_{B B^{\\top}}^{2}\\leq\\left(1+\\log_{2}T\\right)\\left\\|\\pmb{g}_{t}\\right\\|_{2}^{2}\\;.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "B.3 Proof of Proposition 3 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "$n=\\log_{2}T$ $\\mathbf{H}_{n}$ $n$ $\\tau\\,\\in\\,\\bigl\\{2^{i}:i=0,\\dots,\\log_{2}T\\bigr\\}$ $N_{\\tau}=T/\\tau$ $\\mathcal{T}_{1}^{(\\tau)},\\dots,\\mathcal{T}_{N_{\\tau}}^{(\\tau)}$ IN) be a partition of [T] into interadsof length $\\tau$ Defne heavrage comparatorin interval $\\mathcal{T}_{i}^{(\\tau)}$ io be $\\begin{array}{r}{\\bar{\\pmb u}_{i}^{(\\tau)}=\\frac{1}{\\tau}\\sum_{t\\in\\mathcal{Z}_{i}^{(\\tau)}}{\\pmb u}_{t}}\\end{array}$ and define the squared path-length at time-scale $\\tau<T$ to be ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\bar{P}(\\vec{u},\\tau):=\\sum_{i=1}^{N_{\\tau}/2}\\left\\lVert\\bar{\\pmb u}_{2i-1}^{(\\tau)}-\\bar{\\pmb u}_{2i}^{(\\tau)}\\right\\rVert_{2}^{2},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and $\\bar{P}(\\vec{u},T)=\\left\\|\\bar{\\pmb{u}}_{1}^{(T)}\\right\\|_{2}^{2}=\\left\\|\\bar{\\pmb{u}}\\right\\|_{2}^{2}$ Then, seting ${\\bf S}=[{\\bf H}_{n}{\\bf H}_{n}^{\\top}]^{-1}$ and $M=\\mathbf{S}\\otimes I_{d}$ we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\|{\\widetilde u}\\|_{M}^{2}\\leq\\|{\\bar{u}}\\|_{2}^{2}+\\displaystyle\\frac{1}{4}\\sum_{i=0}^{\\log_{2}(T)}{\\bar{P}}({\\vec{u}},2^{i})\\leq\\|{\\bar{u}}\\|_{2}^{2}+\\displaystyle\\frac{1}{4}\\log\\left(T\\right)\\operatorname*{max}_{\\tau}{\\bar{P}}({\\vec{u}},\\tau),}}\\\\ {{\\displaystyle\\|{\\widetilde g}_{t}\\|_{M^{-1}}^{2}=\\|g_{t}\\|_{2}^{2}\\left(1+\\log T\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof The proof of the claim $\\lVert\\widetilde{\\pmb{g}}_{t}\\rVert_{M^{-1}}^{2}\\ =\\ \\lVert\\widetilde{\\pmb{g}}_{t}\\rVert_{{\\bf H H}^{\\top}}^{2}\\ =\\ \\lVert\\pmb{g}_{t}\\rVert_{2}^{2}\\left[\\log_{2}(T)+1\\right]$ is provided in Lemma 7. ", "page_idx": 24}, {"type": "text", "text": "To see the form of $\\|\\widetilde{\\boldsymbol{u}}\\|_{M}^{2}$ , let us first write ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left\\lVert\\widetilde{\\mathbf{u}}\\right\\rVert_{M}^{2}=\\left\\langle\\widetilde{\\mathbf{u}},[H H^{\\top}]^{-1}\\widetilde{\\mathbf{u}}\\right\\rangle=\\left\\langle H^{-1}\\widetilde{\\mathbf{u}},H^{-1}\\widetilde{\\mathbf{u}}\\right\\rangle=\\left\\lVert H^{-1}\\widetilde{\\mathbf{u}}\\right\\rVert_{2}^{2}\\,.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The result then follows by showing that ", "page_idx": 24}, {"type": "equation", "text": "$$\nH^{-1}\\tilde{u}=\\frac{1}{2}\\left(\\begin{array}{c}{2\\tilde{u}}\\\\ {\\overline{{u}}_{1}^{(T/2)}-\\overline{{u}}_{2}^{(T/2)}}\\\\ {\\overline{{u}}_{1}^{(T/4)}-\\overline{{u}}_{2}^{(T/4)}}\\\\ {\\overline{{u}}_{3}^{(T/4)}-\\overline{{u}}_{4}^{(T/4)}}\\\\ {\\vdots}\\\\ {u_{1}-u_{2}}\\\\ {u_{3}-u_{4}}\\\\ {\\vdots}\\\\ {u_{T-1}-u_{T}}\\end{array}\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "so that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|{H^{-1}\\tilde{u}}\\right\\|_{2}^{2}=\\underbrace{\\left\\|{\\tilde{u}}\\right\\|_{2}^{2}}_{\\tilde{P}(T)}+\\underbrace{\\frac{1}{4}\\left\\|{\\tilde{u}_{1}^{(T/2)}-\\bar{u}_{2}^{(T/2)}}\\right\\|_{2}^{2}}_{\\tilde{P}(T/2)}+\\underbrace{\\frac{1}{4}\\left\\|{\\tilde{u}_{1}^{(T/4)}-\\bar{u}_{2}^{(T/4)}}\\right\\|_{2}^{2}+\\frac{1}{4}\\left\\|{\\tilde{u}_{3}^{(T/4)}-\\bar{u}_{4}^{(T/4)}}\\right\\|_{2}^{2}}_{\\tilde{P}(T/4)}}\\\\ {+\\ldots+\\underbrace{\\frac{1}{4}\\left\\|{u_{1}-u_{2}}\\right\\|_{2}^{2}+\\frac{1}{4}\\left\\|{u_{3}-u_{4}}\\right\\|_{2}^{2}+\\ldots+\\frac{1}{4}\\left\\|{u_{T-1}-u_{T}}\\right\\|_{2}^{2}}_{=\\tilde{P}(1)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where for brevity we have dropped the argument $\\vec{\\pmb{u}}$ on $\\bar{P}(\\vec{u},\\tau)$ ", "page_idx": 24}, {"type": "text", "text": "Equation (10) is best shown via example; the general case is mostly a tedius exercise which we provide at the end. Assume $T=4$ , then the Haar matrix of order $n=\\log_{2}(T)=2$ is ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbf H_{2}=\\left(\\begin{array}{l l l l}{1}&{1}&{1}&{0}\\\\ {1}&{1}&{-1}&{0}\\\\ {1}&{-1}&{0}&{1}\\\\ {1}&{-1}&{0}&{-1}\\end{array}\\right)=\\underbrace{\\left(\\begin{array}{l l l l}{\\frac12}&{\\frac12}&{\\frac{1}{\\sqrt{2}}}&{0}\\\\ {\\frac{1}{2}}&{\\frac{1}{2}}&{\\frac{-1}{\\sqrt{2}}}&{0}\\\\ {\\frac{1}{2}}&{-\\frac{1}{2}}&{0}&{\\frac{1}{\\sqrt{2}}}\\\\ {\\frac{1}{2}}&{-\\frac{1}{2}}&{0}&{\\frac{-1}{\\sqrt{2}}}\\end{array}\\right)}_{=:\\tilde{\\mathbf H}_{2}}\\underbrace{\\left(\\begin{array}{l l l l}{2}&{0}&{0}&{0}\\\\ {0}&{2}&{0}&{0}\\\\ {0}&{0}&{\\sqrt{2}}&{0}\\\\ {0}&{0}&{0}&{\\sqrt{2}}\\end{array}\\right)}_{=:D_{2}}\\,.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "It is well-known tha for any $T$ the columns of $\\mathbf{H}_{\\mathrm{log}_{2}(T)}$ form an orthogonal basis of $\\mathbb{R}^{T}$ [41, Chapter 6.1.1], which implies that $\\ensuremath{\\widetilde{\\mathbf{H}}}_{2}$ is orthonormal. So, $\\bar{\\widetilde{\\mathbf{H}}}_{2}^{-1}=\\widetilde{\\mathbf{H}}_{2}^{\\top}$ and ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{H}_{2}^{-1}=(\\tilde{\\mathbf{H}}_{2}D_{2})^{-1}=D_{2}^{-1}\\tilde{\\mathbf{H}}_{2}^{-1}=D_{2}^{-1}\\tilde{\\mathbf{H}}_{2}^{\\top}}\\\\ &{\\qquad=\\left(\\begin{array}{l l l l}{\\frac{1}{2}}&{0}&{0}&{0}\\\\ {0}&{\\frac{1}{2}}&{0}&{0}\\\\ {0}&{0}&{\\frac{1}{\\sqrt{2}}}&{0}\\\\ {0}&{0}&{0}&{\\frac{1}{\\sqrt{2}}}\\end{array}\\right)\\left(\\begin{array}{l l l l}{\\frac{1}{2}}&{\\frac{1}{2}}&{\\frac{1}{2}}&{\\frac{1}{2}}\\\\ {\\frac{1}{2}}&{\\frac{1}{2}}&{-\\frac{1}{2}}&{-\\frac{1}{2}}\\\\ {\\frac{1}{\\sqrt{2}}}&{-\\frac{1}{\\sqrt{2}}}&{0}&{0}\\\\ {0}&{0}&{\\frac{1}{\\sqrt{2}}}&{\\frac{-1}{\\sqrt{2}}}\\end{array}\\right)=\\left(\\begin{array}{l l l l}{\\frac{1}{4}}&{\\frac{1}{4}}&{\\frac{1}{4}}&{\\frac{1}{4}}\\\\ {\\frac{1}{4}}&{\\frac{1}{4}}&{-\\frac{1}{4}}&{-\\frac{1}{4}}\\\\ {\\frac{1}{2}}&{-\\frac{1}{2}}&{0}&{0}\\\\ {0}&{0}&{\\frac{1}{2}}&{\\frac{-1}{2}}\\end{array}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "which leads to Equation (10) after applying the Kronecker product: ", "page_idx": 25}, {"type": "equation", "text": "$$\nH^{-1}\\tilde{u}=\\left(\\begin{array}{l l l l}{\\frac{I_{d}}{4}}&{\\frac{I_{d}}{4}}&{\\frac{I_{d}}{4}}&{\\frac{I_{d}}{4}}\\\\ {\\frac{I_{d}}{4}}&{\\frac{I_{d}}{4}}&{-\\frac{I_{d}}{4}}&{-\\frac{I_{d}}{4}}\\\\ {\\frac{I_{d}}{2}}&{-\\frac{I_{d}}{2}}&{\\mathbf{0}}&{\\mathbf{0}}\\\\ {\\mathbf{0}}&{\\mathbf{0}}&{\\frac{I_{d}}{2}}&{\\frac{-I_{d}}{2}}\\end{array}\\right)\\left(\\begin{array}{l}{u_{1}}\\\\ {\\vdots}\\\\ {\\vdots}\\\\ {u_{T}}\\end{array}\\right)=\\left(\\begin{array}{l}{\\frac{u_{1}+u_{2}+u_{3}+u_{4}}{4}}\\\\ {\\frac{u_{1}+u_{2}+u_{3}-u_{4}}{4}}\\\\ {\\frac{u_{1}-u_{2}}{4}}\\\\ {\\frac{u_{3}-u_{4}}{2}}\\end{array}\\right)=\\frac{1}{2}\\left(\\begin{array}{l}{2\\bar{u}}\\\\ {\\bar{u}_{1}^{(T/2)}-\\bar{u}_{2}^{(T/2)}}\\\\ {\\phantom{\\frac{u_{1}}{2}}\\!\\!\\!\\!u_{1}-u_{2}}\\\\ {u_{3}-u_{4}}\\end{array}\\right).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "More generally, start with $d=1$ begin again by factoring ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbf{H}_{n}^{-1}=D_{n}^{-1}\\widetilde{\\mathbf{H}}_{n}^{\\top}=D_{n}^{-2}\\mathbf{H}_{n}^{\\top},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "wherenow $\\widetilde{\\mathbf{H}}_{n}$ is the normalized Haar basis matrix of order $n=\\log_{2}(T)$ and ", "page_idx": 25}, {"type": "equation", "text": "$$\nD_{n}=\\mathrm{Diag}\\left(\\sqrt{T},\\underbrace{\\sqrt{T}}_{2^{0}},\\underbrace{\\sqrt{T/2}}_{2^{1}},\\sqrt{T/2},\\underbrace{\\sqrt{T/4}}_{2^{2}},\\ldots,\\underbrace{\\sqrt{T/4}}_{2^{n-1}},\\ldots,\\underbrace{\\sqrt{2},\\ldots,\\sqrt{2}}_{2^{n-1}}\\right).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The result is then attained by unrolling the recursion for $\\mathbf{H}_{n}^{\\top}\\widetilde{\\pmb{u}}$ given by Lemma 5 and factoring in the normalization factors $D_{n}^{-2}$ . The result for $d>1$ is then immediately implied by observing that the block matrix $\\mathbf{H}_{n}^{-1}\\otimes\\pmb{I}_{d}$ will act upon the vector components of $\\widetilde{\\boldsymbol{u}}\\in\\mathbb{R}^{d T}$ in an identical way to how $\\mathbf{H}_{n}^{-1}$ acts upon a vector of scalars. \u53e3 ", "page_idx": 25}, {"type": "text", "text": "B.4 Proof of Proposition 4 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Proposition 4. The algorithm characterized by applying Theorem 2 with $S=[H_{n}H_{n}^{\\top}]^{-1}$ canbe implementedwith ${\\mathcal{O}}\\left(d\\log T\\right)$ per-roundcomputation. ", "page_idx": 25}, {"type": "text", "text": "Proof. Note that the losses passed to the 1-dimensional parameter-free algorithm are $\\langle\\widetilde{\\pmb{v}}_{t},\\widetilde{\\pmb{g}}_{t}\\rangle=$ $\\langle\\widetilde{\\mathbf{v}}_{t},\\mathbf{e}_{t}\\otimes\\mathbf{g}_{t}\\rangle$ , and since $\\mathbf{e}_{t}\\otimes\\mathbf{g}_{t}$ has only $d$ active indices we can compute the 1-dimensional learner's losses in $O(d)$ . As such, the 1-dimensional learner can be implemented in $O(d)$ per-round computation. ", "page_idx": 25}, {"type": "text", "text": "For the direction learner, we are to show that each of the relevant variables can be maintained using Only ${\\mathcal{O}}(d\\log T)$ per-round computation. ", "page_idx": 25}, {"type": "text", "text": "Using Proposition 3, we immediately have $V_{t+1}=V_{t}+\\|\\widetilde{\\pmb{g}}_{t}\\|_{M^{-1}}^{2}=V_{t}+(\\log T+1)\\left\\|\\pmb{g}_{t}\\right\\|^{2}$ $V_{t+1}$ can be maintained using only $O(d)$ per-round computation (i.e., to compute $\\|\\pmb{\\{g}}_{t}\\|^{2})$ ", "page_idx": 25}, {"type": "text", "text": "For the scaling factor $\\left\\|\\widetilde{\\pmb{\\theta}}_{t+1}\\right\\|_{M^{-1}}$ observe that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\lVert\\widetilde{\\pmb{\\theta}}_{t+1}\\right\\rVert_{M^{-1}}^{2}=\\left\\lVert\\widetilde{\\pmb{g}}_{t}\\right\\rVert_{M^{-1}}^{2}+\\left\\lVert\\widetilde{\\pmb{\\theta}}_{t}\\right\\rVert_{M^{-1}}^{2}+2\\left\\langle\\widetilde{\\pmb{\\theta}}_{t},M^{-1}\\widetilde{\\pmb{g}}_{t}\\right\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Hence,weagain have $O(d)$ per-roundcomputationto compute $\\|\\widetilde{\\pmb{g}}_{t}\\|_{M^{-1}}^{2}$ ,and leting $\\pmb{h}_{t}=\\pmb{H}_{n}^{\\top}\\pmb{e}_{t}$ wecan decompose the last term as ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\langle\\overline{{\\theta}}_{s}\\left(H,H_{s}^{T}\\otimes L\\right)\\right\\rangle\\otimes_{\\mathbf{J}}\\left\\langle\\overline{{\\theta}}_{s}\\left(H,H_{s}^{T}\\otimes H_{s}\\right)\\right\\rangle}&{=\\left\\langle\\overline{{\\theta}}_{s}\\left(H,H_{s}^{T}\\otimes H_{s}\\right)\\right\\rangle}\\\\ &{=\\left\\langle\\overline{{\\theta}}_{s}\\left(H_{s}^{T}\\otimes H_{s}H_{s}\\otimes g_{t}\\right)\\right\\rangle}\\\\ &{=-\\frac{\\nu_{1}}{\\sqrt{2}}(\\epsilon^{T}\\otimes g_{t})\\cdot\\left(H_{s}H_{s}\\otimes g_{t}\\right)}\\\\ &{=\\frac{\\nu_{1}}{\\sqrt{2}}\\epsilon^{T}\\otimes H_{s}H_{s}^{T}\\otimes_{\\mathbf{J}}\\left(H_{s}H_{s}^{T}\\otimes H_{s}\\right)}\\\\ &{=\\frac{\\nu_{1}}{\\sqrt{2}}\\epsilon^{T}\\otimes H_{s}h_{t}\\otimes(g_{t},g_{t})}\\\\ &{=\\frac{\\nu_{1}}{\\sqrt{2}}\\left(h_{s}h_{t})\\otimes_{\\mathbf{J}}\\left(g_{t},g_{t}\\right)}\\\\ &{=-\\left\\langle\\overline{{\\theta}}_{s}\\left(H_{s}^{T}\\otimes H_{s}\\right),h_{t}\\right\\rangle}\\\\ &{=\\left\\langle\\overline{{\\theta}}_{s}\\left(H_{s}^{T}\\otimes H_{s}\\right)\\right\\rangle}\\\\ &{=\\left\\langle\\overline{{\\theta}}_{s}\\left(\\sum_{u=0}^{T}h_{u}\\right)\\right\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "From Proposition 5, for any $t$ the vector $h_{t}=H_{n}^{\\top}\\mathbf{e}_{t}$ has only $\\log T+1$ active non-zero elements by construction of the Haar basis, so given $\\Lambda_{t}$ , the product $\\mathbf{\\nabla}\\Lambda_{t}h_{t}$ takes a linear combination of $\\log T+1$ vectors in $\\mathbb{R}^{d}$ , for ${\\mathcal{O}}(d\\log T)$ operations. Note that the variable $\\Lambda_{t}$ can also be maintained with ${\\mathcal{O}}(d\\log T)$ operations since each term is $\\mathbf{\\boldsymbol{g}}_{t}\\mathbf{\\boldsymbol{h}}_{t}^{\\top}$ , which involves updating ${\\mathcal{O}}(\\log T)$ columns of $\\mathbf{A}_{t-1}\\in\\mathbb{R}^{d\\times T}$ . Hence overall we can maintain $\\left\\|\\widetilde{\\pmb{\\theta}}_{t+1}\\right\\|_{M^{-1}}$ using ${\\mathcal{O}}(d\\log T)$ per-round computation. Lastly, consider the variable $\\widetilde{\\pmb{\\theta}}_{t+1}$ . Observe that we can maintain a variable $\\begin{array}{r l}{\\widehat{\\pmb{\\theta}}_{t+1}}&{{}=}\\end{array}$ $-\\left(\\dot{\\pmb{H}}_{n}^{\\top}\\otimes\\pmb{I}_{d}\\right)\\sum_{s=1}^{t}\\widetilde{\\pmb{g}}_{s}$ using ${\\mathcal{O}}(d\\log T)$ computation: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\widehat{\\boldsymbol{\\theta}}_{t+1}=-\\left(\\boldsymbol{H}_{n}^{\\top}\\otimes\\boldsymbol{I}_{d}\\right)\\sum_{s=1}^{t}\\widetilde{\\boldsymbol{g}}_{s}=\\widehat{\\boldsymbol{\\theta}}_{t}-\\left(\\boldsymbol{H}_{n}^{\\top}\\boldsymbol{e}_{t}\\otimes\\boldsymbol{g}_{t}\\right)=\\widehat{\\boldsymbol{\\theta}}_{t}-\\left(\\boldsymbol{h}_{t}\\otimes\\boldsymbol{g}_{t}\\right),\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "since $\\pmb{h}_{t}\\otimes\\pmb{g}_{t}$ is a block vector containing $\\log\\left(T\\right)+1$ non-zeros blocks of length $d$ Hence, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{\\pmb{\\theta}}_{t+1}=(\\pmb{H}_{n}\\pmb{H}_{n}^{\\top}\\otimes\\pmb{I}_{n})\\displaystyle\\sum_{s=1}^{t}\\pmb{g}_{s}=(\\pmb{H}_{n}\\otimes\\pmb{I}_{n})(\\pmb{H}_{n}^{\\top}\\otimes\\pmb{I}_{n})\\displaystyle\\sum_{s=1}^{t}\\widetilde{\\pmb{g}}_{s}}\\\\ &{\\qquad=(\\pmb{H}_{n}\\otimes\\pmb{I}_{n})\\widehat{\\pmb{\\theta}}_{t+1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and again via the construction of the Haar basis, each row of $H_{n}$ (i.e., each column of $\\boldsymbol{H}_{n}^{\\top}$ )has only $\\log T+1$ non-zero entries, we can compute each $d\\times1$ block of $\\widetilde{\\pmb{\\theta}}_{t+1}$ using ${\\mathcal{O}}(d\\log T)$ computation. Finally, observe that in order to implement the direction learner, we need only compute the $\\dot{t}^{\\mathrm{th}}\\,d\\times1$ block of $\\widetilde{\\pmb{\\theta}}_{t}$ . Indeed, since for each $t$ , the vector $\\widetilde{\\pmb{g}}_{t}=\\mathbf{e}_{t}\\otimes\\pmb{g}_{t}$ has only $d$ non-zero indices, it suffices to retrieve the corresponding indices of $\\widetilde{\\pmb{v}}_{t}$ to implement direction learner. \u53e3 ", "page_idx": 26}, {"type": "text", "text": "We note that the memory overhead of maintaining each of these variables can also likely be reduced by more careful bookkeeping, and acknowledging the fact that the algorithm only really needs to retrieve the $t^{\\mathrm{th}}$ block of $\\widetilde{\\pmb{w}}_{t}$ , since the losses are $\\widetilde{\\pmb{g}}_{t}=\\mathbf{e}_{t}\\otimes\\pmb{g}_{t}$ . We omit these considerations here for brevity. ", "page_idx": 26}, {"type": "text", "text": "C  Proofs for Section 5 (Recovering Variance- Variability Coupling Guarantees) ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "C.1 Proof of Theorem 3 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Theorem 3. Let WealthT $\\begin{array}{r}{\\mathrel{\\mathop:}=\\,-\\sum_{t=1}^{T}\\langle\\widetilde{\\pmb{g}}_{t},\\widetilde{\\pmb{w}}_{t}\\rangle}\\end{array}$ denote the \u201cwealth\" of an algorithm $\\boldsymbol{\\mathcal{A}}$ and let $(f,f^{*})$ be a Fenchel conjugate pair Then $\\boldsymbol{\\mathcal{A}}$ guarantees WealthT $\\geq\\ f_{T}^{*}\\big(-\\sum_{t=1}^{T}\\widetilde{\\pmb{g}}_{t}\\big)$ for any sequence $\\widetilde{\\pmb g}_{1},\\dots,\\widetilde{\\pmb g}_{T}$ if and only if $R_{T}(\\vec{\\pmb{u}})\\,\\leq\\,f_{T}(\\widetilde{\\pmb{u}})$ for any sequence $\\vec{\\pmb u}\\,=\\,({\\pmb u}_{1},\\dots,{\\pmb u}_{T})$ in $\\mathcal{W}$ where $\\widetilde{\\pmb u}=(\\pmb u_{1}^{\\top},\\dots,\\pmb u_{T}^{\\top})^{\\top}$ is the concatenation of the sequence $\\vec{\\pmb{u}}$ into a vector. ", "page_idx": 27}, {"type": "text", "text": "Proof. Thanks to Proposition 1, the proof is essentially the same as the usual one. We provide the argument here for completeness. ", "page_idx": 27}, {"type": "text", "text": "From Proposition 1, $\\begin{array}{r}{R_{T}(\\vec{u})=R_{T}^{\\mathrm{Seq}}(\\widetilde{\\pmb{u}})=\\sum_{t=1}^{T}\\big\\langle\\widetilde{\\pmb{g}}_{t},\\widetilde{\\pmb{w}}_{t}-\\widetilde{\\pmb{u}}\\big\\rangle}\\end{array}$ for $\\widetilde{\\pmb{g}}_{t}=\\mathbf{e}_{t}\\otimes\\pmb{g}_{t}$ and $\\begin{array}{r}{\\widetilde u=\\sum_{t=1}^{T}\\mathbf e_{t}\\otimes}\\end{array}$ $\\pmb{u}_{t}$ . Hence, recalling the definition of the Fenchel conjugate, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{R_{T}(\\vec{u})=\\displaystyle\\sum_{t=1}^{T}\\left\\langle g_{t},w_{t}-u_{t}\\right\\rangle=\\displaystyle\\sum_{t=1}^{T}\\left\\langle\\widetilde{g}_{t},\\widetilde{w}_{t}-\\widetilde{u}\\right\\rangle=-\\,\\mathrm{Wealth}_{T}-\\displaystyle\\sum_{t=1}^{T}\\left\\langle\\widetilde{g}_{t},\\widetilde{u}\\right\\rangle}\\\\ &{}&{\\,\\,\\,\\,\\,\\leq\\Big\\langle-\\displaystyle\\sum_{t=1}^{T}\\widetilde{g}_{t},\\widetilde{u}\\Big\\rangle-f_{T}^{*}\\Big(-\\displaystyle\\sum_{t=1}^{T}\\widetilde{g}_{t}\\Big)\\leq\\operatorname*{sup}_{\\theta}\\left\\langle\\theta,\\widetilde{u}\\right\\rangle-f_{T}^{*}(\\theta)=f_{T}(\\widetilde{u})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Similarly, for the other direction, suppose we have $R_{T}(\\vec{u})=R_{T}^{\\mathrm{Seq}}(\\widetilde{\\pmb{u}})\\leq f_{T}(\\widetilde{\\pmb{u}})$ for any $\\widetilde{\\pmb{u}}$ . Then re-arranging, we have Wealt $\\mathrm{h}_{T}\\,\\geq\\,\\left\\langle-\\sum_{t=1}^{T}\\widetilde{\\pmb{g}}_{t},\\widetilde{\\pmb{u}}\\right\\rangle-f_{T}(\\widetilde{\\pmb{u}})$ , and since this holds for any $\\widetilde{\\pmb{u}}$ we can choose the one that tightens the bound to get WealthT $\\begin{array}{r l}{\\mathrm{~}}&{{}\\geq\\operatorname*{sup}_{\\widetilde{\\pmb u}}\\left\\langle-\\sum_{t=1}^{T}\\widetilde{\\pmb g}_{t},\\widetilde{\\pmb u}\\right\\rangle-f_{T}(\\widetilde{\\pmb u})=}\\end{array}$ $\\textstyle f_{T}^{*}(-\\sum_{t=1}^{T}{\\widetilde{\\pmb g}}_{t})$ \u53e3 ", "page_idx": 27}, {"type": "text", "text": "D  Supporting Lemmas ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Lemma 8. Let $\\Sigma\\in\\mathbb{R}^{T\\times T}$ be the finite-difference operator, having entries ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\Sigma_{i j}=\\left\\{{\\O\\atop0}\\begin{array}{l l}{{1}}&{{i f\\,i=j}}\\\\ {{-1}}&{{i f\\,j=i+1}}\\\\ {{0}}&{{o t h e r w i s e}}\\end{array}}\\right..\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Then, ", "page_idx": 27}, {"type": "text", "text": "1. The inverse of $\\Sigma$ the upper-triangular matrix of $1\\,\\dot{s}$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\Sigma_{i j}^{-1}=\\left\\{\\begin{array}{l l}{{1}}&{{i f j\\ge i}}\\\\ {{0}}&{{o t h e r w i s e}}\\end{array},\\quad\\forall i,j\\ .\\right.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "2. The eigenvalues of $\\Sigma$ and $\\Sigma^{-1}$ are $\\lambda_{i}=1$ for all $i\\in[T]$ ", "page_idx": 27}, {"type": "text", "text": "3. $x\\mapsto x^{\\top}\\Sigma x$ is positive definite. ", "page_idx": 27}, {"type": "text", "text": "Moreover, the analogous properties hold for the block matrix\u2265? Ia  RdTxdT. ", "page_idx": 27}, {"type": "text", "text": "Proof. The inverse of $\\Sigma$ is the upper-triangular matrix $\\Delta$ characterized by entries ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\Delta_{i j}={\\binom{1}{0}}_{\\mathrm{\\otherwise}}^{\\mathrm{\\if}\\ j\\ \\geq\\ i}\\ .\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "To see why, observe that we have $\\Sigma_{T,T}\\Delta_{T,T}=1$ and for $i<T$ wehave ", "page_idx": 27}, {"type": "equation", "text": "$$\n[\\Sigma\\Delta]_{i j}=\\sum_{i,j}\\Sigma_{i k}\\Delta_{k j}=\\Delta_{i j}-\\Delta_{i+1,j}={\\left\\{\\begin{array}{l l}{1}&{{\\mathrm{if~}}i=j}\\\\ {0}&{{\\mathrm{otherwise}}}\\end{array}\\right.},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Next, since $\\Sigma$ and $\\Sigma^{-1}$ are upper-triangular, their eigenvalues are equal to their diagonal entries, and hence both have eigenvalues $\\lambda_{i}=1$ for all $i$ ", "page_idx": 28}, {"type": "text", "text": "To see that the asymmetric matrix $\\Sigma$ is positive definite, it suffices to show that the symmetric part of $\\Sigma$ , i.e., the matrix $\\pmb{\\Sigma}_{S}=(\\pmb{\\Sigma}+\\pmb{\\Sigma}^{\\top})/2$ , is positive definite [23]. Luckily, $\\pmb{\\Sigma}_{S}$ is also a well-known variation of the discrete difference operator and is known to be positive definite [see, e.g., Theorem 7.4.7 in 39]. ", "page_idx": 28}, {"type": "text", "text": "For the block matrix $\\pmb{{\\cal B}}=\\pmb{\\Sigma}\\otimes\\pmb{{\\cal I}}_{d}$ , the inverse is given immediately by the inverse property of the Kronecker product: $\\boldsymbol{B}^{-1}=(\\boldsymbol{\\Sigma}\\otimes\\boldsymbol{I}_{d})^{-1}=\\boldsymbol{\\Sigma}^{-1}\\otimes\\boldsymbol{I}_{d}$ . We also have that $\\pmb{{\\cal B}}=\\pmb{\\Sigma}\\otimes\\pmb{{\\cal I}}_{d}$ and $B^{-1}$ have eigenvalues $\\lambda_{i}=1$ for all $i\\in[d T]$ , since both are again upper-triangular with 1's on their main diagonal. Finally, we have positive definiteness of $_B$ using the fact that the symmetric part of $\\pmb{{\\cal B}}=\\pmb{\\Sigma}\\otimes\\pmb{{\\cal I}}_{d}$ .s $\\begin{array}{r}{\\frac{1}{2}(\\dot{\\boldsymbol{B}}+\\boldsymbol{B}^{\\top})=\\frac{1}{2}\\big(\\Sigma\\otimes I_{d}+\\Sigma^{\\top}\\otimes I_{d}\\big)=\\frac{1}{2}(\\dot{\\Sigma}+\\Sigma^{\\top})\\otimes I_{d}}\\end{array}$ by the distributive property, hence $_B$ is the Kronecker product of two symmetric positive definite matrices, so $_B$ is positive definite [38, Chapter 2]. \u53e3 ", "page_idx": 28}, {"type": "text", "text": "We borrow the following eigenvalue bound from [43]. ", "page_idx": 28}, {"type": "text", "text": "Theorem 5. (Wolkowicz and Styan [43, Theorem 2.1]) Let $\\pmb{A}$ bea symmetric $n\\times n$ matrixwith eigenvalues $\\lambda_{1}(A)\\leq...\\leq\\lambda_{n}(A)$ Then ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\lambda_{\\operatorname*{max}}(A)\\leq{\\frac{\\operatorname{Tr}(A)}{n}}+{\\sqrt{(n-1)\\left[{\\frac{\\operatorname{Tr}(A^{\\top}A)}{n}}-\\left({\\frac{\\operatorname{Tr}(A)}{n}}\\right)^{2}\\right]}}\\,.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Our abstract and introduction clear state the main claims of our paper. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 29}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: We clearly state the problem setting and the assumptions therein. We clearly state which results are optimal up to logarithmic factors. We discuss the computational complexity of our newly proposed algorithm at the end of Section 4.1. Attaining the tightest matching logarithmic dependencies in our lower bound is still an open problem, as mentioned in the conclusion. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should refect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational effciency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 29}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: All the theorems provide the full set of assumptions. All of our main results are proven explicitly either in the main text or in the appendix. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 30}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: This is a theoretical paper that studies theoretical guarantees for online learning algorithms. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b)  If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 30}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: This is a theoretical paper that studies theoretical guarantees for online learning algorithms. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 31}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: This is a theoretical paper that studies theoretical guarantees for online learning algorithms. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 31}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: This is a theoretical paper that studies theoretical guarantees for online learning algorithms. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 32}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: This is a theoretical paper that studies theoretical guarantees for online learning algorithms. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 32}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Given the theoretical nature of this work, there are no ethical concerns to be addressed. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 32}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: This is a theoretical paper that studies theoretical guarantees for online learning algorithms. As stated in the guidelines, this is foundational research and it is not tied to particular applications, let alone deployments. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 33}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: No models nor data is associated to this paper. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 33}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: No assets were used in this paper. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 34}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 34}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 34}, {"type": "text", "text": "\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 35}]