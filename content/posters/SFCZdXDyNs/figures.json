[{"figure_path": "SFCZdXDyNs/figures/figures_1_1.jpg", "caption": "Figure 1: An example where a CLIP [Radford et al., 2021] model trained on a 40M subset of a Shutterstock data set exhibits d\u00e9j\u00e0 vu memorization of objects present in a training image. Public set is a separate collection of 20M images from Shutterstock that has no overlap with the training set. The objects annotated in orange are true positives, i.e., the ones present in the target image, and the objects annotated in blue are false positives. Our test recovers significantly more memorized objects for the target VLM (trained on the target image) compared to the reference VLM (not trained on the target image).", "description": "This figure shows an example of d\u00e9j\u00e0 vu memorization using a CLIP model. Two models were trained; one with the target image and one without.  Given a target image and caption, both models were used to retrieve relevant images from a public dataset. The model trained with the target image showed significantly better recall of objects from the target image compared to the model not trained with the target image. This demonstrates that the model remembers specific details from the training images beyond what can be inferred from the caption alone.", "section": "3 D\u00e9j\u00e0 vu Memorization for Vision-Language Models"}, {"figure_path": "SFCZdXDyNs/figures/figures_5_1.jpg", "caption": "Figure 2: Utility and d\u00e9j\u00e0 vu memorization of ViT-B-32 CLIP models with varying training set sizes. Model utility is quantified in terms of ImageNet zero-shot accuracy. Population-level memorization of models is measured using the metrics defined in Section 3.2 over various public sets (a): training set sampled from filtered LAION and ImageNet is used as public set. (b): training set sampled from filtered LAION and a holdout filtered LAION-50M set is used as public set. (c): training set sampled from Shutterstock and a holdout SS-20M set is used as public set. For the memorization metrics, we report the mean \u00b1 std values (std \u2264 0.003) over 100 repetitions of randomly sampling 10% of records with replacement.", "description": "This figure shows the relationship between the training data size, model utility (ImageNet zero-shot accuracy), and the degree of memorization (PPG, PRG, AUCG).  It demonstrates that even with good generalization performance (high zero-shot accuracy), memorization remains significant, particularly for smaller training set sizes.  Three different datasets and public sets are compared to showcase the consistency of the findings.", "section": "4 Evaluating D\u00e9j\u00e0 vu Memorization"}, {"figure_path": "SFCZdXDyNs/figures/figures_5_2.jpg", "caption": "Figure 2: Utility and d\u00e9j\u00e0 vu memorization of ViT-B-32 CLIP models with varying training set sizes. Model utility is quantified in terms of ImageNet zero-shot accuracy. Population-level memorization of models is measured using the metrics defined in Section 3.2 over various public sets (a): training set sampled from filtered LAION and ImageNet is used as public set. (b): training set sampled from filtered LAION and a holdout filtered LAION-50M set is used as public set. (c): training set sampled from Shutterstock and a holdout SS-20M set is used as public set. For the memorization metrics, we report the mean \u00b1 std values (std \u2264 0.003) over 100 repetitions of randomly sampling 10% of records with replacement.", "description": "This figure shows the impact of training set size on both the model's utility (ImageNet zero-shot accuracy) and the degree of memorization (measured using PPG, PRG, and AUCG). Three different datasets were used: filtered LAION with ImageNet as the public set, filtered LAION with a holdout LAION-50M set as the public set, and Shutterstock with a holdout SS-20M set as the public set.  The results indicate a trade-off between model utility and memorization; larger training sets lead to better generalization but don't eliminate memorization entirely.", "section": "4 Evaluating D\u00e9j\u00e0 vu Memorization"}, {"figure_path": "SFCZdXDyNs/figures/figures_6_1.jpg", "caption": "Figure 4: Sample-level memorization gap between target and reference models when predicting top-10 objects for different top-L records. Models are trained on disjoint 10M subsets of filtered LAION data set for 200 epochs and ImageNet public set is used for the KNN test. The model exhibits very strong d\u00e9j\u00e0 vu memorization on a small subset of samples, as indicated by the large precision/recall/F-score gaps when L is small.", "description": "This figure shows sample-level memorization results.  Two plots are shown, one where records are sorted by minimum embedding distance between target captions and public images, and another where records are sorted by the decreasing number of correct object predictions for the target model. Each plot shows precision, recall, and F-score gaps between the target and reference models for different top-L records (L=1, 5, 10). The large gaps for small L (especially top-1) indicate that the model exhibits strong d\u00e9j\u00e0 vu memorization on a small subset of training data points.", "section": "4.2 Measuring Sample-Level Memorization"}, {"figure_path": "SFCZdXDyNs/figures/figures_7_1.jpg", "caption": "Figure 5: Effect of mitigation on ViT-B-32 OpenCLIP models trained on 10M subset of filtered LAION. Memorization evaluation is done using ImageNet as public set. Default setting is highlighted with asterisk. For the memorization metrics, we report the mean \u00b1 std values (std \u2264 0.003) over 100 repetitions of randomly sampling 10% of records with replacement. Among these mitigations, text masking has the best trade-off that reduces memorization without sacrificing utility.", "description": "This figure shows the effect of different mitigation techniques on memorization in ViT-B-32 OpenCLIP models trained on a 10M subset of the filtered LAION dataset.  The techniques evaluated are early stopping, temperature scaling, weight decay, and text masking.  The results are evaluated using ImageNet as a public set and reported as the mean \u00b1 std (standard deviation) of memorization metrics (PPG, PRG, AUCG) over 100 random sampling trials.  The default settings are marked with asterisks.  The graph shows that text masking offers the best compromise, reducing memorization significantly without substantially impacting model utility.", "section": "5 Mitigation"}, {"figure_path": "SFCZdXDyNs/figures/figures_15_1.jpg", "caption": "Figure 6: Comparing images from ImageNet and COCO data sets. The ImageNet images only have single label per image but COCO images have complex scenes with multiple object labels. Additionally, COCO images have accompanying text captions. Label annotations with bounding boxes are highlighted in blue for both the images.", "description": "This figure compares example images from the ImageNet and COCO datasets to highlight the difference in their complexity and labeling. ImageNet images typically have a single label, while COCO images often depict complex scenes with multiple objects and detailed captions.", "section": "4 Evaluating D\u00e9j\u00e0 vu Memorization"}, {"figure_path": "SFCZdXDyNs/figures/figures_16_1.jpg", "caption": "Figure 2: Utility and d\u00e9j\u00e0 vu memorization of ViT-B-32 CLIP models with varying training set sizes. Model utility is quantified in terms of ImageNet zero-shot accuracy. Population-level memorization of models is measured using the metrics defined in Section 3.2 over various public sets (a): training set sampled from filtered LAION and ImageNet is used as public set. (b): training set sampled from filtered LAION and a holdout filtered LAION-50M set is used as public set. (c): training set sampled from Shutterstock and a holdout SS-20M set is used as public set. For the memorization metrics, we report the mean \u00b1 std values (std \u2264 0.003) over 100 repetitions of randomly sampling 10% of records with replacement.", "description": "This figure displays the impact of training set size on both the utility and memorization level of ViT-B-32 CLIP models.  The utility is measured by ImageNet zero-shot accuracy.  Memorization is assessed using population-level metrics (PPG, PRG, AUCG) calculated across three different public image sets, each with varying degrees of overlap with the training data. The results show a decrease in memorization with increasing training set sizes, indicating improved generalization.", "section": "4 Evaluating D\u00e9j\u00e0 vu Memorization"}, {"figure_path": "SFCZdXDyNs/figures/figures_17_1.jpg", "caption": "Figure 4: Sample-level memorization gap between target and reference models when predicting top-10 objects for different top-L records. Models are trained on disjoint 10M subsets of filtered LAION data set for 200 epochs and ImageNet public set is used for the KNN test. The model exhibits very strong d\u00e9j\u00e0 vu memorization on a small subset of samples, as indicated by the large precision/recall/F-score gaps when L is small.", "description": "This figure shows the sample-level memorization gap between a target model (trained on the target image-text pair) and a reference model (not trained on the target image-text pair).  The results are shown for different numbers of nearest neighbors (Top-1, Top-5, Top-10).  The x-axis represents the number of top-L records considered, and the y-axis represents the gap in precision, recall, and F1-score between the target and reference models.  The figure demonstrates that the memorization gap is much larger for a small subset of training samples, indicating stronger memorization for these specific samples.", "section": "Measuring Sample-Level Memorization"}, {"figure_path": "SFCZdXDyNs/figures/figures_17_2.jpg", "caption": "Figure 5: Effect of mitigation on ViT-B-32 OpenCLIP models trained on 10M subset of filtered LAION. Memorization evaluation is done using ImageNet as public set. Default setting is highlighted with asterisk. For the memorization metrics, we report the mean \u00b1 std values (std \u2264 0.003) over 100 repetitions of randomly sampling 10% of records with replacement. Among these mitigations, text masking has the best trade-off that reduces memorization without sacrificing utility.", "description": "This figure shows the impact of four different mitigation techniques on a ViT-B-32 OpenCLIP model trained on a 10M subset of the filtered LAION dataset. The mitigation techniques are early stopping, temperature scaling, weight decay, and text masking.  The model's performance is evaluated using ImageNet zero-shot accuracy and three memorization metrics: population precision gap (PPG), population recall gap (PRG), and AUC gap (AUCG). The results show that text masking provides the best trade-off between reducing memorization and maintaining model utility.", "section": "5 Mitigation"}, {"figure_path": "SFCZdXDyNs/figures/figures_18_1.jpg", "caption": "Figure 4: Sample-level memorization gap between target and reference models when predicting top-10 objects for different top-L records. Models are trained on disjoint 10M subsets of filtered LAION data set for 200 epochs and ImageNet public set is used for the KNN test. The model exhibits very strong d\u00e9j\u00e0 vu memorization on a small subset of samples, as indicated by the large precision/recall/F-score gaps when L is small.", "description": "This figure shows the sample-level memorization results.  The x-axis represents the top-L records (sorted by minimum embedding distance or decreasing number of correct predictions), and the y-axis shows the gap in precision, recall, and F1-score between target and reference models for the top 10 objects.  The figure demonstrates that strong memorization is present in a small subset of samples, indicated by large gaps, especially when L is small (meaning only the closest or best-predicted records are considered).", "section": "Measuring Sample-Level Memorization"}, {"figure_path": "SFCZdXDyNs/figures/figures_18_2.jpg", "caption": "Figure 4: Sample-level memorization gap between target and reference models when predicting top-10 objects for different top-L records. Models are trained on disjoint 10M subsets of filtered LAION data set for 200 epochs and ImageNet public set is used for the KNN test. The model exhibits very strong d\u00e9j\u00e0 vu memorization on a small subset of samples, as indicated by the large precision/recall/F-score gaps when L is small.", "description": "This figure visualizes the sample-level memorization gap between a target model (trained with the specific image-text pair) and a reference model (not trained with that pair).  It shows the precision, recall, and F1-score gaps when predicting the top 10 objects in a target image, based on its caption.  The x-axis represents different numbers of nearest neighbors (NNs) considered (Top-L Records), and the y-axis shows the gap between the target and reference models' metrics.  The figure reveals that the memorization gap is significantly higher for a smaller subset of samples, indicating a disproportionate memorization effect by the model on certain samples.", "section": "Measuring Sample-Level Memorization"}, {"figure_path": "SFCZdXDyNs/figures/figures_19_1.jpg", "caption": "Figure 1: An example where a CLIP [Radford et al., 2021] model trained on a 40M subset of a Shutterstock data set exhibits d\u00e9j\u00e0 vu memorization of objects present in a training image. Public set is a separate collection of 20M images from Shutterstock that has no overlap with the training set. The objects annotated in orange are true positives, i.e., the ones present in the target image, and the objects annotated in blue are false positives. Our test recovers significantly more memorized objects for the target VLM (trained on the target image) compared to the reference VLM (not trained on the target image).", "description": "This figure shows an example of how a CLIP model, trained on a subset of the Shutterstock dataset, memorizes objects from a training image.  A target image and its caption are input to the model, which retrieves similar images from a separate, non-overlapping public dataset. The model trained on the target image (target VLM) retrieves images with significantly more of the target image's objects than a model not trained on the target image (reference VLM).  Objects correctly identified are highlighted in orange (true positives), while incorrectly identified objects are in blue (false positives). This demonstrates the model's memorization of specific training images.", "section": "1 Introduction"}, {"figure_path": "SFCZdXDyNs/figures/figures_20_1.jpg", "caption": "Figure 1: An example where a CLIP [Radford et al., 2021] model trained on a 40M subset of a Shutterstock data set exhibits d\u00e9j\u00e0 vu memorization of objects present in a training image. Public set is a separate collection of 20M images from Shutterstock that has no overlap with the training set. The objects annotated in orange are true positives, i.e., the ones present in the target image, and the objects annotated in blue are false positives. Our test recovers significantly more memorized objects for the target VLM (trained on the target image) compared to the reference VLM (not trained on the target image).", "description": "This figure demonstrates the concept of d\u00e9j\u00e0 vu memorization using a CLIP model.  A target image and its caption are input to the model. The model then retrieves similar images from a separate, public dataset. The figure shows that a model trained on the target image (target VLM) retrieves images containing significantly more objects from the target image (true positives) than a model not trained on it (reference VLM).  The objects correctly identified are highlighted in orange, and the incorrectly identified objects are in blue. This difference highlights how the model memorizes details from its training data, even if it's not explicitly shown in the image caption.", "section": "1 Introduction"}]