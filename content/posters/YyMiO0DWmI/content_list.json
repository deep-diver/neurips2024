[{"type": "text", "text": "Cross-Device Collaborative Test-Time Adaptation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Guohao $\\mathbf{Chen}^{1\\,2*}$ Shuaicheng $\\mathbf{Niu}^{3*}$ Deyu Chen1 Shuhai Zhang1 2 Changsheng $\\mathbf{Li^{4\\dagger}}$ Yuanqing $\\mathbf{Li}^{1\\,2}$ Mingkui Tan1 2\u2020 ", "page_idx": 0}, {"type": "text", "text": "1South China University of Technology, 2Pazhou Laboratory, 3Nanyang Technological University, 4Beijing Institute of Technology, ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In this paper, we propose test-time Collaborative Lifelong Adaptation (CoLA), which is a general paradigm that can be incorporated with existing advanced TTA methods to boost the adaptation performance and efficiency in a multi-device collaborative manner. Specifically, we maintain and store a set of device-shared domain knowledge vectors, which accumulates the knowledge learned from all devices during their lifelong adaptation process. Based on this, CoLA conducts two collaboration strategies for devices with different computational resources and latency demands. 1) Knowledge reprogramming learning strategy jointly learns new domain-specific model parameters and a reweighting term to reprogram existing shared domain knowledge vectors, termed adaptation on principal agents. 2) Similarity-based knowledge aggregation strategy solely aggregates the knowledge stored in shared domain vectors according to domain similarities in an optimizationfree manner, termed adaptation on follower agents. Experiments verify that CoLA is simple but effective, which boosts the efficiency of TTA and demonstrates remarkable superiority in collaborative, lifelong, and single-domain TTA scenarios, e.g., on follower agents, we enhance accuracy by over $30\\%$ on ImageNet-C while maintaining nearly the same efficiency as standard inference. The source code is available at https://github.com/Cascol-Chen/COLA. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The conventional pipeline of deep learning typically trains a model and deploys it across numerous devices with frozen parameters. This pipeline has demonstrated great success in various applications, such as autonomous driving cars [35, 14], embodied robots [18], and many other smart devices [28, 29]. However, during deployment, the model on each device may encounter test samples drawn from a domain different from the training one. In some cases, the testing environment even changes continuously and periodically, such as changes in weather. Unfortunately, the deep model often struggles to generalize to unseen testing domains and its performance may degrade significantly. ", "page_idx": 0}, {"type": "text", "text": "To resolve domain shifts, test-time adaptation (TTA) [51, 21, 54, 64, 2, 7, 59, 26, 39, 25, 52, 37] has emerged as a promising research field. TTA updates a given model w.r.t. a testing sample using self-/unsupervised objectives, such as rotation prediction [13], contrastive learning [2, 30, 50], entropy minimization [54, 64, 25, 37], etc. Compared to conventional domain adaptation [31, 45, 24] or fine-tuning [61, 33] methods that require performing offilne model learning on the whole pre-collected target dataset, TTA distinguishes itself with minimal overhead by utilizing each test sample only once for immediate post-inference adaptation. This renders TTA more adaptable in real-world applications. ", "page_idx": 0}, {"type": "text", "text": "However, prior TTA methods mainly validate their effectiveness on a single device, i.e., re-adapting the model from scratch on each. In practice, models are often deployed across multiple devices. ", "page_idx": 0}, {"type": "image", "img_path": "YyMiO0DWmI/tmp/be2d5a1682ce7ce8fb0ae45e1b7ee6b92e56823fe5d3061b5975037ea39b0cb2.jpg", "img_caption": ["(a) Prior single-device TTA ", "(b) Our collaborative TTA "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Comparison w.r.t. (a) prior single-device TTA vs. (b) our collaborative TTA. Prior TTA operates on each device independently and may be infeasible in resource-limited devices. In contrast, our collaborative TTA allows devices to share knowledge. Based on this, on different devices, one can choose to solely aggregate the shared knowledge for TTA (Follower Agents), or further conduct backpropagation for knowledge aggregation and new domain knowledge learning (Principal Agents). ", "page_idx": 1}, {"type": "text", "text": "As shown in Figure 1, in the multi-device adaptation scenario, single-device adaptation methods suffer from the following limitations. First, single-device TTA neglects useful knowledge learned from other devices and adapts independently. Since different devices may frequently encounter similar or even identical testing domains, ignoring this shared knowledge often leads to suboptimal adaptation performance, as demonstrated in Table 2. Second, due to limited resources or latency demands, some devices may not support the backpropagation operation required by learning-based TTA methods [54, 38], rendering single-device TTA infeasible. Third, even on a single device, models may also encounter dynamically and periodically changing domain shifts. Although recent works have proposed continual TTA to mitigate the catastrophic forgetting issue, such as antiforgetting regularizer [38] or restoration schemes [56], these methods still struggle to accumulate previously learned knowledge over a long-term adaptation process, as shown in Table 1. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we propose a test-time Collaborative Lifelong Adaptation (CoLA) method to enable knowledge accumulation, sharing, and exploitation across devices. Our approach exploits both the previously learned knowledge from other devices and the device itself to achieve efficient and collaborative TTA. Specifically, we represent the knowledge learned on each domain of each device by a domain vector, and automatically detect domain changes on each device during its continual adaptation process. These domain vectors are stored and shared across devices upon domain changes for collaborative TTA and catastrophic forgetting mitigation. Based on the shared domain vectors, we first introduce a knowledge reprogramming learning method for Principal Agents, e.g., the resourceabundant devices, where we enhance TTA performance and efficiency by leveraging available shared knowledge while learning new domain-specific parameters in case existing knowledge is insufficient. The newly learned parameters/knowledge are subsequently stored for shared domain vector set updating. Furthermore, we devise an optimization-free collaborative TTA method, to reduce the computation consumption of TTA and thus enable TTA in latency-sensitive scenarios or resourcelimited devices, which we term as Follower Agents in CoLA. We achieve this by directly aggregating the domain knowledge shared by principal agents according to domain similarities. ", "page_idx": 1}, {"type": "text", "text": "Main novelty and contributions. 1) We introduce a novel and practical collaborative lifelong adaptation paradigm to TTA. This paradigm addresses a practical demand in real-world applications to perform effective adaptation on numerous devices with varying resources and latency requirements simultaneously, meanwhile keeping privacy preserved and communication efficient. 2) We devise domain vectors to explicitly store the domain knowledge and share them across devices for collaborative TTA. Based on this, we devise two collaborative strategies, i.e., knowledge reprogramming learning for resource-abundant principal agents and similarity-based knowledge aggregation for resource-limited or latency-sensitive follower agents. 3) Extensive experiments demonstrate the superiority of our CoLA regarding the scenarios of collaborative, lifelong, and single-domain TTA in a plug-and-play manner. By leveraging available shared knowledge, on principal agents, we achieve an up to $78.0\\times$ speed up on ETA compared with the baseline without collaborative learning on ImageNet-C. On follower agents, we enhance the accuracy by over $30\\%$ while maintaining nearly the same computation and memory efficiency as standard inference on ImageNet-C. ", "page_idx": 1}, {"type": "image", "img_path": "YyMiO0DWmI/tmp/6f969bc16d9a12d2f34c526914ef8f08c6d2c9cbf350d472662a94363511573e.jpg", "img_caption": ["Figure 2: An illustration of our proposed CoLA. We maintain a shared domain vector set $\\tau$ to explicitly store the knowledge learned by each principal agent during adaptation. Based on $\\tau$ , for Principal Agents, we jointly learn the domain-specific parameters $\\Delta\\pmb{\\theta}$ and the reweighting term $_{\\alpha}$ via backward propagation, where the learned knowledge is then stored in $\\tau$ . For Follower Agents, we adaptively aggregate the shared knowledge in $\\tau$ in a forward-only manner, based on the domain similarities, which prioritizes knowledge derived from domains that are similar to the testing domain. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "2 Problem statement and motivation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Let $f_{\\theta}(\\cdot)$ be the model trained on a labeled dataset $\\mathcal{D}_{t r a i n}\\,=\\,\\{(\\mathbf{x}_{i},y_{i})\\}$ and $\\mathbf{x}_{i}\\sim P\\left(\\mathbf{x}\\right)$ . After training, $f_{\\theta}(\\cdot)$ is often deployed on various devices, on each device $f_{\\theta}(\\cdot)$ may encounter test samples drawn from a shifted and dynamically changing domain distribution $Q(\\mathbf{x})$ , where $Q(\\mathbf{x})\\neq\\bar{P(\\mathbf{x})}$ . Under such domain shifts, deep models are often very sensitive and suffer from severe performance degradation. To address this, on each device, one can adapt $f_{\\theta}(\\cdot)$ to $\\mathbf{x}$ by optimizing some self/unsupervised learning objective at test time: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\pmb{\\theta}_{l}}\\mathcal{L}(\\mathbf{x};\\pmb{\\theta}_{f},\\pmb{\\theta}_{l}),\\ \\ \\mathbf{x}\\sim Q(\\mathbf{x}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\theta_{f}$ and $\\theta_{l}$ denote frozen and learnable model parameters, respectively. ", "page_idx": 2}, {"type": "text", "text": "Motivation. Eqn. (1) is known as a single-device test-time adaptation (TTA) method, which naively re-adapts $f_{\\theta}(\\cdot)$ from scratch on each device. In our multi-device adaptation scenario, this independent adaptation manner neglects the valuable knowledge learned from other devices and often obtains limited performance, as in Figure 3(a). Therefore, there is an urgent demand to devise multi-device collaborative TTA methods, to enhance the adaptation performance and efficiency. To this end, the key technical challenge lies in devising a collaboration scheme that effectively exploits the knowledge from other devices while ensuring privacy preservation and communication efficiency. ", "page_idx": 2}, {"type": "text", "text": "Domain knowledge vectors. In real-world scenarios, a model is often deployed in environments that may change continuously and cyclically, e.g., day $\\rightarrow\\mathrm{night}\\rightarrow\\mathrm{day}.$ . Moreover, the model deployed on different devices may encounter similar environments, experiencing similar domain shifts. In such cases, when a device encounters test samples drawn from a domain previously seen by itself or by other devices, it is unnecessary to conduct adaptation from scratch. Instead, leveraging the previously acquired knowledge can achieve enhanced adaptation. Inspired by this, we seek to explicitly store the knowledge learned on each domain of each device, and then exploit this knowledge for collaborative TTA. We term this knowledge as domain vectors and introduce its definition below. ", "page_idx": 2}, {"type": "text", "text": "Formally, given $m$ devices with each having $n$ domains, we use a domain vector $\\Delta\\pmb{\\theta}^{i,j}{=}\\pmb{\\theta}_{l}^{i,j}-\\pmb{\\theta}_{l}^{o}$ to denote the knowledge learned on the $i$ -th domain of the $j$ -th device. Here, $\\theta_{l}^{i,j}$ denotes the learned parameters on the $i$ -th domain of the $j$ -th device, and $\\theta_{l}^{o}$ denotes the corresponding original learnable parameters. For privacy and efficiency considerations, we select the affine parameters of the norm layers as learnable parameters $\\theta_{l}$ and transmit domain vectors between devices for knowledge sharing, which consumes negligible extra cost as in Table 5. We store each knowledge vector $\\Delta\\pmb{\\theta}^{i,j}$ in a set ", "page_idx": 2}, {"type": "text", "text": "Input: Test samples {Dm}mM=1, where $\\mathcal{D}^{m}{=}\\{\\mathbf{x}_{t}^{m}\\}_{t=1}^{T}$ denotes the batches of test samples on the   \n$m$ -th device, the model $f_{\\theta}(\\cdot)$ and its stem (first) layer $f_{\\pmb{\\theta}_{s}}(\\cdot)$ , threshold $z$ .   \n1: Initialize: shared domain vectors $\\mathbf{\\mathcal{T}=}\\{\\mathbf{0}\\}$ , $\\phi_{d}{=}\\mathbf{0}$ for each device.   \n2: for $t=1,2,\\ldots,T$ do   \n3: for each device (in parallel) do   \n4: Calculate batch statistics $\\hat{\\phi}_{t}$ over $f_{\\theta_{s}}(\\mathbf{x}_{t}^{m})$ ;   \n5: Update distribution statistics $\\phi_{d}$ by Eqn. (6);   \n6: For Principal Agent: // knowledge reprogramming learning, Sect. 3.1   \n7: if $D(\\phi_{d},\\hat{\\phi}_{t})>z$ then \u25b7domain change detection, Eqn. (7)   \n8: Update domain vectors $\\tau$ by storing the newly learned $\\Delta\\pmb{\\theta}$ and reset $\\phi_{d}=\\mathbf{0}$ .   \n9: end if   \n10: Predict $\\hat{y}_{t}^{m}$ by $f_{\\theta}(\\mathbf{x}_{t}^{m})$ based on Eqn. (2);   \n11: Update $_{\\alpha}$ and $\\Delta\\pmb{\\theta}$ via Eqn. (2) with backpropagation;   \n12: For Follower Agent: // similarity-based aggregation, Sect. 3.2   \n13: Update $\\rho_{i}$ for different domain knowledge via Eqn. (5);   \n14: Predict $\\hat{y}_{t}^{m}$ by $f_{\\theta}(\\mathbf{x}_{t}^{m})$ based on Eqn. (3);   \n15: end for   \n16: end for   \nOutput: Predictions $\\{\\hat{y}_{t}^{m}\\mid t=1,...,T$ and $m=1,...,M\\}$ . ", "page_idx": 3}, {"type": "text", "text": "$\\mathcal{T}\\!=\\!\\{\\Delta\\pmb{\\theta}^{i,j}\\}_{i=1,j=1}^{n,m}$ . During the continual adaptation process, we dynamically expand $\\tau$ by storing a new $\\Delta\\pmb{\\theta}^{i,j}$ in $\\tau$ once $\\Delta\\pmb{\\theta}^{i,j}$ is learned. For simplicity of presentation, we omit $\\tau$ as $\\{\\Delta\\pmb{\\theta}_{i}\\}_{i=1}^{N}$ where $N{=}m n$ and exploit $\\tau$ to devise collaborative TTA strategies in the following sections. ", "page_idx": 3}, {"type": "text", "text": "3 Cross-device collaborative test-time adaptation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this paper, we propose a test-time Collaborative Lifelong Adaptation (CoLA) method. In CoLA, we seek to conduct collaborative TTA across multiple devices by exploiting a set of shared domain vectors, termed $\\tau$ . We automatically detect the domain changes on each device during a continual adaptation process and explicitly store the current domain knowledge in $\\tau$ once the testing domain is changed (c.f. Sect. 3.3). Then, based on $\\tau$ , we develop two distinct collaboration strategies. In practice, users can determine which strategy to use according to the computational resource of their device or their latency requirements. First, the collaborative knowledge reprogramming learning strategy (c.f. Sect. 3.1) is designed for \u201cprincipal agents\", i.e., the devices that will dominate the learning of new knowledge and have sufficient resources for backpropagation-based model updates. This strategy jointly learns new domain-specific model parameters and a reweighting term to reprogram the knowledge learned from previously encountered distributions from both the device itself and other devices, through backpropagation-based optimization. Second, the optimization-free collaborative TTA (c.f. Sect. 3.2) is designed for \u201cfollower agents\", i.e., the devices that are resourcelimited or latency-sensitive. This strategy mainly exploits the knowledge shared from principal agents, by aggregating the valuable shared knowledge according to distribution similarities. We summarize the pseudo-code in Algorithm 1 and illustrate the overall pipeline of CoLA in Figure 2. ", "page_idx": 3}, {"type": "text", "text": "3.1 Collaborative test-time adaptation via knowledge reprograming learning ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We conduct collaborative adaptation with both the knowledge learned from other devices and the device itself. This latter one is particularly advantageous for lifelong adaptation since, in practice, a deployed model may encounter diverse and evolving domains. By storing and reprogramming all knowledge learned from previously encountered domains, we naturally mitigate the issue of catastrophic forgetting during lifelong TTA (see results and analysis in Table 1). To this end, on each device, we detect domain changes and store the learned model parameters for each specific domain once the test domain changes. We then reprogram the knowledge stored in these model parameters by reweighting as below, facilitating seamless adaptation to changing environments. ", "page_idx": 3}, {"type": "text", "text": "As aforementioned, we assume there exist $N$ sets of parameters, i.e., domain vectors, learned from previously encountered domains across all devices, denoted as $\\pmb{\\mathcal{T}}{=}\\{\\Delta\\pmb{\\theta}_{i}\\}_{i=1}^{N}$ , where $\\Delta\\pmb{\\theta}_{i}{=}\\pmb{\\theta}_{i}\\mathrm{~-~}$ $\\theta_{l}^{o}$ . For flexibility, we denote null knowledge as $\\Delta\\pmb{\\theta}_{0}{=}\\mathbf{0}$ and include it in $\\tau$ . We use $\\tau$ to illustrate our collaborative learning scheme here and put details for detecting and storing each $\\Delta\\pmb{\\theta}_{i}$ in Sect. 3.3. Based on $\\tau$ , we learn a reweighting term that adaptively aggregates shared knowledge via backpropagation, while learning new knowledge simultaneously if existing knowledge is insufficient. The overall optimization problem is given by: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\alpha,\\Delta\\theta}\\mathcal{L}(\\mathbf{x};\\,\\theta_{f},\\theta_{l}),\\;\\;\\mathrm{where}\\;\\theta_{l}=\\theta_{l}^{o}+\\sum_{i=0}^{N}\\alpha_{i}\\Delta\\theta_{i}+\\Delta\\theta\\;\\;\\mathrm{and}\\;\\Delta\\theta_{i}\\in\\mathcal{T}=\\{\\Delta\\theta_{i}\\}_{i=0}^{N}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here, $\\Delta\\pmb{\\theta}$ denotes learnable parameters for current round of adaptation, and $_{\\alpha}$ denotes the normalized weights for different domain knowledge, i.e., $\\textstyle\\sum_{i}\\alpha_{i}=1$ . Thus, knowledge reprogramming and new knowledge learning are decoupled into the optimization of $_{\\alpha}$ and $\\Delta\\pmb{\\theta}$ . Note that we introduce $\\Delta\\pmb{\\theta}_{0}$ to ensure more flexibility in reprogramming, e.g., by setting $\\alpha_{0}=1$ , one can entirely disregard previously learned knowledge when it is not beneficial for adapting to currently encountered domain. Moreover, when no knowledge has been accumulated, i.e., $\\boldsymbol{\\mathcal{T}}=\\{\\mathbf{0}\\}$ , Eqn. (2) simplifies to the conventional TTA. We put details of the initialization for $_{\\alpha}$ and $\\tau$ in Appendix B due to page limits. ", "page_idx": 4}, {"type": "text", "text": "Adaptive temperature scaling for fast adaptation. Promptly re-weighting the appropriate knowledge for aggregation is key to fast adaptation when the testing distribution suddenly changes. However, this process is hindered when the logits of $_{\\alpha}$ are sharpened, making re-weighting difficult to favor another knowledge. To mitigate this, we further introduce an adaptive scaling temperature during the optimization phase, which helps adjust the sharpness of $_{\\alpha}$ adaptively while maintaining the smoothness of the original logits. The calculation of $_{\\alpha}$ can be thus expressed as $\\alpha=\\mathrm{softmax}(\\beta\\!\\cdot\\!T_{l})$ , where $\\beta$ is the logit vector and $T_{l}$ is a learnable temperature. ", "page_idx": 4}, {"type": "text", "text": "3.2 Collaborative test-time adaptation via similarity-based knowledge aggregation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The computational constraints of devices, combined with the real-time demands of various applications, often necessitate TTA to be as efficient as possible. To this end, we propose a forward-only collaborative TTA strategy for devices operating in the follower agent mode. Here, the follower agent adapts a given model by aggregating the previously learned knowledge and the knowledge shared from principal agents without learning new domain-specific parameters, aiming to maintain almost the same efficiency as pure inference. Formally, given domain vectors $\\scriptstyle\\mathcal{T}=\\{\\Delta\\theta_{i}\\ \\}_{i=0}^{N}$ shared from $m$ principal resource-abundant devices with each having $n$ encountered domains and $\\Delta\\pmb{\\theta}_{0}=\\mathbf{0}$ , the goal of adaptation is to find appropriate normalized weights $\\gamma^{*}{\\in}\\mathbb{R}^{N+1}$ such that: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\gamma^{*}=\\underset{\\gamma}{\\arg\\operatorname*{min}}\\,\\mathcal{L}(\\mathbf{x};\\,\\theta_{f},\\theta_{l}),\\;\\;\\mathrm{where}\\;\\theta_{l}=\\theta_{l}^{o}+\\sum_{i=0}^{N}\\gamma_{i}\\Delta\\theta_{i}\\;\\;\\mathrm{and}\\;\\Delta\\theta_{i}\\in\\mathcal{T}=\\{\\Delta\\theta_{i}\\}_{i=0}^{N}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here, since backpropagation-based learning is not supported, we directly assign the specific value of each $\\gamma_{i}$ approximately according to distribution similarities. We estimate the distribution by calculating the feature statistics, i.e., the mean and standard deviation of the features from the first stem layer. Formally, let $\\phi_{d}$ denote the statistics of the current distribution that is online estimated via Eqn. (6), and $\\phi_{i}$ be the distribution statistics on which $\\Delta\\pmb{\\theta}_{i}$ is adapted (i.e., the corresponding $\\phi_{d}$ while learning $\\Delta\\pmb{\\theta}_{i}$ ), we re-weight the knowledge from different distributions by: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\gamma=\\mathrm{softmax}(\\rho),\\quad\\mathrm{where}~\\,\\rho_{i}=1\\,/\\,\\left(D(\\phi_{d},\\phi_{i})+\\epsilon\\right)\\!.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here, $\\rho$ is a logit vector, $D(\\cdot,\\cdot)$ is a distance for which we adopt KL divergence as in Eqn. (7), and $\\epsilon$ is a small constant for numerical stability. In this way, we can adaptively prioritize shared knowledge learned from distributions that are similar to the current distribution. Note that a principal agent with abundant resources may also leverage Eqn. (3) for efficient TTA in real-time scenarios. ", "page_idx": 4}, {"type": "text", "text": "Exploiting diverse knowledge for aggregation. Aggregating the advantages of different knowledge is the key to achieving satisfying robustness under various distribution shifts, as shown in Figure 3 (b). However, when distributions are highly similar, the re-weighting logit $\\rho_{i}$ shall become sufficiently large (e.g., $\\rho_{i}>10_{.}$ ) and the softmax function tends to simplify to the max function, which hinders the potential of Eqn. (3) from aggregating a diverse set of knowledge. To alleviate this, we further introduce a pre-defined temperature scaling factor $T_{f}$ to soften $\\rho_{i}$ , thereby encouraging the aggregation of more existing knowledge. Then, $\\rho_{i}$ is re-defined as: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\rho_{i}=1\\,/\\,\\left(T_{f}\\cdot D(\\phi_{d},\\phi_{i})+\\epsilon\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Remark. It is worth noting that our CoLA can be incorporated with existing TTA techniques as a plug-and-play module for a more effective solution (as in Table 1 and Table 3). Furthermore, unlike prior methods [4] that impose intensive transmission of both data and model weights, CoLA offers several merits for real-world implementation: 1) CoLA involves only the transmission of learned parameters $\\Delta\\pmb{\\theta}_{i}$ , which preserves user privacy and imposes much less communication burden (e.g., the affine parameters of the norm layers in ViT-Base [8], which are typically updated in TTA [54, 38], occupy only $0.15\\;\\mathrm{MB})$ ). 2) the domain vectors are preserved and shared intermittently with a shift detector, which further reduces the communication burden by a considerable margin. 3) CoLA is decentralized and flexible, which allows all agents to join or leave the collaboration at any time. ", "page_idx": 5}, {"type": "text", "text": "3.3 Automatic domain shift detection for constructing domain knowledge vectors $\\tau$ ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we introduce the construction of the domain knowledge vectors $\\tau$ shared across multiple devices. As aforementioned, explicitly accumulating/storing the learned knowledge from each domain of each device in $\\tau$ is a key step in our CoLA for collaborative learning. However, in practice, during the lifelong adaptation process of each device, we do not have any prior information on the domain labels regarding a given test sample stream. To conquer this, we devise an efficient distribution shift detector to identify whether the test distribution changes, and then automatically store the currently learned model weights to the domain vector set $\\tau$ once the domain change is detected. We achieve this by measuring the discrepancy between the distribution\u2019s statistics $\\phi_{d}$ and the statistics of the current test batch $\\hat{\\phi}_{t}$ . Formally, let $f_{\\theta_{s}}(\\cdot)$ be the stem layer of $f_{\\theta}(\\cdot)$ , i.e., the first layer. $\\hat{\\phi}_{t}$ comprises the mean $\\hat{\\mu}_{t}$ and standard deviation $\\hat{\\sigma}_{t}$ calculated over $f_{\\pmb{\\theta}_{s}}(\\mathbf{x}_{t})$ . Then, we estimate the statistics $\\phi_{d}$ from the observed test samples $\\{\\mathbf{x}_{t}\\}_{t=1}^{T}$ via exponential moving average: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\phi_{d}=\\lambda\\hat{\\phi}_{t}+(1-\\lambda)\\phi_{d},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\lambda$ is a moving average factor belongs to $[0,1]$ . Inspired by existing distance-based detection methods [19], we capture the magnitude of distribution shifts by a distance function $D(\\cdot,\\cdot)$ as follows. ", "page_idx": 5}, {"type": "equation", "text": "$$\nD(\\phi_{d},\\hat{\\phi}_{t})=\\frac{1}{H}\\sum_{i=1}^{H}K L(\\phi_{d,i}||\\hat{\\phi}_{t,i})+K L(\\hat{\\phi}_{t,i}||\\phi_{d,i}),\\,\\,K L(\\phi_{1}||\\phi_{2})=\\frac{1}{2\\sigma_{2}^{2}}(\\sigma_{1}^{2}+(\\mu_{1}-\\mu_{2})^{2}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $H$ denotes the dimension of statistics, and $K L(\\cdot||\\cdot)$ is the KL-divergence simplified from [19]. Here, a distribution shift is detected when $D(\\phi_{d},\\hat{\\phi}_{t})>z$ , where $z$ is a pre-defined threshold. This simple design offers several merits: 1) It imposes minimal computational and memory costs without necessitating data preservation. 2) By leveraging the features from the stem layer, we can promptly detect and respond to distribution shifts, rendering it well-suited for the online nature of TTA. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Datasets and models. We conduct experiments on the ImageNet-1k [6], as well as five benchmarks for OOD generalization, i.e., ImageNet-C [16] (contains corrupted images in 15 types of 4 main categories and each type has 5 severity levels), ImageNet-R (various artistic renditions of 200 ImageNet classes) [15], ImageNet-Sketch [55], ImageNet-A [17], and ImageNet-V2 [44]. We use ViT-Base [8] as the source model unless stated otherwise. The model is trained on the source ImageNet-1K [6] training set and the model weights are obtained from the timm repository [60]. ", "page_idx": 5}, {"type": "text", "text": "Compared methods and implementation details. We compared our proposed CoLA with 1) Backpropagation-based methods: CoTTA [56], ETA [38], EATA [38], SAR [39], and DeYO [25]. 2) Backpropagation-free methods: LAME [3] and T3A [21]. For Eqn. (2), We directly leverage the learnable test-time objectives from the integrated TTA methods. $\\Delta\\pmb{\\theta}$ is optimized by following the update rules of the integrated baseline as listed in Appendix C. $_{\\alpha}$ is updated via the AdamW optimizer with a learning rate of 0.1. The shift detection threshold $z$ is set to 0.1. For follower agents, we consistently set $T_{f}$ in Eqn. (3) to 5 for all experiments. More details are put in Appendix A and C. ", "page_idx": 5}, {"type": "text", "text": "Table 1: Comparison on ImageNet-C (level 5) regarding Accuracy $(\\%)$ under lifelong adaptation for 10 rounds, in total of 150 corruptions, on a single principal resource-abundant device. We report the average accuracy of 15 corruptions in each round here and put more results in Appendix D. ", "page_idx": 6}, {"type": "table", "img_path": "YyMiO0DWmI/tmp/1733db043d362f15cea886d04239e4dd0e8eab046f509787f114654369fe8cdc.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Table 2: Effectiveness under collaborative adaptation across principal resource-abundant devices w.r.t. Accuracy $(\\%)$ . Results are evaluated on ImageNet-C (level 5, containing 15 corruption types of 4 groups). We share learned weights across devices post-adaptation to each group of corruptions. ", "page_idx": 6}, {"type": "table", "img_path": "YyMiO0DWmI/tmp/589e5d124b21d995b4c33c28abb4da7713ab093717c3c25d1b366c8ebef33d78.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "4.1 Comparison with state-of-the-art methods ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Results under lifelong test-time adaptation. We evaluate the long-term effectiveness of our CoLA on ImageNet-C [44] within a challenging lifelong TTA scenario where the model is online adapted to 15 corruptions over 10 rounds (total 150 corruptions), and the parameters will never be reset. We put more details of the experimental protocol in Appendix C due to page limits. From Table 1, we derive the following observations. 1) Our CoLA achieves new state-of-the-art results on the first round, last round, and the average of adaptation, suggesting our superiority. 2) Most methods, including CoTTA [56] and EATA [38] with an anti-forgetting strategy, experience performance degradation as the number of adaptation rounds increases (e.g., ETA\u2019s performance degrades from $\\bar{6}1.4\\%$ to $35.1\\%$ on the average accuracy), indicating the difficulty of the evaluated scenario. 3) By integrating our CoLA with existing methods, we enhance the performance steadily with more adaptations, demonstrating our effectiveness in accumulating and exploiting learned knowledge for long-term adaptation. 4) Although EATA mitigates performance degradation by introducing an anti-forgetting regularization, it suffers from the stability-plasticity trade-off, i.e., the average accuracy drops from $61.4\\%$ (ETA) to $60.4\\%$ (EATA) in the first round. In contrast, our CoLA enhances ETA\u2019s performance even at the first round of adaptation, i.e., $61.4\\%$ (ETA) vs. $62.0\\%$ $(\\mathrm{ETA+CoLA})$ ), indicating that CoLA does not limit the learning ability. The sensitivity analyses on threshold $z$ are provided in Appendix E. ", "page_idx": 6}, {"type": "text", "text": "Results under collaborative test-time adaptation. To evaluate our CoLA under the collaborative TTA scenario, we first assess its performance across multiple principal resource-abundant devices. From Table 2, our CoLA outperforms the integrated baseline from the adaptation to the second group of corruptions, e.g., the accuracy of $58.0\\%$ $\\mathrm{{(SAR+CoLA)}}$ ) vs. $54.4\\%$ (SAR) on \u2018Blur\u2019 in Devices 1. Moreover, this improvement becomes increasingly more pronounced as more knowledge is shared across devices, e.g., improving the accuracy from $47.7\\%$ (SAR) to $55.5\\%$ $\\mathrm{SAR+CoLA})$ on \u2018Noise\u2019 in Device 3. This demonstrates our effectiveness in facilitating knowledge sharing and exploitation across principal devices via our knowledge reprogramming learning scheme, i.e., with Eqn. (2). ", "page_idx": 6}, {"type": "table", "img_path": "YyMiO0DWmI/tmp/c0f4a1027b52a87321ca24388b9f4472430a6dd3d9da94787a2188b3d8f89966.jpg", "table_caption": ["Table 3: Comparison on ImageNet-C (level 5) regarding Accuracy $(\\%)$ under lifelong adaptation on resource-limited follower devices. $\\mathrm{T}3\\mathrm{A}^{*}$ resets the model after adaptation on each corruption. CoLA exploits the learned weights from Table 2 (e.g., ETA $^+$ CoLA at the 7-th row) for Eqn. (3). "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 4: Comparison under single-domain TTA (on one principal device) w.r.t. Acc $(\\%)$ . Results are averaged over 15 corruptions on ImageNet-C (level 5). L.S denotes label distribution shifts, M.S denotes mixed domain shifts per SAR [39]. ", "page_idx": 7}, {"type": "table", "img_path": "YyMiO0DWmI/tmp/f0bb73a7767a2d1a0d7e5b79e4185173644828534f7fd36d07046ed8b3d81c9c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 5: Comparison w.r.t. wall-clock time and memory on ImageNet-C (Gaussian, level 5) on an A100 GPU. C/R refers to accuracy on ImageNet-C/R. BP is short for back-propagation. CoLA utilizes weights of $\\mathrm{ETA+CoLA}$ in Table 2. ", "page_idx": 7}, {"type": "table", "img_path": "YyMiO0DWmI/tmp/a986f8b2d24eaeadee34fb00fcfba9b5308ce6ca68f357fb03b0a15a2457e06e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Given learned knowledge from resource-abundant principal devices (totaling 34 weights occupying $5.0\\,\\mathrm{MB})$ ), we further evaluate the effectiveness of CoLA on resource-limited follower devices. From Table 3, we observe that existing TTA methods struggle to improve the performance of the source model without model updates, highlighting the urgent need for a more effective solution. In contrast, by exploiting shared knowledge adaptively in a forward-only manner with Eqn. (3), CoLA achieves a substantial performance gain, e.g., enhancing the average accuracy from $29.9\\%$ to $64.1\\%$ in CoLA (ETA). Note that we also verify CoLA\u2019s sample efficiency as well as its computation and memory efficiency in Figure 3 (a) and Table 5. These results collectively underscore the importance of cross-device collaboration and our effectiveness regarding the scenario of collaborative TTA. ", "page_idx": 7}, {"type": "text", "text": "Results under single-domain test-time adaptation. Following DeYo [25], we validate our CoLA in both the wild scenario (i.e., imbalanced label distribution shifts and mixture of distribution shifts) and the mild scenario of single-domain TTA, where the model is reset post-adaptation to each corruption. Here, CoLA saves learned weights for every adaptation to 10 batches of samples while maintaining a maximum of 32 weights (totaling 4.7 MB) by discarding the unused ones according to $\\alpha_{i}$ . ", "page_idx": 7}, {"type": "text", "text": "From Table 4, within all evaluated scenarios, incorporating CoLA consistently improves the performance by a considerable margin (e.g., $+2.0\\%$ on SAR w.r.t. overall average accuracy). Interestingly, the enhancement from CoLA may even help surpass a stronger baseline, e.g., the average accuracy of $64.4\\%$ $\\mathrm{\\DeltaETA+CoLA}_{\\prime}$ vs. $64.1\\%$ (DeYO) on the mild scenario, demonstrating our effectiveness. This improvement mainly stems from our ability to alleviate error accumulation. Given multiple saved weights, instead of naively selecting the newest weight that may have adapted to noise, CoLA dynamically favors the more optimal one via loss optimization. This renders CoLA more robust to scenarios where perturbations may occur. We also visualize $\\alpha_{i}$ in Appendix E to offer more insights. ", "page_idx": 7}, {"type": "text", "text": "4.2 Ablation studies and more discussions ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Effectiveness of $T_{l}$ on sample efficiency in Eqn. (2). Sample efficiency is particularly important in scenarios where the availability of target data is limited or early adaptation performance is paramount. As shown in Figure 3 (a), by leveraging the learned knowledge from other devices, $\\mathrm{ETA+CoLA}$ achieves an up to $78.0\\times$ speed up compared with ETA, i.e., $51.7\\%$ accuracy with 640 samples $(\\mathrm{ETA+CoLA})$ ) vs. $51.37\\%$ accuracy with 49,920 samples (ETA), demonstrating the importance of cross-device collaboration and our effectiveness to facilitate knowledge sharing and utilization. Moreover, upon integrating $T_{l}$ in Eqn. (2), CoLA demonstrates superiority without necessitating too many test samples, i.e., $53.2\\%$ accuracy $\\mathrm{(ETA+CoLA)}$ vs. $22.2\\%$ accuracy $\\mathrm{ETA+CoLA}$ w.o. $T_{l}$ ) with 960 samples, suggesting the effectiveness of $T_{l}$ to promote swift adaptation under distribution shifts. We also provide more comparison in Appendix E regarding sample efficiency on unseen distributions. ", "page_idx": 7}, {"type": "image", "img_path": "YyMiO0DWmI/tmp/35932b5d2509e106280569ef75832ce25ceab435fe98219192f0849f7fb1f519.jpg", "img_caption": ["Figure 3: Ablation study of CoLA. In left, we compare the sample efficiency on Device2 in Table 2, Gaussian. Model\u2019s accuracy is recorded on the entire test set after adapting to $N$ test samples. In right, we evaluate the effectiveness of $T_{f}$ on seen (i.e., ImageNet-C, Gaussian) and unseen distributions (i.e., ImageNet-R/Sketch), where CoLA exploits weights of $\\mathrm{ETA+CoLA}$ in Table 2 for Eqn. (3). "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "YyMiO0DWmI/tmp/c65ed02a023c5d81a48d484a178f859695782586bc086afc77a88c1046ccf061.jpg", "table_caption": ["Table 6: Effectiveness of CoLA (Eqn. 2) on unseen distributions with weights learned on ImageNet-C from Table 2. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "YyMiO0DWmI/tmp/c4dc742ac4323561255f43ede3b71d0f46cbe3628374c5630bbb62d19df20f83.jpg", "table_caption": ["Table 7: Effectiveness of CoLA on prompt tuning. Results are reported on ImageNet and its variants with CLIPRN50 [43]. CoLA exploits 78 hard prompts for Eqn. (2). "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Effectiveness of $T_{f}$ on robustness in Eqn. (5). We evaluate the robustness of Eqn. (3) on seen/unseen distributions (i.e., whether resource-abundant devices have encountered the evaluated distribution). From Figure 3 (b), our CoLA consistently outperforms NoAdapt regardless of $T_{f}$ , demonstrating our effectiveness. On seen distribution, our improvement is particularly significant while the performance is insensitive to $T_{f}$ in a reasonable range. However, when $T_{f}$ is set to infinity (i.e., averaging different weights), performance experiences degradation as appropriate knowledge plays a less significant role. On unseen distribution (e.g., ImageNet-R), aggregating a minority of knowledge may be insufficient to address distribution shifts. In this case, $T_{f}$ plays an important role in enhancing robustness by aggregating the strengths of diverse knowledge. We fix $T_{f}$ to 5 in experiments without careful tuning. ", "page_idx": 8}, {"type": "text", "text": "Memory and computation consumption of CoLA. Besides achieving strong performance across various scenarios, we demonstrate that CoLA incurs negligible computation and memory costs. From Table 5, on resource-limited follower devices, CoLA (Eqn. 3) substantially outperforms T3A in terms of accuracy and runtime memory (i.e., $55.7\\%$ vs. $9.5\\%$ , and $821.9\\,\\mathrm{MB}$ vs. 909.9 MB) while maintaining nearly the same efficiency as NoAdapt. On resource-abundant principal devices, CoLA enhances the performance of ETA by a considerable margin (i.e., $+2.4\\%$ on ImageNet-C) while incurring only $5.7\\,\\mathrm{MB}$ of additional runtime memory and 3s of latency, indicating that CoLA is even lighter than the regularizer introduced in EATA. Note that CoLA (Eqn. 3) determines appropriate re-weighting for different knowledge with a batch of test samples, outperforming ETA $^{+}$ CoLA w.r.t average accuracy on ImageNet-C. This underscores the potential of collaborative TTA in real world, where a device may adapt effectively using only negligible costs given adequate shared knowledge. We also show that CoLA can efficiently scale to over 10,000 domain vectors in Appendix E. ", "page_idx": 8}, {"type": "text", "text": "Generalization of Eqn. (2) on unseen distributions. Following Figure 3 (b), we further validate the effectiveness of CoLA (Eqn. 2) on unseen distributions. From Table 6, CoLA consistently enhances performance on unseen distributions, yielding a notable $4.4\\%$ improvement on SAR in terms of average accuracy. Such improvement can be attributed to the transferability of knowledge learned from similar domains [63]. These findings collectively indicate that the effectiveness of CoLA is not limited to previously encountered distributions on both principal agents and follower agents. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Prompt tuning with CoLA using multiple hard prompts. Besides aggregating learned knowledge from other devices, we demonstrate that CoLA can also benefti from aggregating diverse knowledge from humans (i.e., manual-designed hard prompts). As shown in Table 7, compared with TPT which is limited to leveraging a single hard prompt, CoLA enhances the adaptation performance on 4 out of 5 datasets (e.g., $+1.5\\%$ on ImageNet w.r.t. accuracy). These results collectively indicate that CoLA effectively exploits both the knowledge of humans and the knowledge from optimization, which may bring new perspectives to the design of learning algorithms when introducing diverse human prior knowledge is beneficial, e.g., chain-of-thoughts [58]. All used prompts are listed in Appendix C. ", "page_idx": 9}, {"type": "text", "text": "Differences and advantages over federated TTA [1, 23]. The main difference is that our CoLA conducts collaborative learning at the testing phase, whereas federated TTA conducts collaborative learning during federated source training. For instance, FedTHE $^+$ [23] federatedly trains a global and a local personalized model for each client, then adaptively ensemble their outputs at test time. ATP [1] federatedly learns module-specific adaptation rates across clients during training for test-time adaptation. However, these methods still conduct TTA independently on each devices during testing, and thus inherits the limitation of the single-device TTA methods. Moreover, in federated TTA [1, 23], the training phase and test-time adaptation phase are highly correlated, which means they can only use their own federated-trained models during TTA. This makes these methods restricted for real-world applications. In contrast, our CoLA enhances test-time model adaptation performance and efficiency by leveraging knowledge from multiple devices in the application environment, which essentially establishes a new unsupervised on-time TTA paradigm. Moreover, our CoLA paradigm can be applied to any pre-trained models, and thus offers much better flexibility in deployment. Additional comparisons with FedAvg [32] for collaborative adaptation are also provided in Appendix E. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we propose a multi-device Collaborative Lifelong Adaptation (CoLA) paradigm for test-time adaptation (TTA), which addresses a practical scenario where multiple devices with different computational resources and latency requirements need to perform TTA simultaneously. In particular, we first accumulate a set of shared domain knowledge vectors with an efficient domain shift detector. Based on this, we develop a knowledge reprogramming learning strategy on principal agents, which leverages backpropagation-based optimization to aggregate existing knowledge while learning new domain-specific parameters simultaneously. To further improve adaptation efficiency, we introduce an optimization-free TTA strategy on follower agents, which solely aggregates the shared domain vectors based on domain similarity. In CoLA, all devices/agents work collaboratively while keeping privacy preserved and communication efficient. Experiments verify that CoLA boosts the performance and efficiency of existing TTA solutions in collaborative, lifelong, and single-domain TTA scenarios. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was partially supported by National Natural Science Foundation of China (NSFC) 62072190 and TCL Science and Technology Innovation Fund. The authors thank Jinwu Hu and Yu Hu for discussions on domain knowledge reprogramming, and Yaofo Chen for consultations on cloud-edge test-time adaptation. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] W. Bao, T. Wei, H. Wang, and J. He. Memo: Test time robustness via adaptation and augmentation. In Advances in Neural Information Processing Systems, volume 36, 2024.   \n[2] A. Bartler, A. B\u00fchler, F. Wiewel, M. D\u00f6bler, and B. Yang. Mt3: Meta test-time training for self-supervised test-time adaption. In International Conference on Artificial Intelligence and Statistics, pages 3080\u20133090. PMLR, 2022. [3] M. Boudiaf, R. Mueller, I. Ben Ayed, and L. Bertinetto. Parameter-free online test-time adaptation. In IEEE Conference on Computer Vision and Pattern Recognition, pages 8344\u2013 8353, 2022. [4] Y. Chen, S. Niu, S. Xu, H. Song, Y. Wang, and M. Tan. Towards robust and efficient cloud-edge elastic model adaptation via selective entropy distillation. In International Conference on Learning Representations, 2024. [5] Z. Chen, J. Chen, Z. Xie, E. Xu, Y. Feng, and S. Liu. Multi-expert attention network with unsupervised aggregation for long-tailed fault diagnosis under speed variation. KnowledgeBased Systems, 252:109393, 2022. [6] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image database. In IEEE Conference on Computer Vision and Pattern Recognition, pages 248\u2013 255, 2009.   \n[7] Z. Deng, Z. Chen, S. Niu, T. Li, B. Zhuang, and M. Tan. Efficient test-time adaptation for super-resolution with second-order degradation and reconstruction. In Advances in Neural Information Processing Systems, 2023. [8] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021.   \n[9] Y. Du, S. Luo, Y. Xin, M. Chen, S. Feng, M. Zhang, and C. Wang. Multi-source fully test-time adaptation. Neural Networks, page 106661, 2024.   \n[10] F. Fleuret et al. Test time adaptation through perturbation robustness. In Advances in Neural Information Processing Systems Workshop, 2021.   \n[11] Y. Gandelsman, Y. Sun, X. Chen, and A. Efros. Test-time training with masked autoencoders. In Advances in Neural Information Processing Systems, volume 35, pages 29374\u201329385, 2022.   \n[12] J. Gao, J. Zhang, X. Liu, E. Shelhamer, T. Darrell, and D. Wang. Back to the source: Diffusiondriven test-time adaptation. In IEEE Conference on Computer Vision and Pattern Recognition, 2023.   \n[13] S. Gidaris, P. Singh, and N. Komodakis. Unsupervised representation learning by predicting image rotations. In International Conference on Learning Representations, 2018.   \n[14] S. Grigorescu, B. Trasnea, T. Cocias, and G. Macesanu. A survey of deep learning techniques for autonomous driving. Journal of field robotics, 37(3):362\u2013386, 2020.   \n[15] D. Hendrycks, S. Basart, N. Mu, S. Kadavath, F. Wang, E. Dorundo, R. Desai, T. Zhu, S. Parajuli, M. Guo, et al. The many faces of robustness: A critical analysis of out-of-distribution generalization. In IEEE Conference on Computer Vision and Pattern Recognition, pages 8340\u20138349, 2021.   \n[16] D. Hendrycks and T. Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. In International Conference on Learning Representations, 2019.   \n[17] D. Hendrycks, K. Zhao, S. Basart, J. Steinhardt, and D. Song. Natural adversarial examples. In IEEE Conference on Computer Vision and Pattern Recognition, pages 15262\u201315271, 2021.   \n[18] Y. J. Heo, D. Kim, W. Lee, H. Kim, J. Park, and W. K. Chung. Collision detection for industrial collaborative robots: A deep learning approach. IEEE Robotics and Automation Letters, 4(2):740\u2013746, 2019.   \n[19] J. Hong, L. Lyu, J. Zhou, and M. Spranger. MECTA: Memory-economic continual test-time model adaptation. In International Conference on Learning Representations, 2023.   \n[20] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. Lora: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022.   \n[21] Y. Iwasawa and Y. Matsuo. Test-time classifier adjustment module for model-agnostic domain generalization. In Advances in Neural Information Processing Systems, volume 34, 2021.   \n[22] L. Jiang and T. Lin. Test-time robust personalization for federated learning. In International Conference on Learning Representations, 2022.   \n[23] L. Jiang and T. Lin. Test-time robust personalization for federated learning. In International Conference on Learning Representations, 2023.   \n[24] J. N. Kundu, N. Venkat, R. V. Babu, et al. Universal source-free domain adaptation. In IEEE Conference on Computer Vision and Pattern Recognition, pages 4544\u20134553, 2020.   \n[25] J. Lee, D. Jung, S. Lee, J. Park, J. Shin, U. Hwang, and S. Yoon. Entropy is not enough for test-time adaptation: From the perspective of disentangled factors. In International Conference on Learning Representations, 2024.   \n[26] J. Liang, R. He, and T. Tan. A comprehensive survey on test-time adaptation under distribution shifts. International Journal of Computer Vision, pages 1\u201334, 2024.   \n[27] J. Liang, D. Hu, and J. Feng. Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation. In International Conference on Machine Learning, pages 6028\u20136039, 2020.   \n[28] Y. Liang, Z. Cai, J. Yu, Q. Han, and Y. Li. Deep learning based inference of private information using embedded sensors in smart devices. IEEE Network, 32(4):8\u201314, 2018.   \n[29] J. Lin, W.-M. Chen, Y. Lin, C. Gan, S. Han, et al. Mcunet: Tiny deep learning on iot devices. In Advances in Neural Information Processing Systems, volume 33, pages 11711\u201311722, 2020.   \n[30] Y. Liu, P. Kothari, B. van Delft, B. Bellot-Gurlet, T. Mordan, and A. Alahi. Tt $^{++}$ : When does self-supervised test-time training fail or thrive? In Advances in Neural Information Processing Systems, volume 34, 2021.   \n[31] M. Long, H. Zhu, J. Wang, and M. I. Jordan. Unsupervised domain adaptation with residual transfer networks. In Advances in Neural Information Processing Systems, volume 29, 2016.   \n[32] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas. Communication-efficient learning of deep networks from decentralized data. In Artificial intelligence and statistics, pages 1273\u20131282. PMLR, 2017.   \n[33] Y. Ming and Y. Li. How does fine-tuning impact out-of-distribution detection for vision-language models? International Journal of Computer Vision, 132(2):596\u2013609, 2024.   \n[34] M. J. Mirza, P. J. Soneira, W. Lin, M. Kozinski, H. Possegger, and H. Bischof. Actmad: Activation matching to align distributions for test-time-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 24152\u201324161, 2023.   \n[35] K. Muhammad, A. Ullah, J. Lloret, J. Del Ser, and V. H. C. de Albuquerque. Deep learning for safe autonomous driving: Current challenges and future directions. IEEE Transactions on Intelligent Transportation Systems, 22(7):4316\u20134336, 2020.   \n[36] Z. Nado, S. Padhy, D. Sculley, A. D\u2019Amour, B. Lakshminarayanan, and J. Snoek. Evaluating prediction-time batch normalization for robustness under covariate shift. arXiv preprint arXiv:2006.10963, 2020.   \n[37] S. Niu, C. Miao, G. Chen, P. Wu, and P. Zhao. Test-time model adaptation with only forward passes. In International Conference on Machine Learning, 2024.   \n[38] S. Niu, J. Wu, Y. Zhang, Y. Chen, S. Zheng, P. Zhao, and M. Tan. Efficient test-time model adaptation without forgetting. In International Conference on Machine Learning, pages 16888\u2013 16905, 2022.   \n[39] S. Niu, J. Wu, Y. Zhang, Z. Wen, Y. Chen, P. Zhao, and M. Tan. Towards stable test-time adaptation in dynamic wild world. In International Conference on Learning Representations, 2023.   \n[40] Y. Oh, J. Lee, J. Choi, D. Jung, U. Hwang, and S. Yoon. Efficient diffusion-driven corruption editor for test-time adaptation. arXiv preprint arXiv:2403.10911, 2024.   \n[41] Z. Pei, Z. Cao, M. Long, and J. Wang. Multi-adversarial domain adaptation. In AAAI Conference on Artificial Intelligence, 2018.   \n[42] Z. Qiu, Y. Zhang, H. Lin, S. Niu, Y. Liu, Q. Du, and M. Tan. Source-free domain adaptation via avatar prototype generation and adaptation. In International Joint Conference on Artificial Intelligence, 2021.   \n[43] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[44] B. Recht, R. Roelofs, L. Schmidt, and V. Shankar. Do imagenet classifiers generalize to imagenet? In International Conference on Machine Learning, pages 5389\u20135400, 2019.   \n[45] K. Saito, K. Watanabe, Y. Ushiku, and T. Harada. Maximum classifier discrepancy for unsupervised domain adaptation. In IEEE Conference on Computer Vision and Pattern Recognition, pages 3723\u20133732, 2018.   \n[46] S. Schneider, E. Rusak, L. Eck, O. Bringmann, W. Brendel, and M. Bethge. Improving robustness against common corruptions by covariate shift adaptation. In Advances in Neural Information Processing Systems, volume 33, pages 11539\u201311551, 2020.   \n[47] M. Shu, W. Nie, D.-A. Huang, Z. Yu, T. Goldstein, A. Anandkumar, and C. Xiao. Test-time prompt tuning for zero-shot generalization in vision-language models. In Advances in Neural Information Processing Systems, volume 35, pages 14274\u201314289, 2022.   \n[48] J. Song, J. Lee, I. S. Kweon, and S. Choi. Ecotta: Memory-efficient continual test-time adaptation via self-distilled regularization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11920\u201311929, 2023.   \n[49] J. Song, K. Park, I. Shin, S. Woo, C. Zhang, and I. S. Kweon. Test-time adaptation in the dynamic world with compound domain knowledge management. IEEE Robotics and Automation Letters, 2023.   \n[50] Y. Su, X. Xu, T. Li, and K. Jia. Revisiting realistic test-time training: Sequential inference and adaptation by anchored clustering regularized self-training. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024.   \n[51] Y. Sun, X. Wang, Z. Liu, J. Miller, A. Efros, and M. Hardt. Test-time training with selfsupervision for generalization under distribution shifts. In International Conference on Machine Learning, pages 9229\u20139248, 2020.   \n[52] M. Tan, G. Chen, J. Wu, Y. Zhang, Y. Chen, P. Zhao, and S. Niu. Uncertainty-calibrated test-time model adaptation without forgetting. arXiv preprint arXiv:2403.11491, 2024.   \n[53] A. Tang, L. Shen, Y. Luo, N. Yin, L. Zhang, and D. Tao. Merging multi-task models via weight-ensembling mixture of experts. In International Conference on Machine Learning, 2024.   \n[54] D. Wang, E. Shelhamer, S. Liu, B. Olshausen, and T. Darrell. Tent: Fully test-time adaptation by entropy minimization. In International Conference on Learning Representations, 2021.   \n[55] H. Wang, S. Ge, Z. Lipton, and E. P. Xing. Learning robust global representations by penalizing local predictive power. In Advances in Neural Information Processing Systems, pages 10506\u2013 10518, 2019.   \n[56] Q. Wang, O. Fink, L. Van Gool, and D. Dai. Continual test-time domain adaptation. In IEEE Conference on Computer Vision and Pattern Recognition, 2022.   \n[57] X. Wang, Y. Tsvetkov, S. Ruder, and G. Neubig. Efficient test time adapter ensembling for low-resource language varieties. arXiv preprint arXiv:2109.04877, 2021.   \n[58] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou, et al. Chainof-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems, volume 35, pages 24824\u201324837, 2022.   \n[59] Z. Wen, S. Niu, G. Li, Q. Wu, M. Tan, and Q. Wu. Test-time model adaptation for visual question answering with debiased self-supervisions. IEEE Transactions on Multimedia, 2023.   \n[60] R. Wightman. Pytorch image models. https://github.com/rwightman/ pytorch-image-models, 2019.   \n[61] Y. Xin, S. Luo, H. Zhou, J. Du, X. Liu, Y. Fan, Q. Li, and Y. Du. Parameter-efficient fine-tuning for pre-trained vision models: A survey. arXiv preprint arXiv:2402.02242, 2024.   \n[62] E. Yang, Z. Wang, L. Shen, S. Liu, G. Guo, X. Wang, and D. Tao. Adamerging: Adaptive model merging for multi-task learning. In International Conference on Learning Representations, 2024.   \n[63] L. Zhang and X. Gao. Transfer adaptation learning: A decade survey. IEEE Transactions on Neural Networks and Learning Systems, 2022.   \n[64] M. M. Zhang, S. Levine, and C. Finn. Memo: Test time robustness via adaptation and augmentation. In Advances in Neural Information Processing Systems, 2022.   \n[65] Y. Zhang, B. Hooi, L. Hong, and J. Feng. Self-supervised aggregation of diverse experts for test-agnostic long-tailed recognition. In Advances in Neural Information Processing Systems, volume 35, pages 34077\u201334090, 2022.   \n[66] Y. Zhang, S. Niu, Z. Qiu, Y. Wei, P. Zhao, J. Yao, J. Huang, Q. Wu, and M. Tan. Covid-da: deep domain adaptation from typical pneumonia to covid-19. arXiv preprint arXiv:2005.01577, 2020.   \n[67] Y. Zhang, Y. Wei, Q. Wu, P. Zhao, S. Niu, J. Huang, and M. Tan. Collaborative unsupervised domain adaptation for medical image diagnosis. IEEE Transactions on Image Processing, 29:7834\u20137844, 2020.   \n[68] H. Zhuang, Z. Weng, H. Wei, R. Xie, K.-A. Toh, and Z. Lin. Acil: Analytic class-incremental learning with absolute memorization and privacy protection. In Advances in Neural Information Processing Systems, volume 35, pages 11602\u201311614, 2022. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Supplementary Materials for \u201c Cross-Device Collaborative Test-time Adaptation \u201d ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Related work 16 ", "page_idx": 14}, {"type": "text", "text": "B More design details of CoLA 18 ", "page_idx": 14}, {"type": "text", "text": "C More implementation details 20 ", "page_idx": 14}, {"type": "text", "text": "C.1 More details on datasets . 20   \nC.2 More experimental protocols on evaluation 20   \nC.3 More experimental protocols on methods . 22 ", "page_idx": 14}, {"type": "text", "text": "D More experimental results 23 ", "page_idx": 14}, {"type": "text", "text": "E Additional discussions 25 ", "page_idx": 14}, {"type": "text", "text": "F Limitations and future works 29 ", "page_idx": 14}, {"type": "text", "text": "G Broader impact 29 ", "page_idx": 14}, {"type": "table", "img_path": "YyMiO0DWmI/tmp/a6bda917febca4792f15702bf97d87148e64de991e8787ad97602e2feddcd514.jpg", "table_caption": ["Table A: Characteristics of problem settings that adapt a trained model to a potentially shifted domain. \u2018Online\u2019 adaptation predicts a single or batch of incoming test samples immediately. \u2018#Devices\u2019 is the number of devices involved in TTA. \u2018Learned Knowledge\u2019 is the knowledge from model adaptation. "], "table_footnote": ["CoLA (Ours) $\\mathbf{x}^{t}$ $\\mathcal{L}(\\mathbf{x}^{t})$ M Accumulated Applicable Preserved Intermittent "], "page_idx": 15}, {"type": "text", "text": "A Related work ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We summarize our main differences in Table A and discuss the related works in the following. ", "page_idx": 15}, {"type": "text", "text": "Test-time adaptation (TTA) seeks to enhance the model performance on unseen, potentially shifted test data, by directly learning from the test data itself. We categorize the related TTA works into the following four groups for discussion, according to 1) the number of devices involved in adaptation; 2) their dependence on backward propagation; and 3) the availability of multiple models. ", "page_idx": 15}, {"type": "text", "text": "\u2022 Single-device backpropagation-based TTA. Test-Time Training (TTT) [51] first proposes this pipeline. During the training phase, TTT methods train a source model with both a supervised and a self-supervised branch. Given a test sample during testing, they typically update the shared encoder with the self-supervised objectives, such as rotation prediction [51], contrastive learning [30, 2], reconstruction learning [11]. To avoid altering the model training phase and access to source data, Fully TTA methods directly update an on-the-fly model via unsupervised learning objectives, including, but not limited to, entropy minimization [54, 39], prediction consistency maximization [64, 10] and feature distribution alignment [34]. ", "page_idx": 15}, {"type": "text", "text": "In pursuit of efficient backpropagation-based TTA, the attempts of existing methods can be generally categorized into: 1) Sample Efficiency. As test data are not equally important for adaptation, some recent works [38, 39, 47, 25] have devised various sample selection strategies to identify reliable and non-redundant samples for test-time learning. It reduces the noise in the gradient and the number of samples for TTA, thereby enhancing adaptation performance and efficiency. 2) Memory Efficiency: EcoTTA [48] reduces run-time memory by optimizing only the parameter-efficient adapters. MECTA [19] reduces the batch size at testing, while it further proposes a domain-aware batch normalization layer to stabilize TTA using only a small batch size. ", "page_idx": 15}, {"type": "text", "text": "Nevertheless, these methods focus on single-device adaptation, where all devices adapt from scratch. In this sense, valuable knowledge learned from other devices is neglected, damaging the adaptation performance and efficiency. Moreover, these methods still rely on computationally intensive backpropagation for model updates, which hinders their applicability in resource-limited devices or latency-sensitive scenarios. To address this, we propose a gradient-based and forward-only collaboration paradigm, facilitating knowledge accumulation, sharing, and utilization across devices. ", "page_idx": 15}, {"type": "text", "text": "Recently, TTA-CDKM [49] proposes to learn and reuse multiple groups of model parameters for better adaptation, with each group representing the knowledge of a domain. However, TTA-CDKM exploits only a parameter group during inference, and thus fails to aggregate the strength of diverse domain knowledge. On the other hand, TTA-CDKM updates the stored parameters when adapting to each batch of test samples, which introduces intensive communication costs for weight synchronization that are impractical for multi-device collaboration. In contrast, our CoLA effectively learns to aggregate diverse strengths of domain vectors at testing while ensuring communication efficiency. ", "page_idx": 15}, {"type": "text", "text": "\u2022 Single-device forward-only TTA. In the development of BP-free TTA, early research mostly focused on calibrating the statistics of batch normalization layers by leveraging the test data to estimate test statistics [36, 46]. Nevertheless, these strategies only conduct adaptation on batch normalization layers, limiting their applicability to various architectures. In pursuit of a more general forward-only TTA solution, existing methods can be generally divided into: 1) Input-Level Adaptation, where the corrupted test images are reconstructed before making predictions based on a diffusion model [12, 40]. 2) Output-Level Adaptation, where T3A [21] proposed a prototype-based classifier for adaptive predictions, and LAME [3] directly corrects the predicted logits. However, since BP-free TTA does not leverage model feedback, i.e., un/self-supervised learning objectives, for knowledge acquisition, it often results in suboptimal performance when dealing with out-of-distribution testing data. In our paper, we address this by facilitating knowledge sharing and utilization across devices, where we adaptively aggregate the learned knowledge of other devices in a forward-only manner. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "\u2022 Single-device test-time aggregation. Given multiple pretrained models, one can leverage unsupervised objectives at test time to adaptively aggregate their diverse strengths. Based on the level on which aggregation is performed, existing solutions can be generally divided into: ", "page_idx": 16}, {"type": "text", "text": "1) Output-wise test-time aggregation: Early research mainly focuses on output-wise test-time aggregation, where the aggregation is conducted on the output logit of each model. EMEA [57] first proposed this pipeline. Given multiple models trained on different datasets or different label distributions, this paradigm introduced a reweighting vector to aggregate the outputs logit of various models during testing, where the reweighting vector is optimized based on entropy minimization [57, 65], consistency loss [5], etc. More Recently, Mute [9] jointly updates the reweighting vector and all pretrained models at test time. Nevertheless, these strategies necessitate forward passes for each candidate model, i.e., $\\mathcal{O}(N)$ forward passes with $N$ the number of candidate models. Thus, they typically consume substantially more computation power, impeding their feasibility at edge devices. ", "page_idx": 16}, {"type": "text", "text": "2) Parameter-wise test-time aggregation. In contrast, parameter-wise test-time aggregation methods directly merge several candidate parameters into a single model without necessitating additional forward passes, i.e., $O(1)$ forward passes neglect of $N$ , rendering it more efficient in computation. To this end, Adamerging [62] first introduces the learnable parameter-wise weighting vectors, which aggregate various models before performing forward passes. Building upon this, WEMOE [53] further introduces the mixture of experts (MoE) into this paradigm, where the parameters from different models are considered as different experts, while the parameter-wise weighting vector is generated by a router that is trained during testing, with the entropy minimization objective. ", "page_idx": 16}, {"type": "text", "text": "Nevertheless, existing test-time aggregation solutions assume the availability of pretrained models from the target domain, which struggles to fulflil the conventional TTA settings. Unlike these methods, we perform both new knowledge learning and existing knowledge aggregation simultaneously at test time, where knowledge is accumulated across all devices from previous learning. ", "page_idx": 16}, {"type": "text", "text": "\u2022 Multi-device test-time adaptation. Existing TTA methods typically conduct adaptation on a single device, where multi-device test-time adaptation is heavily overlooked. In recent academics, FedTHE $^+$ [22] proposes a global-local scheme for federated test-time adaptation. Specifically, during each round of federated training, each device trains the global model on the local data, where the trained model is then sent back to the server. During testing, it adaptively merges the output logit of the global and local model based on entropy minimization and feature alignment loss. Nevertheless, it necessitates alteration to model training, limiting their applicability to scenarios where training data is unavailable. ATP [1] obtains module-specific adaptation rates via federated learning across clients and applies the adaptation rates in TTA. However, these methods still conduct TTA independently on each devices, and thus inherits the limitation of the single-device TTA method. More recently, CEMA [4] proposes a cloud-edge collaboration paradigm for TTA. In CEMA, edge devices perform pure inference to fliter reliable and informational samples to reduce communication burden, where the computationally intensive model updates are offloaded to the cloud server. Nevertheless, CEMA necessitates intensive transmission of both data and model weights, which introduces a heavy communication burden and may violate user privacy. Unlike these methods, we conducts collaborative adaptation at test time and necessitate only the intermittent transmission of updated model parameters, which is more practical in real-world implementation. ", "page_idx": 16}, {"type": "text", "text": "Unsupervised domain adaptation (UDA). Conventional UDA tackles distribution shifts by jointly optimizing a source model on both labeled source data and unlabeled target data to learn domaininvariant features [31, 41, 45, 67, 66]. To avoid access to source data, recently CPGA [42] generates feature prototypes for each category with pseudo-labeling. SHOT [27] learns a target-specific feature extractor by information maximization for representation alignment. Nevertheless, these methods necessitate pre-collected target datasets for offline adaptation, which limits their applicability in the real world. In contrast, our method adapts in an online manner and does not access the model training phase or access to source data, which facilitates a more practical adaptation paradigm. ", "page_idx": 16}, {"type": "text", "text": "B More design details of CoLA ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In the section, we further elaborate more details of our methods regarding the design of saving domain knowledge upon distribution shifts ", "page_idx": 17}, {"type": "text", "text": "Initialization of $_{\\alpha}$ and $\\Delta\\pmb{\\theta}$ in Eqn. (2) for stable adaptation warm-up. Since ground-truth labels are absent in our online TTA scenario to provide stable learning signals for Eqn. (2), a careful initialization of $_{\\alpha}$ and $\\Delta\\pmb{\\theta}$ is crucial upon distribution changes. In this sense, to ensure the stability of model performance pre and post encountering a new domain while enabling flexible aggregation of existing knowledge, we devise the initialization strategy by considering the following requirements: 1) the overall model weights $\\theta_{l}^{n+1}$ initialized after encountering the domain $n{+}1$ should remain the same as $\\theta_{l}^{n}$ that learned on the $n$ -th domain; 2) $\\Delta\\pmb{\\theta}$ should be as small as possible to enable flexible adaptation, e.g., when the new samples are all from in-distribution, one can obtain the solution of $\\theta_{l}{=}\\theta_{l}^{o}$ by setting $\\alpha_{0}{=}1$ if $\\Delta\\pmb{\\theta}$ is negligible; 3) for $\\alpha{=}\\mathrm{softmax}(\\beta)$ , the initial $\\beta$ should not be very sharp, e.g., $\\beta{=}[0,\\operatorname*{inf},0,...,0]$ , as it will hinder the model learning from selecting diverse knowledge. ", "page_idx": 17}, {"type": "text", "text": "Formally, let $\\{\\Delta\\pmb{\\theta}^{(i)}\\}_{i=1}^{L}$ be the learned parameter on the $i$ -th layer in $\\Delta\\pmb{\\theta}$ . We define $\\Delta\\pmb{\\theta}$ \u2019s magnitude as the maximum scale of parameters across different layers $\\begin{array}{r}{\\xi=\\operatorname*{max}\\{\\frac{1}{J}\\sum_{j=1}^{J}|\\Delta\\pmb{\\theta}^{(i,j)}|\\}_{i=1}^{L}}\\end{array}$ , where $J$ is the number of parameters in each layer. To this end, we satisfy the above constraint by defining the reweighting logit $\\Delta\\theta_{n+1}$ as: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\beta_{n+1}=\\frac{1}{T_{l}}\\ln{\\left(\\left(s-1\\right)\\sum_{i=0}^{n}e^{\\beta_{i}T_{l}}\\right)},\\ s=\\operatorname*{max}\\{1,\\frac{\\xi}{w_{m}}\\}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Here, $\\beta_{n+1}$ is designed so that the magnitude $\\xi^{\\prime}$ of the recalculated $\\Delta\\pmb{\\theta}$ satisfies $\\xi^{\\prime}\\leq w_{m}$ , where $w_{m}$ is a constrained value set to 0.01. Moreover, to improve numerical stability in case $T_{l}$ is 0, we clip the value of $\\beta_{n+1}$ between $[-10,10]$ . The preserved knowledge $\\Delta\\theta_{n+1}$ is then shared across devices. ", "page_idx": 17}, {"type": "text", "text": "We next provide a proposition to validate the reasonableness of the design for $\\beta_{n+1}$ : ", "page_idx": 17}, {"type": "text", "text": "Proposition 1. Given $\\beta_{n+1}$ in Eqn. (8), the constraints that $\\pmb{\\theta}_{l}^{n+2}=\\pmb{\\theta}_{l}^{n+1}$ and $\\xi^{\\prime}\\leq w_{m}$ holds. ", "page_idx": 17}, {"type": "text", "text": "Proof. Assume that the model has learned from $n$ previously domains. According to Eqn. (2), when the model adapts on the $(n{+}1)$ -th domain, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\pmb{\\theta}_{l}^{n+1}=\\pmb{\\theta}_{l}^{o}+\\alpha_{0}\\Delta\\pmb{\\theta}_{0}+\\alpha_{1}\\Delta\\pmb{\\theta}_{1}+\\cdot\\cdot\\cdot+\\alpha_{n}\\Delta\\pmb{\\theta}_{n}+\\Delta\\pmb{\\theta},\\,\\mathrm{~where~}\\pmb{\\alpha}=\\mathrm{softmax}(\\pmb{\\beta}\\cdot\\pmb{T}_{l}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "When encountering the $(n{+}2)$ -th domain, we should save learned knowledge $\\Delta\\pmb{\\theta}$ from the $(n{+}1)$ -th domain. To adaptively aggregates prior knowledge and saved knowledge, we initialize $\\alpha_{n+1}$ for $\\Delta\\theta_{n+1}$ . Besides, we initialize $\\Delta\\pmb{\\theta}^{\\prime}$ in order to learn new knowledge from the $(n{+}2)$ -th domain. So we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\pmb{\\theta}_{l}^{n+2}=\\pmb{\\theta}_{l}^{o}+\\alpha_{0}^{\\prime}\\Delta\\pmb{\\theta}_{0}+\\alpha_{1}^{\\prime}\\Delta\\pmb{\\theta}_{1}+\\cdot\\cdot\\cdot+\\alpha_{n}^{\\prime}\\Delta\\pmb{\\theta}_{n}+\\alpha_{n+1}\\Delta\\pmb{\\theta}_{n+1}+\\Delta\\pmb{\\theta}^{\\prime}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "After adding $\\alpha_{n+1}$ , to maintain the vector $_{\\alpha}$ as normalized weights, $\\alpha_{i}$ will change to ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\alpha_{i}^{\\prime}=\\frac{e^{\\beta_{i}T_{l}}}{\\left(\\sum_{j=0}^{n}e^{\\beta_{j}T_{l}}\\right)+e^{\\beta_{n+1}T_{l}}}}\\\\ {\\quad=\\frac{e^{\\beta_{i}T_{l}}}{\\sum_{j=0}^{n}e^{\\beta_{j}T_{l}}}\\cdot\\frac{\\sum_{j=0}^{n}e^{\\beta_{j}T_{l}}}{\\left(\\sum_{j=0}^{n}e^{\\beta_{j}T_{l}}\\right)+e^{\\beta_{n+1}T_{l}}}}\\\\ {\\quad=\\alpha_{i}\\cdot\\frac{\\sum_{j=0}^{n}e^{\\beta_{j}T_{l}}}{\\left(\\sum_{j=0}^{n}e^{\\beta_{j}T_{l}}\\right)+e^{\\beta_{n+1}T_{l}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "As we want to ensure the stability of model performance, the overall model weights $\\theta_{l}^{n+2}$ should be the same as $\\theta_{l}^{n+1}$ that learned on the $(n{+}1)$ -th domain, so $\\Delta\\pmb{\\theta}_{n+1}=\\pmb{\\theta}_{l}^{n+2}-\\pmb{\\theta}_{l}^{o}=\\pmb{\\theta}_{l}^{n+1}-\\pmb{\\theta}_{l}^{o}$ . ", "page_idx": 17}, {"type": "text", "text": "Thus, we expand $\\theta_{l}^{n+2}$ as: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\theta_{l}^{n+2}=\\theta_{l}^{\\prime}+\\alpha_{0}^{\\prime}\\Delta\\theta_{0}+\\alpha_{l}^{\\prime}\\Delta\\theta_{1}+\\cdots+\\alpha_{n}^{\\prime}\\Delta\\theta_{n}+\\alpha_{n+1}\\Delta\\theta_{n+1}+\\Delta\\theta^{\\prime}}\\\\ &{\\quad=\\theta_{l}^{\\prime}+\\frac{\\sum_{i=0}^{n}e^{\\beta_{l}T_{i}}}{(\\sum_{i=0}^{n}e^{\\beta_{l}T_{i}})}+e^{\\beta_{n+1}T_{i}}(\\alpha_{0}\\Delta\\theta_{0}+\\alpha_{1}\\Delta\\theta_{1}+\\cdots+\\alpha_{n}\\Delta\\theta_{n})}\\\\ &{\\quad\\quad+\\frac{e^{\\beta_{n+1}T_{i}}}{(\\sum_{i=0}^{n}e^{\\beta_{l}T_{i}})}+e^{\\beta_{n+1}T_{i}}\\cdot\\Delta\\theta_{n+1}+\\Delta\\theta^{\\prime}}\\\\ &{\\quad=\\theta_{l}^{\\prime}+\\frac{\\sum_{i=0}^{n}e^{\\beta_{l}T_{i}}}{(\\sum_{i=0}^{n}e^{\\beta_{l}T_{i}})}+e^{\\beta_{n+1}T_{i}}(\\theta_{l}^{n+1}-\\theta_{l}^{\\prime}-\\Delta\\theta)}\\\\ &{\\quad\\quad+\\frac{e^{\\beta_{n+1}T_{i}}}{(\\sum_{i=0}^{n}e^{\\beta_{l}T_{i}})+e^{\\beta_{n+1}T_{i}}}\\cdot(\\theta_{l}^{n+1}-\\theta_{l}^{\\prime})+\\Delta\\theta^{\\prime}}\\\\ &{\\quad=\\theta_{l}^{n+1}-\\frac{\\sum_{i=0}^{n}e^{\\beta_{l}T_{i}}}{(\\sum_{i=0}^{n}e^{\\beta_{l}T_{i}})+e^{\\beta_{n+1}T_{i}}}\\cdot\\Delta\\theta+\\Delta\\theta^{\\prime}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Considering $\\pmb{\\theta}_{l}^{n+2}=\\pmb{\\theta}_{l}^{n+1}$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pmb{\\theta}_{l}^{n+1}-\\frac{\\sum_{i=0}^{n}e^{\\beta_{i}T_{l}}}{\\left(\\sum_{i=0}^{n}e^{\\beta_{i}T_{l}}\\right)+e^{\\beta_{n+1}T_{l}}}\\cdot\\pmb{\\Delta\\theta}+\\Delta\\pmb{\\theta^{\\prime}}=\\pmb{\\theta}_{l}^{n+1}}\\\\ {\\Rightarrow}&{\\Delta\\pmb{\\theta}^{\\prime}=\\frac{\\sum_{i=0}^{n}e^{\\beta_{i}T_{l}}}{\\left(\\sum_{i=0}^{n}e^{\\beta_{i}T_{l}}\\right)+e^{\\beta_{n+1}T_{l}}}\\cdot\\Delta\\pmb{\\theta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "As $\\Delta\\pmb{\\theta}^{\\prime}$ should be as small as possible to enable flexible adaptation, the magnitude of $\\Delta\\pmb{\\theta}^{\\prime}$ should be no larger than $w_{m}$ , i.e. $\\xi^{\\prime}\\leq w_{m}$ . So we scale $\\Delta\\pmb{\\theta}$ by a factor of $s$ , i.e. $\\textstyle\\Delta\\theta^{\\prime}={\\frac{1}{s}}\\cdot\\Delta\\theta$ . If the magnitude of $\\xi^{\\prime}$ already satisfies $\\xi^{\\prime}\\leq w_{m}$ , we set $s=1$ . Otherwise, we set $\\begin{array}{r}{s=\\frac{\\xi}{w_{m}}}\\end{array}$ . $\\begin{array}{r}{s=\\operatorname*{max}\\{1,\\frac{\\xi}{w_{m}}\\}}\\end{array}$ ", "page_idx": 18}, {"type": "text", "text": "Thus, we have and ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\frac{\\sum_{i=0}^{n}e^{\\beta_{i}T_{l}}}{(\\sum_{i=0}^{n}e^{\\beta_{i}T_{l}})+e^{\\beta_{n+1}T_{l}}}=\\frac{1}{s}}\\\\ {\\Rightarrow}&{\\displaystyle\\beta_{n+1}=\\frac{1}{T_{l}}\\ln\\left((s-1)\\sum_{i=0}^{n}e^{\\beta_{i}T_{l}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "C More implementation details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "C.1 More details on datasets ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this paper, we conduct experiments on ImageNet-1K [6] and its five variants to evaluate the out-of-distribution generalization ability, i.e., ImageNet-C [16], ImageNet-R [15], ImageNet-A [17], ImageNet-V2 [44], and ImageNet-Sketch [55]. ", "page_idx": 19}, {"type": "image", "img_path": "YyMiO0DWmI/tmp/bdbb446c49a9c9ce9410f45b0cc6ffb94f48b1232e845d03c8595ef9069f6fde.jpg", "img_caption": ["Figure A: Visualizations of images in ImageNet and ImageNet-C/V2/A/R/Sketch. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "ImageNet-C consists of various versions of corruption applied to 50,000 validation images from ImageNet. The dataset encompasses 15 distinct corruption types of 4 main groups, including Gaussian noise, shot noise, impulse noise, defocus blur, glass blur, motion blur, zoom blur, snow, frost, fog, brightness, contrast, elastic transformation, pixelation, and JPEG compression. Each corruption type is characterized by 5 different levels of severity, with higher severity levels indicating a more severe distribution shift. In our experiments, we specifically utilize severity level 5 for evaluation. ", "page_idx": 19}, {"type": "text", "text": "ImageNet-R contains 30,000 images featuring diverse artistic renditions of 200 ImageNet classes.   \nThese images are predominantly sourced from Flickr and filtered by Amazon MTurk annotators. ", "page_idx": 19}, {"type": "text", "text": "ImageNet-A comprises 7,500 images covering 200 ImageNet classes. These images are derived from real-world, naturally occurring examples that lead to a notable degradation in classifier performance. ", "page_idx": 19}, {"type": "text", "text": "ImageNet-V2 is a newly collected test dataset extracted from the same test distribution as ImageNet. It comprises three test sets, each containing 10,000 new images and covering 1000 ImageNet classes. Following previous TTA methods [36], we utilize the Matched Frequency subset of ImageNet-V2 for evaluation, in which the images are sampled to match the class frequency distributions of the original ImageNet validation dataset. ", "page_idx": 19}, {"type": "text", "text": "ImageNet-Sketch consists of 50,899 images represented as black and white sketches, encompassing 1000 ImageNet classes. Each class contains approximately 50 images. ", "page_idx": 19}, {"type": "text", "text": "C.2 More experimental protocols on evaluation ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We use ViT-Base [8] as the source model for all experiments except Table 7. The model is trained on the source ImageNet-1K training set and we directly obtain the model weights3 from timm4 repository [60]. In Table 7, we adopts CLIP-RN50 [43] as the source model by following TPT [47]. The model weights5 are directly obtained from the original $\\mathrm{CLIP}^{6}$ repository [43]. All experiments are conducted on a single NVIDIA A100 GPUS, using PyTorch framework with version 1.8.0. ", "page_idx": 19}, {"type": "text", "text": "Evaluation on lifelong TTA. In Table 1, the model is online adapted to 15 corruptions over 10 rounds (total 150 corruptions), where the parameters will never be reset. Specifically, the corruptions comes in the following order for each round: Gaussian Noise $\\rightarrow$ Defocus $3\\mathrm{lur}\\to\\mathrm{Snow}\\to\\mathrm{C}$ ontrast $\\rightarrow$ Shot Noise $\\rightarrow$ Glass Blur $\\rightarrow\\mathrm{Frost\\rightarrow}$ Elastic Transform $\\rightarrow$ Impulse No $\\mathrm{ise\\toMotion\\;Blur\\;-}$ $\\rightarrow\\mathrm{Fog}$ $\\rightarrow$ Pixelate $\\rightarrow$ Brightness $\\rightarrow$ Zoom Blur $\\rightarrow$ JPEG Compression. Here, subsequent corruptions differ in type, which poses a more significant challenge for TTA methods to leverage previously learned knowledge for adaptation. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "Evaluation on collaborative TTA. In Table 2, we evaluate our effectiveness in collaborating on multiple resource-abundant principal devices, where the learned knowledge on each device is shared with others post-adaptation to each type of corruption. Specifically, the corruption order in each type of corruption is as follows. 1) Noise: Gaussian Noise $\\rightarrow$ Shot Noise $\\rightarrow$ Impulse Noise. 2) Blur: Defocus Blur $\\rightarrow$ Glass Blur $\\rightarrow$ Motion B $\\mathrm{iur}\\to7\\mathrm{oom}$ Blur. 3) Weather: Snow $\\to\\mathrm{Frost}\\to\\mathrm{Fog}\\to$ Brightness. 4) Digital: Contrast $\\rightarrow$ Elastic Transformation $\\rightarrow$ Pixelate $\\rightarrow$ JPEG Compression. ", "page_idx": 20}, {"type": "text", "text": "Then, given learned knowledge from Table 2, we further verify our effectiveness on resource-limited follower devices in Table 3, e.g., CoLA (SAR) utilizes the learned weights of $\\mathrm{SAR+CoLA}$ from Table 2, and CoLA (ETA) utilizes the learned weights of $\\mathrm{ETA+CoLA}$ from Table 2. Here, the resource-limited follower devices adapt in a lifelong manner per EATA [38], where the corruptions come in the following order: Gaussian Noise $\\rightarrow$ Shot Noise $\\rightarrow$ Impulse Noise $\\rightarrow$ Defocus Blur $\\rightarrow$ Glass Blur $\\rightarrow\\mathrm{Motion~Blur}\\rightarrow Z\\mathrm{oom~Blur}\\rightarrow\\mathrm{Snow}\\rightarrow\\mathrm{Frost}\\rightarrow\\mathrm{Fog}\\rightarrow\\mathrm{Brightness}\\rightarrow\\mathrm{Contrast}\\rightarrow\\mathrm{Arposion}.$ $\\rightarrow$ Elastic Transformation $\\rightarrow$ Pixelate $\\rightarrow$ JPEG Compression. ", "page_idx": 20}, {"type": "text", "text": "Evaluation on single-domain TTA. In Table 4, we validate our CoLA in both the wild scenario (i.e., imbalanced label distribution shifts and mixed domain shifts) and the mild scenario of single-domain TTA, where the model is reset post-adaptation to each corruption. Here, imbalanced label distribution shifts denotes scenarios where test data come in a class order, mixed domain shifts denotes scenarios where test data are drawn from multiple randomly mixed domains with different distribution shifts. ", "page_idx": 20}, {"type": "text", "text": "Evaluation on prompt tuning. In Table 7, CoLA aggregates 78 diverse hard prompts with the class token at the end for Eqn. (2), where different prompts are padded to the same length by inserting empty spaces at the beginning. Note that adaptation is conducted in an episodic manner by following TPT [47], where we reset $\\Delta\\theta\\,=\\,0$ post-adaptation to each test sample in Eqn. (2), while $_{\\alpha}$ is continually optimized without reset due to its stability benefti from normalization. Results in Table 7 further demonstrate our effectiveness across various TTA scenarios. The utilized hard prompts for Eqn. (2) are originally developed by [43], as listed below: ", "page_idx": 20}, {"type": "text", "text": "List of Hard Prompts: ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "a drawing of the {class}, art of a {class}, itap of the {class}, a drawing of a {class}, a origami {class}, a photo of a nice {class}, a blurry photo of a {class}, a close-up photo of the {class}, a photo of a clean {class}, a photo of a weird {class}, a photo of a small {class}, a photo of the large {class}, a pixelated photo of the {class}, a embroidered {class}, a photo of the clean {class}, the origami {class}, the plushie {class}, a photo of a cool {class}, a sculpture of the {class}, a low resolution photo of the {class}, a bad photo of the {class}, a jpeg corrupted photo of a {class}, a rendition of the {class}, a photo of the cool {class}, a low resolution photo of a {class}, a cropped photo of the {class}, the plastic {class}, a sculpture of a {class}, a pixelated photo of a {class}, itap of a {class}, a doodle of a {class}, a sketch of a {class}, a plastic {class}, itap of my {class}, a close-up photo of a {class}, a bright photo of a $\\{\\mathrm{class}\\}$ , art of the {class}, graffiti of the {class}, a tattoo of a {class}, a sketch of the $\\{\\mathrm{class}\\}$ , a dark photo of a {class}, a tattoo of the {class}, a photo of the dirty {class}, a black and white photo of the {class}, a photo of a {class}, a painting of the {class}, a cropped photo of a {class}, a photo of a large {class}, a photo of the weird {class}, graffiti of a {class}, a painting of a {class}, a cartoon {class}, the cartoon {class}, a good photo of the {class}, a jpeg corrupted photo of the {class}, a bad photo of a {class}, a photo of the small {class}, a rendering of the {class}, a photo of a dirty {class}, a rendition of a {class}, a blurry photo of the {class}, the toy $\\{\\mathrm{class}\\}$ , the embroidered $\\{\\mathrm{class}\\}$ , a rendering of a $\\{\\mathrm{class}\\}$ , a photo of a hard to see {class}, a dark photo of the {class}, a doodle of the {class}, a good photo of a {class}, a photo of the {class}, a photo of many {class}, a plushie {class}, a photo of the nice {class}, a bright photo of the {class}, a toy {class}, a photo of the hard to see {class}, a photo of one {class}, a photo of my {class}, a black and white photo of a {class}, a sketch of a {class} ", "page_idx": 20}, {"type": "text", "text": "C.3 More experimental protocols on methods ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "CoLA (Ours). For principal agents, We directly leverage the learnable test-time objectives from the integrated TTA methods, as listed below. In Eqn. (2), $\\Delta\\pmb{\\theta}$ is optimized by following the update rules and the hyper-parameters of the integrated baseline, $_{\\alpha}$ and $T_{l}$ is updated via the AdamW optimizer with a learning rate of 0.1, and we introduce a weight decay of 0.1 on $_{\\alpha}$ . We set the shift detection threshold $z$ to 0.1 for all experiments. Moreover, we introduce a weight decay on $\\Delta\\pmb{\\theta}$ for Table 1 and Table K, i.e., $0.1/0.4/0.4$ for SAR/ETA/DeYO in Table 1 and $0/0.4/10$ for EATA/ETA/SAR in Table K, respectively. This weight decay, which can be viewed as a simple implementation of the regularizer in EATA [38], helps learn compact and non-redundant knowledge in the domain vectors, thereby benefiting knowledge accumulation. For follower agents, we consistently set $T_{f}$ in Eqn. (3) to 5 for all experiments. Moving average factor $\\lambda$ is set to 0.2. ", "page_idx": 21}, {"type": "text", "text": "$\\mathbf{S}\\mathbf{A}\\mathbf{R}^{7}$ [39]. We follow all hyper-parameters that are set in SAR unless it does not provide. Specifically, we use SGD as the update rule, with a momentum of 0.9, batch size of 64, and a learning rate of 0.001. The entropy threshold $E_{0}$ is set to $0.4\\!\\times\\!\\ln C$ , where $C$ is the number of task classes. The trainable parameters are the affine parameters of the layer normalization layers from blocks 1 to blocks 8. ", "page_idx": 21}, {"type": "text", "text": "ETA & EATA8 [38]. We follow all hyper-parameters that are set in ETA/EATA unless it does not provide. Specifically, we use SGD as the update rule, with a momentum of 0.9, batch size of 64, and a learning rate of 0.001. The entropy threshold $E_{0}$ is set to $0.4\\!\\times\\!\\ln C$ , where $C$ is the number of task classes. For EATA, i.e., ETA with an anti-forgetting regularizer, we use 2,000 samples to estimate the importance of each parameter. The trainable parameters are all affine parameters of layer normalization layers. ", "page_idx": 21}, {"type": "text", "text": "$\\mathbf{DeYO^{9}}$ [25]. We follow all hyper-parameters that are set in SAR unless it does not provide. Specifically, we use SGD as the update rule, with a momentum of 0.9, batch size of 64, and a learning rate of 0.001. The entropy threshold $E_{0}$ is set to $0.4\\!\\times\\!\\ln C$ and the entropy factor $\\tau_{\\mathrm{Ent}}$ is set to $0.5\\!\\times\\!\\ln C$ , where $C$ is the number of task classes. The Pseudo-Label Probability Difference (PLPD) threshold $\\tau_{\\mathrm{PLPD}}$ is set to 0.3 in Table 4, and 0.2 for other experiments by following the original paper. Trainable parameters are the affine parameters of the layer normalization layers from blocks 1 to blocks 8. ", "page_idx": 21}, {"type": "text", "text": "CoTTA10 [56]. We follow all hyperparameters that are set in CoTTA unless it does not provide. Specifically, we use SGD as the update rule, with a momentum of 0.9, and a batch size of 64. The learning rate is chosen from {0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001} and the augmentation threshold $p_{t h}$ is chosen from {0.1, 0.2}. In all experiments, we consistently set the learning rate to 0.001 and $p_{t h}$ to 0.1 given the optimal accuracy observed in Table 1. For images below the threshold, we conduct 32 augmentations including color jitter, random affine, Gaussian blur, random horizontal filp, and Gaussian noise. The restoration probability of is set to 0.01 and the EMA factor $\\alpha$ for teacher update is set to 0.999. The trainable parameters are all the parameters in ViT-Base. ", "page_idx": 21}, {"type": "text", "text": "$\\mathbf{LAME}^{11}$ [3]. For fair comparison, we maintain a consistent batch size of 64 for LAME, aligning it with the same batch size used by other methods in our evaluation. We use the kNN affinity matrix with the value of $\\mathbf{k}$ chosen from $\\{1,5,10,20\\}$ , and for all experiments, we consistently set it to 5 based on the optimal accuracy observed in Table 3. ", "page_idx": 21}, {"type": "text", "text": "$\\mathbf{T3A}^{12}$ [21]. We follow all hyper-parameters that are set in T3A unless it does not provide. Specifically, the batch size is set to 64. The number of supports to restore $M$ is chosen from {1, 5, 20, 50, 100}, and for all experiments, we set it to 20 based on the optimal accuracy observed in Table 3. ", "page_idx": 21}, {"type": "text", "text": "$\\mathbf{TPT}^{13}$ [47]. We follow all hyper-parameters that are set in TPT unless it does not provide. Specifically, we use AdamW as the update rule, with batch size of 1 and a learning rate of 0.005. Learnable tokens are initialized from the hard prompt of \u2018a photo of a\u2019. The confidence threshold $\\rho$ is set to 0.1 and the number of TPT steps is set to 1. ", "page_idx": 21}, {"type": "text", "text": "D More experimental results ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In the main paper, we only report the results averaged over 15 corruptions in ImageNet-C (i.e., in Table 1 and Table 4) or over each group of corruptions (i.e., in Table 2) due to page limit. In this section, we offer additional results to enable a more comprehensive comparison. ", "page_idx": 22}, {"type": "text", "text": "Results under lifelong test-time adaptation. The knowledge sharing scheme in CoLA can also be used to mitigate catastrophic forgetting in lifelong TTA, similar to the aim of conventional supervised continual learning [68]. To further verify this, we provide more detailed results of Table 1, regarding the Accuracys on the first and last rounds of adaptation. From Table C, our CoLA outperforms the integrated baseline in the first round of adaptation, e.g., the accuracy of $62.0\\%$ $\\mathrm{\\DeltaETA+CoLA}$ ) vs. $61.4\\%$ (ETA). This mainly stems from the latter corruptions encountered in the first round, where CoLA demonstrates superiority by leveraging the learned knowledge from previous adaptation with Eqn. (2). More importantly, our enhancement becomes particularly significant at the last round of adaptation, e.g., the accuracy of $65.4\\%$ $\\mathrm{(ETA+CoLA)}$ ) vs. $35.1\\%$ (ETA), further indicating our effectiveness in accumulating and utilizing learned knowledge for long-range adaptation. Note that CoLA applies weight decay on $\\Delta\\pmb{\\theta}$ as claimed in Section C, thus achieving a lower performance on the first corruption. Here, the relatively limited performance of CoTTA [56] is attributed to its sensitivity to corruption order, as shown in Table D. ", "page_idx": 22}, {"type": "text", "text": "Results under collaborative test-time adaptation. We provide more detailed results of Table 2, regarding the Accuracys on each device. From Table D, our CoLA outperforms the integrated baseline from the adaptation to the second group of corruption, e.g., the accuracy of $53.6\\%$ $\\mathrm{SAR+CoLA})$ vs. $38.9\\%$ (SAR) on \u2018Gaussian\u2019 in Device 2. Moreover, this improvement becomes increasingly more pronounced as more knowledge is shared across devices, e.g., improving the accuracy from $37.5\\%$ (SAR) to $53.9\\%$ $\\mathrm{{(SAR+CoLA)}}$ on \u2018Gaussian\u2019 in Device 3. This phenomenon underscores the importance of cross-device collaboration and our effectiveness regarding collaborative TTA. ", "page_idx": 22}, {"type": "text", "text": "Results under single-domain test-time adaptation. We provide more detailed results of Table 4, regarding the Accuracys under mild scenarios and online imbalanced label distribution shifts. From Table B, incorporating CoLA with existing TTA solutions enhances the adaptation performance on most corruptions under both scenarios, demonstrating our effectiveness. Specifically, CoLA showcases a more pronounced improvement in SAR and ETA. This can be attributed to the inadequacy of prediction entropy to identify reliable samples for model updates, thereby suffering more from error accumulation. CoLA alleviates this by dynamically favoring a more optimal checkpoint based on loss minimization instead of the newest saved one that may have learned from erroneous predictions, rendering CoLA more robust to noise. We also visualize $\\alpha_{i}$ in Appendix E to offer more insights. ", "page_idx": 22}, {"type": "text", "text": "Table B: Comparisons on ImageNet-C (level 5) regarding Accuracy $(\\%)$ under single-domain TTA. In mild scenarios, the test samples come in random order by following Tent [54]. Label Shifts is short for online imbalanced label distribution shifts, where test samples come in class order per SAR [39]. ", "page_idx": 22}, {"type": "table", "img_path": "YyMiO0DWmI/tmp/281819f15ed33092d76419c763076eb4d138cd84f5cc044b3b39711441c68506.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "YyMiO0DWmI/tmp/b7c27e113cd8546da9a46856bbceec96c5c53571fe74b2feb2a4e461794becd2.jpg", "table_caption": ["Table C: Comparison on ImageNet-C (severity level 5) regarding Accuracy $(\\%)$ under lifelong adaptation for 10 rounds. Here, we provide additional results on the first and last rounds of adaptation. "], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "YyMiO0DWmI/tmp/71c7816c2668e4153fdacaac2a2b9477bb667412e12573bfc35d71ce82d3835a.jpg", "table_caption": ["Table D: Effectiveness under collaborative adaptation across resource-abundant principal devices w.r.t. Acc. $(\\%)$ . Results are evaluated on ImageNet-C (severity level 5, containing 15 corruption types of 4 groups). We share learned weights across devices post-adaptation to each group of corruptions. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "E Additional discussions ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Robustness of CoLA against potential harmful knowledge. Note that CoLA conducts test-time learning to adaptively aggregate prior knowledge. If the prior knowledge shared from some devices is harmful, using such knowledge shall not decrease the test-time objective. Thus, such prior knowledge will not be used for model adaptation. In this sense, CoLA remains stable even with harmful prior knowledge (e.g., the domain knowledge in some devices is not helpful for principal agents to do TTA). This stability also beneftis from our initialization strategy as shown in Table E, which carefully initializes $_{\\alpha}$ and $\\Delta\\pmb{\\theta}$ to prevent the aggregation of potentially harmful knowledge for stable warm-up. ", "page_idx": 24}, {"type": "text", "text": "Table E: Robustness of our CoLA using only pre-trained parameters and $N$ harmful prior knowledge, i.e., the randomly initialized domain vectors. Results are obtained on ImageNet-C (Gaussian, level 5). ", "page_idx": 24}, {"type": "table", "img_path": "YyMiO0DWmI/tmp/d01e8052efb32e67ad32557cf6f040db01062bed501800d76660c366ed3831fc.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "Scalability of CoLA with more collaborative devices. CoLA scales well with an increasing number of devices, i.e., with more shared domain vectors. From Table F, ETA/ETA $^{+}$ CoLA consistently beneftis from additional participating devices, e.g., ETA $^{+}$ CoLA achieves an accuracy of $65.2\\%$ with 11 devices compared to $61.8\\%$ with one device. This highlights the importance of cross-device collaboration and CoLA \u2019s effectiveness under more large-scale multi-device collaborative TTA. ", "page_idx": 24}, {"type": "text", "text": "Table F: Effectiveness of CoLA with an increasing number of principal devices (with back-propagation capability). Here, each device continuously encounters 15 domains from ImageNet-C (level 5) in different domain orders. Results are averaged over all principal devices. ", "page_idx": 24}, {"type": "table", "img_path": "YyMiO0DWmI/tmp/c57c01335e45f46c7f4c829c7be48bb06112cc94f2c3944d6f9b120f938c3e73.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "Efficiency of CoLA with increasing domain vectors. Our CoLA is both computation and memory efficient at exploiting domain vectors. From Table H, CoLA efficiently scales to over 10,000 domain vectors, incurring only an additional 11s of runtime and 1,502MB of extra memory, yet remains substantially more efficient than CoTTA. We believe that 10,000 domain vectors should be adequate for handling most real-world applications with proper management. ", "page_idx": 24}, {"type": "text", "text": "Table G: Efficiency comparison on ImageNet-C (Gaussian, level 5) using a single A100. $N$ is the number of domain vectors, which we initialize as random parameters in this table. ", "page_idx": 24}, {"type": "table", "img_path": "YyMiO0DWmI/tmp/bdd991f0a88fcf2fd11ffb53e4660c6ce196bb5a130a5f8f13e126d97f9f5407.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "Sensitivity of threshold $z$ for shift detection. CoLA remains effective among a wide range of threshold $z$ , as shown in Table A. From the results, while a stricter threshold saves more domain vectors, CoLA achieves a stable performance of around $64.7\\%$ . When threshold $z$ increases, CoLA saves significantly fewer domain vectors and still enhances the performance significantly, i.e., the average accuracy of $62.2\\%$ in $\\mathrm{ETA+CoLA}$ $_{z=10}$ ) vs. $46.4\\%$ in ETA. ", "page_idx": 24}, {"type": "text", "text": "Table H: Sensitivity of threshold $z$ . Experiments follow the settings of Table 1, i.e., single-device lifelong adaptation, and CoLA is incorporated with ETA. We report average accuracy over 10 rounds, each comprising 15 corruptions of ImageNet-C. The average accuracy of the ETA baseline is $46.4\\%$ . ", "page_idx": 24}, {"type": "table", "img_path": "YyMiO0DWmI/tmp/04894b5341b380c7bd2c0bd30479a56d39c49b926e1298de96a43989fc4a54cf.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "Advantages of CoLA over FedAvg [32] in collaborative TTA. We further compare our CoLA with FedAvg [32] in cross-device collaborative/federated learning. From Table I, CoLA consistently outperforms FedAvg when incorporated with the baseline for collaborative TTA. More importantly, FedAvg, which simply averages the model parameters on different devices, may deteriorate model performance (i.e., the average accuracy of $61.2\\%$ in ETA vs. $58.0\\%$ in ETA $^{+}$ FedAvg). This is because different devices may encounter different distribution shifts, and thus the knowledge from other devices may not be beneficial for adapting to the current domain. In contrast, our CoLA addresses this by learning at test time to optimize the aggregation of knowledge from different devices. ", "page_idx": 25}, {"type": "table", "img_path": "YyMiO0DWmI/tmp/440ac9d2e287cabb54f406c4db470dd9220f8416673005d82105878d9e7d679d.jpg", "table_caption": ["Table I: Effectiveness of CoLA and FedAvg [32] for cross-device collaborative TTA w.r.t. Acc. $(\\%)$ . The experiments follow the settings of Table 2 in the main paper. "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "Robustness of CoLA under small batch sizes. The stability of CoLA under small batch sizes is primarily determined by the base algorithms, such as ETA and SAR, rather than CoLA itself. This is because CoLA is a plug-and-play module designed to be incorporated with existing methods. We provide further empirical results to verify this robustness. From Table J, CoLA achieves a consistent result when incorporated with SAR, demonstrating no performance degradation as the batch size reduces from 16 to 2. Meanwhile, CoLA can also help improve stability under small batch sizes. For instance, as the batch size reduces from 4 to 2, ETA $^{+}$ CoLA achieves nearly no performance degradation while the baseline ETA\u2019s performance degrades by $1.4\\%$ . ", "page_idx": 25}, {"type": "table", "img_path": "YyMiO0DWmI/tmp/0491ab11b6b906dd62db347d90d1df0cc7d48336d7de04ab488b0b5f410b806e.jpg", "table_caption": ["Table J: Robustness of our CoLA under various batch sizes. We follow the same settings of Table 2 in the main paper and report average accuracy over all devices and corruptions here. "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "Effectiveness of CoLA on ResNet models. Table K demonstrates the effectiveness of our CoLA on ResNet-50, where CoLA updates and stores the affine parameters of batch normalization layers in ResNet. From Table K, CoLA consistently enhances the performance of ETA/EATA/SAR throughout 10 rounds of adaptation and addresses the issue of performance degradation in long-term adaptation. These results are consistent with Table 1 in the main paper using ViT-Base, further indicating CoLA\u2019s effectiveness in accumulating and exploiting learned knowledge with diverse model architectures. ", "page_idx": 25}, {"type": "table", "img_path": "YyMiO0DWmI/tmp/fe6de91779019acd1716d10e0abdd69395d517ab625177e53b1b587876f5681a.jpg", "table_caption": ["Table K: Effectiveness of CoLA on ResNet-50 in the lifelong TTA scenarios following Table 1. "], "table_footnote": [""], "page_idx": 25}, {"type": "text", "text": "Effectiveness of CoLA in sample efficiency on unseen distributions. We validate the effectiveness of Eqn. (2) to facilitate sample-efficient TTA on unseen distributions. From Figure B, CoLA consistently enhances the sample efficiency and the overall performance, i.e., with an up to $30.0\\times$ speed up on both ImageNet-R and ImageNet-Sketch, indicating that the effectiveness of CoLA is not limited to previously encountered distributions. More interestingly, compared with SAR, CoLA helps mitigate overfitting on ImageNet-Sketch. This can be attributed to that, in the unsupervised adaptation, SAR learns on more erroneous predictions since its performance is particularly limited at the beginning of adaptation. This phenomenon further demonstrates the importance of sample efficiency and our effectiveness in leveraging shared knowledge for efficient TTA. ", "page_idx": 26}, {"type": "image", "img_path": "YyMiO0DWmI/tmp/6b842bf0f4e412ce1a15764b2ab039fc8418838412c794c37502fca35d3601c1.jpg", "img_caption": ["Figure B: Additional comparisons w.r.t sample efficiency on unseen distributions. Here, CoLA leverages learned weights on ImageNet-C (i.e., from $\\mathrm{SAR+CoLA}$ in Table 2, the fifth row) while the effectiveness in sample efficiency is evaluated on ImageNet-R and ImageNet-Sketch. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Effectiveness of CoLA in mitigating error accumulation. We provide visualization of the learned $\\beta_{i}$ in Figure C. Here, all domain vectors are learned on the same domain, i.e., Gaussian Noise, where we save learned weights post-adaptation to 10 batches of samples. Generally, a later saved domain vector should be prioritized as it learns on more samples. Nevertheless, in the context of unsupervised TTA, a model may learn from erroneous pseudo-labels, where the performance would significantly deteriorate, known as error accumulation. From Figure C, CoLA adaptively assigns lower $\\beta_{i}$ on domain vectors that may have accumulated error, e.g., from the 80-th to the 90-th domain vectors, according to loss optimization. Thus, CoLA demonstrates potential effectiveness in mitigating error accumulation, enhancing ETA\u2019s performance by $+7.6\\%$ on ImageNet-C under label distribution shifts. ", "page_idx": 26}, {"type": "image", "img_path": "YyMiO0DWmI/tmp/bd81c9b2d30c3cfc452f0c595b91b142019f85dbf9f670a3af0027a6b26be2c8.jpg", "img_caption": ["Figure C: Visualization of $\\beta_{i}$ for $\\mathrm{ETA+CoLA}$ on ImageNet-C(severity level 5, Gaussian) under online imbalanced label distribution shifts. CoLA saves learned weights for every adaptation to 10 batches of samples while no weights are discarded for visualization. Learned temperature $T_{l}$ is 0.49. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Effectiveness of our domain shift detector. We provide additional results to demonstrate the effectiveness of our domain distance function, i.e., Eqn. (7), for shift detection. In our experiments, we consistently set $\\phi_{d}$ to 0.1 for shift detection. From Figure D, our domain shift detector demonstrates sensitivity between corruptions from different groups, e.g., with a distance of 99 between \u2018impulse noise\u2019 and \u2018contrast\u2019, and a distance of 32 between \u2018gaussian noise\u2019 and \u2018defocus blur\u2019. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "image", "img_path": "YyMiO0DWmI/tmp/c821d9fdcbe3ff4226085016a989274b77f1037410e0835d0d188d1136360a33.jpg", "img_caption": ["Figure D: Effectiveness of our domain distance function, i.e., $D(\\cdot,\\cdot)$ defined in Eqn. (7), to capture the magnitude of domain shift. Here, we estimate the domain distance between each corruption in ImageNet-C (level 5, with 15 corruptions). The statistic of each domain is estimated via Eqn. (6). "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Statistical comparison. We re-run Table 2 in the main paper with 5 different seeds and report the mean and std of each method in Table L. The results show that CoLA performs stably with small stds and it lowers the std of ETA&DeYo, suggesting CoLA\u2019s stability. ", "page_idx": 27}, {"type": "table", "img_path": "YyMiO0DWmI/tmp/26b844aee0a77afe7b9fb05b0c3972ad4cd23c159a39eb4b0738683c220b7ded.jpg", "table_caption": ["Table L: Statistical comparison. Experiments follow the settings of Table 2. "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "F Limitations and future works ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Finetuning LLM. We mainly verify our CoLA with vision models in the context of test-time adaptation. Since our formulation in Eqn. (2) does not necessitate being optimized during testing, it\u2019s an interesting future work to verify CoLA on more scenarios. For instance, finetuning the large language models, where multiple finetuned weights, e.g., using LoRA [20], are publicly available. ", "page_idx": 28}, {"type": "text", "text": "Shrinking the shared vectors. Our CoLA continuously expands the size of the shared domain vectors across devices. Intuitively, on the same domain, one can reduce memory consumption by preserving only the best-performing domain vector. Although we have demonstrated a feasible strategy for shared vectors shrinking on the single-domain adaptation, where we discard the unused ones according to $\\alpha_{i}$ in Table 4. Nevertheless, it\u2019s still challenging to perform share vectors shrinking across multiple devices and we leave it for future works. ", "page_idx": 28}, {"type": "text", "text": "G Broader impact ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "This paper aims to advance the field of test-time adaptation for out-of-distribution generalization. The societal impact of our work lies primarily in its potential to expand the usability of machine learning models in real-world settings, particularly on self-driving cars, embodied agents/robots, etc. By enhancing the performance of machine learning models on various real-world devices, our method helps make AI technology more broadly accessible. Ethically, our approach eliminates the need for data transfer between devices, thereby improving data privacy and security. ", "page_idx": 28}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: This paper proposes CoLA, a collaborative lifelong adaptation framework for deep models, enabling efficient cross-device domain adaptation by sharing and accumulating knowledge across devices during test-time adaptation, significantly enhancing performance in varying domain scenarios without sacrificing efficiency. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 29}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We have discussed the limitations of our methods in Appendix F. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 29}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We have provided a proposition and its complete proof in Section B. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 30}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We have fully disclosed all information needed to reproduce the main experimental results of the paper in Appendix C. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 30}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We have provided all source code in the introduction. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 31}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "We have provided detailed experimental settings in Appendix C. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 31}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: Error bars are reported in Appendix E. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We have provided detailed information about our compute resources in Appendix C. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 32}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The paper meets the NeurIPS Code of Ethics. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 32}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We have discussed this in Appendix G. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: This paper poses no such risks. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 33}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: The creators or original owners of assets (e.g., code, data, models) used in the paper are properly credited, and the license and terms of use are explicitly mentioned and properly respected. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 34}, {"type": "text", "text": "Answer: [No] ", "page_idx": 34}, {"type": "text", "text": "Justification: We will release new assets upon acceptance. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 34}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 34}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 34}, {"type": "text", "text": "Answer: [No] ", "page_idx": 34}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 34}]