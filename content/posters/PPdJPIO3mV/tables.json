[{"figure_path": "PPdJPIO3mV/tables/tables_6_1.jpg", "caption": "Table 1: Image-Text Retrieval comparison. PITOME without training are in blue, and with training in gray. PITOME achieves SOTA while saving 36% \u2013 56% in FLOPS and speeding up by \u00d71.4 to \u00d71.6 compared to the base models.", "description": "This table presents a comparison of the image-text retrieval performance of PITOME against other state-of-the-art methods on Flickr30k and MSCOCO datasets, using different backbone models (CLIP, ALBEF, BLIP).  It shows Recall@k scores (a measure of retrieval accuracy), FLOPS (floating point operations - a measure of computational cost), and speed improvements. Results are shown both for off-the-shelf (no retraining) and retrained models.  PITOME consistently achieves top performance with significant reductions in computational cost and improved speed.", "section": "4.1 Image & Text Retrieval"}, {"figure_path": "PPdJPIO3mV/tables/tables_7_1.jpg", "caption": "Table 3: Off-the-shelf LLaVA-1.5 7B (r=0.9) and LLaVA-1.5-13B (r=0.925) performance vs. PITOME and other token pruning/merging methods on six VQA datasets: VQA-v2 [69], GQA [70], VisWiz [71], TextVQA [73], MME [74] ScienceQA image (ScienceQA\u00b9) [72].", "description": "This table compares the off-the-shelf performance of the PITOME algorithm against several baseline methods on six different visual question answering (VQA) datasets.  The results show the accuracy scores obtained using two different LLAVA models (LLaVA-1.5-7B and LLaVA-1.5-13B) before and after applying various token merging techniques. The performance is measured across various metrics relevant for VQA datasets.", "section": "4.2 Visual Question Answering (VQA) with Large Vision-Language Models"}, {"figure_path": "PPdJPIO3mV/tables/tables_7_2.jpg", "caption": "Table 4: Inference time of LLaVA-1.5-7B and LLaVA-1.5-13B models when running on five V100-GPUs and five A100-GPUs.", "description": "This table presents the inference time of two different sizes of LLaVA models (LLaVA-1.5-7B and LLaVA-1.5-13B) when performing visual question answering tasks.  The inference times are measured using both V100 and A100 GPUs for various VQA datasets including VQA-v2, GQA, VizWiz, ScienceQA, TextVQA, and MME. The table shows the model's inference speed across different datasets and hardware configurations. The results highlight the effect of model size and GPU type on the inference time.", "section": "4.2 Visual Question Answering (VQA) with Large Vision-Language Models"}, {"figure_path": "PPdJPIO3mV/tables/tables_8_1.jpg", "caption": "Table 5: Image Classification: Performance of PITOME on Imagenet-1k, both off-the-shelf (OTS acc) and after retraining (Trained acc), across ViT backbones. We benchmark with different architectures and merging/pruning methods.", "description": "This table presents a comparison of the performance of PITOME against other state-of-the-art methods for image classification on the ImageNet-1k dataset.  The comparison includes both off-the-shelf performance (without retraining) and performance after retraining.  Different ViT backbones (ViT-T, ViT-S, ViT-B, ViT-L, ViT-H) are used, along with other advanced architectures and merging/pruning methods. The table shows accuracy, FLOPs (floating point operations), and training speedup for each model.", "section": "4.3 Image Classification on Imagenet-1k"}, {"figure_path": "PPdJPIO3mV/tables/tables_8_2.jpg", "caption": "Table 5: Image Classification: Performance of PITOME on Imagenet-1k, both off-the-shelf (OTS acc) and after retraining (Trained acc), across ViT backbones. We benchmark with different architectures and merging/pruning methods.", "description": "This table presents a comparison of PITOME's performance against other state-of-the-art methods for image classification on the ImageNet-1k dataset using various ViT backbones.  It shows both the off-the-shelf accuracy and accuracy after retraining, along with the FLOPs (floating-point operations) and speed-up achieved by each method. The table is organized by ViT backbone type and then by the different methods including ToMe, ToFu, DiffRate, and PITOME.  The results highlight PITOME's superior performance in terms of accuracy and FLOPs reduction with minimal performance degradation compared to other methods. It also demonstrates that PITOME training is faster than its counter parts.", "section": "4.3 Image Classification on Imagenet-1k"}, {"figure_path": "PPdJPIO3mV/tables/tables_9_1.jpg", "caption": "Table 7: Impact of different settings in Steps 2 and 3. Image-Text Retrieval and Text CLS.", "description": "This table presents the ablation study results for the PITOME algorithm. It shows the impact of removing the token protecting step (Step 2) and using random split in the merging step (Step 3) on the performance of image-text retrieval and text classification tasks. The results are evaluated using Recall Sum for image-text retrieval and accuracy for text classification, with different ratios of remaining tokens (r). The table demonstrates that both steps are crucial for PITOME's performance and that using a random split instead of an ordered approach significantly reduces performance.", "section": "4.5 PITOME Ablation Studies"}, {"figure_path": "PPdJPIO3mV/tables/tables_9_2.jpg", "caption": "Table 8: Impact of the constant a on the image-text retrieval task. Results are in recall sum; higher is better.", "description": "This table presents the ablation study results on the impact of the constant \\( \\alpha \\) on the performance of the proposed PITOME algorithm for image-text retrieval.  Different values of \\( \\alpha \\) (1.0, 0.5, and 0.0) are tested with varying ratios \\( r \\) of remaining tokens. The recall sum, which is the sum of recall@1, recall@5, recall@10 for both image and text retrieval, is used as the evaluation metric. Higher recall sum indicates better performance. The results show the impact of the smoothing constant \\( \\alpha \\) on the algorithm performance for various token compression ratios.", "section": "4.5 PITOME Ablation Studies"}, {"figure_path": "PPdJPIO3mV/tables/tables_18_1.jpg", "caption": "Table 1: Image-Text Retrieval comparison. PITOME without training are in blue, and with training in gray. PITOME achieves SOTA while saving 36% \u2013 56% in FLOPS and speeding up by \u00d71.4 to \u00d71.6 compared to the base models.", "description": "This table presents the results of image-text retrieval experiments using three different backbone models (CLIP, ALBEF, and BLIP) on two datasets (Flickr30k and MSCOCO).  It compares the performance of PITOME (both with and without retraining) against other state-of-the-art merging or pruning methods. The metrics used are Recall@k (R@1, R@5, R@10, and Ri@1, Ri@5, Ri@10), FLOPS, and speedup. The results show PITOME outperforms other methods while significantly reducing FLOPS and improving speed.", "section": "4.1 Image & Text Retrieval"}, {"figure_path": "PPdJPIO3mV/tables/tables_18_2.jpg", "caption": "Table 1: Image-Text Retrieval comparison. PITOME without training are in blue, and with training in gray. PITOME achieves SOTA while saving 36%\u201356% in FLOPS and speeding up by \u00d71.4 to \u00d71.6 compared to the base models.", "description": "This table presents the performance comparison of PITOME with and without retraining on image-text retrieval tasks using various models (CLIP, BLIP, ALBEF) and datasets (Flickr30k, MSCOCO). The results show that PITOME significantly outperforms the baselines in terms of recall metrics (R@1, R@5, R@10), while achieving substantial reductions in FLOPS and inference times, demonstrating its efficiency in accelerating the retrieval process.", "section": "4.1 Image & Text Retrieval"}, {"figure_path": "PPdJPIO3mV/tables/tables_20_1.jpg", "caption": "Table 10: Performance of PITOME versus baselines algorithms when training BERT and Distiled-BERT when retrained from scratch.", "description": "This table presents a comparison of the performance of PITOME against several baseline algorithms in text classification tasks using two different models (BERT and DistilBERT) and two datasets (SST-2 and IMDB).  It shows the accuracy, FLOPS (floating-point operations), and speedup achieved by each method with different compression ratios (r=0.8, r=0.75, r=0.7). The results demonstrate PITOME's improved performance and efficiency compared to other approaches.", "section": "D Additional Experiments on Text Classification Task"}, {"figure_path": "PPdJPIO3mV/tables/tables_30_1.jpg", "caption": "Table 1: Image-Text Retrieval comparison. PITOME without training are in blue, and with training in gray. PITOME achieves SOTA while saving 36%\u201356% in FLOPS and speeding up by \u00d71.4 to \u00d71.6 compared to the base models.", "description": "This table compares the performance of PITOME against other state-of-the-art methods for image-text retrieval tasks using various models (CLIP, ALBEF, BLIP) on two datasets (Flickr30k and MSCOCO).  The results highlight PITOME's superior performance (achieving state-of-the-art results) while significantly reducing computational cost (FLOPS) and inference time.  It shows results for both off-the-shelf (no retraining) and retrained models, demonstrating improvements in both scenarios.", "section": "4.1 Image & Text Retrieval"}, {"figure_path": "PPdJPIO3mV/tables/tables_31_1.jpg", "caption": "Table 1: Image-Text Retrieval comparison. PITOME without training are in blue, and with training in gray. PITOME achieves SOTA while saving 36% \u2013 56% in FLOPS and speeding up by \u00d71.4 to \u00d71.6 compared to the base models.", "description": "This table compares the performance of PITOME against other state-of-the-art methods on image-text retrieval tasks using different backbone models (CLIP, ALBEF, BLIP). It shows the recall@k (a measure of retrieval accuracy), FLOPS (floating-point operations, reflecting computational cost), and speedup achieved by each method.  The results highlight PITOME's superior performance and efficiency gains in both off-the-shelf and retrained scenarios.", "section": "4.1 Image & Text Retrieval"}, {"figure_path": "PPdJPIO3mV/tables/tables_32_1.jpg", "caption": "Table 1: Image-Text Retrieval comparison. PITOME without training are in blue, and with training in gray. PITOME achieves SOTA while saving 36% \u2013 56% in FLOPS and speeding up by \u00d71.4 to \u00d71.6 compared to the base models.", "description": "This table presents the results of image-text retrieval experiments using three different backbone models (CLIP, ALBEF, and BLIP) on two datasets (Flickr30k and MSCOCO).  It compares the performance of PITOME (both with and without retraining) against other state-of-the-art methods. The metrics used are recall@k, FLOPS (floating-point operations), and speedup.  The results demonstrate PITOME's superior performance and efficiency gains compared to the base models and other existing methods.", "section": "4.1 Image & Text Retrieval"}]