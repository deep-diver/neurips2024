[{"figure_path": "PPdJPIO3mV/figures/figures_1_1.jpg", "caption": "Figure 1: A comparison of token merging methods. Patches of the same color are merged. Green arrows highlight incorrect merges, avoided by PITOME. Position of tokens with high attention scores (cyan borders, zoom for clarity) in PITOME are maintained proportionality akin to ViT-base 384.", "description": "This figure compares four different token merging methods: ViT-base 384 (original), PITOME (proposed method), ToMe, and DiffRate.  Each method is visualized using a color-coded representation of merged image patches. The green arrows highlight instances where ToMe and DiffRate incorrectly merge dissimilar tokens, a problem avoided by PITOME.  PITOME preserves the relative positions of tokens with high attention scores (indicated by cyan borders) better than the alternative methods, thus maintaining a more accurate representation of the original image.", "section": "1 Introduction"}, {"figure_path": "PPdJPIO3mV/figures/figures_3_1.jpg", "caption": "Figure 2: a) PITOME can be inserted inside transformer block; b) Energy scores are computed to identify mergeable and protective tokens; c) Our algorithm gradually merges tokens in each block. Here Fmer(.) is the merging operation that receives \u00c2\u00b9 as input for compressing, XWK (key matrices) as the token features of X\u00b9 following prior work [15, 43, 18, 19], and r is the fraction of remaining tokens. The output \u00c2m \u2208 RrNxh serves as input for the MLP layer to produce Xl+1 \u2208 RrN\u00d7h. We present the PITOME Fmer(.) function in the next section.", "description": "This figure illustrates the PITOME algorithm's integration into a transformer block (a), the process of identifying mergeable and protective tokens using energy scores (b), and the gradual merging of tokens within each block (c).  Panel (a) shows PITOME's placement within the transformer architecture. Panel (b) details the steps involved in assigning energy scores to tokens, identifying mergeable and protected tokens, and then splitting the mergeable tokens into two sets (A and B) for merging using the Bipartite Soft Matching (BSM) algorithm. Panel (c) visually demonstrates the merging of tokens across different layers of the ViT architecture.", "section": "3 Methodology"}, {"figure_path": "PPdJPIO3mV/figures/figures_6_1.jpg", "caption": "Figure 3: Off-the-shelf Image-Text Retrieval comparison between PITOME v.s. merging/pruning methods on different backbones on tasks when varying the number of merged tokens. Here, Recall sum =Rt@1 + Rt@5 + Rt@10+ Ri@1 + Ri@5 + Ri@10 is close to 600, indicating recall scores at top 1,5, and 10 for retrieving image and text reached close to 100%. PITOME curves, in most cases, are above other baselines.", "description": "This figure compares the performance of PITOME against other merging/pruning methods on image-text retrieval tasks using various backbones.  The x-axis represents the number of floating-point operations (FLOPS), and the y-axis represents the recall sum (sum of recall@1, recall@5, and recall@10 for both image and text retrieval).  The plot shows that PITOME consistently outperforms other methods across different backbones and datasets, achieving near-perfect recall scores while maintaining a smaller FLOPS count.", "section": "4.1 Image & Text Retrieval"}, {"figure_path": "PPdJPIO3mV/figures/figures_7_1.jpg", "caption": "Figure 3: Off-the-shelf Image-Text Retrieval comparison between PITOME v.s. merging/pruning methods on different backbones on tasks when varying the number of merged tokens. Here, Recall sum =Rt@1 + Rt@5 + Rt@10+ Ri@1 + Ri@5 + Ri@10 is close to 600, indicating recall scores at top 1,5, and 10 for retrieving image and text reached close to 100%. PITOME curves, in most cases, are above other baselines.", "description": "This figure compares the performance of PITOME against other merging and pruning methods on image-text retrieval tasks.  It shows Recall@k scores (k=1,5,10 for both images and texts) for different models (CLIP, ALBEF, BLIP) and datasets (Flickr30k and MS-COCO) with varying FLOPS. The recall sum is used as an overall performance measure, with higher values representing better performance.  The plots demonstrate that PITOME consistently outperforms other methods, especially as the number of merged tokens increases (lower FLOPS).", "section": "4.1 Image & Text Retrieval"}, {"figure_path": "PPdJPIO3mV/figures/figures_9_1.jpg", "caption": "Figure 6: Ablation studies of PITOME.", "description": "The figure shows the ablation study results for PITOME, comparing its performance against various settings.  Specifically, it compares PITOME against versions that do not protect informative tokens, utilize random token splitting instead of energy-based ordering, use attention scores instead of the proposed energy score, and use a fixed number of removed tokens per layer instead of a ratio. The x-axis represents the number of GFLOPs used, while the y-axis represents the recall sum. The plot demonstrates the contribution of each component to PITOME's performance. ", "section": "4.5 PITOME Ablation Studies"}, {"figure_path": "PPdJPIO3mV/figures/figures_9_2.jpg", "caption": "Figure 3: Off-the-shelf Image-Text Retrieval comparison between PITOME v.s. merging/pruning methods on different backbones on tasks when varying the number of merged tokens. Here, Recall sum =Rt@1 + Rt@5 + Rt@10+ Ri@1 + Ri@5 + Ri@10 is close to 600, indicating recall scores at top 1,5, and 10 for retrieving image and text reached close to 100%. PITOME curves, in most cases, are above other baselines.", "description": "This figure compares the performance of PITOME against other token merging and pruning methods on three different backbones for image-text retrieval task.  The x-axis represents the number of floating point operations (gflops), and the y-axis represents the recall sum which is a metric showing the effectiveness of the models in image-text retrieval. The curves indicate that PITOME consistently outperforms the others, achieving almost perfect recall scores across different models and datasets.", "section": "4.1 Image & Text Retrieval"}, {"figure_path": "PPdJPIO3mV/figures/figures_17_1.jpg", "caption": "Figure 8: Token merging outputs can be seen as coarsened graph from an input graph.", "description": "This figure illustrates the process of token merging as a graph coarsening operation.  The input graph G shows individual tokens (nodes) and their relationships (edges). The tokens highlighted in blue are considered as \"candidate nodes\" for merging,  while gray nodes are less important or unique.  After the merging process, as shown in the coarsened graph Gc, several candidate nodes are merged into a new node (orange), which effectively summarizes the information from the merged nodes, resulting in a smaller, more concise graph representation of the original token space.", "section": "F Token Merging Outputs Visualization"}, {"figure_path": "PPdJPIO3mV/figures/figures_19_1.jpg", "caption": "Figure 9: Off-the-shelf performance of all backbones for image-text retrieval task using different token merging schedules.", "description": "This figure compares the performance of different token merging schedules on image-text retrieval tasks using various backbones (CLIP-B, CLIP-L, BLIP, BLIP2).  It shows the Recall sum (a metric combining recall@1, recall@5, and recall@10 for both image and text retrieval) against the number of floating point operations (FLOPS). The comparison highlights the impact of using a fixed number of tokens to merge versus using a ratio to determine the number of tokens to merge across layers.  It suggests that using a ratio-based approach is superior for off-the-shelf performance in most cases.", "section": "4.1 Image & Text Retrieval"}, {"figure_path": "PPdJPIO3mV/figures/figures_20_1.jpg", "caption": "Figure 1: A comparison of token merging methods. Patches of the same color are merged. Green arrows highlight incorrect merges, avoided by PITOME. Position of tokens with high attention scores (cyan borders, zoom for clarity) in PITOME are maintained proportionality akin to ViT-base 384. of tokens in each sample, resulting in a mismatch of dimensions and consequently preventing the batching of samples with consistent dimensions.", "description": "This figure compares different token merging methods, highlighting the advantages of PITOME.  It shows how PITOME avoids incorrect merges and preserves tokens with high attention scores, unlike other methods. The visualization uses color-coded patches to represent merged tokens.", "section": "1 Introduction"}, {"figure_path": "PPdJPIO3mV/figures/figures_21_1.jpg", "caption": "Figure 1: A comparison of token merging methods. Patches of the same color are merged. Green arrows highlight incorrect merges, avoided by PITOME. Position of tokens with high attention scores (cyan borders, zoom for clarity) in PITOME are maintained proportionality akin to ViT-base 384.", "description": "This figure compares different token merging methods: PITOME, ToMe, and DiffRate.  Each method's output is visualized on a sample image, with patches of the same color representing merged tokens.  Green arrows indicate incorrect merges avoided by PITOME.  The figure highlights that PITOME, unlike other methods, preserves the spatial relationship and attention scores of the tokens, achieving similar results to a standard ViT-base 384 model. This suggests PITOME is superior because it avoids merging important tokens.", "section": "1 Introduction"}, {"figure_path": "PPdJPIO3mV/figures/figures_29_1.jpg", "caption": "Figure 1: A comparison of token merging methods. Patches of the same color are merged. Green arrows highlight incorrect merges, avoided by PITOME. Position of tokens with high attention scores (cyan borders, zoom for clarity) in PITOME are maintained proportionality akin to ViT-base 384.", "description": "This figure compares different token merging methods, highlighting the advantages of PITOME.  It shows how different algorithms merge similar image patches (represented by color), with green arrows indicating incorrect merges that PITOME avoids.  The figure also demonstrates that PITOME maintains the relative positions of important tokens, shown by cyan borders, offering better proportionality compared to other methods.", "section": "1 Introduction"}, {"figure_path": "PPdJPIO3mV/figures/figures_29_2.jpg", "caption": "Figure 1: A comparison of token merging methods. Patches of the same color are merged. Green arrows highlight incorrect merges, avoided by PITOME. Position of tokens with high attention scores (cyan borders, zoom for clarity) in PITOME are maintained proportionality akin to ViT-base 384.", "description": "This figure compares three different token merging methods: PITOME, ToMe, and DiffRate.  Each method is shown applied to the same image, represented as a grid of patches.  Patches of the same color indicate that those patches have been merged together by the algorithm. Green arrows point out instances where ToMe and DiffRate incorrectly merged patches, while PITOME avoids these mistakes.  The cyan borders highlight patches with high attention scores.  PITOME is shown to preserve the relative positions of these high-attention score patches more accurately than the other methods, mimicking the ViT-base 384 model.", "section": "1 Introduction"}, {"figure_path": "PPdJPIO3mV/figures/figures_29_3.jpg", "caption": "Figure 1: A comparison of token merging methods. Patches of the same color are merged. Green arrows highlight incorrect merges, avoided by PITOME. Position of tokens with high attention scores (cyan borders, zoom for clarity) in PITOME are maintained proportionality akin to ViT-base 384.", "description": "This figure compares different token merging methods: PITOME, ToMe, and DiffRate.  Each method is applied to the same image, and the resulting merged token patches are color-coded.  Green arrows highlight instances where ToMe and DiffRate incorrectly merge patches that are visually distinct, a problem avoided by PITOME. PITOME successfully preserves important tokens with high attention scores (shown with cyan borders), maintaining image structure more closely than the other methods.", "section": "1 Introduction"}, {"figure_path": "PPdJPIO3mV/figures/figures_29_4.jpg", "caption": "Figure 1: A comparison of token merging methods. Patches of the same color are merged. Green arrows highlight incorrect merges, avoided by PITOME. Position of tokens with high attention scores (cyan borders, zoom for clarity) in PITOME are maintained proportionality akin to ViT-base 384.", "description": "This figure compares different token merging methods: PITOME, ToMe, and DiffRate.  It highlights how PITOME avoids incorrect merges (shown by green arrows) and preserves the positional information of important tokens (those with high attention scores, indicated by cyan borders) better than the other methods. The result from PITOME is more similar to the original ViT-base 384 model.", "section": "1 Introduction"}, {"figure_path": "PPdJPIO3mV/figures/figures_29_5.jpg", "caption": "Figure 1: A comparison of token merging methods. Patches of the same color are merged. Green arrows highlight incorrect merges, avoided by PITOME. Position of tokens with high attention scores (cyan borders, zoom for clarity) in PITOME are maintained proportionality akin to ViT-base 384.", "description": "This figure compares different token merging methods: PITOME, ToMe, and DiffRate.  It highlights the effectiveness of PITOME in preserving the positional information of important tokens (high attention scores) while avoiding incorrect merges, unlike the other methods. The color-coding shows which image patches are merged together by each algorithm. ", "section": "1 Introduction"}, {"figure_path": "PPdJPIO3mV/figures/figures_30_1.jpg", "caption": "Figure 1: A comparison of token merging methods. Patches of the same color are merged. Green arrows highlight incorrect merges, avoided by PITOME. Position of tokens with high attention scores (cyan borders, zoom for clarity) in PITOME are maintained proportionality akin to ViT-base 384.", "description": "This figure compares different token merging methods, highlighting the advantages of PITOME (ours) over existing methods like ToMe and DiffRate.  PITOME avoids incorrect merging and preserves tokens with high attention scores, demonstrating its ability to maintain more of the original information.", "section": "1 Introduction"}, {"figure_path": "PPdJPIO3mV/figures/figures_31_1.jpg", "caption": "Figure 1: A comparison of token merging methods. Patches of the same color are merged. Green arrows highlight incorrect merges, avoided by PITOME. Position of tokens with high attention scores (cyan borders, zoom for clarity) in PITOME are maintained proportionality akin to ViT-base 384.", "description": "This figure compares token merging methods: PITOME, ToMe, and DiffRate.  Different colors represent groups of merged tokens. Green arrows show incorrect merges made by ToMe and DiffRate, which PITOME avoids.  Cyan borders highlight tokens with high attention scores; PITOME preserves their positions better than the other methods, maintaining a spatial structure similar to the original ViT-base 384.", "section": "1 Introduction"}, {"figure_path": "PPdJPIO3mV/figures/figures_32_1.jpg", "caption": "Figure 1: A comparison of token merging methods. Patches of the same color are merged. Green arrows highlight incorrect merges, avoided by PITOME. Position of tokens with high attention scores (cyan borders, zoom for clarity) in PITOME are maintained proportionality akin to ViT-base 384.", "description": "This figure compares different token merging methods, highlighting PITOME's advantage in accuracy.  Different color patches represent merged tokens. Green arrows point out incorrect merges in other methods which are avoided by PITOME, while the preservation of tokens with high attention scores (cyan borders) shows PITOME's proportionality to ViT-base 384.", "section": "1 Introduction"}]