[{"figure_path": "qd8blc0o0F/figures/figures_2_1.jpg", "caption": "Figure 1: Illustration of normalization layers. We denote by B, N and C the number of graphs (batch size), nodes, and channels (node features), respectively. For simplicity of presentation, we use the same number of nodes for all graphs. We color in blue the elements used to compute the statistics employed inside the normalization layer.", "description": "This figure illustrates different normalization layers used in graph neural networks, showing how they compute statistics across different dimensions of the input data (batch size, number of nodes, number of channels).  The colored blue elements highlight the specific components used in the calculations for each normalization method.", "section": "2 Normalization layers for GNNs"}, {"figure_path": "qd8blc0o0F/figures/figures_3_1.jpg", "caption": "Figure 2: A batch of two graphs, where subtracting the mean of the node features computed across the batch, as in BatchNorm and related methods, results in the loss of capacity to compute node degrees.", "description": "This figure shows an example illustrating the limitations of BatchNorm and similar normalization methods. It demonstrates that when subtracting the mean computed across the batch, the features of nodes with degrees less than the average turn negative, which are then set to 0 by the ReLU activation function, thus hindering the prediction of the node degrees.", "section": "2 Normalization layers for GNNs"}, {"figure_path": "qd8blc0o0F/figures/figures_4_1.jpg", "caption": "Figure 3: Illustration of a GRANOLA layer. Given node features \u0124(l\u22121) and the adjacency matrix Ab, we feed them to a GNNLAYER to extract intermediate node features \u0124(l). Then, we predict normalization parameters using GNNNORM, which takes sampled RNF R(l)b, \u0124(l), Ab. Including R(l)b with Ab and \u0124(l) enhances the expressiveness of GRANOLA ensuring full adaptivity.", "description": "This figure illustrates the architecture of the GRANOLA layer.  It takes as input the node features from the previous layer (H(l\u22121)) and the adjacency matrix (Ab) of the graph. These inputs are processed by a GNN layer (GNNLAYER) to produce intermediate node features (\u0124(l)). Simultaneously, random node features (R(l)b) are sampled and, along with \u0124(l) and Ab, fed into a normalization GNN (GNNNORM). The normalization GNN outputs the normalization parameters (\u03b3(l)b,n, \u03b2(l)b,n), which are then used to normalize the intermediate features (\u0124(l)) resulting in the final normalized features (H(l)).  The inclusion of random node features is crucial for achieving full adaptivity of the normalization to the input graph structure.", "section": "3.2 GRANOLA"}, {"figure_path": "qd8blc0o0F/figures/figures_8_1.jpg", "caption": "Figure 4: Training convergence of GRANOLA compared with existing normalization techniques show that GRANOLA achieves faster convergence and overall lower (better) MAE.", "description": "This figure shows the training loss curves for various normalization methods, including GRANOLA, across three different datasets: ZINC-12K, MOLHIV, and MOLTOX21.  The plots demonstrate that GRANOLA consistently achieves faster convergence (reaching a lower loss in fewer epochs) and ultimately lower training loss (mean absolute error, or MAE) than competing normalization techniques.", "section": "5 Experimental Results"}, {"figure_path": "qd8blc0o0F/figures/figures_19_1.jpg", "caption": "Figure 2: A batch of two graphs, where subtracting the mean of the node features computed across the batch, as in BatchNorm and related methods, results in the loss of capacity to compute node degrees.", "description": "This figure shows an example demonstrating the limitations of BatchNorm and similar methods (that subtract the mean across all nodes) in graph neural networks.  Two graphs are displayed: one with nodes having degrees 1 and 2, and another with nodes having degrees 1 and 3.  When the mean node degree is subtracted, nodes with degrees less than the mean have negative values. Because of the ReLU activation function commonly used in GNNs, the negative values become 0, losing the information about the original node degree and preventing the network from accurately learning node degree prediction.  This highlights the need for adaptive normalization techniques in GNNs.", "section": "2 Normalization layers for GNNs"}, {"figure_path": "qd8blc0o0F/figures/figures_19_2.jpg", "caption": "Figure 2: A batch of two graphs, where subtracting the mean of the node features computed across the batch, as in BatchNorm and related methods, results in the loss of capacity to compute node degrees.", "description": "This figure shows two graphs where BatchNorm (or similar methods) is applied. (a) shows node degrees after a message-passing layer.  (b) shows the result after subtracting the mean node degree across both graphs.  Features of nodes with lower than average degrees become negative. (c) shows that after applying a ReLU activation, all negative values are set to zero, hindering the ability of the network to distinguish between nodes with different degrees.", "section": "2 Normalization layers for GNNs"}]