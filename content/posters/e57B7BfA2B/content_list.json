[{"type": "text", "text": "FlowDCN: Exploring DCN-like Architectures for Fast Image Generation with Arbitrary Resolution ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Shuai Wang Zexian Li Tianhui Song Xubin Li Tiezheng Ge Nanjing University Alibaba Group Nanjing University Alibaba Group Alibaba Group ", "page_idx": 0}, {"type": "image", "img_path": "e57B7BfA2B/tmp/de32be3b68b884617723a3cf5b0b273dce1f1dfa5ee9cdc8edae2546cf47c1a9.jpg", "img_caption": ["Figure 1: Selected arbitrary-resolution samples (384x384, 224x448, 448x224, 256x256). Generated from a single FlowDCN-XL/2 model trained on ImageNet $256\\!\\times\\!256$ resolution with ${\\mathrm{CFG}}=4.0.$ . "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Arbitrary-resolution image generation still remains a challenging task in AIGC, as it requires handling varying resolutions and aspect ratios while maintaining high visual quality. Existing transformer-based diffusion methods suffer from quadratic computation cost and limited resolution extrapolation capabilities, making them less effective for this task. In this paper, we propose FlowDCN, a purely convolution-based generative model with linear time and memory complexity, that can efficiently generate high-quality images at arbitrary resolutions. Equipped with a new design of learnable group-wise deformable convolution block, our FlowDCN yields higher flexibility and capability to handle different resolutions with a single model. FlowDCN achieves the state-of-the-art 4.30 sFID on $256\\!\\times\\!256$ ImageNet Benchmark and comparable resolution extrapolation results, surpassing transformer-based counterparts in terms of convergence speed (only $\\frac{1}{5}$ images), visual quality, parameters $8\\%$ reduction) and FLOPs $20\\%$ reduction). We believe FlowDCN offers a promising solution to scalable and flexible image synthesis. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Image generation is an important task in computer vision research, which is aimed at capturing the inherent data distribution of original image datasets and generating high-quality synthetic images through sampling. Diffusion models [1, 2, 3, 4, 5] have recently emerged as a highly promising foundation for training algorithms in image generation, outperforming GAN-based models [6, 7] and Auto-Regressive models [8] by a significant margin. The evolution of diffusion models is fast, transitioning from discrete forms [1] to SDE-based continuous forms [2, 3, 4, 5, 9]. In a nutshell, diffusion models incrementally degrade an image through a time-dependent stochastic perturbation process and then learn the reverse process to restore the original image from its corrupted state. ", "page_idx": 1}, {"type": "text", "text": "Beyond theoretical advancements in diffusion models, the architecture of these models also significantly influences the quality of generated images. Many works [1, 10, 11] in the diffusion domain adopt a standard UNet architecture as the generation backbone, which consists of downsample blocks, upsample blocks, and long residual connections between these components. Inspired by the success of the vision transformer in perception tasks, DiT [12] eliminates the long residual connection in favor of a pure transformer-based architecture. Through rigorous experiments, DiT demonstrates that the UNet inductive bias is not essential for achieving high performance in diffusion models [12]. Meanwhile, PixArt [13, 14] and SD3 [15] venture further by significantly increasing the number of parameters, exploring new frontiers in model architecture and its impact on image generation. ", "page_idx": 1}, {"type": "text", "text": "When considering the generation of images at arbitrary resolution, diffusion transformers need to confront at least two primary challenges. The first is the quadratic computation cost: the architecture of diffusion transformers employs attention mechanisms to aggregate spatial tokens. Owing to the dense nature of attention computations, high-resolution image generation inevitably leads to significant computation and memory demands, both scaling with $\\bar{O}(\\bar{n^{2}})$ complexity. To address the quadratic computation challenge, some methods [16] have adapted recurrent computational strategies from natural language processing. However, these adaptations do not fully capitalize on the strengths of autoregressive tasks and result in slower inference speeds due to the reduced parallelism inherent in RNN-based scanning. The second challenge is resolution extrapolation: many diffusion transformers rely on absolute position embedding (APE) [17] to incorporate positional information, introducing it at the onset of the model. This approach forces subsequent layers to become overfitted to the APE for providing positional context to the attention layers, which presents a significant barrier when extrapolating to different resolutions. To address this issue, FiT [18] has turned to Rotary Positional Encoding [19], incorporating RoPE2D to enhance its resolution extrapolation capabilities. Nevertheless, FiT still requires a training pipeline tailored to arbitrary-resolution generation. ", "page_idx": 1}, {"type": "text", "text": "In contrast, convolutional models are the most common choice of visual encoders, boasting linear complexity and aggregating spatial features based on relative positions. With the support of modern convolution operators [20, 21, 22], convolutional models have demonstrated comparable performance or even surpassed transformers in perception tasks. This naturally leads us to inquire: Can modern convolutional networks achieve arbitrary-resolution generation efficiently and outperform transformer counterparts? To answer this question, we opt for deformable convolution as the basic block for exploration in generation, owing to its superior performance in perception tasks. ", "page_idx": 1}, {"type": "text", "text": "Specifically, we propose a novel approach to decouple the scale and direction prediction of deformable convolution, giving rise to a group-wise multiscale deformable convolution block that enables efficient multiscale feature aggregation. By leveraging this block, we introduce FlowDCN, a modern purely convolution generative model that tackles arbitrary-resolution generation. Thanks to the new design of convolutional deformable block, our FlowDCN yields higher flexibility and capability to handle different resolutions with a single model. The experiments demonstrate that FlowDCN consistently surpasses its diffusion transformer counterparts, DiT [12] and SiT [23]. Notably, on the $256\\mathrm{x}256$ ImageNet benchmark, FlowDCN achieves faster convergence, yielding SoTA sFid of 4.30 and FID of 2.13 under 1.5M steps with batch size 256, while exhibiting $20\\%$ lower latency, $8\\%$ fewer parameters, and approximately $20\\%$ fewer floating-point operations (FLOPs). On the $512\\mathrm{x}512$ ImageNet benchmark, FlowDCN achieves 4.53 sFid o and 2.44 FID under 100K finetuning steps with batch size 256. ", "page_idx": 1}, {"type": "text", "text": "Moreover, our FlowDCN offers a significant advantage in fast arbitrary-resolution generation, as it only requires linear time and memory complexity. Through visualization comparisons, our FlowDCN demonstrates substantially better visual quality even at extremely small sampling steps, such as 3, 4, and 5 steps. To further enhance its visual quality, we propose Scale Adjustment, a simpler technique for extrapolating resolution to unseen dimensions. Our results show that FlowDCN achieves comparable resolution extrapolation capabilities to highly tailored methods, underscoring its potential for generating high-quality images at various resolutions. The contributions can be summarized as: ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "\u2022 We decouple the scale and direction priors of deformable convolution and propose a Groupwise MultiScale Deformable Block. Building upon this block, we propose FlowDCN, a purely convolution-based generative model with high efficiency.   \n\u2022 On 256x256 ImageNet benchmark, under only 1.5M training steps, our FlowDCN-XL/2 achieves 2.13 FID and SoTA 4.30 sFID with Euler solver and classifier free guidance.   \n\u2022 On 512x512 ImageNet benchmark, under only 100K finetuning steps, our FlowDCN-XL/2 achieves 2.44 FID and 4.53 sFID with Euler solver and classifier free guidance.   \n\u2022 We propose a much simple and efficient resolution extrapolation method, deemed as Scale Adjustment. For arbitrary resolution generation, we achieve comparable results to highly tailored methods. ", "page_idx": 2}, {"type": "text", "text": "2 Preliminary ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Linear-based Flow Matching ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Flow matching [4, 5] is a simple but powerful diffusion family. We incorporate linear-based flow matching as the training framework for its simplicity. Given the image sampled $x$ from training distributions and the noise $\\epsilon$ sampled from a Gaussian distribution, linear-based flow matching forward process interpolate $x_{t}$ with $x$ and $\\epsilon$ using the following equation: ", "page_idx": 2}, {"type": "equation", "text": "$$\nx_{t}=t x+(1-t)\\epsilon.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The velocity field of linear-based flow matching [4, 5] is defined as Eq. (2). We train our FlowDCN to predict the time-dependent velocity field between $x$ and $\\epsilon$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\nv_{t}(x_{t})=x-\\epsilon.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "During training, the flow matching objective directly regresses the target velocity: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{L}_{v}=\\int_{0}^{1}\\mathbb{E}[\\|\\ v_{\\theta}(x_{t},t)-v_{t}(x_{t})\\ \\|^{2}]d t.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "For sampling, the common ODE/SDE solver e.g.. Euler method, Heun method can be employed. ", "page_idx": 2}, {"type": "text", "text": "2.2 Deformable Convolution Revisited ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Given an image feature $\\textbf{x}\\in\\,\\mathbb{R}^{H\\times W\\times D}$ , deformable convolution predicts the deformable field $\\Delta\\mathbf{P}(\\mathbf{x})\\in\\mathbb{R}^{H\\times W\\times G\\times K\\times2}$ and the dynamic weights $\\mathbf{W}(\\mathbf{x})\\in\\mathbb{R}^{H\\times W^{\\star}\\times G\\times K}$ from the image feature $\\mathbf{x}$ . Specifically, $H$ and $W$ represent the height and width of the feature spatial shape, $D$ is the feature channel, $K$ is the number of sampling points, and $G$ is the number of groups in the deformable convolution operation. The deformable field and dynamic weights are computed as Eq. (4): ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Delta\\mathbf{P}(\\mathbf{x})=\\mathbf{W}_{\\mathrm{deformable}}^{T}\\mathbf{x}+\\mathbf{b}_{\\mathrm{deformable}},}\\\\ {\\mathbf{W}(\\mathbf{x})=\\mathbf{W}_{\\mathrm{weight}}^{T}\\mathbf{x}+\\mathbf{b}_{\\mathrm{weight}}.\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "For a specific group $g$ in deformable convolution, the sampling position is determined by the base feature position $p_{0}$ , sampling position prior $p_{k}$ , and predicted deformable $\\Delta p_{k}$ from $\\Delta\\mathbf{P}(\\mathbf{x})$ for the $k$ -th sampling point. The dynamic weight $w_{k}$ is provided from $\\mathbf{W}(\\mathbf{x})$ . The deformable convolution aggregates $K$ sparse spatial features according to the sampling location and dynamic weight as following: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbf{y}^{g}(p_{0})=\\sum_{k=0}^{K}w_{k}^{g}\\mathbf{x}^{g}(p_{0}+p_{k}+\\Delta p_{k}(\\mathbf{x})),}}&{}&\\\\ {\\mathbf{y}=\\mathrm{concat}(\\mathbf{y}^{1},\\,\\mathbf{y}^{2},\\,....,\\,\\mathbf{y}^{G}).}&{}&\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The predefined spatial position prior $p_{k}$ is initialized from the regular convolution, commonly using $(-1,-1),(-1,0{\\bar{)}},\\ldots({\\bar{0,}}0),\\ldots({\\bar{1,}}1)$ as the predefined value. ", "page_idx": 2}, {"type": "text", "text": "Deformable convolution introduces long-range dependencies and dynamic aggregation into regular convolutions, bridging the gap between convolution and multi-head self-attention [24]. Thus, deformable convolution shares the efficiency merit of convolution and the dynamics merit of the attention mechanism. In most scenarios, DCN-like architectures are more powerful than common CNNs, we provide comparison experiments of DCN and CNN of flow matching training. Notably, deformable convolution directly predicts the dynamic weights and only aggregates limited features from spatial locations, enjoying a relatively sparse computation diagram. A deformable convolution operator only require s 4KHGW CFLOPs for computation when employing bilinear sampling to aggregate features. ", "page_idx": 3}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "3.1 Multi-Scale Deformable Convolution ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The original deformable convolution has been widely adopted in hierarchical model architectures [25, 24, 20] for perception tasks. However, these models typically progressively downsample the feature maps to increase the reception field growth rate. In contrast, image generation tasks require outputs with more high-frequency details and low-level information. From this perspective, progressively downsampling features would lead to the loss of high-frequency details. One possible solution is to introduce long residual connections to generation models [11, 10]. However, in practice, this approach demands caching image features from the encoder part, which increases peak memory usage during model inference. ", "page_idx": 3}, {"type": "text", "text": "To strike a balance between receptive fields and high-frequency details, we propose decoupling the deformable field into scale and direction, and introduce a novel multiscale deformable convolution. Unlike previous deformable convolutions, our approach assigns different scale priors to different groups. ", "page_idx": 3}, {"type": "text", "text": "Decoupling deformable field to direction and scale. The original deformable convolution directly regresses the deformable field to learn an unbounded and adaptive sampling point generator. However, the vast image spatial range poses a challenge to the learning process, as it leads to unstable regression of the deformable range when extrapolating from local neighbors to distant feature locations. We tackle this problem by decoupling the direction and scale of the deformable field. Specifically, we reorganize the sampling point formulation in Eq. (9). ", "page_idx": 3}, {"type": "equation", "text": "$$\ns(\\mathbf{x})=S_{\\mathrm{max}}*\\mathrm{sigmoid}(\\mathbf{W}_{s}^{T}\\mathbf{x}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "equation", "text": "$$\np=p_{0}+s(\\mathbf{x})*(p_{k}+\\Delta p_{k}(\\mathbf{x})),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $s(\\mathbf x)$ is the learnable scale predicted from the image feature $\\mathbf{x}$ . $S_{\\mathrm{max}}$ is the max scale value of the given deformable convolution, we leave it as a hyper-parameter only related to input resolution, thus we can manually tune it according to input resolution, details are placed in Sec. 3.3. ", "page_idx": 3}, {"type": "text", "text": "Group-wise multi-scale deformable convolution. To keep high-resolution feature maps and own a large reception field growth rate, we propose to assign different scale priors to different groups. This allows deformable groups with large scale priors to aggregate long-dependency features, while those with small scale priors aggregate short-dependency features as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{s^{g}(\\mathbf{x})=S_{\\mathrm{max}}*\\mathrm{sigmoid}(\\mathbf{W}_{s}^{T}\\mathbf{x}+s_{0}^{g}),}\\\\ &{\\quad p=p_{0}+s^{g}(\\mathbf{x})*(p_{k}+\\Delta p_{k}(\\mathbf{x})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Specifically, we initialize the scale priors with Eq. (12) and initialize ${\\bf W}_{s}$ with zeros to obtain linearly increased sigmoid $\\left(s_{0}^{g}\\right)$ along group axis: ", "page_idx": 3}, {"type": "equation", "text": "$$\ns_{0}^{g+1}=\\log(\\frac{g}{G-g}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "3.2 Flow-based Deformable Convolutional Generative Model ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We introduce our novel diffusion generation architecture, dubbed FlowDCN. Rather than directly adopting tailored architectures for image generation, such as long residuals and normalization techniques, we aim to explore the generative capabilities of deformable convolution-based architectures ", "page_idx": 3}, {"type": "image", "img_path": "e57B7BfA2B/tmp/a7ba143d9472d32a108e18b2b4d183e871ecba721f588e4b85ec3ee223bad8ba.jpg", "img_caption": ["(a) FlowDCN Architecture. Our FlowDCN (b) MultiScale DCN Block. Dynamic weight and scale& consists of stacked MultiScaleDCN blocks and direction deformable field are predicted from input feaSwiGLU blocks. We also employ RMSNorm tures, then merged with priors to form the deformable to stabilize training. kernels to extract features. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 2: The Architecture of Our FlowDCN and MultiScale DCN Block. ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "in a faithful manner. To this end, we deliberately discard long residual connections and opt to build a pure convolution-based generative model, preserving the unique characteristics of DCN-like models as much as possible. For training and sampling, we leverage the powerful flow matching algorithm to align our model with the state-of-the-art SiT [23]. ", "page_idx": 4}, {"type": "text", "text": "Deformable convolution generative model. The model architecture is illustrated in Fig. 2a. We aim to build a pure DCN-like generative model to explore the generation ability of DCN-like [20] architectures. To match the base resolution of model input with DiT [12] and SiT [23], we similarly patchify the noisy input via convolution. Inspired by DiT [12], we inject the timestep and label conditions through adaLN-Zero [12, 26]. The basic block is formulated as Eq. (13). Drawing inspiration from LLaMA [27, 28], we replace vanilla FFN and LayerNorm with SwiGLU and RMSNorm, respectively. Note we also provide FFN and LayerNorm version FlowDCN for fair comparisons: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\mathbf x}_{1}={\\mathbf x}+\\mathrm{AdaLN}({\\mathbf y},{\\mathbf t},\\mathrm{MultiScale-DCN}({\\mathbf x})),}\\\\ &{{\\mathbf x}_{2}={\\mathbf x}_{1}+\\mathrm{AdaLN}({\\mathbf y},{\\mathbf t},~\\mathrm{\\mathbfSwiGLU}({\\mathbf x}_{1})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "3.3 Arbitrary Resolution Sampling ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We denote the training resolution as $H_{\\mathrm{train}}\\times W_{\\mathrm{train}}$ and the inference resolution as $H_{\\mathrm{test}}\\times W_{\\mathrm{test}}$ . Notably, our FlowDCN is capable of handling arbitrary resolution that differs from the training resolution. As a reminder, the multiscale deformable convolution block aggregates features based on predicted scales and directions according to Equation (Eq. (9)). In practice, the predicted scale of the multiscale deformable convolution layer is typically fitted to match the training resolution distribution. However, this limits the reception fields of the image features when encountering unseen resolution, ultimately hurting the global semantic consistency [29, 30]. To improve the global semantic consistency, we propose adjusting the scaling factor based on the relative ratio between the training resolution and inference resolution. ", "page_idx": 4}, {"type": "text", "text": "Adjust $S_{\\mathrm{max}}$ to match inference resolution. As shown in Eq. (10), $S_{\\mathrm{max}}$ controls the maximum sampling range in multiscale deformable convolution. As discussed in Sec. 3.1, we treat it as a resolution-dependent hyperparameter. It is straightforward to observe that scaling $S_{\\mathrm{max}}$ with the relative aspect ratio between train size and inference size could match the reception field between train and inference: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{s_{h}^{g}(\\mathbf x)=\\mathrm{sigmoid}(\\mathbf W_{s}^{T}\\mathbf x+s_{0}^{g})\\cdot S_{\\mathrm{max}}\\cdot\\frac{H_{\\mathrm{test}}}{H_{\\mathrm{train}}},}\\\\ &{s_{w}^{g}(\\mathbf x)=\\mathrm{sigmoid}(\\mathbf W_{s}^{T}\\mathbf x+s_{0}^{g})\\cdot S_{\\mathrm{max}}\\cdot\\frac{W_{\\mathrm{test}}}{W_{\\mathrm{train}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "table", "img_path": "e57B7BfA2B/tmp/ebf97ad92da9ff06485056460962c36bb18cb190477f9f6b6053dc5ad8f1e4b4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "table", "img_path": "e57B7BfA2B/tmp/6916c56fe6204d6c4e0573a3a499e6fb706b3cdd0caf6f65b1309723c15a3830.jpg", "table_caption": ["Table 1: Op-level benchmark on standard input shape of Diffusion backbone task. FP16/FP32 results are collected on Nvidia A10 GPU. We use 32 batch sizes for benchmarking. $^\\dagger$ indicates our Tritonlang [32] implementation of DCNv4. N indicates implementation is not available. "], "table_footnote": ["(a) Comparsions with SiT. Our (b) KernelSize $K$ of FlowDCN. (c) Deformable fields learning setFlowDCN outperforms SiT by a sig- large kernel size produces better re- ting. Default achieves best results. nificant margin. sults than small one. "], "page_idx": 5}, {"type": "text", "text": "Table 2: Ablation Studies and Comprasion with other flow-based method on 32x32 CIFAR Dataset. In order to fully align with SiT [23], here we replace our SwiGLU and RMSNorm with FFN and LayerNorm armed in SiT, respectively. Bold font indicates the default setting. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We conduct experiments on 32x32 CIFAR10 and 256x256 ImageNet datasets. The training batch size is set to 256. Similar to SiT [23] and DiT [12], we use Adam optimizer [31] with a constant learning rate 0.0001 during the whole training. We do not adopt any gradient clip techniques for fair comparison. For $32\\mathtt{x}32$ CIFAR10 dataset, we train our model for 25000 steps. As for $256\\mathrm{x}256$ ImageNet dataset, we train for $1.5\\mathrm{M}$ steps. We use $8\\!\\times\\!\\mathrm{Al00}$ GPUs as the default training hardware. ", "page_idx": 5}, {"type": "text", "text": "Efficient deformable convolution implementation. Although DCNv4 [20] proposes a much faster deformable convolution implementation, it is not tailored for image generation input shape. For resolution below $512\\times512$ , there are fewer spatial tokens (only $16\\times16$ tokens for $256\\times256$ resolution) to fully utilize sparse computation strengths, thus DCNv4 exhibits even worse latency compared to attention. To remedy high latency of deformable convolution for low-resolution scenery, we decide to leverage shared memory to reduce the latency of random sampling in deformable convolutions, deemed as DeformConv(shm). We place the performance benchmark at Tab. 1. For high-resolution scenery, We also re-implement DeformConv(DCNv4) in Triton-lang as DeformConv(Triton-lang) to leverage the strengths of compiler [32, 33] to find suitable hyperparameters. ", "page_idx": 5}, {"type": "text", "text": "4.1 32x32 CIFAR Dataset ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The CIFAR10 dataset[35], comprising $50\\small{,}000\\,\\,32\\mathrm{x}32$ small-resolution images from 10 distinct class categories, is considered an ideal benchmark to validate the design of our MultiScale deformable block due to its relatively small scale. We select SiT-S/2 as a comparison baseline, as it also leverages the flow-matching framework. For sampling, we employ the Euler stochastic solver with 1000 sampling steps to generate images. We report the FID [36], sFID [37], and Inception Score [38] as the primary metrics to evaluate the performance of our model. ", "page_idx": 5}, {"type": "text", "text": "Compare with baseline SiT. We summarise the metrics of our FlowDCN and SiT in Tab. 2a. Our FlowDCN achieves 5.47 fid, surpassing its counterpart SiT with 2.0 fid margins. Additionally, our model performs slightly better in terms of sFID and Inception Scores, further demonstrating its superiority. ", "page_idx": 5}, {"type": "text", "text": "Group-wise multiscale design. As showed in Tab. 2a, we denote the variant of FlowDCN that uses vanilla deformable convolution instead of Multiscale deformable convolution as w/o MultiScale. Notably, the absence of group-wise multiscale deformable convolution leads to a 0.25 FID performance degradation. This result demonstrates the effectiveness and power of our proposed group-wise multiscale mechanism. ", "page_idx": 5}, {"type": "text", "text": "Prior initialization. By default, we manually initialize the direction priors with predefined grids $\\{(-1,-1),(-1,0),...(\\dot{0},0),...(1,1)\\}$ , and initialize the scale priors with linearly increased scale along group axis. We also experiment with randomly initialized direction and scale priors in Tab. 2a, donated as w/o PriorInit. Random initialization shows slight performance degradation. ", "page_idx": 6}, {"type": "text", "text": "Sampling points. In Tab. 2b, We train FlowDCN with varying kernel sizes $K$ and observe that the performance consistently improves as the number increases. Specifically, using 32 points to aggregate features, FlowDCN achieves a FID score of 5.13 and an sFID score of 4.43. However, to maintain a relatively sparse pattern, we choose $K=9$ as the default setting, striking a balance between performance and computational efficiency. ", "page_idx": 6}, {"type": "text", "text": "Fixed direction priors. In Tab. 2c, we present the results of training FlowDCN with different prior learning settings. Notably, we find that the fixed direction prior $p_{k}$ in Eq. (9) achieves better results compared to the learnable direction prior. We hypothesize that the learnable direction prior may cause the learning of the deformable field to become unstable, leading to inferior performance. ", "page_idx": 6}, {"type": "text", "text": "Learnable relative scale. In the $s(\\mathbf x)$ column of Tab. 2c, the notation \"learn\" indicates that we predict a relative scale of the deformable fields in addition to the learnable scale priors $s_{0}^{g}$ ( $\\mathbf{W}_{s}^{T}\\mathbf{x}$ in Eq. (10)), whereas \"fixed\" does not predict the relative scales $s(\\mathbf{x})$ in the deformable field. Learning a relative scale for each feature in Tab. 2c achieves better results of 5.47 FID. ", "page_idx": 6}, {"type": "text", "text": "4.2 $256\\!\\times\\!256$ ImageNet Dataset ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Based on our analysis, we select the MultiScale deformable convolution with a kernel size of $K=9$ as the basic block for our Imagenet experiments. Our default setting involves fixing the direction priors and learning relative scales from the deformable field. We manually initialize the direction priors with predefined grids and initialize the scale priors with linearly increased scales. To generate images, we employ an Euler-Maruyama solver with 250 steps for stochastic sampling. We report the FID, sFID, Inception Score, and Precision & Recall as the primary metrics to evaluate the performance of our model. ", "page_idx": 6}, {"type": "table", "img_path": "e57B7BfA2B/tmp/68ecaabfe582c23f35b9e62609fcb93b3b4d635c83f667092899cc3cafd7343b.jpg", "table_caption": ["Table 3: Image generation metrics comparisons between SiT [23], DiT [12] under 400k training steps budgets. All metrics are calculated from the sampled 50k images under 250 Euler SDE sampling steps without classifier-free guidance. $^\\dagger$ : reproduced result. Latency(ms) is the 1-NFE latency and collected from Nvidia A10 GPU with 16 batchsize under float32. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Metrics comparison with baseline SiT. We present the performances of different-size models at 400K training steps in Tab. 8. From Small to XL-size models, our FlowDCN model family consistently outperforms its counterpart DiT [12] and SiT [23] with significant margins. Without RMS/SwiGLU, our FlowDCN-B/2 degrades with 0.6 FID gains but still surpasses SiT by a large margin. In addition to its superior performance and convergence speed, our FlowDCN also boasts a remarkable $8\\%$ reduction in parameters and at least $20\\%$ reduction in FLOPs compared to DiT/SiT. This demonstrates that our FlowDCN surpasses vision transformer-based generation models in multiple aspects. ", "page_idx": 6}, {"type": "image", "img_path": "e57B7BfA2B/tmp/425f8e5736d1c9d53b4cc7f80b162ee4bd7bc9fb3f754b5193862d75b9fc4e37.jpg", "img_caption": ["Figure 3: Visualization Comparison with SiT. Best viewed zoomed-in. We sample both our FlowDCNXL/2 and SiT-XL/2 with Euler ODE solver under 2, 3, 4, 5, 8, 10 steps using the same latent noise. At the fewer steps sampling scenery, our FlowDCN generates slightly clearer and higher-quality images. ", "Table 4: Image generation quality evaluation of and existing approaches on ImageNet $256\\times$ 256. Total images by training steps $\\times$ batch size as reported, and total GFLOPs by Total Images $\\times$ GFLOPs/Image. P refers to Precision and R refers to Recall. "], "img_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "e57B7BfA2B/tmp/05b94b0b15b7407c112fc35dcabbd5717d8d270ec55f9dfa8990576e37edcd76.jpg", "table_caption": ["ImageNet 256\u00d7256 Benchmark "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Comparison with other generative models. We report the final metrics of FlowDCN-XL/2 at Tab. 4. Our FlowDCN achieves much faster convergence speed with nearly $\\frac{1}{5}$ total images compared its No-Long-residuals counterparts. Additionally, using Euler ODE solver and classifier-free guidance with 1.375, our FlowDCN obtains SoTA 4.30 sFID and 2.13 FID results. Training for extra $400\\mathrm{k}$ steps, FlowDCN will be further improved to 2.00 FID. As sFID reflects the spatial structure quality [37], better sFID shows our FlowDCN captures better structure distributions. We notice that the IS metric is lower than other models, however, there is an improvement trend along with training iterations. ", "page_idx": 7}, {"type": "text", "text": "Visual quality comparison with baseline SiT. We sample both our FlowDCN-XL/2 and SiT-XL/2 with Euler ODE solver for 2, 3, 4, 5, 8, 10 steps, employing the same latent noise for both models. Notably, at the fewer steps sampling scenario, our FlowDCN generates slightly clearer and higherquality images. We place the generated images at Fig. 3 and Appendix. ", "page_idx": 7}, {"type": "text", "text": "4.3 $512\\times512$ ImageNet Dataset ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "As training on high-resolution images consumes much more resources, we opt to fine-tune $100\\mathrm{k}$ steps from the same model trained on $256\\times256$ resolution setting of $1.5\\mathrm{M}$ steps (corresponding to ", "page_idx": 7}, {"type": "table", "img_path": "e57B7BfA2B/tmp/3c80235a71a81dd476cdce03f9dc7e3837c243275c6d136003b348177d34f240.jpg", "table_caption": ["Class-Conditional ImageNet 512\u00d7512 ", "Table 5: Benchmarking class-conditional image generation on ImageNet $512\\!\\times\\!512.$ . Our FlowDCNXL/2 is fine-tuned for 100k steps from the same model trained on $256\\times256$ resolution setting of $1.5\\mathrm{M}$ steps "], "table_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "e57B7BfA2B/tmp/62a05c6c37447e12a62f8f1d561e47a0a767bb9922e1dc3fc09f63d7fd2d39e4.jpg", "img_caption": ["Figure 4: Visualization Comparison about $S_{\\mathrm{max}}$ Adjustment. Here are the $512\\times512$ , $256\\times512$ and $512\\times256$ , three type resolution images. We employ the same latent noise as start, sampling with Euler SDE solver for 250 steps. With $S_{\\mathrm{max}}$ Adjustment, sampled images consistently looks better. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "FlowDCN with 384M training images of Tab. 4). Although fine-tuned with limited 100k steps, our FlowDCN demonstrated powerful performance. ", "page_idx": 8}, {"type": "text", "text": "Comparison with other generative models. We report the final metrics of FlowDCN-XL/2 on 512 $\\times\\ 512$ ImageNet Dataset at Tab. 5. Our FlowDCN achieves much better FID and sFID performance compared to its counterparts. Using Euler SDE solver with 250 steps and classifier-free guidance with 1.375, our FlowDCN obtains 4.53 sFID and 2.44 FID results. Using Euler ODE solver with 50 steps and classifier-free guidance with 1.375, our FlowDCN obtains 5.29 sFID and 2.76 FID result. As shown in Tab. 5, our FlowDCN achieves better sFID and captures better spatial structure distributions. ", "page_idx": 8}, {"type": "text", "text": "4.4 Arbitrary Resolution Extension ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "For the resolution extrapolation evaluation, we follow the setting in FiT. We select $320\\mathrm{x}320$ and $224\\!\\times\\!448$ as the evaluation arbitrary resolution. It is worth noting that our FlowDCN can handle arbitrary resolution within a reasonable range, the reasonable range is determined by the training setting and training dataset. As our primary goal is to explore DCN-like architectures in universal image generation, we do not intend to enhance the resolution extrapolation nature by data processing. Therefore, we do not employ any multiple aspect ratio training techniques like FiT[18]. Instead, we directly use the FlowDCN model trained on the center-cropped 256x256 ImageNet dataset for arbitrary resolution extension experiments, showcasing the model\u2019s inherent capabilities. Moreover, we provide resolution extension experiments with various aspect ratio training techniques in the Appendix. ", "page_idx": 8}, {"type": "text", "text": "Metric comparsion. We report the evaluation results on Tab. 6. For Base-size models, our FlowDCNB/2 achieves much better results on $320\\mathrm{x}320$ resolution, with $34.4\\;\\mathrm{FID}$ and 35.7 FID using $S_{\\mathrm{max}}$ adjustment, outperforming FiT and DiT with a large margin. On $224\\!\\times\\!448$ resolution, our FlowDCN", "page_idx": 8}, {"type": "table", "img_path": "e57B7BfA2B/tmp/32583eb2c5a8d3429bd6fa8ed796be9086b4fd41ac1d3cd43b6cef38242f0de0.jpg", "table_caption": [], "table_footnote": ["(a) Metrics Results on Base-Size Models "], "page_idx": 9}, {"type": "table", "img_path": "e57B7BfA2B/tmp/6133f93a38e6e572b074019346d2741026b94806cc0517cd80b0d9a4c1529ae7.jpg", "table_caption": [], "table_footnote": ["(b) Metrics Results on Large-Size Models "], "page_idx": 9}, {"type": "text", "text": "Table 6: Benchmarking resolution extrapolations on ImageNet dataset. On the Base-size Models benchmark, our FlowDCN achieves much better results on $320\\mathrm{x}320$ resolution and comparable results on $224\\!\\!\\times\\!448$ resolution. On the Large-Size Models benchmark, our flowDCN shows comparable extrapolation performance to SoTA models. ", "page_idx": 9}, {"type": "text", "text": "B/2 achieves comparable results. Note our FlowDCN not employs any various aspect ratio training in Tab. 6, so we believe our FlowDCN-B/2 can achieve better results when incorporating such training augmentations. For large-size models, we report our FlowDCN-L/2 and FlowDCN-XL/2 with $S_{\\mathrm{max}}$ adjustment in Tab. 6b, our model shows comparable results to SoTA models. Meanwhile, we notice that FiT performs poorly on $256\\mathrm{x}256$ resolution in Tab. 4, which we hypothesize is due to resolution-related data augmentation hurting the fitting power of original resolution distributions. Furthermore, as FiT employs the $256\\times256$ reference statistics from ADM Eval Suite[10] to evaluate all resolution(even for $224\\times448)$ ), we suspect this evaluation paradigm is unreasonable. ", "page_idx": 9}, {"type": "text", "text": "Visual quality comparison of $S_{\\mathrm{max}}$ . In Tab. 6a, We notice FlowDCN-B/2 with $S_{\\mathrm{max}}$ adjustment does not exhibit better results than directly generating images, we hypothesize that FID and sFID are low level visual quality assessments, not reflecting semantic visual quality. So we also provide the visualization comparisons of our FlowDCN-XL/2 with and without $S_{\\mathrm{max}}$ Adjustment in Fig. 4 and Appendix. With $S_{\\mathrm{max}}$ Adjustment, generated images consistently look better. But not all the cases demand $S_{\\mathrm{max}}$ Adjustment, some images like the bubble and the husky case in Fig. 4, still look good even without $S_{\\mathrm{max}}$ Adjustment. More comparison examples can be found in the Appendix. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we have presented FlowDCN, a novel deformable convolutional network for arbitraryresolution image generation. Our FlowDCN model leverages the strengths of both group-wise multiscale deformable convolutions and linear flow to generate high-quality images of various resolutions with high flexibility. Through extensive experiments, we demonstrate that FlowDCN outperforms the state-of-the-art transformer-based counterparts in terms of performance, convergence speed, and computational efficiency. Additionally, our model exhibits strong resolution extrapolation capabilities, achieving comparable results to previous models on arbitrary resolution without any additional training techniques. We believe that FlowDCN has a great potential to become a powerful tool for a wide range of image generation tasks and applications. ", "page_idx": 9}, {"type": "text", "text": "Limitations and Future Works ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Our current implementation of deformable convolution backward is inefficient to be on par with Attention. Our primary focus remains on optimizing the training speed. Once we have made significant strides in training optimization, we plan to scale up our FlowDCN to accommodate larger model parameters and higher training resolution, paving the way for more advanced explorations. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work is supported by the National Key R&D Program of China (No. 2022ZD0160900), the National Natural Science Foundation of China (No. 62076119), the Fundamental Research Funds for the Central Universities (No. 020214380119), the Nanjing University-China Mobile Communications Group Co., Ltd. Joint Institute, and the Collaborative Innovation Center of Novel Software Technology and Industrialization. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851, 2020. 2 [2] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. 2 [3] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. Advances in Neural Information Processing Systems, 35:26565\u201326577, 2022. 2 [4] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. 2, 3   \n[5] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 2, 3   \n[6] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural image synthesis. arXiv preprint arXiv:1809.11096, 2018. 2, 9   \n[7] Axel Sauer, Katja Schwarz, and Andreas Geiger. Stylegan-xl: Scaling stylegan to large diverse datasets. In ACM SIGGRAPH 2022 conference proceedings, pages 1\u201310, 2022. 2, 9   \n[8] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11315\u201311325, 2022. 2   \n[9] Michael S Albergo, Nicholas M Boff,i and Eric Vanden-Eijnden. Stochastic interpolants: A unifying framework for flows and diffusions. arXiv preprint arXiv:2303.08797, 2023. 2   \n[10] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:8780\u20138794, 2021. 2, 4, 8, 9, 10   \n[11] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: A vit backbone for diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22669\u201322679, 2023. 2, 4, 8, 10   \n[12] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4195\u20134205, 2023. 2, 5, 6, 7, 8, 9, 10   \n[13] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-\\alpha: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. 2   \n[14] Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-\\sigma: Weak-to-strong training of diffusion transformer for 4k text-to-image generation. arXiv preprint arXiv:2403.04692, 2024. 2   \n[15] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas M\u00fcller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. arXiv preprint arXiv:2403.03206, 2024. 2   \n[16] Jing Nathan Yan, Jiatao Gu, and Alexander M Rush. Diffusion models without attention. arXiv preprint arXiv:2311.18257, 2023. 2, 8   \n[17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 2   \n[18] Zeyu Lu, Zidong Wang, Di Huang, Chengyue Wu, Xihui Liu, Wanli Ouyang, and Lei Bai. Fit: Flexible vision transformer for diffusion model. arXiv preprint arXiv:2402.12376, 2024. 2, 8, 9, 10   \n[19] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. 2   \n[20] Yuwen Xiong, Zhiqi Li, Yuntao Chen, Feng Wang, Xizhou Zhu, Jiapeng Luo, Wenhai Wang, Tong Lu, Hongsheng Li, Yu Qiao, et al. Efficient deformable convnets: Rethinking dynamic and sparse operator for vision applications. arXiv preprint arXiv:2401.06197, 2024. 2, 4, 5, 6   \n[21] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11976\u201311986, 2022. 2   \n[22] Xiaohan Ding, Xiangyu Zhang, Jungong Han, and Guiguang Ding. Scaling up your kernels to 31x31: Revisiting large kernel design in cnns. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11963\u201311975, 2022. 2   \n[23] Nanye Ma, Mark Goldstein, Michael S Albergo, Nicholas M Boff,i Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. arXiv preprint arXiv:2401.08740, 2024. 2, 5, 6, 7, 8, 9   \n[24] W Wang, J Dai, Z Chen, Z Huang, Z Li, X Zhu, X Hu, T Lu, L Lu, H Li, et al. Internimage: Exploring largescale vision foundation models with deformable convolutions. arxiv. arXiv preprint arXiv:2211.05778, 2022. 4   \n[25] Xizhou Zhu, Han Hu, Stephen Lin, and Jifeng Dai. Deformable convnets v2: More deformable, better results. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9308\u20139316, 2019. 4   \n[26] Xun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance normalization. In Proceedings of the IEEE international conference on computer vision, pages 1501\u20131510, 2017. 5   \n[27] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 5   \n[28] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 5   \n[29] Yingqing He, Shaoshu Yang, Haoxin Chen, Xiaodong Cun, Menghan Xia, Yong Zhang, Xintao Wang, Ran He, Qifeng Chen, and Ying Shan. Scalecrafter: Tuning-free higher-resolution visual generation with diffusion models. In The Twelfth International Conference on Learning Representations, 2023. 5   \n[30] Shen Zhang, Zhaowei Chen, Zhenyu Zhao, Zhenyuan Chen, Yao Tang, Yuhao Chen, Wengang Cao, and Jiajun Liang. Hidiffusion: Unlocking high-resolution creativity and efficiency in low-resolution trained diffusion models. arXiv preprint arXiv:2311.17528, 2023. 5   \n[31] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 6   \n[32] Philippe Tillet, Hsiang-Tsung Kung, and David Cox. Triton: an intermediate language and compiler for tiled neural network computations. In Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages, pages 10\u201319, 2019. 6   \n[33] Chris Lattner and Vikram Adve. Llvm: A compilation framework for lifelong program analysis & transformation. In International symposium on code generation and optimization, 2004. CGO 2004., pages 75\u201386. IEEE, 2004. 6   \n[34] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:16344\u201316359, 2022. 6   \n[35] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. Advances in neural information processing systems, 2009. 6   \n[36] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30, 2017. 6   \n[37] Charlie Nash, Jacob Menick, Sander Dieleman, and Peter W Battaglia. Generating images with sparse representations. arXiv preprint arXiv:2103.03841, 2021. 6, 8   \n[38] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016. 6   \n[39] Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. Journal of Machine Learning Research, 23(47):1\u201333, 2022. 8   \n[40] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684\u201310695, 2022. 8, 10   \n[41] Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. Masked diffusion transformer is a strong image synthesizer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 23164\u201323173, 2023. 10 ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "table", "img_path": "e57B7BfA2B/tmp/a994c1efab5566ff43f9b6923f9e73dc7af04436bb436ecb7ee1684947205a9e.jpg", "table_caption": ["Table 7: Details of FlowDCN models. We follow DiT for the Small (S), Base (B), Large (L) and XLarge (XL) model configurations. "], "table_footnote": [], "page_idx": 12}, {"type": "text", "text": "B. Comparisons between FlowCNN and FlowDCN on ImageNet $256\\times256$ ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "The relationship between DCN and common CNN. As Eq. (6) states, DCN introduces a deformable field $\\Delta p(x)$ and dynamic weight $w(x)$ . When all features shares the same static weight instead of dynamic, and deformable field $\\Delta p(x)$ degrades to zeros, DCN degenerates to common CNN.Therefore, in most scenarios, DCN-like architectures are more powerful than common CNNs. Furthermore, the $\\,\\!\\,f\\!x\\,p_{k}$ in Tab. 2cindicates that we freeze the $p_{k}$ (not the deformable field $\\Delta p(x))$ and initialize it with a predefined grid. ", "page_idx": 12}, {"type": "text", "text": "Why not try a common CNN architecture. In many computer vision tasks, traditional CNNs have been outperformed by transformers, so we opted to explore the modern, advanced CNN variant, Deformable Convolutional Networks (DCN). Additionally, we conducted a small experiment where we replaced the DCN block in FlowDCN with standard $3\\mathrm{x}3$ and 5x5 group-wise convolution blocks. ", "page_idx": 12}, {"type": "table", "img_path": "e57B7BfA2B/tmp/538ac931272518492e620a4f315feda93ce51d9f4942544c6224d3687447d53e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 12}, {"type": "table", "img_path": "e57B7BfA2B/tmp/dcb463eb3525a8913428f2281d2636d3e1f682277d957eafb7b7f4856390d24c.jpg", "table_caption": ["Table 8: Image generation metrics comparisons between SiT, FlowDCN and FlowCNN under $\\mathbf{400k}$ training steps budgets. ", "Table 9: Benchmarking resolution extrapolations on ImageNet with various aspect ratio training. VAR indicates various aspect ratios training. We follow the same evaluation pipeline of FiT without using CFG. "], "table_footnote": [], "page_idx": 12}, {"type": "text", "text": "C. Resolution Extension with Various Aspect Ratios Training ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "While FlowDCN, trained on fixed-resolution images, is capable of generating images of arbitrary resolution within a reasonable aspect ratio range, its performance can be improved by adopting variable aspect ratio (VAR) training instead of a fixed 256x256 resolution. To ensure a fair comparison with FiT, which inherently uses VAR, we train a FlowDCN-B/2 model from scratch using VAR techniques. We evaluate our model using the same pipeline and reference batch as FiT, without CFG. ", "page_idx": 12}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 13}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 13}, {"type": "text", "text": "Justification: ", "page_idx": 13}, {"type": "text", "text": "Guidelines: ", "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 13}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 13}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 13}, {"type": "text", "text": "Justification: ", "page_idx": 13}, {"type": "text", "text": "Guidelines: ", "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 13}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 13}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 13}, {"type": "text", "text": "Justification: not include theoretical results ", "page_idx": 14}, {"type": "text", "text": "Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 14}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 14}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 14}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 14}, {"type": "text", "text": "Answer: [No] ", "page_idx": 15}, {"type": "text", "text": "Justification: We plan to opensource our code and implementation later. Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 15}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 15}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 15}, {"type": "text", "text": "Answer: [No] ", "page_idx": 15}, {"type": "text", "text": "Justification: Running experiments demands a lot of resources and time. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 16}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 16}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to ", "page_idx": 16}, {"type": "text", "text": "generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 17}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 17}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 17}, {"type": "text", "text": "Justification: Current models are only trained on small datasets. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 17}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 17}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 18}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 18}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 18}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] Justification: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 18}]