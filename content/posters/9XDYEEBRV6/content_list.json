[{"type": "text", "text": "Coded Computing for Resilient Distributed Computing: A Learning-Theoretic Framework ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Parsa Moradi Behrooz Tahmasebi Mohammad Ali Maddah-Ali University of Minnesota MIT CSAIL University of Minnesota moradi@umn.edu bzt@mit.edu maddah@umn.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Coded computing has emerged as a promising framework for tackling significant challenges in large-scale distributed computing, including the presence of slow, faulty, or compromised servers. In this approach, each worker node processes a combination of the data, rather than the raw data itself. The final result then is decoded from the collective outputs of the worker nodes. However, there is a significant gap between current coded computing approaches and the broader landscape of general distributed computing, particularly when it comes to machine learning workloads. To bridge this gap, we propose a novel foundation for coded computing, integrating the principles of learning theory, and developing a framework that seamlessly adapts with machine learning applications. In this framework, the objective is to find the encoder and decoder functions that minimize the loss function, defined as the mean squared error between the estimated and true values. Facilitating the search for the optimum decoding and functions, we show that the loss function can be upper-bounded by the summation of two terms: the generalization error of the decoding function and the training error of the encoding function. Focusing on the second-order Sobolev space, we then derive the optimal encoder and decoder. We show that in the proposed solution, the mean squared error of the estimation decays with the rate of $\\mathcal{O}(S^{3}N^{-3})$ and $\\mathcal{O}(S^{8/5}N^{-3/5})$ in noiseless and noisy computation settings, respectively, where $N$ is the number of worker nodes with at most $S$ slow servers (stragglers). Finally, we evaluate the proposed scheme on inference tasks for various machine learning models and demonstrate that the proposed framework outperforms the state-of-the-art in terms of accuracy and rate of convergence. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The theory of coded computing has been developed to improve the reliability and security of largescale machine learning platforms, effectively tackling two major challenges: (1) the detrimental impact of slow workers (stragglers) on overall computation efficiency, and (2) the threat of faulty or malicious workers that can compromise data accuracy and integrity. These challenges have been well-documented in the literature, including the seminal work [1] from Google. For instance, [2] reported that in a sample set of 3000 matrix multiplication jobs on AWS Lambda, while the median job time was 40 seconds, approximately $5\\%$ of worker nodes took 100 seconds to respond, and two nodes took as long as 375 seconds. Furthermore, coded computing has also been instrumental in addressing privacy concerns, a crucial aspect of distributed computing systems [3\u201312]. ", "page_idx": 0}, {"type": "text", "text": "The concept of coded computing has been motivated by the success of coding in communication over unreliable channels, where instead of transmitting raw data, the transmitter sends a (linear) combination of the data, known as coded data. This redundancy in the coded data enables the receiver to recover the raw data even in the presence of errors or missing values. Similarly, coded computing includes three layers [3, 11, 13] (see Figure 1(a)): ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "(1) The Encoding Layer in which a master node sends a (linear) combination of data, as coded data, to each worker node.   \n(2) The Computing Layer, in which the worker nodes apply a predefined computation to their assigned coded data and send the results back to the master node.   \n(3) The Decoding Layer, in which the master node recovers the final results from the computation results over coded data. In this layer, the decoder leverages the coded redundancy in the computation to recover the missing results of the stragglers and detect and correct the adversarial outputs. ", "page_idx": 1}, {"type": "text", "text": "The existing coded computing has largely built upon algebraic coding theory, drawing inspiration from the renowned Reed-Solomon code construction in communication [14], with proven straggler and Byzantine resiliency [15]. However, the coding in communication is designed for the exact recovery of the messages, built on a foundation that is inconsistent with the computational requirements of machine learning. Developing a code that preserves its specific construction while composing with computation is extremely challenging, leading to significant restrictions. Firstly, current methods are mainly restricted to specific computation functions, such as polynomials and matrix multiplication [3, 11, 13, 16, 17]. Secondly, rooted in algebraic error correction codes, existing approaches are tailored for finite field computations, leading to numerical instability when dealing with real-valued data [18, 19]. Furthermore, these methods are unsuitable for approximate, fixedpoint, or floating-point computing, where exact computation is neither possible nor necessary, such as in machine learning inference or training tasks. Finally, these schemes typically have a recovery threshold, which is the minimum number of samples required to recover results from coded outputs of worker nodes [3, 13]. If the number of workers falls below this threshold, the recovery process fails entirely. ", "page_idx": 1}, {"type": "text", "text": "Several works have attempted to mitigate the aforementioned issues and transform the coded computing scheme into a more robust and adaptable one, applicable to a wide range of computation functions. These efforts include approximating non-polynomial functions with polynomial ones [5, 20], refining the coding mechanism to enhance stability [21\u201325], and leveraging approximation computing techniques to reduce the recovery threshold and increase recovery flexibility [26\u201329]. However, these attempts fail to bridge the existing gap between coded computing and general distributed computing systems. The root cause of these issues lies in the fact that they are grounded in coding theory, based on a foundation that is not compatible with the requirements of large-scale machine learning. Therefore, this paper aims to address the following objective: ", "page_idx": 1}, {"type": "text", "text": "Objective: The main objective of this paper is to develop a new foundation for coded computing, not solely based on coding theory, but also grounded in learning theory, that seamlessly integrates with machine learning applications, offering a more natural and effective solution for general computing. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we establish a learning-theoretic foundation for coded computing, applicable to general computations. We adopt an end-to-end system perspective, that integrates an end-to-end loss function, to find the optimum encoding and decoding functions, focusing on straggler resiliency. We show that the loss function is upper-bounded by the sum of two terms: one characterizing the generalization error of the decoder function and the other capturing the training error of the encoder function. Regularizing the decoder layer, we derive the optimal decoder in the Reproducing Kernel Hilbert space (RKHS) of second-order Sobolev functions. This provides an explicit solution for the optimum decoder function and allows us to characterize the resulting loss of the decoding layer. The decoder loss appears as a regularizing term in optimizing the encoding function and represents the norm in another RKHS. Thus, the optimum solution for the encoding function can be derived, too. We address two noise-free and noisy computation settings, for which we derive the optimal encoder and decoder and corresponding convergence rate. We prove that the proposed framework exhibits a faster convergence rate compared to the state-of-the-art and the numerical evaluations support the theoretical derivations (see Figure 1(b)). ", "page_idx": 1}, {"type": "image", "img_path": "9XDYEEBRV6/tmp/4aa20351199190e6724ae79400bb000a7ebcb14654a9b6e515604997a279a312.jpg", "img_caption": ["Figure 1(a): Coded Computing: Each worker node Figure 1(b): The log-log plot of the expected error processes a combination of data (coded data). The de-versus the number of workers $(N)$ for the proposed coder recovers the final results, even in the presence of framework (LeTCC) and the state-of-the-art BACC [29]. missing outputs from some worker nodes. LeTCC framework not only achieves a lower estimation error but also has a faster convergence rate. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Contributions: The main contributions of this paper are: ", "page_idx": 2}, {"type": "text", "text": "\u2022 We develop a new foundation for coded computing by integrating it with learning theory, rather than relying solely on coding theory. We define the loss function as the mean square error of the computation estimation, averaged over all possible sets of at most $S$ stragglers (Section 3.1). To be able to find the best encoding and decoding functions, we bound the loss function with the summation of two terms, one characterizing the generalization error of the decoder function and the other capturing the training error of the encoder function (Section 3).   \n\u2022 Assuming that the encoder and decoder functions reside in the Hilbert space of secondorder Sobolev functions, we use the theory of RKHSs to find the optimum encoding and decoding functions and characterize the convergence rate for the expected loss in both noise-free and noisy computation regimes (Section 4).   \n\u2022 We have extensively evaluated the proposed scheme across different data points and computing functions including state-of-the-art deep neural networks and demonstrated that our proposed framework considerably outperforms the state-of-the-art in terms of recovery accuracy (Section 5). ", "page_idx": 2}, {"type": "text", "text": "2 Preliminaries and Problem Definition ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Notations ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Throughout this paper, uppercase and lowercase bold letters denote matrices and vectors, respectively. Coded vectors and matrices are indicated by a $\\sim$ sign, as in $\\tilde{\\mathbf{x}},\\tilde{\\mathbf{A}}$ . The set $\\{1,2,\\ldots,n\\}$ is denoted as $[n]$ and symbol $|S|$ denotes the cardinality of the set $S$ . Finally, we represent first, second and $k$ -th order derivative of function $f$ as $f^{\\prime},f^{\\prime\\prime}$ , and $f^{(k)}$ , respectively. ", "page_idx": 2}, {"type": "text", "text": "2.2 Problem Setting ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Consider a master node and a set of $N$ workers. The master node is tasked with computing $\\{\\mathbf{f}(x_{k})\\}_{k=1}^{K}$ using a cluster of $N$ worker nodes, given a set of $K$ data points $\\{\\mathbf{x}_{k}\\}_{k=1}^{K},\\mathbf{x}_{k}\\ \\in\\ \\mathbb{R}^{\\widecheck{d}}$ Here, f $\\colon\\ensuremath{\\mathbb{R}}^{d}\\,\\rightarrow\\,\\ensuremath{\\mathbb{R}}^{m}$ represents an arbitrary function, which could be a simple one-dimensional function or a complex deep neural network, and $K,d,m$ are integers. A naive approach would be to assign the computation of $\\mathbf{f}\\left(\\mathbf{x}_{k}\\right)$ to one worker node for $k\\in[K]$ . However, some worker nodes may act as stragglers, failing to complete their tasks within the required deadline. To mitigate this issue, the master node employs coding and sends $N$ coded data points to each worker node using an encoder function. Each coded data point is a combination of raw data points. Subsequently, each worker applies the function f(\u00b7) to the received coded data and sends the result, coded results, back to the master node. The master node\u2019s goal is to approximately recover $\\widehat{\\mathbf{f}}\\left(\\mathbf{x}_{k}\\right)\\approx\\mathbf{f}\\left(\\mathbf{x}_{k}\\right)$ using a decoder function, even if some worker nodes appear to be stragglers. The redundancy in the coded data and corresponding coded results enables the master node to recover the desirable results, $\\{\\mathbf{f}(\\mathbf{x}_{k})\\}_{k=1}^{K}$ . ", "page_idx": 2}, {"type": "image", "img_path": "9XDYEEBRV6/tmp/641bf0801c34dd8ac0d3371d5d450a9ff42949fbf364a58ed34bb49084778cf0.jpg", "img_caption": ["Figure 2: LeTCC framework. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "3 Proposed Framework: LeTCC ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Here, we propose a novel straggler-resistant Learning-Theoretic Coded Computing (LeTCC) framework for general distributed computing. As depicted in Figure 2, our framework comprises two encoding and decoding layers, with a computing layer sandwiched between them. The framework operates according to the following steps: ", "page_idx": 3}, {"type": "text", "text": "(1) Encoding Layer: The master node ftis an encoder regression function $\\mathbf{u}_{\\mathrm{enc}}\\,:\\,\\mathbb{R}\\,\\rightarrow\\,\\mathbb{R}^{d}$ at points $\\!\\!\\!\\!\\!\\!\\{(\\alpha_{k},\\mathbf{x}_{k})\\}_{k=1}^{K}$ for fixed, distinct, and ordered values $\\alpha_{1}<\\alpha_{2}<\\cdot\\cdot\\cdot<\\alpha_{K}\\in$ $\\mathbb{R}$ . Then, it computes the encoder function $\\mathrm{\\mathbf{u}}_{\\mathrm{enc}}(\\cdot)$ on another set of fixed, distinct, and ordered values $\\{\\dot{\\beta}_{n}\\}_{n=1}^{N}$ where $\\beta_{1}\\,<\\,\\beta_{2}\\,<\\,\\cdot\\,\\cdot\\,<\\,\\beta_{N}\\,\\in\\,\\mathbb{R}$ , with $k\\,\\in\\,[K]$ and $n\\,\\in\\,[N]$ Subsequently, the master node sends the coded data points $\\widetilde{\\mathbf{x}}_{n}=\\mathbf{u}_{\\mathrm{enc}}(\\beta_{n})\\in\\mathbb{R}^{d}$ to worker $n$ for $n\\,\\in\\,[N]$ . Note that each coded data point $\\tilde{\\mathbf{x}}_{n}$ is a combination of all initial points {xk}k=1.   \n(2) Computing Layer: Each worker node $\\boldsymbol n\\,\\in\\,[N]$ computes $\\mathbf{f}\\left(\\widetilde{\\mathbf{x}}_{n}\\right)\\,=\\,\\mathbf{f}\\bigl(\\mathbf{u}_{\\mathrm{enc}}\\bigl(\\beta_{n}\\bigr)\\bigr)$ on its assigned input and sends the result back to the master node.   \n(3) Decoding Layer: The master node receives the results $\\mathbf{f}(\\widetilde{\\mathbf{x}}_{v})_{v\\in\\mathcal{F}}$ from the non-straggler worker nodes in the set $\\mathcal{F}$ . Next, it ftis a decoder regression f\u2208uFnction $\\mathbf{u}_{\\mathrm{dec}}\\,:\\,\\mathbb{R}\\,\\rightarrow\\,\\mathbb{R}^{m}$ at points $(\\beta_{v},\\mathbf{f}(\\widetilde{\\mathbf{x}}_{v}))_{v\\in\\mathcal{F}}=(\\beta_{v},\\mathbf{f}(\\mathbf{u}_{\\mathrm{enc}}(\\beta_{v})))_{v\\in\\mathcal{F}}$ . Finally, using the function $\\mathbf{u}_{\\mathrm{dec}}(\\cdot)$ , the master node computes $\\hat{\\mathbf{f}}(\\mathbf{x}_{k})\\;:=\\;\\mathbf{u}_{\\mathrm{dec}}(\\alpha_{k})$ as an approximation of $\\mathbf{f}\\left(\\mathbf{x}_{k}\\right)$ for $\\textit{k}\\in\\ [K]$ . Recall that $\\mathbf{u}_{\\mathrm{dec}}(\\alpha_{k})\\approx\\mathbf{f}(\\mathbf{u}_{\\mathrm{enc}}(\\alpha_{k}))\\approx\\mathbf{f}(\\mathbf{x}_{k})$ . ", "page_idx": 3}, {"type": "text", "text": "As mentioned above, the master node selects and fixes the regression points, $\\{\\alpha_{k}\\}_{k=1}^{K}$ and $\\{\\beta_{n}\\}_{n=1}^{N}$ which remain constant throughout the entire process. The encoder and decoder functions are the only components subject to optimization. ", "page_idx": 3}, {"type": "text", "text": "Note that the computational efficiency of the encoding and decoding layers is crucial. This includes the fitting process of the encoder and decoder regression functions, as well as the computation of these regression functions at points $\\{\\beta_{v}\\}_{v\\in\\mathcal{F}}$ and $\\{\\alpha_{k}\\}_{k=1}^{K}$ . If the master node\u2019s computation time is not substantially decreased compared to computing $\\{\\mathbf{f}(\\mathbf{x}_{k})\\}_{k=1}^{K}$ by itself, then adopting this framework would not provide any benefits for the master node. ", "page_idx": 3}, {"type": "text", "text": "3.1 Objective ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We view the whole scheme as a unified predictive framework that provides an approximate estimation of the values {f(xk)}kK=1. We denote the estimator function of the LeTCC scheme as $\\hat{\\mathbf{f}}_{\\alpha,\\beta}[\\mathbf{u}_{\\mathrm{enc}},\\mathbf{u}_{\\mathrm{dec}},\\mathcal{F}](\\cdot)$ , where $\\pmb{\\alpha}:=[\\alpha_{1},\\ldots,\\alpha_{K}]^{T},\\beta:=[\\beta_{1},\\ldots,\\beta_{N}]^{T}$ , and $\\mathcal{F}$ represents the set of non-straggler worker nodes. ", "page_idx": 4}, {"type": "text", "text": "Let us define a random variable $F_{S,N}$ distributed over the set of all subsets of $N$ workers with maximum $S$ stragglers, $\\{{\\mathcal{F}}:{\\mathcal{F}}\\subseteq[N],|{\\mathcal{F}}|\\geqslant N-S\\}$ . Also, suppose each worker node $n\\in[N]$ computes the function ${\\bf f}_{n}(x)={\\bf f}(x)+{\\pmb\\epsilon}_{n}$ , where $\\epsilon_{n}$ , $n\\,\\in\\,[N]$ are independent zero-mean noise vectors with covariance ${\\boldsymbol{\\sigma}}^{2}\\mathbf{I}$ . ", "page_idx": 4}, {"type": "text", "text": "This enables us to define the following loss function, which evaluates the framework\u2019s performance: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{R}(\\hat{\\mathbf{f}}):=\\underset{\\epsilon,\\mathcal{F}\\sim F_{S,N}}{\\mathbb{E}}\\left[\\frac{1}{K}\\sum_{k=1}^{K}\\left\\|\\hat{\\mathbf{f}}(\\mathbf{x}_{k})-\\mathbf{f}(\\mathbf{x}_{k})\\right\\|_{2}^{2}\\right]=\\underset{\\epsilon,\\mathcal{F}\\sim F_{S,N}}{\\mathbb{E}}\\left[\\frac{1}{K}\\sum_{k=1}^{K}\\left\\|\\mathbf{u}_{\\mathsf{d e c}}(\\alpha_{k})-\\mathbf{f}(\\mathbf{x}_{k})\\right\\|_{2}^{2}\\right],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\hat{\\mathbf{f}}(\\mathbf{x})\\;:=\\;\\hat{\\mathbf{f}}_{\\alpha,\\beta}[\\mathbf{u}_{\\mathrm{enc}},\\mathbf{u}_{\\mathrm{dec}},\\mathcal{F}](\\mathbf{x})$ to simplify the notation, $\\lVert\\cdot\\rVert_{2}$ represents the $\\ell_{2}$ -norm, and $\\pmb{\\epsilon}\\,=\\,[\\epsilon_{1},...\\,,\\epsilon_{N}]^{T}$ . Our objective is to find $\\mathrm{\\mathbf{u}}_{\\mathrm{enc}}(.)$ and $\\mathrm{\\mathbf{u}}_{\\mathrm{dec}}(.)$ that minimize the objective function (1), which is very challenging, given that $\\widehat{\\mathbf{f}}\\left(.\\right)$ is a composition of $\\mathrm{\\mathbf{u}}_{\\mathrm{enc}}(.)$ and $\\mathbf{u}_{\\mathrm{dec}}(.)$ and the computation in the middle. Here, we take an important step to decompose these two, to gain a deeper understanding of interactions. Adding and subtracting $\\mathbf{f}(\\mathbf{u}_{\\mathrm{enc}}(\\alpha_{k}\\bar{)})$ and utilizing inequality of arithmetic and geometric means (AM-GM), one can obtain an upper bound for (1): ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{R}(\\hat{\\mathbf{f}})=\\underset{\\epsilon,\\mathcal{F}\\sim F_{S,N}}{\\mathbb{E}}\\left[\\frac{1}{K}\\sum_{k=1}^{K}\\|(\\mathbf{u}_{\\mathrm{dec}}(\\alpha_{k})-\\mathbf{f}(\\mathbf{u}_{\\mathrm{enc}}(\\alpha_{k})))+(\\mathbf{f}(\\mathbf{u}_{\\mathrm{enc}}(\\alpha_{k}))-\\mathbf{f}(\\mathbf{x}_{k}))\\|_{2}^{2}\\right]}\\\\ &{\\quad\\leqslant\\underset{\\epsilon\\leq\\frac{K}{K}\\sim F_{S,N}}{\\underbrace{\\mathbb{E}}}\\left[\\frac{2}{K}\\sum_{k=1}^{K}\\|\\mathbf{u}_{\\mathrm{dec}}\\left(\\alpha_{k}\\right)-\\mathbf{f}\\left(\\mathbf{u}_{\\mathrm{enc}}\\left(\\alpha_{k}\\right)\\right)\\|_{2}^{2}\\right]+\\underbrace{\\frac{2}{K}\\sum_{k=1}^{K}\\|\\mathbf{f}(\\mathbf{u}_{\\mathrm{enc}}(\\alpha_{k}))-\\mathbf{f}(\\mathbf{x}_{k})\\|_{2}^{2}}_{\\mathcal{L}_{\\mathrm{enc}}(\\hat{\\mathbf{f}})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The right-hand side of (2) comprises two terms, which uncover an interesting interplay between the encoder and decoder regression functions. Let us elaborate on what each term corresponds to. ", "page_idx": 4}, {"type": "text", "text": "\u2022 $\\mathcal{L}_{\\mathrm{dec}}(\\hat{\\mathbf{f}})$ \u2013 The expected generalization error of the decoder regression: Recall that the master node fits a decoder regression function, $\\mathbf{u}_{\\mathrm{dec}}(\\cdot)$ , at a set of points denoted as $\\{\\big(\\beta_{v},\\mathbf{f}\\big(\\mathbf{u}_{\\mathrm{enc}}(\\beta_{v})\\big)\\big)\\}_{v\\in\\mathcal{F}}$ . $\\mathcal{L}_{\\mathrm{dec}}$ represents the $\\ell_{2}$ -norm of the decoder regression function\u2019s   \nerror on a distinct set \u2208ofF points $\\{\\alpha_{k}\\}_{k=1}^{K}$ , which are different from its training data $\\{\\beta_{v}\\}_{v\\in\\mathcal{F}}$ Consequently, this term provides an unbiased estimate of the decoder\u2019s generalization error. Given that the decoder regression function develops to estimate $\\mathbf{f}(\\mathbf{u}_{\\mathrm{enc}}(\\cdot))$ , the generalization error of the decoder regression is inherently tied to the properties of $\\mathbf{f}\\left(\\mathbf{u}_{\\mathrm{enc}}(\\cdot)\\right)$ . This, in turn, is influenced by characteristics of both the $\\mathbf f(\\cdot)$ and $\\mathrm{\\mathbf{u}}_{\\mathrm{enc}}(\\cdot)$ functions, making the $\\mathcal{L}_{\\mathrm{dec}}(\\hat{\\mathbf{f}})$ a complex interplay of these two functions.   \n\u2022 $\\mathcal{L}_{\\mathrm{enc}}(\\hat{\\mathbf{f}})-\\mathbf{A}$ proxy to the training error of the encoder regression: Remember that the   \nencoder regression is fitted at points $\\{(\\alpha_{k},\\mathbf{x}_{k})\\}_{k=1}^{K}$ . Consequently, the training error is   \ncalculated as $\\begin{array}{r}{\\frac{1}{K}\\sum_{k=1}^{K}\\left|\\left|\\mathbf{u}_{\\mathrm{enc}}(\\alpha_{k})-\\mathbf{x}_{k}\\right|\\right|_{2}^{2}}\\end{array}$ . Therefore, ${\\mathcal{L}}_{\\mathrm{enc}}$ represents the encoder training error magnified by the effect of computing function $\\mathbf f(\\cdot)$ . Specifically, if $\\mathbf f(\\cdot)$ is $q$ -Lipschitz, then $\\mathcal{L}_{\\mathrm{enc}}(\\hat{\\bf f})$ can be upper bounded by: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{2}{K}\\sum_{k=1}^{K}\\|\\mathbf{f}(\\mathbf{u}_{\\mathrm{enc}}(\\alpha_{k}))-\\mathbf{f}(\\mathbf{x}_{k})\\|_{2}^{2}\\leqslant\\frac{2q^{2}}{K}\\sum_{k=1}^{K}\\|\\mathbf{u}_{\\mathrm{enc}}(\\alpha_{k})-\\mathbf{x}_{k}\\|_{2}^{2}\\,.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "4 Main Results ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we examine the proposed framework from a theoretical standpoint. We provide a comprehensive explanation of the design process for the decoder and encoder functions and subsequently analyze the convergence rate. For simplicity, we present the results for a one-dimensional function $f:\\mathbb{R}\\rightarrow\\mathbb{R}$ . These results are generalizable to the case where $f:\\mathbb{R}\\rightarrow\\mathbb{R}^{m}$ , as discussed in Appendix F. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Suppose the regression points, $\\{\\alpha_{k}\\}_{k=1}^{K},\\{\\beta_{n}\\}_{n=1}^{N}$ , are confined to the interval $\\Omega:=(-1,1)$ and $u_{\\mathrm{enc}},u_{\\mathrm{dec}}\\in\\widetilde{\\mathcal{H}}^{2}\\left(\\Omega;\\mathbb{R}\\right)$ , where $\\widetilde{\\mathcal{H}}^{2}\\left(\\Omega;\\mathbb{R}\\right)$ is the reproducing kernel Hilbert space (RKHS) of secondorder Sobole v functions on th e  interval $\\Omega$ induced with the norm $\\begin{array}{r}{\\|f\\|_{\\widetilde{\\mathcal H}^{2}(\\Omega;\\mathbb R)}^{2}:=\\int_{\\Omega}(f^{\\prime\\prime}(t))^{2}\\,d t\\,+}\\end{array}$ $f(-1)^{2}\\,+\\,f^{\\prime}(-1)^{2}$ which is an equivalent norm on Sobolev space in troduced by [30] (see (28) in Appendix A.1). The definition and properties of Sobolev spaces, along with their reproducing kernels and norms, are reviewed in Appendix A.1. ", "page_idx": 5}, {"type": "text", "text": "Decoder Design: Since $\\mathcal{L}_{\\mathrm{dec}}(\\hat{\\mathbf{f}})$ in the decomposition (2) characterizes the generalization error of the decoder function, we propose a regularized objective function for the decoder: ", "page_idx": 5}, {"type": "equation", "text": "$$\nu_{\\mathrm{dec}}^{\\star}=\\operatorname*{argmin}_{u\\in\\tilde{\\mathcal{H}}^{2}(\\Omega;\\mathbb{R})}\\frac{1}{\\lvert\\mathcal{F}\\rvert}\\sum_{v\\in\\mathcal{F}}\\left(u\\left(\\beta_{v}\\right)-f\\left(u_{\\mathrm{enc}}\\left(\\beta_{v}\\right)\\right)\\right)^{2}+\\lambda_{\\mathrm{d}}\\int_{\\Omega}\\left(u^{\\prime\\prime}(t)\\right)^{2}\\,d t.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The first term in (4) corresponds to the mean squared error, while the second term characterizes the smoothness of the decoder function on the interval $\\Omega$ . Equation (4) represents a Kernel Ridge Regression problem (KRR). It can be shown that the solution of (4) has the following form [31, 32]: ", "page_idx": 5}, {"type": "equation", "text": "$$\nd_{0}+d_{1}t+\\sum_{v=1}^{|\\mathcal{F}|}c\\phi_{0}(t,\\beta_{i_{v}}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $d_{0},d_{1}\\,\\in\\,\\mathbb{R}$ , $\\phi_{0}(\\cdot,\\cdot)$ is the kernel function of $\\mathcal{H}_{0}^{2}\\left(\\Omega;\\mathbb{R}\\right)$ (see Definition 2 and (44) in Appendix A.1), and $\\mathbf{c}=[c_{1},\\hdots,c_{|\\mathcal{F}|}]^{T}\\in\\mathbb{R}^{|\\mathcal{F}|}$ . Substituting (5) into the main objective (4), the coefficient vector c can be efficiently computed by optimizing a quadratic equation [33]. This solution is known as the second-order smoothing spline function. The theoretical properties of smoothing splines are reviewed in Appendix A.2. ", "page_idx": 5}, {"type": "text", "text": "Let us define the following variables, which represent the maximum and minimum distances between consecutive data points in the decoder layer, $\\beta_{n_{n=1}}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\Delta_{\\operatorname*{max}}:=\\operatorname*{max}_{n\\in\\{0\\}\\cup[N]}\\{\\beta_{n+1}-\\beta_{n}\\},\\quad\\Delta_{\\operatorname*{min}}:=\\operatorname*{min}_{n\\in[N-1]}\\left\\{\\beta_{n+1}-\\beta_{n}\\right\\},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "with $\\beta_{0}:=-1$ and $\\beta_{N+1}:=1$ . The following theorems provide crucial insights for designing the encoder function as well as deriving the convergence rates. ", "page_idx": 5}, {"type": "text", "text": "Theorem 1 (Upper bound for noiseless computation, $\\sigma_{0}\\;=\\;0_{,}$ ). Consider the LeTCC framework with $N$ worker nodes and at most $S$ stragglers. Assume $\\{\\alpha_{k}\\}_{k=1}^{K}$ are arbitrary and distinct points in $\\Omega\\,=\\,(-1,1)$ and there exist a constant $J$ that $\\begin{array}{r}{\\Delta_{m a x}\\leqslant\\frac{J}{N}}\\end{array}$ . If $f(\\cdot)$ is a $q$ -Lipschitz continuous function, then: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{R}(\\hat{f})\\leqslant C_{1}\\left(\\frac{S+1}{N}\\right)^{3}\\cdot\\|(f\\circ u_{e n c})^{\\prime\\prime}\\|_{L^{2}(\\Omega;\\mathbb{R})}^{2}+\\frac{2q^{2}}{K}\\sum_{k=1}^{K}(u_{e n c}(\\alpha_{k})-x_{k})^{2},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $C_{1}$ is a constant. ", "page_idx": 5}, {"type": "text", "text": "The proof of Theorem 1 and the detailed expression for $C_{1}$ can be found in Appendix B.1. ", "page_idx": 5}, {"type": "text", "text": "Theorem 2 (Upper bound for noisy computation). Consider the LeTCC framework with $N$ worker computes $f_{n}(x)=f(x)+\\epsilon_{n}$ $S$ with $\\mathbb{E}[\\epsilon_{n}]=0$ (N1S)4 \u2a7d\u03bbd \u2a7d\u03bb0 for \u03bb0 \u2208R. Assume each worker node $\\mathbb{E}[\\epsilon_{n}^{2}]\\leqslant\\sigma_{0}^{2}$ e $\\{\\alpha_{k}\\}_{k=1}^{K}$ are arbitrary and distinct points in $\\Omega=(-1,1)$ and suppose there is constant $B$ such that $\\frac{\\Delta_{m a x}}{\\Delta_{m i n}}\\leqslant B$ . Assume is a $q$ -Lipschitz continuous function. Then, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\Im(\\hat{f})\\leqslant4\\left(\\frac{\\sigma_{0}^{2}}{N-S}\\right)^{\\frac{3}{8}}\\left(C_{2}\\cdot C(\\lambda_{0})\\cdot p_{4}(S)\\cdot\\|(f\\circ u_{e n c})^{\\prime\\prime}\\|_{L^{2}(\\Omega;\\mathbb{R})}^{2}\\right)^{\\frac{2}{5}}+\\frac{2q^{2}}{K}\\sum_{k=1}^{K}(u_{e n c}(\\alpha_{k})-x_{k})^{2},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "if ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{1}{(N-S)^{\\frac{1}{5}}\\lambda_{0}^{\\frac{1}{4}}}\\leqslant\\left(\\frac{C_{2}\\cdot C(\\lambda_{0})\\cdot p_{4}(S)\\cdot\\left\\|(f\\circ u_{e n c})^{(2)}\\right\\|_{L^{2}(\\Omega;\\mathbb{R})}^{2}}{\\sigma_{0}^{2}}\\right)^{\\frac{1}{5}}\\leqslant(N-S)^{\\frac{4}{5}}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $C_{2}$ is a constant, $C(\\lambda_{0})=\\mathcal{O}(\\lambda_{0}^{\\frac{1}{2}})$ is an increasing function of $\\lambda_{0}$ , and $p_{4}(S)$ is a degree-4 polynomial in $S$ with positive constant coefficients. ", "page_idx": 6}, {"type": "text", "text": "The proof and expressions for $C_{2}$ and $C(\\lambda_{0})$ are provided in Appendix B.2. ", "page_idx": 6}, {"type": "text", "text": "Encoder Design: The upper bounds established in Theorems 1, 2 hold for all $u_{\\mathrm{enc}}\\,\\in\\,\\widetilde{\\mathcal{H}}^{2}\\left(\\Omega;\\mathbb{R}\\right)$ . However, they do not directly lead to a design for $u_{\\mathrm{enc}}(\\cdot)$ . To address this, we present the f ollowing theorem which bounds the $\\lVert f\\circ u_{\\mathrm{enc}}\\rVert_{L^{2}(\\Omega;\\mathbb{R})}^{2}$ , enabling us to construct $u_{\\mathrm{{enc}}}(\\cdot)$ without compromising the convergence rate. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3. Consider a LeTCC scheme. Assume computing function $f(\\cdot)$ is $q$ -Lipschitz continuous and $\\|f^{\\prime\\prime}\\|_{L^{\\infty}(\\Omega;\\mathbb{R})}\\leqslant\\nu$ . Then: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{R}(\\widehat{f})\\leqslant\\frac{2q^{2}}{K}\\sum_{k=1}^{K}(u_{e n c}(\\alpha_{k})-x_{k})^{2}+\\lambda_{e}\\cdot\\psi\\big(\\left\\|u_{e n c}\\right\\|_{\\widetilde{\\mathcal{H}}^{2}(\\Omega;\\mathbb{R})}^{2}\\big),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "for some monotonically increasing function $\\psi\\;\\;:\\;\\;\\mathbb{R}^{+}\\;\\;\\rightarrow\\;\\;\\mathbb{R}^{+}$ , where $\\lambda_{e}$ is depending on $(N,S,\\sigma_{0},q,\\nu)$ . ", "page_idx": 6}, {"type": "text", "text": "The proof can be found in Appendix B.3. By applying the representer theorem [34], we can de  \nduce that the optimal encoder $\\begin{array}{r}{u_{\\mathrm{enc}}(\\cdot)=\\sum_{k=1}^{K}z_{k}\\phi(\\alpha_{k},\\cdot)}\\end{array}$ ,) . wHhoerwe $u_{\\mathrm{enc}}(\\cdot)$ $\\mathbf{z}\\in\\mathbb{R}^{K}$ , which minimizes the right-hand side of (10) takes the form ,t ao ntdh $\\phi$ nios nt-hlie nkeearrniteyl  fouf , nc aolfc $\\widetilde{\\mathcal{H}}^{2}\\left(\\Omega;\\mathbb{R}\\right)$ ,  vaasl udiessc oufs stehde $g(\\cdot)$   \ncoefficients $\\mathbf{z}$ is challenging. Nevertheless, we demonstrate that the coefficients can be efficiently   \nderived under certain mild assumptions. ", "page_idx": 6}, {"type": "text", "text": "Proposition 1. In the noiseless case, there exists $M\\ \\in\\ \\mathbb{R}$ that depends only on $\\{\\alpha_{k}\\}_{k=1}^{K}$ and $\\{x_{k}\\}_{k=1}^{K}$ , such that: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{R}(\\hat{f})\\leqslant\\widetilde{R}(u_{e n c}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "equation", "text": "$$\n\\widetilde{R}(u):=\\frac{2q^{2}}{K}\\sum_{k=1}^{K}(u(\\alpha_{k})-x_{k})^{2}+\\lambda_{e}\\cdot(m_{1}+m_{2}M)\\left(M+\\int_{\\Omega}u^{\\prime\\prime}(t)^{2}\\,d t\\right),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "See Appendix B.4 for the proof. Proposition 1 states that, under mild assumptions there exists a smoothing spline that minimizes the upper bound given in (12) without changing in convergence rate. ", "page_idx": 6}, {"type": "text", "text": "Convergence Rate: Using Theorems 1, 2, and 3, we can derive the convergence rate of the proposed scheme as stated in the following theorem. ", "page_idx": 6}, {"type": "text", "text": "Theorem 4 (Convergence rate). For LeTCC scheme with $N$ worker nodes and a maximum of $S$ stragglers, $\\mathcal{R}(\\hat{f})\\,\\leqslant\\,\\mathcal{O}\\left(S^{\\frac{8}{5}}N^{-{\\frac{3}{5}}}\\right)$ for the noisy computation, and $\\mathcal{R}(\\hat{f})\\,\\leqslant\\,\\mathcal{O}\\left(S^{3}N^{-3}\\right)$ for the noiseless setting. ", "page_idx": 6}, {"type": "text", "text": "Refer to Appendix B.5 for the proof. Notably, the convergence rate yields from Theorem 4 surpasses the state-of-the-art Berrut coded computing approach upper bound [29] (Figure 1), both with respect to $S$ and $N$ (see Appendix C.1 for detailed comparison). ", "page_idx": 6}, {"type": "text", "text": "5 Experimental Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we extensively evaluate the proposed scheme across various scenarios. Our assessments involve examining multiple deep neural networks as computing functions and exploring the impact of different numbers of stragglers on the scheme\u2019s efficiency. The experiments are run using PyTorch [35] in a single GPU machine. We evaluate the performance of the LeTCC scheme in three different model architectures: ", "page_idx": 6}, {"type": "table", "img_path": "9XDYEEBRV6/tmp/7d5e3ba08d20cb344523cc9a86112b96bc2fa5cf148eb3ceedca10abb43cc727.jpg", "table_caption": ["Table 1: Comparison of the proposed framework (LeTCC) and the state-of-the-art (BACC) in terms of the Root Mean Squared Error (RMSE) and the Relative Accuracy (RelAcc). "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "\u2022 Shallow model: We choose LeNet5 [36] architecture as a known shallow network with approximately $6\\times10^{4}$ parameters, trained on the MNIST [37]. ", "page_idx": 7}, {"type": "text", "text": "\u2022 Deep model with low-dimensional output: In this scenario, we evaluate the proposed scheme when the function is a deep neural network trained on color images in CIFAR-10 [38] dataset. We use the recently introduced RepVGG [39] network with around 26 million parameters which was trained on CIFAR- $10^{1}$ . ", "page_idx": 7}, {"type": "text", "text": "\u2022 Deep model with high-dimensional output: Finally, we demonstrate the performance of the LeTCC scheme in a scenario where the input and output of the computing function are high-dimensional, and the function is a relatively large neural network. We consider the Vision Transformer (ViT) [40] as one of the state-of-the-art base neural networks in computer vision for our prediction model, with more than 80 million parameters (in the base version). The network was trained and fine-tuned on the ImageNet-1K dataset $[41]^{2}$ . ", "page_idx": 7}, {"type": "text", "text": "We use the output of the last softmax layer of each model as the output. ", "page_idx": 7}, {"type": "text", "text": "Hyper-parameters: The entire encoding and decoding process is the same for different functions, as we adhere to a non-parametric approach. The sole hyper-parameters involved are the two smoothing parameters $(\\lambda_{\\mathrm{enc}},\\lambda_{\\mathrm{dec}})$ which are determined using cross-validation and greed search over different values of the smoothing parameters. ", "page_idx": 7}, {"type": "text", "text": "Baseline: We compare LeTCC with the Berrut approximate coded computing (BACC) introduced by [29] as the state-of-the-art coded computing scheme for general computing. The BACC framework is used in [29] for training neural networks and in [42] for inference. Although Berrut coded computing [29] is the only existing coded computing scheme for general functions, we include a comparison of the proposed framework with the Lagrange coded computing scheme [3] for polynomial computation in Appendix D. ", "page_idx": 7}, {"type": "text", "text": "Interpolation Points: We choose Chebyshev points of the first and second kind, $\\{\\alpha_{i}\\}_{k=1}^{K}\\ =$ $\\cos\\left({\\frac{(2k-1)\\pi}{2K}}\\right)$ and $\\begin{array}{r}{\\{\\beta_{n}\\}_{n=1}^{N}=\\cos\\left(\\frac{(n-1)\\pi}{N-1}\\right)}\\end{array}$ , for fair comparison with [29]. ", "page_idx": 7}, {"type": "text", "text": "Evaluation Metrics: We employ two evaluation metrics to assess the performance of the proposed framework: Relative Accuracy (RelAcc) and Root Mean Squared Error (RMSE). RelAcc is defined as the ratio of the base model\u2019s prediction accuracy to the accuracy of the estimated model on the initial data points. RMSE, on the other hand, is our main loss defined in (1) which measures the empirical average of the root mean square difference over multiple batches of and non-straggler set $\\mathcal{F}$ , providing an unbiased estimation of expected mean square error, $\\begin{array}{r}{\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{X},\\mathcal{F}}\\left[\\frac{1}{K}\\sum_{k=1}^{\\bar{K^{-}}}\\|\\mathbf{f}(\\mathbf{x}_{k})-\\bar{\\mathbf{f}^{\\prime}}(\\mathbf{x}_{k})\\|_{2}\\right]}\\end{array}$ , for data distribution $\\mathcal{X}$ . ", "page_idx": 7}, {"type": "text", "text": "Performance Evaluation: Table 1 presents both RMSE and RelAcc metrics side by side. The results demonstrate that LeTCC outperforms BACC across various architectures and configurations, with an average improvement of $15\\%$ , $17\\%$ , and $9\\%$ in RMSE for LeNet, RepVGG, and ViT architectures, respectively, and a $2\\%$ , $5\\%$ , and $4\\%$ enhancement in RelAcc. ", "page_idx": 7}, {"type": "image", "img_path": "9XDYEEBRV6/tmp/c19a0d07d23eba553569ba70d1f0f8cfb270f4dac43193bd8967f85e3796a37b.jpg", "img_caption": ["Figure 3: Performance comparison of LeTCC and BACC with a $95\\%$ confidence interval across a diverse range of stragglers for different models in a low-redundancy regime (smaller $\\frac{N}{K}$ ). "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "In a subsequent analysis, we evaluate the performance of LeTCC in comparison to BACC across a variety of straggler scenarios. For each number of stragglers, $S$ , we randomly select $S$ workers to act as stragglers. Both schemes are then run with the same input data points and straggler configurations, and the process is repeated 20 times. We record the average values of the RelAcc and RMSE metrics, along with their $95\\%$ confidence intervals. Figures 3 and 4 illustrate the performance of both schemes across three model architectures. As shown in both figures, the proposed scheme consistently outperforms BACC for nearly all straggler values. In Figure 3, where NK is relatively small\u2013indicating a system design without excessive redundancy, which is more practical\u2013the proposed scheme demonstrates even greater improvements in both metrics. ", "page_idx": 8}, {"type": "text", "text": "Computational Complexity: The calculation and inference of smoothing spline coefficients can be performed linearly in the number of regression points by leveraging B-spline basis functions [43\u201345]. Consequently, the encoding and decoding processes in LeTCC, which involve evaluating new points and calculating the fitted coefficients, have computational complexities of $\\mathcal{O}(K\\cdot d)$ and ${\\mathcal{O}}(({\\bar{N}}-S)\\cdot m)$ , respectively, where $d$ is the input dimension and $m$ is the output dimension of the computing function $\\mathbf f(\\cdot)$ . This complexity is comparable to that of BACC, which has complexities of $\\mathcal{O}(\\bar{K})$ and $O(N-S)$ for its encoding and decoding layers [29]. Table 2 in Appendix C.2 presents a comparison of the total end-to-end processing time statistics for the LeTCC and BACC schemes. ", "page_idx": 8}, {"type": "text", "text": "Sensitivity Analysis: We additionally investigate the sensitivity of the proposed schemes performance to the value of the smoothing parameter, as well as the sensitivity of the optimal smoothing parameter to the number of stragglers (or workers). The results are presented in Appendix E. As shown in Table 3 and Figure 6, the optimal smoothing parameters and the scheme\u2019s performance exhibit low sensitivity to the number of stragglers (or worker nodes) and to the smoothing parameters, respectively. ", "page_idx": 8}, {"type": "text", "text": "Coded Points: We also compare the coded points $\\{\\widetilde{\\mathbf{x}}_{\\mathbf{n}}\\}_{n=1}^{N}$ sent to the workers in LeTCC and BACC schemes. The results, shown in Figure 7, demonstrate that BACC coded samples exhibit highfrequency noise which causes the scheme to approximate the original prediction worse than LeTCC. ", "page_idx": 8}, {"type": "text", "text": "6 Related Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Coded computing was initially introduced to tackle the challenges of distributed computation, particularly the existence of stragglers or slow workers, and also faulty or adversarial nodes. Traditional approaches to deal with stragglers primarily rely on repetition [1, 46\u201350], where each task is assigned to multiple workers either proactively or reactively. Recently, coded computing approaches have reduced the overhead of repetition by leveraging coding theory and embedding redundancy in the worker\u2019s input data [3, 13, 26, 29, 51\u201355]. This technique, which mainly relies on theory of coding, has been developed for specific types of structured computations, such as polynomial computation and matrix multiplication [3, 7, 12, 13, 17, 52, 56\u201358]. Recently, there have been attempts to generalize coded computing for general computations [4, 5, 29, 59]. Towards extending the application of coding computing to machine learning computation, Kosaian et al. [59] suggest training a neural network to predict coded outputs from coded data points. However, the scheme of Kosaian et al. [59] requires a complex training process and tolerates only one straggler. In another work, JahaniNezhad and Maddah-Ali [29] proposes BACC, a model-agnostic and numerically stable framework for general computations. They successfully employed BACC to train neural networks on a cluster of workers, while tolerating a larger number of stragglers. Building on the BACC framework, Soleymani et al. [42] introduced ApproxIFER scheme, as a straggler resistance and Byzantine-robust prediction serving system. However, the scheme of BACC uses a reasonable rational interpolation (Berrut interpolation [60]), off the shelf, for encoding and decoding, without considering any end-to-end cost function to optimize. In contrast, we theoretically formalize a new foundation of coded computing grounded in learning theory, which can be naturally used for machine learning applications. ", "page_idx": 8}, {"type": "image", "img_path": "9XDYEEBRV6/tmp/b63be75aef1468dfbed9aa7eb14865512fa01dba26c72f2a750774c49c2d1b50.jpg", "img_caption": ["Figure 4: Performance comparison of LeTCC and BACC with a $95\\%$ confidence interval across a diverse range of stragglers for different models in a high-redundancy regime (larger $\\frac{N}{K}$ ). "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "7 Conclusions and Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we developed a new foundation for coded computing based on learning theory, contrasting with existing works that rely on coding theory and use metrics like minimum distance and recovery threshold for design. This shift in foundations removes barriers to using coded computing for machine learning applications, allows us to design optimal encoding and decoding functions, and achieves convergence rates that outperform the state of the art. Moreover, the experimental evaluations validate the theoretical guarantees. While this work focuses on straggler mitigation, future work will extend our proposed scheme to achieve Byzantine robustness and privacy, offering promising avenues for further research. ", "page_idx": 9}, {"type": "text", "text": "8 Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This material is based upon work supported by the National Science Foundation under Grant CIF2348638. Behrooz Tahmasebi is supported by NSF Award CCF-2112665 (TILOS AI Institute) and NSF Award 2134108. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Jeffrey Dean and Luiz Andr\u00e9 Barroso. The tail at scale. Communications of the ACM, 56(2): 74\u201380, 2013.   \n[2] Vipul Gupta, Shusen Wang, Thomas Courtade, and Kannan Ramchandran. Oversketch: Approximate matrix multiplication for the cloud. In 2018 IEEE International Conference on Big Data (Big Data), pages 298\u2013304. IEEE, 2018.   \n[3] Qian Yu, Songze Li, Netanel Raviv, Seyed Mohammadreza Mousavi Kalan, Mahdi Soltanolkotabi, and Salman A Avestimehr. Lagrange coded computing: Optimal design for resiliency, security, and privacy. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 1215\u20131225. PMLR, 2019.   \n[4] Jinhyun So, Basak Guler, and Amir Salman Avestimehr. A scalable approach for privacypreserving collaborative machine learning. In Advances in Neural Information Processing Systems, volume 33, pages 8054\u20138066, 2020.   \n[5] Jinhyun So, Ba\u00b8sak G\u00fcler, and Amir Salman Avestimehr. CodedPrivateML: A fast and privacypreserving framework for distributed machine learning. IEEE Journal on Selected Areas in Information Theory, 2(1):441\u2013451, 2021.   \n[6] Zhuqing Jia and Syed Ali Jafar. On the capacity of secure distributed batch matrix multiplication. IEEE Transactions on Information Theory, 67(11):7420\u20137437, 2021.   \n[7] Malihe Aliasgari, Osvaldo Simeone, and J\u00f6rg Kliewer. Private and secure distributed matrix multiplication with flexible communication load. IEEE Transactions on Information Forensics and Security, 2020.   \n[8] Minchul Kim and Jungwoo Lee. Private secure coded computation. In 2019 IEEE International Symposium on Information Theory (ISIT), pages 1097\u20131101. IEEE, 2019.   \n[9] Wei-Ting Chang and Ravi Tandon. On the upload versus download cost for secure and private matrix multiplication. In 2019 IEEE Information Theory Workshop (ITW), pages 1\u20135. IEEE, 2019.   \n[10] Wei-Ting Chang and Ravi Tandon. On the capacity of secure distributed matrix multiplication. In 2018 IEEE Global Communications Conference (GLOBECOM), pages 1\u20136, 2018.   \n[11] Qian Yu, Mohammad Ali Maddah-Ali, and A Salman Avestimehr. Straggler mitigation in distributed matrix multiplication: Fundamental limits and optimal coding. IEEE Transactions on Information Theory, 66(3):1920\u20131933, 2020.   \n[12] Rafael G.L. DOliveira, Salim El Rouayheb, and David Karpuk. GASP codes for secure distributed matrix multiplication. IEEE Transactions on Information Theory, 66(7):4038\u20134050, 2020.   \n[13] Qian Yu, Mohammad Maddah-Ali, and Salman Avestimehr. Polynomial codes: an optimal design for high-dimensional coded matrix multiplication. Advances in Neural Information Processing Systems, 30, 2017.   \n[14] Irving S Reed and Gustave Solomon. Polynomial codes over certain finite fields. Journal of the society for industrial and applied mathematics, 8(2):300\u2013304, 1960.   \n[15] Venkatesan Guruswami, Atri Rudra, and Madhu Sudan. Essential Coding Theory. Draft is Available, 2022.   \n[16] K. Lee, M. Lam, R. Pedarsani, D. Papailiopoulos, and K. Ramchandran. Speeding up distributed machine learning using codes. IEEE Transactions on Information Theory, 64(3):1514\u2013 1529, 2018.   \n[17] K. Lee, C. Suh, and K. Ramchandran. High-dimensional coded matrix multiplication. In Proceedings of IEEE International Symposium on Information Theory (ISIT), pages 2418\u20132422, 2017. ", "page_idx": 10}, {"type": "text", "text": "[18] Walter Gautschi and Gabriele Inglese. Lower bounds for the condition number of vandermonde matrices. Numerische Mathematik, 52(3):241\u2013250, 1987. ", "page_idx": 11}, {"type": "text", "text": "[19] Gene H. Golub and Charles F. Van Loan. Matrix computations. JHU press, 2013.   \n[20] Jinhyun So, Basak Guler, and Salman Avestimehr. A scalable approach for privacy-preserving collaborative machine learning. Advances in Neural Information Processing Systems, 33:8054\u2013 8066, 2020.   \n[21] Adarsh M. Subramaniam, Anoosheh Heidarzadeh, and Krishna R. Narayanan. Random Khatri-Rao-product codes for numerically-stable distributed matrix multiplication. CoRR, abs/1907.05965, 2019.   \n[22] Aditya Ramamoorthy and Li Tang. Numerically stable coded matrix computations via circulant and rotation matrix embeddings. IEEE Transactions on Information Theory, 68(4):2684\u20132703, 2022.   \n[23] Anindya Bijoy Das, Aditya Ramamoorthy, and Namrata Vaswani. Efficient and robust distributed matrix computations via convolutional coding. IEEE Transactions on Information Theory, 67(9):6266\u20136282, 2021.   \n[24] Mahdi Soleymani, Hessam Mahdavifar, and Amir Salman Avestimehr. Analog Lagrange coded computing. IEEE Journal on Selected Areas in Information Theory, 2(1):283\u2013295, 2021.   \n[25] Mohammad Fahim and Viveck R Cadambe. Numerically stable polynomially coded computing. IEEE Transactions on Information Theory, 67(5):2758\u20132785, 2021.   \n[26] Tayyebeh Jahani-Nezhad and Mohammad Ali Maddah-Ali. CodedSketch: A coding scheme for distributed computation of approximated matrix multiplication. IEEE Transactions on Information Theory, 67(6):4185\u20134196, 2021.   \n[27] Vipul Gupta, Shusen Wang, Thomas Courtade, and Kannan Ramchandran. OverSketch: Approximate matrix multiplication for the cloud. In 2018 IEEE International Conference on Big Data (Big Data), pages 298\u2013304, 2018.   \n[28] Vipul Gupta, Swanand Kadhe, Thomas Courtade, Michael W. Mahoney, and Kannan Ramchandran. Oversketched Newton: Fast convex optimization for serverless systems. In 2020 IEEE International Conference on Big Data (Big Data), pages 288\u2013297, 2020.   \n[29] Tayyebeh Jahani-Nezhad and Mohammad Ali Maddah-Ali. Berrut approximated coded computing: Straggler resistance beyond polynomial computing. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(1):111\u2013122, 2022.   \n[30] George Kimeldorf and Grace Wahba. Some results on tchebycheffian spline functions. Journal of mathematical analysis and applications, 33(1):82\u201395, 1971.   \n[31] Jean Duchon. Splines minimizing rotation-invariant semi-norms in sobolev spaces. In Constructive Theory of Functions of Several Variables: Proceedings of a Conference Held at Oberwolfach April 25\u2013May 1, 1976, pages 85\u2013100. Springer, 1977.   \n[32] Grace Wahba. Spline models for observational data. SIAM, 1990.   \n[33] Grace Wahba. Smoothing noisy data with spline functions. Numerische mathematik, 24(5): 383\u2013393, 1975.   \n[34] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning. MIT press, 2018.   \n[35] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019.   \n[36] Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998. ", "page_idx": 11}, {"type": "text", "text": "[37] Yann LeCun, Corinna Cortes, Chris Burges, et al. MNIST handwritten digit database, 2010. ", "page_idx": 12}, {"type": "text", "text": "[38] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \n[39] Xiaohan Ding, Xiangyu Zhang, Ningning Ma, Jungong Han, Guiguang Ding, and Jian Sun. Repvgg: Making vgg-style convnets great again. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 13733\u201313742, 2021.   \n[40] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.   \n[41] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A largescale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009.   \n[42] Mahdi Soleymani, Ramy E Ali, Hessam Mahdavifar, and A Salman Avestimehr. ApproxIFER: A model-agnostic approach to resilient and robust prediction serving systems. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 8342\u20138350, 2022.   \n[43] Paul HC Eilers and Brian D Marx. Flexible smoothing with b-splines and penalties. Statistical science, 11(2):89\u2013121, 1996.   \n[44] Carl De Boor. Calculation of the smoothing spline with weighted roughness measure. Mathematical Models and Methods in Applied Sciences, 11(01):33\u201341, 2001.   \n[45] Chiu Li Hu and Larry L Schumaker. Complete spline smoothing. Numerische Mathematik, 49: 1\u201310, 1986.   \n[46] Matei Zaharia, Andy Konwinski, Anthony D Joseph, Randy H Katz, and Ion Stoica. Improving mapreduce performance in heterogeneous environments. In Osdi, volume 8, page 7, 2008.   \n[47] Lalith Suresh, Marco Canini, Stefan Schmid, and Anja Feldmann. C3: Cutting tail latency in cloud data stores via adaptive replica selection. In 12th USENIX Symposium on Networked Systems Design and Implementation (NSDI 15), pages 513\u2013527, 2015.   \n[48] Nihar B Shah, Kangwook Lee, and Kannan Ramchandran. When do redundant requests reduce latency? IEEE Transactions on Communications, 64(2):715\u2013722, 2015.   \n[49] Kristen Gardner, Samuel Zbarsky, Sherwin Doroudi, Mor Harchol-Balter, and Esa Hyytia. Reducing latency via redundant requests: Exact analysis. ACM SIGMETRICS Performance Evaluation Review, 43(1):347\u2013360, 2015.   \n[50] Manmohan Chaubey and Erik Saule. Replicated data placement for uncertain scheduling. In 2015 IEEE International Parallel and Distributed Processing Symposium Workshop, pages 464\u2013472. IEEE, 2015.   \n[51] Songze Li, Seyed Mohammadreza Mousavi Kalan, Amir Salman Avestimehr, and Mahdi Soltanolkotabi. Near-optimal straggler mitigation for distributed gradient methods. In 2018 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW), pages 857\u2013866. IEEE, 2018.   \n[52] Anindya B Das, Aditya Ramamoorthy, and Namrata Vaswani. Random convolutional coding for robust and straggler resilient distributed matrix computation. arXiv preprint arXiv:1907.08064, 2019.   \n[53] Aditya Ramamoorthy, Anindya Bijoy Das, and Li Tang. Straggler-resistant distributed matrix computation via coding theory: Removing a bottleneck in large-scale data processing. IEEE Signal Processing Magazine, 37(3):136\u2013145, 2020.   \n[54] Can Karakus, Yifan Sun, Suhas Diggavi, and Wotao Yin. Straggler mitigation in distributed optimization through data encoding. Advances in Neural Information Processing Systems, 30, 2017.   \n[55] R. Tandon, Q. Lei, A. G. Dimakis, and N. Karampatziakis. Gradient coding: Avoiding stragglers in distributed learning. In Proceedings of the 34th International Conference on Machine Learning (ICML), pages 3368\u20133376, Sydney, Australia, August 2017.   \n[56] Aditya Ramamoorthy and Li Tang. Numerically stable coded matrix computations via circulant and rotation matrix embeddings. IEEE Transactions on Information Theory, 68(4):2684\u20132703, 2021.   \n[57] Qian Yu, Mohammad Ali Maddah-Ali, and Amir Salman Avestimehr. Straggler mitigation in distributed matrix multiplication: Fundamental limits and optimal coding. IEEE Transactions on Information Theory, 66(3):1920\u20131933, 2020.   \n[58] Mohammad Fahim and Viveck R. Cadambe. Numerically stable polynomially coded computing. IEEE Transactions on Information Theory, 67(5):2758\u20132785, 2021.   \n[59] Jack Kosaian, KV Rashmi, and Shivaram Venkataraman. Parity models: A general framework for coding-based resilience in ml inference. arXiv preprint arXiv:1905.00863, 2019.   \n[60] J-P Berrut. Rational functions for guaranteed and experimentally well-conditioned global interpolation. Computers & Mathematics with Applications, 15(1):1\u201316, 1988.   \n[61] Giovanni Leoni. A first course in Sobolev spaces, volume 181. American Mathematical Society, 2024.   \n[62] Robert A Adams and John JF Fournier. Sobolev spaces. Elsevier, 2003.   \n[63] Alain Berlinet and Christine Thomas-Agnan. Reproducing kernel Hilbert spaces in probability and statistics. Springer Science & Business Media, 2011.   \n[64] David L Ragozin. Error bounds for derivative estimates based on spline smoothing of exact or noisy data. Journal of approximation theory, 37(4):335\u2013355, 1983.   \n[65] Florencio I Utreras. Convergence rates for multivariate smoothing spline functions. Journal of approximation theory, 52(1):1\u201327, 1988.   \n[66] Heinrich W Guggenheimer, Alan S Edelman, and Charles R Johnson. A simple estimate of the condition number of a linear system. The College Mathematics Journal, 26(1):2\u20135, 1995. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Preliminaries ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Sobolev spaces and Sobolev norms ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Let $\\Omega$ be an open interval in $\\mathbb{R}$ and $M$ be a positive integer. We denote by $L^{p}\\left(\\Omega;\\mathbb{R}^{M}\\right)$ the class of all measurable functions $g:\\mathbb{R}\\to\\mathbb{R}^{M}$ that satisfy: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\int_{\\Omega}|g_{j}(t)|^{p}\\,d t\\ <\\infty,\\quad\\forall j\\in[M],\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $g(\\cdot)=[g_{1}(\\cdot),\\cdot\\cdot\\cdot,g_{M}(\\cdot)]^{T}$ . The space $L^{p}\\left(\\Omega;\\mathbb{R}^{M}\\right)$ can be endowed with the following norm, known as the $L^{p}$ norm: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\|g\\|_{L^{p}(\\Omega;\\mathbb{R}^{M})}:=\\left(\\sum_{j=1}^{M}\\int_{\\Omega}|g_{i}(t)|^{p}\\,d t\\right)^{\\frac{1}{p}},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "for $1\\leqslant p<\\infty$ , and ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\|g\\|_{L^{\\infty}(\\Omega;\\mathbb{R}^{M})}:=\\operatorname*{max}_{j\\in[M]}\\operatorname*{sup}_{t\\in\\Omega}|g_{j}(t)|.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "for $p=\\infty$ . Additionally, a function $g:\\Omega\\to\\mathbb{R}^{M}$ is in $L_{\\mathrm{loc}}^{p}\\left(\\Omega;\\mathbb{R}^{M}\\right)$ if it lies in $L^{p}(V;\\mathbb{R}^{M})$ for all compact subsets $V\\subseteq\\Omega$ . ", "page_idx": 14}, {"type": "text", "text": "Definition 1 (Sobolev Space). The Sobolev space $\\mathbb{W}^{m,p}\\left(\\Omega;\\mathbb{R}^{M}\\right)$ is the space of all functions $g\\,\\in\\,L^{p}\\left(\\Omega;\\mathbb{R}^{M}\\right)$ such that all weak derivatives of order $i$ , denoted by $g^{(i)}$ , belong to $L^{p}\\left(\\Omega;\\mathbb{R}^{M}\\right)$ for $i\\in[m]$ . This space is endowed with the norm: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left\\|g\\right\\|_{\\mathbb{W}^{m,p}(\\Omega;\\mathbb{R}^{M})}:=\\left(\\left\\|g\\right\\|_{L^{p}(\\Omega;\\mathbb{R}^{M})}^{p}+\\sum_{i=1}^{m}\\left\\|g^{(i)}\\right\\|_{L^{p}(\\Omega;\\mathbb{R}^{M})}^{p}\\right)^{\\frac{1}{p}},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "for $1\\leqslant p<\\infty$ , and ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\|g\\|_{\\mathbb{W}^{m,\\infty}(\\Omega;\\mathbb{R}^{M})}:=\\operatorname*{max}\\left\\{\\|g\\|_{L^{\\infty}(\\Omega;\\mathbb{R}^{M})}\\,,\\operatorname*{max}_{i\\in[m]}\\left\\|g^{(i)}\\right\\|_{L^{\\infty}(\\Omega;\\mathbb{R}^{M})}\\right\\},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "for $p=\\infty$ . ", "page_idx": 14}, {"type": "text", "text": "Similarly, $\\mathbb{W}_{\\mathrm{loc}}^{m,p}\\left(\\Omega;\\mathbb{R}^{M}\\right)$ is defined as the space of all functions $g\\,\\in\\,L_{\\mathrm{loc}}^{p}\\left(\\Omega;\\mathbb{R}^{M}\\right)$ with all weak derivatives of order $i$ belonging to $L_{\\mathrm{loc}}^{p}\\left(\\Omega;\\mathbb{R}^{M}\\right)$ for $i\\in[m]$ . $\\|g\\|_{\\mathbb{W}_{\\mathrm{loc}}^{m,p}(\\Omega;\\mathbb{R}^{M})}$ and $\\|g\\|_{\\mathbb{W}_{\\mathrm{loc}}^{m,\\infty}(\\Omega;\\mathbb{R}^{M})}$ are defined similar to (16) and (19) respectively, using $L_{\\mathrm{loc}}^{p}\\left(\\Omega;\\mathbb{R}\\right)$ instead of $L^{p}\\left(\\Omega;\\mathbb{R}\\right)$ . ", "page_idx": 14}, {"type": "text", "text": "Definition 2. (Sobolev Space with compact support): Denoted by $\\mathbb{W}_{0}^{m,p}\\left(\\Omega;\\mathbb{R}^{M}\\right)$ collection of functions $g$ defined on interval $\\Omega=(a,b)$ such that $g(a)=\\mathbf{0},g^{\\prime}(a)=\\mathbf{0},\\ldots,g^{(m-1)}(a)=\\mathbf{0}$ and $\\left\\|g^{(m)}\\right\\|_{L^{2}(\\Omega;\\mathbb{R})}<\\infty$ . This space can be endowed with the following norm: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\Vert g\\Vert_{\\mathbb{W}^{m,p}(\\Omega;\\mathbb{R}^{M})}:=\\left\\Vert g^{(m)}\\right\\Vert_{L^{p}(\\Omega;\\mathbb{R}^{M})},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "for $1\\leqslant p<\\infty$ , and ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\lVert g\\rVert_{\\mathbb{W}^{m,\\infty}(\\Omega;\\mathbb{R}^{M})}:=\\left\\lVert g^{(m)}\\right\\rVert_{L^{\\infty}(\\Omega;\\mathbb{R}^{M})},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "for $p=\\infty$ . ", "page_idx": 14}, {"type": "text", "text": "The next theorem provides an upper bound for $L^{p}$ norm of functions in the Sobolev space, which plays a crucial role in the proof of the main theorems of the paper. ", "page_idx": 14}, {"type": "text", "text": "Theorem 5 (Theorem 7.34, [61]). Let $\\Omega\\ \\subseteq\\ \\mathbb{R}$ be an open interval and let $g\\,\\,\\in\\,\\,\\mathbb{W}_{l o c}^{1,1}\\left(\\Omega;\\mathbb{R}^{M}\\right)$ . Assume $1\\leqslant p,q,r\\leqslant\\infty$ and $r\\geqslant q$ . Then: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\|g\\|_{L^{r}(\\Omega;\\mathbb{R}^{M})}\\leqslant\\ell^{\\frac{1}{r}-\\frac{1}{q}}\\,\\|g\\|_{L^{q}(\\Omega;\\mathbb{R}^{M})}+\\ell^{1-\\frac{1}{p}+\\frac{1}{r}}\\,\\|g^{\\prime}\\|_{L^{p}(\\Omega;\\mathbb{R}^{M})}\\,,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "for every $0<\\ell<\\mathcal{L}^{1}(\\Omega)$ . ", "page_idx": 14}, {"type": "text", "text": "Note that $\\mathcal{L}^{1}(\\Omega)$ in Theorem 5 is the length of the interval $\\Omega$ . ", "page_idx": 15}, {"type": "text", "text": "Corollary 1. Suppose g \u2208Wl1o,c1 \u2126; RM and \u2126\u2286R be an open interval. If\u2225\u2225gg\u2032\u2225\u2225LL22((\u2126\u2126;;RRMM)) $\\begin{array}{r}{\\!\\!\\!\\!\\!\\!f\\frac{\\|g\\|_{L^{2}\\left(\\Omega;\\mathbb{R}^{M}\\right)}}{\\|g^{\\prime}\\|_{L^{2}\\left(\\Omega;\\mathbb{R}^{M}\\right)}}<\\mathcal{L}^{1}(\\Omega),}\\end{array}$ then: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\|g\\|_{L^{\\infty}(\\Omega;\\mathbb{R}^{M})}\\leqslant2\\sqrt{\\|g\\|_{L^{2}(\\Omega;\\mathbb{R}^{M})}\\cdot\\|g^{\\prime}\\|_{L^{2}(\\Omega;\\mathbb{R}^{M})}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. Substituting $p,q\\,=\\,2$ and $r\\,=\\,\\infty$ in Theorem 5 and optimizing over $\\ell$ , one can derive the optimum value of $\\ell$ , denoted by $\\ell^{*}$ as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\ell^{*}=\\frac{\\|g\\|_{L^{2}(\\Omega;\\mathbb{R}^{M})}}{\\|g^{\\prime}\\|_{L^{2}(\\Omega;\\mathbb{R}^{M})}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since $\\frac{\\|g\\|_{L^{2}\\left(\\Omega;\\mathbb{R}^{M}\\right)}}{\\|g^{\\prime}\\|_{L^{2}\\left(\\Omega;\\mathbb{R}^{M}\\right)}}\\,<\\,\\mathcal{L}^{1}(\\Omega)$ , the optimum value is in the valid interval mentioned in Theorem 5. ", "page_idx": 15}, {"type": "text", "text": "Corollary 2 (Corollary 7.36, [61]). Let $\\Omega\\,=\\,(a,b)$ , let $1\\leq p,q,r\\leq\\infty$ be such that $1+1/r\\ge$ $1/p$ and $r\\,\\geq\\,q$ and let $g\\,\\in\\,\\mathbb{W}_{l o c}^{1,1}\\left(\\Omega;\\mathbb{R}^{M}\\right)$ with $g^{\\prime}\\,\\in\\,L^{p}\\left(\\Omega;\\mathbb{R}^{M}\\right)$ . Let $x_{0}\\,\\in\\,[a,b]$ be such that $\\left|g\\left(x_{0}\\right)\\right|=\\operatorname*{min}_{\\left[a,b\\right]}\\left|g\\right|$ . Then ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left\\|g-g\\left(x_{0}\\right)\\right\\|_{L^{r}\\left(\\Omega;\\mathbb{R}^{M}\\right)}\\leq8\\|g\\|_{L^{q}\\left(\\Omega;\\mathbb{R}^{M}\\right)}^{\\alpha}\\left\\|g^{\\prime}\\right\\|_{L^{p}\\left(\\Omega;\\mathbb{R}^{M}\\right)}^{1-\\alpha}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\alpha:=0\\;i f\\,r=q$ and $1-1/p+1/r=0$ and otherwise ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\alpha:=\\frac{1-1/p+1/r}{1-1/p+1/q}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Corollary 3. Theorem 5 and Corollary 2 hold true when $f\\in\\mathbb{W}^{2,2}\\left(\\Omega;\\mathbb{R}^{M}\\right)$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. We prove the above corollary by showing that $\\begin{array}{r l r}{\\mathbb{W}^{2,2}\\left(\\Omega;\\mathbb{R}^{M}\\right)}&{\\subseteq}&{\\mathbb{W}^{1,1}\\left(\\Omega;\\mathbb{R}^{M}\\right)\\;\\;\\subseteq}\\end{array}$ $\\mathbb{W}_{\\mathrm{loc}}^{1,1}\\left(\\Omega;\\mathbb{R}^{M}\\right)$ . Using Cauchy-Schwarz inequality, one can show: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\Vert f\\Vert_{L^{1}(\\Omega;\\mathbb{R}^{M})}=\\sum_{j=1}^{M}\\int_{\\Omega}|f_{j}(t)|\\,d t\\leqslant\\sum_{j=1}^{M}\\left(\\int_{\\Omega}1^{2}\\,d t\\cdot\\int_{\\Omega}|f_{j}(t)|^{2}\\,d t\\right)^{\\frac{1}{2}}}}\\\\ &{}&{\\leqslant\\left(\\mathcal{L}^{1}(\\Omega)\\right)^{\\frac{1}{2}}\\cdot\\displaystyle\\sum_{j=1}^{M}\\left(\\int_{\\Omega}|f_{j}(t)|^{2}\\,d t\\right)^{\\frac{1}{2}}<\\infty,}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where (a) follows from the bounded length of $\\Omega$ and $f\\in\\mathbb{W}^{2,2}\\left(\\Omega;\\mathbb{R}\\right)$ . Similarly: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\Vert f^{\\prime}\\Vert_{L^{1}(\\Omega;\\mathbb{R}^{M})}=\\sum_{j=1}^{M}\\int_{\\Omega}|f_{j}^{\\prime}(t)|\\,d t\\leqslant\\sum_{j=1}^{M}\\left(\\displaystyle\\int_{\\Omega}1^{2}\\,d t\\cdot\\int_{\\Omega}|f_{j}^{\\prime}(t)|^{2}\\,d t\\right)^{\\frac12}}}\\\\ &{}&{\\leqslant\\left(\\mathcal{L}^{1}(\\Omega)\\right)^{\\frac12}\\cdot\\displaystyle\\sum_{j=1}^{M}\\left(\\displaystyle\\int_{\\Omega}|f_{j}^{\\prime}(t)|^{2}\\,d t\\right)^{\\frac12}<\\infty.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Equations (24) and (25) prove that $\\mathbb{W}^{2,2}\\left(\\Omega;\\mathbb{R}^{M}\\right)\\ \\subseteq\\ \\mathbb{W}^{1,1}\\left(\\Omega;\\mathbb{R}^{M}\\right)$ . Now, suppose $f\\ \\ \\in$ W1,1 $\\left(\\Omega;\\mathbb{R}^{M}\\right)$ . For every closed subset $[c,d]\\subset\\Omega$ we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{j=1}^{M}\\left(\\int_{c}^{d}|f_{j}(t)|\\,d t\\right)\\overset{\\mathrm{(a)}}{\\leqslant}\\sum_{j=1}^{M}\\left(\\int_{\\Omega}|f_{j}(t)|\\,d t\\right)\\overset{\\mathrm{(b)}}{\\leqslant}\\infty,}\\\\ {\\displaystyle\\sum_{j=1}^{M}\\left(\\int_{c}^{d}|f_{j}^{\\prime}(t)|\\,d t\\right)\\overset{\\mathrm{(a)}}{\\leqslant}\\sum_{j=1}^{M}\\left(\\int_{\\Omega}|f_{j}^{\\prime}(t)|\\,d t\\right)\\overset{\\mathrm{(b)}}{\\leqslant}\\infty,}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where (a) is due to $[c,d]\\ \\subset\\ \\Omega$ and (b) is because of $f~\\in~\\mathbb{W}^{1,1}\\left(\\Omega;\\mathbb{R}^{M}\\right)$ . Therefore, $f~\\in$ $\\mathbb{W}_{\\mathrm{loc}}^{1,1}\\left(\\Omega;\\mathbb{R}^{M}\\right)$ . \u53e3 ", "page_idx": 15}, {"type": "text", "text": "Equivalent norms. There have been various norms defined on Sobolev spaces in the literature that are equivalent to (16) (see [62], [63, Ch. 7], and [32, Sec. 10.2]). Note that two norms $\\lVert\\cdot\\rVert_{W_{1}}\\,,\\lVert\\cdot\\rVert_{W_{2}}$ are equivalent if there exist positive constants $\\eta_{1},\\eta_{2}$ such that $\\eta_{1}\\cdot\\|g\\|_{W_{2}}\\leqslant\\|g\\|_{W_{1}}\\leqslant\\eta_{2}\\big\\rangle_{\\!\\!\\{g\\|_{W_{2}}}\\!\\!\\big\\}$ . The equivalent norm in which we are interested is the one introduced in [30]. Let $\\Omega=(a,b)\\subset\\mathbb{R}$ . We define $\\widetilde{\\mathbb{W}}^{m,p}\\left(\\Omega;\\mathbb{R}^{M}\\right)$ as the Sobolev space endowed with the following norm: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\|g\\|_{\\widetilde{\\mathbb{W}}^{m,p}(\\Omega;\\mathbb{R}^{M})}:=\\left(\\sum_{j=1}^{M}\\left(g_{j}(a)^{p}+\\sum_{i=1}^{m-1}\\left(g_{j}^{(i)}(a)\\right)^{p}\\right)+\\left\\|g^{(m)}\\right\\|_{L^{p}(\\Omega;\\mathbb{R}^{M})}^{p}\\right)^{\\frac{1}{p}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The following lemma derives the equivalence constants $(\\eta_{1},\\eta_{2})$ for the norms $\\lVert\\cdot\\rVert_{\\mathbb{W}^{2,2}(\\Omega;\\mathbb{R})}$ and $\\|\\cdot\\|_{\\widetilde{\\mathbb{W}}^{2,2}(\\Omega;\\mathbb{R})}$ ", "page_idx": 16}, {"type": "text", "text": "Lemma 1. Let $\\Omega=(a,b)$ be an arbitrary open interval in $\\mathbb{R}$ . Then for every $g\\in\\mathbb{W}^{2,2}\\left(\\Omega;\\mathbb{R}\\right)$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\|g\\|_{\\mathbb{W}^{2,2}(\\Omega;\\mathbb{R})}^{2}\\leqslant\\left[2(b-a)\\operatorname*{max}\\{1,(b-a)\\}\\left(2\\operatorname*{max}\\{1,(b-a)\\}^{2}+1\\right)+1\\right]\\cdot\\|g\\|_{\\widetilde{\\mathbb{W}}^{2,2}(\\Omega;\\mathbb{R})}^{2}}\\\\ &{\\displaystyle\\|g\\|_{\\widetilde{\\mathbb{W}}^{2,2}(\\Omega;\\mathbb{R})}^{2}\\leqslant\\left(\\frac{4}{(b-a)}\\operatorname*{max}\\{1,(b-a)\\}^{2}+1\\right)\\cdot\\|g\\|_{\\mathbb{W}^{2,2}(\\Omega;\\mathbb{R})}^{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. By expanding $g$ around the point $a$ and utilizing the integral remainder form of Taylor\u2019s expansion, for every $x\\in(a,b)$ we have: ", "page_idx": 16}, {"type": "equation", "text": "$$\ng(x)=g(a)+\\int_{a}^{x}g^{\\prime}(t)\\,d t.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle g(x)^{2}=g(a)^{2}+\\left(\\int_{a}^{x}g^{\\prime}(t)\\,d t\\right)^{2}+2g(a)\\cdot\\int_{a}^{x}g^{\\prime}(t)\\,d t}\\\\ {\\displaystyle\\overset{\\mathrm{(a)}}{\\leqslant}2g(a)^{2}+2\\left(\\int_{a}^{x}g^{\\prime}(t)\\,d t\\right)^{2}}\\\\ {\\displaystyle\\overset{\\mathrm{(b)}}{\\leqslant}2g(a)^{2}+2(x-a)\\int_{a}^{x}g^{\\prime}(t)^{2}\\,d t}\\\\ {\\displaystyle\\overset{\\mathrm{(c)}}{\\leqslant}2g(a)^{2}+2(b-a)\\int_{a}^{b}g^{\\prime}(t)^{2}\\,d t,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where (a) follows from the $\\begin{array}{r}{2g(a)\\cdot\\int_{a}^{x}g^{\\prime}(t)\\,d t\\,\\leqslant\\,g(a)^{2}\\,+\\,\\left(\\int_{a}^{x}g^{\\prime}(t)\\,d t\\right)^{2}}\\end{array}$ , (b) is based on CauchySchwarz inequality, and (c) is due to $x\\leqslant b$ . Following the same steps as (32), we have: ", "page_idx": 16}, {"type": "equation", "text": "$$\ng^{\\prime}(x)^{2}\\leqslant2g^{\\prime}(a)^{2}+2(b-a)\\int_{a}^{b}g^{\\prime\\prime}(t)^{2}\\,d t.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Integrating both sides of (33) we have: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\int_{a}^{b}g^{\\prime}(y)^{2}\\,d y\\leqslant2\\displaystyle\\int_{a}^{b}g^{\\prime}(a)^{2}\\,d y+2(b-a)\\displaystyle\\int_{a}^{b}\\left(\\displaystyle\\int_{a}^{b}g^{\\prime\\prime}(t)^{2}\\,d t\\right)\\,d y}\\\\ {\\displaystyle=2(b-a).g^{\\prime}(a)^{2}+2(b-a)^{2}\\displaystyle\\int_{a}^{b}g^{\\prime\\prime}(t)^{2}\\,d t}\\\\ {\\displaystyle\\overset{(a)}{\\leqslant}2(b-a)\\cdot\\operatorname*{max}\\{1,(b-a)\\}\\cdot\\left(g(a)^{2}+g^{\\prime}(a)^{2}+\\displaystyle\\int_{a}^{b}g^{\\prime\\prime}(t)^{2}\\,d t\\right)}\\\\ {\\displaystyle=2(b-a)\\cdot\\operatorname*{max}\\{1,(b-a)\\}\\cdot\\|g\\|_{\\widetilde{\\mathbb{V}}^{2,2}(\\Omega;\\mathbb{R})}^{2}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where (a) is based on adding the positive term $(b-a)\\cdot g(a)^{2}$ to the right-hand side. Based on the (32) and (34) we have: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\int_{a}^{b}g(x)^{2}\\,d x\\stackrel{(0)}{\\leqslant}2\\int_{a}^{b}g(a)^{2}\\,d x+2(b-a)\\int_{a}^{b}\\left(\\int_{a}^{b}g^{\\prime}(t)^{2}\\,d t\\right)\\,d x}\\\\ {\\displaystyle\\qquad\\qquad=2(b-a).g(a)^{2}+2(b-a)^{2}\\int_{a}^{b}g^{\\prime}(t)^{2}\\,d t}\\\\ {\\displaystyle\\qquad\\stackrel{(b)}{\\leqslant}2(b-a)\\cdot g(a)^{2}+4(b-a)^{3}\\cdot g^{\\prime}(a)^{2}+4(b-a)^{4}\\int_{a}^{b}g^{\\prime\\prime}(t)^{2}\\,d t}\\\\ {\\displaystyle\\qquad\\leqslant4(b-a)\\cdot\\operatorname*{max}\\{1,(b-a)^{3}\\}\\cdot\\|g\\|_{\\widetilde{\\mathbb{W}}^{2,2}(\\Omega;\\mathbb{R})}^{2}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where (a) follows by integrating both sides of (32) and (b) is due to (33). Using (35) and (34) and the fact that $\\begin{array}{r}{\\int_{a}^{b}g^{\\prime\\prime}(t)^{2}\\,d t\\leqslant\\|g\\|_{\\mathbb{W}^{2,2}(\\Omega;\\mathbb{R})}^{2}.}\\end{array}$ , we have: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l l}&{\\displaystyle\\|g\\|_{\\mathbb{W}^{2,2}(\\Omega;\\mathbb{R})}^{2}=\\int_{a}^{b}g^{\\prime\\prime}(t)^{2}\\,d t+\\int_{a}^{b}g^{\\prime}(t)^{2}\\,d t+\\int_{a}^{b}g(t)^{2}\\,d t}\\\\ &{\\displaystyle\\leqslant\\left[2(b-a)\\operatorname*{max}\\{1,(b-a)\\}\\left(2\\operatorname*{max}\\{1,(b-a)\\}^{2}+1\\right)+1\\right]\\cdot\\|g\\|_{\\widetilde{\\mathbb{W}}^{2,2}(\\Omega;\\mathbb{R})}^{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "For the other side, using the same steps as in (32), we have: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{g(a)^{2}=g(x)^{2}+\\left(\\displaystyle\\int_{a}^{x}g^{\\prime}(t)\\,d t\\right)^{2}-2g(a).\\displaystyle\\int_{a}^{x}g^{\\prime}(t)\\,d t}\\\\ &{\\phantom{a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a}\\leqslant2g(x)^{2}+2g(a).}\\\\ &{\\phantom{a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a}\\leqslant2g(x)^{2}+1}\\\\ &{\\phantom{a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a}\\leqslant2g(x)^{2}+2(b-a)\\displaystyle\\int_{a}^{b}g^{\\prime}(t)^{2}\\,d t}\\\\ &{\\phantom{a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a}\\leqslant2g(a)^{2}+3(b-a)^{2}}\\\\ &{\\phantom{a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a}\\leqslant2g(a)^{2}}\\\\ &{\\phantom{a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a}\\!\\!\\!\\!\\leqslant2g(t)^{2}+2g(a)^{2}}\\\\ &{\\phantom{a a a a a a a a a a a a}\\leqslant2g(a)^{2}+2g(b-a)^{2}\\,d t f}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where (a) is because of $\\begin{array}{r}{-2g(a)\\cdot\\int_{a}^{x}g^{\\prime}(t)\\,d t\\,\\leqslant\\,g(a)^{2}\\,+\\,\\left(\\int_{a}^{x}g^{\\prime}(t)\\,d t\\right)^{2}}\\end{array}$ , (b) follows from CauchySchwarz inequality, and (c) is due to $x\\leqslant b$ . Integrating both sides of (37), we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{b-a)\\cdot g(a)^{2}=\\displaystyle\\int_{a}^{b}g(a)^{2}\\,d x\\leqslant2\\displaystyle\\int_{a}^{b}g(x)^{2}\\,d x+2(b-a)\\displaystyle\\int_{a}^{b}\\left(\\displaystyle\\int_{a}^{b}g^{\\prime}(t)^{2}\\,d t\\right)\\,d x}&{}&\\\\ {\\overset{\\mathrm{(a)}}{=}2\\displaystyle\\int_{a}^{b}g(t)^{2}\\,d t+2(b-a)^{2}\\displaystyle\\int_{a}^{b}g^{\\prime}(t)^{2}\\,d t}&{}&\\\\ {\\overset{\\mathrm{(b)}}{\\leqslant}2\\operatorname*{max}\\{1,(b-a)\\}^{2}\\left(\\displaystyle\\int_{a}^{b}g(t)^{2}\\,d t+\\displaystyle\\int_{a}^{b}g^{\\prime}(t)^{2}\\,d t+\\displaystyle\\int_{a}^{b}g^{\\prime\\prime}(t)^{2}\\,d t\\right)}&{}&\\\\ {\\leqslant2\\operatorname*{max}\\{1,(b-a)\\}^{2}\\|g\\|_{\\mathbb{W}^{2,2}(\\Omega;\\mathbb{R})}^{2}\\,,}&{}&{\\quad{\\mathrm{(38)}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where (a) follows by a change of variable from $x$ to $t$ and (b) follows by adding the positive term $\\begin{array}{r}{2\\operatorname*{max}\\{1,(b-a)\\}^{2}\\cdot\\int_{a}^{b}g^{\\prime\\prime}(t)^{2}\\,d t}\\end{array}$ to the right side. Thus, (38) directly results in ", "page_idx": 17}, {"type": "equation", "text": "$$\ng(a)^{2}\\leqslant\\frac{2}{(b-a)}\\operatorname*{max}\\{1,(b-a)\\}^{2}\\left\\|g\\right\\|_{\\mathbb{W}^{2,2}(\\Omega;\\mathbb{R})}^{2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Following same steps as (38) and (39) results in ", "page_idx": 17}, {"type": "equation", "text": "$$\ng^{\\prime}(a)^{2}\\leqslant\\frac{2}{(b-a)}\\operatorname*{max}\\{1,(b-a)\\}^{2}\\left\\|g\\right\\|_{\\mathbb{W}^{2,2}(\\Omega;\\mathbb{R})}^{2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Considering the fact that $\\begin{array}{r}{\\int_{a}^{b}g^{\\prime\\prime}(t)^{2}\\,d t\\leqslant\\|g\\|_{\\mathbb{W}^{2,2}(\\Omega;\\mathbb{R})}^{2}}\\end{array}$ , we can complete the proof: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\|g\\|_{\\widetilde{\\mathbb{W}}^{2,2}(\\Omega;\\mathbb{R})}^{2}=g(a)^{2}+g^{\\prime}(a)^{2}+\\displaystyle\\int_{a}^{b}g^{\\prime\\prime}(t)^{2}\\,d t}&{}\\\\ {\\displaystyle\\leqslant\\left(\\displaystyle\\frac{4}{(b-a)}\\operatorname*{max}\\{1,(b-a)\\}^{2}+1\\right)\\cdot\\|g\\|_{\\mathbb{W}^{2,2}(\\Omega;\\mathbb{R})}^{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Corollary 4. Based on Lemma $^{\\,l}$ , for $\\Omega=(-1,1)$ we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{1}{9}\\cdot\\|g\\|_{\\widetilde{\\mathbb{W}}^{2,2}(\\Omega;\\mathbb{R})}^{2}\\leqslant\\|g\\|_{\\mathbb{W}^{2,2}(\\Omega;\\mathbb{R})}^{2}\\leqslant73\\cdot\\|g\\|_{\\widetilde{\\mathbb{W}}^{2,2}(\\Omega;\\mathbb{R})}^{2}\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Corollary 4 directly follows from Lemma 1 by substituting $(a,b)=(-1,1)$ . ", "page_idx": 18}, {"type": "text", "text": "Corollary 5. The result of Lemma $^{\\,l}$ remains valid for multi-dimensional cases, where $g:\\mathbb{R}\\to\\mathbb{R}^{M}$ , for some $M>1$ . ", "page_idx": 18}, {"type": "text", "text": "Corollary 5 directly follows from applying Lemma 1 to each component of the function $g(\\cdot)$ and using the definition of vector-valued function norm: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|g\\|_{\\mathbb{W}^{2,2}(\\Omega;\\mathbb{R}^{M})}^{2}=\\sum_{j=1}^{M}\\left\\|g^{(j)}\\right\\|_{\\mathbb{W}^{2,2}(\\Omega;\\mathbb{R})}^{2},\\quad\\|g\\|_{\\widetilde{\\mathbb{W}}^{2,2}(\\Omega;\\mathbb{R}^{M})}^{2}=\\sum_{j=1}^{M}\\left\\|g^{(j)}\\right\\|_{\\widetilde{\\mathbb{W}}^{2,2}(\\Omega;\\mathbb{R})}^{2}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proposition 2. ([61, Section 7.2], [63, Theorem 121],[32]) For any open interval $\\Omega\\ \\subseteq\\ \\mathbb{R}$ and $m,M\\in\\mathbb{N},$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{H}^{m}\\left(\\Omega;\\mathbb{R}^{M}\\right):=\\mathbb{W}^{m,2}\\left(\\Omega;\\mathbb{R}^{M}\\right),}\\\\ {\\widetilde{\\mathcal{H}}^{m}\\left(\\Omega;\\mathbb{R}^{M}\\right):=\\widetilde{\\mathbb{W}}^{m,2}\\left(\\Omega;\\mathbb{R}^{M}\\right),}\\\\ {\\mathcal{H}_{0}^{m}\\left(\\Omega;\\mathbb{R}^{M}\\right):=\\mathbb{W}_{0}^{m,2}\\left(\\Omega;\\mathbb{R}^{M}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "are Reproducing Kernel Hilbert Spaces (RKHSs). ", "page_idx": 18}, {"type": "text", "text": "The full expression of the kernel function of $\\mathcal{H}^{m}\\left(\\Omega;\\mathbb{R}\\right)$ and $\\widetilde{\\mathcal{H}}^{m}\\left(\\Omega;\\mathbb{R}\\right)$ and other equivalent norms of Sobolev spaces can be found in [63, Section 4]. For $\\widetilde{\\mathcal{H}}^{m}\\left(\\Omega;\\mathbb{R}\\right)$ the kernel function is as follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\nR(t,s)=\\sum_{j=0}^{m-1}{\\frac{t^{j}s^{j}}{j!^{2}}}+\\int_{\\Omega}{\\frac{(t-x)_{+}^{m-1}(s-x)_{+}^{m-1}}{(m-1)!^{2}}}\\,d x,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $(\\cdot)_{+}$ is positive part function. ", "page_idx": 18}, {"type": "text", "text": "A.2 Smoothing Splines ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Consider the data model $y_{i}=f(t_{i})+\\epsilon_{i}$ for $i=1,\\dots,n$ , where $t_{i}\\in\\Omega=(a,b)\\subset\\mathbb{R},\\mathbb{E}[\\epsilon_{i}]=0$ , and $\\mathbb{E}[\\epsilon_{i}^{2}]\\leqslant\\sigma_{0}^{2}$ . Assuming $f\\in\\widetilde{\\mathbb{W}}^{m,2}\\left(\\Omega;\\mathbb{R}\\right)$ , the solution to the following optimization problem is referred to as the smoothing splin e: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbf{S}_{\\lambda,n,m}[\\mathbf{y}]:=\\operatorname*{argmin}_{f\\in\\mathbb{N}^{m,2}(\\Omega;\\mathbb{R})}\\frac{1}{n}\\sum_{i=1}^{n}\\left(f\\left(t_{i}\\right)-y_{i}\\right)^{2}+\\lambda\\int_{\\Omega}\\left(f^{(m)}(t)\\right)^{2}\\,d t,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\textbf{y}=~\\left[y_{1},\\ldots,y_{n}\\right]$ . Based on Proposition 2, $\\widetilde{\\mathcal{H}}^{m}\\left(\\Omega;\\mathbb{R}\\right)\\;:=\\;\\widetilde{\\mathbb{W}}^{m,2}\\left(\\Omega;\\mathbb{R}\\right)$ with the norm $\\|\\cdot\\|_{\\widetilde{\\mathbb{W}}^{m,2}(\\Omega;\\mathbb{R})}$ is a RKHS for some kernel function $\\phi(\\cdot,\\cdot)$ . Therefor e  for any $v\\,\\,\\in\\,\\,{\\widetilde{\\mathbb{W}}}^{m,2}\\left(\\Omega;\\mathbb{R}\\right)$ , we  have: ", "page_idx": 18}, {"type": "equation", "text": "$$\nv(t)=\\langle v(\\cdot),\\phi(\\cdot,t)\\rangle_{\\mathscr{\\widetilde{H}}^{m}(\\Omega;\\mathbb{R})}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "It can be shown that $\\phi(t,s)=R^{P}(t,s)+R^{0}(t,s)$ where $R^{0}(t,s)$ is kernel function of $\\mathcal{H}_{0}^{m}\\left(\\Omega;\\mathbb{R}\\right)$ and $R^{P}(t,s)$ is a null space of $\\mathcal{H}_{0}^{m}\\left(\\Omega;\\mathbb{R}\\right)$ which is the space of all polynomials with degree less than $m$ . ", "page_idx": 18}, {"type": "text", "text": "The solution of (45) has the following form [31, 32]: ", "page_idx": 18}, {"type": "equation", "text": "$$\nu^{*}(\\cdot)=\\sum_{i=1}^{m}d_{i}\\zeta_{i}(\\cdot)+\\sum_{j=1}^{n}c_{j}\\nu_{j}(\\cdot),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\nu_{j}(\\cdot)=R^{0}(\\cdot,t_{j})$ for $j\\in[n]$ , and $\\{\\zeta_{i}(\\cdot)\\}_{i=1}^{m}$ are the basis functions of the space of polynomials of degree at most $m-1$ . Substituting $\\boldsymbol{u}^{*}$ into (45) and optimizing over $\\mathbf{c}=[c_{1},\\hdots,c_{n}]^{T}$ and $\\mathbf{d}=[d_{1},\\overline{{\\,\\dots\\,}},d_{m}]^{T}$ , we obtain the following result: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbf{S}_{\\lambda,n,m}[\\mathbf{y}](\\mathbf{y})=\\mathbf{Q}\\left(\\mathbf{Q}^{\\mathbf{T}}\\mathbf{Q}+\\lambda\\mathbf{I}\\right)^{-1}\\mathbf{Q}^{\\mathbf{T}}\\mathbf{y},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\quad{\\bf Q}_{n\\times(n+m)}=[\\begin{array}{l l}{{\\bf T}_{n\\times m}}&{{\\bf\\Sigma}_{n\\times n}}\\end{array}],}\\\\ &{\\quad\\quad\\quad\\bf{\\Sigma}_{(n+m)\\times(n+m)}=\\left[\\begin{array}{l l}{{\\bf0}_{m\\times m}}&{{\\bf0}_{m\\times n}}\\\\ {{\\bf0}_{n\\times1}}&{{\\bf\\Sigma}_{n\\times n}}\\end{array}\\right],}\\\\ &{\\quad\\quad\\quad\\quad\\quad{\\bf T}_{i j}=\\nu_{i}(t_{j}),}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\Sigma_{i j}=R^{0}(t_{i},t_{j}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Equation (48) states that the smoothing spline fitted on the data points $\\mathbf{y}$ is a linear operator: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbf{S}_{\\lambda,n,m}[\\mathbf{y}](\\mathbf{z}):=\\mathbf{A}_{\\lambda}\\mathbf{z},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "for $\\mathbf{z}\\in\\mathbb{R}^{n}$ , where $\\mathbf{A}_{\\lambda}:=\\mathbf{Q}\\left(\\mathbf{Q}^{\\mathbf{T}}\\mathbf{Q}+\\lambda\\mathbf{T}\\right)^{-1}\\mathbf{Q}^{\\mathbf{T}}$ . It can be shown that the $\\boldsymbol{u}^{*}(\\cdot)$ is a natural spline [32, 33]. Thus, if $\\{b_{i}(\\cdot)\\}_{i=1}^{n}$ is a basis function for an $m$ -th order natural spline (such as truncated power or B-spline basis functions), we have: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{u^{*}(t)=\\sum_{i=1}^{n}\\xi_{i}b_{i}(t),}}\\\\ &{}&{\\xi=\\left(\\mathbf{N}^{T}\\mathbf{N}+\\lambda\\Phi\\right)^{-1}\\mathbf{N}^{T}\\mathbf{y},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\begin{array}{r}{\\mathbf{N}_{i j}=b_{i}(t_{j}),\\Phi_{i j}=\\int_{\\Omega}b_{i}^{\\prime\\prime}(t)b_{j}^{\\prime\\prime}(t)\\,d t}\\end{array}$ for $i,j\\in[n]$ , and $\\pmb{\\xi}:=[\\xi_{1},\\dots,\\xi_{n}]^{T}$ . ", "page_idx": 19}, {"type": "text", "text": "To characterize the estimation error of the smoothing spline, $|f-\\mathbf{S}_{\\lambda,n,m}(\\mathbf{y})|$ , we need to define two variables analogous to those in (6), which quantify the minimum and maximum consecutive distance of the regression points {ti}in=1: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Delta_{\\operatorname*{max}}:=\\operatorname*{max}_{i\\in\\{0\\}\\cup[n]}\\left\\{t_{i+1}-t_{i}\\right\\},\\quad\\Delta_{\\operatorname*{min}}:=\\operatorname*{min}_{i\\in[n-1]}\\left\\{t_{i+1}-t_{i}\\right\\},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where boundary points are defined as $(t_{0},t_{n+1}):=(a,b)$ . The following theorem offers an upper bound for the $j$ -th derivative of the smoothing spline estimator error function in the absence of noise $\\left.\\sigma_{0}\\right.=0$ ). ", "page_idx": 19}, {"type": "text", "text": "Theorem 6. ([64, Theorem 4.10]) Consider data model $y_{i}=f(t_{i})$ with $\\{t_{i}\\}_{i=1}^{n}$ belong to $\\Omega=[a,b]$ for $i\\in[n]$ . Let ", "page_idx": 19}, {"type": "equation", "text": "$$\nL=p_{2(m-1)}(\\frac{\\Delta_{m a x}}{\\Delta_{m i n}})\\cdot\\frac{n\\Delta_{m a x}}{b-a}\\frac{\\lambda}{2}+D(m)\\cdot(\\Delta_{m a x})^{2m}\\,,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $p_{d}(\\cdot)$ is a degree $d$ polynomial with positive weights and $D(m)$ is a function of $m$ . Then for each $j\\in\\{0,1,\\dotsc,m\\}$ and any $f\\in\\mathbb{W}^{m,2}\\left(\\Omega;\\mathbb{R}\\right)$ , there exist a function $H(m,j)$ such that: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left\\|(f-\\mathbf{S}_{\\lambda,n,m}(\\mathbf{y}))^{(j)}\\right\\|_{L^{2}(\\Omega;\\mathbb{R})}^{2}\\leqslant H(m,j)\\left(1+\\left(\\frac{L}{(b-a)^{2m}}\\right)\\right)^{\\frac{j}{m}}\\cdot L^{\\frac{(m-j)}{m}}\\cdot\\left\\|f^{(m)}\\right\\|_{L^{2}(\\Omega;\\mathbb{R})}^{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Note that $(f-\\mathbf{S}_{\\lambda,n,m}(\\mathbf{y}))^{(0)}:=f-\\mathbf{S}_{\\lambda,n,m}(\\mathbf{y})$ . In the presence of noise, where $\\sigma_{0}>0$ , we can exploit the linearity of the smoothing spline operator and the mutual independence of the noise terms to conclude that: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{\\epsilon}\\left[\\|\\big(f-\\mathbf{S}_{\\lambda,n,m}(\\mathbf{y})\\big)\\|_{L^{2}(\\Omega;\\mathbb{R})}^{2}\\right]=\\mathbb{E}_{\\epsilon}\\left[\\|\\big(f-\\mathbf{S}_{\\lambda,n,m}[\\mathbf{y}](\\mathbf{f}+\\epsilon)\\big)\\|_{L^{2}(\\Omega;\\mathbb{R})}^{2}\\right]}&{}\\\\ {=\\|\\big(f-\\mathbf{S}_{\\lambda,n,m}[\\mathbf{y}](\\mathbf{f})\\big)\\|_{L^{2}(\\Omega;\\mathbb{R})}^{2}}&{}\\\\ {+\\left.\\mathbb{E}_{\\epsilon}\\left[\\|\\big(f-\\mathbf{S}_{\\lambda,n,m}[\\mathbf{y}](\\epsilon)\\big)\\|_{L^{2}(\\Omega;\\mathbb{R})}^{2}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\mathbf{f}=[f(t_{1}),\\ldots,f(t_{n})]^{T}$ . The first term in (56) can be upper bounded using Theorem 6. The following theorem establishes an upper bound for the second term when $\\frac{\\Delta_{\\operatorname*{max}}}{\\Delta_{\\operatorname*{min}}}$ is bounded: ", "page_idx": 19}, {"type": "text", "text": "Theorem 7. ([65, Section $5J$ ) Consider data model $y_{i}=f(t_{i})\\!+\\!\\epsilon_{i}$ , where $\\mathbb{E}[\\epsilon_{i}]=0$ and $\\mathbb{E}[\\epsilon_{i}^{2}]\\leqslant\\sigma_{0}^{2}$ for $i~\\in~[n]$ . Assume there exist a constant $B~>~0$ such that ${\\frac{\\Delta_{m a x}}{\\Delta_{m i n}}}~\\leqslant~B\\,$ . Then for each $j~\\in$ $\\{0,1,\\ldots,m\\}$ , there exist a constant $\\lambda_{0}>0$ and function $Q(m,j,\\lambda_{0})$ such that: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\epsilon}\\left[\\left\\|\\left(f-\\mathbf{S}_{\\lambda,n,m}[\\mathbf{y}](\\epsilon)\\right)^{(j)}\\right\\|_{L^{2}(\\Omega;\\mathbb{R})}^{2}\\right]\\leqslant\\frac{Q(m,j,\\lambda_{0})\\cdot\\sigma_{0}^{2}}{n}\\lambda^{\\frac{-(2j+1)}{2m}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "for $\\lambda\\leqslant\\lambda_{0}$ and $n\\lambda^{\\frac{1}{2m}}\\geqslant1$ . ", "page_idx": 20}, {"type": "text", "text": "Note that based on [65], $Q(m,j,\\lambda_{0})=w(m,j)\\lambda_{0}^{\\frac{1}{2m}}+{\\tilde{w}}(m,j).$ ", "page_idx": 20}, {"type": "text", "text": "B Proof of Theorems ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Recall from (2) that $\\mathcal{R}(\\hat{f})\\leqslant\\mathcal{L}_{\\mathrm{enc}}(\\hat{f})+\\mathcal{L}_{\\mathrm{dec}}(\\hat{f})$ , where ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathcal{L}_{\\mathrm{dec}}(\\hat{f})=\\mathbb{E}_{\\epsilon,\\mathcal{F}\\sim F_{S,N}}\\left[\\frac{2}{K}\\sum_{k=1}^{K}\\left(u_{\\mathrm{dec}}(\\alpha_{k})-f(u_{\\mathrm{enc}}(\\alpha_{k}))\\right)^{2}\\right],}}\\\\ &{}&{\\mathcal{L}_{\\mathrm{enc}}(\\hat{f})=\\displaystyle\\frac{2}{K}\\sum_{k=1}^{K}\\left(f(u_{\\mathrm{enc}}(\\alpha_{k}))-f(x_{k})\\right)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We begin by deriving a general intermediate bound for $\\mathcal{L}_{\\mathrm{dec}}$ and ${\\mathcal{L}}_{\\mathrm{enc}}$ which will be a key component in the proofs of Theorems 2 and 1. The subsequent subsections will then provide the remaining details to complete the proofs of both theorems. ", "page_idx": 20}, {"type": "text", "text": "Lemma 2. Let $f:\\mathbb{R}\\rightarrow\\mathbb{R}$ be a $q$ -Lipschitz continuous function. Then: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{L}_{e n c}\\leqslant\\frac{2q^{2}}{K}\\sum_{k=1}^{K}(u_{e n c}(\\alpha_{k})-x_{k})^{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. Using Lipschitz property, we have: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{\\mathrm{enc}}=\\displaystyle\\frac{2}{K}\\displaystyle\\sum_{k=1}^{K}\\left(f(u_{\\mathrm{enc}}(\\alpha_{k}))-f(x_{k})\\right)^{2}}\\\\ &{\\phantom{\\mathcal{L}_{\\mathrm{enc}}=}\\displaystyle\\frac{2}{K}\\displaystyle\\sum_{k=1}^{K}\\left(|f(u_{\\mathrm{enc}}(\\alpha_{k}))-f(x_{k})|\\right)^{2}}\\\\ &{\\phantom{\\mathcal{L}_{\\mathrm{enc}}=}\\displaystyle\\frac{2}{K}\\displaystyle\\sum_{k=1}^{K}\\left(q\\cdot|u_{\\mathrm{enc}}(\\alpha_{k})-x_{k}|\\right)^{2}}\\\\ &{\\phantom{\\mathcal{L}_{\\mathrm{enc}}=}\\displaystyle\\frac{2q^{2}}{K}\\displaystyle\\sum_{k=1}^{K}(u_{\\mathrm{enc}}(\\alpha_{k})-x_{k})^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "As previously mentioned, $\\mathcal{L}_{\\mathrm{dec}}$ represents the expected generalization error of the decoder function. To leverage the results from Theorems 6 and 7 we must establish that the composition of $f$ with the encoder $u_{\\mathrm{{enc}}}$ belongs to the Sobolev space $\\mathbb{W}^{2,2}\\left(\\Omega;\\mathbb{R}\\right)$ . ", "page_idx": 20}, {"type": "text", "text": "Lemma 3. Let $f:\\mathbb{R}\\rightarrow\\mathbb{R}$ be a $q$ -Lipschitz continuous function with $\\|f^{\\prime\\prime}\\|_{L^{\\infty}(\\Omega;\\mathbb{R})}\\leqslant\\nu$ and $\\Omega\\subset\\mathbb{R}$ be an open interval. If $u_{e n c}\\in\\mathbb{W}^{2,2}\\left(\\Omega;\\mathbb{R}\\right)$ then $f\\circ u_{e n c}\\in\\mathbb{W}^{2,2}\\left(\\Omega;\\mathbb{R}\\right)$ . ", "page_idx": 20}, {"type": "text", "text": "Proof. Let us define $f_{0}(t):=f(t)-f(0)$ . Thus $f_{0}(0)=0$ and $f_{0}$ is $q$ -Lipschitz. Using Lipschitz property ", "page_idx": 20}, {"type": "equation", "text": "$$\n|f_{0}(u_{\\mathrm{enc}}(t))|^{2}=|f_{0}(u_{\\mathrm{enc}}(t))-f_{0}(0)|^{2}\\leqslant q^{2}\\cdot u_{\\mathrm{enc}}(t)^{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Integrating both sides of (62): ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\int_{\\Omega}(f_{0}\\circ u_{\\mathrm{enc}}(t))^{2}\\,d t\\leqslant q^{2}\\cdot\\int_{\\Omega}u_{\\mathrm{enc}}(t)^{2}\\,d t\\overset{\\mathrm{(a)}}{<}\\infty,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where (a) follows by $u_{\\mathrm{enc}}\\in\\mathbb{W}^{2,2}\\left(\\Omega;\\mathbb{R}\\right)$ . Given that $f_{0}$ is $q$ -Lipschitz, its derivative is bounded in the $L^{\\infty}\\left(\\Omega;\\mathbb{R}\\right)$ -norm, i.e., $\\|f_{0}^{\\prime}\\|_{L^{\\infty}(\\Omega;\\mathbb{R})}\\leqslant q$ . Thus ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\int_{\\Omega}\\,((f_{0}\\circ u_{\\mathrm{enc}}(t))^{\\prime})^{2}\\,d t\\overset{(\\mathrm{a})}{=}\\int_{\\Omega}\\,(f_{0}^{\\prime}(u_{\\mathrm{enc}}(t)))^{2}\\cdot u_{\\mathrm{enc}}^{\\prime}(t)^{2}\\,d t\\leqslant q^{2}\\cdot\\int_{\\Omega}u_{\\mathrm{enc}}^{\\prime}(t)^{2}\\,d t\\overset{(\\mathrm{b})}{<}\\infty,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where (a) follows by chain rule and (b) follows by $u_{\\mathrm{enc}}\\in\\mathbb{W}^{2,2}\\left(\\Omega;\\mathbb{R}\\right)$ . For the second derivative we have: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\int_{\\Omega}\\left[f_{0}(u_{\\mathrm{enc}}(t))^{\\prime\\prime}\\right]^{2}\\,d t\\overset{\\{u_{\\mathrm{enc}}^{\\prime\\prime}(t)\\}}{=}\\int_{\\Omega}\\left[u_{\\mathrm{enc}}^{\\prime\\prime}(t)\\cdot f_{0}^{\\prime}(u_{\\mathrm{enc}}(t))+u_{\\mathrm{enc}}^{\\prime}(t)^{2}\\cdot f_{0}^{\\prime\\prime}(u_{\\mathrm{enc}}(t))\\right]^{2}\\,d t}&{}\\\\ {\\overset{\\mathrm{(b)}}{\\leq}\\int_{\\Omega}\\left[u_{\\mathrm{enc}}^{\\prime\\prime}(t)^{2}+u_{\\mathrm{enc}}^{\\prime}(t)^{4}\\right]\\left[f_{0}^{\\prime}(u_{\\mathrm{enc}}(t))^{2}+f_{0}^{\\prime\\prime}(u_{\\mathrm{enc}}(t))^{2}\\right]\\,d t}&{}\\\\ {\\overset{\\mathrm{(c)}}{\\leq}\\left(q^{2}+\\nu^{2}\\right)\\displaystyle\\int_{\\Omega}\\left[u_{\\mathrm{enc}}^{\\prime\\prime}(t)^{2}+u_{\\mathrm{enc}}^{\\prime}(t)^{4}\\right]\\,d t}&{}\\\\ {=(q^{2}+\\nu^{2})\\left(\\|u_{\\mathrm{enc}}^{\\prime\\prime}(t)\\|_{L^{2}(\\Omega;\\mathbb{R})}^{2}+\\|u_{\\mathrm{enc}}^{\\prime}(t)\\|_{L^{4}(\\Omega;\\mathbb{R})}^{4}\\right)}&{}\\\\ {\\overset{\\mathrm{(d)}}{\\leq}(q^{2}+\\nu^{2})\\left(\\|u_{\\mathrm{enc}}^{\\prime\\prime}(t)\\|_{L^{2}(\\Omega;\\mathbb{R})}^{2}+\\left(\\|u_{\\mathrm{enc}}^{\\prime}(t)\\|_{L^{2}(\\Omega;\\mathbb{R})}+\\|u_{\\mathrm{enc}}^{\\prime\\prime}(t)\\|_{L^{2}(\\Omega;\\mathbb{R})}\\right)^{4}\\right)}&{}\\\\ {\\overset{\\mathrm{(e)}}{\\leq}\\infty,}&{(65)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where (a) follows from the chain rule, (b) is derived using the Cauchy-Schwarz inequality, (c) is due to $\\|f_{0}^{\\prime\\prime}\\|_{L^{\\infty}(\\Omega;\\mathbb{R})}\\leqslant\\nu,$ , (d) follows from Theorem 5 with $r=4,p=q=2,l=1$ , and (e) is a result of $u_{\\mathrm{enc}}\\,\\in\\,\\mathbb{W}^{2,2}\\,(\\Omega;\\mathbb{R})$ . Equations (63), (64), and (63) demonstrate that $f_{0}\\circ u_{\\mathrm{enc}}\\,\\in\\,\\mathbb{W}^{2,2}\\,(\\Omega;\\mathbb{R})$ . Note that $\\Omega$ is bounded, and every constant function belongs to $\\mathbb{W}^{2,2}\\left(\\Omega;\\mathbb{R}\\right)$ . Thus, we can conclude that $f_{0}\\circ u_{\\mathrm{enc}}(t)+f(0)=f\\circ\\dot{u_{\\mathrm{enc}}}(t)\\in\\mathbb{W}^{2,2}\\left(\\Omega;\\mathbb{R}\\right)$ . \u53e3 ", "page_idx": 21}, {"type": "text", "text": "Let us define the function $h(t):=u_{\\mathrm{dec}}(t)-f(u_{\\mathrm{enc}}(t))$ . Based on Lemma 3, and given that $u_{\\mathrm{dec}}$ and $f\\circ u_{\\mathrm{enc}}$ belong to the Sobolev space $\\mathbb{W}^{2,2}\\left(\\Omega;\\mathbb{R}\\right)$ , it follows that $h\\in\\mathbb{W}^{2,2}\\left(\\Omega;\\mathbb{R}\\right)$ . The subsequent lemmas establish upper bounds for $\\|h\\|_{L^{\\infty}(\\Omega;\\mathbb{R})}$ and $\\|h^{\\prime}\\|_{L^{\\infty}(\\Omega;\\mathbb{R})}$ , leveraging properties of functions in Sobolev spaces. ", "page_idx": 21}, {"type": "text", "text": "Lemma 4. If $\\Omega=(-a,a)$ , then: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|h\\|_{L^{2}(\\Omega;\\mathbb{R})}\\leqslant\\|h^{\\prime}\\|_{L^{2}(\\Omega;\\mathbb{R})}\\cdot\\sqrt{x_{0}^{2}+a^{2}}\\leqslant\\|h^{\\prime}\\|_{L^{2}(\\Omega;\\mathbb{R})}\\cdot\\sqrt{2}a\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. Assume $\\exists\\,x_{0}\\in\\Omega:h\\left(x_{0}\\right)=0$ . Therefore, $\\begin{array}{r}{|h(x)|=\\left|\\int_{x_{0}}^{x}h^{\\prime}(x)\\,d x\\right|}\\end{array}$ for $x\\in[x_{0},a)$ . Thus ", "page_idx": 21}, {"type": "equation", "text": "$$\n|h(x)|\\leqslant\\left|\\int_{x_{0}}^{x}h^{\\prime}(x)\\,d x\\right|\\stackrel{(\\mathrm{a})}{\\leqslant}\\int_{x_{0}}^{x}|h^{\\prime}(x)|\\ d x\\stackrel{(\\mathrm{b})}{\\leqslant}\\left(\\int_{x_{0}}^{x}1^{2}\\,d x\\right)^{\\frac{1}{2}}\\left(\\int_{x_{0}}^{x}|h^{\\prime}(x)|^{2}\\,d x\\right)^{\\frac{1}{2}},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where (a) and (b) are followed by the triangle and Cauchy-Schwarz inequalities respectively. Integrating the square of both sides over the interval $[x_{0},a)$ yields: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\int_{x_{0}}^{a}\\left|h(x)\\right|^{2}d x\\leqslant\\int_{x_{0}}^{a}\\left(x-x_{0}\\right)\\,d x\\cdot\\int_{x_{0}}^{a}\\left(\\int_{x_{0}}^{a}\\left|h^{\\prime}(x)\\right|^{2}\\,d x\\right)\\,d x^{\\prime}\\,\\overset{(a)}{\\leqslant}\\int_{x_{0}}^{a}\\left(x-x_{0}\\right)\\cdot\\int_{-a}^{a}\\left|h^{\\prime}(x)\\right|^{2},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where (a) follows by $x_{0}<a$ . On the other side, we have the following for every $x\\in(-a,x_{0}]$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\n|h(x)|=\\left|\\int_{x}^{x_{0}}h^{\\prime}(x)\\,d x\\right|\\leqslant\\int_{x}^{x_{0}}|h^{\\prime}(x)|\\ d x\\leqslant\\left(\\int_{x}^{x_{0}}1^{2}\\,d x\\right)^{\\frac{1}{2}}\\cdot\\left(\\int_{x}^{x_{0}}|h^{\\prime}(x)|^{2}\\ d x\\right)^{\\frac{1}{2}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Therefore, we have a similar inequality: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\int_{-a}^{x_{0}}|h(x)|^{2}\\,d x\\leqslant\\int_{-a}^{x_{0}}\\left(x_{0}-x\\right)\\,d x\\cdot\\int_{-a}^{a}|h^{\\prime}(x)|^{2}\\,d x.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Using (68) and (70) completes the proof: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\left\\|h\\right\\|_{L^{2}(\\Omega)}^{2}=\\displaystyle\\int_{-a}^{x_{0}}\\left|h(x)\\right|^{2}d x+\\displaystyle\\int_{x_{0}}^{a}\\left|h(x)\\right|^{2}d x}\\\\ {\\displaystyle\\leqslant\\left\\|h^{\\prime}\\right\\|_{L^{2}(\\Omega;\\mathbb{R})}^{2}\\cdot\\left(\\displaystyle\\int_{-a}^{x_{0}}\\left(x_{0}-x\\right)\\,d x+\\displaystyle\\int_{x_{0}}^{a}\\left(x-x_{0}\\right)\\,d x\\right)}\\\\ {\\displaystyle\\leqslant\\left\\|h^{\\prime}\\right\\|_{L^{2}(\\Omega;\\mathbb{R})}^{2}\\cdot\\left(x_{0}\\left(x_{0}+a\\right)-\\left(\\displaystyle\\frac{x_{0}^{2}-a^{2}}{2}\\right)+\\left(\\displaystyle\\frac{a^{2}-x_{0}^{2}}{2}\\right)-x_{0}\\left(a-x_{0}\\right)\\right)}\\\\ {\\displaystyle=\\left\\|h^{\\prime}\\right\\|_{L^{2}(\\Omega;\\mathbb{R})}^{2}\\cdot\\left(x_{0}^{2}+a^{2}\\right)\\leqslant\\left\\|h^{\\prime}\\right\\|_{L^{2}(\\Omega;\\mathbb{R})}^{2}2a^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Thus, if $x_{0}$ exists, the proof is complete. In the next step, we prove the existence of $x_{0}\\in\\Omega$ such that $h\\left(x_{0}\\right)=0$ . Recall that $h(t)=u_{\\mathrm{dec}}(t)-f(u_{\\mathrm{enc}}(t))$ and $u_{\\mathrm{dec}}(\\cdot)$ is the solution of (4). Assume there is no such $x_{0}$ . Since $h\\in\\mathbb{W}^{2,2}\\left(\\Omega;\\mathbb{R}\\right)$ , then $h(\\cdot)$ is continuous. Therefore, if there exist $t_{1},t_{2}\\in\\Omega$ such that $h(t_{1})\\,<\\,0$ and $h(t_{2})\\,>\\,0$ , then the intermediate value theorem states that there exists $x_{0}\\,\\in\\,\\left(t_{1},t_{2}\\right)$ such that $h(x_{0})\\,=\\,0$ . Thus, $h(t)\\,>\\,0$ or $h(t)\\,<\\,0$ for all $t\\,\\in\\,\\Omega$ . Without loss of generality, assume the first case where $h(t)>\\bar{0}$ for all $t\\in\\Omega$ . It means that $u_{\\mathrm{dec}}(t)>f(u_{\\mathrm{enc}}(t))$ for all $t\\in\\Omega$ . Let us define ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\beta^{*}:=\\operatorname*{argmin}_{\\beta\\in\\mathcal{F}}u_{\\operatorname*{dec}}(\\beta)-f\\big(u_{\\operatorname*{enc}}(\\beta)\\big).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Let $\\bar{u}_{\\mathrm{dec}}(t):=u_{\\mathrm{dec}}(t)-u_{\\mathrm{dec}}(\\beta^{*})$ . Note that $\\begin{array}{r}{\\int_{\\Omega}\\left(\\bar{u}_{\\mathrm{dec}}^{\\prime\\prime}(t)\\right)^{2}\\,d t=\\int_{\\Omega}\\left({u}_{\\mathrm{dec}}^{\\prime\\prime}(t)\\right)^{2}\\,d t.}\\end{array}$ . Therefore, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{v\\in\\mathcal{F}}\\left[u_{\\mathrm{dec}}\\left(\\beta_{v}\\right)-f\\left(u_{\\mathrm{enc}}\\left(\\beta_{v}\\right)\\right)\\right]^{2}=\\displaystyle\\sum_{v\\in\\mathcal{F}}\\left[\\bar{u}_{\\mathrm{dec}}\\left(\\beta_{v}\\right)+u_{\\mathrm{dec}}(\\beta^{*})-f\\left(u_{\\mathrm{enc}}\\left(\\beta_{v}\\right)\\right)\\right]^{2}}&{}\\\\ {=\\displaystyle\\sum_{v\\in\\mathcal{F}}\\left[\\bar{u}_{\\mathrm{dec}}\\left(\\beta_{v}\\right)-f\\left(u_{\\mathrm{enc}}\\left(\\beta_{v}\\right)\\right)\\right]^{2}+\\left|\\mathcal{F}\\right|\\cdot u_{\\mathrm{dec}}(\\beta^{*})^{2}}&{}\\\\ {\\displaystyle+\\,2u_{\\mathrm{dec}}(\\beta^{*})\\sum_{v\\in\\mathcal{F}}\\left[\\bar{u}_{\\mathrm{dec}}(\\beta_{v})-f(u_{\\mathrm{enc}}(\\beta_{v}))\\right]}&{}\\\\ {\\displaystyle\\overset{\\mathrm{(a)}}{\\geqslant}\\displaystyle\\sum_{v\\in\\mathcal{F}}\\left[\\bar{u}_{\\mathrm{dec}}\\left(\\beta_{v}\\right)-f\\left(u_{\\mathrm{enc}}\\left(\\beta_{v}\\right)\\right)\\right]^{2},}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where (a) follows from $u_{\\mathrm{dec}}(\\beta^{*})\\,>\\,0$ and $\\bar{u}_{\\mathrm{dec}}(\\beta_{v})\\,>\\,f\\big(u_{\\mathrm{enc}}(\\beta_{v})\\big)$ for all $v\\,\\in\\,{\\mathcal{F}}$ . This leads to a contradiction since it implies that $u_{\\mathrm{dec}}$ is not the solution of the (4). Therefore, our initial assumption must be wrong. Thus, there exists $x_{0}\\in\\Omega$ such that $h(x_{0})=0$ . \u53e3 ", "page_idx": 22}, {"type": "text", "text": "Lemma 5. Let $\\Omega=(-1,1)$ . For $h(t)=u_{d e c}(t)-f(u_{e n c}(t))$ we have: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|h\\|_{L^{\\infty}(\\Omega;\\mathbb{R})}\\leqslant2\\,\\|h\\|_{L^{2}(\\Omega;\\mathbb{R})}^{\\frac{1}{2}}\\cdot\\|h^{\\prime}\\|_{L^{2}(\\Omega;\\mathbb{R})}^{\\frac{1}{2}}<\\infty,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|h^{\\prime}\\|_{L^{\\infty}(\\Omega;\\mathbb{R})}\\leqslant\\|h^{\\prime}\\|_{L^{2}(\\Omega;\\mathbb{R})}+\\|h^{\\prime\\prime}\\|_{L^{2}(\\Omega;\\mathbb{R})}<\\infty.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. Using Lemma 4 one can conclude ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{\\|h\\|_{L^{2}(\\Omega;\\mathbb{R})}}{\\|h^{\\prime}\\|_{L^{2}(\\Omega;\\mathbb{R})}}\\leqslant\\sqrt{2}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Since $h\\,\\in\\,\\mathbb{W}^{2,2}\\,(\\Omega;\\mathbb{R})$ we can apply Corollary 3 and Theorem 5 with $r\\,=\\,\\infty$ and $p,q\\,=\\,2$ to complete the proof of (74). Furthermore, using Theorem 5 with $r\\,=\\,\\infty$ and $p,q\\,=\\,2$ and $\\ell=1$ completes the proof of (75). \u53e3 ", "page_idx": 22}, {"type": "text", "text": "Building upon Lemma 5 and starting from (58), we can derive an upper bound for $\\mathcal{L}_{\\mathrm{dec}}$ : ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathcal{L}_{\\mathrm{dec}}(u_{\\mathrm{dec}})=\\mathbb{E}_{\\epsilon,r}\\left[\\frac{2}{K}\\sum_{k=1}^{K}(u_{\\mathrm{dec}}(\\alpha_{k})-f(u_{\\mathrm{tec}}(\\alpha_{k})))_{2}^{2}\\right]}\\\\ {\\displaystyle\\triangleq\\frac{2}{K}\\sum_{k=1}^{K}\\mathbb{E}_{\\epsilon,r}\\left[h(\\alpha_{k})^{2}\\right]}\\\\ {\\displaystyle\\overset{\\mathrm{(i)}}{\\underset{\\mathrm{(}}{K}}\\sum_{k=1}^{K}\\mathbb{E}_{\\epsilon,r}\\left[\\|h\\|_{L^{\\infty}(\\Omega;\\mathbb{R})}^{2}\\right]}\\\\ {\\displaystyle\\overset{\\mathrm{(c)}}{\\underset{\\mathrm{(}}{K}}2\\mathbb{E}_{\\epsilon,r}\\left[\\|h\\|_{L^{2}(\\Omega;\\mathbb{R})}\\cdot\\|h^{\\prime}\\|_{L^{2}(\\Omega;\\mathbb{R})}\\right]}\\\\ {\\displaystyle\\overset{\\mathrm{(d)}}{\\underset{\\mathrm{(}}{K}}2\\mathbb{E}_{\\epsilon,r}\\left[\\|h\\|_{L^{2}(\\Omega;\\mathbb{R})}^{2}\\right]^{\\frac{1}{2}}\\cdot\\mathbb{E}_{\\epsilon,r}\\left[\\|h^{\\prime}\\|_{L^{2}(\\Omega;\\mathbb{R})}^{2}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where (a) follows from the definition of $h(t)$ , (b) is due to the fact that $h(t)\\leqslant\\|h\\|_{L^{\\infty}(\\Omega;\\mathbb{R})}$ for $t\\in\\Omega$ (c) follows by applying Lemma 5, and (d) is a result of applying the Cauchy-Schwarz inequality. ", "page_idx": 23}, {"type": "text", "text": "B.1 Proof of Theorem 1 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Proof. As previously mentioned, $u_{\\mathrm{dec}}(\\cdot)$ is a second-order smoothing spline fitted on the data points $\\left\\{\\left(\\beta_{i_{1}},f(u_{\\mathrm{enc}}(\\beta_{i_{1}})),\\dots,(\\beta_{i_{|\\mathcal{F}|}},f(u_{\\mathrm{enc}}(\\beta_{i_{|\\mathcal{F}|}})))\\right\\}$ , where $\\mathcal{F}:=\\,\\left\\lbrace\\beta_{i1},\\ldots,\\beta_{i_{|\\mathcal{F}|}}\\right\\rbrace$ represents the set of non-straggler worker nodes, and $\\underline{{f}}\\,\\circ\\,u_{\\mathrm{enc}}\\;:=\\;\\bigl\\{\\,f\\bigl(u_{\\mathrm{enc}}\\bigl(\\beta_{i_{1}}\\bigr)\\bigr),\\ldots,f\\bigl(u_{\\mathrm{enc}}\\bigl(\\beta_{i_{|\\mathcal{F}|}}\\bigr)\\bigr)\\bigr\\}$ is the corresponding set of computation results from these non-straggler workers. By the definition given in (50), $\\mathbf{S}_{\\lambda_{\\mathrm{d}},|\\mathcal{F}|,2}(\\cdot)$ denotes the smoothing spline operator for the decoder layer. Hence, we have the following: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\mathcal{F}}\\left[\\|h\\|_{L^{2}(\\Omega;\\mathbb{R})}^{2}\\right]=\\mathbb{E}_{\\mathcal{F}}\\left[\\big\\|f\\circ u_{\\mathrm{enc}}-\\mathbf{S}_{\\lambda_{\\mathrm{d}},|\\mathcal{F}|,2}[\\mathbf{f}]\\big\\|_{L^{2}(\\Omega;\\mathbb{R})}^{2}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\mathbf{f}\\,=\\,\\bigl\\{f\\bigl(u_{\\mathrm{enc}}(\\beta_{i_{1}})\\bigr),\\dots,f\\bigl(u_{\\mathrm{enc}}\\bigl(\\beta_{i_{|\\mathcal{F}|}}\\bigr)\\bigr)\\bigr\\}$ . Let us define the following variables analogous to those in (6): ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\Delta_{\\operatorname*{max}}^{\\mathcal{F}}:=\\operatorname*{max}_{f\\in\\{0,\\ldots,|\\mathcal{F}|\\}}\\left\\{\\beta_{i_{f+1}}-\\beta_{i_{f}}\\right\\},\\quad\\Delta_{\\operatorname*{min}}^{\\mathcal{F}}:=\\operatorname*{min}_{f\\in\\{1,\\ldots,|\\mathcal{F}|-1\\}}\\left\\{\\beta_{i_{f+1}}-\\beta_{i_{f}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Since there are at most $S$ stragglers among worker nodes, $\\Delta_{\\mathrm{max}}^{\\mathcal{F}}\\,\\leqslant\\,(S+1)\\cdot\\Delta_{\\mathrm{max}}$ and $\\frac{\\Delta_{\\operatorname*{max}}^{\\mathcal{F}}}{\\Delta_{\\operatorname*{min}}^{\\mathcal{F}}}\\ \\leqslant$ $\\begin{array}{r}{(S+1)\\cdot\\frac{\\Delta_{\\mathrm{max}}}{\\Delta_{\\mathrm{min}}}}\\end{array}$ . Additionally because $\\begin{array}{r}{\\Delta_{\\operatorname*{max}}=\\mathcal{O}(\\frac{1}{N})}\\end{array}$ , there exists a constant $J$ such that $\\begin{array}{r}{\\Delta_{\\mathrm{max}}\\leqslant\\frac{J}{N}}\\end{array}$ and correspondingly \u2206mFax \u2a7d $\\begin{array}{r}{\\Delta_{\\mathrm{max}}^{\\mathcal{F}}\\,\\leqslant\\,\\frac{J(S+1)}{N}\\,\\leqslant\\,\\frac{J(S+1)}{N-S}}\\end{array}$ . Applying Theorem 6 with $\\Omega=(-1,1),m=2$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\mathcal{F}}\\left[\\left\\|h\\right\\|_{L^{2}(\\Omega;\\mathbb{R})}^{2}\\right]\\leqslant H_{0}\\left\\|\\left(f\\circ u_{\\mathrm{enc}}\\right)^{(2)}\\right\\|_{L^{2}(\\Omega;\\mathbb{R})}^{2}\\cdot\\mathbb{E}_{\\mathcal{F}}\\left[L\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathcal{F}}\\left[\\left\\|h^{\\prime}\\right\\|_{L^{2}(\\Omega;\\mathbb{R})}^{2}\\right]\\leqslant H_{1}\\left\\|(f\\circ u_{\\mathrm{enc}})^{(2)}\\right\\|_{L^{2}(\\Omega;\\mathbb{R})}^{2}\\cdot\\mathbb{E}_{\\mathcal{F}}\\left[L^{\\frac{1}{2}}(1+\\frac{L}{16})^{\\frac{1}{2}}\\right],\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where L = p2 \u00b7 (N\u2212S4)\u2206mFax\u03bbd + D(2) \u00b7 \u2206mFax 4 and H0, H1 := H(2, 0), H(2, 1) as defined in Theorem 6. Thus, we have: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mathcal{F}}[L]\\leqslant\\mathbb{E}_{\\mathcal{F}}\\left[p_{2}\\left(\\frac{\\Delta_{\\operatorname*{max}}^{\\mathcal{F}}}{\\Delta_{\\operatorname*{min}}^{\\mathcal{F}}}\\right)\\cdot\\frac{(N-S)\\Delta_{\\operatorname*{max}}^{\\mathcal{F}}}{4}\\lambda_{\\mathrm{d}}+D\\cdot\\left(\\Delta_{\\operatorname*{max}}^{\\mathcal{F}}\\right)^{4}\\right]}\\\\ &{\\ \\ \\ \\ \\ \\ \\leqslant\\mathbb{E}_{\\mathcal{F}}\\left[p_{2}\\left(\\frac{\\Delta_{\\operatorname*{max}}^{\\mathcal{F}}}{\\Delta_{\\operatorname*{min}}^{\\mathcal{F}}}\\right)\\right]\\cdot\\frac{J(S+1)}{4}\\lambda_{\\mathrm{d}}+D\\cdot\\mathbb{E}_{\\mathcal{F}}\\left[{(\\Delta_{\\operatorname*{max}}^{\\mathcal{F}})^{4}}\\right]}\\\\ &{\\ \\ \\ \\ \\ \\ \\stackrel{(\\ b)}{\\leqslant}\\mathbb{E}_{\\mathcal{F}}\\left[p_{2}\\left(\\frac{\\Delta_{\\operatorname*{max}}^{\\mathcal{F}}}{\\Delta_{\\operatorname*{min}}^{\\mathcal{F}}}\\right)\\right]\\cdot\\frac{J(S+1)}{4}\\lambda_{\\mathrm{d}}+D J^{4}\\frac{(S+1)^{4}}{N^{4}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $D:=D(2)$ and (a) and (b) follow from $\\begin{array}{r}{\\Delta_{\\mathrm{max}}^{\\mathcal{F}}\\,\\leqslant\\,\\frac{J(S+1)}{N-S}}\\end{array}$ and $\\begin{array}{r}{\\Delta_{\\mathrm{max}}^{\\mathcal{F}}\\leqslant\\frac{J(S+1)}{N}}\\end{array}$ respectively. Substituting in (90),(91), and (91), we have: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathcal{F}}\\left[\\|h\\|_{L^{2}(\\Omega;\\mathbb{R})}^{2}\\right]\\leqslant H_{0}\\left\\|\\big(f\\circ u_{\\mathrm{enc}}\\big)^{(2)}\\right\\|_{L^{2}(\\Omega;\\mathbb{R})}^{2}\\cdot\\left(\\mathbb{E}_{\\mathcal{F}}\\left[p_{2}\\left(\\frac{\\Delta_{\\operatorname*{max}}^{\\mathcal{F}}}{\\Delta_{\\operatorname*{min}}^{\\mathcal{F}}}\\right)\\right]\\cdot\\frac{J(S+1)}{4}\\lambda_{\\mathrm{d}}+D J^{4}\\frac{(S+1)^{4}}{N^{4}}\\right)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{\\bar{\\tau}}_{\\mathcal{F}}\\left[\\|h^{\\prime}\\|_{L^{2}(\\Omega;\\mathbb{R})}^{2}\\right]\\leqslant H_{1}\\left\\|(f\\circ u_{\\mathrm{enc}})^{(2)}\\right\\|_{L^{2}(\\Omega;\\mathbb{R})}^{2}\\cdot\\left(\\mathbb{E}_{\\mathcal{F}}\\left[p_{2}\\left(\\frac{\\Delta_{\\mathsf{m a x}}^{\\mathcal{F}}}{\\Delta_{\\mathsf{m i n}}^{\\mathcal{F}}}\\right)\\right]\\cdot\\frac{J(S+1)}{4}\\lambda_{\\mathsf{d}}+D J^{4}\\frac{(S+1)^{4}}{N^{4}}\\right.}\\\\ {\\left.\\cdot\\left(1+\\frac{\\mathbb{E}_{\\mathcal{F}}\\left[p_{2}\\left(\\frac{\\Delta_{\\mathsf{m a x}}^{\\mathcal{F}}}{\\Delta_{\\mathsf{m i n}}^{\\mathcal{F}}}\\right)\\right]\\cdot\\frac{J(S+1)}{4}\\lambda_{\\mathsf{d}}+D J^{4}\\frac{(S+1)^{4}}{N^{4}}}{16}\\right)\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Therefore, we can derive an upper bound for (77) based on the above inequalities. This upper bound holds for any $\\lambda_{\\mathrm{d}}$ . To obtain an upper bound independent of $\\lambda_{\\mathrm{d}}$ , we minimize the right-hand side of this bound with respect to $\\lambda_{\\mathrm{d}}$ . Since all upper bounds in (85) and (84) are increasing functions of $\\lambda_{\\mathrm{d}}$ , and the upper bound of $\\mathcal{L}_{\\mathrm{dec}}$ in (77) is an increasing function of $\\mathbb{E}_{\\mathcal{F}}\\left[\\left\\|h\\right\\|_{L^{2}(\\Omega;\\mathbb{R})}^{2}\\right]$ and $\\mathbb{E}_{\\mathcal{F}}\\left[\\left\\|h^{\\prime}\\right\\|_{L^{2}(\\Omega;\\mathbb{R})}^{2}\\right]$ , setting $\\lambda_{\\mathrm{d}}^{*}=0$ minimizes the right-hand side of (77) with respect to $\\lambda_{\\mathrm{d}}$ . Substituting $\\lambda_{\\mathrm{d}}=0$ , we obtain: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathcal{L}_{\\mathrm{dec}}\\leqslant\\left\\|(f\\circ u_{\\mathrm{enc}})^{(2)}\\right\\|_{L^{2}(\\Omega;\\mathbb{R})}^{2}\\left[(H_{0}\\cdot r(S,N))^{\\frac{1}{2}}\\left(H_{1}\\cdot r(S,N)^{\\frac{1}{2}}\\left(1+\\frac{r(S,N)}{16}\\right)^{\\frac{1}{2}}\\right)^{\\frac{1}{2}}\\right]}\\\\ {=\\left\\|(f\\circ u_{\\mathrm{enc}})^{(2)}\\right\\|_{L^{2}(\\Omega;\\mathbb{R})}^{2}\\left[H_{0}^{\\frac{1}{2}}H_{1}^{\\frac{1}{2}}\\cdot r(S,N)^{\\frac{3}{4}}\\cdot\\left(1+\\frac{r(S,N)}{16}\\right)^{\\frac{1}{4}}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where r(S, N) := DJ4 (SN+41)4 . Note that, since $S+1\\leqslant N$ then $\\begin{array}{r}{1+\\frac{r(S,N)}{16}\\,\\leqslant\\,2\\operatorname*{max}(1,D J^{4})}\\end{array}$ . Defining $\\eta:=2\\operatorname*{max}(1,D J^{4})$ , we have: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{dec}}\\leqslant\\Big\\|(f\\circ u_{\\mathrm{enc}})^{(2)}\\Big\\|_{L^{2}(\\Omega;\\mathbb{R})}^{2}\\left[H_{0}^{\\frac{1}{2}}H_{1}^{\\frac{1}{2}}\\eta^{\\frac{1}{4}}\\cdot r(S,N)^{\\frac{3}{4}}\\right]\\!,}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Defining $\\begin{array}{r}{C:=H_{0}^{\\frac{1}{2}}H_{1}^{\\frac{1}{2}}\\eta^{\\frac{1}{4}}}\\end{array}$ and applying Lemma 2, completes the proof. ", "page_idx": 24}, {"type": "text", "text": "B.2 Proof of Theorem 2 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Using the decomposition (56) we have: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\boldsymbol{\\epsilon},\\mathcal{F}}\\left[\\left\\|h\\right\\|_{L^{2}(\\Omega;\\mathbb{R})}^{2}\\right]=\\mathbb{E}_{\\mathcal{F}}\\left[\\left\\|f\\circ u_{\\mathrm{enc}}-\\mathbf{S}_{\\lambda_{\\mathrm{d}},|\\mathcal{F}|,2}[\\mathbf{f}]\\right\\|_{L^{2}(\\Omega;\\mathbb{R})}^{2}\\right]}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad+\\mathbb{E}_{\\boldsymbol{\\epsilon},\\mathcal{F}}\\left[\\left\\|f\\circ u_{\\mathrm{enc}}-\\mathbf{S}_{\\lambda_{\\mathrm{d}},|\\mathcal{F}|,2}[\\boldsymbol{\\epsilon}]\\right\\|_{L^{2}(\\Omega;\\mathbb{R})}^{2}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\mathbf{f}\\,=\\,\\bigl\\{\\,f(u_{\\mathrm{enc}}(\\beta_{i_{1}})),\\dots,f(u_{\\mathrm{enc}}(\\beta_{i_{|\\mathcal{F}|}}))\\bigr\\}$ and $\\epsilon\\,=\\,\\bigl\\{\\epsilon_{i_{1}},\\dots,\\epsilon_{i_{|\\mathcal{F}|}}\\bigr\\}$ . Same as (79) we define the following variables: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\Delta_{\\operatorname*{max}}^{\\mathcal{F}}:=\\operatorname*{max}_{f\\in\\{0,\\ldots,|\\mathcal{F}|\\}}\\left\\{\\beta_{i_{f+1}}-\\beta_{i_{f}}\\right\\},\\quad\\Delta_{\\operatorname*{min}}^{\\mathcal{F}}:=\\operatorname*{min}_{f\\in\\{1,\\ldots,|\\mathcal{F}|-1\\}}\\left\\{\\beta_{i_{f+1}}-\\beta_{i_{f}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Again we have $\\Delta_{\\mathrm{max}}^{\\mathcal{F}}\\leqslant(S+1)\\cdot\\Delta_{\\mathrm{max}}$ and $\\begin{array}{r}{\\frac{\\Delta_{\\operatorname*{max}}^{\\mathcal{F}}}{\\Delta_{\\operatorname*{min}}^{\\mathcal{F}}}\\leqslant(S+1)\\cdot\\frac{\\Delta_{\\operatorname*{max}}}{\\Delta_{\\operatorname*{min}}}\\overset{(\\mathrm{a})}{\\leqslant}(S+1)B}\\end{array}$ , where (a) is because of the bounded condition that we have. Therefore, $\\frac{\\Delta_{\\operatorname*{max}}^{\\mathcal{F}}}{\\Delta_{\\operatorname*{min}}^{\\mathcal{F}}}$ is bounded. Additionally, since $\\begin{array}{r}{\\Delta_{\\operatorname*{min}}\\leqslant\\frac{2}{N}}\\end{array}$ then $\\frac{\\Delta_{\\operatorname*{max}}}{\\Delta_{\\operatorname*{min}}}\\leqslant B$ implies that both $\\begin{array}{r}{\\Delta_{\\operatorname*{max}}=\\mathcal{O}(\\frac{1}{N})}\\end{array}$ . Thus, there exists a constant $J$ such that $\\begin{array}{r}{\\Delta_{\\mathrm{max}}\\leqslant\\frac{J}{N}}\\end{array}$ Therefore, we have $\\begin{array}{r}{\\Delta_{\\operatorname*{max}}^{\\mathcal{F}}\\leqslant\\Delta_{\\operatorname*{max}}\\cdot(S+1)\\leqslant\\frac{J(S+1)}{N}\\leqslant\\frac{J(S+1)}{N-S}}\\end{array}$ . Applying Theorems 6 and 7 with $\\Omega=(-1,1),m=2$ , we have: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\epsilon,\\mathcal{F}}\\left[\\|h\\|_{L^{2}(\\Omega;\\mathbb{R})}^{2}\\right]\\leqslant H_{0}\\left\\|(f\\circ u_{\\mathrm{enc}})^{(2)}\\right\\|_{L^{2}(\\Omega;\\mathbb{R})}^{2}\\cdot\\mathbb{E}_{\\mathcal{F}}\\left[L\\right]+\\frac{Q_{0}\\sigma_{0}^{2}}{N-S}\\lambda_{\\mathrm{d}}^{-\\frac{1}{4}},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\epsilon,\\mathcal{F}}\\left[\\|h^{\\prime}\\|_{L^{2}(\\Omega;\\mathbb{R})}^{2}\\right]\\leqslant H_{1}\\left\\|(f\\circ u_{\\mathrm{enc}})^{(2)}\\right\\|_{L^{2}(\\Omega;\\mathbb{R})}^{2}\\cdot\\mathbb{E}_{\\mathcal{F}}\\left[L^{\\frac{1}{2}}(1+\\frac{L}{16})^{\\frac{1}{2}}\\right]+\\frac{Q_{1}\\sigma_{0}^{2}}{N-S}\\lambda_{\\mathrm{d}}^{-\\frac{3}{4}},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $\\begin{array}{r}{L~=~p_{2}\\left(\\frac{\\Delta_{\\operatorname*{max}}^{F}}{\\Delta_{\\operatorname*{min}}^{F}}\\right)~\\cdot~\\frac{(N-S)\\Delta_{\\operatorname*{max}}^{F}}{4}\\lambda_{\\mathbf{d}}~+~D(2)~\\cdot~\\left(\\Delta_{\\operatorname*{max}}^{F}\\right)^{4},~H_{0},H_{1}~:=~H(2,0),H(2,1),}\\end{array}$ and $Q_{0}(\\lambda_{0}),Q_{1}(\\lambda_{0})\\ :=\\ Q(2,0,\\lambda_{0}),Q(2,1,\\lambda_{0})$ as defined in Theorems 6 and 7. Since $\\frac{\\Delta_{\\operatorname*{max}}^{\\mathcal{F}}}{\\Delta_{\\operatorname*{min}}^{\\mathcal{F}}}\\ \\leqslant$ $(S+1)B$ , we have: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mathcal{F}}[L]\\leqslant\\mathbb{E}_{\\mathcal{F}}\\left[\\tilde{p_{2}}(S+1)\\cdot\\frac{(N-S)\\Delta_{\\operatorname*{max}}^{\\mathcal{F}}}{4}\\lambda_{\\mathrm{d}}+D\\cdot\\left(\\Delta_{\\operatorname*{max}}^{\\mathcal{F}}\\right)^{4}\\right]}\\\\ &{\\~~~~~~~\\leqslant\\tilde{p_{2}}(S+1)\\cdot\\frac{J(S+1)}{4}\\lambda_{\\mathrm{d}}+D J^{4}\\frac{(S+1)^{4}}{(N-S)^{4}}}\\\\ &{~~~~~~=p_{3}(S+1)\\cdot\\lambda_{\\mathrm{d}}+D J^{4}\\frac{(S+1)^{4}}{(N-S)^{4}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $D:=D(2),\\tilde{p_{2}}(S+1)=p_{2}\\left(B(S+1)\\right)$ , $\\begin{array}{r}{p_{3}(S+1):=\\tilde{p_{2}}(S+1)\\cdot\\frac{J(S+1)}{4}}\\end{array}$ is a degree three polynomial of $(S+1)$ , and (a) follows from the $\\begin{array}{r}{\\Delta_{\\mathrm{max}}^{\\mathcal{F}}\\leqslant\\frac{J(S+1)}{N-S}}\\end{array}$ J(NS+S1 ). Substituting in (90), we have: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\Xi_{\\epsilon,\\mathcal{F}}\\left[\\|h\\|_{L^{2}(\\Omega;\\mathbb{R})}^{2}\\right]\\leqslant H_{0}\\left\\|\\left(f\\circ u_{\\mathrm{enc}}\\right)^{(2)}\\right\\|_{L^{2}(\\Omega;\\mathbb{R})}^{2}\\left(p_{3}(S+1)\\cdot\\lambda_{\\mathrm{d}}+D J^{4}\\frac{(S+1)^{4}}{(N-S)^{4}}\\right)+\\frac{Q_{0}(\\lambda_{0})\\sigma_{0}^{2}}{N-S}\\lambda_{\\mathrm{d}}^{-2}}}\\\\ &{}&{\\stackrel{(\\mathrm{u})}{\\leqslant}H_{0}\\left\\|\\left(f\\circ u_{\\mathrm{enc}}\\right)^{(2)}\\right\\|_{L^{2}(\\Omega;\\mathbb{R})}^{2}\\lambda_{\\mathrm{d}}\\cdot\\left(p_{3}(S+1)+D J^{4}(S+1)^{4}\\right)+\\frac{Q_{0}(\\lambda_{0})\\sigma_{0}^{2}}{N-S}\\lambda_{\\mathrm{d}}^{-\\frac{1}{4}}}\\\\ &{}&{\\stackrel{(\\mathrm{b})}{=}H_{0}\\left\\|\\left(f\\circ u_{\\mathrm{enc}}\\right)^{(2)}\\right\\|_{L^{2}(\\Omega;\\mathbb{R})}^{2}\\lambda_{\\mathrm{d}}\\cdot p_{4}(S+1)+\\frac{Q_{0}(\\lambda_{0})\\sigma_{0}^{2}}{N-S}\\lambda_{\\mathrm{d}}^{-\\frac{1}{4}},\\qquad\\qquad\\qquad\\quad(93)}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where (a) follows from the fact that $\\lambda_{\\mathrm{d}}^{-1}(N-S)^{-4}\\leqslant1$ , as assumed in Theorem 7 and (b) is by definition $p_{4}(S+1):=p_{3}(S+1)+{\\bar{D}}J^{4}(S+1)^{4}$ is a degree four polynomial of $(S+1)$ . An analogous upper bound can be derived for $\\mathbb{E}_{\\epsilon,\\mathcal{F}}\\left[\\|h^{\\prime}\\|_{L^{2}(\\Omega;\\mathbb{R})}^{2}\\right]$ as ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\left[\\displaystyle\\mathbb{I}_{\\epsilon,\\gamma}\\left[\\left\\|h^{\\prime}\\right\\|_{L^{2}(\\Omega;\\mathbb{R})}^{2}\\right]\\leqslant H_{1}\\left\\|\\left(f\\circ u_{\\mathrm{enc}}\\right)^{(2)}\\right\\|_{L^{2}(\\Omega;\\mathbb{R})}^{2}\\cdot\\lambda_{\\underline{{d}}}^{\\frac{1}{2}}\\cdot p_{4}(S+1)^{\\frac{1}{2}}\\cdot\\left(1+\\lambda_{\\underline{{\\mathfrak{d}}}}\\frac{p_{4}(S+1)}{16}\\right)^{\\frac{1}{2}}}&{}&\\\\ {\\qquad}&{+\\frac{Q_{1}(\\lambda_{0})\\sigma_{0}^{2}}{N-S}\\lambda_{\\underline{{d}}}^{-\\frac{3}{4}}}&{}&\\\\ {\\overset{(a)}{\\leq}H_{1}\\left\\|\\left(f\\circ u_{\\mathrm{enc}}\\right)^{(2)}\\right\\|_{L^{2}(\\Omega;\\mathbb{R})}^{2}\\cdot\\lambda_{\\underline{{d}}}^{\\frac{1}{2}}\\cdot p_{4}(S+1)^{\\frac{1}{2}}\\cdot\\left(1+\\lambda_{0}\\frac{p_{4}(S+1)}{16}\\right)^{\\frac{1}{2}}}&{}&{}\\\\ {\\qquad}&{+\\frac{Q_{1}(\\lambda_{0})\\sigma_{0}^{2}}{N-S}\\lambda_{\\underline{{d}}}^{-\\frac{3}{4}}}&{}&\\\\ {\\overset{(b)}{=}\\left\\|\\left(f\\circ u_{\\mathrm{enc}}\\right)^{(2)}\\right\\|_{L^{2}(\\Omega;\\mathbb{R})}^{2}\\cdot\\lambda_{\\underline{{d}}}^{\\frac{1}{2}}\\cdot p_{4}(S+1)^{\\frac{1}{2}}}&{}&{\\displaystyle+\\frac{Q_{1}(\\lambda_{0})\\sigma_{0}^{2}}{N-S}\\lambda_{\\underline{{d}}}^{-\\frac{3}{4}},\\qquad(c\\in\\mathbb{Z})^{\\frac{1}{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where (a) follows from the definition $\\begin{array}{r l r}{\\lambda_{\\mathrm{d}}}&{{}\\leqslant}&{\\lambda_{0}}\\end{array}$ , (b) is derived from the definition $\\begin{array}{r l}{\\tilde{\\eta}}&{{}:=}\\end{array}$ $\\begin{array}{r}{H_{1}\\left(1+\\lambda_{0}\\frac{p_{4}(S+1)}{16}\\right)^{\\frac{1}{2}}}\\end{array}$ . By applying the upper bound for $\\mathcal{L}_{\\mathrm{dec}}$ from (77) and incorporating the results from (93) and (94), we can deduce the following: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{\\mathrm{dec}}\\overset{(\\mathrm{a})}{\\leqslant}2\\left(\\left\\|(f\\circ u_{\\mathrm{enc}})^{(2)}\\right\\|_{L^{2}(\\Omega;\\mathbb{R})}^{2}\\cdot\\mu_{0}(S)\\lambda_{\\mathrm{d}}+\\displaystyle\\frac{Q_{0}(\\lambda_{0})\\sigma_{0}^{2}}{N-S}\\lambda_{\\mathrm{d}}^{-\\frac{1}{4}}\\right)^{\\frac{1}{2}}}\\\\ &{\\qquad\\qquad\\cdot\\left(\\left\\|(f\\circ u_{\\mathrm{enc}})^{(2)}\\right\\|_{L^{2}(\\Omega;\\mathbb{R})}^{2}\\cdot\\mu_{1}(S)\\lambda_{\\mathrm{d}}^{\\frac{1}{2}}+\\displaystyle\\frac{Q_{1}(\\lambda_{0})\\sigma_{0}^{2}}{N-S}\\lambda_{\\mathrm{d}}^{-\\frac{3}{4}}\\right)^{\\frac{1}{2}}}\\\\ &{\\overset{(\\mathrm{b})}{\\leqslant}2\\lambda_{\\mathrm{d}}^{\\frac{1}{4}}\\left(\\left\\|(f\\circ u_{\\mathrm{enc}})^{(2)}\\right\\|_{L^{2}(\\Omega;\\mathbb{R})}^{2}\\cdot\\mu_{\\mathrm{max}}(S)\\lambda_{\\mathrm{d}}^{\\frac{1}{2}}+\\displaystyle\\frac{Q_{\\mathrm{max}}(\\lambda_{0})\\sigma_{0}^{2}}{N-S}\\lambda_{\\mathrm{d}}^{-\\frac{3}{4}}\\right)}\\\\ &{=2\\lambda_{\\mathrm{d}}^{\\frac{3}{4}}\\cdot\\left\\|(f\\circ u_{\\mathrm{enc}})^{(2)}\\right\\|_{L^{2}(\\Omega;\\mathbb{R})}^{2}\\cdot\\mu_{\\mathrm{max}}(S)+\\displaystyle\\frac{Q_{\\mathrm{max}}(\\lambda_{0})\\sigma_{0}^{2}}{N-S}\\lambda_{\\mathrm{d}}^{-\\frac{1}{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where (a) follows by the definitions $\\mu_{0}(S):=H_{0}\\cdot p_{4}(S+1)$ and $\\mu_{1}(S):=\\tilde{\\eta}\\cdot p_{4}(S+1)^{\\frac{1}{2}}$ , and (b) is due to $Q_{0}(\\lambda_{0}),Q_{1}(\\lambda_{0})\\leqslant Q_{\\operatorname*{max}}(\\lambda_{0}):=\\operatorname*{max}\\{Q_{0}(\\lambda_{0}),Q_{1}(\\lambda_{0})\\}$ and $\\dot{\\mu}_{0}(S),\\mu_{1}(S)\\leqslant\\dot{\\mu}_{\\operatorname*{max}}(S):=$ $\\operatorname*{max}\\{\\mu_{0}(S),\\mu_{1}(S)\\}$ . Therefore, we can conclude that: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{dec}}\\leqslant2\\lambda_{\\mathrm{d}}^{\\frac{3}{4}}\\cdot\\mu_{\\operatorname*{max}}(S)\\cdot\\left\\|(f\\circ u_{\\mathrm{enc}})^{(2)}\\right\\|_{L^{2}(\\Omega;\\mathbb{R})}^{2}+\\lambda_{\\mathrm{d}}^{-\\frac{1}{2}}\\cdot\\frac{Q_{\\operatorname*{max}}(\\lambda_{0})\\sigma_{0}^{2}}{N-S}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Based on the definition of $\\mu_{\\mathrm{max}}(S)$ we have: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu_{\\operatorname*{max}}(S)=\\operatorname*{max}\\left\\{H_{0}\\cdot p_{4}(S),H_{1}\\left(1+\\lambda_{0}\\frac{p_{4}(S)}{16}\\right)^{\\frac{1}{2}}p_{4}(S)^{\\frac{1}{2}}\\right\\}}\\\\ &{\\qquad\\qquad\\leqslant H_{\\operatorname*{max}}\\cdot p_{4}(S)^{\\frac{1}{2}}\\cdot\\operatorname*{max}\\left\\{p_{4}(S),1+\\frac{\\lambda_{0}p_{4}(S)}{16}\\right\\}^{\\frac{1}{2}},}\\\\ &{\\qquad\\leqslant H_{\\operatorname*{max}}\\cdot p_{4}(S)^{\\frac{1}{2}}\\cdot\\left(\\frac{1+p_{4}(S)}{16}\\right)^{\\frac{1}{2}}\\operatorname*{max}\\left\\{16,\\lambda_{0}\\right\\}^{\\frac{1}{2}},}\\\\ &{\\qquad=H_{\\operatorname*{max}}\\cdot\\widetilde{p_{4}}(S)\\operatorname*{max}\\left\\{4,\\lambda_{0}\\right\\}^{\\frac{1}{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where Hmax := max{H0, H1} andp4(S) := p4(S)12 \u00b7 1+p146(S) . Based on definition of $Q_{\\mathrm{max}}(\\lambda_{0})$ (mentioned in Theorem 7), we hav e: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{Q}_{\\operatorname*{max}}(\\lambda_{0})=\\operatorname*{max}\\{w_{0}\\lambda_{0}^{\\frac{1}{4}}+\\tilde{w}_{0},w_{1}\\lambda_{0}^{\\frac{1}{4}}+\\tilde{w}_{1}\\}}\\\\ &{\\qquad\\qquad\\leqslant w_{\\operatorname*{max}}\\lambda_{0}^{\\frac{1}{4}}+\\tilde{w}_{\\operatorname*{max}},}\\\\ &{\\qquad\\qquad\\leqslant2w_{\\operatorname*{max}}\\operatorname*{max}\\left\\{\\lambda_{0},\\left(\\frac{\\tilde{w}_{\\operatorname*{max}}}{w_{\\operatorname*{max}}}\\right)^{4}\\right\\}^{\\frac{1}{4}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $w_{\\mathrm{max}}:=\\operatorname*{max}\\{w_{0},w_{1}\\}$ and $\\tilde{w}_{\\mathrm{max}}:=\\operatorname*{max}\\{\\tilde{w}_{0},\\tilde{w}_{1}\\}$ . Therefore we have: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\dot{\\ddots}_{\\mathrm{dec}}\\leqslant2\\lambda_{\\mathrm{d}}^{\\frac34}\\cdot H_{\\operatorname*{max}}\\cdot\\widetilde{p}_{4}(S)\\operatorname*{max}\\left\\{4,\\lambda_{0}\\right\\}^{\\frac12}\\cdot\\left\\|(f\\circ u_{\\mathrm{enc}})^{(2)}\\right\\|_{L^{2}(\\Omega;\\mathbb{R})}^{2}+\\lambda_{\\mathrm{d}}^{-\\frac12}\\frac{2\\sigma_{0}^{2}w_{\\operatorname*{max}}\\operatorname*{max}\\left\\{\\lambda_{0}^{\\frac14},\\frac{\\widetilde{w}_{\\operatorname*{max}}}{w_{\\operatorname*{max}}}\\right\\}}{N-S}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Since (96) holds for all $\\lambda_{\\mathrm{d}}$ , by minimizing the right-hand side of (96) with respect to $\\lambda_{\\mathrm{d}}$ , we obtain: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\lambda_{\\mathrm{d}}^{*}=\\left(\\frac{3H_{\\mathrm{max}}\\cdot\\tilde{p_{4}}(S)\\cdot(N-S)\\cdot\\left\\lVert(f\\circ u_{\\mathrm{enc}})^{(2)}\\right\\rVert_{L^{2}(\\Omega;\\mathbb{R})}^{2}}{4w_{\\mathrm{max}}\\sigma_{0}^{2}}\\right)^{-\\frac{4}{5}}\\left(\\frac{\\operatorname*{max}\\left\\{4,\\lambda_{0}\\right\\}^{\\frac{1}{2}}}{\\operatorname*{max}\\left\\{\\lambda_{0}^{\\frac{1}{4}},\\frac{\\tilde{w}_{\\mathrm{max}}}{w_{\\mathrm{max}}}\\right\\}}\\right)^{-\\frac{4}{5}}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "By substituting the expression for $\\lambda_{\\mathrm{d}}^{*}$ into (96), we have: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{dec}}\\leqslant4\\left(\\frac{3H_{\\mathrm{max}}}{4w_{\\mathrm{max}}}\\right)^{\\frac{2}{5}}\\left(\\frac{\\sigma_{0}}{N-S}\\right)^{\\frac{3}{5}}\\cdot\\widetilde{\\rho_{4}}(S)^{\\frac{2}{5}}\\cdot\\left\\|\\left(f\\circ u_{\\mathrm{enc}}\\right)^{(2)}\\right\\|_{L^{2}(\\Omega;\\mathbb{R})}^{\\frac{4}{5}}\\left(\\frac{\\operatorname*{max}\\left\\{4,\\lambda_{0}\\right\\}^{\\frac{1}{2}}}{\\operatorname*{max}\\left\\{\\lambda_{0}^{\\frac{1}{4}},\\frac{\\widetilde{w}_{\\mathrm{max}}}{w_{\\mathrm{max}}}\\right\\}}\\right)^{\\frac{2}{5}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Thus, defining $\\begin{array}{r}{C_{2}:=\\frac{3H_{\\operatorname*{max}}}{4w_{\\operatorname*{max}}},C(\\lambda_{0}):=\\left(\\frac{\\operatorname*{max}\\{4,\\lambda_{0}\\}^{\\frac{1}{2}}}{\\operatorname*{max}\\{\\lambda_{0}^{\\frac{1}{4}},\\frac{\\tilde{w}_{\\operatorname*{max}}}{w_{\\operatorname*{max}}}\\}}\\right);}\\\\ {.}\\end{array}$ , and using previously driven upper bound for ${\\mathcal{L}}_{\\mathrm{enc}}$ in Lemma 2 completes the proof. ", "page_idx": 26}, {"type": "text", "text": "B.3 Proof of Theorem 3 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "The upper bounds presented in Theorems 1 and 2 depend on $\\left\\Vert\\left(f\\circ u_{\\mathrm{enc}}\\right)^{(2)}\\right\\Vert_{L^{2}(\\Omega;\\mathbb{R})}^{2}.$ , with exponents of $\\frac{2}{5}$ and 1, respectively. By applying the chain rule, we can d emonstrate th at: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\int_{0}^{1}\\left[l(w_{\\infty}(t))^{\\top}\\right]^{2}\\,d t\\,\\Bigg[l(w_{\\infty}^{*}(t),f^{\\prime}(w_{\\infty}(t))+u_{\\infty}^{*}(t)\\,l^{2}\\,r^{2}(w_{\\infty}(t)))^{2}\\,d t}\\\\ &{\\stackrel{(a)}{\\le}\\displaystyle\\int_{0}^{1}\\left[l(w_{\\infty}^{*}(t)^{2}+\\nu_{\\infty}^{*}(t)({f^{\\prime}(w_{\\infty}(t))}^{\\top}+l^{2}\\,r^{2}(w_{\\infty}(t)))^{2}\\right]\\,d t}\\\\ &{\\stackrel{(c)}{\\le}\\displaystyle(l^{2}+\\nu^{2})\\int_{0}^{1}\\left[l(w_{\\infty}^{*}(t)^{2}+\\nu_{\\infty}^{*}(t))^{4}\\,d t\\right.}\\\\ &{\\left.=(l^{2}+\\nu^{2})\\left(1\\alpha_{\\infty}^{*}(t)\\right)\\Gamma_{l}^{2}\\alpha_{0,1}^{*}+\\left[l\\alpha_{\\infty}^{*}(t)\\right]\\Gamma_{l}^{4}\\alpha_{1,2}^{*}\\right)}\\\\ &{\\stackrel{(c)}{\\le}\\displaystyle(l^{2}+\\nu^{2})\\left(1\\alpha_{\\infty}^{*}(t)\\right)\\Gamma_{l}^{2}\\alpha_{0,3}^{*}+\\left(1\\alpha_{\\infty}^{*}(t)({l}_{E,\\geq,0,3}^{*}+1\\alpha_{\\infty}^{*}(t)\\right)\\Gamma_{l}^{6}\\alpha_{3,3}^{*}\\right)^{4}\\Bigg)}\\\\ &{\\stackrel{(c)}{\\le}\\displaystyle(a^{2}+\\nu^{2})\\left(1\\alpha_{\\infty}^{*}(t)\\right)\\Gamma_{l}^{2}\\alpha_{0,3}^{*}+4\\left(1\\alpha_{\\infty}^{*}(t)\\Gamma_{l}^{2}\\alpha_{0,3}^{*}+1\\alpha_{\\infty}^{*}(t)\\right)\\Gamma_{l}^{2}\\alpha_{0,3}^{*}\\Bigg)^{2}}\\\\ &{\\stackrel{(c)}{\\le}\\displaystyle(a^{2}+\\nu^{2})\\left(1\\alpha_{\\infty}^{*}\\right)\\frac{1}{l^{2}\\left(k^{2}+1\\right)}\\alpha_{0,\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where (a) follows from the chain rule, (b) is derived using the Cauchy-Schwarz inequality, (c) is due to $\\|f_{0}^{\\prime\\prime}\\|_{L^{\\infty}(\\Omega;\\mathbb{R})}\\leqslant\\nu$ , (d) follows from Theorem 5 with $r=4,p=q=2,l=1$ , (e) follows from AM-GM inequality, (f) is a result of adding positive terms $\\lVert u_{\\mathrm{enc}}\\rVert_{\\mathbb{W}^{2,2}(\\Omega;\\mathbb{R})}^{2}$ and $\\lVert u_{\\mathrm{enc}}^{\\prime}\\rVert_{\\mathbb{W}^{2,2}(\\Omega;\\mathbb{R})}^{2}$ to the first term and $\\lVert u_{\\mathrm{enc}}\\rVert_{\\mathbb{W}^{2,2}(\\Omega;\\mathbb{R})}^{2}$ to the second term in the parenthesis, (g) is result of applying Corollary 4, (h) is by defining $\\psi(t):=73t+4\\times73^{2}t^{2}$ , and (i) is because of Proposition 2. Combining (102) with Theorems 1 and 2 we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathcal{R}(\\hat{f})\\leqslant\\frac{2q^{2}}{K}\\sum_{k=1}^{K}(u_{\\mathrm{enc}}(\\alpha_{k})-x_{k})^{2}+\\lambda_{\\mathrm{e}}\\cdot\\psi\\left(\\left\\|u_{\\mathrm{enc}}\\right\\|_{\\widetilde{\\mathcal{H}}^{2}(\\Omega;\\mathbb{R})}^{2}\\right),\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "for the noiseless setting and ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathcal{R}(\\widehat{f})\\leqslant\\frac{2q^{2}}{K}\\sum_{k=1}^{K}(u_{\\mathrm{enc}}(\\alpha_{k})-x_{k})^{2}+\\widetilde{\\lambda}_{\\mathrm{e}}\\cdot\\psi\\left(\\|u_{\\mathrm{enc}}\\|_{\\widetilde{\\mathcal{H}}^{2}(\\Omega;\\mathbb{R})}^{2}\\right)^{\\frac{2}{5}}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "for the noisy setting, where the parameters $\\lambda_{\\mathrm{e}}$ and $\\widetilde{\\lambda_{\\mathrm{e}}}$ are as follows: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\lambda_{\\mathrm{e}}=C_{1}{\\displaystyle\\frac{(S+1)^{3}}{N^{3}}}\\cdot(q^{2}+\\nu^{2})}}\\\\ {{\\widetilde{\\lambda}_{\\mathrm{e}}=2C(\\lambda_{0})^{\\frac{3}{4}}\\left(\\frac{\\sigma_{0}^{2}}{N-S}\\right)^{\\frac{3}{5}}\\cdot p_{4}(S)^{\\frac{2}{5}}\\cdot(q^{2}+\\nu^{2})^{\\frac{2}{5}}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Note that since $\\psi(t)$ and $\\gamma(t):=t^{\\frac{2}{5}}$ are monotonically increasing in $\\mathbb{R}^{+}$ , its composition is monotonically increasing as well. Moreover, $\\lambda_{\\mathrm{e}}$ and $\\widetilde{\\lambda_{\\mathrm{e}}}$ share the same exponent of $N$ as in Theorems 1 and 2, respectively. Consequently, the provided  u pper bound does not compromise the convergence rate. ", "page_idx": 27}, {"type": "text", "text": "B.4 Proof of Proposition 1 ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "For part (i), we know that for $t\\leqslant M$ , we have: ", "page_idx": 28}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "$\\psi(t)=73t+4\\times73^{2}t^{2}\\leqslant t(73+4\\times73^{2}t)\\leqslant t(73+4\\times73^{2}\\cdot M)=t(m_{1}+m_{2}M),$ (107) where $m_{1}:=73,m_{2}:=4\\times73^{2}$ . Therefore, if $\\lVert u_{\\mathrm{enc}}\\rVert_{\\widetilde{\\mathcal{H}}^{2}(\\Omega;\\mathbb{R})}^{2}\\leqslant M$ , then: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\psi(\\|u_{\\mathrm{enc}}\\|_{\\widetilde{\\mathcal{H}}^{2}(\\Omega;\\mathbb{R})}^{2})\\leqslant(m_{1}+m_{2}M)\\,\\|u_{\\mathrm{enc}}\\|_{\\widetilde{\\mathcal{H}}^{2}(\\Omega;\\mathbb{R})}^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad=(m_{1}+m_{2}M)\\,\\bigg(u_{\\mathrm{enc}}(-1)^{2}+u_{\\mathrm{enc}}^{\\prime}(-1)^{2}+\\int_{\\Omega}|u_{\\mathrm{enc}}^{\\prime\\prime}(t)|^{2}\\,d t\\bigg)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\overset{\\mathrm{(a)}}{\\leqslant}(m_{1}+m_{2}M)\\,\\bigg(M+\\int_{\\Omega}|u_{\\mathrm{enc}}^{\\prime\\prime}(t)|^{2}\\,d t\\bigg)\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where (a) is because of $\\|u_{\\mathrm{enc}}\\|_{\\widetilde{\\mathcal{H}}^{2}(\\Omega;\\mathbb{R})}^{2}\\leqslant M$ . Thus, we have: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathcal{R}(\\hat{f})\\leqslant\\frac{2q^{2}}{K}\\sum_{k=1}^{K}(u_{\\mathrm{enc}}(\\alpha_{k})-x_{k})^{2}+\\lambda_{\\mathrm{e}}\\cdot\\psi\\left(\\|u_{\\mathrm{enc}}\\|_{\\widetilde{\\mathcal{H}}^{2}(\\Omega;\\mathbb{R})}^{2}\\right)}}\\\\ &{}&{\\leqslant\\frac{2q^{2}}{K}\\sum_{k=1}^{K}(u_{\\mathrm{enc}}(\\alpha_{k})-x_{k})^{2}+\\lambda_{\\mathrm{e}}\\cdot(m_{1}+m_{2}M)\\left(M+\\displaystyle\\int_{\\Omega}|u_{\\mathrm{enc}}^{\\prime\\prime}(t)|^{2}\\,d t\\right)}\\\\ &{}&{\\leqslant\\widetilde{R}(u_{\\mathrm{enc}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "For part (ii), let $\\widetilde{u_{\\mathrm{enc}}}(t)$ be a natural spline used as the encoder function fitted to the data points $\\{(\\overline{{\\alpha_{k}}},x_{k})\\}_{k=1}^{K}$ .  Th en, we have: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathrm{\\tilde{\\delta}}_{\\mathrm{e}}(m_{1}+m_{2}M)\\left(M+\\int_{\\Omega}\\vert(u^{*})^{\\prime\\prime}(t)\\vert^{2}\\,d t\\right)\\leqslant\\widetilde{R}(u^{*})}\\\\ {\\displaystyle\\leqslant\\widetilde{R}(u_{\\mathrm{enc}})\\overset{(b)}{\\cong}\\lambda_{\\mathrm{e}}(m_{1}+m_{2}M)\\left(M+\\int_{\\Omega}\\vert\\widetilde{u_{\\mathrm{enc}}}^{\\prime\\prime}(t)\\vert^{2}\\,d t\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where (a) follows from the optimality of $u^{*}(\\cdot)$ , and (b) follows from $\\widetilde{u_{\\mathrm{enc}}}(\\alpha_{k})\\,=\\,x_{k}$ for $k\\,\\in\\,[K]$ .   \nTherefore, $\\begin{array}{r}{\\int_{\\Omega}|(u^{*})^{\\prime\\prime}(t)|^{2}\\,d t\\leqslant\\int_{\\Omega}|\\widetilde{u_{\\mathrm{enc}}}^{\\prime\\prime}(t)|^{2}\\,d t}\\end{array}$ . ", "page_idx": 28}, {"type": "text", "text": "Since $\\boldsymbol{u}^{*}(\\cdot)$ is smoothing spline, it h as the representation in natural spline space (as mentioned in (51)): ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\boldsymbol{u}^{*}(t)=\\sum_{k=1}^{K}\\xi_{k}\\boldsymbol{b}_{k}(t),\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where, $\\{b_{k}(\\cdot)\\}_{k=1}^{K}$ is a basis functions of second order natural splines. Therefore, using CauchySchwarz inequality, we have: ", "page_idx": 28}, {"type": "equation", "text": "$$\n|u^{*}(t)|^{2}\\leqslant\\left(\\sum_{k=1}^{K}\\xi^{2}\\right)\\left(\\sum_{k=1}^{K}|b_{k}(t)|^{2}\\right),\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "and ", "page_idx": 28}, {"type": "equation", "text": "$$\n|(u^{*}(t))^{\\prime}|^{2}\\leqslant\\left(\\sum_{k=1}^{K}\\xi^{2}\\right)\\left(\\sum_{k=1}^{K}|b_{k}^{\\prime}(t)|^{2}\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Both (112) and (113) hold for all $t\\in\\Omega$ . Thus: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|(u^{*})^{\\prime}(-1)|^{2}\\leqslant\\|u^{*}\\|_{L^{\\infty}(\\Omega;\\mathbb{R})}^{2}\\leqslant\\|\\pmb{\\xi}\\|_{2}^{2}\\cdot\\left(\\displaystyle\\sum_{k=1}^{K}\\|b_{k}\\|_{L^{\\infty}(\\Omega;\\mathbb{R})}^{2}\\right),}\\\\ {|(u^{*})^{\\prime}(-1)|^{2}\\leqslant\\|(u^{*})^{\\prime}\\|_{L^{\\infty}(\\Omega;\\mathbb{R})}^{2}\\leqslant\\|\\pmb{\\xi}\\|_{2}^{2}\\cdot\\left(\\displaystyle\\sum_{k=1}^{K}\\|b_{k}^{\\prime}\\|_{L^{\\infty}(\\Omega;\\mathbb{R})}^{2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\pmb{\\xi}:=[\\xi_{1},\\ldots,\\xi_{K}]^{T}\\,=\\,\\left(\\mathbf{N}^{T}\\mathbf{N}+\\lambda\\Phi\\right)^{-1}\\mathbf{N}^{T}\\mathbf{x}$ with $\\begin{array}{r}{\\mathbf{N}_{i j}\\,=\\,b_{i}(\\alpha_{j}),\\Phi_{i j}\\,=\\,\\int_{\\Omega}b_{i}^{\\prime\\prime}(t)b_{j}^{\\prime\\prime}(t)\\,d t}\\end{array}$ for $i,j\\ \\in\\ [K]$ as defined in (51), and $\\textbf{x}:=\\;\\left[x_{1},\\ldots,x_{K}\\right]$ . Noted that $\\{\\|b_{k}^{\\prime}\\|_{L^{\\infty}(\\Omega;\\mathbb{R})}^{2}\\}_{k=1}^{K}$ and $\\{\\left\\|b_{k}\\right\\|_{L^{\\infty}(\\Omega;\\mathbb{R})}^{2}\\}_{k=1}^{K}$ depend only on $\\{\\alpha_{k}\\}_{k=1}^{K}$ . ", "page_idx": 29}, {"type": "text", "text": "Lemma 6. $\\begin{array}{r}{\\gamma\\widetilde\\xi:=\\left(\\mathbf{N}^{T}\\mathbf{N}\\right)^{-1}\\mathbf{N}^{T}\\mathbf{x},\\,t h e n\\left\\Vert\\xi\\right\\Vert_{2}^{2}\\leqslant K\\cdot\\kappa(\\Phi)\\cdot\\left\\Vert\\widetilde\\xi\\right\\Vert_{2}^{2}<\\frac{2K}{\\vert\\operatorname*{det}\\Phi\\vert}\\left(\\frac{\\|\\Phi\\|_{F}^{2}}{K}\\right)^{\\frac{K}{2}}\\left\\Vert\\widetilde\\xi\\right\\Vert_{2}^{2},\\,w h e r e=\\kappa\\kappa(\\Phi)\\cdot\\left\\Vert\\widetilde\\xi\\right\\Vert_{2}^{2}.}\\end{array}$   \n$\\kappa(\\Phi)$ is condi tion number of $\\Phi$ . ", "page_idx": 29}, {"type": "text", "text": "Proof. By defining $\\widetilde{\\mathbf{N}}:=\\mathbf{N}\\Phi^{-\\frac{1}{2}}$ and rearranging the expression for $\\hat{\\pmb\\xi}$ , we obtain: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{\\xi}=\\left(\\mathbf{N}^{T}\\mathbf{N}+\\lambda\\Phi\\right)^{-1}\\mathbf{N}^{T}\\mathbf{x}=\\Phi^{-1/2}\\left(\\left[\\mathbf{N}\\Phi^{-1/2}\\right]^{T}\\left[\\mathbf{N}\\Phi^{-1/2}\\right]+\\lambda\\mathbf{I}\\right)^{-1}\\left[\\mathbf{N}\\Phi^{-1/2}\\right]^{T}\\mathbf{x}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\Phi^{-1/2}\\left(\\widetilde{\\mathbf{N}}^{T}\\widetilde{\\mathbf{N}}+\\lambda\\mathbf{I}\\right)^{-1}\\widetilde{\\mathbf{N}}^{T}\\mathbf{x}.\\qquad\\qquad\\qquad\\qquad\\qquad\\:(\\mathbf{N}\\Phi^{-1/2})^{T}\\mathbf{x}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Define $\\mathbf{z}:=\\mathbf{\\Big(}\\widetilde{\\mathbf{N}}^{T}\\widetilde{\\mathbf{N}}\\Big)^{-1}\\,\\widetilde{\\mathbf{N}}^{T}\\mathbf{x}$ . Thus, $\\widetilde{\\mathbf{N}}^{T}\\mathbf{x}\\,=\\,\\widetilde{\\mathbf{N}}^{T}\\widetilde{\\mathbf{N}}\\mathbf{z}$ . Thus, by applying the Cauchy-Schwarz inequality, we h ave : ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\left\\|\\left(\\widetilde{\\mathbf{N}}^{T}\\widetilde{\\mathbf{N}}+\\lambda\\mathbf{I}\\right)^{-1}\\widetilde{\\mathbf{N}}^{T}\\mathbf{x}\\right\\|_{2}=\\left\\|\\left(\\widetilde{\\mathbf{N}}^{T}\\widetilde{\\mathbf{N}}+\\lambda\\mathbf{I}\\right)^{-1}\\widetilde{\\mathbf{N}}^{T}\\widetilde{\\mathbf{N}}\\mathbf{z}\\right\\|_{2}\\leqslant\\left\\|\\left(\\widetilde{\\mathbf{N}}^{T}\\widetilde{\\mathbf{N}}+\\lambda\\mathbf{I}\\right)^{-1}\\widetilde{\\mathbf{N}}^{T}\\widetilde{\\mathbf{N}}\\right\\|_{2}\\cdot\\|\\mathbf{z}\\|_{2}\\,.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Let $\\widetilde{\\mathbf{N}}=\\mathbf{U}\\mathbf{D}\\mathbf{V}^{T}$ be the singular value decomposition of $\\widetilde{\\bf N}$ . Therefore, we have: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\|\\left(\\widetilde{\\mathbf{N}}^{T}\\widetilde{\\mathbf{N}}+\\lambda\\mathbf{I}\\right)^{-1}\\widetilde{\\mathbf{N}}^{T}\\widetilde{\\mathbf{N}}\\right\\|_{2}=\\left\\|\\left(\\mathbf{V}\\mathbf{D}^{2}\\mathbf{V}^{T}+\\lambda\\mathbf{I}\\right)^{-1}\\mathbf{V}\\mathbf{D}^{2}\\mathbf{V}^{T}\\right\\|_{2}}&{}\\\\ {=\\left\\|\\mathbf{V}^{-T}\\left(\\mathbf{D}^{2}+\\lambda\\mathbf{I}\\right)^{-1}\\mathbf{V}^{-1}\\nabla\\mathbf{D}^{2}\\mathbf{V}^{T}\\right\\|_{2}}&{}\\\\ {=\\left\\|\\mathbf{V}^{-T}\\left(\\mathbf{D}^{2}+\\lambda\\mathbf{I}\\right)^{-1}\\mathbf{D}^{2}\\mathbf{V}^{T}\\right\\|_{2}}&{}\\\\ {\\overset{(a)}{=}\\left\\|\\left(\\mathbf{D}^{2}+\\lambda\\mathbf{I}\\right)^{-1}\\mathbf{D}^{2}\\right\\|_{2}}&{}\\\\ {=\\left\\|\\operatorname{diag}\\left(\\frac{\\lambda_{1}^{2}}{\\lambda_{1}^{2}+\\lambda},\\cdots,\\frac{\\lambda_{K}^{2}}{\\lambda_{K}^{2}+\\lambda}\\right)\\right\\|_{2}}&{}\\\\ {\\leq\\sqrt{K},}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where (a) is because $\\mathbf{V}$ is an unitary matrix and $\\lambda_{1},\\ldots,\\lambda_{K}$ are eigenvalues of $\\widetilde{\\bf N}$ . Continuing from (116), we obtain: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\left\\|\\Phi^{1/2}\\widetilde{\\pmb{\\xi}}\\right\\|_{2}=\\left\\|\\left(\\widetilde{\\mathbf{N}}^{T}\\widetilde{\\mathbf{N}}+\\lambda\\mathbf{I}\\right)^{-1}\\widetilde{\\mathbf{N}}^{T}\\widetilde{\\mathbf{N}}\\right\\|_{2}\\leqslant\\sqrt{K}\\left\\|\\mathbf{z}\\right\\|_{2}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Let us define $\\mathbf{x}_{0}:=\\left(\\mathbf{N}^{T}\\mathbf{N}\\right)^{-1}\\mathbf{N}^{T}\\mathbf{x}$ . Thus, we have: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{z}=\\left(\\widetilde{\\mathbf{N}}^{T}\\widetilde{\\mathbf{N}}\\right)^{-1}\\widetilde{\\mathbf{N}}^{T}\\mathbf{x}}\\\\ &{\\quad=\\left(\\Phi^{-1/2}\\mathbf{N}^{T}\\mathbf{N}\\Phi^{-1/2}\\right)^{-1}\\Phi^{-1/2}\\mathbf{N}^{T}\\mathbf{x}}\\\\ &{\\quad=\\Phi^{1/2}\\left(\\mathbf{N}^{T}\\mathbf{N}\\right)^{-1}\\mathbf{N}^{T}\\mathbf{x}}\\\\ &{\\quad=\\Phi^{1/2}\\mathbf{x}_{0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Therefore, we can bound the $\\|\\pmb{\\xi}\\|_{\\Phi}:=\\sqrt{\\pmb{\\xi}^{T}\\Phi\\pmb{\\xi}}$ : ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\left\\|\\pmb{\\xi}\\right\\|_{\\Phi}=\\left\\|\\Phi^{1/2}\\pmb{\\xi}\\right\\|_{2}\\leqslant\\sqrt{K}\\cdot\\|\\pmb{\\mathbf{z}}\\|_{2}=\\sqrt{K}\\cdot\\left\\|\\pmb{\\mathbf{x}}_{0}\\right\\|_{\\Phi}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Since $\\Phi$ is symmetric, by Rayleigh-Ritz theorem we know that: ", "page_idx": 30}, {"type": "equation", "text": "$$\n0\\overset{\\mathtt{(a)}}{<}\\lambda_{\\operatorname*{min}}^{\\Phi}\\leqslant\\frac{\\xi^{T}\\Phi\\xi}{\\xi^{T}\\xi}=\\frac{\\|\\pmb{\\xi}\\|_{\\Phi}^{2}}{\\|\\pmb{\\xi}\\|_{2}^{2}}\\leqslant\\lambda_{\\operatorname*{max}}^{\\Phi},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $\\lambda_{\\operatorname*{min}}^{\\Phi},\\lambda_{\\operatorname*{max}}^{\\Phi}$ are minimum and maximum eigenvalues of $\\Phi$ , and (a) is due to the fact that since $\\Phi$ is kernel matrix of RKHS space and $\\{b_{k}(\\cdot)\\}_{k=1}^{K}$ are basis functions, it is positive definite. Thus, we have: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\|\\pmb{\\xi}\\|_{2}^{2}\\leqslant\\frac{1}{\\lambda_{\\operatorname*{min}}^{\\Phi}}\\,\\|\\pmb{\\xi}\\|_{\\Phi}^{2}\\leqslant\\frac{K}{\\lambda_{\\operatorname*{min}}^{\\Phi}}\\cdot\\|\\mathbf{x}_{0}\\|_{\\Phi}^{2}\\leqslant\\frac{K\\lambda_{\\operatorname*{max}}^{\\Phi}}{\\lambda_{\\operatorname*{min}}^{\\Phi}}\\,\\|\\mathbf{x}_{0}\\|_{2}^{2}=K\\kappa(\\Phi)\\,\\|\\mathbf{x}_{0}\\|_{2}^{2}\\,.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Applying the bound for condition number introduce in [66], we can complete the proof: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\kappa(\\Phi)<{\\frac{2}{|\\operatorname*{det}\\Phi|}}\\left({\\frac{\\|\\Phi\\|_{F}^{2}}{K}}\\right)^{\\frac{K}{2}},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $\\|\\cdot\\|_{F}$ is the Frobenius norm. ", "page_idx": 30}, {"type": "text", "text": "Using Lemma 6, (114), and (115) we have: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{u^{*}\\|_{\\widetilde{\\mathcal{H}}^{2}(\\Omega;\\mathbb{R})}\\leqslant\\|\\xi\\|_{2}^{2}\\cdot\\left(\\displaystyle\\sum_{k=1}^{K}\\|b_{k}\\|_{L^{\\infty}(\\Omega;\\mathbb{R})}^{2}+\\displaystyle\\sum_{k=1}^{K}\\|b_{k}^{\\prime}\\|_{L^{\\infty}(\\Omega;\\mathbb{R})}^{2}\\right)+\\displaystyle\\int_{\\Omega}|\\widehat{u_{\\mathrm{enc}}}^{\\prime\\prime}(t)|^{2}\\,d t}\\\\ &{\\qquad\\qquad\\overset{(\\mathrm{a})}{\\leqslant}\\displaystyle\\frac{2K}{|\\operatorname*{det}\\Phi|}\\left(\\frac{\\|\\Phi\\|_{F}^{2}}{K}\\right)^{\\frac{K}{2}}\\left\\|\\widetilde{\\xi}\\right\\|_{2}^{2}\\left(\\displaystyle\\sum_{k=1}^{K}\\|b_{k}\\|_{L^{\\infty}(\\Omega;\\mathbb{R})}^{2}+\\displaystyle\\sum_{k=1}^{K}\\|b_{k}^{\\prime}\\|_{L^{\\infty}(\\Omega;\\mathbb{R})}^{2}\\right)+\\displaystyle\\int_{\\Omega}|\\widehat{u_{\\mathrm{enc}}}^{\\prime\\prime}(t)|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where (a) follows by applying Lemma 6. Setting $M$ equal to the right-hand side of Equation (125) completes the proof. ", "page_idx": 30}, {"type": "text", "text": "B.5 Proof of Theorem 4 ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Consider a natural spline $\\widetilde{u_{\\mathrm{enc}}}(t)$ as the encoder function fitted on the data points $\\{(\\alpha_{k},x_{k})\\}_{k=1}^{K}$ . Let $u_{\\mathrm{enc}}^{*}(t)$ denote the optima l  encoder minimizing the upper bound in (10). Then, we have: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\mathcal{R}(\\hat{f})\\leqslant\\frac{2q^{2}}{K}\\sum_{k=1}^{K}(u_{\\mathrm{enc}}^{*}(\\alpha_{k})-x_{k})^{2}+\\lambda_{\\mathrm{e}}\\cdot g\\big(\\,\\|u_{\\mathrm{enc}}^{*}\\|_{\\widetilde{\\mathcal{H}}^{2}(\\Omega;\\mathbb{R})}^{2}\\,\\big)}\\\\ &{\\displaystyle\\leqslant\\frac{2q^{2}}{K}\\sum_{k=1}^{K}(\\widetilde{u_{\\mathrm{enc}}}(\\alpha_{k})-x_{k})^{2}+\\lambda_{\\mathrm{e}}\\cdot g\\big(\\,\\|\\widetilde{u_{\\mathrm{enc}}}\\|_{\\widetilde{\\mathcal{H}}^{2}(\\Omega;\\mathbb{R})}^{2}\\,\\big)}\\\\ &{\\displaystyle\\overset{(\\ b)}{=}\\lambda_{\\mathrm{e}}\\cdot g\\big(\\,\\|\\widetilde{u_{\\mathrm{enc}}}\\|_{\\widetilde{\\mathcal{H}}^{2}(\\Omega;\\mathbb{R})}^{2}\\,\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where (a) follows from the optimality of $u_{\\mathrm{enc}}^{*}(t)$ , and (b) is due to the fact that $\\widetilde{u_{\\mathrm{enc}}}(\\alpha_{k})=x_{k}$ , since $\\widetilde{u_{\\mathrm{enc}}(\\cdot)}$ is a natural spline. Note that $g\\big(\\,\\|\\widetilde{u_{\\mathrm{enc}}}\\|_{\\widetilde{\\mathcal{H}}^{2}(\\Omega;\\mathbb{R})}^{2}\\big)$ is independent of $N$ an d $S$ , and depends only on $\\alpha_{k}$ and $x_{k}$ for $k\\in[K]$ . Additionally,  base d on the Theorem 3 and (105), $\\lambda_{\\mathrm{e}}=\\mathcal{O}(S^{3}N^{-3})$ and $\\lambda_{\\mathrm{e}}=\\mathcal{O}(S^{\\frac{8}{5}}N^{-{\\frac{3}{5}}})$ for the noiseless and noisy cases, respectively. Thus, the upper bound provided in (10) converges at most at the rate of $O(S^{3}N^{-3})$ for the noiseless case and $\\mathcal{O}(S^{\\frac{8}{5}}N^{-\\frac{3}{5}})$ for the noisy case. ", "page_idx": 30}, {"type": "text", "text": "C Comparison with Berrut Coded Computing ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "C.1 Convergence rate ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "The upper bound of the infinity norm for the estimation provided in [29] for the coded computing scheme with $N$ workers and maximum of $S$ stragglers is as follows: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\left\\|\\hat{f}_{\\mathrm{BACC}}(t)-f\\circ u_{\\mathrm{enc}}(t)\\right\\|_{L^{\\infty}(\\Omega;\\mathbb{R})}\\leqslant2(1+R)\\sin\\left(\\frac{(S+1)\\pi}{2N}\\right)\\|f\\circ u_{\\mathrm{enc}}^{\\prime\\prime}(t)\\|_{L^{\\infty}(\\Omega;\\mathbb{R})}\\,,\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "if $N-s$ is odd, and ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\displaystyle\\Big\\|\\hat{f}_{\\mathtt{B A C C}}(t)-f\\circ u_{\\mathrm{enc}}(t)\\Big\\|_{L^{\\infty}(\\Omega;\\mathbb{R})}\\leqslant2(1+R)\\sin\\left(\\frac{(S+1)\\pi}{2N}\\right)\\left(\\|f\\circ u_{\\mathrm{enc}}^{\\prime\\prime}(t)\\|_{L^{\\infty}(\\Omega;\\mathbb{R})}\\right.}\\\\ {\\displaystyle\\left.+\\,\\|f\\circ u_{\\mathrm{enc}}^{\\prime}(t)\\|_{L^{\\infty}(\\Omega;\\mathbb{R})}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "if $N-s$ is even, where $\\begin{array}{r}{R=\\frac{(s+1)(s+3)\\pi^{2}}{4}}\\end{array}$ . Since $\\|\\cdot\\|_{L^{2}(\\Omega;\\mathbb{R})}$ is upper bounded by $\\lVert\\cdot\\rVert_{L^{\\infty}(\\Omega;\\mathbb{R})}$ , we can directly derive a convergence rate for the squared f\u02c6BACC(t) \u2212f \u25e6uenc(t)  2L2(\u2126;R) \u2a7d $\\Big\\|\\hat{f}_{\\mathtt{B A C C}}(t)-f\\circ u_{\\mathrm{enc}}(t)\\Big\\|_{L^{2}(\\Omega;\\mathbb{R})}^{2}\\leqslant\\mathscr{L}(\\Omega)\\cdot\\Big\\|\\hat{f}_{\\mathtt{B A C C}}(t)-f\\circ u_{\\mathrm{enc}}(t)\\Big\\|_{L^{\\infty}(\\Omega;\\mathbb{R})}^{2}\\leqslant\\mathscr{O}(S^{4}N^{-2}).$ $L^{2}\\left(\\Omega;\\mathbb{R}\\right)$ -norm of the error as $N$ increases: (129) ", "page_idx": 31}, {"type": "text", "text": "Compared to our results, the upper bound for LeTCC provided in Theorem 1, $\\mathcal{O}(S^{3}N^{-3})$ , is less sensitive to the number of stragglers and converges faster with increasing $N$ . Note that, since the $\\|\\cdot\\|_{L^{2}(\\Omega;\\mathbb{R})}$ is upper bounded by $\\lVert\\cdot\\rVert_{L^{\\infty}(\\Omega;\\mathbb{R})}$ , the statement above does not guarantee faster convergence of the proposed scheme compared to Berrut approach. ", "page_idx": 31}, {"type": "text", "text": "It should be noted that [29] does not analyze the noisy setting. ", "page_idx": 31}, {"type": "text", "text": "C.2 Computational complexity ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "From the experimental view, we compare the whole encoding and decoding time (on a single CPU machine) for LeTCC and BACC frameworks, as shown in the following table: ", "page_idx": 31}, {"type": "text", "text": "Table 2: Average and std of end-to-end processing time of LeTCC and BACC for different architectures ", "page_idx": 31}, {"type": "table", "img_path": "9XDYEEBRV6/tmp/befd9297cae8a4e32f56a6fe65f7cf72282443e8f38e71e0576377adba95b288.jpg", "table_caption": ["As shown in Table 2, the end-to-end processing time of the proposed framework is on par with BACC. "], "table_footnote": [], "page_idx": 31}, {"type": "text", "text": "D Comparison with Lagrange Coded Computing ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Although the only existing coded computing scheme for general functions is Berrut coded computing [29], with which we have compared our proposed scheme, other schemes are designed for specific computations, such as polynomial functions [3] and matrix multiplication [13]. To provide further comparison, we evaluate our proposed scheme against Lagrange coded computing (LCC) [3], which is specifically designed for polynomial computations, as follows: ", "page_idx": 31}, {"type": "text", "text": "D.1 Accuracy of function approximation ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "LCC is applicable only to polynomial computing functions [3]. Additionally, to enable recovery, the number of servers required must be at least $(K-1)\\times\\deg(f)+S+1$ worker nodes [3, 29]; otherwise, the master node cannot recover any results. Moreover, LCC is designed for computation over finite fields and encounters serious instability when computing over real numbers, particularly when $(K-1)\\times\\deg(f)$ is around 25 or higher [18, 29]. ", "page_idx": 31}, {"type": "text", "text": "We compare the proposed framework with LCC in Figure 5. Note that if $N<(K{-}1){\\times}\\deg(f){+}S{+}1$ , LCC cannot operate effectively. To adapt LCC for such cases, we approximate results by fitting a lower-degree polynomial to the available workers\u2019 outputs. We run LeTCC and LCC on the same set of input data and a fixed polynomial function for 20 trials, plotting the average performance and corresponding $95\\%$ confidence intervals in Figure 5. Figures 5a and 5b illustrate the performances of LCC and LeTCC for a low-degree polynomial and a small number of data points $(\\deg(f)\\,=\\,3$ and $K\\,=\\,5,$ ). In contrast, Figures 5c and 5d show performance for a higher-degree polynomial and a larger dataset $(\\deg(f)\\,=\\,15$ and $K\\,=\\,10$ ). As shown in Figures 5a and 5b, LCC achieves exact results for $S\\le7$ . However, at larger values of $S$ , as well as larger polynomial degree (as in Figures 5c and 5d), the proposed approach, without any parameter tuning, outperforms LCC in terms of both computational stability (lower variance) and recovery accuracy. ", "page_idx": 31}, {"type": "image", "img_path": "9XDYEEBRV6/tmp/b0a495ad9b661f777ed15e55b53c1d5eb0400ecbfd2a44fb5a45f334a85ef281.jpg", "img_caption": [], "img_footnote": [], "page_idx": 32}, {"type": "text", "text": "Figure 5: Average performance of $\\mathtt{L e T C C}$ and Lagrange Coded Computing, with a $95\\%$ confidence interval. Plots (a) and (d) show the overall performance, while the zoomed-in subplots (b) and (c) highlight the performance for smaller range of stragglers. ", "page_idx": 32}, {"type": "text", "text": "D.2 Computational complexity ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Encoding and decoding complexities in LCC are $\\mathcal{O}(N\\cdot\\log^{2}(K)\\cdot\\log\\log(K)\\cdot d)$ and $\\mathcal{O}((N-$ $S)\\cdot\\log^{2}((N-S))\\cdot\\log\\log((N-S))\\cdot m)$ , respectively, where $d$ and $m$ are input and output dimensions of the computing function $f(\\cdot)$ , respectively [3]. In contrast, as mentioned before, for smoothing splines, the encoding and decoding process, which involves evaluation on new points and calculating the fitted coefficients, have the computational complexity of $\\mathcal{O}(K.d)$ and $\\mathcal{O}((N-s).m)$ . Consequently, the computational complexity of the proposed scheme is less than LCC. ", "page_idx": 32}, {"type": "text", "text": "E Sensitivity Analysis ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "E.1 Sensitivity to number of stragglers ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "The smoothing parameters for each model show low sensitivity to the number of stragglers (or $\\frac{S}{N}$ arvgkagellru eernsso.  fdoTersh )eL.  efToNlole otf5win idwn itgth ht e iapmnaardla  RsmemeptoeVor,tG hwiGne g w uipstaehr ,rd o rsnessu pmdeibfcfteiervrsee lonyft. $(N,\\bar{K})=(100,6\\bar{0})$ $(N,K)=(60,20)$ As shown in Table 3, the optimal values of $\\lambda_{\\mathrm{e}}$ and $\\lambda_{\\mathrm{d}}$ exhibit low sensitivity to the number of stragglers. ", "page_idx": 32}, {"type": "text", "text": "Table 3: Optimal smoothing parameters for different number of stragglers for LeNet and RepVGG architectures. ", "page_idx": 32}, {"type": "table", "img_path": "9XDYEEBRV6/tmp/a4b7dbc57bbd1a9489b04ac45101e40218b9e9723c38debd92eebe8e304e5583.jpg", "table_caption": [], "table_footnote": [], "page_idx": 32}, {"type": "text", "text": "E.2 Sensitivity to smoothing parameters ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "To assess the performance of the proposed scheme with respect to the smoothing parameters, we vary each parameter individually around its optimal point while holding the other parameter fixed at their optimal value. We then record the average percentage increase in RMSE relative to the RMSE at the ", "page_idx": 32}, {"type": "text", "text": "optimal point. Figure 6 presents these results for LeNet with $(N,K,S)=(100,60,20)$ (Figures 6a and 6b) and for RepVGG with $(N,K,S)=(60,20,35)$ (Figures 6c and 6d). ", "page_idx": 33}, {"type": "image", "img_path": "9XDYEEBRV6/tmp/923eeff8ea3ecbe0b7eb28d77bbeb59be2d9ef993ba2842178f718c3e8a1cdbd.jpg", "img_caption": ["Figure 6: Sensitivity of LeTCC performance with respect to $\\log_{10}(\\lambda_{\\mathrm{d}})$ and $\\log_{10}(\\lambda_{\\mathrm{e}})$ . The yellow line represents the performance when the variable smoothing parameter is set to zero. "], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "As shown in Figure 6, the presence of more stragglers increases the sensitivity of LeTCC with respect to its smoothing parameter. However, even in a high-straggler regime, the RMSE increases by only around $3\\%$ when the smoothing parameter deviates from its optimal value by a scale of 10. ", "page_idx": 33}, {"type": "text", "text": "F High-dimensional computing function ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Let us consider more general cases where $\\boldsymbol{f}=[f_{1},\\dots,f_{m}]$ is a vector-valued function, where each component function $f_{j}:\\mathbb{R}\\to\\mathbb{R}$ is $q_{j}$ -Lipschitz continuous. Based on (2), we have: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\mathbb{R}(\\hat{\\mathbf{f}})\\leqslant\\operatorname*{\\mathbb{\\Phi}}_{\\epsilon,\\mathcal{F}\\sim F_{S,N}}\\left[\\frac{2}{K}\\sum_{k=1}^{K}\\|\\mathbf{u}_{\\mathrm{dec}}(\\alpha_{k})-\\mathbf{f}(u_{\\mathrm{enc}}(\\alpha_{k}))\\|_{2}^{2}\\right]+\\frac{2}{K}\\sum_{k=1}^{K}\\|\\mathbf{f}(u_{\\mathrm{enc}}(\\alpha_{k}))-\\mathbf{f}(x_{k})\\|_{2}^{2}\\,.}\\\\ &{\\displaystyle\\leqslant\\operatorname*{\\mathbb{\\Phi}}_{\\epsilon,\\mathcal{F}\\sim F_{S,N}}\\left[\\frac{2}{K}\\sum_{k=1}^{K}\\sum_{j=1}^{m}\\left(u_{\\mathrm{dec}_{j}}(\\alpha_{k})-f_{j}(u_{\\mathrm{enc}}(\\alpha_{k}))\\right)_{2}^{2}\\right]+\\frac{2\\sum_{j=1}^{m}q_{j}^{2}}{K}\\sum_{k=1}^{K}\\|u_{\\mathrm{enc}}(\\alpha_{k})-x_{k}\\|_{2}^{2}\\,}\\\\ &{\\displaystyle=\\sum_{j=1}^{m}\\epsilon\\frac{\\mathbb{E}}{\\mathcal{F}\\sim F_{S,N}}\\left[\\frac{2}{K}\\sum_{k=1}^{K}\\left(u_{\\mathrm{dec}_{j}}(\\alpha_{k})-f_{j}(u_{\\mathrm{enc}}(\\alpha_{k}))\\right)_{2}^{2}\\right]+\\frac{2\\sum_{j=1}^{m}q_{j}^{2}}{K}\\sum_{k=1}^{K}\\|u_{\\mathrm{enc}}(\\alpha_{k})-x_{k}\\|_{2}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Let us define the following objective for the decoder function: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbf{u}_{\\mathrm{dec}}^{\\star}=\\underset{\\mathbf{u}\\in\\mathcal{H}^{2}(\\Omega;\\mathbb{R}^{M})}{\\mathrm{argmin}}\\frac{1}{\\lvert\\mathcal{F}\\rvert}\\sum_{v\\in\\mathcal{F}}\\left\\lVert\\mathbf{u}\\left(\\beta_{v}\\right)-\\mathbf{f}\\left(u_{\\mathrm{enc}}\\left(\\beta_{v}\\right)\\right)\\right\\rVert_{2}^{2}+\\sum_{j=1}^{m}\\lambda_{\\mathrm{d}}\\int_{\\Omega}\\left(u_{j}^{\\prime\\prime}(t)\\right)^{2}\\,d t.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "The solution to (130), denoted as $\\mathbf{u}_{\\mathrm{dec}}^{\\star}$ , is a vector-valued function, where each component $u_{\\mathrm{dec}_{j}}(\\cdot)$ is a smoothing spline function fitted to the data points $\\{(\\beta_{v},f_{j}\\left(u_{\\mathrm{enc}}\\left(\\beta_{v}\\right)\\right))\\}_{v\\in\\mathcal{F}}$ . As a result, By defining $\\scriptstyle q\\ =\\ {\\sqrt{\\sum_{j=1}^{m}q_{j}^{2}}}$ and scaling up all upper bounds for $\\mathcal{L}_{\\mathrm{dec}}$ by a factor of $m$ , all previous results and theorems seamlessly extend to high-dimensional computing functions. ", "page_idx": 34}, {"type": "text", "text": "G Coded data points ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Figures 7b and 7c display coded samples generated by BACC and LeTCC, respectively, derived from the same initial data points depicted in Figure $^{7\\mathrm{a}}$ . These samples are presented for the MNIST dataset with parameters $(N,K)=(70,30)$ . From the figures, it is apparent (Specifically in paired ones that are shown with the same color) that while both schemes\u2019 coded samples are a weighted combination of multiple initial samples, BACC\u2019s coded samples exhibit high-frequency noise. This observation suggests that LeTCC regression functions produce more refined coded samples without any disruptive noise. ", "page_idx": 34}, {"type": "image", "img_path": "9XDYEEBRV6/tmp/f3feff02ea981233419eae3255c6e98a4b06fea5b2204598ceb95ec9c61f114b.jpg", "img_caption": ["(a) Initial inputs "], "img_footnote": [], "page_idx": 34}, {"type": "image", "img_path": "9XDYEEBRV6/tmp/6cba6a859381c543bbcfd24dc81553698555f3d720b683ac42116903c4b04a87.jpg", "img_caption": ["(b) BACC coded samples "], "img_footnote": [], "page_idx": 34}, {"type": "image", "img_path": "9XDYEEBRV6/tmp/74e91bc0903529e90186b05b2a180d17346f2aac0455b0eb7dae015910e9e9ef.jpg", "img_caption": ["(c) LeTCC coded samples "], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "Figure 7: Comparison of coded samples between BACC and LeTCC frameworks. Figure 7a represents the initial data points $\\{\\mathbf{x}_{k}\\}_{k=1}^{K}$ for $K\\,=\\,30$ . Figures 7b and 7c display $N\\,=\\,70$ coded samples $\\{\\widetilde{\\mathbf{x}}_{n}\\}_{n=1}^{N}$ from BACC and LeTCC, respectively. Samples with clear differences are highlighted with the same color. ", "page_idx": 34}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We detailed our contributions clearly in the abstract and the introduction sections of the paper. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 35}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification:We provided all the details regarding the assumptions, conditions, and limitations in the framework explanations (Section 3), theorems (Section 4), as well as the experiments section (Section 5) in the paper. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 35}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: Our paper includes theoretical results in Section 4. In our theorems, we clearly mentioned all the required assumptions, and a complete (and correct) proof of them is available in appendices (e.g., see Appendix B). Please see Section 3 for a full definition of the problem and introduction to the notations used in the paper. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 36}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We completely explained our proposed framework in Section 3 and we also provided details regarding our empirical evaluations in Section 3 in the paper. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in ", "page_idx": 36}, {"type": "text", "text": "some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 37}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 37}, {"type": "text", "text": "Answer: [No] ", "page_idx": 37}, {"type": "text", "text": "Justification: We provided references to all the open datasets that we used in the paper.   \nRegarding the code, we are happy to share it later if required. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so No is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 37}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: We provided full experimental details in the paper (see Section 5). ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 37}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: The details are provided in Section 5. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 37}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 38}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: We provided all the details regarding our experiments in Section 5. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 38}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: We followed the NeurIPS code of ethics in our paper. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 38}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: Our paper is focused on developing a new framework for coded distributed computing and should be categorized as foundational research. We believe this work has no direct societal impact that should be explained in the paper. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 38}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 39}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: This is not applicable to our work and this paper poses no such risks. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 39}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: This paper does not use existing assets. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 39}, {"type": "text", "text": "", "page_idx": 40}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 40}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: Our paper does not involve these. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 40}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: Our paper does not involve these. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 40}, {"type": "text", "text": "", "page_idx": 41}]