[{"figure_path": "RxkcroC8qP/figures/figures_0_1.jpg", "caption": "Figure 1: EEG/MEG-based zero-shot brain decoding and reconstruction. Left: Overview of three visual decoding tasks using EEG/MEG data under natural image stimulus. Right: Our reconstruction examples.", "description": "This figure illustrates the EEG/MEG-based zero-shot brain decoding and reconstruction framework. The left panel shows a schematic of the three visual decoding tasks (classification, retrieval, and generation) performed using EEG/MEG data obtained during natural image stimulation.  The right panel showcases example images that were reconstructed from EEG/MEG signals using the proposed framework. The figure visually represents the system's ability to decode and reconstruct visual information from brain activity.", "section": "Abstract"}, {"figure_path": "RxkcroC8qP/figures/figures_2_1.jpg", "caption": "Figure 2: EEG/MEG-based visual decoding and generation framework. The EEG encoder is designed as a flexible replacement component. After aligning with image features, the EEG features are used for zero-shot retrieval and classification tasks, and the reconstructed images are obtained through a two-stage generator.", "description": "This figure illustrates the overall framework for visual decoding and reconstruction using EEG/MEG data.  It shows how EEG signals are encoded using an EEG encoder (which is a flexible component and can be replaced with different architectures). The encoded EEG features are then aligned with image features (using CLIP). This alignment allows for zero-shot image classification and retrieval.  Furthermore, a two-stage generation process reconstructs images from the aligned EEG features,  involving a prior diffusion model for high-level features and a separate process for low-level features which are combined to produce the final reconstructed image. ", "section": "2 Method"}, {"figure_path": "RxkcroC8qP/figures/figures_3_1.jpg", "caption": "Figure 3: The structure of ATM. The original EEG sequences of different variates are independently embedded into tokens. Channel-wise attention is applied to embedded variate tokens with enhanced interpretability revealing electrode correlations. And representations of each token are extracted by the shared feedforward network (FFN). Then Temporal-Spatial convolution can prevent overfitting and enhance the ability of Temporal-Spatial modeling.", "description": "This figure details the architecture of the Adaptive Thinking Mapper (ATM), a novel EEG encoder.  It shows how raw EEG signals are processed through several stages: an embedding layer that converts the EEG data into tokens, a channel-wise attention layer to focus on relevant EEG channels, a temporal-spatial convolution module for capturing both temporal and spatial relationships, and finally an MLP projector to output a fixed-size EEG embedding.  The use of different components is highlighted, and the overall flow illustrates the process of converting raw EEG data into meaningful feature representations suitable for downstream tasks.", "section": "2.1 ATM for EEG Embedding"}, {"figure_path": "RxkcroC8qP/figures/figures_4_1.jpg", "caption": "Figure 4: EEG/MEG-based decoding and reconstruction performance. Left: Comparisons of nine encoders on the THINGS-EEG dataset, including within-subject and cross-subject performance. Right: Comparisons on the THINGS-MEG dataset, similar to left. Our method achieves the highest performance compared to other competing encoders in EEG/MEG-based visual decoding tasks.", "description": "This figure compares the performance of nine different EEG/MEG encoders on two datasets (THINGS-EEG and THINGS-MEG) for visual decoding tasks.  The left panel shows the results for the THINGS-EEG dataset, while the right panel shows the results for the THINGS-MEG dataset.  The visual decoding tasks include classification and retrieval, as well as image generation. The performance metrics shown are within-subject and cross-subject accuracy for classification and retrieval, and the quality of generated images.  The results demonstrate that the proposed ATM encoder outperforms the other eight encoders.", "section": "3 Experiments"}, {"figure_path": "RxkcroC8qP/figures/figures_5_1.jpg", "caption": "Figure 5: EEG-based image retrieval and classification. (a) The paradigm of EEG-based image retrieval and classification. (b) Samples of the top-5 accuracy in EEG-image retrieval tasks. See Appendix G for additional images results. (c) Average in-subject classification accuracy across different methods. (d) Average in-subject retrieval accuracy across different methods.", "description": "This figure shows the results of EEG-based image retrieval and classification experiments.  Panel (a) illustrates the experimental setup, showing how EEG signals are used to perform both classification (identifying the category of an image) and retrieval (finding images similar to a seen image). Panel (b) displays examples of the top-five most similar images retrieved for a given EEG signal. Panels (c) and (d) present bar graphs comparing the performance of different EEG encoding models across multiple experimental conditions (2-way, 4-way, 10-way classification/retrieval). These graphs show that the proposed ATM model performs the best in both classification and retrieval tasks compared to other existing methods.", "section": "3.2 EEG Decoding Performance"}, {"figure_path": "RxkcroC8qP/figures/figures_6_1.jpg", "caption": "Figure 6: EEG guidance image generation. (a) The paradigm of image generation. (b) The similarity between random visual objects and the EEG embeddings, and the similarity between generated visual objects and the target EEG embeddings. (c) Comparison between the original image and the image generated using the corresponding EEG data. (see Appendix C for details). (d) The similarity between visual objects and target EEG embeddings as the guidance scale changes, and the diversity of visual objects as the guidance scale changes. See Appendix G for additional results.", "description": "This figure illustrates the two-stage EEG-guided image generation pipeline.  Panel (a) shows the overall process: EEG data is fed into an encoder to generate EEG embeddings, which are then used by a two-stage generation process to create images.  The first stage generates prior CLIP embeddings based solely on EEG input, and the second stage uses both high-level CLIP embeddings and low-level image features from the EEG to refine the image using a pre-trained diffusion model. Panel (b) presents a comparison of similarity distributions between randomly selected image-EEG pairs and generated image-EEG pairs.  The similarity distributions are compared to highlight the effectiveness of the approach. Panel (c) shows example pairs of original images and their reconstructions. Finally, panel (d) shows how the balance between similarity and diversity of the generated images changes according to the guidance scale used in the generation process.", "section": "2.3 EEG Guidance Image Generation"}, {"figure_path": "RxkcroC8qP/figures/figures_7_1.jpg", "caption": "Figure 7: Performance of different EEG/MEG time windows on EEG-guided visual retrieval and reconstruction. (a) The retrieval accuracy of the expanding EEG windows at intervals [0, t] and at intervals [t-100, t] respectively. (b) The retrieval accuracy of the expanding MEG windows. (c) Samples reconstructed as the EEG window expands. When the EEG time window is greater than 200ms, the reconstructed image is stable. See Appendix H for more detailed explanations.", "description": "This figure analyzes the impact of different EEG/MEG time windows on visual decoding performance.  It shows the retrieval accuracy for expanding time windows, comparing two methods of selecting the time window ([0,t] and [t-100,t]).  The figure also displays reconstructed images at various time points, illustrating the stabilization of image quality above 200ms.  MEG data shows similar results but extends over a longer time period.", "section": "3.4 Temporal Analysis"}, {"figure_path": "RxkcroC8qP/figures/figures_7_2.jpg", "caption": "Figure 8: EEG-guided retrieval and reconstruction using EEG from different brain regions. (a) The EEG electrodes assigned to five brain regions. (b) Top-1 and top-5 retrieval accuracy, using only the EEG channels in each leaved region and all channels. (c) Reconstructed images obtained using only the electrode channels in each individual region and all channels.", "description": "This figure demonstrates the impact of different brain regions on EEG-based visual decoding and reconstruction.  Panel (a) shows how EEG electrodes are divided into five brain regions (frontal, central, temporal, parietal, occipital). Panel (b) presents the accuracy of top-1 and top-5 image retrieval, comparing performance when using EEG data from individual regions against using data from all regions combined. Panel (c) displays reconstructed images using EEG signals from each brain region separately and from all regions together, illustrating the contribution of each region to the overall reconstruction quality. The occipital region shows the best performance in both retrieval and reconstruction tasks.", "section": "3.5 Spatial Analysis"}, {"figure_path": "RxkcroC8qP/figures/figures_16_1.jpg", "caption": "Figure 4: EEG/MEG-based decoding and reconstruction performance. Left: Comparisons of nine encoders on the THINGS-EEG dataset, including within-subject and cross-subject performance. Right: Comparisons on the THINGS-MEG dataset, similar to left. Our method achieves the highest performance compared to other competing encoders in EEG/MEG-based visual decoding tasks.", "description": "This figure compares the performance of nine different EEG/MEG encoders on two datasets: THINGS-EEG and THINGS-MEG.  The left panel shows the performance on the THINGS-EEG dataset, broken down into within-subject and cross-subject results for image classification, retrieval (top 1 and top 5), and image generation.  The right panel shows similar results for the THINGS-MEG dataset.  The ATM-S encoder (the authors' method) consistently outperforms other encoders across various tasks and evaluation metrics.", "section": "3 Experiments"}, {"figure_path": "RxkcroC8qP/figures/figures_17_1.jpg", "caption": "Figure 10: Comparison between one-stage and two-stage EEG guidance image reconstruction. (a) We present the images that subjects seen (Seen), our reconstructed images directly using EEG embeddings (One-stage), and the reconstructed images from low level and high level image embeddings obtained by the prior diffusion (Two-stage). These results indicate that the strategy of our two-stage generation can better reconstruct the seen visual stimulus. (b) We employed ATM-S to compare the generated images with the original images in a retrieval task. Our result indicates that the images generated in two stages significantly enhance the performance of the original model on the retrieval task.", "description": "This figure compares the image reconstruction results using a one-stage method (directly from EEG embeddings) versus a two-stage method (using a prior diffusion model to generate image embeddings before reconstruction).  The two-stage approach shows improved results in reconstructing both semantic and low-level visual features of the original images, as demonstrated by visual examples and retrieval task performance improvements.", "section": "2.3 EEG Guidance Image Generation"}, {"figure_path": "RxkcroC8qP/figures/figures_19_1.jpg", "caption": "Figure 11: Example of our reconstructions for Subject 8 output from different pipelines. From left to right: reconstruction using only CLIP (i.e., Only CLIP), using only CLIP and the semantic pipeline (i.e., CLIP + Semantic), using only the low level pipeline (i.e., Only low level), using only CLIP and the low level pipeline (i.e., CLIP + low level), using joint CLIP, low level, and semantic pipelines.", "description": "This figure shows examples of image reconstructions for Subject 8 using different combinations of the CLIP, semantic, and low-level pipelines.  It demonstrates how using different components impacts the quality and details of the resulting images. The \"Seen Image\" column displays the original images shown to the subject. The remaining columns each show reconstructions using a different combination of image generation pipelines.", "section": "C.3 Low-level pipeline"}, {"figure_path": "RxkcroC8qP/figures/figures_21_1.jpg", "caption": "Figure 1: EEG/MEG-based zero-shot brain decoding and reconstruction. Left: Overview of three visual decoding tasks using EEG/MEG data under natural image stimulus. Right: Our reconstruction examples.", "description": "This figure provides a high-level overview of the proposed EEG/MEG-based zero-shot brain decoding and reconstruction framework.  The left panel illustrates the three main decoding tasks: classification, retrieval, and generation, showing how visual perception sequences are encoded into brain embeddings using EEG/MEG data. The right panel showcases example reconstructed images generated by the model, demonstrating its capability to translate brain activity into visual representations.", "section": "Abstract"}, {"figure_path": "RxkcroC8qP/figures/figures_22_1.jpg", "caption": "Figure 13: Visualization of the conceptual representation analysis. (a) Conceptual representations were obtained from EEG embeddings using concept encoder. (b) The similar matrix between EEG embeddings and real concept embeddings. (c) Concept embedding similarity matrix after cluster rearrangement (k=5).", "description": "This figure shows the results of a concept analysis performed on the EEG embeddings.  The left panel (a) illustrates the process:  EEG embeddings are fed into a concept encoder to produce 42-dimensional vectors representing different concepts. These are then compared to the actual concepts associated with the original images, revealing high similarity. Panel (b) shows a similarity matrix representing the correlation between the EEG-derived concept embeddings and the true concept embeddings, demonstrating a strong relationship. Panel (c) displays a clustered similarity matrix after applying k-means clustering (k=5) to the concept embeddings, showing improved clarity in the relationships.", "section": "2.3 EEG Guidance Image Generation"}, {"figure_path": "RxkcroC8qP/figures/figures_23_1.jpg", "caption": "Figure 14: Examples of EEG-guided visual reconstruction. From top to bottom, we exhibit the best, median, and worst 12 generated images, respectively. We show the images subjects seen and the generated images by our two-stage image generator.", "description": "This figure shows the results of the EEG-guided visual reconstruction. The figure is divided into three rows: best, median and worst. Each row contains 12 pairs of images showing the original image seen by subjects and the corresponding reconstructed images generated using the two-stage image generation model proposed in the paper. This visualization helps to understand the performance and quality of the image reconstruction model under different conditions.", "section": "G Additional images results"}, {"figure_path": "RxkcroC8qP/figures/figures_24_1.jpg", "caption": "Figure 15: Additional retrieval results", "description": "This figure shows additional retrieval results.  For each of several seen images, the top 10 most similar images retrieved by the model are shown.  This provides a visual demonstration of the model's performance on image retrieval.", "section": "G Additional images results"}, {"figure_path": "RxkcroC8qP/figures/figures_25_1.jpg", "caption": "Figure 14: Examples of EEG-guided visual reconstruction. From top to bottom, we exhibit the best, median, and worst 12 generated images, respectively. We show the images subjects seen and the generated images by our two-stage image generator.", "description": "This figure shows examples of images reconstructed from EEG data using the proposed two-stage image generation method.  The top row displays the original images that were shown to the subjects. The subsequent rows present the reconstructed images, categorized into \"Best,\" \"Median,\" and \"Worst\" based on their similarity to the original images. The figure visually demonstrates the model's ability to reconstruct images with varying degrees of accuracy, highlighting the potential and limitations of EEG-based image reconstruction.", "section": "G Additional images results"}, {"figure_path": "RxkcroC8qP/figures/figures_26_1.jpg", "caption": "Figure 14: Examples of EEG-guided visual reconstruction. From top to bottom, we exhibit the best, median, and worst 12 generated images, respectively. We show the images subjects seen and the generated images by our two-stage image generator.", "description": "This figure shows examples of images reconstructed from EEG data using the proposed two-stage image generation method.  The top row displays the original images ('Seen') that subjects viewed while their EEG data was recorded. Subsequent rows showcase reconstructed images, organized into groups representing the 'Best', 'Median', and 'Worst' reconstruction quality, with 12 examples in each group. The quality is assessed by comparing the CLIP embeddings of generated and original images. The figure aims to visually demonstrate the model's ability to reconstruct images with varying degrees of accuracy.", "section": "G Additional images results"}, {"figure_path": "RxkcroC8qP/figures/figures_27_1.jpg", "caption": "Figure 10: Comparison between one-stage and two-stage EEG guidance image reconstruction. (a) We present the images that subjects seen (Seen), our reconstructed images directly using EEG embeddings (One-stage), and the reconstructed images from low level and high level image embeddings obtained by the prior diffusion (Two-stage). These results indicate that the strategy of our two-stage generation can better reconstruct the seen visual stimulus. (b) We employed ATM-S to compare the generated images with the original images in a retrieval task. Our result indicates that the images generated in two stages significantly enhance the performance of the original model on the retrieval task.", "description": "This figure compares the image reconstruction results of one-stage and two-stage EEG-guided image generation methods.  The two-stage approach, using a prior diffusion model to generate image embeddings from EEG data before generating the final image, shows better reconstruction of both semantic and low-level visual features compared to a single-stage method which directly generates images from EEG data.  A retrieval task using ATM-S shows that the two-stage method also achieves a significant improvement in retrieval accuracy.", "section": "2.3 EEG Guidance Image Generation"}, {"figure_path": "RxkcroC8qP/figures/figures_28_1.jpg", "caption": "Figure 4: EEG/MEG-based decoding and reconstruction performance. Left: Comparisons of nine encoders on the THINGS-EEG dataset, including within-subject and cross-subject performance. Right: Comparisons on the THINGS-MEG dataset, similar to left. Our method achieves the highest performance compared to other competing encoders in EEG/MEG-based visual decoding tasks.", "description": "This figure compares the performance of nine different EEG/MEG encoders on two datasets (THINGS-EEG and THINGS-MEG) across three visual decoding tasks: classification, retrieval, and generation.  The left panel shows the results for the THINGS-EEG dataset, while the right panel presents the results for the THINGS-MEG dataset. The performance is evaluated for both within-subject (the same subject for training and testing) and cross-subject settings (training on a subset of subjects, testing on the remaining subjects).  The figure highlights that the proposed ATM encoder consistently outperforms other state-of-the-art encoders across both datasets and evaluation settings.", "section": "3 Experiments"}, {"figure_path": "RxkcroC8qP/figures/figures_28_2.jpg", "caption": "Figure 1: EEG/MEG-based zero-shot brain decoding and reconstruction. Left: Overview of three visual decoding tasks using EEG/MEG data under natural image stimulus. Right: Our reconstruction examples.", "description": "This figure demonstrates the EEG/MEG-based zero-shot brain decoding and reconstruction framework. The left panel shows a schematic of the three main visual decoding tasks (classification, retrieval, and generation) using EEG/MEG data obtained during natural image viewing. The right panel showcases example images reconstructed by the model.", "section": "Abstract"}, {"figure_path": "RxkcroC8qP/figures/figures_29_1.jpg", "caption": "Figure 14: Examples of EEG-guided visual reconstruction. From top to bottom, we exhibit the best, median, and worst 12 generated images, respectively. We show the images subjects seen and the generated images by our two-stage image generator.", "description": "This figure shows examples of images reconstructed from EEG data using the proposed two-stage image generation method.  The top row displays the original images seen by the subjects.  The subsequent rows show the best, median, and worst performing reconstructions, ranked according to their similarity to the original images. This visualization demonstrates the model's ability to generate images that capture the visual stimulus but also highlights the variability and limitations of the reconstruction process.", "section": "G Additional images results"}, {"figure_path": "RxkcroC8qP/figures/figures_29_2.jpg", "caption": "Figure 4: EEG/MEG-based decoding and reconstruction performance. Left: Comparisons of nine encoders on the THINGS-EEG dataset, including within-subject and cross-subject performance. Right: Comparisons on the THINGS-MEG dataset, similar to left. Our method achieves the highest performance compared to other competing encoders in EEG/MEG-based visual decoding tasks.", "description": "This figure compares the performance of nine different EEG/MEG encoders on two datasets (THINGS-EEG and THINGS-MEG) for three visual decoding tasks: classification, retrieval, and generation.  The left panel shows the results for the THINGS-EEG dataset, while the right panel shows results for the THINGS-MEG dataset.  The performance is evaluated across different scenarios (within-subject and cross-subject) for each task.  The figure highlights that the proposed ATM encoder in this paper outperforms other encoders.", "section": "3 Experiments"}, {"figure_path": "RxkcroC8qP/figures/figures_30_1.jpg", "caption": "Figure 14: Examples of EEG-guided visual reconstruction. From top to bottom, we exhibit the best, median, and worst 12 generated images, respectively. We show the images subjects seen and the generated images by our two-stage image generator.", "description": "This figure shows examples of images reconstructed from EEG data using a two-stage generation method.  The top row displays the original images seen by the subjects. The middle and bottom rows show the reconstructed images, categorized as \"best,\" \"median,\" and \"worst\" based on their similarity to the originals. The figure demonstrates the effectiveness of the method in reconstructing images with varying degrees of accuracy.", "section": "3.3 Image Generation Performance"}, {"figure_path": "RxkcroC8qP/figures/figures_30_2.jpg", "caption": "Figure 14: Examples of EEG-guided visual reconstruction. From top to bottom, we exhibit the best, median, and worst 12 generated images, respectively. We show the images subjects seen and the generated images by our two-stage image generator.", "description": "This figure shows examples of images reconstructed from EEG data using the proposed two-stage method.  The top row shows the original images seen by the subjects. The rows below show 12 examples each of the best, median, and worst reconstructions, ranked by similarity to the original images.  The quality of the reconstruction varies considerably, with the best reconstructions closely resembling the originals and the worst reconstructions showing little resemblance.", "section": "G Additional images results"}, {"figure_path": "RxkcroC8qP/figures/figures_31_1.jpg", "caption": "Figure 14: Examples of EEG-guided visual reconstruction. From top to bottom, we exhibit the best, median, and worst 12 generated images, respectively. We show the images subjects seen and the generated images by our two-stage image generator.", "description": "This figure shows example image reconstruction results using EEG data.  The top row shows the original images viewed by subjects.  The subsequent rows display reconstruction results categorized as 'Best', 'Median', and 'Worst' based on a similarity metric comparing the generated images to the original images. The figure illustrates the varying degrees of success in reconstructing the images from EEG data, ranging from high-fidelity matches to blurry or semantically incorrect results.", "section": "G Additional images results"}, {"figure_path": "RxkcroC8qP/figures/figures_31_2.jpg", "caption": "Figure 4: EEG/MEG-based decoding and reconstruction performance. Left: Comparisons of nine encoders on the THINGS-EEG dataset, including within-subject and cross-subject performance. Right: Comparisons on the THINGS-MEG dataset, similar to left. Our method achieves the highest performance compared to other competing encoders in EEG/MEG-based visual decoding tasks.", "description": "This figure compares the performance of nine different EEG encoders on two datasets (THINGS-EEG and THINGS-MEG) across three visual decoding tasks: classification, retrieval, and generation.  The left panel shows results for the THINGS-EEG dataset, while the right panel shows results for the THINGS-MEG dataset.  The performance is evaluated using within-subject and cross-subject metrics. The figure highlights that the proposed ATM encoder significantly outperforms other encoders.", "section": "3 Experiments"}, {"figure_path": "RxkcroC8qP/figures/figures_32_1.jpg", "caption": "Figure 14: Examples of EEG-guided visual reconstruction. From top to bottom, we exhibit the best, median, and worst 12 generated images, respectively. We show the images subjects seen and the generated images by our two-stage image generator.", "description": "This figure shows examples of images reconstructed from EEG data using a two-stage image generation method. The top row displays the original images that were shown to the subjects, and the bottom row shows the images reconstructed by the model. The images are grouped into three categories: best, median, and worst, based on how well the reconstructed images match the originals.", "section": "G Additional images results"}, {"figure_path": "RxkcroC8qP/figures/figures_32_2.jpg", "caption": "Figure 14: Examples of EEG-guided visual reconstruction. From top to bottom, we exhibit the best, median, and worst 12 generated images, respectively. We show the images subjects seen and the generated images by our two-stage image generator.", "description": "This figure shows examples of images reconstructed from EEG data using the proposed two-stage method. The top row displays the original images that subjects saw, while the subsequent rows illustrate the best, median, and worst reconstruction results based on the similarity between generated and original images.  The results highlight the model's ability to reconstruct images with varying levels of accuracy, capturing both semantic and low-level details in successful reconstructions, and failing to do so in less successful instances.", "section": "G Additional images results"}, {"figure_path": "RxkcroC8qP/figures/figures_33_1.jpg", "caption": "Figure 4: EEG/MEG-based decoding and reconstruction performance. Left: Comparisons of nine encoders on the THINGS-EEG dataset, including within-subject and cross-subject performance. Right: Comparisons on the THINGS-MEG dataset, similar to left. Our method achieves the highest performance compared to other competing encoders in EEG/MEG-based visual decoding tasks.", "description": "This figure compares the performance of nine different EEG encoders on two datasets: THINGS-EEG and THINGS-MEG.  The left side shows the performance on THINGS-EEG, broken down into within-subject and across-subject performance for image classification and retrieval tasks, as well as image generation. The right side shows similar comparisons using the THINGS-MEG dataset. The key takeaway is that the proposed ATM encoder significantly outperforms all other encoders across all tasks and datasets, highlighting its effectiveness in visual decoding using EEG/MEG data.", "section": "3 Experiments"}, {"figure_path": "RxkcroC8qP/figures/figures_34_1.jpg", "caption": "Figure 4: EEG/MEG-based decoding and reconstruction performance. Left: Comparisons of nine encoders on the THINGS-EEG dataset, including within-subject and cross-subject performance. Right: Comparisons on the THINGS-MEG dataset, similar to left. Our method achieves the highest performance compared to other competing encoders in EEG/MEG-based visual decoding tasks.", "description": "This figure compares the performance of nine different EEG/MEG encoders on two datasets, THINGS-EEG and THINGS-MEG, for three visual decoding tasks (classification, retrieval, and generation).  The left panel shows the results for THINGS-EEG, while the right panel displays the results for THINGS-MEG.  The performance is evaluated using within-subject and cross-subject metrics for classification and retrieval, as well as by top-1 and top-5 accuracy for image generation. The results indicate that the proposed ATM encoder significantly outperforms other methods, demonstrating the effectiveness of the proposed approach for EEG/MEG-based visual decoding.", "section": "3 Experiments"}, {"figure_path": "RxkcroC8qP/figures/figures_34_2.jpg", "caption": "Figure 4: EEG/MEG-based decoding and reconstruction performance. Left: Comparisons of nine encoders on the THINGS-EEG dataset, including within-subject and cross-subject performance. Right: Comparisons on the THINGS-MEG dataset, similar to left. Our method achieves the highest performance compared to other competing encoders in EEG/MEG-based visual decoding tasks.", "description": "This figure compares the performance of different EEG encoders on two datasets: THINGS-EEG and THINGS-MEG.  The left panel shows results for the THINGS-EEG dataset, broken down by within-subject and cross-subject performance for several decoding tasks (classification and retrieval).  The right panel repeats the analysis but for the THINGS-MEG dataset. The figure demonstrates the superiority of the authors' proposed method (ATM-S) across all tasks and datasets compared to other state-of-the-art encoders.", "section": "3 Experiments"}, {"figure_path": "RxkcroC8qP/figures/figures_35_1.jpg", "caption": "Figure 4: EEG/MEG-based decoding and reconstruction performance. Left: Comparisons of nine encoders on the THINGS-EEG dataset, including within-subject and cross-subject performance. Right: Comparisons on the THINGS-MEG dataset, similar to left. Our method achieves the highest performance compared to other competing encoders in EEG/MEG-based visual decoding tasks.", "description": "This figure compares the performance of nine different EEG encoders on two datasets: THINGS-EEG and THINGS-MEG.  The left panel shows results for the THINGS-EEG dataset, while the right panel shows results for the THINGS-MEG dataset.  The performance is evaluated across three visual decoding tasks: classification, retrieval (top-1 and top-5), and generation (top-1 and top-5).  The results demonstrate that the proposed ATM encoder outperforms all other encoders across both datasets and across all three tasks. ", "section": "3 Experiments"}]