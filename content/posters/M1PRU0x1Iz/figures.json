[{"figure_path": "M1PRU0x1Iz/figures/figures_3_1.jpg", "caption": "Figure 1: Overview of FedAvP. (a) The server sends global model parameters wg, and policy parameters \u03b8g to clients. Clients train local models with augmented data, and the server aggregates them to compute Wgr+1. Clients update policies on Wgr+1 using validation data, and the server aggregates these policies. (b) Clients update the model and policy parameters via first-order approximation. The server aggregates client updates to form the updated global model Wgr+1 and policy parameters \u03b8gr+1.", "description": "This figure illustrates the FedAvP algorithm's workflow.  Panel (a) shows the federated meta-policy optimization, where the server distributes global model parameters and policy parameters to clients. Clients then augment their local data and update their local models. These updates are aggregated at the server to produce the updated global model. Next, clients use the updated global model to update their local policies, which are aggregated at the server. Panel (b) presents a simplified first-order approximation of this process, to improve privacy and reduce communication overhead.", "section": "3 Approach: FedAvP"}, {"figure_path": "M1PRU0x1Iz/figures/figures_3_2.jpg", "caption": "Figure 1: Overview of FedAvP. (a) The server sends global model parameters w<sub>gr</sub>, and policy parameters \u03b8<sub>gr</sub> to clients. Clients train local models with augmented data, and the server aggregates them to compute w<sub>gr+1</sub>. Clients update policies on w<sub>gr+1</sub> using validation data, and the server aggregates these policies. (b) Clients update the model and policy parameters via first-order approximation. The server aggregates client updates to form the updated global model w<sub>gr+1</sub> and policy parameters \u03b8<sub>gr+1</sub>.", "description": "This figure illustrates the FedAvP algorithm's workflow.  Panel (a) shows the federated meta-policy loss method, where the server distributes global model and policy parameters to clients. Clients train locally on augmented data, updating both their local models and policies. These are then aggregated by the server to update the global model and policy. Panel (b) shows a first-order approximation of this process, simplifying the calculation and reducing communication costs by approximating the meta-policy gradient.", "section": "3 Approach: FedAvP"}, {"figure_path": "M1PRU0x1Iz/figures/figures_5_1.jpg", "caption": "Figure 1: Overview of FedAvP. (a) The server sends global model parameters wgr, and policy parameters \u03b8gr to clients. Clients train local models with augmented data, and the server aggregates them to compute wgr+1. Clients update policies on wgr+1 using validation data, and the server aggregates these policies. (b) Clients update the model and policy parameters via first-order approximation. The server aggregates client updates to form the updated global model wgr+1 and policy parameters \u03b8gr+1.", "description": "This figure illustrates the FedAvP algorithm's workflow.  Panel (a) shows the federated meta-policy optimization where the server distributes global model and policy parameters to clients, who then locally train models with augmented data and send updates back for aggregation. Panel (b) depicts a faster, first-order approximation of this process, still involving local model and policy updates followed by aggregation on the server.", "section": "3 Approach: FedAvP"}, {"figure_path": "M1PRU0x1Iz/figures/figures_7_1.jpg", "caption": "Figure 2: Visualization of global policies learned in CIFAR-100, SVHN and FEMNIST.", "description": "This figure visualizes the learned global data augmentation policies for CIFAR-100, SVHN, and FEMNIST datasets. Each subfigure represents a heatmap showing the probability of selecting pairs of augmentation operations. The brighter the color, the higher the probability. This visualization helps understand how the algorithm learns different augmentation strategies for different datasets.", "section": "4 Experiments"}, {"figure_path": "M1PRU0x1Iz/figures/figures_7_2.jpg", "caption": "Figure 3: Statistics of personalized policies between different clients on CIFAR-100.", "description": "This figure shows the Euclidean distances between the personalized policies of clients participating in each round and the global policy for that round on CIFAR-100.  With \u03b1 = 5.0 (i.i.d. data), the personalized policies of clients tend not to deviate from the global policy. With \u03b1 = 0.1 (non-i.i.d. data), the deviation from the global policy is initially high but decreases as training progresses, particularly after about 100 rounds. The variance of the Euclidean distances also follows this pattern.  This illustrates how FedAvP adapts policies to heterogeneous data distributions.", "section": "4.3 Policy Adaptation on Clients"}, {"figure_path": "M1PRU0x1Iz/figures/figures_16_1.jpg", "caption": "Figure 4: Results of the reconstruction attacks in Table 2. The first row of each result represents the random client\u2019s training samples, and the second row is the reconstructed samples by the server. We visualized high-PSNR samples selected from random samples. The numbers below indicate the PSNR values of the reconstructed samples.", "description": "This figure shows the results of reconstruction attacks from Table 2.  For each method (FedAvg, FedGen, FedMix, FedFA, FedAvP, and the defense method ATSPrivacy), it displays the original samples from a client with a small amount of data (Client(S)) and a client with a large amount of data (Client(L)), as well as the samples reconstructed by the server using the gradient information. The PSNR (Peak Signal-to-Noise Ratio) values, which represent the image quality of the reconstructed samples relative to the original samples, are also shown.", "section": "4.4 Reconstruction Attack"}, {"figure_path": "M1PRU0x1Iz/figures/figures_16_2.jpg", "caption": "Figure 2: Visualization of global policies learned in CIFAR-100, SVHN and FEMNIST.", "description": "This figure visualizes the global policies learned by FedAvP for three different datasets: CIFAR-100, SVHN, and FEMNIST. Each subfigure shows a heatmap representing the probability of selecting different pairs of augmentation operations.  The heatmaps reveal the distinct augmentation strategies learned for each dataset, highlighting the algorithm's ability to adapt to heterogeneous data distributions.", "section": "4 Experiments"}, {"figure_path": "M1PRU0x1Iz/figures/figures_17_1.jpg", "caption": "Figure 5: Training loss convergence of our FedAvP algorithm", "description": "The figure shows the training loss convergence curves for the proposed FedAvP algorithm across different datasets and heterogeneity levels (alpha values). The x-axis represents the training round, and the y-axis represents the training loss. The curves illustrate how the training loss decreases over time for FedAvP, demonstrating its effectiveness in optimizing the augmentation policies for federated learning.", "section": "4 Experiments"}, {"figure_path": "M1PRU0x1Iz/figures/figures_17_2.jpg", "caption": "Figure 5: Training loss convergence of our FedAvP algorithm", "description": "The figure shows the training loss curves for the FedAvP algorithm across different datasets and heterogeneity levels (alpha values).  It demonstrates the convergence behavior of the algorithm's training loss over multiple rounds, providing insight into its training dynamics and stability. The different lines represent the training loss for each dataset and heterogeneity setting.", "section": "Experiments"}, {"figure_path": "M1PRU0x1Iz/figures/figures_17_3.jpg", "caption": "Figure 5: Training loss convergence of our FedAvP algorithm", "description": "The figure shows the training loss convergence curves for the FedAvP algorithm across four different datasets and heterogeneity levels.  The x-axis represents the training round, while the y-axis shows the training loss.  The plots illustrate how the training loss decreases over time for each dataset and heterogeneity setting, providing insights into the convergence behavior of the FedAvP algorithm in various scenarios.", "section": "4 Experiments"}]