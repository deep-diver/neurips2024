[{"type": "text", "text": "FedAvP: Augment Local Data via Shared Policy in Federated Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Minui Hong\u2217 Junhyeog Yun\u2217 Insu Jeon\u2020 Gunhee Kim\u2217 Seoul National University, Seoul, South Korea \u2217{alsdml123,junhyeog,gunhee}@snu.ac.kr \u2020{insuj3on}@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Federated Learning (FL) allows multiple clients to collaboratively train models without directly sharing their private data. While various data augmentation techniques have been actively studied in the FL environment, most of these methods share input-level or feature-level data information over communication, posing potential privacy leakage. In response to this challenge, we introduce a federated data augmentation algorithm named FedAvP that shares only the augmentation policies, not the data-related information. For data security and efficient policy search, we interpret the policy loss as a meta update loss in standard FL algorithms and utilize the first-order gradient information to further enhance privacy and reduce communication costs. Moreover, we propose a meta-learning method to search for adaptive personalized policies tailored to heterogeneous clients. Our approach outperforms existing best performing augmentation policy search methods and federated data augmentation methods, in the benchmarks for heterogeneous FL. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Federated Learning (FL) is a collaborative learning approach that allows multiple clients to learn without sharing their private information [1\u20136]. A central server coordinates the training process across multiple devices and aggregates the locally trained models into a global one; thus reducing the communication cost of exchanging raw data and mitigating the risk of privacy leakage associated with data sharing [7]. ", "page_idx": 0}, {"type": "text", "text": "However, the limited accessibility of data in FL still poses many challenges, such as insufficient training data and local data bias. To address these challenges, there has been a growing interest in federated data augmentation techniques [8\u201312]. They aim to increase the diversity and volume of data available at each client, thereby improving the overall robustness and performance of the federated models. For example, FedMix [8] improves performance and privacy by averaging multiple images to facilitate data mixup among clients. Similarly, FedFA [11] utilizes feature statistics to mitigate local data biases, to improve model generalization. Despite their beneftis, these methods often apply the sharing of input-level [8, 10, 12] or feature-level [13, 11] information. Such information sharing poses additional privacy concerns since malicious attackers could potentially reconstruct original data by applying gradient matching loss [14] on the additional input and feature information. ", "page_idx": 0}, {"type": "text", "text": "We propose a novel federated data augmentation algorithm named FedAvP (Augmet Local Data via Shared Policy in Federated Learning), which shares only the augmentation policies during training. Thus, each client does not need to share its own data or data-related information directly but obtains collective knowledge on how to augment the dataset at local learning. Previously, AutoAugment [15] utilizes reinforcement learning (RL) to automatically find the optimal data augmentation policy for a target dataset. While AutoAugment requires extensive GPU resources, more efficient and faster policy search has been studied [16\u201320]. However, these methods are not designed for FL environments but they perform policy search for public batch datasets. The data scarcity and heterogeneity in FL could fail these standard policy search frameworks. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To improve the robustness and generalization of model training in the heterogeneous FL setting, we first introduce a Federated Meta-Policy Loss (FMPL) specifically designed to compute a gradient of augmentation policy that updates a shared data augmentation policy for each client\u2019s unique environment. Our approach guides the policy gradient to account for the effects of data augmentation on the unseen local data. The policy gradient utilizes higher-order information; the impact of the data augmentation is estimated by its effect on the validation loss observed after a few gradient descent steps with the initial augmented data. However, computing the direct meta-policy gradient in FL requires an additional communication step between the server and clients. To bypass this, we also develop an alternative meta-policy search method that utilizes a first-order approximation. We further demonstrate that the adaptive policy search technique can adapt to heterogeneous data distributions among clients in the FL environment. ", "page_idx": 1}, {"type": "text", "text": "In the experiments, our FedAvP demonstrates superior performance on CIFAR-10/100 [21], SVHN [22], and FEMNIST [23] datasets within an FL context, compared to existing federated learning algorithms, including FedAvg [2], FedProx [4], FedDyn [5], FedExP [6], and federated data augmentation algorithms, including FedGen [9], FedMix [8], and FedFA [11]. Moreover, to further leverage the potential performance of these algorithms, we also conducted experiments applying data augmentation techniques such as RandAugment [17] and TrivialAugment [24] to these algorithms for comparison. We also evaluate our algorithm in environments where data is non-i.i.d. with heterogeneous clients [25]. We further compare the effectiveness of the utility of sharing this policy across clients for searching, in contrast to conducting a local policy search. ", "page_idx": 1}, {"type": "text", "text": "Our primary contributions are as follows. ", "page_idx": 1}, {"type": "text", "text": "1. We propose FedAvP(Augment Local Data via Shared Policy in Federated Learning) as the first algorithm in federated learning that facilitates shared augmentation policies among clients for federated policy search, to the best of our knowledge.   \n2. We introduce the federated meta-policy loss for effective policy search, and further propose a first-order approximation to this loss to enhance privacy and reduce communication costs.   \n3. Enabling meta-learning, our algorithm allows for rapid adaptation of a personalized policy by each client, addressing the challenge of highly heterogeneous data distributions among clients in the FL environment. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Automated Data Augmentation. AutoAugment [15] utilizes RL to find an optimal data augmentation policy for a target dataset automatically. FastAA [16] proposes a more efficient search strategy by training small NNs in parallel without iterative training, using the density matching method. RandAugment [17] suggests a simplified search method composed of two hyper-parameters, which find the augmentation policy without a separate search process. TrivialAugment [24] further simplifies the algorithm and applies a single augmentation to each image as a parameter-free method. MetaAugment [19] proposes a sample-aware augmentation policy network to capture the variability of training samples more accurately than previous dataset-based search methods. Deep AutoAugment [18] proposes a fully automated search method that builds a multi-layer data augmentation pipeline from scratch by stacking augmentation layers. All of these recent data augmentation methods were developed under the assumption that all training data is accessible on a server. This assumption is invalid in FL since data privacy is a significant concern and it is challenging to tune these algorithms for each of the numerous heterogeneous local datasets. Therefore, our study aims to develop a new data augmentation policy search algorithm that takes into account the distributed FL process while preserving data security. RandAugment [17] and TrivialAugment [24] can be applied simultaneously to many clients in a federated learning environment, so we compared these methods in our experiments. ", "page_idx": 1}, {"type": "text", "text": "Federated Data Augmentation. The standard Federated Learning (FL) framework, such as FedAvg [2], typically performs iterative local model updates at each client and a global update at a server. Since the clients and server only communicate through the model parameters instead of raw data, it enables secured and decentralized learning [1, 3]. Despite its beneftis, FL still has challenges such as convergence degradation and model overftiting due to heterogeneity and sparsity among the data caused by the differences in client\u2019s actions and preferences [25]. To address these challenges, federated data augmentation (FDA) employs a data augmentation approach instead of a model-centric approach. FedMix [8] applies Mixup [26] to FL, augmenting data by linear interpolation of two random training examples and their labels. For privacy reasons, FedMix transfers mixup data to the server by averaging multiple images from the local device. In FedGen [9], the server learns a lightweight generator to ensemble user information, which is then broadcasted to users to regulate local training using the learned knowledge. FedFA [11] assumes that the data distribution of the clients can be summarized by the statistics of the latent features (i.e., mean and standard deviation). This allows learning local models by regularizing the gradients of the latent representations, weighted by the variances of the feature statistics estimated from the entire client federation. StatMix [13] sends the mean and standard deviation information of local client images to the server and makes it available for learning for each client. These methods use input-level data (image) averaging or feature-level statistics to prevent direct data transfer. Unlike previous methods, our methodology focuses on transmitting only the policy information optimized for local datasets from each client. ATSPriavacy [27] has demonstrated that searching for transformation policies can also protect against reconstruction attacks in Federated Learning (FL), while preserving performance. We compare our algorithm with this approach in terms of both vulnerability to reconstruction attacks and performance in the experimental section ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "3 Approach: FedAvP ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We introduce FedAvP (Augment Local Data via Shared Policy in Federated Learning), which performs data augmentation search by sharing policies among clients in a federated learning (FL) environment. Starting from the problem formulation (\u00a73.1), we address the challenges of heterogeneous clients with a proposal for adaptive policy search $(\\S3.2)$ . Finally, we extend its applicability through integration with the FedAvg algorithm [1] and joint learning $(\\S3.3)$ . ", "page_idx": 2}, {"type": "text", "text": "3.1 Problem Formulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Our approach is based on the idea of centralized FL [7]; it is not possible to share personal data neither between the server and clients nor among clients. This is different from traditional automated policy search algorithms, which find the optimal policy in a single batch dataset [15, 16, 20, 19, 18]. ", "page_idx": 2}, {"type": "text", "text": "Objective. We begin with the standard FL algorithm, FedAvg [1], aiming to find augmentation policies that minimize a given objective function as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{Model}\\colon\\operatorname*{min}_{w}\\sum_{k=1}^{K}\\alpha_{k}\\ell\\left(w;t_{p_{\\theta}}\\big(D_{k}^{\\mathrm{train}}\\big)\\right),\\quad\\mathrm{Policy}\\colon\\operatorname*{min}_{p_{\\theta}}\\sum_{k=1}^{K}\\alpha_{k}\\ell\\left(w;D_{k}^{\\mathrm{val}}\\right).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "$K$ is the number of clients used to train the global model $w$ , and $D_{k}$ is the local data of client $k$ . The transformation policy $p_{\\theta}$ is used to augment the data denoted by $t_{p_{\\theta}}(D_{k}^{\\mathrm{train}})$ with coefficients $\\alpha_{k}$ satisfying $\\sum\\alpha_{k}\\stackrel{\\_}{=}1$ and $\\alpha_{k}~\\geq~0$ . Assuming client $k$ has $n_{k}$ data samples, $\\alpha_{k}$ is defined as $\\alpha_{k}\\ =\\ {\\frac{n_{k}}{n}}$ wh ere $n\\,=\\,\\sum_{k}n_{k}$ . Under a global policy assumption, all clients share the same $p_{\\theta}$ Alternatively, if we as sume that each client has its own transformation policy $p_{\\theta^{k}}^{\\mathrm{local}}$ , we represent $p_{\\theta}^{\\mathrm{local}}=\\{p_{\\theta^{1}}^{\\mathrm{local}},p_{\\theta^{2}}^{\\mathrm{local}},\\dots,p_{\\theta^{K}}^{\\mathrm{local}}\\}$ . ", "page_idx": 2}, {"type": "text", "text": "Search Space. We use the augmentation space having a sequence of two operations, following the search space from previous studies on automated policy search [17, 19]. We examine a set of 17 operations in total, including {Identity, ShearX/Y, TranslateX/Y, Rotate, AutoContrast, Equalize, Solarize, Posterize, Contrast, Color, Brightness, Sharpness, RandFlip, RandCutout, RandCrop}. Each operation includes a random magnitude, rescaled and uniformly sampled from the normalized interval $[0,1]$ . Assuming each of the $K$ clients has individual policy, the entire search space consists of $K$ joint distributions, each having a size of $17\\times17$ . For a global policy, we learn a single $17\\times17$ dimensional joint distribution corresponding to the two operations. Specifically, $p_{\\theta}$ has a value in $[0,1]$ using the sigmoid function on $\\theta$ . For augmentation sampling, we normalize the policy parameter vector $p_{\\theta}$ whose sum to be one, and sample from a joint distribution with a regularization term $\\epsilon$ . Operation pairs $(o p1,o p2)$ are drawn from a mixed distribution: ", "page_idx": 2}, {"type": "equation", "text": "$$\n(o p1,o p2)\\sim(1-\\epsilon)\\cdot\\frac{p_{\\theta}}{\\sum(p_{\\theta})}+\\epsilon\\cdot\\frac{1}{17^{2}}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "image", "img_path": "M1PRU0x1Iz/tmp/b8b5d70813574a1d4d3b4381532484662145d40094556b5a41ee81d25ecaec5f.jpg", "img_caption": ["(a) Federated policy optimization using our FMPL "], "img_footnote": [], "page_idx": 3}, {"type": "image", "img_path": "M1PRU0x1Iz/tmp/4b516909065f31ce642a3d942859905f064c2c82dee130fb092fac35efe2744a.jpg", "img_caption": ["(b) First-order approximation of federated policy optimization "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 1: Overview of FedAvP. (a) The server sends global model parameters $w_{g_{r}}$ and policy parameters $\\theta_{g_{r}}$ to clients. Clients train local models with augmented data, and the server aggregates them to compute $w_{g_{r+1}}$ . Clients update policies on $w_{g_{r+1}}$ using validation data, and the server aggregates these policies. (b) Clients update the model and policy parameters via first-order approximation. The server aggregates client updates to form the updated global model $w_{g_{r+1}}$ and policy parameters $\\theta_{g_{r+1}}$ . ", "page_idx": 3}, {"type": "text", "text": "This sampling strategy utilizes a uniform probability across all operation pairs for a balanced exploration and exploitation, as adopted in previous work [19]. ", "page_idx": 3}, {"type": "text", "text": "3.2 Policy Optimization ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To search the policy in a differentiable manner, we adopt the concept from meta-learning [28], where a model is trained on training data $n$ times by SGD algorithm $\\acute{n}$ inner steps), followed by validation of one outer step. In our context, training is performed on augmented data, followed by evaluation on validation data [29, 19]. However, directly applying this concept of inner and outer steps to FL is quite challenging due to the added complexity of FL, which involves training local models and aggregating them to update the global model. ", "page_idx": 3}, {"type": "text", "text": "Since our goal is to optimize the global augmentation policy, we redefine the inner and outer steps from the FL perspective. In a standard FL setting, the server sends a global model to the clients for each round, which is trained with their local training data. Afterward, the aggregation process occurs on the server to update the global model. We can regard one round of local training and aggregation as one inner step. After $r$ rounds, where the global model is updated $r$ times, validating the final updated model on each client can be considered one outer step. We set $r=1$ , meaning validation occurs after each round. ", "page_idx": 3}, {"type": "text", "text": "Federated Meta-Policy Loss. In a single round of client updates, we first perform local training on each client and aggregate the local models to update the global weights, which are then redistributed to the clients for computing the validation loss. Figure 1 (a) illustrates this process. In each round, the initial weight for a client $k$ , denoted $w_{0}^{k}$ , is set to the global weight $w_{g_{r}}$ . The local training consists of $N$ iterations, with a batch size $B$ of the training data $D_{k,n}^{\\mathrm{train}}$ at each iteration $n$ . A transformation according to the policy $p_{\\theta_{n}^{k}}$ is applied to each data sample. The local loss at iteration $n$ for client $k$ is calculated as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname{Local}\\operatorname{Loss}=\\frac{1}{B}\\sum_{i=1}^{B}P_{\\theta_{n}^{k}}^{i}\\ell(w_{n}^{k};t_{p_{\\theta_{n}^{k}}}^{i}(D_{k,n}^{\\operatorname{train}(i)})),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $P_{\\theta_{n}^{k}}^{i}$ is the unnormalized probability $p_{\\theta_{n}^{k}}(o p1,o p2)$ for the $i$ -th transformation, when the sampled transformation $t_{p_{\\theta_{n}^{k}}}^{i}$ is the operations $(o p1,o p2)$ . This reweighting strategy is inspired by recent sample reweighting [29, 19]. Following the local updates with the augmentation policy $p_{\\theta^{k}}$ , we aggregate the results to obtain the new global weights $w_{g_{r+1}}$ , which are redistributed to the clients for validation loss assessment. At the start of round $r$ , after distributing the global weights $w_{g_{r}}$ to each participating client, the procedure for the federated meta-policy loss can be summarized as follows: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "1. Each client $k$ performs local updates by optimizing their local weights $w_{n}^{k}$ using the augmented data with the local loss in Eq. (3). The updated local weights are then aggregated according to $\\begin{array}{r}{w_{g_{r+1}}=\\sum_{k}\\alpha_{k}w_{N}^{k}}\\end{array}$ . 2. The aggregated global weights $w_{g_{r+1}}$ are then sent back to the same clients. 3. The Federated Meta-Policy Loss (FMPL) is computed on each client\u2019s validation set. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{FMPL}=\\ell_{D_{k}^{\\mathrm{val}}}\\big(w_{g_{r+1}}\\big).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "For this loss, the gradient with respect to the augmentation policy $p_{\\theta}$ is computed using backpropagation. Nonetheless, this approach presents two significant challenges. Firstly, it necessitates access to validation gradient information of other clients, which poses privacy concerns. Secondly, it requires revisiting the same clients for additional updates, thereby doubling the communication overhead. ", "page_idx": 4}, {"type": "text", "text": "First-order Approximation. To ensure security by preventing access to other clients\u2019 gradients and to reduce communication costs, we derive an approximation for the policy gradient with respect to $\\theta_{n-1}^{k}$ at the local step $n$ of the client $k$ by a Taylor expansion, as in the following proposition. See Appendix $\\mathbf{B}$ for more details. ", "page_idx": 4}, {"type": "text", "text": "Proposition 1. Consider the federated meta-policy loss derived from the updated weight $w_{n}^{k}$ for client $k$ at step n using a first-order Taylor expansion: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\ell_{D_{k}^{v a l}}(w_{g_{r+1}})\\approx\\ell_{D_{k}^{v a l}}(w_{n}^{k})+\\nabla\\ell_{D_{k}^{v a l}}(w_{n}^{k})^{T}(w_{g_{r}}-w_{n}^{k}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "When computing the policy gradient of the loss with respect to $\\theta_{n-1}^{k}$ , the first-order gradient approximation is ", "page_idx": 4}, {"type": "equation", "text": "$$\n-\\alpha_{k}\\cdot l r\\frac{\\partial(\\nabla\\ell_{D_{k}^{\\nu a l}}(w_{n}^{k})^{T}\\nabla\\ell_{t_{p_{\\theta_{n-1}}^{k}}(D_{k,n-1}^{t r a i n})}(w_{n-1}^{k}))}{\\partial\\theta_{n-1}^{k}},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $w_{n}^{k}=w_{g_{r}}-l r\\cdot g_{w_{0}^{k}}^{a u g}-\\ldots-l r\\cdot g_{w_{n-1}^{k}}^{a u g}$ and $\\alpha_{k}$ is a coefficient proportional to the client\u2019s data size. ", "page_idx": 4}, {"type": "text", "text": "In Proposition 1, gawukg $g_{w_{n}^{k}}^{\\mathrm{aug}}=\\nabla\\ell_{t_{p}{_{\\theta_{n}^{k}}}(D_{k,n}^{\\mathrm{train}})}(w_{n}^{k})$ is the gradient obtained from the local loss in Eq. (3) at step $n$ for client $k$ . We calculate the gradient within the same client to prevent gradient leakage across clients. We optimize a policy that maximizes the inner product of the gradient obtained from sampling the validation data and the gradient obtained through augmentation using $p_{\\theta_{n-1}^{k}}$ . The validation data are not used separately; instead, they are replaced by sampling the next batch, inspired by Reptile [30]. Proposition 1 also indicates that policy gradients can be computed concurrently with local updates. We utilize this approach to joint training $(\\S3.3)$ , which enables simultaneous model and policy training. ", "page_idx": 4}, {"type": "text", "text": "Gradient Clipping. From the policy gradient of Eq. (6), it becomes evident that policy search aims to maximize the dot product of the gradients derived from both augmentation and validation data. This approach, however, can introduce a bias toward augmentations with larger gradient magnitudes due to the nature of the dot product. Inspired by [31, 18], we mitigate the influence of gradient magnitude by gradient clipping [32, 33]. We apply gradient clipping to the gradients from both validation and augmentation data in Eq. (6) using a regularizer hyperparameter $c$ . ", "page_idx": 4}, {"type": "text", "text": "Adaptive Policy Search. Our first-order approximation computes policy gradients at each local update step $n$ without aggregation. These gradients are then averaged across all steps for each client to update the policy after a single communication round. However, this method can slow the policy search, since it only permits one gradient descent update per communication round. Drawing inspiration from meta-learning [28, 30, 34], which quickly adapts to various tasks using neural network training, we propose an adaptive policy search. The augmentation policy is represented as a vector parameter $p_{\\theta}=\\mathrm{sigmoid}([\\theta_{1},\\theta_{2},\\ldots,\\theta_{289}])$ , where $[\\theta_{1},\\theta_{2},\\ldots,\\theta_{289}]$ denotes an $17\\times17$ joint distribution. We use a neural network comprising the following dense layers: one dummy embedding layer, two 100-dim hidden layers, and an output layer shaped to the $17\\times17$ distribution size. We update our policy as done in Reptile [30, 35]. That is, the local policy updates on each client correspond to the inner steps in the Reptile, while the global policy updates on the server are analogous to the outer steps, as detailed in Algorithm 1. We train the policy neural network by increasing the dot-product between policy gradients on each client as follows: ", "page_idx": 4}, {"type": "image", "img_path": "M1PRU0x1Iz/tmp/92ef2f34fde770e42810f8a72d82fb3dccb6a6db5ce4ec76a87a8dea64418908.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\theta_{g_{r+1}}\\approx\\theta_{g_{r}}-\\eta\\lambda\\frac{\\partial}{\\partial\\theta_{0}^{k}}\\mathbb{E}\\Bigg[\\sum_{j=0}^{n}L_{k,j}-\\frac{\\lambda}{2}\\sum_{j=0}^{n}\\sum_{s=0}^{j-1}\\langle\\nabla L_{k,j}\\cdot\\nabla L_{k,s}\\rangle\\Bigg],\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $L_{k,j}=\\ell_{D_{k,j}^{\\mathrm{val}}}^{\\mathrm{FMPL}}(\\theta_{0}^{k})$ is the federated meta-policy loss in Eq. (4) computed on the client $k$ \u2019s $j$ -th validation data batch using the global policy parameters $\\theta_{0}^{k}$ . $\\langle\\nabla L_{k,j}\\cdot\\nabla L_{k,s}\\rangle$ is the dot-product between policy gradients on the client $k$ . See Appendix $\\mathbf{C}$ for more details. This process enables the policy neural network to learn in a direction that enhances the dot-product between the policy gradients of each client, thereby facilitating efficient policy search and enabling personalized policy search. We incorporate this strategy in the joint training in $\\S3.3$ . An experiment result regarding this will be conducted in $\\S4$ . ", "page_idx": 5}, {"type": "text", "text": "3.3 Joint Training ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "As done in AutoAugment, one could employ a pretrained network to perform policy search [16, 18]. However, such methods require a separate training phase and present complications in FL environments, since training a pretrained network beforehand is cumbersome, and the separate phase is disadvantageous for parallel training. Note that we previously adopt an adaptive policy search that can simultaneously train the model and policy search. At each local update step $n$ , the weights $w_{n+1}^{k}$ and gradients concurrently update the model and policy by comparing the gradient on validation data at wn+1. ", "page_idx": 5}, {"type": "text", "text": "Algorithm 1 illustrates this joint training of our FedAvP. It begins by initializing the global model parameter $w_{g_{r}}$ and the policy parameter $\\theta_{g_{r}}$ , which are then sent to the clients from the server. Each client generates augmented data $t_{p_{\\theta_{n}}}(D_{k,n}^{\\mathrm{train}})$ using the policy parameter $p_{\\theta_{n}}$ at every step $n$ and updates the model accordingly. Subsequently, the gradient $\\nabla\\ell_{D_{k,n}^{\\mathrm{val}}}(w_{n+1}^{k})$ is computed using the newly updated $w_{n+1}^{k}$ . Following Proposition 1, the policy parameter $\\theta_{n}^{k}$ is updated to maximize the gradient on both validation and augmentation data $\\nabla\\ell_{t_{p_{\\theta_{n}}}(D_{k,n}^{\\mathrm{train}})}(w_{n}^{k})$ . This process of model and policy updates is repeated in every local update. Afterwards, the model and policy parameters from each client are aggregated at the server using $\\alpha_{k}$ . ", "page_idx": 5}, {"type": "text", "text": "Fast Update One limitation of joint learning is that it requires backpropagation for the policy gradient at every step. To reduce the computation load on local clients, the policy can be updated periodically instead of at every local step $n$ , specifically when $n$ mod $\\tau\\,==\\,0$ . In all our experiments, we set $\\tau=5$ . See the variant of the algorithm in Appendix A.3. We also reduced the hidden size of the policy neural network to two 25-dimensional hidden layers. In our experiments in $\\S4$ , we will compare the performance of the Fast Update. ", "page_idx": 5}, {"type": "table", "img_path": "M1PRU0x1Iz/tmp/0879666d2bc20cd51ae63deccb6fb062aeeb5408109d4b7d7d7fe744d3f4a49d.jpg", "table_caption": [], "table_footnote": ["Table 1: Classification accuracies with different heterogeneity degrees $\\alpha=5.0$ and $\\alpha=0.1$ ) across CIFAR-100/10, SVHN, and FEMNIST datasets. We report results averaged over 3 random seeds with variances for FedAvP (Fast Update) and FedAvP. "], "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1 Experimental setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Environments. Previous FL studies have demonstrated that the standard algorithms are effective and converged when the data is i.i.d. [36\u201338]. To evaluate robust performance in non-i.i.d. data, we set up our experimental environment by distributing the CIFAR-10/100 [21] and SVHN [22] datasets with different levels of data heterogeneity among clients. We assign the data to 130 clients based on a Dirichlet distribution with different hyperparameters of $\\alpha=[5.0,0.1]$ , as done in pFLBench [25]. The smaller $\\alpha$ is, the higher the degree of heterogeneity is. Among these clients, only 100 randomly selected clients participate in the training, while the remaining 30 are nominated as out-of-distribution (OOD) clients. In each communication round, only 10 clients are sampled. We employed a standard CNN model, consistent with those in previous studies [39\u201341], for the global model. Experiments involving the larger model and OOD clients are provided in Appendix A.2. For the FEMNIST dataset [23], we introduced variability in data size by distributing data based on the writers [8, 11]. Further environments details are provided in Appendix A. Our code is available at https://github.com/alsdml/FedAvP. ", "page_idx": 6}, {"type": "text", "text": "Baselines. For a comprehensive evaluation, we compared our method with state-of-the-art federated learning algorithms such as FedAvg [2], FedProx [4], FedDyn [5], FedExP [6], and federated data augmentation algorithms including FedGen [9], FedMix [8], and FedFA [11]. To further explore the potential performance of these algorithms, we conducted experiments applying data augmentation techniques such as RandAugment [17] and TrivialAugment [24]. We also compared the results with those using default augmentations (random crops and horizontal flipping). Additionally, for comparison with our proposed model, we included FedAvP (W/ Local Policy), which trains each local client without policy aggregation, and FedAvP (Fast Update) which reduces computation load on local clients (Algorithm 2). Further details of baselines are provided in Appendix A. ", "page_idx": 6}, {"type": "image", "img_path": "M1PRU0x1Iz/tmp/a5e2ad1d166773836f3b79647ce2ce8918e1fbafec10770e845790d9b6a1ee63.jpg", "img_caption": ["Figure 2: Visualization of global policies learned in CIFAR-100, SVHN and FEMNIST. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "4.2 Performance on Non-i.i.d. Settings ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Settings. Table 1 reports the test accuracy, which measures the accuracy on the test dataset of the 100 participating clients. The test accuracy is calculated as the weighted average of each client\u2019s accuracy by the number of data points they have. We compared each baseline with three data augmentations: $^+$ Default (random crops and horizontal filpping), $+\\mathbf{R}$ andAugment, and $^+$ TrivialAugment. For FedAvP (W/ Local Policy), each client has its own transformation policy and policy aggregation is removed from our algorithm (Algorithm 1). For FedAvP (Fast Update), we used periodic local updates (Algorithm 2). For FedAvP, we used full local updates (Algorithm 1). ", "page_idx": 7}, {"type": "text", "text": "Results. The application of automated data augmentation algorithms such as RandAugment and TrivialAugment within a federated learning framework does not consistently enhance performance across all cases. In contrast, our algorithm learned distinct augmentations for each dataset, as depicted in Figure 2. When compared to FedAvP (W/ Local Policy) and FedAvP, notable performance improvements were observed in highly non-i.i.d. scenarios such as $\\alpha\\,=\\,0.1$ in CIFAR-100 and SVHN. For instance, the standard deviation of local dataset sizes in CIFAR-100 with $\\alpha=5.0$ was relatively small at 19.43, while it was significantly higher at 118.00 for CIFAR-100 and 533.60 for SVHN with $\\alpha=0.1$ , indicating that imbalanced datasets in non-i.i.d. settings pose challenges for training local policies with smaller local datasets. Although FedAvP (Fast Update) experienced some performance declines compared to FedAvP, it generally achieved higher performance than baseline methods across most datasets. ", "page_idx": 7}, {"type": "text", "text": "4.3 Policy Adaptation on Clients ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In our FedAvP, at the beginning of each round, the participating clients receive a global policy from the server, which is then optimized into a local policy using each client\u2019s local data. That is, this optimization adapts the global policy into a personalized policy on the local data of each client. The clients use the personalized policy to train their local model to achieve high performance and to aggregate well with other local models in the server. Figure 3 shows the statistics on the Euclidean distances between ", "page_idx": 7}, {"type": "image", "img_path": "M1PRU0x1Iz/tmp/d4c44896fc5ddc2d0944790b6eaff18cf40231ba91686def20454a284b473d34.jpg", "img_caption": ["Figure 3: Statistics of personalized policies between different clients on CIFAR-100. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "the personalized policies of clients participating in each round and the global policy for that round. With $\\alpha=5.0$ in CIFAR-100, the data is sufficiently i.i.d., and thus the personalized policies of clients tend not to deviate from the global policy. On the other hand, with $\\alpha=0.1$ , the deviation from the global policy is initially high but decreases as training progresses, particularly after about 100 rounds. The variance of the Euclidean distances also follows this pattern. ", "page_idx": 7}, {"type": "text", "text": "4.4 Reconstruction Attack ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Settings. In collaborative learning systems, it has been reported that gradient leakage attacks can occur [14, 42, 43], leveraging gradients to reconstruct the original training data. We conducted these reconstruction attack [14] experiments to evaluate whether our algorithm, which shares policies in a federated learning setting, provides enhanced privacy compared to FedGen, FedMix, and FedFA. Specifically, FedGen shares information at the generator and label distribution levels, FedMix shares at the input level, and FedFA shares at the feature level. ", "page_idx": 8}, {"type": "text", "text": "We included a defense algorithm in the performance comparison, ATSPrivacy [27]. Our experiments were conducted on CIFAR-100 with $\\alpha=0.1$ , involving two clients: Client(L), which had the most training data (895 data points), and Client(S), with the least (156 data points). For ATSPrivacy, we used policies from [27] that showed the highest accuracy performance. Further experimental details are provided in Appendix A.1. ", "page_idx": 8}, {"type": "text", "text": "Results. The experimental results for the Reconstruction Attack are summarized in Table 2. Lower PSNR values indicate that the reconstructed images are less similar to the original data, thereby reflecting better privacy preservation. Fed", "page_idx": 8}, {"type": "table", "img_path": "M1PRU0x1Iz/tmp/bd8ef9f0a6e078916cfe12c4578f1b77a1b90fa0082045edf90648c77200a4e4.jpg", "table_caption": [], "table_footnote": ["Table 2: Reconstruction Attack Results "], "page_idx": 8}, {"type": "text", "text": "Gen+label+generator utilizes the label distribution of local data and the generative model. FedMix+input and FedFA+feature represent the results of attacks utilizing input level and feature level information, respectively. FedAvP $^+$ policy gradients denotes the results of attacks using the policy gradient of our algorithm. ATSPrivacy recorded the lowest PSNR values, indicating that it makes reconstruction difficult. However, this was coupled with a decline in accuracy performance. Despite increases in PSNR for FedGen, FedMix and FedFA, our algorithm\u2019s use of policy gradients did not elevate PSNR values. ", "page_idx": 8}, {"type": "table", "img_path": "M1PRU0x1Iz/tmp/feca631ab3a797548117ba32ada96e63ce0063286c2a75ba8804c563dbf37418.jpg", "table_caption": ["4.5 Computation and Communication Cost "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "For computational comparison, we measured the time taken to reach a target accuracy of $35\\%$ on the CIFAR-100 dataset with $\\alpha=0.1$ . Regarding communication comparison in FedMix, the method involves sending an average image to the server prior to training, which is detailed in the \u201cBefore\u201d section of the table. For FedAvP (Fast Update) using small neural networks, although it is higher than these baselines, the increase of 0.38MB from the cost of FedAvg is about $2.48\\%$ compared to the gradient transmission cost of the model. See Appendix A.4 for additional results. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We proposed a novel federated data augmentation method named FedAvP (Augment Local Data via Shared Policy in Federated Learning). It shares the augmentation policies during training rather than preprocessed or encoded data such as the average of data or statistics of features. Direct exposure of personal information was constrained, yet clients still beneftied from the policies learned and shared across the clients to augment their local data. We also proposed Federated Meta-Policy Loss (FMPL) and used the first-order gradient information to enhance privacy with reduced communication costs. A potential limitation of our algorithm is the introduction of Joint Training. This approach requires consideration when applying our federated data augmentation method to existing federated learning algorithms. Also, our algorithm assumed the use of FedAvg, which does not account for model personalization [44, 45] in the computation of FMPL. Investigating a policy loss that aligns with model personalization algorithms would be interesting. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Acknowledgment ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was partially supported by Samsung Advanced Institute of Technology and Center for Applied Research in Artificial Intelligence(CARAI) grant funded by Defense Acquisition Program Administration(DAPA) and Agency for Defense Development(ADD) (UD230017TD). Gunhee Kim is the corresponding author. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In AISTATS, pages 1273\u20131282. PMLR, 2017.   \n[2] Jakub Kone\u02c7cn\\`y, H Brendan McMahan, Daniel Ramage, and Peter Richt\u00e1rik. Federated optimization: Distributed machine learning for on-device intelligence. arXiv preprint arXiv:1610.02527, 2016.   \n[3] Viraaji Mothukuri, Reza M Parizi, Seyedamin Pouriyeh, Yan Huang, Ali Dehghantanha, and Gautam Srivastava. A survey on security and privacy of federated learning. Future Gener. Comput. Syst., 115:619\u2013 640, 2021.   \n[4] Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated optimization in heterogeneous networks. Proceedings of Machine learning and systems, 2:429\u2013450, 2020.   \n[5] Durmus Alp Emre Acar, Yue Zhao, Ramon Matas, Matthew Mattina, Paul Whatmough, and Venkatesh Saligrama. Federated learning based on dynamic regularization. In International Conference on Learning Representations, 2020.   \n[6] Divyansh Jhunjhunwala, Shiqiang Wang, and Gauri Joshi. Fedexp: Speeding up federated averaging via extrapolation. In The Eleventh International Conference on Learning Representations, 2022.   \n[7] Sawsan AbdulRahman, Hanine Tout, Hakima Ould-Slimane, Azzam Mourad, Chamseddine Talhi, and Mohsen Guizani. A survey on federated learning: The journey from centralized to distributed on-site learning and beyond. IEEE Internet Things J., 8(7):5476\u20135497, 2020.   \n[8] Tehrim Yoon, Sumin Shin, Sung Ju Hwang, and Eunho Yang. Fedmix: Approximation of mixup under mean augmented federated learning. In ICLR, 2020.   \n[9] Zhuangdi Zhu, Junyuan Hong, and Jiayu Zhou. Data-free knowledge distillation for heterogeneous federated learning. In International conference on machine learning, pages 12878\u201312889. PMLR, 2021.   \n[10] MyungJae Shin, Chihoon Hwang, Joongheon Kim, Jihong Park, Mehdi Bennis, and Seong-Lyun Kim. Xor mixup: Privacy-preserving data augmentation for one-shot federated learning. arXiv preprint arXiv:2006.05148, 2020.   \n[11] Tianfei Zhou and Ender Konukoglu. Fedfa: Federated feature augmentation. In ICLR, 2022.   \n[12] Mohammad Rasouli, Tao Sun, and Ram Rajagopal. Fedgan: Federated generative adversarial networks for distributed data. arXiv preprint arXiv:2006.07228, 2020.   \n[13] Dominik Lewy, Jacek Ma\u00b4ndziuk, Maria Ganzha, and Marcin Paprzycki. Statmix: Data augmentation method that relies on image statistics in federated learning. In ICONIP, pages 574\u2013585. Springer, 2022.   \n[14] Ligeng Zhu, Zhijian Liu, and Song Han. Deep leakage from gradients. Advances in neural information processing systems, 32, 2019.   \n[15] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment: Learning augmentation policies from data. arXiv preprint arXiv:1805.09501, 2018.   \n[16] Sungbin Lim, Ildoo Kim, Taesup Kim, Chiheon Kim, and Sungwoong Kim. Fast autoaugment. Advances in neural information processing systems, 32, 2019.   \n[17] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data augmentation with a reduced search space. In CVPRW, pages 702\u2013703, 2020.   \n[18] Yu Zheng, Zhi Zhang, Shen Yan, and Mi Zhang. Deep autoaugment. arXiv preprint arXiv:2203.06172, 2022.   \n[19] Fengwei Zhou, Jiawei Li, Chuanlong Xie, Fei Chen, Lanqing Hong, Rui Sun, and Zhenguo Li. Metaaugment: Sample-aware data augmentation policy learning. In AAAI, volume 35, pages 11097\u201311105, 2021.   \n[20] Xinyu Zhang, Qiang Wang, Jian Zhang, and Zhao Zhong. Adversarial autoaugment. arXiv preprint arXiv:1912.11188, 2019.   \n[21] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \n[22] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y. Ng. Reading Digits in Natural Images with Unsupervised Feature Learning. NIPSW, 2011.   \n[23] Sebastian Caldas, Sai Meher Karthik Duddu, Peter Wu, Tian Li, Jakub Konecny, H. Brendan McMahan, Virginia Smith, and Ameet Talwalkar. LEAF: A Benchmark for Federated Settings. arXiv preprint arXiv:1812.01097, 2019.   \n[24] Samuel G M\u00fcller and Frank Hutter. Trivialaugment: Tuning-free yet state-of-the-art data augmentation. In ICCV, pages 774\u2013782, 2021.   \n[25] Daoyuan Chen, Dawei Gao, Weirui Kuang, Yaliang Li, and Bolin Ding. pfl-bench: A comprehensive benchmark for personalized federated learning. Advances in neural information processing systems, 35:9344\u20139360, 2022.   \n[26] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. In ICLR, 2018.   \n[27] Wei Gao, Shangwei Guo, Tianwei Zhang, Han Qiu, Yonggang Wen, and Yang Liu. Privacy-preserving collaborative learning with automatic transformation search. In CVPR, pages 114\u2013123, 2021.   \n[28] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In ICML, pages 1126\u20131135. PMLR, 2017.   \n[29] Jun Shu, Qi Xie, Lixuan Yi, Qian Zhao, Sanping Zhou, Zongben Xu, and Deyu Meng. Meta-weight-net: Learning an explicit mapping for sample weighting. Advances in neural information processing systems, 32, 2019.   \n[30] Alex Nichol and John Schulman. Reptile: a scalable metalearning algorithm. arXiv preprint arXiv:1803.02999, 2(3):4, 2018.   \n[31] Xinyi Wang, Hieu Pham, Paul Michel, Antonios Anastasopoulos, Jaime Carbonell, and Graham Neubig. Optimizing data usage via differentiable rewards. In ICML, pages 9983\u20139995. PMLR, 2020.   \n[32] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural networks. In ICML, pages 1310\u20131318. Pmlr, 2013.   \n[33] Tomas Mikolov, Anoop Deoras, Stefan Kombrink, Lukas Burget, and Jan Cernocky\\`. Empirical evaluation and combination of advanced language modeling techniques. In Interspeech, number s 1, pages 605\u2013608, 2011.   \n[34] Zhenguo Li, Fengwei Zhou, Fei Chen, and Hang Li. Meta-sgd: Learning to learn quickly for few-shot learning. arXiv preprint arXiv:1707.09835, 2017.   \n[35] Yihan Jiang, Jakub Konec\u02c7ny\\`, Keith Rush, and Sreeram Kannan. Improving federated learning personalization via model agnostic meta learning. arXiv preprint arXiv:1909.12488, 2019.   \n[36] Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of FedAvg on Non-IID data. arXiv preprint arXiv:1907.02189, 2019.   \n[37] Peter Kairouz, H Brendan McMahan, Brendan Avent, Aur\u00e9lien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open problems in federated learning. Found. Trends Mach. Learn., 14(1\u20132):1\u2013210, 2021.   \n[38] Mohammed Aledhari, Rehma Razzak, Reza M Parizi, and Fahad Saeed. Federated learning: A survey on enabling technologies, protocols, and applications. IEEE Access, 8:140699\u2013140725, 2020.   \n[39] Aviv Shamsian, Aviv Navon, Ethan Fetaya, and Gal Chechik. Personalized federated learning using hypernetworks. In International Conference on Machine Learning, pages 9489\u20139502. PMLR, 2021.   \n[40] Paul Pu Liang, Terrance Liu, Liu Ziyin, Nicholas B Allen, Randy P Auerbach, David Brent, Ruslan Salakhutdinov, and Louis-Philippe Morency. Think locally, act globally: Federated learning with local and global representations. arXiv preprint arXiv:2001.01523, 2020.   \n[41] Jaehoon Oh, Sangmook Kim, and Se-Young Yun. Fedbabu: Towards enhanced representation for federated image classification. arXiv preprint arXiv:2106.06042, 2021.   \n[42] Jonas Geiping, Hartmut Bauermeister, Hannah Dr\u00f6ge, and Michael Moeller. Inverting gradients-how easy is it to break privacy in federated learning? Advances in neural information processing systems, 33:16937\u201316947, 2020.   \n[43] Bo Zhao, Konda Reddy Mopuri, and Hakan Bilen. idlg: Improved deep leakage from gradients. arXiv preprint arXiv:2001.02610, 2020.   \n[44] Viraj Kulkarni, Milind Kulkarni, and Aniruddha Pant. Survey of personalization techniques for federated learning. In WorldS4, pages 794\u2013797. IEEE, 2020.   \n[45] Alysa Ziying Tan, Han Yu, Lizhen Cui, and Qiang Yang. Towards personalized federated learning. IEEE Trans Neural Netw Learn Syst, 2022.   \n[46] Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. Optuna: A next-generation hyperparameter optimization framework. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, pages 2623\u20132631, 2019.   \n[47] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019.   \n[48] Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017.   \n[49] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \n[50] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.   \n[51] Felix Sattler, Simon Wiedemann, Klaus-Robert M\u00fcller, and Wojciech Samek. Robust and communicationefficient federated learning from non-iid data. IEEE transactions on neural networks and learning systems, 31(9):3400\u20133413, 2019.   \n[52] Alexey Dosovitskiy. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.   \n[53] Liangqiong Qu, Yuyin Zhou, Paul Pu Liang, Yingda Xia, Feifei Wang, Ehsan Adeli, Li Fei-Fei, and Daniel Rubin. Rethinking architecture design for tackling data heterogeneity in federated learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10061\u201310071, 2022.   \n[54] Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H Vincent Poor. Tackling the objective inconsistency problem in heterogeneous federated optimization. Advances in neural information processing systems, 33:7611\u20137623, 2020.   \n[55] Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In International conference on machine learning, pages 5132\u20135143. PMLR, 2020.   \n[56] Qinbin Li, Yiqun Diao, Quan Chen, and Bingsheng He. Federated learning on non-iid data silos: An experimental study. In 2022 IEEE 38th international conference on data engineering (ICDE), pages 965\u2013978. IEEE, 2022.   \n[57] Alex Nichol, Joshua Achiam, and John Schulman. On first-order meta-learning algorithms. arXiv preprint arXiv:1803.02999, 2018. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Implementation Details ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Model Consistent with prior studies, we employed a standard CNN model for all experiments as referenced in [39\u201341]. The global model consists of three convolutional layers with 64 fliters and 3x3 kernels, followed by three fully-connected layers of 256, 128, and the final classification layer. All experiments are run on a cluster of 32 NVIDIA GTX 1080 GPUs. ", "page_idx": 12}, {"type": "text", "text": "Datasets To evaluate robust performance in non-i.i.d. data, we set up our experimental environment by distributing the CIFAR-10/100 [21] and SVHN [22] datasets with varying levels of data heterogeneity among clients. We allocated the data to 130 clients based on a Dirichlet distribution with different hyperparameters of $\\alpha\\,=\\,[5.0,0.1]$ , following the procedure in pFL-Bench [25]. The lower the $\\alpha$ , the higher the degree of heterogeneity. Among these clients, only 100 were randomly selected to participate in the training, while the remaining 30 were designated as out-of-distribution (OOD) clients. For the CIFAR10/100, the number of training rounds $R$ was set to 1000. For the FEMNIST, the number of training rounds $R$ was set to 500. For the SVHN, the number of training rounds $R$ was set to 100, 300, and 500, as reported in Table 5. The results reported in the main paper, Table 1, are for the 500 round. In each communication round, only 10 clients are sampled, and the remaining 30 clients serve as out-of-distribution (OOD) clients. ", "page_idx": 12}, {"type": "text", "text": "Hyperparameters For the FedAvP algorithm, the hyperparameters include the server policy learning rate $\\eta$ client policy learning rate $\\lambda$ , gradient clipping threshold $c$ , and a regularization term $\\epsilon$ . In our experimentation, we tuned $\\eta$ within [0.4, 0.9], $\\lambda$ within $[0.1\\sim0.9]$ , $c$ within $[0.4\\sim1.0]$ , and $\\epsilon$ within $[0.0\\sim0.5]$ . The validation batch size was also explored within [64, 128, 192]. A common hyperparameter across all methods was local epoch set to 5, and local batch is set to 64. The client model learning rate $\\gamma$ was searched within the range of $[0.1\\sim0.3]$ . This comprehensive parameter optimization was conducted using an optimization tool known as Optuna1 [46]. We utilized both the Tree-structured Parzen Estimator algorithm and Random Sampler as hyperparameter samplers within Optuna. The Adaptive Policy Network was consistently implemented in all experiments with FedAvP, comprising the following dense layers: an embedding layer, two hidden layers with 100 neurons each, and an output layer shaped to the $17\\times17$ distribution size. With FedAvP (Fast Update), it comprised the following dense layers: an embedding layer, two hidden layers with 25 neurons each, and an output layer shaped to the $17\\times17$ distribution size. ", "page_idx": 12}, {"type": "text", "text": "Baselines In all baseline experiments, Random Crop and HorizontalFlip were applied as default augmentations. For the baseline algorithms\u2019 hyperparameter settings, specifically for RandAugment (RA) [17], we leveraged a PyTorch implementation [47]. Hyperparameter values were determined in alignment with the procedures described by the authors. For the CIFAR-100 dataset, the hyperparameter $M$ was investigated within the range of [2, 6, 10, 14] and $N$ within [1, 2]. For CIFAR-10, the hyperparameter $M$ was investigated within [4, 5, 7, 9, 11] and $N$ within [2, 3]. For FEMNIST, the hyperparameter $M$ was investigated within [4, 5, 7, 9, 11] and $N$ within [1, 2, 3]. For SVHN, the hyperparameter $M$ was investigated within [5, 7, 9, 11] and $N$ within [3]. For TrivialAugment (TA) [24], we utilized the PyTorch library\u2019s built-in algorithm [47]. For RandAugment and TrivialAugment, Random Crop and HorizontalFlip were applied first, followed by RandCutout [48]. As for FedMix [8], the hyperparameter $\\lambda$ was investigated within $\\bar{[0.01\\sim0.1]}$ . The client model learning rate $\\gamma$ was searched within the range of $[0.005\\sim0.3]$ . The hyperparameter $M$ used \"All\", meaning the average image of all images of each client was used. In the case of FedFA [11], we searched the hyperparameters $p$ and $\\alpha$ , each within the range of $[0.0\\sim1.0]$ . The client model learning rate $\\gamma$ was searched within the range of $[0.005\\sim0.3]$ . ", "page_idx": 12}, {"type": "text", "text": "A.1 Reconstruction Attack Details ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We performed experiments using reconstruction attacks [14] to assess if our policy-sharing algorithm offers better privacy in a federated learning context, in comparison to FedMix [8] and FedFA [11], which share data at the input and feature levels respectively. Additionally, we conducted experiments to compare our algorithm with FedGen [9], which shares the label distribution of the client\u2019s training data and a generative model that produces a latent from learned latent feature space over the clients\u2019 local training data. Despite our algorithm not being specifically designed as a defense against reconstruction attacks, we included ATSPrivacy [27] in our performance evaluation for comparison. ATSPrivacy aims to identify the best policy to counter reconstruction attacks by conducting a policy search. Although ATSPrivacy\u2019s primary goal isn\u2019t to enhance performance through policy sharing in collaborative learning environments, we chose it as a baseline because it also involves policy search in the context of collaborative learning. We have described below the attack algorithms that utilize input-level, feature-level, and policy gradient information. ", "page_idx": 12}, {"type": "text", "text": "A.1.1 Reconstruction Attack using Additional Information ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "It has been reported that in Collaborative learning systems, Gradient leakage attacks are feasible [14, 42, 43]. These attacks use gradients to reconstruct the training data. We consider a scenario with a central server and clients where learning occurs through the exchange of gradients. Assume we have a given gradient $\\nabla W(x,y)$ , then we can optimize for a dummy data and label pair $\\bar{(x^{\\prime},y^{\\prime})}$ by minimizing the following objective: ", "page_idx": 13}, {"type": "equation", "text": "$$\nx^{*},y^{*}=\\arg\\operatorname*{min}_{x^{\\prime},y^{\\prime}}||\\nabla W(x,y)-\\nabla W(x^{\\prime},y^{\\prime})||,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $||\\cdot||$ denotes a norm distance measure, $\\nabla W(x,y)$ is the given gradient, $(x,y)$ is the client\u2019s sample data and label, and $(x^{\\prime},y^{\\prime})$ are the targets of optimization. Following [42], we utilize cosine similarity as a cost function instead of norm distance, like in the case of ", "page_idx": 13}, {"type": "equation", "text": "$$\nx^{*},y^{*}=\\arg\\operatorname*{min}_{x^{\\prime},y^{\\prime}}\\left[1-\\ell(\\nabla W(x,y),\\nabla W(x^{\\prime},y^{\\prime}))\\right],\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\ell(x,y)=\\left\\langle x,y\\right\\rangle/(\\|x\\|\\cdot\\|y\\|)$ . ", "page_idx": 13}, {"type": "text", "text": "Here, we assume that input-level information is available, specifically the mean image of the client\u2019s data $x_{m e a n}$ in the FedMix [8]. The method for reconstruction attack utilizing input-level information is as follows: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\boldsymbol{x}^{*},\\boldsymbol{y}^{*}=\\arg\\operatorname*{min}_{\\boldsymbol{x}^{\\prime},\\boldsymbol{y}^{\\prime}}\\Big[(1-\\alpha_{i n p u t})\\cdot(1-\\ell(\\nabla W(\\boldsymbol{x},\\boldsymbol{y}),\\nabla W(\\boldsymbol{x}^{\\prime},\\boldsymbol{y}^{\\prime})))}\\\\ {+\\alpha_{i n p u t}\\cdot\\big\\|\\boldsymbol{x}^{\\prime}-x_{m e a n}\\big\\|\\Big],\\quad\\quad\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\alpha_{i n p u t}$ is a hyperparameter for the additional term, it is fixed at 0.1 in experiments where input-level information is available and set to 0 in scenarios without input-level information. $x_{m e a n}$ represents the mean image of the client\u2019s data. Cosine similarity was utilized to measure gradient distance, as described in [42]. Similarly, in cases where the client provides feature-level information to the server, as in the FedFA [11], this information can also be used to help with reconstruction. The method for reconstruction attack utilizing feature-level information is as follows: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{x^{*},y^{*}=\\arg\\operatorname*{min}_{x^{\\prime},y^{\\prime}}\\Big[(1-\\alpha_{f e a t}-\\beta_{f e a t})\\cdot(1-\\ell(\\nabla W(x,y),\\nabla W(x^{\\prime},y^{\\prime})))}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\alpha_{f e a t}\\cdot\\mathbb{E}_{k}\\left\\|\\bar{\\mu}^{k}-\\mu^{k^{\\prime}}\\right\\|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\beta_{f e a t}\\cdot\\mathbb{E}_{k}\\left\\|\\bar{\\sigma}^{k}-\\sigma^{k^{\\prime}}\\right\\|\\Big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\bar{\\mu}^{k}$ and $\\bar{\\sigma}^{k}$ are the momentum updated feature statistics of layer $k$ in the client model, $\\mu^{k^{\\prime}}$ and $\\sigma^{k^{\\prime}}$ are the feature statistics of layer $k$ with regard to $x^{\\prime}$ , $\\alpha_{f e a t}$ and $\\beta_{f e a t}$ are hyperparameters for the additional terms fixed at 0.1 and 0.0, respectively. ", "page_idx": 13}, {"type": "text", "text": "In the FedGen algorithm, clients transmit the label distribution of their local training data and the generative model. This information allows us to improve the cost function for reconstruction, which is formulated as follows: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{x^{*},y^{*}=\\arg\\underset{x^{\\prime},y^{\\prime}}{\\operatorname*{min}}\\;\\Big[(1-\\alpha_{g e n}-\\beta_{g e n})\\cdot(1-\\ell(\\nabla W(x,y),\\nabla W(x^{\\prime},y^{\\prime})))}&{}\\\\ {+\\alpha_{g e n}\\cdot\\big\\|c-y^{\\prime}\\big\\|}&{}\\\\ {+\\beta_{g e n}\\cdot(1-\\ell(W^{p}(z|z\\sim G(y^{\\prime})),W(x^{\\prime}))\\Big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $G$ is a generative model to generate the latent distribution, $\\boldsymbol{W}^{p}$ refers to the final layer in the client model that takes a latent as an input and outputs prediction logits, $c$ is the label distribution of the client\u2019s local training data, $\\alpha_{g e n}$ and $\\beta_{g e n}$ are hyperparameters for the additional terms fixed at 0.0001 and 0.0001, respectively. Also, we can initialize $y^{\\prime}$ with $c$ . ", "page_idx": 13}, {"type": "text", "text": "A.1.2 Reconstruction Attack using Policy Gradient Information ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "For the FedAvP algorithm, clients send the policy gradient information. Such policy gradient information can also be utilized as a term of cost function for reconstruction in the following manner: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{x^{*},y^{*}=\\arg\\operatorname*{min}_{x^{\\prime},y^{\\prime}}\\Big[(1-\\alpha_{p o l i c y})\\cdot(1-\\ell(\\nabla W(x,y),\\nabla W(x^{\\prime},y^{\\prime})))}\\\\ {+\\alpha_{p o l i c y}\\cdot(1-\\ell(\\nabla\\ell_{x,y}^{\\mathrm{FMPL}}(\\theta),\\nabla\\ell_{x^{\\prime},y^{\\prime}}^{\\mathrm{FMPL}}(\\theta)))\\Big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\alpha_{p o l i c y}$ is a hyperparameter for the additional term fixed at 0.1. ", "page_idx": 13}, {"type": "text", "text": "When additional information, such as input-level or feature-level, is available in addition to gradient information, it can be utilized, potentially posing a security risk. On the other hand, we have confirmed that even when utilizing the policy gradient information additionally, the reconstruction risk does not increase. The results of the experiments regarding this are summarized in Table 2. ", "page_idx": 13}, {"type": "text", "text": "A.1.3 Experimental details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Our reconstruction attack experiments were conducted on CIFAR-100 with $\\alpha=0.1$ , featuring heterogeneous data distribution. Specifically, after 1000 rounds of training, we selected clients for the reconstruction attack. The selection criteria were the client with the most training data (895 data points), Client(L), and the one with the least (156 data points), Client(S). For both clients, we randomly sampled 150 training data points to perform the reconstruction attack. In all experiments, the batch size and reconstruction step for the reconstruction attack were set to 1 and 2400, respectively. We used the Adam optimizer [49]. Gradients obtained from 1 round after 1000 rounds of training were used for the reconstruction attack. ", "page_idx": 14}, {"type": "text", "text": "For ATSPrivacy, we trained the 6 policies with FedAvg used in the CIFAR-100 dataset of the author\u2019s paper [27], namely 3-1-7, 43-18-18, (3-1-7, 43-18-18), 21-13-3, 7-4-15, and (21-13-3, 7-4-15), in the CIFAR-100 with $\\alpha=0.1$ ) environment. We performed the reconstruction attack on the policies (7-4-15) and (21-13-3, 7-4-15), which showed the best accuracy performance. The (21-13-3, 7-4-15) policy is a hybrid policy described in the author\u2019s paper [27], and we applied it by randomly sampling one of the two policies. ", "page_idx": 14}, {"type": "text", "text": "A.2 Additional Results with a Larger Model ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We conducted experiments on a model with more parameters than those used in the main experiments. The model used in these experiments is a simplified version of VGG11 [50, 51], where all dropout and batch normalization layers are removed, and the filters and the size of all fully-connected layers are reduced by a factor of 2. This model contains about three times more network parameters than the networks used in the main paper. We verify the performance of our model on non-IID datasets, specifically SVHN-10 $(\\alpha=0.1)$ ), with different communication rounds. ", "page_idx": 14}, {"type": "table", "img_path": "M1PRU0x1Iz/tmp/628b0091eb3e5e67dbc2eba52608a2affb4c88a83ab5da4a5f63c197dd1ff868.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "Table 5: Classification accuracies with the different communication rounds of $R=[100,300,500]$ in SVHN dataset with $\\alpha=0.1$ using the VGG11s model. ", "page_idx": 14}, {"type": "text", "text": "A.3 Fast Update Algorithm ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "To reduce computation for local clients from joint learning, we perform periodic policy updates, specifically when $n$ mod $\\tau==0$ . In all our experiments, we set $\\tau=5$ and reduced the hidden size of the policy neural network to two 25-dimensional hidden layers with FedAvP (Fast Update) algorithm. ", "page_idx": 15}, {"type": "table", "img_path": "M1PRU0x1Iz/tmp/92347c2c289c0725a9d5b9c862776a008a747eae178f2709d47c56b49798b86e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "A.4 Additional Results of Computation cost ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "For additional computational comparisons, we measured the time taken to reach a target accuracy of $40\\%$ on the CIFAR-100 dataset with $\\alpha=5.0$ and the time taken to reach a target accuracy of $75\\%$ on the CIFAR-10 dataset with $\\alpha=5.0$ . In the case of CIFAR-100 with $\\alpha=5.0$ , FedAvP (Fast Update) achieved results more than 1 hour faster than FedAvg. Our method generally produced faster results compared to applying RandAugment and TrivialAugment which enhanced the performance of FedAvg in table 1. ", "page_idx": 15}, {"type": "table", "img_path": "M1PRU0x1Iz/tmp/4e647d4ecd3e3e3053905c91ef5c3215b1d3b0fb8d92f62fb767df0924c194d4.jpg", "table_caption": [], "table_footnote": ["Table 6: Computation Time in CIFAR-100 "], "page_idx": 15}, {"type": "table", "img_path": "M1PRU0x1Iz/tmp/4ffa050eac1e3a17dd44ff69268e24a0758d9a56149cd71425a7e56460b918f3.jpg", "table_caption": ["Table 7: Computation Time in CIFAR-10 "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "A.5 Ablation Results of Hyperparameters $\\epsilon$ ", "text_level": 1, "page_idx": 15}, {"type": "table", "img_path": "M1PRU0x1Iz/tmp/b8eaf192af26aa78ac3d7efdcdb03a1b58fb6315e9749a5e8398da972b239705.jpg", "table_caption": ["Table 8: CIFAR-10 with $\\alpha=5.0$ . "], "table_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "M1PRU0x1Iz/tmp/950a535d24e94b85c82a19c46eb97726d585bb6a963f366dfca439129f922075.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "M1PRU0x1Iz/tmp/33b0f3ee4c09ee6745ac9cad75346adb480db942a35d209cf700eec4376fc7d6.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure 4: Results of the reconstruction attacks in Table 2. The first row of each result represents the random client\u2019s training samples, and the second row is the reconstructed samples by the server. We visualized high-PSNR samples selected from random samples. The numbers below indicate the PSNR values of the reconstructed samples. ", "page_idx": 16}, {"type": "image", "img_path": "M1PRU0x1Iz/tmp/9c0a9e909c1fa2e47b0c7ff36a11f386fabb9a687eea82d844becb9e21959ab3.jpg", "img_caption": ["(a) CIFAR-100 with $\\alpha=5.0$ "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "M1PRU0x1Iz/tmp/f8d17cda2d716ec24ca602fa0eece5ba5661191af17aa9c0992f961442c05d8b.jpg", "img_caption": ["(b) CIFAR-100 with $\\alpha=0.1$ "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "M1PRU0x1Iz/tmp/0d3205c2e1b901a98e863581e592dd2e7598d153a63119373064d1ab05155d07.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "", "img_caption": ["Figure 5: Training loss convergence of our FedAvP algorithm ", "(d) SVHN with $\\alpha=0.1$ "], "img_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "M1PRU0x1Iz/tmp/1678b5c810fe171102ac58f08f1419a79a449d5a21ccd1d3ab736f5bd0440e79.jpg", "table_caption": ["A.6 Additional Results of Non-i.i.d. Experiments "], "table_footnote": ["Table 9: Classification accuracies with different heterogeneity degrees $\\alpha=5.0$ and $\\alpha=0.1$ ) across CIFAR-100/10, SVHN, and FEMNIST datasets. "], "page_idx": 18}, {"type": "text", "text": "A.7 The Scalability of FedAvP ", "text_level": 1, "page_idx": 18}, {"type": "table", "img_path": "M1PRU0x1Iz/tmp/89721dc79250cca62bf175d186144823169776f12b1b1f2a3ee28ac3ff62bec2.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Table 10: Test accuracy $(\\%$ ) of FedAvP (Fast Update) on SVHN with $\\alpha=0.1$ across different rounds and search spaces. ", "page_idx": 18}, {"type": "table", "img_path": "M1PRU0x1Iz/tmp/b5242e45efc350758efc4d936cd524b9c384fe2f769bc24f73675331e716cd5b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Table 11: Test accuracy $(\\%$ ) of FedAvP (Fast Update) on SVHN with $\\alpha=5.0$ across different rounds and search spaces. ", "page_idx": 18}, {"type": "text", "text": "A.8 Additional Results with ViT-T ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We present additional results using the ViT-T model[52, 53] on CIFAR-100 with different heterogeneity degrees. ", "page_idx": 19}, {"type": "table", "img_path": "M1PRU0x1Iz/tmp/22173ec395053ae478b2960dc923b5d1fab38b8d74b6efb179a756b6223806e4.jpg", "table_caption": [], "table_footnote": ["Table 12: Test accuracy $\\%)$ using ViT-T on CIFAR-100 with $\\alpha=5.0$ and $\\alpha=0.1$ . "], "page_idx": 19}, {"type": "text", "text": "A.8.1 Computation Time of ViT-T on CIFAR-100 ", "text_level": 1, "page_idx": 19}, {"type": "table", "img_path": "M1PRU0x1Iz/tmp/9a2f9609520b530946c9d850c2fc0e5af4fc371151ed26fd913154f46347f055.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "M1PRU0x1Iz/tmp/4ff053fe3fa9bdaccd530e9f029ab259d34ab3f1e15f6909bda67bcde8f7c2c7.jpg", "table_caption": ["Table 13: Computation time and rounds to reach $30\\%$ test accuracy on CIFAR-100 with $\\alpha=5.0$ . "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "Table 14: Computation time and rounds to reach $25\\%$ test accuracy on CIFAR-100 with $\\alpha=0.1$ . ", "page_idx": 19}, {"type": "text", "text": "A.9 Comparison with Other Classic Non-IID Methods ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We conducted additional experiments with non-IID algorithms, including FedNova [54] and SCAFFOLD [55]. ", "page_idx": 19}, {"type": "table", "img_path": "M1PRU0x1Iz/tmp/93ec7425575970d4c9586e08c1644be431e6afc897becca4ce73b8a84dbc98b2.jpg", "table_caption": [], "table_footnote": ["Table 15: Test accuracy $(\\%)$ on various datasets under non-IID settings with $\\alpha=0.1$ . "], "page_idx": 19}, {"type": "text", "text": "A.10 Additional Results of Non-i.i.d. Experiments with Equally-Weighted Metric ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In the main paper, we followed the weighted accuracy metric as described in the pFL-Bench [25]. Here, we provide additional results for non-i.i.d. experiments using an equally-weighted metric on CIFAR-100. ", "page_idx": 20}, {"type": "table", "img_path": "M1PRU0x1Iz/tmp/8de457c3b765086e98d0f1c5aa8a2211161f137b8634795024fa11682ee34cba.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Table 16: Test accuracy $(\\%)$ on CIFAR-100 with $\\alpha=5.0$ and $\\alpha=0.1$ using an equally-weighted metric. ", "page_idx": 20}, {"type": "text", "text": "A.11 Experiments on Extreme label-skew Settings ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The table below presents the results across different datasets and partitioning strategies, specifically the quantity-based label skew settings described in [56]. Here, $C$ is the number of different labels held by each client. In extreme label skew cases, such as $C=1$ , where data labels are highly partitioned, our algorithm shows slightly lower performance on CIFAR-100 $C=1$ ). However, in all other cases, our algorithm demonstrates improved performance. ", "page_idx": 21}, {"type": "table", "img_path": "M1PRU0x1Iz/tmp/61b1bcde07d97a218f43f82c1ac7b2a47879b5b86f0b869dd7640c936274bb78.jpg", "table_caption": [], "table_footnote": ["Table 17: Test accuracy $(\\%)$ on CIFAR-100 and SVHN datasets under quantity-based label skew settings. $C$ denotes the number of different labels held by each client. "], "page_idx": 21}, {"type": "text", "text": "B Proof of Proposition 1 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Consider the federated meta-policy loss derived from the updated weight $w_{n}^{k}$ for client $k$ at step $n$ using a first-order Taylor expansion: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\ell_{D_{k}^{\\mathrm{sa}}}(w_{g_{r+1}})\\approx\\ell_{D_{k}^{\\mathrm{va}}}(w_{n}^{k})+\\nabla\\ell_{D_{k}^{\\mathrm{va}}}(w_{n}^{k})^{T}(w_{g_{r+1}}-w_{n}^{k})\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "When calculating the policy gradient of this loss with respect to $\\theta_{n-1}^{k}$ for client $k$ , the first-order gradient approximation is as follows: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\quad}&{\\displaystyle\\partial(\\nabla\\ell_{D_{k}^{\\mathrm{su}}}(w_{n}^{k})^{T}\\nabla\\ell_{t_{p_{\\theta_{n-1}^{k}}}(D_{k,n-1}^{\\mathrm{min}})}(w_{n-1}^{k}))}\\\\ &{\\quad-\\alpha_{k}\\cdot l r\\frac{\\partial}{\\partial{}\\theta_{n-1}^{k}}}&{,}\\\\ &{\\quad}&{\\displaystyle\\mathrm{where}\\enskip w_{n}^{k}=w_{g_{r}}-l r\\cdot g_{w_{0}^{k}}^{\\mathrm{aug}}-\\ldots-l r\\cdot g_{w_{n-1}^{k}}^{\\mathrm{aug}},}\\\\ &{\\quad}&{\\displaystyle g_{w_{n}^{k}}^{\\mathrm{aug}}=\\nabla\\ell_{t_{p_{\\theta_{n}^{k}}}}(D_{k,n}^{\\mathrm{tai}})(w_{n}^{k}),}\\\\ &{\\quad}&{w_{g_{r+1}}=\\sum\\alpha_{k}w_{N}^{k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{\\partial\\ell_{D_{k}^{\\mathrm{val}}}(w_{g_{r+1}})}{\\partial\\theta_{n-1}^{k}}\\approx\\frac{\\partial\\ell_{D_{k}^{\\mathrm{val}}}(w_{n}^{k})}{\\partial\\theta_{n-1}^{k}}+\\frac{\\partial(\\nabla\\ell_{D_{k}^{\\mathrm{val}}}(w_{n}^{k})^{T}(w_{g_{r+1}}-w_{n}^{k}))}{\\partial\\theta_{n-1}^{k}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "First, let\u2018s calculate starting from the left term in Eq. (19) ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\frac{\\partial\\ell_{D_{n}}(w_{n}^{k})}{\\partial\\theta_{n-1}^{k}}}&{}\\\\ &{=\\nabla\\ell_{D_{k}^{\\prime}}(w_{n}^{k})^{T}\\cdot\\frac{\\partial w_{n}^{k}}{\\partial\\theta_{n-1}^{k}}}\\\\ &{=\\nabla\\ell_{D_{k}^{\\prime}}(w_{n}^{k})^{T}\\cdot\\frac{\\partial(w_{n-1}^{k}-I^{*}\\cdot p_{w_{n-1}^{k}}^{u})}{\\partial\\theta_{n-1}^{k}}}\\\\ &{=-I^{*}\\cdot\\nabla\\ell_{D_{k}^{\\prime}}(w_{n}^{k})^{T}\\frac{\\partial(\\theta_{n-1}^{k}-I^{*})}{\\partial\\theta_{n-1}^{k}}}\\\\ &{=-I^{*}\\cdot\\nabla\\ell_{D_{k}^{\\prime}}(w_{n}^{k})^{T}\\frac{\\partial(\\theta_{n-1}^{k})}{\\partial\\theta_{n-1}^{k}}}\\\\ &{=-I^{*}\\cdot\\frac{\\partial(\\nabla\\ell_{D_{k}^{\\prime}}(w_{n}^{k})^{T}\\cdot\\nabla\\ell_{\\ell_{D_{k}^{\\prime}}}(D_{\\ell_{k}^{\\prime},n-1}^{k})(w_{n-1}^{k}))}{\\partial\\theta_{n-1}^{k}}}\\\\ &{=-I^{*}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "(using the chain rule) ", "page_idx": 22}, {"type": "equation", "text": "$$\n({\\mathrm{using~}}{\\frac{\\partial w_{n-1}^{k}}{\\partial\\theta_{n-1}^{k}}}=0\\;)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "(using the definition of $g_{w_{n-1}^{k}}^{\\mathrm{aug}}$ ", "page_idx": 22}, {"type": "text", "text": "Next, we calculate the right term in Eq. (19) ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{\\partial(\\nabla\\ell_{D_{k}^{\\mathrm{val}}}(w_{n}^{k})^{T}(w_{g_{r+1}}-w_{n}^{k}))}{\\partial\\theta_{n-1}^{k}}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\n=\\frac{\\partial w_{n}^{k}}{\\partial\\theta_{n-1}^{k}}\\cdot\\frac{\\partial\\nabla\\ell_{D_{k}^{\\mathrm{val}}}(w_{n}^{k})^{T}}{\\partial w_{n}^{k}}\\cdot(w_{g_{r+1}}-w_{n}^{k})+\\nabla\\ell_{D_{k}^{\\mathrm{val}}}(w_{n}^{k})^{T}\\cdot\\frac{\\partial(w_{g_{r+1}}-w_{n}^{k})}{\\partial\\theta_{n-1}^{k}}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "(using the chain rule and $(f g)^{\\prime}=f^{\\prime}g+f g^{\\prime})$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n=\\nabla\\ell_{D_{k}^{\\mathrm{val}}}(w_{n}^{k})^{T}\\cdot\\frac{\\partial(w_{g_{r+1}}-w_{n}^{k})}{\\partial\\theta_{n-1}^{k}}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "(treating $\\nabla\\ell_{D_{k}^{\\mathrm{val}}}(w_{n}^{k})$ as a constant for first-order) ", "page_idx": 23}, {"type": "equation", "text": "$$\n=\\nabla\\ell_{D_{k}^{\\mathrm{val}}}(w_{n}^{k})^{T}\\cdot\\frac{\\partial(\\boldsymbol{\\alpha}_{k}\\cdot\\boldsymbol{w}_{N}^{k}-\\boldsymbol{w}_{n}^{k})}{\\partial\\theta_{n-1}^{k}}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "(using the definition of $w_{g_{r+1}}$ in (18)) ", "page_idx": 23}, {"type": "equation", "text": "$$\n=\\nabla\\ell_{D_{k}^{\\mathrm{val}}}(w_{n}^{k})^{T}\\cdot\\frac{\\partial((\\alpha_{k}-1)\\cdot w_{n}^{k})}{\\partial\\theta_{n-1}^{k}}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "(using the definition of wkN in (16)) and treating gawukg as a constant for first-order) ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=(\\alpha_{k}-1)\\cdot\\nabla\\ell_{D_{k}^{\\mathrm{val}}}(w_{n}^{k})^{T}\\cdot\\frac{\\partial(w_{n}^{k})}{\\partial\\theta_{n-1}^{k}}}\\\\ &{=(\\alpha_{k}-1)\\cdot\\nabla\\ell_{D_{k}^{\\mathrm{val}}}(w_{n}^{k})^{T}\\cdot\\frac{\\partial(w_{n-1}^{k}-l r\\cdot g_{w_{n-1}^{k}}^{\\mathrm{aug}})}{\\partial\\theta_{n-1}^{k}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "(using the definition of $w_{n}^{k}$ in (16)) ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=(\\alpha_{k}-1)\\cdot\\nabla\\ell_{D_{k}^{\\mathrm{sa}}}(w_{n}^{k})^{T}\\cdot\\frac{\\partial(-l r\\cdot g_{w_{n-1}^{\\mathrm{sa}}}^{\\mathrm{ug}})}{\\partial\\theta_{n-1}^{k}}}\\\\ &{(\\mathrm{using~}\\frac{\\partial w_{n-1}^{k}}{\\partial\\theta_{n-1}^{k}}=0)}\\\\ &{=-l r\\cdot(\\alpha_{k}-1)\\cdot\\nabla\\ell_{D_{k}^{\\mathrm{sa}}}(w_{n}^{k})^{T}\\cdot\\frac{\\partial(g_{w_{n-1}^{k}}^{\\mathrm{sa}})}{\\partial\\theta_{n-1}^{k}}}\\\\ &{=-l r\\cdot(\\alpha_{k}-1)\\cdot\\frac{\\partial(\\nabla\\ell_{D_{k}^{\\mathrm{sa}}}(w_{n}^{k})^{T}\\cdot\\nabla\\ell_{U_{\\ell_{n-1}}}(D_{k,n-1}^{\\mathrm{ra}})(w_{n-1}^{k}))}{\\partial\\theta_{n-1}^{k}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "By combining Eq. (24) and Eq. (33), ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{\\partial\\ell_{D_{k}^{\\mathrm{val}}}(w_{g_{r+1}})}{\\partial\\theta_{n-1}^{k}}\\approx-\\alpha_{k}\\cdot l r\\frac{\\partial(\\nabla\\ell_{D_{k}^{\\mathrm{val}}}(w_{n}^{k})^{T}\\nabla\\ell_{t_{p_{\\theta_{n-1}}^{k}}}(D_{k,n-1}^{\\mathrm{rain}})^{\\left(w_{n-1}^{k}\\right)})}{\\partial\\theta_{n-1}^{k}}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "C Proof of Adaptive Policy Search ", "text_level": 1, "page_idx": 24}, {"type": "equation", "text": "$$\n\\theta_{g_{r+1}}\\approx\\theta_{g_{r}}-\\eta\\lambda\\frac{\\partial}{\\partial\\theta_{0}^{k}}\\mathbb{E}\\Bigg[\\sum_{j=0}^{n}L_{k,j}-\\frac{\\lambda}{2}\\sum_{j=0}^{n}\\sum_{s=0}^{j-1}\\langle\\nabla L_{k,j}\\cdot\\nabla L_{k,s}\\rangle\\Bigg],\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $L_{k,j}\\equiv\\ell_{D_{k,j}^{\\mathrm{val}}}^{\\mathrm{FMPL}}(\\theta_{0}^{k})$ represents the the federated meta-policy loss in computed on the client $k$ \u2019s $j$ -th validation data batch using the global policy parameters $\\theta_{0}^{k}$ received from the server. $\\langle\\nabla L_{k,j}$ \u00b7 $\\nabla L_{k,s}\\rangle$ represents the dot-product between policy gradients on the client $k$ . ", "page_idx": 24}, {"type": "text", "text": "Proof. We refer to the proof of reptile [30, 57] and use the following definitions: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\boldsymbol{\\theta}_{g_{r+1}}=\\boldsymbol{\\theta}_{g_{r}}+\\eta\\sum_{\\mathbf{\\theta}}\\alpha_{k}(\\theta_{*}^{k}-\\theta_{g_{r}})}\\\\ &{\\boldsymbol{\\theta}_{h+1}^{k}=\\theta_{h}^{k}-\\lambda\\boldsymbol{\\ell}_{F_{h}}^{k}(\\theta_{n}^{k})}\\\\ &{\\quad g_{n}^{k}=\\boldsymbol{\\ell}_{F_{n}}^{k}(\\theta_{n}^{k})}\\\\ &{\\quad\\boldsymbol{\\overline{{g}}}_{h}^{k}=\\boldsymbol{\\ell}_{F_{n}}^{k}(\\theta_{n}^{k})}\\\\ &{\\quad\\boldsymbol{\\overline{{H}}}_{n}^{k}=\\boldsymbol{\\ell}_{F_{n}}^{\\prime\\prime}(\\theta_{0}^{k})}\\\\ &{\\quad\\boldsymbol{\\theta}_{g_{r}}=\\boldsymbol{\\theta}_{0}^{k}}\\\\ &{\\quad\\theta_{*}^{k}=\\theta_{h+1}^{k}}\\\\ &{\\quad L_{k,j}=\\ell_{D_{h}^{\\mathrm{HD}}}^{\\mathrm{HD}}(\\theta_{0}^{k})=\\boldsymbol{\\ell}_{F_{j}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "(abbreviation for FMPL) ", "page_idx": 24}, {"type": "text", "text": "First, let\u2018s calculate $g_{n}^{k}$ in Eq. (38) to ${\\cal O}(\\lambda^{2})$ as follows: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{g_{n}^{k}=\\ell_{F_{n}}^{\\prime}(\\theta_{0}^{k})+\\ell_{F_{n}}^{\\prime\\prime}(\\theta_{0}^{k})(\\theta_{n}^{k}-\\theta_{0}^{k})+O(\\lambda^{2})}}\\\\ {{\\displaystyle{\\quad=\\overline{{{g}}}_{n}^{k}+\\overline{{{H}}}_{n}^{k}(\\theta_{n}^{k}-\\theta_{0}^{k})+O(\\lambda^{2})}}}\\\\ {{\\displaystyle{\\quad=\\overline{{{g}}}_{n}^{k}-\\lambda\\overline{{{H}}}_{n}^{k}\\sum_{j=0}^{n-1}g_{j}^{k}+O(\\lambda^{2})}}}\\\\ {{\\displaystyle{\\quad=\\overline{{{g}}}_{n}^{k}-\\lambda\\overline{{{H}}}_{n}^{k}\\sum_{j=0}^{n-1}\\overline{{{g}}}_{j}^{k}+O(\\lambda^{2})}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "(using the definitions of (39) and (40)) ", "page_idx": 24}, {"type": "equation", "text": "$$\n(\\operatorname{using}\\,\\theta_{n}^{k}-\\theta_{0}^{k}=-\\lambda\\sum_{j=0}^{n-1}g_{j}^{k})\n$$", "text_format": "latex", "page_idx": 24}, {"type": "equation", "text": "$$\n({\\mathrm{using~}}g_{j}^{k}={\\overline{{g}}}_{j}^{k}+{\\cal O}(\\lambda^{2}))\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Then, we define the following in (36): ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\theta_{*}^{k}=\\theta_{g_{r}}-\\lambda g_{i n n e r}^{k}}\\\\ {\\theta_{n+1}^{k}=\\theta_{0}^{k}-\\lambda g_{i n n e r}^{k}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "(using the definitions of (41) and (42) ) ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{g_{i n n e r}^{k}=\\displaystyle\\sum_{j=0}^{n}\\ell_{F_{j}}^{\\prime}(\\theta_{j}^{k})}}\\\\ {{\\displaystyle{=\\sum_{j=0}^{n}g_{j}^{k}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "equation", "text": "$$\n(\\operatorname{using}\\,\\theta_{n+1}^{k}=\\theta_{0}^{k}-\\lambda\\sum_{j=0}^{n}\\ell_{F_{j}}^{\\prime}(\\theta_{j}^{k}))\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "(using the definition of $\\ell_{F_{j}}^{\\prime}(\\theta_{j}^{k})$ in (38)) ", "page_idx": 24}, {"type": "text", "text": "By combining Eq. (36) and Eq. (49) and taking the expectation over the clients and local updates, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\theta_{g_{r+1}}\\approx\\theta_{g_{r}}+\\eta\\mathbb{E}[-\\lambda g_{i n n e r}^{k}]}}\\\\ &{\\qquad\\approx\\theta_{g_{r}}-\\eta\\lambda\\mathbb{E}[\\displaystyle\\sum_{j=0}^{n}\\ell_{F_{j}}^{\\prime}(\\theta_{0}^{k})-\\lambda\\displaystyle\\sum_{j=0}^{n}\\sum_{s=0}^{j-1}\\ell_{F_{j}}^{\\prime\\prime}(\\theta_{0}^{k})\\cdot\\ell_{F_{s}}^{\\prime}(\\theta_{0}^{k})]}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\approx\\theta_{g_{r}}-\\eta\\lambda\\mathbb{E}[\\displaystyle\\sum_{j=0}^{n}\\ell_{F_{j}}^{\\prime}(\\theta_{0}^{k})-\\frac{\\lambda}{2}\\displaystyle\\sum_{j=0}^{n}\\sum_{s=0}^{j-1}(\\ell_{F_{j}}^{\\prime\\prime}(\\theta_{0}^{k})\\cdot\\ell_{F_{s}}^{\\prime}(\\theta_{0}^{k})+\\ell_{F_{j}}^{\\prime}(\\theta_{0}^{k})\\cdot\\ell_{F_{s}}^{\\prime\\prime}(\\theta_{0}^{k}))]}\\\\ &{\\approx\\theta_{g_{r}}-\\eta\\lambda\\displaystyle\\frac{\\partial}{\\partial\\theta_{0}^{k}}\\mathbb{E}\\left[\\displaystyle\\sum_{j=0}^{n}L_{k,j}-\\frac{\\lambda}{2}\\displaystyle\\sum_{j=0}^{n}\\sum_{s=0}^{j-1}\\langle\\nabla L_{k,j}\\cdot\\nabla L_{k,s}\\rangle\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Do not remove the checklist: The papers not including the checklist will be desk rejected. ", "page_idx": 26}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We stated the paper\u2019s contributions and scope in Section 1. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 26}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: We stated the paper\u2019s limitations in Section 5. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 26}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: We provided the paper\u2019s proof in appendix B and C . ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 27}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: We provide algorithms including 1 and 2, and their experimental settings in Appendix A. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 27}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We provide the code for our paper and its instructions in a zip file. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 28}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We provide detailed experimental settings in Appendix A Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 28}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: We provided our algorithm\u2019s accuracies with their variances in Table 1. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We provided computation time and communication cost in 4 and A. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 29}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: Our research proposes a federated data augmentation method, is non-harmful, and complies with the Code of Ethics. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 29}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: Our research proposes a federated data augmentation method that does not have negative impacts on society. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: Our research proposes a federated data augmentation method that does not have a high risk for misuse. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 30}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We have cited all the assets used in the paper. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 30}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 31}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: our paper does not release new assets. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 31}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 31}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 31}]