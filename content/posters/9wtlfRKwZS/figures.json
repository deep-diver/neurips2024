[{"figure_path": "9wtlfRKwZS/figures/figures_64_1.jpg", "caption": "Figure 1: Training loss and training accuracy of Vision Transformers with different numbers of heads. (a) gives the curves of training loss, while (b) gives the curves of training accuracy.", "description": "This figure shows the training loss and training accuracy curves for Vision Transformers with varying numbers of heads, while keeping the number of layers constant at 6.  The x-axis represents the training epoch, and the y-axis shows either training loss (left panel) or training accuracy (right panel).  Multiple curves are shown, each representing a different number of heads (ranging from 4 to 40). The figure demonstrates the relationship between the model's width (number of heads) and its training performance. As the number of heads increases, the model typically achieves lower training loss and higher training accuracy.", "section": "H Experiments"}, {"figure_path": "9wtlfRKwZS/figures/figures_64_2.jpg", "caption": "Figure 2: Training loss and training accuracy of Vision Transformers with different depths. (a) gives the curves of training loss, while (b) gives the curves of training accuracy.", "description": "This figure shows the training curves of Vision Transformer models with varying depths (number of layers) while keeping the number of heads constant at 8.  The left panel displays the training loss, demonstrating how the loss decreases over the training epochs. The right panel shows the training accuracy, illustrating how the model's performance improves as the number of training epochs increases.  Different curves represent different depths, allowing for a comparison of the impact of depth on training performance.", "section": "H Experiments"}]