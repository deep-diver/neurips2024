[{"figure_path": "anyZgGLQ6n/tables/tables_5_1.jpg", "caption": "Table 1: Averaged normalized scores on Gym locomotion and AntMaze tasks over five random seeds.", "description": "This table presents the average normalized scores achieved by different offline reinforcement learning algorithms across various locomotion and AntMaze tasks. The results are averaged over five independent runs with different random seeds to ensure reliability and statistical significance.  The algorithms compared include BC, MOPO, OneStep, TD3BC, CQL, IQL, OSR, SDC, and the proposed SCAS method.  The table allows for a quantitative comparison of the performance of these methods on diverse, challenging benchmark tasks.", "section": "6.2 Comparisons on Offline RL Benchmarks"}, {"figure_path": "anyZgGLQ6n/tables/tables_7_1.jpg", "caption": "Table 1: Averaged normalized scores on Gym locomotion and AntMaze tasks over five random seeds.", "description": "This table presents the average normalized scores achieved by different offline reinforcement learning algorithms across various locomotion and AntMaze tasks.  The scores are normalized against random and expert policy returns, providing a standardized performance metric. Results are averaged over five random seeds for each algorithm and task combination, reflecting performance consistency and stability. The table also includes the runtime of each algorithm, along with an indication of whether hyperparameter tuning was employed, offering insights into computational efficiency and generalizability.", "section": "6.2 Comparisons on Offline RL Benchmarks"}, {"figure_path": "anyZgGLQ6n/tables/tables_20_1.jpg", "caption": "Table 2: Hyperparameters in SCAS.", "description": "This table lists the hyperparameters used in the SCAS algorithm, categorized into policy training, dynamics training, and architecture.  It specifies the optimizer, learning rates, batch size, discount factor, gradient steps, target network update rate, policy update frequency, number of critics, inverse temperature (alpha), balance coefficient (lambda), and noise scale (sigma).  The architecture section details the input and output layers for the actor, critic, and dynamics networks.", "section": "Implementation Details"}, {"figure_path": "anyZgGLQ6n/tables/tables_22_1.jpg", "caption": "Table 3: Comparisons in modified maze2d-open-v0 over five random seeds.", "description": "This table presents a comparison of the performance of several offline reinforcement learning algorithms on a modified version of the maze2d-open-v0 environment. The modification involves removing transitions that contain states within a specific region, creating out-of-distribution (OOD) states during testing. The algorithms are evaluated based on two metrics: the average number of steps spent in OOD states during testing and the D4RL score, which is a normalized performance metric. The results show that SCAS significantly outperforms other algorithms in terms of both metrics, indicating better robustness to OOD states.", "section": "E.2 Additional Results on OOD State Correction"}, {"figure_path": "anyZgGLQ6n/tables/tables_22_2.jpg", "caption": "Table 1: Averaged normalized scores on Gym locomotion and AntMaze tasks over five random seeds.", "description": "This table presents a comparison of the average normalized scores achieved by different offline reinforcement learning algorithms across various locomotion and AntMaze tasks from the D4RL benchmark.  The scores are averaged over five separate runs with different random seeds to provide a measure of the algorithms' robustness and performance consistency. The algorithms compared include several state-of-the-art methods, as well as SCAS (the method proposed in the paper). The table also indicates whether hyperparameter tuning was performed for each algorithm.", "section": "6.2 Comparisons on Offline RL Benchmarks"}, {"figure_path": "anyZgGLQ6n/tables/tables_23_1.jpg", "caption": "Table 1: Averaged normalized scores on Gym locomotion and AntMaze tasks over five random seeds.", "description": "This table presents the average normalized scores achieved by different offline reinforcement learning algorithms across various locomotion and AntMaze tasks.  The scores are normalized relative to random and expert policy returns, providing a standardized performance measure. Results are averaged over five random seeds to account for variability. The table offers a comprehensive comparison of the performance of several methods including SCAS (the proposed method), highlighting SCAS's competitive performance compared to baselines on multiple benchmarks.", "section": "6.2 Comparisons on Offline RL Benchmarks"}, {"figure_path": "anyZgGLQ6n/tables/tables_23_2.jpg", "caption": "Table 6: Comparisons on the D4RL benchmark. Here +SCAS means adding the SCAS regularizer. The results are averaged over 5 random seeds.", "description": "This table compares the performance of several offline reinforcement learning algorithms on the D4RL benchmark. It shows the average normalized scores achieved by CQL, TD3BC, and IQL, both with and without the SCAS regularizer. The results highlight the performance improvement achieved by adding the SCAS regularizer to these algorithms across different tasks in the D4RL benchmark.", "section": "6.2 Comparisons on Offline RL Benchmarks"}]