[{"type": "text", "text": "Offline Reinforcement Learning with OOD State Correction and OOD Action Suppression ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yixiu Mao1, Qi Wang1, Chen Chen1, $\\mathbf{Y}\\mathbf{u}\\mathbf{n}\\,\\mathbf{Q}\\mathbf{u}^{1}$ , Xiangyang Ji1 1Department of Automation, Tsinghua University myx21@mails.tsinghua.edu.cn, xyji@tsinghua.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In offline reinforcement learning (RL), addressing the out-of-distribution (OOD) action issue has been a focus, but we argue that there exists an OOD state issue that also impairs performance yet has been underexplored. Such an issue describes the scenario when the agent encounters states out of the offline dataset during the test phase, leading to uncontrolled behavior and performance degradation. To this end, we propose SCAS, a simple yet effective approach that unifies OOD state correction and OOD action suppression in offline RL. Technically, SCAS achieves value-aware OOD state correction, capable of correcting the agent from OOD states to high-value in-distribution states. Theoretical and empirical results show that SCAS also exhibits the effect of suppressing OOD actions. On standard offline RL benchmarks, SCAS achieves excellent performance without additional hyperparameter tuning. Moreover, beneftiing from its OOD state correction feature, SCAS demonstrates enhanced robustness against environmental perturbations. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Deep reinforcement learning (RL) shows promise in solving sequential decision-making problems, gaining increasing interest for real-world applications [42, 57, 63, 53, 7]. However, deploying RL algorithms in extensive scenarios poses persistent challenges, such as risk-sensitive exploration [13] and time-consuming episode collection [27]. Recent advances view offilne RL as a hopeful solution to these challenges [34]. Offline RL aims to learn a policy from a fixed dataset without further interactions [32]. It can tap into existing large-scale datasets for safe and efficient learning [23, 37, 50]. ", "page_idx": 0}, {"type": "text", "text": "In offline RL research, a well-known concern is the out-of-distribution (OOD) action issue: the evaluation of OOD actions causes extrapolation error [12], which can be exacerbated by bootstrapping and result in severe value overestimation [34]. To address this issue, a large body of work has emerged to directly or indirectly suppress OOD actions during training, employing various techniques such as policy constraint [12, 30, 10], value penalization [31, 2, 6], and in-sample learning [29, 14, 71]. ", "page_idx": 0}, {"type": "text", "text": "Distinguished from most previous works, this paper argues that, apart from the OOD action issue, there exists an OOD state issue that also impairs performance yet has received limited attention in the field. Such an issue refers to the agent encountering states out of the offline dataset during the policy deployment phase (i.e., test phase). The occurrence of OOD states can be attributed to OOD actions, stochastic environments, and real-world perturbations. Since typical offline RL algorithms do not involve policy training in OOD states, the agent tends to behave in an uncontrolled manner once entering OOD states in the test phase. This can further exacerbate the state deviation from the offline dataset and lead to severe degradation in performance [34, 75]. ", "page_idx": 0}, {"type": "text", "text": "In mitigating this OOD state issue, existing limited work attempts to train the policy to correct the agent from OOD states to in-distribution (ID) states [75, 22]. Technically, Zhang et al. [75] construct a dynamics model and a state transition model and align them to guide the agent to ID regions, while ", "page_idx": 0}, {"type": "image", "img_path": "anyZgGLQ6n/tmp/cd5f021ce9f7e797f60a0e5e8d19b52afdd81aec58901e16fbec68262bc3c309.jpg", "img_caption": ["Figure 1: The resulting state distributions of offilne RL algorithms and optimal values of states. (a,b,c) The state distributions generated by the learned policies of various algorithms compared with that of the offline dataset on halfcheetah-medium-expert. (d) The corresponding optimal value of each state, which is obtained by running TD3 online to convergence. SCAS-induced state distribution is almost entirely within the support of the offilne distribution and avoids the low-value areas, while CQL and TD3BC tend to produce OOD states with extremely low values. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Jiang et al. [22] resort to an inverse dynamics model for policy constraint. However, they deal with the OOD state and OOD action issues separately, requiring extra OOD action suppression components and complex distribution modeling, which sacrifices computational efficiency and algorithmic simplicity. Moreover, correcting the agent to all ID states impartially could be problematic, especially when the dataset contains substantial suboptimal states. As a result, the performance of prior methods also leaves considerable room for improvement. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we aim to address these two fundamental OOD issues simultaneously by proposing a simple yet effective approach for offline RL. We term our method SCAS due to its integration of OOD State Correction and OOD Action Suppression. We start with solving an analytical form of a value-aware state transition distribution, which is within the dataset support but skewed toward highvalue states. Then, we align it with the dynamics induced by the trained policy on perturbed states via KL divergence. This operation intends to correct the agent from OOD states to high-value ID states, a concept we refer to as value-aware OOD state correction. Through some derivations, it also eliminates the necessity of training a multi-modal state transition model. Furthermore, we show theoretically and empirically that, while designed for OOD state correction, SCAS regularization also exhibits the effect of OOD action suppression. We evaluate SCAS on the offilne RL benchmarks including D4RL [9] and NeoRL [49]. SCAS achieves excellent performance with consistent hyperparameters without additional tuning. Moreover, benefiting from its OOD state correction ability, SCAS demonstrates improved robustness against environmental perturbations. ", "page_idx": 1}, {"type": "text", "text": "To summarize, the main contributions of this work are: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We systematically analyze the underexplored OOD state issue in offline RL and propose a simple yet effective approach SCAS unifying OOD state correction and OOD action suppression. \u2022 Our approach achieves value-aware OOD state correction, which circumvents modeling complex distributions and significantly improves performance over vanilla OOD state correction methods. \u2022 Empirically1, our approach demonstrates superior performance on standard offilne RL benchmarks and enhanced robustness in perturbed environments without additional hyperparameter tuning. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In reinforcement learning, we generally characterize the environment as a Markov Decision Process (MDP) $\\mathcal{M}\\,=\\,(S,\\mathcal{A},P,R,\\gamma,d_{0})$ , with state space $\\boldsymbol{S}$ , action space $\\boldsymbol{\\mathcal{A}}$ , transition dynamics $P:S\\times A\\to\\Delta(S)$ , reward function $R:S\\times A\\to\\mathbb{R}$ , discount factor $\\gamma\\in[0,1)$ , and initial state distribution $d_{0}$ [61]. The agent interacts with the environment and seeks a policy $\\pi:S\\to\\Delta(A)$ to maximize the expected discounted return $\\eta(\\pi)$ : ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\eta(\\pi)=\\mathbb{E}_{s_{0}\\sim d_{0},a_{t}\\sim\\pi(\\cdot|s_{t}),s_{t+1}\\sim P(\\cdot|s_{t},a_{t})}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}R(s_{t},a_{t})\\right].\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "For any policy $\\pi$ , we defin he value function as $\\begin{array}{r}{V^{\\pi}(s)=\\mathbb{E}_{\\pi}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}R(s_{t},a_{t})|s_{0}=s\\right]}\\end{array}$ and the state-action value function ( $Q$ -value function) as $\\begin{array}{r}{\\mathcal{Q}^{\\pi}(s,a)=\\mathbb{E}_{\\pi}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}R(s_{t},a_{t})|s_{0}=s,a_{0}=a\\right]}\\end{array}$ . ", "page_idx": 2}, {"type": "text", "text": "Offline RL. In offline RL, the agent can only access a static dataset $\\mathcal{D}=\\{(s_{t}^{i},a_{t}^{i},s_{t+1}^{i},r_{t}^{i})\\}$ . We denote the empirical behavior policy of $\\mathcal{D}$ by $\\beta(a|s)$ and the empirical dynamics model by $M(s^{\\prime}|s,a)$ , both of which depict the conditional distributions observed in the dataset [12]. Typical actor-critic algorithms [56, 18] evaluate policy $\\pi$ by minimizing Bellman loss: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{L_{Q}(\\theta)=\\mathbb{E}_{(s,a,s^{\\prime})\\sim\\mathcal{D}}[(Q_{\\theta}(s,a)-R(s,a)-\\gamma\\mathbb{E}_{a^{\\prime}\\sim\\pi_{\\phi}(\\cdot|s^{\\prime})}Q_{\\theta^{\\prime}}(s^{\\prime},a^{\\prime}))^{2}],}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\pi_{\\phi}$ and $Q_{\\theta}$ are the parameterized policy and $Q$ function, and $Q_{\\theta^{\\prime}}$ is a target network whose parameters are updated via Polyak averaging [42]. ", "page_idx": 2}, {"type": "text", "text": "Simultaneously, policy improvement in policy iteration is achieved via maximizing the Q-value: ", "page_idx": 2}, {"type": "equation", "text": "$$\nL_{\\pi}(\\phi)=-\\mathbb{E}_{s\\sim\\mathcal{D},a\\sim\\pi_{\\phi}}\\left[Q_{\\theta}\\left(s,a\\right)\\right].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "OOD action issue. In offilne RL, OOD actions refer to actions outside the support of the behavior policy $\\beta(\\cdot|s)$ at a specific state $s\\in\\mathcal{D}$ . Since the $\\mathrm{^Q}$ -values of OOD actions can be poorly estimated and the policy improvement is towards maximizing the estimated $Q_{\\theta}$ , the resulting policy tends to prioritize the OOD actions with overestimated values, leading to poor performance [12]. ", "page_idx": 2}, {"type": "text", "text": "3 OOD State Correction ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The following focuses on the OOD state issue and OOD state correction in offilne RL. In Section 3.1, we systematically analyze the OOD state issue, introduce the concept of OOD state correction, and point out limitations of prior methods. Then we present the proposed approach SCAS in Section 3.2. ", "page_idx": 2}, {"type": "text", "text": "3.1 OOD State Issue in Offline RL ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In offilne RL, OOD states refer to states not in the offilne dataset. The OOD state issue (Definition 1) pertains to scenarios where the agent enters OOD states during the test phase, potentially resulting in catastrophic failure [34]. However, such a topic is rarely investigated in the literature, and existing studies lack deep insights. We mathematically formulate the OOD state issue as follows. ", "page_idx": 2}, {"type": "text", "text": "Definition 1 (OOD state issue). There exists $s\\in{\\mathcal{S}}$ , such that $d_{\\mathcal{M}_{\\tau}}^{\\pi}(s)>0$ and $d_{\\mathcal{D}}(s)=0,$ , where $\\mathcal{M}_{T}$ is the MDP of the test environment, $\\pi$ is any learned policy, $d_{\\mathcal{M}_{\\tau}}^{\\pi^{\\prime}}$ is the state probability density induced by $\\pi$ in $\\mathcal{M}_{T}$ , and $d_{D}$ is the state probability density in the offline dataset. ", "page_idx": 2}, {"type": "text", "text": "Origins and consequence of OOD states. During the test phase, the OOD states occur primarily in three scenarios: (i) OOD actions: the learned policy, not perfectly constrained within the support of the behavior policy, executes unreliable OOD actions, leading to OOD states. (ii) Stochastic environment: the initial state of the actual environment may fall outside the offline dataset. In addition, stochastic dynamics can also lead to states outside the dataset, even when taking ID actions in ID states. (iii) Perturbations: commonly seen in real-world robot applications, some unexpected perturbations can propel the agent into OOD states (e.g., wind, human interference). ", "page_idx": 2}, {"type": "text", "text": "During offline training, the typical Bellman updates involve only ID states, and the policies in OOD states are not trained. As a result, when encountering OOD states in the test phase, the agent would exhibit uncontrolled behavior, and the state deviation from the offline dataset can be further exacerbated over time steps, severely degrading performance [34]. ", "page_idx": 2}, {"type": "text", "text": "OOD state correction. To mitigate this OOD state issue, an intuitive solution is to train a policy capable of correcting the agent from OOD states to $\\mathrm{ID}$ states, a concept known as $o o D$ state correction [75]. Specifically, during offilne training, we can perturb the original state $s$ in the dataset into $\\hat{s}$ to generate substantial OOD states. Then consider the scenario where the agent starts from $\\hat{s}$ , follows the trained policy $\\pi$ , and transitions to the next state $\\hat{s}^{\\prime}$ . To reduce state deviation, $\\hat{s}^{\\prime}$ is expected to be close to the offline dataset. Thus we can align the distribution of $\\hat{s}^{\\prime}$ with an ID state distribution to regularize the policy and achieve OOD state correction. ", "page_idx": 2}, {"type": "text", "text": "Continuing the above train of thought, SDC [75] generates the $\\mathrm{ID}$ state distribution by feeding the original state $s$ into a trained state transition model $N(s^{\\prime}|s)$ of the dataset. This model characterizes the conditional state transition distribution in the dataset and is implemented by a conditional variational auto-encoder (CVAE) [58]. After pretraining a dynamics model $M(s^{\\prime}|s,\\bar{a})$ and the state transition model $N(s^{\\prime}|s)$ , SDC introduces the following policy regularizer for OOD state correction: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\pi}\\underset{s\\sim\\mathcal{D}}{\\mathbb{E}}~\\underset{\\hat{s}\\sim\\mathcal{N}_{\\sigma}(s)}{\\mathbb{E}}\\left[\\mathrm{MMD}(M(\\cdot|\\hat{s},\\pi(\\cdot|\\hat{s})),N(\\cdot|s))\\right],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\hat{s}$ is a Gaussian noise perturbed version of the original state $s,\\sigma$ is the standard deviation of the Gaussian, $M(\\cdot|\\hat{s},\\pi(\\cdot|\\hat{s}))$ is shorthand for $\\mathbb{E}_{\\hat{a}\\sim\\pi(\\cdot|\\hat{s})}M(\\cdot|\\hat{s},\\hat{a})$ , and MMD is the maximum mean discrepancy measure. More recently, OSR [22] directly aligns the trained policy distribution at the perturbed state with a CVAE inverse dynamics model to constrain the policy in OOD states. ", "page_idx": 3}, {"type": "text", "text": "Limitations. However, the regularizers of prior methods are only designed to deal with this OOD state issue. To mitigate OOD actions, they require an additional conservative $\\mathrm{^Q}$ learning (CQL) term [31] in value estimation to penalize Q-values of OOD actions. In addition, the state transition distribution and the inverse dynamics distribution are multi-modal in many scenarios [43]. The necessity of extra OOD action suppression components and complex distribution modeling compromises their computational efficiency and algorithmic simplicity. Moreover, correcting the agent to all ID states impartially could be problematic, particularly when the offline dataset contains a large portion of suboptimal states. In such cases, vanilla OOD state correction can lead to suboptimal behaviors. Consequently, there is also significant potential for improvement in the performance of prior methods. ", "page_idx": 3}, {"type": "text", "text": "For a more comprehensive discussion of related work, please refer to Appendix A. ", "page_idx": 3}, {"type": "text", "text": "3.2 Value-aware OOD State Correction ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The objective of this work is to formulate a simple yet effective policy regularizer for offilne RL that unifies OOD state correction and OOD action suppression. Moreover, we aim to achieve value-aware OOD state correction, involving the correction of the agent from OOD states to high-value ID states. ", "page_idx": 3}, {"type": "text", "text": "Value-aware state transition. For the $\\mathrm{ID}$ state distribution to which the agent is corrected, we expect a value-aware state transition distribution $N^{*}(\\cdot|s)$ that lies within the support of the dataset state transition distribution $N(\\cdot|s)$ but is skewed toward high-value states $s^{\\prime}$ . To ensure stability and, more importantly, to enable our subsequently designed algorithm to circumvent modeling complex distributions, we seek a soft optimal version of it. To this end, we consider the following problem2: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{N^{*}}\\,\\underset{s\\sim\\mathcal{D}}{\\mathbb{E}}\\left[\\alpha\\underset{s^{\\prime}\\sim N^{*}(\\cdot\\vert s)}{\\mathbb{E}}V(s^{\\prime})-\\mathrm{D}_{\\mathrm{KL}}(N^{*}(\\cdot\\vert s)\\Vert N(\\cdot\\vert s))\\right],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\alpha$ is a hyperparameter to balance the two terms. ", "page_idx": 3}, {"type": "text", "text": "The optimization problem above has a closed-form solution: ", "page_idx": 3}, {"type": "equation", "text": "$$\nN^{*}(s^{\\prime}|s)=\\frac{1}{Z(s)}\\exp\\left(\\alpha V\\left(s^{\\prime}\\right)\\right)N(s^{\\prime}|s),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\begin{array}{r}{Z(s)=\\sum_{s^{\\prime}}\\exp\\left(\\alpha V\\left(s^{\\prime}\\right)\\right)N(s^{\\prime}|s)}\\end{array}$ is a normalization factor. It can be seen from Eq. (6) that $\\operatorname{supp}(N^{*}(\\cdot|s))\\subseteq\\operatorname{supp}(N(\\cdot|s))$ . Note that $\\alpha$ is a key hyperparameter that controls the significance of the values of next states in SCAS\u2019s OOD state correction. As $\\alpha$ increases, $N^{*}(\\cdot|s)$ becomes more skewed toward the optimal $s^{\\prime}$ in the support of $N(\\cdot|s)$ . ", "page_idx": 3}, {"type": "text", "text": "OOD state correction. In order to produce substantial OOD states, we perturb each state $s\\in\\mathcal{D}$ with Gaussian noise ${\\mathcal{N}}(0,\\sigma^{2})$ , resulting in perturbed state $\\hat{s}$ . It is worth noting that the dataset used for RL training remains unchanged. We perturb the states solely to formulate the regularizer. ", "page_idx": 3}, {"type": "text", "text": "We anticipate the following value-aware OOD state correction scenario, where the agent starts from OOD state $\\hat{s}$ , follows the trained policy $\\pi$ , and transitions to the high-value ID state $s^{\\prime}$ in the distribution of $N^{*}(\\cdot|s)$ . To this end, we train the policy $\\pi$ to align the dynamics induced by $\\pi$ on the perturbed state $\\hat{s}$ with the value-aware state transition distribution at the original state $s$ via KL divergence. That is, we regularize $\\pi$ by minimizing: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\pi}\\underset{s\\sim\\mathcal{D}}{\\mathbb{E}}~\\underset{\\hat{s}\\sim\\mathcal{N}_{\\sigma}(s)}{\\mathbb{E}}\\mathrm{D}_{\\mathrm{KL}}\\big(N^{*}(\\cdot|s)\\|M(\\cdot|\\hat{s},\\pi(\\cdot|\\hat{s}))\\big).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "By substituting the analytical solution of $N^{*}$ from Eq. (6) into the KL divergence, we have ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\underset{\\pi}{\\arg\\operatorname*{min}}\\,\\mathrm{D}_{\\mathrm{KL}}(N^{*}(\\cdot|s)\\|M(\\cdot|\\hat{s},\\pi(\\cdot|\\hat{s})))=\\underset{\\pi}{\\arg\\operatorname*{max}}\\underset{s^{\\prime}\\sim N(\\cdot|s)}{\\mathbb{E}}\\,\\left[\\frac{\\exp\\left(\\alpha V\\left(s^{\\prime}\\right)\\right)}{Z(s)}\\log M(s^{\\prime}|\\hat{s},\\pi(\\cdot|\\hat{s}))\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Note that $N$ is the state transition distribution in the dataset, and $s\\sim\\mathcal{D},s^{\\prime}\\sim N(\\cdot|s)$ is equivalent to $(s,s^{\\prime})\\sim\\mathcal{D}$ . Thus minimizing Eq. (7) is equivalent to maximizing following regularizer: ", "page_idx": 4}, {"type": "equation", "text": "$$\nR(\\pi)=\\underset{(s,s^{\\prime})\\sim\\mathcal{D}}{\\mathbb{E}}\\underset{\\hat{s}\\sim\\mathcal{N}_{\\sigma}(s)}{\\mathbb{E}}\\left[\\frac{\\exp\\left(\\alpha V\\left(s^{\\prime}\\right)\\right)}{Z(s)}\\log M(s^{\\prime}|\\hat{s},\\pi(\\cdot|\\hat{s}))\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "As a result, $R(\\pi)$ effectively eliminates the need for a pre-trained multi-modal state transition model $N$ or $N^{*}$ ) and enables direct sampling from the dataset for optimization. ", "page_idx": 4}, {"type": "text", "text": "However, the normalization factor $Z(s)$ in $R(\\pi)$ can be challenging to compute. We note that the regularizer $R(\\pi)$ is derived from the minimization of the KL divergence in Eq. (7). Since we aim to minimize this KL at every state $s$ in $\\mathcal{D}$ and $Z(s)$ only affects the relative weights at different $s$ , it matters less to precisely restore the correct state weights in $\\mathcal{D}$ by computing $Z(s)$ , which is empirically hard to estimate and may bring more instability. Thus, we replace $Z(s)$ in $R(\\pi)$ with an empirical normalizer $\\exp(\\alpha V(s))$ for computational stability: ", "page_idx": 4}, {"type": "equation", "text": "$$\nR_{1}(\\pi)=\\underset{(s,s^{\\prime})\\sim\\mathcal{D}}{\\mathbb{E}}\\underset{\\hat{s}\\sim\\mathcal{N}_{\\sigma}(s)}{\\mathbb{E}}\\left[\\frac{\\exp\\left(\\alpha V\\left(s^{\\prime}\\right)\\right)}{\\exp\\left(\\alpha V\\left(s\\right)\\right)}\\log M(s^{\\prime}|\\hat{s},\\pi(\\cdot|\\hat{s}))\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We provide further rationale behind this choice of the empirical normalizer in Appendix C.1. ", "page_idx": 4}, {"type": "text", "text": "Tractable optimization. Now we shift focus to the optimization of $R_{1}(\\pi)$ . The expectation with respect to $\\pi$ can be moved outside the logarithm by Jensen\u2019s inequality: ", "page_idx": 4}, {"type": "equation", "text": "$$\nR_{1}(\\pi)\\geq\\underset{(s,s^{\\prime})\\sim\\mathcal{D}}{\\mathbb{E}}\\underset{\\hat{s}\\sim\\mathcal{N}_{\\sigma}(s)}{\\mathbb{E}}\\underset{a\\sim\\pi(\\cdot\\vert\\hat{s})}{\\mathbb{E}}\\left[\\frac{\\exp\\left(\\alpha V\\left(s^{\\prime}\\right)\\right)}{\\exp\\left(\\alpha V\\left(s\\right)\\right)}\\log M(s^{\\prime}\\vert\\hat{s},a)\\right],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the equality holds when $\\pi$ is deterministic. In general, it is convenient to maximize the lower bound in Eq. (10) using the reparameterization trick. However, to ensure the equality case in Eq. (10), we opt to train a deterministic policy $\\pi$ . In this case, we can directly maximize $R_{1}(\\pi)$ by computing the gradient of $\\pi$ using automatic differentiation [46]. ", "page_idx": 4}, {"type": "text", "text": "In contrast to model-based RL methods that typically use the learned dynamics model to roll out multi-step trajectories for policy training [20, 73], our algorithm utilizes the dynamics model to propagate the gradient of policy and regularize policy training, resulting in significantly enhanced computational efficiency. Moreover, the nature of one-step dynamics prediction in our method is advantageous for maintaining relatively high prediction accuracy. ", "page_idx": 4}, {"type": "text", "text": "4 Analysis of OOD Action Suppression ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "This section focuses on the OOD action issue and shows that the proposed regularizer also exhibits the effect of OOD action suppression. In other words, it can also prevent the policy from taking OOD actions, thereby simultaneously addressing the fundamental OOD action issue in offilne RL. In offilne RL, OOD actions are exclusively defined on ID states. This is because actor-critic training is limited to ID states, and any actions on OOD states would not affect training and cause the OOD action issue mentioned in Section 2. Consequently, for the analysis of OOD actions, it is essential to consider ID states. We define $\\bar{R},\\bar{R}_{1}$ as the ID state version of $R,R_{1}$ , where $\\hat{s}=s$ . $\\bar{R}$ and $\\bar{R}_{1}$ can be regarded as special cases of $R$ and $R_{1}$ , when $\\hat{s}$ sampled from ${\\mathcal{N}}(s,\\sigma^{2})$ is equal to $s$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bar{R}(\\pi)=\\underset{(s,s^{\\prime})\\sim\\mathcal{D}}{\\mathbb{E}}\\left[\\frac{\\exp\\left(\\alpha V\\left(s^{\\prime}\\right)\\right)}{Z(s)}\\log M(s^{\\prime}|s,\\pi(\\cdot|s))\\right],}\\\\ {\\bar{R}_{1}(\\pi)=\\underset{(s,s^{\\prime})\\sim\\mathcal{D}}{\\mathbb{E}}\\left[\\frac{\\exp\\left(\\alpha V\\left(s^{\\prime}\\right)\\right)}{\\exp\\left(\\alpha V\\left(s\\right)\\right)}\\log M(s^{\\prime}|s,\\pi(\\cdot|s))\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The proposed regularizer functions as follows: when the agent encounters OOD states, it drives the agent to choose actions leading to ID states, as discussed in Section 3.2. When the agent is in ID states, the ID state part of it comes into play. In the following, we show that it helps circumvent taking OOD actions by analyzing the maximizer of $\\bar{R},\\bar{R}_{1}$ in tabular MDPs. ", "page_idx": 5}, {"type": "text", "text": "Proposition 1. Suppose that the environment dynamics is deterministic, then both $\\bar{R}(\\pi)$ and $\\bar{R}_{1}(\\pi)$ achieve their global maximum at the policy $\\pi^{*}$ , where3 ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\pi^{*}(a|s)=\\frac{1}{Z(s)}\\exp\\left(\\alpha V\\left(M(s,a)\\right)\\right)\\beta(a|s)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The support of $\\pi^{*}$ is within that of the behavior policy $\\beta$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{supp}(\\pi^{*}(\\cdot|s))\\subseteq\\mathrm{supp}(\\beta(\\cdot|s)),\\;\\forall s\\sim\\mathcal{D}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "and $\\pi^{*}$ makes the following equation hold: ", "page_idx": 5}, {"type": "equation", "text": "$$\nN^{*}(\\cdot|s)=M(\\cdot|s,\\pi^{*}(\\cdot|s)),\\;\\forall s\\sim\\mathcal{D}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Under the deterministic dynamics condition, Proposition 1 shows that $\\pi^{*}$ is constrained within the support of the behavior policy. Thus, our regularizer helps to keep the policy from taking OOD actions. Moreover, $\\pi^{*}$ is able to exactly align $\\bar{M}(\\cdot|s,\\pi^{*}(\\cdot|\\bar{s}))$ with $N^{*}(\\cdot|s)$ , indicating the guidance of the agent to the high-value ID state distributions. ", "page_idx": 5}, {"type": "text", "text": "Furthermore, we show in Proposition 2 that even under stochastic dynamics, the optimization of $\\bar{R}$ and $\\bar{R}_{1}$ still yields policies constrained within the support of $\\beta$ . Hence, SCAS also exhibits the effect of OOD action suppression in this more general scenario. ", "page_idx": 5}, {"type": "text", "text": "Proposition 2. When the dynamics is stochastic, the maximizers of both $\\bar{R}(\\pi)$ and $\\bar{R}_{1}(\\pi)$ are constrained within the support of the behavior policy: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{supp}(\\pi^{*}(\\cdot|s))\\subseteq\\mathrm{supp}(\\beta(\\cdot|s)),\\;\\forall s\\sim\\mathcal{D}}\\\\ &{\\mathrm{supp}(\\pi_{1}^{*}(\\cdot|s))\\subseteq\\mathrm{supp}(\\beta(\\cdot|s)),\\;\\forall s\\sim\\mathcal{D}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "5 Implementation Details ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "SCAS is easy to implement and we design the practical algorithm to be as simple as possible, retaining algorithmic simplicity and improving computational efficiency. ", "page_idx": 5}, {"type": "text", "text": "Dynamics model. We employ a deterministic dynamics model $M_{\\omega}$ . The loss for training the model is ", "page_idx": 5}, {"type": "equation", "text": "$$\nL_{M}(\\omega)=\\mathbb{1}_{(s,a,s^{\\prime})\\sim\\mathcal{D}}^{\\mathbb{E}}[\\|M_{\\omega}(s,a)\\!-\\!s^{\\prime}\\|_{2}^{2}]\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Policy improvement. With a deterministic model, we replace the log-likelihood in $R_{1}(\\pi)$ with mean squared error. It is a common approach in RL algorithms to convert a maximum likelihood estimation problem into a re", "page_idx": 5}, {"type": "table", "img_path": "anyZgGLQ6n/tmp/b837117b6465552742a53e11d1d8438cb7937d8ea9694956931a4af29bfe6e45.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "gression problem when dealing with Gaussians with fixed variance [10]. As discussed in Section 3.2, we also adopt a deterministic policy model $\\pi_{\\phi}$ . Thus, we have the following policy regularizer: ", "page_idx": 5}, {"type": "equation", "text": "$$\nR_{2}(\\pi_{\\phi})=\\underset{(s,s^{\\prime})\\sim\\mathcal{D}}{\\mathbb{E}}\\underset{\\hat{s}\\sim\\mathcal{N}_{\\sigma}(s)}{\\mathbb{E}}\\left[\\frac{\\exp\\left(\\alpha V_{\\theta}\\left(s^{\\prime}\\right)\\right)}{\\exp\\left(\\alpha V_{\\theta}\\left(s\\right)\\right)}\\|M_{\\omega}\\big(\\hat{s},\\pi_{\\phi}(\\hat{s})\\big)-s^{\\prime}\\|_{2}^{2}\\right],\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $V_{\\theta}(s)=Q_{\\theta}(s,\\bar{\\pi}_{\\phi}(s))$ and $\\bar{\\pi}_{\\phi}$ means $\\pi_{\\phi}$ with detached gradients. Using deterministic policy also simplifies the training process without learning a $V$ -function. Combining $R_{2}(\\pi_{\\phi})$ with the standard policy improvement objective, we update the policy by maximizing: ", "page_idx": 5}, {"type": "equation", "text": "$$\nJ_{\\pi}(\\phi)=(1-\\lambda)\\mathbb{E}_{s\\sim\\mathcal{D}}\\left[Q_{\\theta}\\left(s,\\pi_{\\phi}(s)\\right)\\right]+\\lambda R_{2}(\\pi_{\\phi}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\lambda$ is a hyperparameter to balance the two terms. Additionally, following $\\mathrm{TD3+BC}$ [10], we also normalize $Q_{\\theta}$ in the first term in each mini-batch to maintain a balanced scale across tasks. ", "page_idx": 6}, {"type": "text", "text": "Overall algorithm. Putting everything together, we present our final algorithm in Algorithm 1. ", "page_idx": 6}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we conduct several experiments to examine the performance and properties of SCAS.   \nPlease refer to Appendices D and E for experimental details and additional results. ", "page_idx": 6}, {"type": "text", "text": "6.1 Empirical Evidence of OOD State Correction and OOD Action Suppression ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "OOD state correction. To examine the OOD state correction ability, we compare the state distributions generated by the learned policies of different algorithms with the state distribution of the offilne dataset. In detail, we first train SCAS, CQL [31], and $\\mathrm{TD3+BC}$ [10], and then collect 50,000 samples by running the trained policies separately. We also sample 50,000 states randomly from the offilne dataset for comparison. Figures 1(a) to 1(c) plot the state distributions in halfcheetah-mediumexpert [9] with t-SNE [62], and Figure 1(d) visualizes the optimal value of each state. We access these values from the learned value function obtained by running TD3 [11] online to convergence. ", "page_idx": 6}, {"type": "text", "text": "In Figures 1(a) and 1(b), we observe that the policies learned by CQL and $\\mathrm{TD}3{+}\\mathrm{BC}$ tend to produce OOD states. As depicted in Figure 1(d), these OOD states have extremely low values, so entering them can be detrimental to performance. In contrast, the state distribution induced by SCAS is almost entirely within the support of the offilne distribution, demonstrating the OOD state correction ability of SCAS. Moreover, we also note that in the low-value area of the offilne state distribution (the grey circle in Figure 1(d)), SCAS exhibits a very low state density, which could be attributed to SCAS\u2019s value-aware OOD state correction. We refer the reader to Appendix E.2 for additional experiments validating the OOD state correction effects. ", "page_idx": 6}, {"type": "text", "text": "OOD action suppression. We empirically evaluate the OOD action suppression effects through the lens of value estimates. We compare SCAS with three baselines: (1) ordinary offpolicy RL which is SCAS with $\\dot{\\lambda}\\,=$ 0 (all other implementations are the same); (2) SDC [75] without additional CQL [31] term to suppress OOD actions; (3) OSR [22] without additional CQL term. We conduct experiments on D4RL datasets [9]. Since value over-estimation (divergence) is the main consequence and evidence of OOD actions [12], we plot the learned Q-values of SCAS and the baselines in Figure 2. ", "page_idx": 6}, {"type": "image", "img_path": "anyZgGLQ6n/tmp/7d5356d3baf90663bb3f72661b196adc6d1f432066e468fc1b8c9697e5346d32.jpg", "img_caption": ["Figure 2: Oracle Q-values of SCAS (estimated by MC return) and learned Q-values of SCAS and other algorithms across optimization steps. Only SCAS\u2019s OOD state correction term can achieve OOD action suppression and prevent value over-estimation (divergence). "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "We also include the oracle Q-values of SCAS by rollouting the trained policy for 1, 000 episodes and evaluating the Monte-Carlo return. Additional results are provided in Appendix E.1. ", "page_idx": 6}, {"type": "text", "text": "The results show that the learned Q-values of ordinary off-policy RL, SDC without CQL, and OSR without CQL diverge at early learning stages, suggesting that the algorithms suffer from severe OOD actions. By contrast, the learned Q-values of SCAS stay close to the oracle Q-values. This indicates that SCAS regularization alone is able to suppress OOD actions. ", "page_idx": 6}, {"type": "text", "text": "6.2 Comparisons on Offline RL Benchmarks ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Tasks. We evaluate SCAS on D4RL [9] and NeoRL [49] benchmarks. In D4RL, we conduct experiments on Gym locomotion tasks and much more challenging AntMaze tasks. Due to the space limit, the results on NeoRL are deferred to Table 4 in Appendix E.3. ", "page_idx": 6}, {"type": "table", "img_path": "anyZgGLQ6n/tmp/67a9ee10aa87b3d648f0116d23a02bc313f9d9c24b6474053456f9b5b30df189.jpg", "table_caption": ["Table 1: Averaged normalized scores on Gym locomotion and AntMaze tasks over five random seeds. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Baselines. We compare SCAS with prior state-of-the-art offline RL methods as well as the ones specifically designed for OOD state correction, including BC [48], MOPO [73], OneStep RL [5], CQL [31], $\\mathrm{TD3+BC}$ [10], IQL [29], SDC [75] and OSR [22]. ", "page_idx": 7}, {"type": "text", "text": "Hyperparamter tuning. Offline RL methods are appealing for their ability to generate effective policies without online interaction. Nevertheless, many existing offline RL works involve datasetspecific hyperparameter tuning. The reduction of hyperparameter tuning is crucial for improving practical applicability. In this work, SCAS uses a single set of hyperparameters for all datasets in D4RL and NeoRL benchmarks to obtain the reported results. ", "page_idx": 7}, {"type": "text", "text": "Comparisons with baselines. On D4RL, comparisons of performance, runtime, and hyperparameter tuning information are shown in Table 1. We refer the reader to Appendix E.8 for learning curve details of SCAS. On the Gym locomotion tasks, SCAS outperforms prior methods on most datasets and achieves the highest total score with a single set of hyperparameters. On the challenging AntMaze tasks, SCAS performs better than IQL and outperforms other methods by a very large margin. In NeoRL (Table 4), SCAS performs comparably to MOBILE [59] and outperforms other baselines. ", "page_idx": 7}, {"type": "text", "text": "Runtime. We present the runtime of algorithms at the bottom of Table 1. SCAS exhibits significantly lower runtime than MOPO, SDC, and OSR and is comparable to other model-free baselines. ", "page_idx": 7}, {"type": "text", "text": "Generality. SCAS is a generic model-based regularizer that can be easily integrated into existing offline RL algorithms. The corresponding results and analysis are provided in Appendix E.5. ", "page_idx": 7}, {"type": "text", "text": "6.3 Comparisons in Perturbed Environments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we evaluate the algorithms in a more real-world setting where the agent receives uncertain perturbations during test time. OOD state correction is even more critical in such scenarios since the agent can enter OOD states after perturbation. To simulate this scenario, we add varying steps of Gaussian noise with a magnitude of 0.5 to the actions conducted by the policy during test time. Specifically, the policy is trained on standard D4RL datasets but is tested in the perturbed environments. We control the strength of perturbations by adjusting the number of perturbation steps. ", "page_idx": 7}, {"type": "image", "img_path": "anyZgGLQ6n/tmp/98392e7aaa3abebb9449dd0e42c4dc0a37bd92c32fbcf439408fc010407fb4ec.jpg", "img_caption": ["Figure 3: Comparisons in the perturbed environments with varying perturbation levels. The perturbation steps are the steps of Gaussian noise added to the conducted actions in an episode. SCAS exhibits better robustness against environmental perturbations during the test phase. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "anyZgGLQ6n/tmp/f3e9a559c2c0324603b9ceb44412d045180b89128ed302641f8f1c4a7bfa7d99.jpg", "img_caption": ["Figure 4: Parameter study on the inverse temperature $\\alpha$ and the balance coefficient $\\lambda$ . (a) An appropriately large $\\alpha$ is crucial for achieving good performance. (b) The proposed SCAS regularization is essential and demonstrates robustness to changes in $\\lambda$ . "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 3 shows the results of $\\mathrm{TD3+BC}$ , CQL, SDC, and SCAS on various datasets over five random seeds. We observe that SCAS consistently outperforms previous methods across different perturbation levels and also exhibits less performance degradation against perturbations. Therefore, SCAS enjoys better robustness against perturbations in the complex and unpredictable environments. ", "page_idx": 8}, {"type": "text", "text": "6.4 Parameter Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We examine the effects of the inverse temperature $\\alpha$ , the balance coefficient $\\lambda$ , and the noise scale $\\sigma$ . Due to the space limit, the results for $\\sigma$ and on additional datasets are deferred to Appendix E.6. A sensitivity analysis on dynamics model errors is also provided in Appendix E.7. ", "page_idx": 8}, {"type": "text", "text": "Inverse temperature $\\alpha$ . $\\alpha$ is the key hyperparameter in SCAS for achieving value-aware OOD state correction. If $\\alpha=0$ , the effect degenerates to vanilla OOD state correction. Figure 4(a) displays the learning curves of SCAS with different $\\alpha$ . The results show that a large $\\alpha$ is crucial for achieving good performance (also verified on more tasks in Figure 6), clearly demonstrating the effectiveness of our value-aware OOD state correction. However, too large $\\alpha$ $(\\alpha=10)$ ) induces less satisfying performance, probably due to the increased variance of the learning objective. ", "page_idx": 8}, {"type": "text", "text": "Balance coefficient $\\lambda$ . $\\lambda$ in Eq. (20) controls the balance between vanilla policy improvement and SCAS regularization. We vary $\\lambda$ within the range [0, 1] and present the learning curves of SCAS in Figure 4(b). Notably, SCAS is able to converge to good performance over a very wide range of $\\lambda$ (also verified on more tasks in Figure 7). An interesting finding is that even when $\\lambda=1$ and the signal from RL improvement (max Q) is removed, SCAS still performs well on most tasks. This could be attributed to the fact that value-aware OOD state correction implies some sort of improvement in policy by maximizing the values of policy-induced next states. ", "page_idx": 8}, {"type": "text", "text": "7 Conclusion and Limitations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this paper, we systematically analyze the OOD state issue in offline RL and propose SCAS, a simple yet effective approach that unifies OOD state correction and OOD action suppression. SCAS also achieves value-aware OOD state correction, significantly improving performance over vanilla ", "page_idx": 8}, {"type": "text", "text": "OOD state correction. Empirical results validate the properties of SCAS, showcasing its superior performance on the offline RL benchmarks and its enhanced robustness in perturbed environments. ", "page_idx": 9}, {"type": "text", "text": "However, our work also has some limitations. For example, current SCAS primarily focuses on continuous control tasks. In discrete settings, algorithmic components like state perturbation strategy would be different, which would be an interesting direction for future work. Moreover, we anticipate employing more advanced dynamics models, such as ensembles [73] and diffusion models [21], to further improve the performance of our method. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgment ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We thank the anonymous reviewers for feedback on an early version of this paper. This work was supported by the National Key R&D Program of China under Grant 2018AAA0102801, National Natural Science Foundation of China under Grant 61827804. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. In International conference on machine learning, pages 22\u201331. PMLR, 2017.   \n[2] Gaon An, Seungyong Moon, Jang-Hyun Kim, and Hyun Oh Song. Uncertainty-based offline reinforcement learning with diversified q-ensemble. Advances in neural information processing systems, 34:7436\u20137447, 2021.   \n[3] Chenjia Bai, Lingxiao Wang, Zhuoran Yang, Zhi-Hong Deng, Animesh Garg, Peng Liu, and Zhaoran Wang. Pessimistic bootstrapping for uncertainty-driven offilne reinforcement learning. In International Conference on Learning Representations, 2022. URL https://openreview. net/forum?id $\\equiv$ Y4cs1Z3HnqL.   \n[4] Jacob Beck, Risto Vuorio, Evan Zheran Liu, Zheng Xiong, Luisa Zintgraf, Chelsea Finn, and Shimon Whiteson. A survey of meta-reinforcement learning. arXiv preprint arXiv:2301.08028, 2023. [5] David Brandfonbrener, Will Whitney, Rajesh Ranganath, and Joan Bruna. Offline rl without off-policy evaluation. Advances in Neural Information Processing Systems, 34:4933\u20134946, 2021.   \n[6] Ching-An Cheng, Tengyang Xie, Nan Jiang, and Alekh Agarwal. Adversarially trained actor critic for offline reinforcement learning. arXiv preprint arXiv:2202.02446, 2022. [7] Jonas Degrave, Federico Felici, Jonas Buchli, Michael Neunert, Brendan Tracey, Francesco Carpanese, Timo Ewalds, Roland Hafner, Abbas Abdolmaleki, Diego de Las Casas, et al. Magnetic control of tokamak plasmas through deep reinforcement learning. Nature, 602(7897): 414\u2013419, 2022.   \n[8] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International conference on machine learning, pages 1126\u20131135. PMLR, 2017.   \n[9] Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020.   \n[10] Scott Fujimoto and Shixiang Shane Gu. A minimalist approach to offilne reinforcement learning. Advances in neural information processing systems, 34:20132\u201320145, 2021.   \n[11] Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actor-critic methods. In International conference on machine learning, pages 1587\u20131596. PMLR, 2018.   \n[12] Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without exploration. In International conference on machine learning, pages 2052\u20132062. PMLR, 2019.   \n[13] Javier Garc\u0131a and Fernando Fern\u00e1ndez. A comprehensive survey on safe reinforcement learning. Journal of Machine Learning Research, 16(1):1437\u20131480, 2015.   \n[14] Divyansh Garg, Joey Hejna, Matthieu Geist, and Stefano Ermon. Extreme q-learning: Maxent RL without entropy. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id $\\equiv$ SJ0Lde3tRL.   \n[15] Seyed Kamyar Seyed Ghasemipour, Dale Schuurmans, and Shixiang Shane Gu. Emaq: Expected-max q-learning operator for simple yet effective offilne and online rl. In International Conference on Machine Learning, pages 3682\u20133691. PMLR, 2021.   \n[16] Sven Gronauer and Klaus Diepold. Multi-agent deep reinforcement learning: a survey. Artificial Intelligence Review, 55(2):895\u2013943, 2022.   \n[17] Shangding Gu, Long Yang, Yali Du, Guang Chen, Florian Walter, Jun Wang, and Alois Knoll. A review of safe reinforcement learning: Methods, theory and applications. arXiv preprint arXiv:2205.10330, 2022.   \n[18] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic actor. In International conference on machine learning, pages 1861\u20131870. PMLR, 2018.   \n[19] Zhang-Wei Hong, Aviral Kumar, Sathwik Karnik, Abhishek Bhandwaldar, Akash Srivastava, Joni Pajarinen, Romain Laroche, Abhishek Gupta, and Pulkit Agrawal. Beyond uniform sampling: Offline reinforcement learning with imbalanced datasets. Advances in Neural Information Processing Systems, 36:4985\u20135009, 2023.   \n[20] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-based policy optimization. Advances in neural information processing systems, 32, 2019.   \n[21] Michael Janner, Yilun Du, Joshua Tenenbaum, and Sergey Levine. Planning with diffusion for flexible behavior synthesis. In International Conference on Machine Learning, pages 9902\u20139915. PMLR, 2022.   \n[22] Ke Jiang, Jia-Yu Yao, and Xiaoyang Tan. Recovering from out-of-sample states via inverse dynamics in offilne reinforcement learning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[23] Alistair EW Johnson, Tom J Pollard, Lu Shen, Li-wei H Lehman, Mengling Feng, Mohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark. Mimic-iii, a freely accessible critical care database. Scientific data, 3(1):1\u20139, 2016.   \n[24] Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H Campbell, Konrad Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, et al. Modelbased reinforcement learning for atari. arXiv preprint arXiv:1903.00374, 2019.   \n[25] Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel: Model-based offilne reinforcement learning. Advances in neural information processing systems, 33:21810\u201321823, 2020.   \n[26] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \n[27] Jens Kober, J Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey. The International Journal of Robotics Research, 32(11):1238\u20131274, 2013.   \n[28] Ilya Kostrikov, Rob Fergus, Jonathan Tompson, and Ofir Nachum. Offline reinforcement learning with fisher divergence critic regularization. In International Conference on Machine Learning, pages 5774\u20135783. PMLR, 2021.   \n[29] Ilya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit q-learning. In International Conference on Learning Representations, 2022. URL https: //openreview.net/forum?id=68n2s9ZJWF8.   \n[30] Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing offpolicy q-learning via bootstrapping error reduction. Advances in Neural Information Processing Systems, 32, 2019.   \n[31] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline reinforcement learning. Advances in Neural Information Processing Systems, 33: 1179\u20131191, 2020.   \n[32] Sascha Lange, Thomas Gabel, and Martin Riedmiller. Batch reinforcement learning. In Reinforcement learning, pages 45\u201373. Springer, 2012.   \n[33] Jongmin Lee, Wonseok Jeon, Byungjun Lee, Joelle Pineau, and Kee-Eung Kim. Optidice: Offline policy optimization via stationary distribution correction estimation. In International Conference on Machine Learning, pages 6120\u20136130. PMLR, 2021.   \n[34] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.   \n[35] Ryan Lowe, Yi I Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-agent actor-critic for mixed cooperative-competitive environments. Advances in neural information processing systems, 30, 2017.   \n[36] Jiafei Lyu, Xiaoteng Ma, Xiu Li, and Zongqing Lu. Mildly conservative q-learning for offline reinforcement learning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. URL https:// openreview.net/forum?id $\\equiv$ VYYf6S67pQc.   \n[37] Will Maddern, Geoffrey Pascoe, Chris Linegar, and Paul Newman. 1 year, $1000\\,\\mathrm{km}$ : The oxford robotcar dataset. The International Journal of Robotics Research, 36(1):3\u201315, 2017.   \n[38] Liyuan Mao, Haoran Xu, Weinan Zhang, and Xianyuan Zhan. ODICE: Revealing the mystery of distribution correction estimation via orthogonal-gradient update. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum? id=L8UNn7Llt4.   \n[39] Yixiu Mao, Hongchang Zhang, Chen Chen, Yi Xu, and Xiangyang Ji. Supported trust region optimization for offline reinforcement learning. In International Conference on Machine Learning, pages 23829\u201323851. PMLR, 2023.   \n[40] Yixiu Mao, Hongchang Zhang, Chen Chen, Yi Xu, and Xiangyang Ji. Supported value regularization for offline reinforcement learning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[41] Tatsuya Matsushima, Hiroki Furuta, Yutaka Matsuo, Ofir Nachum, and Shixiang Gu. Deployment-efficient reinforcement learning via model-based offline optimization. In International Conference on Learning Representations, 2021. URL https://openreview. net/forum?id $\\equiv$ 3hGNqpI4WS.   \n[42] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. nature, 518(7540):529\u2013533, 2015.   \n[43] Thomas M Moerland, Joost Broekens, Aske Plaat, Catholijn M Jonker, et al. Model-based reinforcement learning: A survey. Foundations and Trends\u00ae in Machine Learning, 16(1):1\u2013118, 2023.   \n[44] Ofir Nachum, Bo Dai, Ilya Kostrikov, Yinlam Chow, Lihong Li, and Dale Schuurmans. Algaedice: Policy gradient from arbitrary experience. arXiv preprint arXiv:1912.02074, 2019.   \n[45] Ashvin Nair, Abhishek Gupta, Murtaza Dalal, and Sergey Levine. Awac: Accelerating online reinforcement learning with offline datasets. arXiv preprint arXiv:2006.09359, 2020.   \n[46] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017.   \n[47] Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression: Simple and scalable off-policy reinforcement learning. arXiv preprint arXiv:1910.00177, 2019.   \n[48] Dean A Pomerleau. Alvinn: An autonomous land vehicle in a neural network. Advances in neural information processing systems, 1, 1988.   \n[49] Rong-Jun Qin, Xingyuan Zhang, Songyi Gao, Xiong-Hui Chen, Zewen Li, Weinan Zhang, and Yang Yu. Neorl: A near real-world benchmark for offilne reinforcement learning. Advances in Neural Information Processing Systems, 35:24753\u201324765, 2022.   \n[50] Yun Qu, Boyuan Wang, Jianzhun Shao, Yuhang Jiang, Chen Chen, Zhenbin Ye, Linc Liu, Junfeng Yang, Lin Lai, Hongyang Qin, et al. Hokoff: Real game dataset from honor of kings and its offline reinforcement learning benchmarks. In Thirty-seventh Conference on Neural Information Processing Systems Track on Datasets and Benchmarks, 2023.   \n[51] Yun Qu, Boyuan Wang, Yuhang Jiang, Jianzhun Shao, Yixiu Mao, Cheems Wang, Chang Liu, and Xiangyang Ji. Choices are more important than efforts: Llm enables efficient multi-agent exploration. arXiv preprint arXiv:2410.02511, 2024.   \n[52] Tabish Rashid, Mikayel Samvelyan, Christian Schroeder De Witt, Gregory Farquhar, Jakob Foerster, and Shimon Whiteson. Monotonic value function factorisation for deep multi-agent reinforcement learning. Journal of Machine Learning Research, 21(178):1\u201351, 2020.   \n[53] Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari, go, chess and shogi by planning with a learned model. Nature, 588(7839):604\u2013609, 2020.   \n[54] Jianzhun Shao, Yun Qu, Chen Chen, Hongchang Zhang, and Xiangyang Ji. Counterfactual conservative q learning for offline multi-agent reinforcement learning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/ forum?id $\\equiv$ 62zmO4mv8X.   \n[55] Jianzhun Shao, Hongchang Zhang, Yun Qu, Chang Liu, Shuncheng He, Yuhang Jiang, and Xiangyang Ji. Complementary attention for multi-agent reinforcement learning. In International Conference on Machine Learning, pages 30776\u201330793. PMLR, 2023.   \n[56] David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller. Deterministic policy gradient algorithms. In International conference on machine learning, pages 387\u2013395. Pmlr, 2014.   \n[57] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowledge. nature, 550(7676):354\u2013359, 2017.   \n[58] Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning structured output representation using deep conditional generative models. Advances in neural information processing systems, 28, 2015.   \n[59] Yihao Sun, Jiaji Zhang, Chengxing Jia, Haoxin Lin, Junyin Ye, and Yang Yu. Model-bellman inconsistency for model-based offline reinforcement learning. In International Conference on Machine Learning, pages 33177\u201333194. PMLR, 2023.   \n[60] Richard S Sutton. Dyna, an integrated architecture for learning, planning, and reacting. ACM Sigart Bulletin, 2(4):160\u2013163, 1991.   \n[61] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.   \n[62] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(11), 2008.   \n[63] Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Micha\u00ebl Mathieu, Andrew Dudzik, Junyoung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350\u2013354, 2019.   \n[64] Cheems Wang, Yiqin Lv, Yixiu Mao, Yun Qu, Yi Xu, and Xiangyang Ji. Robust fast adaptation from adversarially explicit task distribution generation. arXiv preprint arXiv:2407.19523, 2024.   \n[65] Qi Wang and Herke Van Hoof. Model-based meta reinforcement learning using graph structured surrogate models and amortized policy search. In International Conference on Machine Learning, pages 23055\u201323077. PMLR, 2022.   \n[66] Qi Wang, Yiqin Lv, Zheng Xie, Jincai Huang, et al. A simple yet effective strategy to robustify the meta learning paradigm. Advances in Neural Information Processing Systems, 36, 2024.   \n[67] Ziyu Wang, Alexander Novikov, Konrad Zolna, Josh S Merel, Jost Tobias Springenberg, Scott E Reed, Bobak Shahriari, Noah Siegel, Caglar Gulcehre, Nicolas Heess, et al. Critic regularized regression. Advances in Neural Information Processing Systems, 33:7768\u20137778, 2020.   \n[68] Jialong Wu, Haixu Wu, Zihan Qiu, Jianmin Wang, and Mingsheng Long. Supported policy optimization for offline reinforcement learning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id $\\equiv$ KCXQ5HoM-fy.   \n[69] Yifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement learning. arXiv preprint arXiv:1911.11361, 2019.   \n[70] Tengyang Xie, Ching-An Cheng, Nan Jiang, Paul Mineiro, and Alekh Agarwal. Bellmanconsistent pessimism for offline reinforcement learning. Advances in neural information processing systems, 34:6683\u20136694, 2021.   \n[71] Haoran Xu, Li Jiang, Jianxiong Li, Zhuoran Yang, Zhaoran Wang, Victor Wai Kin Chan, and Xianyuan Zhan. Offline RL with no OOD actions: In-sample learning via implicit value regularization. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=ueYYgo2pSSU.   \n[72] Rui Yang, Chenjia Bai, Xiaoteng Ma, Zhaoran Wang, Chongjie Zhang, and Lei Han. RORL: Robust offline reinforcement learning via conservative smoothing. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=_QzJJGH_KE.   \n[73] Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y Zou, Sergey Levine, Chelsea Finn, and Tengyu Ma. Mopo: Model-based offline policy optimization. Advances in Neural Information Processing Systems, 33:14129\u201314142, 2020.   \n[74] Tianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey Levine, and Chelsea Finn. Combo: Conservative offline model-based policy optimization. Advances in neural information processing systems, 34:28954\u201328967, 2021.   \n[75] Hongchang Zhang, Jianzhun Shao, Yuhang Jiang, Shuncheng He, Guanwen Zhang, and Xiangyang Ji. State deviation correction for offilne reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 9022\u20139030, 2022.   \n[76] Hongchang Zhang, Yixiu Mao, Boyuan Wang, Shuncheng He, Yi Xu, and Xiangyang Ji. In-sample actor critic for offline reinforcement learning. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id= dfDv0WU853R. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Related Work ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Model-free offline RL. In offilne RL, extrapolation error and overestimation caused by OOD actions pose significant challenges. Among model-free solutions, value regularization methods penalize the $Q$ -values of OOD actions [31, 2, 28, 36, 3, 72, 40], while policy constraint approaches compel the trained policy to be close to the behavior policy, either explicitly via divergence penalties [69, 30, 10], implicitly by weighted behavior cloning [47, 45, 67, 39], or directly through specific parameterization of the policy [12, 15]. Relatively independently, in-sample learning methods formulate the Bellman target using only the actions in the dataset to avoid OOD actions [5, 29, 76, 71]. Recently, some works aim to learn the optimal policy within the support of the dataset (known as in-support or in-sample optimal policy) in a theoretically sound way and are less affected by the average quality of the dataset [39, 40, 68]. However, existing popular offline RL approaches primarily focus on the OOD action issue during training and often neglect the OOD state issue during the test phase. ", "page_idx": 14}, {"type": "text", "text": "Model-based offilne RL. Model-based RL methods learn a model of the environment and generate synthetic data from that model to optimize the policy [60, 20, 24]. To ensure conservatism in offilne RL, Kidambi et al. [25] and Yu et al. [73] estimate the uncertainty in the model and apply reward penalties for state-action pairs with high uncertainty. Some model-based approaches also introduce conservatism similarly to model-free ones, employing techniques like value regularization [74] and policy constraint [41]. Recently, Sun et al. [59] conducts uncertainty quantification through the inconsistency of Bellman estimations under the learned dynamics ensemble. However, model-based methods often come with a high computational burden [20], and their effectiveness relies heavily on the quality of the trained model [43]. In contrast, our algorithm leverages the dynamics model to propagate policy gradients, make one-step predictions, and regularize policy training, leading to significantly improved computational efficiency and relatively high prediction accuracy. ", "page_idx": 14}, {"type": "text", "text": "OOD state correction. In offline RL, OOD state correction deserves more attention as the state deviation during the test phase can accumulate over time steps, severely degrading performance [34]. Existing limited solutions aim to train the policy to correct the agent from OOD states to ID states [75, 22]. Specifically, SDC [75] builds a dynamics model and a state transition model, and aligns the policy-induced next state distributions at OOD states with the state transition model. On the other hand, OSR [22] utilizes an inverse dynamics model to constrain the policy at OOD states. Compared with prior methods, our proposed SCAS efficiently unifies OOD state correction and OOD action suppression in offline RL and additionally achieves value-aware OOD state correction. The DICE series of works [44, 33, 38] share similar motivations with SCAS to some extent; however, there are significant differences between the two. Firstly, DICE is based on a linear programming framework of RL, while SCAS is based on a dynamic programming framework. Therefore, the theoretical foundations and learning paradigms of the two are inherently different. Secondly, SCAS only corrects encountered OOD states, whereas DICE algorithms require the policy-induced occupancy distribution to align with the dataset distribution. Therefore, DICE\u2019s constraints are stricter, potentially making it more susceptible to the average quality of the dataset. Lastly, theoretical and empirical evidence indicate that DICE algorithms have a problem of gradient cancellation [38], which imposes certain limitations on their practical effectiveness. ", "page_idx": 14}, {"type": "text", "text": "B Proofs ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we present the proofs for the theories in the paper. ", "page_idx": 14}, {"type": "text", "text": "B.1 Derivation of the Value-aware State Transition Distribution ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We show that Eq. (6) is the optimal solution of the optimization problem Eq. (5): ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{N^{*}}\\operatorname*{\\mathbb{E}}_{s\\sim\\mathcal{D}}\\left[\\alpha_{s^{\\prime}\\sim N^{*}(\\cdot|s)}V(s^{\\prime})-{\\mathrm{D}}_{\\mathrm{KL}}(N^{*}(\\cdot|s)\\|N(\\cdot|s))\\right]\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We can optimize $N^{*}$ at each $s\\in\\mathcal{D}$ separately. Thus we consider the following optimization problem: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{max}_{\\tilde{N}}\\alpha\\underset{s^{\\prime}\\sim\\tilde{N}(\\cdot|s)}{\\mathbb{E}}V(s^{\\prime})-\\mathrm{D}_{\\mathrm{KL}}(\\tilde{N}(\\cdot|s)\\|N(\\cdot|s))}\\\\ &{\\quad\\quad s.t.\\displaystyle\\sum_{s^{\\prime}}\\tilde{N}(s^{\\prime}|s)=1,\\;\\forall s\\in\\mathcal{D}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This constrained optimization problem is convex, and the Lagrangian is: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\tilde{N})=\\alpha_{s^{\\prime}\\sim\\tilde{N}(\\cdot|s)}V(s^{\\prime})-\\mathrm{D}_{\\mathrm{KL}}(\\tilde{N}(\\cdot|s)\\|N(\\cdot|s))+\\nu\\left(\\sum_{s^{\\prime}}\\tilde{N}(s^{\\prime}|s)-1\\right)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The KKT condition gives: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{\\partial\\mathcal{L}}{\\partial\\tilde{N}(s^{\\prime}|s)}=\\alpha V(s^{\\prime})-\\log\\tilde{N}(s^{\\prime}|s)-1+\\log N(s^{\\prime}|s)+\\nu=0\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Solving for $\\tilde{N}$ gives the closed form solution $N^{*}$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\nN^{*}(s^{\\prime}|s)=\\exp\\left(\\alpha V\\left(s^{\\prime}\\right)-1+\\nu\\right)N(s^{\\prime}|s),\\;\\forall s\\sim\\mathcal{D}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By the condition $\\begin{array}{r}{\\sum_{s^{\\prime}}N^{*}(s^{\\prime}|s)=1}\\end{array}$ , we can directly solve the Lagrangian multiplier $\\nu$ and replace $\\exp(\\nu-1)$ with  a normalization factor: ", "page_idx": 15}, {"type": "equation", "text": "$$\nN^{*}(s^{\\prime}|s)=\\frac{1}{Z(s)}\\exp\\left(\\alpha V\\left(s^{\\prime}\\right)\\right)N(s^{\\prime}|s),\\;\\forall s\\sim\\mathcal{D}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\begin{array}{r}{Z(s)=\\sum_{s^{\\prime}}\\exp\\left(\\alpha V\\left(s^{\\prime}\\right)\\right)N(s^{\\prime}|s)}\\end{array}$ is the normalization factor. ", "page_idx": 15}, {"type": "text", "text": "B.2 Proof of Proposition 1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proposition 3 (Proposition 1 in the main paper). Suppose that the environment dynamics is deterministic, then both $\\bar{R}(\\dot{\\pi})$ and $\\bar{R}_{1}(\\pi)$ achieve their global maximum at the policy $\\pi^{*}$ , where4 ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\pi^{*}(a|s)=\\frac{1}{Z(s)}\\exp\\left(\\alpha V\\left(M(s,a)\\right)\\right)\\beta(a|s)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The support of $\\pi^{*}$ is within that of the behavior policy $\\beta$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathrm{supp}(\\pi^{*}(\\cdot|s))\\subseteq\\mathrm{supp}(\\beta(\\cdot|s)),\\;\\forall s\\sim\\mathcal{D}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and $\\pi^{*}$ makes the following equation hold: ", "page_idx": 15}, {"type": "equation", "text": "$$\nN^{*}(\\cdot|s)=M(\\cdot|s,\\pi^{*}(\\cdot|s)),\\;\\forall s\\sim\\mathcal{D}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. We start with $\\bar{R}(\\pi)$ . ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\underset{\\pi}{\\mathrm{argmax}}\\ \\tilde{R}(\\pi)}\\\\ &{=\\underset{\\pi}{\\mathrm{argmax}}\\ \\underset{(s,s^{\\prime})\\sim\\mathcal{D}}{\\mathbb{E}}\\ \\frac{1}{\\left[\\frac{1}{Z(s)}\\exp\\left(\\alpha V\\left(s^{\\prime}\\right)\\right)\\log M\\left(s^{\\prime}|s,\\pi(\\cdot|s)\\right)\\right]}}\\\\ &{=\\underset{\\pi}{\\mathrm{argmax}}\\ \\underset{s\\sim\\mathcal{D}}{\\mathbb{E}}\\ \\underset{s^{\\prime}\\sim\\mathcal{N}}{\\mathbb{E}}\\ \\frac{1}{\\left[\\frac{1}{Z(s)}\\exp\\left(\\alpha V\\left(s^{\\prime}\\right)\\right)\\log M\\left(s^{\\prime}|s,\\pi(\\cdot|s)\\right)\\right]}}\\\\ &{=\\underset{\\pi}{\\mathrm{argmax}}\\ \\underset{s\\sim\\mathcal{D}}{\\mathbb{E}}\\ \\underset{s^{\\prime}\\sim\\mathcal{N}}{\\mathbb{E}}\\ \\underset{s^{\\prime}\\sim\\mathcal{N}}{\\mathbb{E}}\\ \\big[\\log M\\left(s^{\\prime}|s,\\pi(\\cdot|s)\\right)\\big]}\\\\ &{=\\underset{\\pi}{\\mathrm{argmin}}\\ \\underset{s\\sim\\mathcal{D}}{\\mathbb{E}}\\ \\underset{s^{\\prime}\\sim\\mathcal{N}}{\\mathbb{E}}\\ \\underset{s^{\\prime}\\sim\\mathcal{N}}{\\mathbb{E}}\\ \\big[\\log M^{*}(s^{\\prime}|s)-\\log M(s^{\\prime}|s,\\pi(\\cdot|s))\\big]}\\\\ &{=\\underset{\\pi}{\\mathrm{argmin}}\\ \\underset{s\\sim\\mathcal{D}}{\\mathbb{E}}\\ \\underset{s^{\\prime}\\sim\\mathcal{N}}{\\mathbb{E}}\\mathrm{[K^{*}(\\cdot|s)}\\big(\\log{N^{*}(\\cdot|s)}-\\log M(s^{\\prime}|s,\\pi(\\cdot|s))\\big]}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The third equality holds because of the relationship between $N^{*}$ and $N$ in Eq. (6): ", "page_idx": 16}, {"type": "equation", "text": "$$\nN^{*}(s^{\\prime}|s)=\\frac{1}{Z(s)}\\exp\\left(\\alpha V\\left(s^{\\prime}\\right)\\right)N(s^{\\prime}|s),\\;\\forall s\\sim\\mathcal{D}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, the maximizer of $\\bar{R}(\\pi)$ is equal to the solution of the minimization problem in Eq. (35).   \nNow consider the two distributions $N^{*}(\\cdot|s)$ and $M(\\cdot|s,\\pi(\\cdot|s))$ in Eq. (35). ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{{\\cal N}^{*}(s^{\\prime}|s)=\\displaystyle\\frac{1}{Z(s)}\\exp\\left(\\alpha V\\left(s^{\\prime}\\right)\\right){\\cal N}(s^{\\prime}|s)}}\\\\ {{{\\displaystyle=\\frac{1}{Z(s)}\\exp\\left(\\alpha V\\left(s^{\\prime}\\right)\\right)\\sum_{a}\\beta(a|s){\\cal M}(s^{\\prime}|s,a)}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For analytical clarity, we use the notation $M$ with slightly different meanings in different cases: in the stochastic setting, $M:S\\times{\\mathcal{A}}\\to\\Delta(S)$ ; in the deterministic setting, $M:\\mathcal{S}\\times\\mathcal{A}\\rightarrow\\mathcal{S}$ . With the deterministic dynamics assumption, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle N^{*}(s^{\\prime}|s)=\\frac{1}{Z(s)}\\exp\\left(\\alpha V\\left(s^{\\prime}\\right)\\right)\\sum_{a}\\beta(a|s)\\mathbb{I}\\left[M(s,a)=s^{\\prime}\\right]}}\\\\ {~~}\\\\ {{\\displaystyle=\\sum_{a}\\frac{1}{Z(s)}\\exp\\left(\\alpha V\\left(s^{\\prime}\\right)\\right)\\beta(a|s)\\mathbb{I}\\left[M(s,a)=s^{\\prime}\\right]}}\\\\ {{\\displaystyle=\\sum_{a}\\frac{1}{Z(s)}\\exp\\left(\\alpha V\\left(M(s,a)\\right)\\right)\\beta(a|s)\\mathbb{I}\\left[M(s,a)=s^{\\prime}\\right]}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "On the other hand, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{M(s^{\\prime}|s,\\pi(\\cdot|s))=\\displaystyle\\sum_{a}M(s^{\\prime}|s,a)\\pi(a|s)}\\\\ &{~~~~~~~~~~~~~~~~~~~~~~~~~~~~~=\\displaystyle\\sum_{a}\\pi(a|s)\\mathbb{I}\\left[M(s,a)=s^{\\prime}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Now we define $\\pi^{*}(a|s)$ as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\pi^{*}(a|s):=\\frac{1}{Z(s)}\\exp\\left(\\alpha V\\left(M(s,a)\\right)\\right)\\beta(a|s)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We first show that $\\pi^{*}$ is a valid policy, that is, $\\pi^{*}$ is normalized. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pi^{*}(a|s)=\\frac{1}{Z(s)}\\exp\\left(\\alpha V\\left(M(s,a)\\right)\\right)\\beta(a|s)}\\\\ &{\\qquad\\qquad=\\frac{\\exp\\left(\\alpha V\\left(M(s,a)\\right)\\right)\\beta\\left(a|s\\right)}{\\sum_{s^{\\prime}}\\exp\\left(\\alpha V\\left(s^{\\prime}\\right)\\right)N(s^{\\prime}|s)}}\\\\ &{\\qquad\\qquad=\\frac{\\exp\\left(\\alpha V\\left(M(s,a)\\right)\\right)\\beta\\left(a|s\\right)}{\\sum_{s^{\\prime}}\\exp\\left(\\alpha V\\left(s^{\\prime}\\right)\\right)\\sum_{a}\\beta\\left(a|s\\right)M\\left(s^{\\prime}|s,a\\right)}}\\\\ &{\\qquad\\qquad=\\frac{\\exp\\left(\\alpha V\\left(M(s,a)\\right)\\right)\\beta\\left(a|s\\right)}{\\sum_{a}\\sum_{s^{\\prime}}\\exp\\left(\\alpha V\\left(s^{\\prime}\\right)\\right)\\beta\\left(a|s\\right)M\\left(s^{\\prime}|s,a\\right)}}\\\\ &{\\qquad=\\frac{\\exp\\left(\\alpha V\\left(M(s,a)\\right)\\right)\\beta\\left(a|s\\right)}{\\sum_{a}\\exp\\left(\\alpha V\\left(M(s,a)\\right)\\right)\\beta\\left(a|s\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, $\\textstyle\\sum_{a}\\pi^{*}(a|s)=1$ . ", "page_idx": 16}, {"type": "text", "text": "Substitute Eq. (44) into Eq. (41), ", "page_idx": 16}, {"type": "equation", "text": "$$\nN^{*}(s^{\\prime}|s)=\\sum_{a}\\pi^{*}(a|s)\\mathbb{I}\\left[M(s,a)=s^{\\prime}\\right]\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Comparing Eq. (43) with Eq. (50), it holds that $N^{*}(s^{\\prime}|s)=M(s^{\\prime}|s,\\pi^{*}(\\cdot|s)),\\forall s\\sim\\mathcal{D}$ . As a result, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\boldsymbol{s}\\sim\\mathcal{D}}\\operatorname{D}_{\\mathrm{KL}}(N^{*}(\\cdot|\\boldsymbol{s})||M(\\cdot|\\boldsymbol{s},\\pi^{*}(\\cdot|\\boldsymbol{s})))=0\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Considering the non-negativity of KL divergence, the optimization problem in Eq. (35) achieves its global minimum at $\\pi^{*}$ . Therefore, $\\bar{R}(\\pi)$ also achieves its global maximum at $\\pi^{*}$ . ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{~argmax~}\\;\\hat{R}_{1}(\\pi)}\\\\ &{=\\underset{\\pi}{\\operatorname{argmax}}\\;\\underset{\\left(s,s^{\\prime}\\right)\\sim\\mathcal{P}}{\\operatorname{targmax}}\\;\\left[\\exp\\left(\\alpha\\left(V\\left(s^{\\prime}\\right)-V\\left(s\\right)\\right)\\right)\\log M\\left(s^{\\prime}\\right\\vert s,\\pi\\left(\\left\\vert s\\right)\\right)\\right]}\\\\ &{=\\underset{\\pi}{\\operatorname{argmax}}\\;\\underset{\\left(s,s^{\\prime}\\right)\\sim\\mathcal{P}}{\\operatorname{targmax}}\\;\\left[\\frac{Z\\left(s\\right)}{\\exp\\left(\\alpha V\\left(s\\right)\\right)Z\\left(s\\right)}\\exp\\left(\\alpha V\\left(s^{\\prime}\\right)\\right)\\log M\\left(s^{\\prime}\\left\\vert s,\\pi\\left(\\left\\vert s\\right)\\right)\\right]}\\\\ &{=\\underset{\\pi}{\\operatorname{argmax}}\\;\\underset{s\\sim\\mathcal{P}}{\\operatorname{targmax}}\\;\\frac{\\mathbb{E}}{\\pi}\\;\\underset{s\\in\\mathcal{N}}{\\operatorname{targm}}\\left[\\frac{Z\\left(s\\right)}{\\exp\\left(\\alpha V\\left(s\\right)\\right)Z\\left(s\\right)}\\exp\\left(\\alpha V\\left(s^{\\prime}\\right)\\right)\\log M\\left(s^{\\prime}\\left\\vert s,\\pi\\left(\\left\\vert s\\right)\\right)\\right]}\\\\ &{=\\underset{\\pi}{\\operatorname{argmax}}\\;\\underset{s\\sim\\mathcal{P}}{\\operatorname{targmax}}\\;\\frac{\\mathbb{E}}{\\pi}\\;\\underset{s\\in\\mathcal{N}}{\\operatorname{targm}}\\;\\left[\\frac{Z\\left(s\\right)}{\\exp\\left(\\alpha V\\left(s\\right)\\right)}\\log M\\left(s^{\\prime}\\left\\vert s,\\pi\\left(\\left\\vert s\\right)\\right)\\right]}\\\\ &{=\\underset{\\pi}{\\operatorname{argmin}}\\;\\underset{s\\sim\\mathcal{P}}{\\operatorname{targm}}\\;\\frac{\\mathbb{E}}{\\pi}\\;\\underset{s\\in\\mathcal{N}}{\\operatorname{targm}}\\;\\left[\\frac{Z\\left(s\\right)}{\\exp\\left(\\alpha V\\left(s\\right)\\right)}\\left(\\log M^{\\ast}\\left(s^{\\prime}\\left\\vert s\\right)-\\log M\\left(s^{\\prime}\\right\\vert s,\\pi\\left(\\left\\vert s\\right)\\right)\\right)\\right]}\\\\ &{=\\underset{\\pi}{\\operatorname{argmin}}\\;\\underset{s\\sim\\mathcal{P}}{\\operatorname{targm}}\\;\\left[\\frac{Z\\left(s\\right)}{\\exp\\left(\\alpha V\\left(s\\right)\\right)}\\mathrm{Det}\\left(N^{\\ast}\\left(s\\right)\\right)\\mathrm{Id}\\left(s^{\\prime}\\left\\vert s,\\pi\\left(\\left\\vert s\\right\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The fourth equality holds because of the relationship between $N^{*}$ and $N$ in Eq. (36). ", "page_idx": 17}, {"type": "text", "text": "As shown above, it holds that $N^{*}(s^{\\prime}|s)=M(s^{\\prime}|s,\\pi^{*}(\\cdot|s)),\\forall s\\sim\\mathcal{D}.$ . As a result, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}_{s\\sim\\mathcal{D}}\\,\\left[\\frac{Z(s)}{\\exp\\left(\\alpha V\\left(s\\right)\\right)}\\mathrm{D}_{\\mathrm{KL}}(N^{*}(\\cdot|s)\\|M(\\cdot|s,\\pi(\\cdot|s)))\\right]=0\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Considering $Z(s)/\\mathrm{exp}\\left(\\alpha V\\left(s\\right)\\right)>\\,0$ and the non-negativity of KL divergence, the optimization problem in Eq. (58) achieves its global minimum at $\\pi^{*}$ . Therefore, $\\bar{R}_{1}(\\pi)$ also achieves its global maximum at $\\pi^{*}$ . ", "page_idx": 17}, {"type": "text", "text": "In conclusion, when the environment dynamics is deterministic, both $\\bar{R}(\\pi)$ and $\\bar{R}_{1}(\\pi)$ achieve their global maximum at the policy $\\pi^{*}$ , and $\\pi^{*}$ makes the following equation hold: ", "page_idx": 17}, {"type": "equation", "text": "$$\nN^{*}(\\cdot|s)=M(\\cdot|s,\\pi^{*}(\\cdot|s)),\\;\\forall s\\sim\\mathcal{D}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Moreover, because $\\begin{array}{r}{\\pi^{*}(a|s)=\\frac{1}{Z(s)}\\exp\\left(\\alpha V\\left(M(s,a)\\right)\\right)\\beta(a|s)}\\end{array}$ , the support of $\\pi^{*}$ is included by that of the behavior policy $\\beta$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname{supp}(\\pi^{*}(\\cdot|s))\\subseteq\\operatorname{supp}(\\beta(\\cdot|s)),\\;\\forall s\\sim\\mathcal{D}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "B.3 Proof of Proposition 2 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proposition 4 (Proposition 2 in the main paper). When the dynamics is stochastic, the maximizers of both $\\bar{R}(\\pi)$ and $\\stackrel{\\cdot}{R}_{1}\\stackrel{\\cdot}{(\\pi)}$ are constrained within the support of the behavior policy: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{supp}(\\pi^{*}(\\cdot|s))\\subseteq\\mathrm{supp}(\\beta(\\cdot|s)),\\;\\forall s\\sim\\mathcal{D}}\\\\ &{\\mathrm{supp}(\\pi_{1}^{*}(\\cdot|s))\\subseteq\\mathrm{supp}(\\beta(\\cdot|s)),\\;\\forall s\\sim\\mathcal{D}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. We start with $\\bar{R}(\\pi)$ . ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{R}(\\pi):=\\underset{(s,s^{\\prime})\\sim\\mathcal{D}}{\\mathbb{E}}\\,\\left[\\frac{1}{Z(s)}\\exp\\left(\\alpha V\\left(s^{\\prime}\\right)\\right)\\log M(s^{\\prime}|s,\\pi(\\cdot|s))\\right]}\\\\ &{\\qquad=\\underset{(s,s^{\\prime})\\sim\\mathcal{D}}{\\mathbb{E}}\\,\\left[\\frac{1}{Z(s)}\\exp\\left(\\alpha V\\left(s^{\\prime}\\right)\\right)\\log\\left(\\underset{a}{\\sum}M(s^{\\prime}|s,a)\\pi(a|s)\\right)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Let $\\pi$ denote any valid policy. For $\\forall s\\in\\mathcal{D}$ , define $\\epsilon(s)$ and $n(s)$ as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle{\\epsilon(s):=\\sum_{a}\\mathbb{I}[\\beta(a|s)=0]\\pi(a|s)}}\\\\ {\\displaystyle{n(s):=\\sum_{a}\\mathbb{I}[\\beta(a|s)>0]}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "For $\\forall s\\in\\mathcal{D}$ , there exists at least one action $a$ such that $(s,a)\\in\\mathcal{D}$ . Thus it holds that $n(s)>0,\\forall s\\in$ $\\mathcal{D}$ . Then for $\\forall s\\in\\mathcal{D},\\forall\\pi$ , define $\\pi_{\\mathrm{in}}$ as follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pi_{\\mathrm{in}}(a|s)=\\left\\{\\begin{array}{l l}{\\pi(a|s)+\\frac{\\epsilon(s)}{n(s)},}&{\\beta(a|s)>0,}\\\\ {0,}&{\\beta(a|s)=0.}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "$\\pi_{\\mathrm{in}}$ can be seen as a projection of $\\pi$ onto $\\beta$ \u2019s support. Besides, for $\\forall s\\in\\mathcal{D}$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{a}\\pi_{\\mathrm{in}}(a|s)=\\displaystyle\\sum_{a}\\mathbb{I}[\\beta(a|s)>0]\\left(\\pi(a|s)+\\frac{\\epsilon(s)}{n(s)}\\right)}&{}\\\\ {\\displaystyle=\\sum_{a}\\mathbb{I}[\\beta(a|s)>0]\\pi(a|s)+\\epsilon(s)}&{}\\\\ {\\displaystyle=\\sum_{a}\\mathbb{I}[\\beta(a|s)>0]\\pi(a|s)+\\sum_{a}\\mathbb{I}[\\beta(a|s)=0]\\pi(a|s)}&{}\\\\ {\\displaystyle=\\sum_{a}\\pi(a|s)}&{}\\\\ {\\displaystyle=1}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Thus $\\pi_{\\mathrm{in}}$ is a valid policy. ", "page_idx": 18}, {"type": "text", "text": "Now we compare $\\bar{R}(\\pi_{\\mathrm{in}})$ with $\\bar{R}(\\pi)$ . For $\\forall(s,s^{\\prime})\\in\\mathcal{D}$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{a}M(s^{\\prime}|s,a)\\pi_{\\mathrm{in}}(a|s)-\\sum_{a}M(s^{\\prime}|s,a)\\pi(a|s)}\\\\ &{=\\displaystyle\\sum_{a}M(s^{\\prime}|s,a)\\left(\\pi_{\\mathrm{in}}(a|s)-\\pi(a|s)\\right)}\\\\ &{=\\displaystyle\\sum_{\\{a|\\beta(a|s)>0\\}}M(s^{\\prime}|s,a)\\left(\\pi_{\\mathrm{in}}(a|s)-\\pi(a|s)\\right)}\\\\ &{=\\displaystyle\\sum_{\\{a|\\beta(a|s)>0\\}}M(s^{\\prime}|s,a)\\frac{\\epsilon(s)}{n(s)}}\\\\ &{>\\displaystyle\\sum_{\\{a|\\beta(a|s)>0\\}}M(s^{\\prime}|s,a)\\frac{\\epsilon(s)}{n(s)}}\\\\ &{>\\ 0}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The second equality holds because, in tabular MDPs, the empirical dynamics model $M$ exactly computes the conditional distribution observed in the dataset. For transitions not contained in the dataset, $M=0$ [12]. The final inequality holds because $\\epsilon(s)\\geq0$ . ", "page_idx": 18}, {"type": "text", "text": "Therefore, ", "text_level": 1, "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\bar{R}(\\pi_{\\mathrm{in}})-\\bar{R}(\\pi)}\\\\ &{=\\underset{(s,s^{\\prime})\\sim\\mathcal{D}}{\\mathbb{E}}\\left[\\frac{1}{Z(s)}\\exp\\left(\\alpha V\\left(s^{\\prime}\\right)\\right)\\log\\left(\\frac{\\sum_{a}M(s^{\\prime}|s,a)\\pi_{\\mathrm{in}}(a|s)}{\\sum_{a}M(s^{\\prime}|s,a)\\pi(a|s)}\\right)\\right]}\\\\ &{\\geq\\underset{(s,s^{\\prime})\\sim\\mathcal{D}}{\\mathbb{E}}\\left[\\frac{1}{Z(s)}\\exp\\left(\\alpha V\\left(s^{\\prime}\\right)\\right)\\log\\left(1\\right)\\right]}\\\\ &{\\geq\\quad0}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Now suppose $\\pi$ is not constrained within the support of the behavior policy at some state $s_{1}\\in\\mathcal{D}$ : $\\operatorname{supp}(\\pi(\\cdot|s_{1}))\\nsubseteq\\operatorname{supp}(\\beta(\\cdot|s_{1}))$ . That is, $\\exists\\widetilde{a}_{1}$ such that $\\pi(\\tilde{a}_{1}|s_{1})>0$ and $\\beta(\\tilde{a}_{1}|s_{1})=0$ . Thus it holds that $\\begin{array}{r}{\\epsilon(s_{1})=\\sum_{a}\\mathbb{I}[\\beta(a|s_{1})=0]\\pi(a|s_{1})>0}\\end{array}$ . On the other hand, since $s_{1}\\in\\mathcal{D}$ , there exists at least one action $a_{1}$ and one state $s_{1}^{\\prime}$ such that $(s_{1},a_{1},s_{1}^{\\prime})\\in\\mathcal{D}$ . Thus it holds that $\\beta(a_{1}|s_{1})>0$ and $M(s_{1}^{\\prime}|s_{1},a_{1})>0$ . As a result, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{a}M(s_{1}^{\\prime}|s_{1},a)\\pi_{\\mathrm{in}}(a|s_{1})-\\sum_{a}M(s_{1}^{\\prime}|s_{1},a)\\pi(a|s_{1})}\\\\ &{=\\displaystyle\\sum_{\\{a|\\beta(a|s_{1})>0\\}}M(s_{1}^{\\prime}|s_{1},a)\\frac{\\epsilon(s_{1})}{n(s_{1})}}\\\\ &{>\\ \\ 0}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "In such case, $\\bar{R}(\\pi_{\\mathrm{in}})>\\bar{R}(\\pi)$ . Therefore, if $\\pi$ is not constrained within the support of the behavior policy at some state $s_{1}\\in\\mathcal{D}$ , we can find another policy $\\pi_{\\mathrm{in}}$ that is constrained within the support of the behavior policy and achieves higher objective function $\\bar{R}(\\pi_{\\mathrm{in}})$ . Consequently, $\\bar{R}(\\pi)$ must achieve its maximum at support constrained policy $\\pi^{*}$ : $\\operatorname{supp}(\\pi^{*}(\\cdot|s))\\subseteq\\operatorname{supp}(\\beta(\\cdot|s)),$ $\\forall s\\sim\\mathcal{D}$ . ", "page_idx": 19}, {"type": "text", "text": "Now we consider $\\bar{R}_{1}(\\pi)$ . ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{R}_{1}(\\pi):=\\underset{(s,s^{\\prime})\\sim\\mathcal{D}}{\\mathbb{E}}\\,\\left[\\exp\\left(\\alpha\\left(V\\left(s^{\\prime}\\right)-V\\left(s\\right)\\right)\\right)\\log M(s^{\\prime}|s,\\pi(\\cdot|s)\\right)\\right]}\\\\ &{\\qquad=\\underset{(s,s^{\\prime})\\sim\\mathcal{D}}{\\mathbb{E}}\\,\\left[\\exp\\left(\\alpha\\left(V\\left(s^{\\prime}\\right)-V\\left(s\\right)\\right)\\right)\\log\\left(\\underset{a}{\\sum}M(s^{\\prime}|s,a)\\pi(a|s)\\right)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "With the same definition of $\\epsilon(s),n(s)$ and $\\pi_{\\mathrm{in}}$ as in Eq. (66), Eq. (67) and Eq. (68), it also holds that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\bar{R}_{1}(\\pi_{\\mathrm{in}})-\\bar{R}_{1}(\\pi)}\\\\ &{=\\underset{(s,s^{\\prime})\\sim\\mathcal{D}}{\\mathbb{E}}\\left[\\exp\\left(\\alpha\\left(V\\left(s^{\\prime}\\right)-V\\left(s\\right)\\right)\\right)\\log\\left(\\frac{\\sum_{a}M(s^{\\prime}\\left|s,a\\right)\\pi_{\\mathrm{in}}\\left(a\\right|s\\right)}{\\sum_{a}M(s^{\\prime}\\left|s,a\\right)\\pi\\left(a\\right|s\\right)}\\right)\\right]}\\\\ &{\\geq\\underset{(s,s^{\\prime})\\sim\\mathcal{D}}{\\mathbb{E}}\\left[\\exp\\left(\\alpha\\left(V\\left(s^{\\prime}\\right)-V\\left(s\\right)\\right)\\right)\\log\\left(1\\right)\\right]}\\\\ &{\\geq\\quad0}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "As before, when supposing $\\pi$ is not constrained within the support of the behavior policy at some state $s_{1}\\in\\mathcal{D}$ , it holds that $\\tilde{R}_{1}(\\pi_{\\mathrm{in}})>\\bar{R}_{1}(\\pi)$ . Therefore, $\\bar{R}_{1}(\\pi)$ must achieve its maximum at support constrained policy $\\pi_{1}^{*}$ : $\\operatorname{supp}(\\pi_{1}^{*}(\\cdot|s))\\subseteq\\operatorname{supp}(\\beta(\\cdot|s)),\\;\\forall s\\sim\\mathbb{Z}$ . ", "page_idx": 19}, {"type": "text", "text": "In conclusion, when the environment dynamics is stochastic, the maximizers of both $\\bar{R}(\\pi)$ and $\\bar{R}_{1}(\\pi)$ are constrained within the support of the behavior policy: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname{supp}(\\pi^{*}(\\cdot|s))\\subseteq\\operatorname{supp}(\\beta(\\cdot|s)),\\,\\operatorname{supp}(\\pi_{1}^{*}(\\cdot|s))\\subseteq\\operatorname{supp}(\\beta(\\cdot|s)),\\,\\forall s\\sim\\mathcal{D}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "C Further Discussions ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "C.1 Rationale for Choosing $\\exp(\\alpha V(s))$ as the Empirical Normalizer ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Firstly, choosing $\\exp(\\alpha V(s))$ is intended to obtain something similar to the advantage function. With this normalizer, the weight of our regularizer is $\\exp(\\alpha(V(s^{\\prime})-V(s)))$ , which is comparable to the weight $\\exp(\\alpha A(s,a))$ in Advantage Weighted Regression (AWR) [47]. Here, $V(s^{\\prime})\\;{\\overline{{-\\;V(s)}}}$ represents the relative advantage of the next state $s^{\\prime}$ compared to the current state $s$ , while $A(s,a)$ reflects the relative advantage of taking action $a$ in $s$ compared to following the current policy. Comparison of the objectives of SCAS and AWR: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{SCAS:}}&{\\exp(\\alpha(V(s^{\\prime})-V(s)))\\log(M(s^{\\prime}|\\hat{s},\\pi(\\hat{s})))}\\\\ &{\\mathrm{AWR:}\\quad\\exp(\\alpha A(s,a))\\log\\pi(a|s)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Secondly, as discussed in the paper, introducing any normalizer that depends only on $s$ (independent of $s^{\\prime}$ ) does not affect the development and analysis of our method; it is merely for computational stability. In AWR-based methods, there also exists a normalizer $Z(s)$ and they usually disregard it [47, 45]. The rationale behind this is similar. ", "page_idx": 19}, {"type": "text", "text": "C.2 Pessimism and Robustness in SCAS ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In a specific sense, SCAS, which unifies OOD state correction and OOD action suppression, also integrates pessimism and state robustness. (1) Regarding pessimism: The OOD action suppression effect of SCAS aligns with the pessimism commonly discussed in offilne RL work (being pessimistic about OOD actions) [31, 70, 30, 3, 54]. Unlike traditional policy constraint methods [69, 30, 10, 47], our approach does not require the training policy to align with the behavior policy; it only requires the successor states to be within the dataset support, which is a more relaxed constraint. (2) Regarding state robustness: The OOD state correction effect of SCAS is aimed at improving the agent\u2019s robustness to OOD states during the test phase. Compared with previous works, SCAS unifies OOD state correction and OOD action suppression and additionally achieves value-aware OOD state correction. Some offline RL literature on state robustness differs from our approach; they typically consider noisy observations [72], such as sensor errors. In contrast, SCAS addresses state robustness concerning actual OOD states encountered during test time, rather than noisy observations. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "C.3 Regularization Effect at ID States ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In SCAS, there is regularization on the policy\u2019s output actions at ID states. In our regularizer, the perturbed states $\\hat{s}$ are sampled from $\\mathcal{N}(s,\\sigma^{2})$ , and a large portion of $\\hat{s}$ will fall near the original ID state $s$ or even be approximately equal to $s$ . Therefore, the policy\u2019s output actions at ID states are also regularized. For this part of the regularization, its role is equivalent to the ID state regularizer analyzed in Section 4, which has been theoretically shown to have the effect of suppressing OOD actions. Moreover, the experimental results in Section 6 also demonstrate that our OOD state correction regularizer addresses the traditional issue with OOD actions. ", "page_idx": 20}, {"type": "text", "text": "C.4 Differences between the OOD Action Issue and the OOD State Issue ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We further elucidate the differences between the well-known OOD action issue and the OOD state issue we analyzed. Most offilne RL works focus on the OOD action issue in the training phase. That is, the trained policy outputs OOD actions to compute the target Q, which results in extrapolation error and value divergence during training [12]. In contrast, the OOD state issue we defined and analyzed is in the test phase. That is, the agent can enter states out of the offilne dataset during test, potentially resulting in catastrophic failure. ", "page_idx": 20}, {"type": "text", "text": "D Experimental Details ", "text_level": 1, "page_idx": 20}, {"type": "table", "img_path": "anyZgGLQ6n/tmp/08994ce18fc637d35c91dd0c4253a4afbc57abae5e16e467a6e3ef7df4e733d8.jpg", "table_caption": ["Table 2: Hyperparameters in SCAS. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "All hyperparameters of SCAS are included in Table 2. Note that we use this same set of hyperparameters to obtain all the results reported in this paper (except for parameter study). Following $\\mathrm{TD3+BC}$ [10], we normalize the states in all datasets except for antmaze-large. We clip the exponentiated weight $\\exp\\left(\\alpha V_{\\theta}\\left(s^{\\prime}\\right)-\\alpha V_{\\theta}\\left(s\\right)\\right)$ in Eq. (19) to $(-\\infty,50]$ . Following the suggestions in the benchmark [9], we subtract 1 from the rewards for the Antmaze datasets. ", "page_idx": 20}, {"type": "text", "text": "Our evaluation criteria follow those used in most previous works. For the Gym locomotion tasks, we average returns over 10 evaluation trajectories and 5 random seeds, while for the Ant Maze tasks, we average over 100 evaluation trajectories and 5 random seeds. The reported results are the normalized scores, which are offered by the D4RL benchmark [9] to measure how the learned policy compared with random and expert policy: ", "page_idx": 21}, {"type": "equation", "text": "$$\n{\\mathrm{D4RL~score}}=100\\times{\\frac{{\\mathrm{learned~policy~return-random~policy~return}}}{{\\mathrm{expert~policy~return-random~policy~return}}}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The results of baselines reported in Table 1 are obtained as follows. We re-run OSR [22] on all datasets using their official codebase5 and tune the hyperparameters for each dataset as specified in their paper. We implement SDC [75] and re-run it on all datasets. We use the SDC-related hyperparameters as specified in their paper, and sweep the CQL-related hyperparameters in {1,2,5,10,20} for each dataset. We re-run OneStep RL [5] on all datasets using their official codebase6and the default hyperparameters. We implement BC [48] based on the $\\mathrm{TD}3\\substack{+\\mathrm{BC}}$ repository7and re-run it on all datasets. The results of other baselines are taken from [3] and [68]. The runtime in Table 1 is obtained by running offline RL algorithms on halfcheetah-medium-replay-v2 on a GeForce RTX 3090. ", "page_idx": 21}, {"type": "text", "text": "Figures 1(a) to 1(d) share the same embedding function obtained by running t-SNE on the set of all 200,000 samples (50,000 samples each from the dataset, CQL, $\\mathrm{TD3+BC}$ , and SCAS). This ensures a clear visual comparison. Figure 1(d) contains all the 200,000 samples, which is the union of the points in Figures 1(a) to 1(c). ", "page_idx": 21}, {"type": "text", "text": "E Additional Experimental Results ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "E.1 Additional Value Estimation Results ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Under the same setting of Figure 2, we conduct experiments on the additional datasets. The results are shown in Figure 5. We omit the Q values of Off-policy RL, SDC w/o CQL, and OSR w/o CQL at higher numbers of optimization steps, because these Q values diverge in the early learning stage, and plotting their Q values at later optimization steps would result in an excessive range on the vertical axis. The additional results also show that only SCAS\u2019s OOD state correction term can achieve OOD action suppression and prevent value over-estimation. ", "page_idx": 21}, {"type": "image", "img_path": "anyZgGLQ6n/tmp/0d4ad7c0273e53753ee5923c4459db2fc3b4d628cb7c7eedd869aab4941d27bb.jpg", "img_caption": ["Figure 5: Oracle Q-values of SCAS (estimated by MC return) and learned Q-values of SCAS and other algorithms across optimization steps. Here Off-policy RL is SCAS with weight $\\lambda=0$ in Eq. (20). Only SCAS\u2019s OOD state correction term can achieve OOD action suppression and prevent value over-estimation (divergence). "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "E.2 Additional Results on OOD State Correction ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "To further examine the OOD state correction effects of SCAS, we conduct experiments on a modified D4RL maze2d-open-v0 [9]. It is a 2D point robot navigation task in a rectangle map with vertices $(0,0)$ and $(3,5)$ . The agent needs to reach the goal at $(2,3)$ . We modify the dataset by removing all ", "page_idx": 21}, {"type": "text", "text": "the transitions containing states in a rectangle with vertices $(0,0)$ and (1.5, 2.5). During test, we let the initial state be randomly distributed in this OOD region. We train algorithms over $\\mathrm{\\breve{10}^{6}}$ gradient steps and average returns over 1000 evaluation trajectories. ", "page_idx": 22}, {"type": "text", "text": "The results of BC [48], $\\mathrm{TD3+BC}$ [10], CQL [31], MOPO [73], IQL [29], and SCAS are reported in Table 3. With the OOD state correction signals, SCAS corrects the agent out of the OOD region more quickly and stably, achieving significantly better performance than typical offline RL methods. ", "page_idx": 22}, {"type": "table", "img_path": "anyZgGLQ6n/tmp/6c09536d05cdb9b856a6bd38cdee53b20bf7320cb6e99aa09c3db99a3220eb8d.jpg", "table_caption": ["Table 3: Comparisons in modified maze2d-open-v0 over five random seeds. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "E.3 Comparisons on the NeoRL Benchmark ", "text_level": 1, "page_idx": 22}, {"type": "table", "img_path": "anyZgGLQ6n/tmp/39a9b25cec180d4b6e182579c15659ffb12ef26bcbf6e7a3c9ab871ce4cc743f.jpg", "table_caption": ["Table 4: Averaged normalized scores on the NeoRL benchmark over four random seeds. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "We also evaluate SCAS on the NeoRL benchmark [49]. NeoRL is a benchmark designed to simulate real-world scenarios by collecting datasets using a more conservative policy, aligning closely with realistic data collection scenarios. The narrow and limited data makes it challenging for offline RL algorithms. The results are shown in Table 4. The results of baselines are taken directly from the MOBILE paper [59]. According to Appendix C in [59], these results are obtained by tuning hyperparameters per dataset. For SCAS, we use the same fixed set of hyperparameters as specified in Appendix D. Without additional hyperparameter tuning, SCAS still performs comparably to MOBILE and outperforms other baselines in total scores. ", "page_idx": 22}, {"type": "text", "text": "E.4 Comparisons with Additional Baselines ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The original SCAS requires only one single hyperparameter configuration in implementations. For a fair comparison with DW [19], EDAC [2], RORL [72], SQL [71], and EQL [71], we roughly select $\\lambda$ from $\\{0.025,\\,0.25\\}$ for each dataset, referring to this variant as SCAS-ht. The results of SCAS-ht and these methods are reported in Table 5. Among the ensemble-free methods, SCAS-ht achieves the highest performance in both mujoco locomotion and antmaze domains. Compared with ensemble-based methods, SCAS-ht also performs better on antmaze tasks. DW [19] reweights ID data points by their values for behavior regularization and does not account for OOD states during the test phase. In contrast, our approach considers an OOD state correction scenario, resulting in enhanced robustness during the test phase and better performance. ", "page_idx": 22}, {"type": "text", "text": "E.5 Results of Combining SCAS Regularizer into Various Offline RL Objectives ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The SCAS regularizer is compatible with various offline RL objectives. We conduct experiments to combine SCAS with CQL [31], IQL [29], and TD3BC [10]. Comparisons between these combined algorithms and the original ones are shown in Table 6. We find that applying the SCAS regularizer leads to improved performance for these popular algorithms, which could be attributed to the OOD state correction effects of SCAS. However, we also find that these combined methods do not achieve better performance than the original SCAS (comparable on most tasks and worse on some tasks). We hypothesize that this is because SCAS already has the effect of OOD action suppression, and when combined with offilne RL objectives that also aim for OOD action suppression, it may become overly conservative. As a result, the combined algorithms may perform worse than the original SCAS on some sub-optimal datasets. ", "page_idx": 22}, {"type": "table", "img_path": "anyZgGLQ6n/tmp/33f6555dc61530d122b60113cfe44913b8044d494d400183edd9684ba431c782.jpg", "table_caption": ["Table 5: Comparisons with additional baselines on the D4RL benchmark. Here SCAS-ht means SCAS with slight hyperparameter tuning, selecting $\\lambda$ from $\\{0.025,0.25\\}$ . The results of SCAS-ht are averaged over 5 random seeds and the others are taken from their papers. "], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "anyZgGLQ6n/tmp/1235d83ea69c22ca9994832f90b51167b06a9b070431ef96e42bfb01661847d6.jpg", "table_caption": ["Table 6: Comparisons on the D4RL benchmark. Here $+{\\mathrm{SCAS}}$ means adding the SCAS regularizer. The results are averaged over 5 random seeds. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "E.6 Additional Parameter Study Results ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this section, we present additional parameter study results conducted on four challenging Antmaze tasks, including antmaze-large-play-v2, antmaze-large-diverse-v2, antmaze-medium-play-v2, and antmaze-medium-diverse-v2. ", "page_idx": 23}, {"type": "text", "text": "Inverse Temperature $\\alpha$ . The inverse temperature $\\alpha$ is the key hyperparameter in SCAS for achieving value-aware OOD state correction. It controls the significance of the values of next states in SCAS\u2019s ", "page_idx": 23}, {"type": "text", "text": "OOD state correction. If $\\alpha=0$ , the effect corresponds to vanilla OOD state correction. As $\\alpha$ gets larger, SCAS is more inclined to correct the agent to the high-value ID states. Thus we can assess the effectiveness of value-aware OOD state correction compared to vanilla OOD state correction by varying $\\alpha$ . Here we test SCAS with different $\\alpha$ and the results are shown in Figure 6. We observe that a large $\\alpha$ is crucial for achieving good performance on all the antmaze tasks, clearly demonstrating the effectiveness of our value-aware OOD state correction. However, too large $\\alpha$ $\\alpha=10$ ) induces less satisfying performance, probably due to the increased variance of the learning objective. In general, we find that choosing $\\alpha=5$ leads to the best performance. ", "page_idx": 24}, {"type": "image", "img_path": "anyZgGLQ6n/tmp/a6d929bda36a4292f3375c6d99935bac060cd20ff6ae6f82e96b6bf058b9973f.jpg", "img_caption": ["Figure 6: Additional results from the parameter study on the inverse temperature $\\alpha$ . The curves are averaged over 5 random seeds, with the shaded area representing the standard deviation across seeds. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Balance Coefficient $\\lambda$ . The balance coefficient $\\lambda$ controls the balance between vanilla policy improvement and SCAS regularization. If we set $\\lambda=0$ , SCAS degenerates into the vanilla off-policy RL algorithm. Here we vary $\\lambda$ in $\\{0,0.25,0.5,0.75,1\\}$ and present the corresponding learning curves of SCAS in Figure 7. Notably, SCAS is able to converge to good performance over a very wide range of $\\lambda$ . However, if $\\lambda=0$ , the vanilla off-policy RL suffers from extrapolation error and overestimation, demonstrating poor performance. We also observe a very interesting fact that even when $\\lambda=1$ and the signal from RL improvement (max Q) is removed, SCAS still performs well on most tasks. This could be attributed to the fact that value-aware OOD state correction implies some sort of improvement in policy by maximizing the values of policy-induced next states. ", "page_idx": 24}, {"type": "image", "img_path": "anyZgGLQ6n/tmp/c141308d308fa5e3102e1cdc158ba515c4859a2525fc70fcf6c9ef4c78295ae0.jpg", "img_caption": ["Figure 7: Additional results from the parameter study on the balance coefficient $\\lambda$ . The curves are averaged over 5 random seeds, with the shaded area representing the standard deviation across seeds. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Noise Scale $\\sigma$ . The noise scale $\\sigma$ is the standard deviation of the Gaussian noise added to the original states for formulating the SCAS regularizer. Here we test SCAS with different $\\sigma$ and present the corresponding learning curves in Figure 8. We observe a significant performance drop with too large $\\sigma$ $(\\sigma=1)$ ) on all the tasks, due to the heavily corrupted learning signal. On the other hand, when $\\sigma=0$ (without noise), the performance is also less satisfying. With $\\sigma=0$ , SCAS is still able to prevent the agent at ID states from entering OOD states, maintaining the agent in safe regions, but it cannot correct the agent from OOD states to $\\mathrm{ID}$ states as reliably as the original SCAS. In general, we find that choosing $\\sigma=0.001$ or 0.01 leads to the best performance. ", "page_idx": 24}, {"type": "image", "img_path": "anyZgGLQ6n/tmp/f5a95f4b2665d9fba422cf7fc32f14d676bb0c620e127e1d1f7100c3036a6c59.jpg", "img_caption": [], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Figure 8: Additional results from the parameter study on the noise scale $\\sigma$ . The curves are averaged over 5 random seeds, with the shaded area representing the standard deviation across seeds. ", "page_idx": 25}, {"type": "image", "img_path": "anyZgGLQ6n/tmp/6352dbb744680270d9d7990eba475ab0cbd246fe2cc95bf4d4775f365ea487b2.jpg", "img_caption": ["Figure 9: Performance of SCAS under different dynamics model checkpoints, which are obtained at different steps in the dynamics model training process. The figure plots the training loss of the dynamics model $M_{\\omega}$ and the corresponding normalized return of SCAS over 5 random seeds. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "E.7 Sensitivity Analysis on Dynamics Model Errors ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "To empirically investigate SCAS under different dynamics model errors, we run SCAS using different checkpoints of the trained dynamics model, which are obtained at different steps in the dynamics model training process. The model error is controlled by the number of trained steps. The results are shown in Figure 9. The figure plots the training loss of the dynamics model $M_{\\omega}$ and the corresponding normalized return of SCAS over 5 random seeds. We observe that the performance of SCAS increases with the number of trained steps of the dynamics model (i.e. the accuracy of the model) and stabilizes at a high level. ", "page_idx": 25}, {"type": "text", "text": "E.8 Learning Curves of SCAS ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Learning curves on Gym locomotion tasks and Antmaze tasks are presented in Figure 10 and Figure 11 respectively. The curves are averaged over 5 random seeds, with the shaded area representing the standard deviation across seeds. ", "page_idx": 25}, {"type": "text", "text": "F Broader Impact ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Offilne RL holds promise for facilitating practical RL applications in domains like robotics, healthcare, and education, where data collection is often costly or risky. However, it is important to recognize its potential negative societal impacts. One concern is that biases in offline data may transfer to the learned policy. In addition, offline RL may affect employment by automating tasks traditionally performed by humans, like factory automation or autonomous driving. Addressing these challenges will contribute to the responsible development and deployment of offline RL algorithms. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "From an academic standpoint, this research systematically analyze the OOD state issue in offline RL and propose SCAS, a simple yet effective approach that unifies OOD state correction and OOD action suppression. This work potentially offers researchers a new perspective on analyzing the OOD state issue and enhancing test-time robustness in offline RL. Besides, SCAS also holds the promise to be extended to safe RL [1, 17, 13], meta RL [8, 65, 66, 64, 4], and multi-agent RL [35, 52, 55, 51, 16]. ", "page_idx": 26}, {"type": "image", "img_path": "anyZgGLQ6n/tmp/8d3337e21979088eebe98698912547209179074a3e10220cf15e9f745332cb5e.jpg", "img_caption": ["Figure 10: Learning curves of SCAS on Gym locomotion tasks. The curves are averaged over 5 random seeds, with the shaded area representing the standard deviation across seeds. "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "anyZgGLQ6n/tmp/9105f24b47f0b16639a6eddba9ff96e7b19ad449c9890984347e877a04a9e5fb.jpg", "img_caption": [], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Figure 11: Learning curves of SCAS on AntMaze tasks. The curves are averaged over 5 random seeds, with the shaded area representing the standard deviation across seeds. ", "page_idx": 27}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 28}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: Please refer to Section 7. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 28}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Please refer to Appendix B. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 29}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Please refer to Appendix D. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 29}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: Please refer to the code in the supplemental material. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 30}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Please refer to Appendix D. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 30}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: The results in the paper are accompanied by standard deviations across multiple seeds. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Please refer to Appendix D. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 31}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The research conducted in the paper conforms, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 31}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Please refer to Appendix F. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 32}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The creators or original owners of assets used in the paper are properly credited and the license and terms of use are explicitly mentioned and properly respected. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 32}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 33}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: The code is well documented and anonymized. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 33}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 33}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 33}]