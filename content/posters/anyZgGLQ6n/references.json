{"references": [{"fullname_first_author": "Sergey Levine", "paper_title": "Offline reinforcement learning: Tutorial, review, and perspectives on open problems", "publication_date": "2020-05-01", "reason": "This paper provides a comprehensive overview of offline reinforcement learning, addressing key challenges and future directions, thus forming a strong foundation for the current research."}, {"fullname_first_author": "Aviral Kumar", "paper_title": "Conservative Q-learning for offline reinforcement learning", "publication_date": "2020-00-00", "reason": "This paper introduces Conservative Q-Learning (CQL), a highly influential algorithm for offline RL that addresses the out-of-distribution action problem, directly impacting the current work's focus on offline RL safety."}, {"fullname_first_author": "Scott Fujimoto", "paper_title": "A minimalist approach to offline reinforcement learning", "publication_date": "2021-00-00", "reason": "This work offers a simple yet effective offline RL algorithm, TD3+BC, providing a strong baseline for comparison and highlighting the importance of simplicity in offline RL."}, {"fullname_first_author": "Justin Fu", "paper_title": "D4RL: Datasets for deep data-driven reinforcement learning", "publication_date": "2020-00-00", "reason": "The D4RL benchmark is crucial to the current work, providing standard datasets for evaluating offline RL algorithms, thus shaping the empirical evaluation in this paper."}, {"fullname_first_author": "Hongchang Zhang", "paper_title": "State deviation correction for offline reinforcement learning", "publication_date": "2022-00-00", "reason": "This paper directly addresses the OOD state issue, a key focus of the current paper, proposing a method to correct agent states to stay within the data distribution during the test phase."}]}