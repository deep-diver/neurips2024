[{"figure_path": "ALU676zGFE/tables/tables_6_1.jpg", "caption": "Table 1: Person-wise gaze point and pair-wise social gaze annotation (positive/negative) statistics for our datasets. VSGaze unifies annotation types across VAT, ChildPlay, VideoCoAtt and UCO-LAEO.", "description": "This table presents the statistics of gaze points and social gaze annotations for several datasets used in the paper.  It shows the number of person-wise gaze point annotations and pair-wise social gaze annotations (both positive and negative examples) for each dataset. The table highlights the unification of annotation types within the VSGaze dataset, which combines data from multiple sources.", "section": "4.1 Datasets"}, {"figure_path": "ALU676zGFE/tables/tables_7_1.jpg", "caption": "Table 2: Comparison against gaze following methods on VSGaze. All models were trained on VSGaze. PP indicates social gaze predictions from post-processing gaze following outputs (\u2714) vs predictions from decoders (X). Best results are in bold, second best results are underlined.", "description": "This table compares the performance of the proposed multi-person temporal gaze following and social gaze prediction model against several state-of-the-art gaze following methods. The models are evaluated using various metrics, such as average precision (AP), F1 score (F1), and average distance (Dist.) on the VSGaze dataset. The table also shows the impact of using social gaze predictions from post-processing gaze following results (PP) versus predictions directly from the decoders.", "section": "5 Results"}, {"figure_path": "ALU676zGFE/tables/tables_8_1.jpg", "caption": "Table 3: Comparison against task specific methods fine-tuned on individual datasets. Best multi-person results are in bold, overall best results are underlined. Multi indicates multi-person (\u2714) vs single-person (X) gaze following methods. Ours is initialized from training on GazeFollow, while Ours\u2020 is initialized from training on VSGaze.", "description": "This table compares the performance of the proposed multi-person gaze following model against several state-of-the-art single-person models on four different datasets: GazeFollow, VAT, ChildPlay, and UCO-LAEO.  The table shows the AUC, Average Distance, and Minimum Distance metrics, along with an indicator of whether the method was multi-person or single-person. It also presents results for two versions of the proposed model: one initialized from training on GazeFollow, and another initialized from training on the VSGaze dataset.  The best results for multi-person methods and overall best results are highlighted.", "section": "5 Results"}, {"figure_path": "ALU676zGFE/tables/tables_8_2.jpg", "caption": "Table 2: Comparison against gaze following methods on VSGaze. All models were trained on VSGaze. PP indicates social gaze predictions from post-processing gaze following outputs (\u2714) vs predictions from decoders (X). Best results are in bold, second best results are underlined.", "description": "This table compares the performance of the proposed multi-person gaze following model against several state-of-the-art single-person methods on the VSGaze dataset.  It shows the average distance error (Dist.), average precision at 10% (AP10) and F1 scores for looking at humans (LAH), looking at each other (LAEO), and shared attention (SA). The table also contrasts the results of using post-processing of gaze following results to obtain social gaze labels versus directly predicting them from the decoders. The best and second-best performing models are highlighted.", "section": "5 Results"}, {"figure_path": "ALU676zGFE/tables/tables_8_3.jpg", "caption": "Table 3: Comparison against task specific methods fine-tuned on individual datasets. Best multi-person results are in bold, overall best results are underlined. Multi indicates multi-person (\u2714) vs single-person (X) gaze following methods. Ours is initialized from training on GazeFollow, while Ours\u2020 is initialized from training on VSGaze.", "description": "This table compares the performance of the proposed multi-person gaze following model against state-of-the-art single-person models on four different datasets.  It shows the average distance and average precision (AP) metrics for each model.  The table also highlights whether the model is multi-person or single-person and how it was initialized (either from GazeFollow or VSGaze).  The best performing model in each category is bolded, and the second best is underlined.", "section": "5 Results"}, {"figure_path": "ALU676zGFE/tables/tables_8_4.jpg", "caption": "Table 3: Comparison against task specific methods fine-tuned on individual datasets. Best multi-person results are in bold, overall best results are underlined. Multi indicates multi-person (\u2714) vs single-person (X) gaze following methods. Ours is initialized from training on GazeFollow, while Ours\u2020 is initialized from training on VSGaze.", "description": "This table compares the performance of the proposed multi-person gaze following model against several single-person and multi-person state-of-the-art methods on four different datasets.  It highlights the model's performance when fine-tuned on individual datasets versus when initialized from training on a larger unified dataset (VSGaze).  The metrics used are average distance error (Dist.) and Average Precision for LAEO (APLAEO).", "section": "5 Results"}, {"figure_path": "ALU676zGFE/tables/tables_14_1.jpg", "caption": "Table 2: Comparison against gaze following methods on VSGaze. All models were trained on VSGaze. PP indicates social gaze predictions from post-processing gaze following outputs (\u2714) vs predictions from decoders (X). Best results are in bold, second best results are underlined.", "description": "This table compares the performance of the proposed multi-person temporal gaze following and social gaze prediction model against several state-of-the-art single-person gaze following methods.  It shows performance metrics such as average precision (AP) at 10% recall, F1 scores for Looking at Humans (LAH) and Looking at Each Other (LAEO) social gaze tasks, and average precision for Shared Attention (SA). The comparison highlights the model's performance with and without post-processing social gaze predictions from gaze following outputs, demonstrating the benefits of the proposed joint prediction approach.", "section": "5 Results"}, {"figure_path": "ALU676zGFE/tables/tables_15_1.jpg", "caption": "Table 2: Comparison against gaze following methods on VSGaze. All models were trained on VSGaze. PP indicates social gaze predictions from post-processing gaze following outputs (\u2714) vs predictions from decoders (X). Best results are in bold, second best results are underlined.", "description": "This table compares the performance of different methods for gaze following on the VSGaze dataset.  It shows the performance using different approaches, including those that predict social gaze from post-processing gaze following results and those using dedicated decoders for both tasks.  Metrics include average precision (AP) for in-out gaze predictions, average precision at 10 (AP10), and F1 scores for Looking At Humans (LAH) and Looking At Each Other (LAEO) social gaze tasks, and average precision (AP) for Shared Attention (SA).  The best results for each metric are highlighted in bold, and the second-best results are underlined.", "section": "5 Results"}, {"figure_path": "ALU676zGFE/tables/tables_15_2.jpg", "caption": "Table 2: Comparison against gaze following methods on VSGaze. All models were trained on VSGaze. PP indicates social gaze predictions from post-processing gaze following outputs (\u2714) vs predictions from decoders (X). Best results are in bold, second best results are underlined.", "description": "This table compares the performance of the proposed multi-person temporal gaze following and social gaze prediction model against several state-of-the-art single-person gaze following methods.  The comparison is done using the VSGaze dataset.  The table shows several metrics including average precision (AP) for in-out gaze prediction, average precision for the social gaze task and F1 scores for LAH and LAEO. The table highlights the performance gain achieved by jointly modeling gaze following and social gaze tasks compared to traditional post-processing approaches.", "section": "5 Results"}, {"figure_path": "ALU676zGFE/tables/tables_16_1.jpg", "caption": "Table 2: Comparison against gaze following methods on VSGaze. All models were trained on VSGaze. PP indicates social gaze predictions from post-processing gaze following outputs (\u2714) vs predictions from decoders (X). Best results are in bold, second best results are underlined.", "description": "This table compares the performance of the proposed multi-person temporal gaze following and social gaze prediction model against existing state-of-the-art single-person gaze following methods.  The comparison is done on the VSGaze dataset.  It shows the Average Precision at 10 (AP10), F1-scores for Looking at Humans (LAH) and Looking at Each Other (LAEO), and Average Precision for Shared Attention (SA).  The table also contrasts results when social gaze predictions are derived from post-processing gaze following outputs versus directly from the model's decoders, highlighting the impact of the joint prediction framework.", "section": "5 Results"}]