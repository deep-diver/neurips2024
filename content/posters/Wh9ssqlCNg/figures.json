[{"figure_path": "Wh9ssqlCNg/figures/figures_1_1.jpg", "caption": "Figure 1: Our accelerated MoCo-v3 achieves standard MoCo-v3 performance using only 1/5 of the training budget on ImageNet-100 and 1/3 on ImageNet-1k. The training budget (x-axis) is measured as the training time normalized by the forward pass of the base non-accelerated backbone model, in million (M) units. The results for ImageNet-100 are shown in Fig. 1a and for ImageNet-1k in Fig. 1b.", "description": "This figure demonstrates the effectiveness of the proposed accelerated MoCo-v3 method.  It shows that the accelerated version achieves comparable performance to the standard MoCo-v3 model, but using significantly less training time (1/5 for ImageNet-100 and 1/3 for ImageNet-1k).  The x-axis represents the training budget in millions of forward passes, and the y-axis represents the accuracy.", "section": "1 Introduction"}, {"figure_path": "Wh9ssqlCNg/figures/figures_2_1.jpg", "caption": "Figure 2: Framework overview. We propose a method for accelerating augmentation invariance pre-training of transformer neural networks. Acceleration is achieved by compressing the ViT's input sequence length using two strategies: (1) randomized token dropout and (2) flexible patch scaling. We further introduce a gradient error analysis framework to assess the efficacy of an acceleration strategy, enabling us to define an optimal acceleration schedule that adjusts to the training progress. The acceleration strategy can be applied to a variety of methods. For example, SimCLR optimizes both encoders by gradient descent, while MoCo and DINO use a momentum encoder to compute the representations for the Key view. The loss function also differs across algorithms.", "description": "This figure illustrates the proposed framework for accelerating augmentation invariance pre-training in Vision Transformers (ViTs).  It shows how two compression strategies \u2013 randomized token dropout and flexible patch scaling \u2013 reduce the input sequence length, thereby speeding up training. The framework also incorporates gradient error analysis to determine an optimal acceleration schedule.", "section": "Accelerating Model Training"}, {"figure_path": "Wh9ssqlCNg/figures/figures_4_1.jpg", "caption": "Figure 1: Our accelerated MoCo-v3 achieves standard MoCo-v3 performance using only 1/5 of the training budget on ImageNet-100 and 1/3 on ImageNet-1k. The training budget (x-axis) is measured as the training time normalized by the forward pass of the base non-accelerated backbone model, in million (M) units. The results for ImageNet-100 are shown in Fig. 1a and for ImageNet-1k in Fig. 1b.", "description": "This figure showcases the training efficiency gains of the proposed accelerated MoCo-v3 model.  The plots compare the standard MoCo-v3 model with the accelerated version across two image datasets, ImageNet-100 and ImageNet-1k. The x-axis represents the training budget (time normalized by the forward pass of the base model), while the y-axis shows the model's performance. The results demonstrate significant speedups (4x on ImageNet-100 and 3.3x on ImageNet-1k) while maintaining comparable performance to the non-accelerated model.", "section": "1 Introduction"}, {"figure_path": "Wh9ssqlCNg/figures/figures_4_2.jpg", "caption": "Figure 3: Accelerated MoCo-v3 sample costs for varying dropout rates and patch sizes. We assume uncompressed key sequences.", "description": "This figure shows the sample costs for the accelerated MoCo-v3 model with different combinations of token dropout rates and patch sizes.  The x-axis represents the token dropout rate, ranging from 0 to 0.9, while the y-axis represents the patch size, ranging from 16 to 48. Each cell in the heatmap shows the sample cost (a normalized measure of computation time) for a specific combination of dropout rate and patch size.  The heatmap illustrates the trade-off between compression level and computational cost; higher compression (larger dropout rates and larger patch sizes) results in lower sample costs, but potentially at the expense of accuracy.", "section": "Combined Sequence Compression"}, {"figure_path": "Wh9ssqlCNg/figures/figures_6_1.jpg", "caption": "Figure 4: Error profile of accelerated gradients. From top to bottom, the three panels show the CA-MSE, squared bias and cost-adjusted variance of the gradient estimates, using different acceleration strategies and at different stages of training.", "description": "This figure visualizes the trade-off between bias and variance of gradient estimates at different training stages using various acceleration strategies (different token dropout rates and patch sizes).  The top panel shows the cost-adjusted mean squared error (CA-MSE), the middle panel shows the squared bias, and the bottom panel shows the cost-adjusted variance. Each panel shows the error at five different stages (0%, 25%, 50%, 75%, and 100%) of the training process. The figure helps in identifying the optimal acceleration strategy at each stage by minimizing the CA-MSE.", "section": "5 Gradient Estimation Analysis of Acceleration Strategies"}, {"figure_path": "Wh9ssqlCNg/figures/figures_7_1.jpg", "caption": "Figure 6: Cosine vs Polynomial (a = 2) learning rate decay schedules.", "description": "This figure compares two different learning rate decay schedules: cosine annealing and polynomial decay (with \u03b1 = 2).  The x-axis represents the training iteration, and the y-axis shows the learning rate.  The graph illustrates how the learning rate decreases over the course of training for both methods, with the polynomial schedule maintaining a higher learning rate for a longer period than cosine annealing. This difference in learning rate decay impacts the training process and the model's performance, especially in scenarios with constrained training budgets.", "section": "6.2 Constant Gradient Acceleration Strategies"}, {"figure_path": "Wh9ssqlCNg/figures/figures_8_1.jpg", "caption": "Figure 4: Error profile of accelerated gradients. From top to bottom, the three panels show the CA-MSE, squared bias and cost-adjusted variance of the gradient estimates, using different acceleration strategies and at different stages of training.", "description": "This figure visualizes the impact of different acceleration strategies on gradient estimation at various stages of training.  It shows the cost-adjusted mean squared error (CA-MSE), bias, and variance of the gradient estimates.  Each panel shows these measures across different acceleration strategies (varying levels of token dropout and patch size) and training progress (0%, 25%, 50%, 75%, and 100%).  It helps to understand the trade-offs between acceleration (lower computation cost) and accuracy (lower error in gradient estimation).", "section": "5 Gradient Estimation Analysis of Acceleration Strategies"}, {"figure_path": "Wh9ssqlCNg/figures/figures_8_2.jpg", "caption": "Figure 7: Training curves using constant symmetric patch scaling (training budget: 100M).", "description": "This figure shows the training curves obtained using constant symmetric patch scaling with a training budget of 100 million units.  The x-axis represents the training budget used (in millions), while the y-axis shows the KNN accuracy achieved.  Different colored lines represent experiments using different patch sizes (16, 20, 24, 30, and 40). The plot visually demonstrates how different patch sizes affect model training performance within the given budget constraint.", "section": "6.2 Constant Gradient Acceleration Strategies"}, {"figure_path": "Wh9ssqlCNg/figures/figures_9_1.jpg", "caption": "Figure 8: Training curve of three acceleration strategies: constant patch size, constant token dropout, and dynamic scheduling of joint patch scaling and token dropout.", "description": "This figure compares three different acceleration strategies for training a model: using a constant patch size, a constant token dropout ratio, and a dynamic schedule that adjusts both patch size and token dropout ratio over time.  The x-axis represents the training budget (in millions of units), and the y-axis shows the Nearest Neighbor (NN) accuracy achieved. The dynamic schedule outperforms the constant strategies, achieving higher accuracy with a lower budget. A star also marks the NN accuracy of the baseline MoCo model.", "section": "6.3 Optimized Acceleration Schedules"}]