[{"figure_path": "LGXeIx75sc/figures/figures_0_1.jpg", "caption": "Figure 1: Personalized segmentation task involves segmenting a specific reference object in a new scene. Our method is capable to accurately identify the specific reference instance in the target image, even when other objects from the same class are present. While other methods capture visually or semantically similar objects, our method can successfully extract the identical instance, by using a new personalized feature map and fusing semantic and appearance cues. Red and green indicate incorrect and correct segmentations respectively.", "description": "This figure shows examples of personalized segmentation. The task is to segment a specific object (e.g., a dog, a van, or a cat) in a new image, even if there are other similar objects present. The authors' method successfully identifies the target instance, unlike other methods (DINOv2 and PerSAM) that either capture visually or semantically similar objects. Red indicates incorrect segmentation, while green indicates correct segmentation.", "section": "Introduction"}, {"figure_path": "LGXeIx75sc/figures/figures_3_1.jpg", "caption": "Figure 2: (a) PCA visualization of QSA features obtained from the first self-attention block in the last layer of the U-Net module, at various diffusion timesteps. Objects with similar textures and colors have similar features. The dog's color in I\u2081 is similar to the colors of both the dog and the cat in I2, indicating textural similarity. Additionally, the localization is sharper at larger timesteps. (b) Visualization of the cross-attention map FSCT for a given prompt \"dog\". Note the higher region correlation (brighter colors) corresponding to the dog, while overlooking the cat in the bottom image.", "description": "This figure visualizes the features extracted from a pre-trained text-to-image diffusion model.  (a) shows a Principal Component Analysis (PCA) of features from the self-attention block of the U-Net at different diffusion timesteps, demonstrating that features with similar colors and textures cluster together. (b) shows the cross-attention map generated using the prompt \"dog\", highlighting the regions corresponding to dogs in the image.", "section": "3.2 Are instance features even encoded in a pre-trained text-to-image model?"}, {"figure_path": "LGXeIx75sc/figures/figures_4_1.jpg", "caption": "Figure 3: An overview of our Personalized Diffusion Features Matching approach. PDM combines semantic and appearance features for zero-shot personalized retrieval and segmentation. We first extract features from the reference, I<sub>r</sub> and target I<sub>t</sub> images. Appearance similarity is determined by dot product of cropped foreground features from the reference feature map, F<sup>AM</sup><sub>r</sub> and the target feature map F<sup>A</sup><sub>t</sub> (Eq. 5). Semantic similarity is calculated as the product between class name token C and the target semantic feature map F<sup>S</sup><sub>t</sub> to create a Semantic Map (Eq. 6). The final similarity map S<sup>DF</sup> combines both maps by average pooling. Note, that while the appearance and semantic maps attend on two dogs, their fusion yields a single and correct result.", "description": "This figure illustrates the Personalized Diffusion Features Matching (PDM) approach.  PDM uses features extracted from a reference image and a target image to perform personalized retrieval and segmentation.  It combines appearance and semantic cues. Appearance similarity is calculated using a dot product between cropped foreground features from the reference and target image feature maps. Semantic similarity uses the dot product of the class name token and the target semantic feature map.  These two similarities are combined using average pooling to create a final similarity map used for retrieval and segmentation.", "section": "3 Method"}, {"figure_path": "LGXeIx75sc/figures/figures_5_1.jpg", "caption": "Figure 4: Examples of personalized retrieval and segmentation benchmarks. Current benchmarks mostly show one single instance in an image or multiple instances from different object classes. Our benchmark for both retrieval and segmentation introduces a realistic and challenging case where multiple instances from the same object class are in the image, e.g. two dogs or multiple cars.", "description": "This figure showcases examples from different datasets used for personalized retrieval and segmentation tasks.  It highlights a key difference between existing benchmarks (ROxford, DAVIS, PerSeg) and the proposed PerMIR benchmark. The existing benchmarks primarily contain images with either a single instance of an object or multiple instances of different object classes. In contrast, the proposed PerMIR benchmark includes images with multiple instances of the same object class, making the task of personalized retrieval and segmentation more challenging and realistic.", "section": "4 Evaluation Datasets for Personalized Retrieval and Segmentation"}, {"figure_path": "LGXeIx75sc/figures/figures_7_1.jpg", "caption": "Figure 5: Qualitative Comparison: (a) Personalized Segmentation: Red and green indicate incorrect and correct segmentations, respectively. Our method accurately recognizes the reference instance despite significant variations (view angle, pose, or scale), while other methods often capture false positives from the same category. (b) Image Retrieval: Top-1 retrieved image is shown for each method. Note how our model identifies images containing the same instance, despite their small size and large variations. Other methods tend to capture only semantic similarity. Retrieval images have been zoomed in and cropped for clarity.", "description": "This figure provides a qualitative comparison of the proposed method (PDM) against other methods for personalized segmentation and retrieval. The left side shows that PDM accurately segments the target object even with variations in pose and view, unlike other methods which often incorrectly segment similar objects. The right side demonstrates PDM's ability to retrieve images containing the identical target object, even with size and variation differences, surpassing other methods that primarily capture semantic similarities.", "section": "5.1 Personalized Image Segmentation"}, {"figure_path": "LGXeIx75sc/figures/figures_12_1.jpg", "caption": "Figure 3: An overview of our Personalized Diffusion Features Matching approach. PDM combines semantic and appearance features for zero-shot personalized retrieval and segmentation. We first extract features from the reference, I<sub>r</sub> and target I<sub>t</sub> images. Appearance similarity is determined by dot product of cropped foreground features from the reference feature map, F<sub>AM</sub> and the target feature map F<sub>A</sub> (Eq. 5). Semantic similarity is calculated as the product between class name token C and the target semantic feature map F<sub>S</sub> to create a Semantic Map (Eq. 6). The final similarity map S<sub>DF</sub> combines both maps by average pooling. Note, that while the appearance and semantic maps attend on two dogs, their fusion yields a single and correct result.", "description": "This figure illustrates the Personalized Diffusion Features Matching (PDM) approach.  PDM uses a combination of semantic and appearance features extracted from both reference and target images to achieve zero-shot personalized retrieval and segmentation. The process involves calculating appearance similarity (using a dot product of cropped features), semantic similarity (using a class name token and semantic feature map), and then combining these maps via average pooling to create a final similarity map that localizes the target object precisely.  The figure highlights that even if both appearance and semantic maps show multiple similar instances, the combination leads to the correct identification of a single specific instance.", "section": "3 Method"}, {"figure_path": "LGXeIx75sc/figures/figures_14_1.jpg", "caption": "Figure 4: Examples of personalized retrieval and segmentation benchmarks. Current benchmarks mostly show one single instance in an image or multiple instances from different object classes. Our benchmark for both retrieval and segmentation introduces a realistic and challenging case where multiple instances from the same object class are in the image, e.g. two dogs or multiple cars.", "description": "This figure shows examples from different personalized retrieval and segmentation benchmarks.  Existing benchmarks typically show either a single instance of an object or multiple instances of objects from different classes.  The authors highlight that this makes the task easier than real-world scenarios. They then introduce their own benchmarks (PerMIR and PerMIS), which include multiple instances of the same object class within a single image to increase the difficulty and realism of the task.  This makes it more challenging to distinguish between similar instances using only semantic features, necessitating the use of personalized features.", "section": "4 Evaluation Datasets for Personalized Retrieval and Segmentation"}, {"figure_path": "LGXeIx75sc/figures/figures_15_1.jpg", "caption": "Figure 5: Qualitative Comparison: (a) Personalized Segmentation: Red and green indicate incorrect and correct segmentations, respectively. Our method accurately recognizes the reference instance despite significant variations (view angle, pose, or scale), while other methods often capture false positives from the same category. (b) Image Retrieval: Top-1 retrieved image is shown for each method. Note how our model identifies images containing the same instance, despite their small size and large variations. Other methods tend to capture only semantic similarity. Retrieval images have been zoomed in and cropped for clarity.", "description": "This figure compares the performance of the proposed method (PDM) against other state-of-the-art methods for personalized segmentation and retrieval.  The top row shows examples where the PDM successfully identifies and segments the target object despite variations in view, pose, or scale. Other methods often fail, identifying similar but incorrect objects. The bottom row demonstrates the superior instance retrieval capability of PDM, identifying images with the same object as the query even if the size and appearance differ significantly.  Other methods struggle with this task, often relying on semantic similarity rather than exact instance identification.", "section": "5.1 Personalized Image Segmentation"}]