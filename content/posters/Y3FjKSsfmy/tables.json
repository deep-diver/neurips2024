[{"figure_path": "Y3FjKSsfmy/tables/tables_5_1.jpg", "caption": "Table 1: Comparison of the feature whitening technique from VICREG and SO on CIFAR-10.", "description": "This table presents a comparison of the performance of the BYOL self-supervised learning method with and without two different techniques: feature whitening from the VICREG method and Soft Orthogonality (SO).  The results are shown in terms of Top-1 and Top-5 accuracy on the CIFAR-10 dataset.  The table demonstrates the effect of each technique on the overall accuracy of the BYOL method.", "section": "5 Analysis of Dimensional Collapse and the Effects of OR in SSL"}, {"figure_path": "Y3FjKSsfmy/tables/tables_8_1.jpg", "caption": "Table 2: Classification accuracy on CIFAR-100 (CNN backbones). SSL methods (in Solo-learn) are trained with or without OR on CIFAR-100. The best results are in bold, the second best in italics.", "description": "This table presents the Top-1 and Top-5 classification accuracy results on CIFAR-100 using various CNN backbones and 13 SSL methods.  The results are shown for both the original SSL methods and those modified with Orthogonality Regularization (OR) using either Soft Orthogonality (SO) or Spectral Restricted Isometry Property Regularization (SRIP).  The best and second-best results for each model/backbone are highlighted.", "section": "6 Numerical Experiments"}, {"figure_path": "Y3FjKSsfmy/tables/tables_8_2.jpg", "caption": "Table 3: Classification accuracy on CIFAR-100 (VITs). DINO (in Solo-learn) is trained with or without OR on CIFAR-100.", "description": "This table presents the classification accuracy results on the CIFAR-100 dataset using the DINO (in Solo-learn) self-supervised learning method.  The results are broken down by the type of Vision Transformer (ViT) encoder used (VIT-tiny, VIT-small, VIT-base) and whether or not Orthogonal Regularization (OR) was applied during training.  The \"Top-1\" and \"Top-5\" columns indicate the accuracy of the top-1 and top-5 predictions, respectively.", "section": "6 Numerical Experiments"}, {"figure_path": "Y3FjKSsfmy/tables/tables_8_3.jpg", "caption": "Table 4: Performance of SSL methods on LightlySSL. ResNet18 is used on CIFRA-10, CIFAR-100 and IMAGENET-100, and ResNet50 is employed on IMAGENET-1K.", "description": "This table presents the performance of 13 self-supervised learning (SSL) methods on four different datasets (CIFAR-10, CIFAR-100, IMAGENET-100, and IMAGENET-1K) using two different backbones: ResNet18 and ResNet50.  The results show the performance of both the original SSL methods and those with the addition of Soft Orthogonality (SO) as a regularizer.  The table highlights the impact of the proposed orthogonality regularization technique on the performance of various SSL methods across different datasets and backbones.", "section": "6 Numerical Experiments"}, {"figure_path": "Y3FjKSsfmy/tables/tables_9_1.jpg", "caption": "Table 5: Comparison of the feature whitening technique from VICREG and SO on CIFAR-10.", "description": "This table compares the performance of three different BYOL models trained on CIFAR-10: a baseline BYOL model, a BYOL model with Soft Orthogonality (SO) regularization, and a BYOL model using the feature whitening technique from the VICREG method.  The results are reported as Top-1 and Top-5 accuracies, showcasing the impact of different regularization methods on the model's performance.  The table highlights how SO regularization, in contrast to the feature whitening technique, leads to improved results.", "section": "5 Analysis of Dimensional Collapse and the Effects of OR in SSL"}, {"figure_path": "Y3FjKSsfmy/tables/tables_9_2.jpg", "caption": "Table 6: Classification accuracy on transfer learning datasets.", "description": "This table presents the classification accuracy achieved on various transfer learning datasets after pre-training with different methods (BYOL with and without Orthogonality Regularization).  It demonstrates the generalization capability of the models trained with orthogonality regularization, showcasing improved performance across a range of downstream tasks.", "section": "6 Numerical Experiments"}, {"figure_path": "Y3FjKSsfmy/tables/tables_9_3.jpg", "caption": "Table 5: Classification and objection detection performance. BYOL is trained with or without OR on IMAGENET-1k (ResNet50 with batchsize 128, Epoch 100). The best results are in bold, the second best in italics.", "description": "This table compares the performance of BYOL with and without Orthogonality Regularization (OR) on the ImageNet-1k dataset.  The model used is ResNet50, trained for 100 epochs with a batch size of 128.  The results show the Top-1 and Top-5 accuracy for image classification and Average Precision (AP) metrics for object detection on the validation set. The best performing model in each category is highlighted in bold.", "section": "6.2 OR Works on Large-scale Dataset"}, {"figure_path": "Y3FjKSsfmy/tables/tables_15_1.jpg", "caption": "Table 1: Comparison of the feature whitening technique from VICREG and SO on CIFAR-10.", "description": "This table compares the top-1 and top-5 accuracies of three different BYOL models trained on CIFAR-10: the original BYOL model, BYOL with soft orthogonality (SO) regularization, and BYOL with the feature whitening technique from VICREG.  It demonstrates the impact of different regularization techniques on model performance, specifically highlighting the effectiveness of SO compared to feature whitening.", "section": "5 Analysis of Dimensional Collapse and the Effects of OR in SSL"}, {"figure_path": "Y3FjKSsfmy/tables/tables_20_1.jpg", "caption": "Table 1: Comparison of the feature whitening technique from VICREG and SO on CIFAR-10.", "description": "This table compares the top-1 and top-5 accuracy of the BYOL model on CIFAR-10 dataset with and without using the feature whitening technique from VICREG and Soft Orthogonality (SO). It demonstrates the effect of each technique on improving the model's performance.", "section": "5 Analysis of Dimensional Collapse and the Effects of OR in SSL"}, {"figure_path": "Y3FjKSsfmy/tables/tables_21_1.jpg", "caption": "Table 1: Comparison of the feature whitening technique from VICREG and SO on CIFAR-10.", "description": "This table compares the top-1 and top-5 classification accuracy of the BYOL model on the CIFAR-10 dataset using three different methods: the original BYOL model, BYOL with soft orthogonality (SO) regularization, and BYOL with the feature whitening technique from the VICREG model.  The results show that both SO regularization and the feature whitening technique improve the accuracy compared to the original BYOL model. ", "section": "5 Analysis of Dimensional Collapse and the Effects of OR in SSL"}]