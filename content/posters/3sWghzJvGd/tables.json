[{"figure_path": "3sWghzJvGd/tables/tables_1_1.jpg", "caption": "Table 1: Reward values on unseen perturbed states by rotation (\u03b1) or mask (\u03b2%) with N(0.15, 0.5).", "description": "This table presents the reward values obtained from testing a world model on unseen perturbed states.  The perturbations are introduced in two ways: rotation (\u03b1) and masking (\u03b2%).  Gaussian noise N(0.15, 0.5) is added to the perturbations.  The table shows how the reward varies with different batch sizes (8, 16, 32, 64) and levels of perturbation (\u03b1 = 10, 20, 30 and \u03b2 = 25, 50, 75).  The results illustrate the impact of latent representation errors during training on the model's generalization capabilities to unseen states.", "section": "3.1 Latent Representation Errors in CNN Encoder-Decoder Networks"}, {"figure_path": "3sWghzJvGd/tables/tables_8_1.jpg", "caption": "Table 1: Reward values on unseen perturbed states by rotation (\u03b1) or mask (\u03b2%) with N(0.15, 0.5).", "description": "This table presents the reward values obtained from experiments conducted on unseen perturbed states.  The perturbations applied were rotations (\u03b1 degrees) and masking (\u03b2% of the image), both with added Gaussian noise (N(0.15, 0.5)).  The results are broken down by batch size (8, 16, 32, 64) used during training. The table shows that moderate batch sizes seem to lead to better generalization performance.", "section": "3.1 Latent Representation Errors in CNN Encoder-Decoder Networks"}, {"figure_path": "3sWghzJvGd/tables/tables_8_2.jpg", "caption": "Table 1: Reward values on unseen perturbed states by rotation (\u03b1) or mask (\u03b2%) with N(0.15, 0.5).", "description": "This table presents the reward values obtained from testing a world model on unseen, perturbed states.  The perturbations applied are rotation (at different angles \u03b1) and masking (\u03b2% of the image is masked with Gaussian noise N(0.15, 0.5)). The table shows the reward values for different batch sizes (8, 16, 32, 64) used during training of the world model, allowing analysis of how different training parameters influence performance on unseen test data.", "section": "3.1 Latent Representation Errors in CNN Encoder-Decoder Networks"}, {"figure_path": "3sWghzJvGd/tables/tables_8_3.jpg", "caption": "Table 1: Reward values on unseen perturbed states by rotation (\u03b1) or mask (\u03b2%) with N(0.15, 0.5).", "description": "This table presents the reward values obtained by the model on unseen perturbed states. The perturbations were introduced through rotation (\u03b1) and masking (\u03b2%) with Gaussian noise N(0.15, 0.5).  The table shows the impact of different batch sizes (8, 16, 32, 64) and perturbation levels (\u03b1 = 10, 20, 30; \u03b2 = 25, 50, 75) on the reward values.  It demonstrates the effect of latent representation errors during the training phase on generalization capabilities.", "section": "3.1 Latent Representation Errors in CNN Encoder-Decoder Networks"}, {"figure_path": "3sWghzJvGd/tables/tables_26_1.jpg", "caption": "Table 1: Reward values on unseen perturbed states by rotation (\u03b1) or mask (\u03b2%) with N(0.15, 0.5).", "description": "This table presents the reward values obtained by the model on unseen perturbed states. The perturbations applied were rotation and masking a percentage of the image with Gaussian noise. Different levels of perturbation were tested (\u03b1 = 10, 20, 30, \u03b2 = 25, 50, 75) with varying batch sizes (8, 16, 32, 64). The table helps in understanding how the model generalizes under different levels of perturbations and batch sizes.", "section": "3.1 Latent Representation Errors in CNN Encoder-Decoder Networks"}, {"figure_path": "3sWghzJvGd/tables/tables_27_1.jpg", "caption": "Table 1: Reward values on unseen perturbed states by rotation (\u03b1) or mask (\u03b2%) with N(0.15, 0.5).", "description": "This table presents the reward values obtained from an experiment designed to evaluate the generalization capability of a world model on unseen perturbed states. The states were perturbed using two methods: rotation and masking.  The rotation perturbation parameter is denoted by '\u03b1', while the masking percentage is denoted by '\u03b2'. The experiment uses Gaussian noise N(0.15, 0.5).  Different batch sizes (8, 16, 32, 64) were tested to assess their influence on the results.", "section": "3.1 Latent Representation Errors in CNN Encoder-Decoder Networks"}, {"figure_path": "3sWghzJvGd/tables/tables_27_2.jpg", "caption": "Table 1: Reward values on unseen perturbed states by rotation (\u03b1) or mask (\u03b2%) with N(0.15, 0.5).", "description": "This table presents the reward values obtained from an experiment on unseen perturbed states.  The perturbations are introduced through rotation (\u03b1) and masking (\u03b2%), using a Gaussian noise distribution with mean 0.15 and standard deviation 0.5. The table shows the impact of different perturbation levels and batch sizes on the reward values, highlighting how different configurations influence generalization performance in the context of latent representation errors.", "section": "3.1 Latent Representation Errors in CNN Encoder-Decoder Networks"}, {"figure_path": "3sWghzJvGd/tables/tables_27_3.jpg", "caption": "Table 1: Reward values on unseen perturbed states by rotation (\u03b1) or mask (\u03b2%) with N(0.15, 0.5).", "description": "This table presents the reward values obtained by the model on unseen perturbed states.  The perturbations applied are rotation (with different degrees \u03b1) and masking (with different percentages \u03b2%). Gaussian noise N(0.15, 0.5) is added to the perturbed states. Different batch sizes (8, 16, 32, 64) are used during the training phase. The results show how the reward varies under different perturbation types and levels, as well as different batch sizes used for training.", "section": "3.1 Latent Representation Errors in CNN Encoder-Decoder Networks"}, {"figure_path": "3sWghzJvGd/tables/tables_28_1.jpg", "caption": "Table 1: Reward values on unseen perturbed states by rotation (\u03b1) or mask (\u03b2%) with N(0.15, 0.5).", "description": "This table presents the results of an experiment evaluating the performance of a world model on unseen, perturbed states.  The perturbations applied were rotation (\u03b1) and masking (\u03b2%), both with added Gaussian noise N(0.15, 0.5).  The table shows reward values achieved by the model using different batch sizes (8, 16, 32, 64) and perturbation levels.", "section": "3.1 Latent Representation Errors in CNN Encoder-Decoder Networks"}, {"figure_path": "3sWghzJvGd/tables/tables_28_2.jpg", "caption": "Table 1: Reward values on unseen perturbed states by rotation (\u03b1) or mask (\u03b2%) with N(0.15, 0.5).", "description": "This table presents the results of an experiment evaluating the impact of latent representation errors on the generalization capability of a world model.  The experiment uses various perturbations (rotation and masking) applied to unseen states,  introducing varying degrees of error.  Reward values are reported across different batch sizes and levels of perturbation. The data suggests that moderate latent representation errors can be beneficial for generalization.", "section": "3.1 Latent Representation Errors in CNN Encoder-Decoder Networks"}, {"figure_path": "3sWghzJvGd/tables/tables_28_3.jpg", "caption": "Table 1: Reward values on unseen perturbed states by rotation (\u03b1) or mask (\u03b2%) with N(0.15, 0.5).", "description": "This table presents the results of an experiment evaluating the performance of a world model on unseen, perturbed states. The perturbations are in the form of rotations (\u03b1) and masking (\u03b2%), with Gaussian noise N(0.15, 0.5) added. The reward values are reported for different batch sizes (8, 16, 32, 64). This experiment aims to understand the impact of latent representation errors on generalization and shows the generalization ability of the model under different conditions.", "section": "3.1 Latent Representation Errors in CNN Encoder-Decoder Networks"}]