[{"figure_path": "SYjxhKcXoN/figures/figures_1_1.jpg", "caption": "Figure 1: Pipeline of LFME. Experts and the target model are trained simultaneously. To obtain a target model that is an expert on all source domains, we learn multiple experts specialized in corresponding domains to help guide the target model during training. For each sample, the guidance is implemented with a logit regularization term that enforces similarity between the logit of the target model and probability from the corresponding expert. Only the target model is utilized in inference. Please refer to Algorithm 1 for detailed implementations.", "description": "The figure illustrates the training and testing pipeline of the LFME framework. During training, multiple expert models are trained on different source domains, each specializing in a particular domain. Simultaneously, a target model is trained, receiving guidance from the expert models via a logit regularization term. This term enforces similarity between the logits of the target model and the probability outputs of the corresponding expert models. In the testing phase, only the target model is used to make predictions on unseen data. The figure highlights the flow of data and guidance between the expert models, the target model, and the test data.", "section": "1 Introduction"}, {"figure_path": "SYjxhKcXoN/figures/figures_4_1.jpg", "caption": "Figure 2: Values of probabilities, logits, and rescaling factors(i.e. q, z, F, F') from the ERM model and LFME. Models are trained on three source domains from PACS with the same settings.", "description": "The figure shows the average values of probabilities, logits and rescaling factors obtained from ERM and LFME models trained on three source domains from PACS dataset.  The plots visualize the changes in probability distributions (q), logits (z), and rescaling factors (F and F') over training iterations.  It illustrates how LFME regularizes logits and results in smoother probability distributions compared to ERM. This difference is further analyzed to explain why LFME improves Domain Generalization.", "section": "Deeper Analysis: How the Simple Logit Regularization Term Benefits DG?"}, {"figure_path": "SYjxhKcXoN/figures/figures_7_1.jpg", "caption": "Figure 3: Qualitative comparisons. The compared methods make unsatisfactory predictions about several objects, such as clouds with varying shapes, car logo, or people and car in the shadow. Please zoom in for a better view.", "description": "This figure shows a qualitative comparison of semantic segmentation results from different methods on real-world images.  The results from the baseline, SD, PinMem, and the proposed LFME method are shown alongside the ground truth. The figure highlights how the LFME method produces more accurate segmentations, especially for challenging objects such as those partially obscured or with unusual shapes or lighting conditions.", "section": "5 Experiments"}, {"figure_path": "SYjxhKcXoN/figures/figures_15_1.jpg", "caption": "Figure 1: Pipeline of LFME. Experts and the target model are trained simultaneously. To obtain a target model that is an expert on all source domains, we learn multiple experts specialized in corresponding domains to help guide the target model during training. For each sample, the guidance is implemented with a logit regularization term that enforces similarity between the logit of the target model and probability from the corresponding expert. Only the target model is utilized in inference.", "description": "This figure illustrates the training and testing pipeline of the LFME framework.  During training, multiple expert models are trained simultaneously, each specializing in a different source domain.  The target model, which will be used for inference, also trains concurrently.  The experts guide the target model by regularizing its logits to be similar to the probability distribution from the corresponding expert.  During testing, only the target model is used, resulting in a efficient, single model deployment.", "section": "1 Introduction"}, {"figure_path": "SYjxhKcXoN/figures/figures_16_1.jpg", "caption": "Figure 3: Qualitative comparisons. The compared methods make unsatisfactory predictions about several objects, such as clouds with varying shapes, car logo, or people and car in the shadow. Please zoom in for a better view.", "description": "This figure shows qualitative comparisons of the proposed LFME method against baseline and other state-of-the-art methods on the semantic segmentation task. It highlights the superior performance of LFME in handling challenging scenarios with significant domain shifts, such as variations in object shapes, presence of shadows, and unusual background contexts.  The results visually demonstrate LFME's ability to generate more accurate and comprehensive segmentations compared to other methods.", "section": "5 Experiments"}, {"figure_path": "SYjxhKcXoN/figures/figures_18_1.jpg", "caption": "Figure 2: Values of probabilities, logits, and rescaling factors(i.e. q, z, F, F') from the ERM model and LFME. Models are trained on three source domains from PACS with the same settings.", "description": "The figure shows the changes in probabilities (q), logits (z), and rescaling factors (F, F') during the training process of both ERM and LFME models. The models were trained on three source domains from the PACS dataset using identical settings.  The plots illustrate how LFME, through its logit regularization, leads to a smoother probability distribution and a more balanced logit range compared to the ERM model. These differences are linked to the ability of LFME to incorporate more information and focus on more challenging samples during training, ultimately improving generalization performance.", "section": "4 Deeper Analysis: How the Simple Logit Regularization Term Benefits DG?"}, {"figure_path": "SYjxhKcXoN/figures/figures_18_2.jpg", "caption": "Figure 1: Pipeline of LFME. Experts and the target model are trained simultaneously. To obtain a target model that is an expert on all source domains, we learn multiple experts specialized in corresponding domains to help guide the target model during training. For each sample, the guidance is implemented with a logit regularization term that enforces similarity between the logit of the target model and probability from the corresponding expert. Only the target model is utilized in inference. Please refer to Algorithm 1 for detailed implementations.", "description": "This figure illustrates the training and testing pipeline of the proposed LFME framework. During training, multiple experts are trained simultaneously with the target model. Each expert specializes in a different source domain. The experts guide the target model by regularizing its logits using the experts' output probabilities, ensuring the target model learns from the expertise of each domain. During testing, only the target model is used for inference.", "section": "1 Introduction"}]