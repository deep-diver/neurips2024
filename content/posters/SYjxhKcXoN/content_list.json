[{"type": "text", "text": "LFME: A Simple Framework for Learning from Multiple Experts in Domain Generalization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Liang Chen1 Yong Zhang2\u2217 Yibing Song Zhiqiang Shen1 Lingqiao Liu3\u2217 ", "page_idx": 0}, {"type": "text", "text": "1 MBZUAI 2 Meituan Inc. 3 The University of Adelaide {liangchen527, zhangyong201303, yibingsong.cv}@gmail.com Zhiqing.Shen@mbzuai.ac.ae lingqiao.liu@adelaide.edu.au ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Domain generalization (DG) methods aim to maintain good performance in an unseen target domain by using training data from multiple source domains. While success on certain occasions are observed, enhancing the baseline across most scenarios remains challenging. This work introduces a simple yet effective framework, dubbed learning from multiple experts (LFME), that aims to make the target model an expert in all source domains to improve DG. Specifically, besides learning the target model used in inference, LFME will also train multiple experts specialized in different domains, whose output probabilities provide professional guidance by simply regularizing the logit of the target model. Delving deep into the framework, we reveal that the introduced logit regularization term implicitly provides effects of enabling the target model to harness more information, and mining hard samples from the experts during training. Extensive experiments on benchmarks from different DG tasks demonstrate that LFME is consistently beneficial to the baseline and can achieve comparable performance to existing arts. Code is available at https://github.com/liangchen527/LFME. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Deep networks trained with sufficient labeled data are expected to perform well when the training and test domains with similar distributions [74, 4]. However, test domains in real-world often exhibit unexpected characteristics, leading to significant performance degradation for the trained model. Such a problem is referred to as distribution shift and is ubiquitous in common tasks such as image classification [43, 27] and semantic segmentation [19, 40]. Various domain generalization (DG) approaches have been proposed to address the distribution shift problem lately, such as invariant representation learning [53, 69, 57, 62, 29], augmentation [91, 48, 79, 47], adversarial learning [24, 46, 81, 49], meta-learning [44, 2, 22, 45], to name a few. Yet, according to [27], most arts perform inferior to the classical Empirical Risk Minimization (ERM) when applied with restricted hyperparameter search and evaluation protocol. Both the experiments in [27] and our findings suggest that existing models are incapable of consistently improving ERM in all evaluated datasets. The consistent improvement for ERM thus becomes our motivation to further explore DG. ", "page_idx": 0}, {"type": "text", "text": "Our approach derives from the observation in [10] that some of the data encountered at test time are similar to one or more source domains, and in which case, utilizing expert models specialized in the domains might aid the model in making a better prediction. The observation can be better interpreted with the example in Fig. 1. Given experts trained in the \u201cinfograph\", \u201creal\", and \u201cquickdraw\" domains, and test samples from the novel \u201csketch\" domain. Due to the large domain shift, it would be better to rely mostly on the expert trained on the similar \u201cquickdraw\" domain than others. ", "page_idx": 0}, {"type": "image", "img_path": "SYjxhKcXoN/tmp/e789184ad349e1a62770a103894e66b6680d184f14053c6476cebfb47b6a8231.jpg", "img_caption": ["Figure 1: Pipeline of LFME. Experts and the target model are trained simultaneously. To obtain a target model that is an expert on all source domains, we learn multiple experts specialized in corresponding domains to help guide the target model during training. For each sample, the guidance is implemented with a logit regularization term that enforces similarity between the logit of the target model and probability from the corresponding expert. Only the target model is utilized in inference. Please refer to Algorithm 1 for detailed implementations. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "However, the test domain information is often unavailable in DG, indicating that we can not specifically train an expert specialized in a particular domain. In light of this, obtaining a target model that is an expert on all source domains seems to be a practical alternative for handling potential arbitrary test domains. A naive implementation would be training multiple experts on each domain, and dynamically aggregate them to form the target model. So that any encountered test samples can be predicted by corresponding experts who are familiar with their characteristics. Nevertheless, such a practice has two inherent limitations: (1) designing an effective aggregation mechanism is essential and inevitable for the naive model. In fact, our experimental study indicates that using naive aggregating strategies, such as averaging, may deteriorate the performance. (2) the overall framework requires much more resources for deployment when there are many training domains, since all the experts must be leveraged during the test phase. ", "page_idx": 1}, {"type": "text", "text": "This work proposes a simple framework for learning from multiple experts (LFME), capable of obtaining an expert specialized in all source domains while avoiding the aforementioned limitations. Specifically, during the training stage, instead of heuristically aggregating different experts, we suggest training a universal target model that directly inherits knowledge from all these experts, which is achieved by a simple logit regularization term that enforces the logit of the target model to be similar to probability from the corresponding expert. With this approach, the target model is expected to leverage professional guidance from multiple experts, evolving into an expert across all source domains. During the test phase, only the target model is utilized. As a result, both model aggregations and extra memory and computation resources are not required during the deployment, since we only leverage one model. The overall training and test pipelines are illustrated in Fig. 1. ", "page_idx": 1}, {"type": "text", "text": "Our method can be interpreted through the lens of knowledge distillation (KD), where the core idea is transferring knowledge by training the student (i.e. the target model) with soft labels from teachers (i.e. experts) [31]. Unlike traditional KD [31] that uses teachers\u2019 output probabilities as soft labels in a cross entropy loss, we employ a logit regularization term that uses experts\u2019 probabilities to refine the logit of the target model in a regression manner, which can be regarded as extending the effectiveness of mean squared error (MSE) loss in classification [37] within the KD realm. ", "page_idx": 1}, {"type": "text", "text": "To gain a deeper understanding of the effectiveness of our logit regularization term, we perform in-depth analyses and uncover that its merit over the baseline can be explained in twofold. (1) It implicitly regularizes the probability of the target model within a smaller range, enabling it to use more information for prediction and improve DG accordingly. It is noteworthy that the effect is achieved inherently differs from that by label smoothing (LS) [71], as LFME does not require explicitly calibration for the output probability. Expanding upon this analysis, we find that a simple combination of cross entropy and MSE losses achieves comparable performance among existing arts. Given its straightforward implementation without unnecessary complexities, this expanding may offer a \"free lunch\" for DG; (2) It further boosts generalization by helping the target model to focus more on hard samples from the experts, supported by our theoretical finding. Through experiments on different datasets, we find that hard samples from the experts are more beneficial for generalization than those from the model itself. Given that hard sample mining is essential for easing distribution shift [35, 42], this discovery may offer valuable guidance for future research endeavors. ", "page_idx": 1}, {"type": "text", "text": "Through evaluations on the classification task with the DomainBed benchmark [27] and segmentation task with the synthetic [63, 64] to real [20, 83, 55] setting, we illustrate that LFME is consistently beneficial to the baseline and can obtain favorable performance against current arts (other KD ideas included). Our method favors extreme simplicity, adding only one hyper-parameter, that can be tuned in a rather large range, upon training the baseline ERM. ", "page_idx": 2}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "General DG methods. Domain generalization (DG), designed to enable a learned model to maintain robust results in unknown distribution shift, is gaining increasing attention in the research community lately. The problem can be traced back to a decade ago [6], and various approaches and applications have been proposed to push the generalization boundary ever since [53, 26, 46, 32, 81, 49, 2, 16, 11, 13, 82, 12]. The pioneering work [4] theoretically proves that the DG performance is bounded by both the intra-domain accuracy and inter-domain differences. Most previous arts focus on reducing the inter-domain differences by learning domain-invariant features with ideas such as kernel methods [53, 26], feature disentanglement [10, 61], and gradient regularization techniques [69, 62]. Same endeavors also include different learning skills: adversarial training is leveraged [24, 81] to enforce representations to be domain agnostic; meta-learning is utilized [2, 45] to simulate distribution shifts during training. Other works aim to improve the intra-domain accuracy: some suggest explicitly mining hard samples or representations with handcraft designs, such as masking out dominant features [36], weighting more on hard samples [42], or both [35]. LFME falls into this category as the target model can also mine hard samples from the experts, beneficial for excelling in all source domains (in Sec. 6.5). ", "page_idx": 2}, {"type": "text", "text": "Utilizing experts for DG. Methods with the most relevant motivations with our LFME are perhaps those also involves experts [86, 28, 89, 90]. In DAELDG [90], a shared feature extractor is adopted, which is followed by different classifiers (i.e. expert) that correspond to specific domains. Their experts are trained by enforcing the outputs to be similar to the average output from the non-expert classifiers. Different from our work, it uses the average outputs from different experts as the final result. In Meta-DMoE [89], similar to LFME, different experts are trained on their specific domains where a traditional KD idea is adopted: the feature from the target model is enforced to be similar to the transformer-processed version of their experts\u2019 output features. Notably, Meta-DMoE and LFME share very distinct objectives for the expert models. Specifically, Meta-DMoE aims to adapt the trained target model to a new domain in test. To facilitate adaptation, their target model is assumed to be capable of identifying domain-specific information (DSI), and is enforced to extract DSI similar to those from domain experts. In their settings, domain experts are expected to thrive in all domains and are used not in their trained domains but rather in an unseen one. Differently, LFME expects its target model to be expert in all source domains, where domain experts provide professional guidance for the target model only in their corresponding domains. Additionally, Meta-DMoE involves meta-learning and test-time training, which is more complicated than the end-to-end training adopted in LFME. Unlike their empirical design, our method is more self-contained, supported by in-depth analysis to explain its efficacy (in Sec. 4). We further show in our experiments (in Sec. 5.1 and E) that these related two methods perform inferior to our design, and the improvements from their basic designs: using average performance (in Sec. 6.4) or enforcing feature similarity between the target model and experts (in Sec. 6.3) are subtle compared to our logit regularization. ", "page_idx": 2}, {"type": "text", "text": "DG in semantic segmentation. Different from image classification, semantic segmentation involves classifying each pixel of the image, and the generalizing task often expects a model trained from the synthetic environments to perform well on real-world views. Directly extending general DG ideas to semantic segmentation is not easy. Current solutions mainly consist of domain randomization [34, 85], normalization [19, 56, 73], or using some task-related designs, such as the class memory bank in [40]. Different from some existing DG methods, LFME can be directly extended to ease the distribution shift problem in the semantic segmentation task without requiring any tweaks, and we show that it can obtain competitive performance against recent arts specially designed for the task. ", "page_idx": 2}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Problem Setting. In the vanilla DG setting, we are given $M$ source domains $\\mathcal{D}_{s}=\\{\\mathcal{D}_{1},\\mathcal{D}_{2},...,\\mathcal{D}_{M}\\}$ , where ${\\mathcal{D}}_{i}$ is the $i$ -th source domain containing data-label pairs $(x_{n}^{i},y_{n}^{i})$ sampled from different probabilities on the joint space $\\chi\\times\\!y$ , the goal is to learn a model from ${\\mathcal{D}}_{s}$ for making predictions on the data from the unseen target domain $\\mathcal{D}_{M+1}$ . For either DG or the downstream semantic segmentation task, source and target domains are considered with an identical label space, and we assume that there are a total of $K$ classes. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Learning multiple experts. LFME trains all the experts and the target model simultaneously, and the training procedure is illustrated in the upper part of Fig. 1. As each expert corresponds to a specific domain, given the $M$ source domains, a total of $M$ experts have to be trained during this stage, and the training process of each expert is the same as that of the ERM model. Given a training batch $\\mathcal{B}\\in\\mathcal{D}_{s}$ , for the $i$ -th expert $E_{i}$ , we only use data from the $i$ -th domain, and the computed loss regarding the data-label pair $(x^{i},y^{i})\\in(\\mathcal{B}\\cap\\mathcal{D}_{i})$ can be written as, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{i}=\\mathcal{H}(q^{E_{i}},y^{i}),\\mathrm{~s.t.~}q_{c}^{E_{i}}=\\mathrm{softmax}(z_{c}^{E_{i}})=\\frac{e x p(z_{c}^{E_{i}})}{\\sum_{j}^{K}e x p(z_{j}^{E_{i}})},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $q^{E_{i}}\\in\\mathbb{R}^{K}$ is the output probability computed by applying the softmax function over the output logits $z^{E_{i}}$ with $z^{E_{i}}=E_{i}(x^{i});$ H denotes the cross-entropy loss: $\\begin{array}{r}{\\mathcal{H}(q,y)=\\sum_{c}^{K}-y_{c}\\log q_{c}}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "Learning the target model. Data-label pairs $(x,y)\\in{\\mathcal{B}}$ from all domains are used for training the target model $T$ . The main classification loss $\\mathcal{L}_{c l a}$ is computed similar to Eq. (1): $\\mathcal{L}_{c l a}=\\mathcal{H}(q,y)$ , s.t. $\\begin{array}{r}{q=\\mathrm{softmax}(z)=\\frac{e x p(z_{c})}{\\sum_{j}^{K}e x p(z_{j})}}\\end{array}$ \ud835\udc52\ud835\udc3e\ud835\udc65\ud835\udc52\ud835\udc5d\ud835\udc65(\ud835\udc5d\ud835\udc67(\ud835\udc50\ud835\udc67)\ud835\udc57) and \ud835\udc67= \ud835\udc47(\ud835\udc65). Then, to incorporate professional guidance from the experts, we further introduce a logit regularization term $\\mathcal{L}_{g u i d}$ , to assist $T$ to become an expert on all source domains, which is computed by using the probabilities from the experts as a label for $T$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{g u i d}=\\|z-q^{E}\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $q^{E}$ is the concatenate probabilities from different experts along the batch dimension 2, $\\|\\cdot\\|$ denotes the $L_{2}$ norm, and this term is only enforced on the target model. Note we use the normalized version of $z^{E}\\;(i.e.\\;q^{E})$ for computing $\\mathcal{L}_{g u i d}$ , which can be regarded as extending the effectiveness of MSE loss [37] in the KD realm. Our experimental studies (in Sec. 6.3) find it leads to better performance, and the following contents also elaborate on the motivation. Then, the overall loss $\\mathcal{L}_{a l l}$ for updating the target model can be represented as, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{a l l}=\\mathcal{L}_{c l a}+\\frac{\\alpha}{2}\\mathcal{L}_{g u i d},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\alpha$ is the weight parameter, the only additional parameter upon ERM. We train the target model and the experts simultaneously for simplicity. Please refer to pseudocode in Algorithm 1 for details. ", "page_idx": 3}, {"type": "text", "text": "Rationality (comprehension from a KD perspective). Our logit regularization term can be viewed as a new KD form, wherein the fundamental principle is to utilize the teachers\u2019 (i.e. experts) outputs as soft labels for the student (i.e. target model) in a training objective [31]. In the context of classification tasks, the cross entropy loss $\\mathcal{H}(q,y)$ is widely used in the literature. Based on this objective, an intuitive revision to achieve distillation is by replacing the ground-truth label $y$ with $q^{\\dot{E}}$ in a cross entropy regularization manner (i. $\\therefore{\\mathcal{H}}(q,q^{E}))$ , which builds the rationality for the pioneering KD art [31]. Nevertheless, a recent study [37] suggests that the MSE loss $\\|z-y\\|$ (without applying softmax function on $z$ ) performs as well as the cross entropy loss when being applied in the classification task. Correspondingly, a distillation scheme motivated by this objective can thus be utilizing the soft label $q^{E}$ in a regression manner, which comes to our logit-regularized term: $\\|z-q^{E}\\|$ . From this perspective, the rationality of the introduced logit regularization term aligns with the principle of KD, and it can be regarded as extending the applicability of MSE loss in classification to the KD realm. We compare our new KD form with other ideas in Sec 6.3, demonstrating its superior performance against existing KD ideas in the DG task. We delve deep into our method and explain the effectiveness of LFME in the following section. ", "page_idx": 3}, {"type": "text", "text": "Computational cost. Inherited from KD, training LFME inevitably requires more resources as both the teacher and student have to be involved during training. However, LFME uses the same test resources as the baseline ERM given only the target model is utilized. Meanwhile, please also note that the computational cost for LFME is not proportional w.r.t the domain size. Instead, the training cost will always be doubled compared to ERM, as each sample will require two forward passes: one for the target model and the other for the corresponding expert. Please refer to Tab. 11 for training time comparisons between different arts. In Sec. F, we show that simply increasing training resources for current arts cannot improve their performances, suggesting it may not be a primary factor in DG. ", "page_idx": 3}, {"type": "image", "img_path": "SYjxhKcXoN/tmp/db14b64d8464f84aab3c9b38fb2f636dce0dfd6f597265960763ad4aec812086.jpg", "img_caption": ["(a) $q$ from ERM (b) \ud835\udc5efrom LFME (c) \ud835\udc67from ERM (d) \ud835\udc67from LFME (e) F and F \u2032 Figure 2: Values of probabilities, logits, and rescaling factors(i.e. \ud835\udc5e, \ud835\udc67, $\\mathcal{F}$ , F \u2032) from the ERM model and LFME. Models are trained on three source domains from PACS with the same settings. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "4 Deeper Analysis: How the Simple Logit Regularization Term Benefits DG? ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "4.1 Enabling the Target Model to Use More Information ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Specifically, for the baseline model, using only the classification loss $\\mathcal{L}_{c l a}$ encourages the probability $q$ to be diverse: the ground truth $q_{*}$ to approximate 1 and 0 for others. Consequently, the corresponding logits $z_{*}$ will increase ceaselessly, i.e. $z_{*}\\to+\\infty$ , and vice versa for $z_{c}$ , i.e. $z_{c}\\rightarrow-\\infty,\\forall c\\neq*$ (depicted in Fig. 2 (a) and (c)), as this is the solution for minimizing \u2212log \ud835\udc52\ud835\udc52\ud835\udc65\ud835\udc65\ud835\udc5d\ud835\udc5d((\ud835\udc67\ud835\udc67\u2217\ud835\udc50)) . From another point of view, $\\mathcal{L}_{c l a}$ encourages the model to explicitly focus on the dominant and exclusive features that are strongly discriminative but may be biased towards simplistic patterns [25]. ", "page_idx": 4}, {"type": "text", "text": "Differently, when $\\mathcal{L}_{g u i d}$ is imposed, the logits $z$ will approximate the range of $q^{E}$ (i.e. [0,1]). Eventually, the final logits will balance these two losses (i.e. $z_{*}\\in(q_{*}^{E},+\\infty)$ and $z_{c}\\in(-\\infty,q_{c}^{E})\\forall c\\neq*$ as shown in Fig. 2 (d)), resulting in a smoother distribution of $q$ , where $q_{c}$ $(\\forall c\\neq*)$ in LFME will be larger than it is in ERM (see Fig. 2 (b)). Since both two losses encourage the model to make a good prediction $(i.e.~z_{*}$ is expected to be the largest in both losses), the increase of $q_{c}$ indicates that besides learning the dominant features, the target model will be enforced to learn other information that is shared with others. Compared with ERM that prevents the model from learning other features, LFME is more likely to make good predictions when certain types of features are missing while others exist in unseen domains. We provide a \u201cfree lunch\" inspired by the analysis in Sec. 6.1, and more justifications (including visual and empirical evidence) to support this analysis in Sec. D.1 and D.2. In Sec. C, we demonstrate that other KD ideas face challenges in achieving the same merit. ", "page_idx": 4}, {"type": "text", "text": "The above finding can also be endorsed by the information theory [68]. Specifically, with a smoother distribution of $q$ , the entropy, which measures information, will naturally increases [51], suggesting a theoretical basis for utilizing more information in LFME. According to the principle of maximum entropy [38], the improvement for generalization is thus foreseeable [88]. ", "page_idx": 4}, {"type": "text", "text": "Note this effect is achieved inherently differs from that by label smoothing (LS) [71], as LFME does not involve hand-crafted settings to deliberately calibrate the output probability, which is essential in LS. In Sec. 6.2, we show LS is ineffective in DG compared to LFME. Besides the advantage of avoiding problems raised by potential improper heuristic designs, we show in the following that the logit regularization in LFME provides another merit over LS. ", "page_idx": 4}, {"type": "text", "text": "4.2 Enabling the Target Model to Mine Hard Samples from the Experts ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "This effect is realized by example re-weighting, motivated by proposition 2 in [72]. The single sample gradient of $\\mathcal{L}_{a l l}$ with respect to the $c$ -th logit value $z_{c}$ can be formulated as, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{\\partial\\mathcal{L}_{a l l}}{\\partial z_{c}}=q_{c}-y_{c}+\\alpha\\big(z_{c}-q_{c}^{E}\\big).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "When the target probability corresponds to the ground-truth ${{y}_{*}}=1$ , Eq. (4) is reduced into, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{\\partial\\mathcal{L}_{a l l}}{\\partial z_{\\ast}}=q_{\\ast}-1+\\alpha\\big(z_{\\ast}-q_{\\ast}^{E}\\big).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In this situation, the rescaling factor $\\mathcal{F}$ is given by, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{F}=\\frac{\\partial\\mathcal{L}_{a l l}}{\\partial z_{*}}/\\frac{\\partial\\mathcal{L}_{c l a}}{\\partial z_{*}}=1-\\alpha\\frac{z_{*}-q_{*}^{E}}{1-q_{*}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "On the other hand, for $\\forall c\\neq*$ , summing the gradient values over the finite indexes gives, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\sum_{c\\neq*}\\frac{\\partial\\mathcal{L}_{a l l}}{\\partial z_{c}}=\\sum_{c\\neq*}q_{c}+\\alpha\\sum_{c\\neq*}(z_{c}-q_{c}^{E}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Table 1: Evaluations in DomainBed with default settings (3 random seeds each with 20 trials). Top5 and score count how often a method achieves the top 5 performance and outperforms ERM. Results with $^{\\dagger}$ are from the ResNet50 backbone and others are with ResNet18. Best and second bests results are highlighted. Results with SWAD are cited from [8], and all others are reevaluated in our device. ", "page_idx": 5}, {"type": "table", "img_path": "SYjxhKcXoN/tmp/951910c1d26c4fb2b7b52668fadf9629c1d101a553da59c45ffb482f881ec6c3.jpg", "table_caption": [], "table_footnote": ["and the rescaling factor $\\mathcal{F^{\\prime}}$ in this situation is, "], "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{F}^{\\prime}=\\sum_{c\\neq*}\\frac{\\partial\\mathcal{L}_{a l l}}{\\partial z_{c}}/\\sum_{c\\neq*}\\frac{\\partial\\mathcal{L}_{c l a}}{\\partial z_{c}}=1-\\alpha\\frac{1-\\sum_{c\\neq*}z_{c}-q_{*}^{E}}{1-q_{*}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We observe that both $\\mathcal{F}$ and $\\mathcal{F^{\\prime}}$ are strictly monotonically increased regarding the value of $q_{*}^{E}$ . Given almost all values of the rescaling factors are negative as can be observed in Fig. 2 (e) (except in the few initial steps where $z_{\\ast}$ and $q_{*}^{E}$ are both small and $\\mathcal{L}_{g u i d}$ barely contributes) 3, with the same logits, a smaller $q_{*}^{E}$ , in which case the expert is less confident, will lead to larger $|\\mathcal{F}|$ and $|\\mathcal{F}^{\\prime}|$ . This phenomenon indicates that with the logit regularization term, the target model will focus more on the harder samples from the experts. Empirical findings supporting this analysis are in Sec. D.3 and D.4. Note that the segmentation task also utilizes one-hot labels and cross-entropy loss, making it applicable to the two analyses presented. ", "page_idx": 5}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "5.1 Generalization in Image classification ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Datasets and Implementation details. We conduct experiments on 5 datasets in DomainBed [27], namely PACS [43] (9,991 images, 7 classes, 4 domains), VLCS [23] (10,729 images, 5 classes, 4 domains), OfficeHome [76] (15,588 images, 65 classes, 4 domains), TerraInc [3] (24,788 images, 10 classes, 4 domains), and DomainNet [59] (586,575 images, 345 classes, 6 domains). We use the ImageNet [21] pretrained ResNet [30] as the backbone for both the experts and the target model. Following the designs in DomainBed, the hyper-parameter $\\scriptstyle{\\frac{\\alpha}{2}}$ in Eq. (3) is randomly selected in a range of [0.01,10]. To ensure fair comparisons, all methods are reevaluated using the default settings in DomainBed in the same device with each of them evaluated for $3\\times20$ times in different domains. Training-domain validation is adopted as the evaluation protocol. Other settings (batch size, learning rate, dropout, etc.) are dynamically selected for each trial according to DomainBed. ", "page_idx": 5}, {"type": "table", "img_path": "SYjxhKcXoN/tmp/2703b327f18861043b3100810825f852e38f7c906a7e6237298fcb14027fe28e.jpg", "table_caption": ["Table 2: Evaluations on the semantic segmentation task. Results with \u2020 are directly cited from [19], others are reevaluated in our device. Best results are colored as red. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Results with ResNet18. Results are listed in Tab. 1. We observe that ERM can obtain competitive results among the models compared, which leads more than half of the sophisticated designs, and only 6 methods lead ERM in most datasets (with scores $\\geq3$ ). We also note that current strategies cannot guarantee improvements for ERM in all situations, given that none of them with a score of 5. Differently, our method can consistently improve ERM in all evaluated datasets and lead others in average accuracy. Specifically, our approach obtains the leading results in 4 out of the 5 datasets, and it also improves ERM by a large margin (nearly 8pp) in the difficult TerraInc dataset. Compared to methods that explicitly explore hard samples or representations (i.e. VREx and RSC) and that use MoE (i.e. Meta-DMoE), the performances of LFME are superior to them in all cases. ", "page_idx": 6}, {"type": "text", "text": "Results with ResNet50. Because larger networks require more training resources, we only reevaluate some of leading methods (i.e. ERM, Fish, CORAL, and SD). in our device when experimenting with ResNet50. We note that our method surpasses the baseline ERM model in all datasets and leads it by 1.8 in average. Meanwhile, our method can still outperform the second best (i.e. SD) by 0.7 in average. These results indicate that our method can consistently improve the baseline model, and it can perform favorably against existing arts when implemented with a larger ResNet50 backbone. ", "page_idx": 6}, {"type": "text", "text": "We also combine LFME with SWAD [8]. Same with the original design, the hyper-parameter searching space in this setting is smaller than the original DomainBed. We use the reported numbers in [8] for comparisons. As shown, our method can also improve the baseline and obtain competitive results when combined with SWAD. These results further validate the effectiveness of our method. ", "page_idx": 6}, {"type": "text", "text": "5.2 Generalization in Semantic Segmentation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Datasets and Implementation details. The training and test processes of the compared algorithms involve 5 different datasets: 2 synthetics for training, where each dataset is considered a specific domain, and 3 real datasets for evaluation. Synthetic: GTAV [63] has 24,966 images from 19 categories; Synthia [64] contains 9,400 images of 16 categories. Real: Cityscapes [20] 3,450 finely annotated and 20,000 coarsely-annotated images collected from 50 cities; Bdd100K [83] contains 8K diverse urban driving scene images; Mapillary [55] includes 25K street view images. Following the design [40], we use the pretrained DeepLabv $^{3+}$ [18] and ResNet50 as the segmentation backbone for experiments. The maximum iteration step is set to 120K with a batch size of 4, and the evaluations are conducted after the last iteration step. The hyper-parameter $\\scriptstyle{\\frac{\\alpha}{2}}$ in Eq. (3) is fixed as 1 in this experiment for simplicity. We use the mean Intersection over Union (mIoU) and mean accuracy (mAcc) averaged over all classes as criteria to measure the segmentation performance. ", "page_idx": 6}, {"type": "text", "text": "Experimental results. Results are shown in Tab. 2. To better justify the effectiveness of our method, we reevaluate the baseline model, which aggregates and trains on all source data, and PinMem in our device. We also implement SD [60], which is a leading method in the image classification task. Results from other methods are directly cited from [19]. We observe SD does not perform as effectively as it does in the classification task, which decreases the baseline model in most situations. In comparison, our LFME can boost the baseline in all datasets, similar to that in the classification task. It also performs favorably against existing methods specially designed for the semantic segmentation task, obtaining best results in 2 out of the 3 evaluated datasets in both mIOU and mAcc. ", "page_idx": 6}, {"type": "image", "img_path": "SYjxhKcXoN/tmp/eee07d679077168127325f30880e0e38a592dc4c43b7d131bf0ddd0361b115e1.jpg", "img_caption": ["Figure 3: Qualitative comparisons. The compared methods make unsatisfactory predictions about several objects, such as clouds with varying shapes, car logo, or people and car in the shadow. Please zoom in for a better view. "], "img_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "SYjxhKcXoN/tmp/7283fe1673faf7a9d00fbf82652ca3816e1f67ad86a5ff0a3ac90ed741f86f69.jpg", "table_caption": ["Table 3: Evaluations of free lunch for DG (i.e. $\\operatorname{ERM+}$ ) and different LS ideas (i.e. LS [71], MbLS [50], and ACLS [58]). We use the suggested settings in their original papers for evaluating. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Visualized examples are provided in Fig. 3. We note that when it comes to unseen objects (i.e. clouds with different shapes and a new car logo), or objects with an unfamiliar background, such as the person and car hidden in the shadow, due to the large distribution shift between real and synthetic data, compared methods make unsatisfactory predictions. In comparison, LFME can provide reasonable predictions in these objects, demonstrating its effectiveness against current arts regarding generalization to new scenes. These results validate the effectiveness of LFME and its strong applicability in the generalizable semantic segmentation task. ", "page_idx": 7}, {"type": "text", "text": "6 Analysis ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Analyses in this section are conducted on the widely-used PACS dataset unless otherwise mentioned.   \nExperimental settings are same as that detailed in Sec. 5.1. Please see the appendix for more analysis. ", "page_idx": 7}, {"type": "text", "text": "6.1 A Free Lunch for DG ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "As stated in Sec. 4.1, when the basis of discrimination is not compromised ( $\\boldsymbol{g}_{*}$ corresponds to a larger value in the label and vice versa for $q_{c}$ for $\\forall c\\neq*]$ ), the increase of $q_{c}$ can encourage the model to learn more information that is shared with other classes, and use them to improve generalization. Based on this analysis, it seems that using the one-hot label to regularize the logit is also reasonable. To validate this hypothesis, we replace $q^{\\check{E}}$ in Eq. (3) with the ground truth and reformulate $\\mathcal{L}_{a l l}$ into, ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{L}_{a l l}=\\mathcal{H}(\\mathrm{softmax}(z),y)+\\frac{\\alpha}{2}\\|z-y\\|^{2},\\enspace\\mathrm{s.t.}\\ z=T(x).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Denoting as $\\mathrm{ERM+}$ , results listed in the second row in Tab. 3 suggest that this idea can improve ERM in all unseen domains. Because the hard sample information is absent in this strategy, we observe it performs inferior to LFME. However, this alternative does not require experts or any other special designs, even the setting of the hyper-parameter $\\alpha$ cannot be wrong: Eq. (9) will approximate the baseline either with $\\alpha=0$ or $\\alpha$ approximates $+\\infty$ . Thus, we argue this simple modification can serve as a free lunch to improve DG. More evaluations of this idea are in our appendix. ", "page_idx": 7}, {"type": "text", "text": "6.2 Compare with Label Smoothing ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "As detailed in Sec. 4.1, LFME will explicitly constrain the probability of the target model within a smaller range. This effect may resemble the LS technique that aims to implicitly calibrate the output probability. However, compared to LS, LFME has two advantages. First, LFME does not involve heuristic designs of hyper-parameters for determining its probability, which is essentially required in existing LS ideas, such as $\\epsilon$ in [71] and predefined margin in [50, 58], avoiding the possibility of deteriorating the performance when not choosing them properly; Second, LFME can explicitly mine hard samples from the experts, further ensuring improvements for DG. We evaluate 3 LS methods in both PACS and the difficult TerraInc datasets: (1) the pioneer LS method from [71]); (2) margin-based LS (MbLS) [50] that penalizes logits deviate from the maxima; (3) adaptive and conditional LS (ACLS) that can adaptively determine the degree of smoothing for different classes. Results are illustrated in 3rd-5th columns in Tab. 3. We observe that although these LS methods can improve the baseline, they are all inferior to LFME, validating the effectiveness of LFME against LS. ", "page_idx": 8}, {"type": "text", "text": "6.3 Compare with Other Knowledge Distillation Ideas ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To test the effectiveness of our logit regularization ( $\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\dot{l}.e.\\parallel\\!\\!\\!z-q^{E}\\|^{2})$ , we compare it with several other KD options, namely different combinations of the logits or probabilities from the target model and the experts, including $\\|z-z^{E}\\|^{2}$ (which is the basic design in Meta-DMoE [89]), $\\lVert\\breve{q}-z^{E}\\rVert^{2}$ , and $\\lVert q-q^{E}\\rVert^{2}$ . Moreover, we also compare it with a common practice in KD that uses the probability from the teacher (i.e. experts) as a label for the student (i.e. target model), which reformulates the Eq. (3) into: $\\begin{array}{r}{\\mathcal{L}_{a l l}=(1-\\mathit{\\hat{\\Pi}}_{2}^{\\cdot})\\mathcal{H}(q,y)+\\mathit{\\frac{\\alpha}{2}}\\mathcal{H}(q,q^{E})}\\end{array}$ , and denoted as $\\mathcal{H}(q,q^{E})$ in Tab. 4. Following the suggestion in [41], we gradually increase $\\scriptstyle{\\frac{\\alpha}{2}}$ over the iteration steps to achieve better performance. Results listed in 2nd-5th columns in Tab. 4 show that not all KD strategies can improve DG and the logit regularization choice achieves the best result in terms of average accuracy. This can be explained by the analysis in Sec. 4.1 that our logit regularization can help using more information. Please also the explanation in Sec. C for details. ", "page_idx": 8}, {"type": "text", "text": "6.4 Compare with Naive Aggregation Ideas ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To examine the effectiveness of LFME, we compare it with three different variants that employ all the experts during inference: (1) Avg_Expt, which averages the output from all experts for prediction, similar to DAELDG [90]; (2) Model soup (MS)_EXPT, which uniformly combines the weights of different experts. This is inspired by the MS idea in [78]; (3) Conf_Expt that utilizes the output from the most confident expert as the final prediction. Inspired by the finding in [77], the expert with the output of the smallest entropy value is regarded as the most confident one in a test sample; (4) Dyn_Expt, which is a learning-based approach that estimates ", "page_idx": 8}, {"type": "table", "img_path": "SYjxhKcXoN/tmp/dc32decf2f580e3005d44d417d4ccfad9a2cbb1f3795dda61ab7cc927a52bf28.jpg", "table_caption": ["Table 4: Out-of-domain evaluations of different KD ideas (using different $\\mathcal{L}_{g u i d}$ in Eq. (2)) and naive aggregations. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "the domain label of each sample and dynamically assigns corresponding weights to the experts via a weighting module. Results are listed in 6th-9th columns in Tab. 4, where both the designs of handcraft (i.e. Avg_Expt, MS_Expt, and Conf_Expt) and learning-based (i.e. Dyn_Expt) aggregation skills fail to improve the baseline model. This is because the hand-craft designs in Avg_Expt, MS_Expt, and Conf_Expt are rather unrealistic in practice as different models may contribute differently in the test phase, we thus cannot use a simple average or select the most confident expert for predicting. Meanwhile, the learning-based Dyn_Expt will inevitably introduce another generalization problem regarding the weighting module, complicating the setting. In comparison, LFME avoids the nontrivial aggregation design and can improve ERM in all source domains, further validating its effectiveness. ", "page_idx": 8}, {"type": "table", "img_path": "SYjxhKcXoN/tmp/9cf01bec0f0e1837bd4fab17216e9b97f098c912f0c9e9c4349a29213c98bcef.jpg", "table_caption": ["Table 5: In-domain evaluations of different models. "], "table_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "SYjxhKcXoN/tmp/c6e167446c734c0d484c9777f559073edc78569a455ecc6a326d202c26bfc557.jpg", "table_caption": ["Table 6: Sensitivity analysis of LFME regarding the involved weight parameter $\\scriptstyle{\\frac{\\alpha}{2}}$ in Eq. (3). LFME degrades to ERM when $\\begin{array}{r}{\\frac{\\alpha}{2}=0}\\end{array}$ . "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "6.5 Does the Target Model Become an Expert on All Source Domains? ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The training process of LFME aggregates all professional knowledge from the experts into one target model, aiming to make the target model an expert on all source domains. To examine if the goal has been achieved, we conduct in-domain validations for the target model and compare the performances with that of the experts and the ERM model in the PACS and TerraInc datasets. Note in these experiments, ERM and LFME are trained using the same leave-one-out strategy, and the performances are averaged over the trials on three different target domains. Results are listed in Tab. 5. We observe that ERM performs inferior to the experts in the in-domain setting. The results are not surprising. As stated earlier, when encountering data similar to the source domain, it would be better to rely mostly on the corresponding expert than the model also contaminated with other patterns. In comparison, our method obtains the best results in all source domains because it implicitly focuses more on the hard samples from the experts, which is shown to be an effective way to improve the performance in many arts [33, 84]. These results validate that the proposed strategy can extract professional knowledge from different experts, and enable the target model to become an expert in all source domains. ", "page_idx": 9}, {"type": "text", "text": "6.6 Selection of the Hyper-Parameter ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Compared with the baseline ERM, our method involves only one additional hyper-parameter (i.e. $\\scriptstyle{\\frac{\\alpha}{2}}$ in Eq. 3) which is randomly selected in the range of [0.01,10] in DomainBed. To evaluate the sensitiveness of LFME regarding this hyper-parameter, we conduct experiments in the PACS and TerraInc datasets by tuning it in a larger range. As seen in Tab. 6, LFME can obtain relatively better performance with either $\\textstyle{\\frac{\\alpha}{2}}=0.01$ or 10, and it performs on par with ERM even when $\\begin{array}{r}{\\frac{\\alpha}{2}=1000}\\end{array}$ . These results indicate that our method is insensitive w.r.t the hyper-parameter. This is mainly because a very large $\\scriptstyle{\\frac{\\alpha}{2}}$ will enforce the model to only learn from domain experts, given that $q_{*}^{E}$ is mostly aligned with the label $y$ (as seen in Fig. 2 (a)), the model will perform similarly to another form of the baseline [37] in this case. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper introduced a simple yet effective method for DG where the professional guidances of experts specialized in specific domains are leveraged. We achieve the guidance by a logit regularization term that enforces similarity between logits of the target model and probability of the corresponding expert. After training, the target model is expected to be an expert on all source domains, thus thriving in arbitrary test domains. Through deeper analysis, we reveal that the proposed strategy implicitly enables the target model to use more information for prediction and mine hard samples from the experts during training. By conducting experiments in related tasks, we show that our method is consistently beneficial to the baseline and performs favorably against existing arts. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement. This work was supported by the Centre for Augmented Reasoning, an initiative by the Department of Education, Australian Government. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Martin Arjovsky, L\u00e9on Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. arXiv preprint arXiv:1907.02893, 2019.   \n[2] Yogesh Balaji, Swami Sankaranarayanan, and Rama Chellappa. Metareg: Towards domain generalization using meta-regularization. In NeurIPS, 2018.   \n[3] Sara Beery, Grant Van Horn, and Pietro Perona. Recognition in terra incognita. In ECCV, 2018.   \n[4] Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. A theory of learning from different domains. Machine learning, 79(1):151\u2013175, 2010.   \n[5] Gilles Blanchard, Aniket Anand Deshmukh, Urun Dogan, Gyemin Lee, and Clayton Scott. Domain generalization by marginal transfer learning. arXiv preprint arXiv:1711.07910, 2017.   \n[6] Gilles Blanchard, Gyemin Lee, and Clayton Scott. Generalizing from several related classification tasks to a new unlabeled sample. In NeurIPS, 2011.   \n[7] Fabio M Carlucci, Antonio D\u2019Innocente, Silvia Bucci, Barbara Caputo, and Tatiana Tommasi. Domain generalization by solving jigsaw puzzles. In CVPR, 2019.   \n[8] Junbum Cha, Sanghyuk Chun, Kyungjae Lee, Han-Cheol Cho, Seunghyun Park, Yunsung Lee, and Sungrae Park. Swad: Domain generalization by seeking flat minima. NeurIPS, 2021.   \n[9] Junbum Cha, Kyungjae Lee, Sungrae Park, and Sanghyuk Chun. Domain generalization by mutualinformation regularization with pre-trained models. In ECCV, 2022.   \n[10] Prithvijit Chattopadhyay, Yogesh Balaji, and Judy Hoffman. Learning to balance specificity and invariance for in and out of domain generalization. In ECCV, 2020.   \n[11] Chaoqi Chen, Jiongcheng Li, Xiaoguang Han, Xiaoqing Liu, and Yizhou Yu. Compound domain generalization via meta-knowledge encoding. In CVPR, 2022.   \n[12] Chaoqi Chen, Luyao Tang, Yue Huang, Xiaoguang Han, and Yizhou Yu. Coda: generalizing to open and unseen domains with compaction and disambiguation. NeurIPS, 2023.   \n[13] Chaoqi Chen, Luyao Tang, Feng Liu, Gangming Zhao, Yue Huang, and Yizhou Yu. Mix and reason: Reasoning over semantic topology with data mixing for domain generalization. In NeurIPS, 2022.   \n[14] Liang Chen, Yong Zhang, Yibing Song, Anton van den Hengel, and Lingqiao Liu. Domain generalization via rationale invariance. In ICCV, 2023.   \n[15] Liang Chen, Yong Zhang, Yibing Song, Ying Shan, and Lingqiao Liu. Improved test-time adaptation for domain generalization. In CVPR, 2023.   \n[16] Liang Chen, Yong Zhang, Yibing Song, Jue Wang, and Lingqiao Liu. Ost: Improving generalization of deepfake detection via one-shot test-time training. NeurIPS, 2022.   \n[17] Liang Chen, Yong Zhang, Yibing Song, Zhen Zhang, and Lingqiao Liu. A causal inspired early-branching structure for domain generalization. IJCV, pages 1\u201321, 2024.   \n[18] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. In ECCV, 2018.   \n[19] Sungha Choi, Sanghun Jung, Huiwon Yun, Joanne T Kim, Seungryong Kim, and Jaegul Choo. Robustnet: Improving domain generalization in urban-scene segmentation via instance selective whitening. In CVPR, 2021.   \n[20] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In CVPR, 2016.   \n[21] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, 2009.   \n[22] Qi Dou, Daniel Coelho de Castro, Konstantinos Kamnitsas, and Ben Glocker. Domain generalization via model-agnostic learning of semantic features. In NeurIPS, 2019.   \n[23] Chen Fang, Ye Xu, and Daniel N Rockmore. Unbiased metric learning: On the utilization of multiple datasets and web images for softening bias. In ICCV, 2013.   \n[24] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Fran\u00e7ois Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. JMLR, 17(1):2096\u20132030, 2016.   \n[25] Robert Geirhos, J\u00f6rn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, and Felix A Wichmann. Shortcut learning in deep neural networks. Nature Machine Intelligence, 2(11):665\u2013673, 2020.   \n[26] Muhammad Ghifary, David Balduzzi, W Bastiaan Kleijn, and Mengjie Zhang. Scatter component analysis: A unified framework for domain adaptation and domain generalization. IEEE TPAMI, 39(7):1414\u20131430, 2016.   \n[27] Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. In ICLR, 2021.   \n[28] Jiang Guo, Darsh J Shah, and Regina Barzilay. Multi-source domain adaptation with mixture of experts. arXiv preprint arXiv:1809.02256, 2018.   \n[29] Sivan Harary, Eli Schwartz, Assaf Arbelle, Peter Staar, Shady Abu-Hussein, Elad Amrani, Roei Herzig, Amit Alfassy, Raja Giryes, Hilde Kuehne, et al. Unsupervised domain generalization by learning a bridge across domains. In CVPR, 2022.   \n[30] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.   \n[31] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2(7), 2015.   \n[32] Shoubo Hu, Kun Zhang, Zhitang Chen, and Laiwan Chan. Domain generalization via multidomain discriminant analysis. In UAI, 2020.   \n[33] Chen Huang, Chen Change Loy, and Xiaoou Tang. Local similarity-aware deep feature embedding. In NeurIPS, 2016.   \n[34] Jiaxing Huang, Dayan Guan, Aoran Xiao, and Shijian Lu. Fsdr: Frequency space domain randomization for domain generalization. In CVPR, 2021.   \n[35] Zeyi Huang, Haohan Wang, Dong Huang, Yong Jae Lee, and Eric P Xing. The two dimensions of worst-case training and their integrated effect for out-of-domain generalization. In CVPR, 2022.   \n[36] Zeyi Huang, Haohan Wang, Eric P Xing, and Dong Huang. Self-challenging improves cross-domain generalization. In ECCV, 2020.   \n[37] Like Hui and Mikhail Belkin. Evaluation of neural architectures trained with square loss vs cross-entropy in classification tasks. In ICLR, 2021.   \n[38] Edwin T Jaynes. Information theory and statistical mechanics. Physical review, 106(4):620, 1957.   \n[39] Daehee Kim, Youngjun Yoo, Seunghyun Park, Jinkyu Kim, and Jaekoo Lee. Selfreg: Self-supervised contrastive regularization for domain generalization. In ICCV, 2021.   \n[40] Jin Kim, Jiyoung Lee, Jungin Park, Dongbo Min, and Kwanghoon Sohn. Pin the memory: Learning to generalize semantic segmentation. In CVPR, 2022.   \n[41] Kyungyul Kim, ByeongMoon Ji, Doyoung Yoon, and Sangheum Hwang. Self-knowledge distillation with progressive refinement of targets. In ICCV, 2021.   \n[42] David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai Zhang, Remi Le Priol, and Aaron Courville. Out-of-distribution generalization via risk extrapolation (rex). In ICML, 2021.   \n[43] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Deeper, broader and artier domain generalization. In ICCV, 2017.   \n[44] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Learning to generalize: Meta-learning for domain generalization. In AAAI, 2018.   \n[45] Da Li, Jianshu Zhang, Yongxin Yang, Cong Liu, Yi-Zhe Song, and Timothy M Hospedales. Episodic training for domain generalization. In ICCV, 2019.   \n[46] Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C Kot. Domain generalization with adversarial feature learning. In CVPR, 2018.   \n[47] Pan Li, Da Li, Wei Li, Shaogang Gong, Yanwei Fu, and Timothy M Hospedales. A simple feature augmentation for domain generalization. In ICCV, 2021.   \n[48] Xiaotong Li, Yongxing Dai, Yixiao Ge, Jun Liu, Ying Shan, and Ling-Yu Duan. Uncertainty modeling for out-of-distribution generalization. In ICLR, 2022.   \n[49] Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang, and Dacheng Tao. Deep domain generalization via conditional invariant adversarial networks. In ECCV, 2018.   \n[50] Bingyuan Liu, Ismail Ben Ayed, Adrian Galdran, and Jose Dolz. The devil is in the margin: Margin-based label smoothing for network calibration. In CVPR, 2022.   \n[51] David JC MacKay. Information theory, inference and learning algorithms. Cambridge university press, 2003.   \n[52] Saeid Motiian, Marco Piccirilli, Donald A Adjeroh, and Gianfranco Doretto. Unified deep supervised domain adaptation and generalization. In ICCV, 2017.   \n[53] Krikamol Muandet, David Balduzzi, and Bernhard Sch\u00f6lkopf. Domain generalization via invariant feature representation. In ICML, 2013.   \n[54] Hyeonseob Nam, HyunJae Lee, Jongchan Park, Wonjun Yoon, and Donggeun Yoo. Reducing domain gap by reducing style bias. In CVPR, 2021.   \n[55] Gerhard Neuhold, Tobias Ollmann, Samuel Rota Bulo, and Peter Kontschieder. The mapillary vistas dataset for semantic understanding of street scenes. In CVPR, 2017.   \n[56] Xingang Pan, Ping Luo, Jianping Shi, and Xiaoou Tang. Two at once: Enhancing learning and generalization capacities via ibn-net. In ECCV, 2018.   \n[57] Prashant Pandey, Mrigank Raman, Sumanth Varambally, and Prathosh Ap. Generalization on unseen domains via inference-time label-preserving target projections. In CVPR, 2021.   \n[58] Hyekang Park, Jongyoun Noh, Youngmin Oh, Donghyeon Baek, and Bumsub Ham. Acls: Adaptive and conditional label smoothing for network calibration. In ICCV, 2023.   \n[59] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain adaptation. In ICCV, 2019.   \n[60] Mohammad Pezeshki, Oumar Kaba, Yoshua Bengio, Aaron C Courville, Doina Precup, and Guillaume Lajoie. Gradient starvation: A learning proclivity in neural networks. In NeurIPS, 2021.   \n[61] Vihari Piratla, Praneeth Netrapalli, and Sunita Sarawagi. Efficient domain generalization via commonspecific low-rank decomposition. In ICML, 2020.   \n[62] Alexandre Rame, Corentin Dancette, and Matthieu Cord. Fishr: Invariant gradient variances for out-ofdistribution generalization. In ICML, 2022.   \n[63] Stephan R Richter, Vibhav Vineet, Stefan Roth, and Vladlen Koltun. Playing for data: Ground truth from computer games. In ECCV, 2016.   \n[64] German Ros, Laura Sellart, Joanna Materzynska, David Vazquez, and Antonio M Lopez. The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes. In CVPR, 2016.   \n[65] Yangjun Ruan, Yann Dubois, and Chris J Maddison. Optimal representations for covariate shift. In ICLR, 2022.   \n[66] Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. In ICLR, 2020.   \n[67] Shiv Shankar, Vihari Piratla, Soumen Chakrabarti, Siddhartha Chaudhuri, Preethi Jyothi, and Sunita Sarawagi. Generalizing across domains via cross-gradient training. 2018.   \n[68] Claude Elwood Shannon. A mathematical theory of communication. The Bell system technical journal, 27(3):379\u2013423, 1948.   \n[69] Yuge Shi, Jeffrey Seely, Philip HS Torr, N Siddharth, Awni Hannun, Nicolas Usunier, and Gabriel Synnaeve. Gradient matching for domain generalization. In ICLR, 2021.   \n[70] Baochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation. In ECCV, 2016.   \n[71] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In CVPR, 2016.   \n[72] Jiaxi Tang, Rakesh Shivanna, Zhe Zhao, Dong Lin, Anima Singh, Ed H Chi, and Sagar Jain. Understanding and improving knowledge distillation. arXiv preprint arXiv:2002.03532, 2020.   \n[73] Zhiqiang Tang, Yunhe Gao, Yi Zhu, Zhi Zhang, Mu Li, and Dimitris N Metaxas. Crossnorm and selfnorm for generalization under distribution shifts. In ICCV, pages 52\u201361, 2021.   \n[74] Vladimir Vapnik. Principles of risk minimization for learning theory. In NeurIPS, 1991.   \n[75] Vladimir Vapnik. The nature of statistical learning theory. Springer science & business media, 1999.   \n[76] Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep hashing network for unsupervised domain adaptation. In CVPR, 2017.   \n[77] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In ICLR, 2021.   \n[78] Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, et al. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In ICML, 2022.   \n[79] Qinwei Xu, Ruipeng Zhang, Ya Zhang, Yanfeng Wang, and Qi Tian. A fourier-based framework for domain generalization. In CVPR, 2021.   \n[80] Shen Yan, Huan Song, Nanxiang Li, Lincan Zou, and Liu Ren. Improve unsupervised domain adaptation with mixup training. arXiv preprint arXiv:2001.00677, 2020.   \n[81] Fu-En Yang, Yuan-Chia Cheng, Zu-Yun Shiau, and Yu-Chiang Frank Wang. Adversarial teacher-student representation learning for domain generalization. In NeurIPS, 2021.   \n[82] Huaxiu Yao, Xinyu Yang, Xinyi Pan, Shengchao Liu, Pang Wei Koh, and Chelsea Finn. Leveraging domain relations for domain generalization. arXiv preprint arXiv:2302.02609, 2023.   \n[83] Fisher Yu, Haofeng Chen, Xin Wang, Wenqi Xian, Yingying Chen, Fangchen Liu, Vashisht Madhavan, and Trevor Darrell. Bdd100k: A diverse driving dataset for heterogeneous multitask learning. In CVPR, 2020.   \n[84] Yuhui Yuan, Kuiyuan Yang, and Chao Zhang. Hard-aware deeply cascaded embedding. In ICCV, 2017.   \n[85] Xiangyu Yue, Yang Zhang, Sicheng Zhao, Alberto Sangiovanni-Vincentelli, Kurt Keutzer, and Boqing Gong. Domain randomization and pyramid consistency: Simulation-to-real generalization without accessing target domain data. In ICCV, 2019.   \n[86] Seniha Esen Yuksel, Joseph N Wilson, and Paul D Gader. Twenty years of mixture of experts. IEEE TNNLS, 23(8):1177\u20131193, 2012.   \n[87] Marvin Zhang, Henrik Marklund, Nikita Dhawan, Abhishek Gupta, Sergey Levine, and Chelsea Finn. Adaptive risk minimization: A meta-learning approach for tackling group distribution shift. arXiv preprint arXiv:2007.02931, 2020.   \n[88] Guanhua Zheng, Jitao Sang, and Changsheng Xu. Understanding deep learning generalization by maximum entropy. In ICLR, 2018.   \n[89] Tao Zhong, Zhixiang Chi, Li Gu, Yang Wang, Yuanhao Yu, and Jin Tang. Meta-dmoe: Adapting to domain shift by meta-distillation from mixture-of-experts. NeurIPS, 2022.   \n[90] Kaiyang Zhou, Yongxin Yang, Yu Qiao, and Tao Xiang. Domain adaptive ensemble learning. IEEE TIP, 30:8008\u20138018, 2021.   \n[91] Kaiyang Zhou, Yongxin Yang, Yu Qiao, and Tao Xiang. Domain generalization with mixstyle. In ICLR, 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we provide ", "page_idx": 14}, {"type": "text", "text": "1. Limitations and future works in Section A   \n2. Pseudocode of LFME in Section B.   \n3. Explanations of why existing KD ideas perform inferior to our LFME in Section C.   \n4. More analysis regarding LFME in Section D;   \n5. Compare with MoE-based methods and other related methods in Section E.   \n6. Existing Ideas with Same Training Resources in Section F.   \n7. Detailed settings of the learning-based aggregation method Dyn_Expt in Sec. 6.4 from the   \nmanuscript in Section G; ", "page_idx": 14}, {"type": "text", "text": "8. Detailed results of LFME in the unseen domains of the different datasets from DomainBed, and detailed results of LFME in the different categories from the semantic segmentation task in Section H. ", "page_idx": 14}, {"type": "text", "text": "A Limitations and Future Works ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Although our method shows competitive results for generalization, there are certain occasions when it will encounter setbacks. First, LFME is not applicable for the setting where the domain labels are unavailable in the training data, such as those collected from the internet. Since the training of the experts requires data grouped by domain labels. Second, LFME cannot handle the situation when only one source domain is provided, preventing it from performing in a more difficult single-source generalization task. How to apply LFME to a more general setting will be our primary task in future works. Besides, as the designs and theoretical supports are built mainly for the classification task, finding a proper solution to extend them to the regression tasks is also a promising direction in potential future works. ", "page_idx": 14}, {"type": "text", "text": "B Pseudo Code of LFME ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "This section provides the Pytorch-style pseudocode of our method. As detailed in Alg. 1, the implementation of our method is embarrassingly simple, introducing only one hyper-parameter upon the baseline ERM. ", "page_idx": 14}, {"type": "text", "text": "C Compare with Current KD Based on the Analysis in Sec. 4.1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "As observed in Sec. 6.3, some existing KD ideas, perform inferior to our LFME. Based on our analysis, we find this is because existing ideas have difficulties achieving the beneficial effect introduced in Sec. 4.1. As detailed, our logit regularization ensures using more information for the target model by implicitly regularizing the probability in a much smaller range. This is hard to achieve by $\\mathcal{H}(q,q^{E})$ and $\\|q-q^{E}\\|^{2}$ , where their probability $q$ will be in a similar range as that in ERM because the soft label $q^{E}$ is still within the range of [0,1], enforcing similarity between $q$ and $q^{E}$ will not significantly change its distribution. Moreover, this effect is also hard to achieve by $\\bar{\\|}z-z^{E}\\bar{\\|}$ and $\\lVert q-z^{\\breve{E}}\\rVert^{2}$ , which do not provide a specific range regularization effect for the probability. ", "page_idx": 14}, {"type": "text", "text": "D Further Analysis ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "This section provides more evidence to support the two analyses in Sec. 4 in the manuscript. The experiments are conducted on the widely-used PACS dataset and the difficult TerraInc dataset unless mentioned otherwise. ", "page_idx": 14}, {"type": "text", "text": "D.1 More Justification for Enabling Target Model to Use More Information ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "For the baseline model, we infer that the cross-entropy loss alone will enforce the model to only learn discriminative features that are specific to the label category. Because there are no relevant features to interpret the non-label classes in ERM, the confidence for the target class will approach the maximum during training, and vice versa for other non-label classes. Differently, when the output probability has a smoother distribution, the model will also learn information that is shared with other classes to improve the corresponding non-label probabilities. Introducing more information during training will inevitably cause the phenomenon of low confidence for the label category, as probabilities for the non-label classes will increase (their summation increases from nearly 0 in Fig. 2 (a) to 0.35 in Fig. 2 (b)). Note that low confidence does not mean low accuracy. Because the label category corresponds to the largest logit value in both terms, and both the discriminative and newly introduced information can be used to characterize the label information, exploring more information can also boost classification. This finding is further validated by the in-domain results in Tab. 5, where LFME is shown to be able to also improve the in-domain test results compared to ERM, indicating that low confidence in our case actually leads to higher accuracy. ", "page_idx": 14}, {"type": "image", "img_path": "SYjxhKcXoN/tmp/de451266591e39c9aa0694fb31a34416218e6fd9a8fe3bf7afb8af04dc728bc9.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Visual examples to support the above analysis are shown in Fig. 4. We observe that compared to the baseline ERM, our method can utilize more regions for prediction. Such as that in the first \u201chorse\" examples, our model utilizes both the head, tail, and body for prediction, while the ERM seems to focus only on the head region. These visual samples align with our analysis that compared to the baseline ERM, LFME tends to use more information for prediction. ", "page_idx": 15}, {"type": "text", "text": "D.2 Other Alternatives to Enlarge $q_{c}$ for $\\forall c\\neq*$ ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "This section provides more evidence to support the first analysis in Sec. 4 by conducting experiments on two variants that also enlarge $q_{c}$ for $\\forall c\\neq*$ . (1) we test a model that directly enlarges $q_{c}$ for $\\forall c\\neq*$ . Specifically, we use the probability from LFME as the label to guide the probability output of a new target model, i.e. $\\mathcal{L}_{g u i d}\\dot{=}\\|q-q^{L\\tilde{F}M E}\\|^{2}$ with $q$ and $q^{L F M E}$ from the new target and LFME models, respectively. Ideally, this modification (i.e. LFME_Guid) can also improve the baseline model because it also encourages the model to learn more information shared with other classes ", "page_idx": 15}, {"type": "image", "img_path": "SYjxhKcXoN/tmp/0dd9e5deb4f067f1c6ee55a53360a4a810b6305870a9461dc3d7de90252f2340.jpg", "img_caption": ["Figure 4: Grad-CAM visualizations of samples from the unseen \u201ccartoon\" domain of the PACS benchmark, which is the most challenging domain for ERM and our method according to Tab. 3. Compared to the baseline ERM, highlight regions from our method contain more information related to the label category. These visualizations can further validate our analysis in Sec. 4 that with the proposed strategy, the target model can explore more information for prediction. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Table 7: Further evidences to support our deep analyses in Sec. 4. Evaluations are conducted on the widely-used PACS and difficult TerraInc datasets using settings from the DomainBed benchmark. Here $\\mathrm{ERM+}$ is the free lunch introduced in Sec. 6.1 that replaces $q^{E}$ in Eq. (3) with the one-hot label; LFME_Guid denotes imposing the guidance from LFME to the ERM model by including $\\mathcal{L}_{g u i d}=\\|q-q^{L F M E}\\|^{2}$ where $q$ and $\\overset{\\triangledown}{\\boldsymbol{q}^{L F}}\\boldsymbol{\\breve{M}}\\boldsymbol{E}$ are probabilities from the ERM model and LFME; Self_Guid is the model that replaces probabilities from the experts in Eq. (3) with that from itself, i.e. $\\mathcal{L}_{g u i d}=\\|z-\\mathrm{softmax}(z)\\|^{2}$ where softmax $(z)$ is followed with a detach operation; $\\mathrm{ERM+}$ w/ expt denotes explicitly focus more on the hard samples from the experts based on $\\mathrm{ERM+}$ ; $\\mathrm{ERM+}$ w/ self denotes explicitly focus more on the hard samples from the model itself on the basis of $\\mathrm{ERM+}$ . All methods are with the same ResNet18 backbone and are examined for 60 trials in each unseen domain. ", "page_idx": 16}, {"type": "table", "img_path": "SYjxhKcXoN/tmp/b7689f747f8243d4cea546dc7274cd9505b3ba93500fdbe027e78fe50bcde9ab.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "without compromising discrimination; (2) we use the output probability from the target model itself to replace that from the experts in Eq. (3), which reformulates $\\mathcal{L}_{g u i d}$ into $\\|z\\!-\\!\\mathrm{softmax}(z)\\|^{2}$ (softmax(\ud835\udc67) is followed with a detach operation in updating). According to our analyses in Sec. 4, this alternative (i.e. Self_Guid) should also boost generalization because it enforces the target model to learn more information and focus more on the hard sample from itself; ", "page_idx": 16}, {"type": "text", "text": "Results are illustrated in Tab. 7. We observe that both the two variants LFME_Guid and Self_Guid are beneficial to the baseline models, leading ERM in all unseen domains. These results further validate our first analysis in Sec. 4, and we can conclude that when the basic discrimination is not compromised, either directly or indirectly enlarging $q_{c}$ for $\\forall c\\neq*$ can help the target model learn more information, and improve generalization accordingly. ", "page_idx": 16}, {"type": "text", "text": "D.3 Hard Samples from the Experts ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "As analyzed in Sec. 4, LFME not only encourages the target model to learn more information but also implicitly helps it to focus more on the hard samples from the experts. To examine if the classifications of hard samples from the experts are indeed improved, we plot the classification ratio $\\begin{array}{r}{\\mathcal{R}=\\frac{\\overline{{p}}_{*}}{\\operatorname*{max}(\\overline{{p}})}}\\end{array}$ , where $\\overline{{p}}$ denotes the normalized probability: $\\begin{array}{r}{\\overline{{p}}=\\frac{p-\\operatorname*{min}(p)}{\\operatorname*{max}(p)-\\operatorname*{min}(p)}}\\end{array}$ , from the target model on these hard samples. We use the ratio because it can quantify the correct predictions regarding the hard samples more precisely than the accuracy: the closer $\\mathcal{R}$ approaches 1, the better the corresponding model performs on the sample. In this setting, $1/3$ samples with larger losses in a training batch are considered hard samples. We compare $\\mathcal{R}$ from LFME with that from ERM to highlight the difference. As can be observed in Fig. 5 (a), most $\\mathcal{R}$ from LFME is larger than that from ERM over the iterations, denoting LFME is better at handling these hard samples than ERM. As a comparison, $\\mathcal{R}$ from these two models almost overlap in the easy samples as shown in Fig. 5 (b). These results validate our analysis that LFME implicitly helps the target model to focus more on the hard samples compared with ERM. ", "page_idx": 17}, {"type": "text", "text": "Meanwhile, to validate if the hard samples from experts can indeed help generalization, we conduct experiments by explicitly putting more weights on the hard samples from the experts on the basis of the free launch $\\mathrm{ERM+}$ (i.e. ERM+w/ expt). Specifically, the target objective based on Eq. (9) is adjusted into: $w\\mathcal{L}_{a l l}$ , where $w$ is the weight for the training samples and is determined based on the loss of the experts. Basically, the larger the corresponding loss from the experts, the larger the value of $w$ should be, and we use the strategy from [35] to implement this hard sample mining process. As can be observed in Tab. 7, mining hard samples from the experts can improve the base model of $\\mathrm{ERM+}$ , especially in the difficult TerraInc dataset. These results validate that mining hard samples from experts is beneficial for generalization. ", "page_idx": 17}, {"type": "text", "text": "D.4 Comparisons Between Hard Samples from Different Models: Why Experts are Required ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We note in Tab. 7, the model Self_Guid performs inferior to LFME in both the two datasets, and the improvements are also marginal compared with $\\mathrm{ERM+}$ on account of variances. These results indicate the hard samples from the model itself may be less effective for generalization compared with that from the experts. To validate this hypothesis, we conduct experiments by explicitly focusing more on the hard samples from the model itself on the basis of $\\mathrm{ERM+}$ (i.e. ERM $^+$ w/ self). The implementation is the same as that in $\\mathrm{ERM+}$ w/ expt. ", "page_idx": 17}, {"type": "text", "text": "Results listed in the 6th row in Tab. 7 validate this hypothesis, where the hard samples from the model itself can hardly improve generalization compared with that from the experts. We presume the main reason is that besides the dominant specific domain information, hard samples from the experts also contain some out-of-the-domain information that makes it challenging for the corresponding expert. Examples are shown in Fig. 6. We observe that compared with hard samples from the model itself (i.e. Fig. 6 (b)), hard samples from the experts (i.e. Fig. 6 (a)) contain more ambiguous data located in the mixed region or the boundary of two domains than, indicating the experts may be more effective at revealing samples that with characteristics from different domains. By implicitly emphasizing these out-of-the-specific-domain samples, can the target model learn domain-agnostic features, thus improving generalization accordingly. Because the naive training strategies are fed with data from different domains at the same time, the out-of-the-specific-domain information is difficult to discover in their framework, explaining why hard samples from other strategies may not contribute. This is also the reason why the experts must be involved in the overall framework. ", "page_idx": 17}, {"type": "text", "text": "E Compare with MoE-Based Ideas and Other Arts ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Since our idea involves training multiple experts, one may connect it to the idea of mixture of experts (MoE) [86]. However, it is noteworthy that our LFME is not a simple implementation of existing techniques. The introduced logit regularization term is new and can be properly explained for its effectiveness. Some similar ideas that use MoE have been explored in other works for improving DG. For example, in DAELDG [90], following a shared feature extractor, each domain corresponds to a specific classifier (i.e. expert), whose output is enforced to be similar to the average outputs from the non-expert classifiers. Different from our work, it uses the average outputs from different experts as the final result. In Meta-DMoE [89], a meta-learning-based framework is employed to enforce the feature of the target model to be similar to the aggregated features from the experts, which is different from our logit regularization idea. In Sec. D.1 and 6.3, we implement the basic designs of these two ideas with the framework of LFME and show their ineffectiveness. Besides the comparison with Meta-DMoE in Tab. 1, in this section, we further compare LFME with them using their original settings in the PACS and OfficeHome datasets with the benchmark provided in [90]. We also compare with several other recent DG methods. Results in Tab. 8 show that LFME performs favorably against DAELDG, and outperforms Meta-DMoE, further validating the advantages of the logit regularization term. We also note LFME performs better than other arts with a different benchmark, demonstrating the effectiveness of the proposed approach. ", "page_idx": 17}, {"type": "image", "img_path": "SYjxhKcXoN/tmp/85692b77c51398760f7383b17051877231ab7ce1e20967763c140cc1ea2f933c.jpg", "img_caption": ["Figure 5: Classification ratio comparisons of ERM and LFME in the hard and easy samples from the difficult TerraInc dataset. The closer the ratio $\\mathcal{R}$ approaches 1, the better the corresponding prediction. Here the hard samples are specified by the experts: the $1/3$ samples in a training batch with larger losses from the experts, and the easy samples are the leading $1/3$ samples with smaller losses. The two models perform evenly well on the easy samples, while LFME obtains better results than ERM in the hard samples. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "SYjxhKcXoN/tmp/29c2c4b4118d62d12a926f4a6391d4ba3341015745005f061ae92d1ab94256d5.jpg", "img_caption": ["Figure 6: T-SNE visualizations of hard samples (larger dots) from different models. Here the data from different domains are clustered by their styles [91] (i.e. feature statistics from the first layer in the same ResNet18 model), and 10 percents of samples with larger loss are considered hard samples in a domain. Hard samples from the experts contain more ambiguous data located in the mixed region or the boundary of two different domains. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "F Existing Ideas with Same Training Resources ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Since LFME requires learning different experts during training, inevitably using more parameters than existing methods, one may wonder if the effectiveness of LFME derives from using these extra parameters from the experts. To examine this idea, we conduct experiments by implementing some leading arts (i.e. CORAL, SD) and the baseline ERM with $M+1$ times of model size (where $M$ is the domain number). Specifically, the feature extractor for the larger method will contain $M+1$ branches, each with the same pretrained ResNet backbone. We concate the final outputs from the different branches and use it as input for a classifier to obtain the final result. Note that in this setting, a sample will go through $M+1$ times of forward passes for both training and inference, which is more than that of LFME design. Results are shown in Tab. 9. We note that compared to the results from the original models, when using the same pretrained knowledge, naively expanding model size cannot improve the performance. The reason may be that a well-pretrained small backbone can already saturate on limited training data (as shown in Table 5, ERM can achieve more than 0.96 acc in the source domains), thus it is unnecessary to use more parameters in these datasets. ", "page_idx": 18}, {"type": "table", "img_path": "SYjxhKcXoN/tmp/7c555a8b021471b5fe1e23e2114a54b468b358c215e16daecfa1b44c055edaff.jpg", "table_caption": ["Table 8: Out-of-domain evaluations of other related methods with the benchmark provided in [90]. "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "SYjxhKcXoN/tmp/0ef0b31215f6ee726d12502d612070fa9fd1c7fc39950ad96d8ef80d2adca960.jpg", "table_caption": ["Table 9: Existing methods with same resources during training. We expand existing ideas by concatenating $M+1$ branches to ensure they use the same parameters as LFME during training. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "G Detailed Settings of Dyn_Expt ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Besides the different experts $E_{i}$ where $i$ denotes the domain label, Dyn_Expt also uses an extra weighting network $W$ to estimate the labels of the training data. During the training stage, the experts are trained the same as that in LFME, and the weighting network is trained using the corresponding domain labels by minimizing $\\mathcal{H}(W(x^{i}),i)$ . During the test phase, Dyn_Expt dynamically combines the outputs from the experts with the corresponding domain probability: for $\\forall x\\in\\mathcal{D}_{M+1}$ , the final result is $\\textstyle\\sum_{i}^{M}W(x)_{i}E_{i}(x)$ . We use the same backbone for both the experts and the weighting module in this experiment. ", "page_idx": 19}, {"type": "text", "text": "H Detailed Results ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "This section presents the detailed results on the semantic segmentation task in Tab. 10, and detailed results in the unseen domains from the different unseen domains of DomainBed benchmark in Tab. 11, 12, 13 14, and 15. ", "page_idx": 19}, {"type": "table", "img_path": "SYjxhKcXoN/tmp/8fcf8e90e81c8f3742d93dde855e4f647832f96898225214f0c8cf64ac8185b4.jpg", "table_caption": ["Table 10: Evaluations on the semantic segmentation task with the metrics of Mean IoU $(\\%)$ and per-class IoU $(\\%)$ . Source data is from the synthetic GTAV [63] and Synthia [64] datasets, and the target data is from the real-world Cityscapes [20], BDD100K [83], and the Mapillary [55] datasets. All models are with the same backbone: DeepLab $N3+$ with ResNet50, and results with \u2020 are from [19]. "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "SYjxhKcXoN/tmp/f19f5de8989f7ee8993d2e5424c6def5a4beb133eaac6ec1f99990de20df3fca.jpg", "table_caption": ["Table 11: Average accuracies on the PACS [43] datasets using the default hyper-parameter settings in DomainBed [27]. TT denotes the average training time (minutes) for one trial in a target domain. "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "SYjxhKcXoN/tmp/cf2733d8c9ef5fd989e0afc0b8fde36bbc7b18b48b858d1f46a047a334d17525.jpg", "table_caption": ["Table 12: Average accuracies on the VLCS [23] datasets using the default hyper-parameter settings in DomainBed [27]. "], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "SYjxhKcXoN/tmp/788006f765e30ac1f555d5901600ce5f0a84a54537c0c5ca37f0b579fe69460d.jpg", "table_caption": ["Table 13: Average accuracies on the OfficeHome [76] datasets using the default hyper-parameter settings in DomainBed [27]. "], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "SYjxhKcXoN/tmp/3dfaacaef1074fe0e89e671142d3015382e3b2a8e976c21079ffb03d42841082.jpg", "table_caption": ["Table 14: Average accuracies on the TerraInc [3] datasets using the default hyper-parameter settings in DomainBed [27]. "], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "SYjxhKcXoN/tmp/d1467b61bc72954821d9527455f05a011589aaaab34d29126137189d300221af.jpg", "table_caption": ["Table 15: Average accuracies on the DomainNet [59] datasets using the default hyper-parameter settings in DomainBed [27]. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope, which matches our theoretical analyses and experimental findings. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 23}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: Limitations have been discussed in Sec. A. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 23}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We provide our theoretical statements and proof in Sec. 4. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 24}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We provide pseudocode in Sec. B, and our code in the supplemental material. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 24}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: Code is provided at https://github.com/liangchen527/LFME. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 25}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Implementation details are described in our experimental sections. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 25}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: We report error bars for our experiments and detailed how they are computed. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: For all experiments, we use a Tesla V100 GPU with 32 GB memory, the actual running time is detailed in Tab. 11. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 26}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We confirm our work does not violate NeurIPS Code of Ethics, in any respect. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 26}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: This work aims to improve the generalizability of a deep model. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: This paper does not involve such risks. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 27}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Benchmarks used in this data are properly credited. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 27}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 28}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 28}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 28}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 28}]