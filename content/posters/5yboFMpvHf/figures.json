[{"figure_path": "5yboFMpvHf/figures/figures_1_1.jpg", "caption": "Figure 1: Left: Matryoshka Representation Learning. Right: Feature extractor and prediction header.", "description": "This figure illustrates the Matryoshka Representation Learning (MRL) architecture. The left panel shows the overall MRL process: a feature extractor takes an input (x) and produces Matryoshka representations (different colors and sizes) from which multiple predictions (\u0177) are derived. Each prediction is associated with a loss (l), and the losses are aggregated.  The right panel shows the architecture of both the feature extractor and prediction header components, which include convolutional layers (Conv), fully connected layers (FC), and a final output layer for the prediction (\u0177). The figure helps explain how MRL learns multi-granular representations.", "section": "1 Introduction"}, {"figure_path": "5yboFMpvHf/figures/figures_3_1.jpg", "caption": "Figure 2: The workflow of FedMRL.", "description": "This figure illustrates the workflow of the Federated Model Heterogeneous Matryoshka Representation Learning (FedMRL) approach. It shows how a global homogeneous small model interacts with heterogeneous local models at each client.  The process involves adaptive representation fusion, where generalized and personalized representations are combined and projected into a fused representation.  This fused representation then undergoes multi-granular representation learning, generating Matryoshka representations processed by both the global and local model headers. The final outputs and losses are used to update all model parameters, promoting effective knowledge interaction and personalized model training.", "section": "3 The Proposed FedMRL Approach"}, {"figure_path": "5yboFMpvHf/figures/figures_7_1.jpg", "caption": "Figure 3: Left six: average test accuracy vs. communication rounds. Right two: individual clients' test accuracy (%) differences (FedMRL - FedProto).", "description": "This figure presents the results of the FedMRL algorithm compared to the FedProto algorithm. The left six plots show the average test accuracy versus the number of communication rounds for different datasets and settings (CIFAR-10 and CIFAR-100 with varying numbers of clients). The right two plots show the difference in test accuracy between FedMRL and FedProto for each individual client in the N=100, C=10% settings. These results highlight FedMRL's superior performance and consistent faster convergence speed compared to FedProto.", "section": "5.2 Results and Discussion"}, {"figure_path": "5yboFMpvHf/figures/figures_7_2.jpg", "caption": "Figure 4: Communication rounds, number of communicated parameters, and computation FLOPS required to reach 90% and 50% average test accuracy targets on CIFAR-10 and CIFAR-100.", "description": "This figure compares FedProto and FedMRL in terms of communication rounds, number of communicated parameters, and computation FLOPs needed to achieve 90% and 50% average test accuracy on CIFAR-10 and CIFAR-100 datasets. It demonstrates the efficiency of FedMRL in terms of communication and computation costs compared to FedProto, highlighting its advantage in faster convergence and lower resource consumption.", "section": "5.2 Results and Discussion"}, {"figure_path": "5yboFMpvHf/figures/figures_8_1.jpg", "caption": "Figure 5: Robustness to non-IIDness (Class & Dirichlet).", "description": "This figure displays the robustness of FedMRL and FedProto to different levels of Non-IID data. The left two subfigures show the results for varying numbers of classes assigned to each client (class-based non-IIDness), while the right two subfigures show the results for varying Dirichlet parameters (Dirichlet-based non-IIDness).  FedMRL consistently outperforms FedProto across different Non-IID settings, demonstrating its robustness.", "section": "5.3 Case Studies"}, {"figure_path": "5yboFMpvHf/figures/figures_8_2.jpg", "caption": "Figure 6: Left two: sensitivity analysis results. Right two: ablation study results.", "description": "This figure presents the results of sensitivity analysis and ablation study on the hyperparameter d1 (representation dimension of the homogeneous small model). The left two subfigures show the impact of d1 on the average test accuracy for CIFAR-10 and CIFAR-100 datasets.  It indicates that smaller d1 values generally lead to higher accuracy. The right two subfigures compare the performance of FedMRL with and without the Matryoshka Representation Learning (MRL) module. FedMRL with MRL consistently outperforms FedMRL without MRL, demonstrating the benefit of incorporating MRL. The accuracy gap between the two decreases as d1 increases, suggesting that the benefits of MRL reduce when the global and local headers learn increasingly similar information.", "section": "5.3 Case Studies"}, {"figure_path": "5yboFMpvHf/figures/figures_17_1.jpg", "caption": "Figure 7: Accuracy of four optional inference models: mix-small (the whole model without the local header), mix-large (the whole model without the global header), single-small (the homogeneous small model), single-large (the client heterogeneous model).", "description": "This figure compares the performance of four different inference models on CIFAR-10 and CIFAR-100 datasets. The models tested are: mix-small (combining the homogeneous small model, client's heterogeneous model feature extractor, and representation projector), mix-large (combining the homogeneous small model feature extractor, client's heterogeneous model, and representation projector), single-small (only using the homogeneous small model), and single-large (only using the client's heterogeneous model). The x-axis represents the representation dimension (d1) of the homogeneous small model, while the y-axis shows the test accuracy.  The results indicate that mix-small and mix-large models generally achieve similar and higher accuracy compared to using only the homogeneous small model or the client's heterogeneous model alone.", "section": "5.2.2 Individual Client Test Accuracy"}]