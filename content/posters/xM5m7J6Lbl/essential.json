{"importance": "This paper is crucial for AI safety and governance research. It provides **a novel framework for quantifying AI alignment in social decision-making**, addressing a critical gap in current research.  The proposed methods, while theoretically focused, **open doors for practical AI safety mechanisms** and **inspire new directions** in evaluating and ensuring the beneficial use of autonomous agents in critical societal roles.", "summary": "This paper introduces a novel quantitative definition of AI alignment for social decision-making, proposing probably approximately aligned policies and a method to safeguard any autonomous agent's actions, ensuring societal safety.", "takeaways": ["A novel quantitative definition of AI alignment for social decision-making is proposed.", "The concept of Probably Approximately Aligned (PAA) policies and a method to guarantee their existence are introduced.", "A simple method to ensure the safety of any autonomous agent's policy by safeguarding its actions is presented."], "tldr": "Current AI alignment methods lack formal guarantees, hindering their use in critical applications like government. This paper tackles this challenge by focusing on social decision-making. It introduces the problem of AI misalignment, particularly the difficulty in formally guaranteeing the safety and reliability of AI systems used in critical decision-making processes. The lack of transparency and the inherent complexity of social systems create significant challenges in ensuring that AI agents' objectives align with societal well-being. \nThe paper proposes a novel quantitative definition of alignment based on utility and social choice theory. It introduces the concept of 'probably approximately aligned' (PAA) policies, offering a measure of near-optimal alignment.  Further, it presents a method to verify and guarantee the safety of policies, even in the presence of 'black-box' AI agents. This approach relies on a sufficient condition derived from social choice theory and leverages world models to assess potential societal impacts before actions are taken. The results demonstrate the existence of PAA policies under specific conditions, offering a theoretical foundation for building safer and more trustworthy AI systems for social decision-making.", "affiliation": "ETH Zurich", "categories": {"main_category": "AI Theory", "sub_category": "Safety"}, "podcast_path": "xM5m7J6Lbl/podcast.wav"}