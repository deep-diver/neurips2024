[{"figure_path": "HOSh0SKklE/tables/tables_8_1.jpg", "caption": "Table 1: Measured expansion and error bounds for the covered sets Si. Expansion values for the family of sets M'(Sgood, F) are measured using the heuristic described in Section 5 and shown in the (Sbad, Sgood) exp. column. This column shows our heuristic finds expansion in practice. Pseudolabel error ai = P(\u1ef9 \u2260y|Si). Worst-case error of trained classifier f on the weak labels \u1ef9, err(f, \u1ef9|Si), across 5 independent training runs. This column shows the student can't exactly fit the teacher labels using this representation. Value of the error upper bound in Theorem 4.1 (specifically, the tighter version, B.1), computed using the numbers from the other columns (details in Appendix E). For label i = 0, the bound being strictly less than the error a\u2081 of the teacher \u1ef9 suggests pseudolabel correction may occur. Finally, the actual worst-case error of trained classifier f on the true labels y, err(f, y|Si), across 5 independent training runs, shows pseudolabel correction does occur for label i = 0.", "description": "This table presents the results of measuring expansion and calculating error bounds for the covered sets (Si) based on Theorem 4.1.  It demonstrates the empirical evidence supporting the theory's assumptions and the occurence of pseudolabel correction.  The table includes measured expansion values, pseudolabel errors, classifier errors on weak labels, calculated error bounds, and actual classifier errors on true labels. The comparison between the calculated bound and the actual error highlights whether pseudolabel correction is likely to occur for each label.", "section": "Experiments"}, {"figure_path": "HOSh0SKklE/tables/tables_31_1.jpg", "caption": "Table 2: Comparison of measured values of the amount of expansion c (\"exp. c val.\") on the data described in Section 6. Theorem B.1 (our pseudolabel correction result) requires M'(Sgood, F) to expand on the pair (Sbad, Sgood) for some c > 0. That is, it requires robust non-mistakes to expand to points with the wrong pseudolabels. We call this \u201cgood-to-bad\u201d or G2B expansion. On the other hand, Theorem C.2 requires M(Sbad, F) to expand on the pair (Sgood, Shad) for some c > ai/(1 \u2212 ai). In other words, it requires robust mistakes to expand to points with the correct pseudolabel. We call this \"bad-to-good\" or B2G expansion. These results show that empirically, we may have the G2B c > 0, so our bounds apply, but the B2G c < ai/(1 - ai), so Theorem C.2 does not apply. This is the case for the i = 1 values.", "description": "This table compares the measured expansion values for two different types of expansion: good-to-bad (G2B) and bad-to-good (B2G).  It highlights that while Theorem B.1 (pseudolabel correction) only requires G2B expansion to hold for a positive constant c, Theorem C.2 (a result from prior work) requires the stronger condition that B2G expansion holds for a constant c > \u03b1i/(1-\u03b1i). The table shows empirical measurements of these expansion values for two different classes (i=0 and i=1), demonstrating that G2B expansion holds in practice for both classes, whereas the B2G expansion requirement of Theorem C.2 is not satisfied for the i=1 class.", "section": "C Connections and comparisons to existing bounds"}, {"figure_path": "HOSh0SKklE/tables/tables_36_1.jpg", "caption": "Table 3: Test accuracy breakdown for linear probe trained with true (gold) and weak labels on the IMDb data. Performance of the weakly-trained model is broken down across the covered sets (So, S1) and the uncovered sets (To, T1). One standard deviation across five training folds is shown in parentheses. Pseudolabel correction and coverage expansion occur in different amounts depending on the class. For example, the student consistently improves over \u1ef9 on So but not on S1. This justifies our choice to analyze these effects separately for different classes.", "description": "This table presents the test accuracy results for linear probes trained using both gold and weak labels on the IMDB dataset.  It breaks down the accuracy across the covered sets (So, S1), where the weak labels are available, and the uncovered sets (To, T1), where they are not. The results show the impact of weak supervision on the student model's performance, demonstrating variation in pseudolabel correction and coverage expansion across different classes. Standard deviations across five training folds are included for better understanding of the variability.", "section": "6 Experiments"}, {"figure_path": "HOSh0SKklE/tables/tables_36_2.jpg", "caption": "Table 4: Measured expansion values and error bounds for the uncovered sets T\u2081. Expansion values for the families M(Sbad, F) on (Sgood, T\u2081) and M'(Sgood, F) on (Sbad, T\u2081), are measured using the heuristic described in Section 5. The detection of both types of expansion (expansion from T\u2081 to Sgood and to Shad) gives evidence for the extra structure we described in Section 4.1 and justifies our use of Theorem B.2, which uses this structure, instead of Theorem B.3, which only uses expansion from T\u2081 to Sgood and gives a looser bound. Worst-case value of the error bound in Theorem B.2, computed using the smallest expansion values and largest weak errors err(f, \u1ef9|Si) from the 5 training runs. The err(f, \u1ef9|Si) and ai values used in the bound computation are identical to the values in Table 1. Unlike in Table 1, where the \u201cbaseline\u201d for pseudolabel correction effects is to have error bounds strictly better than ai, for coverage expansion, the more relevant comparison is against random/arbitrary guessing. The actual worst-case error of the student on each T\u2081 is shown as err(f, \u1ef9|T\u2081). As suggested by our bound values, the errors on each T\u2081 are non-trivial (much better than random or arbitrary guessing).", "description": "This table presents the measured expansion values and error bounds for the uncovered sets (T\u2081, T\u2082).  It shows the results of applying Theorem B.2 to calculate the error bounds. The expansion values are obtained using the heuristic from Section 5, showing expansion from T\u2081 to both Sgood and Shad.  The table compares the calculated error bounds with the actual worst-case errors observed in the experiments, demonstrating the effectiveness of the proposed theoretical framework.", "section": "4 Error Bounds for Weakly-Supervised Classifiers"}]