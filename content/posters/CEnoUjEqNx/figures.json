[{"figure_path": "CEnoUjEqNx/figures/figures_4_1.jpg", "caption": "Figure 1: Trajectories of two learning algorithms (Multiplicative Weights and Blum-Mansour) playing Rock-Paper-Scissors. The axis corresponds to the probability of playing Rock and Paper. The blue dot corresponds to the probabilities at Nash equilibrium while the red dot is the last iterate after 10000 steps with learning rate \u03b7 = 0.001.", "description": "This figure shows the trajectories of two learning algorithms, Multiplicative Weights and Blum-Mansour, when playing the Rock-Paper-Scissors game.  The x and y axes represent the probabilities of playing Rock and Paper, respectively. The blue dot marks the Nash equilibrium point, while the red dot indicates the final point reached by each algorithm after 10,000 iterations with a learning rate of 0.001. The figure illustrates the difference in convergence behavior between the two algorithms.  Multiplicative Weights approaches the Nash equilibrium but does not converge to it in the last iteration, while Blum-Mansour shows better convergence to the equilibrium point.", "section": "3 Convergence of Symmetric Swap Regret in Symmetric Zero Sum Games"}, {"figure_path": "CEnoUjEqNx/figures/figures_4_2.jpg", "caption": "Figure 2: Trajectories of Multiplicative Weights and Blum-Mansour for Rock-Paper-Scissors-Lizard-Spock (a the 5-strategy generalization of Rock-Paper-Scissors).", "description": "This figure shows the trajectories of two learning algorithms, Multiplicative Weights and Blum-Mansour, when playing the Rock-Paper-Scissors-Lizard-Spock game (a five-strategy extension of Rock-Paper-Scissors).  The x and y axes represent the probability of playing two specific actions (e.g., Rock and Paper). The blue dot indicates the Nash equilibrium, while the red dot marks the final iterate after a set number of iterations. The figure illustrates the difference in convergence behavior between the two algorithms in this more complex game setting.", "section": "Convergence of Symmetric Swap Regret in Symmetric Zero Sum Games"}, {"figure_path": "CEnoUjEqNx/figures/figures_13_1.jpg", "caption": "Figure 3: This figure shows the trajectory of two players running the Blum-Mansour no-swap-regret algorithm against each other initialized with asymmetric starting conditions. Unlike the symmetric dynamics, these do not ultimately converge to the unique symmetric Nash equilibrium (the blue point).", "description": "This figure shows trajectories of two players using the Blum-Mansour algorithm, a no-swap-regret learning algorithm.  The players start with asymmetric initial conditions (different starting probabilities for actions) in a symmetric zero-sum game (Rock-Paper-Scissors). Unlike Figure 1, where symmetric initializations led to convergence to the Nash equilibrium, here the trajectories do not converge, illustrating that symmetric initializations are crucial for the convergence result.", "section": "3 Convergence of Symmetric Swap Regret in Symmetric Zero Sum Games"}]