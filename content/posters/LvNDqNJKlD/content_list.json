[{"type": "text", "text": "A Framework for Bilevel Optimization on Riemannian Manifolds ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Andi Han1 Bamdev Mishra2 Pratik Jawanpuria2 Akiko Takeda1 ", "page_idx": 0}, {"type": "text", "text": "1RIKEN AIP 2Microsoft, India 3University of Tokyo andi.han $@$ riken.jp {bamdevm, pratik.jawanpuria} $@$ microsoft.com. takeda $@$ mist.i.u-tokyo.ac.jp ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Bilevel optimization has gained prominence in various applications. In this study, we introduce a framework for solving bilevel optimization problems, where the variables in both the lower and upper levels are constrained on Riemannian manifolds. We present several hypergradient estimation strategies on manifolds and analyze their estimation errors. Furthermore, we provide comprehensive convergence and complexity analyses for the proposed hypergradient descent algorithm on manifolds. We also extend our framework to encompass stochastic bilevel optimization and incorporate the use of general retraction. The efficacy of the proposed framework is demonstrated through several applications. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Bilevel optimization is a hierarchical optimization problem where the upper-level problem depends on the solution of the lower-level, i.e., ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x\\in\\mathbb{R}^{d_{x}}}F(x)=f(x,y^{*}(x)),\\qquad{\\mathrm{~s.t.~}}y^{*}(x)=\\underset{y\\in\\mathbb{R}^{d_{y}}}{\\operatorname{arg\\,min}}\\,g(x,y).\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "Applications involving bilevel optimization include meta learning [16], hyperparameter optimization [18], and neural architecture search (NAS) [53], to name a few. The lower-level problem is usually assumed to be strongly convex. ", "page_idx": 0}, {"type": "text", "text": "Common strategies for solving such problem can be classified into two categories: single-level reformulation [29, 60] and approximate hypergradient descent [19, 40]. The former aims to reformulate the bilevel optimization problem into a single-level one using the optimality conditions of the lower-level problem as constraints. However, this may impose a large number of constraints for machine learning applications. The latter scheme directly solves the bilevel problem through iteratively updating the lower and upper-level parameters and, hence, is usually more efficient. Nevertheless, existing works have mostly focused on unconstrained bilevel optimization [19, 32, 40, 11, 52, 45, 14]. ", "page_idx": 0}, {"type": "text", "text": "In this work, we study bilevel optimization problems where $x$ and $y$ are on Riemannian manifolds $\\mathcal{M}_{x}$ and $\\mathcal{M}_{y}$ , respectively. We focus on the setup where the lower-level function $g(x,y)$ is geodesic strongly convex (a generalized notion of convexity on manifolds, defined in Section 2) in $y$ . This ensures the lower-level problem has a unique solution $y^{*}(x)$ given $x$ . The upper-level function $f$ can be nonconvex on $\\mathcal{M}_{x}\\times\\mathcal{M}_{y}$ . Because the unconstrained bilevel optimization is a special case of our formulation on manifolds, such a formulation includes a wider class of applications. Examples of Riemannian bilevel optimization include Riemannian meta learning [64] and NAS over SPD networks [62]. Moreover, there has been a surge of interest of min-max optimization over Riemannian manifolds [37, 41, 73, 27, 25, 67, 35], which also gets subsumed in the framework of bilevel optimization with $g=-f$ . ", "page_idx": 0}, {"type": "text", "text": "Contributions. (i) We derive intrinsic Riemannian hypergradient via the implicit function theorem and propose four strategies for estimating the hypergradient, i.e., through Hessian inverse, conjugate gradient, truncated Neumann series, and automatic differentiation. We then provide hypergradient estimation error bounds for all the proposed strategies. (ii) We introduce the Riemannian hypergradient descent algorithm to solve bilevel optimization problems on manifolds and provide convergence guarantees. We also generalize the framework to the stochastic setting and to allow the use of retraction. (iii) The efficacy of the proposed modeling is shown on several problem instances including hyper-representation over SPD matrices, Riemannian meta learning, and unsupervised domain adaptation. The proofs, extensions, and experimental details are deferred to the appendix sections. ", "page_idx": 1}, {"type": "text", "text": "Related works in unconstrained setting. Unconstrained bilevel optimization where the lowerlevel problem is strongly convex has been widely studied [19, 32, 40, 11, 52, 45, 14]. A crucial ingredient is the notion of hypergradient in bilevel optimization problems and its computation. There exist strategies for approximating the hypergradient, e.g., using conjugate gradient [40], Neumann series [19], iterative differentiation [21], and Nystr\u00f6m method [31]. While bilevel optimization with constraints is relatively unexplored, a few works exists that impose constraints only for the upper level problem [32, 10]. Recently, linearly lower-level constrained bilevel optimization has been explored in [65, 68], where a projected gradient method is employed for the lower-level problem. ", "page_idx": 1}, {"type": "text", "text": "Related works on manifolds. There has been limited work on bilevel optimization problems on manifolds. [7] studies semivectorial bilevel optimization on Riemannian manifolds where the upperlevel is a scalar optimization problem while the lower-level is a multiobjective problem under greatest coalition. [50, 49] reformulate bilevel problems on manifolds into a single-level problem based on the KKT conditions on manifolds. However, for all those works, it is unclear whether there exists an algorithm that efficiently solves the problem in large-scale settings. In contrast, we aim to provide a general framework for solving bilevel optimization on Riemannian manifolds. [47] is a contemporary work that also proposes gradient-based algorithms for bilevel optimization on Riemannian manifolds. The main differences of our work with respect to [47] are as follows: (1) We provide an analysis for various hypergradient estimators while [47] focuses on conjugate gradient for deterministic setting and Neumann series for stochastic setting; (2) We provide an analysis for retraction which is more computationally efficient than exponential map and parallel transport employed in [47]; and (3) We explore the utility of Riemannian bilevel optimization in various machine learning applications, which is not the case with [47]. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries and notations ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "A Riemannian manifold $\\mathcal{M}$ is a smooth manifold equipped with a smooth inner product structure (a Riemannian metric) $\\langle\\cdot,\\cdot\\rangle_{p}:T_{z}\\mathcal{M}\\times T_{z}\\mathcal{M}\\to\\mathbb{R}$ for any $z\\in\\mathcal{M}$ and its tangent space $T_{z}\\mathcal{M}$ . The induced norm is thus $\\|u\\|_{z}=\\sqrt{\\langle u,u\\rangle_{z}}$ for any $u\\in T_{z}\\mathcal{M}$ . A geodesic $c:[0,1]\\to\\mathcal{M}$ generalizes the line segment in the Euclidean space as the locally shortest path on manifolds. The exponential map on a manifold is defined as $\\mathrm{Exp}_{z}(u)=c(1)$ for a geodesic $c$ that satisfies $c(0)=z,c^{\\prime}(0)=u$ . In a totally normal neighbourhood $\\boldsymbol{\\mathcal{U}}$ where exponential map has a smooth inverse, the Riemannian $T_{z_{2}}\\mathcal{M}$ ei $\\begin{array}{r}{d(x,y)=\\Vert\\mathrm{Exp}_{x}^{-1}(y)\\Vert_{x}=\\Vert\\mathrm{Exp}_{y}^{-1}(\\bar{x})\\Vert_{y}}\\end{array}$ . roTdhue cpt,a ri.aell.,e $\\langle u,v\\rangle_{z_{1}}=\\langle\\Gamma_{z_{1}}^{z_{2}}u,\\Gamma_{z_{1}}^{z_{2}}\\bar{v}\\rangle_{z_{2}},$ $\\Gamma_{z_{1}}^{z_{2}}:T_{z_{1}}\\mathcal{M}\\rightarrow$ $\\forall u,v\\in$ $T_{z_{1}}\\mathcal{M}$ . The (Cartesian) product of Riemannian manifolds $\\mathcal{M}_{x}\\times\\mathcal{M}_{y}$ is also a Riemannian manifold. For a differentiable function $f:\\mathcal{M}\\rightarrow\\mathbb{R}$ , the Riemannian gradient $\\mathcal G f(z)\\in T_{z}\\mathcal M$ is the tangent vector that satisfies $\\langle\\mathcal{G}f(z),u\\rangle_{z}=\\mathrm{D}f(z)[u]$ for all $u\\in T_{z}\\mathcal{M}$ . Here D is the differential operator and $\\mathrm \u1e0a f \u1e0c (z)[u]$ represents the directional derivative of $f$ at $z$ along $u$ . For a twice differentiable function $f$ , Riemannian Hessian $\\mathcal{H}f(z)$ is defined as the covariant derivative of Riemannian gradient. ", "page_idx": 1}, {"type": "text", "text": "Geodesic convexity extends the convexity notion in the Euclidean space to Riemannian manifolds. A geodesic convex set ${\\mathcal{Z}}\\subseteq M$ is where any two points can be joined by a geodesic. A function $f:\\mathcal{M}\\rightarrow\\mathbb{R}$ is said to be geodesic (strongly) convex if for all geodesics $c\\colon[0,1]\\to{\\mathcal{Z}}$ , $f(c(t))$ is (strongly) convex in $t\\in[0,1]$ . If the function is smooth, then $f$ is called $\\mu$ -geodesic strongly convex if and only if $f(\\mathrm{Exp}_{z}(t\\bar{u_{}}))\\geq f(z)+t\\langle\\mathcal G f(z),u\\rangle_{z}+t^{2}\\frac{\\mu}{2}\\|u\\|_{z}^{2}\\forall t\\in[\\bar{0},\\bar{1}]$ . An equivalent second-order characterization is $\\mathcal{H}(z)\\succeq\\mu\\mathrm{id}$ , where we denote id as the identity operator. ", "page_idx": 1}, {"type": "text", "text": "For a bifunction $\\phi:\\mathcal{M}_{x}\\times\\mathcal{M}_{y}\\to\\mathbb{R}$ , we denote $\\mathcal{G}_{x}\\phi(x,y),\\mathcal{G}_{y}\\phi(x,y)$ as the Riemannian (partial) gradient and $\\mathcal{H}_{x}\\phi(x,y),\\mathcal{H}_{y}\\phi(\\stackrel{.}{x},y)$ as the Riemannian Hessian. The Riemannian cross-derivatives are linear operators $\\mathcal{G}_{x y}^{2}\\phi(x,y)\\,:\\,T_{y}\\mathcal{M}_{y}\\,\\rightarrow\\,T_{x}\\mathcal{M}_{x},\\mathcal{G}_{y x}^{2}\\phi(x,y)\\,:\\,T_{x}\\mathcal{M}_{x}\\,\\rightarrow\\,T_{y}\\mathcal{M}_{y}$ defined as $\\mathcal{G}_{x y}^{2}\\phi(x,y)[v]=\\mathrm{D}_{y}\\mathcal{G}_{x}\\bar{\\phi}(x,y)[v]$ for any $v\\in T_{y}\\M_{y}$ (with D representing the differential operator) and similarly for $\\mathcal{G}_{y x}^{2}\\phi(x,y)$ . For a linear operator $T:\\,T_{x}{\\mathcal{M}}_{x}\\,\\rightarrow\\,T_{y}{\\mathcal{M}}_{y}$ , the adjoint operator, denoted as $T^{\\dagger}$ is defined with respect to the Riemannian metric, i.e., $\\langle T[u],v\\rangle_{y}=\\langle T^{\\dagger}[v],u\\rangle_{x}$ for any $u\\in T_{x}\\mathcal{M}_{x},v\\in T_{y}\\mathcal{M}_{y}$ . The operator norm of $T$ is defined as $\\begin{array}{r}{\\|T\\|_{y}:=\\operatorname*{sup}_{u\\in T_{x}\\mathcal{M}_{x}:\\|u\\|_{x}=1}\\|T[u]\\|_{y}}\\end{array}$ . ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "3 Proposed Riemannian hypergradient algorithm ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this work, we consider the constrained bilevel optimization problem ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x\\in\\mathcal{M}_{x}}F(x):=f(x,y^{*}(x)),\\qquad\\mathrm{~s.t.~}y^{*}(x)=\\underset{y\\in\\mathcal{M}_{y}}{\\arg\\operatorname*{min}}\\,g(x,y),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathcal{M}_{x},\\mathcal{M}_{y}$ are two Riemannian manifolds and $f,g:\\mathcal{M}_{x}\\times\\mathcal{M}_{y}\\rightarrow\\mathbb{R}$ are real-valued jointly smooth functions. We focus on the setting where the lower-level function $g(x,y)$ is geodesic strongly convex. This ensures the lower-level problem has a unique solution $y^{*}(x)$ for a given $x$ . The upper-level function $f$ can be nonconvex on $\\mathcal{M}_{x}\\times\\mathcal{M}_{y}$ . ", "page_idx": 2}, {"type": "text", "text": "We propose to minimize $F(x)$ directly within the Riemannian optimization framework. To this end, we need the notion of the Riemannian gradient of $F(x):=\\bar{f}(x,y^{*}(x))$ , which we call the Riemannian hypergradient. ", "page_idx": 2}, {"type": "text", "text": "Proposition 1. The differential of $y^{*}(x)$ and the Riemannian hypergradient of $F(x)$ are given by ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{D}y^{*}(x)}&{=-\\mathcal{H}_{y}^{-1}g(x,y^{*}(x))\\circ\\mathcal{G}_{y x}^{2}g(x,y^{*}(x))}\\\\ {\\mathcal{G}F(x)}&{=\\mathcal{G}_{x}f(x,y^{*}(x))-\\mathcal{G}_{x y}^{2}g(x,y^{*}(x))[\\mathcal{H}_{y}^{-1}g(x,y^{*}(x))[\\mathcal{G}_{y}f(x,y^{*}(x))]].}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The above proposition crucially relies on the implicit function theorem on manifolds [25] and requires the invertibility of the Hessian of the lower level function $f$ with respect to $y$ . This is guaranteed in our setup as $f$ is geodesic strongly convex in $y$ . Hence, there exists a unique differentiable function $y^{*}(x)$ that maps $x$ to the lower-level solution. We show the Riemannian hypergradient descent (RHGD) algorithm for (1) in Algorithm 1. ", "page_idx": 2}, {"type": "table", "img_path": "LvNDqNJKlD/tmp/1f595b73eb83e07af2d23f35aef1b10fcc37d957cb81781dfb300d67c466edd3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "We highlight that Step 8 of Algorithm 1 approximates the Riemannian hypergradient. In the rest of the section, we discuss various computationally efficient ways to estimate the Riemannian hypergradient and discuss the corresponding theoretical guarantees for RHGD. The error of hypergradient approximation comes from the inaccuracies of $y_{k+1}$ to $y^{\\ast}(x_{k})$ and also from the Hessian inverse. ", "page_idx": 2}, {"type": "text", "text": "3.1 Hypergradient estimation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "When the inverse Hessian of the lower-level problem can be computed efficiently, we can estimate the hypergradient directly by evaluating the Hessian inverse (HINV) at $y_{k+1}$ , i.e., ${\\widehat{\\mathcal{G}}}_{\\mathrm{hinv}}F(x_{k})=$ $\\mathcal{G}_{x}f(x_{k},y_{k+1})\\,-\\,-\\mathcal{G}_{x y}^{2}g(x_{k},y_{k+1})\\big[\\mathcal{H}_{y}^{-1}g(x_{k},y_{k+1})[\\mathcal{G}_{y}f(x_{k},y_{k+1})]\\big]$ . However, computing the inverse Hessian is computationally expensive in many scenarios. We now discuss three practical strategies for estimating the Riemannian hypergradient when $y k{+}1$ is given. ", "page_idx": 2}, {"type": "text", "text": "Conjugate gradient approach (CG). When evaluating the Hessian inverse is difficult, we can solve the linear system $\\bar{\\mathcal{H}}_{y}g(x_{k},y_{k+1})[u]\\;=\\;\\mathcal{G}_{y}f(x_{k},\\bar{y}_{k+1})$ for some $u\\;\\in\\;T_{y_{k+1}}\\/{\\mathcal{M}}_{y}$ . To this end, we employ the tangent space conjugate gradient algorithm (Appendix F, Algorithm 3) that solves the linear system on the tangent space $T_{y_{k+1}}\\M_{y}$ with only access to Hessian-vector products, i.e., $\\widehat{\\mathcal{G}}_{\\mathrm{cg}}F(x_{k})\\,=\\,\\mathcal{G}_{x}f(x_{k},y_{k+1})\\,-\\,\\mathcal{G}_{x y}^{2}g(x_{k},y_{k+1})[\\widehat{v}_{k}^{T}]$ , where $\\hat{v}_{k}^{T}$ is computed as a solution to $\\mathcal{H}_{y}g(x_{k},y_{k+1})[\\hat{v}_{k}^{T}]\\,=\\,\\mathcal{G}_{y}f(x_{k},y_{k+1})$ , where $T$ is the number of iterations of the tangent space conjugate gradient algorithm. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Truncated Neumann series approach (NC). The Neumann series states for an invertible operator $H$ such that $\\|H\\|\\leq1$ , its inverse $\\begin{array}{r}{\\dot{H}^{-1}=\\sum_{i=0}^{\\infty}(\\mathrm{id}-H)^{i}}\\end{array}$ , where id is the identity operator. An alternative approach to estimate the Hessian inverse is to use a truncated Neumann series, which leads to the following approximated hypergradient, $\\begin{array}{r}{\\widehat{\\mathcal{G}}_{\\mathrm{ns}}F(x_{k})=\\mathcal{G}_{x}f(x_{k},y_{k+1})\\!-\\!\\mathcal{G}_{x y}^{2}g(x_{k},y_{k+1})[\\gamma\\sum_{i=0}^{T-1}(\\mathrm{id}-}\\end{array}$ $\\gamma\\mathcal{H}_{y}g(x_{k},y_{k+1}))^{i}[\\mathcal{G}_{y}f(x_{k},y_{k+1})]]$ , where $\\gamma$ is chosen such that $({\\mathrm{id}}-\\gamma{\\mathcal{H}}_{y}g(x_{k},y_{k+1}))\\,\\succ\\,0$ . $\\gamma$ can be set as $\\begin{array}{r}{\\gamma=\\frac{1}{L}}\\end{array}$ , where the gradient operator is $L$ -Lipschitz (discussed later in Definition 1). Empirically, we observe that this approach is faster than the conjugate gradient approach. However, it requires estimating $T$ and $L$ beforehand. ", "page_idx": 3}, {"type": "text", "text": "Automatic differentiation approach (AD). Another hypergradient estimation strategy follows the idea of iterative differentiation by backpropagation. After running several iterations of gradient update to obtain $y k{+}1$ (which is a function of $x_{k}$ ), we can use automatic differentiation to compute directly the Riemannian gradient of $f(x_{k},y_{k+1}(x_{k}))$ with respect to $x_{k}$ . We can compute the Riemannian hypergradient from the differential in the direction of arbitrary $u\\in T_{x_{k}}\\M_{x}$ using basic chain rules. ", "page_idx": 3}, {"type": "text", "text": "3.2 Theoretical analysis ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "This section provides theoretical analysis for the proposed hypergradient estimators as well as the Riemannian hypergradient descent. First, we require the notion of Lipschitzness of functions and operators defined on Riemannian manifolds. Below, we introduce the definition in terms of bi-functions and bi-operators and state the assumptions that are required for the analysis. ", "page_idx": 3}, {"type": "text", "text": "Definition 1 (Lipschitzness). (1) For a bifunction $f:\\mathcal{M}_{x}\\times\\mathcal{M}_{y}\\to\\mathbb{R}$ , we say $f$ has $L$ Lipschitz Riemannian gradient in $\\mathcal{U}_{x}\\times\\mathcal{U}_{y}\\subseteq\\mathcal{M}_{x}\\times\\mathcal{M}_{y}$ if it satisfies for any $x,x_{1},x_{2}\\in\\mathcal{U}_{x},y,y_{1},y_{2}\\in$ $\\mathcal{U}_{y}$ , $\\|\\Gamma_{y_{1}}^{y_{2}}\\mathcal{G}_{y}f(x,y_{1})-\\mathcal{G}_{y}f(x,y_{2}^{-})\\|_{y_{2}}\\,\\le\\,L d(y_{1},y_{2})$ , $\\|\\mathcal{G}_{x}f(x,y_{1})-\\mathcal{G}_{x}f(x,y_{2})\\|_{x}\\,\\leq\\,L d(y_{1},y_{2}),$ $\\|\\Gamma_{x_{1}}^{x_{2}}\\mathcal{G}_{x}f(x_{1},y)-\\mathcal{G}_{x}f(x_{2},y)\\|_{x_{2}}\\leq L d(x_{1},x_{2})$ and $\\begin{array}{r}{\\|\\mathcal{G}_{y}f(x_{1},y)-\\mathcal{G}_{y}f(x_{2},y)\\|_{y}\\leq L d(x_{1},x_{2}).}\\end{array}$ (2) For an operator $\\mathcal{G}(x,y)~:~T_{y}\\mathcal{M}_{y}~\\rightarrow~T_{x}\\mathcal{M}_{x}$ , we say $\\mathcal{G}(x,y)$ is $\\rho$ -Lipschitz if it satisfies, $\\|\\Gamma_{x_{1}}^{x_{2}}\\mathcal{G}(x_{1},y)-\\mathcal{G}(x_{2},y)\\|_{x_{2}}\\leq\\rho\\,\\mathring{d}(x_{1},x_{2})$ and $\\|\\mathcal{G}(x,y_{1})-\\mathcal{G}(x,y_{2})\\Gamma_{y_{1}}^{y_{2}}\\|_{x}\\leq\\rho\\,d(y_{1},y_{2})$ . (3) For an operator $\\mathcal{H}(x,y)\\;:\\;T_{y}\\mathcal{M}_{y}\\;\\rightarrow\\;T_{y}\\mathcal{M}_{y}$ , we say $\\mathcal{H}(x,y)$ is $\\rho$ -Lipschitz if it satisfies, $\\|\\Gamma_{y_{1}}^{y_{2}}\\mathcal{H}(x,y_{1})\\Gamma_{y_{2}}^{y_{1}}-\\mathcal{H}(x,y_{2})\\|_{y_{2}}\\overset{<}{\\leq}\\rho\\,\\mathring{d}(y_{1},y_{2})$ and $\\begin{array}{r}{\\|\\mathcal{H}(x_{1},y)-\\mathcal{H}(x_{2},y)\\|_{y}\\leq\\rho\\,d(x_{1},x_{2})}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "It is worth mentioning that Definition 1 implies the joint Lipschitzness over the product manifold $\\mathcal{M}_{x}\\times\\mathcal{M}_{y}$ , which is verified in Appendix C.2. Due to the possible nonconvexity for the upper level problem, the optimality is measured in terms of the Riemannian gradient norm of $F(x)$ . ", "page_idx": 3}, {"type": "text", "text": "Definition 2 ( $\\epsilon$ -stationary point). We call $x\\in\\mathcal{M}_{x}$ an $\\epsilon$ -stationary point of bilevel optimization (1) if it satisfies $\\|\\mathcal{G}F(x)\\|_{x}^{2}\\leq\\epsilon$ . ", "page_idx": 3}, {"type": "text", "text": "Assumption 1. All the iterates in the lower level problem are bounded in a compact subset that contains the optimal solution, i.e., there exists a constants $D_{k}>0$ , for all $k$ such that $\\bar{d}(y_{k}^{s},y^{*}(x_{k}))\\leq$ $D_{k}$ for all $s$ . Such a neighbourhood has unique geodesic. We take $\\bar{D}:=\\operatorname*{max}_{k}\\{D_{1},...,D_{k}\\}$ . ", "page_idx": 3}, {"type": "text", "text": "Assumption 2. Function $f(x,y)$ has bounded Riemannian gradients, i.e., $\\|\\mathcal{G}_{y}f(x,y)\\|_{y}\\,\\le\\,M$ , $\\|\\mathcal{G}_{x}f(\\bar{x_{}},y)\\|_{x}\\leq M$ for all $(x,y)\\in\\mathcal{U}$ and the Riemannian gradients are $L$ -Lipschitz in $\\boldsymbol{\\mathcal{U}}$ . ", "page_idx": 3}, {"type": "text", "text": "Assumption 3. Function $g(x,y)$ is $\\mu$ -geodesic strongly convex in $y\\,\\in\\,\\mathcal{U}_{y}$ for any $x\\in\\mathcal{U}_{x}$ and has $L$ Lipschitz Riemannian gradient $\\mathcal{G}_{x}g(x,y),\\mathcal{G}_{y}g(x,y)$ in $\\boldsymbol{\\mathcal{U}}$ . Further, the Riemannian Hessian $\\mathcal{H}_{y}g(x,y)$ , cross derivatives $\\mathcal{G}_{x y}^{2}g(x,y)$ , $\\mathcal{G}_{y x}^{2}g(x,y)$ are $\\rho$ -Lipschitz in $\\boldsymbol{\\mathcal{U}}$ . ", "page_idx": 3}, {"type": "text", "text": "Assumption 1 is standard in Riemannian optimization literature by properly bounding the domain of variables, which allows to express Riemannian distance in terms of (inverse) Exponential map. Also, the boundedness of the domain implies the bound on curvature, as is required for analyzing convergence for geodesic strongly convex lower-level problems [41, 71]. Assumptions 2 and 3 are common regularity conditions imposed on $f$ and $g$ in the bilevel optimization literature. This translates into the smoothness of the function $F$ and $\\mathrm{D}{\\boldsymbol{y}}^{*}({\\boldsymbol{x}})$ (discussed in Appendix C.3). ", "page_idx": 3}, {"type": "text", "text": "Table 1: Comparison of first-order and second-order complexities for reaching $\\epsilon$ -stationarity. For stochastic algorithms, including HGD-NS, RSHGD-HINV, the complexities are measured with respect to the component functions $f_{i},g_{i}$ . Here, $G_{f},G_{g}$ are the gradient complexities of function $f,g$ , respectively, to reach an $\\epsilon$ -stationary point of (1). Also, we denote $J V_{g}$ , $H V_{g}$ as the complexity of computing the second-order cross derivative and Hessian-vector product of function $g$ . ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\frac{\\mathrm{Methods}}{\\mathrm{HGD-CG}\\,[40]}\\qquad\\begin{array}{c c c c}{G_{f}}&{G_{g}}&{J V_{g}}&{H V_{g}}\\\\ {\\big|O(\\kappa_{1}^{3}\\epsilon^{-1})}&{\\tilde{O}(\\kappa_{1}^{4}\\epsilon^{-1})}&{O(\\kappa_{1}^{3}\\epsilon^{-1})}&{\\tilde{O}(\\kappa_{1}^{3}\\xi\\epsilon^{-1})}\\\\ {\\big|O(\\kappa_{2}^{3}\\epsilon^{-1})}&{\\tilde{O}(\\kappa_{1}^{4}\\epsilon^{-1})}&{\\tilde{O}(\\kappa_{1}^{4}\\epsilon^{-1})}&{\\tilde{O}(\\kappa_{1}^{4}\\epsilon^{-1})}\\\\ {\\big|\\tilde{O}(\\kappa_{1}^{5}\\epsilon^{-2})}&{\\tilde{O}(\\kappa_{1}^{9}\\epsilon^{-2})}&{O(\\kappa_{1}^{5}\\epsilon^{-2})}&{\\tilde{O}(\\kappa^{6}\\epsilon^{-2})}\\end{array}}\\\\ &{}&{\\frac{\\mathrm{SHGD-HINS}}{\\mathrm{RHGD-HINN}}\\qquad\\begin{array}{c c c c}{O(\\kappa_{1}^{3}\\epsilon^{-1})}&{\\tilde{O}(\\kappa_{1}^{5}\\epsilon^{-1})}&{O(\\kappa_{1}^{3}\\epsilon^{-1})}&{\\mathrm{NA}}\\\\ {O(\\kappa_{1}^{4}\\epsilon^{-1})}&{\\tilde{O}(\\kappa_{1}^{6}\\zeta\\epsilon^{-1})}&{O(\\kappa_{1}^{4}\\epsilon^{-1})}&{\\tilde{O}(\\kappa_{1}^{4}\\epsilon^{-1})}\\\\ {O(\\kappa_{1}^{3}\\epsilon^{-1})}&{\\tilde{O}(\\kappa_{1}^{5}\\zeta\\epsilon^{-1})}&{O(\\kappa_{1}^{3}\\epsilon^{-1})}&{\\tilde{O}(\\kappa_{1}^{4}\\epsilon^{-1}).}\\end{array}}\\\\ &{}&{\\qquad\\qquad\\cdot\\mathrm{AD}}\\\\ &{}&{\\qquad\\qquad\\qquad\\cdot\\mathrm{AD}}\\\\ &{}&{\\mathrm{RSHGD-HINN}\\qquad\\Big|O(\\kappa_{1}^{5}\\epsilon^{-1})}&{\\tilde{O}(\\kappa_{1}^{5}\\zeta\\epsilon^{ \n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We first bound the estimation error of the proposed schemes of approximated hypergradient as follows. For the hypergradient computed by automatic differentiation, we highlight that due to the presence of exponential map in the chain of differentiation, it is non-trivial to explicitly express $\\dot{\\mathrm{D}_{x_{k}}}y_{k}^{S}$ . Here, we adopt the property of exponential map (which is locally linear) in the ambient space [1], i.e., $\\mathrm{Exp}_{x}(\\dot{u})=x^{\\dot{+}}\\dot{u}+\\dot{O}(\\|u\\|_{x}^{\\dot{2}})$ . This requires the use of tangent space projection of $\\xi$ in the ambient space as $\\mathcal{P}_{x}(\\xi)$ , which is solved for the $v$ such that $\\langle v,\\xi\\rangle_{x}\\doteq\\langle u,\\bar{\\xi}\\rangle$ for any $\\xi\\in T_{x}\\mathcal{M}$ . ", "page_idx": 4}, {"type": "text", "text": "For notation simplicity, we denote $\\begin{array}{r}{\\kappa_{l}:=\\frac{L}{\\mu}}\\end{array}$ and $\\begin{array}{r}{\\kappa_{\\rho}:=\\frac{\\rho}{\\mu}}\\end{array}$ . For analysis, we consider $\\kappa_{\\rho}=\\Theta(\\kappa_{l})$ . ", "page_idx": 4}, {"type": "text", "text": "Lemma 1 (Hypergradient approximation error bound). Under Assumptions 1, 2, 3, we can bound the error for approximated hypergradient as ", "page_idx": 4}, {"type": "text", "text": "1. HINV: $\\|\\widehat{\\mathcal{G}}_{\\mathrm{hinv}}F(x_{k})-\\mathcal{G}F(x_{k})\\|_{x_{k}}\\le(L+\\kappa_{\\rho}M+\\kappa_{l}L+\\kappa_{l}\\kappa_{\\rho}M)d\\big(y^{*}(x_{k}),y_{k+1}\\big).$ 2. CG: $\\begin{array}{r l}&{\\|\\widehat{\\mathcal{G}}_{\\mathrm{cg}}F(x_{k})-\\mathcal{G}F(x_{k})\\|_{x_{k}}\\,\\le\\,\\big(L+\\kappa_{\\rho}M+L\\big(1+2\\sqrt{\\kappa_{l}}\\big)\\big(\\kappa_{l}+\\frac{M\\kappa_{\\rho}}{\\mu}\\big)\\big)d(y^{*}(x_{k}),y_{k+1})\\,+\\,}\\end{array}$ $\\begin{array}{r}{2L\\sqrt{\\kappa_{l}}\\Big(\\frac{\\sqrt{\\kappa_{l}}-1}{\\sqrt{\\kappa_{l}}+1}\\Big)^{T}\\lVert\\hat{v}_{k}^{0}-\\Gamma_{y^{*}(x_{k})}^{y_{k+1}}v_{k}^{*}\\rVert_{y_{k+1}}}\\end{array}$ , where $v_{k}^{*}=\\mathcal{H}_{y}^{-1}g(x_{k},y^{*}(x_{k}))[\\mathcal{G}_{y}f(x_{k},y^{*}(x_{k}))]$ . 3. NS: $\\begin{array}{r}{\\|\\widehat{\\mathcal{G}}_{\\mathrm{ns}}F(x_{k})-\\mathcal{G}F(x_{k})\\|_{x_{k}}\\leq(L+\\kappa_{l}L+\\kappa_{\\rho}M+\\kappa_{l}\\kappa_{\\rho}M)d(y^{*}(x_{k}),y_{k+1})+\\kappa_{l}M(1-\\gamma\\mu)^{T}.}\\end{array}$ 4. $A D$ : Suppose further there exist $C_{1},C_{2},C_{3}~>~0$ such that $\\|\\mathrm{D}_{x_{k}}y_{k}^{s}\\|_{y_{k}^{s}}\\;\\le\\;C_{1}$ , $\\parallel\\!\\Gamma_{x}^{y}\\mathcal{P}_{x}v\\mathrm{~-~}$ $v\\|_{y}~\\leq~C_{2}d(x,y)\\|v\\|_{y}$ and $\\mathrm{D}_{x}\\mathrm{Exp}_{x}(u)\\;=\\;\\mathcal{P}_{\\mathrm{Exp}_{x}(u)}\\big(\\mathrm{id}\\,+\\,\\mathrm{D}_{x}u\\big)\\,+\\,\\mathcal{E}$ where $\\|\\mathcal{E}\\|_{\\mathrm{Exp}_{x}(u)}~\\le$ $C_{3}\\|\\boldsymbol{\\mathrm{D}}_{x}\\boldsymbol{u}\\|_{x}\\|\\boldsymbol{u}\\|_{x}$ for any $x,y\\in\\mathcal{U}$ and $v\\in T_{y}\\M_{y},$ , $u\\in T_{x}\\mathcal{M}_{x}$ . Then, $\\begin{array}{r l}&{\\|\\widehat{\\mathcal{G}}_{\\mathrm{ad}}F(x_{k})-\\mathcal{G}F(x_{k})\\|_{x_{k}}\\,\\le\\,\\bigl(\\frac{2M\\widetilde{C}}{\\mu-\\eta_{y}\\zeta L^{2}}+L(1+\\kappa_{l})\\bigr)(1+\\eta_{y}^{2}\\zeta L^{2}-\\eta_{y}\\mu)^{\\frac{S-1}{2}}d\\bigl(y_{k},y^{*}(x_{k})\\bigr)\\,+}\\\\ &{M\\kappa_{l}(1-\\eta_{y}\\mu)^{S},\\,w h e r e\\,\\widetilde{C}:=(\\kappa_{l}+1)\\rho+(C_{2}+\\eta_{y}C_{3})L\\bigl((1-\\eta_{y}\\mu)C_{1}+\\eta_{y}L\\bigr).}\\end{array}$ ", "page_idx": 4}, {"type": "text", "text": "From Lemma 1, it is evident that the exact Hessian inverse exhibits the tightest bound, which is followed by conjugate gradient (CG) and truncated Neumann series (NS). Automatic differentiation (AD) presents the worst upper bound on the error due to the introduction of curvature constant $\\zeta$ , resulting in $\\begin{array}{r}{(1-\\Theta(\\frac{\\mu^{2}}{L^{2}\\zeta}))^{S}=(1-\\Theta(\\frac{1}{\\kappa_{l}^{2}\\zeta}))^{S}}\\end{array}$ for the trailing term, which could be much larger than $\\begin{array}{r}{(1-\\gamma\\mu)^{T}=(1-\\Theta(\\frac{1}{\\kappa_{l}}))^{T}}\\end{array}$ for NS and $\\begin{array}{r}{\\left(\\frac{\\sqrt{\\kappa_{l}}-1}{\\sqrt{\\kappa_{l}}+1}\\right)^{T}\\,=\\,(1-\\Theta(\\frac{1}{\\sqrt{\\kappa_{l}}}))^{T}}\\end{array}$ for CG. Further, the error critically relies on the number of inner iterations $S$ compared with $T$ for CG and NS, and the constants $C_{1},C_{2},C_{3}$ can be large for manifolds with high curvature. We now present the main convergence result with the four proposed hypergradient estimation strategies. ", "page_idx": 4}, {"type": "text", "text": "Theorem 1. Denote $\\Delta_{0}:=F(x_{0})+d^{2}(y_{0},y^{\\ast}(x_{0}))$ and $\\begin{array}{r}{L_{F}:=\\left(\\frac{L}{\\mu}+1\\right)\\left(L+\\frac{\\tau M}{\\mu}+\\frac{\\rho L M}{\\mu^{2}}+\\frac{L^{2}}{\\mu}\\right)=}\\end{array}$ . Under Assumptions $^{\\,l}$ , 2, 3, we have the following bounds on the hypergradient norm obtained by Algorithm $^{\\,I}$ . ", "page_idx": 4}, {"type": "text", "text": "\u2022 HINV: Let \u03b7x = $\\begin{array}{r}{\\eta_{x}=\\frac{1}{20L_{F}}}\\end{array}$ and $S\\ge\\widetilde\\Theta(\\kappa_{l}^{2}\\zeta)$ . We have $\\begin{array}{r}{\\operatorname*{min}_{k=0,\\ldots,K-1}\\|\\mathcal{G}F(x_{k})\\|_{x_{k}}^{2}\\leq80L_{F}\\Delta_{0}/K.}\\end{array}$ ", "page_idx": 4}, {"type": "text", "text": "$\\begin{array}{r}{\\Lambda\\,:=\\,C_{v}^{2}+\\kappa_{l}^{2}(\\frac{5M^{2}C_{0}^{2}D^{2}}{\\mu}+1)}\\end{array}$ M\u00b5\u03ba\u03c1+ M\u03ba\u00b5\u03c1\u03bal+ \u03bal2 + \u03bal. Choosing $\\begin{array}{r l r}{\\eta_{x}}&{{}=}&{\\frac{1}{24\\Lambda}}\\end{array}$ $S~\\ge~{\\widetilde\\Theta}(\\kappa_{l}^{2}\\zeta)$ $T_{\\mathrm{cg}}~\\geq~{\\widetilde\\Theta}(\\sqrt{\\kappa_{l}})$ $\\begin{array}{r l}{\\operatorname*{min}_{k=0,\\dots,K-1}\\|{\\mathcal{G}}F(x_{k})\\|_{x_{k}}^{2}}&{\\le}\\end{array}$ $\\begin{array}{r}{\\frac{96\\Lambda}{K}\\big(\\Delta_{0}+\\|v_{0}^{*}\\|_{y^{*}(x_{0})}^{2}\\big)}\\end{array}$ .   \n\u2022 NS: Choosing $\\begin{array}{r}{\\eta_{x}=\\frac{1}{20L_{F}},S\\geq\\widetilde\\Theta(\\kappa_{l}^{2}\\zeta),}\\end{array}$ , and $\\begin{array}{r}{T_{\\mathrm{ns}}\\ge\\widetilde\\Theta(\\kappa\\log(\\frac{1}{\\epsilon}))}\\end{array}$ for an arbitrary $\\epsilon>0$ , we have $\\begin{array}{r}{\\operatorname*{min}_{k=0,\\dots,K-1}\\|\\mathcal G F({\\boldsymbol x}_{k})\\|_{{\\boldsymbol x}_{k}}^{2}\\leq\\frac{80L_{F}}{K}\\Delta_{0}+\\frac{\\epsilon}{2}}\\end{array}$ .   \n\u2022 $A D$ : Choosing $\\begin{array}{r l r}{\\eta_{x}}&{{}=}&{\\frac{1}{20L_{F}}}\\end{array}$ 201LF and S \u2265 \u0398 (\u03bal2 \u03b6 log( \u03f51 )) for an arbitrary \u03f5 > 0, we have $\\begin{array}{r}{\\operatorname*{min}_{k=0,\\dots,K-1}\\|\\mathcal G F({\\boldsymbol x}_{k})\\|_{{\\boldsymbol x}_{k}}^{2}\\leq\\frac{80L_{F}}{K}\\Delta_{0}+\\frac{\\epsilon}{2}}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "Complexity analysis. Based on the convergence guarantees in Theorem 1, we have analyzed (in Corollary 1), the computational complexity of the proposed algorithm with four different hypergradient estimation strategies in reaching the $\\epsilon_{\\mathrm{:}}$ -stationary point. The results are summarized in Table 1. For reference, we also provide the computational cost of Euclidean algorithms which solve bilevel Euclidean optimization problem [40]. We notice that except for CG, the gradient complexity for $f$ (i.e., $G_{f}$ ) matches the Euclidean version. For conjugate gradient, the complexity is higher by $O(\\kappa_{l})$ , which is due to the additional distortion from the use of vector transport when tracking the error of conjugate gradient at each epoch. In terms of gradient complexity for $g$ (i.e., $G_{g}^{\\ \\ }$ ), all deterministic methods require a higher complexity by at least $\\widetilde{\\cal O}(\\kappa_{l}\\zeta)$ compared to the Euclidean baselines. This is because of the curvature distortion when analy zing the convergence for geodesic strongly convex functions. Similar comparisons can be also made with respect to the computations of cross-derivatives and Hessian vector products. ", "page_idx": 5}, {"type": "table", "img_path": "LvNDqNJKlD/tmp/e8cfe41550fbe11702cb4f6aa29e93dcfab658bb6e964c1ccba06aef47082034.jpg", "table_caption": ["3.3 Extension to stochastic bilevel optimization "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "In this section, we consider the bilevel optimization problem (1) in the stochastic setting, where $\\begin{array}{r}{f(\\dot{x},y^{\\ast}(x)):=\\frac{1}{n}\\sum_{i=1}^{n}f_{i}(x,y^{\\ast}(x))}\\end{array}$ and $\\begin{array}{r}{g(x,y)\\;:=\\;\\frac{1}{m}\\sum_{i=1}^{m}g_{i}(x,y)}\\end{array}$ . The algorithm for solving the stochastic bilevel optimization problem is in Algorithm 2, where we sample $B_{1},B_{2},B_{3},B_{4}$ afresh every iteration. The batch index is omitted for clarity. The batches are sampled uniformly at random with replacement such that the mini-batch gradient is an unbiased estimate of the full gradient. Here, we denote $\\begin{array}{r}{f_{\\mathcal{B}}(x,y):=\\frac{1}{|\\mathcal{B}|}\\sum_{i\\in\\mathcal{B}}{f_{i}(x,y)}}\\end{array}$ and similarly for $g$ . We let $[n]:=\\{1,\\ldots,n\\}$ . In Step 10 of Algorithm 2, we can employ any hypergradient estimator proposed in Section 3.1. In this work, we only show convergence un", "page_idx": 5}, {"type": "text", "text": "der the Hessian inverse approximation of hypergradient, i.e., $\\widehat{\\mathcal{G}}F(x_{k})\\;=\\;\\mathcal{G}_{x}f_{\\mathcal{B}_{2}}(x_{k},\\breve{y}_{k+1})\\;-$ $\\mathcal{G}_{x y}^{2}g_{8_{3}}(x_{k},y_{k+1})[\\mathcal{H}_{y}^{-1}g_{B_{4}}(\\bar{x}_{k},y_{k+1})[\\mathcal{G}_{y}f_{B_{2}}(\\bar{x_{k}},y_{k+1})]]$ . Similar  analysis can be followed for other approximation strategies. The theoretical guarantees are in Theorem 2, where we require Assumption 4, which is common in existing works for analyzing stochastic algorithms on Riemannian manifolds [42, 23, 22]. ", "page_idx": 5}, {"type": "text", "text": "Assumption 4. Under stochastic setting, Assumption 1 holds and Assumptions 2, 3 are satisfied for component functions $f_{i}(x,y),g_{j}(x,y)$ , for all $i\\in[n],j\\in[m]$ . Further, stochastic gradient, Hessian, and cross derivatives are unbiased estimates. ", "page_idx": 5}, {"type": "text", "text": "Theorem 2. Under Assumption $^{4}$ , consider Algorithm 2. Suppose we choose $\\begin{array}{r}{\\eta_{x}\\,=\\,\\frac{1}{20L_{F}},S\\,\\ge\\,}\\end{array}$ $\\widetilde{\\Theta}(\\kappa_{l}^{2}\\zeta)$ , and $|\\beta_{1}|,|\\beta_{2}|,|\\beta_{3}|,|\\beta_{4}|\\;\\;\\geq\\;\\;\\Theta(\\kappa_{l}^{2}\\epsilon^{-1})$ for an arbitrary $\\epsilon\\mathrm{~\\ensuremath~{~>~}~0~}$ . Then we have $\\begin{array}{r}{\\operatorname*{min}_{k=0,\\dots,K-1}\\mathbb{E}\\|\\mathcal{G}F(x_{k})\\|_{x_{k}}^{2}\\,\\le\\,\\frac{80L_{F}\\Delta_{0}}{K}\\,+\\,\\frac{\\epsilon}{2}}\\end{array}$ and the gradient complexity to reach $\\epsilon$ -stationary solution is $G_{f}=O(\\kappa_{l}^{5}\\epsilon^{-2}),G_{g}=\\widetilde O(\\kappa_{l}^{9}\\zeta\\epsilon^{-2}),J V_{g}=O(\\kappa_{l}^{5}\\epsilon^{-2}).$ . ", "page_idx": 5}, {"type": "text", "text": "In Table 1, we compare our attained complexities with that of stocBiO [40], which makes use of a truncated Neumann series. With exact Hessian inverse, we can match the $G_{f}$ and $J V_{g}$ complexities with stocBio. For the $G_{g}$ complexity, the additional curvature constant is inevitable from the convergence analysis for geodesic strongly convex functions. Nevertheless we observe the same order dependency on $\\kappa_{l}$ . This is mainly due to the analysis where we choose a smaller stepsize $\\begin{array}{r}{\\eta_{y}=\\Theta\\bar{(}\\frac{\\mu}{L^{2}})}\\end{array}$ compared to $\\Theta\\big(\\frac{2}{L\\!+\\!\\mu}\\big)$ in [40]. The larger stepsize, despite increasing the convergence rate, also increases the variance under stochastic setting. We believe an order of $\\Theta(\\kappa_{l})$ lower can be established for stocBio, following our analysis. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "3.4 Extension to retraction ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Our analysis till now has been limited to the use of the exponential map. However, the retraction mapping is often preferred over the exponential map due to its lower computational cost. Here, we show that use of retraction in our algorithms also leads to similar convergence guarantees. ", "page_idx": 6}, {"type": "text", "text": "Assumption 5. There exist constants $\\overline{{c}}\\geq1,c_{R}\\geq0$ such that $d^{2}(x,y)\\leq\\bar{c}\\|u\\|_{x}^{2}$ and $\\Vert\\mathrm{Exp}_{x}^{-1}(y)-$ $u\\|_{x}\\leq\\bar{c}_{R}\\|u\\|^{2}$ , for any $x,y=\\mathrm{Retr}_{x}(u)\\in\\mathcal{U}$ . ", "page_idx": 6}, {"type": "text", "text": "Assumption 5 is standard (e.g. in [42, 23]) in bounding the error between exponential map and retraction given that retraction is a first-order approximation to the exponential map. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3. Suppose Assumptions 1, 2, 3 and 5 hold and let $\\widetilde{L}_{F}=4\\kappa_{l}c_{R}M+5\\bar{c}L_{F}$ . Then consider Algorithm 1 with exponential map replaced with general r etraction. We can obtain the following bounds. ", "page_idx": 6}, {"type": "text", "text": "\u2022 HINV: Le $\\begin{array}{r}{t\\;\\eta_{x}=\\Theta(1/\\tilde{L}_{F}),S\\ge\\widetilde{\\Theta}(\\kappa_{l}^{2}\\zeta).\\;T h e n\\operatorname*{min}_{k=0,\\ldots,K-1}\\|\\mathcal{G}F(x_{k})\\|_{x_{k}}^{2}\\le16\\widetilde{L}_{F}\\Delta_{0}/K.}\\end{array}$   \n\u2022 $\\eta_{x}=\\Theta(1/\\widetilde{\\Lambda}),S\\geq\\widetilde{\\Theta}(\\kappa_{l}^{2}\\zeta),T_{\\mathrm{cg}}\\geq\\widetilde{\\Theta}(\\sqrt{\\kappa_{l}}).$ here $\\begin{array}{r}{\\widetilde{\\Lambda}=C_{v}^{2}\\bar{c}+\\kappa_{l}^{2}(\\frac{5M^{2}C_{0}^{2}\\bar{D}^{2}}{\\mu}+\\bar{c})}\\end{array}$ . Then $\\begin{array}{r}{\\operatorname*{min}_{k=0,\\ldots,K-1}\\|\\mathcal{G}F(x_{k})\\|_{x_{k}}^{2}\\leq\\frac{96\\tilde{\\Lambda}}{K}(\\Delta_{0}+\\|v_{0}^{*}\\|_{y^{*}(x_{0})}^{2}).}\\end{array}$   \n\u2022 NS: Let $\\eta_{x}=\\Theta(1/\\widetilde{L}_{F}),S\\ge\\widetilde{\\Theta}(\\kappa_{l}^{2}\\zeta)$ . Then for an arbitrary $\\epsilon>0$ , $T_{\\mathrm{ns}}\\,\\geq\\,\\widetilde\\Theta(\\kappa_{l}\\log(1/\\epsilon))$ , we   \nhave $\\begin{array}{r}{\\operatorname*{min}_{k=0,\\ldots,K-1}\\|\\mathcal{G}F({\\boldsymbol x}_{k})\\|_{{\\boldsymbol x}_{k}}^{2}\\leq\\frac{16\\widetilde{L}_{F}}{K}\\Delta_{0}+\\frac{\\epsilon}{2}}\\end{array}$ .   \n\u2022 AD: Let $\\eta_{x}~=~\\Theta(1/\\widetilde{L}_{F})$ , $S\\;\\ge\\;\\widetilde\\Theta(\\kappa_{l}^{2}\\zeta\\log(1/\\epsilon))$ . Then for an arbitrary $\\epsilon\\mathrm{~>~0,~}$ , we have mink=0,...,K\u22121 \u2225GF(xk)\u22252xk \u226416KL F \u22060 + \u03f5. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3 demonstrates that employing a general retraction preserves the same order of convergence and complexity as the exponential map in Theorem 1. This is due to the fact that $\\widetilde{\\cal L}_{F}=\\Theta({\\cal L}_{F})$ and $\\widetilde{\\Lambda}=\\Theta(\\Lambda)$ , where $L_{F}$ and $\\Lambda$ are as defined in Theorem 1. In addition, when exponential map is used, Theorem 3 recovers the results in Theorem 1 as $c_{R}=0$ and $\\bar{c}=1$ . ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "This section explores various applications of bilevel optimization problems over manifolds. All the experiments are implemented based on Geoopt [44] and the codes are available at https: //github.com/andyjm3/rhgd. ", "page_idx": 6}, {"type": "text", "text": "4.1 Synthetic problem ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We consider the following bilevel optimization problem on the Stiefel manifold $\\mathrm{St}(d,r)=\\{\\mathbf{W}\\in\\qquad{\\mathrm{we}}$ $\\mathbb{R}^{d\\times r}:\\mathbf{W}^{\\top}\\mathbf{W}=\\mathbf{I}_{r}\\right\\rbrace$ and SPD manifold $\\mathbb{S}_{++}^{d}=\\left\\{\\mathbf{M}\\in\\mathbb{R}^{d\\times d}:\\mathbf{M}\\succ0\\right\\}$ (in Appendix A): $\\operatorname*{max}_{\\mathbf{w}\\in\\mathrm{St}(d,r)}\\mathrm{tr}(\\mathbf{M}^{*}\\mathbf{X}^{\\top}\\mathbf{Y}\\mathbf{W}^{\\top}),\\quad\\mathrm{~s.t.~}\\mathbf{M}^{*}=\\underset{\\mathbf{M}\\in\\mathbb{S}_{++}^{d}}{\\arg\\operatorname*{min}}\\ \\left\\langle\\mathbf{M},\\mathbf{X}^{\\top}\\mathbf{X}\\right\\rangle+\\left\\langle\\mathbf{M}^{-1},\\mathbf{W}\\mathbf{Y}^{\\top}\\mathbf{Y}\\mathbf{W}^{\\top}+\\nu\\mathbf{I}\\right\\rangle,$ where $\\mathbf{X}\\in\\mathbb{R}^{n\\times d},\\mathbf{Y}\\in\\mathbb{R}^{n\\times r}$ , with $n\\geq d\\geq r$ , are given matrices and $\\nu>0$ is the regularization parameter. The above is a synthetically constructed problem that aims to maximize the similarity between $\\mathbf{X}$ and $\\mathbf{Y}$ in different feature dimensions. We align $\\mathbf{X}$ and $\\mathbf{Y}$ to the same dimension via $\\mathbf{W}\\in\\mathrm{St}(d,r)$ and also learn an appropriate geometric metric $\\mathbf{M}\\in\\mathbb{S}_{++}^{d}$ in the lower-level problem [69]. The geodesic convexity of the lower-level problem and the Hessian inverse expression are discussed in Appendix H.1. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "image", "img_path": "LvNDqNJKlD/tmp/524e6713e4a15729bb2cc1f68af5fe2cf518f6499390ced24339acc8c5058901.jpg", "img_caption": ["Figure 1: Figures (a) & (b) show the plot of objective of the upper-level problem (Upper Objective) for different strategies. HINV and CG strategies have fastest convergence, followed by NS and AD. The corresponding estimation errors are shown in (c). Figure (d) specifically shows the robustness of approximation error obtained by NS across different $\\gamma$ and $T$ values. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Results. We generate random data matrices $\\mathbf{X},\\mathbf{Y}$ with $n\\,=\\,100,d\\,=\\,50$ , and $r\\,=\\,20$ . We set $\\nu=0.01$ and fix $\\eta_{x}=\\eta_{y}=0.5$ . We compare the three proposed strategies for approximating the hypergradient where we select $\\gamma=1.0$ and $T_{\\mathrm{ns}}=50$ for Neumann series (NS) and set maximum iterations $T_{\\mathrm{cg}}$ for conjugate gradient (CG) to be 50 and break once the residual reaches a tolerance of $10^{-10}$ . We set the number of outer iterations (epochs) $K$ to be 200. Figure 1 compares RHGD with different approximation strategies implemented with $S=20$ or 50 number of inner iterations. ", "page_idx": 7}, {"type": "text", "text": "4.2 Hyper-representation over SPD manifolds ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Hyper-representation [54, 61] aims to solve a regression/classification task while searching for the best representation of the data. It can be formulated as a bilevel optimization problem, where the lower-level optimizes the regression/classification parameters while the upper-level searches for the optimal embedding of the inputs. Suppose we are given a set of SPD matrices, $\\mathbf{\\mathcal{D}}=\\{{\\bf A}_{i}\\}_{i=1}^{n}$ where Ai \u2208Sd++ and the task is to learn a low-dimensional embedding of $\\mathbf{A}_{i}$ while remaining close to their semantics labels. In particular, we partition the set into a training set $\\mathcal{D}_{\\mathrm{tr}}$ and validation set $\\mathcal{D}_{\\mathrm{val}}$ . ", "page_idx": 7}, {"type": "text", "text": "Shallow hyper-representation for regression. We consider a shallow learning paradigm over $\\mathcal{D}$ through the regression task. The representation is parameterized with $\\mathbf{W}^{\\top}\\mathbf{\\bar{A}}_{i}^{\\top}\\mathbf{W}$ for $\\textbf{W}\\in$ $\\operatorname{St}(d,r)$ . The requirement of orthogonality on W follows [38, 30, 33] that ensures the learned representations are SPD. The learned representation is then transformed to a Euclidean space for performing regression, namely through a matrix logarithm (that acts as a bijective map between the space of SPD matrices and symmetric matrices) and a vectorization operation $\\operatorname{vec}(\\cdot)$ that extract the upper-triangular part of the symmetric matrix. The bilevel optimization problem is ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\mathbf{w}\\in\\mathrm{St}(d,r)}{\\operatorname*{min}}\\sum_{i\\in\\mathcal{D}_{\\mathrm{val}}}\\frac{(\\mathrm{vec}(\\log\\mathrm{m}(\\mathbf{W}^{\\top}\\mathbf{A}_{i}\\mathbf{W}))\\beta^{*}-y_{i})^{2}}{2|\\mathcal{D}_{\\mathrm{val}}|},}\\\\ &{\\;\\mathrm{s.t.}\\;\\beta^{*}=\\underset{\\beta\\in\\mathbb{R}^{r(r+1)/2}}{\\operatorname{arg\\,min}}\\sum_{i\\in\\mathcal{D}_{\\mathrm{tr}}}\\frac{(\\mathrm{vec}(\\log\\mathrm{m}(\\mathbf{W}^{\\top}\\mathbf{A}_{i}\\mathbf{W}))\\beta-y_{i})^{2}}{2|\\mathcal{D}_{\\mathrm{tr}}|}+\\frac{\\lambda}{2}\\|\\beta\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The regularization $\\lambda>0$ ensures the lower-level problem is strongly convex. The upper-level problem is on the validation set while the lower-level problem is on the training set. We generate random W, $\\mathbf{A}_{i}$ and $\\beta$ and construct $\\mathbf{y}$ with $y_{i}=\\operatorname{vec}(\\log\\!\\mathrm{m}(\\mathbf{W}^{\\top}\\mathbf{A}_{i}\\mathbf{W}))\\beta+\\epsilon_{i}$ , where $\\bar{\\epsilon_{i}}\\sim\\mathcal{N}(0,1)$ . We generate $200\\;{\\bf A}_{i}$ with $\\left|\\mathcal{D}_{\\mathrm{val}}\\right|=100$ and $|\\mathcal{D}_{\\mathrm{tr}}|=100$ . In Figure 2a, we show the loss on validation set (the upper loss) in terms of number of outer iterations. We compare both the deterministic (RHGD) and stochastic (RSHGD) versions of Riemannian hypergradient descent. We again observe that the best performance is attained by either the ground-truth Hessian inverse or the conjugate gradient. NS requires carefully selecting the hyperparameters $\\gamma,T$ , which pose difficulties in real applications. For the stochastic versions, all the methods perform similarly. ", "page_idx": 7}, {"type": "text", "text": "Deep hyper-representation for classification. We now explore a 2-layer SPD network [38] for classifying ETH-80 image set [46]. The dataset consists of 8 classes, each with 10 objects. Each object is represented by an image set consisting of images taken from different viewing angles. Here, we represent each image set by taking the covariance matrix of the images in the same set after resizing them into $10\\times10$ . This results in 80 SPD matrices ${\\bf A}_{i}$ of size $100\\times100$ for classification. Let $\\boldsymbol{\\Phi}(\\mathbf{\\check{A}}_{i})=\\operatorname{vec}(\\log\\operatorname{m}(\\mathbf{W}_{2}^{\\top}\\operatorname{ReEig}(\\mathbf{W}_{1}^{\\top}\\mathbf{A}_{i}\\mathbf{W}_{1})\\mathbf{W}_{2}))$ ) be the output of the 2 layer network where $\\operatorname{ReEig}(\\mathbf{A})=\\mathbf{U}\\operatorname*{max}\\{\\epsilon\\mathbf{I},\\Sigma\\}\\mathbf{U}^{\\top}$ is the eigenvalue rectifying activation with the eigenvectors $\\mathbf{U}$ and eigenvalues $\\Sigma$ of A. We consider the same bilevel optimization as above except the least-squares loss function becomes the cross-entropy loss. Here we sample 5 samples from each class to form the training set and the rest as the validation set. We set $d_{1}=20,d_{2}=5$ , and fix learning rate to be 0.1 for both lower and upper problems. Figures 2b and 2c show the good performance on the validation accuracy (upper-level loss). ", "page_idx": 7}, {"type": "image", "img_path": "LvNDqNJKlD/tmp/2ca0fe8f0ae8b21bd98d680344980980664faa711476034f9f1797ef4a1949dd.jpg", "img_caption": ["Figure 2: Figures (a), (b), and (c) show the performance of RHGD on the hyper-representation problems on SPD networks. Figure (d) shows the good generalization performance of our proposed RHGD algorithms over the projected gradient PHGD baselines on the MiniImageNet dataset. "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "LvNDqNJKlD/tmp/947dfddc51f7d2224f5ab1acb21fe7e4c968bff3a19b9afc44e5d8c7bc8ecf22.jpg", "table_caption": ["Table 2: Classification accuracy on the Caltech-Office dataset. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "4.3 Riemannian meta learning ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Meta learning [16, 34] allows adaptation of models to new tasks with minimal amount of additional data and training, by distilling past learning experiences. A recent work [64] considers meta learning with orthogonality constraint. In particular, the upper-level optimization searches for the base parameters shared by all tasks while the lower level optimizes over the task-specific parameters to ensure generalization ability. Let $P_{7}$ denote the distribution of meta tasks and for each training epoch, we sample $m$ tasks $\\dot{\\mathcal{D}^{\\ell}}\\sim P_{\\mathcal{T}},\\ell=1,...,m$ . Each task is composed of a support and query set denoted by $\\mathcal{D}_{\\mathrm{s}}^{\\ell},\\bar{\\mathcal{D}}_{\\mathrm{q}}^{\\ell}$ , and the task is to learn a set of base parameters $\\Theta$ such that the model can quickly adapt to the query set from the support set by adjusting only a few parameters $w$ . For each task, the task-specific parameter $w_{\\ell}^{*}$ is learned from the support set, which is used to update the base parameters by minimizing the loss over the query set. In standard settings, $w_{\\ell}$ corresponds to the final linear layer of a neural network [39, 40]. Here, we adopt the setup with $w_{\\ell}$ to be the last layer parameters in the Euclidean space while enforcing $\\Theta$ on the Stiefel manifold. The problem of Riemannian metalearning is min\u0398\u2208St $\\begin{array}{r}{\\frac{1}{m}\\sum_{\\ell=1}^{m}\\mathcal{L}(\\Theta,\\bar{w}_{\\ell}^{*};\\mathcal{D}_{\\mathrm{q}}^{\\ell})}\\end{array}$ s.t. $\\begin{array}{r}{w_{\\ell}^{*}=\\arg\\operatorname*{min}_{w_{\\ell}}\\frac{1}{m}\\sum_{\\ell=1}^{m}\\mathcal{L}(\\Theta,w_{\\ell};\\mathcal{D}_{\\mathrm{s}}^{\\ell})+\\mathcal{R}(w_{\\ell})}\\end{array}$ , where $\\mathcal{D}_{\\mathrm{s}}^{\\ell}$ , $\\mathcal{D}_{\\mathbf{q}}^{\\ell}$ are the support and query sets for task $\\ell$ and $\\mathcal{R}(\\cdot)$ is a regularizer that ensures strong convexity of the lower-level problem. ", "page_idx": 8}, {"type": "text", "text": "Results. We consider 5-ways 5-shots meta learning over the MiniImageNet dataset [59] where the backbone network is a 4-block CNN with the kernel of the first 2 layers constrained to be orthogonal in terms of the output channel (following [48]). The kernel size is $3\\times3$ and we consider 16 output channels with a padding of 1. Each convolutional block consists of a convolutional layer, followed by a ReLU activation, a max-pooling and a batch normalization layer. $\\Theta$ , thus, has the dimension $(16*3*3)\\times16=144\\times16$ , which is constrained to the Stiefel manifold. ", "page_idx": 8}, {"type": "text", "text": "In Figure 2d, we plot the test accuracy averaged for over 200 tasks. We compare RHGD with an extrinsic update baseline PHGD, which projects the update from the Euclidean space to the Stiefel manifold at every iteration. We observe the RHGD converges faster compared to the extrinsic update PHGD, thereby showing the benefit of the Riemannian modeling. ", "page_idx": 8}, {"type": "text", "text": "4.4 Unsupervised domain adaptation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Given two marginals $\\pmb{\\mu}\\in\\mathbb{R}^{n},\\pmb{\\nu}\\in\\mathbb{R}^{m}$ with equal total mass, i.e., $\\mu^{\\top}\\mathbf{1}_{n}\\,=\\,\\nu^{\\top}\\mathbf{1}_{m}\\,=\\,1$ where we assume unit mass without loss of generality. Let $\\Pi(\\mu,\\nu):=\\{\\Gamma\\in\\mathbb{R}^{n\\times m}:\\mathbf{T}>0,\\mathbf{r}\\mathbf{1}_{m}=\\}$ ", "page_idx": 8}, {"type": "text", "text": "$\\boldsymbol{\\mu},\\mathbf{T}^{\\intercal}\\mathbf{1}_{n}=\\boldsymbol{\\nu}\\}$ be the set of doubly stochastic matrices with strictly positive entries. From [15, 57], it is known that the set forms a Riemannian manifold with the Fisher metric. ", "page_idx": 9}, {"type": "text", "text": "Given a supervised source dataset $\\textbf{X}\\in\\mathbb{R}^{n\\times d}$ and an unsupervised target dataset $\\textbf{Y}\\in\\,\\mathbb{R}^{m\\times d}$ $(n,m\\,\\geq\\,d)$ , we consider the unsupervised domain adaptation problem to classify target domain instances. Using the optimal transport framework [58, 12], we pose this as a bilevel problem: ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{min}_{\\Gamma\\in\\Pi(\\mu,\\nu)}\\langle\\Gamma,\\mathcal{C}(\\mathbf{X}\\mathbf{M}^{*-1/2},\\mathbf{Y}\\mathbf{M}^{*-1/2})\\rangle-\\lambda H(\\mathbf{F}),}\\\\ &{\\mathrm{s.t.}\\ \\mathbf{M}^{*}=\\arg\\operatorname*{min}_{\\mathbf{M}\\in\\mathbb{S}_{++}^{d}}\\alpha\\mathrm{dist}^{2}(\\mathbf{M},\\mathbf{X}^{\\top}\\mathbf{X})+(1-\\alpha)\\mathrm{dist}^{2}(\\mathbf{M},\\mathbf{Y}^{\\top}\\mathbf{T}^{\\top}\\mathbf{Y}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "where $H(\\Gamma)\\ \\ =\\ -\\langle\\Gamma,\\log\\Gamma\\rangle$ is the entropy regularization, $\\mathcal{C}(\\mathbf{X},\\mathbf{Y})\\;\\;=\\;\\;\\mathrm{diag}(\\mathbf{X}\\mathbf{X}^{\\top})\\mathbf{1}_{m}^{\\top}\\;+$ $\\mathbf{1}_{n}\\mathrm{diag}(\\mathbf{YY}^{\\top})\\mathbf{-2XY}^{\\top}$ is the pairwise squared distance matrix, and $\\Pi$ denotes the doubly stochastic manifold [15]. Here, dist is the geodesic distance between SPD matrices and $\\alpha\\in[0,1]$ . ", "page_idx": 9}, {"type": "text", "text": "The lower-level problem in (3) finds $\\mathbf{M}^{*}$ which is the weighted geometric mean between ${\\bf X}^{\\top}{\\bf X}$ and $\\mathbf{Y}^{\\top}\\mathbf{I}^{\\top}\\mathbf{I}\\mathbf{Y}\\mathbf{Y}$ [6]. Conceptually, learning of M allows to align the features of the source and target instances. The upper-level problem, on the other hand, minimizes the Mahalanobis distance between the source and target domain instances parameterized by $\\mathbf{M}^{*-1}$ . An interpretation is that the matrix $\\mathbf{M}^{*}$ leads to whitening of the data (i.e., $\\mathbf{X}\\mathbf{M}^{*\\,-1/2}$ is the whitened data) [5]. ", "page_idx": 9}, {"type": "text", "text": "After the transport plan $\\mathbf{T}^{*}$ is learned, we employ barycentric projection using $\\mathbf{T}^{*}$ to transport the source points to the target domain and and employ the nearest neighbour (1-NN) classifier for the target dataset classification . For barycentric projection, we project the source samples $\\mathbf{X}$ to the target Y by solving xi = arg minxi\u2208Rd $\\begin{array}{r}{\\mathbf{x}_{i}\\,=\\,\\arg\\operatorname*{min}_{\\mathbf{x}_{i}\\in\\mathbb{R}^{d}}\\sum_{j=1}^{m}\\bar{\\mathbf{I}_{i,j}^{*}}\\,\\bar{\\|}\\mathbf{M^{*}}^{-1/2}\\mathbf{x}_{i}-\\mathbf{M^{*}}^{-1/2}\\mathbf{y}_{j}\\,\\|^{2}\\,=\\,\\bar{\\mu}_{i}^{-1}(\\sum_{j=1}^{m}\\mathbf{I}_{i,j}^{*}\\bar{\\mathbf{y}_{j}})}\\end{array}$ Then, a nearest-neighbour (NN) classifier is used to classify the samples in the target given the source labels based on the distance computed with ${\\bf M}^{*}$ , i.e., $\\mathcal{C}(\\dot{\\mathbf{X}}\\mathbf{M}^{*\\,-1/2},\\mathbf{Y}\\mathbf{M}^{*\\,-1/2})$ . ", "page_idx": 9}, {"type": "text", "text": "Results. We consider the Caltech-Office dataset [20], which is commonly used for domain adaptation. The dataset contains images from four domains in ten classes, i.e., Amazon (A), the Caltech image dataset (C), DSLR (D), and Webcam (W), each with containing 958, 1123, 157, and 295 samples respectively. Hence, there are 12 domain adaptation tasks, e.g., $\\mathrm{A}{\\rightarrow}\\mathrm{D}$ implies A is the source and D is the target. Each domain has the same ten classes. The goal is to classify images from target domain given source domain. For preprocessing, we normalize the samples to have unit norm and reduce the dimensionality to 128 by mean pooling every 64 columns. ", "page_idx": 9}, {"type": "text", "text": "We compare our proposed bilevel approach (3) with single-level optimal transport baselines, i.e., solving $\\mathrm{min}_{\\mathbf{T}\\in\\Pi(\\pmb{\\mu},\\pmb{\\nu})}\\langle\\Gamma,\\mathcal{C}(\\mathbf{X},\\mathbf{Y})\\rangle-\\lambda H(\\mathbf{T})$ , followed by the same barycentric projection. Specifically, the baselines are: (1) optimal transport where $\\lambda=0$ (labelled as OT-EMD) and (2) optimal transport with the Sinkhorn algorithm (labelled as OT-SKH). OT-EMD employs the earth mover distance while OT-SKH employs the Sinkhorn distance [13]. We implement the two OT baselines with the POT Python library [17]. $\\lambda$ is tuned in OT-SKH for each source-target pair. The best validation results are obtained by setting $\\lambda=5\\times10^{-3}$ for all the problem pairs except the $\\mathrm{W}{\\rightarrow}\\mathrm{D}$ pair for which $\\lambda=10^{-3}$ gives the best result. For our proposed bilevel approach, we set $\\lambda=0$ and $\\alpha=0.5$ . ", "page_idx": 9}, {"type": "text", "text": "In Table 2, we observe that the proposed bilevel approach obtains better generalization performance than the baselines across all the tasks. This showcases the utility of learning the whitening metric ${{\\bf{M}}^{-1}}$ in a bilevel setting. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we have proposed a framework for tackling bilevel optimization over Riemannian manifolds. We discussed various hypergradient approximation strategies (conjugate gradients, truncated Neumann series, and automatic differentiation) and provide error bounds. Our proposed algorithms rely only on gradient updates and make use of retraction which scale well across problems. We illustrate the efficacy of the proposal approach in several machine learning applications. ", "page_idx": 9}, {"type": "text", "text": "Although in this work, we focus on geodesic strongly convex lower-level problems, our framework can be extended to relax such assumption to (geodesic) convexity with an extra strongly convex regularizer [2], or to (Riemannian) PL condition where a global minimizer exists [9]. Furthermore, we believe there is potential to improve the current results in stochastic bilevel optimization by reducing the strict requirements on batch size. Additionally, the dependency on the curvature constant could also be further optimized. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] P-A Absil, Robert Mahony, and Rodolphe Sepulchre. Optimization algorithms on matrix manifolds. Princeton University Press, 2008. [2] Jan Harold Alcantara and Akiko Takeda. Theoretical smoothing frameworks for general nonsmooth bilevel problems. Technical report, arXiv preprint arXiv:2401.17852, 2024.   \n[3] Foivos Alimisis, Antonio Orvieto, Gary B\u00e9cigneul, and Aurelien Lucchi. A continuous-time perspective for modeling acceleration in Riemannian optimization. In International Conference on Artificial Intelligence and Statistics (AISTATS), 2020. [4] Vincent Arsigny, Pierre Fillard, Xavier Pennec, and Nicholas Ayache. Geometric means in a novel vector space structure on symmetric positive-definite matrices. SIAM Journal on Matrix Analysis and Applications, 29(1):328\u2013347, 2007.   \n[5] Mikel Artetxe, Gorka Labaka, and Eneko Agirre. Generalizing and improving bilingual word embedding mappings with a multi-step framework of linear transformations. In the AAAI Conference on Artificial Intelligence (AAAI), 2018.   \n[6] Rajendra Bhatia. Positive definite matrices. In Positive Definite Matrices. Princeton university press, 2009.   \n[7] Henri Bonnel, L\u00e9onard Todjihound\u00e9, and Constantin Udri\u00b8ste. Semivectorial bilevel optimization on Riemannian manifolds. Journal of Optimization Theory and Applications, 167:464\u2013486, 2015.   \n[8] Nicolas Boumal. An introduction to optimization on smooth manifolds. Cambridge University Press, 2023.   \n[9] Lesi Chen, Jing Xu, and Jingzhao Zhang. On finding small hyper-gradients in bilevel optimization: Hardness results and improved analysis. In Annual Conference on Learning Theory (COLT), 2024.   \n[10] Tianyi Chen, Yuejiao Sun, Quan Xiao, and Wotao Yin. A single-timescale method for stochastic bilevel optimization. In International Conference on Artificial Intelligence and Statistics (AISTATS), 2022.   \n[11] Tianyi Chen, Yuejiao Sun, and Wotao Yin. Closing the gap: Tighter analysis of alternating stochastic gradient methods for bilevel problems. In Advances in Neural Information Processing Systems (NeurIPS), 2021.   \n[12] Nicolas Courty, R\u00e9mi Flamary, Devis Tuia, and Alain Rakotomamonjy. Optimal transport for domain adaptation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 39(9):1853\u20131865, 2017.   \n[13] Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Advances in Neural Information Processing Systems (NeurIPS), 2013.   \n[14] Mathieu Dagr\u00e9ou, Pierre Ablin, Samuel Vaiter, and Thomas Moreau. A framework for bilevel optimization that enables stochastic and global variance reduction algorithms. In Advances in Neural Information Processing Systems (NeurIPS), 2022.   \n[15] Ahmed Douik and Babak Hassibi. Manifold optimization over the set of doubly stochastic matrices: A second-order geometry. IEEE Transactions on Signal Processing, 67(22):5761\u2013 5774, 2019.   \n[16] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International Conference on Machine Learning (ICML), 2017.   \n[17] R\u00e9mi Flamary, Nicolas Courty, Alexandre Gramfort, Mokhtar Z Alaya, Aur\u00e9lie Boisbunon, Stanislas Chambon, Laetitia Chapel, Adrien Corenflos, Kilian Fatras, Nemo Fournier, et al. POT: Python optimal transport. Journal of Machine Learning Research, 22(1):3571\u20133578, 2021.   \n[18] Luca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, and Massimiliano Pontil. Bilevel programming for hyperparameter optimization and meta-learning. In International Conference on Machine Learning (ICML), 2018.   \n[19] Saeed Ghadimi and Mengdi Wang. Approximation methods for bilevel programming. Technical report, arXiv preprint arXiv:1802.02246, 2018.   \n[20] Boqing Gong, Yuan Shi, Fei Sha, and Kristen Grauman. Geodesic flow kernel for unsupervised domain adaptation. In Conference on Computer Vision and Pattern Recognition (CVPR), 2012.   \n[21] Riccardo Grazzi, Luca Franceschi, Massimiliano Pontil, and Saverio Salzo. On the iteration complexity of hypergradient computation. In International Conference on Machine Learning (ICML), 2020.   \n[22] Andi Han and Junbin Gao. Riemannian stochastic recursive momentum method for non-convex optimization. In International Joint Conference on Artificial Intelligence (IJCAI), 2021.   \n[23] Andi Han and Junbin Gao. Improved variance reduction methods for Riemannian non-convex optimization. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(11):7610\u2013 7623, 2022.   \n[24] Andi Han, Bamdev Mishra, Pratik Jawanpuria, and Junbin Gao. Learning with symmetric positive definite matrices via generalized Bures-Wasserstein geometry. In International Conference on Geometric Science of Information (GSI), 2023.   \n[25] Andi Han, Bamdev Mishra, Pratik Jawanpuria, and Junbin Gao. Nonconvex-nonconcave minmax optimization on Riemannian manifolds. Transactions on Machine Learning Research, 2023.   \n[26] Andi Han, Bamdev Mishra, Pratik Jawanpuria, and Junbin Gao. Riemannian accelerated gradient methods via extrapolation. In International Conference on Artificial Intelligence and Statistics (AISTATS), 2023.   \n[27] Andi Han, Bamdev Mishra, Pratik Jawanpuria, Pawan Kumar, and Junbin Gao. Riemannian Hamiltonian methods for min-max optimization on manifolds. SIAM Journal on Optimization, 33(3):1797\u20131827, 2023.   \n[28] Andi Han, Bamdev Mishra, Pratik Kumar Jawanpuria, and Junbin Gao. On Riemannian optimization over positive definite matrices with the Bures-Wasserstein geometry. Advances in Neural Information Processing Systems (NeurIPS), 2021.   \n[29] Pierre Hansen, Brigitte Jaumard, and Gilles Savard. New branch-and-bound rules for linear bilevel programming. SIAM Journal on Scientific and Statistical Computing, 13(5):1194\u20131217, 1992.   \n[30] Mehrtash Harandi, Mathieu Salzmann, and Richard Hartley. Dimensionality reduction on SPD manifolds: The emergence of geometry-aware methods. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40(1):48\u201362, 2017.   \n[31] Ryuichiro Hataya and Makoto Yamada. Nystr\u00f6m method for accurate and scalable implicit differentiation. In International Conference on Artificial Intelligence and Statistics (AISTATS), 2023.   \n[32] Mingyi Hong, Hoi-To Wai, Zhaoran Wang, and Zhuoran Yang. A two-timescale stochastic algorithm framework for bilevel optimization: Complexity analysis and application to actorcritic. SIAM Journal on Optimization, 33(1):147\u2013180, 2023.   \n[33] Inbal Horev, Florian Yger, and Masashi Sugiyama. Geometry-aware principal component analysis for symmetric positive definite matrices. In Asian Conference on Machine Learning (ACML), 2016.   \n[34] Timothy Hospedales, Antreas Antoniou, Paul Micaelli, and Amos Storkey. Meta-learning in neural networks: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(9):5149\u20135169, 2021.   \n[35] Zihao Hu, Guanghui Wang, Xi Wang, Andre Wibisono, Jacob D Abernethy, and Molei Tao. Extragradient type methods for Riemannian variational inequality problems. In International Conference on Artificial Intelligence and Statistics (AISTATS), 2024.   \n[36] Feihu Huang and Shangqian Gao. Riemannian gradient methods for stochastic composition problems. Neural Networks, 153:224\u2013234, 2022.   \n[37] Feihu Huang and Shangqian Gao. Gradient descent ascent for minimax problems on Riemannian manifolds. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023.   \n[38] Zhiwu Huang and Luc Van Gool. A Riemannian network for spd matrix learning. In AAAI Conference on Artificial Intelligence (AAAI), 2017.   \n[39] Kaiyi Ji, Jason D Lee, Yingbin Liang, and H Vincent Poor. Convergence of meta-learning with task-specific adaptation over partial parameters. In Advances in Neural Information Processing Systems (NeurIPS), 2020.   \n[40] Kaiyi Ji, Junjie Yang, and Yingbin Liang. Bilevel optimization: Convergence analysis and enhanced design. In International Conference on Machine Learning (ICML), 2021.   \n[41] Michael Jordan, Tianyi Lin, and Emmanouil-Vasileios Vlatakis-Gkaragkounis. First-order algorithms for min-max optimization in geodesic metric spaces. In Advances in Neural Information Processing Systems (NeurIPS), 2022.   \n[42] Hiroyuki Kasai, Hiroyuki Sato, and Bamdev Mishra. Riemannian stochastic recursive gradient algorithm. In International Conference on Machine Learning (ICML), 2018.   \n[43] Philip A Knight. The Sinkhorn\u2013Knopp algorithm: convergence and applications. SIAM Journal on Matrix Analysis and Applications, 30(1):261\u2013275, 2008.   \n[44] Max Kochurov, Rasul Karimov, and Serge Kozlukov. Geoopt: Riemannian optimization in PyTorch. Technical report, arXiv preprint arXiv:2005.02819, 2020.   \n[45] Jeongyeol Kwon, Dohyun Kwon, Stephen Wright, and Robert D Nowak. A fully first-order method for stochastic bilevel optimization. In International Conference on Machine Learning (ICML), 2023.   \n[46] Bastian Leibe and Bernt Schiele. Analyzing appearance and contour based methods for object categorization. In Conference on Computer Vision and Pattern Recognition (CVPR), 2003.   \n[47] Jiaxiang Li and Shiqian Ma. Riemannian bilevel optimization. Technical report, arXiv preprint arXiv:2402.02019, 2024.   \n[48] Jun Li, Fuxin Li, and Sinisa Todorovic. Efficient Riemannian Optimization on the Stiefel Manifold via the Cayley Transform. In International Conference on Learning Representations (ICLR), 2019.   \n[49] Jiagen Liao and Zhongping Wan. Inexact restoration methods for semivectorial bilevel programming problem on Riemannian manifolds. Axioms, 11(12):696, 2022.   \n[50] Jiagen Liao and Zhongping Wan. On the Karush-Kuhn-Tucker reformulation of the bilevel optimization problems on Riemannian manifolds. Filomat, 36(11):3609\u20133624, 2022.   \n[51] Zhenhua Lin. Riemannian geometry of symmetric positive definite matrices via Cholesky decomposition. SIAM Journal on Matrix Analysis and Applications, 40(4):1353\u20131370, 2019.   \n[52] Bo Liu, Mao Ye, Stephen Wright, Peter Stone, and Qiang Liu. Bome! bilevel optimization made easy: A simple first-order approach. In Advances in Neural Information Processing Systems (NeurIPS), 2022.   \n[53] Hanxiao Liu, Karen Simonyan, and Yiming Yang. DARTS: Differentiable architecture search. In International Conference on Learning Representations (ICLR), 2018.   \n[54] Jonathan Lorraine, Paul Vicol, and David Duvenaud. Optimizing millions of hyperparameters by implicit differentiation. In International Conference on Artificial Intelligence and Statistics (AISTATS), 2020.   \n[55] Luigi Malag\u00f2, Luigi Montrucchio, and Giovanni Pistone. Wasserstein Riemannian geometry of Gaussian densities. Information Geometry, 1:137\u2013179, 2018.   \n[56] David Mart\u00ednez-Rubio, Christophe Roux, Christopher Criscitiello, and Sebastian Pokutta. Accelerated methods for Riemannian min-max optimization ensuring bounded geometric penalties. Technical report, arXiv preprint arXiv:2305.16186, 2023.   \n[57] Bamdev Mishra, NTV Satyadev, Hiroyuki Kasai, and Pratik Jawanpuria. Manifold optimization for non-linear optimal transport problems. Technical report, arXiv preprint arXiv:2103.00902, 2021.   \n[58] Gabriel Peyre and Marco Cuturi. Computational optimal transport: With applications to data science. Foundations and Trends in Machine Learning, 11(5-6):355\u2013607, 2019.   \n[59] Mengye Ren, Sachin Ravi, Eleni Triantafillou, Jake Snell, Kevin Swersky, Josh B. Tenenbaum, Hugo Larochelle, and Richard S. Zemel. Meta-learning for semi-supervised few-shot classification. In International Conference on Learning Representations (ICLR), 2018.   \n[60] Chenggen Shi, Jie Lu, and Guangquan Zhang. An extended Kuhn\u2013Tucker approach for linear bilevel programming. Applied Mathematics and Computation, 162(1):51\u201363, 2005.   \n[61] Daouda Sow, Kaiyi Ji, and Yingbin Liang. On the convergence theory for Hessian-free bilevel algorithms. Advances in Neural Information Processing Systems (NeurIPS), 2022.   \n[62] Rhea Sukthanker, Zhiwu Huang, Suryansh Kumar, Erik Endsjo Goron, Yan Wu, and Luc Van Gool. Neural architecture search of spd manifold networks. In International Joint Conference on Artificial Intelligence (IJCAI), 2021.   \n[63] Yue Sun, Nicolas Flammarion, and Maryam Fazel. Escaping from saddle points on Riemannian manifolds. Advances in Neural Information Processing Systems (NeurIPS), 2019.   \n[64] Hadi Tabealhojeh, Peyman Adibi, Hossein Karshenas, Soumava Kumar Roy, and Mehrtash Harandi. RMAML: Riemannian meta-learning with orthogonality constraints. Pattern Recognition, 140:109563, 2023.   \n[65] Ioannis Tsaknakis, Prashant Khanduri, and Mingyi Hong. An implicit gradient-type method for linearly constrained bilevel problems. In International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2022.   \n[66] Mengdi Wang, Ethan X Fang, and Han Liu. Stochastic compositional gradient descent: algorithms for minimizing compositions of expected-value functions. Mathematical Programming, 161:419\u2013449, 2017.   \n[67] Xi Wang, Deming Yuan, Yiguang Hong, Zihao Hu, Lei Wang, and Guodong Shi. Riemannian optimistic algorithms. Technical report, arXiv preprint arXiv:2308.16004, 2023.   \n[68] Quan Xiao, Han Shen, Wotao Yin, and Tianyi Chen. Alternating projected sgd for equalityconstrained bilevel optimization. In International Conference on Artificial Intelligence and Statistics (AISTATS), 2023.   \n[69] Pourya Zadeh, Reshad Hosseini, and Suvrit Sra. Geometric mean metric learning. In International Conference on Machine Learning (ICML), 2016.   \n[70] Dewei Zhang and Sam Davanloo Tajbakhsh. Riemannian stochastic gradient method for nested composition optimization. Technical report, arXiv preprint arXiv:2207.09350, 2022.   \n[71] Hongyi Zhang, Sashank J Reddi, and Suvrit Sra. Riemannian SVRG: Fast stochastic optimization on Riemannian manifolds. In Advances in Neural Information Processing Systems (NeurIPS), 2016.   \n[72] Hongyi Zhang and Suvrit Sra. First-order methods for geodesically convex optimization. In Conference on Learning Theory (COLT), 2016.   \n[73] Peiyuan Zhang, Jingzhao Zhang, and Suvrit Sra. Sion\u2019s minimax theorem in geodesic metric spaces and a Riemannian extragradient algorithm. SIAM Journal on Optimization, 33(4):2885\u2013 2908, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A Riemannian geometries of considered manifolds ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Symmetric positive definite (SPD) manifold. The SPD manifold of size $d$ is denoted as $\\mathbb{S}_{++}^{d}:=$ $\\{\\mathbf{X}\\in\\mathbb{R}^{d\\times d}:\\mathbf{X}^{\\top}=\\mathbf{X},\\mathbf{X}\\succ0\\}$ and the commonly considered Riemannian metric is the affineinvariant metric $\\langle\\mathbf{U},\\mathbf{V}\\rangle_{\\mathbf{X}}=\\operatorname{tr}(\\mathbf{X}^{-1}\\mathbf{U}\\mathbf{X}^{-1}\\mathbf{V})$ [6], for U $\\mathbf{\\zeta}^{\\lceil},\\mathbf{V}\\in T_{\\mathbf{X}}\\mathbb{S}_{++}^{d}$ . Other Riemannian metrics, such as (generalized) Bures-Wasserstein [55, 24], log-Euclidean [4] and log-Cholesky [51] metrics can also be considered. The exponential map is given by $\\mathrm{Exp}_{\\mathbf{X}}(\\mathbf{U})=\\mathbf{\\bar{X}}\\mathrm{expm}(\\mathbf{\\dot{X}}^{-1}\\mathbf{U})$ where $\\mathrm{{expm}(\\cdot)}$ denotes the principal matrix exponential. The corresponding logarithm map is given by $\\log_{\\mathbf{X}}(\\overset{.}{\\mathbf{Y}})\\,=\\,\\mathbf{X}\\mathrm{logm}(\\mathbf{\\bar{X}}^{-1}\\bar{\\mathbf{Y}})$ . Its Riemannian gradient of a real-valued function $f$ is derived as grad $f(\\dot{\\mathbf{X}})\\ =\\ \\mathbf{X}\\nabla f(\\mathbf{X})\\mathbf{X}$ and the Riemannian Hessian is $\\mathrm{Hess}f(\\mathbf{X})[\\mathbf{U}]\\,=\\,\\mathrm{Dgrad}f(\\mathbf{X})[\\mathbf{U}]\\,-$ $\\{{\\bf U X}^{-1}\\mathrm{grad}f({\\bf X})\\}_{\\mathrm{S}}={\\bf X}\\nabla^{2}f({\\bf X})[{\\bf U}]{\\bf X}+\\{{\\bf U}\\nabla f({\\bf X}){\\bf X}\\}_{\\mathrm{S}}$ where we use $\\{\\mathbf{A}\\}\\mathrm{s}:=(\\mathbf{A}+\\mathbf{A}^{\\top})/2$ . ", "page_idx": 15}, {"type": "text", "text": "Stiefel manifold. The Stiefel manifold is the set of orthonormal matrices, i.e., $\\operatorname{St}(d,r):=\\{\\mathbf{X}\\in\\mathbf{\\alpha}$ $\\mathbb{R}^{d\\times r}:\\mathbf{X}^{\\top}\\mathbf{X}=\\mathbf{I}\\}$ . A common Riemannian metric is the Euclidean inner product. We consider the QR-based retraction in the experiment, which is $\\operatorname{Retr}\\mathbf{x}(\\mathbf{U})=\\operatorname{qf}(\\mathbf{X}+\\mathbf{U})$ where qf $(\\cdot)$ extracts the Q-factor from the QR decomposition. Let the orthogonal projection to the tangent space be denoted as $\\mathrm{P\\mathbf{x}(\\mathbf{U})=U-\\bar{\\mathbf{X}}\\{\\mathbf{X}^{\\top}\\mathbf{U}\\}\\mathrm{s}}$ . Then, the Riemannian gradient and Riemannian Hessian are given by grad $f(\\mathbf{X})=\\operatorname{P}_{\\mathbf{X}}(\\nabla f(\\mathbf{X}))$ and $\\mathrm{Hess}f(\\mathbf{X})[\\mathbf{U}]=\\mathrm{P}_{\\mathbf{X}}\\bar{(}\\nabla^{2}f(\\mathbf{X})[\\mathbf{U}]-\\mathbf{U}\\{\\mathbf{X}^{\\top}\\nabla f(\\mathbf{X})\\}\\mathrm{s})$ . ", "page_idx": 15}, {"type": "text", "text": "Doubly stochastic manifold. The doubly stochastic manifold (or coupling manifold) between two discrete probability measures $\\mu,\\nu$ with marginals $\\mathbf{a}\\in\\mathbb{R}^{m},\\mathbf{b}\\in\\mathbb{R}^{n}$ is the set $\\Pi(\\mu,\\nu)\\,=\\,\\{\\mathbf{r_{\\alpha}}\\in$ $\\mathbb{R}^{m\\times n}:\\Gamma_{i j}>0,\\mathbf{\\Gamma}\\mathbf{1}_{n}=\\mathbf{a},\\mathbf{\\Gamma}^{\\top}\\mathbf{1}_{m}=\\mathbf{b}\\}$ . It can be equipped with the Fisher information metric, defined as $\\begin{array}{r}{\\langle\\mathbf{U},\\mathbf{V}\\rangle_{\\Gamma}=\\sum_{i,j}(U_{i j}V_{i j})/\\Gamma_{i j}}\\end{array}$ for any $\\mathbf{U},\\bar{\\mathbf{V}}\\in T_{\\Gamma}\\Pi(\\mu,\\nu)$ . The retraction is given by $\\operatorname{Retr}_{\\Gamma}(\\mathbf{U})=\\operatorname{Sinkhorn}(\\mathbf{\\Gamma}\\Gamma\\odot\\exp(\\mathbf{U}\\oslash\\Gamma))$ where exp, $\\odot,\\emptyset$ are elementwise exponential, product, and division operations. Sinkhorn $(\\cdot)$ represents the Sinkhorn-Knopp iterations for balancing a matrix [43]. ", "page_idx": 15}, {"type": "text", "text": "B Important Lemmas ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proposition 2 ([8]). In a totally normal neighbourhood $\\mathcal{U}\\subseteq\\mathcal{M}$ , a function $f:\\mathcal{U}\\to\\mathbb{R}$ is $\\mu$ -geodesic strongly convex, then it satisfies for all $x,y\\in\\mathcal{U}$ ", "page_idx": 15}, {"type": "equation", "text": "$$\nf(y)\\geq f(x)+\\langle\\mathrm{grad}f(x),\\mathrm{Exp}_{x}^{-1}(y)\\rangle_{x}+\\frac{\\mu}{2}d^{2}(x,y).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "If a function $f$ has $L$ -Lipschitz Riemannian gradient, then it satisfies for all $x,y\\in\\mathcal{U}$ ", "page_idx": 15}, {"type": "equation", "text": "$$\nf(y)\\leq f(x)+\\langle\\mathrm{grad}f(x),\\mathrm{Exp}_{x}^{-1}(y)\\rangle_{x}+\\frac{L}{2}d^{2}(x,y).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Lemma 2 ([63, 26]). There exists a constant $C_{0}>0$ such that for any $y_{1},y_{2},y_{3}\\in\\mathcal{U}_{y}$ , $u\\in T_{y_{1}}\\mathcal{M}_{y}$ , $\\|\\Gamma_{y_{2}}^{y_{3}}\\Gamma_{y_{1}}^{y_{2}}u-\\Gamma_{y_{1}}^{y_{3}}u\\|\\leq C_{0}d(y_{1},y_{2})d(y_{2},y_{3})\\|u\\|_{y_{1}}$ ", "page_idx": 15}, {"type": "text", "text": "Lemma 3 (Trigonometric distance bound [72, 71, 28]). Let $x_{a},x_{b},x_{c}\\;\\in\\;\\mathcal{U}\\;\\subseteq\\;\\mathcal{M}$ and denote $\\boldsymbol{a}=d(\\boldsymbol{x}_{b},\\boldsymbol{x}_{c})$ , $b=d(x_{a},x_{c})$ and $c=d(x_{a},x_{b})$ as the geodesic side lengths. Then, ", "page_idx": 15}, {"type": "equation", "text": "$$\na^{2}\\leq\\zeta b^{2}+c^{2}-2\\langle\\mathrm{Exp}_{x_{a}}^{-1}(x_{b}),\\mathrm{Exp}_{x_{a}}^{-1}(x_{c})\\rangle_{x_{a}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\begin{array}{r}{\\zeta=\\frac{\\sqrt{\\vert\\kappa^{-}\\vert}\\bar{D}}{\\operatorname{tanh}(\\sqrt{\\vert\\kappa^{-}\\vert}\\bar{D})}\\,i f\\kappa^{-}<0}\\end{array}$ and $\\zeta=1\\;i f\\kappa^{-}\\geq0.$ . Here, $\\bar{D}$ denotes the diameter of U and $\\kappa^{-}$ denotes the lower bound of the sectional curvature of $\\boldsymbol{\\mathcal{U}}$ . ", "page_idx": 15}, {"type": "text", "text": "C Proofs for Section 3.1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "C.1 Proof of Proposition 1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof of Proposition $^{\\,l}$ . By the first-order optimality condition, $y^{*}(x)$ satisfies $\\mathcal{G}_{y}g(x,y^{*}(x))=0\\in$ $T_{y^{*}(x)}\\M_{y}$ . Based on Theorem 5 in [25], taking the (implicit) derivative of the equality with respect to $x$ yields $\\mathcal{G}_{y x}^{2}g(x,y^{*}(x))[u]+\\mathcal{H}_{y}g(x,y^{*}(x))[\\mathrm{D}y^{*}(x)[u]]\\,=\\,0$ for any $u\\in T_{x}\\mathcal{M}_{x}$ . This gives ", "page_idx": 15}, {"type": "text", "text": "$\\mathrm{D}y^{*}(x)=-\\mathcal{H}_{y}^{-1}g(x,y^{*}(x))\\circ\\mathcal{G}_{y x}^{2}g(x,y^{*}(x))$ . Notice that $\\mathrm{D}y^{*}(x):T_{x}{\\mathcal{M}}_{x}\\rightarrow T_{y^{*}(x)}{\\mathcal{M}}_{y}(x)$ , its adjoint operator $(\\mathrm{D}y^{*}(x))^{\\dag}$ is derived as follows. For any $u\\in T_{x}\\mathcal{M}_{x}$ , $v\\in T_{y^{*}(x)}\\mathscr{M}_{y}$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle(\\mathrm{D}y^{*}(x))^{\\dagger}[v],u\\rangle_{x}=\\langle\\mathrm{D}y^{*}(x)[u],v\\rangle_{y^{*}(x)}=-\\langle\\big(\\mathcal{H}_{y}^{-1}g(x,y^{*}(x))\\circ\\mathcal{G}_{y x}^{2}g(x,y^{*}(x))\\big)[u],v\\rangle_{y^{*}(x)}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad=-\\langle\\mathcal{G}_{y x}^{2}g(x,y^{*}(x))[u],\\mathcal{H}_{y}^{-1}g(x,y^{*}(x))[v]\\rangle_{y^{*}(x)}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad=-\\langle\\big(\\mathcal{G}_{x y}^{2}g(x,y^{*}(x))\\circ\\mathcal{H}_{y}^{-1}g(x,y^{*}(x))\\big)[v],u\\rangle_{x}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the first equality uses the definition of adjoint operator and the third equality is due to the self-adjointness of Riemannian Hessian (inverse) and the last equality is due to Proposition D.2 in [27] that $\\mathcal{G}_{x y}^{2}g$ and $\\mathcal{G}_{y x}^{2}g$ are adjoint operators. By identification, we have $(\\mathrm{D}y^{\\ast}\\dot{(}x))^{\\dagger}\\,=$ $-\\mathcal{G}_{x y}^{2}g(x,y^{\\ast}(x))\\circ\\bar{\\mathcal{H}}_{y}^{-1}g(x,\\bar{y}^{\\ast}(x))$ . ", "page_idx": 16}, {"type": "text", "text": "Finally by the chain rule, we obtain (from the definition of Riemannian gradient), for any $u\\in T_{x}\\mathcal{M}_{x}$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\mathcal{G}F(x),u\\rangle_{x}=\\langle\\mathcal{G}_{x}f(x,y^{*}(x)),u\\rangle_{x}+\\mathrm{D}_{y}f(x,y^{*}(x))[\\mathrm{D}y^{*}(x)[u]]}\\\\ &{\\qquad\\qquad=\\langle\\mathcal{G}_{x}f(x,y^{*}(x)),u\\rangle_{x}+\\langle\\mathcal{G}_{y}f(x,y^{*}(x)),\\mathrm{D}y^{*}(x)[u]\\rangle_{y}}\\\\ &{\\qquad\\qquad=\\langle\\mathcal{G}_{x}f(x,y^{*}(x)),u\\rangle_{x}+\\langle(\\mathrm{D}y^{*}(x))^{\\dagger}\\mathcal{G}_{y}f(x,y^{*}(x)),u\\rangle_{x}}\\\\ &{\\qquad\\qquad=\\langle\\mathcal{G}_{x}f(x,y^{*}(x))-\\mathcal{G}_{x y}^{2}g(x,y^{*}(x))[\\mathcal{H}_{y}^{-1}g(x,y^{*}(x))[\\mathcal{G}_{y}f(x,y^{*}(x))]],u\\rangle_{x}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "By identification the proof is complete. ", "page_idx": 16}, {"type": "text", "text": "C.2 On Lipschitzness of gradients ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proposition 3. If a bifunction $f(x,y)$ has $L$ -Lipschitz Riemannian gradient, then it satisfies $\\begin{array}{r}{\\|\\mathcal{G}f(z_{1})\\!-\\!\\Gamma_{z_{2}}^{z_{1}}\\mathcal{G}f(z_{2})\\|_{z_{1}}\\leq2L d(z_{1},z_{2}),}\\end{array}$ , where we let $z=(x,y)$ . If an operator $\\mathcal{G}(x,y):T_{y}\\mathcal{M}_{y}\\rightarrow$ $T_{x}\\mathcal{M}_{x}$ is $\\rho$ -Lipschitz, then it satisfies $||\\mathcal{G}(z_{1})\\rrangle-\\Gamma_{x_{2}}^{x_{1}}\\mathcal{G}(z_{2})\\Gamma_{y_{1}}^{y_{2}}||_{x_{1}}\\,\\le\\,\\rho\\,d(z_{1},z_{2})$ . If an operator $\\mathcal{H}(x,y):T_{y}\\mathcal{M}_{y}\\rightarrow T_{x}\\mathcal{M}_{x}$ is $\\rho$ -Lipschitz, then it satisfies $\\|\\mathcal{H}(z_{1})-\\Gamma_{y_{2}}^{y_{1}}\\mathcal{H}(z_{2})\\Gamma_{y_{1}}^{y_{2}}\\|_{y_{1}}\\leq\\rho\\,d(z_{1},z_{2})$ . ", "page_idx": 16}, {"type": "text", "text": "Proof of Proposition 3. From the definition of Riemannian gradient of product manifold we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{(z_{1})-\\Gamma_{z_{2}}^{z_{1}}\\mathcal{G}f(z_{2})\\|_{z_{1}}=\\|\\mathcal{G}_{x}f(x_{1},y_{1})-\\Gamma_{x_{2}}^{x_{1}}\\mathcal{G}_{x}f(x_{2},y_{2})\\|_{x_{1}}+\\|\\mathcal{G}_{y}f(x_{1},y_{1})-\\Gamma_{y_{2}}^{y_{1}}\\mathcal{G}_{y}f(x_{2},y_{2})\\|_{y_{1}}}&{}\\\\ {\\leq\\|\\mathcal{G}_{x}f(x_{1},y_{1})-\\mathcal{G}_{x}f(x_{1},y_{2})\\|_{x_{1}}+\\|\\mathcal{G}_{x}f(x_{1},y_{2})-\\Gamma_{x_{2}}^{x_{1}}\\mathcal{G}_{x}f(x_{2},y_{2})\\|_{x_{1}}}&{}\\\\ {+\\;\\|\\mathcal{G}_{y}f(x_{1},y_{1})-\\mathcal{G}_{y}f(x_{2},y_{1})\\|_{y_{1}}+\\|\\mathcal{G}_{y}f(x_{2},y_{1})-\\Gamma_{y_{2}}^{y_{1}}f(x_{2},y_{2})\\|_{y_{1}}}&{}\\\\ {\\leq L d(y_{1},y_{2})+L d(x_{1},x_{2})+L d(x_{1},x_{2})+L d(y_{1},y_{2})=2L d(z_{1},z_{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where we use triangle inequality of Riemannian norm. ", "page_idx": 16}, {"type": "text", "text": "Similarly, for the other two claims, we verify ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{G}(z_{1})-\\Gamma_{x_{2}}^{x_{1}}\\mathcal{G}(z_{2})\\Gamma_{y_{1}}^{y_{2}}\\|_{z_{1}}=\\|\\mathcal{G}(x_{1},y_{1})-\\mathcal{G}(x_{1},y_{2})\\Gamma_{y_{1}}^{y_{2}}\\|_{x_{1}}+\\|\\mathcal{G}(x_{1},y_{2})\\Gamma_{y_{1}}^{y_{2}}-\\Gamma_{x_{2}}^{x_{1}}\\mathcal{G}(x_{2},y_{2})\\Gamma_{y_{1}}^{y_{2}}\\|_{x_{1}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\rho d(y_{1},y_{2})+\\rho d(x_{1},x_{2})=\\rho d(z_{1},z_{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The same arguments also hold for $\\mathcal{H}(x,y)$ and hence the proof is omitted. ", "page_idx": 16}, {"type": "text", "text": "C.3 Boundedness of ingredients ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Lemma 4. Under Assumptions 1, 2, 3, we can show ", "page_idx": 16}, {"type": "text", "text": "4.1 $\\|\\mathcal{G}_{y x}^{2}g(x,y)\\|_{y}=\\|\\mathcal{G}_{x y}^{2}g(x,y)\\|_{x}\\le L$ holds for any $(x,y)\\in\\mathcal{U}_{x}\\times\\mathcal{U}_{y}$ .   \n4.2 $\\|\\mathrm \u1e0a y \u1e0c ^{*}(x)\\|_{y^{*}(x)}\\leq\\kappa_{l}$ and $\\begin{array}{r}{\\|\\mathrm{D}y^{*}(x_{1})-\\Gamma_{y^{*}(x_{2})}^{y^{*}(x_{1})}\\mathrm{D}y^{*}(x_{2})\\Gamma_{x_{1}}^{x_{2}}\\|_{y^{*}(x_{1})}\\leq L_{y}d(x_{1},x_{2})}\\end{array}$ , for any $x,x_{1},x_{2}\\in\\mathcal{U}_{x}$ , where we let $L_{y}:=\\kappa_{l}^{2}\\kappa_{\\rho}+2\\kappa_{l}\\kappa_{\\rho}+\\kappa_{\\rho}$ .   \n4. $3\\ d(y^{*}(x_{1}),y^{*}(x_{2}))\\leq\\kappa_{l}d(x_{1},\\bar{x_{2}})$ , for any $x_{1},x_{2}\\in\\mathcal{U}_{x}$   \n4.4 For any $x,x_{1},x_{2}\\in\\mathcal{U}_{x},y,y_{1},y_{2}\\in\\mathcal{U}_{y}$ $\\begin{array}{r l}&{\\|\\Gamma_{y_{1}}^{y_{2}}\\mathcal{H}_{y}^{-1}g(x,y_{1})\\Gamma_{y_{2}}^{y_{1}}-\\mathcal{H}_{y}^{-1}g(x,y_{2})\\|_{y_{2}}\\leq\\frac{\\kappa_{\\rho}}{\\mu}d(y_{1},y_{2}),}\\\\ &{\\|\\mathcal{H}^{-1}g(x_{1},y)-\\mathcal{H}^{-1}g(x_{2},y)\\|_{y}\\leq\\frac{\\kappa_{\\rho}}{\\mu}d(x_{1},x_{2}).}\\end{array}$ ", "page_idx": 16}, {"type": "text", "text": "4.5 Let $L_{F}:=\\left(\\kappa_{l}+1\\right)\\left(L+\\kappa_{\\rho}M+\\kappa_{\\rho}\\kappa_{l}M+\\kappa_{l}L\\right)$ . Then for any $x_{1},x_{2}\\in\\mathcal{U}_{x}$ , $\\|\\Gamma_{x_{1}}^{x_{2}}\\mathcal{G}F(x_{1})-$ $\\mathcal{G}F(x_{2})\\Vert_{x_{2}}\\leq L_{F}d(x_{1},x_{2})$ . ", "page_idx": 17}, {"type": "text", "text": "Proof of Lemma 4. (4.1) First we have for any $v\\in T_{y}\\M_{y}$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{G}_{x y}^{2}g(x,y)[v]\\|_{x}=\\|\\mathrm{D}_{y}\\mathcal{G}_{x}g(x,y)[v]\\|_{x}\\leq\\operatorname*{lim}_{t\\to0}\\frac{\\|\\mathcal{G}_{x}g(x,\\mathrm{Exp}_{y}(t v))-\\mathcal{G}_{x}g(x,y)\\|_{x}}{|t|}\\leq\\operatorname*{lim}_{t\\to0}\\frac{L\\|t v\\|_{y}}{|t|}=\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where we use the fact that $d(\\mathrm{Exp}_{y}(\\xi),y)=\\|\\xi\\|_{y}$ . The operator norm is the same between $\\mathcal{G}_{x y}^{2}g(x,y)$ and $\\mathcal{G}_{y x}^{2}g(x,y)$ is due to the adjointness. This proves the first claim. ", "page_idx": 17}, {"type": "text", "text": "(4.2) We first verify $\\mathrm{D}{\\boldsymbol{y}}^{*}({\\boldsymbol{x}})$ can be bounded as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|\\mathrm{D}y^{*}(x)\\|_{y^{*}(x)}=\\|\\mathcal{H}_{y}^{-1}g(x,y^{*}(x))\\|_{y^{*}(x)}\\|\\mathcal{G}_{y x}^{2}g(x,y^{*}(x))\\|_{y^{*}(x)}\\le\\frac{L}{\\mu},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and $\\mathrm{D}{\\boldsymbol{y}}^{*}({\\boldsymbol{x}})$ is also Lipschitz as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{D}y^{*}(x_{1})-\\Gamma_{y^{*}(x_{2})}^{\\nu^{*}(x_{1})}\\mathrm{D}y^{*}(x_{2})\\Gamma_{x_{1}}^{x_{2}}\\big|y^{*}(x_{1})}\\\\ &{\\leq\\|\\mathcal{H}_{y^{*}}^{2}y(x_{1},y^{*}(x_{1}))-\\Gamma_{y^{*}(x_{1})}^{\\nu^{*}(x_{1})}\\mathcal{H}_{y^{*}}^{1}y(x_{2},y^{*}(x_{2}))\\Gamma_{y^{*}(x_{1})}^{\\nu^{*}(x_{2})}\\|_{y^{*}(x_{1})}\\|\\mathcal{G}_{y^{*}(x_{1})}^{2}\\,\\mathrm{g}(x_{1},y^{*}(x_{1}))\\|_{y^{*}(x_{1})}}\\\\ &{\\quad+\\|\\mathcal{H}_{y}^{-1}y(x_{2},y^{*}(x_{2}))\\|_{x_{2}}\\|\\Gamma_{y^{*}(x_{1})}^{\\nu^{*}(x_{2})}\\mathcal{G}_{y^{*}}^{2}(x_{1},y^{*}(x_{1}))-\\mathcal{G}_{y^{*}}^{2}y(x_{2},y^{*}(x_{2}))\\Gamma_{x_{1}}^{x_{2}}\\|_{y^{*}(x_{2})}}\\\\ &{\\leq L\\|\\mathcal{H}_{y^{*}}^{-1}y(x_{1},y^{*}(x_{1}))-\\mathcal{H}_{y}^{-1}\\mathcal{G}(x_{2},y^{*}(x_{1}))\\|_{y^{*}(x_{1})}+L\\|\\mathcal{H}_{y}^{1}y(x_{2},y^{*}(x_{1}))-\\Gamma_{y^{*}(x_{2})}^{\\nu^{*}(x_{1})}\\mathcal{H}_{y}^{-1}y(x_{2},y^{*}(x_{2}))}\\\\ &{\\quad+\\frac{1}{\\mu}\\|\\mathcal{G}_{y^{*}}^{2}y(x_{1},y^{*}(x_{1}))-\\mathcal{G}_{y^{*}}^{2}g(x_{2},y^{*}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "(4.3) Now suppose we let $c:[0,1]\\to\\mathcal{M}_{y}$ , defined as $c(t):=y^{*}(\\gamma(t))$ where $\\gamma:[0,1]\\rightarrow\\mathcal{M}_{x}$ is a geodesic that connects $x_{1},x_{2}$ , i.e., $\\gamma(0)\\stackrel{\\circ}{=}x_{1},\\gamma(1)=x_{2}$ . Then ", "page_idx": 17}, {"type": "equation", "text": "$$\nl(y^{*}(x_{1}),y^{*}(x_{2}))=\\int_{0}^{1}\\|c^{\\prime}(t)\\|_{c(t)}d t=\\int_{0}^{1}\\|\\mathrm{D}y^{*}(\\gamma(t))[\\gamma^{\\prime}(t)]\\|_{c(t)}d t\\leq\\frac{L}{\\mu}\\int_{0}^{1}\\|\\gamma^{\\prime}(t)\\|_{\\gamma(t)}d t=\\frac{L}{\\mu}d(\\int_{0}^{1}\\|\\mathrm{D}y^{*}(x_{1})\\|_{\\gamma(t)}d t).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where we use the fact that the manifold is complete. ", "page_idx": 17}, {"type": "text", "text": "(4.4) For the second claim, we first notice for any (invertible) linear operators $A,B,A^{-1}-B^{-1}=$ $\\!A^{-1}(B-A)B^{-1}$ and thus $\\|A^{-1}-B^{-1}\\|\\leq\\|\\dot{A}^{-1}\\|\\|A-B\\|\\|B^{-1}\\|$ for some well-defined norm $\\Vert\\cdot\\Vert$ . Here substituting $A=\\Gamma_{y_{1}}^{y_{2}}\\mathcal{H}_{y}g(x,y_{1})\\Gamma_{y_{2}}^{y_{1}}$ , $B=\\mathcal{H}_{y}g(x,y_{2})$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\Gamma_{y_{1}}^{y_{2}}\\mathcal{H}_{y}^{-1}g(x,y_{1})\\Gamma_{y_{2}}^{y_{1}}-\\mathcal{H}_{y}^{-1}g(x,y_{2})\\|_{y_{2}}}\\\\ &{=\\|\\mathcal{H}_{y}^{-1}g(x,y_{1})\\|_{y_{2}}\\|\\Gamma_{y_{1}}^{y_{2}}\\mathcal{H}_{y}g(x,y_{1})\\Gamma_{y_{2}}^{y_{1}}-\\mathcal{H}_{y}g(x,y_{2})\\|_{y_{2}}\\|\\mathcal{H}_{y}^{-1}g(x,y_{2})\\|_{y_{2}}}\\\\ &{\\le\\frac{\\rho}{\\mu^{2}}d(y_{1},y_{2}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where we notice $(\\Gamma_{y_{1}}^{y_{2}}\\mathcal{H}_{y}(x,y_{1})\\Gamma_{y_{2}}^{y_{1}})^{-1}\\,=\\,\\Gamma_{y_{1}}^{y_{2}}\\mathcal{H}_{y}(x,y_{1})^{-1}\\Gamma_{y_{2}}^{y_{1}}$ and use the isometry property of parallel transport. The same argument applies for $\\|\\mathcal{H}^{-1}g(x_{1},\\bar{y})-\\mathcal{H}^{-1}g(x_{2},y)\\|_{y}$ . ", "page_idx": 17}, {"type": "text", "text": "(4.5) we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Gamma_{x_{1}}^{x_{2}}\\mathcal{G}F(x_{1})-\\mathcal{G}F(x_{2})\\|_{x_{2}}}\\\\ &{\\le\\|\\Gamma_{x_{1}}^{x_{2}}\\mathcal{G}_{x}f(x_{1},y^{*}(x_{1}))-\\mathcal{G}_{x}f(x_{2},y^{*}(x_{2}))\\|_{x_{2}}}\\\\ &{+\\left\\|\\Gamma_{x_{1}}^{x_{2}}\\mathcal{G}_{x y}^{2}g(x_{1},y^{*}(x_{1}))-\\mathcal{G}_{x y}^{2}g(x_{2},y^{*}(x_{2}))\\Gamma_{y^{*}(x_{1})}^{y^{*}(x_{2})}\\|_{x_{2}}\\|\\mathcal{H}_{y}^{-1}g(x_{1},y^{*}(x_{1}))\\|_{y^{*}(x_{1})}\\|\\mathcal{G}_{y}f(x_{1},y^{*}(x_{1}))\\right\\|_{x_{1}}}\\\\ &{+\\left\\|\\mathcal{G}_{x y}^{2}\\mathcal{G}(x_{2},y^{*}(x_{2}))\\|_{x_{2}}\\|\\Gamma_{y^{*}(x_{1})}^{y^{*}(x_{2})}\\mathcal{H}_{y}^{-1}g(x_{1},y^{*}(x_{1}))-\\mathcal{H}_{y}^{-1}g(x_{2},y^{*}(x_{2}))\\Gamma_{y^{*}(x_{1})}^{y^{*}(x_{2})}\\|_{y^{*}(x_{2})}\\|\\mathcal{G}_{y}f(x_{1},y^{*}(x_{1}))\\right\\|_{x_{1}}}\\\\ &{+\\left\\|\\mathcal{G}_{x y}^{2}g(x_{2},y^{*}(x_{2}))\\right\\|_{x_{2}}\\|\\mathcal{H}_{y}^{-1}g(x_{2},y^{*}(x_{2}))\\|_{y^{*}(x_{2})}\\|\\Gamma_{y^{*}(x_{1})}^{y^{*}(x_{2})}\\mathcal{G}_{y}f(x_{1},y^{*}(x_{1}))-\\mathcal{G}_{y}f(x_{2},y^{*}(x_{2}))\\|\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "From Assumption 2, 3 and Lemma 4, we can obtain ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Gamma_{x_{1}}^{x_{2}}\\mathcal{G}_{x}f(x_{1},y^{*}(x_{1}))-\\mathcal{G}_{x}f(x_{2},y^{*}(x_{2}))\\|_{x_{2}}}\\\\ &{\\le\\|\\mathcal{G}_{x}f(x_{1},y^{*}(x_{1}))-\\mathcal{G}_{x}f(x_{1},y^{*}(x_{2}))\\|_{x_{1}}+\\|\\Gamma_{x_{1}}^{x_{2}}\\mathcal{G}_{x}f(x_{1},y^{*}(x_{2}))-\\mathcal{G}_{x}f(x_{2},y^{*}(x_{2}))\\|_{x_{2}}}\\\\ &{\\le L d(y^{*}(x_{1}),y^{*}(x_{2}))+L d(x_{1},x_{2})=(\\frac{L^{2}}{\\mu}+L)d(x_{1},x_{2}).}\\\\ &{\\Gamma_{y^{*}(x_{1})}^{y^{*}(x_{2})}\\mathcal{G}_{y}f(x_{1},y^{*}(x_{1}))-\\mathcal{G}_{y}f(x_{2},y^{*}(x_{2}))\\|_{y^{*}(x_{2})}}\\\\ &{\\le\\|\\Gamma_{y^{*}(x_{1})}^{y^{*}(x_{2})}\\mathcal{G}_{y}f(x_{1},y^{*}(x_{1}))-\\mathcal{G}_{y}f(x_{1},y^{*}(x_{2}))\\|_{y^{*}(x_{2})}+\\|\\mathcal{G}_{y}f(x_{1},y^{*}(x_{2}))-\\mathcal{G}_{y}f(x_{2},y^{*}(x_{2}))\\|_{y^{*}(x_{2})}}\\\\ &{\\le(\\frac{L^{2}}{\\mu}+L)d(x_{1},x_{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Similarly, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Gamma_{x_{1}}^{x_{2}}\\mathcal{G}_{x y}^{2}g(x_{1},y^{*}(x_{1}))-\\mathcal{G}_{x y}^{2}g(x_{2},y^{*}(x_{2}))\\Gamma_{y^{*}(x_{1})}^{y^{*}(x_{2})}\\|_{x_{2}}}\\\\ &{\\leq\\|\\mathcal{G}_{x y}^{2}g(x_{1},y^{*}(x_{1}))-\\mathcal{G}_{x y}^{2}g(x_{1},y^{*}(x_{2}))\\Gamma_{y^{*}(x_{1})}^{y^{*}(x_{2})}\\|_{x_{1}}+\\|\\Gamma_{x_{1}}^{x_{2}}\\mathcal{G}_{x y}^{2}g(x_{1},y^{*}(x_{2}))-\\mathcal{G}_{x y}^{2}g(x_{2},y^{*}(x_{2}))\\|_{x_{1}}}\\\\ &{\\leq(\\frac{\\rho L}{\\mu}+\\rho)d(x_{1},x_{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "\u2225 $\\begin{array}{r l}&{\\Gamma_{y^{*}(x_{1})}^{y^{*}(x_{2})}\\mathcal{H}_{y}^{-1}g(x_{1},y^{*}(x_{1}))-\\mathcal{H}_{y}^{-1}g(x_{2},y^{*}(x_{2}))\\Gamma_{y^{*}(x_{1})}^{y^{*}(x_{2})}\\big\\Vert_{y^{*}(x_{2})}}\\\\ &{\\leq\\|\\mathcal{H}_{y}^{-1}g(x_{1},y^{*}(x_{1}))-\\mathcal{H}_{y}^{-1}g(x_{2},y^{*}(x_{1}))\\|_{y^{*}(x_{1})}+\\|\\Gamma_{y^{*}(x_{1})}^{y^{*}(x_{2})}\\mathcal{H}_{y}^{-1}g(x_{2},y^{*}(x_{1}))\\Gamma_{y^{*}(x_{2})}^{y^{*}(x_{1})}-\\mathcal{H}_{y}^{-1}g(x_{2},y^{*}(x_{1}))\\|_{y^{*}(x_{2})}}\\\\ &{\\leq\\big(\\frac{\\rho}{\\mu^{2}}+\\frac{\\rho L}{\\mu^{3}}\\big)d(x_{1},x_{2}).}\\end{array}$ 2, y\u2217(x2))\u2225y\u2217(x2) ", "page_idx": 18}, {"type": "text", "text": "Combining all the results together, we can show ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Gamma_{x_{1}}^{x_{2}}\\mathcal{G}F(x_{1})-\\mathcal{G}F(x_{2})\\|_{x_{2}}\\leq\\Big(\\displaystyle\\frac{L^{2}}{\\mu}+L+(\\frac{\\rho L}{\\mu}+\\rho)\\frac{M}{\\mu}+L M\\big(\\frac{\\rho}{\\mu^{2}}+\\frac{\\rho L}{\\mu^{3}}\\big)+\\frac{L}{\\mu}\\big(\\frac{L^{2}}{\\mu}+L\\big)\\Big)d(x_{1},x_{2})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\big(\\displaystyle\\frac{L}{\\mu}+1\\big)\\big(L+\\frac{\\rho M}{\\mu}+\\frac{\\rho L M}{\\mu^{2}}+\\frac{L^{2}}{\\mu}\\big)d(x_{1},x_{2}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which completes the proof. ", "page_idx": 18}, {"type": "text", "text": "C.4 On strong convexity of the lower-level problem ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Lemma 5 (Convergence under strong convexity). Under Assumptions 1, 2, 3, suppose $\\begin{array}{r}{\\eta_{y}<\\frac{\\mu}{L^{2}\\zeta}}\\end{array}$ , where $\\zeta\\ \\geq\\ 1$ is a curvature constant defined in Lemma 3, then we have $d^{2}(y_{k}^{s+1},y^{*}(x_{k}))\\ \\leq$ $(1+\\eta_{y}^{2}\\zeta L^{2}-\\eta_{y}\\mu)d^{2}(y_{k}^{s},y^{\\ast}(x_{k}))$ . ", "page_idx": 18}, {"type": "text", "text": "Proof of Lemma 5. We apply the trigonometric distance bound from Lemma 3 to obtain ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d^{2}(y_{k}^{s+1},y^{*}(x_{k}))\\leq d^{2}(y_{k}^{s},y^{*}(x_{k}))+\\eta_{y}^{2}\\zeta\\Vert\\mathcal{G}_{y}g(x_{k},y_{k}^{s})\\Vert_{y_{k}^{s}}^{2}+2\\eta_{y}\\langle\\mathcal{G}_{y}g(x_{k},y_{k}^{s}),\\mathrm{Exp}_{y_{k}^{s}}^{-1}y^{*}(x_{k})\\rangle_{y_{k}^{s}}}\\\\ &{\\qquad\\qquad\\qquad\\leq d^{2}(y_{k}^{s},y^{*}(x_{k}))+\\eta_{y}^{2}\\zeta\\Vert\\mathcal{G}_{y}g(x_{k},y_{k}^{s})\\Vert_{y_{k}^{s}}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\ 2\\eta_{y}\\big(g(x_{k},y^{*}(x_{k}))-g(x_{k},y_{k}^{s})-\\frac{\\mu}{2}d^{2}(y_{k}^{s},y^{*}(x_{k}))\\big)}\\\\ &{\\qquad\\qquad\\qquad\\leq(1+\\eta_{y}^{2}\\zeta L^{2}-\\eta_{y}\\mu)d^{2}(y_{k}^{s},y^{*}(x_{k})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the second inequality is due to geodesic strong convexity and the third inequality is due to ", "page_idx": 18}, {"type": "text", "text": "$\\|\\mathcal{G}_{y}g(x_{k},y_{k}^{s})\\|_{y_{k}^{s}}^{2}=\\|\\mathcal{G}_{y}g(x_{k},y_{k}^{s})-\\Gamma_{y^{*}(x_{k})}^{y_{k}^{s}}\\mathcal{G}_{y}g(x_{k},y^{*}(x_{k}))\\|_{y_{k}^{s}}^{2}\\leq L^{2}d^{2}(y_{k}^{s},y^{*}(x_{k}))$ that $y^{\\ast}(x_{k})$ is optimal. Here, we require $\\begin{array}{r}{\\eta_{y}<\\frac{\\mu}{L^{2}\\zeta}}\\end{array}$ in order for $1+\\eta_{y}^{2}\\zeta L^{2}-\\eta_{y}\\mu<1$ . \u53e3 ", "page_idx": 18}, {"type": "text", "text": "C.5 Proof of Lemma 1 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Proof of Lemma $^{\\,l}$ . Hessian inverse: for the Hessian inverse approximation, we let ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\overline{{\\mathcal{G}}}f(x,y)=\\mathcal{G}_{x}f(x,y)-\\mathcal{G}_{x y}^{2}g(x,y)\\big[\\mathcal{H}_{y}^{-1}g(x,y)[\\mathcal{G}_{y}f(x,y)]\\big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "It can be seen that $\\mathcal{G}F(x_{k})\\,=\\,\\overline{{\\mathcal{G}}}f(x_{k},y^{*}(x_{k}))$ and ${\\widehat{\\mathcal{G}}}_{\\mathrm{hinv}}F(x_{k})\\,=\\,{\\overline{{\\mathcal{G}}}}f(x_{k},y_{k+1})$ . Then for any $x\\in\\mathcal{U}_{x},y_{1},y_{2}\\in\\mathcal{U}_{y}$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overline{{\\mathcal{G}}}f(x,y_{1})-\\overline{{\\mathcal{G}}}f(x,y_{2})\\|_{x}}\\\\ &{\\leq\\|\\mathcal{G}_{x}f(x,y_{1})-\\mathcal{G}_{x}f(x,y_{2})\\|_{x}+\\|\\mathcal{G}_{x y}^{2}g(x,y_{1})-\\mathcal{G}_{x y}^{2}g(x,y_{2})\\Gamma_{y_{1}}^{y_{2}}\\|_{x}\\|\\mathcal{H}_{y}^{-1}g(x,y_{1})\\|_{y_{1}}\\|\\mathcal{G}_{y}f(x,y_{1})\\|_{y}}\\\\ &{\\quad+\\,\\|\\mathcal{G}_{x y}^{2}g(x,y_{2})\\|_{x}\\|\\Gamma_{y_{1}}^{y_{2}}\\mathcal{H}_{y}^{-1}g(x,y_{1})[\\mathcal{G}_{y}f(x,y_{1})]-\\mathcal{H}^{-1}g(x,y_{2})[\\mathcal{G}_{y}f(x,y_{2})]\\|_{y_{2}}}\\\\ &{\\leq\\big(L+\\frac{\\rho M}{\\mu}\\big)d(y_{1},y_{2})+L\\|\\Gamma_{y_{1}}^{y_{2}}\\mathcal{H}_{y}^{-1}g(x,y_{1})-\\mathcal{H}_{y}^{-1}g(x,y_{2})\\Gamma_{y_{1}}^{y_{2}}\\|_{y_{2}}\\|\\mathcal{G}_{y}f(x,y_{1})\\|_{y}}\\\\ &{\\quad+\\,L\\|\\mathcal{H}_{y}^{-1}g(x,y_{2})\\|_{y}\\|\\Gamma_{y_{1}}^{y_{2}}\\mathcal{G}_{y}f(x,y_{1})-\\mathcal{G}_{y}f(x,y_{2})\\|_{y_{2}}}\\\\ &{\\leq\\big(L+\\frac{\\rho M+L^{2}}{\\mu}+\\frac{L M\\rho}{\\mu^{2}}\\big)d(y_{1},y_{2}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where we use Assumption 2, 3 and Lemma 4. ", "page_idx": 19}, {"type": "text", "text": "Conjugate gradient: we let $v_{k}^{*}=\\mathcal{H}_{y}^{-1}g(x_{k},y^{*}(x_{k}))[\\mathcal{G}_{y}f(x_{k},y^{*}(x_{k}))]\\in T_{y^{*}(x_{k})}\\mathcal{M}_{y}$ and let $\\hat{v}_{k}^{*}=$ $\\mathcal{H}_{y}^{-1}g(x_{k},y_{k+1})[\\mathcal{G}_{y}f(x_{k},y_{k+1})]\\in\\bar{T}_{y_{k+1}}\\mathcal{M}_{y}$ . We first bound ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\widehat{\\mathcal{G}}_{\\mathrm{cg}}F(x_{k})-\\mathcal{G}F(x_{k})\\|_{x_{k}}}\\\\ &{\\leq\\|{\\mathcal{G}}_{x}f(x_{k},y_{k+1})-{\\mathcal{G}}_{x}f(x_{k},y^{*}(x_{k}))\\|_{x_{k}}+\\|{\\mathcal{G}}_{x y}^{2}g(x_{k},y_{k+1})\\|_{x_{k}}\\|\\widehat{v}_{k}^{T}-\\Gamma_{y^{*}(x_{k})}^{y_{k+1}}v_{k}^{*}\\|_{y_{k+1}}}\\\\ &{\\quad+\\,\\|{\\mathcal{G}}_{x y}^{2}g(x_{k},y^{*}(x_{k}))-{\\mathcal{G}}_{x y}^{2}g(x_{k},y_{k+1})\\Gamma_{y^{*}(x_{k})}^{y_{k+1}}\\|_{x_{k}}\\|v_{k}^{*}\\|_{y^{*}(x_{k})}}\\\\ &{\\leq L d(y^{*}(x_{k}),y_{k+1})+L\\|\\widehat{v}_{k}^{T}-\\Gamma_{y^{*}(x_{k})}^{y_{k+1}}v_{k}^{*}\\|_{y_{k+1}}+\\rho\\,d(y^{*}(x_{k}),y_{k+1})\\|v_{k}^{*}\\|_{y^{*}(x_{k})}}\\\\ &{\\leq\\big(L+\\kappa_{\\rho}M\\big)d(y^{*}(x_{k}),y_{k+1})+L\\|\\widehat{v}_{k}^{T}-\\Gamma_{y^{*}(x_{k})}^{y_{k+1}}v_{k}^{*}\\|_{y_{k+1}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\|v_{k}^{*}\\|_{y^{*}(x_{k})}\\leq M/\\mu$ . From standard convergence result eq. 6.19 in [8], we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\|\\hat{v}_{k}^{T}-\\hat{v}_{k}^{*}\\|_{y_{k+1}}^{2}\\leq4\\kappa_{l}\\Big(\\frac{\\sqrt{\\kappa_{l}}-1}{\\sqrt{\\kappa_{l}}+1}\\Big)^{2T}\\|\\hat{v}_{k}^{0}-\\hat{v}_{k}^{*}\\|_{y_{k+1}}^{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "This leads to ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\hat{v}_{k}^{T}-\\Gamma_{y^{*}(x_{k})}^{y_{k+1}}v_{k}^{*}\\|_{y_{k+1}}}&{(x_{k})}\\\\ &{\\leq\\|\\hat{v}_{k}^{T}-\\hat{v}_{k}^{*}\\|_{y_{k+1}}+\\|\\Gamma_{y^{*}(x_{k})}^{y_{k+1}}v_{k}^{*}-\\hat{v}_{k}^{*}\\|_{y_{k+1}}}\\\\ &{\\leq2\\sqrt{\\kappa_{l}}\\Big(\\displaystyle\\frac{\\sqrt{\\kappa_{l}}-1}{\\sqrt{\\kappa_{l}}+1}\\Big)^{T}\\|\\hat{v}_{k}^{0}-\\hat{v}_{k}^{*}\\|_{y_{k+1}}+\\|\\Gamma_{y^{*}(x_{k})}^{y_{k+1}}v_{k}^{*}-\\hat{v}_{k}^{*}\\|_{y_{k+1}}}\\\\ &{\\leq2\\sqrt{\\kappa_{l}}\\Big(\\displaystyle\\frac{\\sqrt{\\kappa_{l}}-1}{\\sqrt{\\kappa_{l}}+1}\\Big)^{T}\\|\\hat{v}_{k}^{0}-\\Gamma_{y^{*}(x_{k})}^{y_{k+1}}v_{k}^{*}\\|_{y_{k+1}}+\\Big(1+2\\sqrt{\\kappa_{l}}\\Big(\\displaystyle\\frac{\\sqrt{\\kappa_{l}}-1}{\\sqrt{\\kappa_{l}}+1}\\Big)^{T}\\Big)\\|\\Gamma_{y^{*}(x_{k})}^{y_{k+1}}v_{k}^{*}-\\hat{v}_{k}^{*}\\|_{y_{k+1}}}\\\\ &{\\leq2\\sqrt{\\kappa_{l}}\\Big(\\displaystyle\\frac{\\sqrt{\\kappa_{l}}-1}{\\sqrt{\\kappa_{l}}+1}\\Big)^{T}\\|\\hat{v}_{k}^{0}-\\Gamma_{y^{*}(x_{k})}^{y_{k+1}}v_{k}^{*}\\|_{y_{k+1}}+\\Big(1+2\\sqrt{\\kappa_{l}}\\Big)\\big(\\kappa_{l}+\\displaystyle\\frac{M\\kappa_{\\rho}}{\\mu}\\big)d\\big(y^{*}(x_{k}),y_{k+1}\\big),\\quad(z^{*}(x_{k}),y_{k+1})}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where in the last inequality, we use the definition of $v_{k}^{*}$ and $\\hat{v}_{k}^{*}$ and the Lipschitzness assumptions.   \nCombining the results yield the desired result. ", "page_idx": 19}, {"type": "text", "text": "Neumann series: let $\\begin{array}{r}{\\widehat{\\mathcal{H}}_{k}(y):=\\gamma\\sum_{i=0}^{T-1}(\\mathrm{id}-\\gamma\\mathcal{H}_{y}g(x_{k},y))^{i}}\\end{array}$ . Then we can bound ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\widehat{\\mathcal{G}}_{\\mathrm{ns}}F(x_{k})-\\mathcal{G}F(x_{k})\\|_{x_{k}}}\\\\ &{\\leq L\\,d(y^{*}(x_{k}),y_{k+1})}\\\\ &{+\\|{\\mathcal{G}}_{x y}^{2}g(x_{k},y_{k+1})\\|_{x_{k}}\\|\\widehat{\\mathcal{H}}_{k}(y_{k+1})-\\Gamma_{y^{*}(x_{k})}^{y_{k+1}}{\\mathcal{H}}_{y}^{-1}g(x_{k},y^{*}(x_{k}))\\Gamma_{y k+1}^{y^{*}(x_{k})}\\|_{y_{k+1}}\\|{\\mathcal{G}}_{y}f(x_{k},y_{k+1})\\|_{y_{k+1}}}\\\\ &{+\\|{\\mathcal{G}}_{x y}^{2}g(x_{k},y_{k+1})\\|_{x_{k}}\\|{\\mathcal{H}}_{y}^{-1}g(x_{k},y^{*}(x_{k}))\\|_{y^{*}(x_{k})}\\|\\Gamma_{y k+1}^{y^{*}(x_{k})}{\\mathcal{G}}_{y}f(x_{k},y_{k+1})-{\\mathcal{G}}_{y}f(x_{k},y^{*}(x_{k}))\\|_{y^{*}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{+\\;\\|{\\mathcal G}_{x y}^{2}g(x_{k},y_{k+1})-{\\mathcal G}_{x y}^{2}g(x_{k},y^{*}(x_{k}))\\Gamma_{y k+1}^{y^{*}(x_{k})}\\|_{x_{k}}\\|{\\mathcal H}_{y}^{-1}g(x_{k},y^{*}(x_{k}))\\|_{y^{*}(x_{k})}\\|{\\mathcal G}_{y}f(x_{k},y^{*}(x_{k}))\\|_{y^{*}(x_{k})}}\\\\ &{\\leq(L+\\kappa_{l}L+\\kappa_{\\rho}M)d(y^{*}(x_{k}),y_{k+1})+L M\\|\\widehat{\\mathcal H}_{k}(y_{k+1})-\\Gamma_{y^{*}(x_{k})}^{y_{k+1}}\\mathcal H_{y}^{-1}g(x_{k},y^{*}(x_{k}))\\Gamma_{y_{k+1}}^{y^{*}(x_{k})}\\|_{y_{k+1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We now bound ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{\\mathcal{H}}_{k}(y_{k+1})-\\Gamma_{y^{*}(x_{k})}^{y_{k+1}}\\mathcal{H}_{y}^{-1}g(x_{k},y^{*}(x_{k}))\\Gamma_{y_{k+1}}^{y^{*}(x_{k})}\\vert_{y_{k+1}}}\\\\ &{\\leq\\vert\\vert\\widehat{\\mathcal{H}}_{k}(y_{k+1})-\\mathcal{H}_{y}^{-1}g(x_{k},y_{k+1})\\vert\\vert_{y_{k+1}}+\\vert\\vert\\mathcal{H}_{y}^{-1}g(x_{k},y_{k+1})-\\Gamma_{y^{*}(x_{k})}^{y_{k+1}}\\mathcal{H}_{y}^{-1}g(x_{k},y^{*}(x_{k}))\\Gamma_{y_{k+1}}^{y^{*}(x_{k})}\\vert\\vert_{y_{k}}}\\\\ &{\\leq\\vert\\vert\\gamma\\sum_{i=T}^{\\infty}(\\mathrm{id}-\\gamma\\mathcal{H}_{y}g(x_{k},y_{k+1}))^{i}\\vert\\vert_{y_{k+1}}+\\frac{\\kappa_{\\rho}}{\\mu}d(y^{*}(x_{k}),y_{k+1})}\\\\ &{\\leq\\frac{\\left(1-\\gamma\\mu\\right)^{T}}{\\mu}+\\frac{\\kappa_{\\rho}}{\\mu}d(y^{*}(x_{k}),y_{k+1}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where we use the lower bound on $\\mathscr{H}_{y}g(x_{k},y_{k+1})$ . Substituting the results back the bound yields the desired result. ", "page_idx": 20}, {"type": "text", "text": "Automatic differentiation: Given $y_{k}^{s+1}=\\mathrm{Exp}_{y_{k}^{s}}(-\\eta_{y}\\mathcal{G}_{y}g(x_{k},y_{k}^{s}))$ , we can show its differential is ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{D}_{x_{k}}y_{k}^{s+1}=\\mathcal{P}_{y_{k}^{s+1}}\\big(\\mathrm{D}_{x_{k}}y_{k}^{s}-\\eta_{y}\\mathcal{G}_{y x}^{2}g(x_{k},y_{k}^{s})-\\eta_{y}\\mathcal{H}_{y}g(x_{k},y_{k}^{s})\\mathrm{D}_{x_{k}}y_{k}^{s}\\big)+\\mathcal{E}_{k}^{s}}\\\\ &{\\quad\\quad\\quad\\quad=\\mathcal{P}_{y_{k}^{s+1}}\\big((\\mathrm{id}-\\eta_{y}\\mathcal{H}_{y}g(x_{k},y_{k}^{s}))\\mathrm{D}_{x_{k}}y_{k}^{s}-\\eta_{y}\\mathcal{G}_{y x}^{2}g(x_{k},y_{k}^{s})\\big)+\\mathcal{E}_{k}^{s}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathcal{E}_{k}^{s}\\|_{y_{k}^{s+1}}\\leq C_{3}\\|(\\mathrm{id}-\\eta_{y}\\mathcal{H}_{y}g(x_{k},y_{k}^{s}))\\mathrm{D}_{x_{k}}y_{k}^{s}-\\eta_{y}\\mathcal{G}_{y x}^{2}g(x_{k},y_{k}^{s})\\|_{y_{k}^{s}}\\|\\mathcal{G}_{y}f(x_{k},y_{k}^{s})\\|_{y_{k}^{s}}}\\\\ &{\\qquad\\qquad\\leq\\eta_{y}^{2}C_{3}\\big((1-\\eta_{y}\\mu)C_{1}+\\eta_{y}L\\big)\\|\\mathcal{G}_{y}f(x_{k},y_{k}^{s})\\|_{y_{k}^{s}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "In addition, we notice $\\widehat{\\mathcal{G}}_{\\mathrm{ad}}F(x_{k})=\\mathcal{G}_{x}f(x_{k},y_{k}^{S})+(\\mathrm{D}_{x_{k}}y_{k}^{S})^{\\dagger}[\\mathcal{G}_{y}f(x_{k},y_{k}^{S})]$ and we can bound ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{S a d}^{\\tau}\\operatorname{\\langle}^{\\lambda_{k}\\rangle}-\\mathsf{y}^{\\tau}\\operatorname{\\langle}^{\\lambda_{k}}\\jmath\\|x_{k}}\\\\ &{\\le\\|\\mathcal{G}_{x}f(x_{k},y_{k}^{S})-\\mathcal{G}_{x}f(x_{k},y^{*}(x_{k}))\\|_{x_{k}}+\\|(\\mathsf{D}_{x_{k}}y_{k}^{S})^{\\dagger}-(\\mathsf{D}_{x_{k}}y^{*}(x_{k}))^{\\dagger}\\Gamma_{y_{k}^{S}}^{y^{*}(x_{k})}\\|_{x_{k}}\\|\\mathcal{G}_{y}f(x_{k},y_{k}^{S})\\|_{y_{k}^{S}}}\\\\ &{\\quad+\\left\\|\\mathsf{D}_{x_{k}}y^{*}(x_{k})\\right\\|_{y^{*}(x_{k})}\\|\\Gamma_{y_{k}^{S}}^{y^{*}(x_{k})}\\mathcal{G}_{y}f(x_{k},y_{k}^{S})-\\mathcal{G}_{y}f(x_{k},y^{*}(x_{k}))\\|_{y^{*}(x_{k})}}\\\\ &{\\le(L+L\\kappa_{l})(1+\\eta_{y}^{2}\\zeta L^{2}-\\eta_{y}\\mu)^{\\frac{S}{2}}d(y^{*}(x_{k}),y_{k}^{0})+M\\|\\Gamma_{y^{*}(x_{k})}^{y_{k}^{S}}\\mathsf{D}_{x_{k}}y^{*}(x_{k})-\\mathsf{D}_{x_{k}}y_{k}^{S}\\|_{y_{k}^{S}},\\qquad(6)}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the second inequality uses Lemma 4.2. Then we bound ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\Gamma_{p_{k}^{\\prime}}^{(\\epsilon)}(\\Delta_{t},y_{k}^{*}(\\epsilon)_{i})-\\Omega_{p_{k}^{\\prime}}\\{\\epsilon_{k}\\}^{i+1}\\|_{\\phi_{k}^{\\prime}(\\Delta_{t})}}\\\\ &{\\le[\\Gamma_{p_{k}^{\\prime}+1}^{(\\epsilon)}D_{p_{k}^{\\prime}}y(\\Delta_{t})-\\mathcal{P}_{p_{k}^{\\prime}+1}(\\Delta_{t}-y_{k})y_{k}^{*}(x_{k},y_{k}^{*}(x_{k}^{\\epsilon}))]\\Gamma_{p_{k},\\eta}[y][-\\eta_{k},y_{k}^{\\epsilon}(y(x_{k},y_{k}^{*}))]\\|_{\\phi_{k}^{\\prime}(\\Delta_{t})}}\\\\ &{\\quad+\\eta_{k}^{\\prime}\\mathcal{C}_{p_{k}^{\\prime}}((1-\\eta_{k})\\phi(\\Delta_{t}+y_{k})[\\phi_{k}^{\\prime}(x_{k},y_{k}^{*})])\\|_{\\phi_{k}^{\\prime}}}\\\\ &{=\\|\\Gamma_{p_{k}^{\\prime}+1}^{(\\epsilon)}(\\Delta_{t},y_{k}^{\\epsilon}(x_{k})+\\eta_{k},y_{k}^{\\epsilon}(x_{k},y_{k}^{*}(x_{k})))\\Omega_{p_{k}^{\\prime}}y(\\Delta_{t})+\\eta_{k}\\phi_{k}_{k}^{\\prime}(x_{k},y_{k}^{*}(x_{k}))}\\\\ &{\\quad-\\mathcal{P}_{p_{k}^{\\prime}+1}((\\Delta_{t}-y_{k})\\phi_{k}(\\Delta_{t},y_{k}^{*}))\\|_{\\mathcal{D}_{k},\\eta}[y]-\\eta_{k}\\phi_{k}^{\\prime}(y(x_{k},y_{k}^{*})]\\|_{\\phi_{k}^{\\prime}(\\Delta_{t})}}\\\\ &{\\quad+\\eta_{k}^{\\prime}\\mathcal{C}_{p_{k}^{\\prime}}((1-\\eta_{k})\\phi(\\Delta_{t}+y_{k})[\\phi_{k}^{\\prime}(x_{k},y_{k}^{*})])\\|_{\\phi_{k}^{\\prime}}}\\\\ &{=\\|\\Gamma_{p_{k} \n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad+\\eta_{y}\\big(\\mathcal{H}_{y}g(x_{k},y_{k}^{s})-\\Gamma_{y^{*}}^{y_{k}^{s}}(x_{k})\\mathcal{H}_{y}g(x_{k},y^{*}(x_{k}))\\big)\\Gamma_{y_{k}^{*}}^{y^{*}(x_{k})}\\big)[\\mathcal{D}_{x_{k},y_{k}^{s}}]+\\eta_{y}\\mathcal{G}_{y^{2}}^{2}g(x_{k},y_{k}^{s})-\\Gamma_{y^{*}}^{y_{k}^{s}}(x_{k})\\mathcal{G}_{y^{2},z}^{2}}\\\\ &{\\quad+(\\eta_{y}C_{2}+\\eta_{y}^{2}C_{3})\\big((1-\\eta_{y}\\mu)C_{1}+\\eta_{y}Z\\big)\\|\\mathcal{G}_{y}f(x_{k},y_{k}^{s})\\|_{y_{k}^{s}}}\\\\ &{\\leq(1-\\eta_{y}\\mu)\\big\\|\\Gamma_{y^{*}(x_{k})}^{y_{k}^{s}}\\big)\\big(\\lambda_{x_{k},y}^{s}(x_{k})-\\mathsf{D}_{x_{k},y_{k}^{s}}\\big\\|_{y_{k}^{s}}+\\eta_{y}(\\kappa_{l}+1)\\rho d(y_{k}^{s},y^{*}(x_{k}))}\\\\ &{\\quad+(\\eta_{y}C_{2}+\\eta_{y}^{2}C_{3})\\big((1-\\eta_{y}\\mu)C_{1}+\\eta_{y}Z\\big)\\|\\mathcal{G}_{y}f(x_{k},y_{k}^{s})\\big\\|_{y_{k}^{s}}}\\\\ &{\\leq(1-\\eta_{y}\\mu)\\big\\|\\Gamma_{y^{*}(x_{k})}^{y_{k}^{s}}\\big)\\big(x_{x_{k},y}\\!*\\!(x_{k})-\\!\\mathsf{D}_{x_{k},y_{k}^{s}}\\big\\|_{y_{k}^{s}}+\\eta_{y}(\\kappa_{l}+1)\\rho(1+\\eta_{y}^{2}\\!\\!\\zeta L^{2}-\\eta_{y}\\mu)^{\\frac{s}{2}}d(y_{k}^{0},y^{*}(x_{k}))}\\\\ &{\\quad+\\eta_{y}(C_{2}+\\eta_{y}C_{3} \n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the first equality uses the expression of $\\mathrm{D}_{x_{k}}y^{*}(x_{k})$ (Proposition 1). The second last inequality follows from Lemma 5 and the last inequality is due to the smoothness of Riemannian gradient and Lemma 5, i.e., $\\|\\mathcal{G}_{y}f(x_{k},y_{k}^{s})\\|_{y_{k}^{s}}\\,=\\,\\|\\Gamma_{y^{*}(x_{k})}^{y_{k}^{s}}\\mathcal{G}_{y}f(x_{k},y^{*}(x_{k}))\\,-\\,\\mathcal{G}_{y}f(x_{k},y_{k}^{s})\\|_{y_{k}^{s}}\\,\\le$ $L d(y_{k}^{s},y^{*}(x_{k}))\\leq L(1+\\eta_{y}^{2}\\zeta L^{2}-\\eta_{y}\\mu)^{\\frac{s}{2}}d(y_{k}^{0},y^{*}(x_{k}))$ . ", "page_idx": 21}, {"type": "text", "text": "Finally, applying the bound recursively, we obtain ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\Gamma_{y^{*}(x_{k})}^{\\mathfrak{g}_{k}^{\\flat}}\\sum_{x_{k}\\not\\in\\mathcal{X}_{k}}y^{*}(x_{k})-\\sum_{x_{k},y_{k}^{\\flat}}\\delta\\big|_{y_{k}^{\\flat}}\\leq(1-\\eta_{y}\\mu)^{S}\\big\\|\\Gamma_{y^{*}(x_{k})}^{\\mathfrak{g}_{k}}\\sum_{x_{k},y^{*}}(x_{k})-\\sum_{x_{k},y_{k}^{\\flat}}\\|_{y_{k}^{\\flat}}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\eta_{y}\\tilde{C}\\displaystyle\\sum_{s=0}^{S-1}(1-\\eta_{y}\\mu)^{S-1-s}(1+\\eta_{y}^{2}\\zeta L^{2}-\\eta_{y}\\mu)^{\\frac{s}{2}}d(y_{k}^{0},y^{*}(x_{k}))}\\\\ &{\\leq\\kappa_{l}(1-\\eta_{y}\\mu)^{S}+\\eta_{y}\\tilde{C}\\displaystyle\\sum_{s=0}^{S-1}(1+\\eta_{y}^{2}\\zeta L^{2}-\\eta_{y}\\mu)^{S-1-\\frac{s}{2}}d(y_{k}^{0},y^{*}(x_{k})}\\\\ &{\\leq\\kappa_{l}(1-\\eta_{y}\\mu)^{S}+\\eta_{y}\\tilde{C}\\displaystyle\\frac{(1+\\eta_{y}^{2}\\zeta L^{2}-\\eta_{y}\\mu)^{\\frac{s-1}{2}}}{1-(1+\\eta_{y}^{2}\\zeta L^{2}-\\eta_{y}\\mu)^{\\frac{s}{2}}}d(y_{k}^{0},y^{*}(x_{k}))}\\\\ &{\\leq\\kappa_{l}(1-\\eta_{y}\\mu)^{S}+\\frac{2\\tilde{C}}{\\mu-\\eta_{y}\\zeta L^{2}}(1+\\eta_{y}^{2}\\zeta L^{2}-\\eta_{y}\\mu)^{\\frac{s-1}{2}}d(y_{k}^{0},y^{*}(x_{k}))}\\\\ &{\\leq\\kappa_{l}(1-\\eta_{y}\\mu)^{S}+\\frac{2\\tilde{C}}{\\mu-\\eta_{y}\\zeta L^{2}}(1+\\eta_{y}^{2}\\zeta L^{2}-\\eta_{y}\\mu)^{\\frac{s-1}{2}}d(y_{k}^{\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where we let $\\widetilde{C}:=(\\kappa_{l}+1)\\rho+(C_{2}+\\eta_{y}C_{3})L\\big((1-\\eta_{y}\\mu)C_{1}+\\eta_{y}L\\big)$ and we note that $\\mathrm{D}_{x_{k}}y_{k}^{0}=0$ . Combining the above result with (6) gives ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\widehat{\\mathcal{G}}_{\\mathrm{ad}}F(x_{k})-\\mathcal{G}F(x_{k})\\|_{x_{k}}}\\\\ &{\\leq(L+L\\kappa_{l})(1+\\eta_{y}^{2}\\zeta L^{2}-\\eta_{y}\\mu)^{\\frac{S}{2}}d(y^{\\ast}(x_{k}),y_{k}^{0})+M\\|\\Gamma_{y^{*}(x_{k})}^{y_{k}^{S}}\\mathrm{D}_{x_{k}}y^{*}(x_{k})-\\mathrm{D}_{x_{k}}y_{k}^{S}\\|_{y_{k}^{S}}}\\\\ &{\\leq\\Big(\\displaystyle\\frac{2M\\tilde{C}}{\\mu-\\eta_{y}\\zeta L^{2}}+L(1+\\kappa_{l})\\Big)(1+\\eta_{y}^{2}\\zeta L^{2}-\\eta_{y}\\mu)^{\\frac{S-1}{2}}d(y_{k},y^{*}(x_{k}))+M\\kappa_{l}(1-\\eta_{y}\\mu)^{S},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where we use the fact that $1+\\eta_{y}^{2}\\zeta L^{2}-\\eta_{y}\\mu\\leq1$ ", "page_idx": 21}, {"type": "text", "text": "C.6 Proof of Theorem 1 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Proof of Theorem $^{\\,l}$ . By smoothness of $F(x)$ (Lemma 4.5), we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F(x_{k+1})-F(x_{k})\\leq-\\eta_{x}\\langle\\mathcal{G}F(x_{k}),\\widehat{\\mathcal{G}}F(x_{k})\\rangle_{x_{k}}+\\frac{\\eta_{x}^{2}L_{F}}{2}\\Vert\\widehat{\\mathcal{G}}F(x_{k})\\Vert_{x_{k}}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq-\\big(\\frac{\\eta_{x}}{2}-\\eta_{x}^{2}L_{F}\\big)\\Vert\\mathcal{G}F(x_{k})\\Vert_{x_{k}}^{2}+\\big(\\frac{\\eta_{x}}{2}+\\eta_{x}^{2}L_{F}\\big)\\Vert\\mathcal{G}F(x_{k})-\\widehat{\\mathcal{G}}F(x_{k})\\Vert_{x_{k}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Now we consider the different hypergradient estimator separately. ", "page_idx": 21}, {"type": "text", "text": "1. Hessian inverse: Let $C_{\\mathrm{hinv}}:=L+\\kappa_{\\rho}M+\\kappa_{l}L+\\kappa_{l}\\kappa_{\\rho}M.$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{G}F(x_{k})-\\widehat{\\mathcal{G}}_{\\mathrm{hinv}}F(x_{k})\\|_{x_{k}}^{2}\\leq C_{\\mathrm{hinv}}^{2}d^{2}(y^{*}(x_{k}),y_{k+1})\\leq C_{\\mathrm{hinv}}^{2}(1+\\eta_{y}^{2}\\zeta L^{2}-\\eta_{y}\\mu)^{S}d^{2}(y^{*}(x_{k}),y_{k}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where we notice $y_{k+1}=y_{k}^{S}$ and apply Lemma 5. Furthermore, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ l^{2}(y_{k},y^{*}(x_{k}))}\\\\ &{\\leq2d^{2}(y_{k-1}^{S},y^{*}(x_{k-1}))+2d^{2}(y^{*}(x_{k}),y^{*}(x_{k-1}))}\\\\ &{\\leq2(1+\\eta_{y}^{2}\\zeta L^{2}-\\eta_{y}\\mu)^{S}d^{2}(y^{*}(x_{k-1}),y_{k-1})+2\\eta_{x}^{2}\\kappa_{l}^{2}\\|\\widehat{f}_{\\mathrm{hinv}}F(x_{k-1})\\|_{x_{k}}^{2}}\\\\ &{\\leq2(1+\\eta_{y}^{2}\\zeta L^{2}-\\eta_{y}\\mu)^{S}d^{2}(y^{*}(x_{k-1}),y_{k-1})+4\\eta_{x}^{2}\\kappa_{l}^{2}\\|\\widehat{f}_{\\mathrm{hinv}}F(x_{k-1})-\\mathcal{G}F(x_{k-1})\\|_{x_{k-1}}^{2}}\\\\ &{\\quad+\\,4\\eta_{x}^{2}\\kappa_{l}^{2}\\|\\mathcal{G}F(x_{k-1})\\|_{x_{k-1}}^{2}}\\\\ &{\\leq2(1+\\eta_{y}^{2}\\zeta L^{2}-\\eta_{y}\\mu)^{S}d^{2}(y^{*}(x_{k-1}),y_{k-1})+4\\eta_{x}^{2}\\kappa_{l}^{2}C_{\\mathrm{hinv}}^{2}(1+\\eta_{y}^{2}\\zeta L^{2}-\\eta_{y}\\mu)^{S}d^{2}(y^{*}(x_{k-1}),y_{k-1})}\\\\ &{\\quad+\\,4\\eta_{x}^{2}\\kappa_{l}^{2}\\|\\mathcal{G}F(x_{k-1})\\|_{x_{k-1}}^{2}}\\\\ &{=2(1+2\\eta_{x}^{2}\\kappa_{l}^{2}C_{\\mathrm{hinv}}^{2})(1+\\eta_{y}^{2}\\zeta L^{2}-\\eta_{y}\\mu)^{S}d^{2}(y^{*}(x_{k-1}),y_{k-1})+4\\eta_{x}^{2}\\kappa_{l}^{2}\\|\\mathcal{G}F(x_{k-1})\\|_{x_{k-1}}^{2}\\quad(9)}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where we apply Lemma 5 and 4.3 in the second inequality. ", "page_idx": 22}, {"type": "text", "text": "Construct a Lyapunov function $R_{k}:=F(x_{k})+d^{2}(y_{k},y^{\\ast}(x_{k}))$ . Then, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\overline{{\\mathfrak{z}}}_{k+1}-R_{k}=F(x_{k+1})-F(x_{k})+\\left(d^{2}(y_{k+1},y^{*}(x_{k+1}))-d^{2}(y_{k},y^{*}(x_{k}))\\right)}\\\\ {\\leq-\\left(\\frac{\\eta_{x}}{2}-\\eta_{x}^{2}L_{F}\\right)\\lVert\\mathcal{G}F(x_{k})\\rVert_{x_{k}}^{2}+\\frac{(\\eta_{x}}{2}+\\eta_{x}^{2}L_{F})\\lVert\\mathcal{G}F(x_{k})-\\hat{\\mathcal{G}}_{\\mathrm{hinv}}F(x_{k})\\rVert_{x_{k}}^{2}}\\\\ {\\quad}&{\\quad+\\left(\\left((2+4\\eta_{x}^{2}\\kappa_{l}^{2}C_{\\mathrm{hinv}}^{2})(1+\\eta_{y}^{2}\\zeta L^{2}-\\eta_{y}\\mu)^{S}-1\\right)\\!d^{2}(y^{*}(x_{k}),y_{k})+4\\eta_{x}^{2}\\kappa_{l}^{2}\\lVert\\mathcal{G}F(x_{k})\\rVert_{x_{k}}^{2}}\\\\ {\\leq-\\left(\\frac{\\eta_{x}}{2}-\\eta_{x}^{2}L_{F}-4\\eta_{x}^{2}\\kappa_{l}^{2}\\right)\\lVert\\mathcal{G}F(x_{k})\\rVert_{x_{k}}^{2}}\\\\ {\\quad}&{\\quad+\\left(\\left(2+C_{\\mathrm{hinv}}^{2}(\\frac{\\eta_{x}}{2}+\\eta_{x}^{2}L_{F})+4\\eta_{x}^{2}\\kappa_{l}^{2}C_{\\mathrm{hinv}}^{2}\\right)(1+\\eta_{y}^{2}\\zeta L^{2}-\\eta_{y}\\mu)^{S}-1\\right)d^{2}(y^{*}(x_{k}),y_{k})}\\\\ {\\leq-\\left(\\frac{\\eta_{x}}{2}-5\\eta_{x}^{2}L_{F}\\right)\\lVert\\mathcal{G}F(x_{k})\\rVert_{x_{k}}^{2}}\\\\ {\\quad}&{\\quad+\\left(\\left(2+C_{\\mathrm{hinv}}^{2}(\\frac{\\eta_{x}}{2}+5\\eta_{x}^{2}L_{F})\\right)(1+\\eta_{y}^{2}\\zeta L^{2}-\\eta_{y}\\mu)^{S}-1\\right)d \n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where we combine (7) and (9) in the first inequality and use $\\kappa_{l}^{2}\\leq L_{F}$ in the third inequality. Now setting $\\begin{array}{r}{\\eta_{x}=\\frac{1}{20L_{F}}}\\end{array}$ , we can simplify the inequality as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R_{k+1}-R_{k}\\leq-\\displaystyle\\frac{1}{80L_{F}}\\|\\mathcal{G}F(x_{k})\\|_{x_{k}}^{2}+\\Big((2+\\frac{3C_{\\mathrm{hinv}}^{2}}{80L_{F}})(1+\\eta_{y}^{2}\\zeta L^{2}-\\eta_{y}\\mu)^{S}-1\\Big)d^{2}\\big(y^{*}(x_{k}),y_{k}\\big)}\\\\ &{\\qquad\\qquad\\leq-\\displaystyle\\frac{1}{80L_{F}}\\|\\mathcal{G}F(x_{k})\\|_{x_{k}}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where we choose S \u2265log(160L8F 0+L3FC2  ) $\\begin{array}{r}{S\\ge\\log(\\frac{80L_{F}}{160L_{F}+3C_{\\mathrm{hinv}}^{2}})/\\log(1+\\eta_{y}^{2}\\zeta L^{2}-\\eta_{y}\\mu)=\\widetilde\\Theta(\\kappa_{l}^{2}\\zeta)}\\end{array}$ for the last inequality. Summing over $k=0,...K-1$ yields ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{1}{K}\\sum_{k=0}^{K-1}\\|\\mathcal G F(x_{k})\\|_{x_{k}}^{2}\\leq\\frac{80L_{F}(R_{0}-R_{K})}{K}\\leq\\frac{80L_{F}\\Delta_{0}}{K},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which suggests $\\begin{array}{r}{\\operatorname*{min}_{k=0,\\ldots,K-1}\\|\\mathcal G F(\\boldsymbol x_{k})\\|_{x_{k}}^{2}\\leq\\frac{80L_{F}\\Delta_{0}}{K}}\\end{array}$ . ", "page_idx": 22}, {"type": "text", "text": "2. Conjugate gradient: Let $\\begin{array}{r}{C_{\\mathrm{cg}}:=L+\\kappa_{\\rho}M+L\\big(1+2\\sqrt{\\kappa_{l}}\\big)\\big(\\kappa_{l}+\\frac{M\\kappa_{\\rho}}{\\mu}\\big)}\\end{array}$ . Then we can show ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathcal{G}F(x_{k})-\\widehat{\\mathcal{G}}_{\\mathrm{cg}}F(x_{k})\\|_{x_{k}}^{2}}\\\\ &{\\leq2C_{\\mathrm{cg}}^{2}(1+\\eta_{y}^{2}\\zeta L^{2}-\\eta_{y}\\mu)^{S}d^{2}(y^{*}(x_{k}),y_{k})+8L^{2}\\kappa_{l}\\Big(\\frac{\\sqrt{\\kappa_{l}}-1}{\\sqrt{\\kappa_{l}}+1}\\Big)^{2T}\\|\\hat{v}_{k}^{0}-\\Gamma_{y^{*}(x_{k})}^{y_{k+1}}v_{k}^{*}\\|_{y_{k+1}}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where it follows from Lemma 1 and Lemma 5. Then following similar analysis as in Hessian inverse case ", "page_idx": 22}, {"type": "equation", "text": "$$\nd^{2}(y_{k+1},y^{*}(x_{k+1}))\\leq2(1+\\eta_{y}^{2}\\zeta L^{2}-\\eta_{y}\\mu)^{S}d^{2}(y^{*}(x_{k}),y_{k})+4\\eta_{x}^{2}\\kappa_{l}^{2}\\|\\widehat{\\mathcal{G}}_{\\mathrm{eg}}F(x_{k})-\\mathcal{G}F(x_{k})\\|_{x_{k}}^{2}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad+\\,4\\eta_{x}^{2}\\kappa_{l}^{2}\\|\\mathcal{G}F(x_{k})\\|_{x_{k}}^{2}}\\\\ &{\\leq(2+8\\eta_{x}^{2}\\kappa_{l}^{2}C_{\\mathrm{cg}}^{2})(1+\\eta_{y}^{2}\\zeta L^{2}-\\eta_{y}\\mu)^{S}d^{2}(y^{*}(x_{k}),y_{k})}\\\\ &{\\quad+\\,32\\eta_{x}^{2}\\kappa_{l}^{3}L^{2}\\big(\\frac{\\sqrt{\\kappa_{l}}}{\\sqrt{\\kappa_{l}}+1}\\big)^{2T}\\|\\hat{v}_{k}^{0}-\\Gamma_{y^{*}(x_{k})}^{y_{k+1}}v_{k}^{*}\\|_{y_{k+1}}^{2}+4\\eta_{x}^{2}\\kappa_{l}^{2}\\|\\mathcal{G}F(x_{k})\\|_{x_{k}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Further, noticing $\\hat{v}_{k}^{0}=\\Gamma_{y_{k}}^{y_{k+1}}\\hat{v}_{k-1}^{T}$ , we bound $\\begin{array}{r}{\\|\\hat{v}_{k}^{0}-\\Gamma_{y^{*}(x_{k})}^{y_{k+1}}v_{k}^{*}\\|_{y_{k+1}}=\\|\\hat{v}_{k-1}^{T}-\\Gamma_{y_{k+1}}^{y_{k}}\\Gamma_{y^{*}(x_{k})}^{y_{k+1}}v_{k}^{*}\\|_{y_{k}}}\\end{array}$ as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{v}_{i}^{\\beta}-\\Gamma_{\\gamma}^{\\gamma}(\\hat{v}_{i})\\hat{v}_{i+1}}\\\\ &{\\leq\\|\\hat{v}_{i-1}^{\\gamma}-\\Gamma_{\\gamma}^{\\gamma_{0}}(\\hat{v}_{i})\\hat{v}_{i}\\|_{\\mathcal{H}}+\\frac{M C_{0}\\tilde{D}}{\\mu}d(y_{i+1},y^{*}(x_{i}))}\\\\ &{\\leq\\|\\hat{v}_{i-1}^{\\gamma}-\\Gamma_{\\gamma}^{\\gamma_{0}}(\\hat{v}_{i+1})\\hat{v}_{i}+\\|_{\\mathcal{H}}+\\|\\hat{v}_{i}^{\\gamma}-\\Gamma_{\\gamma}^{\\gamma_{0}}(\\hat{v}_{i-1})\\hat{v}_{i-1}^{*}\\|_{\\mathcal{H}}+\\frac{M C_{0}\\tilde{D}}{\\mu}d(y_{i+1,y}+\\hat{v}_{i})}\\\\ &{\\leq\\|\\hat{v}_{i-1}^{\\gamma}-\\Gamma_{\\gamma}^{\\gamma_{0}}(\\hat{v}_{i-1})\\hat{v}_{i-1}^{*}\\|_{\\mathcal{H}}+\\|\\hat{v}_{i}^{\\gamma}-\\Gamma_{\\gamma}^{\\gamma_{0}}(\\hat{v}_{i-1})\\hat{v}_{i-1}^{*}\\|_{\\mathcal{H}}+\\frac{M C_{0}\\tilde{D}}{\\mu}\\hat{D}(d(y_{i},y^{*}(x_{i}))+d(y_{i+1,y}+\\hat{v}_{i})}\\\\ &{\\leq2\\sqrt{\\kappa_{i}}(\\frac{\\sqrt{\\kappa_{i}}-1}{\\sqrt{\\kappa_{i}}+1})^{\\gamma_{i}}\\|\\hat{v}_{i-1}^{\\alpha}-\\Gamma_{\\gamma}^{\\gamma_{0}}(\\hat{v}_{i-1})\\hat{v}_{i-1}^{*}\\|_{\\mathcal{H}}+(1+\\sqrt{\\kappa_{i}})(\\kappa_{i}+\\frac{M\\kappa_{i}}{\\mu})d(y^{*}(x_{i-1}),y_{i})}\\\\ &{\\quad+\\|\\hat{v}_{i}^{\\gamma}-\\Gamma_{\\gamma}^{\\gamma_{0}}(\\hat{v}_{i-1})\\hat{v}_{i-1}^{*}\\|_{\\mathcal{H}}+\\frac{2M_{0}\\tilde{D}}{\\mu}d(y_{i\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where we use Lemma 2 in the first and third inequalities. The second last inequality follows from (5) and $d(y_{k+1},y^{*}(x_{k}))\\leq d(y_{k},y^{*}(x_{k}))$ . The last inequality follows from Lemma 5 and $\\kappa_{l}\\ge1$ . Now we bound ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\check{x}_{k}^{-1}-\\check{\\mathbf{P}}_{k}^{\\prime}(x_{k-1})\\check{x}_{k-1}^{\\ast}\\|_{1}^{s}-\\operatorname*{in}_{k}^{\\ast}\\mathbb{P}_{k}^{\\prime}(x_{k})\\|_{1}^{s}}\\\\ &{=\\|\\check{\\eta}_{k}^{-1}y(x_{k})y(x_{k})\\big\\|_{1}^{s}\\big\\|_{\\mathcal{F}_{k}^{\\prime}(x_{k-1})}\\mathbb{P}_{k}^{\\prime}(x_{k-1})\\big\\|-\\Gamma_{y^{\\prime}(x_{k-1})}^{\\forall^{\\prime}(x_{k})}\\check{\\eta}_{k}^{-1}y(x_{k-1})y^{*}(x_{k-1})\\big\\|_{1}^{s}y(x_{k-1},y^{*}(x_{k-1})}\\\\ &{\\leq M\\big\\|\\check{R}_{y}^{\\prime}\\big\\|_{1}^{-s}y(x_{k})\\cdot\\Gamma_{y^{\\prime}(x_{k-1})}^{\\forall^{\\prime}(x_{k})}\\big\\|_{\\bar{R}_{y}^{\\prime}}\\mathbb{P}(x_{k-1,y^{*}}(x_{k-1}))\\Gamma_{y^{\\prime}(x_{k-1})}^{\\forall^{\\prime}(x_{k-1})}\\big\\|_{\\bar{R}^{\\prime}(x_{k-1})}}\\\\ &{\\quad+\\left\\|\\check{\\eta}_{k}^{\\top}\\big\\|_{1}^{s}\\big\\|_{1}^{s}\\mathcal{G}_{y}\\big[\\varepsilon_{k,y}^{-1}y(x_{k})\\big]-\\mathcal{G}_{y}\\big(x_{k-1,y}\\big\\|_{1}^{s}\\big\\|_{F^{\\prime}(x_{k-1})}\\big\\|_{1}^{s}}\\\\ &{\\leq M\\big\\|\\check{R}_{y}^{\\prime}\\big\\|_{2}^{s}y(x_{k})y^{*}(x_{k})\\big\\|_{1}^{s}\\mathcal{M}_{y}^{\\top}y(x_{k-1})\\big\\|_{1}^{s}y(x_{k})}\\\\ &{\\quad+M\\big\\|\\check{R}_{y}^{\\prime}\\big\\|_{1}^{s}y(x_{k-1},y^{*}(x_{k}))-\\Gamma_{y^{\\prime}(\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where we let $\\begin{array}{r}{C_{v}:=\\frac{M\\kappa_{\\rho}}{\\mu}+\\frac{M\\kappa_{\\rho}\\kappa_{l}}{\\mu}+\\kappa_{l}^{2}+\\kappa_{l}}\\end{array}$ M\u03ba\u00b5\u03c1\u03bal+ \u03bal2 + \u03bal. Combining (13) and (12), we obtain ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\hat{v}_{k}^{0}-\\Gamma_{y^{*}(x_{k})}^{y_{k+1}}v_{k}^{*}\\|_{y_{k+1}}^{2}}\\\\ &{\\leq20\\kappa\\iota(\\frac{\\sqrt{\\kappa\\iota}-1}{\\sqrt{\\kappa\\iota}+1})^{2T}\\|\\hat{v}_{k-1}^{0}-\\Gamma_{y^{*}(x_{k-1})}^{y_{k}}v_{k-1}^{*}\\|_{y_{k}}^{2}+20\\kappa\\iota(\\kappa\\iota+\\frac{M\\kappa_{\\rho}}{\\mu})^{2}(1+\\eta_{y}^{2}\\zeta L^{2}-\\eta_{y}\\mu)^{S}d(y^{*}(\\kappa\\iota+\\frac{M\\kappa_{\\rho}}{\\mu})+\\iota^{2}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\n-\\left.5\\eta_{x}^{2}C_{v}^{2}\\right\\Vert\\widehat{\\mathcal{G}}_{\\mathrm{cg}}F(x_{k-1})-\\mathcal{G}F(x_{k-1})\\right\\Vert_{x_{k-1}}^{2}+\\left.5\\eta_{x}^{2}C_{v}^{2}\\Vert\\mathcal{G}F(x_{k-1})\\Vert_{x_{k-1}}^{2}+\\frac{5M^{2}C_{0}^{2}\\bar{D}^{2}}{\\mu^{2}}d^{2}(y_{k},y^{*}(x_{k})).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Now we define a Lyapunov function $\\begin{array}{r}{R_{k}:=F(x_{k})+d^{2}(y_{k},y^{*}(x_{k}))+\\|\\hat{v}_{k}^{0}-\\Gamma_{y^{*}(x_{k})}^{y_{k+1}}v_{k}^{*}\\|_{y_{k+1}}^{2}.}\\end{array}$ . Then ", "page_idx": 24}, {"type": "text", "text": ") \u2212d2(yk, y\u2217(xk)) M\u00b5\u03ba\u03c1)2(1 + \u03b7y2\u03b6L2 \u2212\u03b7y\u00b5)Sd2(y\u2217(xk), yk) $\\begin{array}{r l}&{\\quad+\\iiint_{{\\Omega}}^{{\\mathbf{A}}}\\left(\\frac{\\partial}{\\partial x_{1}}\\nabla_{\\overline{{S}}}f_{i}^{\\alpha}+\\frac{\\alpha}{\\sqrt{2}}\\right)\\left(\\mu_{1}^{\\alpha}-\\nu\\right)\\mathrm{d}\\Omega^{\\alpha}\\mathrm{d}\\Omega^{\\alpha}+2\\eta\\mathrm{d}\\Omega^{\\alpha}\\mathrm{d}\\Omega^{\\alpha}\\mathrm{d}\\Omega^{\\alpha}\\mathrm{d}\\Omega^{\\alpha}}\\\\ &{\\quad+\\delta\\Biggl(\\mu_{1}^{\\alpha}\\mathrm{d}_{2}^{\\alpha}\\mathrm{d}_{3}\\nu\\log\\left(-\\eta\\right)\\mathrm{d}\\Omega^{\\alpha}\\mathrm{d}\\Omega^{\\alpha}\\mathrm{d}\\Omega^{\\alpha}\\Biggr)\\mathrm{d}\\Omega^{\\alpha}\\mathrm{d}\\Omega^{\\alpha}\\mathrm{d}\\Omega^{\\alpha}}\\\\ &{=-\\left(\\frac{\\eta}{\\sqrt{2}}-\\eta\\right)\\mathrm{d}\\Omega^{\\alpha}\\mathrm{d}_{3}\\nu\\log\\left(\\mathrm{d}_{3}\\eta\\right)\\mathrm{d}\\Omega^{\\alpha}+\\frac{\\alpha}{\\sqrt{2}}+\\eta\\mathrm{d}_{2}^{\\alpha}\\mathrm{d}_{3}\\nu\\log\\left(\\mathrm{d}_{3}\\eta\\right)-\\delta\\ o(\\mathrm{d}_{4}^{\\alpha}\\nu\\log\\left(-\\eta\\right)\\mathrm{d}_{2}^{\\alpha}\\right)}\\\\ &{\\quad+\\frac{\\delta^{\\alpha}}{\\sqrt{2}}\\eta\\mathrm{d}_{3}^{\\alpha}+\\eta\\mathrm{d}_{4}\\nu\\log\\left(\\mathrm{d}_{3}\\eta\\right)+\\frac{\\alpha}{\\sqrt{2}}\\left(3\\eta\\mathrm{d}_{6}\\nu\\log\\left(\\frac{\\eta}{\\sqrt{2}}\\right)\\mathrm{d}_{1}\\eta\\right)+\\frac{\\alpha}{\\sqrt{2}}\\eta\\mathrm{d}_{2}^{\\alpha}\\mathrm{d}\\eta\\mathrm{d}\\Omega^{\\alpha}\\mathrm{d}\\eta}\\\\ &{\\quad+\\left(2\\eta\\mathrm{d}_{5}^{\\alpha}\\mathrm{d}_{1}^{\\alpha}\\right)^{\\alpha}-1\\right)\\rho\\mathrm{d}\\eta-\\frac{\\delta}{\\sqrt{2}}\\eta\\mathrm{d}_{1}\\eta\\mathrm{d}\\Omega^{\\alpha}\\mathrm{d}\\eta}\\\\ &{\\leq-\\frac{\\eta}{\\sqrt{2}}\\eta\\mathrm{d}_{2}^{\\alpha}-\\delta\\eta\\mathrm{d}_{6}^{\\alpha}\\mathrm{d}_{5}^{\\alpha}-\\delta\\eta\\mathrm{d}_{6}^{\\alpha}$ \u2264 \u2212961\u039b\u2225GF(xk)\u22252xk +  32\u03b7x2\u03bal3 L2(5M 2\u00b5C202 D\u00af2 + 1) + 20\u03bal + 4\u039b \u03bal + 1 T \u22121 \u2225v\u02c6k0 \u2212\u0393yyk\u2217(+x1k)vk\u2217\u22252yk+1 +   5M 2C02 D\u00af2 + 1 (2 + 8\u03b7x2\u03bal2 Cc2g) + 20\u03bal \u03bal + M\u03ba\u03c1 2 + C2 g (1 + \u03b7y2\u03b6L2 \u2212\u03b7y\u00b5)S \u22121 d2(yk, y\u2217(xk)) \u00b52 \u00b5 16\u039b \u2264 \u2212961\u039b\u2225GF(xk)\u22252xk ", "page_idx": 24}, {"type": "text", "text": "where we use (7), (14) in the first inequality and (11) in the second inequality. In the third inequality, we let \u039b := Cv2 + \u03bal2 ( 5M 2\u00b5C02 D\u00af2 of gen cahnodo sbee cause $L_{F}=\\Theta(\\kappa_{l}^{3})$ ( 1a0n)d $\\Lambda=\\Theta(\\kappa_{l}^{4})$ ,h  iwnee cqauna liwtiyt. hTohute  lloassst erality have $L_{F}\\leq\\Lambda$ . We also $\\begin{array}{r}{\\eta_{x}=\\frac{1}{24\\Lambda}}\\end{array}$ and use  for the fourt inequality follows by choosing $\\begin{array}{r l}&{\\mathfrak{I}\\geq-\\log\\Big(\\big(\\frac{5M^{2}C_{0}^{2}\\bar{D}^{2}}{\\mu^{2}}+1\\big)(2+8\\eta_{x}^{2}\\kappa_{l}^{2}C_{\\mathrm{cg}}^{2})+20\\kappa_{l}\\big(\\kappa_{l}+\\frac{M\\kappa_{\\rho}}{\\mu}\\big)^{2}\\Big)/\\log(1+\\eta_{y}^{2}\\zeta L^{2}-\\eta_{y}\\mu)=\\widetilde{\\Theta}(2)}\\\\ &{\\mathrm{~\\}\\geq-\\displaystyle\\frac{1}{2}\\log\\Big(32\\eta_{x}^{2}\\kappa_{l}^{3}L^{2}\\big(\\frac{5M^{2}C_{0}^{2}\\bar{D}^{2}}{\\mu^{2}}+1\\big)+20\\kappa_{l}\\big)/\\log\\Big(\\frac{\\sqrt{\\kappa_{l}}-1}{\\sqrt{\\kappa_{l}}+1}\\Big)=\\widetilde{\\Theta}(\\sqrt{\\kappa_{l}}).}\\end{array}$ ", "page_idx": 24}, {"type": "text", "text": "Finally, telescoping the inequality, we obtain ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\frac{1}{K}\\sum_{k=0}^{K-1}\\|\\mathcal{G}F(x_{k})\\|_{x_{k}}\\le\\frac{96\\Lambda R_{0}}{K}=\\frac{96\\Lambda}{K}\\Big(F(x_{k})+d^{2}(y_{0},y^{*}(x_{0}))+\\|v_{0}^{*}\\|_{y^{*}(x_{0})}^{2}\\Big),\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where we use the fact that $\\hat{v}_{k}^{0}=0$ and the isometry property of parallel transport. ", "page_idx": 25}, {"type": "text", "text": "3. Truncated Neumann series: Let $C_{\\mathrm{ns}}\\;:=\\;L+\\kappa_{l}L+\\kappa_{\\rho}M+\\kappa_{l}\\kappa_{\\rho}M$ . Here we notice that $C_{\\mathrm{ns}}=C_{\\mathrm{hinv}}$ . Then by Lemma 1, we see ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\widehat{\\mathcal{G}}_{\\mathrm{ns}}F(x_{k})-\\mathcal{G}F(x_{k})\\|_{x_{k}}^{2}\\leq2C_{\\mathrm{ns}}^{2}(1+\\eta_{y}^{2}\\zeta L^{2}-\\eta_{y}\\mu)^{S}d^{2}(y^{*}(x_{k}),y_{k})+2\\kappa_{l}^{2}M^{2}(1-\\gamma\\mu)^{2T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Similar in the previous analysis, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{d^{2}(y_{k+1},y^{*}(x_{k+1}))\\leq2(1+\\eta_{y}^{2}\\zeta L^{2}-\\eta_{y}\\mu)^{S}d^{2}(y^{*}(x_{k}),y_{k})+4\\eta_{x}^{2}\\kappa_{l}^{2}\\|\\widehat{\\mathcal{G}}_{\\mathrm{ns}}F(x_{k})-\\mathcal{G}F(x_{k})\\|_{x_{k}}^{2}}&{{}}\\\\ {+\\ 4\\eta_{x}^{2}\\kappa_{l}^{2}\\|\\mathcal{G}F(x_{k})\\|_{x_{k}}^{2}}&{{}}&{{(16)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Let the Lyapunov function be $R_{k}:=F(x_{k})+d^{2}(y_{k},y^{\\ast}(x_{k}))$ . Then ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R_{k+1}-R_{k}}\\\\ &{\\leq-\\big(\\frac{\\eta_{x}}{2}-\\eta_{x}^{2}L_{F}-4\\eta_{x}^{2}\\kappa_{l}^{2}\\big)\\|\\mathscr{G}F(x_{k})\\|_{x_{k}}^{2}+\\big(\\frac{\\eta_{x}}{2}+\\eta_{x}^{2}L_{F}+4\\eta_{x}^{2}\\kappa_{l}^{2}\\big)\\|\\mathscr{G}F(x_{k})-\\widehat{\\mathscr{G}}_{\\mathrm{ns}}F(x_{k})\\|_{x_{k}}^{2}}\\\\ &{\\quad+\\big(2(1+\\eta_{y}^{2}\\zeta L^{2}-\\eta_{y}\\mu)^{S}-1\\big)d^{2}(y_{k},y^{*}(x_{k}))}\\\\ &{\\leq-\\displaystyle\\frac{1}{80L_{F}}\\|\\mathscr{G}F(x_{k})\\|_{x_{k}}^{2}+\\big(\\big(2+\\frac{3C_{\\mathrm{ns}}^{2}}{40L_{F}}\\big)(1+\\eta_{y}^{2}\\zeta L^{2}-\\eta_{y}\\mu)^{S}-1\\big)d^{2}(y_{k},y^{*}(x_{k}))}\\\\ &{\\quad+\\displaystyle\\frac{3}{40L_{F}}\\kappa_{l}^{2}M^{2}(1-\\gamma\\mu)^{2T}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where we set $\\begin{array}{r}{\\eta_{x}=\\frac{1}{20L_{F}}}\\end{array}$ and apply (15) in the second inequality. ", "page_idx": 25}, {"type": "text", "text": "yNieolwd ssetting $\\begin{array}{r}{S\\geq\\log(\\frac{40L_{F}}{80L_{F}+3C_{\\mathrm{ns}}^{2}})/\\log(1+\\eta_{y}^{2}\\zeta L^{2}-\\eta_{y}\\mu)=\\widetilde{\\Theta}(\\kappa_{l}^{2}\\zeta)}\\end{array}$ and telescoping the results ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\frac{1}{K}\\sum_{k=0}^{K-1}\\|\\mathcal{G}F(x_{k})\\|_{x_{k}}^{2}\\leq\\frac{80L_{F}R_{0}}{K}+6\\kappa_{l}^{2}M^{2}(1-\\gamma\\mu)^{2T}\\leq\\frac{80L_{F}R_{0}}{K}+\\frac{\\epsilon}{2}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where we set $\\begin{array}{r}{T\\ge-\\frac{1}{2}\\log(\\frac{12\\kappa_{l}^{2}M^{2}}{\\epsilon})/\\log(1-\\gamma\\mu)=\\widetilde\\Theta(\\kappa\\log(\\frac{1}{\\epsilon})).}\\end{array}$ ", "page_idx": 25}, {"type": "text", "text": "4. Automatic differentiation: Let \u00b5\u22122\u03b7My \u03b6C L2 + L(1 + \u03bal). Then ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\mathcal{G}F(x_{k})-\\widehat{\\mathcal{G}}_{\\mathrm{ad}}F(x_{k})\\|_{x_{k}}^{2}\\leq2C_{\\mathrm{ad}}^{2}(1+\\eta_{y}^{2}\\zeta L^{2}-\\eta_{y}\\mu)^{S-1}d^{2}(y_{k},y^{*}(x_{k}))+2M^{2}\\kappa_{l}^{2}(1-\\eta_{y}\\mu)^{2S}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and similarly ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d^{2}(y_{k+1},y^{*}(x_{k+1}))\\leq2(1+\\eta_{y}^{2}\\zeta L^{2}-\\eta_{y}\\mu)^{S}d^{2}(y^{*}(x_{k}),y_{k})+4\\eta_{x}^{2}\\kappa_{l}^{2}\\|\\widehat{\\mathcal{G}}_{\\mathrm{ad}}F(x_{k})-\\mathcal{G}F(x_{k})\\|_{x_{k}}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\,4\\eta_{x}^{2}\\kappa_{l}^{2}\\|\\mathcal{G}F(x_{k})\\|_{x_{k}}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Let the Lyapunov function be $R_{k}:=F(x_{k})+d^{2}(y_{k},y^{\\ast}(x_{k}))$ . Then ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{T}_{k+1}-R_{k}\\leq-\\big(\\frac{\\eta_{x}}{2}-\\eta_{x}^{2}L_{F}-4\\eta_{x}^{2}\\kappa_{l}^{2}\\big)\\|\\mathcal{G}F(x_{k})\\|_{x_{k}}^{2}+\\big(\\frac{\\eta_{x}}{2}+\\eta_{x}^{2}L_{F}+4\\eta_{x}^{2}\\kappa_{l}^{2}\\big)\\|\\mathcal{G}F(x_{k})-\\widehat{\\mathcal{G}}_{\\mathrm{ad}}F(x_{k})}\\\\ &{\\qquad\\qquad\\qquad+\\left(\\big(2(1+\\eta_{y}^{2}\\zeta L^{2}-\\eta_{y}\\mu)^{S-1}-1\\big)d^{2}(y_{k},y^{*}(x_{k}))\\big)\\right.}\\\\ &{\\qquad\\qquad\\leq-\\frac{1}{80L_{F}}\\|\\mathcal{G}F(x_{k})\\|_{x_{k}}^{2}+\\Big(\\big((2+\\frac{3C_{\\mathrm{ad}}^{2}}{40L_{F}})(1+\\eta_{y}^{2}\\zeta L^{2}-\\eta_{y}\\mu)^{S-1}-1\\big)d^{2}(y_{k},y^{*}(x_{k}))}\\\\ &{\\qquad\\qquad+\\frac{3}{40L_{F}}M^{2}\\kappa_{l}^{2}(1-\\eta_{y}\\mu)^{2S}}\\\\ &{\\qquad\\qquad\\leq-\\frac{1}{80L_{F}}\\|\\mathcal{G}F(x_{k})\\|_{x_{k}}^{2}+\\frac{3}{40L_{F}}M^{2}\\kappa_{l}^{2}(1-\\eta_{y}\\mu)^{2S}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where we set \u03b7x =201LF and choose S \u2265log80L4F0 +L3FC2 $\\begin{array}{r}{S\\ge\\log\\frac{40L_{F}}{80L_{F}+3C_{a d}^{2}}/\\log(1+\\eta_{y}^{2}\\zeta L^{2}-\\eta_{y}\\mu)+1=\\widetilde\\Theta(\\kappa_{l}^{2}\\zeta).}\\end{array}$ Telescoping the result gives ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\frac{1}{K}\\sum_{k=0}^{K-1}\\|\\mathcal{G}F(x_{k})\\|_{x_{k}}^{2}\\le\\frac{80L_{F}R_{0}}{K}+6M^{2}\\kappa_{l}^{2}(1-\\eta_{y}\\mu)^{2S}\\le\\frac{80L_{F}R_{0}}{K}+\\frac{\\epsilon}{2},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "by choosing $\\begin{array}{r}{S\\ge\\frac{1}{2}\\log(\\frac{\\epsilon}{12M^{2}\\kappa_{l}^{2}})\\log(1-\\eta_{y}\\mu)=\\widetilde\\Theta(\\kappa_{l}^{2}\\zeta\\log(\\frac{1}{\\epsilon}))}\\end{array}$ . Hence we set $S\\ge\\widetilde\\Theta(\\kappa_{l}^{2}\\zeta\\log(\\frac{1}{\\epsilon}))$ for both conditions to hold. \u53e3 ", "page_idx": 26}, {"type": "text", "text": "C.7 Proof of Corollary 1 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "The computational cost of gradient and Hessian for each method for approximating the hypergradient are as follows. ", "page_idx": 26}, {"type": "text", "text": "Corollary 1. The complexities of reaching an \u03f5-stationary solution are ", "page_idx": 26}, {"type": "text", "text": "\u2022 Hessian inverse: $G_{f}=O(\\kappa_{l}^{3}\\epsilon^{-1})$ $\\kappa_{l}^{3}\\epsilon^{-1}),\\,G_{g}=\\widetilde{O}(\\kappa_{l}^{5}\\zeta\\epsilon^{-1}),\\,J V_{g}=O(\\kappa_{l}^{3}\\epsilon^{-1}),\\,H V_{g}=N A.$   \n\u2022 Conjugate gradient: $G_{f}\\;=\\;O\\big(\\kappa_{l}^{4}\\epsilon^{-1}\\big),\\;G_{g}\\;=\\;\\widetilde{O}\\big(\\kappa_{l}^{6}\\zeta\\epsilon^{-1}\\big),\\;J V_{g}\\;=\\;O\\big(\\kappa_{l}^{4}\\epsilon^{-1}\\big),\\;H V_{g}\\;=\\;$ $\\widetilde{O}(\\kappa_{l}^{4.5}\\epsilon^{-1})$ .   \n\u2022 Truncated Neumann series: $G_{f}\\ =\\ O(\\kappa_{l}^{3}\\epsilon^{-1}),G_{g}\\ =\\ \\widetilde O(\\kappa_{l}^{5}\\zeta\\epsilon^{-1}),\\ J V_{g}\\ =\\ O(\\kappa_{l}^{3}\\epsilon^{-1}),$ $H V_{g}=\\widetilde{O}(\\kappa_{l}^{4}\\epsilon^{-1}\\log(\\epsilon^{-1}))$ .   \n\u2022 Automatic differentiation: $\\begin{array}{r c l c r c l}{{G_{f}}}&{{=}}&{{{\\cal O}(\\kappa_{l}^{3}\\epsilon^{-1}),G_{g}}}&{{=}}&{{\\widetilde{\\cal O}(\\kappa_{l}^{5}\\zeta\\epsilon^{-1}\\log(\\epsilon^{-1})),J V_{g}}}&{{=}}&{{}}\\end{array}$ $\\widetilde{O}(\\kappa_{l}^{5}\\zeta\\epsilon^{-1}\\log(\\epsilon^{-1})),\\,H V_{g}=\\widetilde{O}(\\kappa_{l}^{5}\\zeta\\epsilon^{-1}\\log(\\epsilon^{-1}))$ . ", "page_idx": 26}, {"type": "text", "text": "Proof of Corollary $^{\\,I}$ . From the convergence established in Theorem 1, we see the iterations in order to reach $\\epsilon$ -stationary solution are given by ", "page_idx": 26}, {"type": "text", "text": "\u2022 (Hessian inverse) $K=O(L_{F}\\epsilon^{-1})=O(\\kappa_{l}^{3}\\epsilon^{-1}),S=\\widetilde O(\\kappa_{l}^{2}\\zeta)$ . \u2022 (Conjugate gradient) $K=O(\\Lambda\\epsilon^{-1})=O(\\kappa_{l}^{4}\\epsilon^{-1}),S=\\widetilde{O}(\\kappa_{l}^{2}\\zeta),T=\\widetilde{O}(\\sqrt{\\kappa_{l}}).$ \u2022 (Truncated Neumann serie $\\mathrm{:}\\;K=O(\\kappa_{l}^{3}\\epsilon^{-1}),S=\\widetilde O(\\kappa_{l}^{2}\\zeta),T=\\widetilde O(\\kappa_{l}\\log(\\epsilon^{-1})).$ \u2022 (Automatic differentiation) $K=O(\\kappa_{l}^{3}\\epsilon^{-1})$ , $S=\\widetilde{O}\\big(\\kappa_{l}^{2}\\zeta\\log(\\epsilon^{-1})\\big)$ . ", "page_idx": 26}, {"type": "text", "text": "Then based on Algorithm 1, the gradient complexities are $G_{f}\\,=\\,2K$ and $G_{g}\\,=\\,K S$ and crossderivative and Hessian product complexities are $J V_{g}\\;=\\;K;H V_{g}\\;=\\;K T$ for CG and NS and $J V_{g}=K S$ , $H V_{g}=K\\bar{S}$ for AD (which we approximate based on the analysis in Lemma 1). We notice here for the Hessian inverse, because we do not compute Hessian vector product, we write NA for Hessian vector product based on the Neumann series. This completes the proof. \u53e3 ", "page_idx": 26}, {"type": "text", "text": "D Proofs for Section 3.3 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We first show Lemma 4 holds for each $f_{i}(x,y),g_{i}(x,y)$ . Further, the variance of the estimate can be bounded as follows. We here use $[\\cdot]$ to denote all possible derivatives, including $x,y,x y,y x$ . ", "page_idx": 26}, {"type": "text", "text": "Lemma 6. Under Assumption 4, we have for any $x,y\\in\\mathcal{U},(l)\\,\\mathbb{E}\\|\\mathcal{G}_{[\\cdot]}\\,f_{i}(x,y)-\\mathcal{G}_{[\\cdot]}\\,f(x,y)\\|_{[\\cdot]}^{2}\\leq M^{2}$ . (2 $\\begin{array}{r}{\\jmath\\,\\mathbb{E}\\|\\mathcal{G}_{[\\cdot]}^{2}g_{i}(x,y)-\\mathcal{G}_{[\\cdot]}^{2}g(x,y)\\|_{[\\cdot]}^{2}\\leq L^{2}.\\;(3)\\,\\mathbb{E}\\|\\mathcal{H}_{y}^{-1}g_{i}(x,y)-\\mathcal{H}_{y}^{-1}g(x,y)\\|_{y}^{2}\\leq\\mu^{-2}.}\\end{array}$ ", "page_idx": 26}, {"type": "text", "text": "For notation, denote the flitration $\\mathcal{F}_{k}:=\\left\\{y_{0},x_{0},y_{1},x_{1},...,x_{k},y_{k+1}\\right\\}$ and here we let $\\mathbb{E}_{k}:=\\mathbb{E}[\\cdot|\\mathcal{F}_{k}]$ . With a slight abuse of notation, we further consider $\\mathcal{F}_{k}^{s}:=\\left\\{y_{0},x_{0},y_{1},x_{1},...,y_{k},y_{k}^{1},...,y_{k}^{s}\\right\\}$ and correspondingly let $\\mathbb{E}_{k}^{s}:=\\mathbb{E}[\\cdot|\\mathcal{F}_{k}^{s}]$ . ", "page_idx": 26}, {"type": "text", "text": "Lemma 7 (Convergence under strong convexity and stochastic setting). Under stochastic setting and under the Assumption that $g$ is geodesic strongly convex, we can show $\\mathbb{E}_{k}^{s}d^{2}(y_{k}^{s+1},y^{\\ast}(x_{k}\\bar{)})\\ \\leq$ $\\begin{array}{r l}&{(1\\,+\\,\\eta_{y}^{2}\\zeta L^{2}\\,-\\,\\eta_{y}\\mu)d^{2}(y_{k}^{s},y^{*}(x_{k}))\\,+\\,\\frac{\\eta_{y}^{2}\\zeta M^{2}}{|B_{1}|}}\\\\ &{\\eta_{y}\\mu)^{S}d^{2}(y_{k},y^{*}(x_{k}))+\\frac{\\eta_{y}\\zeta M^{2}}{\\mu-\\eta_{y}\\zeta L^{2}}\\frac{1}{|B_{1}|}.}\\end{array}$ and $\\begin{array}{r l r}{\\mathbb{E}_{k-1}d^{2}(y_{k+1},y^{*}(x_{k}))}&{{}\\!\\!\\leq\\!\\!}&{(1\\,+\\,\\eta_{y}^{2}\\zeta L^{2}\\,-\\,}\\end{array}$ ", "page_idx": 27}, {"type": "text", "text": "Lemma 8. Under Assumption 4, we can bound Ek\u2225G F(xk) \u2212GF(xk)\u22252xk \u2264 4M 2+|B126|M 2\u03bal2 + $\\begin{array}{r}{\\frac{8M^{2}\\kappa_{l}^{2}}{|B_{3}|}+\\frac{16M^{2}\\kappa_{l}^{2}}{|B_{4}|}+2C_{\\mathrm{hinv}}^{2}d^{2}(y_{k+1},y^{*}(x_{k})).}\\end{array}$ ", "page_idx": 27}, {"type": "text", "text": "D.1 Proofs for the lemmas ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Proof of Lemma 6. Here we only prove one and the rest follows exactly. Due to the unbiasedness of the stochastic estimate, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\|\\mathcal{G}_{x}f_{i}(x,y)-\\mathcal{G}f(x,y)\\|_{x}^{2}=\\mathbb{E}\\|\\mathcal{G}_{x}f_{i}(x,y)\\|_{x}^{2}-\\|\\mathcal{G}_{x}f(x,y)\\|_{x}^{2}\\leq\\mathbb{E}\\|\\mathcal{G}_{x}f_{i}(x,y)\\|_{x}^{2}\\leq M^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where we use Assumption 4. ", "page_idx": 27}, {"type": "text", "text": "Proof of Lemma 7. Similarly from the proof of Lemma 5, we take expectation over $\\mathcal{F}_{k}^{s}$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{k}^{s}d^{2}(y_{k}^{s+1},y^{*}(x_{k}))}\\\\ &{\\le\\mathbb{E}_{k}^{s}\\big[d^{2}(y_{k}^{s},y^{*}(x_{k}))+\\eta_{y}^{2}\\zeta\\mathbb{E}_{k}^{s}\\big\\|{\\mathcal{G}_{y}\\mathcal{G}_{B_{1}}(x_{k},y_{k}^{s})}\\big\\|_{y_{k}^{s}}^{2}+2\\eta_{y}\\langle\\mathcal{G}_{y}\\mathcal{G}_{B_{1}}(x_{k},y_{k}^{s}),\\mathrm{Exp}_{y_{k}^{s}}^{-1}y^{*}(x_{k})\\rangle_{y_{k}^{s}}\\big]}\\\\ &{\\le d^{2}(y_{k}^{s},y^{*}(x_{k}))+\\eta_{y}^{2}\\zeta\\mathbb{E}_{k}^{s}\\big\\|{\\mathcal{G}_{y}\\mathcal{G}_{B_{1}}(x_{k},y_{k}^{s})}-{\\mathcal{G}_{y}\\mathcal{G}(x_{k},y_{k}^{s})}\\big\\|_{y_{k}^{s}}^{2}+\\eta_{y}^{2}\\zeta\\|\\mathcal{G}_{y}{\\mathcal{G}(x_{k},y_{k}^{s})}\\big\\|_{y_{k}^{s}}^{2}}\\\\ &{\\quad+\\,2\\eta_{y}\\langle\\mathcal{G}_{y}\\mathcal{G}(x_{k},y_{k}^{s}),\\mathrm{Exp}_{y_{k}^{s}}^{-1}y^{*}(x_{k})\\rangle_{y_{k}^{s}}}\\\\ &{\\le(1+\\eta_{y}^{2}\\zeta L^{2}-\\eta_{y}\\mu)d^{2}(y_{k}^{s},y^{*}(x_{k}))+\\eta_{y}^{2}\\zeta\\mathbb{E}_{k}^{s}\\frac{1}{|B_{1}|^{2}}\\sum_{i\\in B_{1}}\\mathbb{E}_{k}^{s}\\|\\mathcal{G}_{y}{\\mathcal{G}}_{i}(x_{k},y_{k}^{s})-\\mathcal{G}_{y}{\\mathcal{G}}(x_{k},y_{k}^{s})\\|_{y_{k}^{s}}^{2}}\\\\ &{\\le(1+\\eta_{y}^{2}\\zeta L^{2}-\\eta_{y} \n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where we use the strong convexity and the fact that $\\begin{array}{r l}{\\mathbb{E}\\|\\mathcal{G}_{y}g_{\\mathcal{B}_{1}}(x,y)\\ -\\ \\mathcal{G}_{y}g(x,y)\\|_{y}^{2}}&{{}=}\\end{array}$ $\\begin{array}{r}{\\frac{1}{|B_{1}|^{2}}\\mathbb{E}\\|\\sum_{i\\in B_{1}}(\\mathcal{G}_{y}g_{i}(x,y)-\\mathcal{G}_{y}g(x,y))\\|_{y}^{2}=\\frac{1}{|B_{1}|^{2}}\\sum_{i\\in B_{1}}\\mathbb{E}\\|\\mathcal{G}_{y}g_{i}(x,y)-\\mathcal{G}_{y}g(x,y)\\|_{y}^{2}}\\end{array}$ in the third inequality and Lemma 6 in the last inequality. Further, we telescope the inequality and taking the expectation $\\mathbb{E}_{k-1}$ gets ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{k-1}d^{2}(y_{k}^{S},y^{*}(x_{k}))\\leq(1+\\eta_{y}^{2}\\zeta L^{2}-\\eta_{y}\\mu)^{S}d^{2}(y_{k},y^{*}(x_{k}))+\\frac{\\eta_{y}^{2}\\zeta M^{2}}{|B_{1}|}\\sum_{s=0}^{S-1}(1+\\eta_{y}^{2}\\zeta L^{2}-\\eta_{y}\\mu)^{s}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq(1+\\eta_{y}^{2}\\zeta L^{2}-\\eta_{y}\\mu)^{S}d^{2}(y_{k},y^{*}(x_{k}))+\\frac{\\eta_{y}\\zeta M^{2}}{\\mu-\\eta_{y}\\zeta L^{2}}\\frac{1}{|B_{1}|},}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where we use the fact that $\\begin{array}{r}{\\sum_{s=0}^{S-1}\\theta^{s}\\le\\frac{1}{1-\\theta}}\\end{array}$ for $0<\\theta<1$ . ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{f o f L e m m a~\\delta.\\mathrm{{\\small~Recall}}\\quad}&{\\mathrm{{\\small~that}}\\quad}&{\\mathcal{G F}(x_{k})}&{=\\mathcal{G}_{x}f(x,y^{*}(x))}\\\\ &{(x,y^{*}(x))\\big[\\mathcal{H}_{y}^{-1}g(x,y^{*}(x))[\\mathcal{G}_{y}f(x,y^{*}(x))]\\big].\\mathrm{{\\small~Then}}}&&{}\\\\ &{\\mathbb{E}_{k}\\|\\hat{\\mathcal{G}}F(x_{k})-\\mathcal{G}F(x_{k})\\|_{x_{k}}^{2}}\\\\ &{\\quad}&{\\leq2\\mathbb{E}_{k}\\|\\hat{\\mathcal{G}}F(x_{k})-\\hat{\\mathcal{G}}_{\\mathrm{hinv}}F(x_{k})\\|_{x_{k}}^{2}+2\\|\\widehat{\\mathcal{G}}_{\\mathrm{hinv}}F(x_{k})-\\mathcal{G}F(x_{k})\\|_{x_{k}}^{2}}\\\\ &{\\quad}&{\\leq2\\mathbb{E}_{k}\\|\\hat{\\mathcal{G}}F(x_{k})-\\hat{\\mathcal{G}}_{\\mathrm{hinv}}F(x_{k})\\|_{x_{k}}^{2}+2C_{\\mathrm{hinv}}^{2}d^{2}(y_{k+1},y^{*}(x_{k}))}&{\\quad\\mathrm{(1)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the second inequality uses Lemma 1. Now we bound the first term $\\mathbb{E}_{k}\\Vert\\widehat{\\mathcal{G}}F(x_{k})\\mathrm{~-~}$ $\\widehat{\\mathcal{G}}_{\\mathrm{hinv}}F(x_{k})\\|_{x_{k}}^{2}$ as follows. ", "page_idx": 27}, {"type": "text", "text": "First we bound ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{k}\\|\\mathcal{H}_{y}^{-1}g_{B_{4}}(x_{k},y_{k+1})[\\mathcal{G}_{y}f_{B_{2}}(x_{k},y_{k+1})]-\\mathcal{H}_{y}^{-1}g(x_{k},y_{k+1})[\\mathcal{G}_{y}f(x_{k},y_{k+1})]\\|_{y_{k+1}}^{2}}&{{}=0}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq2\\mathbb{E}_{k}\\|\\mathcal{H}_{y}^{-1}g(x_{k},y_{k+1})[\\mathcal{G}_{y}f_{B_{2}}(x_{k},y_{k+1})-\\mathcal{G}_{y}f(x_{k},y_{k+1})]\\|_{y_{k+1}}^{2}}\\\\ &{\\quad+\\,2\\mathbb{E}_{k}\\|(\\mathcal{H}_{y}^{-1}g_{B_{4}}(x_{k},y_{k+1})-\\mathcal{H}_{y}^{-1}g(x_{k},y_{k+1}))[\\mathcal{G}_{y}f_{B_{2}}(x_{k},y_{k+1})]\\|_{y_{k+1}}^{2}}\\\\ &{\\leq2\\|\\mathcal{H}_{y}^{-1}g(x_{k},y_{k+1})\\|_{y_{k+1}}^{2}\\mathbb{E}_{k}\\|\\mathcal{G}_{y}f_{B_{2}}(x_{k},y_{k+1})-\\mathcal{G}_{y}f(x_{k},y_{k+1})\\|_{y_{k+1}}^{2}}\\\\ &{\\quad+\\,2\\mathbb{E}_{k}\\|\\mathcal{H}_{y}^{-1}g_{B_{4}}(x_{k},y_{k+1})-\\mathcal{H}_{y}^{-1}g(x_{k},y_{k+1})\\|_{y_{k+1}}^{2}\\mathbb{E}_{k}\\|\\mathcal{G}_{y}f_{B_{2}}(x_{k},y_{k+1})\\|_{y_{k+1}}^{2}}\\\\ &{\\leq\\frac{2M^{2}}{\\mu^{2}}(\\frac{1}{|\\mathcal{B}_{2}|}+\\frac{1}{|\\mathcal{B}_{4}|})}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where we notice that $\\begin{array}{r}{\\|\\mathcal{G}_{y}f_{B_{2}}(x_{k},y_{k+1})\\|_{y_{k+1}}\\leq\\frac{1}{|B_{2}|}\\sum_{i\\in B_{2}}\\|\\mathcal{G}_{y}f_{i}(x_{k},y_{k+1})\\|_{y_{k+1}}\\leq M.}\\end{array}$ ", "page_idx": 28}, {"type": "text", "text": "Hence, we can bound ", "text_level": 1, "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{k}\\|\\widehat{\\mathcal{G}}F(x_{k})-\\widehat{\\mathcal{G}}_{\\mathrm{hinv}}F(x_{k})\\|_{x_{k}}^{2}}\\\\ &{\\ \\ \\leq2\\mathbb{E}_{k}\\|\\mathcal{G}_{x}f_{B_{2}}(x_{k},y_{k+1})-\\mathcal{G}_{x}f(x_{k},y_{k+1})\\|_{x_{k}}^{2}+\\frac{8M^{2}}{\\mu^{2}}\\big(\\frac{1}{|B_{2}|}+\\frac{1}{|B_{4}|}\\big)\\mathbb{E}_{k}\\|\\mathcal{G}_{x y}^{2}g_{B_{3}}(x_{k},y_{k+1})\\|_{x_{k}}^{2}}\\\\ &{\\ \\ \\ \\ +\\,4\\mathbb{E}_{k}\\|\\mathcal{G}_{x y}^{2}g(x_{k},y_{k+1})-\\mathcal{G}_{x y}^{2}g_{B_{3}}(x_{k},y_{k+1})\\|_{x_{k}}^{2}\\|\\mathcal{H}_{y}^{-1}g(x_{k},y_{k+1})[\\mathcal{G}_{y}f(x_{k},y_{k+1})]\\|_{y_{k+1}}^{2}}\\\\ &{\\ \\ \\ \\leq\\frac{2M^{2}}{|B_{2}|}+8M^{2}\\kappa_{l}^{2}\\big(\\frac{1}{|B_{2}|}+\\frac{1}{|B_{4}|}\\big)+\\frac{4M^{2}\\kappa_{l}^{2}}{|B_{3}|},}&{(18)^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where we use Lemma 6 in the last inequality. Combining (18) with (17) yields the desired result. ", "page_idx": 28}, {"type": "text", "text": "D.2 Proof of Theorem 2 ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Proof of Theorem 2. From the smoothness of $F(x)$ (i.e., (7)) and taking full expectation we obtain, $\\begin{array}{r}{\\mathfrak{L}[F(x_{k+1})-F(x_{k})]\\le-\\big(\\frac{\\eta_{x}}{2}-\\eta_{x}^{2}L_{F}\\big)\\mathbb{E}\\|\\mathcal{G}F(x_{k})\\|_{x_{k}}^{2}+\\big(\\frac{\\eta_{x}}{2}+\\eta_{x}^{2}L_{F}\\big)\\mathbb{E}\\|\\mathcal{G}F(x_{k})-\\widehat{\\mathcal{G}}F(x_{k})\\|_{x_{k}}^{2}.}\\end{array}$ Further, we can bound ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}d^{2}(y_{k+1},y^{*}(x_{k+1}))}\\\\ &{\\ \\leq2\\mathbb{E}d^{2}(y_{k+1},y^{*}(x_{k}))+4\\eta_{x}^{2}\\kappa_{l}^{2}\\mathbb{E}\\|\\widehat{\\mathcal{G}}F(x_{k})-\\mathcal{G}F(x_{k})\\|_{x_{k}}^{2}+4\\eta_{x}^{2}\\kappa_{l}^{2}\\mathbb{E}\\|\\mathcal{G}F(x_{k})\\|_{x_{k}}}\\\\ &{\\ \\leq2(1+\\eta_{y}^{2}\\zeta L^{2}-\\eta_{y}\\mu)^{S}\\mathbb{E}d^{2}(y_{k},y^{*}(x_{k}))+\\frac{2\\eta_{y}\\zeta M^{2}}{\\mu-\\eta_{y}\\zeta L^{2}}\\frac{1}{|B_{1}|}+4\\eta_{x}^{2}\\kappa_{l}^{2}\\mathbb{E}\\|\\mathcal{G}F(x_{k})\\|_{x_{k}}}\\\\ &{\\ \\ \\ +4\\eta_{x}^{2}\\kappa_{l}^{2}\\mathbb{E}\\|\\widehat{\\mathcal{G}}F(x_{k})-\\mathcal{G}F(x_{k})\\|_{x_{k}}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where we use Lemma 7 and 8 in the second inequality. ", "page_idx": 28}, {"type": "text", "text": "Next, we construct a Lyapunov function as $R_{k}:=F(x_{k})+d^{2}(y_{k},y^{\\ast}(x_{k}))$ . Then ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad_{\\mathrm{F}}(\\nu_{k+\\mathrm{res}},\\nu_{k+\\mathrm{s}})-_{\\mathrm{F}}(x_{k})]+\\mathbb{E}[d^{2}(\\nu_{k+1},\\nu^{*}(x_{k+1})-d^{2}(\\nu_{k},\\nu^{*}(x_{k}))]}\\\\ &{\\le\\mathbb{E}\\big[F(x_{k+\\mathrm{res}})-F(x_{k})\\big]^{2}\\mathbb{E}\\big[\\mathcal{S}(F(x_{k}))\\big]_{\\le t}^{2}+\\frac{(\\eta_{k}^{2}-\\eta_{k}^{2})F_{k}+\\eta_{k}^{2}\\sigma_{k}^{2}\\nu^{2}}{2\\mu_{k}\\nu}\\mathbb{E}\\big[\\mathcal{G}F(x_{k})-\\hat{\\mathcal{G}}F(x_{k})\\big]_{\\ge0}^{2}}\\\\ &{\\quad+\\Big((2(1+\\eta_{k}^{2})\\zeta^{2}-\\eta_{k}\\mu_{k}^{2})\\nabla-1\\big)\\mathbb{E}d^{2}(\\eta_{k},\\nu^{*}(x_{k}))+\\frac{2\\eta_{k}\\sqrt{d^{2}}}{\\mu_{k}-\\eta_{k}\\sqrt{\\zeta^{2}}}\\mathbb{E}\\big[\\mathcal{G}\\big.\\nu.}\\\\ &{=-\\frac{1}{8\\mathrm{LF}}\\mathbb{E}\\big[\\|\\mathcal{F}(x_{k})\\|_{\\infty}^{2}+\\frac{3}{8\\mathrm{LF}}\\mathbb{E}\\big[\\mathbb{E}_{k}\\big](\\mathcal{F}F(x_{k})-\\hat{\\mathcal{G}}F(x_{k})\\big]_{\\ge1}^{2}\\big]}\\\\ &{\\quad+\\Big((2(1+\\eta_{k}^{2})\\zeta^{2}-\\eta_{k}\\mu_{k}^{2})\\nabla-1\\big)\\mathbb{E}d^{2}(y_{k},\\nu^{*}(x_{k}))+\\frac{2\\eta_{k}\\sqrt{d^{2}}}{\\mu_{k}\\sqrt{\\zeta^{2}}}\\frac{\\big(1-\\eta_{k}^{2}\\big)}{[1+\\eta_{k}]}}\\\\ &{\\le-\\frac{1}{8\\mathrm{LF}}\\mathbb{E}\\big[\\|\\mathcal{F}(x_{k})\\|_{\\infty}^{2}+\\frac{3}{8\\mathrm{LF}}\\big(\\frac{4\\lambda^{2}-16\\lambda\\eta^{2}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad+\\cfrac{3}{80L_{F}}\\Big(\\frac{4M^{2}+16M^{2}\\kappa_{l}^{2}}{|\\mathcal{B}_{2}|}+\\frac{8M^{2}\\kappa_{l}^{2}}{|\\mathcal{B}_{3}|}+\\frac{16M^{2}\\kappa_{l}^{2}}{|\\mathcal{B}_{4}|}\\Big)+\\big(\\frac{3C_{\\mathrm{hinv}}^{2}}{40L_{F}}+2\\big)\\frac{\\eta_{y}\\zeta M^{2}}{\\mu-\\eta_{y}\\zeta L^{2}}\\frac{1}{|\\mathcal{B}_{1}|}}\\\\ &{\\leq-\\cfrac{1}{80L_{F}}\\mathbb{E}\\|\\mathcal{G}F(x_{k})\\|_{x_{k}}^{2}+\\cfrac{3}{80L_{F}}\\Big(\\frac{4M^{2}+16M^{2}\\kappa_{l}^{2}}{|\\mathcal{B}_{2}|}+\\frac{8M^{2}\\kappa_{l}^{2}}{|\\mathcal{B}_{3}|}+\\frac{16M^{2}\\kappa_{l}^{2}}{|\\mathcal{B}_{4}|}\\Big)+\\big(\\frac{3C_{\\mathrm{hinv}}^{2}}{40L_{F}}+2\\big)\\frac{\\eta_{y}\\zeta M^{2}}{\\mu-\\eta_{y}\\zeta}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "where we choose \u03b7x =2 01LF in the first equality and S \u2265log(80LF4 0+L3FCh2inv ) $\\begin{array}{r}{S\\ge\\log(\\frac{40L_{F}}{80L_{F}+3C_{\\mathrm{hinv}}^{2}})/\\log(1\\!+\\!\\eta_{y}^{2}\\zeta L^{2}\\!-\\!\\eta_{y}\\mu)=}\\end{array}$ $\\widetilde{\\Theta}(\\kappa_{l}^{2}\\zeta)$ for the last inequality. Telescoping the result gives ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\frac{1}{K}\\sum_{k=0}^{K-1}\\mathbb{E}\\|\\mathcal{G}F(x_{k})\\|_{x_{k}}^{2}\\leq\\frac{80L_{F}R_{0}}{K}+\\Big(\\frac{12M^{2}+48M^{2}\\kappa_{l}^{2}}{|{\\mathcal{B}}_{2}|}+\\frac{24M^{2}\\kappa_{l}^{2}}{|{\\mathcal{B}}_{3}|}+\\frac{48M^{2}\\kappa_{l}^{2}}{|{\\mathcal{B}}_{4}|}\\Big)}\\\\ {\\displaystyle+\\left(6C_{\\mathrm{hinv}}^{2}+160L_{F}\\right)\\!\\frac{\\eta_{y}\\zeta M^{2}}{\\mu-\\eta_{y}\\zeta L^{2}}\\frac{1}{|{\\mathcal{B}}_{1}|}}\\\\ {\\displaystyle}&{\\leq\\frac{80L_{F}R_{0}}{K}+\\frac{\\epsilon}{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where the last inequality follows from the choice that $\\begin{array}{r}{|\\mathcal{B}_{1}|\\,\\geq\\,(24C_{\\mathrm{hinv}}^{2}+640L_{F})\\frac{8\\eta_{y}\\zeta M^{2}}{\\mu-\\eta_{y}\\zeta L^{2}}/\\epsilon\\,=\\,}\\end{array}$ $\\Theta(\\kappa_{l}^{4}/\\epsilon)$ $\\begin{array}{r}{|B_{2}|\\,\\ge\\,\\frac{144M^{2}+576M^{2}\\kappa_{L}^{2}}{\\epsilon}\\,=\\,\\Theta(\\kappa_{l}^{2}/\\epsilon),\\,|B_{3}|\\,\\ge\\,\\frac{288M^{2}\\kappa_{L}^{2}}{\\epsilon}\\,=\\,\\Theta(\\kappa_{l}^{2}/\\epsilon),\\,|B_{4}|\\,\\ge\\,\\frac{576M^{2}\\,,\\,|B_{4}|^{2}}{\\epsilon}\\,=\\,\\frac{2}{3}\\frac{\\epsilon}{\\epsilon}\\,=\\,0.}\\end{array}$ $\\begin{array}{r}{|B_{4}|\\ge\\frac{576M^{2}\\kappa_{l}^{2}}{\\epsilon}=}\\end{array}$ $\\Theta(\\kappa_{l}^{2}/\\epsilon)$ in the last inequality. ", "page_idx": 29}, {"type": "text", "text": "In order to reach $\\epsilon$ -stationary solution, we require $K=O(\\kappa_{l}^{3}\\epsilon^{-1})$ and thus the (stochastic) gradient complexity for $f$ is $G_{f}\\,=\\,2K|\\mathcal{B}_{2}|=O(\\kappa_{l}^{5}\\epsilon^{-2})$ and for $g$ is $G_{g}\\,=\\,K S|B_{1}|\\,=\\,\\widetilde O(\\kappa_{l}^{9}\\zeta\\epsilon^{-2})$ . The complexity for cross-derivative is $K|B_{3}|=O(\\kappa_{l}^{5}\\epsilon^{-2})$ . \u53e3 ", "page_idx": 29}, {"type": "text", "text": "E Proofs for Section 3.4 ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Proof of Theorem 3. We first give a complete proof for the Hessian inverse estimator as follows. For the other estimators, we only provide a proof sketch. ", "page_idx": 29}, {"type": "text", "text": "Proof for HINV. (1) First, we derive the convergence under strong convexity using retraction. By the trigonometric distance bound ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d^{2}(y_{k}^{s+1},y^{*}(x_{k}))}\\\\ &{\\leq d^{2}(y_{k}^{s},y^{*}(x_{k}))+\\zeta d^{2}(y_{k}^{s},y_{k}^{s+1})-2\\langle\\mathrm{Exp}_{y_{k}^{s}}^{-1}y_{k}^{s+1},\\mathrm{Exp}_{y_{k}^{s}}^{-1}y^{*}(x_{k})\\rangle_{y_{k}^{s}}}\\\\ &{\\leq d^{2}(y_{k}^{s},y^{*}(x_{k}))+\\eta_{y}^{2}\\langle c\\vert\\|_{\\mathcal{G}}(x_{k},y_{k}^{s})\\|_{y_{k}^{s}}^{2}-2\\langle\\mathrm{Exp}_{y_{k}^{s}}^{-1}y_{k}^{s+1}-\\mathrm{Retr}_{y_{k}^{s}}^{-1}y_{k}^{s+1},\\mathrm{Exp}_{y_{k}^{s}}^{-1}y^{*}(x_{k})\\rangle_{y_{k}^{s}}}\\\\ &{\\quad+\\,2\\eta_{y}\\langle\\mathcal{G}_{y}\\boldsymbol{g}(x_{k},y_{k}^{s}),\\mathrm{Exp}_{y_{k}^{s}}^{-1}y^{*}(x_{k})\\rangle_{y_{k}^{s}}}\\\\ &{\\leq d^{2}(y_{k}^{s},y^{*}(x_{k}))+\\eta_{y}^{2}\\zeta\\sigma\\vert\\mathcal{G}_{y}{g}(x_{k},y_{k}^{s})\\|_{y_{k}^{s}}^{2}+2\\eta_{y}\\langle\\mathcal{G}_{y}{g}(x_{k},y_{k}^{s}),\\mathrm{Exp}_{y_{k}^{s}}^{-1}y^{*}(x_{k})\\rangle_{y_{k}^{s}}}\\\\ &{\\quad+\\,2\\tilde{D}\\|\\mathrm{Exp}_{y_{k}^{s}}^{-1}y_{k}^{s+1}-\\mathrm{Retr}_{y_{k}^{s}}^{-1}y_{k}^{s+1}\\|_{y_{k}^{s}}}\\\\ &{\\leq d^{2}(y_{k}^{s},y^{*}(x_{k}))+\\eta_{y}^{2}(\\zeta\\sigma+2\\tilde{D}\\gamma_{G})\\|\\mathcal{G}_{y}{g\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where we use Assumption 5 in the second inequality and fourth inequality. We require $\\eta_{y}~<$ (\u03b6c+2 D\u00b5\u00afcR)L2 in order to achieve linear convergence. For simplicity, we let \u03c4 = \u00b5\u03b7y \u2212\u03b7y2(\u03b6c+2 D\u00afcR). This leads to $d^{2}(y_{k}^{s+1},y^{*}(x_{k}))\\leq(1-\\tau)d^{2}(y_{k}^{s},y^{*}(x_{k})).$ ", "page_idx": 29}, {"type": "text", "text": "(2) Next, we notice the bound on hypergradient approximation error still holds as $\\parallel\\!\\widehat{\\mathcal{G}}_{\\mathrm{hinv}}F(x_{k})-$ $\\mathcal{G}F(x_{k})\\|_{x_{k}}\\,\\le\\,C_{\\mathrm{hinv}}d\\bigl(y^{\\ast}(x_{k}),y_{k+1}\\bigr)$ , where $C_{\\mathrm{hinv}}\\,=\\,L+\\kappa_{\\rho}M+\\kappa_{l}L+\\kappa_{l}\\kappa_{\\rho}M$ . Further, by $L$ -smoothness, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{l}{F(x_{k+1})-F(x_{k})}\\\\ {\\lefteqn{\\le\\langle\\mathcal{G}F(x_{k}),\\mathrm{Exp}_{x_{k}}^{-1}x_{k+1}\\rangle_{x_{k}}+\\frac{L_{F}}{2}d^{2}(x_{k},x_{k+1})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\leq\\langle\\mathcal{G}F(x_{k}),\\mathrm{Exp}_{x_{k}}^{-1}x_{k+1}-\\mathrm{Retr}_{x_{k}}^{-1}x_{k+1}\\rangle_{x_{k}}-\\eta_{x}\\langle\\mathcal{G}F(x_{k}),\\hat{\\mathcal{G}}F(x_{k})\\rangle_{x_{k}}+\\frac{\\overline{{c}}L_{F}\\eta_{x}^{2}}{2}\\|\\hat{\\mathcal{G}}F(x_{k})\\|_{x_{k}}^{2}}}\\\\ &{\\leq\\left(2\\kappa_{l}M c_{R}+\\frac{\\overline{{c}}L_{F}}{2}\\right)\\eta_{x}^{2}\\|\\hat{\\mathcal{G}}F(x_{k})\\|_{x_{k}}^{2}-\\eta_{x}\\langle\\mathcal{G}F(x_{k}),\\hat{\\mathcal{G}}F(x_{k})\\rangle_{x_{k}}}\\\\ &{\\leq\\left(4\\kappa_{l}M c_{R}+\\overline{{c}}L_{F}\\right)\\eta_{x}^{2}\\|\\hat{\\mathcal{G}}F(x_{k})-\\mathcal{G}F(x_{k})\\|_{x_{k}}^{2}+\\left(4\\kappa_{l}M c_{R}+\\overline{{c}}L_{F}\\right)\\eta_{x}^{2}\\|\\mathcal{G}F(x_{k})\\|_{x_{k}}^{2}}\\\\ &{\\quad+\\frac{\\eta_{x}}{2}\\|\\mathcal{G}F(x_{k})-\\hat{\\mathcal{G}}F(x_{k})\\|_{x_{k}}^{2}-\\frac{\\eta_{x}}{2}\\|\\mathcal{G}F(x_{k})\\|_{x_{k}}^{2}}\\\\ &{=-\\bigg(\\frac{\\eta_{x}}{2}-\\left(4\\kappa_{l}M c_{R}+\\overline{{c}}L_{F}\\right)\\eta_{x}^{2}\\bigg)\\|\\mathcal{G}F(x_{k})\\|_{x_{k}}^{2}+\\left(\\frac{\\eta_{x}}{2}+\\left(4\\kappa_{l}M c_{R}+\\overline{{c}}L_{F}\\right)\\eta_{x}^{2}\\right)\\|\\mathcal{G}F(x_{k})-\\hat{\\mathcal{G}}F(x_{k})\\|_{x_{k}}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where in the third inequality, we bound $\\begin{array}{r}{\\|\\mathcal{G}F(x_{k})\\|_{x_{k}}\\le M+\\frac{L}{\\mu}M\\le\\frac{2L M}{\\mu}}\\end{array}$ ", "page_idx": 30}, {"type": "text", "text": "(3) Then we can bound ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d^{2}(y_{k},y^{*}(x_{k}))\\leq2d^{2}(y_{k-1}^{S},y^{*}(x_{k-1}))+2d^{2}(y^{*}(x_{k}),y^{*}(x_{k-1}))}\\\\ &{\\qquad\\qquad\\qquad\\leq2(1-\\tau)^{S}d^{2}(y^{*}(x_{k-1}),y_{k-1})+2\\eta_{x}^{2}\\kappa_{l}^{2}\\bar{c}\\|\\hat{\\mathcal{G}}_{\\mathrm{hinv}}F(x_{k-1})\\|_{x_{k}}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq2(1+2\\eta_{x}^{2}\\kappa_{l}^{2}C_{\\mathrm{hinv}}^{2}\\bar{c})(1-\\tau)^{S}d^{2}(y^{*}(x_{k-1}),y_{k-1})+4\\eta_{x}^{2}\\kappa_{l}^{2}\\bar{c}\\|\\mathcal{G}F(x_{k-1})\\|_{x_{k-1}}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where the last inequality follows similarly as (9). ", "page_idx": 30}, {"type": "text", "text": "Let a Lyapunov function be $R_{k}:=F(x_{k})+d^{2}(y_{k},y^{\\ast}(x_{k}))$ . Then ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad_{*}\\kappa+1\\quad_{*}\\kappa_{*}}\\\\ &{\\le-\\bigg(\\frac{\\eta_{x}}{2}-(4\\kappa_{l}M c_{R}+\\bar{c}L_{F})\\eta_{x}^{2}\\bigg)\\|\\mathcal{G}F(x_{k})\\|_{x_{k}}^{2}+\\bigg(\\frac{\\eta_{x}}{2}+\\big(4\\kappa_{l}M c_{R}+\\bar{c}L_{F}\\big)\\eta_{x}^{2}\\bigg)\\|\\mathcal{G}F(x_{k})-\\hat{\\mathcal{G}}F(x_{k})}\\\\ &{\\quad+\\bigg(\\big((2+4\\eta_{x}^{2}\\kappa_{l}^{2}C_{\\mathrm{hin}}^{2}\\bar{c})(1-\\tau)^{S}-1\\big)d^{2}\\big(y^{*}(x_{k}),y_{k}\\big)+4\\eta_{x}^{2}\\kappa_{l}^{2}\\bar{c}\\big\\|\\mathcal{G}F(x_{k})\\big\\|_{x_{k}}^{2}\\bigg)}\\\\ &{\\le-\\bigg(\\frac{\\eta_{x}}{2}-\\big(4\\kappa_{l}M c_{R}+\\bar{c}L_{F}\\big)\\eta_{x}^{2}-4\\eta_{x}^{2}\\kappa_{l}^{2}\\bar{c}\\big)\\|\\mathcal{G}F(x_{k})\\|_{x_{k}}^{2}}\\\\ &{\\quad+\\bigg(\\big(2+C_{\\mathrm{hin}}^{2}(\\frac{\\eta_{x}}{2}+(4\\kappa_{l}M c_{R}+\\bar{c}L_{F})\\eta_{x}^{2})+4\\eta_{x}^{2}\\kappa_{l}^{2}C_{\\mathrm{hin}}^{2}\\bar{c}\\big)(1-\\tau)^{S}-1\\bigg)d^{2}\\big(y^{*}(x_{k}),y_{k}\\big)}\\\\ &{\\le-\\bigg(\\frac{\\eta_{x}}{2}-\\bar{L}_{F}\\eta_{x}^{2}\\bigg)\\|\\mathcal{G}F(x_{k})\\|_{x_{k}}^{2}+\\bigg(\\big(2+C_{\\mathrm{hin}}^{2}(\\frac{\\eta_{x}}{2}+\\bar{L}_{F}\\eta_{x}^{2})\\big)(1-\\tau)^{S}-1\\bigg)d^{2}\\big(y^{*}(x_{k}),y_{k}\\big)}\\\\ &{\\le-\\frac{1}{16\\hat{L}_{F}}\\|\\mathcal \n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where we use $\\kappa_{l}^{2}\\bar{c}\\leq L_{F}\\bar{c}$ and let $\\tilde{L}_{F}\\,:=\\,4\\kappa_{l}c_{R}M+5\\bar{c}L_{F}$ in the second last inequality, and we choose $\\begin{array}{r}{\\eta_{x}\\,=\\,\\frac{1}{4\\tilde{L}_{F}}}\\end{array}$ $\\begin{array}{r}{\\mathrm{~,~}S\\geq\\log\\big(\\frac{16\\tilde{L}_{F}}{32\\tilde{L}_{F}+3C_{\\mathrm{hinv}}^{2}}\\big)/\\log(1-\\tau)\\,=\\,\\widetilde\\Theta(\\kappa_{l}^{2}\\zeta)}\\end{array}$ , in the last inequality. Then telescoping the results yields Finally, we sum over $k=0,...,K-1$ , which leads to ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\frac1K\\sum_{k=0}^{K-1}\\|\\mathcal G F(x_{k})\\|_{x_{k}}^{2}\\le\\frac{16\\tilde{L}_{F}R_{0}}K.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Thus in order to achieve $\\epsilon$ -stationary solution, we require $K={\\cal O}(\\tilde{L}_{F}\\epsilon^{-1})={\\cal O}(\\kappa_{l}^{3}\\epsilon^{-1})$ and hence the order of gradient and second-order complexities remain unchanged. ", "page_idx": 30}, {"type": "text", "text": "Extensions to other estimators. To extend the proof to other hypergradient estimators, we first notice that the convergence of inner iterations for solving lower-level problems is agnostic to the choice of hypergradient estimators, i.e., ", "page_idx": 30}, {"type": "equation", "text": "$$\nd^{2}\\big(y_{k}^{s+1},y^{*}(x_{k})\\big)\\leq(1-\\tau)d^{2}\\big(y_{k}^{s},y^{*}(x_{k})\\big),\\quad\\tau=\\mu\\eta_{y}-\\eta_{y}^{2}(\\zeta\\bar{c}+2D c_{R}).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "(1) Hypergradient approximation error. For hypergradient estimator based on Hessian inverse, conjugate gradient, truncated Neumann series, the hypegradient approximation error remains the same as in Lemma 1, given no retraction is involved in the computation. That is, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\widehat{\\mathcal{G}}_{\\mathrm{cg}}F(x_{k})-\\mathcal{G}F(x_{k})\\|_{x_{k}}\\leq C_{\\mathrm{cg}}d(y^{*}(x_{k}),y_{k+1})+2L\\sqrt{\\kappa_{l}}\\big(\\frac{\\sqrt{\\kappa_{l}}-1}{\\sqrt{\\kappa_{l}}+1}\\big)^{T}\\|\\hat{v}_{k}^{0}-\\Gamma_{y^{*}(x_{k})}^{y_{k+1}}v_{k}^{*}\\|_{y_{k+1}},}\\\\ &{\\geqslant C_{\\mathrm{cg}}=L+\\kappa_{\\rho}M+L(1+2\\sqrt{\\kappa_{l}})(\\kappa_{l}+\\frac{M\\kappa_{\\rho}}{\\mu}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "For hypergradient based on automatic differentiation $(\\mathbf{AD})$ , we first show that there exists a constant $C_{4}$ (that depends on $C_{3},\\bar{c},c_{R})$ such that $\\mathrm{D}_{x}\\mathrm{Retr}_{x}(u)\\,=\\,\\mathcal{P}_{\\mathrm{Retr}_{x}(u)}(\\mathrm{id}+\\mathrm{D}_{x}u)+\\mathcal{E}$ . with $\\|\\mathcal{E}\\|_{\\mathrm{Retr}_{x}(u)}\\leq C_{4}\\|\\mathrm{D}_{x}u\\|_{x}\\|u\\|_{x}$ . Such a result can be derived by bounding the difference between retraction and exponential map. Then we follow the analysis for Lemma 1 as follows. Given $y_{k}^{s+1}=\\operatorname{Retr}_{y_{k}^{s}}(-\\bar{\\eta}_{y}\\mathcal{G}_{y}g(x_{k},y_{k}^{\\bar{s}}))$ , we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{D}_{x_{k}}y_{k}^{s+1}=\\mathcal{P}_{y_{k}^{s+1}}\\big((\\mathrm{id}-\\eta_{y}\\mathcal{H}_{y}g(x_{k},y_{k}^{s}))\\mathrm{D}_{x_{k}}y_{k}^{s}-\\eta_{y}\\mathcal{G}_{y x}^{2}g(x_{k},y_{k}^{s})\\big)+\\mathcal{E}_{k}^{s}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $\\|\\mathcal{E}_{k}^{s}\\|_{y_{k}^{s+1}}\\leq\\eta_{y}^{2}C_{4}\\big((1-\\eta_{y}\\mu)C_{1}\\!+\\!\\eta_{y}L\\big)\\|\\mathcal{G}_{y}f(x_{k},y_{k}^{s})\\|_{y_{k}^{s}}$ . The rest of the proof follows exactly from Lemma 1, where we replace the convergence of inner iteration with the updated rate. This gives ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\widehat{\\mathcal{G}}_{\\mathrm{ad}}F(x_{k})-\\mathcal{G}F(x_{k})\\|_{x_{k}}}\\\\ &{\\leq\\Big(\\displaystyle\\frac{2M\\widetilde{C}^{\\prime}}{\\mu-\\eta_{y}(\\zeta\\overline{{c}}+2D c_{R})}+L(1+\\kappa_{l})\\Big)(1-\\tau)^{\\frac{S-1}{2}}d(y_{k},y^{*}(x_{k}))+M\\kappa_{l}(1-\\eta_{y}\\mu)^{S},}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $\\widetilde{C}^{\\prime}:=(\\kappa_{l}+1)\\rho+(C_{2}+\\eta_{y}C_{4})L\\big((1-\\eta_{y}\\mu)C_{1}+\\eta_{y}L\\big)$ ", "page_idx": 31}, {"type": "text", "text": "In summary ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bullet\\;{\\bf A D}\\colon\\|\\widehat{\\mathcal{G}}_{\\mathrm{ad}}F(x_{k})-\\mathcal{G}F(x_{k})\\|_{x_{k}}\\leq C_{\\mathrm{ad}}(1-\\tau)^{\\frac{S-1}{2}}d(y_{k},y^{\\ast}(x_{k}))+M\\kappa_{l}(1-\\eta_{y}\\mu)^{S},}\\\\ &{\\quad\\frac{2M\\tilde{C}^{\\prime}}{\\mu-\\eta_{y}(\\zeta\\bar{c}+2D c_{R})}+L(1+\\kappa_{l}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "(2) Objective decrement. This part is also the same across all hypergradient estimators, i.e., ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overline{{\\gamma}}(x_{k+1})-F(x_{k})}\\\\ &{\\leq-\\bigg(\\displaystyle\\frac{\\eta_{x}}{2}-\\left(4\\kappa_{l}M c_{R}+\\bar{c}L_{F}\\right)\\eta_{x}^{2}\\bigg)\\left\\|\\mathcal G F(x_{k})\\right\\|_{x_{k}}^{2}+\\Big(\\displaystyle\\frac{\\eta_{x}}{2}+\\left(4\\kappa_{l}M c_{R}+\\bar{c}L_{F}\\right)\\eta_{x}^{2}\\Big)\\left\\|\\mathcal G F(x_{k})-\\widehat{\\mathcal G}F(x_{k})\\right\\|_{x_{k}}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "(3) Lyapunov function decrement. The definition of Lyapunov function depends on the choice of hypergradient estimator. ", "page_idx": 31}, {"type": "text", "text": "For CG, we define $\\begin{array}{r}{R_{k}:=F(x_{k})+d^{2}(y_{k},y^{*}(x_{k}))+\\|\\hat{v}_{k}^{0}-\\Gamma_{y^{*}(x_{k})}^{y_{k+1}}v_{k}^{*}\\|_{y_{k+1}}^{2}}\\end{array}$ . Then following similar analysis, we first bound ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d^{2}(y_{k+1},y^{*}(x_{k+1}))\\leq(2+8\\eta_{x}^{2}\\kappa_{l}^{2}C_{\\mathrm{cg}}^{2})(1-\\tau)^{S}d^{2}(y^{*}(x_{k}),y_{k})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\,32\\eta_{x}^{2}\\kappa_{l}^{3}L^{2}\\bigl(\\frac{\\sqrt{\\kappa_{l}}-1}{\\sqrt{\\kappa_{l}}+1}\\bigl)^{2T}\\|\\hat{v}_{k}^{0}-\\Gamma_{y^{*}(x_{k})}^{y_{k+1}}v_{k}^{*}\\|_{y_{k+1}}^{2}+4\\eta_{x}^{2}\\kappa_{l}^{2}\\bar{c}\\|\\mathcal{G}F(x_{k})\\|_{x_{k}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "and ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{v}_{k}^{0}-\\Gamma_{y^{*}(x_{k})}^{y_{k+1}}v_{k}^{*}\\|_{y_{k+1}}}\\\\ &{\\leq2\\sqrt{\\kappa_{l}}\\big(\\frac{\\sqrt{\\kappa_{l}}-1}{\\sqrt{\\kappa_{l}}+1}\\big)^{T}\\|\\hat{v}_{k-1}^{0}-\\Gamma_{y^{*}(x_{k-1})}^{y_{k}}v_{k-1}^{*}\\|_{y_{k}}+2\\sqrt{\\kappa_{l}}(\\kappa_{l}+\\frac{M\\kappa_{\\rho}}{\\mu})(1-\\tau)^{\\frac{\\delta}{2}}d(y^{*}(x_{k-1}),y_{k-1})}\\\\ &{\\quad+\\left\\|v_{k}^{*}-\\Gamma_{y^{*}(x_{k-1})}^{y^{*}(x_{k})}v_{k-1}^{*}\\right\\|_{y_{k}}+\\frac{2M C_{0}D}{\\mu}d(y_{k},y^{*}(x_{k}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Then similarly, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{v_{k}^{*}-\\Gamma_{y^{*}}^{y^{*}(x_{k})}v_{k-1}^{*}\\Vert_{y^{*}(x_{k})}\\leq C_{v}d(x_{k},x_{k-1})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\eta_{x}\\bar{c}C_{v}\\Vert\\widehat{\\mathcal{G}}_{\\mathrm{cg}}F(x_{k-1})-\\mathcal G F(x_{k-1})\\Vert_{x_{k-1}}+\\eta_{x}\\bar{c}C_{v}\\Vert\\mathcal G F(x_{k-1})\\Vert_{x_{k-1}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where in the second inequality, we use the bound between retraction and exponential map as well as triangle inequality. Then combining the above two results, gives ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\|\\hat{v}_{k}^{0}-\\Gamma_{y^{*}(x_{k})}^{y_{k+1}}v_{k}^{*}\\|_{y_{k+1}}^{2}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\leq20\\kappa_{l}\\big(\\frac{\\sqrt{\\kappa_{l}}-1}{\\sqrt{\\kappa_{l}}+1}\\big)^{2T}\\|\\hat{v}_{k-1}^{0}-\\Gamma_{y^{*}(x_{k-1})}^{y_{k}}v_{k-1}^{*}\\|_{y_{k}}^{2}+20\\kappa_{l}\\big(\\kappa_{l}+\\frac{M\\kappa_{\\rho}}{\\mu}\\big)^{2}(1-\\tau)^{S}d(y^{*}(x_{k-1}),y_{k-1})}\\\\ {\\displaystyle\\quad+\\,5\\bar{c}\\eta_{x}^{2}C_{v}^{2}\\|\\widehat{Q}_{\\mathrm{eg}}F(x_{k-1})-\\mathcal{G}F(x_{k-1})\\|_{x_{k-1}}^{2}+5\\bar{c}\\eta_{x}^{2}C_{v}^{2}\\|\\mathcal{G}F(x_{k-1})\\|_{x_{k-1}}^{2}+\\frac{5M^{2}C_{0}^{2}D^{2}}{\\mu^{2}}d^{2}\\big(y_{k},y^{*}(x_{k-1})\\big)^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Then we can show ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{k+1}-R_{k}}\\\\ &{\\leq-(\\frac{\\eta_{k}}{2}-\\eta_{k}^{2}\\hat{L}_{k}-5\\eta_{k}^{2}C_{\\epsilon}^{2})\\|\\mathcal{G}(\\boldsymbol{F}(\\boldsymbol{x}_{k})\\|_{\\mathcal{S}_{k}}^{2}+(\\frac{\\eta_{k}}{2}+\\eta_{k}^{2}\\hat{L}_{k}+5\\eta_{\\epsilon}^{2}C_{\\epsilon}^{2})\\|\\mathcal{G}\\boldsymbol{F}(\\boldsymbol{x}_{k})-\\hat{\\mathcal{G}}_{\\boldsymbol{\\epsilon}_{k}}\\boldsymbol{F}(\\boldsymbol{x}_{k})\\|_{\\mathcal{F}_{k}}^{2}}\\\\ &{\\quad+\\left(\\frac{5\\hat{M}^{2}C_{\\epsilon}^{2}D^{2}}{\\mu^{2}}+1\\right)d^{2}(y_{k+1},y^{*}(\\boldsymbol{x}_{k+1}))+\\left(20\\kappa_{k}(\\kappa_{k}+\\frac{M\\kappa_{\\epsilon}}{\\mu})^{2}(1-\\tau)^{\\delta}-1\\right)d^{2}(y^{*}(\\boldsymbol{x}_{k}),y_{k})}\\\\ &{\\quad+\\left(20\\kappa_{k}(\\frac{\\sqrt{\\kappa_{l}}-1}{\\sqrt{\\kappa_{l}}})^{2\\tau}-1\\right)\\|\\hat{\\nu}_{k}^{0}-\\Gamma_{y^{*}(\\epsilon_{k})}^{0+1}\\boldsymbol{\\nu}_{k}^{*}\\|_{\\mathcal{F}_{k}+1}^{2}}\\\\ &{\\leq-\\Big(\\frac{\\eta_{k}}{2}-6\\eta_{k}^{2}\\Big)\\|\\mathcal{G}(\\boldsymbol{F}(\\boldsymbol{x}_{k})\\|_{\\mathcal{F}_{k}}^{2}+(\\frac{\\eta_{k}}{2}+6\\eta_{k}^{2}\\hat{\\mathcal{N}})\\|\\mathcal{F}(\\boldsymbol{x}_{k})-\\hat{\\mathcal{G}}_{\\boldsymbol{\\epsilon}_{k}}\\boldsymbol{F}(\\boldsymbol{x}_{k})\\|_{\\mathcal{F}_{k}}^{2}}\\\\ &{\\quad+\\left((\\frac{5\\hat{M}^{2}C_{\\epsilon}^{2}D^{2}}{\\mu^{2}}+1)(2+8\\eta_{k}^{2}\\hat{L}_{k}^{2}C_{\\epsilon}^{2})+20\\kappa_{k}(\\kappa_{\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where we let \u039b  := Cv2c\u00af + \u03bal2 ( 5M 2\u00b5C02D2 and without loss of generality $\\tilde{L}_{F}\\leq\\tilde{\\Lambda}$ . The second inequality is by (19). The last inequality is by appropriately choosing $S,T$ , which is on the same order as the exponential map case. Then telescoping the result yields ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\frac1K\\sum_{k=0}^{K-1}\\|\\mathcal G F(x_{k})\\|_{x_{k}}^{2}\\le\\frac{96\\widetilde{\\Lambda}R_{0}}{K}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "For NS, we define $R_{k}=F(x_{k})+d^{2}(y_{k},y^{*}(x_{k}))$ and derive ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{t^{2}(y_{k+1},y^{*}(x_{k+1}))\\leq2(1-\\tau)^{S}d^{2}(y^{*}(x_{k}),y_{k})+4\\eta_{x}^{2}\\kappa_{l}^{2}\\bar{c}||\\widehat{\\mathcal{G}}_{\\mathrm{ns}}F(x_{k})-\\mathcal{G}F(x_{k})||_{x_{k}}^{2}+4\\eta_{x}^{2}\\kappa_{l}^{2}\\bar{c}||\\mathcal{G}F(x_{k+1})||_{x_{k}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Then we can follow exactly the same proof as HINV that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\frac{1}{K}\\sum_{k=0}^{K-1}\\|\\mathcal G F(x_{k})\\|_{x_{k}}^{2}\\leq\\frac{16\\tilde{L}_{F}R_{0}}{K}+\\frac{\\epsilon}{2}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "by appropriately choosing $S,T$ and $\\eta_{x}$ . ", "page_idx": 32}, {"type": "text", "text": "For AD, we define $R_{k}:=F(x_{k})+d^{2}(y_{k},y^{\\ast}(x_{k}))$ . Then following the same analysis except for the choice of $S$ , we can show, ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\frac{1}{K}\\sum_{k=0}^{K-1}\\|\\mathcal G F(x_{k})\\|_{x_{k}}^{2}\\leq\\frac{16\\tilde{L}_{F}R_{0}}{K}+\\frac{\\epsilon}{2}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Thus the proof is now complete. ", "page_idx": 32}, {"type": "text", "text": "F Tangent space conjugate gradient ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "In Algorithm 3, we show the tangent space conjugate gradient algorithm for solving the linear system $\\mathcal{H}[v]=\\mathcal{G}$ . Similar to [40], we set the initialization to be the transported output of $\\hat{v}_{k-1}^{T}$ from last iteration, where $\\hat{v}_{-1}^{T}=0$ , which is beneficial for convergence analysis. For practical purposes, we notice setting $v_{0}=0$ provides sufficient accurate solution without the expensive parallel transport operation. ", "page_idx": 32}, {"type": "text", "text": "Algorithm 3 Tangent space conjugate gradient $\\mathrm{TSCG}(\\mathcal{H},\\mathcal{G},v_{0},T)$   \n1: Set $r_{0}=\\mathcal{G}\\in T_{x}\\mathcal{M},p_{0}=r_{0}.$ .   \n2: for $t=0,...,T-1$ do   \n3: Compute $\\bar{r}_{t+1}=\\mathcal{H}[v_{t}]$ .   \n4: $\\begin{array}{r}{\\alpha_{t+1}=\\frac{\\|r_{t}\\|_{x}^{2}}{\\langle p_{t},\\mathcal{H}[p_{t}]\\rangle_{x}}}\\end{array}$ .   \n5: $v_{t+1}=v_{t}+\\alpha_{t+1}p_{t}$ .   \n6: $r_{t+1}=r_{t}-\\alpha_{t+1}\\mathcal{H}[p_{t}]$ .   \n7: $\\begin{array}{r}{\\beta_{t+1}=\\frac{\\|r_{t+1}\\|_{x}^{2}}{\\|r_{t}\\|_{x}^{2}}}\\end{array}$   \n8: $p_{t+1}=r_{t+1}+\\beta_{t+1}p_{t}$ .   \n9: end for   \n10: Output: $v_{T}$   \nAlgorithm 4 Riemannian bilevel solver for min-max optimization   \n1: Initialize $x_{0}\\in\\mathcal{M}_{x},y_{0}\\in\\mathcal{M}_{y}.$ .   \n2: for $k=0,...,K-1$ do   \n3: $y_{k}^{0}=y_{k}$ .   \n45:: fo $s=0,...,S-1$ .   \n$y_{k}^{s+1}=\\mathrm{Exp}_{y_{k}^{s}}(-\\eta_{y}\\,\\mathcal{G}_{y}g(x_{k},y_{k}^{s}))$   \n6: end for   \n7: Update $x_{k+1}=\\mathrm{Exp}_{x_{k}}{\\big(}-\\eta_{x}\\mathcal{G}_{x}f(x_{k},y_{k+1}){\\big)}$ , where $y_{k+1}=y_{k}^{S}$ .   \n8: end for ", "page_idx": 33}, {"type": "text", "text": "G Extensions: on Riemannian mix-max and compositional optimization ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "The bilevel optimization considered in the paper (1) generalizes the two other widely studied problems, namely the min-max optimization and compositional optimization. ", "page_idx": 33}, {"type": "text", "text": "G.1 Min-max optimization on Riemannian manifolds ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Riemannian min-max problems have gained increasing interest over the recent years [37, 41, 27, 73, 67, 25, 56, 35], which takes the form of ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x\\in{\\mathcal{M}}_{x}}\\operatorname*{max}_{y\\in{\\mathcal{M}}_{y}}f(x,y),\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and can be seen as a special case of bilevel optimization problem (1) where $g(x,y)=-f(x,y)$ . Because the problem is nonconvex in $x$ , the order of minimization and maximization matters [73, 25]. Nevertheless, under the assumption where $f$ is geodesic strongly convex in $y$ , the optimal solution $x^{*}$ satisfies $\\mathcal{G}F(x^{*})=0$ , where $\\mathcal{G}F(x)=\\mathcal{G}_{x}f(x,y^{*}(x))$ due to ${\\mathcal G}_{y}f(x,y^{*}(x))={\\mathcal G}_{y}g(x,y^{*}(x))=0$ . Thus Algorithm 1 reduces to alternating gradient descent ascent over Riemannian manifolds, as outlined in Algorithm 4. ", "page_idx": 33}, {"type": "text", "text": "Here we adapt the convergence analysis to the min-max optimization setting. Given we no longer require second-order derivatives, we restate assumptions for functions $f,g$ below. ", "page_idx": 33}, {"type": "text", "text": "Assumption 6. (1) Assumption 1 holds. (2) Function $f(x,y),g(x,y)$ have $L$ -Lipschitz Riemannian gradients. (3) Further, $g(x,y)$ is $\\mu$ -geodesic strongly convex in $y$ . ", "page_idx": 33}, {"type": "text", "text": "Under the min-max setup and Assumption 6, we see $\\mathcal{G}F(x)=\\mathcal{G}_{x}f(x,y^{*}(x))$ and thus the Lipschitz constant can be derived as $L_{F}=(\\kappa_{l}+1)L=\\Theta(\\kappa_{l})$ . Further we can directly apply Theorem 1 for the Hessian inverse with $C_{\\mathrm{hinv}}=L$ , which leads to the following convergence result. ", "page_idx": 33}, {"type": "text", "text": "Theorem 4. Under Assumption $^{6}$ , choosing $\\begin{array}{r l r}{S}&{{}\\ge}&{\\widetilde\\Theta(\\kappa_{l}^{2}\\zeta)}\\end{array}$ , $\\begin{array}{r l r}{\\eta_{x}}&{{}=}&{\\frac{1}{20L_{F}}}\\end{array}$ 201L , we have mink=0,...,K\u22121 \u2225GF(xk)\u22252xk \u2264 80(\u03bal+K1)L\u22060 and to reach $\\epsilon$ -stationary solution, we require gradient complexities as $G_{f}=O(\\kappa\\iota\\epsilon^{-1})$ and $G_{g}=\\widetilde{O}(\\kappa_{l}^{3}\\zeta\\epsilon^{-1})$ . ", "page_idx": 33}, {"type": "text", "text": "G.2 Compositional optimization on Riemannian manifolds ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Compositional problems on Riemannian manifolds have been considered in [36, 70], which requires to solve ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x\\in\\mathcal{M}_{x}}\\psi(\\phi(x)),\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where $\\psi:\\mathcal{M}_{y}\\rightarrow\\mathbb{R}$ and $\\phi:\\mathcal{M}_{x}\\rightarrow\\mathcal{M}_{y}$ . It is worth noting that in both works [36, 70], the inner function $\\phi:\\mathcal{M}_{x}\\to\\mathbb{R}^{d}$ is vector-valued. In contrast, we consider a general manifold-valued function $\\phi$ . Because the function $\\phi$ can be potentially complex and may be stochastic, we follow [11] to reformulate (20) into a bilevel optimization problem by letting ", "page_idx": 34}, {"type": "equation", "text": "$$\nf(x,y):=\\psi(y^{*}(x)),\\mathrm{~s.t.~}y^{*}(x)=\\underset{y\\in\\mathcal{M}_{y}}{\\arg\\operatorname*{min}}\\{g(x,y):=\\frac{1}{2}d^{2}(\\phi(x),y)\\}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "As long as the squared Riemannian distance is geodesic strongly convex, the reformulation is equivalent to the original problem (20). As formally stated in Lemma 9, this is satisfied for nonpositively curved space, like Euclidean space, hyperbolic manifold, SPD manifold with affine invariant metric. For positively curved space, the strong convexity is guaranteed when restricting the domain relative to the curvature. ", "page_idx": 34}, {"type": "text", "text": "Lemma 9. Let $\\mathcal{U}\\subseteq\\mathcal{M}$ has sectional curvature lower and upper bounded by $\\kappa^{-}$ and $\\kappa^{+}$ respectively. Further U has diameter upper bounded by $\\bar{D}$ , which satisfies $\\begin{array}{r}{\\bar{D}<\\frac{\\pi}{\\sqrt{\\kappa^{+}}}\\,\\dot{i}\\!f\\kappa^{+}>0}\\end{array}$ . Then let $\\delta=1$ when $\\kappa^{+}\\leq0$ and $\\begin{array}{r}{\\delta=\\frac{\\sqrt{\\kappa^{+}}\\bar{D}}{\\tan(\\sqrt{\\kappa^{+}}\\bar{D})}}\\end{array}$ when $\\kappa^{+}>0$ and consider $\\zeta$ be the same curvature constant as in Lemma 3. Then function $\\mathcal{H}_{y}g(x,y)$ has Riemannian Hessian bounded within $[\\delta,\\zeta]$ in spectrum. ", "page_idx": 34}, {"type": "text", "text": "Proof of Lemma 9. The proof follows from Lemma 2 in [3]. Consider an arbitrary curve $\\gamma:[0,1]\\rightarrow$ $\\mathcal{M}$ , and let $f(x)\\;=\\;{\\textstyle\\frac{1}{2}}\\mathring{d}^{2}(x,p)$ , for some $p\\,\\in\\,{\\mathcal{M}}$ . From [3], we know that $\\mathcal{H}f(\\gamma(t))[\\dot{\\gamma^{\\prime}}(t)]\\,=$ $-\\nabla_{\\gamma^{\\prime}(t)}\\mathrm{Exp}_{\\gamma(t)}^{-1}(p)$ and under the conditions, $\\begin{array}{r}{\\delta\\|\\gamma^{\\prime}(t)\\|_{\\gamma(t)}^{2}\\,\\le\\,\\langle\\nabla_{\\gamma^{\\prime}(t)}\\mathrm{Exp}_{\\gamma(t)}^{-1}(p),-\\gamma^{\\prime}(t)\\rangle_{\\gamma(t)}\\,\\le\\,}\\end{array}$ $\\zeta\\|\\gamma^{\\prime}(t)\\|_{\\gamma(t)}^{2}$ , where we denote $\\mathbf{v}$ as the covariant derivative. This immediately leads to ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\delta\\|\\gamma^{\\prime}(t)\\|_{\\gamma(t)}^{2}\\leq\\langle\\gamma^{\\prime}(t),\\mathcal{H}f(\\gamma(t))[\\gamma^{\\prime}(t)]\\rangle_{\\gamma(t)}\\leq\\zeta\\|\\gamma^{\\prime}(t)\\|_{\\gamma(t)}^{2}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "which completes the proof. ", "page_idx": 34}, {"type": "text", "text": "Thus, for positively curved manifold, if $\\begin{array}{r}{\\bar{D}<\\frac{\\pi}{2\\sqrt{\\kappa^{+}}}}\\end{array}$ , we have $\\delta>0$ , which ensures geodesic strong convexity of the inner problem. As shown in Lemma 12 in [3], $\\mathcal{G}_{y}d^{2}(\\phi(x),y)=2\\mathrm{Exp}_{y}^{-1}\\phi(x)$ and the Riemannian gradient descent on $y$ lead to ", "page_idx": 34}, {"type": "equation", "text": "$$\ny_{k}^{s+1}=\\mathrm{Exp}_{y_{k}^{s}}\\big(-\\eta_{y}\\mathrm{Exp}_{y_{k}^{s}}^{-1}\\phi(x_{k})\\big),\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "which suggests $y_{k}^{s+1}$ lies on a geodesic that connects $y_{k}^{s}$ and $\\phi(\\boldsymbol{x}_{k})$ . When $S=1$ and when the lower-level function $g$ is vector-valued, the algorithm recovers the deterministic version of SCGD [66]. ", "page_idx": 34}, {"type": "text", "text": "However, unlike in the Euclidean space, the Riemannian Hessian does not simplify to the identity operator, but rather the covariant derivative of inverse exponential map and the cross derivatives $\\dot{\\mathcal{G}_{x y}^{2}}g(x,y)\\neq-(\\mathrm{D}\\phi(x))^{\\dagger}$ . ", "page_idx": 34}, {"type": "text", "text": "Assumption 7. (1) Assumption 1 holds and further $\\begin{array}{r}{\\bar{D}<\\frac{\\pi}{2\\sqrt{\\kappa^{+}}}}\\end{array}$ if $\\kappa^{+}>0$ . (2) Function $f(x,y)$ has Riemannian gradients that are bounded by and are $L$ -Lipschitz. (3) Function $g$ has $\\rho$ -Lipschitz Riemannian Hessian and cross derivatives. ", "page_idx": 34}, {"type": "text", "text": "We notice that for function $g$ we only require second-order derivatives to be Lipschitz because the first-order Lipschitzness can be inferred from Lemma 9. ", "page_idx": 34}, {"type": "text", "text": "Theorem 5. Under Assumption 7, Theorem 1 holds with $L=\\zeta,\\mu=\\delta$ . ", "page_idx": 34}, {"type": "text", "text": "To prove the convergence, we only need to show Lemma 4 holds. It can be readily proved from Lemma 9 and Assumption 7 that Lemma 4 holds with $L=\\zeta,\\mu=\\delta$ . Hence the convergence follows directly. ", "page_idx": 34}, {"type": "text", "text": "H Experimental details ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "H.1 Synthetic problem ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "We first verify the lower-level problem is geodesic strongly convex. ", "page_idx": 35}, {"type": "text", "text": "Proposition 4. For any A, $\\mathbf{B}\\succ0,$ , function $f(\\mathbf{M})=\\left\\langle\\mathbf{M},\\mathbf{A}\\right\\rangle+\\left\\langle\\mathbf{M}^{-1},\\mathbf{B}\\right\\rangle$ is $\\mu$ -geodesic strongly convex in $\\mathcal{U}\\subset\\mathbb{S}_{++}^{d}$ with $\\begin{array}{r}{\\mu=\\lambda_{a,-}\\lambda_{-}+\\frac{\\lambda_{b,-}\\lambda_{-}}{\\lambda_{+}^{2}}}\\end{array}$ , where $\\lambda_{a,-},\\lambda_{b,-}$ are the minimum eigenvalue of $\\mathbf{A},\\mathbf{B}$ and $\\lambda_{\\pm}$ are the bounds for maximum and minimum eigenvalues for $\\mathbf{M}\\in\\mathcal{U}$ . ", "page_idx": 35}, {"type": "text", "text": "The inverse of Riemannian Hessian of function $f(\\mathbf{M})=\\left\\langle\\mathbf{M},\\mathbf{A}\\right\\rangle+\\left\\langle\\mathbf{M}^{-1},\\mathbf{B}\\right\\rangle$ is derived as, for any symmetric U, $\\mathcal{H}^{-1}f(\\mathbf{M})[\\mathbf{U}]=\\mathbf{M}^{1/2}\\mathbf{G}\\mathbf{M}^{1/2}$ where $\\mathbf{G}$ is the solution to the Lyapunov equation ( $\\Im(\\mathbf{M}^{1/2}\\mathbf{A}\\mathbf{M}^{1/2}+\\mathbf{M}^{-1/2}\\Bar{\\mathbf{B}}\\mathbf{M}^{-1/2})+(\\mathbf{M}^{1/2}\\mathbf{A}\\mathbf{M}^{1/2}+\\mathbf{M}^{-1/2}\\mathbf{B}\\mathbf{M}^{-1/2})\\mathbf{G}=\\Bar{\\mathbf{M}}^{-1/2}\\mathbf{U}\\mathbf{M}^{-1/2}.$ . ", "page_idx": 35}, {"type": "text", "text": "Proof of Proposition 4. We first derive the Euclidean gradient and Hessian as ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\nabla f(\\mathbf{M})=\\mathbf{A}-\\mathbf{M}^{-1}\\mathbf{B}\\mathbf{M}^{-1},\\quad\\nabla^{2}f(\\mathbf{M})[\\mathbf{U}]=\\mathbf{M}^{-1}\\mathbf{U}\\mathbf{M}^{-1}\\mathbf{B}\\mathbf{M}^{-1}+\\mathbf{M}^{-1}\\mathbf{B}\\mathbf{M}^{-1}\\mathbf{U}\\mathbf{M}^{-1}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "for any $\\mathbf{U}=\\mathbf{U}^{\\top}$ . The Riemannian gradient and Hessian are derived as ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathcal{G}f(\\mathbf{M})=\\mathbf{M}\\mathbf{A}\\mathbf{M}-\\mathbf{B}}\\\\ &{\\mathcal{H}f(\\mathbf{M})[\\mathbf{U}]=\\mathbf{U}\\mathbf{M}^{-1}\\mathbf{B}+\\mathbf{B}\\mathbf{M}^{-1}\\mathbf{U}+\\{\\mathbf{U}\\nabla f(\\mathbf{M})\\mathbf{M}\\}\\mathrm{s}}\\\\ &{\\quad\\quad\\quad\\quad=\\mathbf{U}\\mathbf{M}^{-1}\\mathbf{B}+\\mathbf{B}\\mathbf{M}^{-1}\\mathbf{U}+\\frac{1}{2}\\big(\\mathbf{U}\\mathbf{A}\\mathbf{M}-\\mathbf{U}\\mathbf{M}^{-1}\\mathbf{B}+\\mathbf{M}\\mathbf{A}\\mathbf{U}-\\mathbf{B}\\mathbf{M}^{-1}\\mathbf{U}\\big)}\\\\ &{\\quad\\quad\\quad\\quad=\\frac{1}{2}\\big(\\mathbf{U}\\mathbf{A}\\mathbf{M}+\\mathbf{M}\\mathbf{A}\\mathbf{U}+\\mathbf{U}\\mathbf{M}^{-1}\\mathbf{B}+\\mathbf{B}\\mathbf{M}^{-1}\\mathbf{U}\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where we let $\\{\\mathbf{A}\\}\\mathrm{s}=(\\mathbf{A}+\\mathbf{A}^{\\top})/2$ . To show the function is geodesic strongly convex, it suffices to show $\\mathcal{H}f(\\mathbf{\\bar{M}})$ is positive definite, which is to show $\\langle\\mathcal{H}f(\\mathbf{M})[\\mathbf{U}],\\mathbf{U}\\rangle_{\\mathbf{M}}\\geq\\bar{\\mu}\\|\\mathbf{U}\\|_{\\mathbf{M}}^{2}>0$ for any $\\mathbf{U}=\\mathbf{U}^{\\top}$ . To this end, we vectorize the Riemannian Hessian in terms of $\\mathbf{U}$ as $\\mathrm{vec}(2\\mathcal{H}f(\\mathbf{M})[\\mathbf{U}])=$ $(\\mathbf{M}\\mathbf{A}\\otimes\\mathbf{I}+\\mathbf{I}\\otimes\\mathbf{M}\\mathbf{A}+\\mathbf{B}\\mathbf{M}^{-1}\\otimes\\mathbf{I}+\\mathbf{I}\\otimes\\mathbf{B}\\mathbf{M}^{-1})\\mathrm{vec}(\\mathbf{U})$ , where $\\otimes$ denotes the Kronecker product. Then, we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\mathcal{H}f(\\mathbf{M})[\\mathbf{U}],\\mathbf{U}\\rangle_{\\mathbf{M}}}\\\\ &{=\\mathrm{tr}(\\mathbf{M}^{-1}\\mathbf{U}\\mathbf{M}^{-1}\\mathcal{H}f(\\mathbf{M})[\\mathbf{U}])}\\\\ &{=\\frac{1}{2}\\mathrm{vec}(\\mathbf{U})^{\\top}(\\mathbf{M}^{-1}\\otimes\\mathbf{M}^{-1})(\\mathbf{M}\\mathbf{A}\\otimes\\mathbf{I}+\\mathbf{I}\\otimes\\mathbf{M}\\mathbf{A}+\\mathbf{B}\\mathbf{M}^{-1}\\otimes\\mathbf{I}+\\mathbf{I}\\otimes\\mathbf{B}\\mathbf{M}^{-1})\\mathrm{vec}(\\mathbf{U})}\\\\ &{=\\frac{1}{2}\\mathrm{vec}(\\mathbf{U})^{\\top}(\\mathbf{A}\\otimes\\mathbf{M}^{-1}+\\mathbf{M}^{-1}\\otimes\\mathbf{A}+\\mathbf{M}^{-1}\\mathbf{B}\\mathbf{M}^{-1}\\otimes\\mathbf{M}^{-1}+\\mathbf{M}^{-1}\\otimes\\mathbf{M}^{-1}\\mathbf{B}\\mathbf{M}^{-1})\\mathrm{vec}(\\mathbf{U})}\\\\ &{\\ge(\\lambda_{a,-}\\lambda_{m,-}+\\frac{\\lambda_{b,-}-\\lambda_{m,-}}{\\lambda_{m,+}^{2}})\\mathrm{vec}(\\mathbf{U})^{\\top}(\\mathbf{M}^{\\top}\\otimes\\mathbf{M}^{-1})\\mathrm{vec}(\\mathbf{U})=\\mu\\|\\mathbf{U}\\|_{\\mathbf{M}}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where we let $\\lambda_{a,\\pm}$ be the maximum/minimum eigenvalues of $\\mathbf{A}$ and similarly for $\\lambda_{b,\\pm},\\lambda_{m,\\pm}$ . ", "page_idx": 35}, {"type": "text", "text": "The Hessian inverse can be derived subsequently. This completes the proof. ", "page_idx": 35}, {"type": "text", "text": "H.2 Computational time for each hypergradient estimator ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "This section report the average runtime in seconds (over 10 runs) for single evaluation of hypergradient using four different strategies (for Hypergradient estimation) for the synthetic problem (Section 4.1, Figure 1). The hyper-parameters are set to be the same as the main experiment. We see in general automatic differentiation (AD) is the most efficient strategy. Nevertheless, according to Figure 1(a), it is less accurate compared to other strategies. ", "page_idx": 35}, {"type": "table", "img_path": "LvNDqNJKlD/tmp/522d2409cd967466d74e53147c813b52cf1521d16135021f4d5759f0cdcfa7d0.jpg", "table_caption": ["Table 3: Comparison of runtime for single computation of hypergradient. "], "table_footnote": [], "page_idx": 35}, {"type": "text", "text": "H.3 Hyperparameter selection ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "The selection of hyper-parameters is performed to reflect the best performance. The stepsize is selected from the range [1e-3, 5e-3, 1e-2, 5e-2, 1e-1, 5e-1, 1] and for Neumann series is selected from [0.1, 0.5, 1.0, 1.5, 2.0] and number of inner iterations is selected from [5, 10, 30, 50, 100]. Figure 1(d) shows the sensitivity of hypergradient error as we vary and the number of inner iterations. ", "page_idx": 36}, {"type": "text", "text": "H.4 Computational Complexity ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "This section lists out the computational complexity for each task considered in the experiment section. In Table 4, we present an estimate of the per-iteration complexity of computing the gradient, Hessian/Jacobian-vector products. We highlight that we only provide estimates of the complexities given that there may not exist closed form expressions for the gradient and second-order derivatives. ", "page_idx": 36}, {"type": "text", "text": "Here, $n_{v},n_{t}$ denote the size of validation set and training set respectively. For meta learning, $m$ denotes the number tasks and $n$ denotes the number of samples for each task. For domain adaptation, $m,n$ denote the number of samples for two domains, $s$ denotes the number of Sinkhorn iterations. ", "page_idx": 36}, {"type": "table", "img_path": "LvNDqNJKlD/tmp/093b409f571db53df030280433bdb2321b6dcff5c6cb448cff192b30400f6aff.jpg", "table_caption": ["Table 4: Per-iteration complexity estimate for each task "], "table_footnote": [], "page_idx": 36}, {"type": "text", "text": "I Experiment Configurations ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "All the experiments are conducted on a single NVIDIA RTX 4060 GPU. All datasets used in the paper are publicly available, which are properly cited in the main paper. We include detailed setups for the experiments in the main paper as well as documented in code (provided as supplementary material). ", "page_idx": 36}, {"type": "text", "text": "J Broader Impact ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "This paper proposes new algorithms and are of theoretical in nature. We do not foresee any immediate negative societal impact that we feel obliged to report. ", "page_idx": 36}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: Please refer to Section 3 for theoretical developments and Section 4 for empirical studies. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 37}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Justification: We include discussions of limitations in the conclusion section, i.e., Section 5. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 37}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: We have included all the assumptions and complete proof in the paper. Please refer to Section 3 and Section C, D, E. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 38}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: We have provided the implementation details for reproducing the experiments, along with the code in the supplementary material. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 38}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: We have provided the code and data as supplementary material to ensure reproducibility. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 39}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: We have included the details in the experiment section. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 39}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: We have reported standard deviation in Figure 2(b) and (c). For Figure 1, the results barely change as we vary the random seed. For Riemannian meta learning, it becomes time costly to report standard deviation. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 39}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 40}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: We have included the detailed configuration in Appendix I. We report the time of execution in Appendix H.2. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 40}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: The paper conform, in every respect, with the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 40}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: We have included a broader impact section, i.e., Section J. Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 40}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 41}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: This paper does not release data or models that have high risks for misuse. Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 41}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: In the experiment section, this paper has cited the papers when using exisiting assets, inluding code and data. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 41}, {"type": "text", "text": "", "page_idx": 42}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: This paper does not introduce new assets. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 42}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: The paper does not involve human subjects. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 42}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: This paper does not involve human subjects. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 42}, {"type": "text", "text": "", "page_idx": 43}]