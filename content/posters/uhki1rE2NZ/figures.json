[{"figure_path": "uhki1rE2NZ/figures/figures_1_1.jpg", "caption": "Figure 1: An example of a 2d loss function with scale invariance: l(0) = l(X0) for a scalar X and 0 \u2208 R2. Because of the symmetry, the gradient Vl must be tangential to the circles whose center is the origin. This implies that the norm ||0|| does not change during gradient flow training. However, when the training is stochastic or discrete-time, SGD must move outward. If the model starts at Ot, it must move to a larger circle. As an illustrative example, this loss function has a unique and attractive fixed point: ||0|| = \u221e. SGD will diverge after training under scale invariance. Also, see Remark 4.4 for a discussion of the difference between discrete-time and continuous-time dynamics.", "description": "This figure illustrates the effect of scale invariance on the dynamics of SGD.  In a deterministic setting (gradient flow), the parameters stay on the same circle, as the gradient is tangential. However, with SGD (stochastic), the noise causes the parameter vector to move outwards to larger circles, diverging to infinity. This illustrates that even with a small amount of noise, the dynamics of SGD can be qualitatively different from gradient flow.", "section": "Related Works"}, {"figure_path": "uhki1rE2NZ/figures/figures_5_1.jpg", "caption": "Figure 2: Comparison between GD and SGD for matrix factorizations. Left: Example of a learning trajectory. The convergence speed is almost exponential-like in experiments. Mid: evolution of 10 individual elements of \u2206ij := (UTUU \u2013 WwW\u00b9)ij. As the theory shows, they all move close to zero and fluctuate with a small variance. Right: Converged solutions of SGD agree with the prediction of Theorem 5.2, but are an order of magnitude away from the solution found by GD, even if they start from the same init.", "description": "This figure compares the performance of gradient descent (GD) and stochastic gradient descent (SGD) in matrix factorization. The left panel shows a learning trajectory, illustrating that SGD converges exponentially fast. The middle panel tracks 10 individual elements of the matrix difference (UTUU - WWW), demonstrating that they converge to zero under SGD, fluctuating minimally around zero.  The right panel highlights that the final solutions obtained by SGD, despite agreeing with the theoretical predictions, significantly differ from GD solutions (by an order of magnitude), irrespective of the initial parameters.", "section": "5.1 Generalized Matrix Factorization"}, {"figure_path": "uhki1rE2NZ/figures/figures_6_1.jpg", "caption": "Figure 3: A two-layer linear network after training. Here, the problem setting is the same as Figure 8. The theoretical prediction is computed from Theorem 5.2. Left: balance of the norm is only achieved when x = 1, namely, when the data has an isotropic covariance. We also test SGD with a small weight decay (10\u207b\u00b9), which is sufficiently small that the solution we obtained for SGD without SGD still holds approximately. In contrast, training with GD + WD always converges to a norm-balanced solution. Right: the sharpness of the converged model trained with SGD. We see that for some data distributions, SGD converges to a sharper solution, whereas it converges to flatter solutions for other data distributions. This flattening and sharpening effect are both due to the noise-balance effect of SGD. Here, we find that the systematic error between experiment and theory is due to the use of a finite learning rate and decreases as we decrease \u03b7.", "description": "This figure compares the results of training a two-layer linear network with SGD and GD, with and without weight decay, for different data covariances. The left panel shows that SGD leads to norm-balanced solutions only when the data covariance is isotropic (\u03c6x = 1), while GD always converges to norm-balanced solutions. The right panel shows that SGD's solutions exhibit progressive sharpening or flattening depending on the data distribution, unlike GD. The difference is explained by the noise equilibrium of SGD.", "section": "5 Applications"}, {"figure_path": "uhki1rE2NZ/figures/figures_7_1.jpg", "caption": "Figure 4: Dynamics of the stability condition S during the training of a rank-1 matrix factorization problem. The solid lines show the training of SGD with Kaiming init. When the learning rate (\u03b7 = 0.008) is too large, SGD diverges (orange line). However, when one starts training at a small learning rate (0.001) and increases \u03b7 to 0.008 after 5000 iterations, the training remains stable. This is because SGD training improves the stability condition during training, which is in agreement with the theory. In contrast, the stability condition of GD and that of SGD with a Xavier init increases only slightly. Also, note that both Xavier and Kaiming init. under SGD converges to the same stability condition because the equilibrium is unique.", "description": "This figure shows the dynamics of the stability condition S (trace of Hessian) during the training process of a rank-1 matrix factorization problem. It compares the performance of SGD with Kaiming initialization (with and without warmup), SGD with Xavier initialization, and GD with Kaiming initialization. The results demonstrate that with a proper learning rate scheduling (warmup), SGD with Kaiming init. improves the stability condition, while other methods do not improve or even worsen the stability during training. The unique equilibrium under SGD is also illustrated.", "section": "5 Applications"}, {"figure_path": "uhki1rE2NZ/figures/figures_8_1.jpg", "caption": "Figure 7: When there is scaling symmetry, the norm of the parameters increases monotonically under SGD but remains unchanged under GD. Left: evolution of the total model norm for two-layer nonlinear networks where there is a rescaling symmetry. Mid: evolution of the second layer. Right: evolution of the first layer. This shows that the evolution of each layer can be vastly different, but the total norm of the parameters with the scaling symmetry is always monotonically increasing. Also, note that for net B, each layer also has the rescaling symmetry, and so the norm of each layer for net-B is also increasing. In contrast, net-A does not have layer-wise symmetry, and the individual norms can be either increasing or decreasing.", "description": "This figure compares the evolution of parameter norms in two different neural networks (Net A and Net B) trained using SGD and GD.  Both networks exhibit scaling symmetry, meaning their loss remains unchanged when all parameters are scaled by a constant factor.  The figure demonstrates that, under SGD, the total norm of parameters and individual layer norms monotonically increase, even when layer-wise symmetry is absent.  In contrast, GD training maintains a constant parameter norm in Net B, which possesses both global and layer-wise scaling symmetries.", "section": "A Additional Experiments"}, {"figure_path": "uhki1rE2NZ/figures/figures_9_1.jpg", "caption": "Figure 6: The latent representations of a two-layer tanh net trained under SGD (left) are similar across different layers, in agreement with the theory. However, the learned representations are dissimilar under GD (right). Here, we plot the matrices WEW (first and third plots) and U\u017dU (second and fourth plots). Note that the quantity WEW is equal to the covariance of the preactivation representation of the first layer. This means that SGD and GD learn qualitatively different features after training. Also, see Appendix A.4 for other activations.", "description": "This figure compares the learned latent representations of a two-layer tanh network trained using SGD and GD.  The left panels show that the latent representations learned with SGD are similar across layers, consistent with the paper's theory.  The right panels demonstrate that GD produces dissimilar representations across layers.  The plots visualize the matrices W\u03a3W (covariance of pre-activations in the first layer) and U\u03a3U. This highlights a key difference in the feature learning of SGD vs. GD.", "section": "5 Applications"}, {"figure_path": "uhki1rE2NZ/figures/figures_13_1.jpg", "caption": "Figure 7: When there is scaling symmetry, the norm of the parameters increases monotonically under SGD but remains unchanged under GD. Left: evolution of the total model norm for two-layer nonlinear networks where there is a rescaling symmetry. Mid: evolution of the second layer. Right: evolution of the first layer. This shows that the evolution of each layer can be vastly different, but the total norm of the parameters with the scaling symmetry is always monotonically increasing. Also, note that for net B, each layer also has the rescaling symmetry, and so the norm of each layer for net-B is also increasing. In contrast, net-A does not have layer-wise symmetry, and the individual norms can be either increasing or decreasing.", "description": "The figure shows the evolution of the norm of parameters in two different neural networks (Net-A and Net-B) trained using SGD and GD. Net-B has layer-wise rescaling symmetry, causing its parameter norms to always increase monotonically under SGD but remain constant under GD. Net-A lacks this layer-wise symmetry, resulting in varying norms in each layer under SGD.", "section": "A Additional Experiments"}, {"figure_path": "uhki1rE2NZ/figures/figures_14_1.jpg", "caption": "Figure 8: The convergence of matrix factorization to the noise equilibria is robust against different hyperparameter settings. The task is an autoencoding task where y = x \u2208 R100. The distribution of x is controlled by a parameter : X1:50 ~ N(0, x), X51:100 \u039d(0,2 \u2013 \u03a6\u03b1). This directly controls the overall covariance of x. The output noise covariance is set to be identity. Unless it is the independent variable, \u03b7, S and d are set to be 0.1, 100 and 2000, respectively. Left: using different learning rates. Mid: different data dimension: dx = dy = d. Right: different batch size S.", "description": "The figure shows how the convergence speed to the noise equilibria is affected by different hyperparameters such as learning rate, data dimension, and batch size. The robustness of the convergence to the equilibria against these variations is demonstrated.", "section": "5.1 Generalized Matrix Factorization"}, {"figure_path": "uhki1rE2NZ/figures/figures_15_1.jpg", "caption": "Figure 6: The latent representations of a two-layer tanh net trained under SGD (left) are similar across different layers, in agreement with the theory. However, the learned representations are dissimilar under GD (right). Here, we plot the matrices WEW (first and third plots) and U'U (second and fourth plots). Note that the quantity WEW is equal to the covariance of the preactivation representation of the first layer. This means that SGD and GD learn qualitatively different features after training. Also, see Appendix A.4 for other activations.", "description": "This figure compares the latent representations learned by SGD and GD for a two-layer tanh network.  The left side shows that SGD produces similar representations across different layers, aligning with the paper's theory. The right side demonstrates that GD produces vastly different representations.  The matrices shown (WEW and U'U) highlight the covariance of preactivation representations, demonstrating a qualitative difference in feature learning between the two optimization algorithms.", "section": "5 Applications"}]