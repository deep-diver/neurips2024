{"references": [{"fullname_first_author": "Chris R\u00e9", "paper_title": "Can longer sequences help take the next leap in ai?", "publication_date": "2022-06-00", "reason": "This paper is foundational to the current work as it discusses the importance of long sequences in AI, providing a base for the research in this paper about handling long contexts in LLMs."}, {"fullname_first_author": "Jiaqi Li", "paper_title": "Loogle: Can long-context language models understand long contexts?", "publication_date": "2023-11-00", "reason": "This paper directly addresses the core challenge of long-context understanding in LLMs, motivating the work presented here which proposes a solution for improving long context performance."}, {"fullname_first_author": "Nelson F Liu", "paper_title": "Lost in the middle: How language models use long contexts", "publication_date": "2023-07-00", "reason": "This paper introduces the \"lost-in-the-middle\" phenomenon, which is the primary focus of this paper's investigation and proposed solution."}, {"fullname_first_author": "Jianlin Su", "paper_title": "Roformer: Enhanced transformer with rotary position embedding", "publication_date": "2024-00-00", "reason": "This paper describes Rotary Position Embedding (RoPE), a key component of many LLMs that is directly relevant to and addressed by the proposed method in this paper."}, {"fullname_first_author": "Shouyuan Chen", "paper_title": "Extending context window of large language models via positional interpolation", "publication_date": "2023-06-00", "reason": "This paper is among the most important as it explores positional interpolation as a method for extending context windows in LLMs, which is directly related to the proposed approach in this paper."}]}