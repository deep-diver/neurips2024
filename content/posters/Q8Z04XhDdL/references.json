{"references": [{"fullname_first_author": "A. Krizhevsky", "paper_title": "Imagenet classification with deep convolutional neural networks", "publication_date": "2012", "reason": "This paper introduced AlexNet, a groundbreaking deep convolutional neural network that significantly advanced the field of image recognition and deep learning."}, {"fullname_first_author": "K. He", "paper_title": "Deep residual learning for image recognition", "publication_date": "2016", "reason": "This paper introduced ResNet, a crucial architecture that addressed the vanishing gradient problem in very deep networks, enabling the training of significantly deeper and more accurate models."}, {"fullname_first_author": "W. Fedus", "paper_title": "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity", "publication_date": "2022", "reason": "This paper introduced Switch Transformers, a pioneering approach to scaling transformer models to extremely large sizes by employing a mixture-of-experts (MoE) architecture for efficient parameter usage."}, {"fullname_first_author": "J. Puigcerver", "paper_title": "From sparse to soft mixtures of experts", "publication_date": "2023", "reason": "This paper introduced SoftMoE, a significant improvement to the MoE method which made MoE training more stable and efficient, improving accuracy while maintaining reasonable computational costs."}, {"fullname_first_author": "A. Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2020", "reason": "This work introduced the Vision Transformer (ViT), demonstrating the effectiveness of transformer architectures for image recognition tasks, which inspired many subsequent vision transformer models."}]}