[{"figure_path": "Q8Z04XhDdL/figures/figures_1_1.jpg", "caption": "Figure 1: (a) MoE Jetpack converts dense checkpoints into initialization weights for MoE models, facilitating faster convergence and improved performance while maintaining equivalent FLOPs. Here, Exp. represents individual experts, E denotes the number of experts, and L indicates the total number of layers. (b) Performance comparison among ViT trained from scratch, pre-trained and fine-tuned ViT, Soft MoE [6] trained from scratch, and MoE Jetpack across multiple datasets.", "description": "This figure demonstrates the core idea of MoE Jetpack and its performance. (a) shows how MoE Jetpack leverages pre-trained dense models to initialize MoE models efficiently.  It replaces some layers of a dense model with MoE layers, initialized using weights from the dense model. (b) compares the performance of models trained using different methods across multiple datasets, showing that MoE Jetpack significantly outperforms training from scratch and achieves better results than Soft MoE.", "section": "1 Introduction"}, {"figure_path": "Q8Z04XhDdL/figures/figures_3_1.jpg", "caption": "Figure 1: (a) MoE Jetpack converts dense checkpoints into initialization weights for MoE models, facilitating faster convergence and improved performance while maintaining equivalent FLOPs. Here, Exp. represents individual experts, E denotes the number of experts, and L indicates the total number of layers. (b) The Performance Comparison on Different Datasets", "description": "This figure shows two parts: (a) illustrates the architecture of MoE Jetpack. It converts dense checkpoints into initialization weights for MoE models to speed up convergence and improve performance while keeping FLOPs same. (b) shows the performance comparison of different model architectures across various datasets, including ImageNet-1k, CIFAR-10, CIFAR-100, STL-10, and more, highlighting the advantages of MoE Jetpack over training from scratch and pre-trained and fine-tuned ViT.", "section": "1 Introduction"}, {"figure_path": "Q8Z04XhDdL/figures/figures_4_1.jpg", "caption": "Figure 3: The Adaptive Dual-path MoE structure enhances the SpheroMoE Router by adapting it into a dual-branch system, designed to optimize computational efficiency and model performance. This configuration directs high-impact tokens to a core path with fewer but larger experts while routing less critical tokens to a universal path equipped with a greater number of smaller experts.", "description": "This figure illustrates the Adaptive Dual-path MoE, a key component of the SpheroMoE layer. It shows how input tokens are processed through two distinct pathways: a \"core\" path for high-impact tokens and a \"universal\" path for less critical ones. The core path uses fewer, larger experts, while the universal path utilizes more, smaller experts, optimizing computational efficiency and performance.", "section": "3.2 SpheroMoE Layer"}, {"figure_path": "Q8Z04XhDdL/figures/figures_7_1.jpg", "caption": "Figure 4: CIFAR-100 accuracy across different ratios of core (dark) to universal (light) experts, highlighting optimal performance at a 1/3 core ratio.", "description": "This figure shows the performance of the MoE Jetpack model on the CIFAR-100 dataset with varying ratios of core experts to universal experts.  The x-axis represents the CIFAR-100 accuracy, and the y-axis represents the ratio of core experts to universal experts. The figure demonstrates that the optimal performance is achieved when the ratio of core experts to universal experts is approximately 1:3. This suggests that a balanced combination of specialized (core) and generalized (universal) experts is crucial for maximizing the model's effectiveness.", "section": "4.3 Ablations"}, {"figure_path": "Q8Z04XhDdL/figures/figures_8_1.jpg", "caption": "Figure 5: Comparison of convergence speeds using MoE Jetpack versus training from scratch on ImageNet (left) and CIFAR-100 (right). MoE Jetpack achieves target accuracies significantly faster, demonstrating a 2x speed increase on ImageNet and an 8x increase on CIFAR-100.", "description": "This figure compares the convergence speed of MoE Jetpack against training MoE models from scratch.  The left panel shows the results for ImageNet-1k, demonstrating that MoE Jetpack achieves the target accuracy roughly twice as fast. The right panel shows the results for CIFAR-100, illustrating that MoE Jetpack is about eight times faster.  The significant speedup highlights one of the key advantages of MoE Jetpack.", "section": "4.4 Analysis"}, {"figure_path": "Q8Z04XhDdL/figures/figures_8_2.jpg", "caption": "Figure 6: (a) The attention maps generated by five experts in response to the input image, highlighting the experts\u2019 specialization. (b) These line charts show varying contributions of core and universal experts, with core experts\u2019 influence peaking in later layers, emphasizing their detailed feature refinement, contrasted with the consistent input of universal experts.", "description": "This figure visualizes the attention maps of five different experts (three core experts and two universal experts) responding to the same input image and their contributions to the final output tokens across different layers of the MoE model. The attention maps show that each expert focuses on specific regions of the input image which indicates specialization.  The line charts demonstrate the different contributions made by core and universal experts across twelve layers of the MoE. Core experts show increasing influence in later layers, highlighting their role in detailed feature refinement. Conversely, universal experts maintain a relatively consistent level of contribution across all layers, indicating a more uniform integration of broader contextual information.", "section": "4.4 Analysis"}, {"figure_path": "Q8Z04XhDdL/figures/figures_16_1.jpg", "caption": "Figure 7: Visualization of the attention map identified by the most important core experts and universal experts across different layers (MoE Layer 07 to MoE Layer 12). The images show the regions deemed most relevant by each type of expert at each layer.", "description": "This figure visualizes the attention maps generated by the most important core and universal experts across different layers of the MoE Jetpack model. It shows how these experts focus on different regions of the input image, with core experts focusing on more detailed features and universal experts providing broader contextual information. The visualization highlights the dynamic allocation and specialization of experts within the model, illustrating how their contributions evolve across different layers.", "section": "Dynamic Allocation and Focus Regions of Experts in MoE Jetpack"}]