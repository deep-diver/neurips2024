[{"type": "text", "text": "CausalDiff: Causality-Inspired Disentanglement via Diffusion Model for Adversarial Defense ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Mingkun Zhang ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "CAS Key Laboratory of AI Safety Institute of Computing Technology, CAS zhangmingkun20z@ict.ac.cn ", "page_idx": 0}, {"type": "text", "text": "Keping Bi ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Key Laboratory of Network Data Science and Technology Institute of Computing Technology, CAS bikeping@ict.ac.cn ", "page_idx": 0}, {"type": "text", "text": "Wei Chen ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "CAS Key Laboratory of AI Safety Institute of Computing Technology, CAS chenwei2022@ict.ac.cn ", "page_idx": 0}, {"type": "text", "text": "Quanrun Chen ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "School of Statistics University of International Business and Economics qchen@uibe.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Jiafeng Guo ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xueqi Cheng ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Data Science and Technology Institute of Computing Technology, CAS guojiafeng@ict.ac.cn ", "page_idx": 0}, {"type": "text", "text": "Key Laboratory of Network ", "page_idx": 0}, {"type": "text", "text": "CAS Key Laboratory of AI Safety Institute of Computing Technology, CAS cxq@ict.ac.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Despite ongoing efforts to defend neural classifiers from adversarial attacks, they remain vulnerable, especially to unseen attacks. In contrast, humans are difficult to be cheated by subtle manipulations, since we make judgments only based on essential factors. Inspired by this observation, we attempt to model label generation with essential label-causative factors and incorporate label-non-causative factors to assist data generation. For an adversarial example, we aim to discriminate the perturbations as non-causative factors and make predictions only based on the labelcausative factors. Concretely, we propose a casual diffusion model (CausalDiff) that adapts diffusion models for conditional data generation and disentangles the two types of casual factors by learning towards a novel casual information bottleneck objective. Empirically, CausalDiff has significantly outperformed state-of-the-art defense methods on various unseen attacks, achieving an average robustness of $86.39\\%$ $(+4.01\\%)$ on CIFAR-10, $56.25\\%$ $(+3.13\\%)$ on CIFAR-100, and $82.62\\%$ $(+4.93\\%)$ on GTSRB (German Traffic Sign Recognition Benchmark). ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Neural classifiers, despite their impressive performance in various applications, are susceptible to adversarial attacks [1, 2], which can deceive them into making erroneous judgments on subtly manipulated examples. Such vulnerabilities pose severe threats in safety-critical scenarios such as face recognition [3, 4] and autonomous driving [5]. There has been extensive work on defending against adversarial attacks such as certified defenses [6, 7], adversarial training [8, 9], and adversarial purification Samangouei et al. [10]. ", "page_idx": 0}, {"type": "image", "img_path": "BZLdXBjB8O/tmp/56dc9779083633065b26e0ec0d9a00eec7f93c82a4924ffa32f8771a508bcacd.jpg", "img_caption": ["Figure 1: Illustration of training (Left) and inference (Right) processes of our proposed CausalDiff model. During training, the model constructs a structural causal model leveraging a conditional diffusion model, disentangling the (label) $\\mathrm{\\bfY}.$ -causative feature $S$ and the $\\mathrm{\\bfY}.$ -non-causative feature $Z$ through maximization of the Causal Information Bottleneck (CIB). In the inference stage, CausalDiff first purifies an adversarial example X\u02dc, yielded by perturbing $X$ according to the target victim model parameterized by $\\theta$ , to obtain the benign counterpart $X^{*}$ . Then, it infers the Y-causative feature $S^{*}$ for label prediction. We visualize the vectors of $S$ and $Z$ inferred from a perturbed image of a horse using a diffusion model. We find that $S$ captures the general concept of a horse, even when the input image only shows the head, while $Z$ carries information about the horse\u2019s skin color. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Although effective, these methods have some limitations. Certified defense methods have limited practicality due to the small certified region that can theoretically guarantee robustness. Adversarial training approaches suffer from a significant decline in robustness against unseen attacks since they take effect by adding adversarial examples into the training set. Purification methods, not designed for specific attacks, struggle to determine the optimal denoising level for unforeseen attacks with differing degrees of perturbation. Consequently, they face challenges in effectively defending against unseen attacks. ", "page_idx": 1}, {"type": "text", "text": "In reality, attack behaviors are often unpredictable. Is there a way of strengthening a model to act like humans, i.e., be insensitive to subtle perturbations and robust against various unforeseen attacks? Given an image of an object, we typically identify the key visual features that are necessary to determine its category and disregard other factors such as styles, backgrounds, details, or perturbations. This allows us to make robust judgments. Inspired by this human decision-making process, we would like to learn a model that can disentangle the essential features for determining the category from other non-essential ones. ", "page_idx": 1}, {"type": "text", "text": "It is natural to treat the essential features as the causal factors of the label, and both essential and the other features as the causal factors of the entire image. Then, we can learn to disentangle them by modeling the process of data generation and label prediction with a structural causal model (SCM) [11, 12](shown in Fig. 1 (Left)). According to the theoretical results provided by Liu et al. [13], the identifiability of such SCMs can be guaranteed under mild conditions. Although similar SCMs have also been employed in modeling the generation of multi-domain data [14] and adversarial examples [15, 16], they either aim to enhance out-of-distribution robustness or protect the model from a certain type of attack. In contrast, our research focuses on modeling the generation of native in-domain data to enhance adversarial robustness against various unseen attacks. ", "page_idx": 1}, {"type": "text", "text": "Fig. 1 (Left) depicts our SCM, where $Y$ denotes the category (e.g., horse) of an input image $X$ ; $S$ denotes the essential semantic features of determining $Y$ (i.e., $Y$ -causative factors), such as the characteristics of eyes, ears, nose, mouth, etc. of horses; and $Z$ represents the other features (i.e., $Y$ -non-causative factors) that are not needed to predict $Y$ but are important to generate $X$ , such as the fur color and the image background. Given an adversarial example produced by an unknown attack, our model aims to disentangle its $Y$ -causative features $S$ from the spurious factors in $Z$ and make a robust prediction. To successfully learn the disentanglement, our SCM is guided by the tasks of data generation and label prediction, where there are three major challenges: ", "page_idx": 1}, {"type": "text", "text": "1) How do we model the conditional generation of $X$ given $S$ and $Z$ effectively? To this end, we employ a well-recognized diffusion model with state-of-the-art (SOTA) generative performance and efficiency, i.e., the Denoising Diffusion Probabilistic Model (DDPM) [17], as the backbone. We further adapt it for conditional generation from latent variables rather than random noise. 2) What training objective should we use to effectively learn the disentanglement of $S$ and $Z?$ With this regard, we propose a Causal Information Bottleneck (CIB) optimization objective. CIB aligns the information in the latent variables $(S,Z)$ with observed variables $(X,Y)$ with a bottleneck set by the mutual information (MI) between $S,Z$ and $X$ . The derived function will minimize the MI between $S$ and $Z$ while learning the other causal relations, ensuring their disentanglement within the causal framework. 3) Given an adversarial example, what inference strategies shall we adopt to make robust predictions? As shown in Fig. 1 (Right), according to how the adversarial example X\u02dc is generated, we first purify it to yield a benign example $X^{*}$ and then infer the Y-causative factor $S$ based on $X^{*}$ for final classification. We name the entire causal defense framework based on diffusion models as CausalDiff. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "The experimental results on facing various unseen attacks, encompassing both black-box and whitebox ones, show that CausalDiff has superior performance compared to representative adversarial ", "page_idx": 2}, {"type": "image", "img_path": "BZLdXBjB8O/tmp/64d44bd6741e63e5df782eb56eeeb7365d7cd9391f8ea8bd8b74f49b9a3361af.jpg", "img_caption": ["Figure 2: Adversarial robustness of four models against 100-step PGD attack under varying attack strength indicated by $\\epsilon$ -budget. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Table 1: The experimental results of four models on toy data. The variation of latent $v$ and logits $p(y|v)$ is measured between clean and adversarial examples. The model margin is estimated by the minimal adversarial perturbation required to flip the label, employing both $\\ell_{2}$ and $\\ell_{\\infty}$ norm. ", "page_idx": 2}, {"type": "table", "img_path": "BZLdXBjB8O/tmp/3e9af9a1789ef54ce619abd82c012d67ba9ce335fd028e3d853f67d791f706d0.jpg", "table_caption": [], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "defense baselines including state-of-the-art (SOTA) methods. Specifically, our CausalDiff achieves robustness of $86.39\\%$ $(+4.01\\%)$ on CIFAR10, $56.25\\%$ $(+3.13\\%)$ on CIFAR-100 [18], and $81.79\\%$ $(+4.93\\%)$ on GTSRB [19]. ", "page_idx": 2}, {"type": "text", "text": "In summary, we highlight our contributions as follows: 1) We propose a novel causal diffusion framework (CausalDiff) to defend against unseen attacks by modeling the generation of native indomain data with the category(Y)-causative factors and the other Y-non-causal factors; 2) We propose a Causal Information Bottleneck (CIB) objective to disentangle Y-causative from Y-non-causative factors during causal model training and an associated inference algorithm for adversarial defense; 3) CausalDiff significantly outperforms SOTA methods in defending against various unseen attacks. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Adversarial Defense. Adversarial training primarily focuses on optimizing the training algorithm [8, 20, 21], incorporating data augmentation [22, 23, 9], and enhancing acceleration [24, 25]. Despite its effectiveness, the trained models could still be vulnerable to unseen attacks [26, 27]. Adversarial purification, orthogonal to our work, utilizes a generative model to purify adversarial noise from examples before classification. Leveraging diffusion models [28, 17, 29, 30], diffusion-based purification have shown to be effective [31\u201335, 27]. ", "page_idx": 2}, {"type": "text", "text": "Causal Learning for Robustness. Causal representation learning [12, 36, 14, 37] focuses on discovering invariant mechanisms within structural causal models and has achieved remarkable performance in improving model transferability [14, 13, 38\u201340] and interpretability [41, 42]. Moreover, in terms of adversarial robustness, researchers [43, 15, 44, 45, 16, 46\u201348] have attempted to model the attack behaviors in causal structures to identify the adversarial factors. However, modeling particular attack types will limit the model robustness on other types of attacks [27]. ", "page_idx": 2}, {"type": "text", "text": "Conditional Diffusion Model. Diffusion models [28, 17, 29, 30, 49] have achieved compelling image generation capabilities. To equip them with the ability of controllable generation, the sampling process can be guided by 1) classifier confidence to generate images of a certain category [30, 50, 51], and 2) semantic embedding of text content or a certain style [52\u201355]. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "3 Pilot Study on Toy Data ", "text_level": 1, "page_idx": 3}, {"type": "image", "img_path": "BZLdXBjB8O/tmp/fe584c0ebd5cf423f0a40b5fd006c202a102519b55f6e61a1d40a0c6370cbaeb.jpg", "img_caption": ["Figure 3: Visualizations of feature space for the two categories on toy data by T-SNE for (a) discriminative model, (b) generative model, (c) causal model without disentanglement, and (d) causal model with disentanglement. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "To compare our proposed causal model with other representative models in terms of adversarial robustness, we constructed a toy dataset according to the hypothesis that the essential factors are the basis of generating labels and they also determine the generation of data together with label-noncausative features. Pilot studies on this toy data will provide insights into whether our causal model will work and why it works. ", "page_idx": 3}, {"type": "text", "text": "3.1 Experimental Settings ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Toy Data Construction. We constructed 2000 samples following the causal structure in Fig. 1 (Left). Specifically, for each data point, we randomly sample the vectors $s$ and $z$ from two different normal distributions, project $s$ to a score $y_{s}$ with random weights, obtain its label by comparing $y_{s}$ with the medium score, and generate the representation $x$ by projecting the concatenation $\\left[s;z\\right]$ with another random matrix. Please refer to Appendix B.3 for detailed information. ", "page_idx": 3}, {"type": "text", "text": "Models for Comparisons. We investigated four representative models: 1) a Discriminative model for classification, 2) a Generative model that predicts the label of an adversarial example with $\\operatorname*{max}_{y}p(x|y)$ [27], 3) a causal model without disentanglement (Causalwithout Disent.) that models the generation of both x and y with the same latent factor $v$ , 4) our causal model with disentanglement $(C a u s a l_{w i t h\\;D i s e n t.})$ ) that is illustrated in Fig. 1. The concrete structures of the four models and further details are presented in Appendix B.3. ", "page_idx": 3}, {"type": "text", "text": "3.2 Experimental Observations ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Adversarial Robustness. We evaluate the model\u2019s robustness against a 100-step PGD attack with varying $\\epsilon$ -budgets under the $\\ell_{\\infty}$ bound [8]. Model performance in terms of both clean accuracy (when $\\epsilon=0$ ) and robust accuracy is examined. ", "page_idx": 3}, {"type": "text", "text": "As shown in Fig. 2: 1) The Discriminative model exhibits the highest clean accuracy but suffers a rapid decline in robustness, dropping to $0\\%$ when the attack budget $\\epsilon$ reaches 1.2., which highlights its vulnerability. 2) The Generative model has the lowest clean accuracy while its robustness does not dramatically regress with larger attack budgets. The low clean accuracy may be due to the inconsistency between the generation process it modeled and the way this toy dataset is constructed. The small gap between clean and robust accuracy indicates its decent effectiveness in defending against adversarial attacks. 3) Causalwith Disent. obtains the second-best clean accuracy, yet its robustness gradually declines with increasing attack strength, maintaining $61.4\\%$ robustness at $\\epsilon=6.4$ . 4) Our model, Causalwith Disent., has slightly lower clean accuracy than Causal without Disent. but the best robustness among the four. As the attack strength increases, it maintains at least $71.8\\%$ robust accuracy at $\\epsilon=6.4$ , indicating that it is promising to enhance model robustness by causal modeling with disentanglement of the label-causative factors from the non-causative ones. ", "page_idx": 3}, {"type": "text", "text": "Sensitivity to Perturbations. To delve further into how the model behavior changes when defending against adversarial perturbations, we measure the variation between latent variables of clean and adversarial examples, denoted as $\\triangle\\ v\\,=\\,1\\,-\\,c o s i n e(v,v_{\\mathrm{adv}})$ , where $v$ and $\\boldsymbol{v}_{\\mathrm{adv}}$ represents the latent vector of clean and adversarial example, respectively $\\boldsymbol{v}=s$ for $\\operatorname{Causal}_{w i t h\\ D i s e n t.})$ ). We also compute the change of predicted logits, $\\triangle$ $\\grave{\\mathbf{\\uprho}}\\bar{p}(y|v)\\bar{=}p(y|v)-p(y|v_{\\mathrm{adv}})$ , with $y$ being the true class label. Note that a larger variation in the latent factor for prediction, or the predicted logits, results in increased insensitivity to perturbations. ", "page_idx": 4}, {"type": "text", "text": "The experimental results, as shown in Tab. 4, indicate that $\\mathrm{Causal}_{w i t h\\ D i s e n t.}$ , compared to Causalwithout Disent., exhibits less sensitivity in both latent variables and prediction outcomes. This suggests that by disentangling the label-causative factor $s$ , it becomes more challenging for attackers to perturb the model and alter its prediction. The small variation of $v$ in the Generative model is probably attributed to the lack of discriminability in $v$ . ", "page_idx": 4}, {"type": "text", "text": "Prediction Margins. To intuitively explain the sensitivity of each model to perturbations, we estimate the minimal adversarial perturbation $\\|\\delta\\|$ under PGD required to flip the correct model prediction $y$ of a sample $x$ . It can be interpreted as the prediction margins in terms of perturbation, denoted as $\\mathrm{nargin}(x,y)=\\mathrm{min}\\,\\|\\delta\\|$ , subject to $p(y|x+\\delta)<p(\\bar{y}|x+\\delta)$ [56]. We measure the $\\|\\delta\\|$ under $\\ell_{2}$ and $\\ell_{\\infty}$ norms. Additionally, we visualized the latent vector space of each model to intuitively observe the margin of the classification boundary. ", "page_idx": 4}, {"type": "text", "text": "As shown in Tab. 4, Causalwith Disent. has the largest prediction margin. This implies that an attacker would need to add significantly more perturbation to successfully cheat our model. We can draw similar conclusions from Fig. 3, which illustrates the distribution of predictive features extracted by each model for correctly classified clean samples across categories. Again, Causalwith Disent. has the largest margin between the classes, indicating that it has high confidence in its prediction and increases the cost and difficulty for an attacker to succeed. ", "page_idx": 4}, {"type": "text", "text": "4 Causal Diffusion Model ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To enhance model robustness on real-world data, in this section, we propose a Causal Diffusion (CausalDiff) Modeling approach, that couples our previously studied SCM (shown in Fig. 6(d)) with diffusion models. We will introduce the three major components in CausalDiff: conditional diffusion generation, causal information bottleneck optimization, and adversarial example inference. We take the Denoising Diffusion Probabilistic Model (DDPM) [17] as an instance for illustration and it can be easily adapted to other diffusion models. ", "page_idx": 4}, {"type": "text", "text": "4.1 Conditional Diffusion Generation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Standard diffusion models generate images based on random noise which do not apply to the conditional generation of $X$ based on $S$ and $Z$ in our SCM. In standard diffusion models, at each time step $t$ during denoising, a UNet $\\epsilon_{\\theta}(x_{t},t)$ is employed to decode an image given input $x_{t}$ . While diffusion models are highly effective in image generation, they lack an explicit decoder component for generating images from latent variables. To handle this, we develop a conditional DDPM using latent variables $S$ and $Z$ controlling the generation process. This approach draws inspiration from both the class-conditional diffusion model [51] as well as the style control mechanism introduced in DiffAE [52]. The output $h_{\\mathrm{out}}^{t}$ at each layer of the UNet depends on $t$ and $x_{t}$ , i.e., ", "page_idx": 4}, {"type": "equation", "text": "$$\nh_{\\mathrm{out}}^{t}=t_{\\mathrm{s}}\\cdot\\mathrm{GroupNorm}(h)+t_{\\mathrm{b}},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $h$ is the feature map of $x_{t},t_{\\mathrm{s}}$ and $t_{\\mathrm{b}}$ are the scale and bias of timestep $t$ . ", "page_idx": 4}, {"type": "text", "text": "Inspired by the class-conditional diffusion model [51] and the style control mechanism in DiffAE [52], we adapt the standard diffusion generation to be conditioned on $S$ and $Z$ in addition to timestep $t$ . Specifically, the UNet becomes $\\epsilon_{\\theta}(x_{t},t,s,z)$ , the final output $h_{\\mathrm{out}}^{t,s,z}$ is calculated based on the original $h_{\\mathrm{out}}^{t}$ , the hidden state $s$ and $z$ (representing causal factor $S$ and $Z$ ) encoded from input $x$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\nh_{\\mathrm{out}}^{t,s,z}=z_{\\mathrm{s}}\\cdot h_{\\mathrm{out}}^{t}+s_{\\mathrm{b}},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $z_{s}$ and $s_{b}$ are produced from the affine projections of $z$ and $s$ respectively, i.e., $z_{\\mathrm{s}}=\\mathrm{Affine}_{z}(z)$ and $s_{\\mathrm{b}}=\\mathrm{Affine}_{s}(s)$ . As such, the label-causative factor $S$ acts as a bias that can affect the direction of the latent vector and change its semantics, while the label-non-causative factor $Z$ can only scale the latent vector in a similar way to style control. ", "page_idx": 4}, {"type": "text", "text": "4.2 Causal Information Bottleneck Optimization ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To learn the causal factors $S,Z$ in our SCM and disentangle them, we propose a Causal Information Bottleneck (CIB) optimization objective. It maximizes the mutual information between the latent factors $S,Z$ and the observed data sample $(X,Y)$ with an information bottleneck that constrains the information retained in $S,Z$ with respect to $X$ . ", "page_idx": 5}, {"type": "text", "text": "Specifically, to align the information captured in the latent factors with the observed data $(X,Y)$ , we maximize the mutual information between them, denoted as $\\operatorname{I}(\\mathrm{X},\\mathrm{Y};\\mathrm{S},\\mathrm{Z})$ , which can be derived as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname{I}(X,Y;S,Z)=\\operatorname{I}(X;S,Z)+\\operatorname{I}(Y;S)-\\operatorname{I}(S;Z)-\\operatorname{I}(X;Y).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "A detailed proof is presented in Appendix A.1. Among the resultant terms, $\\operatorname{I}(X;Y)$ is solely dependent on the observed data, independent of latent variables or the causal model, and thus can be ignored in the learning process. Maximizing $\\operatorname{I}(X;S,Z)$ will urge $S$ and $Z$ to capture ample information about $X$ . $\\operatorname{I}(S;Y)$ indicates that the $Y$ -causative factor $S$ should be correlated with $Y$ . The term, $-\\mathrm{I}(S;Z)$ ensures $S$ and $Z$ to be effectively disentangled. ", "page_idx": 5}, {"type": "text", "text": "Existing work on optimizing similar SCMs adapts the Evidence Lower BOund (ELBO) from Variational Autoencoders (VAE) to formulate the causal ELBO objectives [14]. This objective only maximizes the likelihood of $(X,Y)$ , and does not consider the latent factors. Consequently, the final optimization goal of causal ELBO differs from $I(X,Y;S,Z)$ in that it does not have $-\\mathrm{I}(S;Z)$ in Eq. (3), which is crucial for disentanglement. Further details are discussed in Appendix A.4. ", "page_idx": 5}, {"type": "text", "text": "To avoid $X$ contain too many unimportant details, we constrain the mutual information between $X$ and the latent factors $S,Z$ with an information bottleneck $\\mathrm{I}_{c}$ . Then, the updated objective becomes: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\operatorname{I}(X,Y;S,Z),\\quad s.t.\\operatorname{I}(X;S,Z)\\leq\\operatorname{I}_{c}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Employing Lagrange multiplier $\\lambda\\!\\ge\\!0$ , we formulate our objective as ma $\\mathrm{x}\\,\\mathrm{I}(X,Y;S,Z)\\!-\\!\\lambda(\\mathrm{I}(X;S,Z)$ $-\\mathrm{I}_{c})$ . Since $I_{c}$ is a constant, it is equal to maximize the Causal Information Bottleneck (CIB): ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname{CIB}(X,Y,S,Z)=\\operatorname{I}(X;S,Z)+\\operatorname{I}(Y;S)-\\operatorname{I}(S;Z)-\\lambda\\operatorname{I}(X;S,Z).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "$I(X;S,Z)$ and $-\\lambda I(X;S,Z)$ indicate two opposing optimization directions. Because it is unclear whether $(1-\\lambda)$ should be positive or negative, we approximate these terms via two separate lower bounds instead of combining them. ", "page_idx": 5}, {"type": "text", "text": "To maximize the Causal Information Bottleneck (CIB) in Eq. (5), we derive its lower bound as the concrete training loss function. When using the diffusion model $\\epsilon_{\\theta}$ , classifier $f_{y}(s;\\theta)$ (to estimate $p_{\\theta}(y|s))$ , and the encoder $f_{s,z}(\\boldsymbol{x};\\boldsymbol{\\theta})$ (to estimate $p_{\\theta}(s,z|x)$ ), the lower bound of CIB is: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{p(x,s,z)}\\big[\\log p_{\\theta}(x|s,z)\\big]\\!+\\!\\mathbb{E}_{p(y,s)}\\big[\\log p_{\\theta}(y|s)\\big]\\!-\\!\\mathbb{I}_{\\mathrm{CLUB}}^{\\theta}(S;Z)\\!-\\!\\lambda\\mathbb{E}_{p(x)}\\big[\\mathcal{D}_{\\mathrm{KL}}\\big(p_{\\theta}(s,z|x)\\|q(s,z)\\big)\\big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $p_{\\theta}(s|z)$ is a variational distribution to estimate $p(s|z)$ and $\\mathrm{I}_{\\mathrm{CLUB}}^{\\theta}(S;Z)=\\mathbb{E}_{p(s,z)}[\\log p_{\\theta}(s|\\hat{z})]$ $-\\ \\mathbb{E}_{p(z)}\\mathbb{E}_{p(s)}[\\log p_{\\theta}(s|z)]$ represents the Contrastive Log-Ratio Upper Bound (CLUB) of mutual information proposed by Cheng et al. [57]. $p_{\\theta}(x|s,z)$ represents the likelihood estimated by the conditional diffusion model. $\\mathcal{D}_{\\mathrm{KL}}$ refers to the Kullback-Leibler (KL) divergence [58]; and $q(\\cdot)$ denotes the prior distribution of the latent variable, e.g., a normal distribution $\\mathcal{N}(\\mathbf{0},\\mathbf{I})$ . A detailed proof can be found in Appendix A.2. ", "page_idx": 5}, {"type": "text", "text": "Loss Function. Thus, maximizing the lower bound of CIB is equal to minimizing the loss function: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathcal{L}(x,y,s,z;\\theta)\\!\\!}&{=\\left.\\alpha\\mathbb{E}_{\\epsilon,t}\\right\\Vert\\epsilon_{\\theta}(x_{t},t,s,z)\\!-\\!\\epsilon_{t}\\Vert_{2}^{2}\\!+\\!\\gamma\\mathcal{L}_{\\mathrm{CE}}(s,y;\\theta)}\\\\ &{+\\left.\\eta\\right\\Vert_{\\mathrm{CLUB}}^{\\theta}(S;Z)+\\lambda\\mathcal{D}_{\\mathrm{KL}}(p_{\\theta}(s,z|x)\\Vert q(s,z)),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\alpha,\\lambda,\\gamma,\\eta$ determine the weighting of each term in the optimization process. A detailed derivation can be found in Appendix A.3. ", "page_idx": 5}, {"type": "text", "text": "Algorithm. We pretrain the model with the data reconstruction loss (the first term in Eq. (7)) alone before training the model with the entire loss, so that the model can learn the causal factors, disentanglement, and classification from a decent starting point. For space concern, we illustrate the training process in Appendix B.2 involves the training algorithm (Algorithm 1) and the pretraining algorithm (Algorithm 2). ", "page_idx": 5}, {"type": "table", "img_path": "BZLdXBjB8O/tmp/26d6eae27278a83ffd52f6a65f4011b2706a4ca22aa1eab3a65bd21b7b379005.jpg", "table_caption": ["Table 2: Clean accuracy and adversarial robustness on CIFAR-10 against StAdv under $\\ell_{\\infty}$ $\\epsilon=0.05)$ ) norm bound and AutoAttack (AA) under $\\ell_{2}$ $\\epsilon\\,=\\,0.5)$ ) as well as $\\ell_{\\infty}$ $(\\epsilon\\,=\\,8/255)$ bound. We calculate the average robustness across three attack methods to evaluate the model\u2019s robustness against unseen attacks. We use underlining to highlight the best robustness for each attack method within each defense category, and bold font to denote the state-of-the-art (SOTA) across all methods. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "4.3 Adversarial Example Inference ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Guided by the causal generation of an adversarial example $\\tilde{X}$ according to Fig. 1 (Right), we illustrate the process for robust classification. Following a typical attack paradigm, X\u02dc is produced by adding an adversarial perturbation to a target clean example $X$ when attacking a model $\\theta$ . To make a robust prediction on $\\dot{\\tilde{X}}$ , our robust inference process comprises three steps: 1) purifying $\\tilde{X}$ to benign $X$ by the unconditional diffusion model $\\epsilon_{\\theta}(x_{t},t),2)$ inferring $S$ and $Z$ from $X$ utilizing the causal model $\\epsilon_{\\theta}(x_{t},t,s,z)$ , and 3) predicting $Y$ based on $S$ using a classifier $f_{y}(s;\\theta)$ . ", "page_idx": 6}, {"type": "text", "text": "Adversarial Purification (AP). We follow the concept of Likelihood Maximization (LM) [33, 27] to purify the adversarial example $\\tilde{X}$ to a benign $X^{*}$ by maximizing the data log-likelihood $\\log p_{\\theta}(x)$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\nx^{*}=\\arg\\operatorname*{max}_{x}\\log p_{\\theta}(\\tilde{x}).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Concretely, we maximize its lower bound utilizing the unconditional diffusion model $\\epsilon_{\\theta}(x_{t},t)$ trained according to Section C.2. Chen et al. [27] suggest using one random timestep $t$ during each purification iteration while we believe that smaller timesteps should be more effective since they retain more information from the original example. Thus, we limit the random selection to within the first 50 timesteps. This way significantly boosts adversarial robustness, which will be discussed in Section 5.3. ", "page_idx": 6}, {"type": "text", "text": "Causal Factor Inference (CFI). In order to infer the causal and non-causal factors $S$ and $Z$ , which can reconstruct the original image $X$ , we optimize the latent variables by maximizing the conditional likelihood $p_{\\theta}(x|s,z)$ employing the trained conditional diffusion model $\\epsilon_{\\theta}(x_{t},t,s,z)$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\ns^{*},z^{*}=\\arg\\operatorname*{max}_{s,z}\\log p_{\\theta}(x^{*}|s,z).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Similarly to purification, we obtain $s^{*}$ and $z^{*}$ by maximizing the lower bound $-\\mathbb{E}_{\\epsilon,t}[\\bar{w_{t}}]|\\epsilon_{\\theta}(x_{t}\\bar{,t},s,z)\\;-\\;\\epsilon||\\big]$ using the conditional diffusion model $\\bar{\\epsilon_{\\theta}(x_{t},t,s,z)}$ . For efficiency concerns, instead of using all the timesteps for estimation as in Chen et al. [27], we sample $N_{\\mathrm{purify}}=5$ timesteps at the same intervals across the entire timesteps. ", "page_idx": 6}, {"type": "text", "text": "Latent-S-Based Classification (LSBC). After obtaining $s^{*}$ according to Eq. (9), we use the trained classifier $f_{y}(s;\\theta)$ to predict label $Y$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\ny^{*}=\\arg\\operatorname*{max}_{y}\\log p_{\\theta}(y|s^{*}).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "table", "img_path": "BZLdXBjB8O/tmp/d15180024ed060ee6576d91eeccf89afb6ff37af30c44e22231821c508d7955b.jpg", "table_caption": ["Table 3: Clean accuracy and adversarial robustness against AutoAttack (AA) on GTSRB (Left) and CIFAR-100 (Right) dataset. We use $\\epsilon=8/255$ as $\\ell_{\\infty}$ and $\\epsilon=0.5$ as $\\ell_{2}$ norm bound. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Within our inference pipeline, both adversarial purification and causal factor inference leverage the diffusion model learned toward the CIB optimization objective while they take effect independently. When we combine these two approaches, adversarial robustness could be enhanced further. The concrete inference algorithm in Algorithm 3 is presented as Appendix B.2. ", "page_idx": 7}, {"type": "text", "text": "4.4 Comparison with Adversarial Purification ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "First, our CausalDiff can be viewed as semantic-level purification. Instead of pixel-level denoising, CausalDiff purifies an image in the latent space, trying to remove the effect of perturbation by putting it to the label-non-causative features. Second, conventional purification inevitably loses information essential for classification during denoising. By disentangling label-causative features from labelnon-causative features, CausalDiff can retain essential information in the label-causative features to a large extent. Third, unlike pixel-level purification which does not know the optimal denoising level for various attacks, CausalDiff acts adaptively on different attacks by the causal inference of $S$ and $Z$ . Fourth, they can be combined to further enhance adversarial robustness. ", "page_idx": 7}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we introduce the experimental settings in Section 5.1. Section 5.2 presents the main results defending against unforeseen attacks on CIFAR-10, CIFAR-100, and GTSRB datasets. We then evaluate the effectiveness of individual components of CausalDiff in Section 5.3. Due to space constraints, we provide analyses of core components during training and inference in Appendix C.1 and Appendix C.2. Additionally, we showcase examples in Appendix C.3. ", "page_idx": 7}, {"type": "text", "text": "5.1 Experimental Settings ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Datasets and Model Architecture. Our experiments utilize the CIFAR-10, CIFAR-100 [18] and GTSRB [19] datasets. CIFAR-10 and CIFAR-100 each consists of 50,000 training images, categorized into 10 and 100 classes, respectively. GTSRB comprises 39,209 training images (each histogram equalized and resized to $3\\times32\\times32$ ) of German traffic signs, categorized into 43 classes. We use DDPM [17] as the diffusion model. Further details are available in Appendix B.2. ", "page_idx": 7}, {"type": "text", "text": "Attack Evaluation Method. For the CIFAR-10 dataset, we utilize seven types of attack strategies with both $\\ell_{\\infty}$ and $\\ell_{2}$ norm bounds for evaluation. These strategies include StAdv attack [60], $\\mathrm{\\BPDA{+}E O T}$ , and AutoAttack [2] (AA), which comprises white-box attacks such as APGD-ce, APGD-t, FAB-t, and a black-box Square Attack. For CIFAR-100, we follow the setting of Chen et al. [27], evaluating against the $\\ell_{\\infty}$ threat model with $\\epsilon=8/255$ . For the GTSRB dataset, we utilize four types of attacks as well as fog corruptions. Attack methods include AutoAttack, which comprises APGD-ce, APGD-t, FAB-t, and Square Attack. ", "page_idx": 7}, {"type": "text", "text": "5.2 Comparisons on Unseen Attacks ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "CIFAR-10. From the experimental results for CIFAR-10 presented in Table 2, we have several observations: 1) The adversarial training methods perform robustly on the same type of attacks they use for training but poorly on other unseen attacks. The only exception is that when the models employ the most effective attack (i.e., ${\\bf A T}\\!\\!-\\!\\!\\boldsymbol{\\ell}_{\\infty},$ ) for adversarial training, the robustness regarding $\\ell_{2}$ is not hurt much. 2) In contrast, purification methods, especially the ones based on more powerful diffusion models (DDPM and EDM), have decent robustness on each unseen attack and much better average robustness. This is reasonable since they learn to purify the adversarial samples without targeting any specific attacks. 3) Our CausalDiff performs the best not only regarding the average robustness but also on each type of attack. Remarkably, the average robust accuracy $(86.39\\%)$ is only $3.84\\%$ lower than the clean accuracy $(90.23\\%)$ and it boosts the robustness on the most challenging attack $(\\ell_{\\infty})$ to $83.01\\%$ . Notably, the reported CausalDiff is based on DDPM, which acts less effectively than EDM, which can be seen from AT and LM purification. Grounded on a stronger backbone, CausalDiff could achieve even better performance. Table 4 shows the performance under the $\\mathrm{BPDA}+\\mathrm{EOT}\\left(\\mathrm{EOT}=20\\right)$ ) attack. Under this attack, models whose gradients are not accessible and also be compared (e.g., ADP [31]). The results again confirm the superior performance of CausalDiff across different attack types. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "CIFAR-100. The experimental results on CIFAR-100 in Table 3 (Right) indicate that CausalDiff achieves the highest robustness among all, even with a much lower clean accuracy. The low clean accuracy is likely due to the insufficient training samples to learn $S,Z$ , and its weaker backbone - ", "page_idx": 8}, {"type": "image", "img_path": "BZLdXBjB8O/tmp/794db8f0537f30b4bc72e5d1bcdb086c12f8c8baf1bcf02f5a1757118abddb8e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "BZLdXBjB8O/tmp/6d0102a5243071b4d9c04c536b36917f70445526bab1377074799ed31bc8aede.jpg", "table_caption": ["Table 4: Clean and robust accuracy on CIFAR-10 against $\\mathbf{BPDA+EOT}$ against $\\ell_{\\infty}$ $\\epsilon=8/255)$ threat model. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 4: Visualization by T-SNE of the feature space, inferred by our CausalDiff, of the label-causative factor $s$ , label-non-causative factor $z$ , and their concatenation. ", "page_idx": 8}, {"type": "text", "text": "DDPM. We believe more augmented data and a stronger diffusion backbone like EDM (used by RDC [27]) could further enhance the performance. ", "page_idx": 8}, {"type": "text", "text": "GTSRB. The left part of Table 3 show that CausalDiff also has compelling robustness on traffic sign classification, in terms of not only unforeseen adversarial attacks but also natural corruptions like fog. Evaluations based on different types of tasks, and different numbers of classes (10, 100, and 43) have all shown the efficacy of CausalDiff. ", "page_idx": 8}, {"type": "text", "text": "5.3 Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "As mentioned in Section 4.3, our CausalDiff contains Adversarial Purification (AP), Causal Factor Inference (CFI), and Latent-S-Based Classification (LSBC). The last block of Table 2 shows the individual effect of the AP and CFI in CausalDiff. We can see: 1) Our DDPM-based purification (CausalDiff w/o CFI, achieved by AP plus a standard classifier) performs similarly to LM-EDM and is significantly better than LM-DDPM, showing that the strategy of sampling within small timesteps for purification is much more effective than entire timesteps. (We show more analysis on this in Appendix C.2. ) 2) The core component of CasualDiff - causal disentanglement (CausalDiff w/o AP) outperforms all the baselines in terms of average robustness except RDC which incorporates an extra LM purification step. It shows that modeling the generative of native in-domain data can enhance the model\u2019s inherent robustness and thus effectively defend against various types of attacks. ", "page_idx": 8}, {"type": "text", "text": "5.4 Visualization of Latent Factors ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To understand how the latent causal factors $S$ and $Z$ in CausalDiff take effect during adversarial classification, we visualize $S,\\,Z$ , and their concatenation using t-SNE in Fig. 4. We randomly sampled 5000 correctly classified test samples from CIFAR-10 for visualization. We find that the Y-causative factor $S$ of samples in each category are located in the same cluster, with clear margins between different clusters except the categories - dog, cat, and bird. These three categories share more commonalities than the others and are not surprising to have blurred boundaries. Additionally, the $S$ vectors of airplanes (dark blue) are near those of birds (green), and trucks have $S$ vectors near automobiles. In contrast to the $S$ vectors, the vectors of $Z$ do not exhibit correlations with the categories. This also aligns with our objective to extract the Y-non-causative factors to $Z$ . The vectors of their concatenation, i.e., $[s;z]$ , also display in clusters but with much more blurred boundaries. These observations are consistent with our commonsense knowledge, showing that CausalDiff has learned reasonable Y-causative factors by $S$ and Y-non-causative factors by $Z$ . ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We develop a causal model based on diffusion model to improve adversarial robustness. A pilot study on toy data suggests that the model defends against adversarial attacks by leveraging label-causative features to resist perturbations and expand the model\u2019s margin. Moreover, on the CIFAR-10, CIFAR100 and GTSRB datasets, our model appears to capture semantic features consistent with the human decision-making process and surpass all baseline models, achieving state-of-the-art performance in adversarial robustness, particularly against unseen attacks. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work is supported by the Strategic Priority Research Program of the Chinese Academy of Sciences, Grant No. XDB0680101, CAS Project for Young Scientists in Basic Research under Grant No. YSBR-034, and Innovation Project of ICT CAS under Grant No. E261090. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] I. J. Goodfellow, J. Shlens, and C. Szegedy, \u201cExplaining and harnessing adversarial examples,\u201d arXiv preprint arXiv:1412.6572, 2014.   \n[2] F. Croce and M. Hein, \u201cReliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks,\u201d in International conference on machine learning. PMLR, 2020, pp. 2206\u20132216.   \n[3] K. Eykholt, I. Evtimov, E. Fernandes, B. Li, A. Rahmati, C. Xiao, A. Prakash, T. Kohno, and D. Song, \u201cRobust physical-world attacks on deep learning visual classification,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 1625\u20131634.   \n[4] M. Sharif, S. Bhagavatula, L. Bauer, and M. K. Reiter, \u201cAccessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition,\u201d in Proceedings of the 2016 acm sigsac conference on computer and communications security, 2016, pp. 1528\u20131540.   \n[5] J. Levinson, J. Askeland, J. Becker, J. Dolson, D. Held, S. Kammel, J. Z. Kolter, D. Langer, O. Pink, V. Pratt et al., \u201cTowards fully autonomous driving: Systems and algorithms,\u201d in 2011 IEEE intelligent vehicles symposium (IV). IEEE, 2011, pp. 163\u2013168.   \n[6] N. Carlini, G. Katz, C. Barrett, and D. L. Dill, \u201cProvably minimally-distorted adversarial examples,\u201d arXiv preprint arXiv:1709.10207, 2017.   \n[7] M. Hein and M. Andriushchenko, \u201cFormal guarantees on the robustness of a classifier against adversarial manipulation,\u201d Advances in neural information processing systems, vol. 30, 2017.   \n[8] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, \u201cTowards deep learning models resistant to adversarial attacks,\u201d arXiv preprint arXiv:1706.06083, 2017.   \n[9] Z. Wang, T. Pang, C. Du, M. Lin, W. Liu, and S. Yan, \u201cBetter diffusion models further improve adversarial training,\u201d arXiv preprint arXiv:2302.04638, 2023.   \n[10] P. Samangouei, M. Kabkab, and R. Chellappa, \u201cDefense-gan: Protecting classifiers against adversarial attacks using generative models,\u201d arXiv preprint arXiv:1805.06605, 2018.   \n[11] J. Pearl and D. Mackenzie, The book of why: the new science of cause and effect. Basic books, 2018.   \n[12] B. Sch\u00f6lkopf, F. Locatello, S. Bauer, N. R. Ke, N. Kalchbrenner, A. Goyal, and Y. Bengio, \u201cToward causal representation learning,\u201d Proceedings of the IEEE, vol. 109, no. 5, pp. 612\u2013634, 2021.   \n[13] C. Liu, X. Sun, J. Wang, H. Tang, T. Li, T. Qin, W. Chen, and T.-Y. Liu, \u201cLearning causal semantic representation for out-of-distribution prediction,\u201d Advances in Neural Information Processing Systems, vol. 34, pp. 6155\u20136170, 2021.   \n[14] X. Sun, B. Wu, X. Zheng, C. Liu, W. Chen, T. Qin, and T.-Y. Liu, \u201cRecovering latent causal factor for generalization to distributional shifts,\u201d Advances in Neural Information Processing Systems, vol. 34, pp. 16 846\u201316 859, 2021.   \n[15] Y. Zhang, M. Gong, T. Liu, G. Niu, X. Tian, B. Han, B. Sch\u00f6lkopf, and K. Zhang, \u201cCausaladv: Adversarial robustness through the lens of causality,\u201d arXiv preprint arXiv:2106.06196, 2021.   \n[16] Q. Ren, Y. Chen, Y. Mo, Q. Wu, and J. Yan, \u201cDice: Domain-attack invariant causal learning for improved data privacy protection and adversarial robustness,\u201d in Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2022, pp. 1483\u20131492.   \n[17] J. Ho, A. Jain, and P. Abbeel, \u201cDenoising diffusion probabilistic models,\u201d Advances in Neural Information Processing Systems, vol. 33, pp. 6840\u20136851, 2020.   \n[18] A. Krizhevsky, G. Hinton et al., \u201cLearning multiple layers of features from tiny images,\u201d 2009.   \n[19] J. Stallkamp, M. Schlipsing, J. Salmen, and C. Igel, \u201cThe german traffic sign recognition benchmark: a multi-class classification competition,\u201d in The 2011 international joint conference on neural networks. IEEE, 2011, pp. 1453\u20131460.   \n[20] H. Zhang, Y. Yu, J. Jiao, E. Xing, L. El Ghaoui, and M. Jordan, \u201cTheoretically principled trade-off between robustness and accuracy,\u201d in International conference on machine learning. PMLR, 2019, pp. 7472\u20137482.   \n[21] C. Qin, J. Martens, S. Gowal, D. Krishnan, K. Dvijotham, A. Fawzi, S. De, R. Stanforth, and P. Kohli, \u201cAdversarial robustness through local linearization,\u201d Advances in Neural Information Processing Systems, vol. 32, 2019.   \n[22] S. Gowal, S.-A. Rebuff,i O. Wiles, F. Stimberg, D. A. Calian, and T. A. Mann, \u201cImproving robustness using generated data,\u201d Advances in Neural Information Processing Systems, vol. 34, pp. 4218\u20134233, 2021.   \n[23] S.-A. Rebuff,i S. Gowal, D. A. Calian, F. Stimberg, O. Wiles, and T. Mann, \u201cFixing data augmentation to improve adversarial robustness,\u201d arXiv preprint arXiv:2103.01946, 2021.   \n[24] A. Shafahi, M. Najibi, M. A. Ghiasi, Z. Xu, J. Dickerson, C. Studer, L. S. Davis, G. Taylor, and T. Goldstein, \u201cAdversarial training for free!\u201d Advances in Neural Information Processing Systems, vol. 32, 2019.   \n[25] H. Jiang, P. He, W. Chen, X. Liu, J. Gao, and T. Zhao, \u201cSmart: Robust and efficient fine-tuning for pre-trained natural language models through principled regularized optimization,\u201d arXiv preprint arXiv:1911.03437, 2019.   \n[26] C. Laidlaw, S. Singla, and S. Feizi, \u201cPerceptual adversarial robustness: Defense against unseen threat models,\u201d arXiv preprint arXiv:2006.12655, 2020.   \n[27] H. Chen, Y. Dong, Z. Wang, X. Yang, C. Duan, H. Su, and J. Zhu, \u201cRobust classification via a single diffusion model,\u201d arXiv preprint arXiv:2305.15241, 2023.   \n[28] J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli, \u201cDeep unsupervised learning using nonequilibrium thermodynamics,\u201d in International Conference on Machine Learning. PMLR, 2015, pp. 2256\u20132265.   \n[29] Y. Song and S. Ermon, \u201cGenerative modeling by estimating gradients of the data distribution,\u201d Advances in neural information processing systems, vol. 32, 2019.   \n[30] Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole, \u201cScore-based generative modeling through stochastic differential equations,\u201d arXiv preprint arXiv:2011.13456, 2020.   \n[31] J. Yoon, S. J. Hwang, and J. Lee, \u201cAdversarial purification with score-based generative models,\u201d in International Conference on Machine Learning. PMLR, 2021, pp. 12 062\u201312 072.   \n[32] C. Xiao, Z. Chen, K. Jin, J. Wang, W. Nie, M. Liu, A. Anandkumar, B. Li, and D. Song, \u201cDensepure: Understanding diffusion models towards adversarial robustness,\u201d arXiv preprint arXiv:2211.00322, 2022.   \n[33] W. Nie, B. Guo, Y. Huang, C. Xiao, A. Vahdat, and A. Anandkumar, \u201cDiffusion models for adversarial purification,\u201d arXiv preprint arXiv:2205.07460, 2022.   \n[34] Q. Wu, H. Ye, and Y. Gu, \u201cGuided diffusion model for adversarial purification from random noise,\u201d arXiv preprint arXiv:2206.10875, 2022.   \n[35] J. Wang, Z. Lyu, D. Lin, B. Dai, and H. Fu, \u201cGuided diffusion model for adversarial purification,\u201d arXiv preprint arXiv:2205.14969, 2022.   \n[36] J. Mitrovic, B. McWilliams, J. Walker, L. Buesing, and C. Blundell, \u201cRepresentation learning via invariant causal mechanisms,\u201d arXiv preprint arXiv:2010.07922, 2020.   \n[37] B. Sch\u00f6lkopf, \u201cCausality for machine learning,\u201d in Probabilistic and Causal Inference: The Works of Judea Pearl, 2022, pp. 765\u2013804.   \n[38] Y. Chen, Y. Zhang, Y. Bian, H. Yang, M. Kaili, B. Xie, T. Liu, B. Han, and J. Cheng, \u201cLearning causally invariant representations for out-of-distribution generalization on graphs,\u201d Advances in Neural Information Processing Systems, vol. 35, pp. 22 131\u201322 148, 2022.   \n[39] W. Wang, X. Lin, F. Feng, X. He, M. Lin, and T.-S. Chua, \u201cCausal representation learning for out-of-distribution recommendation,\u201d in Proceedings of the ACM Web Conference 2022, 2022, pp. 3562\u20133571.   \n[40] R. Wang, M. Yi, Z. Chen, and S. Zhu, \u201cOut-of-distribution generalization with causal invariant transformations,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 375\u2013385.   \n[41] R. Moraffah, M. Karami, R. Guo, A. Raglin, and H. Liu, \u201cCausal interpretability for machine learning-problems, methods and evaluation,\u201d ACM SIGKDD Explorations Newsletter, vol. 22, no. 1, pp. 18\u201333, 2020.   \n[42] G. Xu, T. D. Duong, Q. Li, S. Liu, and X. Wang, \u201cCausality learning: A new perspective for interpretable machine learning,\u201d arXiv preprint arXiv:2006.16789, 2020.   \n[43] C. Zhang, K. Zhang, and Y. Li, \u201cA causal view on robustness of neural networks,\u201d Advances in Neural Information Processing Systems, vol. 33, pp. 289\u2013301, 2020.   \n[44] K. Tang, M. Tao, and H. Zhang, \u201cAdversarial visual robustness by causal intervention,\u201d arXiv preprint arXiv:2106.09534, 2021.   \n[45] S. Yang, T. Guo, Y. Wang, and C. Xu, \u201cAdversarial robustness through disentangled representations,\u201d in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 35, no. 4, 2021, pp. 3145\u20133153.   \n[46] B.-K. Lee, J. Kim, and Y. M. Ro, \u201cMitigating adversarial vulnerability through causal parameter estimation by adversarial double machine learning,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 4499\u20134509.   \n[47] J. Kim, B.-K. Lee, and Y. M. Ro, \u201cDemystifying causal features on adversarial examples and causal inoculation for robust network by adversarial instrumental variable regression,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 12 302\u201312 312.   \n[48] H. Hua, J. Yan, X. Fang, W. Huang, H. Yin, and W. Ge, \u201cCausal information bottleneck boosts adversarial robustness of deep neural network,\u201d arXiv preprint arXiv:2210.14229, 2022.   \n[49] T. Karras, M. Aittala, T. Aila, and S. Laine, \u201cElucidating the design space of diffusion-based generative models,\u201d Advances in Neural Information Processing Systems, vol. 35, pp. 26 565\u2013 26 577, 2022.   \n[50] P. Dhariwal and A. Nichol, \u201cDiffusion models beat gans on image synthesis,\u201d Advances in Neural Information Processing Systems, vol. 34, pp. 8780\u20138794, 2021.   \n[51] J. Ho and T. Salimans, \u201cClassifier-free diffusion guidance,\u201d arXiv preprint arXiv:2207.12598, 2022.   \n[52] K. Preechakul, N. Chatthee, S. Wizadwongsa, and S. Suwajanakorn, \u201cDiffusion autoencoders: Toward a meaningful and decodable representation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 10 619\u201310 629.   \n[53] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, \u201cHigh-resolution image synthesis with latent diffusion models,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022, pp. 10 684\u201310 695.   \n[54] A. Nichol, P. Dhariwal, A. Ramesh, P. Shyam, P. Mishkin, B. McGrew, I. Sutskever, and M. Chen, \u201cGlide: Towards photorealistic image generation and editing with text-guided diffusion models,\u201d arXiv preprint arXiv:2112.10741, 2021.   \n[55] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen, \u201cHierarchical text-conditional image generation with clip latents,\u201d arXiv preprint arXiv:2204.06125, 2022.   \n[56] G. W. Ding, Y. Sharma, K. Y. C. Lui, and R. Huang, \u201cMma training: Direct input space margin maximization through adversarial training,\u201d arXiv preprint arXiv:1812.02637, 2018.   \n[57] P. Cheng, W. Hao, S. Dai, J. Liu, Z. Gan, and L. Carin, \u201cClub: A contrastive log-ratio upper bound of mutual information,\u201d in International conference on machine learning. PMLR, 2020, pp. 1779\u20131788.   \n[58] S. Kullback, Information theory and statistics. Courier Corporation, 1997.   \n[59] R. S. Zimmermann, L. Schott, Y. Song, B. A. Dunn, and D. A. Klindt, \u201cScore-based generative classifiers,\u201d arXiv preprint arXiv:2110.00473, 2021.   \n[60] C. Xiao, J.-Y. Zhu, B. Li, W. He, M. Liu, and D. Song, \u201cSpatially transformed adversarial examples,\u201d arXiv preprint arXiv:1801.02612, 2018.   \n[61] T. Wu, L. Tong, and Y. Vorobeychik, \u201cDefending against physically realizable attacks on image classification,\u201d arXiv preprint arXiv:1909.09552, 2019.   \n[62] M. Hill, J. Mitchell, and S.-C. Zhu, \u201cStochastic security: Adversarial defense using long-run dynamics of energy-based models,\u201d arXiv preprint arXiv:2005.13525, 2020.   \n[63] M. Boudiaf, J. Rony, I. M. Ziko, E. Granger, M. Pedersoli, P. Piantanida, and I. B. Ayed, \u201cA unifying mutual information view of metric learning: cross-entropy vs. pairwise losses,\u201d in European conference on computer vision. Springer, 2020, pp. 548\u2013564. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Proof of Propositions ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we will present the detailed proof for the theoretical results mentioned in the main paper. ", "page_idx": 13}, {"type": "text", "text": "A.1 Proof of Causal Information Bottleneck (CIB) ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "According to the Structural Causal Model (SCM), we have $p(x,y,s,z)=p(s)p(z)p(x|s,z)p(y|s)$ . Thus, the Causal Information Bottleneck (CIB) can be represented as: ", "page_idx": 13}, {"type": "text", "text": "Therefore, ", "page_idx": 13}, {"type": "text", "text": "A.2 Proof of the Lower Bound of CIB ", "text_level": 1, "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad[1.5\\cdot7.5\\cdot2.5(x_{{\\varepsilon}+\\varepsilon,x_{\\varepsilon}})\\sin\\frac{p(x_{\\varepsilon},x_{\\varepsilon},x_{\\varepsilon})}{p(x_{\\varepsilon},x_{\\varepsilon})}]}\\\\ &{=\\frac{5\\cdot76\\cdot(\\alpha(\\varepsilon,x_{\\varepsilon}))\\cdot16^{2(1-\\frac{3}{2})(1-\\frac{3}{2})(1-3)/6}}{1-\\frac{7}{2}(\\alpha(\\varepsilon,x_{\\varepsilon}))\\cdot16^{2(1-\\frac{3}{2})(1-3)/6}}}\\\\ &{=\\frac{5\\cdot76\\cdot(\\alpha(\\varepsilon,x_{\\varepsilon}))\\cdot16^{16}\\cdot(\\alpha(\\varepsilon,x_{\\varepsilon}))\\cdot16^{16}}{1-\\frac{3}{2}(\\alpha(\\varepsilon,x_{\\varepsilon}))\\cdot16^{16}\\cdot(\\alpha(\\varepsilon,x_{\\varepsilon}))}}\\\\ &{=\\frac{5\\cdot76\\cdot(\\alpha(\\varepsilon,x_{\\varepsilon}))\\cdot16^{16}\\cdot(\\alpha(\\varepsilon,x_{\\varepsilon}))\\cdot16^{-3}}{1-\\frac{3}{2}(\\alpha(\\varepsilon,x_{\\varepsilon}))\\cdot16^{16}\\cdot(\\alpha(\\varepsilon,x_{\\varepsilon}))\\cdot16^{-3}}}\\\\ &{=\\frac{16\\cdot76\\cdot(\\alpha(\\varepsilon,x_{\\varepsilon}))\\cdot16^{16}\\cdot(\\alpha(\\varepsilon,x_{\\varepsilon})-2)}{1-\\frac{3}{2}(\\alpha(\\varepsilon,x_{\\varepsilon}))\\cdot16^{16}\\cdot(\\alpha(\\varepsilon,x_{\\varepsilon}))\\cdot16^{-3}}-1.65,}\\\\ &{=\\frac{17\\cdot4.5\\cdot2}{1-\\frac{3}{2}(\\alpha(\\varepsilon,x_{\\varepsilon}))\\cdot16^{16}\\cdot(\\alpha(\\varepsilon,x_{\\varepsilon}))\\cdot16^{-3}}-1.65,}\\\\ &{=\\frac{17\\cdot4.5\\cdot2}{1-\\frac{3}{2}(\\alpha(\\varepsilon,x_{\\varepsilon}))\\cdot16^{16}\\cdot(\\alpha(\\varepsilon,x_{\\varepsilon}))\\cdot16^{-3}}-1.65,}\\\\ &{=\\frac{17\\cdot4.5\\cdot2}{1-\\frac{3}{2}(\\alpha(\\varepsilon,x_{\\varepsilon}))\\cdot16^{16}\\cdot(\\alpha(\\varepsilon,x_{\\varepsilon}))\\cdot16^{16}\\cdot(\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "According to the result in Eq. (11), we further prove its lower bound shown in Eq. (6) in this section.   \nAs for the lower bound of the recontruction term $\\operatorname{I}(X;S,Z)$ , we have:   \n$\\begin{array}{r l}&{\\operatorname{I}(X;S,Z)=\\mathbb{E}_{p(x,s,z)}|\\log\\frac{p(x|s,z)}{p(x)}|}\\\\ &{=\\mathbb{E}_{p(x,s,z)}|\\log\\frac{p(x|s,z)\\,p(\\sigma(x|s,z)}{p(x)\\,p(x)\\,z}),\\quad\\mathrm{~when~using~a~variational~distribution~}p_{\\theta}(x|s,z)\\mathrm{~to~appri~}}\\\\ &{=\\mathbb{E}_{p(x,s,z)}|\\log\\frac{p(x|s,z)}{p(x)}|+\\mathbb{E}_{p(x,z)}\\mathbb{E}_{p(x|s,z)}|\\log\\frac{p(x|s,z)}{p(x)\\,(x)\\,s}|}\\\\ &{=\\mathbb{E}_{p(x,s,z)}|\\log\\frac{p(x|s,z)}{p(x)}|+\\mathbb{E}_{p(s,z)}|\\mathcal{D}_{\\mathrm{KL}}\\frac{p(x|s,z)}{p_{\\theta}(x|s,z)}|,\\quad\\mathrm{where~}\\mathcal{D}_{\\mathrm{KL}}(\\cdot)\\mathrm{~represents~KL.~divergent}}\\\\ &{\\geq\\mathbb{E}_{p(x,s,z)}|\\log\\frac{p(x|s,z)}{p(x)}|}\\\\ &{=\\mathbb{E}_{p(x,s,z)}|\\log p(x|s,z)|+\\mathbb{E}_{p(x,s,z)}|\\log\\frac{1}{p(x)}|}\\end{array}$ oximate $p(x|s,z)$ , = Ep(x,s,z)[log p\u03b8(x|s, z)] + H(x), where $\\mathcal{H}(x)$ indicates entropy of $x$ . ", "page_idx": 13}, {"type": "text", "text": "As for the upper bound of $\\operatorname{I}(X;S,Z)$ , we have: ", "page_idx": 14}, {"type": "text", "text": "$\\operatorname{I}(X;S,Z)=\\mathbb{E}_{p(x,s,z)}[\\log{\\frac{p(x|s,z)}{p(x)}}]$   \n$=\\mathbb{E}_{p(x,s,z)}[\\log\\frac{p(x|s,z)q(s,z)}{p(x)q(s,z)}]$ , when using a prior distribution $q(s,z)$ to estimate $p(s,z)$ ,   \n= Ep(x,s,z)[log pq((xs|,s ,z z)) ] \u2212DKL(p(s, z)||q(s, z)), where $\\mathcal{D}_{\\mathrm{KL}}(\\cdot)$ represents KL-divergence, p(x|s, z)   \n\u2264Ep(x,s,z)[log q(s, z)   \n= Ep(x)Ep(s,z|x)[log pq((s,s ,z z|x)) ]   \n= Ep(x)[DKL(p(s, z|x)||q(s, z))]. ", "page_idx": 14}, {"type": "text", "text": "Regarding the label prediction term $\\operatorname{I}(Y;S)$ , we can maximize the mutual information between factor $S$ and label $Y$ by maximize $\\mathbb{E}_{p(y,s)}[\\log p_{\\theta}(y|s)]$ as well as employing a cross-entropy loss function, according to Boudiaf et al. [63]. ", "page_idx": 14}, {"type": "text", "text": "As for the disentangle term $\\operatorname{I}(S;Z)$ , according to the Contrastive Log-Ratio Upper Bound (CLUB) of mutual information proposed by Cheng et al. [57], we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{I}(S;Z)\\leq\\mathrm{I}_{\\mathrm{CLUB}}^{\\theta}(S;Z)=\\mathbb{E}_{p(s,z)}[\\log p_{\\theta}(s|z)]-\\mathbb{E}_{p(z)}\\mathbb{E}_{p(s)}[\\log p_{\\theta}(s|z)],}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $p_{\\theta}(s|z)$ is a variational distribution to estimate $p(s|z)$ . ", "page_idx": 14}, {"type": "text", "text": "Thus, we have proved the results in Eq. (6) that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\Pi(X;S,Z)+\\operatorname{I}(Y;S)-\\operatorname{I}(S;Z)-\\lambda\\Pi(X;S,Z)}\\\\ &{\\geq\\mathbb{E}_{p(x,s,z)}[\\log p_{\\theta}(x|s,z)]+\\mathbb{E}_{p(y,s)}[\\log p_{\\theta}(y|s)]-\\operatorname{I}_{\\mathrm{CLUB}}^{\\theta}(S;Z)-\\lambda\\,\\mathbb{E}_{p(x)}[\\mathcal{D}_{\\mathrm{KL}}(p_{\\theta}(s,z|x)\\|q(s,z))]}\\\\ &{=\\mathbb{E}_{p(x,s,z)}[\\log p_{\\theta}(x|s,z)]+\\mathbb{E}_{p(y,s)}[\\log p_{\\theta}(y|s)]-\\mathbb{E}_{p(s,z)}[\\log p_{\\theta}(s|z)]}\\\\ &{\\quad+\\,\\mathbb{E}_{p(z)}\\mathbb{E}_{p(s)}[\\log p_{\\theta}(s|z)]-\\lambda\\,\\mathbb{E}_{p(x)}[\\mathcal{D}_{\\mathrm{KL}}(p_{\\theta}(s,z|x)\\|q(s,z))].}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "A.3 Detailed Derivation of Loss Function ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Based on the result in Eq. (15), we can optimize the Causal Information Bottleneck (CIB) $\\operatorname{I}(X,Y;S,Z)$ by maximizing its theoratically lower bound. In this part, we discuss the detailed derivation for the lower bound of CIB in term of designing the loss function proposed in Eq. (7). ", "page_idx": 14}, {"type": "text", "text": "Specifically, for the reconstruction term $\\mathbb{E}_{p(x,s,z)}[\\log p_{\\theta}(x|s,z)]$ , we can maximize the log-likelihood estimated by our conditional diffusion model: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\log p_{\\theta}(x|s,z)\\geq-\\mathbb{E}_{\\epsilon,t}[w_{t}\\|\\epsilon_{\\theta}(x_{t},t,s,z)-\\epsilon\\|]+C,}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\epsilon_{\\theta}(x_{t},t,s,z)$ denotes our conditional diffusion model, and constant $C$ is negligible [17]. ", "page_idx": 14}, {"type": "text", "text": "Regarding the label prediction term $\\mathbb{E}_{p(y,s)}[\\log p_{\\theta}(y|s)]$ , following the results proposed by Boudiaf et al. [63], we can leveraging a cross-entropy loss function to maximize the mutual information between factor $S$ and label $Y$ . ", "page_idx": 14}, {"type": "text", "text": "As for the disentanglement term $\\operatorname{I}(S;Z)$ , we following the optimization strategies proposed by Cheng et al. [57], leveraging a predictor $p_{\\theta}$ to learn the relationship between $S$ and $Z$ so as to estimate $\\mathrm{I}_{\\mathrm{CLUB}}^{\\theta}(S;Z)$ . ", "page_idx": 14}, {"type": "text", "text": "Overall, we have proved the loss function of our Causal Information Bottleneck (CIB) proposed in Eq. (7): ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathcal{L}(x,y,s,z;\\theta)=\\alpha\\,\\mathbb{E}_{\\epsilon,t}\\,\\|\\epsilon_{\\theta}\\,(x_{t},t,s,z)-\\epsilon_{t}\\|_{2}^{2}+\\gamma\\mathcal{L}_{\\mathrm{CE}}(s,y;\\theta)}\\\\ &{+\\,\\eta\\{\\mathbb{E}_{p(s,z)}[\\log p_{\\theta}(s|z)]-\\mathbb{E}_{p(z)}\\mathbb{E}_{p(s)}[\\log p_{\\theta}(s|z)]\\}+\\lambda\\,\\mathcal{D}_{\\mathrm{KL}}(p_{\\theta}(s,z|x)\\|q(s,z)),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\mathbb{E}_{p(s,z)}[\\log p_{\\theta}(s|z)]-\\mathbb{E}_{p(z)}\\mathbb{E}_{p(s)}[\\log p_{\\theta}(s|z)]$ is the estimation of $\\mathrm{I}_{\\mathrm{CLUB}}^{\\theta}(S;Z)$ . ", "page_idx": 14}, {"type": "text", "text": "Require: Dataset $\\mathcal{D}$ ; The CausalDiff parameterized by $\\theta$ pretrained by Algorithm 2 involves an UNet $\\epsilon_{\\theta}(x_{t},t,s,z)$ with diffusion timestep $T$ , an encoder $f_{(s,z)}(x;\\theta)$ , a classifier $f_{y}(s;\\theta)$ , and an MI estimater $f_{\\mathrm{CLUB}}(s,z;\\theta)$ for the CLUB loss; an optimizer $\\operatorname{optim}(\\cdot)$ , dropout probability $p_{\\mathrm{drop}}$ of $s,z$ for simultaneously training an unconditional generation, hyperparameters $\\alpha,\\lambda,\\gamma,\\eta$ . ", "page_idx": 15}, {"type": "text", "text": "Initialize: Load $f_{s,z}(x;\\theta)$ and $\\epsilon_{\\theta}$ from the pretrained model $\\theta$ . Initialize $f_{y}(s;\\theta)$ and $f_{\\mathrm{CLUB}}(s,z;\\theta)$ randomly.   \nfor $\\scriptstyle\\mathtt{e p}=1$ to $N_{2}$ do Get mini-batch $(x,y)\\sim\\mathcal{D}$ $s,z=f_{(s,z)}(x;\\theta)$ $t\\sim\\mathrm{Uniform}(\\{1,2,...,T\\}),\\epsilon\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})$ if $r a n d(0,1)\\leq p_{\\mathrm{drop}}$ then Compute loss $\\mathcal{L}(x;\\theta)=\\left\\|\\epsilon_{\\theta}\\left(x_{t},t\\right)-\\epsilon\\right\\|_{2}^{2}$ Update $\\epsilon_{\\theta}$ with optim $(\\theta,\\mathcal{L}(x;\\theta))$ else Compute loss $\\mathcal{L}(x,y,s,z;\\theta)$ according to Eq. (7) Update $\\epsilon_{\\theta}(x_{t},t,s,z)$ , $f_{(s,z)}(x;\\theta)$ , and $f_{y}(s;\\theta)$ by $\\operatorname{optim}(\\theta,\\mathcal{L}(x,y,s,z;\\theta))$ end if Update $f_{\\mathrm{CLUB}}(s,z;\\theta)$ according to the CLUB algorithm [57]   \nend for ", "page_idx": 15}, {"type": "text", "text": "A.4 ELBO (Evidence Lower BOund) V.S. MI (Mutual Information) ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "the Causal Evidence Lower BOund (ELBO) for multi-domain datasets as proposed by Sun et al. [14], we directly formulate the Causal ELBO within our causal structure, as depicted in Fig. 1 (Left). Given that $p(x,\\overdot{y},s,z)=p(s)p(z)p(x|s,z)p(y|s)$ , we can derive the Causal ELBO in single domain as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{3LBO}\\ =\\mathbb{E}_{p(x,y}[\\mathbb{E}_{q_{\\psi}(s,z\\mid x,y)}\\log\\frac{p_{\\theta}(x,y,s,z)}{q_{\\psi}(s,z\\mid x,y)}]}\\\\ &{\\quad\\quad=\\mathbb{E}_{p(x,y)}\\Big\\{\\mathbb{E}_{q_{\\psi}(s,z\\mid x,y)}\\Big[\\log p_{\\theta}(x\\mid s,z)+\\log\\frac{p_{\\theta}(s,z)}{q_{\\psi}(s,z\\mid x)}+\\log p_{\\theta}(y\\mid s)+\\log\\frac{q_{\\psi}(y\\mid x)}{q_{\\psi}(y\\mid\\underline{{s}})}\\Big]\\Big\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $p_{\\theta}$ is to learn the ground-truth $p$ and $q_{\\psi}$ is variational distribution to mimic $p_{\\theta}$ . ", "page_idx": 15}, {"type": "text", "text": "Specifically, the causal ELBO, compared to our Causal Information Bottleneck (CIB), incorporates the reconstruction term $\\log p_{\\theta}(x\\mid s,z)$ , the insensitivity term lo gqp\u03c8\u03b8((ss,,zz|)x), and the label prediction term $\\log p_{\\theta}(y\\mid s)$ . However, it overlooks the disentanglement of latent factors, a critical aspect for effectively learning the causal model. Additionally, we have empirically assessed the robustness of models trained with either CIB or causal ELBO (equivalent to $\\eta=0$ ) in section C.1. This evaluation aims to investigate the efficacy of the disentanglement term $\\operatorname{I}(S;Z)$ . ", "page_idx": 15}, {"type": "text", "text": "B More Implementation Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "B.1 Baselines ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We include representative defense methods of adversarial training (AT), purification, and other types (e.g., generative-model-based approach) as baselines. Specifically, we compare with the AT methods [9, 23] that use DDPM or the Elucidating Diffusion Model (EDM) [49] for data augmentation and a theoretical framework TRADES [20] for adversarial training. We also include causality-based AT baselines such as CausalAdv [15] and DICE [16]. Purification baselines include DiffPure [33] (grounded on Score SDE [30]) and diffusion-based Likelihood Maximization (LM)[27] with EDM (as in the original paper) and DDPM (we reproduced). Other defense baselines comprise generative classifiers such as Score-Based Generative Classifier (SBGC) [59], [27], and the causality-based ", "page_idx": 15}, {"type": "text", "text": "Require: Dataset $\\mathcal{D}$ ; a diffusion model $\\epsilon_{\\theta}$ , an encoder $f_{(s,z)}(x;\\theta)$ , a classifier $f_{y}(s;\\theta)$ ; an optimizer optim $(\\cdot)$ , probability $p_{\\mathrm{drop}}$ of training samples for unconditional diffusion, diffusion training epoch $N_{1}$ . ", "page_idx": 16}, {"type": "text", "text": "Initialize: randomly initialize parameter $\\theta$ .   \nfor $\\scriptstyle\\mathtt{e p=1}$ to $N_{1}$ do $(x,y)\\sim\\mathcal{D}$ $s,z=f_{(s,z)}(x;\\theta)$ $s,z\\gets\\phi$ with probability $p_{\\mathrm{drop}}\\qquad\\triangleright$ randomly mask the latent factors with probability pdrop $t\\sim\\mathrm{Uniform}(\\bar{\\{}1,2,...,\\bar{T}\\bar{\\}}),\\epsilon$ $\\dot{\\epsilon}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})$ $\\mathcal{L}(x,s,z;\\theta)=\\left\\|\\epsilon_{\\theta}\\left(x_{t},t,s,z\\right)-\\epsilon\\right\\|_{2}^{2}$ Update $\\epsilon_{\\theta}$ and $f_{(s,z)}(x;\\theta)$ by $\\operatorname{optim}(\\theta,\\mathcal{L}(x,s,z;\\theta))$   \nend for ", "page_idx": 16}, {"type": "text", "text": "Algorithm 3 Adversarially Robust Inference Algorithm ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Require: A fully-trained causal model involves diffusion model $\\epsilon_{\\theta}(x_{t},t,s,z)$ , encoder $\\overline{{f_{(s,z)}(x;\\theta)}}$ , and classifier $f_{y}(s;\\theta)$ ; test image $x$ , purification optimization steps $N_{\\mathrm{purify}}$ , causal factor inference steps $N_{\\mathrm{infer}}$ , number of sampling steps $N_{t}$ for inference,optimizer $\\mathrm{\\partialptim_{purify}}$ for purification, optimizer op $\\mathrm{\\tim}_{\\mathrm{infer}}$ for causal factor inference. ", "page_idx": 16}, {"type": "text", "text": "Initialize: purified image $x^{*}=x$   \nStage 1: Adversarial Purification   \nfor iter=1 to $N_{\\mathrm{purify}}$ do t \u223cUniform $(\\{\\mathrm{i},2,...,50\\})$ ), $\\boldsymbol{\\epsilon}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})$ $\\mathcal{L}(x^{*},\\theta)=\\left\\|\\epsilon_{\\theta}\\left(x_{t},t\\right)-\\epsilon\\right\\|_{2}^{2}$ Update $x^{*}$ by $\\mathrm{optim}_{\\mathrm{purify}}(x^{*};\\mathcal{L}(x^{*},\\theta))$   \nend for   \nStage 2: Causal Factor Inference   \nInitial $s^{*},z^{*}=f_{(s,z)}(x^{*};\\theta)$   \nfor iter ${}^{=1}$ to $N_{\\mathrm{infer}}$ do Sample $N_{t}$ timesteps $t$ at equal intervals from 1 to $T$ Sample $N_{t}\\,\\epsilon\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})$ corresponds to each $t$ Compute $\\mathcal{L}(x^{*},s^{*},z^{*};\\theta)=\\mathbb{E}_{\\epsilon,t}\\left\\|\\epsilon_{\\theta}\\left(x_{t}^{*},t,s,z\\right)-\\epsilon\\right\\|_{2}^{2}$ Update $s^{*},z^{*}$ by $)p t i m_{\\mathrm{infer}}(s^{*},z^{*};\\mathcal{L}(x^{*},s^{*},z^{*};\\theta))$ )   \nend for   \nStage 3: S-based Classification   \n$y=f_{y}(s^{*};\\theta)$ ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "generative model CAMA [43]. Notably, we also include the SOTA method on unseen attacks - Robust Diffusion Classifier (RDC), which incorporates purification and conditional generation based on labels for defense. ", "page_idx": 16}, {"type": "text", "text": "B.2 Implementation Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We use DDPM [17] as our generative model. For the encoder $f_{s,z}(\\boldsymbol{x};\\boldsymbol{\\theta})$ and the classification model $f_{s}(y;\\theta)$ , we employ WideResNet-70-16 (WRN-70-16) as the backbone. ", "page_idx": 16}, {"type": "text", "text": "Training Strategy. Considering the different complexities in learning $f_{x}(s,z;\\theta)$ , $f_{(s,z)}(x;\\theta)$ and $f_{y}(s;\\theta)$ , attributed to the differing difficulty in generation and discrimination task, we segment the training of the entire causal model into two distinct phases. As outlined in Section 4.2, we employ a two-stage training process for our CausalDiff. ", "page_idx": 16}, {"type": "image", "img_path": "BZLdXBjB8O/tmp/1d461a06a0611cf7f39343b0f1e9850c7ff0f389dd9b681f46a97ebf0751130e.jpg", "img_caption": ["Figure 5: SCM of models for pilot study including (a) discriminative model, (b) generative model, (c) causal model without disentanglement, and (d) causal model with disentanglement. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "BZLdXBjB8O/tmp/d639fb73fcc129edaf9858254d14c703099427dfe2d23f04c64b895d3c6a6514.jpg", "img_caption": ["Figure 6: Architecture of models for pilot study including (a) discriminative model, (b) generative model, (c) causal model without disentanglement, and (d) causal model with disentanglement. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "In the pretrain phase on CIFAR-10 and GTSRB datasets, focused on generation, primarily trains the conditional diffusion model $f_{x}(s,z;\\theta)$ along with its corresponding encoder $f_{(s,z)}(x;\\theta)$ according to Algorithm 2, setting $N_{1}=1440$ . For CIFAR-100, we added 10,000 images generated by EDM [49] to our training set. Considering the computational cost, we only pretrain for $N_{1}=500$ epochs. Note that the augmented data is used only during the pretraining phase while not in the joint training phase. ", "page_idx": 17}, {"type": "text", "text": "Subsequently, we conduct joint training of the whole CausalDiff model for $N_{2}=560$ , amounting to a total of 2000 epochs. The second phase, targeting discrimination and leveraging label information to guide disentanglement, involves the joint training of the entire causal model. Note that, in order to simultaneously train an unconditional diffusion model for adversarial purification, we follow Ho and Salimans [51] to mask the condition $s$ and $z$ with probability $p_{\\mathrm{drop}}=0.1$ . Thus, a single shared model is used for both adversarial purification and causal factor inference. ", "page_idx": 17}, {"type": "text", "text": "Both the pretraining and joint training phases utilize a learning rate of $1e^{-4}$ and a batch size of 128. For simplicity, we follow the setting of $w_{t}=1$ [17]. We set $\\alpha=1$ ., $\\gamma=1e^{-2}$ , $\\eta=1e^{-5},\\lambda=1e^{-2}$ as the weights for the loss function in Eq. (7). ", "page_idx": 17}, {"type": "text", "text": "Since we need a standard diffusion model $\\epsilon_{\\theta}(x_{t},t)$ for purification during adversarial inference, we apply dropout of $s$ and $z$ with a ratio of $p_{\\mathrm{drop}}=0.1$ for conditional diffusion generation as in Ho et al. [17] during both pretraining and training. Thus, the unconditional probability of generating $x$ can also be estimated using the same model by masking $s$ and $z$ . ", "page_idx": 17}, {"type": "text", "text": "Inference Strategy. Leveraging the trained CausalDiff, we can infer its label from an adversarial example in accordance with Algorithm 3, following the inference pipeline outlined in Section 4.3. Specifically, we set $N_{\\mathrm{purify}}=5$ and use momentum-SGD as our $\\mathrm{\\optim_{purify}}$ , with a learning rate of ", "page_idx": 17}, {"type": "image", "img_path": "BZLdXBjB8O/tmp/5f53646bec581d27ea3fb0d279df4ac2e7bc7b89d0e8e78e16f5d1313967c169.jpg", "img_caption": ["Figure 7: Impact of $\\eta$ for disentanglement term in loss function on clean accuracy and robust accuracy. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "0.1. For causal factor inference, we choose $N_{t}=10$ to sample 10 timesteps per iteration and adopt $N_{\\mathrm{infer}}=10$ with momentum-SGD as $\\mathrm{optim}_{\\mathrm{infer}}$ , setting the learning rate to $\\bar{1}e^{-5}$ . ", "page_idx": 18}, {"type": "text", "text": "Regarding white-box attacks, we perform a gradient backpropagation throughout the entire pipeline of our CausalDiff approach, which includes purification, causal factors inference, and classification. This implies that the attacker possesses sufficient knowledge of the causal model. ", "page_idx": 18}, {"type": "text", "text": "Attack Evaluation. Attack evaluation for CIFAR-10 dataset includes a 100-step StAdv attack [60] with $\\epsilon\\:=\\:0.05$ under the $\\ell_{\\infty}$ norm bound, BPDA+EOT $\\scriptstyle{\\mathrm{EOT}}=20)$ ) against the $\\ell_{\\infty}$ threat model with $\\epsilon\\,=\\,8/255$ , and AutoAttack [2] (AA), which comprises 100-step white-box attacks such as APGD-ce, APGD-t, FAB-t, and a 5000-step black-box Square Attack under both $\\ell_{2}$ $\\epsilon=0.5)$ ) and $\\ell_{\\infty}$ $\\epsilon=8/255)$ constraints. ", "page_idx": 18}, {"type": "text", "text": "For the GTSRB dataset, we utilize four types of attacks as well as two types of corruptions to evaluate robustness against adversarial attacks and the influence of the natural environment. Specifically, the attack methods include AutoAttack, which comprises 100-step white-box attacks such as APGD-ce, APGD-t, FAB-t, and a 5000-step black-box Square Attack under both $\\ell_{2}$ $\\epsilon=0.5)$ ) and $\\ell_{\\infty}$ $\\epsilon=8/255)$ constraints. The corruptions include Fog and Brightness. Following Nie et al. [33], we randomly sample 512 samples for evaluation. ", "page_idx": 18}, {"type": "text", "text": "B.3 Details for Pilot Study ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Data Construction. For each data point of the 2000 samples, we sample a vector $s$ with dimension $h_{s}=8$ from a normal distribution with mean $-1$ and variance 1, i.e., $s\\sim\\mathcal{N}(-1,1)$ , and a vector $z\\sim\\mathcal{N}(1,1)$ with dimension $h_{z}\\,=\\,8$ . Subsequently, we projected the concatenation of $s$ and $z$ , denoted as $\\left[s;z\\right]$ , to a sample $x$ , with a random initialized matrix $A_{x}(A_{x}\\:\\in\\:\\mathcal{R}^{(h_{s}+h_{z})\\times h_{x}})$ , i.e., $x=[s;z]\\cdot A_{x}$ . Similarly, we produced the score $y_{s}$ of $x$ with $y_{s}=s\\cdot A_{y}$ , where $A_{y}\\in\\mathcal{R}^{h_{s}\\times1}$ . To obtain samples with balanced binary labels, we consider the label $y$ of the sample with $y_{s}$ above the median as 1 and the others as 0. ", "page_idx": 18}, {"type": "text", "text": "Methods for Comparisons. In the pilot study detailed in Section 3, we conducted an investigation and analysis on four models: 1) Discriminative: a discriminative model that learns to classify the samples with a two-layer perceptron (MLP), 2) Generative [27]: a generative model that learns the generation of the sample $x$ conditioning on its label $y$ and predict the label of an adversarial example by calculating the $\\operatorname*{max}_{y}p(x|y)$ , 3) Causal without Disent.: a causal model that models the generation of both $\\mathbf{X}$ and y with the same causal factor $v$ (Causal modeling without Disentanglement), 4) Causal with Disent.: our model that disentangles the label-causative factor $s$ and another factor $z$ during the generation of $x$ . For the latter two causal models, given an adversarial example, the hidden vectors $v$ or $s,z$ are inferred for Causal without Disent. and with Disent. which are then used for the final label prediction. This section comprehensively presents the designed causal structures in Fig. 5 and the model architectures in Fig. 6. ", "page_idx": 18}, {"type": "text", "text": "Regarding implementation, we trained each of the four models for 20 epochs, optimizing the model parameters using the Adam optimizer with a learning rate of $1e^{-3}$ . The latent dimension for each model was set to 64. For evaluation, we employed a 100-step PGD attack with $\\epsilon\\mathrm{~=~}0.3$ and $\\alpha\\,=\\,2/255$ within the $\\ell_{\\infty}$ norm boundary. The variation in latent variables and predicted logits between adversarial examples and clean images, as presented in Table 4, is measured on adversarial examples generated with $\\epsilon=10$ and $\\alpha=0.05$ within the $\\ell_{\\infty}$ norm boundary, using a 100-step PGD attack. ", "page_idx": 19}, {"type": "text", "text": "C More Experimental Results ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "C.1 Analyses on Core Components of Training ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Effect of $\\mathrm{I}_{\\mathrm{CLUB}}^{\\theta}(S;Z)$ in Casual Information Bottleneck (CIB) To examine the impact of our introduced disentanglement term $\\mathrm{I}_{\\mathrm{CLUB}}^{\\theta}(S;Z)$ in CIB (See Equation (6)), we vary $\\eta$ in Equation (7) and evaluate the robustness against the most challenging attack in AutoAttack [2], i.e., with $\\ell_{\\infty}$ norm bounded by $\\epsilon=8/255$ . Larger $\\eta$ will cause the diffusion model to collapse, so we do not include the results of larger $\\eta$ . As we mentioned in Section 4.2, our CIB regresses to the ELBO objective in [14] when $\\eta=0$ . As shown in Fig. 7, CausalDiff has better clean accuracy as well as robustness when $\\eta>0$ and yields the best robustness when $\\eta=10^{-5}$ $69.14\\%$ compared to $65.04\\%$ when $\\eta=0$ ). It confirms that $\\mathrm{I}_{\\mathrm{CLUB}}^{\\theta}(S;Z)$ in the loss function is beneficial to disentangle the Y-causative from Y-non-causative factors and can further enhance both clean accuracy and robustness. ", "page_idx": 19}, {"type": "text", "text": "C.2 Analyses on Core Components of Inference ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Causal Factor Inference Method. We also evaluate the robustness using the encoder $f_{(s,z)}(x;\\theta)$ to get latent variables instead of inferring $s$ and $z$ through the conditional diffusion model. The results reveal that classification by the encoder (without purification) achieves a clean accuracy of $91.99\\%$ but $0.00\\%$ against AutoAttack under both $\\ell_{\\infty}$ and $\\ell_{2}$ threat models. This decline might be attributed to imprecise modeling around $x$ (i.e., $x+\\delta\\!\\!\\!\\mid$ ), which results in an inability to resist adversarial perturbations on $x$ . ", "page_idx": 19}, {"type": "text", "text": "Timestep $t$ Sampling Strategies for Purification As discussed in Section 5.2, our purification method (CausalDiffw/o Causal Factor Inference in Table 2) markedly surpasses the direct adaptation of likelihood maximization (LM-DDPM in Table 2), as proposed by Chen et al. [27], applied to DDPM. This improvement stems from a refined strategy in sampling the timestep $t$ . ", "page_idx": 19}, {"type": "text", "text": "As demonstrated in Fig. 9, we found that smaller timesteps perform better in distinguishing between the distributions of clean and adversarial samples. Specifically, we presented the negative loglikelihood estimated by expectation $\\mathbb{E}_{\\epsilon}[w_{t}\\|\\epsilon_{\\theta}(x_{t},t)-\\bar{\\epsilon}\\|]$ for the given timestep over 512 examples. This may caused by a larger timestep implies a greater degree of noise addition for estimating likelihood, which might overshadow the adversarial perturbations, unexpected for purification. ", "page_idx": 19}, {"type": "text", "text": "C.3 Visualization of Cases ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We visualize the generated images leveraging the conditional generation of our CausalDiff, providing an intuitive depiction of the label-causative factor $s^{*}$ and the label-non-causative factor $z^{*}$ . Fig. 8 illustrates that after inferring from the benign example $x^{*}$ , perturbations are alleviated. The image $x_{s^{*}}$ conditioned on $s^{*}$ showcases that $s^{*}$ captures the core predictive features, reflecting the general concept of the category (for instance, a common semantic representation of \u2019horse\u2019 from an image of a brown horse\u2019s head), or even enhances the predictive features such as the dog\u2019s face or the ship\u2019s body, whereas $z^{*}$ retains specific and non-predictive image details. ", "page_idx": 19}, {"type": "text", "text": "We also visualize the purified example $x^{*}$ , conditionally generated $\\boldsymbol{x}_{s^{\\ast}}^{\\ast}$ and $x_{z^{\\ast}}^{\\ast}$ when encountering an adversarial example $\\tilde{x}$ . These cases demonstrate that, despite the presence of perturbations in the clean images, our CausalDiff effectively captures the correct predictive information of $s^{*}$ , maintaining alignment with the clean data. ", "page_idx": 19}, {"type": "image", "img_path": "BZLdXBjB8O/tmp/534792d6af4ece5e0c832d5c3ca896b7c9242c78cfde4918c28905c52b08db0f.jpg", "img_caption": ["Figure 8: Reconstruction images $x_{\\left[s_{c}^{*};z_{c}^{*}\\right]}$ when given clean example $x$ , where $s_{c}^{*}$ and $z_{c}^{*}$ are inferred from the clean example $x$ by our CausalDiff; generated images $x_{s_{c}^{*}}$ and $x_{z_{c}^{*}}$ conditioned on $s_{c}^{*}$ and $z_{c}^{*}$ , respectively; purified image $x^{*}$ utilizing the unconditional diffusion (with $s,z$ masked) when given an adversarial example $\\tilde{x}$ ; generated images $x_{s^{*}}$ and $x_{z^{*}}$ conditioned on $s^{*}$ and $z^{*}$ , respectively, where $s^{*}$ and $z^{*}$ are inferred from the purified image $x^{*}$ . "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "BZLdXBjB8O/tmp/2d86dfd30d1ef9da70565caca01dcda23bf6ee0b48339c508efc7069858f9282.jpg", "img_caption": ["Figure 9: Distribution of likelihood for adversarial and benign examples across various timesteps $t$ . "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "C.4 Speed Test of Inference Time ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We evaluate the computational complexity of CausalDiff and DiffPure [33] as well as a discriminative model (WRN-70-16) by measuring the inference time in seconds for a single sample (average on 100 examples from CIFAR-10 dataset) on two types of GPUs, including NVIDIA A6000 GPU and 4090 GPU (Our experiments leverage 4 A6000 GPUs and 4 4090 GPUs). The results are shown in Table 5. ", "page_idx": 20}, {"type": "table", "img_path": "BZLdXBjB8O/tmp/3aa90cb65fc1c5f61115ac75b67dedc3f62313ba8569a820e6d7283907123fbf.jpg", "table_caption": ["Table 5: Comparison of NFEs (Number of Function Evaluations) across different models on GPUs "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "D Limitation ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Although our CausalDiff significantly narrows the gap in classification accuracy between adversarial and clean examples, it requires an inference cost of $1+N_{1}+N_{2}$ NFEs (Number of Function Evaluations), where efficiency improvements are needed. Note that $N_{1}$ indicates the purification step (e.g. 5) and $N_{2}$ indicates the step of causal factor inference (e.g., 10) and 1 NFE for latent-S-based classification. Furthermore, our CausalDiff represents a new framework, meaning it requires training from scratch. Perhaps in the future, an efficient implementation of robust inference could be achieved by embedding causal mechanisms into the existing models in a plug-and-play manner. ", "page_idx": 21}, {"type": "text", "text": "E Broader Impact ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Our CausalDiff model, built upon a powerful generative framework, aims to align with human decision-making mechanisms to enhance the stability and trustworthiness of neural networks. This approach holds potential for advancing the field of Machine Learning, particularly in safety-sensitive applications such as autonomous driving and facial recognition. ", "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 22}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 22}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 22}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 22}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 22}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 22}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 22}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: see abstract ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Appendix D ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 23}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: Section 4.2 and Appendix A ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Section 5, Appendix C and Appendix B ", "page_idx": 23}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 23}, {"type": "text", "text": "\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 24}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: We described our experimental details in Appendix B and we will open source our code in camera-ready version. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 24}, {"type": "text", "text": "\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 25}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Appendix B Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 25}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [No] ", "page_idx": 25}, {"type": "text", "text": "Justification: Adversarial attack evaluation incurs high computational costs. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Appendix C.4 ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. ", "page_idx": 25}, {"type": "text", "text": "\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 26}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: Appendix E Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 26}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: Appendix E Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 26}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA]   \nJustification: Appendix E   \nGuidelines: \u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Appendix 4 Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: NA ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 27}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] Justification: NA ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 28}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] Justification: NA Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 28}]