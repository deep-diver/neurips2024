[{"figure_path": "qInb7EUmxz/figures/figures_1_1.jpg", "caption": "Figure 1: The performance interference of extra unlabeled data distillation. The four approaches are (a) iCaRL, (b) PodNet, (c) R-DFCIL and (d) our persistence homology distillation methods. Avg and Avg_old mean the average incremental accuracy of all tasks and old tasks, respectively.", "description": "This figure shows the impact of using extra unlabeled data for distillation in semi-supervised continual learning.  Four different methods are compared: iCaRL, PodNet, R-DFCIL, and the authors' proposed Persistence Homology Distillation (PsHD).  The performance is measured by the average incremental accuracy across all tasks (Avg) and the average accuracy on previously learned tasks (Avg_old). The results highlight how PsHD mitigates the negative impact of noisy or inaccurate information in unlabeled data, unlike other methods.", "section": "1 Introduction"}, {"figure_path": "qInb7EUmxz/figures/figures_3_1.jpg", "caption": "Figure 2: Illustration of topological data analysis. (a) Filtration of simplicial complex, (b) corresponding persistence barcode, and (c) persistence diagram.", "description": "This figure illustrates the process of topological data analysis using persistence homology. (a) shows a filtration of a simplicial complex, where points are gradually connected to form higher-dimensional simplices as a threshold parameter increases. (b) displays the persistence barcode, which represents the lifespan of topological features (connected components, loops, voids, etc.) across different scales. Each bar corresponds to a topological feature, with its length indicating its persistence. Features with longer lifespans are considered more significant. (c) shows the persistence diagram, which is a scatter plot of the birth and death times of topological features. Points close to the diagonal represent short-lived features (noise), while points far from the diagonal signify persistent features (structural information).", "section": "3.1 Preliminaries on Persistence Homology"}, {"figure_path": "qInb7EUmxz/figures/figures_4_1.jpg", "caption": "Figure 3: Illustration of our proposed persistence homology distillation for semi-supervised continual learning. (a) represents the backbone of SSCL, LCE and LSSL are the cross-entropy loss on labeled data and semi-supervised loss on unlabeled data. LCL means the continual learning loss on the memory buffer. (b) corresponds to our PsHD loss, serving as LCL, employed on the replied samples.", "description": "This figure illustrates the proposed persistence homology distillation (PsHD) method for semi-supervised continual learning.  Panel (a) shows the overall architecture, highlighting the cross-entropy loss (LCE) for labeled data, the semi-supervised loss (LSSL) for unlabeled data, and the continual learning loss (LCL) based on the memory buffer. Panel (b) focuses on the PsHD loss (LCL), which uses persistence homology to capture the topological structure of both new and old features, creating a more robust and stable learning process that is less sensitive to noise.", "section": "3 Methodology"}, {"figure_path": "qInb7EUmxz/figures/figures_8_1.jpg", "caption": "Figure 4: Visualization of activation heatmap during the continual learning process, where the categories belong to Task0. 1-3(a) correspondence to PsHD without the Lhd, and 1-3(b) correspondence to PsHD with Lhd. The red area localizes class-specific discriminative regions.", "description": "This figure visualizes the activation heatmaps of old categories during continual learning with and without persistence homology distillation.  It shows how the model's attention to relevant image regions changes across tasks. The (a) columns represent results without the proposed distillation method (PsHD), while (b) columns showcase the results with PsHD.  The color intensity indicates the level of activation; redder areas highlight regions of higher activation, indicating stronger attention. The figure demonstrates the positive impact of the PsHD method on preserving the focus on relevant features across tasks, preventing catastrophic forgetting.", "section": "4.3 Parameter Analysis"}, {"figure_path": "qInb7EUmxz/figures/figures_9_1.jpg", "caption": "Figure 5: Effectiveness of h-simplex features in persistent homology. H0_Avg and H01_Avg represent the average incremental accuracy based on considering 0-dimensional holes and 0,1-dimensional holes persistence. BWT evaluates the forgetting degree.", "description": "This figure visualizes the impact of the hyperparameter \u03bb (lambda) and the choice of considering 0-dimensional holes (H0) or both 0 and 1-dimensional holes (H01) on the performance of the proposed persistence homology distillation method across three datasets: CIFAR-10, CIFAR-100, and ImageNet-100.  For each dataset and setting, it shows the average incremental accuracy (Avg\u2191) and backward transfer (BWT\u2193) as a function of \u03bb.  The goal is to demonstrate the model's robustness to noise and the effectiveness of different topological feature representations.", "section": "4.3 Parameter Analysis"}, {"figure_path": "qInb7EUmxz/figures/figures_17_1.jpg", "caption": "Figure 4: Visualization of activation heatmap during the continual learning process, where the categories belong to Task0. 1-3(a) correspondence to PsHD without the Lhd, and 1-3(b) correspondence to PsHD with Lhd. The red area localizes class-specific discriminative regions.", "description": "This figure visualizes the activation heatmaps of old categories during continual learning with and without the proposed persistence homology distillation (PsHD).  The top row shows the results without PsHD, while the bottom row uses PsHD. Each column represents a task in the continual learning process.  The heatmaps highlight the regions of the image that are most important for classification, allowing for a visual comparison of how effectively each method maintains focus on relevant features across tasks.  The redder areas indicate stronger activations.", "section": "4.3 Parameter Analysis"}]