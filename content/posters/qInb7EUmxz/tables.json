[{"figure_path": "qInb7EUmxz/tables/tables_6_1.jpg", "caption": "Table 1: Average incremental accuracy and average accuracy of various methods on 5-tasks CIFAR-10, 10-tasks CIFAR-100 and 10-tasks ImageNet-100 settings following the learning sequence [14], with a memory buffer size of 2000. The improvements of PsHD compared to the state-of-the-art methods are highlighted in blue color.", "description": "This table presents the average incremental accuracy and average accuracy for different continual learning methods (iCaRL, iCaRL with Fixmatch, DSGD, DSGD with Fixmatch, DER, DER with Fixmatch and PsHD) across three benchmark datasets (CIFAR-10, CIFAR-100, and ImageNet-100) under different numbers of labeled samples.  The results are shown for different percentages of labeled data and highlight the improvements achieved by the proposed PsHD method compared to the state-of-the-art.  A memory buffer size of 2000 samples is used.", "section": "4.2 Quantitative Results"}, {"figure_path": "qInb7EUmxz/tables/tables_6_2.jpg", "caption": "Table 2: Average accuracy with the standard derivation of different methods test with 5-tasks CIFAR-10 and 10-tasks CIFAR-100 settings following learning sequence of [13] with 500 samples replayed. The data with underline is the best performance within existing methods.", "description": "This table presents the average accuracy and standard deviation of various methods on CIFAR-10 and CIFAR-100 datasets under different supervision levels (0.8%, 5%, 25%). The learning sequence follows the NNCSL method [13], and the memory buffer size is set to 500.  The best-performing method in each setting is underlined.", "section": "4.1 Experiment Settings"}, {"figure_path": "qInb7EUmxz/tables/tables_7_1.jpg", "caption": "Table 3: Average accuracy of different knowledge distillation methods applied on SSCL.", "description": "This table compares the average accuracy of various knowledge distillation methods (logits, feature, relation, and topology distillation) on two datasets (CIFAR-10 and CIFAR-100) with different labeling ratios (5% and 25%). The results demonstrate the effectiveness of different distillation methods in semi-supervised continual learning and highlight the performance of PsHD (Persistence Homology Distillation), a novel topology-based method, which achieves the highest accuracy across all settings. This showcases the benefit of leveraging intrinsic topological features for robust knowledge preservation in continual learning.", "section": "4.2 Quantitative Results"}, {"figure_path": "qInb7EUmxz/tables/tables_7_2.jpg", "caption": "Table 4: Comparison of distillation methods with Gaussian noise interference on CIFAR10 with 5% supervision. \u03c3 is the standard deviation.", "description": "This table presents a comparison of the stability of different distillation methods (Podnet, LUCIR, R-DFCIL, DSGD, TopKD, and PsHD) under different levels of Gaussian noise (\u03c3 = 0.2, 1.0, 1.2).  For each method and noise level, the table shows the Backward Transfer (BWT) and Average Accuracy (AA\u2191).  BWT indicates the degree of forgetting (a negative value means less forgetting). AA\u2191 is the average accuracy. The results demonstrate the relative robustness of each method to noise interference.", "section": "4.3 Stability to Noise Interference"}, {"figure_path": "qInb7EUmxz/tables/tables_8_1.jpg", "caption": "Table 5: Ablation study of proposed persistence homology distillation", "description": "This table presents the ablation study of the proposed persistence homology distillation method. It shows the average and last accuracy on CIFAR-10 and CIFAR-100 datasets with 5% and 25% label ratios, respectively. The results are compared for different configurations: using only semi-supervised loss (LSSL), using only persistence homology distillation loss (Lhd), and using both losses.  The improvements achieved by adding the proposed Lhd loss are highlighted, demonstrating its effectiveness in enhancing the overall performance.", "section": "4.3 Parameter Analysis"}, {"figure_path": "qInb7EUmxz/tables/tables_9_1.jpg", "caption": "Table 6: Effect of data allocation in memory buffer", "description": "This table shows the average accuracy of different methods with varying percentages of labeled data in the memory buffer.  The results demonstrate the impact of the labeled data ratio on model performance in semi-supervised continual learning.  It shows how the proportion of labeled vs. unlabeled data in the memory buffer influences the overall accuracy of the model.", "section": "4.3 Parameter Analysis"}, {"figure_path": "qInb7EUmxz/tables/tables_9_2.jpg", "caption": "Table 7: Average accuracy of different methods following learning sequence of [13] with memory buffer size 5120. * represents that the size of memory buffer is 2000.", "description": "This table presents the average accuracy results of different continual learning methods on CIFAR-10 and CIFAR-100 datasets under various label ratios (0.8%, 5%, and 25%).  The methods compared include PseudoER, CCIC, CSL, NNCSL, and the proposed PsHD. The table also shows the results when the memory buffer size is reduced to 2000 (PsHD*).  The results demonstrate the performance improvements of PsHD, especially its memory efficiency when the buffer size is reduced.  The learning sequence follows [13].", "section": "4.4 Efficiency Analysis"}, {"figure_path": "qInb7EUmxz/tables/tables_14_1.jpg", "caption": "Table 1: Average incremental accuracy and average accuracy of various methods on 5-tasks CIFAR-10, 10-tasks CIFAR-100 and 10-tasks ImageNet-100 settings following the learning sequence [14], with a memory buffer size of 2000. The improvements of PsHD compared to the state-of-the-art methods are highlighted in blue color.", "description": "This table presents a comparison of the average incremental accuracy and average accuracy achieved by different continual learning methods on three benchmark datasets: CIFAR-10, CIFAR-100, and ImageNet-100.  The results are presented for different label ratios (5%, 25% for CIFAR-10/100, and 1%, 7.7% for ImageNet-100) and reflect performance after 5, 10 and 10 tasks, respectively.  The table highlights the improvement obtained by the proposed PsHD method compared to the state-of-the-art.", "section": "4.2 Quantitative Results"}, {"figure_path": "qInb7EUmxz/tables/tables_16_1.jpg", "caption": "Table 7: Average accuracy of different methods following learning sequence of [13] with memory buffer size 5120. * represents that the size of memory buffer is 2000.", "description": "This table compares the average accuracy of different methods for semi-supervised continual learning using a memory buffer of size 5120.  It shows the average accuracy across six different experimental settings (different datasets and labeling ratios).  The table also includes results with a smaller memory buffer (2000), demonstrating the memory efficiency of PsHD.  The methods compared include several state-of-the-art methods, such as NNCSL, DER_Fix, and DSGD, along with the proposed PsHD method.", "section": "4.4 Efficiency Analysis"}]