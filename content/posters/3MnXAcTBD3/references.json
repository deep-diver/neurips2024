{"references": [{"fullname_first_author": "M. Assran", "paper_title": "Stochastic gradient push for distributed deep learning", "publication_date": "2019-00-00", "reason": "This paper introduces a fundamental algorithm for distributed deep learning, which is highly relevant to the core methodology of the current research."}, {"fullname_first_author": "A. Koloskova", "paper_title": "A unified theory of decentralized SGD with changing topology and local updates", "publication_date": "2020-00-00", "reason": "This paper provides a comprehensive theoretical framework for decentralized stochastic gradient descent, which is essential for understanding the convergence behavior of related algorithms."}, {"fullname_first_author": "S. Pu", "paper_title": "Distributed stochastic gradient tracking methods", "publication_date": "2021-00-00", "reason": "This paper proposes and analyzes gradient tracking methods, which serve as a foundation for many advanced distributed optimization algorithms, including the one discussed in this paper."}, {"fullname_first_author": "W. Shi", "paper_title": "Extra: An exact first-order algorithm for decentralized consensus optimization", "publication_date": "2015-00-00", "reason": "This paper presents a foundational algorithm for decentralized consensus optimization, providing a crucial baseline for performance comparison."}, {"fullname_first_author": "S. Pu", "paper_title": "Push-pull gradient methods for distributed optimization in networks", "publication_date": "2020-00-00", "reason": "This paper introduces the push-pull method, which forms the basis for the B-ary tree push-pull method proposed in this research."}]}