{"references": [{"fullname_first_author": "Martin Abadi", "paper_title": "Deep learning with differential privacy", "publication_date": "2016-00-00", "reason": "This paper is foundational for the field of differentially private deep learning, introducing the core concepts and techniques used in the current work."}, {"fullname_first_author": "Xuechen Li", "paper_title": "Large language models can be strong differentially private learners", "publication_date": "2021-00-00", "reason": "This paper demonstrates that large language models can be trained with differential privacy, providing a key result that the current work builds on."}, {"fullname_first_author": "Mathilde Caron", "paper_title": "Emerging properties in self-supervised vision transformers", "publication_date": "2021-00-00", "reason": "This paper introduces a self-supervised learning method for vision transformers, which is utilized as a key component of the proposed DP continual pre-training strategy."}, {"fullname_first_author": "Ilya Mironov", "paper_title": "Renyi differential privacy", "publication_date": "2017-00-00", "reason": "This paper introduces the Renyi differential privacy framework which is used as the privacy accounting mechanism in this paper's experimental setup."}, {"fullname_first_author": "Alexey Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2020-00-00", "reason": "This paper introduces Vision Transformers (ViT), the core model architecture used for the experiments in this paper, establishing its relevance and impact in the field."}]}