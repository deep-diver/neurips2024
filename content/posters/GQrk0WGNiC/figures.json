[{"figure_path": "GQrk0WGNiC/figures/figures_1_1.jpg", "caption": "Figure 1: Comparison among the convergence of standard SGD, clipped SGD without noise, noisy SGD without clipping, and DP-SGD in different tasks and training stages.", "description": "This figure compares the convergence behavior of different optimization methods across various tasks and training stages.  It showcases standard SGD, SGD with gradient clipping, SGD with added noise, and DP-SGD (Differentially Private Stochastic Gradient Descent). The results are shown for both pre-training and fine-tuning phases in different tasks. It illustrates how these methods affect training loss and accuracy over training steps or epochs, providing insights into their efficiency and the impact of differential privacy mechanisms.", "section": "2 Understanding DP training through the lens of Hessian"}, {"figure_path": "GQrk0WGNiC/figures/figures_2_1.jpg", "caption": "Figure 2: Summary of results in Section 5. First three figures compare the downstream and few-shot performance and the data efficiency (circle's radius proportional to pre-training data size) of the DP pre-trained models; the last figure shows the performance of DP pre-trained models defending against privacy attacks (lower is stronger in defense).", "description": "This figure summarizes the results presented in Section 5 of the paper.  It consists of four subfigures. The first three subfigures compare the performance of differentially private (DP) pre-trained models against other state-of-the-art models on various downstream tasks (ImageNet-11k, CIFAR-10, Places365, iNat2021, and Aircraft). The size of the circles in these subfigures represents the amount of data used for pre-training, illustrating data efficiency. The fourth subfigure shows the performance of DP models in resisting membership inference attacks (MIA), a common privacy attack, with lower MIA scores indicating stronger privacy protection.", "section": "5 DP vision foundation models on ImageNet"}, {"figure_path": "GQrk0WGNiC/figures/figures_2_2.jpg", "caption": "Figure 3: Noise levels by privacy accountants.", "description": "This figure shows the relationship between batch size (B) and the noise level (\u03c3(B)\u00b2/B) for three different privacy accounting methods: RDP, GDP, and PRV.  The dashed line represents the theoretical O(1/B) relationship. The graph illustrates how the noise level decreases as the batch size increases, and it shows that the different privacy accounting methods have different noise levels at various batch sizes. This information is crucial for understanding how to set the privacy parameters in differentially private training.", "section": "1.5 Privacy budget"}, {"figure_path": "GQrk0WGNiC/figures/figures_2_3.jpg", "caption": "Figure 4: Per-sample gradient clipping in (3).", "description": "This figure illustrates the different clipping functions used in the paper.  The x-axis represents the per-sample gradient norm, and the y-axis represents the clipping factor (Ci).  The figure shows the behavior of five different clipping functions: AUTO-V, AUTO-S, and three versions of re-parameterized clipping with different sensitivity bounds (R=0.1, R=0.2, R=1).  The figure is important because it visually demonstrates how the different clipping functions affect the magnitude of the gradients before they are used in the differentially private optimization process. Different clipping methods impact the convergence of the algorithm in various ways. This figure is crucial for understanding and comparing the different clipping techniques used within the context of differentially private optimization.", "section": "2.1 Per-iteration improvement of DP-SGD"}, {"figure_path": "GQrk0WGNiC/figures/figures_4_1.jpg", "caption": "Figure 5: Illustration of different terms in (6) and (7). Left sub-plots depict the denominators in (6) and (7). Right sub-plots depict the whole terms and optimal batch sizes.", "description": "This figure visualizes the different terms of equations (6) and (7) from the paper, which describe the per-sample loss improvement for DP-SGD and standard SGD, respectively.  The left subplots show the denominators of these equations, illustrating how they vary with the batch size (B). The right subplots show the complete per-sample loss improvement calculations, including the optimal batch size (Bop) for each method.  The figure helps to visually understand the impact of different terms in the equations on the overall loss improvement, particularly the effect of DP noise and batch size on the rate of convergence.", "section": "2 Understanding DP training through the lens of Hessian"}, {"figure_path": "GQrk0WGNiC/figures/figures_6_1.jpg", "caption": "Figure 5: Illustration of different terms in (6) and (7). Left sub-plots depict the denominators in (6) and (7). Right sub-plots depict the whole terms and optimal batch sizes.", "description": "This figure illustrates the different terms of equations (6) and (7) which represent the per-sample loss improvement for DP-SGD and standard SGD. The left subplots show the denominators of the equations, highlighting the impact of different components like Hessian, covariance and noise. The right subplots show the overall per-sample loss improvement with varying batch size, indicating the existence of an optimal batch size that balances noise and convergence speed in DP training.", "section": "2 Understanding DP training through the lens of Hessian"}, {"figure_path": "GQrk0WGNiC/figures/figures_7_1.jpg", "caption": "Figure 7: Convergence of GPT2-small on CodeParrot with different pre-training strategies (\u20ac = 8).", "description": "This figure shows the test loss curves for GPT2-small trained on CodeParrot dataset using different pre-training strategies with epsilon=8. The strategies compared are:\n\n*   **Fully Public:** Standard training without any differential privacy (DP).\n*   **Fully Private:** Training with DP applied to the entire dataset.\n*   **Only Public:** Training only on public data without DP.\n*   **Mixed (PubRatio=1%):** Training with DP, using 1% public data.\n*   **Mixed (PubRatio=10%):** Training with DP, using 10% public data.\n\nThe plot also indicates the 'SwitchPoint' - the point at which the model switches from public to private training. The results demonstrate the effectiveness of the proposed DP continual pre-training strategy.", "section": "4 Continual pre-training with DP"}, {"figure_path": "GQrk0WGNiC/figures/figures_22_1.jpg", "caption": "Figure 8: Ablation study of switching from non-DP to DP training with AdamW on CIFAR100 dataset. When switching (T = 1200), we re-initialize different states in the AdamW optimizer in different linestyles. \u201cR1\u201d, \u201cR2\u201d, and \u201cRS\u201d indicate m, v and t are re-initialized, respectively. \u201cN\u201d indicates no re-initialization, and \u201cRef\u201d is the reference behavior of continual training with non-DP AdamW.", "description": "This figure shows the ablation study result of different strategies of re-initializing the AdamW optimizer states during the training stage switching. The experiment uses ViT-Base model trained from scratch on CIFAR100 dataset. During the first three epochs, it trains with vanilla AdamW, then switches to DP-AdamW and continues for another one epoch. During switching, the learning rate is fixed, and different states are reset to zeros. The graph plots the training accuracy against the steps. It indicates that re-initializing the first-order momentum m (R1) is the best strategy to achieve high performance when switching from public to private training.", "section": "B.3 Details in training stage switching"}, {"figure_path": "GQrk0WGNiC/figures/figures_23_1.jpg", "caption": "Figure 9: The process of membership inference attack (MIA).", "description": "The figure illustrates the process of a membership inference attack (MIA).  ImageNet train and test data are fed into a ViT model.  The model's outputs (logits and loss) for both training and test data are then used to create an MIA train and test dataset. This dataset consists of 10% of the training data and 50% of the test data labeled as \"1\" (member), and 2.4% of the training data and 50% of the test data labeled as \"0\" (non-member).  A classification model is trained on this MIA dataset to determine whether an image is from the original ImageNet training set or not. The accuracy of this classification model serves as a measure of the privacy protection afforded by the system.", "section": "B.4 Details for MIA"}]