[{"figure_path": "fs28jccJj5/figures/figures_1_1.jpg", "caption": "Figure 1: (a) Accuracy, energy consumption (refer to Appendix A.1), and parameter size of the\npropsoed SpikedAttention and other spike-based SNNs on ImageNet classification (15). (b) The\nstructure of the proposed fully spike-based attention module (more details are in Section 4).", "description": "Figure 1(a) compares the performance of SpikedAttention against other spike-based SNNs on ImageNet classification in terms of accuracy, energy consumption, and parameter size.  It shows that SpikedAttention achieves state-of-the-art accuracy with significantly lower energy consumption. Figure 1(b) illustrates the architecture of the fully spike-based attention module proposed in the paper, which is a key component of SpikedAttention.", "section": "1 Introduction"}, {"figure_path": "fs28jccJj5/figures/figures_2_1.jpg", "caption": "Figure 2: The computation of attention in previous SNN-based transformers (a) with the softmax (14), and (b) without the softmax operation (12). Both involve non-spike computations. Total timestep T of each spike tensor is set to 1 for simplicity.", "description": "This figure illustrates the computation of attention in previous SNN-based transformers. It highlights two approaches: one with softmax and one without. Both approaches use non-spike computations during the attention mechanism. The figure simplifies the representation by using a timestep T of 1 for the spike tensor. The red box indicates non-spike computation parts.", "section": "2 Related Work"}, {"figure_path": "fs28jccJj5/figures/figures_4_1.jpg", "caption": "Figure 3: The proposed trace-driven dot product in SpikedAttention. A global trace decays by its base (B) at each timestep and the trace is transferred to each neuron's local memory when its first spike, from either SQ or SK, is observed. The V(QKT)(1,1) is the potential of the neuron (QKT)1,1.", "description": "This figure illustrates the trace-driven matrix multiplication method proposed in the paper.  It shows how a global trace, decaying with each timestep by a factor of B (the base of the phase coding), is used to accumulate the values associated with each spike. This method is designed to efficiently perform spike-based matrix multiplication, reducing the need for long timesteps. The global trace is transferred to each neuron's local memory when its first spike is encountered. This results in an efficient computation of dot products using only spikes and local memory operations. ", "section": "4 Fully Spike-based Attention: SpikedAttention"}, {"figure_path": "fs28jccJj5/figures/figures_5_1.jpg", "caption": "Figure 4: The proposed winner-oriented spike shift (WOSS) for approximating softmax. (a) Each WOSS neuron increases its potential (Vwoss\u2081) by the incoming input spike Sz(i)(t). The first (winner) spike among S\u2082(t) activates a (global) inhibitory neuron which depresses all neurons. (b) Potential at neuron i (winner) and its corresponding output spike at t = T + 0. We are shifting the first spike time from t = 1 to t = 0. The following spikes are time-shifted by the same amount. (c) Potential at neuron j (non-winner) and its corresponding output spike at t = T + 2 (effectively, t = 2) due to the global inhibition and the threshold shift.", "description": "This figure illustrates the Winner-Oriented Spike Shift (WOSS) mechanism used in SpikedAttention to approximate the softmax function.  It shows how the potential of neurons is updated based on incoming spikes and a global inhibitory signal, resulting in a spike-based approximation of softmax without using exponential functions. The winner neuron fires early, and others fire later, reflecting the softmax probability distribution.", "section": "4 Fully Spike-based Attention: SpikedAttention"}, {"figure_path": "fs28jccJj5/figures/figures_7_1.jpg", "caption": "Figure 5: Scatter plots showing the correlation between the actual activation values in Swin Transformer (x-axis) and the decoded spike values in converted SpikedAttention (y-axis). The parameters are set to T = 40, B = 1.15.", "description": "This figure shows scatter plots illustrating the correlation between the real activation values from the original Swin Transformer model and the corresponding decoded spike values after conversion to SpikedAttention. The plots are separated by different stages of the SpikedAttention model, including trace-driven QK' computations (both first and last blocks), WOSS-Softmax operations (first and last blocks), and the final classification layer.  The parameters used for the conversion are T=40 and B=1.15.  The strong positive correlations demonstrate that the spike-based representation accurately captures the information present in the original model's activations.", "section": "5 Experimental Results"}, {"figure_path": "fs28jccJj5/figures/figures_15_1.jpg", "caption": "Figure 6: Comparison of attention maps based on Score-CAM (44) between Swin-T (baseline ANN) and SpikedAttention (Ours; B = 1.16 and T = 40) on ImageNet-1K at the output of various attention blocks.", "description": "This figure compares attention maps generated by a baseline ANN (Swin-T) and the proposed SpikedAttention SNN model on ImageNet.  Score-CAM was used to visualize the attention maps from four different attention blocks (Attn #9 - #12) within the networks. The goal is to show that SpikedAttention successfully converts the attention mechanism into a spike-based representation while maintaining similar attention patterns to the original ANN.", "section": "5 Experimental Results"}, {"figure_path": "fs28jccJj5/figures/figures_15_2.jpg", "caption": "Figure 7: (a) Normalized logarithmic error (Eq(x)) with respect to the normalized activation (x/max(X)) at a given timestep T = 4. (b) Logarithmic error (Eq(x)) and the accuracy on the ImageNet classification task at various base B values at a given timestep T = 40.", "description": "This figure shows the relationship between the base B used in the one-spike phase coding scheme and both the approximation error and accuracy on the ImageNet dataset. The logarithmic error measures how well the spike-based approximation matches the actual activation values.  The left subplot (a) shows this error at timestep T=4, while the right subplot (b) shows the error and resulting accuracy at timestep T=40, demonstrating the optimal base B to minimize error and maximize accuracy.", "section": "E Selecting the Optimal Base B for a Given Timestep T"}, {"figure_path": "fs28jccJj5/figures/figures_16_1.jpg", "caption": "Figure 8: Accuracy loss and the energy consumption at various timesteps when converting Swin-T to an SNN for ImageNet classification. (a) SpikedAttention from Swin-T without any modification, and (b) SpikedAttention from Swin-T with inserted ReLUs.", "description": "This figure shows the trade-off between accuracy and energy consumption when varying the timestep (T) in the conversion of Swin Transformer to SpikedAttention on the ImageNet dataset. Two scenarios are compared: one without ReLU activation and another with ReLU.  As the timestep decreases, accuracy loss increases, but energy consumption decreases significantly.  The results indicate that a balance must be struck between accuracy and energy efficiency in choosing a suitable timestep.  Using ReLU seems to increase the energy consumption.", "section": "G Impact of Total Timestep on Accuracy and Energy Consumption"}, {"figure_path": "fs28jccJj5/figures/figures_17_1.jpg", "caption": "Figure 9: Energy consumption of MA-BERT and SpikedAttention, and accuracy loss of BERT-to-SNN conversion on the GLUE SST2 dataset according to the maximum input length (i.e., 64, 128, 192 and 256).", "description": "This figure shows the energy consumption and accuracy loss comparison between MA-BERT (ANN) and SpikedAttention (SNN) for different input lengths on the GLUE SST2 dataset.  As the maximum input length increases, both models show increased energy consumption. However, SpikedAttention demonstrates significantly lower energy consumption with a small accuracy loss across all tested input lengths, highlighting its efficiency.", "section": "H Energy Consumption at Various Input Token Lengths"}]