[{"type": "text", "text": "Group and Shuffle: Efficient Structured Orthogonal Parametrization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Mikhail Gorbunov Nikolay Yudin Vera Soboleva Aibek Alanov HSE University HSE University AIRI, AIRI, gorbunovmikh73@gmail.com HSE University HSE University ", "page_idx": 0}, {"type": "text", "text": "Alexey Naumov Maxim Rakhuba HSE University, HSE University Steklov Mathematical Institute of Russian Academy of Sciences ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The increasing size of neural networks has led to a growing demand for methods of efficient fine-tuning. Recently, an orthogonal fine-tuning paradigm was introduced that uses orthogonal matrices for adapting the weights of a pretrained model. In this paper, we introduce a new class of structured matrices, which unifies and generalizes structured classes from previous works. We examine properties of this class and build a structured orthogonal parametrization upon it. We then use this parametrization to modify the orthogonal fine-tuning framework, improving parameter and computational efficiency. We empirically validate our method on different domains, including adapting of text-to-image diffusion models and downstream task fine-tuning in language modeling. Additionally, we adapt our construction for orthogonal convolutions and conduct experiments with 1-Lipschitz neural networks. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Orthogonal transforms have proven useful in different deep learning tasks. For example, they were shown to stabilize CNNs [Li et al., 2019, Singla and Feizi, 2021] or used in RNNs to combat the problem of exploding/vanishing gradients [Arjovsky et al., 2016]. Recent works OFT (Orthogonal Fine-Tuning) and BOFT (Butterfly Orthogonal Fine-Tuning) [Qiu et al., 2023, Liu et al., 2024b] use learnable orthogonal matrices for parameter-efficient fine-tuning of neural networks, which prevents training instabilities and overftiting that alternative methods like LoRA [Hu et al., 2022] suffer from. ", "page_idx": 0}, {"type": "text", "text": "Nevertheless, parametrization of orthogonal matrices is a challenging task, and the existing methods typically lack in either computational efficiency or expressiveness. Classical methods like Cayley parametrization and matrix exponential map cannot operate under low parameter budget, while Givens rotations and Householder reflections requires computing products of several matrices, which makes their use less efficient in deep learning tasks. Alternative approach in OFT method uses blockdiagonal matrix structure in an attempt to be more computationally efficient and use less trainable parameters. Unfortunately, this simple structure can be too restrictive. Thus, arises the problem of constructing dense orthogonal matrix while still being parameter-efficient. While attempting to tackle this task, BOFT method uses a variation of butterfly matrices, parametrizing orthogonal matrices as a product of several matrices with different sparsity patterns, enforcing orthogonality on each of them. This parametrization is able to construct dense matrices while still being parameter-efficient. However it requires to compute a product of multiple matrices (typically up to 6) which can be computationally expensive. In this paper, we aim to overcome these issues and build dense orthogonal matrices in a more efficient way. ", "page_idx": 0}, {"type": "text", "text": "We present a novel structured matrix class parametrized by an alternating product of block-diagonal matrices and several permutations. Multiplying by these matrices can be seen as a consecutive application of independent linear transforms within certain small groups and then shuffling the elements between them, hence the name Group-and-Shuffle matrices (or $g{s}$ -matrices for short). This class generalizes Monarch matrices [Dao et al., 2022] and with the right permutation choices, is able to form dense orthogonal matrices more effectively compared to approach proposed in BOFT, decreasing number of matrices in the product as well as the number of trainable parameters. We build efficient structured orthogonal parametrization with this class and use it to construct a new parameter-efficient fine-tuning method named GSOFT. ", "page_idx": 1}, {"type": "text", "text": "Our contributions: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We introduce a new class of structured matrices, called $\\mathit{g s}$ , that is more effective at forming dense matrices than block butterfly matrices from the BOFT method. \u2022 Using $\\mathit{g s}$ -matrices, we propose an efficient structured orthogonal parametrization, provide theoretical insights and study its performance in the orthogonal fine-tuning framework. \u2022 We adapt our ideas for convolutional architectures, providing a framework to compress and speed-up orthogonal convolution layers. ", "page_idx": 1}, {"type": "text", "text": "2 Orthogonal Fine-tuning ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Orthogonal Fine-tuning method (OFT) introduced in [Qiu et al., 2023] is a Parameter-Efficient Fine-Tuning (PEFT) method which fine-tunes pre-trained weight matrices through a learnable orthogonal block-diagonal matrix. Some of the properties that make orthogonal transforms desirable are preservation of pair-wise angles of neurons, spectral properties and hyperspherical energy. More precisely, OFT optimizes an orthogonal matrix $\\dot{Q}\\in\\mathbb{R}^{\\dot{d}\\times{d}}$ for a pre-trained frozen weight matrix $\\dot{\\boldsymbol{W}}^{0}\\in\\dot{\\mathbb{R}}^{d\\times n}$ and modifies the multiplication $y=(W^{0})^{\\top}x$ to $y=\\mathbf{\\dot{\\bar{(}}}Q W^{0})^{\\top}x$ . Note that the identity matrix $I$ is orthogonal, which makes it a natural initialization for $Q$ . OFT uses block-diagonal structure for $Q$ , parameterizing it as ", "page_idx": 1}, {"type": "equation", "text": "$$\nQ=\\mathrm{diag}(Q_{1},Q_{2},\\ldots,Q_{r}),\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $Q_{i}\\in\\mathbb{R}^{b\\times b}$ are small orthogonal matrices and $b r=d$ . Orthogonality is enforced by Cayley parametrization, i.e. ", "page_idx": 1}, {"type": "equation", "text": "$$\nQ_{i}=(I+K_{i})(I-K_{i})^{-1},\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $K_{i}$ are skew-symmetric: $K_{i}=-K_{i}^{\\top}$ . This ensures orthogonality of $Q_{i}$ and, hence, of $Q$ . ", "page_idx": 1}, {"type": "text", "text": "Nevertheless, block-diagonal matrices can be too restrictive, as they divide neurons into $r$ independent groups based on their indices. This motivates the construction of dense parameter-efficient orthogonal matrices. To address this problem, the Orthogonal Butterfly method (BOFT) was introduced [Liu et al., 2024b]. BOFT uses block-butterfly structure to construct $Q$ . Essentially, $Q$ is parameterized as a product of $m$ orthogonal sparse matrices: ", "page_idx": 1}, {"type": "equation", "text": "$$\nQ=B_{m}B_{m-1}\\ldots B_{1}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Each matrix $B_{i}$ is a block-diagonal matrix up to a permutation of rows and columns, consisting of $r$ block matrices of sizes $b\\times b$ . Similarly to OFT, the orthogonality is enforced by the Cayley parametrization applied to each block. However, BOFT method has some areas for improvement as well. To construct a dense matrix, BOFT requires at least ", "page_idx": 1}, {"type": "equation", "text": "$$\nm=1+\\lceil\\log_{2}(r)\\rceil\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "matrices. For example, the authors of BOFT use $m\\,=\\,5$ or 6 matrices in the BOFT method for fine-tuning of Stable Diffusion [Rombach et al., 2022]. Large amount of stacked matrices leads to significant time and memory overhead during training. There is also a room for improvement in terms of parameter-efficiency. To overcome these issues, we introduce a new class of structured matrices that we denote $\\mathit{g s}$ (group-and-shuffle) that generalizes Monarch matrices [Dao et al., 2022, Fu et al., 2024] and show how to use this class to construct parameter-efficient orthogonal parametrization. Similarly to BOFT, our approach uses block-diagonal matrices and permutations, but requires only ", "page_idx": 1}, {"type": "equation", "text": "$$\nm=1+\\lceil\\log_{b}(r)\\rceil\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "matrices of the same size to construct a dense matrix. See details in Section 5.2. The reduced requirements on $m$ allow us to use $m\\,=\\,2$ in experiments to maximize computational efficiency, while still maintaining accurate results. ", "page_idx": 1}, {"type": "text", "text": "3 $\\mathcal{G S}$ -matrices ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Our motivation within this work is to utilize orthogonal matrices of the form: ", "page_idx": 2}, {"type": "equation", "text": "$$\nA=P_{L}(L P R)P_{R}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where matrices $L$ and $R$ are block-diagonal matrices with $r$ blocks of sizes $b\\times b$ and $P_{L},P,P_{R}$ are certain permutation matrices, e.g. $\\bar{P}_{L}=P^{\\top},P_{R}=I$ in the orthogonal fine-tuning setting and $P_{R}=P,P_{L}=I$ for convolutional architectures. Note that although the case $P_{L}=\\bar{P}^{\\top},P_{R}^{\\phantom{\\top}}=I$ resembles Monarch matrices [Dao et al., 2022], they are unable to form such a structure, e.g., with equal-sized blocks in $L$ and $R$ . The issue is that the Monarch class has a constraint that interconnects the number of blocks in matrix $L$ and the number of blocks in matrix $R$ (see Appendix C for details). Moreover, Monarch matrices have not been considered with orthogonality constraints. ", "page_idx": 2}, {"type": "text", "text": "To build matrices of the form (1), we first introduce a general class of $\\mathit{g s}$ -matrices and study its properties. We then discuss orthogonal matrices from this class in Section 4. ", "page_idx": 2}, {"type": "text", "text": "3.1 Definition of $\\mathit{g s}$ -matrices ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Definition 3.1. An $m\\times n$ matrix $A$ is in $\\mathcal{G S}(P_{L},P,P_{R})$ class with $k_{L},k_{R}$ blocks and block sizes $b_{L}^{1}\\times b_{L}^{2},\\,b_{R}^{1}\\times b_{R}^{2}\\,i f$ ", "page_idx": 2}, {"type": "equation", "text": "$$\nA=P_{L}(L P R)P_{R},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $L\\,=\\,\\mathrm{diag}(L_{1},L_{2},\\ldots,L_{k_{L}}),L_{i}\\,\\in\\mathbb{R}^{b_{L}^{1}\\times b_{L}^{2}}$ , $R\\,=\\,\\mathrm{diag}(R_{1},R_{2},\\ldots,R_{k_{R}});$ , $R_{i}\\,\\in\\,\\mathbb{R}^{b_{R}^{1}\\times b_{R}^{2}}$ , $P_{L},P,P_{R}$ are permutation matrices and $b_{L}^{2}\\cdot k_{L}=b_{R}^{1}\\cdot k_{R}=s,b_{L}^{1}\\cdot k_{L}=m,b_{R}^{2}\\cdot k_{R}=n$ . ", "page_idx": 2}, {"type": "text", "text": "In practice, we fix $P_{L},P,P_{R}$ depending on the application and only make matrices $L,R$ subject for change. $g{s}$ -matrices are hardware-efficient, as they are parametrized by two simple types of operations that can implemented efficiently: multiplications by block-diagonal matrices and permutations. ", "page_idx": 2}, {"type": "text", "text": "Let us also illustrate a forward pass $A x\\equiv L P R x$ for a matrix $A\\in\\mathcal{G S}(I,P,I)$ as a building block for the more general class with two additional permutations. The first operation $y=R x$ consists of several fully-connected layers, applied individually to subgroups of $x$ , see Figure 1. The next multiplication $L P y$ ensures that these groups interact with each other. Indeed, the permutation matrix $P$ shuffles the entries of $y$ into new subgroups. These subgroups are then again processed by a number of fully-connected layers using $L$ . This motivates the naming for our class of matrices: Group-and-Shuffle or $\\mathit{g s}$ for short. ", "page_idx": 2}, {"type": "text", "text": "Another useful insight on these matrices is that the class $\\mathcal{G S}(I,P,I)$ consists of block matrices with low-rank blocks. The permutation matrix $P$ is responsible for the formation of these blocks and defines their ranks (note that rank may vary ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\overbrace{\\sum_{\\alpha=1}^{\\alpha}}^{\\mathrm{~\\alpha~\\,~\\,~}^{\\mathrm{~I~}}}\\overrightarrow{\\mathrm{~o~}}\\,\\!\\!\\!\\sum_{\\alpha=1}^{\\mathrm{~\\alpha~\\,~\\,~R~}}}\\\\ {\\overbrace{\\sum_{\\alpha=1}^{\\mathrm{~\\alpha~\\,~\\,~}^{\\mathrm{~O~}}}\\!\\!\\!\\sum_{\\alpha=1}^{\\mathrm{~\\alpha~\\,~\\,~}^{\\mathrm{~O~}}}\\!\\!\\!\\!\\alpha}^{\\mathrm{~\\alpha~\\,~\\,~}^{\\mathrm{~O~}}}}\\\\ {\\overbrace{\\sum_{\\alpha=1}^{\\mathrm{~\\alpha~\\,~}^{\\mathrm{~O~}}}\\!\\!\\!\\!\\sum_{\\alpha=1}^{\\mathrm{~\\alpha~\\,~}^{\\mathrm{~O~}}}\\!\\!\\!\\!\\sum_{\\alpha=1}^{\\mathrm{~\\alpha~\\,~}^{\\mathrm{~O~}}}\\!\\!\\!\\!\\alpha}^{\\mathrm{~\\alpha~\\,~\\,~}^{\\mathrm{~O~}}}}\\\\ {\\overbrace{(P^{\\mathrm{R~}})^{\\mathrm{~\\alpha~\\,~}^{\\mathrm{~P~}}}\\!\\!\\!\\!\\sum_{\\alpha=1}^{\\mathrm{~\\alpha~\\,~}^{\\mathrm{~O~}}}\\!\\!\\!\\!\\!\\alpha}^{\\mathrm{~\\alpha~\\,~}^{\\mathrm{~O~}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Figure 1: $\\mathcal{G S}(I,P,I)$ matrices with $b_{L}^{1}=b_{L}^{2}=3$ , $b_{R}^{1}=b_{R}^{2}=2$ , $k_{L}\\,=\\,2,k_{R}\\,=\\,3$ . Edges between nodes denote nonzero weights. ", "page_idx": 2}, {"type": "text", "text": "from block to block). The result below formally describes our findings and is key to the projection operation that we describe afterwards. ", "page_idx": 2}, {"type": "text", "text": "Proposition 1. Let $A$ be a matrix from $\\mathcal{G S}(I,P,I)$ with a permutation matrix $P$ defined by the function $\\sigma:\\{0,.\\,.\\,.\\,,n-1\\}\\rightarrow\\{0,.\\,.\\,.\\,,n-1\\}$ . Let $\\{v_{i}^{\\top}\\}-b e$ the rows of the blocks $R_{1},\\ldots,R_{k_{R}}$ , $\\{u_{i}\\}$ \u2013 the columns of the blocks $L_{1},\\L_{\\cdot}\\ .\\ ,L_{k_{L}}$ in the consecutive order. Then the matrix $A$ can be written as a block matrix with $k_{L}\\times k_{R}$ blocks using the following formula for each block $A_{k_{1},k_{2}}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\nA_{k_{1},k_{2}}=\\sum_{\\lfloor\\frac{\\sigma(i)}{k_{L}}\\rfloor=k_{1}}\\!\\!\\!\\!{u_{\\sigma(i)}v_{i}^{\\top}}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Note that we use zero-indexing for this proposition for simplicity of formulas. ", "page_idx": 2}, {"type": "image", "img_path": "7EQx56YSB2/tmp/fa69e1ceb01d4a5e22a023280fe2722e86fa530ea809ba41ae56b0f64c302376.jpg", "img_caption": ["Figure 2: Illustration of Proposition 1 that provides block low-rank interpretation of $\\mathcal{G S}(I,P,I)$ matrices. The matrix $R$ contains 2 blocks and matrix $L$ contains 4 blocks. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Let us illustrate this proposition in Figure 2. We consider $\\mathcal{G S}(I,P,I)$ with $k_{L}=4$ and $k_{R}=2$ blocks in $L$ and $R$ and with the block sizes $3\\times3$ and $6\\times6$ respectively. Let us consider the leading block $A_{00}$ of the size $3\\times6$ . According to Proposition 1, $A_{00}\\dot{=}\\;u_{0}v_{2}^{\\top}\\dot{+}\\;u_{2}v_{4}^{\\top}$ . Indeed, let us take a closer look, e.g., at the term $u_{2}v_{4}^{\\top}$ . In the permutation matrix $P$ , we have a nonzero element in the position $(2,4)$ as $i=4$ and $\\sigma(4)=2$ . Therefore, we select the third column $u_{2}$ in $L_{1}$ and the fifth row $v_{4}^{\\top}$ in $R_{1}$ . This leads to adding a rank-one term $u_{2}v_{4}^{\\top}$ to $A_{00}$ as we see in the formula above. ", "page_idx": 3}, {"type": "text", "text": "Another direct corollary from Proposition 1 is a projection operation $\\pi\\colon\\mathbb{R}^{m\\times n}\\to\\mathcal{G S}(P_{L},P,P_{R})$ that satisfies: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\pi(A)\\in\\underset{B\\in\\mathcal{G S}(P_{L},P,P_{R})}{\\arg\\operatorname*{min}}\\|A-B\\|_{F},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\|\\cdot\\|_{F}$ is the Frobenius norm. Thanks to the block low-rank representation of matrices from $\\ddot{\\mathcal{G}}\\mathcal{S}(P_{L},P,P_{R})$ , the projection $\\pi$ is simply constructed using SVD truncations of the blocks $(P_{L}^{\\top}A P_{R}^{\\top})_{k_{1},k_{2}}$ and is summarized in Algorithm 1. ", "page_idx": 3}, {"type": "text", "text": "Algorithm 1 Projection $\\pi(\\cdot)$ of $A$ onto $\\mathcal{G}S(P_{L},P,P_{R})$ Input: $A,P_{L},P,P_{R}$ Return: $L,R$ for $k_{1}=1\\dots k_{L}$ do for $k_{2}=1\\ldots k_{R}$ do Compute SVD of $(P_{L}^{T}A P_{R}^{T})_{k_{1},k_{2}}=U\\Sigma V^{\\top}$ ; Set $r=r_{k_{1},k_{2}}$ \u2013 rank of block determined by $P$ ; Take $U_{r}=U[:r,:]$ , $\\Sigma_{r}=\\Sigma[:r,:r],V_{r}=V[:r,:]$ ; Pack columns of $U_{r}\\Sigma_{r}^{1/2}$ into $L_{k_{1}}$ and rows of $\\Sigma_{r}^{1/2}V_{r}$ into $R_{k_{2}}$ according to $P$ ; end for end for ", "page_idx": 3}, {"type": "text", "text": "4 Orthogonal $\\mathcal{G S}(P_{L},P,P_{R})$ matrices ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we study the orthogonality constraint for the $\\mathcal{G}S(P_{L},P,P_{R})$ to obtain structured orthogonal representation. This is one of the main contributions of our paper and we utilize this class in all the numerical experiments. Since we are interested only in square orthogonal matrices, we additionally assume that $m=n$ and $b_{L}^{1}=b_{L}^{2}=b_{L}$ ; $b_{R}^{1}=b_{R}^{2}=b_{R}$ . Similarly to parametrizations in OFT and BOFT, a natural way to enforce orthogonality of $\\mathcal{G}S(P_{L},P,P_{R})$ -matrices is to enforce orthogonality of each block of $L$ and $R$ . This indeed leads an orthogonal matrix since permutation matrices are also orthogonal as well as a product of orthogonal matrices. However, it is not immediately obvious that there exist no orthogonal matrices from $\\mathcal{G}S(P_{L},P,P_{R})$ that cannot be represented this way. Surprisingly, we find that such a way to enforce orthogonality is indeed sufficient for covering of all orthogonal matrices from $\\mathcal{G S}(P_{L},P,P_{R})$ . ", "page_idx": 3}, {"type": "text", "text": "Theorem 1. Let $A$ be any orthogonal matrix from $\\mathcal{G}S(P_{L},P,P_{R})$ . Then, $A$ admits $P_{L}(L P R)P_{R}$ representation with the matrices $L,R$ consisting of orthogonal blocks. ", "page_idx": 3}, {"type": "text", "text": "Proof. Matrices $P_{L},P_{R}$ are orthogonal as they are permutation matrices. It means that it is sufficient to prove theorem in the case when $A$ is from $\\mathcal{G S}(I,P,I)$ , which means that we can use low blockrank structure interpretation from Proposition 1. Consider a skeleton decomposition of the blocks $A_{i j}=U_{i j}V_{i j}^{\\top}$ , $U_{i j}^{\\,\\,\\,}\\in\\mathbb{R}^{b_{L}\\times r_{i j}}$ , $V_{i j}\\in\\bar{\\mathbb{R}}^{b_{R}\\times r_{i j}}$ such that $U_{i j}^{\\top}U_{i j}=I_{r_{i j}}$ (this can be ensured, e.g., using the QR decomposition). Then ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\nA=\\left(\\begin{array}{c c c}{U_{1,1}V_{1,1}^{\\top}}&{\\hdots}&{U_{1,k_{L}}V_{1,k_{L}}^{\\top}}\\\\ {\\vdots}&{\\ddots}&{\\vdots}\\\\ {U_{k_{L},1}V_{k_{L},1}^{\\top}}&{\\hdots}&{U_{k_{L},k_{R}}V_{k_{L},k_{R}}^{\\top}}\\end{array}\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Take the $j$ -th block-column of $A$ . Since $A$ is an orthogonal matrix, we get: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left(V_{1,j}U_{1,j}^{\\top}\\quad.\\dots\\quad V_{k_{L},j}U_{k_{L},j}^{\\top}\\right)\\left(\\begin{array}{c}{U_{1,j}V_{1,j}^{\\top}}\\\\ {\\vdots}\\\\ {U_{k_{L},j}V_{k_{L},j}^{\\top}}\\end{array}\\right)=I_{b_{R}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Multiplying matrices in the l.h.s. we get $V_{1,j}U_{1,j}^{\\top}U_{1,j}V_{1,j}^{\\top}+\\cdot\\cdot+V_{k_{L},j}U_{k_{L},j}^{\\top}U_{k_{L},j}V_{k_{L},j}^{\\top}=I_{b_{R}}$ . Since $U_{i j}^{\\top}U_{i j}=I_{r_{i j}}$ we conclude $V_{1,j}V_{1,j}^{\\top}+\\cdot\\cdot\\cdot+V_{k_{L},j}V_{k_{L},j}^{\\top}=I_{b_{R}}$ . This implies that $\\left(V_{1,j}\\quad...\\quad V_{k_{L},j}\\right)$ is an orthogonal matrix. Note that if we now parameterize $A=L P R$ with the matrices $V_{i j}$ packed into $R$ and $U_{i j}$ packed into $L$ , then $\\left(V_{1,j}\\quad...\\quad V_{k_{L},j}\\right)$ is exactly the $j$ -th block matrix in $R$ up to permutation of rows. Therefore, every block in $R$ is an orthogonal matrix. Since we now proved that $\\mathring{V}_{i j}^{\\top}V_{i j}=I$ , we can use same the derivation for the rows of $A$ and conclude that blocks of $L$ are also orthogonal. \u53e3 ", "page_idx": 4}, {"type": "text", "text": "5 $\\mathcal{G}S(P_{m+1},.~.~.~,P_{1})$ matrices ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section we describe an the extension of $\\mathit{g s}$ -matrices that uses more than two block-diagonal matrices and show that with the right permutations choices $\\mathit{g s}$ -matrices are more effective than block butterfly matrices in forming dense matrices. Here by dense matrices we imply matrices that do not contain zero entries at all. ", "page_idx": 4}, {"type": "text", "text": "Definition 5.1. $A$ is said to be in $\\mathcal{G S}(P_{m+1},\\ldots,P_{1})\\;i f$ ", "page_idx": 4}, {"type": "equation", "text": "$$\nA=P_{m+1}\\prod_{i=m}^{1}(B_{i}P_{i}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where each matrix $B_{i}$ is a block-diagonal matrix with $k_{i}$ blocks of size $b_{i}^{1}\\times b_{i}^{2}$ , matrices $P_{i}$ are permutation matrices and $b_{i}^{1}\\cdot k_{i}=b_{i+1}^{\\bar{2}}\\cdot k_{i+1}$ . ", "page_idx": 4}, {"type": "text", "text": "Remark 1. Similarly to the case $m=2$ described in Section 3, we may use orthogonal blocks in $B_{i}$ , $i=1,\\dotsc,m+1$ to obtain orthogonal matrices. However, it is not clear if an analog to Theorem 1 is correct in this case as well. ", "page_idx": 4}, {"type": "text", "text": "Remark 2. For each of the classes of Block Butterfly matrices [Chen et al., 2022], Monarch matrices [Dao et al., 2022] and order-p Monarch matrices [Fu et al., 2024], there exist permutation matrices $P_{m+1},\\ldots,P_{1}$ such that $\\mathcal{G}S(P_{m+1},...\\,P_{1})$ coincides with a respective class. Indeed, Monarch matrices have the form of alternating block-diagonal matrices and permutations with some specific size constraints and sparse matrices in the product of Block Butterfly matrices can be easily transformed to block-diagonal matrices with permutations of rows and columns. ", "page_idx": 4}, {"type": "text", "text": "5.1 Choosing permutation matrices ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We suggest using the following matrices with $k=k_{i}$ for $P_{i}$ . Note that this is efficient for forming dense matrices as follows from the proof of Theorem 2. This is by contrast to the permutations used in [Fu et al., 2024] that are restricted to particular matrix sizes. ", "page_idx": 4}, {"type": "text", "text": "Definition 5.2 ([Dao et al., 2022]). Let $P_{(k,n)}$ be a permutation matrix given by permutation \u03c3 on $\\{0,1,\\ldots,n-1\\}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\sigma(i)=(i\\,m o d\\,k)\\cdot\\frac{n}{k}+\\left\\lfloor\\frac{i}{k}\\right\\rfloor.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Applying this permutation to a vector can be viewed as reshaping an input of size $n$ into an $\\textstyle k\\times{\\frac{n}{k}}$ matrix in a row-major order, transposing it, and then vectorizing the result back into a vector (again in row-major column). We provide several examples of such permutations in Figure 3. ", "page_idx": 5}, {"type": "image", "img_path": "7EQx56YSB2/tmp/305d1b2e197a41ad3207d0028aed8d33d88a6163afc271aea4e5dcef3a889caf.jpg", "img_caption": ["Figure 3: Illustraion of $P_{(k,12)}$ permutations for $k\\in\\{3,4,6,2\\}$ . "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "5.2 Comparison to block butterfly matrices and BOFT ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Block Butterfly matrices were introduced in [Chen et al., 2022] and are used to construct orthogonal matrices in the BOFT method. Block Butterfly matrix class is a special case of higher-order $g{s}$ - matrices with $k_{i}\\;=\\;r$ and $b_{i}^{1}\\;=\\;b_{i}^{2}\\;=\\;b\\;=\\;\\dot{2}s$ and certain permutation choices. However, we argue that the choice of these permutations are sub-optimal for construction of dense matrix and using permutations from Definition 5.2 is more effective. When using block-diagonal matrices with $r$ blocks, block butterfly matrices need $1+\\lceil\\log_{2}(r)\\rceil$ matrices to construct a dense matrix. For $\\mathit{g s}$ -matrices we have the following result. ", "page_idx": 5}, {"type": "text", "text": "Theorem 2. Let $k_{i}\\,=\\,r,b_{i}^{1}\\,=\\,b_{i}^{2}\\,=\\,b$ . Then using $m=1+\\lceil\\log_{b}(r)\\rceil$ is sufficient for the class $\\mathcal{G S}(P_{L},P_{(r,b r)},...,P_{(r,b r)},P_{R})$ to form a dense matrix for any $P_{L},P_{R}$ . Moreover, the choice of $P_{2}=\\cdot\\cdot\\cdot=P_{m}=P_{(r,b r)}$ is optimal in the sense that all matrices from $\\mathcal{G S}(P_{m+1},...,P_{1})$ contain zero blocks for any integer $m<1+\\lceil\\log_{b}(r)\\rceil$ and any permutations $P_{1},\\dotsc,P_{m+1}$ . ", "page_idx": 5}, {"type": "text", "text": "Proof. See Appendix D. ", "page_idx": 5}, {"type": "text", "text": "For example, let us consider a case of constructing a dense orthogonal matrix of the size $1024\\times1024$ . Suppose also that we use block matrices with block size 32. Constructing a dense matrix with Block Butterfly matrices requires $1+\\log_{2}(32)=6$ butterfly matrices, which leads to $6\\times32^{3}$ parameters in the representation. $\\mathcal{G S}(P_{L},P,P_{R})$ matrices with $P=P_{(32,1024)}$ only need two matrices to construct a dense matrix yielding $2\\times32^{3}$ parameters. The $\\mathcal{G}S(P_{L},P,P_{R})$ parametrization is also naturally more computationally efficient as fewer number of multiplications is both faster and requires less cached memory for activations. ", "page_idx": 5}, {"type": "text", "text": "6 Applications ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "6.1 Orthogonal fine-tuning with $\\mathcal{G}S(P_{L},P,P_{R})$ (GSOFT) ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We utilize the pipeline of OFT and BOFT methods with the exception of parametrizing $Q$ with orthogonal permuted $\\mathcal{G S}(P_{L},P,P_{R})$ matrices. In particular, for parametrization of $\\bar{Q^{\\big}}\\in\\ \\mathbb{R}^{d\\times d}$ , we utilize the $\\mathcal{G}S(P^{\\top},P,I)$ class, i.e. $Q\\,=\\,P^{\\top}L P R$ , where ${\\cal L}\\,=\\,\\mathrm{diag}(L_{1},...\\,L_{r})$ , $L_{i}\\,\\in\\,\\mathbb{R}^{b\\times b}$ , $R=\\mathrm{diag}(R_{1},\\ldots,R_{r})$ , $R_{i}\\in\\mathbb{R}^{b\\times b}$ . For consistency, we use the same notation for the number of blocks and block sizes as in BOFT and OFT methods. We use $P_{(r,b r)}$ as a permutation matrix $P$ . To enforce orthogonality, we parameterize each block in matrices $L,R$ with the Cayley parametrization. We initialize $Q$ as an identity matrix by initializing each block to be an identity matrix. Additional techniques like magnitude scaling and multiplicative dropout that are used in OFT and BOFT can be utilized the same way in our method, though we only use scaling in our experiments. Note that likewise in OFT, BOFT weights of the matrix $Q$ can be merged with the pretrained weight $W$ producing no inference overhead. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "6.2 Two-sided orthogonal fine-tuning (Double GSOFT) ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Consider SVD decomposition of a matrix $W^{0}=U\\Sigma V^{\\top}$ . Applying orthogonal fine-tuning, we get $W^{\\prime}=(Q U)\\Sigma V^{\\top}$ , which is an SVD decomposition for the adapted weight $W^{\\prime}$ . This shows that we can only change left singular vectors $U$ with the standard orthogonal fine-tuning paradigm. At the same time, the LoRA method modifies both matrices $U$ and $V$ . Moreover, recent papers [Meng et al., 2024, Li et al., 2023] show that initializing matrices $A,B$ with singular vectors can additionally boost performance of LoRA. This motivates an extension of orthogonal fine-tuning method, that can adapt both matrices $U$ and $V$ . We introduce a simple approach that multiplies pre-trained weight matrices from both sides, rather than one. This method modifies forward pass from $z=(W^{0})^{\\top}x$ to ", "page_idx": 6}, {"type": "equation", "text": "$$\nz=(Q_{U}W^{0}Q_{V})^{\\top}x\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Where $Q_{U}$ and $Q_{V}$ are parametrized as orthogonal $\\mathit{g s}$ -matrices. In cases where BOFT utilizes 5-6 matrices, we can leverage the fact that our method uses only 2 and adapt both sides while still using less matrices and trainable parameters than BOFT. ", "page_idx": 6}, {"type": "text", "text": "6.3 GS Orthogonal Convolutions ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Recall, that due to linearity of a multichannel convolution operation, we can express the convolution of tensor $X\\in\\mathbb{R}^{c_{i n}\\times h\\times w}$ with a kernel $L\\in\\mathbb{R}^{c_{o u t}\\times c_{i n}\\times k\\times k^{\\star}}L\\star X$ in terms of matrix multiplication [Singla and Feizi, 2021]: ", "page_idx": 6}, {"type": "equation", "text": "$$\nY=L\\star X\\quad\\Leftrightarrow\\quad v e c(Y)=\\left[\\begin{array}{c c c}{L_{0,0}}&{\\cdot\\cdot\\cdot}&{L_{0,c_{i n}-1}}\\\\ {\\vdots}&{\\ddots}&{\\vdots}\\\\ {L_{c_{o u t}-1,0}}&{\\cdot\\cdot\\cdot}&{L_{c_{o u t}-1,c_{i n}-1}}\\end{array}\\right]v e c(X),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $L_{i,j}$ is doubly Toeplitz matrix, corresponding to convolution between $i$ -th and $j$ -th channels and $v e c({\\bar{X}})$ is a vectorization of tensor into a vector in a row-major order. Thus, the convolution is essentially a block matrix, where each block represents a standard convolution operation. Using this block interpretation (2), we may apply the concept of $g{s}$ matrices to convolutional layers as well. Considering each convolution between channels as an element of our block matrix, we can set some of these blocks to zero, obtaining some additional structure. Thus, we can construct block matrix which has block-diagonal structure, corresponding to grouped convolution (further, in all equations we will denote it as $\\mathtt{G r c o n v}$ ). Then, defining ChShuffle as a permutation of channels, like in [Zhang et al., 2018], we obtain structure, which is similar to GSOFT, defined in Section 6: ", "page_idx": 6}, {"type": "equation", "text": "$$\nY=\\mathsf{G r C o n v}_{2}\\big(\\mathtt{C h S h u f f l e}_{2}\\big(\\mathtt{G r C o n v}_{1}\\big(\\mathtt{C h S h u f f l e}_{1}(X)\\big)\\big)\\big).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The proposed $\\mathit{g s}$ convolutional layer shuffles information between each pair of input channels and requires less parameters and FLOPs during computations. In this example we can also choose permutations of channels and change kernel size. This convolutional layer can be treated as $\\mathcal{G S}(P_{m+1},...,P_{1})$ matrix in vectorized view, that is why choosing permutations between convolutional layers is also very important for information transition properties. In Appendix F we explain the choice of ChShuffle operation. ", "page_idx": 6}, {"type": "text", "text": "We can use the proposed layer to construct orthogonal convolutions (transformations with an orthogonal Jacobian matrix) similarly to skew orthogonal convolution (SOC) architecture, that uses Taylor expansion of a matrix exponential. One major downside of methods such as SOC and BCOP [Li et al., 2019] is that they require more time than basic convolution operation. For instance, in the SOC method, one layer requires multiple applications of convolution (6 convolutions per layer). In our framework, we propose a parametrization of a convolutional layer, in which imposing an orthogonality to convolutions has fewer number of FLOPs and parameters thanks to the usage of grouped convolutions. ", "page_idx": 6}, {"type": "text", "text": "Let us discuss in more details how SOC works and the way we modify it. In SOC, a convolutional filter is parametrized in the following way: ", "page_idx": 7}, {"type": "equation", "text": "$$\nL=M-{\\mathsf{C o n v T r a n s p o s e}}(M),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $M\\in\\mathbb{R}^{c_{i n}\\times c_{o u t}\\times r\\times s}$ is an arbitrary kernel and the ConvTranspose is the following operation: ", "page_idx": 7}, {"type": "equation", "text": "$$\n{\\mathsf{C o n v T r a n s p o s e}}(M)_{i,j,k,l}=M_{j,i,r-k-1,s-l-1}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "This parametrization of filter $L$ makes the matrix from Equation 2 skew-symmetric. As matrix exponential of skew-symmetric matrix is an orthogonal matrix, in SOC the authors define convolution exponential operation, which is equivalent to matrix exponential in matrix-vector notation: ", "page_idx": 7}, {"type": "text", "text": "Definition 6.1. [Singla and Feizi, 2021] Let $X\\in\\mathbb{R}^{c\\times h\\times w}$ be an input tensor and $L\\in\\mathbb{R}^{c\\times c\\times k\\times k}$ be a convolution kernel. Then, define convolution exponential $L\\star_{e}X$ as follows: ", "page_idx": 7}, {"type": "equation", "text": "$$\nL\\star_{e}X=X+{\\frac{L\\star X}{1!}}+{\\frac{L\\star^{2}\\,X}{2!}}+\\,.\\,.\\,.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $L\\star^{i}X$ is a convolution with kernel $L$ applied i times consequently. ", "page_idx": 7}, {"type": "text", "text": "As mentioned above, with proper initialization we get a convolutional layer with orthogonal Jacobian matrix. Using the parametrization of convolution layer from the Equation 3 and substituting there two grouped convolution exponentials (e.g. in our parametrization we have the same convolution exponential, but we have grouped convolution instead of basic one) with the parameterized kernel: ", "page_idx": 7}, {"type": "equation", "text": "$$\nY=\\mathsf{G r E x p C o n v_{2}}\\big(\\mathtt{C h S h u f f1e}_{2}\\big(\\mathtt{G r E x p C o n v_{1}}\\big(\\mathtt{C h S h u f f1e}_{1}(X)\\big)\\big)\\big)\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "In our experiments we tried different layer architectures and we found that making kernel size of the second convolution equal to 1 speeds up our convolutional layer, maintaining quality metrics. Thus, if convolutional layer consists of two grouped convolutional exponentials, the second convolutional exponential has kernel $.s i z e=1\\times1$ ", "page_idx": 7}, {"type": "text", "text": "7 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "All the experiments below were conducted on NVIDIA V100-SXM2-32Gb GPU. We ran all the experiments within $\\mathord{\\sim}2000$ GPU hours. ", "page_idx": 7}, {"type": "text", "text": "Source code is available at: https://github.com/Skonor/group_and_shuffle ", "page_idx": 7}, {"type": "text", "text": "7.1 Natural language understanding ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We report result on the GLUE [Wang et al., 2018] benchmark with RoBERTa-base [Liu et al., 2019] model. Benchmark includes several classification tasks that evaluate general language understanding. We follow training settings of [Liu et al., 2024b, Zhang et al., 2023]. We apply adapters for all linear layers and only tune learning rate for all methods. Table 1 reports best results on the evaluation set from the whole training. LoRA, OFT and BOFT are implemented with PEFT library [Mangrulkar et al., 2022]. GSOFT method outperforms OFT, BOFT and also have a slight edge over LoRA. Note that even though skew-symmetric $K$ theoretically matrix only requires approximately half the parameters of a full matrix, in practice it is parametrized as $K=A\\dot{-}A^{T}$ for the ease of computations. However, after fine-tuning, one can only save upper-triangular part of $K$ . Doing this, orthogonal fine-tuning methods become approximately 2 times more efficient in terms of memory savings. ", "page_idx": 7}, {"type": "text", "text": "7.2 Subject-driven generation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Subject-driven generation [Ruiz et al., 2023, Gal et al., 2022] is an important and challenging task in the field of generative modelling. Given several photos of a particular concept, we want to introduce it to the diffusion model so that we can generate this particular object in different scenes described by textual prompts. The main way to do this is to fine-tune the model. However, the large number of fine-tuning parameters together with the lack of training images make the model prone to overftiting, i.e. the model reconstructs the concept almost perfectly, but starts to ignore the textual prompt during generation. To solve this problem and stabilize the fine-tuning process, different lightweight parameterizations [Qiu et al., 2023, Liu et al., 2024b, Hu et al., 2022, Tewel et al., 2023, Han et al., ", "page_idx": 7}, {"type": "table", "img_path": "7EQx56YSB2/tmp/b25c1105606c1eefe5622d7f52aad5e904b3f6fd980d34e97697c666fedd8911.jpg", "table_caption": ["Table 1: Results on GLUE benchmark with RoBERTa-base model. We report Pearson correlation for STS-B, Matthew\u2019s correlation for CoLA and accuracy for other tasks. # Params denotes number of trainable parameters "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "7EQx56YSB2/tmp/f7f67646d4283b3e57d8099754a29a30eb22227ce768d471e028ec8abd50765e.jpg", "table_caption": ["Table 2: Results on subject-driven generation. # Params denotes the number of training parameters in each parametrization. Training time is computed for 3000 iterations on a single GPU V100 in hours. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "2023] and regularization techniques [Ruiz et al., 2023, Kumari et al., 2023] are widely used in this task. Therefore, we chose this setting to evaluate the effectiveness of the proposed orthogonal parameterization compared to other approaches. ", "page_idx": 8}, {"type": "text", "text": "We use StableDiffusion [Rombach et al., 2022] and the Dreambooth [Ruiz et al., 2023] dataset for all our experiments. The following parameterizations were considered as baselines in this task: full (q, k, v and out.0 layers in all cross- and self- attentions of the UNet are trained), LoRA [Hu et al., 2022] and BOFT [Liu et al., 2024b] applied to the same layer. We use our GSOFT parameterization and a two-sided orthogonal GSOFT (Double GSOFT) applied to the same layers as baselines. For a more comprehensive comparison, we consider different hyperparameters for the models, adjusting the total number of optimized parameters. More training and evaluation details can be found in Appendix E. ", "page_idx": 8}, {"type": "text", "text": "CLIP image similarity, CLIP text similarity and visual comparison for this task are presented in Table 2 and Figure 4. As the results show, GSOFT and DoubleGSOFT are less prone to overfitting compared to the baselines. They show better alignment with text prompts while maintaining a high level of concept fidelity. Furthermore, both methods with optimal hyperparameters are more efficient than BOFT and comparable to LoRA and full parameterization in terms of training time. See Appendix E for more visual and quantitative comparison. ", "page_idx": 8}, {"type": "text", "text": "7.3 GS Orthogonal Convolutions ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Following [Singla and Feizi, 2021], we train LipConvnet-n on CIFAR-100 dataset. LipConvnet-n is 1-Lipschitz neural network, i.e. neural network with Lipschitz constant equal to 1, his property provides certified adversarial robustness. LipConvnet uses orthogonal convolutions and gradient preserving activations in order to maintain 1-Lipschitz property. ", "page_idx": 8}, {"type": "text", "text": "LipConvnet-n architecture consists of 5 equal blocks, each having $\\frac{n}{5}$ skew orthogonal convolutions, where the last convolution at each level downsamples image size. We replace the skew orthogonal convolution layer with the structured version using $\\mathit{g s}$ orthogonal convolutions and test it in the setting of [Singla and Feizi, 2021], using the same hyperparameters (learning rate, batch size and scheduler stable during testing). In layers where we have two GrExpConv, the second convolution has kernel size equal to 1. ", "page_idx": 8}, {"type": "text", "text": "We also use a modified activation function (MaxMinPermuted instead of MaxMin), which uses different pairing of channels. This makes activations aligned with the ChShuffle operation and grouped convolutions. The choice of permutation for ChShuffle also slightly differs from permutations defind in Definition 5.2 because of the interplay between activations and convolutional layers. We provide definitions and intuition regarding activations and permutations for ChShuffle in Appenix F. ", "page_idx": 8}, {"type": "image", "img_path": "7EQx56YSB2/tmp/b51eca78ece90867f7e27d5777160ef5340f146725e2f8b38e82d27ae7491334.jpg", "img_caption": ["Figure 4: Subject-driven generation visual results on 3000 training iterations. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Table 3: Results of training LipConvnet-15 architecture on CIFAR-100. $(a,b)$ in \u201cGroups\u201d column denotes number of groups in two grouped exponential convolutions (with kernel sizes 3 and 1). $(a,-)$ corresponds to only one $\\mathit{g s}$ orthogonal convolutional layer. Before each grouped layer with $k$ groups use a ChShuffle operator. ", "page_idx": 9}, {"type": "table", "img_path": "7EQx56YSB2/tmp/d75aa4fc6406c092cb6b29d11ef1a0977600f45d91baa19a1e68ee9a3985e278.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "8 Concluding remarks ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we introduce a new class of structured matrices, called $\\mathit{g s}$ -matrices, build a structured orthogonal parametrization with them and use them in several domains within deep learning applications. However, we hope that our orthogonal parametrization can be adapted to different settings in future (including tasks outside of deep learning), as it makes orthogonal parametrizations less of a computational burden. $\\mathit{g s}$ -matrices without orthogonality constraints is another promising direction. ", "page_idx": 9}, {"type": "text", "text": "9 Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Although our method for orthogonal fine-tuning is faster than BOFT, it is still slower than LoRA during training. Additionally, since our parametrization provides a trade-off between expressivity and parameter-efficiency, it might be unable to represent some particular orthogonal matrices, which might be required in other settings apart from parameter-efficient fine-tuning. ", "page_idx": 9}, {"type": "text", "text": "10 Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The article was prepared within the framework of the HSE University Basic Research Program. This research was supported in part through computational resources of HPC facilities at HSE University [Kostenetskiy et al., 2021]. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Cem Anil, James Lucas, and Roger Grosse. Sorting out Lipschitz function approximation. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 291\u2013301. PMLR, 09\u201315 Jun 2019. URL https://proceedings.mlr.press/v97/anil19a. html.   \nMartin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks. In International conference on machine learning, pages 1120\u20131128. PMLR, 2016.   \nBeidi Chen, Tri Dao, Kaizhao Liang, Jiaming Yang, Zhao Song, Atri Rudra, and Christopher Re. Pixelated butterfly: Simple and efficient sparse training for neural network models. In International Conference on Learning Representations (ICLR), 2022.   \nTri Dao, Beidi Chen, Nimit S Sohoni, Arjun Desai, Michael Poli, Jessica Grogan, Alexander Liu, Aniruddh Rao, Atri Rudra, and Christopher Re. Monarch: Expressive structured matrices for efficient and accurate training. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 4690\u2013 4721. PMLR, 17\u201323 Jul 2022. URL https://proceedings.mlr.press/v162/dao22a.html.   \nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. Advances in Neural Information Processing Systems, 36, 2024.   \nAli Edalati, Marzieh Tahaei, Ivan Kobyzev, Vahid Partovi Nia, James J Clark, and Mehdi Rezagholizadeh. Krona: Parameter efficient tuning with kronecker adapter. arXiv preprint arXiv:2212.10650, 2022.   \nDan Fu, Simran Arora, Jessica Grogan, Isys Johnson, Evan Sabri Eyuboglu, Armin Thomas, Benjamin Spector, Michael Poli, Atri Rudra, and Christopher R\u00e9. Monarch mixer: A simple sub-quadratic gemm-based architecture. Advances in Neural Information Processing Systems, 36, 2024.   \nRinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022.   \nLigong Han, Yinxiao Li, Han Zhang, Peyman Milanfar, Dimitris Metaxas, and Feng Yang. Svdiff: Compact parameter space for diffusion fine-tuning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7323\u20137334, 2023.   \nKyle Helfrich, Devin Willmott, and Qiang Ye. Orthogonal recurrent neural networks with scaled cayley transform. In International Conference on Machine Learning, pages 1969\u20131978. PMLR, 2018.   \nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In International conference on machine learning, pages 2790\u20132799. PMLR, 2019.   \nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id= nZeVKeeFYf9.   \nStephanie Hyland and Gunnar R\u00e4tsch. Learning unitary operators with help from u (n). In Proceedings of the AAAI Conference on Artificial Intelligence, volume 31, 2017.   \nRabeeh Karimi Mahabadi, James Henderson, and Sebastian Ruder. Compacter: Efficient low-rank hypercomplex adapter layers. Advances in Neural Information Processing Systems, 34:1022\u20131035, 2021.   \nPS Kostenetskiy, RA Chulkevich, and VI Kozyrev. HPC resources of the higher school of economics. In Journal of Physics: Conference Series, volume 1740, page 012050, 2021.   \nNupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1931\u20131941, 2023.   \nVadim Lebedev, Yaroslav Ganin, Maksim Rakhuba, Ivan V. Oseledets, and Victor S. Lempitsky. Speeding-up convolutional neural networks using fine-tuned cp-decomposition. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http: //arxiv.org/abs/1412.6553.   \nBrian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691, 2021.   \nQiyang Li, Saminul Haque, Cem Anil, James Lucas, Roger B Grosse, and J\u00f6rn-Henrik Jacobsen. Preventing gradient attenuation in lipschitz constrained convolutional networks. Advances in neural information processing systems, 32, 2019.   \nXiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4582\u20134597, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.353. URL https://aclanthology.org/2021.acl-long.353.   \nYixiao Li, Yifan Yu, Chen Liang, Pengcheng He, Nikos Karampatziakis, Weizhu Chen, and Tuo Zhao. Loftq: Lora-fine-tuning-aware quantization for large language models, 2023.   \nShih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang, KwangTing Cheng, and Min-Hung Chen. Dora: Weight-decomposed low-rank adaptation. arXiv preprint arXiv:2402.09353, 2024a.   \nWeiyang Liu, Zeju Qiu, Yao Feng, Yuliang Xiu, Yuxuan Xue, Longhui Yu, Haiwen Feng, Zhen Liu, Juyeon Heo, Songyou Peng, Yandong Wen, Michael J. Black, Adrian Weller, and Bernhard Sch\u00f6lkopf. Parameter-efficient orthogonal finetuning via butterfly factorization. In The Twelfth International Conference on Learning Representations, 2024b. URL https://openreview. net/forum?id=7NzgkEdGyr.   \nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.   \nSourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, Sayak Paul, and Benjamin Bossan. Peft: State-of-the-art parameter-efficient fine-tuning methods. https://github.com/ huggingface/peft, 2022.   \nFanxu Meng, Zhaohui Wang, and Muhan Zhang. Pissa: Principal singular values and singular vectors adaptation of large language models, 2024.   \nAlexander Novikov, Dmitrii Podoprikhin, Anton Osokin, and Dmitry P Vetrov. Tensorizing neural networks. Advances in neural information processing systems, 28, 2015.   \nBernd Prach, Fabio Brau, Giorgio Buttazzo, and Christoph H Lampert. 1-lipschitz layers compared: Memory speed and certifiable robustness. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 24574\u201324583, 2024.   \nZeju Qiu, Weiyang Liu, Haiwen Feng, Yuxuan Xue, Yao Feng, Zhen Liu, Dan Zhang, Adrian Weller, and Bernhard Sch\u00f6lkopf. Controlling text-to-image diffusion by orthogonal finetuning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https: //openreview.net/forum?id=K30wTdIIYc.   \nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International conference on machine learning, pages 8821\u20138831. Pmlr, 2021.   \nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical textconditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022.   \nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684\u201310695, 2022.   \nNataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22500\u201322510, 2023.   \nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:36479\u201336494, 2022.   \nSahil Singla and Soheil Feizi. Skew orthogonal convolutions. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 9756\u20139766. PMLR, 18\u201324 Jul 2021. URL https://proceedings.mlr.press/v139/singla21a.html.   \nSahil Singla, Surbhi Singla, and Soheil Feizi. Improved deterministic l2 robustness on cifar-10 and cifar-100. arXiv preprint arXiv:2108.04062, 2021.   \nYoad Tewel, Rinon Gal, Gal Chechik, and Yuval Atzmon. Key-locked rank one editing for text-toimage personalization. In ACM SIGGRAPH 2023 Conference Proceedings, pages 1\u201311, 2023.   \nEugene Vorontsov, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. On orthogonality and learning recurrent networks with long term dependencies. In International Conference on Machine Learning, pages 3570\u20133578. PMLR, 2017.   \nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018.   \nYuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. Elite: Encoding visual concepts into textual embeddings for customized text-to-image generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 15943\u201315953, 2023.   \nXiaojun Xu, Linyi Li, and Bo Li. Lot: Layer-wise orthogonal training on improving l2 certified robustness. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 18904\u201318915. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/ file/77d52754ff6b2de5a5d96ee921b6b3cd-Paper-Conference.pdf.   \nYifan Yang, Jiajun Zhou, Ngai Wong, and Zheng Zhang. Loretta: Low-rank economic tensor-train adaptation for ultra-low-parameter fine-tuning of large language models, 2024.   \nQingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo Zhao. Adaptive budget allocation for parameter-efficient fine-tuning. In The Eleventh International Conference on Learning Representations, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. Shufflenet: An extremely efficient convolutional neural network for mobile devices. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6848\u20136856, 2018. Yufan Zhou, Ruiyi Zhang, Tong Sun, and Jinhui Xu. Enhancing detail preservation for customized text-to-image generation: A regularization-free approach. arXiv preprint arXiv:2305.13579, 2023. ", "page_idx": 13}, {"type": "text", "text": "A Related work ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Parameter-Efficient Fine-Tuning (PEFT) With the growth of model sizes, end-to-end training became unavailable for those who want to adapt powerful architectures for specific tasks, as even full fine-tuning became too expensive. This problem sparked research in the direction of parameterefficient fine-tuning methods, including methods that focus on prompt tuning [Lester et al., 2021, Li and Liang, 2021] and adapter tuning (e.g. [Houlsby et al., 2019, Karimi Mahabadi et al., 2021]), which include LoRA [Hu et al., 2022] and its variations [Meng et al., 2024, Zhang et al., 2023, Liu et al., 2024a, Dettmers et al., 2024, Li et al., 2023], that inject learnable low-rank matrices as an additive injection to the weights of pretrained models. OFT [Qiu et al., 2023], BOFT [Liu et al., 2024b] and our method use similar approach to LoRA, but learn multiplicative injection rather than an additive one. ", "page_idx": 14}, {"type": "text", "text": "Structured sparsity Structured sparsity is an approach that replaces dense weight layers with different structured ones, such as matrix factorizations or tensor decompositions in order to compress or speed-up models [Dao et al., 2022, Chen et al., 2022, Novikov et al., 2015, Lebedev et al., 2015]. Some of these techniques were also adapted to PEFT methods in works like [Karimi Mahabadi et al., 2021, Edalati et al., 2022, Yang et al., 2024] or BOFT [Liu et al., 2024b] method, that utilizes a variation of butterfly matrices as a parametrization for parameter-efficient orthogonal matrices, imposing orthogonality to each butterfly factor. See details in Section 2. Monarch matrices [Dao et al., 2022, Fu et al., 2024] are most relevant to our work as our proposed matrix class is their generalization that utilizes similar structure. ", "page_idx": 14}, {"type": "text", "text": "Subject-driven generation The emergence of large text-to-image models [Ramesh et al., 2022, 2021, Saharia et al., 2022, Rombach et al., 2022] has propelled the advancement of personalized generation techniques in the research field. Customizing a text-to-image model to generate specific concepts based on multiple input images presents a key challenge. Various methods [Ruiz et al., 2023, Gal et al., 2022, Kumari et al., 2023, Han et al., 2023, Qiu et al., 2023, Zhou et al., 2023, Wei et al., 2023, Tewel et al., 2023] have been proposed to address this challenge, requiring either extensive fine-tuning of the model as a whole [Ruiz et al., 2023] or specific parts [Kumari et al., 2023] to accurately reconstruct concept-related training images. While this facilitates precise learning of the input concept, it also raises concerns regarding overfitting, potentially limiting the model\u2019s flexibility in generating diverse outputs in response to different textual prompts. Efforts to mitigate overftiting and reduce computational burden have led to the development of lightweight parameterization techniques [Qiu et al., 2023, Liu et al., 2024b, Hu et al., 2022, Tewel et al., 2023, Han et al., 2023] such as those proposed among others. These methods aim to preserve editing capabilities while sacrificing some degree of concept fidelity. The primary objective is to identify parameterization strategies that enable high-quality concept learning without compromising the model\u2019s ability to edit and generate variations of the concept. Our investigation indicates that the orthogonal parameterization approach we propose represents a significant step towards achieving this goal. ", "page_idx": 14}, {"type": "text", "text": "Orthogonal transforms In papers [Vorontsov et al., 2017, Hyland and R\u00e4tsch, 2017, Helfrich et al., 2018] authors work in the setting of dense matrices and try to parameterize essentially the whole manifold of orthogonal matrices. For $n\\,\\times\\,n$ matrices it requires computing inverses or exponential maps of $n\\times n$ matrices at every optimization step. This takes ${\\mathcal{O}}(n^{3})$ time, which can be computationally challenging for larger architectures. Moreover, such a parametrization utilizes ${\\mathcal{O}}(n^{2})$ trainable parameters, which makes it inapplicable for PEFT (see the OFT paper [Qiu et al., 2023] for more details). Our proposed method is different in a sense that it provides a trade-off between expressivity (describing only a subset of the orthogonal manifold) and efficiency. ", "page_idx": 14}, {"type": "text", "text": "In [Li et al., 2019, Singla et al., 2021] authors discuss main issues of bounding of Lipschitz constant of neural networks and provide Gradient-Norm-Preserving (GNP) architecture in order to avoid vanishing of gradients while bounding Lipschitz constant. The authors propose a specific convolutional layer (Block Convolutional Orthogonal Parametrization) which Jacobian is orthogonal, also providing orthogonal activations with Lipschitz constant equal to 1. These constraints guarantee that the norm of the gradient will not change through backward pass. In other works [Singla and Feizi, 2021, Singla et al., 2021] authors provide a modification of the orthogonal convolutions (Skew Orthogonal Convolution) in terms of hardware-efficiency. Authors provide neural network architecture where each layer is 1-Lipschitz and make a comparison between these two convolutional layers. The work [Li et al., 2019] was outperformed by the SOC method that we utilize (there is a comparison in the SOC method [Singla and Feizi, 2021]). In [Xu et al., 2022], the authors utilize periodicity of convolution and their padding is not equivalent to a widely-used zero-padded convolution. Survey [Prach et al., 2024] suggests that [Xu et al., 2022] and other 1-Lipschitz architectures yield worse robustness metrics than SOC, which we use as a baseline in our paper. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "B Proof of Prop. 1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof. Let $R^{\\prime}=P R$ . $R^{\\prime}$ can be viewed as a block matrix with $k_{L}\\times k_{R}$ blocks of sizes $b_{2}^{L}\\times b_{2}^{R}$ . $L$ can be viewed as a block matrix with $k_{L}\\times k_{L}$ blocks from which only diagonal are non-zero. The $A$ can be written in the following form: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left(\\begin{array}{c c c c}{A_{0,0}}&{\\cdots}&{A_{0,k_{R}-1}}\\\\ {\\vdots}&{\\ddots}&{\\vdots}\\\\ {A_{k_{L}-1,0}}&{\\cdots}&{A_{k_{L}-1,k_{R}-1}}\\end{array}\\right)=\\left(\\begin{array}{c c c c}{L_{0}}&{\\cdots}&{0}\\\\ {\\vdots}&{\\ddots}&{\\vdots}\\\\ {0}&{\\cdots}&{L_{k_{L}-1}\\AA}\\end{array}\\right)\\left(\\begin{array}{c c c c}{R_{0,0}^{\\prime}}&{\\cdots}&{R_{0,k_{R}-1}^{\\prime}}\\\\ {\\vdots}&{\\ddots}&{\\vdots}\\\\ {R_{k_{L}-1,0}^{\\prime}}&{\\cdots}&{R_{k_{L}-1,k_{R}-1}^{\\prime}}\\end{array}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Using block matrix product formulas, we get: ", "page_idx": 15}, {"type": "equation", "text": "$$\nA_{k_{1},k_{2}}=L_{k_{1}}R_{k_{1},k_{2}}^{\\prime}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We can now rewrite $L_{k_{1}}R_{k_{1},k_{2}}^{\\prime}$ product in terms of their columns and rows: ", "page_idx": 15}, {"type": "equation", "text": "$$\nL_{k_{1}}R_{k_{1},k_{2}}^{\\prime}=\\left(l_{1}\\ldots l_{b_{L}^{2}}\\right)\\cdot\\left(\\begin{array}{c}{{r_{1}^{\\top}}}\\\\ {{\\vdots}}\\\\ {{r_{b_{L}^{2}}^{\\top}}}\\end{array}\\right)=\\sum_{t}{l_{t}r_{t}^{\\top}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Columns of $L_{k_{1}}$ are just vectors $u_{j}$ such that $\\begin{array}{r}{\\left\\lfloor\\frac{j}{k_{L}}\\right\\rfloor=k_{1}}\\end{array}$ . Let us examine the rows of $R_{k_{1},k_{2}}^{\\prime}$ . Since $R^{\\prime}$ is a matrix formed by permuting the rows of block-diagonal matrix $R$ , $R_{k_{1},k_{2}}^{\\prime}$ can only contain rows that were in the $R_{k_{2}}$ before permutation. Formally, this means that $R_{k_{1},k_{2}}^{\\prime}$ can only contain vector-rows $v_{i}^{T}$ such that $\\lfloor\\frac{i}{k_{R}}\\rfloor\\,=\\,k_{2}$ . Additionally, rows after permutation should get into the $k_{1}$ -block row. That implies $\\begin{array}{r}{\\left\\lfloor\\frac{\\sigma(i)}{k_{L}}\\right\\rfloor=k_{1}}\\end{array}$ . Other rows of $R_{k_{1},k_{2}}^{\\prime}$ are zero-rows. Notice that in (4) non-zero rows $r_{t}^{\\top}$ represented by $v_{i}^{\\top}$ will match exactly with columns $u_{\\sigma(i)}$ that represent $l_{t}$ . Keeping only non-zero terms in $\\textstyle\\sum_{t}l_{t}r_{t}^{\\top}$ gets us to the desired conclusion. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "C Comparison of Monarch matrices and $\\mathcal{G}S$ -matrices ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "$\\mathcal{G}S(P_{L},P,P_{r})$ class is inspired by Monarch matrices [Dao et al., 2022] and their primary goal is to introduce additional flexibility in the block structure of matrices $L$ and $R$ . Generalized Monarch matrices are parameterized as $P_{1}L P_{2}R$ , where $L$ and $R$ are block-diagonal matrices and $P_{1}$ and $P_{2}$ are certain permutations defined in Definition 5.2. This resembles $\\bar{\\mathcal{G}}S(P_{1},P_{2},I)$ matrix class, however in comparison monarch matrices have additional hard constraints on relation between $k_{L}$ and $k_{R}$ . Being more precise, Monarch matrices are a special case of $\\mathcal{G}S(P_{1},P_{2},I)$ matrices with additional constraints ${\\dot{k}}_{L}=b_{R}^{1}$ , $k_{R}=b_{L}^{2}$ . Such constraints lead to several theoretical and practical limitations of Monarch matrices. From theoretical point of view, Monarch matrices can only describe permuted block matrices with blocks of ranks 1. In contrast $\\mathit{g s}$ -matrices with can describe matrices with different rank structure of blocks (including structures where rank of each block is equal to arbitrary $r$ ). From practical point of view, due to this constraint Monarch matrices are often unable to form a desirable block structure of matrices $L$ and $R$ . For demonstration of this phenomena, consider a case of square matrices with square blocks \u2013 the structure needed in Orthogonal fine-tuning paradigm. Formally, we have $b_{L}^{1}=b_{L}^{2}=\\dot{b}_{L}$ , $b_{R}^{1}=b_{2}^{R}=b_{R}$ , $m=n$ . Additional Monarch constraint would mean that $b_{R}=k_{L}$ ; $b_{L}=k_{R}$ . This in turn means that $k_{L}\\cdot k_{R}=n$ . As we can see, it makes impossible to stack two matrices with small number of blocks (say, 4) or large number of blocks, which is required in situations with low parameter budget. In contrast, $\\mathit{g s}$ parametrization allows for both of these structures, which we use in our experiments. ", "page_idx": 15}, {"type": "text", "text": "Note, that the work [Fu et al., 2024] provides a slightly different definition for monarch matrices, introducing order- $\\cdot p$ Monarch matrices. These matrices are also a special case of $\\mathit{g s}$ class, however they are very restrictive as they can only parametrize matrices with both sides equal to $a^{p}$ for some integers $a,p$ . ", "page_idx": 15}, {"type": "image", "img_path": "7EQx56YSB2/tmp/1467095fabb2c316430e3835f00dabc8e5fc9631af83495f8ad395c78f3950bd.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure 5: Demonstration of information transition through a block structure. Each node is connected to exactly $b$ consecutive nodes from the next level. ", "page_idx": 16}, {"type": "text", "text": "D Proof of Theorem 2 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We use information transition framework from [Liu et al., 2024b], representing product of $m$ sparse $d\\times d$ matrices as an transmitting information in a grid with $d\\times(m+1)$ nodes. Edges between nodes $j$ and $i$ represent that element $i,j$ in sparse matrix is non-zero. Element $i,j$ from final matrix can only be non-zero if there exists a path from the $j$ -th node from the right column to the $i$ -th node in the left column (see Figure 5). ", "page_idx": 16}, {"type": "text", "text": "Proof. Consider an information transmission graph for the matrix $B_{i}P_{(r,b r)}$ . In this graph, the first node connects with $b$ first edges, the second node connects with the edges from $b+1$ to $2b$ and so on. Now consider a graph for the product of $m$ such matrices. As shown in Figure 5, now each node from the first level has paths to $b^{k}$ unique nodes from the $k$ th-th level. It means that using $m=\\lceil\\log_{b}(d)\\rceil=\\lceil\\log_{b}(b r)\\rceil\\stackrel{\\cdot}{=}1+\\lceil\\log_{b}(r)\\rceil$ matrices is sufficient to reach all nodes and therefore form a dense matrix. Note that the number of paths for each node is always equal to $b^{m}$ regardless of permutation choice. This observation shows that it is impossible to reach $d$ unique elements on the final level with $m<1+\\lceil\\log_{b}(r)\\rceil$ . \u53e3 ", "page_idx": 16}, {"type": "text", "text": "E Subject-driven generation ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Training details All the models are trained using Adam optimizer with batch size $=4$ , learning rate $=0.00002$ , betas $=(0.9,0.999)$ and weight decay $=0.01$ . The Stable Diffusion-2-base model is used for all experiments. ", "page_idx": 16}, {"type": "text", "text": "Evaluation details We use the DreamBooth dataset for evaluation. The dataset contains 25 different contextual prompts for 30 various objects including pets, toys and furnishings. For each concept we generate 10 images per contextual prompt and 30 images per base prompt \u201ca photo of an $\\mathbf{S}^{\\ast\\ast}$ , resulting in 780 unique concept-prompt pairs and a total of 8400 images for fair evaluation. ", "page_idx": 16}, {"type": "text", "text": "To measure concept fidelity, we use the average pairwise cosine similarity (IS) between CLIP ViTB/32 embeddings of real and generated images as in [Gal et al., 2022]. This means that the image similarity is calculated using only the base prompt, i.e. \u201ca photo of an $S^{*^{\\flat}}$ . Higher values of this metric usually indicate better subject fidelity, while keeping this evaluation scene-independent. To evaluate the correspondence between generated images and contextual prompts (TS), the average cosine similarity between CLIP ViTB/32 embeddings of the prompt and generated images [Ruiz et al., 2023, Gal et al., 2022]. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "Overfitting discussion Typically, the more parameters are trained, the more easily the model overftis. This results in higher image similarity and lower text similarity. In addition, if a model with a large number of training parameters is trained for a long time, the image similarity can often start to decrease as the model starts to collapse and artifacts start to appear. This sometimes leads to models with more trainable parameters having a worse score in both image and text similarity than the models with fewer ones. ", "page_idx": 17}, {"type": "text", "text": "Models with fewer trainable parameters overfti less, but require longer training to capture the concept carefully, and usually have an upper limit on the maximum image similarity: at some point the increase in image similarity becomes small, while text similarity starts to decrease dramatically. Therefore, the very common result of a usual fine-tuning is either an overfitting with poor context preservation or an undertraining with poor concept fidelity. The orthogonal fine-tuning shows a different behavior. The model with less trainable parameters can be trained longer (GSOFT, OFT, BOFT) and capture the concept more carefully without artifacts. At the same time, it overftis less and shows higher text similarity. ", "page_idx": 17}, {"type": "text", "text": "Additional results In Figures 6 we show a graphical representation of the metrics for 1000 and 3000 iterations. Examples of generation for different methods are presented in Figure 7, 8. ", "page_idx": 17}, {"type": "text", "text": "F GS Orthogonal Convolution ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we provide some details and insights about the choice of the ChShuffle permutation and the activation function. ", "page_idx": 17}, {"type": "text", "text": "In experiments, we apply the ChShuffle operation right before grouped convolutional layers. Stacking several layers of that form resembles higher-order $\\mathit{g s}$ -matrices, which motivates the usage of permutations from Definition 5.2 for optimal information transition (see Appendix D). However, in the LipConvnet architecture, the activation function can also shuffle information between channels. Thus, this additional shuffilng of information can negatively affect our information transition properties. In the original SOC paper [Singla and Feizi, 2021], the authors use MaxMin activation, firstly proposed in [Anil et al., 2019]. ", "page_idx": 17}, {"type": "text", "text": "Definition F.1. [Singla and Feizi, 2021] Given a feature tensor $X\\in\\mathbb{R}^{2m\\times n\\times n}$ , the MaxMin $(X)$ activation of a tensor $X$ is defined as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A=X_{:m,:,:},\\ B=X_{m:,:,:},}\\\\ &{M a x M i n(X)_{:m,:,:}=m a x(A,B),}\\\\ &{M a x M i n(X)_{m:,:,:}=m i n(A,B).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This activation shuffles information between different groups in convolution which harms performance of our experiments, as permutations that we use in ChShuffle become sub-optimal in terms of information transmission. Thus, we introduce a modification of MaxMin activation, that splits channels into pairs in a different way. Rather than constructing pairs from different halves of input tensor, we use neighboring channels for forming of pairs (first channel pairs with second, third with fourth and so on). With this modification information does not transfer between groups during activations, which enables more optimal information transmission in-between layers with ChShuffle operator. In further experiments we denote this activation function as MaxMinPermuted and define it below: ", "page_idx": 17}, {"type": "image", "img_path": "7EQx56YSB2/tmp/e8e2f529347e2e848f1442652fb258d7e022287031f2875b2ce18251d3d8aaff.jpg", "img_caption": ["Figure 6: Image and text similarity visualisation for different methods on subject-driven generation. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "7EQx56YSB2/tmp/4b857d9546ac3a55322053bd047d0bc462783d34194ebf2494286526d8ba2de2.jpg", "img_caption": ["Figure 7: Subject-driven generation visual results on 3000 training iterations. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "Definition F.2. Given a feature map $X\\in\\mathbb{R}^{2m\\times n\\times n}$ . MaxMinPermuted $\\operatorname{\\mathrm{/}}(X)$ is defined as follows: ", "page_idx": 18}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\nA=X_{::2,:,},B=X_{1::2,:,},\n$$$$\n\\begin{array}{r l}&{\\boldsymbol{A}=\\boldsymbol{A}_{::2,:,},\\boldsymbol{D}=\\boldsymbol{A}_{1::2,:,:},}\\\\ &{\\boldsymbol{M}\\boldsymbol{a}\\boldsymbol{x}\\boldsymbol{M}\\boldsymbol{i}\\boldsymbol{n}\\boldsymbol{P}e r m{\\boldsymbol{u}}t e d(\\boldsymbol{X})_{::2,:,:}=\\boldsymbol{m}\\boldsymbol{a}\\boldsymbol{x}(\\boldsymbol{A},\\boldsymbol{B}),}\\\\ &{\\boldsymbol{M}\\boldsymbol{a}\\boldsymbol{x}\\boldsymbol{M}\\boldsymbol{i}\\boldsymbol{n}\\boldsymbol{P}e r m\\boldsymbol{u}t e d(\\boldsymbol{X})_{1::2,:,:}=\\boldsymbol{m}\\boldsymbol{i}\\boldsymbol{n}(\\boldsymbol{A},\\boldsymbol{B})}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "However, we also empirically find that it is crucial for the channels that interact within activations functions to also interact during convolutions. This means that they should always stay in the same group. This motivates us to use a slightly different permutation for the ChShuffle operation, which permutes channels in pairs. We use the following permutation ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sigma(i)_{(k,n)}^{p a i r e d}=\\left(\\left\\lfloor\\frac{i}{2}\\right\\rfloor\\bmod k\\right)\\cdot\\frac{n}{k}+2\\cdot\\left\\lfloor\\frac{i}{2k}\\right\\rfloor+(i\\bmod2)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "image", "img_path": "7EQx56YSB2/tmp/3ab0c4ce8191b6b069c96fb9af9629d7bc1fc9b61d9052a87f51ff876d5ed4eb.jpg", "img_caption": ["Figure 8: Subject-driven generation visual results on 1000 training iterations. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Table 4: Comparison of activations on LipConvnet-15 architecture and CIFAR-100. $(a,b)$ in \u201cGroups\u201d column denotes that we have two grouped exponential convolutions (the first one with kernel_size $=$ 3, the second with kernel $\\_s i z e\\,=\\,1]$ ). If $b$ is not mentioned, we have only one $g{s}$ orthogonal convolutional layer. ", "page_idx": 20}, {"type": "table", "img_path": "7EQx56YSB2/tmp/0e69b46a3ca08fdd55d45b024bb10c78bbe5b4f8c2ecb86e5e558fb627d569f3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "It can be seen that using \u201cpaired\u201d permutation used with MinMaxPermuted activation significantly improves quality metrics. ", "page_idx": 20}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Yes, all the assumptions claimed in abstract and introduction were discussed in our paper. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: All the limitations of the work are discussed in Section 9. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Yes, we prove all theorems which were introduced in our paper or we have all the necessary references to theory we used. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Yes, we provide all the necessary information to reproduce the results Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 22}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Yes, we provide a link to github repo in Section 7. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 23}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Yes, in Section 7 we provide all the necessary details. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 23}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [No] ", "page_idx": 23}, {"type": "text", "text": "Justification: We conducted the experiments within limited computational resources and demanding settings. Nevertheless, we plan to add error bars to some of the experiments on convolution NNs. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Yes, we wrote the information about the resources used for experiments in Section 7. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Yes, our research conforms with the NeurIPS Code of Ethics. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 24}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: There is no societal impact of the work performed. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: Our paper poses no such risks. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 25}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Yes, we cite all papers appeared in the text. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 25}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 26}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: We don\u2019t release any new assets in our paper. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 26}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: In our paper there is no crowdsourcing experiments. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 26}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 26}]