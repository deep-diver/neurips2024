[{"type": "text", "text": "Global Lyapunov functions: a long-standing open problem in mathematics, with symbolic transformers ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Alberto Alfarano\u2217 Fran\u00e7ois Charton\u2217 FAIR, Meta FAIR, Meta \u2013 CERMICS, Ecole des Ponts albealfa@meta.com fcharton@meta.com ", "page_idx": 0}, {"type": "text", "text": "Amaury Hayat\u2217 CERMICS, Ecole des Ponts \u2014 Institut Polytechnique de Paris amaury.hayat@enpc.fr ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Despite their spectacular progress, language models still struggle on complex reasoning tasks, such as advanced mathematics. We consider a long-standing open problem in mathematics: discovering a Lyapunov function that ensures the global stability of a dynamical system. This problem has no known general solution, and algorithmic solvers only exist for some small polynomial systems. We propose a new method for generating synthetic training samples from random solutions, and show that sequence-to-sequence transformers trained on such datasets perform better than algorithmic solvers and humans on polynomial systems, and can discover new Lyapunov functions for non-polynomial systems. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "As large language models achieve human-level performance over a broad set of tasks [4, 35, 45], their capability to reason becomes a focus of discussion and research. There is no single definition of reasoning, and work in this area encompasses factuality, real world alignment, compositionality, the discovery and following of rules, &c. Still, mathematics are considered as one of the purest, and most demanding, forms of reasoning [17]. As such, solving research-level mathematical problems is a major milestone in demonstrating the reasoning capabilities of language models. Such an advance in AI would also transform mathematical practice. ", "page_idx": 0}, {"type": "text", "text": "There is little research on applying language models to open problems of mathematics. Except a few papers on combinatorial optimization and graph theory [34, 39], most prior works focus on problems with known solutions [37, 23, 30, 8]. We believe this lack of results is due to two main reasons. First, research problems may require specialized work by mathematicians [6] before they can be handed to language models. Second, most math transformers are trained on sets of problems and solutions which are hard to generate in the case of open problems, when no generic method for finding a solution is known. ", "page_idx": 0}, {"type": "text", "text": "In this paper, we focus on a long-standing, yet easy to formalize, open problem in mathematics: discovering the Lyapunov functions that control the global stability of dynamical systems \u2013 the boundedness of their solutions when time goes to infinity with respect to an equilibrium or an orbit. A famous instance of this problem is the three-body problem: the long-term stability of a system of three celestial bodies subjected to gravitation. The stability problem was studied by Newton, Lagrange and Poincar\u00e9. Lyapunov discovered that stability is guaranteed if an entropy-like function for the system \u2013the Lyapunov function\u2013 can be found. Unfortunately, no method is known for deriving Lyapunov functions in the general case, and Lyapunov functions are only known for a small number of systems. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "We propose a new technique for generating training data from randomly sampled Lyapunov functions. Sequence-to-sequence transformers trained on these datasets achieve near perfect accuracy $(99\\%)$ on held-out test sets, and very high performance $(73\\%)$ on out-of-distribution test sets. We show that higher accuracies $(84\\%)$ can be achieved by enriching the training set with a small number (300) of easier examples that can be solved with existing algorithmic methods. These enriched models greatly outperform state-of-the-art techniques and human performance on a variety of benchmarks. ", "page_idx": 1}, {"type": "text", "text": "Finally, we test the capability of our models to discover yet unknown Lyapunov functions on randomly generated systems. On polynomial systems, the only ones current methods can solve, our models find Lyapunov function for $\\mathrm{i0.i\\%}$ or systems, vs $2.1\\%$ for state-of-the-art techniques. On non-polynomial systems, where no algorithm is known, our best models discover new Lyapunov functions for $12.7\\%$ of systems. Our research demonstrates that generative models can be used to solve research-level problems in mathematics, by providing mathematicians with guesses of possible solutions. The solutions proposed by the black-box model are explicit and their mathematical correctness can be verified. We believe this research is an AI-driven blueprint for solving open problems in mathematics. ", "page_idx": 1}, {"type": "text", "text": "Related works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Most classical methods for finding Lyapunov rely on parameterized families of candidate solutions, and attempt to derive conditions on the parameters [11, 14]. Additional techniques such as backstepping or forwarding [11, Chap. 12], were introduced to leverage the specifics of particular systems (see also physics-based methods [44]). These techniques are limited to specific, or simple, systems. The global Lyapunov functions of polynomial systems that are sums of squares of polynomials of given degree can be found by computational-intensive algorithmic tools, such as SOSTOOLS [32, 33], which leverage the fact that the Lyapunov function belongs to a finite-dimensional space. ", "page_idx": 1}, {"type": "text", "text": "Methods involving neural networks have been proposed in recent years [7, 15, 12, 24, 25]. They train feed-forward networks to approximate Lyapunov functions of a given system, and use a Satisfiability Modulo Theories (SMT) solver as a verifier which proposes potential counter-examples. This approach, very different from ours, was shown to be successful for several well-studied high dimensional systems. However, it only finds local or semi-global Lyapunov functions (see Definition A.3). Since the Lyapunov functions that are found are implicit, it would be hard for mathematicians to check whether they are global Lyapunov functions or not. Semi-global Lyapunov functions are useful in many engineering fields such as robotics, where one wants a system to be robust to small perturbations. In other fields, like epidemics, being resilient to large perturbations is central, and global Lyapunov functions are required. ", "page_idx": 1}, {"type": "text", "text": "Transformers trained on synthetic datasets have been proposed for many problems of mathematics, including arithmetic [28], linear algebra [10], symbolic integration [22], symbolic regression [5], Shortest Vector Problem [40], Gr\u00f6bner basis computation [19] and theorem proving [30]. [8] investigate a problem related to ours: the local stability of dynamical systems. Different architectures were used to solve hard problems in combinatorial optimisation [34], and graph theory [39]. ", "page_idx": 1}, {"type": "text", "text": "2 System stability and Lyapunov functions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "The stability of dynamical systems is a hard mathematical question, which intrigued many generations of mathematicians, from Newton and Lagrange in the 18th century, to Poincar\u00e9 in the 20th in the context of the three-body problem. The main mathematical tool for assessing stability was proposed by Lyapunov, who showed in 1892 that a system is stable if a decreasing entropy-like function \u2013the Lyapunov function\u2013 can be found [20, 11, 26]. Later, the existence of a Lyapunov function was shown to be a necessary condition for the stability of large classes of systems [29, 27, 18]. Unfortunately, these very strong results provide no clue on how to find Lyapunov functions, or just proving their existence for a particular system. In fact, 130 years later, systematic derivations of global Lyapunov functions are only known in a few special cases, and their derivation in the general case remains a well-known open problem. ", "page_idx": 1}, {"type": "text", "text": "In mathematical terms, we consider the dynamical system ", "page_idx": 1}, {"type": "equation", "text": "$$\n{\\dot{x}}=f(x),\n$$", "text_format": "latex", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{\\begin{array}{l l}{\\dot{x}_{0}=-7x_{0}^{5}-4x_{0}^{3}x_{1}^{2}-5x_{0}^{3}}\\\\ {\\dot{x}_{1}=7x_{0}^{4}-3x_{1}-2x_{2}}\\\\ {\\dot{x}_{2}=-8x_{0}^{2}-9x_{2}}\\\\ {V(x)=2x_{0}^{4}+2x_{0}^{2}x_{1}^{2}+3x_{0}^{2}+2x_{1}^{2}+x_{2}^{2}}\\\\ {\\quad\\ \\ \\ \\ \\ \\ \\left\\{\\dot{x}_{0}\\ =\\ -x_{0}+x_{0}x_{1}\\right.}\\\\ {\\dot{x}_{1}\\ =-x_{1}}\\\\ {V(x)=\\ln(1+5x_{0}^{2})+x_{1}^{2}}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "image", "img_path": "kOMrm4ZJ3m/tmp/9d9c230cbce7b9a7f0a8df816de7412aaaf02880ab4cd753b2a0fee2c513d72c.jpg", "img_caption": ["Figure 1: Dynamic of a stable system: trajectories may be complicated but as long as they start in the red ball they remain in the blue ball. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "where $x\\in\\mathbb{R}^{n}$ , $f\\in C^{1}(\\mathbb{R}^{n})$ and $\\begin{array}{r}{\\dot{x}=\\frac{d x}{d t}}\\end{array}$ . We want to know if the system has a stable equilibrium around a point $x^{*}$ such that $f(x^{*})=0$ . We assume, without loss of generality, that $x^{*}=0$ . ", "page_idx": 2}, {"type": "text", "text": "Definition 2.1. The system (1) is stable when, for any $\\varepsilon\\:>\\:0$ , there exists $\\eta\\,>\\,0$ such that, if $\\|x(0)\\|<\\eta$ , the system (1) with initial condition $x(0)$ has a unique solution $x\\in C^{1}([0,+\\infty))$ and ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\|x(t)\\|\\leq\\varepsilon,\\;\\;\\forall\\;t\\in[0,+\\infty).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "In other words, a system is stable if a solution that begins close to the origin $(\\|x(0)\\|<\\eta)$ stays close to the origin at all time $(\\|x(t)\\|\\leq\\varepsilon)$ . Lyapunov proved that the stability is related to the existence of what is now called a Lyapunov function. ", "page_idx": 2}, {"type": "text", "text": "Definition 2.2. The function $V\\in C^{1}(\\mathbb{R}^{n},\\mathbb{R}_{+})$ is said to be a (global) Lyapunov function for the system (1) if the following condition are satisfied ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{V(0)=0,}&{\\underset{||x||\\rightarrow+\\infty}{\\operatorname*{lim}}V(x)=+\\infty,}\\\\ {V(x)>0,}&{\\nabla V(x)\\cdot f(x)\\le0\\;\\mathrm{for}\\;x\\neq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Theorem 2.3 (Lyapunov 1892). If the system (1) has a Lyapunov function, then it is stable. ", "page_idx": 2}, {"type": "text", "text": "In fact, the existence of a Lyapunov function is more powerful and provides additional information. ", "page_idx": 2}, {"type": "text", "text": "Theorem 2.4 (LaSalle, 1961). If the system (1) has a Lyapunov function $V$ , then all the solutions of (1) converge to the largest invariant set of $\\{f(x)\\cdot\\nabla V(x)=0\\}$ . ", "page_idx": 2}, {"type": "text", "text": "In many cases this largest invariant set is reduced to $\\{x^{*}\\,=\\,0\\}$ and the system is said globally asymptotically stable (all solutions converge to the equilibrium, see Appendix A). ", "page_idx": 2}, {"type": "text", "text": "Most dynamical systems are unstable. For instance, the solutions of the simple system ${\\dot{x}}(t)=x(t)$ grow exponentially with time, and the solutions of ${\\dot{x}}(t)=1+x(t)^{2}\\;(x\\in\\mathbb{R})$ always blow up before $t=\\pi$ . No Lyapunov functions can be found for these systems. ", "page_idx": 2}, {"type": "text", "text": "On the other hand, stable systems can have an infinite number of Lyapunov functions. The system ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\left\\{\\dot{x}_{0}(t)=-x_{0}(t)\\right.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "has $V(x)=a_{0}x_{0}^{2}+a_{1}x_{1}^{2}$ as a Lyapunov function for any choice of $a_{0}>0$ and $a_{1}>0$ . ", "page_idx": 2}, {"type": "text", "text": "In the general case, there is no systematic way of discovering a Lyapunov function, or even showing that one exist. Tools exist for small polynomial systems with special \u201csum of squares\u201d (SOS) Lyapunov functions, but they need a lot of resources, do not always find a solution, and fail once the systems involve more than a few variables. ", "page_idx": 2}, {"type": "text", "text": "We also consider a related, but easier, problem: finding nontrivial $V$ which are semi-definite positive, i.e. $V$ verifying $V(x)\\geq0$ instead of $V(x)>0$ in Equation (3). These functions, called barrier functions, form \u201cbarriers\u201d that divide $\\mathbb{R}^{n}$ into two subspaces. A solution starting inside the barrier must remains in the same subspace, which is an invariant set of the system [31, 41]. For polynomial systems, barrier functions are slightly easier to find using SOS solvers. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "3 Experimental settings ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this work, we train sequence-to-sequence transformers [38] to predict a Lyapunov function for a given system, when it exists. We frame the problem as a translation task: problems and solutions are represented as sequences of symbolic tokens, and the model is trained from generated pairs of systems and Lyapunov functions to minimize the cross-entropy between the predicted sequence and the correct solution. We train transformers with 8 layers, 10 attention heads and an embedding dimension of 640 (ablation studies on different model sizes can be found in Appendix C), on batches of 16 examples, using the Adam optimizer [21] with a learning rate of $10^{-4}$ , an initial linear warm-up phase of 10,000 optimization steps, and inverse square root scheduling. All experiments run on 8 V100 GPU with 32 GB of memory, for 3 or 4 epochs of 2.4 million examples per epoch. Training time is between 12 to 15 hours per GPU. ", "page_idx": 3}, {"type": "text", "text": "Tokenization. Model inputs are systems of the form $({\\dot{x}}_{i}=f_{i}(x_{1},\\dots,x_{n}))_{i\\in\\{1,\\dots,n\\}}$ , represented by the $n$ functions $f_{i}$ . Model outputs are single functions $V(x_{1},\\ldots,x_{n})$ . As in [22], functions are represented as trees, with operators in their internal nodes, and variables or constants as their leaves. Trees are then enumerated in Polish (pre-order) notation to produce sequences of tokens that can be processed by the transformer. ", "page_idx": 3}, {"type": "text", "text": "All operators and variables are tokenized as single symbols (e.g. \u2018cos\u2019 or ${\\bf\\Psi}^{x_{1}}{\\bf\\Psi}^{,}$ ). Integer constants are tokenized as sequences of \u201cdigits\u201d in base 1000 (e.g. 1024 as the sequence $[+,\\ \\ 1\\ ,\\ \\ 24])$ , and real constants, in scientific notation, as pairs of two integers (mantissa and exponent, e.g. $-3.14$ as $[-,314,10^{\\star},-,2]\\rangle$ . For instance: ", "page_idx": 3}, {"type": "image", "img_path": "kOMrm4ZJ3m/tmp/6b95f580e448bd75c39a165edbd46f007da8e22ee0dca4a3d738c772da077d58.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "enumerated as the sequences: $[*,\\cos,*,2.1,x_{0},+,x_{1},2]$ and $[\\sin,+,*,3,x_{1},2]$ , and finally tokenized as $[*,\\cos,*,21,10\\,\\AA^{\\circ},-,1,x_{0},+,x_{1},$ 2, SEP, sin, $+,*,3,x_{1}$ , 2] (using SEP as a separator). ", "page_idx": 3}, {"type": "text", "text": "Evaluation. Trained models are evaluated on sets of stable systems. Since systems have an infinite number of Lyapunov functions, we cannot check the model predictions by comparing them to the solutions from the test set, and need to use an external verifier. For polynomial systems, we verify that there exists a small positive polynomial $P$ such that $-\\nabla V\\cdot f$ and $V-P$ are sum of squares (SOS) of polynomials (with $P=0$ for barrier functions), using a Python solver based on SumOfSquares [43]. For non-polynomial systems, we also use a verifier based on shgo that checks (3) numerically. To further ensure correctness we also verify the symbolic solutions using Satisfiability Modulo Theories (SMT) solvers, relying on dReal [13] for verification through interval analysis. This guarantees that equations (3) hold, at least in a chosen ball around the origin. The performances of the two verifiers (numerical solver and SMT) are similar, a comparison is provided in Table 1. Both the SOS and SMT verifiers sometimes fail to return an answer. In that case, we classify the solution as wrong, even though it might have been correct. As a result, model accuracies may be underestimated. ", "page_idx": 3}, {"type": "text", "text": "Model predictions use beam search with early stopping, normalizing log-likelihood scores by their sequence length. We report results with beam size 1 (greedy decoding) and beam size 50. With beam size 50, we consider the model to be correct if one Lyapunov function is found among the 50 guesses. ", "page_idx": 3}, {"type": "text", "text": "4 Data generation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Our models are trained and tested on large datasets of pairs of stable systems and associated Lyapunov functions. Sampling such stable systems raises two difficulties. First, most dynamical systems are unstable, and no general method exists for deciding whether a system is stable. Second, once a stable system is sampled, there is no general technique for finding a Lyapunov function, except in particular cases. In this paper, we rely on Backward generation [22], sampling solutions and generating associated problems, for the general case, and forward generation, sampling systems and calculating their solutions with a solver, for the tractable polynomial systems of small degree. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "4.1 Backward generation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Backward generation methods, sampling problems from their solutions, are only useful if the model can be prevented from learning to reverse the generation procedure, or from \u201creading\u201d the solutions in the generated problems. For instance, when training a model to solve the hard problem of finding the roots of an integer polynomial [9], one can easily generate a polynomial from its roots, i.e. from the roots $3,5$ and 7, generate the polynomial: ", "page_idx": 4}, {"type": "equation", "text": "$$\nP(X)=2(X^{2}+1)(X-3)(X-5)(X-7).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "However, if the model is trained from factorized form of $P(X)$ , it will learn to read the roots in the problem, instead of computing them. On the other hand, the developed and simplified form ", "page_idx": 4}, {"type": "equation", "text": "$$\nP(X)=2X^{5}-30X^{4}+144X^{3}-240X^{2}+142X-210\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "offers no clues. A second difficulty of backward generation is that sampling solutions instead of problems biases the training distribution. A model trained on backward-generated data may not perform well on a forward-generated test set. Finally, prior work [42] observed that, for hard problems, backward generation methods sometimes focus on easier sub-problems (see, for instance, our comment below about choosing $f=-\\nabla V$ in step 2). ", "page_idx": 4}, {"type": "text", "text": "We propose a procedure for generating a stable system $S$ from a random Lyapunov function $V$ . The rationale is the following. Since $V$ must be positive with a strict minimum in 0, and tend to infinity at infinity ((3)), we first generate $V=V_{\\mathrm{proper}}+V_{\\mathrm{cross}}$ where $V_{\\mathrm{proper}}$ belongs to a class of functions with a guaranteed strict minimum in zero and $V_{\\mathrm{cross}}$ to a larger class of non-negative functions, valued 0 at the origin, but with no guarantee of a strict minimum (step 1 and Appendix B). From $V$ , we need to generate $f$ so that the third condition of (3) is met. A naive solution would be $f=-\\nabla V$ since $f\\cdot\\nabla V\\leq0$ would hold. But this would severely limit the systems we create, and turn the Lyapunov function discovery problem (find $V$ from $f$ ) into an easier integration problem (find $V$ from $-\\nabla V)$ . Instead, starting from $f_{0}=-\\nabla V$ , we apply the following transformations: ", "page_idx": 4}, {"type": "text", "text": "\u2022 multiply each coordinate of $f_{0}$ by random non-negative functions $h_{i}^{2}$ (step 4) and call it $\\tilde{f}_{0}$ . \u2022 generate a random function $\\begin{array}{r}{\\stackrel{}{\\phi}={\\overleftarrow{\\sum}}_{i=1}^{p}g_{i}(x)e^{i}(x)}\\end{array}$ (steps 2 and 3), where $e^{i}$ are orthogonal to $\\nabla V(x)$ , and set $f=\\varphi+\\tilde{f}_{0}$ . We have $\\phi\\cdot\\nabla V=0$ and $\\left(\\phi+\\tilde{f}_{0}\\right)\\cdot\\nabla V\\le0$ . ", "page_idx": 4}, {"type": "text", "text": "These transformations guarantee that all conditions in (3) are met. On the other hand, they allow $f$ to span a very large set of systems, since any $f$ satisfying $\\nabla V(x)\\cdot f(x)\\leq0$ can be written as the sum of a function collinear to $\\nabla V(x)$ and a function orthogonal to $\\nabla V(x)$ . ", "page_idx": 4}, {"type": "text", "text": "Specifically, the procedure can be summarized as follows (see Appendix B for more details). ", "page_idx": 4}, {"type": "text", "text": "Step 1 Generate a random function $V$ , satisfying $V(x)>V(0)$ , $\\forall x\\in\\mathbb{R}^{n}\\setminus\\{0\\}$ , and $V(x)\\to+\\infty$ when $\\|x\\|\\to+\\infty$ .   \nStep 2 Compute the gradient $\\nabla V(x)$ and denote $\\mathcal{H}_{x}=\\{z\\in\\mathbb{R}^{n}\\mid z\\cdot\\nabla V(x)=0\\}$ the hyperplane2 orthogonal to $\\nabla V(x)$ , for any $x\\in\\mathbb{R}^{n}$ .   \nStep 3 Select $1\\leq p\\leq n$ at random and sample $p$ vectors $\\{e^{i}(x)\\}_{i\\in\\{1,...,p\\}}$ from hyperplane Hx. Generate p real-valued functions (gi)i\u2208{1,...,p}.   \nStep 4 Select $1~<~k_{1}~\\leq~n$ at random, generate $k_{1}$ random real-valued functions $(h_{i})_{i\\in\\{1,...,k_{1}\\}}$ , set $h_{i}=0$ for $k_{1}+1\\leq i\\leq n$ .   \nStep 5 Build the $n$ functions ", "page_idx": 4}, {"type": "equation", "text": "$$\nf(x)=-(h_{\\pi(i)}^{2}(x)(\\nabla V)_{i}(x))_{i\\in\\{1,\\dots,n\\}}+\\sum_{i=1}^{p}g_{i}(x)e^{i}(x),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "with $\\pi$ a random permutation of $\\{1,...,n\\}$ . ", "page_idx": 4}, {"type": "text", "text": "Step 6 Simplify the functions $f_{i}$ , obscuring patterns from the generative process. ", "page_idx": 4}, {"type": "table", "img_path": "kOMrm4ZJ3m/tmp/613cc2b29482aaf5f2b67788af374832505a5c43615d4c2d6e1cce99f7eb7301.jpg", "table_caption": [], "table_footnote": ["Table 1: SMT and SOS timeout and error rates, benchmarked on correct Lyapunov functions. "], "page_idx": 5}, {"type": "text", "text": "This method produces a stable system $S:{\\dot{x}}=f(x)$ , with $V$ as its Lyapunov function. The difficulty of inferring $V$ from $S$ hinges on a careful choice of the vectors $e^{i}$ . For instance, if we naively select $e^{i}$ as an orthonormal basis of $\\mathcal{H}_{x}$ , computed from $\\nabla V(x)$ by Gram-Schmidt orthogonalization, prefactors like $1/\\|\\nabla V(x)\\|$ appear at step 3, and are unlikely to simplify away at step 6. This provides the model with a shortcut: reading $\\Vert\\nabla V(x)\\Vert$ in $S$ , and using it to recover $\\nabla V$ and then $V$ , not a trivial task, but an easier one than discovering Lyapunov functions. To counter this, we relax the orthonormality condition on $e^{i}(x)$ , so that $\\bar{1/||}\\bar{\\nabla}\\bar{V}(x)||$ never appears, yet keep the $e^{i}(x)$ simple enough for $\\nabla V$ -specific patterns in $\\textstyle\\sum_{i}g_{i}(x)e^{i}(x)$ to simplify away at step 6. We also want to ensure that the $e^{i}$ span all of $\\mathcal{H}_{x}$ , or the systems generated will not be diverse enough. ", "page_idx": 5}, {"type": "text", "text": "In our experiments, we slightly modify this procedure, by running steps 2 to 6 five times for each Lyapunov function $V$ created at step 1. As a result, 5 systems are generated that share the same Lyapunov function (a discussion of this choice can be found in Appendix C.1). From a mathematical point of view, a Lyapunov function describes a hidden quantity in a system, and we believe that providing the model with several systems that share this hidden quantity should help it learn the parts of the system that contribute to this hidden quantity, and therefore learn a Lyapunov function. ", "page_idx": 5}, {"type": "text", "text": "This procedure can be tuned to generate specific classes of systems. By choosing $V$ , $g_{i}$ and $h_{i}$ in particular classes, we can constrain the system functions $f_{i}$ to be polynomials, polynomials of functions (e.g. trigonometric polynomials), or more general functions (see Appendix B.4 for more). ", "page_idx": 5}, {"type": "text", "text": "The Lyapunov functions obtained here are correct by design. Nevertheless, we still performed an evaluation of the solutions both as a safeguard and to benchmark the failure and timeout rates of the SMT and SOS solvers on correct solutions, which we report in Table 1. ", "page_idx": 5}, {"type": "text", "text": "4.2 Forward generation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Whereas the stability problem is unsolved in the general case, methods exist to calculate Lyapunov functions of polynomial systems, when they exist and can be written as a sum of squares of polynomials (see Section 1). These algorithms, of polynomial complexity, are very efficient for small systems, but their CPU and memory requirements explode as the size of the systems grows. We leverage them to generate forward datasets, as follows. ", "page_idx": 5}, {"type": "text", "text": "Step 1 Generate a polynomial system at random Step 2 Use a routine to find a polynomial sum-of-squares (SOS) Lyapunov function. Step 3 Keep the system if such function exists, restart from step 1 otherwise. ", "page_idx": 5}, {"type": "text", "text": "This approach has several limitations. First, since most polynomial systems are not stable, and the computation of SOS Lyapunov function involves a complicated search [33], it is slow and limited to small systems of polynomials with small degree. Second, because not all stable polynomial systems have polynomial SOS Lyapunov functions [1], it can only generate a subset of stable polynomial systems. ", "page_idx": 5}, {"type": "text", "text": "Finally, SOS routines process the constraints in Equation (3) by solving semi-definite programming (SDP) problems. This guarantees that $V$ is a sum-of-squares, hence we have $V(x)\\bar{\\geq0}$ , but not necessarily $V(x)>0$ , for $x\\neq0$ . As a result, these methods can only discover barrier functions. State-of-the-art methods circumvent this by introducing the stronger constraint $\\begin{array}{r}{V(x)\\geq\\dot{\\sum}_{i=1}^{n}\\varepsilon_{i}x_{i}^{2}}\\end{array}$ with $\\varepsilon_{i}$ small [32]. $V$ then has a unique minimum in $x=0$ , which makes it a Lyapunov function, but this further restricts the class of polynomial systems that the method can solve. ", "page_idx": 5}, {"type": "text", "text": "4.3 Datasets ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We generate 2 backward and 2 forward datasets for training and evaluation purpose, and one smaller forward dataset for evaluation purposes (see Table 8 in Appendix B.6 for a list). ", "page_idx": 5}, {"type": "text", "text": "Backward datasets Our main backward set, BPoly, features 1 million non-degenerate polynomial systems $S$ with integer coefficients, and 2 to 5 equations (in equal proportions). We also create BNonPoly, a dataset of 1 million non-degenerate non-polynomial systems with 2 to 5 equations. In this dataset, the coordinates of $f$ are polynomials of general functions, e.g. trigonometric polynomials, or functions such as $3\\cos(x_{1})+2x_{1}e^{x_{2}}$ . For such general systems, no method for discovering a Lyapunov function is known. ", "page_idx": 6}, {"type": "text", "text": "Forward datasets All 2 forward datasets are generated using a solver derived from the SumOfSquares package in Python, and implementing techniques similar to those used in SOSTOOLS (see Appendix B.5). All systems in these datasets are non-zero integer polynomials with 2 to 3 equations, and integer polynomial Lyapunov functions \u2013 the only systems these methods can solve. We create FLyap, a dataset of 100,000 systems having a non-homogeneous polynomial as a Lyapunov function. We also have a dataset focusing on barrier functions (see the end of section 4.2): FBarr features 300,000 systems having a non-homogeneous polynomial as a barrier function. The small size of these datasets is due to the computational cost of SOS methods, and the difficulty of discovering Lyapunov or barrier functions. ", "page_idx": 6}, {"type": "text", "text": "To allow for comparison with SOSTOOL, the state-of-the-art method for discovering Lyapunov functions of polynomial systems, we also generated a test set of 1,500 polynomial systems with integer coefficients that SOSTOOLS can solve (FSOSTOOLS). ", "page_idx": 6}, {"type": "text", "text": "5 Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Our models trained on different datasets achieve near perfect accuracy on held-out test sets, and very high performances on out-of-distribution test sets, especially when enriching the training set with a small number of forward examples. They greatly outperform state-of-the-art techniques and also allow to discover Lyapunov functions for new systems. These results are detailed below. ", "page_idx": 6}, {"type": "text", "text": "5.1 In and out-of-distribution accuracy ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we present the performance of models trained on the 4 datasets. All models achieve high in-domain accuracy \u2013 when tested on held-out test sets from the datasets they were trained on (Table 2). On the forward datasets, barrier functions are predicted with more than $\\bar{90\\%}$ accuracy, and Lyapunov functions with more than $80\\%$ . On backward datasets, models trained on BPoly achieve close to $100\\%$ accuracy. We note that beam search, i.e. allowing several guesses at the solution, brings a significant increase in performance (7 to $10\\%$ with beam size 50, for the low-performing models). We use beam size 50 in all further experiments. ", "page_idx": 6}, {"type": "table", "img_path": "kOMrm4ZJ3m/tmp/2ed8af773ae444e6a027915d2b57019b8a3ac2a04c028830656dc7880a45fc1c.jpg", "table_caption": ["Table 2: In-domain accuracy of models. Beam size (bs) 1 and 50. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "The litmus test for models trained on generated data is their ability to generalize out-of-distribution (OOD). Table 3 presents evaluations of backward models on forward-generated sets (and the other way around). All backward models achieve high accuracy (73 to $75\\%$ ) when tested on forwardgenerated random polynomial systems with a sum-of-squares Lyapunov functions (FLyap). The best performances are achieved by non-polynomial systems (BNonPoly), the most diverse training set. The lower accuracy of backward models on forward-generated sets of systems with barrier functions (FBarr) may be due to the fact that many barrier functions are not necessarily Lyapunov functions. On those test sets, backward models must cope with a different distribution and a (slightly) different task. Forward models, on the other hand, achieve low performance on backward test sets. This is possibly due to the small size of these training set. ", "page_idx": 6}, {"type": "text", "text": "Overall, these results seem to confirm that backward-trained models are not learning to invert their generative procedure. If it were the case, their performance on the forward test sets would be close to zero. They also display good OOD accuracy. ", "page_idx": 6}, {"type": "table", "img_path": "kOMrm4ZJ3m/tmp/61ba3bce762f1e47155426d2a67e2559ba64e24531fded4e57e67363f04da071.jpg", "table_caption": ["Table 3: Out-of-domain accuracy of models. Beam size 50. Columns are the test sets. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "5.2 Enriching training distributions for improved performance ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To improve the OOD performance of backward models, we add to their training set a tiny number of forward-generated examples, as in [16]. Interestingly, this brings a significant increase in performance (Table 4). Adding 300 examples from FBarr to BPoly brings accuracy on FBarr from 35 to $89\\%$ (even though the proportion of forward examples in the training set is only $0.03\\%$ ) and increases OOD accuracy on FLyap by more than 10 points. Adding examples from FLyap brings less improvement. ", "page_idx": 7}, {"type": "text", "text": "These results indicate that the OOD performance of models trained on backward-generated data can be greatly improved by adding to the training set a small number of examples (tens or hundreds) that we know how to solve. Here, the additional examples solve a weaker but related problem: discovering barrier functions. The small number of examples needed to boost performance makes this technique especially cost-effective. ", "page_idx": 7}, {"type": "table", "img_path": "kOMrm4ZJ3m/tmp/e7af4b7089423878f92b93a1a792375f4f435da67d3fcd1949acf9e489a8de06.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "5.3 Comparing with state-of-the-art baselines ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To provide a baseline for our models, we developed findlyap, a Python counterpart to the MATLAB Lyapunov function finder from SOSTOOLS (see Appendix B.5). We also introduce FSOSTOOLS, a test set of 1,500 polynomial systems with integer coefficients that SOSTOOLS can solve. We also tested AI-based tools (see Appendix E for the full list of parameters sweeps we used for each of these methods), such as Fossil 2 [12], ANLC v2 [15] and LyzNet [25]. These methods achieve low accuracies on our test sets. This might be due to the fact that these tools are designed to solve a different problem: discovering local or semi-global Lyapunov function (and potentially finding a control function), while we target global Lyapunov functions. ", "page_idx": 7}, {"type": "table", "img_path": "kOMrm4ZJ3m/tmp/b299d4117615bad79e64cdf33f3e0c2398ef594135e7b2f771a69134fc66b5e7.jpg", "table_caption": ["Table 5: Performance comparison on different test sets. Beam size 50. PolyMixture is BPoly + 300 FBarr. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 5 compares findlyap and AI-based tools to our models on all available test sets. A model trained on BPoly complemented with 500 systems from FBarr (PolyMixture) achieves $84\\%$ on FSOSTOOLS, confirming the high OOD accuracy of mixture models. On all generated test sets, PolyMixture achieves accuracies over $84\\%$ whereas findlyap achieves $15\\%$ on the backward generated test set. This demonstrates that, on polynomial systems, transformers trained from backward-generated data achieve very strong results compared to the previous state of the art. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "On average Transformer-based models are also much faster than SOS methods. When trying to solve a random polynomial system with 2 to 5 equations (as used in Section 5.4), findlyap takes an average of 935.2s (with a timeout of 2400s). For our models, inference and verification of one system takes 2.6s on average with greedy decoding, and 13.9s with beam size 50. ", "page_idx": 8}, {"type": "text", "text": "5.4 Into the wild - discovering new mathematics ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Our ultimate goal is to discover new Lyapunov functions. To test our models\u2019 ability to do so, we generate three datasets of random systems: polynomials systems with 2 or 3 equations (Poly3), polynomial systems with 2 to 5 equations (Poly5), and non-polynomial systems with 2 or 3 equations (NonPoly). For each dataset, we generate 100,000 random systems and eliminate those that are trivially locally exponentially unstable in $x^{*}\\ =\\ 0$ , because the Jacobian of the system has an eigenvalue with strictly positive real part [20]. We compare findlyap and AI based methods with two models trained on polynomial systems, FBarr, and PolyM(ixture) \u2013 a mixture of BPoly and 300 examples from FBarr\u2013 and one model trained on a mixture of BPoly, BNonPoly and 300 examples from FBarr (NonPolyM). ", "page_idx": 8}, {"type": "text", "text": "Table 6 presents the percentage of correct solutions found by our models. On the polynomial datasets, our best model (PolyM) discover Lyapunov functions for 11.8 and $10.1\\%$ of the (degree 3 and degree 5) systems, ten times more than findlyap. For non-polynomial systems, Lyapunov functions are found for $12.7\\%$ of examples. These results demonstrate that language model trained from generated datasets of systems and Lyapunov function can indeed discover yet unknown Lyapunov functions and perform at a much higher level that state-of-the-art SOS solvers. ", "page_idx": 8}, {"type": "table", "img_path": "kOMrm4ZJ3m/tmp/2a492f2c0c234355cb4aa51e8491502f4d7ed80f3ca482409022708037613e0d.jpg", "table_caption": [], "table_footnote": ["Table 6: Discovering Lyapunov comparison for random systems. Beam size 50. PolyM is BPoly + 300 FBarr. NonPolyM is BNonPoly $^+$ BPoly $+\\;300$ FBarr. "], "page_idx": 8}, {"type": "text", "text": "5.5 Expert iteration ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Given the performance on our model in Table 6, we can use the newly solved problems to further fine-tune the model. Specifically, we create a sample of verified model predictions for polynomial systems, FIntoTheWild, we add it to the original training sample and we continue training the model. ", "page_idx": 8}, {"type": "text", "text": "We test different strategy to finetune the model and we report performance on forward benchmarks and \u201cinto the wild\u201d in Table 7. ", "page_idx": 8}, {"type": "text", "text": "n1: Add 20,600 samples from BPoly (20,000), FBarr (50), FLyap (50) and FIntoTheWild (500) to keep similar proportion used during pretraining   \nn2: Add 2,000 samples from FLyap (1,000) and FIntoTheWild (1,000) to improve on both forward benchmark and in the wild   \nn3: Add 50 samples from FIntoTheWild to show that this indeed helps   \nn4: Add 1,000 samples from FIntoTheWild   \nn5: Add 2,000 samples from FIntoTheWild   \nn6: Add 5,000 samples from FIntoTheWild to see if there are benefits to add more samples ", "page_idx": 8}, {"type": "text", "text": "We also retrain a model $(n7)$ from scratch using a mixture of BPoly (1M), FBarr (500), FLyap (500) and FIntoTheWild (2,000). ", "page_idx": 8}, {"type": "text", "text": "We notice that the addition of 1,000 verified predictions to our training set of 1 million improves performance on the \u201cinto to wild\u201d test sets by about $15\\%$ , while not affecting the other test sets $(n4)$ . Adding more examples seems to be detrimental, as it decreases the performance on other benchmarks $_{n5}$ and $n\\delta$ ). We also notice that finetuning with mixed data from other distributions is not efficient $_{n I}$ and $n2$ ) and a small contribution already help to get some improvements (result $n3$ ). Finally, it\u2019s not efficient to pretrain the model from scratch using data from FIntoTheWild $(n7)$ . ", "page_idx": 8}, {"type": "table", "img_path": "kOMrm4ZJ3m/tmp/11e682496f8a0063b3d6f5a539849d517068fa4849eb5b5e5e4ecbf377647882.jpg", "table_caption": ["Table 7: Expert iteration using IntoTheWild correct guesses. The Poly3 and Poly5 test sets are regenerated, to prevent data contamination. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "6 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We have shown that models can be trained from generated datasets to solve a long-standing open problem in mathematics: discovering the Lyapunov functions of stable dynamical systems. For random polynomial systems, our best models can discover Lyapunov functions in five times more cases than state-of-the-art methods. They can also discover Lyapunov functions of non-polynomial systems, for which no algorithm is yet known, and were able to re-discover a non-polynomial Lyapunov function of a polynomial systems discovered by [1] (Appendix F). ", "page_idx": 9}, {"type": "text", "text": "The backward generation method introduced in section 4.1 is the key innovation in this paper. The main problem with such approaches is their tendency to generate training sets with very specific distributions, which prevent models from generalizing to general instances of the problem. Our models can generalize out of their training distributions (Table 3), and we can improve their performance by adding to their training set a tiny number of systems that we know how to solve (Table 5). ", "page_idx": 9}, {"type": "text", "text": "While our models exceed the algorithmic state of the art, one might wonder how they compare to human mathematicians. To this effect, we proposed 75 problems from the FSOSTOOLS dataset (polynomial systems with 2 or 3 equations) as an examination for 25 first year Masters students in mathematics, following a course on the subject. Each student was given 3 systems chosen at random and had a total of $30\\;\\mathrm{min}$ . Their performance was $9.33\\%$ , significantly lower than our models $(84\\%)$ . ", "page_idx": 9}, {"type": "text", "text": "Our work has a number of limitations. Because there is no known way to tell whether a random system is stable, we lack a good benchmark on non-polynomial systems. Also, all the systems studied in this paper are relatively small, at most 5 equations for polynomial systems and 3 for non-polynomial. We believe that scaling to larger models should help tackle larger, and more complex, systems. Finally, this work could be extended to take into account the domain of definition of non-polynomial systems. ", "page_idx": 9}, {"type": "text", "text": "The broader implications of our work extend into two directions: the capability of transformers to reason, and the potential role of AI in scientific discovery. While large language models perform at human level on a broad set of tasks, they are still embarrassingly clumsy on many simple problems of logic and reasoning, to the point that it was suggested that planning and high level reasoning may be an inherent limitation of auto-regressive transformer architectures. Our results suggest that transformers can indeed be trained to discover solutions to a hard problem of symbolic mathematics that humans solve through reasoning, and that this is enabled by a careful selection of training examples, instead of a change of architecture. We do not claim that the Transformer is reasoning but it may instead solve the problem by a kind of \u201csuper-intuition\u201d that stems from a deep understanding of a mathematical problem. ", "page_idx": 9}, {"type": "text", "text": "From a mathematical point of view, we propose a new, AI-based, procedure for finding Lyapunov functions, for a broader class of systems than were previously solvable using current mathematical theories. While this systematic procedure remains a black box, and the \u201cthought process\u201d of the transformer cannot be elucidated, the solutions are explicit and their mathematical correctness can be verified. This suggests that generative models can already be used to solve research-level problems in mathematics, by providing mathematicians with guesses of possible solutions. While a small minority of mathematicians is currently using deep-learning tools, we believe generative models have the potential to foster tremendous progress on a number of research subjects, and may eventually become a central component in the future landscape of mathematical practice. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was performed in part using HPC resources from GENCI\u2013IDRIS (Grant 2023- AD011014527). The authors also acknowledge the Office of Advanced Research Computing (OARC) at Rutgers, The State University of New Jersey. The authors would also like to thank the Master students of the Mathematics and Computer Science Department of the Ecole des Ponts - IP Paris from the year 2023-2024 who attended the course Control of Dynamical Systems. The authors also wish to thank Guillaume Lample for fruitful discussion. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Amir Ali Ahmadi, Miroslav Krstic, and Pablo A. Parrilo. A globally asymptotically stable polynomial vector field with no polynomial lyapunov function. In 2011 50th IEEE Conference on Decision and Control and European Control Conference, pages 7579\u20137580, 2011.   \n[2] Amir Ali Ahmadi, Miroslav Krstic, and Pablo A. Parrilo. A globally asymptotically stable polynomial vector field with no polynomial lyapunov function. In 2011 50th IEEE Conference on Decision and Control and European Control Conference, pages 7579\u20137580, 2011.   \n[3] Martin S. Andersen, Joachim Dahl, and Lieven Vandenberghe. Cvxopt. https://cvxopt. org/, 2023. Version 1.3.2; Free software package for convex optimization based on Python. Last updated August 9, 2023.   \n[4] Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried, Andrew Goff, Jonathan Gray, Hengyuan Hu, Athul Paul Jacob, Mojtaba Komeili, Karthik Konath, Minae Kwon, Adam Lerer, Mike Lewis, Alexander H. Miller, Sasha Mitts, Adithya Renduchintala, Stephen Roller, Dirk Rowe, Weiyan Shi, Joe Spisak, Alexander Wei, David Wu, Hugh Zhang, and Markus Zijlstra. Human-level play in the game of <i>diplomacy</i> by combining language models with strategic reasoning. Science, 378(6624):1067\u20131074, 2022.   \n[5] Luca Biggio, Tommaso Bendinelli, Alexander Neitz, Aurelien Lucchi, and Giambattista Parascandolo. Neural symbolic regression that scales, 2021.   \n[6] Kevin Buzzard, Johan Commelin, and Patrick Massot. Formalising perfectoid spaces. In Proceedings of the 9th ACM SIGPLAN International Conference on Certified Programs and Proofs. ACM, jan 2020.   \n[7] Ya-Chien Chang, Nima Roohi, and Sicun Gao. Neural Lyapunov Control. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.   \n[8] Fran\u00e7ois Charton, Amaury Hayat, and Guillaume Lample. Learning advanced mathematical computations from examples. arXiv preprint arXiv:2006.06462, 2020.   \n[9] Fran\u00e7ois Charton. Computing the roots of polynomials, 2022. https://f-charton.github. io/polynomial-roots.   \n[10] Fran\u00e7ois Charton. Linear algebra with transformers, 2022.   \n[11] Jean-Michel Coron. Control and nonlinearity. American Mathematical Soc., 2007.   \n[12] Alec Edwards, Andrea Peruffo, and Alessandro Abate. Fossil 2.0: Formal certificate synthesis for the verification and control of dynamical models. In Proceedings of the 27th ACM International Conference on Hybrid Systems: Computation and Control, HSCC \u201924, New York, NY, USA, 2024. Association for Computing Machinery.   \n[13] Sicun Gao, Soonho Kong, and Edmund M. Clarke. dreal: An smt solver for nonlinear theories over the reals. In Maria Paola Bonacina, editor, Automated Deduction \u2013 CADE-24, pages 208\u2013214, Berlin, Heidelberg, 2013. Springer Berlin Heidelberg.   \n[14] Peter Giesl. Construction of global Lyapunov functions using radial basis functions, volume 1904. Springer, 2007.   \n[15] Davide Grande, Andrea Peruffo, Enrico Anderlini, and Georgios Salavasidis. Augmented Neural Lyapunov Control. IEEE Access, 11:67979\u201367986, 2023.   \n[16] Samy Jelassi, St\u00e9phane d\u2019Ascoli, Carles Domingo-Enrich, Yuhuai Wu, Yuanzhi Li, and Fran\u00e7ois Charton. Length generalization in arithmetic transformers. arXiv preprint arXiv:2306.15400, 2023.   \n[17] Immanuel Kant. Critique of pure reason. 1787.   \n[18] Christopher M Kellett. Classical converse theorems in lyapunov\u2019s second method. Discrete & Continuous Dynamical Systems-Series B, 20(8), 2015.   \n[19] Hiroshi Kera, Yuki Ishihara, Yuta Kambe, Tristan Vaccon, and Kazuhiro Yokoyama. Learning to compute gr\\\" obner bases. arXiv preprint arXiv:2311.12904, 2023.   \n[20] Hassan K. Khalil. Nonlinear systems. Macmillan Publishing Company, New York, 1992.   \n[21] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \n[22] Guillaume Lample and Fran\u00e7ois Charton. Deep learning for symbolic mathematics. arXiv preprint arXiv:1912.01412, 2019.   \n[23] Guillaume Lample, Marie-Anne Lachaux, Thibaut Lavril, Xavier Martinet, Amaury Hayat, Gabriel Ebner, Aur\u00e9lien Rodriguez, and Timoth\u00e9e Lacroix. HyperTree Proof Search for Neural Theorem Proving. Advances in neural information processing systems, 2022.   \n[24] Jun Liu, Maxwell Fitzsimmons, Ruikun Zhou, and Yiming Meng. Formally verified physicsinformed neural control lyapunov functions. arXiv preprint arXiv:2409.20528, 2024.   \n[25] Jun Liu, Yiming Meng, Maxwell Fitzsimmons, and Ruikun Zhou. Lyznet: A lightweight python tool for learning and verifying neural lyapunov functions and regions of attraction, 2024.   \n[26] Aleksandr Mikha\u0131\u02d8lovich Lyapunov. The general problem of the stability of motion. PhD thesis, Kharkov University, 1892.   \n[27] Jose Luis Massera. On liapounoff\u2019s conditions of stability. Annals of Mathematics, pages 705\u2013721, 1949.   \n[28] Rodrigo Frassetto Nogueira, Zhiying Jiang, and Jimmy Lin. Investigating the limitations of the transformers with simple arithmetic tasks. CoRR, abs/2102.13019, 2021.   \n[29] KP Persidskii. On a theorem of liapunov. In CR (Dokl.) Acad. Sci. URSS, volume 14, pages 541\u2013543, 1937.   \n[30] Stanislas Polu and Ilya Sutskever. Generative language modeling for automated theorem proving, 2020.   \n[31] Stephen Prajna, Ali Jadbabaie, and George J Pappas. A framework for worst-case and stochastic safety verification using barrier certificates. IEEE Transactions on Automatic Control, 52(8):1415\u20131428, 2007.   \n[32] Stephen Prajna, Antonis Papachristodoulou, and Pablo A Parrilo. Introducing sostools: A general purpose sum of squares programming solver. In Proceedings of the 41st IEEE Conference on Decision and Control, 2002., volume 1, pages 741\u2013746. IEEE, 2002.   \n[33] Stephen Prajna, Antonis Papachristodoulou, Peter Seiler, and Pablo A Parrilo. Sostools and its control applications. Positive polynomials in control, pages 273\u2013292, 2005.   \n[34] Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, M. Pawan Kumar, Emilien Dupont, Francisco J. R. Ruiz, Jordan S. Ellenberg, Pengming Wang, Omar Fawzi, Pushmeet Kohli, and Alhussein Fawzi. Mathematical discoveries from program search with large language models. Nature, 625(7995):468\u2013475, 2024.   \n[35] Baptiste Rozi\u00e8re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, J\u00e9r\u00e9my Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre D\u00e9fossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. Code llama: Open foundation models for code, 2024.   \n[36] Guillaume Sagnol and Maximilian Stahlberg. PICOS: A Python interface to conic optimization solvers. Journal of Open Source Software, 7(70):3915, February 2022.   \n[37] Trieu H. Trinh, Yuhuai Wu, Quoc V. Le, He He, and Thang Luong. Solving olympiad geometry without human demonstrations. Nature, 625(7995):476\u2013482, 2024.   \n[38] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998\u20136008, 2017.   \n[39] Adam Zsolt Wagner. Constructions in combinatorics via neural networks. arXiv preprint arXiv:2104.14516, 2021.   \n[40] Emily Wenger, Mingjie Chen, Francois Charton, and Kristin E Lauter. Salsa: Attacking lattice cryptography with transformers. Advances in Neural Information Processing Systems, 35:34981\u201334994, 2022.   \n[41] Xiangru Xu, Paulo Tabuada, Jessy W Grizzle, and Aaron D Ames. Robustness of control barrier functions for safety critical control. IFAC-PapersOnLine, 48(27):54\u201361, 2015.   \n[42] Gal Yehuda, Moshe Gabel, and Assaf Schuster. It\u2019s not what machines can learn, it\u2019s what we cannot teach. arXiv preprint arXiv:2002.09398, 2020.   \n[43] Chenyang Yuan. Sumofsquares.py, 2024. Software version 1.2.0; Available at https://github.com/yuanchenyang/SumOfSquares.py.   \n[44] Ruo-Shi Yuan, Yi-An Ma, Bo Yuan, and Ping Ao. Lyapunov function as potential function: A dynamical equivalence. Chinese Physics B, 23(1):010505, 2013.   \n[45] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. Large language models are human-level prompt engineers, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A Mathematical definitions ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this Appendix, we recall several mathematical definitions and theorems related to the Lyapunov function problem. We first introduce the notion of global asymptotic stability (GAS). ", "page_idx": 13}, {"type": "text", "text": "Definition A.1. We say that the (equilibrium $x^{*}=0$ of the) system (1) is globally asymptotically stable if it is stable and for any $x_{0}\\in\\mathbb{R}^{n}$ there exists a unique solution $x\\in{\\dot{C}}^{1}([0,+\\infty);{\\bar{\\mathbb{R}}}^{n})$ to (1) which satisfies in addition ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{t\\to+\\infty}x(t)=0.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "This notion translates the fact that the equilibrium $x^{*}=0$ is robust even to large perturbations. This notion is related to the existence of a Lyapunov function thanks, for instance, to LaSalle Invariance Principle: ", "page_idx": 13}, {"type": "text", "text": "Theorem A.2 (LaSalle Invariance Principle (global)). Assume there exists a Lyapunov function for the system (1) and let $S$ be the largest subset of $\\{\\nabla V(x)\\cdot f(x)=0\\}$ that is invariant by the dynamics of (1). If $S=\\{0\\}$ , then the system (1) is globally asymptotically stable. ", "page_idx": 13}, {"type": "text", "text": "Note that if $\\nabla V(x)\\cdot f(x)<0$ for any $x\\neq0$ then necessarily $S=\\{0\\}$ . Because finding a (global) Lyapunov function is a challenging mathematical problem, and still an open problem in general, weaker notions exists. ", "page_idx": 13}, {"type": "text", "text": "Definition A.3. The function $V\\in C^{1}(\\mathbb{R}^{n},\\mathbb{R}_{+})$ is said to be a semi-global Lyapunov function for the system (1) if there exists $r>0$ such that the following condition are satisfied ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{V(0)=0,\\quad V(x)>0,}\\\\ {\\nabla V(x)\\cdot f(x)\\le0\\,\\mathrm{for}\\,\\|x\\|\\le r.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Finding a semi-global Lyapunov function is usually easier than finding a global Lyapunov function. A semi-global Lyapunov function is enough to show that the equilibrium $x^{*}=0$ is robust to small perturbations which, for several engineering applications, is enough. More specifically, ", "page_idx": 13}, {"type": "text", "text": "Definition A.4. We say that the (equilibrium $x^{*}=0$ of the) system (1) is locally asymptotically stable if it is stable and if there exists $r>0$ such that for any $\\|x_{0}\\|\\leq r$ there exists a unique solution $x\\in C^{1}([0,+\\infty);\\mathbb{R}^{n})$ to (1) which satisfies in addition ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{t\\to+\\infty}x(t)=0.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Similarly to global Lyapunov function, the existence of a semi-global Lyapunov function is useful to ensure local asymptotic stability ", "page_idx": 13}, {"type": "text", "text": "Theorem A.5 (LaSalle Invariance Principle (local)). Assume there exists a semi-global Lyapunov function $V$ , and let $S$ be the largest subset of $\\{\\nabla V(x)\\cdot f(x)=0\\}$ invariant by the dynamics of (1). If $S=\\{0\\}$ then the system (1) is locally asymptotically stable. ", "page_idx": 13}, {"type": "text", "text": "B Generation procedure ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "B.1 Function generation ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "To generate random functions we sample random trees with unary and binary internal nodes, and then randomly select operators for these nodes, and variables and integers for leaves (as in [22, 8]). Our binary operators are the four operations and the power function. Unary operators are exp, log, sqrt, sin, cos, tan. ", "page_idx": 13}, {"type": "text", "text": "To generate polynomials, we randomly sample a given number of monomials, with integer or real coefficients. The number of monomials, range of the coefficients, and the powers and number of terms of each monomial, are randomly selected between bounds, provided as hyperparameters. ", "page_idx": 13}, {"type": "text", "text": "B.2 Backward generation ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We build globally stable systems by first generating a Lyapunov function $V$ at random, and then building a dynamic system which has $V$ as a Lyapunov function. The procedure is: ", "page_idx": 14}, {"type": "text", "text": "Step 1a: We generate $V$ as $V=V_{\\mathrm{cross}}+V_{\\mathrm{proper}}$ where $V_{\\mathrm{proper}}$ belongs to a given class of positive definite function and $V_{\\mathrm{cross}}$ belongs to a larger class, but of non-negative functions only with $\\bar{V}_{\\mathrm{cross}}(0)=0$ . More specifically, we generate ", "page_idx": 14}, {"type": "equation", "text": "$$\nV_{\\mathrm{cross}}(x)=\\sum_{i=1}^{m}p_{i}^{2}(x),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "with $m$ a random integer, and $p_{i}$ random functions verifying $p_{i}(0)=0$ . The nature of functions $p_{i}$ depends on the systems we want to generate (polynomial or not). Clearly $V_{\\mathrm{cross}}(x)\\,\\geq\\,0$ and $V_{\\mathrm{cross}}(0)=0$ . We similarly generate ", "page_idx": 14}, {"type": "equation", "text": "$$\nV_{\\mathrm{proper}}(x)=\\sum_{i=1}^{n}\\alpha_{i,j}x_{i}^{\\beta_{i}}x_{j}^{\\beta_{j}},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "with $n$ a random integer, $\\beta_{i}$ random positive integers and $A=(\\alpha_{i,j})_{(i,j)\\in\\{1,\\dots,n\\}^{2}}$ a random positive definite matrix, with a given probability of being diagonal. As a consequence, $V_{\\mathrm{proper}}$ is strictly minimal in $x=0$ . When generating barrier functions, we can optionally set $V_{\\mathrm{proper}}\\,{\\stackrel{\\cdot}{=}}\\,{\\dot{0}}$ . ", "page_idx": 14}, {"type": "text", "text": "Step 1b: In this step, we increase the class of functions that can be sampled for $V_{\\mathrm{cross}}$ and $V_{\\mathrm{proper}}$ by several transformations: ", "page_idx": 14}, {"type": "text", "text": "1. Composition of $V_{\\mathbf{proper}}$ with probability $p_{1,c}$ , replace ", "page_idx": 14}, {"type": "equation", "text": "$$\nV_{\\mathrm{proper}}(x)\\gets I(V_{\\mathrm{proper}}(x))\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "with $I$ selected at random from a pre-defined set of increasing-functions (Appendix B.4), 2. Product $V_{\\mathbf{proper}}$ with probability $p_{1,m}$ , replace ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{V_{\\mathrm{proper}}(x)\\gets(V_{\\mathrm{proper}}(x)-V_{\\mathrm{proper}}(0))g(h(x)),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "with $g$ selected at random from a pre-defined set of positive-functions (Appendix B.4), ", "page_idx": 14}, {"type": "text", "text": "3. Composition of $V_{\\mathbf{cross}}$ : for every $i\\in\\{1,...,m\\}$ , with probability $p_{2}$ , replace ", "page_idx": 14}, {"type": "equation", "text": "$$\np_{i}^{2}(x)\\gets b_{i}(\\xi_{i}+p_{i}(x)),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "with $b_{i}$ a real function that is bounded from below with a minimum (not necessarily unique) in $\\xi_{i}$ and chosen at random from a pre-defined set of bounded-functions (Appendix B.4). Recall that pi are the functions appearing in Vcross. ", "page_idx": 14}, {"type": "text", "text": "Step 1c: Gathering the functions $V_{\\mathrm{proper}}$ and $V_{\\mathrm{cross}}$ together, we define the Lyapunov function (candidate) $V(x)=\\overbar{V}_{\\mathrm{cross}}(x)+V_{\\mathrm{proper}}\\dot{(x)}$ . Overall, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\nV(x)=\\left[I\\left(\\sum_{i=1}^{n}\\alpha_{i,j}x_{i}^{\\beta_{i}}x_{j}^{\\beta_{j}}\\right)-I(0)\\right]g\\left(\\sum_{i=1}^{q}\\alpha_{\\sigma(i),\\sigma(j)}x_{\\sigma(i)}^{\\beta_{\\sigma(i)}}x_{\\sigma(j)}^{\\beta_{\\sigma(j)}}\\right)+\\sum_{i=1}^{m}b_{k}(\\xi_{k}+p_{k}(x)),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $I$ is the identity with probability $1-p_{1,c},g$ is the constant function 1 with probability $1-p_{1,m}$ and $b_{k}(x)=x^{2}$ with probability $1-p_{2}$ . Such a Lyapunov function satisfies ", "page_idx": 14}, {"type": "equation", "text": "$$\nV(x)>V(0),\\;\\;\\forall x\\in\\mathbb{R}^{n}\\setminus\\{0\\}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Indeed, ", "page_idx": 14}, {"type": "equation", "text": "$$\nV(0)=\\sum_{i=1}^{m}b_{k}(\\xi_{k})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and $V(x)\\;>\\;b_{k}(\\xi_{k})$ for any $x\\,\\in\\,\\mathbb{R}^{n}\\setminus\\{0\\}$ , since $g$ is a positive function, $I$ is increasing and $(\\alpha_{i,j})_{i,j\\in\\{1,...,n\\}}$ is positive definite. ", "page_idx": 14}, {"type": "text", "text": "Step 2: In this step we create the random vectors orthogonal to $\\nabla V$ that will be useful in the generation of the system $f$ (see Section 4.1). Taking advantage of the form of the condition (3), for any $x\\in\\mathbb R$ , denote ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{H}_{x}=\\{z\\in\\mathbb{R}^{n}\\mid z\\cdot\\nabla V(x)=0\\}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "the hyperplane orthogonal to $\\nabla V(x)$ . Then, for a random $p\\;\\in\\;\\{1,...,n\\}$ , generate $p$ random real-valued functions $(g_{i})_{i\\in\\{1,...,p\\}}$ , and $p$ vectors $\\{e^{i}\\}_{i\\in\\{1,\\dots,p\\}}$ from this hyperplane as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\ne_{j}^{i}={\\biggl\\{}{\\frac{A_{\\tau_{2}(i)}\\ {\\mathrm{if}}\\ j=\\tau_{1}(i)}{-A_{\\tau_{1}(i)}\\ {\\mathrm{if}}\\ j=\\tau_{2}(i)}}{\\biggl\\}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $A\\,=\\,\\nabla V(x)$ and $A_{j}$ refers to the $j-t h$ component of the vector and $\\tau_{1}$ and $\\tau_{2}$ random functions from $\\mathbb{N}\\backslash\\{0\\}$ into $\\left\\{1,{\\ldots},n\\right\\}$ , such that $\\tau_{1}(i)\\neq\\tau_{2}(i)$ . This implies that for any $i\\in\\{1,...,n\\}$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\nabla V(\\boldsymbol{x})\\cdot\\boldsymbol{e}^{i}=(\\nabla V(\\boldsymbol{x}))_{\\tau_{1}(i)}(\\nabla V(\\boldsymbol{x}))_{\\tau_{2}(i)}-(\\nabla V(\\boldsymbol{x}))_{\\tau_{2}(i)}(\\nabla V(\\boldsymbol{x}))_{\\tau_{1}(i)}=0.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Note that, so long $\\nabla V(x)\\neq0$ , one can use this process to construct a generative family of $\\mathcal{H}_{x}$ , and the $e^{i}$ span the whole $\\mathcal{H}_{x}$ . If $\\nabla V(x)=0$ then $\\mathcal{H}_{x}=\\mathbb{R}^{n}$ . ", "page_idx": 15}, {"type": "text", "text": "Step 3: Generate at random $k_{1}$ real-valued functions $(h_{i})_{i\\in\\{1,...,k_{1}\\}}$ , where $1\\leq k_{1}\\leq n$ is chosen at random. Set $h_{i}=0$ for $k_{1}<i\\leq n$ . ", "page_idx": 15}, {"type": "text", "text": "Step 4: Build the system ", "page_idx": 15}, {"type": "equation", "text": "$$\nf(x)=-\\left(h_{\\pi(i)}^{2}(x)(\\nabla V(x))_{i}\\right)_{i\\in\\{1,\\ldots,n\\}}+\\sum_{i=1}^{p}g_{i}(x)e^{i}(x),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "with $\\pi$ a random permutation of $\\{1,...,n\\}$ . ", "page_idx": 15}, {"type": "text", "text": "Overall, the function $f$ satisfies ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\nabla V(x)\\cdot f(x)=-\\left(\\sum_{i=1}^{n}h_{\\pi(i)}^{2}(x)(\\nabla V(x))_{i}^{2}\\right)\\leq0,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "hence $V$ is a Lyapunov function of the system ${\\dot{x}}(t)=f(x(t))$ . ", "page_idx": 15}, {"type": "text", "text": "Step 5: Expand and simplify the equations of $f$ (using Sympy), in order to eliminate obvious patterns due to the generation steps (that the model could recognize and leverage), eliminate duplicated systems in the training set, and limit the length of training sequences. All polynomial systems are expanded into normal form. ", "page_idx": 15}, {"type": "text", "text": "B.3 Backward generation modes ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Polynomial generation: we generate polynomial systems with sum-of-square Lyapunov functions to allow for easy comparison with existing methods such as SOSTOOLS [32, 33]. In this case, all $P_{i}$ are polynomials with no zero-order term and $p_{1,c}=p_{1,m}=p_{2}=0$ . Also, $f_{i}$ and $g_{i}$ are polynomials (Appendix B.1). We generate $f_{i}$ with a degree lower or equal to half the maximal degree of $g_{i}$ and a maximal value of coefficients of the order of the square root of the maximal value of $g_{i}$ . Since the $f_{i}$ are squared in the final system, this allows $f_{i}^{2}$ and $g_{i}$ to have the same order, and prevents the transformer from inferring unwanted additional information by looking at the higher degree monomial. ", "page_idx": 15}, {"type": "text", "text": "Generic generation: $P_{i}$ is generated as $P_{i}(x)=Q_{i}(x)-Q_{i}(0)$ , where $Q_{i}(x)$ is a random function generated as per Appendix B.1 and $f_{i}$ and $g_{i}$ are also generated as per Appendix B.1. Optionally the functions can be generated as polynomials of non-polynomial operators taken from a pre-defined set of operators. ", "page_idx": 15}, {"type": "text", "text": "Other generation modes: we have other generation modes corresponding to interesting particular cases: gradient flow systems, systems where the 2-norm (resp. a weighted 2-norm) is a Lyapunov function, etc. ", "page_idx": 15}, {"type": "text", "text": "B.4 Generation design parameters ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Our generator allows us to generate generic stable systems and yet to have a large control on the distribution. For polynomials, for instance, we have a control on the maximal and average degree, number of monomials, power and number of variables of the monomials, coefficients, etc. We can also specify whether the coefficients are integers, floats, with which precision. Overall we have a total of 36 generation hyper-parameters that influence the distribution of the synthetic data created. The main generation design parameters are: ", "page_idx": 16}, {"type": "text", "text": "\u2022 int_base: encoding base for integers   \n\u2022 max_int: Maximum integer value   \n\u2022 precision: Float numbers precision   \n\u2022 prob_int: Probability of sampling integers vs variables (for non-polynomial expressions)   \n\u2022 min_dim: minimal number of equations in the system   \n\u2022 max_dim: maximal number of equations   \n\u2022 max_degree: maximal degree of polynomial terms in a Lyapunov function   \n\u2022 n_terms: maximal number of terms in polynomials for the Lyapunov function   \n\u2022 nb_ops_proper: maximal number of operators in $V_{p r o p e r}$ (non polynomial generation)   \n\u2022 nb_ops_lyap: maximal number of operators in $V_{p r o p e r}$ (non polynomial generation)   \n\u2022 operators_lyap: list of operators to be considered (non polynomial generation)   \n\u2022 polynomial_V: if true generated expressions are polynomials of (potentially non-polynomial) operators   \n\u2022 pure_polynomial: generate polynomial systems only   \n\u2022 cross_term: $V_{c r o s s}=0$ if False.   \n\u2022 max_nb_cross_term: bound on m in $V_{c r o s s}$   \n\u2022 proba_diagonal: with this probability, the positive definite form of $V_{p r o p e r}$ is imposed to be diagonal   \n\u2022 only_2_norm: if True, the Lyapunov function is the 2-norm.   \n\u2022 strict: if True, generates a strict Lyapunov function (i.e. $\\nabla V\\cdot f<0)$   \n\u2022 proper: if set to false, $V_{p r o p e r}=0$ and $V$ is only a barrier function.   \n\u2022 float_resolution_poly: float resolution of the polynomials generated by generate_bounded_polynomial.   \n\u2022 generate_gradient_flow: When set to True, the backward generation only generates gradient flows systems.   \n\u2022 gen_weight: exponential weight which bias the sampling of $k_{1}$ and $p$ , the number of components of non-zero $h_{i}$ and $g_{i}$ .   \n\u2022 max_order_pure_poly: maximal polynomial order of $h_{i}$   \n\u2022 max_n_term_fwd: maximal number of terms in each equations in the fwd generation   \n\u2022 SOS_checker: if True, uses a SOS verifier to evaluate the candidate Lyapunov function (if False uses the verifier based on shgo)   \n\u2022 SMT_checker: if True, uses an SMT verifier to evaluate the candidate Lyapunov function (if False uses the verifier based on shgo)   \n\u2022 multigen: number of different system generated per Lyapunov function.   \n\u2022 increasing_func: the s\u221aet of increasing functions used in the generation (see Step 1b). Default is $\\{\\exp,\\bar{\\ln}(1+x^{2}),\\sqrt{1+x}\\}$ .   \n\u2022 positive_func: the set of positive functions used in the generation (see Step 1b). Default is $\\mathrm{\\bar{\\{}e x p,1+\\cos(}}x),1+\\sin(x)\\}$ .   \n\u2022 bounded_func: the set of bounded functions used in the generation (see Step 1b). Default is $\\{\\cos,\\sin\\}$ . ", "page_idx": 16}, {"type": "text", "text": "B.5 Forward SOS solver ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "SOSTOOLS is one of the most famous toolbox for sum-of-square optimization, in particular for finding SOS Lyapunov functions [32, 33]. It is natively available in MATLAB and relies on an underlying SDP solver that can be chosen. In Python an analogous toolbox is the package SumOfSquares [43] which relies on the same principle, however does not have specific functionalities for Lyapunov functions. As a consequence we implemented these functionalities in our codebase based on the MATLAB implementations in SOSTOOLS. We implemented a function SOS_checker, which takes in input a system of equations in sympy and a candidate Lyapunov function and checks SOS conditions on $\\bar{V(x)}$ and $-{\\bar{\\nabla}}V(x)\\cdot f({\\dot{x}})$ , and a function findlyap, analogous to the findlyap function in SOSTOOLS, which takes a system of equations in sympy and either returns a function satisfying SOS conditions on $V(x)$ and $\\dot{-}\\nabla V(x)\\cdot\\bar{f}(x)$ , returns false if no such function exists, or returns none if it fails to provide an answer. SumOfSquares relies itself on picos [36] and we use the default solver cvxopt [3]. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "table", "img_path": "kOMrm4ZJ3m/tmp/d906e0722a69ecc4fd9881f057a18499f17678b3ba004924594511df2e27e2d5.jpg", "table_caption": ["B.6 List of datasets "], "table_footnote": ["Table 8: Datasets generated. Backward systems are degree 2 to 5, forward systems degree 2 to 3. All forward systems are polynomial. "], "page_idx": 17}, {"type": "text", "text": "C Additional results ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "C.1 Impact of multigeneration ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In the backward generation procedure, after sampling one random $V$ , it is possible to generate any number of different systems $f_{i}$ such that $V$ is the Lyapunov function for each of the systems $f_{i}$ . We call the maximal number of system generated per Lyapunov function the multigen parameter. The actual number of systems generated per Lyapunov function is chosen at random for each Lyapunov function between 1 and multigen. In Section 5 we reported results using multigen equal to 5. Here we report the in-domain and out-of-domain performance of the models trained on backward BPoly datasets of size 1 million varying the parameter multigen. ", "page_idx": 17}, {"type": "table", "img_path": "kOMrm4ZJ3m/tmp/dabf4c683c5eceb5050003a05f7c33ab5ce5a01a97cf527897d78ac18112e030.jpg", "table_caption": ["Table 9: In-domain and out-of-domain accuracy of models. Beam size 50. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Table 9 shows that generating a moderate amount of different systems with the same Lyapunov function actually improves the model capability to generalize out-of-domain. This suggests that the model is learning, at least partially, to separate the parts of the system which contribute to the Lyapunov function. Above a certain multigen threshold, model performances start to decline. This may be due to the low diversity present in the dataset, i.e. the limited number of different Lyapunov functions the model is trained on (the total number of systems in the training set remains constant so the total number of Lyapunov function decreases with the value of the parameter multigen). ", "page_idx": 17}, {"type": "text", "text": "C.2 Performance of smaller transformer models ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In Section 5 we report results using a transformer with 8 encoder and decoder layers, 10 attention heads and an embedding dimension of 640. We also trained smaller models with 6 encoder and decoder layers, 8 attention heads and an embedding dimension of 512. Tables 10, 11 report the main results. Results are in line with what we showed in section 5 ", "page_idx": 17}, {"type": "table", "img_path": "kOMrm4ZJ3m/tmp/23f40dffb9d87d737dff15952d84005adf12afe5c09bc95387c354ca385fa88e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "kOMrm4ZJ3m/tmp/199f44bafa21ef3b35a57f4707bde1f16d41d314f57cd468191a73c935a1fabc.jpg", "table_caption": ["Table 10: In-domain and out-of-domain accuracy of models. Beam size 50. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Table 11: Performance of mixing backward data (BPoly) with a small number of forward examples on forward benchmark and \u201cinto the wild\u201d. Beam size 50. ", "page_idx": 18}, {"type": "text", "text": "D Comparison of SOS, SMT and shgo ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We compare our model performance when we employ them to discover new Lyapunov function. We report performances with dReal SMT and SOS verifiers for Poly and dReal SMT and shgo for NonPoly distributions, respectively. Table 12 shows that SMT results are slightly lower, because of timeouts (which we report in Table 13, but comparable. Note that the performances on polynomial systems were already theoretically guaranteed thanks to the former SOS verifier. ", "page_idx": 18}, {"type": "table", "img_path": "kOMrm4ZJ3m/tmp/2a0c6a3ccebeab3d5a061f6ee8c29244f87b089176bc4a233e0838c78cc23328.jpg", "table_caption": [], "table_footnote": ["Table 12: Results of SMT with SOS and shgo verifiers for Poly and NonPoly systems, respectively. "], "page_idx": 18}, {"type": "table", "img_path": "kOMrm4ZJ3m/tmp/cae2986420c79d258a274600510b189d449e9336c30cf1c97ce48ef850eb638c.jpg", "table_caption": ["Table 13: SMT timeout and error rates. Most SMT failures are due to timeout. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "E AI method sweep ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "To report the AI-based tools results on the seven benchmarks (BPoly, BNonPoly, FLyap, FBarr, Poly3, Poly5, NonPoly) we did a hyperparameter sweep. To get the best hyperparameter setting, we sweep on FLyap and then fix these hyperparameters for the different datasets. In bold we show the chosen parameters, selected to maximize the correctness on FLyap, subject to the 20 minutes timeout. ", "page_idx": 18}, {"type": "text", "text": "Lyznet [25] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "\u2022 $\\mathbf{lr}=[3\\cdot10^{-5},\\mathbf{10^{-4}},3\\cdot10^{-4}]$   \n\u2022 poi $\\mathbf{\\boldsymbol{\\mathfrak{t}}}\\mathbf{\\boldsymbol{\\mathrm{ts}}}=[\\mathbf{100},\\mathbf{000},\\,300,000,\\,1,000,000]$   \n\u2022 layer width $=[(2{,}20)$ , (3,6), (6,2)]   \n\u2022 epoch $=[1,5,25]$   \n\u2022 net type $=$ [None, Poly] ", "page_idx": 18}, {"type": "text", "text": "Fossil 2.0 [12] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "\u2022 iters $\\mathbf{\\zeta}=[10,5\\mathbf{0},250]$   \n\u2022 activations $=[({\\bf x^{2}}),\\bar{(x^{2}},x^{2}),$ (sigmoid), (sigmoid, sigmoid), $({\\mathrm{poly}}_{4})$ ), $(\\mathsf{p o l y}_{4}$ , $\\mathrm{poly}_{4})$ )]   \n\u2022 hidden neurons $=$ [6, 10, 20]   \n\u2022 data $=$ [500, 1000, 2000]   \n\u2022 ${\\bf l r}=[0.01,0.03,{\\bf0}.]$ 1] ", "page_idx": 19}, {"type": "text", "text": "ANLC v2 [15] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "\u2022 iters $\\mathbf{\\zeta}=[10,5\\mathbf{0},250]$   \n\u2022 activations $=[(\\mathbf{x}^{2},\\bar{\\mathbf{x}},\\mathbf{x}),(x^{2},x^{2},x),(x^{2},x^{2},x,x),(x^{2},x^{2},x^{2},x)]$   \n\u2022 hidden neurons $=$ [6, 10, 20]   \n\u2022 max data $=[500$ , 1000, 2000]   \n\u2022 $\\mathbf{lr}=[\\mathbf{0.01},\\,0.03,\\,0.1]$ ", "page_idx": 19}, {"type": "text", "text": "F Some examples ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "To understand the model performance and compare against the SOSTOOL performance, we manually inspect some systems with 2 or 3 equations where the following conditions hold: (1) the Jacobian of the system has the maximum eigenvalue with real part equal to 0 (i.e. tools like the spectral mapping theorem cannot decide on the stability), (2) no weighted 2-norm functions can be a Lyapunov function, (3) findlyap times out after 4 hours. We show some examples below. ", "page_idx": 19}, {"type": "text", "text": "F.1 A polynomial system with non polynomial solution ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "System ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{{\\dot{x}}_{0}}&{=-x_{0}+x_{0}x_{1}}\\\\ {{\\dot{x}}_{1}}&{=-x_{1}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "It\u2019s known that there is no polynomial Lyapunov function for this system [2]. Our poly models and findlyap failed, as expected. Nonetheless, one of our non-poly models with beam search of beam size 100 proposed $V(x\\bar{)}=\\ln(1+5x_{0}^{2})+x_{1}^{2}$ similar to the one that was recently found in [2]. ", "page_idx": 19}, {"type": "text", "text": "It\u2019s clear that $V(0)=0$ and $V(x)>0$ for all $x\\neq0$ . Also ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V(x)\\cdot f(x)=\\frac{-10x_{0}^{2}+10x_{0}^{2}x_{1}-2x_{1}^{2}(1+5x_{0}^{2})}{1+5x_{0}^{2}}}\\\\ &{\\qquad\\qquad=\\frac{-5x_{0}^{2}-5x_{0}^{2}x_{1}^{2}-5(x_{0}-x_{0}x_{1})^{2}-2x_{1}^{2}}{1+5x_{0}^{2}}\\leq0}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "as desired. ", "page_idx": 19}, {"type": "text", "text": "F.2 A system that has no diagonal Lyapunov function ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "System ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\dot{x}_{0}}&{=2x_{1}^{2}}\\\\ {\\dot{x}_{1}}&{=-10x_{1}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Model inference: Our model recovers $V(x)=10x_{0}^{2}+2x_{0}x_{1}^{2}+3x_{1}^{4}+6x_{1}^{2},$ ", "page_idx": 19}, {"type": "text", "text": "Clearly $V(0)\\,=\\,0$ and $V(x)\\,=\\,9(x_{0})^{2}+(x_{0}+x_{1}^{2})^{2}+2(x_{1}^{2})^{2}+6x_{1}^{2}>\\,0$ for all $x\\neq0$ . Also $\\nabla V(x)\\cdot f(x)=-x_{1}^{2}(116x_{1}^{2}+120)\\le0$ . ", "page_idx": 19}, {"type": "text", "text": "Non existence of a Diagonal Lyapunov function: Suppose for the sake of contradiction that there exists a function $V_{1}$ which satisfies 3 and can be expressed as ", "page_idx": 19}, {"type": "equation", "text": "$$\nV_{1}(x)=\\sum_{i=1}^{n}a_{i}x_{0}^{i}+\\sum_{j=1}^{m}b_{j}x_{1}^{j}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Clearly $V_{1}(0)=0$ . Given that $V_{1}(x_{0},0)>0$ for $x_{0}\\neq0$ , it follows that $n$ is even and $a_{n}>0$ . Also we know that $\\nabla V_{1}(x)\\cdot f(x)=2\\sum_{i=1}^{n}i a_{i}x_{0}^{i-1}x_{1}^{2}-10\\sum_{j=1}^{m}j b_{j}x_{1}^{j}\\leq0$ for all choices of $(x_{0},x_{1})$ . If we let $x_{1}=1$ we obtain $\\nabla V_{1}(x)\\cdot f(x)=2\\sum_{i=1}^{n}i a_{i}x_{0}^{i-1}-10\\sum_{j=1}^{m}j b_{j}$ ixi0\u22121\u221210 jbj. This expression can be seen as a polynomial $g(x_{0})$ with real coefficients and odd degree $n-1$ . The leading coefficient, $2n a_{n}$ , is positive because $a_{n}>0$ and $n\\geq1$ . This means that $\\operatorname*{lim}_{x_{0}\\to+\\infty}g(x_{0})=+\\infty$ , meaning that there exists an $x_{0}$ such that $g(x_{0})>0$ . This contradicts 3. ", "page_idx": 20}, {"type": "text", "text": "F.3 A system with 3 equations and a higher degree ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "System ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{\\begin{array}{l l}{\\dot{x}_{0}}&{=-7x_{0}^{5}-4x_{0}^{3}x_{1}^{2}-5x_{0}^{3}}\\\\ {\\dot{x}_{1}}&{=7x_{0}^{4}-3x_{1}-2x_{2}}\\\\ {\\dot{x}_{2}}&{=-8x_{0}^{2}-9x_{2}}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Model inference: Our model recovers different solutions. Here we show two of them ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\Gamma_{1}(x)=4x_{0}^{4}+10x_{0}^{2}x_{1}^{2}+2x_{0}^{2}x_{1}+10x_{0}^{2}x_{2}^{2}-4x_{0}^{2}x_{2}+20x_{0}^{2}+10x_{1}^{2}x_{2}^{2}+4x_{1}^{2}-2x_{1}x_{2}+8x_{2}^{4}+4x_{2}^{2}}}\\\\ {{V_{2}(x)=2x_{0}^{4}+2x_{0}^{2}x_{1}^{2}+3x_{0}^{2}+2x_{1}^{2}+x_{2}^{2}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We checked with SumOfSquares that $V_{1}>0,V_{2}>0,\\nabla V_{1}\\cdot f\\le0$ and $\\nabla V_{2}\\cdot f\\leq0$ . ", "page_idx": 20}, {"type": "text", "text": "F.4 Other examples ", "text_level": 1, "page_idx": 20}, {"type": "table", "img_path": "kOMrm4ZJ3m/tmp/a2de202161639b3ea4c312bbe51f4fc8b3fe12112984677ecaa3a3a670dc6194.jpg", "table_caption": [], "table_footnote": ["Table 14: Some additional examples generated from our models. "], "page_idx": 20}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The claims in the abstract and introduction summarize the results of section \"Main results\". ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: Yes, in the discussion. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: This paper is mostly experimental, the only theoretical results is the stability of specific systems and we do verify the assumptions and state the theorem we use about Lyapunov functions. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Yes, we detail the experiments, the parameters and the data generator. The code will eventually be provided. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 22}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 23}, {"type": "text", "text": "Answer: [No] ", "page_idx": 23}, {"type": "text", "text": "Justification: The code and datasets are currently pending approval from some of the authors\u2019 institution for being open-sourced. They should be by the conference dates. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 23}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We specify the main training and test details. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 23}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [No] ", "page_idx": 23}, {"type": "text", "text": "Justification: We provide the number of elements in each sets, as well as the accuracies in percentage. Having error bars is not adapted when dealing with the infinite dimensional spaces of functions involved. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 23}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 24}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We provide the type of compute workers, the type of GPU, the memory, and the GPU and CPU time needed for training the models and generating the datasets. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: This paper is about training neural networks to find solutions to mathematical problems. As such, it\u2019s societal aspects (safety, security, discrimination, surveillance, deception and harassment, environment, human rights, Bias and fairness) are either irrelevant or respect the NeurIPS Code of Ethics. The only data coming from human sources are answers to mathematics exercises from 25 students as part of one of their course. All students were all offered the choice of allowing or not the use of these data for the purpose of research, the answers of students who agreed were then anonymized and aggregated. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 24}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: While there is no direct societal impact, there is an impact on the research in mathematics and science in general and we mention it. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 25}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: The models are small language models trained to solve a specific mathematical question. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 25}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: With the exception of common Python\u2019s packages for which we only give the name, we cite the specific packages we are relying on, when relevant. No data or model from another source is used. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 26}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: No new assets are released yet. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 26}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [No] ", "page_idx": 26}, {"type": "text", "text": "Justification: The data coming from human sources are only an element of the discussion and not the main point of the paper. The framework and instructions are given as well in the discussion. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 26}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: The only data coming from human sources are answers to mathematics exercises from 25 students as part of one of their course. All students were all offered the choice of allowing or not the use of these data for the purpose of research, the answers of student who agreed were then anonymized and aggregated. This was done in coordination with the administration of the mathematics department. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}]