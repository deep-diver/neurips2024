[{"type": "text", "text": "Goal-Conditioned On-Policy Reinforcement Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xudong $\\mathbf{Gong^{1,2,\\dagger}}$ Dawei Feng1,2,\u2020 Kele $\\mathbf{X}\\mathbf{u}^{1,2,*}$ Bo Ding1,2\u2217 Huaimin Wang1,2 1 College of Computer, National University of Defense Technology, Changsha, Hunan, China 2 State Key Laboratory of Complex & Critical Software Environment, Changsha, Hunan, China ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Existing Goal-Conditioned Reinforcement Learning (GCRL) algorithms are built upon Hindsight Experience Replay (HER), which densifies rewards through hindsight replay and leverages historical goal-achieving information to construct a learning curriculum. However, when the task is characterized by a non-Markovian reward (NMR), whose computation depends on multiple steps of states and actions, HER can no longer densify rewards by treating a single encountered state as the hindsight goal. The lack of informative rewards hinders policy learning, resulting in rolling out failed trajectories. Consequently, the replay buffer is overwhelmed with failed trajectories, impeding the establishment of an applicable curriculum. To circumvent these limitations, we deviate from existing HER-based methods and propose an on-policy GCRL framework, GCPO, which is applicable to both multi-goal Markovian reward (MR) and NMR problems. GCPO consists of (1) Pre-training from Demonstrations, which pre-trains the policy to possess an initial goal-achieving capability, thereby diminishing the difficulty of subsequent online learning. (2) Online Self-Curriculum Learning, which first estimates the policy\u2019s goal-achieving capability based on historical evaluation information and then selects progressively challenging goals for learning based on its current capability. We evaluate GCPO on a challenging multi-goal long-horizon task: fixed-wing UAV velocity vector control. Experimental results demonstrate that GCPO is capable of effectively addressing both multi-goal MR and NMR problems. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Multi-goal problems are ubiquitous in real-world applications, such as controlling robotic arms to grasp objects at any location on a table [13, 16], and operating fixed-wing Unmanned Aerial Vehicles (UAVs) to navigate towards any specified velocity vector [8, 34], etc. To address the challenge of automatically learning policies capable of achieving and generalizing across a range of diverse goals [47], Goal-Conditioned Reinforcement Learning (GCRL) [48, 37] has emerged as a prominent area of research. Serving as a generalization of standard Reinforcement Learning (RL) [61], GCRL learns goal-conditioned policies [55] through interactions within multi-goal environments [48]. ", "page_idx": 0}, {"type": "text", "text": "In existing GCRL algorithms, Hindsight Experience Replay (HER) [3] plays a pivotal role in facilitating the learning of goal-conditioned policies. First, HER enhances sample efficiency by replacing the desired goals of failed trajectories with achieved states, thereby providing more informative rewards for policy learning [70, 11]. Second, HER contributes to creating a curriculum that enables the policy to progressively master challenging goals [40, 69, 47]. This process involves fitting the current policy\u2019s goal-achieving capability with historical goal-achieving information and selecting goals of appropriate difficulty for the current policy learning. Due to the reliance on replay buffers [72], current research on GCRL predominantly focuses on off-policy RL approaches [51, 25]. ", "page_idx": 0}, {"type": "image", "img_path": "KP7EUORJYI/tmp/12b4317df892b35e8e0a3ac98cfb5c4dd2200794b19f0f9ce29fc6b967010fa5.jpg", "img_caption": ["Figure 1: Left: Illustrations of HER on Markovian reward (MR) and non-Markovian reward (NMR) problems. Right: Performance of HER on MR and NMR problems. NMR[x] represents the NMR depends on the last ${\\bf X}$ consecutive states and actions. Results come from experiments on an easy version (Appendix A.7) of fixed-wing UAV velocity vector control task over 5 random seeds. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Despite its widespread success, HER is subject to a limitation: the reward function must be able to determine goal achievement based solely on the current state, which implies that the reward function must be Markovian [2]. In real-world applications, many reward functions depend on multiple steps of states and actions, i.e., rewards are non-Markovian [24, 1]. For instance, determining whether an UAV has stably achieved a goal based on a sequence of states, or calculating a penalty for control oscillation based on a sequence of actions [33, 8]. For non-Markovian reward (NMR) problems, on one hand, it is not feasible to treat a single state from the sampled trajectory as a hindsight goal; on the other hand, it is challenging for the trajectory obtained through exploration to include the special transition sequence that satisfies the specific NMR. In Fig. 1, we present a specific case to elucidate the NMR problem and illustrate why HER fails to address NMR, and corresponding experimental results to support this explanation. It is evident that as the NMR relies on longer state-action sequences, the performance of HER gradually deteriorates, ultimately becoming indistinguishable from the performance when HER is not employed. This limitation prevents HER from densifying the reward. The lack of informative rewards hinders policy learning, which further leads to rolling out failed trajectories. Consequently, the replay buffer is overwhelmed with failed trajectories, obstructing the creation of a reasonable curriculum. ", "page_idx": 1}, {"type": "text", "text": "In light of the aforementioned limitations of HER in addressing NMR problems, we propose a novel on-policy [62, 57] GCRL framework, termed the Goal-Conditioned Policy Optimization (GCPO) framework, which deviates from existing HER-based off-policy approaches. GCPO comprises: (1) Pre-training from Demonstrations: This phase involves leveraging demonstrations to pre-train the policy offline, equipping it with an initial capability to achieve goals before online learning begins, thereby diminishing the difficulty of subsequent online learning. (2) Online Self-Curriculum Learning: This phase involves periodically evaluating the policy, estimating its goal-achieving capability, and optimizing it through online learning with progressively challenging goals sampled based on its current capability. It is important to note that GCPO is not specifically designed for NMR problems; instead, it is a general on-policy GCRL framework applicable to both multi-goal Markovian reward (MR) and NMR problems. ", "page_idx": 1}, {"type": "text", "text": "To evaluate GCPO, we conduct experiments on the Velocity Vector Control (VVC) task of fixed-wing UAVs. The VVC task represents a typical multi-goal problem that can be formulated as both MR and NMR, and it also requires long interaction sequences, categorizing it as a challenging long-horizon problem [28]. The complexity inherent in the VVC task provides a rigorous testbed for evaluating the efficacy of GCPO. Experimental results indicate that GCPO effectively addresses multi-goal problems, both MR and NMR. Our contributions are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We analyze the reason why existing GCRL algorithms, which rely on HER as a central component, fail in handling NMR problems and validate this finding through experiments. \u2022 We propose an on-policy GCRL framework, GCPO, which incorporates pre-training from demonstrations and online self-curriculum learning to address both MR and NMR problems. ", "page_idx": 1}, {"type": "text", "text": "\u2022 We evaluate the performance of GCPO on the challenging VVC task, demonstrating its effectiveness in solving both MR and NMR problems. Additionally, we conduct ablation studies to analyze the influence of the two components and hyper-parameters on learning. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Goal-conditioned reinforcement learning. Many prior works assume access to a goal-conditioned reward function [15, 73] and view GCRL as a reward-driven, multi-task learning problem [68]. Existing GCRL methods are predominantly based on HER, situating them within the off-policy RL domain [61]. Pitis et al. [47] summarize existing methods and propose a common off-policy GCRL framework, which alternates between collecting experience and optimizing policies. During the policy optimization phase, the hindsight replay method is utilized to relabel transitions sampled from the replay buffer, thereby increasing informative rewards in the training data. When collecting experience, some heuristics or learning methods are employed to select appropriate goals that assist in improving the policy. ", "page_idx": 2}, {"type": "text", "text": "Selecting appropriate goals involves (1) several heuristics for discovery, including reward relevance [4, 65], diversity [14, 10], coverage [38, 6], difficulty [32, 43], etc. (2) selecting goals of appropriate difficulty based on the capability of the current policy, which essentially craft objectives that try to optimize for learning progress. These methods fit the current policy\u2019s goal-achieving capability based on the trajectories stored in the replay buffer and then sampling goals of appropriate difficulty for online learning according to this capability. For instance, RIG [40] samples goals directly from the distribution of achieved goals, DISCERN [69] samples uniformly on the support set of the distribution of achieved goals, and MEGA [47] uses inverse probability weighting sampling [32] on the distribution of achieved goals to samples goals that the current policy can achieve but not well. ", "page_idx": 2}, {"type": "text", "text": "The difference between our method and the aforementioned methods lies in the learning framework: our method is on-policy, whereas the existing methods, all based on HER, are off-policy. This difference manifests in two key aspects: First, our method does not include a component like HER, which cannot be used for solving NMR problems. To achieve an effect similar to HER\u2019s enhancement of informative rewards during learning, we design a component that pre-trains the policy with demonstrations, making it suitable for on-policy RL. Second, our method employs off-policy evaluation [63, 64] to estimate the current policy\u2019s goal-achieving capability, rather than estimating this capability based on the trajectories in the replay buffer. Table 1 summarizes the similarities and differences between GCPO and existing GCRL methods. ", "page_idx": 2}, {"type": "table", "img_path": "KP7EUORJYI/tmp/5ecb1e2fe2840650ba5c835a10a516484086c4be6789a1ae04b0b0c787e2a944.jpg", "table_caption": ["Table 1: Comparison between HER-based methods and GCPO. $p_{a g}$ refers to the distribution of achieved goals, $s u p p(\\cdot)$ refers to the support set of a distribution, $\\mathcal{U}(\\cdot)$ refers to the uniform distribution on a set, and $I P W(\\cdot)$ refers to the inverse probability weighting [32]. "], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "Non-Markovian rewards. Abel et al. [2] underscore the necessity of NMRs by demonstrating the existence of environment-task pairs for which no MR function can realize the task. Moreover, numerous exploration strategies implicitly depend on NMRs [35, 45], and studies considering nonMarkovian discount factors [19, 58] can also be interpreted as special forms of NMRs [46]. The fundamental approach for addressing NMRs involves augmenting the state space to render the reward Markovian [24]. Various techniques have been proposed to achieve this, such as Reward Machines [9] and the Split-MDP [1]. Nevertheless, the expanded reward state space may grow exponentially with the number of acceptable policies and could incorporate an infinite number of simple reward functions [1]. Our work diverges from the aforementioned research by presenting a general GCRL framework, rather than a specialized approach for NMR problems. Unlike existing GCRL methods that depend on HER, which is not applicable to NMR problems, our framework can be applied to both NMR and MR problems. Additionally, our framework is compatible with techniques designed to address NMR challenges, as demonstrated in Appendix F. ", "page_idx": 2}, {"type": "image", "img_path": "KP7EUORJYI/tmp/1b3deee75caa80281eaefef9869f7fcee59a54fe3a05f6a0931121132fb6bfcd.jpg", "img_caption": ["Figure 2: The overall GCPO framework. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "GCPO is designed as an on-policy GCRL framework. We draw upon the key insights from existing HER-based GCRL methods that have led to their success and incorporate two critical components into the GCPO framework: pre-training from demonstrations and online self-curriculum learning. The overall GCPO framework is depicted in Fig. 2 and a practical implementation of GCPO is detailed in Algorithm 1. ", "page_idx": 3}, {"type": "text", "text": "3.1 Preliminaries ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "GCRL can be described by goal-augmented MDP [37] ${\\cal M}\\;=\\;\\langle S,{\\cal A},{\\cal T},r,\\gamma,{\\mathcal G},p_{d g},\\phi\\rangle$ , where $S,A,\\gamma,\\mathcal{G}$ and $p_{d g}$ denote the state space, action space, discount factor, goal space and desired goal distribution of the environment, respectively. $T:S\\times A\\to{\\mathcal{P}}(S)$ is the transition function, where $\\mathcal{P}(\\boldsymbol{\\mathcal{X}})$ denote the probability distribution over a set $\\mathcal{X}$ . $r$ is the goal-conditioned reward function. It can be both Markovian $r\\;=\\;\\{r_{g}|r_{g}\\;:\\;{\\cal S}\\times{\\cal A}\\;\\rightarrow\\;\\mathbb{R},g\\;\\in\\;\\dot{\\cal G}\\}$ and non-Markovian $r\\,=\\,\\{r_{g}|r_{g}\\,:\\,(S\\,\\times\\,A)^{*}\\,\\to\\,\\mathbb{R},g\\,\\in\\,G\\}$ . $\\phi:{\\mathcal{S}}\\rightarrow{\\mathcal{G}}$ is a tractable mapping function that maps the state to a specific goal. The objective of GCRL is to reach goals via a goal-conditioned policy $\\pi:S\\times{\\mathcal{G}}\\to{\\bar{\\mathcal{P}}}(A)$ that maximizes the expectation of the cumulative rewards over the desired goal distribution $J(\\pi)\\,=\\,\\mathbb{E}_{a_{t}\\sim\\pi(\\cdot|s_{t},g),g\\sim p_{d g},s_{t+1}\\sim\\mathcal{T}(\\cdot|s_{t},a_{t})}\\left[\\sum_{t}\\gamma^{t}r_{g}(\\cdot)\\right]$ . Additionally, previous works [47, 37] identify two common definitions of goals: Achieved goal, which refers to the goal accomplished by the policy in the current state. The notation $p_{a g}$ denotes the distribution of achieved goals. Behavioral goal, which represents the specific task that is targeted for sampling within a rollout episode [37]. ", "page_idx": 3}, {"type": "text", "text": "3.2 Pre-Training from Demonstrations. ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "GCRL encounters more substantial exploration challenges compared to standard RL due to the inclusion of an additional goal space [30]. Pre-training from demonstrations is primarily designed to facilitate biased exploration [50]. Specifically, the policy can be pre-trained with Imitation Learning (IL) [74] or goal-conditioned IL [13, 25] on demonstrations. The pre-training provides the policy with a warm start [60, 71], which refers to an initial ability to achieve some of the desired goals. Pre-training is vital for on-policy RL as it enhances informative rewards during online learning. Without such informative rewards, the policy would struggle to acquire any meaningful knowledge or skills [61]. Through subsequent online learning, the policy can effectively discern when it is more advantageous to adhere to states and actions from the demonstration trajectories or to explore superior alternatives. ", "page_idx": 3}, {"type": "text", "text": "In our implementation of GCPO, we utilize Behavioral Cloning (BC) [49] for pre-training the policy, as indicated in line 2 of Algorithm 1. For the demonstration $\\mathcal{D}_{E}$ , the policy is learned by optimizing a supervised loss function to maximize the likelihood of expert actions [54] ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\theta)=-\\mathbb{E}_{(s,a)\\sim\\mathcal{D}_{E}}[\\log\\pi_{\\theta}(a|s)].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Require: demonstrations $\\mathcal{D}_{E}$ , distribution of desired goal $p_{d g}$ , goal weight discount factor $\\kappa$ , online   \nevaluation budget $N$ , probability transform function $f$   \nEnsure: $\\pi_{\\boldsymbol{\\theta}}(\\cdot|s,g)$   \n1: Initialize goal-conditioned policy $\\pi_{\\theta}(\\cdot|s,g)$ , goal buffer $B_{g}$ which stores tuples of achieved goals   \nin evaluation and their corresponding weight $(g,w_{g})$   \n2: pre-train $\\pi_{\\theta}(\\cdot|s,g)$ by Eq. 1 $\\triangleright$ pre-train policy   \n3: while Not converge do   \n4: for all $(g,w_{g})$ in $B_{g}$ do   \n5: $w_{g}\\leftarrow\\kappa\\cdot w_{g}$ $\\triangleright$ decay weight of historically achieved goals   \n6: end for   \n7: sample $N$ goals, $g_{1},g_{2},\\ldots,g_{N}$ uniformly from $p_{d g}$ $\\triangleright$ online policy evaluation   \n8: for all $g$ in $g_{1},g_{2},\\ldots,g_{N}$ do   \n9: if $\\pi_{\\theta}$ finishes $g$ successfully then   \n10: add $(g,1.0)$ to $B_{g}$   \n11: end if   \n12: end for   \n13: estimate $p_{a g}$ with GMM on $B_{g}$ $\\triangleright$ estimate $p_{a g}$   \n14: $\\mathcal{D}\\leftarrow\\mathcal{D}$ $\\triangleright$ roll-out samples   \n15: while Not collect enough online samples do   \n16: sample a goal $g$ from Eq. 2   \n17: sample a trajectory $\\tau$ by $\\pi_{\\theta}$ on $g$   \n18: append $\\tau$ to $\\mathcal{D}$   \n19: end while   \n20: update $\\pi_{\\theta}$ by Eq. 3 on $\\mathcal{D}$ \u25b7update policy   \n21: end while ", "page_idx": 4}, {"type": "text", "text": "3.3 Online Self-Curriculum Learning ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In GCRL, selecting goals that match the current policy\u2019s capabilities is crucial for effective learning [20, 12, 5, 11]. To address this, we design an online self-curriculum learning mechanism that autonomously constructs a curriculum, generating behavioral goals that are incrementally more difficult than those the policy is currently capable of achieving during training. Specifically, online self-curriculum learning consists of three processes: (1) Estimating the current policy\u2019s goal-achieving ability, $p_{a g}$ . This can be done through methods such as online evaluation or off-policy evaluation (OPE) [63, 64]. (2) Setting or learning a probability transform function $f:\\mathcal{P}\\times\\mathcal{P}\\to\\mathcal{P}$ , followed by sampling progressively challenging behavioral goals based on $f(p_{a g},p_{d g})$ for online learning. (3) Conducting online RL learning with behavioral goals sampled in the second part, facilitating the agent\u2019s progression towards more challenging goals. In our implementation of GCPO: ", "page_idx": 4}, {"type": "text", "text": "Gaussian Mixture Model (GMM) [53] is employed to estimate $p_{a g}$ , as detailed in lines 4-13 of Algorithm 1. During the online self-curriculum learning, the policy is periodically evaluated. In each evaluation, the policy is evaluated with $N$ goals sampled from $p_{d g}$ . Information about the achieved goals, along with an initial weight of 1.0, is stored in a goal buffer $B_{g}$ . As the online self-curriculum learning proceeds, the weight of historically achieved goals is reduced by a factor $\\kappa$ . Ultimately, a GMM is used to estimate $p_{a g}$ based on the data in $B_{g}$ (The specific calculation can be referred to Appendix B.). ", "page_idx": 4}, {"type": "text", "text": "Maximum Entropy Gain Exploration (MEGA) [47] is utilized as the probability transform function $f$ , as indicated in lines 14-19 of Algorithm 1. The core idea behind MEGA is to encourage exploration in sparsely explored areas of the achieved goal distribution. In discrete settings, inverse probability weighting [32] can be applied to sample goals from $p_{a g}$ . A goal $g$ is chosen with the probability given by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\left[f_{M E G A}(p_{a g},p_{d g})\\right](g)=\\frac{\\frac{1}{p_{a g}(g)}}{\\sum_{p^{\\prime}}\\frac{1}{p_{a g}(g^{\\prime})}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In continuous settings, a generate and test strategy [42, 47] is employed for goal sampling. Specifically, $M$ goals $\\{g_{i}\\}_{i=1}^{M}$ are randomly sampled from $s u p p(p_{a g})$ , the support set of $p_{a g}$ , and the goal with the minimum density under $f_{M E G A}(p_{a g},p_{a g})$ is selected: $g\\mathrm{~=~}\\mathrm{arg}\\operatorname*{min}_{g_{i}}\\left(\\left[f_{M E G A}(p_{a g},p_{a g})\\right](g_{i})\\right)$ This approach biases the sampling towards goals that are less likely under the current achieved goal distribution, promoting exploration in under-explored regions. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "KL-regularized RL [67, 66, 7, 27] is employed as the on-policy RL algorithm to optimize the policy, as indicated in line 20 of Algorithm 1. To prevent catastrophic forgetting of latent skills and to continuously improve exploration during the RL fine-tuning phase [7], the policy $\\pi_{\\theta}$ is initially set to the pre-trained policy $\\pi_{\\theta_{0}}$ and is then fine-tuned by maximizing the following objective: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{J_{k l}(\\pi_{\\theta})=\\mathbb{E}\\Big[\\sum_{t}\\gamma^{t}\\big(r-\\lambda l o g\\big(\\frac{\\pi_{\\theta}\\left(a_{t}|s_{t}\\right)}{\\pi_{\\theta_{0}}\\left(a_{t}|s_{t}\\right)}\\big)\\big)\\Big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $r$ can represent both Markovian $r(s_{t},a_{t})$ and non-Markovian $r(s_{0:t},a_{0:t})$ rewards, and $\\lambda\\in$ $[0,1]$ controls the strength of the KL regularization. Optimizing Eq. 3 is analogous to optimizing the original RL objective within the log-barrier of $\\pi_{\\theta_{0}}$ , and can be viewed as a trust-region-style [56] learning objective [36]. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Experimental Setups ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "RL environment. Experiments are conducted on the Fixed-Wing UAV Velocity Vector Control (VVC) task [26], which is a representative multi-goal problem. The VVC task is characterized by a long horizon, with the average length of demonstrations exceeding 280 steps (detailed in Appendix A.5). Even for well-trained policies, the average number of steps required to achieve a goal is over 100, and more challenging goals can demand upwards of 300 steps to achieve [27]. This exceeds the horizon typically used in most previous studies [41, 47, 28, 59]. Additionally, the rewards for VVC can be designed as either MR or NMR, thereby accommodating a range of real-world requirements and complexities. The specifics of the VVC environment setup are detailed in Appendix A. Thus, VVC presents a challenging multi-goal, long-horizon problem that poses significant difficulties for policy learning. Standard SAC+HER [3] and PPO [57] are unable to solve the VVC task, as demonstrated in Appendix A.6, further highlighting the task\u2019s complexity. To our knowledge, previous research on NMR algorithms has primarily been tested in simpler environments such as multi-arm bandits [24] and grid worlds [52, 24, 2, 1, 31]. Our work is the first to evaluate the performance of algorithms on complex, real-world NMR problems. Additionally, to demonstrate the broad applicability of GCPO, we conduct experiments on the commonly used RL environments Reach and PointMaze. The corresponding results and analysis can be found in Appendix D. ", "page_idx": 5}, {"type": "text", "text": "Demonstrations. We collect a demonstration set $\\mathcal{D}_{E}$ , also denoted as $\\mathcal{D}_{E}^{0}$ , with a PID controller (detailed in Appendix A.5). Subsequently, we employ the IRPO algorithm [27], which iteratively optimizes policies and demonstrations, to generate $\\mathcal{D}_{E}^{1},\\mathcal{D}_{E}^{2},\\mathcal{D}_{E}^{3}$ . Table 2 presents the quantity and quality of these four demonstration sets. The \u2019#traj\u2019 column represents the number of demonstrations contained within each demonstration set, while \u2019traj length\u2019 indicates the average length of demonstrations in the set. A shorter demonstration length suggests a faster completion of the corresponding goal, indicative of higher demonstration quality. It can be observed that the demonstration quantity and quality of $\\mathcal{D}_{E}^{0},\\bar{D_{E}^{1}},D_{E}^{2},D_{E}^{3}$ increase sequentially. ", "page_idx": 5}, {"type": "text", "text": "4.2 Main Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We compare GCPO with several baselines on the VVC task under different demonstration conditions, including (1) SAC $[29]+\\mathrm{HER}+\\mathrm{MEGA}$ , which is a strong baseline in GCRL; (2) BC, a fundamental yet effective IL algorithm; (3) GCPO without pre-training, which corresponds to PPO $^+$ self-curriculum; (4) GCPO without self-curriculum, which corresponds to $\\mathrm{BC}+\\mathrm{KL}$ -regularized RL. GCPO itself is equivalent to $\\mathrm{BC}+\\mathrm{KL}$ -regularized $\\mathrm{RL+}$ self-curriculum. Table 2 reports the performance of GCPO and the baselines on NMR, and Fig. 3 visualizes the learning progression on both NMR and MR, as well as the final learned policy of GCPO. ", "page_idx": 5}, {"type": "text", "text": "GCPO is applicable to both MR and NMR problems. Table 2 shows that GCPO outperforms all baselines on NMR, with SAC+HER $\\pm$ MEGA achieving only $20\\%$ of GCPO\u2019s performance. This demonstrates the limitations of HER in addressing NMR problems and highlights the superiority of GCPO in these contexts. Furthermore, Fig. 3a illustrates the learning progression of GCPO for both NMR and MR, showing that GCPO is effective in solving both types of problems. In summary, GCPO exhibits versatility and applicability across both MR and NMR problems. ", "page_idx": 5}, {"type": "table", "img_path": "KP7EUORJYI/tmp/c301c367778b6d8ced8d52e408eb46171be2c38d3f9871ba2be7a95665a502d7.jpg", "table_caption": ["Table 2: Comparison between GCPO and baselines on NMR. The mean and variance of $\\%$ success rates are presented over 5 random seeds. Optimal values are highlighted in bold, and sub-optimal values are underlined. "], "table_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "KP7EUORJYI/tmp/c60433f5baa91a3a6e67ff017393662dc1766eedfe4def9d7f3a6e941e69864a.jpg", "img_caption": ["(a) Success rate on MR and NMR (b) Histogram of achieved goals (c) Distribution of goals from selfcurriculum during learning ", "Figure 3: Main results of GCPO. \u2018expert\u2019 refers to the demonstrator that generates demonstrations. \u2018BC\u2019 refers to the pre-trained policy. Results are derived from experiments across 5 random seeds. For sub-figure (a), expert and BC are both evaluated in the NMR setting. For sub-figure (c), the vertical axis represents the training progress, where $0,1,\\cdot\\cdot\\cdot,9$ correspond to $10\\%$ , $20\\bar{\\%}$ , \u00b7 \u00b7 \u00b7 , $100\\%$ of the training progress, respectively. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Pre-training is crucial for the success of GCPO. As evidenced by the experiments in Table 2 for BC, GCPO w/o pre-training, and GCPO, it is observed that without pre-training, GCPO struggles to learn meaningful skills. However, a policy that is merely pre-trained, albeit with non-optimal performance, plays a crucial role in enabling GCPO to develop an effective policy. Fig. 3b provides a histogram of the achieved goals for the trained policies. It is evident that, even with a pre-trained policy that initially exhibits inferior performance compared to the demonstrator, GCPO\u2019s online self-curriculum learning facilitates significant improvement in the policy\u2019s performance, surpassing that of the demonstrator. ", "page_idx": 6}, {"type": "text", "text": "Online self-curriculum facilitates the mastery of challenging goals. Table 2 demonstrates that the application of self-curriculum within GCPO leads to an average $8.2\\%$ increase in policy performance compared to its absence. This enhancement is illustrated in Fig. 3c, which shows that the online selfcurriculum mechanism systematically introduces more difficult goals into the learning progression as the policy gains proficiency. This mechanism effectively explains the advantages of self-curriculum for GCPO in mastering challenging goals. ", "page_idx": 6}, {"type": "text", "text": "4.3 Ablation Studies ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we conduct ablation studies on the demonstration\u2019s quantity and goal distribution, analyze the sensitivity of GCPO to the parameters used for estimating $p_{a g}$ , and compare the effectiveness of different self-curriculum methods. ", "page_idx": 6}, {"type": "text", "text": "4.3.1 Ablation on Quantity of Demonstrations ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To illustrate the influence of the quantity of demonstrations on GCPO, we train GCPO with $10\\%$ , $50\\%$ , and $100\\%$ of $\\mathcal{D}_{E}$ and present the performance of pre-trained policies and GCPO policies in Fig. 4a. ", "page_idx": 6}, {"type": "image", "img_path": "KP7EUORJYI/tmp/d597ca841322fced8856b681b96e5430ac4585223dbaf482bcc352b25da4f3bd.jpg", "img_caption": ["(a) Success rate of GCPO with dif-(b) Histogram of achieved goals of(c) Histogram of achieved goals of ferent demonstration quantity the pre-trained policy GCPO policy "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 4: The influence of demonstration quantity and the distribution of goals covered by demonrespectively. The pre-trained policies obtained from strations on GCPO. $\\mathcal{D}_{1},\\mathcal{D}_{2},\\mathcal{D}_{3}$ represent sets of demonstrations that are difficult, medium, and easy, $\\mathcal{D}_{1}$ , $\\mathcal{D}_{2}$ , and $\\mathcal{D}_{3}$ are denoted as $\\pi_{1}^{0}$ , $\\pi_{2}^{0}$ , and $\\pi_{3}^{\\mathrm{()}}$ respectively. The corresponding GCPO policies are denoted as $\\pi_{1}^{*}$ , $\\pi_{2}^{*}$ , and $\\pi_{3}^{*}$ , respectively. Results are derived from experiments across 5 random seeds. ", "page_idx": 7}, {"type": "text", "text": "Table 3: Performance of GCPO policies trained with different $N$ and $\\kappa$ . In our settings, $N=32$ implies that the number of evaluations throughout the training is approximately equal to the number of goals obtained through discretizing the entire goal space during sampling demonstrations (detailed in Appendix A.5). The mean and variance of $\\%$ success rates are shown over 5 random seeds. Optimal values are highlighted in bold, and sub-optimal values are underlined. ", "page_idx": 7}, {"type": "table", "img_path": "KP7EUORJYI/tmp/7784566149fa1a218d370f036d18e4d01d88fdb4ca73b326804981674bcaaa08.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "As shown, a policy pre-trained with $10\\%$ of $\\mathcal{D}_{E}$ achieves $81.12\\%$ of the performance of the policy pre-trained with $100\\%$ of $\\mathcal{D}_{E}$ , while a policy pre-trained with $50\\%$ of $\\mathcal{D}_{E}$ achieves $96.53\\%$ of the performance. Similarly, a policy trained by GCPO with $10\\%$ of $\\mathcal{D}_{E}$ achieves $86.95\\%$ of the performance of the policy trained by GCPO with $100\\%$ of $\\mathcal{D}_{E}$ , whereas a policy trained by GCPO with $50\\%$ of $\\mathcal{D}_{E}$ achieves $95.94\\%$ of the performance. These results suggest that an increase in the quantity of demonstrations can enhance the performance of GCPO, yet the marginal gains diminish as the quantity of demonstrations grows. Furthermore, these results also indicate that GCPO can still perform well when only a relatively small number of demonstrations are available. ", "page_idx": 7}, {"type": "text", "text": "4.3.2 Ablation on Goal Distribution of Demonstrations ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To demonstrate the influence of the distribution of goals covered by the demonstrations on GCPO, we collect three demonstration sets with significantly different goal difficulty distributions (detailed in Appendix A.4) and train GCPO with them. Fig. 4b presents the distribution of achieved goals of the pre-trained policies, while Fig. 4c depicts that of the GCPO policy. ", "page_idx": 7}, {"type": "text", "text": "It is evident that for both pre-trained policies and GCPO policies, their distributions of achieved goals are centered around the distribution of goals covered by the demonstrations. The reason for this is that the self-curriculum, starting with the distribution of achieved goals of the pre-trained policy, which is determined by the distribution of goals covered by the demonstrations, gradually expands the distribution of achieved goals. The preference for goals in the demonstrations thus influences the learning progression of GCPO, leading the policy learned by GCPO to also exhibit a similar preference for goals. This suggests that when preparing demonstrations for GCPO, it is preferable to sample goals and generate demonstrations as closely as possible to the desired goal distribution $p_{d g}$ . ", "page_idx": 7}, {"type": "text", "text": "Furthermore, in Appendix E.1, we provide a more intuitive case and directly visualize the achieved goals in three-dimensional space, yielding the same conclusions as those from the above analysis. ", "page_idx": 7}, {"type": "image", "img_path": "KP7EUORJYI/tmp/7b88794aa8899a19d15237a4e3abfd53e9f9b34aaf83b0d822c8593ff38eef56.jpg", "img_caption": ["(a) Difficulty of goals sam-(b) Success rate of(c) Histogram of achieved(d) Histogram of achieved pled by self-curriculumself-curriculum andgoals of different self-goals of MEGA and nonmethods non-curriculum methods curriculum methods curriculum methods "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 5: Analysis of the influence of different self-curriculum methods on the learning progression of GCPO, as well as a comparison between self-curriculum and non-curriculum methods. \u2018expert\u2019 and \u2018None\u2019 are two non-curriculum methods, where \u2018expert\u2019 refers to sampling goals from those that the demonstrator can achieve, and \u2018None\u2019 signifies directly sampling from $p_{d g}$ . Results are derived from experiments across 5 random seeds. ", "page_idx": 8}, {"type": "text", "text": "4.3.3 Sensitivity to Parameters of Estimating $p_{a g}$ ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To evaluate the influence of the estimation of $p_{a g}$ on GCPO, we conduct experiments with GCPO across a range of values for $N$ and $\\kappa$ , employing various self-curriculum methods. The results are presented in Table 3. ", "page_idx": 8}, {"type": "text", "text": "The internal three-by-three layout of each sub-table reveals that policies with the highest performance tend to be situated at configurations where $N$ is larger and $\\kappa$ is smaller. This trend suggests that under these conditions, the estimation of $p_{a g}$ is more precise. Ideally, as $N\\to\\operatorname*{inf}$ and $\\kappa\\rightarrow0$ , the estimation of $p_{a g}$ could be perfectly fitted. when examining $N$ and $\\kappa$ independently, the last row of each sub-table indicates that conducting more evaluations during online learning helps GCPO to obtain a well-performing policy, although this comes at the cost of additional computational resources. Conversely, the sum of each row of each sub-table shows no significant difference, implying that GCPO is less sensitive to the setting of $\\kappa$ . ", "page_idx": 8}, {"type": "text", "text": "In summary, from the perspective of estimating $p_{a g}$ , to enhance the performance of GCPO, it is primarily advisable to increase the number of evaluations within the tolerable computational resource constraints. ", "page_idx": 8}, {"type": "text", "text": "4.3.4 Comparison on Different Self-Curriculum Methods ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To demonstrate the influence of different self-curriculum methods on the learning progression and final policy of GCPO, we train GCPO with three distinct self-curriculum methods: RIG, DISCERN, and MEGA. The learning curve and the final policy performance are illustrated in Fig. 5. ", "page_idx": 8}, {"type": "text", "text": "Fig. 5a presents the curve of goal difficulty sampled during learning. It is noted that RIG rapidly samples more challenging goals, followed by DISCERN, and then MEGA. However, when considering the difficulty of the goals sampled at the final stage of learning, DISCERN and MEGA select harder goals than RIG. This observation suggests that RIG, DISCERN, and MEGA exhibit distinctly different learning progressions. ", "page_idx": 8}, {"type": "text", "text": "Fig. 5b depicts the trend of success rate during learning, and Fig. 5c and 5d present the histograms of achieved goals for the policies trained by self-curriculum and non-curriculum methods, respectively. By combining Figs. 5b and 5c, it is evident that there is no significant difference in performance between different self-curriculum methods, whether in the learning progression or in the final policy. In contrast, when combining Figs. 5b and 5d, it is clear that self-curriculum methods outperform non-curriculum methods in both the learning progression and the final policy performance. ", "page_idx": 8}, {"type": "text", "text": "In summary, within the GCPO framework, while different self-curriculum methods exhibit distinct learning progressions, there is no discernible difference in the final policy obtained. Moreover, the self-curriculum methods consistently outperform non-curriculum methods, highlighting the effectiveness of the self-curriculum mechanism in promoting goal-conditioned policy learning. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion and Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we propose an on-policy goal-conditioned reinforcement learning framework, GCPO, designed to address the limitations of existing methods in solving non-Markovian reward (NMR) problems. Through experimental evaluation on the fixed-wing Velocity Vector Control task, we demonstrate the effectiveness of GCPO in handling both Markovian reward (MR) and NMR problems. ", "page_idx": 9}, {"type": "text", "text": "Some limitations should be addressed in future work. Firstly, in the implementation of the two components within GCPO, we employ relatively simple methods, such as behavioral cloning and Gaussian mixture model. Whether the use of alternative methods could lead to more efficient learning and better-performing policies is yet to be further validated. Secondly, under the sparse reward setting, the successful training of GCPO relies on the pre-trained policy possessing a certain level of goal-achieving capability. Otherwise, if the policy achieves nothing, it becomes ineffective in establishing a self-curriculum. Lastly, although the GCPO framework does not have a component like HER that is unsuitable for solving NMR problems and thus capable of solving both MR and NMR problems, the specific implementation of GCPO as introduced in Section 1 does not explicitly incorporate components that are specifically designed to handle NMR problems. In Appendix F, we introduce a simple component to address NMR problems within GCPO and observe some effects. However, it is not clear whether integrating the most advanced methods for handling NMR problems within GCPO would lead to a more effective resolution. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by the National Key R&D Program of China (No. 2021ZD0112904). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] David Abel, Andr\u00e9 Barreto, Michael Bowling, Will Dabney, Steven Hansen, Anna Harutyunyan, Mark K Ho, Ramana Kumar, Michael L Littman, Doina Precup, et al. Expressing non-markov reward to a markov agent. In Multidisciplinary Conference on Reinforcement Learning and Decision Making, 2022.   \n[2] David Abel, Will Dabney, Anna Harutyunyan, Mark K Ho, Michael Littman, Doina Precup, and Satinder Singh. On the expressivity of markov reward. Advances in Neural Information Processing Systems, 34:7799\u20137812, 2021.   \n[3] Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. Advances in Neural Information Processing Systems, 30, 2017.   \n[4] Pierre-Luc Bacon, Jean Harb, and Doina Precup. The option-critic architecture. In Proceedings of the AAAI conference on artificial intelligence, volume 31, 2017.   \n[5] Akhil Bagaria and Tom Schaul. Scaling goal-based exploration via pruning proto-goals. In Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, pages 3451\u20133460, 2023.   \n[6] Akhil Bagaria, Jason K Senthil, and George Konidaris. Skill discovery for exploration and planning using deep skill graphs. In International Conference on Machine Learning, pages 521\u2013531. PMLR, 2021.   \n[7] Bowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampedro, and Jeff Clune. Video pretraining (vpt): Learning to act by watching unlabeled online videos. Advances in Neural Information Processing Systems, 35:24639\u201324654, 2022.   \n[8] Eivind B\u00f8hn, Erlend M Coates, Signe Moe, and Tor Ame Johansen. Deep reinforcement learning attitude control of fixed-wing uavs using proximal policy optimization. In International Conference on Unmanned Aircraft Systems, pages 523\u2013533. IEEE, 2019.   \n[9] Alberto Camacho, Rodrigo Toro Icarte, Toryn Q Klassen, Richard Anthony Valenzano, and Sheila A McIlraith. Ltl and beyond: Formal languages for reward function specification in reinforcement learning. In IJCAI, volume 19, pages 6065\u20136073, 2019.   \n[10] V\u00edctor Campos, Alexander Trott, Caiming Xiong, Richard Socher, Xavier Gir\u00f3-i Nieto, and Jordi Torres. Explore, discover and learn: Unsupervised discovery of state-covering skills. In International Conference on Machine Learning, pages 1317\u20131327. PMLR, 2020.   \n[11] Nicolas Castanet, Sylvain Lamprier, and Olivier Sigaud. Stein variational goal generation for reinforcement learning in hard exploration problems. International Conference on Machine Learning, 2023.   \n[12] C\u00e9dric Colas, Pierre Fournier, Mohamed Chetouani, Olivier Sigaud, and Pierre-Yves Oudeyer. Curious: intrinsically motivated modular multi-goal reinforcement learning. In International conference on machine learning, pages 1331\u20131340. PMLR, 2019.   \n[13] Yiming Ding, Carlos Florensa, Pieter Abbeel, and Mariano Phielipp. Goal-conditioned imitation learning. Advances in Neural Information Processing Systems, 32, 2019.   \n[14] Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need: Learning skills without a reward function. In International Conference on Learning Representations, 2018.   \n[15] Benjamin Eysenbach, Ruslan Salakhutdinov, and Sergey Levine. C-learning: Learning to achieve goals via recursive classification. In International Conference on Learning Representations, 2020.   \n[16] Benjamin Eysenbach, Tianjun Zhang, Sergey Levine, and Russ R Salakhutdinov. Contrastive learning as goal-conditioned reinforcement learning. Advances in Neural Information Processing Systems, 35:35603\u201335620, 2022.   \n[17] Farama. https://robotics.farama.org/, 2024.   \n[18] Farama. https://minari.farama.org/, 2024.   \n[19] William Fedus, Carles Gelada, Yoshua Bengio, Marc G Bellemare, and Hugo Larochelle. Hyperbolic discounting and learning over multiple horizons. arXiv preprint arXiv:1902.06865, 2019.   \n[20] Carlos Florensa, David Held, Xinyang Geng, and Pieter Abbeel. Automatic goal generation for reinforcement learning agents. In International conference on machine learning, pages 1515\u20131528. PMLR, 2018.   \n[21] Daniel Frisch and Uwe D Hanebeck. Gaussian mixture estimation from weighted samples. In 2021 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems (MFI), pages 1\u20135. IEEE, 2021.   \n[22] Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020.   \n[23] Quentin Gallou\u00e9dec, Nicolas Cazin, Emmanuel Dellandr\u00e9a, and Liming Chen. panda-gym: Open-source goal-conditioned environments for robotic learning. 4th Robot Learning Workshop: Self-Supervised and Lifelong Learning at NeurIPS, 2021.   \n[24] Maor Gaon and Ronen Brafman. Reinforcement learning with non-markovian rewards. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 3980\u20133987, 2020.   \n[25] Dibya Ghosh, Abhishek Gupta, Ashwin Reddy, Justin Fu, Coline Manon Devin, Benjamin Eysenbach, and Sergey Levine. Learning to reach goals via iterated supervised learning. In International Conference on Learning Representations, 2020.   \n[26] Xudong Gong, Dawei Feng, Kele Xu, Weijia Wang, Zhangjun Sun, Xing Zhou, Bo Ding, and Huaimin Wang. Flycraft: An efficient goal-conditioned reinforcement learning environment for fixed-wing uav velocity vector control. https://github.com/GongXudong/fly-craft, 2024.   \n[27] Xudong Gong, Dawei Feng, Kele Xu, Yuanzhao Zhai, ChengKang Yao, Weijia Wang, Bo Ding, and Huaimin Wang. Iterative regularized policy optimization with imperfect demonstrations. In International Conference on Machine Learning. PMLR, 2024.   \n[28] Abhishek Gupta, Vikash Kumar, Corey Lynch, Sergey Levine, and Karol Hausman. Relay policy learning: Solving long-horizon tasks via imitation and reinforcement learning. In Conference on Robot Learning, pages 1025\u20131037. PMLR, 2020.   \n[29] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic actor. In International conference on machine learning, pages 1861\u20131870. PMLR, 2018.   \n[30] Edward S Hu, Richard Chang, Oleh Rybkin, and Dinesh Jayaraman. Planning goals for exploration. In The Eleventh International Conference on Learning Representations, 2022.   \n[31] Gregory Hyde and Eugene Santos Jr. Detecting hidden triggers: Mapping non-markov reward functions to markov. arXiv preprint arXiv:2401.11325, 2024.   \n[32] Alexis D Jacq, Manu Orsini, Gabriel Dulac-Arnold, Olivier Pietquin, Matthieu Geist, and Olivier Bachem. On the importance of data collection for training general goal-reaching policies. In Sixteenth European Workshop on Reinforcement Learning, 2023.   \n[33] William Koch, Renato Mancuso, and Azer Bestavros. Neurofilght: next generation filght control firmware. Computing Research Repository, 2019.   \n[34] William Koch, Renato Mancuso, Richard West, and Azer Bestavros. Reinforcement learning for uav attitude control. ACM Transactions on Cyber-Physical Systems, 3(2):1\u201321, 2019.   \n[35] J Zico Kolter and Andrew Y Ng. Near-bayesian exploration in polynomial time. In Proceedings of the 26th annual international conference on machine learning, pages 513\u2013520, 2009.   \n[36] Jianxiong Li, Xiao Hu, Haoran Xu, Jingjing Liu, Xianyuan Zhan, and Ya-Qin Zhang. Proto: Iterative policy regularized offline-to-online reinforcement learning. arXiv preprint arXiv:2305.15669, 2023.   \n[37] Minghuan Liu, Menghui Zhu, and Weinan Zhang. Goal-conditioned reinforcement learning: Problems and solutions. International Joint Conference on Artificial Intelligence, 2022.   \n[38] Marlos C Machado, Marc G Bellemare, and Michael Bowling. A laplacian framework for option discovery in reinforcement learning. In International Conference on Machine Learning, pages 2295\u20132304. PMLR, 2017.   \n[39] Siddharth Mysore, Bassel Mabsout, Renato Mancuso, and Kate Saenko. Regularizing action policies for smooth control with reinforcement learning. In 2021 IEEE International Conference on Robotics and Automation (ICRA), pages 1810\u20131816. IEEE, 2021.   \n[40] Ashvin V Nair, Vitchyr Pong, Murtaza Dalal, Shikhar Bahl, Steven Lin, and Sergey Levine. Visual reinforcement learning with imagined goals. Advances in Neural Information Processing Systems, 31, 2018.   \n[41] Soroush Nasiriany, Vitchyr Pong, Steven Lin, and Sergey Levine. Planning with goalconditioned policies. Advances in Neural Information Processing Systems, 32, 2019.   \n[42] Allen Newell. Artificial intelligence and the concept of mind. Carnegie-Mellon University, Department of Computer Science, 1969.   \n[43] Yaru Niu, Shiyu Jin, Zeqing Zhang, Jiacheng Zhu, Ding Zhao, and Liangjun Zhang. Goats: Goal sampling adaptation for scooping with curriculum reinforcement learning. In 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 1023\u20131030. IEEE, 2023.   \n[44] Panda-Gym. https://panda-gym.readthedocs.io/en/latest/usage/manual_ control.html, 2024.   \n[45] Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration by self-supervised prediction. In International conference on machine learning, pages 2778\u2013 2787. PMLR, 2017.   \n[46] Silviu Pitis, Duncan Bailey, and Jimmy Ba. Rational multi-objective agents must admit nonmarkov reward representations. In NeurIPS ML Safety Workshop, 2022.   \n[47] Silviu Pitis, Harris Chan, Stephen Zhao, Bradly Stadie, and Jimmy Ba. Maximum entropy gain exploration for long horizon multi-goal reinforcement learning. In International Conference on Machine Learning, pages 7750\u20137761. PMLR, 2020.   \n[48] Matthias Plappert, Marcin Andrychowicz, Alex Ray, Bob McGrew, Bowen Baker, Glenn Powell, Jonas Schneider, Josh Tobin, Maciek Chociej, Peter Welinder, et al. Multi-goal reinforcement learning: Challenging robotics environments and request for research. arXiv preprint arXiv:1802.09464, 2018.   \n[49] Dean A Pomerleau. Efficient training of artificial neural networks for autonomous navigation. Neural computation, 3(1):88\u201397, 1991.   \n[50] Jorge Ram\u00edrez, Wen Yu, and Adolfo Perrusqu\u00eda. Model-free reinforcement learning from expert demonstrations: a survey. Artificial Intelligence Review, 55(4):3213\u20133241, 2022.   \n[51] Paulo Rauber, Avinash Ummadisingu, Filipe Mutz, and J\u00fcrgen Schmidhuber. Hindsight policy gradients. In International Conference on Learning Representations, 2018.   \n[52] Gavin Rens and Jean-Fran\u00e7ois Raskin. Learning non-markovian reward models in mdps. arXiv preprint arXiv:2001.09293, 2020.   \n[53] Douglas A Reynolds et al. Gaussian mixture models. Encyclopedia of biometrics, 741(659-663), 2009.   \n[54] Fumihiro Sasaki and Ryota Yamashina. Behavioral cloning from noisy demonstrations. In International Conference on Learning Representations, 2020.   \n[55] Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver. Universal value function approximators. In International Conference on Machine Learning, pages 1312\u20131320. PMLR, 2015.   \n[56] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International conference on machine learning, pages 1889\u20131897. PMLR, 2015.   \n[57] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.   \n[58] Matthias Schultheis, Constantin A Rothkopf, and Heinz Koeppl. Reinforcement learning with non-exponential discounting. Advances in neural information processing systems, 35:3649\u2013 3662, 2022.   \n[59] Wonchul Shin and Yusung Kim. Guide to control: offline hierarchical reinforcement learning using subgoal generation for long-horizon and sparse-reward tasks. In Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, pages 4217\u20134225, 2023.   \n[60] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484\u2013489, 2016.   \n[61] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.   \n[62] Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. Advances in neural information processing systems, 12, 1999.   \n[63] Philip Thomas, Georgios Theocharous, and Mohammad Ghavamzadeh. High-confidence offpolicy evaluation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 29, 2015.   \n[64] Masatoshi Uehara, Chengchun Shi, and Nathan Kallus. A review of off-policy evaluation in reinforcement learning. arXiv preprint arXiv:2212.06355, 2022.   \n[65] Vivek Veeriah, Tom Zahavy, Matteo Hessel, Zhongwen Xu, Junhyuk Oh, Iurii Kemaev, Hado P van Hasselt, David Silver, and Satinder Singh. Discovery of options via meta-learned subgoals. Advances in Neural Information Processing Systems, 34:29861\u201329873, 2021.   \n[66] Nino Vieillard, Tadashi Kozuno, Bruno Scherrer, Olivier Pietquin, R\u00e9mi Munos, and Matthieu Geist. Leverage the average: an analysis of kl regularization in reinforcement learning. Advances in Neural Information Processing Systems, 33:12163\u201312174, 2020.   \n[67] Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Micha\u00ebl Mathieu, Andrew Dudzik, Junyoung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350\u2013354, 2019.   \n[68] Nelson Vithayathil Varghese and Qusay H Mahmoud. A survey of multi-task deep reinforcement learning. Electronics, 9(9):1363, 2020.   \n[69] David Warde-Farley, Tom Van de Wiele, Tejas Kulkarni, Catalin Ionescu, Steven Hansen, and Volodymyr Mnih. Unsupervised control through non-parametric discriminative rewards. In International Conference on Learning Representations, 2018.   \n[70] Jiawei Xu, Shuxing Li, Rui Yang, Chun Yuan, and Lei Han. Efficient multi-goal reinforcement learning via value consistency prioritization. Journal of Artificial Intelligence Research, 77:355\u2013 376, 2023.   \n[71] Haoqi Yuan, Zhancun Mu, Feiyang Xie, and Zongqing Lu. Pre-training goal-based models for sample-efficient reinforcement learning. In International Conference on Learning Representations, 2024.   \n[72] Shangtong Zhang and Richard S Sutton. A deeper look at experience replay. arXiv preprint arXiv:1712.01275, 2017.   \n[73] Tianjun Zhang, Benjamin Eysenbach, Ruslan Salakhutdinov, Sergey Levine, and Joseph E Gonzalez. C-planning: An automatic curriculum for learning goal-reaching tasks. In International Conference on Learning Representations, 2021.   \n[74] Boyuan Zheng, Sunny Verma, Jianlong Zhou, Ivor W Tsang, and Fang Chen. Imitation learning: Progress, taxonomies and challenges. IEEE Transactions on Neural Networks and Learning Systems, (99):1\u201316, 2022. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A The Fixed-Wing Velocity Vector Control Task ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The fixed-wing UAV\u2019s VVC task is to target its velocity vector to a target velocity vector. ", "page_idx": 14}, {"type": "text", "text": "A.1 State, Action, and Goal Space ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The state consists of pitch angle $\\theta$ , roll angle $\\phi$ , yaw angle $\\psi$ , filght path azimuth angle $\\chi$ , filght path elevator angle $\\mu$ , altitude $h$ , roll angular velocity $p$ , true airspeed $v$ , and goal $(v_{g},\\mu_{g},\\chi_{g})$ . The action consists of $a i l,e l e,r u d,p l a$ , which denotes the actuator position of the aileron, elevator, rudder, and power level actuator. The goal space is defined as $\\mathcal{G}:=[\\bar{v_{m i n}},v_{m a x}]\\times[\\mu_{m i n},\\mu_{m a x}]\\times[\\chi_{m i n},\\chi_{m a x}]$ When the environment is reset, a goal $g:=(v,\\mu,\\chi)$ is sampled randomly from $\\mathcal{G}$ . ", "page_idx": 14}, {"type": "text", "text": "A.2 Transition ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The action $(a i l,e l e,r u d,p l a)$ is sent to the Flight Dynamics Model (FDM) to get the next state with the F-16 model. The episode terminates when triggers one of the following two conditions: (1) if $v,\\mu,\\chi$ is close to $(v_{g},\\mu_{g},\\chi_{g})$ within the tolerant error $\\delta$ which is described in the Appendix A.3. (2) if does not trigger the first condition for $T_{m a x}$ steps. ", "page_idx": 14}, {"type": "text", "text": "A.3 Reward Function ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The Markovian reward function is defined as ", "page_idx": 14}, {"type": "equation", "text": "$$\nr_{g}(s_{t})=\\left\\{\\begin{array}{c}{0,\\,i f\\;d(\\phi(s_{t}),g)<\\delta}\\\\ {-1,\\,e l s e,}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\begin{array}{r}{d(\\phi(s_{t}),g)=w_{v}\\frac{\\lVert\\vec{v}_{t}-\\vec{v}_{g}\\rVert_{v}}{\\sigma_{v}}+w_{d}\\frac{\\lVert\\vec{v}_{t}-\\vec{v}_{g}\\rVert_{d}}{\\sigma_{d}}}\\end{array}$ d\u2225\u20d7vt\u2212\u03c3d\u20d7vg\u2225d, wv \u2208[0, 1], wd \u2208[0, 1], wv + wd = 1.0 are weight factors for velocity and direction, $\\sigma_{v},\\sigma_{d}$ are scaling factors for velocity and direction, $\\Vert.\\Vert_{v}$ calculates the difference in modulus of two velocity vectors, $\\lVert.\\rVert_{d}$ calculates the difference in direction of two velocity vectors, and $\\delta$ is a pre-defined tolerant error. ", "page_idx": 14}, {"type": "text", "text": "The non-Markovian reward function is defined as ", "page_idx": 14}, {"type": "equation", "text": "$$\nr_{g}(s_{t-N_{r}+1},\\cdot\\cdot\\cdot,s_{t})=\\left\\{\\begin{array}{c}{0,\\,i f\\ a l l\\ d(\\phi(s_{t^{\\prime}}),g)<\\delta,\\;t^{\\prime}\\in[t-N_{r}+1,t-N_{r}+2,\\cdot\\cdot\\cdot,t]}\\\\ {-1,\\,e l s e,}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $N_{r}$ is the horizon length that the NMR depends on. ", "page_idx": 14}, {"type": "text", "text": "A.4 Goal Difficulty ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In order to evaluate the quality of demonstrations in the following sections, we introduce the goal difficulty ", "page_idx": 14}, {"type": "equation", "text": "$$\nd_{v}(g,v_{0})=\\alpha_{v}+(1-\\alpha_{v})\\frac{|v_{g}-v_{0}|}{|v_{m a x}-v_{m i n}|},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\alpha_{v}\\in[0,1)$ is a base value of difficulty for $v$ . And the same is for $d_{\\mu}(g,\\mu_{0})$ and $d_{\\chi}(g,\\chi_{0})$ . Consequently, the difficulty of the goal is defined as ", "page_idx": 14}, {"type": "equation", "text": "$$\nd(g,v_{0},\\mu_{0},\\chi_{0})=d_{v}(g,v_{0})\\cdot d_{\\mu}(g,\\mu_{0})\\cdot d_{\\chi}(g,\\chi_{0}),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which describes the magnitude of changes in the UAV\u2019s state variables. ", "page_idx": 14}, {"type": "text", "text": "Based on Eq. 7, we sort all goals based on their difficulty and define the following three goal sets: the easy goal set, comprising the 100 simplest goals; the medium goal set, consisting of goals with difficulty values ranked between 3000 and 3100; and the difficult goal set, comprising goals with difficulty values ranked between 7000 and 7100. ", "page_idx": 14}, {"type": "text", "text": "A.5 Demonstrations ", "text_level": 1, "page_idx": 14}, {"type": "image", "img_path": "KP7EUORJYI/tmp/951bd8d2480bf71c0ea8b5fd1b4d86d2ab2ce10d4ff179184508cd10a21b6555.jpg", "img_caption": ["Figure 6: Performance of $\\scriptstyle\\mathrm{SAC}+\\mathrm{HER}$ and PPO on the VVC task. Results come from experiments on attitude control over 5 random seeds. "], "img_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "KP7EUORJYI/tmp/6441cadef61245f86c20c4f60838211fbaee556c06bdfcab6dda71f99e1622a6.jpg", "table_caption": ["Table 5: Parameters used in Environment (a) Easy Version (b) Hard Version "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "A PID controller is used to sample trajectories. For convenience, the goal space is discretized with parameters listed in Table 4. Of the 50715 discretized goals, 10184 trajectories are successfully sampled with an average length of 282.01. These 10184 successful trajectories form the $\\mathcal{D}_{E}$ . $\\mathcal{D}_{E}$ are imperfect: firstly, the quantity of demonstrations is limited with only about $20\\%$ goals successfully sampled; secondly, the quality of demonstrations is low as a well-trained policy can usually finish the goal within 150 steps. ", "page_idx": 15}, {"type": "table", "img_path": "KP7EUORJYI/tmp/7044923062cf732b900b2c71978a071b4e913d7cedcff3150ffad6df73c4ce83.jpg", "table_caption": ["Table 4: Attitude control "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "A.6 The multi-goal long-horizon problem ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The VVC task represents typical multi-goal long-horizon problems. On the one hand, the average length of individual goals (without further division into sub-goals) in expert demonstrations exceeds 280 steps. Even for policies that have been trained thoroughly, the average steps to achieve a goal is over 100, with more challenging goals requiring more than 300 steps to complete, which is longer than the horizon used in most previous studies [41, 47, 28, 59]. On the other hand, when trained with classical RL algorithms, as illustrated in Fig. 6, neither the off-policy $\\scriptstyle\\mathrm{SAC}+\\mathrm{HER}$ nor the on-policy PPO algorithms are able to solve the VVC task. This also indicates the challenges that the long horizon of VVC poses for GCRL algorithms. ", "page_idx": 15}, {"type": "text", "text": "A.7 Environment Hyper-Parameters Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In the specific experiments, we only used the easy version, listed by Table 5a, of the VVC task in Fig. 1 of Sec. 1. In all experiments within the Sec. 4, we utilized the hard version, listed by Table 5b, of the VVC task. ", "page_idx": 15}, {"type": "table", "img_path": "KP7EUORJYI/tmp/b19f7614383fe88f04fbe2154131962aaae393e3539a31f2b1a4701e1816226d.jpg", "table_caption": ["Table 6: Parameters used in BC "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "B Calculation of GMM ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We employ GMM with weighted samples [21] to estimate $p_{a g}$ . For a set of $N$ weighted samples $\\{(x_{i},w_{i})\\}$ , where $x_{i}$ is the $i^{t h}$ sample and $w_{i}$ is the weight of $x_{i}$ , we want to find a Guassian Model density function with $M$ Guassian components $\\begin{array}{r}{f(x)\\,=\\,\\sum_{i=1}^{M}\\pi_{i}N(x|\\mu_{i},\\Sigma_{i})}\\end{array}$ , where $\\begin{array}{r}{N(x|\\mu_{i},\\Sigma_{i})=\\frac{1}{(2\\pi)^{D/2}|\\Sigma_{i}|^{1/2}}\\exp\\{-\\frac{1}{2}(x-\\mu_{i})^{T}\\Sigma_{i}^{-1}(x-\\mu_{i})\\}}\\end{array}$ is the $i^{t h}$ Guassian component, $D$ is the dimension of sample, $\\mu_{i},\\Sigma_{i},\\pi_{i}$ are the mean vector, covariance matrix, and the weight of $N(\\cdot|\\mu_{i},\\Sigma_{i})$ , correspondingly. ", "page_idx": 16}, {"type": "text", "text": "Then, Expectation-Maximization algorithm is employed to estimate the corresponding parameters. In the Expectation Step, the new estimate of each sample corresponds to each Guassian components is calculated, rik = $\\begin{array}{r}{r_{i k}=\\frac{\\bar{\\pi}_{k}N\\left(x_{i}\\vert\\mu_{k},\\Sigma_{k}\\right)}{\\sum_{j=1}^{M}\\pi_{j}N\\left(x_{i}\\vert\\mu_{j},\\Sigma_{j}\\right)}}\\end{array}$ . In the Maximization Step, calculate the weight of Guassian component, $\\begin{array}{r}{\\pi_{k}=\\frac{\\sum_{i=1}^{N}w_{i}r_{i k}}{\\sum_{j=1}^{M}\\sum_{i=1}^{N}w_{i}r_{i j}}}\\end{array}$ , the mean vector, $\\begin{array}{r}{\\mu_{k}=\\frac{\\sum_{i=1}^{N}w_{i}r_{i k}x_{i}}{\\sum_{i=1}^{N}w_{i}r_{i k}}}\\end{array}$ , and the covariance matrix, \u03a3k = iN=1 wirik(xi\u2212\u00b5k)(xi\u2212\u00b5k)T. ", "page_idx": 16}, {"type": "text", "text": "C Implementation Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The Imitation framework is utilized to implement BC algorithm with parameters listed in Table 6a, and the Stable Baselines3 framework for PPO with parameters listed in Table 6b. $128^{*}128$ fully connected network and the Tanh activation function are used for VVC. ", "page_idx": 16}, {"type": "text", "text": "As BC only learns a policy network, we add a warm-up for the value network at the beginning of online learning. When online learning begins, we first freeze the parameter of the policy and train the value network with online samples until it converges, then proceed with the normal RL training. For the parameter $\\lambda$ , $10^{-3}$ is utilized for all the experiments. ", "page_idx": 16}, {"type": "text", "text": "D Experiments on Reach and PointMaze ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We conduct two sets of experiments to demonstrate the general applicability of GCPO. ", "page_idx": 16}, {"type": "text", "text": "D.1 Experimental Setups ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Environments: For the first set of experiments, we conduct evaluations on a customized PointMaze environment (PointMaze_Large_DIVERSE_G-v3) from Gymnasium-Robotics [17] within the Mujoco physics engine. The only modification we made to the environment is to expand the number of desired goals from 7 to 45, making our customized version of PointMaze more challenging than the original version. For the second set of experiments, we employ a customized Reach (PandaReach-v3) [23] task on the Franka Emika Panda robot physics engine. The only modification we made to the environment is to change the distance_threshold used to determine goal reaching from 0.05 to 0.01. ", "page_idx": 16}, {"type": "table", "img_path": "KP7EUORJYI/tmp/67fff00fa857805230cbb3f2121d263533b236ee5b7501337ff4a80858556358.jpg", "table_caption": ["Table 7: Comparison between GCPO and baselines on Reach and PointMaze. The mean and variance of $\\%$ success rates are presented over 5 random seeds. Optimal values are highlighted in bold, and sub-optimal values are underlined. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Consequently, our customized version of the Reach task has a stricter criterion for determining goal arrival, making it more difficult than the original version of Reach. ", "page_idx": 17}, {"type": "text", "text": "Reward Settings: The original rewards for both the PointMaze and Reach tasks are Markovian. To evaluate the performance of our algorithm under different NMR settings, we design two distinct types of NMRs. For the PointMaze, the NMR we designed is: the task is considered successful only if, after the point reaches the goal, it moves away by at least a certain distance and then returns to the goal. For the Reach task, the NMR we designed is: the Panda robot must first pass through a specific waypoint before reaching the goal to be considered successful, and each goal has a different waypoint. Both of these settings strictly adhere to the definition of NMR, where the reward is defined by the states and actions over multiple steps. ", "page_idx": 17}, {"type": "text", "text": "Demonstrations: The demonstrations for PointMaze are sourced from Minari [18] (pointmaze-largev1), while the demonstrations for Reach are generated by us, with reference to the PID controller as described in the official documentation [44]. ", "page_idx": 17}, {"type": "text", "text": "D.2 Results ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We evaluate SAC+HER $^{+}$ MEGA, BC, and GCPO on the PointMaze and Reach tasks under both MR and NMR settings. Table 7 presents the success rates of these algorithms. It can be observed that under the MR settings, GCPO exhibits similar performance to $\\mathrm{SAC+HER+MEGA}$ . However, under the NMR settings, where HER cannot be effective, the performance of GCPO is significantly better than that of SAC. Taking into account the performance of GCPO on the VVC task as illustrated in Section 4.2, we showcase the general applicability of GCPO across a variety of tasks. ", "page_idx": 17}, {"type": "text", "text": "E The Impact of Demonstrations on GCPO Training ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we conduct a detailed experimental analysis of the impact of the distribution of goals that Demonstrations can cover, demonstration quantity, and demonstration quality on the training of GCPO. ", "page_idx": 17}, {"type": "text", "text": "E.1 Goal Distribution Covered by Demonstrations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "To more intuitively illustrate the impact of the goal distribution in demonstrations on GCPO training, we select three subsets of demonstrations from $\\mathcal{D}_{E}$ that are spatially distant from each other, comprising all $\\chi=10$ demonstrations for $\\mathcal{D}_{[\\chi=10]}$ , all $\\chi=90$ demonstrations for $\\mathcal{D}_{[\\chi=90]}$ , and all $\\chi=170$ demonstrations for D[\u03c7=170]. ", "page_idx": 17}, {"type": "text", "text": "It is evident that, for both the pre-trained policy and the GCPO policy, the achieved goals are distributed around the goal distribution of the demonstrations. This suggests that the goal distribution in the demonstrations biases the learning of both pre-training and GCPO, leading to policies that share a similar goal distribution as the demonstrations. Therefore, the best practice when preparing demonstrations is to make the distribution of goals covered by the demonstrations as closely resemble the desired goal distribution as possible. ", "page_idx": 17}, {"type": "image", "img_path": "KP7EUORJYI/tmp/1ecf5b912bf2a114a96250cf03d6e06729dadecaedce17efa28d0c3d505c3533.jpg", "img_caption": ["Figure 7: Visualization of goals covered by demonstrations and achieved by policies. The pre-trained policies with $\\mathcal{D}_{[\\chi=10]}$ , $\\mathcal{D}_{[\\chi=90]}$ , and $\\mathcal{D}_{[\\chi=170]}$ are denoted as $\\pi_{1}^{0},\\;\\pi_{2}^{0}$ , and $\\pi_{3}^{0}$ , respectively. The corresponding GCPO policies are denoted as $\\dot{\\pi}_{1}^{*}$ , $\\pi_{2}^{*}$ , and $\\pi_{3}^{*}$ , respectively. "], "img_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "KP7EUORJYI/tmp/1d07f595bc932e19557a99e3ff532521cc24ec10885800ab2903603f887e737e.jpg", "table_caption": ["Table 8: Performance of GCPO on Reach, PointMaze, and VVC with 1000 demonstrations, and more results on VVC with various demonstration quantities. The mean and variance of $\\%$ success rates are presented over 5 random seeds. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "E.2 Demonstration Quantity ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We note that commonly used RL demonstration datasets typically consist of 1000 demonstrations [22]. Consequently, we evaluate the performance of GCPO on Reach, PointMaze, and VVC with 1000 demonstrations. In addition, we conduct an ablation study on demonstration quantity on VVC. The results are presented in Table 8. ", "page_idx": 18}, {"type": "text", "text": "Demonstrations: The source of demonstrations for PointMaze and Reach is elaborated in Section D. For GCPO, $10\\%\\mathcal{D}_{E}$ is employed, as detailed in Section 4.3.1. Additionally, for comparison purposes, we also present the performance of GCPO on the VVC task using $\\mathcal{D}_{[\\chi=90]}$ , which comprises only 144 demonstrations with a flight path azimuth angle of 90, as described in detail in Appendix E.1, and using $\\mathcal{D}_{E}$ . The results are presented in Table 8. It can be observed that on the relatively simple PointMaze and Reach tasks, GCPO achieved nearly $100\\%$ success rate when using 1000 demonstrations. On the more challenging VVC task, the success rate with 1000 demonstrations reached $81.12\\%$ of the success rate achieved with 10264 demonstrations, while also being significantly higher than the success rate achieved with 144 demonstrations. These results indicate that across tasks of varying complexity, GCPO can achieve good performance with the use of 1000 demonstrations. ", "page_idx": 18}, {"type": "text", "text": "E.3 Demonstration Quality ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "To obtain a demonstration set for comparison with $\\mathcal{D}_{E}$ , we generate trajectories for these goals using all the policies trained in our experiments. For a specific goal, we retain only the shortest trajectory. These generated trajectories are denoted as $\\mathcal{D}^{\\prime}$ . ", "page_idx": 18}, {"type": "table", "img_path": "KP7EUORJYI/tmp/8fe0a828dbce15622fbc5cec27c00ffbaec2934be2b0019eddfbbffcc6befcbb.jpg", "table_caption": ["Table 9: Demonstration quality of $\\mathcal{D}_{E}$ and $\\mathcal{D}^{\\prime}$ "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "KP7EUORJYI/tmp/816b9919f058aa6499c53539808fea7e724da4c89ee2e2663212a041f85ca886.jpg", "table_caption": ["Table 10: Performance of GCPO on VVC with demonstrations of different demonstration qualities. $s(\\pi)$ indicates the success rate of policy $\\pi$ . The mean and variance of $\\%$ success rates are presented over 5 random seeds. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "We use two metrics to measure demonstration quality: Trajectory length $I_{l}$ . Since we employ $(-1,0)$ sparse rewards, this implies that shorter trajectories yield a higher cumulative reward. Control smoothness $I_{s}$ . In control problems, minimal control gains is expected to reduce wear on actuators. Hence, we refer to [39] to define the control smoothness. Trajectory length and control smoothness each describe certain characteristics of demonstrations from the distinct perspectives of reinforcement learning optimization and optimal control, respectively. Table 9 shows the demonstration quality of these two demonstration sets. It can be observed that: From an RL perspective, $\\mathcal{D}^{\\prime}$ is of higher quality because the trajectories are shorter, leading to a higher expected cumulative reward. From a control perspective, $\\mathcal{D}_{E}$ is better because the trajectories are smoother. ", "page_idx": 19}, {"type": "text", "text": "The performance of the BC policy $\\pi_{B C}$ and the GCPO policy $\\pi_{G C P O}$ trained on these two sets of demonstrations is shown in Table 10. The results reveal that $\\pi_{B C}$ closely aligns with the demonstrations on both quality metrics, indicating that demonstration quality has a direct impact on BC. Additionally, the BC policy trained on $\\mathcal{D}^{\\prime}$ has a slightly higher success rate, which we speculate is due to $\\mathcal{D}^{\\prime}$ being more suitable for RL (the network architecture and training hyperparameters used to generate $\\mathcal{D}^{\\prime}$ are the same as those for the BC policy). However, after the self-curriculum learning, the GCPO policy corresponding to $\\mathcal{D}_{E}$ performs better and exhibits a shorter trajectory length. This suggests that the influence of demonstration quality on GCPO\u2019s online learning may not be as direct as pre-training, and further research is required to understand this relationship. ", "page_idx": 19}, {"type": "text", "text": "In summary, on one hand, it is challenging to define demonstration quality suitable for RL through a few metrics. On the other hand, demonstration quality does affect GCPO pre-training. How demonstration quality potentially influences the self-curriculum learning of GCPO remains an intriguing question for further exploration. ", "page_idx": 19}, {"type": "text", "text": "Furthermore, despite the aforementioned challenges, GCPO is capable of training well-performed policies from non-expert demonstrations. The intrinsic reason is that GCPO employs online learning to fine-tune pre-trained policies. Consequently, even if the demonstrations are non-expert and the pre-trained policies perform poorly, GCPO can still continuously optimize these policies through online learning. ", "page_idx": 19}, {"type": "text", "text": "In Section 4.2, although the average trajectory length of $\\mathcal{D}_{E}^{0}$ reached 281.83, covering only $20.24\\%$ of goal space, the GCPO policy trained on it achieves a success rate of $45.87\\%$ , with an average trajectory length of 134.47. This comparison indicates that $\\mathcal{D}_{E}^{0}$ consists of non-expert demonstrations. On the other hand, in contrast to $\\dot{\\mathcal{D}}_{E}^{3}$ , which covers $78.5\\bar{5}\\%$ of the goal space with an average trajectory length of 116.56, $\\mathcal{D}_{E}^{0}$ is only a quarter in size and has trajectories that are 2.42 times longer, implying a substantial decrease in its quantity and quality. Nonetheless, the GCPO policy trained on $\\mathcal{D}_{E}^{0}$ achieves $76.58\\%$ of the success rate of the policy trained on $\\mathcal{D}_{E}^{3}$ ", "page_idx": 19}, {"type": "text", "text": "F Extending GCPO with A Method for Solving NMR Problems ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "To demonstrate the effects of integrating a method for handling NMR problems into GCPO, we extend GCPO with a basic method for addressing NMR problems, which involves extending the input of the policy with consecutive states to make the reward Markovian [24]. We evaluate the extended GCPO on the VVC task and show the result in Fig. 8. ", "page_idx": 19}, {"type": "image", "img_path": "KP7EUORJYI/tmp/52cc37aa3270147ae94173e0bba14c4f9760601f1c2be26ed9b0bada42a12060.jpg", "img_caption": ["Figure 8: Success rate of $\\mathrm{SAC+HER+MEGA}$ and GCPO extended with consecutive states. \u2018w/ framestack\u2019 indicates that the policy input is extended with consecutive states, whereas \u2018w/o framestack\u2019 denotes that the input remains the current state. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "It is evident that, regardless of whether it is the pre-trained policy or the GCPO policy or the SAC+HER $^{+}$ MEGA policy, expanding the input of the policy leads to a certain degree of performance degradation. We conjecture that this is likely due to the expansion of the policy\u2019s search space, which makes the original number of demonstrations and online learning resources relatively insufficient, indirectly increasing the difficulty of solving the policy. Therefore, although expanding the input is theoretically beneficial for addressing NMR problems, in practice, it also requires a balance between the increased difficulty of the problem and the available resources. ", "page_idx": 20}, {"type": "text", "text": "G Societal Impacts ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "This paper introduces a general on-policy goal-conditioned reinforcement learning framework, GCPO, which is capable of addressing both Markovian and non-Markovian reward problems. This flexibility makes GCPO particularly well-suited for a wide range of practical applications that rely on nonMarkovian rewards, such as robotic arm control with stringent stability requirements and drone control that demands steady goal achievement. However, in practical settings, it is imperative to pay close attention to the evaluation of the GCPO policy, carefully analyzing the disparities between the policy\u2019s achieved goal distribution and the desired goal distribution of the actual task. Allowing the policy to attempt goals beyond its capabilities in a production environment could result in hardware failures, damage, and other problems. ", "page_idx": 20}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The main claim we make in the abstract and introduction is that we propose an on-policy goal-conditioned reinforcement learning framework. We detail the proposed framework in Sec. 3 and evaluate the effectiveness of each component in Sec. 4. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: The limitations of the work are discussed in Section 5 ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: Our main contribution is to propose an on-policy goal-conditioned reinforcement learning framework, in which we give references to the theories involved. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We detail all experimental details in Appendix A and C. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 22}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Please refer to https://github.com/GongXudong/GCPO. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 23}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We detail experimental details in Appendix A and C. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 23}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We average all results over 5 random seeds and show error bars suitably on figures. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We show the training epochs of pre-training in Table 6a and compute workers and environment steps of online learning in Table 6b. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We confirm that the research conducted in the paper conforms, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 24}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We discuss the societal impacts in Appendix G. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 24}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 25}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: Our contribution poses no such risks. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 25}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: Our work does not use existing assets. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: Our work does not release new assets. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 26}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: Our work does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 26}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: Our work does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 26}]