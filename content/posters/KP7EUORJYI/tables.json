[{"figure_path": "KP7EUORJYI/tables/tables_2_1.jpg", "caption": "Table 1: Comparison between HER-based methods and GCPO. Pag refers to the distribution of achieved goals, supp(\u00b7) refers to the support set of a distribution, U(\u00b7) refers to the uniform distribution on a set, and IPW(\u00b7) refers to the inverse probability weighting [32].", "description": "This table compares the Goal-Conditioned Policy Optimization (GCPO) method with existing HER-based methods.  It highlights key differences in the type of reinforcement learning (off-policy vs. on-policy), the types of reward functions they can handle (Markovian Reward (MR) vs. Non-Markovian Reward (NMR)), the methods used to increase informative rewards, and the goal sampling methods employed.  The table clarifies that HER-based methods utilize hindsight replay and rely on replay buffers, while GCPO uses pre-training from demonstrations and off-policy evaluation for goal sampling.", "section": "Related Work"}, {"figure_path": "KP7EUORJYI/tables/tables_6_1.jpg", "caption": "Table 2: Comparison between GCPO and baselines on NMR. The mean and variance of % success rates are presented over 5 random seeds. Optimal values are highlighted in bold, and sub-optimal values are underlined.", "description": "This table compares the performance of GCPO against several baseline methods on a Non-Markovian Reward (NMR) problem.  It shows the mean and standard deviation of the success rate (percentage) achieved by each method across five different random seeds. The results are categorized by the quantity and quality of demonstration data used to train the models.  Optimal results are highlighted in bold and suboptimal results are underlined, providing a clear visualization of GCPO's performance relative to baselines under various conditions.", "section": "4.2 Main Results"}, {"figure_path": "KP7EUORJYI/tables/tables_7_1.jpg", "caption": "Table 1: Comparison between HER-based methods and GCPO. Pag refers to the distribution of achieved goals, supp(\u00b7) refers to the support set of a distribution, U(\u00b7) refers to the uniform distribution on a set, and IPW(\u00b7) refers to the inverse probability weighting [32].", "description": "This table compares different goal-conditioned reinforcement learning (GCRL) methods, highlighting the differences between HER-based (off-policy) methods and the proposed GCPO (on-policy) method.  It focuses on the type of reinforcement learning (RL), applicable reward types (Markovian or Non-Markovian), methods for increasing informative rewards, and methods for sampling goals. The table shows that GCPO differs significantly from existing HER-based methods by being on-policy and not relying on Hindsight Experience Replay (HER).", "section": "2 Related Work"}, {"figure_path": "KP7EUORJYI/tables/tables_15_1.jpg", "caption": "Table 1: Comparison between HER-based methods and GCPO. Pag refers to the distribution of achieved goals, supp(\u00b7) refers to the support set of a distribution, U(\u00b7) refers to the uniform distribution on a set, and IPW(\u00b7) refers to the inverse probability weighting [32].", "description": "This table compares the Goal-Conditioned Policy Optimization (GCPO) framework with existing HER-based methods.  It highlights key differences in the type of reinforcement learning (on-policy vs off-policy), the reward types handled (Markovian vs Non-Markovian), methods used for enhancing informative rewards, and the goal sampling strategies employed.  The table summarizes how GCPO differs from existing methods by emphasizing its on-policy nature, ability to handle non-Markovian rewards, and its use of pre-training and self-curriculum learning rather than relying on Hindsight Experience Replay (HER).", "section": "2 Related Work"}, {"figure_path": "KP7EUORJYI/tables/tables_15_2.jpg", "caption": "Table 2: Comparison between GCPO and baselines on NMR. The mean and variance of % success rates are presented over 5 random seeds. Optimal values are highlighted in bold, and sub-optimal values are underlined.", "description": "This table compares the performance of GCPO against several baseline methods on a non-Markovian reward (NMR) problem.  It shows the mean and standard deviation of success rates across five random seeds for each method, with optimal results highlighted. The table is organized by the quantity and quality of demonstration data used to train the models, providing insights into the effect of varying amounts and qualities of training data on model performance.", "section": "4.2 Main Results"}, {"figure_path": "KP7EUORJYI/tables/tables_16_1.jpg", "caption": "Table 1: Comparison between HER-based methods and GCPO. Pag refers to the distribution of achieved goals, supp(\u00b7) refers to the support set of a distribution, U(\u00b7) refers to the uniform distribution on a set, and IPW(\u00b7) refers to the inverse probability weighting [32].", "description": "This table compares Goal-Conditioned Reinforcement Learning (GCRL) methods, specifically highlighting the differences between HER-based methods (off-policy) and the proposed GCPO method (on-policy).  It details the type of reinforcement learning (RL), applicable reward types (Markovian Reward or Non-Markovian Reward), methods for increasing informative rewards, and goal sampling methods used by each algorithm. The table provides a concise overview of existing techniques and how GCPO differs from them.", "section": "2 Related Work"}, {"figure_path": "KP7EUORJYI/tables/tables_17_1.jpg", "caption": "Table 2: Comparison between GCPO and baselines on NMR. The mean and variance of % success rates are presented over 5 random seeds. Optimal values are highlighted in bold, and sub-optimal values are underlined.", "description": "This table compares the performance of GCPO against several baseline methods (SAC + HER + MEGA, BC) on tasks with Non-Markovian Rewards (NMR).  The results, averaged over 5 different random seeds, show success rates (percentage) and their standard deviations.  Optimal results are highlighted in bold, and suboptimal results are underlined, demonstrating GCPO's performance advantage on NMR problems.", "section": "4.2 Main Results"}, {"figure_path": "KP7EUORJYI/tables/tables_18_1.jpg", "caption": "Table 2: Comparison between GCPO and baselines on NMR. The mean and variance of % success rates are presented over 5 random seeds. Optimal values are highlighted in bold, and sub-optimal values are underlined.", "description": "This table compares the performance of GCPO against several baseline methods (SAC+HER+MEGA, BC, GCPO without pre-training, GCPO without self-curriculum) on a Non-Markovian Reward (NMR) problem.  The results are shown for different quantities and qualities of demonstration data used for pre-training.  Success rates are given as mean \u00b1 standard deviation across 5 different random seed runs.", "section": "4.2 Main Results"}, {"figure_path": "KP7EUORJYI/tables/tables_19_1.jpg", "caption": "Table 2: Comparison between GCPO and baselines on NMR. The mean and variance of % success rates are presented over 5 random seeds. Optimal values are highlighted in bold, and sub-optimal values are underlined.", "description": "This table compares the performance of GCPO against several baseline methods on a Non-Markovian Reward (NMR) problem.  The results are averaged over five random trials.  Optimal performance is indicated in bold, while suboptimal results are underlined.  Different demonstration sets (DE, DE, DE, D) with varying quantities and lengths are used for training.", "section": "4.2 Main Results"}, {"figure_path": "KP7EUORJYI/tables/tables_19_2.jpg", "caption": "Table 2: Comparison between GCPO and baselines on NMR. The mean and variance of % success rates are presented over 5 random seeds. Optimal values are highlighted in bold, and sub-optimal values are underlined.", "description": "This table compares the performance of GCPO against several baseline methods on a Non-Markovian Reward (NMR) problem.  The performance metric is the success rate (percentage of successful goal achievements), averaged over 5 random seeds.  The table shows the mean and standard deviation of the success rate.  Optimal results are shown in bold and suboptimal results are underlined. The table also includes information on the quality and quantity of the demonstrations (training data) used for each method. The goal is to showcase the effectiveness of GCPO in handling NMR problems, compared to existing methods.", "section": "4.2 Main Results"}]