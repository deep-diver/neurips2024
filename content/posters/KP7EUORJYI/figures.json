[{"figure_path": "KP7EUORJYI/figures/figures_1_1.jpg", "caption": "Figure 1: Left: Illustrations of HER on Markovian reward (MR) and non-Markovian reward (NMR) problems. Right: Performance of HER on MR and NMR problems. NMR[x] represents the NMR depends on the last x consecutive states and actions. Results come from experiments on an easy version (Appendix A.7) of fixed-wing UAV velocity vector control task over 5 random seeds.", "description": "The figure is composed of two parts. The left part illustrates how Hindsight Experience Replay (HER) works on both Markovian Reward (MR) and Non-Markovian Reward (NMR) problems.  It shows that HER can successfully densify rewards in MR problems by using a single encountered state as the hindsight goal, but it fails in NMR problems because the reward computation depends on multiple steps of states and actions, and a single state cannot serve as a hindsight goal. The right part presents the experimental results of HER on MR and NMR problems, demonstrating that HER's performance deteriorates as the NMR's dependence on consecutive states and actions increases, ultimately becoming indistinguishable from the case without HER. The experiments were conducted on a simplified version of the fixed-wing UAV velocity vector control task.", "section": "1 Introduction"}, {"figure_path": "KP7EUORJYI/figures/figures_3_1.jpg", "caption": "Figure 2: The overall GCPO framework.", "description": "This figure illustrates the GCPO (Goal-Conditioned Policy Optimization) framework, which consists of two main components: Pre-training from Demonstrations and Online Self-Curriculum Learning.  The left side shows the pre-training phase, where a policy is pre-trained using demonstrations (illustrated as UAV flight paths). The right side depicts the online self-curriculum learning phase. This involves three steps: (1) Estimating the distribution of achieved goals (pag); (2) Sampling new goals using a probability transform function f (pag, Pdg) to select increasingly challenging goals; and (3) Online learning by pursuing the sampled goals.  The overall process iteratively refines the policy's ability to achieve diverse goals.", "section": "3 Methodology"}, {"figure_path": "KP7EUORJYI/figures/figures_6_1.jpg", "caption": "Figure 3: Main results of GCPO. \u2018expert\u2019 refers to the demonstrator that generates demonstrations. \u2018BC\u2019 refers to the pre-trained policy. Results are derived from experiments across 5 random seeds. For sub-figure (a), expert and BC are both evaluated in the NMR setting. For sub-figure (c), the vertical axis represents the training progress, where 0, 1, \u2026, 9 correspond to 10%, 20%, \u2026, 100% of the training progress, respectively.", "description": "This figure shows the main results of the Goal-Conditioned Policy Optimization (GCPO) method. Subfigure (a) displays the success rate on both Markovian Reward (MR) and Non-Markovian Reward (NMR) problems. Subfigure (b) presents a histogram of the achieved goals. Subfigure (c) illustrates the distribution of goals generated from self-curriculum learning during training.  The results demonstrate the effectiveness of GCPO in handling both MR and NMR tasks and highlight the role of online self-curriculum learning in the process.", "section": "4.2 Main Results"}, {"figure_path": "KP7EUORJYI/figures/figures_7_1.jpg", "caption": "Figure 3: Main results of GCPO. \u2018expert\u2019 refers to the demonstrator that generates demonstrations. \u2018BC\u2019 refers to the pre-trained policy. Results are derived from experiments across 5 random seeds. For sub-figure (a), expert and BC are both evaluated in the NMR setting. For sub-figure (c), the vertical axis represents the training progress, where 0, 1, \u2026.., 9 correspond to 10%, 20%, \u2026.., 100% of the training progress, respectively.", "description": "This figure presents the main results of the Goal-Conditioned Policy Optimization (GCPO) algorithm. Subfigure (a) shows the success rate of GCPO on both Markovian Reward (MR) and Non-Markovian Reward (NMR) problems. Subfigure (b) displays a histogram of the achieved goals, illustrating the distribution of goals successfully reached by the algorithm. Subfigure (c) depicts the distribution of goals generated by the online self-curriculum learning component of GCPO during the learning process, demonstrating the algorithm\u2019s ability to adapt the difficulty of goals over time.", "section": "4.2 Main Results"}, {"figure_path": "KP7EUORJYI/figures/figures_8_1.jpg", "caption": "Figure 5: Analysis of the influence of different self-curriculum methods on the learning progression of GCPO, as well as a comparison between self-curriculum and non-curriculum methods. \u2018expert\u2019 and \u2018None\u2019 are two non-curriculum methods, where \u2018expert\u2019 refers to sampling goals from those that the demonstrator can achieve, and \u2018None\u2019 signifies directly sampling from Pag. Results are derived from experiments across 5 random seeds.", "description": "This figure presents a comprehensive analysis of different self-curriculum methods' impact on the GCPO learning process. It compares three distinct self-curriculum learning strategies (RIG, DISCERN, and MEGA) with two non-curriculum approaches (sampling from expert-achievable goals and directly from Pag). The analysis encompasses four subplots: (a) the difficulty of goals sampled by each self-curriculum method over time; (b) the success rate of each method during training; (c) a histogram of achieved goals for various self-curriculum methods; and (d) a comparative histogram of achieved goals for self-curriculum versus non-curriculum methods.  The results, generated across five random seeds, offer insights into the effectiveness and distinct characteristics of each goal selection approach.", "section": "4.3 Ablation Studies"}, {"figure_path": "KP7EUORJYI/figures/figures_15_1.jpg", "caption": "Figure 3: Main results of GCPO. \u2018expert\u2019 refers to the demonstrator that generates demonstrations. \u2018BC\u2019 refers to the pre-trained policy. Results are derived from experiments across 5 random seeds. For sub-figure (a), expert and BC are both evaluated in the NMR setting. For sub-figure (c), the vertical axis represents the training progress, where 0, 1, \u2026.., 9 correspond to 10%, 20%, \u2026.., 100% of the training progress, respectively.", "description": "This figure shows the main results of the Goal-Conditioned Policy Optimization (GCPO) algorithm.  Subfigure (a) compares the success rate of GCPO on Markovian Reward (MR) and Non-Markovian Reward (NMR) problems, highlighting GCPO's success in both scenarios. Subfigure (b) displays a histogram of the achieved goals. Subfigure (c) illustrates the distribution of goals selected by the online self-curriculum learning mechanism of GCPO during the training process.", "section": "4.2 Main Results"}, {"figure_path": "KP7EUORJYI/figures/figures_18_1.jpg", "caption": "Figure 3: Main results of GCPO. \u2018expert\u2019 refers to the demonstrator that generates demonstrations. \u2018BC\u2019 refers to the pre-trained policy. Results are derived from experiments across 5 random seeds. For sub-figure (a), expert and BC are both evaluated in the NMR setting. For sub-figure (c), the vertical axis represents the training progress, where 0, 1, \u2026, 9 correspond to 10%, 20%, \u2026, 100% of the training progress, respectively.", "description": "This figure shows the main results of the Goal-Conditioned Policy Optimization (GCPO) method. Subfigure (a) compares the success rate of GCPO on Markovian Reward (MR) and Non-Markovian Reward (NMR) problems. Subfigure (b) displays a histogram of the achieved goals, illustrating the distribution of goals successfully achieved by the learned policy. Subfigure (c) visualizes the distribution of goals sampled from the self-curriculum learning process during training, highlighting how the algorithm selects increasingly challenging goals as the training progresses.", "section": "4.2 Main Results"}, {"figure_path": "KP7EUORJYI/figures/figures_20_1.jpg", "caption": "Figure 3: Main results of GCPO. \u2018expert\u2019 refers to the demonstrator that generates demonstrations. \u2018BC\u2019 refers to the pre-trained policy. Results are derived from experiments across 5 random seeds. For sub-figure (a), expert and BC are both evaluated in the NMR setting. For sub-figure (c), the vertical axis represents the training progress, where 0, 1, \u2026, 9 correspond to 10%, 20%, \u2026, 100% of the training progress, respectively.", "description": "This figure presents the main results of the GCPO algorithm.  Subfigure (a) shows the success rate of GCPO on both Markovian Reward (MR) and Non-Markovian Reward (NMR) problems. (b) displays a histogram of achieved goals, illustrating the distribution of goals successfully achieved during training. Finally, (c) illustrates the distribution of goals selected by the online self-curriculum learning mechanism throughout the training process.", "section": "4.2 Main Results"}]