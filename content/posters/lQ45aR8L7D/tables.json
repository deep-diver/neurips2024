[{"figure_path": "lQ45aR8L7D/tables/tables_3_1.jpg", "caption": "Table 1: Models used in this analysis divided by family.", "description": "This table lists the language models used in the experiments, categorized by their origin (OpenAI, Meta, Mistral) and whether they are fine-tuned versions of base models.  It shows the number of parameters in billions (B) for each model.  The information is crucial for understanding the scope of the experiments and allows readers to assess the diversity and scale of the models tested.", "section": "6.5 Model Details"}, {"figure_path": "lQ45aR8L7D/tables/tables_13_1.jpg", "caption": "Table 1: Models used in this analysis divided by family.", "description": "This table lists the large language models (LLMs) used in the experiments of the paper, categorized by their originating organization (OpenAI, Meta, Mistral).  For each model, the table provides the number of parameters (in billions), whether the model is fine-tuned, and the base model it is derived from (if applicable). This information helps understand the diversity of models and the context for the experiments.", "section": "6.5 Model Details"}]