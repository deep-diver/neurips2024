[{"figure_path": "168NLzTpw8/tables/tables_6_1.jpg", "caption": "Table 1: Comparison of generation and hallucination performance on RefCOCOg. t denotes the temperature parameter. \"1/8\" denotes using 1/8 samples of the total dataset to estimate the layer prior importance. \"full-R\" denotes quality ranking on the whole dataset.", "description": "This table presents a comparison of different models on the RefCOCOg dataset using the METEOR metric for generation performance and CHAIR metrics for hallucination.  It shows the impact of temperature (t) on the Osprey-7b baseline model and compares it to the DoLa model and the proposed method with and without full dataset quality ranking. The table highlights the trade-off between generation quality and hallucination, demonstrating the effectiveness of the proposed method in improving both.", "section": "4.2 Main result on referring expression generation"}, {"figure_path": "168NLzTpw8/tables/tables_7_1.jpg", "caption": "Table 1: Comparison of generation and hallucination performance on RefCOCOg. t denotes the temperature parameter. \"1/8\" denotes using 1/8 samples of the total dataset to estimate the layer prior importance. \"full-R\" denotes quality ranking on the whole dataset.", "description": "This table presents a comparison of different models' performance on the RefCOCOg dataset in terms of generation and hallucination.  The models compared are Osprey-7b (at temperatures 0.2 and 0.9), DoLa, and the proposed 'Ours' method (with and without full dataset ranking). The metrics used are METEOR (higher is better, indicating better semantic quality), CHAIRs and CHAIR (lower is better, representing fewer hallucination issues at sentence and object levels, respectively), Recall (higher is better), average sentence length (Len), and normalized versions of CHAIRs and CHAIR (nCHAIRS and nCHAIR). The results demonstrate the proposed method's superior performance in achieving a balance between descriptive richness and accuracy, mitigating hallucination issues.", "section": "4.2 Main result on referring expression generation"}, {"figure_path": "168NLzTpw8/tables/tables_8_1.jpg", "caption": "Table 2: The impact of different candidate layer groups (buckets) on the performance of RefCOCOg dataset.", "description": "This table presents the performance of the RefCOCOg dataset using different groups of intermediate layers in the proposed model.  It shows that the first group of layers (0-7) performs best across all metrics (METEOR, CHAIRs, CHAIR\u2081, Recall, and Len), indicating that these layers contain the most useful information for generating referring expressions.  Performance decreases as the layer index increases (moving towards the final layer).  The normalized hallucination metrics (nCHAIRs and nCHAIR\u2081) show a similar trend, with the first group having the lowest values, highlighting the effectiveness of focusing on earlier layers for improved region understanding and reduced hallucinations.", "section": "4.3 Performance of different candidate layer groups"}, {"figure_path": "168NLzTpw8/tables/tables_9_1.jpg", "caption": "Table 3: The comparison on Prompted Visual Hallucination Evaluation Benchmark(PHD)(%)", "description": "This table compares the performance of different models on the Prompted Visual Hallucination Evaluation Benchmark (PHD).  The PHD benchmark evaluates models on various aspects of hallucination including object recognition, attribute recognition, sentiment understanding, positional reasoning, and counting. The table shows the performance of Osprey (with temperature parameters t=0.9 and t=0.2), DoLa, and the proposed 'Ours' method, under both neutral and misleading prompt conditions.  Higher percentages indicate better performance.  The 'Ours' method shows improvements across most aspects of hallucination.", "section": "4.4 Transferability of layer prior importance weights"}, {"figure_path": "168NLzTpw8/tables/tables_14_1.jpg", "caption": "Table 1: Comparison of generation and hallucination performance on RefCOCOg. t denotes the temperature parameter. \"1/8\" denotes using 1/8 samples of the total dataset to estimate the layer prior importance. \"full-R\" denotes quality ranking on the whole dataset.", "description": "This table presents a comparison of different models' performance on the RefCOCOg dataset in terms of generation quality (METEOR) and hallucination (CHAIR and nCHAIR).  It shows the impact of temperature (t) and using a subset (1/8) versus the full dataset for layer importance estimation and quality ranking.  Higher METEOR scores indicate better generation, while lower CHAIR and nCHAIR scores indicate less hallucination.", "section": "4.2 Main result on referring expression generation"}, {"figure_path": "168NLzTpw8/tables/tables_15_1.jpg", "caption": "Table 5: Different sizes of subsets used in Probing-based estimation. The range of candidate layers is [0, 7]. \"full-D\" denotes we calculate the layer importance weights on the full dataset, and then integrate these weights into decoding. \"full-R\" denotes cycle-consistency-based ranking on the full dataset. \"top\" and \"avg\" denotes the best/average results we tested on different sampled 1/8 subset.", "description": "This table shows the performance of the proposed method using different sizes of subsets for probing-based importance estimation.  It compares the results using average and best results from 1/8 subsets, a 1/16 subset, the full dataset with importance weights integrated into decoding (full-D), and the full dataset with cycle-consistency-based ranking (full-R). The metrics evaluated include METEOR, CHAIRs, nCHAIRs, CHAIR\u2081, nCHAIR\u2081, Recall, and Len.", "section": "4.3 Performance of different candidate layer groups"}]