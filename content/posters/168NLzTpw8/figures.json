[{"figure_path": "168NLzTpw8/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration of our method. (a) Given an image and a mask appointed to the target object, we first unleash the descriptive regional information of the middle layers and gain various candidate captions. These outputs are sent to a RES model that serves as a \"listener\" and the \"listener\" eliminates the inaccurate candidates. (b) Our proposed strategy to diminish the computational load of RES. First estimate the layer prior importance on a probe set with RES, then leverage it for the RES-free next token prediction.", "description": "This figure illustrates the proposed \"unleash-then-eliminate\" framework for referring expression generation.  Part (a) shows how intermediate layers of the model are used to generate multiple caption candidates, which are then filtered using a referring expression segmentation (RES) model to remove inaccurate ones. Part (b) describes a method to improve efficiency by estimating the importance of different layers using a probing set and then incorporating those weights into the decoding process.", "section": "3 Method"}, {"figure_path": "168NLzTpw8/figures/figures_4_1.jpg", "caption": "Figure 2: Different layers' understanding of the region context varies, where early layers generate rubbish, middle layers tend to generate descriptive text with higher granularity, and the final layers tend to predict shorter and more precise text. The right part shows the frequencies with which the hidden state of each layer had the smallest Wasserstein-2 distance to the [SEG] token (in orange), and the inter-layer transitions of region-level multi-modal alignment (in blue).", "description": "This figure shows that different layers of the MLLM-based REG model have different tendencies for generating descriptions. Early layers produce nonsensical outputs, while intermediate layers produce more detailed and granular descriptions. The final layer tends to produce shorter, more concise descriptions. The right side of the figure shows the frequency distribution of the layer with the smallest Wasserstein-2 distance to the [SEG] token and the inter-layer transition of multi-modal alignment across layers.", "section": "3.2 The intermediate layer contains descriptive information"}, {"figure_path": "168NLzTpw8/figures/figures_5_1.jpg", "caption": "Figure 3: First, we conduct contrastive decoding by subtracting the log probabilities produced by the intermediate layers from those of the final layer. Then, each of the intermediate layers in the candidate subset suggests a probability for predicting the next token. Finally, RES model is used to estimate the performance of the captions by calculating the Intersection over Union (IoU) between MASK and M\u2081, ranking these candidates effectively.", "description": "This figure illustrates the cycle-consistency-based quality ranking process. First, contrastive decoding is performed by subtracting the log probabilities of tokens from intermediate layers from the final layer.  Intermediate layers then predict the next token, generating multiple candidate captions.  These captions are fed into a Referring Expression Segmentation (RES) model, which generates masks (M'). The Intersection over Union (IoU) between the input mask (M) and the generated mask (M') is calculated to rank the captions, effectively selecting the best caption.", "section": "3.3 Cycle-consistency-based intermediate sentence quality ranking"}, {"figure_path": "168NLzTpw8/figures/figures_8_1.jpg", "caption": "Figure 1: Illustration of our method. (a) Given an image and a mask appointed to the target object, we first unleash the descriptive regional information of the middle layers and gain various candidate captions. These outputs are sent to a RES model that serves as a \"listener\" and the \"listener\" eliminates the inaccurate candidates. (b) Our proposed strategy to diminish the computational load of RES. First estimate the layer prior importance on a probe set with RES, then leverage it for the RES-free next token prediction.", "description": "This figure illustrates the proposed \"unleash-then-eliminate\" framework for MLLM-based referring expression generation (REG). Part (a) shows how intermediate layers' information is extracted (unleashed) and then filtered (eliminated) by a referring expression segmentation (RES) model to improve caption quality. Part (b) details a probing-based importance estimation method to reduce the computational cost of using the RES model, by estimating importance weights for different layers and using them in the prediction process.", "section": "3 Method"}, {"figure_path": "168NLzTpw8/figures/figures_8_2.jpg", "caption": "Figure 5: The layer prior importance measured by our method of different groups. It is observable that the weight distributions of the 1/8 subset and the full dataset have similar trends.", "description": "This figure shows the layer prior importance weights calculated using the proposed method. Two sets of weights are displayed: one estimated from the full dataset and another from a smaller subset (1/8 of the full dataset). The x-axis represents the layer index, while the y-axis represents the layer importance.  The plot demonstrates that the layer importance is not uniformly distributed across all layers and that the trends are largely similar for both the full dataset and subset estimations, indicating that a smaller subset could be sufficiently representative for estimating layer importance.", "section": "4.3 Performance of different candidate layer groups"}, {"figure_path": "168NLzTpw8/figures/figures_15_1.jpg", "caption": "Figure 2: Different layers' understanding of the region context varies, where early layers generate rubbish, middle layers tend to generate descriptive text with higher granularity, and the final layers tend to predict shorter and more precise text. The right part shows the frequencies with which the hidden state of each layer had the smallest Wasserstein-2 distance to the [SEG] token (in orange), and the inter-layer transitions of region-level multi-modal alignment (in blue).", "description": "This figure shows how different layers of a multi-modal large language model (MLLM) understand region context. Early layers produce nonsensical output, while middle layers generate more detailed and granular descriptions.  The final layer produces shorter, more precise descriptions. The right side uses two plots to visualize this: one showing the frequency of each layer's hidden state being closest to a special segmentation token ([SEG]), and the other illustrating the changes in region-level multi-modal alignment across layers.", "section": "3.2 The intermediate layer contains descriptive information"}, {"figure_path": "168NLzTpw8/figures/figures_16_1.jpg", "caption": "Figure 7: A case of hallucination in MLLM-based REG, which mistakenly includes the attribute of the other dog to the target.", "description": "This figure shows an example of hallucination in a multi-modal large language model (MLLM)-based referring expression generation (REG) system.  The image depicts a woman celebrating a dog's birthday with a cake. The model incorrectly attributes a characteristic (a black collar) of a different dog in the background to the target dog. This highlights the challenge of preventing MLLMs from hallucinating attributes that are not present in the target object when generating detailed descriptions.", "section": "3.2 The intermediate layer contains descriptive information"}, {"figure_path": "168NLzTpw8/figures/figures_17_1.jpg", "caption": "Figure 2: Different layers' understanding of the region context varies, where early layers generate rubbish, middle layers tend to generate descriptive text with higher granularity, and the final layers tend to predict shorter and more precise text. The right part shows the frequencies with which the hidden state of each layer had the smallest Wasserstein-2 distance to the [SEG] token (in orange), and the inter-layer transitions of region-level multi-modal alignment (in blue).", "description": "This figure shows how different layers of a multi-modal large language model (MLLM) understand region context. Early layers produce nonsensical outputs, while middle layers generate more detailed descriptions. The final layer produces shorter, more concise descriptions. The right side displays the frequency with which each layer's hidden state is closest to the [SEG] token (orange) and the transitions in region-level multi-modal alignment (blue) across layers.  This illustrates the non-monotonic relationship between layer depth and descriptive ability.", "section": "3.2 The intermediate layer contains descriptive information"}]