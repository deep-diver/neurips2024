{"references": [{"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper is foundational in establishing the capability of large language models to perform few-shot learning, which is central to the current research on improving their reasoning capabilities."}, {"fullname_first_author": "Jason Wei", "paper_title": "Chain-of-thought prompting elicits reasoning in large language models", "publication_date": "2022-12-01", "reason": "This paper introduces the Chain-of-Thought prompting method, a key technique that inspires many approaches to improve reasoning in LLMs and is directly compared against in this paper."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Llama 2: Open foundation and fine-tuned chat models", "publication_date": "2023-07-01", "reason": "This paper introduces Llama 2, one of the large language models used in the experiments, therefore a crucial component in evaluating the proposed method."}, {"fullname_first_author": "OpenAI", "paper_title": "GPT-4 technical report", "publication_date": "2024-03-01", "reason": "This paper introduces GPT-4, another large language model used in the experiments and a key model for comparison with Llama 2."}, {"fullname_first_author": "M. D. S. Braine", "paper_title": "On the relation between the natural logic of reasoning and standard logic", "publication_date": "1978-01-01", "reason": "This paper provides a theoretical foundation for understanding the importance of logical relationships in reasoning, which is the core motivation for the proposed method."}]}