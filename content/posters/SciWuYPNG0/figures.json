[{"figure_path": "SciWuYPNG0/figures/figures_1_1.jpg", "caption": "Figure 1: InfoRE (Ours) vs existing methods. In contrast to the existing methods that primarily focus on the reasoning process, our InfoRE emphasizes the re-organization of context information. The [TEXT] in italics indicates that it is optional in the reasoning process.", "description": "This figure compares the proposed InfoRE method with existing methods for large language model reasoning.  Existing methods focus solely on improving the reasoning process itself. In contrast, InfoRE emphasizes reorganizing the input context information to explicitly highlight logical relationships before the reasoning process begins, which is believed to improve the accuracy and reliability of the results. The optional text within square brackets highlights that the original text may be used in conjunction with the re-organized information.", "section": "1 Introduction"}, {"figure_path": "SciWuYPNG0/figures/figures_2_1.jpg", "caption": "Figure 2: Illustration of our information re-organization method with two modules: 1) Information\nRe-Organization, which includes logic relationship extraction and noise pruning. 2) Reasoning using\nre-organized context. The re-organized context in black italicized text is relevant to the question.", "description": "This figure illustrates the InfoRE method's framework.  It shows two main components:\n\n1. **Information Re-organization:** This stage involves extracting logical relationships from the context (using a MindMap structure) and then pruning irrelevant information using a BERT-based model trained with reinforcement learning. The reward function for the RL training is the F1 score.\n2. **Reasoning:**  This stage uses the re-organized context (along with the optional original context) to answer the question using an LLM. The example given in the figure highlights how the re-organized information helps in directly answering the question.", "section": "3 Methodology"}, {"figure_path": "SciWuYPNG0/figures/figures_3_1.jpg", "caption": "Figure 3: Illustration of Pruning model. The representation of [CLS] is used to obtain action probabilities.", "description": "This figure illustrates the architecture of the pruning model used in the InfoRE method.  The input consists of concatenated logical relationships and their corresponding attributes from the extracted context, along with the question.  A pre-trained BERT model processes this input. The output of the BERT model is then passed through a linear layer and a softmax function to produce action probabilities for each logical relationship (keep or delete). The [CLS] token representation is crucial for the model to determine which relationships are relevant for answering the question and which should be removed to reduce noise.", "section": "3.2.2 Pruning"}, {"figure_path": "SciWuYPNG0/figures/figures_8_1.jpg", "caption": "Figure 4: Error Analysis of InfoRE on 2WikiMultiHopQA against Standard baseline method. The first four rectangles are error categories, while \"Corrected\" on the far right denotes the percentage of errors originally made by the baseline method that our method InfoRE has successfully corrected.", "description": "This figure presents a bar chart comparing the error rates of the baseline method (Standard) and the proposed InfoRE method across different error categories in the 2WikiMultiHopQA dataset.  The error categories are: Contextual Misunderstanding (CM), Factual Error (FE), Mathematical Error (ME), and Unanswerable Question (UQ).  The \"Corrected\" category shows the percentage of errors from the Standard method that were successfully addressed by InfoRE.  It visually demonstrates the effectiveness of InfoRE in reducing contextual misunderstanding errors, which were the most frequent type of error in the baseline method.", "section": "5.2 Analysis"}]