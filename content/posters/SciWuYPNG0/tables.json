[{"figure_path": "SciWuYPNG0/tables/tables_5_1.jpg", "caption": "Table 1: Zero-shot performance on claim verification task across three LLMs.", "description": "This table presents a comprehensive performance comparison of three different LLMs (Llama2-70B, GPT-3.5, and GPT-4) across various claim verification tasks using a zero-shot setting.  The results are broken down by the number of hops required for reasoning (2-hop, 3-hop, 4-hop) and show the performance of four different methods: Standard (baseline), InfoRE (the proposed method), CoT (Chain-of-Thought), and InfoRE + CoT (combination of InfoRE and CoT).  The improvements achieved by InfoRE over the baseline are clearly indicated in the table for each task and LLM.  This demonstrates the effectiveness of the InfoRE method in enhancing the reasoning capabilities of LLMs.", "section": "5.1 Main Results"}, {"figure_path": "SciWuYPNG0/tables/tables_6_1.jpg", "caption": "Table 1: Zero-shot performance on claim verification task across three LLMs.", "description": "This table presents a comparison of the performance of three different large language models (LLMs) - Llama2-70B, GPT-3.5, and GPT-4 - on claim verification tasks.  It shows the zero-shot performance (no training data) of each LLM using four methods: Standard (direct reasoning), InfoRE (information re-organization), CoT (chain of thought), and a combination of InfoRE and CoT.  The results are broken down by dataset (HOVER, FEVEROUS, SCIFACT) and, for HOVER, by the number of reasoning hops (2-hop, 3-hop, and 4-hop). The improvement achieved by InfoRE is highlighted.", "section": "5.1 Main Results"}, {"figure_path": "SciWuYPNG0/tables/tables_6_2.jpg", "caption": "Table 3: F1 performance of ablation studies.", "description": "This table presents the results of ablation studies conducted on the 2WikiMultiHopQA dataset using GPT-3.5.  The purpose was to evaluate the individual contributions of the extraction and pruning components within the InfoRE method.  Rows show the F1 score when either extraction or pruning is removed from the model, and also when a similarity-based method replaces the RL-based pruning.  The results demonstrate that both extraction and pruning improve performance, with the extraction step contributing more significantly than the pruning step.", "section": "5.2 Analysis"}, {"figure_path": "SciWuYPNG0/tables/tables_7_1.jpg", "caption": "Table 4: Qualitative evaluation results on 2Wiki-MultiHopQA dataset. Avg R denotes the weighted average ranking score. The larger ranking score denotes better information quality.", "description": "This table presents a qualitative evaluation of the quality of re-organized information generated by GPT-3.5 and GPT-4 models, compared to the original text.  The evaluation is performed using GPT-4 (gpt-4-32k) on 100 samples from the 2WikiMultiHopQA dataset.  Two aspects of information quality are assessed: Depth (presence of multiple relationships and insightful perspectives) and Clarity (clear and precise information). Each aspect is rated on a 3-point scale (1-3), with 3 representing the highest quality. The weighted average score (Avg R) summarizes the overall quality for each information type.", "section": "5.2 Analysis"}, {"figure_path": "SciWuYPNG0/tables/tables_7_2.jpg", "caption": "Table 1: Zero-shot performance on claim verification task across three LLMs.", "description": "This table presents a comprehensive performance comparison of three large language models (LLMs): Llama2-70B, GPT-3.5, and GPT-4, across three different claim verification datasets (HOVER, FEVEROUS, and SCIFACT) using a zero-shot setting.  The performance is measured in terms of F1 score for each model on various hop levels.  Comparisons are made between the standard approach, the Chain-of-Thought (CoT) method, the proposed InfoRE method, and the combination of InfoRE and CoT.  The table showcases the significant improvements achieved by the InfoRE method, particularly in complex multi-hop reasoning scenarios.", "section": "5.1 Main Results"}, {"figure_path": "SciWuYPNG0/tables/tables_13_1.jpg", "caption": "Table 1: Zero-shot performance on claim verification task across three LLMs.", "description": "This table presents a comparison of zero-shot performance on claim verification tasks across three different large language models (LLMs): Llama2-70B, GPT-3.5, and GPT-4.  The performance is measured across different datasets (HOVER, FEVEROUS, SCIFACT) and various experimental setups (Standard, InfoRE, CoT, InfoRE + CoT).  The results show the improvement achieved by the proposed InfoRE method in improving the zero-shot reasoning performance of LLMs.", "section": "5.1 Main Results"}, {"figure_path": "SciWuYPNG0/tables/tables_15_1.jpg", "caption": "Table 1: Zero-shot performance on claim verification task across three LLMs.", "description": "This table presents a comprehensive performance comparison of three large language models (LLMs) \u2014 Llama2-70B, GPT-3.5, and GPT-4 \u2014 on claim verification tasks.  The models are evaluated using several different methods: a standard approach, a chain-of-thought (CoT) approach, the proposed InfoRE method, and a combination of InfoRE and CoT. The results are shown for three different datasets (HOVER with 2-hop, 3-hop, and 4-hop levels, FEVEROUS, and SCIFACT) and demonstrate the impact of each method on the models' ability to perform zero-shot claim verification.", "section": "5.1 Main Results"}, {"figure_path": "SciWuYPNG0/tables/tables_16_1.jpg", "caption": "Table 1: Zero-shot performance on claim verification task across three LLMs.", "description": "This table presents a comparison of the performance of three different large language models (LLMs) - Llama2-70B, GPT-3.5, and GPT-4 - on claim verification tasks.  It shows the zero-shot performance (no fine-tuning) across three different datasets (HOVER, FEVEROUS, SCIFACT) using several methods: Standard (baseline), InfoRE (the proposed method), Chain-of-Thought (CoT), and the combination of InfoRE and CoT. The results are presented as F1 scores for 2-hop, 3-hop, and 4-hop reasoning tasks for the HOVER dataset, and overall F1 score for FEVEROUS and SCIFACT.", "section": "5.1 Main Results"}]