{"references": [{"fullname_first_author": "Tuomas Haarnoja", "paper_title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor", "publication_date": "2018-07-01", "reason": "This paper introduces the Soft Actor-Critic algorithm, a key foundation for the entropy-regularized approach used in the current research."}, {"fullname_first_author": "Sergey Levine", "paper_title": "Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems", "publication_date": "2020-05-01", "reason": "This work provides a comprehensive overview of offline reinforcement learning, which is the main focus of the current paper and essential background reading."}, {"fullname_first_author": "Aviral Kumar", "paper_title": "Conservative Q-Learning for Offline Reinforcement Learning", "publication_date": "2020-12-01", "reason": "This paper introduces Conservative Q-Learning, a crucial method for addressing the overestimation problem in offline RL, a challenge directly tackled by this study."}, {"fullname_first_author": "Zhendong Wang", "paper_title": "Diffusion Policies as an Expressive Policy Class for Offline Reinforcement Learning", "publication_date": "2022-08-01", "reason": "This paper is highly relevant because it introduces the use of diffusion models as policies in offline RL, which is a direct precursor to the current research."}, {"fullname_first_author": "Gaon An", "paper_title": "Uncertainty-based Offline Reinforcement Learning with Diversified Q-Ensemble", "publication_date": "2021-12-01", "reason": "This paper utilizes Q-ensembles to address overestimation, which is a key technique also employed in the current research to enhance robustness."}]}