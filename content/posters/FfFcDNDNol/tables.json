[{"figure_path": "FfFcDNDNol/tables/tables_6_1.jpg", "caption": "Table 1: RLbreaker vs. five baseline attacks in jailbreaking effectiveness on three target models. All the metrics are normalized between 0 and 1 and a higher value indicates more successful attacks. \"N/A\" means not available. The results of the other three models and the left two metrics are shown in Appendix D.1.", "description": "This table compares the effectiveness of RLbreaker against five other state-of-the-art jailbreaking methods across three different large language models (LLMs).  The metrics used are normalized to a scale of 0 to 1, where higher scores indicate better performance.  The table shows the success rates in terms of GPT-Judge and similarity (Sim.) scores for each method on a full set of harmful questions (Full) and a subset of the 50 most harmful questions (Max50). \"N/A\" signifies missing data. Additional results are available in Appendix D.1.", "section": "4.1 Attack Effectiveness and Efficiency"}, {"figure_path": "FfFcDNDNol/tables/tables_6_2.jpg", "caption": "Table 1: RLbreaker vs. five baseline attacks in jailbreaking effectiveness on three target models. All the metrics are normalized between 0 and 1 and a higher value indicates more successful attacks. \"N/A\" means not available. The results of the other three models and the left two metrics are shown in Appendix D.1.", "description": "This table compares the performance of RLbreaker against five other state-of-the-art jailbreaking methods across three different large language models (LLMs): Llama2-70b-chat, Mixtral-8x7B-Instruct, and GPT-3.5-turbo.  The metrics used to evaluate the effectiveness of the attacks are normalized between 0 and 1, with higher values indicating greater success. The table shows that RLbreaker significantly outperforms existing methods.", "section": "4.1 Attack Effectiveness and Efficiency"}, {"figure_path": "FfFcDNDNol/tables/tables_19_1.jpg", "caption": "Table 1: RLbreaker vs. five baseline attacks in jailbreaking effectiveness on three target models. All the metrics are normalized between 0 and 1 and a higher value indicates more successful attacks. \"N/A\" means not available. The results of the other three models and the left two metrics are shown in Appendix D.1.", "description": "This table compares the performance of RLbreaker against five other state-of-the-art jailbreaking attack methods across three different large language models (LLMs).  The metrics used to evaluate performance are normalized between 0 and 1 for easy comparison.  Higher scores indicate more effective jailbreaks.  Note that results for three additional LLMs and two additional metrics are provided in Appendix D.1.", "section": "4.1 Attack Effectiveness and Efficiency"}, {"figure_path": "FfFcDNDNol/tables/tables_20_1.jpg", "caption": "Table 1: RLbreaker vs. five baseline attacks in jailbreaking effectiveness on three target models. All the metrics are normalized between 0 and 1 and a higher value indicates more successful attacks. \"N/A\" means not available. The results of the other three models and the left two metrics are shown in Appendix D.1.", "description": "This table compares the performance of RLbreaker against five other state-of-the-art jailbreaking methods across three different large language models (LLMs).  The metrics used (Sim., GPT-Judge) assess the effectiveness of the attacks.  Higher scores indicate more successful jailbreaks.  Note that results for three additional LLMs and two additional metrics are provided in the appendix.", "section": "4.1 Attack Effectiveness and Efficiency"}, {"figure_path": "FfFcDNDNol/tables/tables_21_1.jpg", "caption": "Table 1: RLbreaker vs. five baseline attacks in jailbreaking effectiveness on three target models. All the metrics are normalized between 0 and 1 and a higher value indicates more successful attacks. \"N/A\" means not available. The results of the other three models and the left two metrics are shown in Appendix D.1.", "description": "This table compares the performance of RLbreaker against five other jailbreaking attack methods on three different large language models (LLMs).  The metrics used to evaluate effectiveness are normalized between 0 and 1, with higher scores indicating more successful attacks.  The table shows success rates using two metrics (Sim. and GPT-Judge) and includes results for a \"Full\" dataset and a subset called \"Max50\".  Some results are marked as \"N/A\" because they were not available.", "section": "4.1 Attack Effectiveness and Efficiency"}, {"figure_path": "FfFcDNDNol/tables/tables_22_1.jpg", "caption": "Table 1: RLbreaker vs. five baseline attacks in jailbreaking effectiveness on three target models. All the metrics are normalized between 0 and 1 and a higher value indicates more successful attacks. \"N/A\" means not available. The results of the other three models and the left two metrics are shown in Appendix D.1.", "description": "This table compares the performance of RLbreaker against five other state-of-the-art jailbreaking methods across three different large language models (LLMs).  The metrics used are normalized to a scale of 0 to 1, with higher scores indicating better attack success.  It shows success rates using two different metrics (Sim. and GPT-Judge) and includes results for two different datasets (Full and Max50).  Note that results for three additional LLMs and two additional metrics can be found in Appendix D.1.", "section": "4.1 Attack Effectiveness and Efficiency"}, {"figure_path": "FfFcDNDNol/tables/tables_23_1.jpg", "caption": "Table 1: RLbreaker vs. five baseline attacks in jailbreaking effectiveness on three target models. All the metrics are normalized between 0 and 1 and a higher value indicates more successful attacks. \"N/A\" means not available. The results of the other three models and the left two metrics are shown in Appendix D.1.", "description": "This table compares the performance of RLbreaker against five other state-of-the-art jailbreaking methods across three different large language models (LLMs).  The metrics used to evaluate performance are normalized between 0 and 1, with higher values indicating more successful attacks.  The table shows that RLbreaker generally outperforms the other methods across different LLMs and metrics, demonstrating its superiority in jailbreaking effectiveness. Some results are marked as N/A (not available), indicating that data for those specific LLM-metric combinations was not collected for that particular experiment.", "section": "4.1 Attack Effectiveness and Efficiency"}, {"figure_path": "FfFcDNDNol/tables/tables_24_1.jpg", "caption": "Table 1: RLbreaker vs. five baseline attacks in jailbreaking effectiveness on three target models. All the metrics are normalized between 0 and 1 and a higher value indicates more successful attacks. \"N/A\" means not available. The results of the other three models and the left two metrics are shown in Appendix D.1.", "description": "This table compares the performance of RLbreaker against five other state-of-the-art jailbreaking methods across three different large language models (LLMs).  The metrics used are normalized between 0 and 1, with higher scores indicating more effective jailbreaking.  The table shows the success rates using two different metrics (Sim. and GPT-Judge) for a full set of harmful questions, and a subset of the 50 most harmful questions (Max50).  The results for three additional LLMs and two additional metrics are included in the appendix.", "section": "4.1 Attack Effectiveness and Efficiency"}, {"figure_path": "FfFcDNDNol/tables/tables_24_2.jpg", "caption": "Table 10: Attack effectiveness when baselines' termination condition is replaced as GPT-Judge. \"Original\" denotes using their own termination condition. \"GPT-Judge\" denotes using GPT-Judge as a termination condition. We report the GPT-Judge score.", "description": "This table compares the effectiveness of several jailbreaking attack methods on a target LLM, using two different termination conditions: the original termination condition used by each method and a new condition using GPT-Judge. The GPT-Judge score measures the percentage of harmful questions answered correctly by the target LLM under each attack.", "section": "4.2 Resiliency against Jailbreaking Defenses"}, {"figure_path": "FfFcDNDNol/tables/tables_25_1.jpg", "caption": "Table 11: RLbreaker's jailbreaking effectiveness on two target LLMs when some reference answers are not available. The percentage within the parentheses indicates the ratio of reference answers in the training sets that are marked as available.", "description": "This table shows the results of the RLbreaker model's performance when a percentage of the reference answers used during training are marked as unavailable.  It demonstrates the model's robustness against incomplete or missing data by showing that its effectiveness in jailbreaking is not significantly affected even with a substantial lack of complete reference answers.", "section": "4.3 Attack Transferability"}]