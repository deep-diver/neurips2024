[{"figure_path": "tVO3b68Oyp/tables/tables_5_1.jpg", "caption": "Table 1: Unsupervised Syllable Boundary Detection and Clustering Accuracy on LibriSpeech [41] Test. For F1 scores, the superscript is tolerance threshold in ms. All other metrics use 50ms. Higher is better.", "description": "This table presents the results of unsupervised syllable boundary detection and clustering experiments using different methods on the LibriSpeech test dataset.  It compares the performance of different approaches, including Feat-Sim, LossPred (the authors' method), and SD-HUBERT.  The metrics used are F1 scores (at different tolerance thresholds), Precision, Recall, R, Cluster Purity and Syllable Purity. Higher scores indicate better performance.", "section": "3.3 Efficient Extraction of Unit Boundaries with SylBoost"}, {"figure_path": "tVO3b68Oyp/tables/tables_6_1.jpg", "caption": "Table 2: Unit Resynthesis. WER/CER results on 4-10 second examples on LibriSpeech [41] test-clean. Hz and Bitrate are measured post Run-Length-Encoding (RLE) on LibriSpeech dev-clean.", "description": "This table presents the Word Error Rate (WER) and Character Error Rate (CER) for resynthesized speech from different models.  It compares the performance of SD-HuBERT with several variations of the SylBoost model, showing improvements in WER and CER as the model is refined with different parameter settings. The Hz and Bitrate columns reflect the temporal resolution and data compression achieved.", "section": "3.2 SylBoost: Bootstrapping Pseudo-Syllabic Units with Iterative Distillation"}, {"figure_path": "tVO3b68Oyp/tables/tables_6_2.jpg", "caption": "Table 3: Main SyllableLM results. We evaluate on sWUGGY (In-Vocab, All, Out-of-Vocab), SBLIMP from ZeroSpeech [39], and tStoryCloze from Hassid et al. [26]. Higher is better. *Estimated.", "description": "This table presents the main results of the SyllableLM model, comparing its performance to other state-of-the-art generative spoken language models.  The evaluation is done across various metrics on three different datasets: sWUGGY, SBLIMP, and tStoryCloze.  Higher scores indicate better performance.  The table includes the number of parameters, number of units, Hz, bits per second (BPS), tokens, GPU hours, and the scores achieved on each dataset. The asterisk indicates estimated values.", "section": "4 Syllable-LM: Speech Unit Language Modeling Over Syllable-Like Units"}, {"figure_path": "tVO3b68Oyp/tables/tables_7_1.jpg", "caption": "Table 4: Boundary detaction with different initialization using Hu-BERT on LS dev-clean", "description": "This table presents the results of boundary detection experiments using different initialization methods with the HuBERT model on the LibriSpeech dev-clean dataset.  The metrics used are F1 score, precision (Pr.), and recall (Re.).  Different initialization methods (-Iter 1, -Iter 2, Loss-Corr, -Iter 1, -Iter 2) are compared, showing the impact of iterative refinement and loss correlation on the accuracy of boundary detection.", "section": "3.3 Efficient Extraction of Unit Boundaries with SylBoost"}, {"figure_path": "tVO3b68Oyp/tables/tables_7_2.jpg", "caption": "Table 6: Holding number of units and unit rate constant. ZeroSpeech development set.", "description": "This table presents the results of experiments where the number of units and unit rate were held constant while other parameters were varied.  The experiments were conducted on the ZeroSpeech development set, measuring the performance of different models.  The table helps to illustrate the impact of these factors on the overall performance and can inform the choice of hyperparameters for optimal model training.", "section": "5.4 Results: Evaluating Unit Quality"}, {"figure_path": "tVO3b68Oyp/tables/tables_8_1.jpg", "caption": "Table 7: Continuation Metrics. We measure PPX@Oracle-VERT and VERT@Oracle-PPX as implemented in Lakhotia et al. [34]", "description": "This table presents the results of continuation metrics, specifically PPX@Oracle-VERT and VERT@Oracle-PPX, for different models.  The metrics are calculated using the methodology described in Lakhotia et al. [34] and are used to evaluate the quality of generated speech continuations.  Lower values generally indicate better performance.", "section": "5.4 Results: Evaluating Unit Quality"}, {"figure_path": "tVO3b68Oyp/tables/tables_15_1.jpg", "caption": "Table 8: Speech pre-training hyper-parameters.", "description": "This table shows the hyperparameters used for pre-training the SyllableLM Base and SyllableLM Large models.  The hyperparameters include the number of layers, embedding dimension, MLP dimension, number of GPUs used, learning rate, Adam \u03b21 and \u03b22, weight decay, learning rate schedule, dropout rate, LayerDrop rate, warmup updates, batch size (in tokens), total number of updates, and whether learned positional embeddings were used.", "section": "A.2 Hardware And Hyperparameters"}, {"figure_path": "tVO3b68Oyp/tables/tables_15_2.jpg", "caption": "Table 9: Inference speed results, measured in Real-Time-Factor, the processed seconds per second. We use 32 Batches with 25 seconds of audio each, which matches the length of our training data. 1 GPU, 16 Cores. Standard error less than 1 sec/sec", "description": "This table shows the inference speed of different models, measured in Real-Time-Factor (RTF). RTF represents the ratio of processed seconds to actual seconds.  A higher RTF indicates faster processing. The table compares the inference speed of the proposed method with other state-of-the-art methods, considering different configurations like the number of units and the frequency.", "section": "A.3 Speedup"}, {"figure_path": "tVO3b68Oyp/tables/tables_16_1.jpg", "caption": "Table 2: Unit Resynthesis. WER/CER results on 4-10 second examples on LibriSpeech [41] test-clean. Hz and Bitrate are measured post Run-Length-Encoding (RLE) on LibriSpeech dev-clean.", "description": "This table presents the Word Error Rate (WER) and Character Error Rate (CER) for resynthesized speech units.  It compares different models, focusing on the impact of the SylBoost method in reducing the WER and CER.  The table also shows the unit rate (Hz) and bitrate (BPS) achieved by each model, highlighting the efficiency of the SylBoost approach in achieving low bitrates while maintaining relatively low WER and CER.", "section": "5.4 Results: Evaluating Unit Quality"}, {"figure_path": "tVO3b68Oyp/tables/tables_17_1.jpg", "caption": "Table 2: Unit Resynthesis. WER/CER results on 4-10 second examples on LibriSpeech [41] test-clean. Hz and Bitrate are measured post Run-Length-Encoding (RLE) on LibriSpeech dev-clean.", "description": "This table presents the Word Error Rate (WER) and Character Error Rate (CER) for resynthesized speech using different models and configurations.  It compares the performance of SD-HuBERT with SylBoost (using HuBERT and Data2Vec2), varying the number of units and the unit rate (Hz). The bitrate (BPS) is also shown, reflecting the data compression achieved. Lower WER and CER values indicate better speech quality.", "section": "5.4 Results: Evaluating Unit Quality"}]