[{"figure_path": "4UvMOnZMam/figures/figures_5_1.jpg", "caption": "Figure 1: Ablation over different architectural choices for RTUs and LRUs. The RTU variants are blue, and the LRU variants are orange. In each subplot, we restrict both architectures in a particular way, reporting prediction error (MSRE) as a function of hidden state size. Across variations, RTUs are often better and, at worst, tie LRU. Here, both architectures were using RTRL.", "description": "This figure shows the ablation study on architectural choices for Recurrent Trace Units (RTUs) and Linear Recurrent Units (LRUs). Different architectural variations are tested, including linear recurrence, linear recurrence with nonlinear output, nonlinear recurrence, linear recurrence with nonlinear projection, and linear recurrence with linear projection.  The Mean Squared Return Error (MSRE) is plotted against the number of hidden units for each architecture.  The results demonstrate that RTUs generally outperform or match the performance of LRUs across different architectural choices, especially when employing Real-Time Recurrent Learning (RTRL).", "section": "4.1 Ablation Study on Architectural Choices for RTUs and LRUs"}, {"figure_path": "4UvMOnZMam/figures/figures_6_1.jpg", "caption": "Figure 2: Learning under resources constraints in Trace Conditioning. Each of the four subplots shows how each algorithm's performance varies as a function of resources. (a) LRU and GRU with T-TBPTT is not competitive with RTUs even as T is increased while restricting the number of hidden units in LRU and GRU so that all algorithms use about the same computation per step. (b) If we allow GRU and LRU's computation to increase (fixed network size) while increasing T, the performance gap remains. (c) Fixing T to a large value to solve the task, we can increase the number of parameters, holding the computation equal for all methods. (d) If we do not require compute to be equal across methods as we scale parameters, then the LRU can eventually match the error of RTU, but GRU cannot. The black dashed line represents the near perfect prediction performance.", "description": "This figure shows the performance of different recurrent neural network architectures (RTUs, LRUs, GRUs) under different computational constraints in a trace conditioning task.  Four subplots illustrate how performance changes with increasing computational resources (measured in FLOPs and number of parameters) and truncation length (for GRUs and LRUs).  RTUs consistently perform well across different resource levels, highlighting their efficiency.", "section": "4.2 Learning under resources constraints"}, {"figure_path": "4UvMOnZMam/figures/figures_7_1.jpg", "caption": "Figure 3: Contrasting runtime in incremental and batch settings. In the incremental setting, evaluated in the animal-learning prediction task, T-BPTT updates scale with truncation length, whereas linear RTRL is constant. With batch updates, evaluated in Ant-P with PPO, linear RTRL remains linear and T-BPTT is slightly more efficient.", "description": "This figure compares the runtime of linear RTRL and T-BPTT methods for incremental and batch settings. The left panel shows that in incremental learning (animal learning prediction task), T-BPTT's runtime scales linearly with the truncation length (T), while linear RTRL remains constant.  The right panel demonstrates that in the batch setting (PPO on the Ant-P environment), linear RTRL maintains its linear runtime, whereas T-BPTT shows more variability but remains relatively efficient, showcasing the advantages of linear RTRL in incremental learning scenarios.", "section": "Linear RTRL Methods in Incremental and Batch Settings"}, {"figure_path": "4UvMOnZMam/figures/figures_8_1.jpg", "caption": "Figure 4: Learning curves on the Mujoco POMDP benchmark. Environments with -P mean that velocity components are occluded from the observations, while -V means that the positions and angles are occluded. All architectures have the same number of recurrent parameters (24k parameter). For each architecture, we show the performance of its best-tuned variant.", "description": "This figure presents the learning curves for different recurrent neural network architectures on the Mujoco Partially Observable Markov Decision Process (POMDP) benchmark.  The benchmark consists of several control tasks where the agent's observations are partially occluded, either by removing velocity information (-P) or by removing position and angle information (-V).  The figure shows how well each architecture learns the task over time, as measured by the undiscounted return.  All architectures are constrained to use the same number of recurrent parameters (24,000), and the plot shows the results for the best hyperparameter setting for each architecture.", "section": "6 Experiments in Memory-Based Control"}, {"figure_path": "4UvMOnZMam/figures/figures_8_2.jpg", "caption": "Figure 5: Reacher, 30 runs with standard errors. The POPGym tasks we consider along with the Reacher POMDP are all long-term memory tasks [32] as the agent must remember and carry the information for a long time. Figure 5 summarizes the results for the reacher POMDP task and the POPGym results can be found in Figure 6. In both cases, we can see that RTUs outperform the other approaches. Non-linear RTUs achieve a better performance than linear RTUs in reacher POMDP, and both achieve a better performance in all tasks than online LRUs. In Reacher POMDP, GRU was able to achieve a similar performance to that of linear RTUs.", "description": "This figure shows the learning curves for different recurrent network architectures on the Reacher POMDP task, which requires remembering important cues over a long time horizon.  Non-linear RTUs significantly outperform linear RTUs, online LRUs, and GRUs, demonstrating their ability to effectively utilize information from the past for better performance in this long-term memory task.", "section": "Experiments in Memory-Based Control"}, {"figure_path": "4UvMOnZMam/figures/figures_9_1.jpg", "caption": "Figure 6: Results across several tasks from the POPGym benchmark.", "description": "This figure presents the results of multiple experiments conducted on various tasks from the POPGym benchmark.  The benchmark likely tests the agents' ability to handle long-term dependencies and partial observability. Each subplot represents a different task (CountRecall, RepeatFirst, Concentration, Autoencode, HigherLower), showing the undiscounted return achieved by different agents (NonLinear RTUs, Linear RTUs, Online LRU, GRU) over time. The figure demonstrates the comparative performance of different recurrent neural network architectures in tasks requiring memory and decision-making under uncertainty.", "section": "6 Experiments in Memory-Based Control"}, {"figure_path": "4UvMOnZMam/figures/figures_15_1.jpg", "caption": "Figure 7: Illustration of the Three State MDP. We used dashed lines for the transitions starting in s3 to make them more visible.", "description": "This figure is a state transition diagram for a three-state Markov Decision Process (MDP). The states are represented as circles labeled s1, s2, and s3.  Solid lines indicate transitions with equal probability (1/3) from s1 and s2 to each of the three states. The dashed lines represent transitions from s3. From s3, the transitions go deterministically back to the previous state in the sequence,  creating a kind of short-term memory effect in the MDP. The self-loops from s1 and s2 indicate that there's also a probability of staying in the same state.", "section": "4.1 Ablation Study on Architectural Choices for RTUs and LRUs"}, {"figure_path": "4UvMOnZMam/figures/figures_15_2.jpg", "caption": "Figure 8: Left: The percentage of correct predictions when training an RNN in the Three State MDP. Right: Number of complex eigenvalues when training an RNN in the Three States MDP. The solid line is the mean over 30 runs, the shaded region area is the standard error, and the lines are individual runs.", "description": "This figure shows the training process of a vanilla RNN on a three-state POMDP task. The left panel displays the prediction accuracy over training steps, reaching 100%. The right panel shows the number of complex eigenvalues in the weight matrix of the RNN during training, averaging above 1.5 which suggests the frequent appearance of complex eigenvalues during training.", "section": "3.1 Revisiting Complex-valued Diagonal Recurrence"}, {"figure_path": "4UvMOnZMam/figures/figures_19_1.jpg", "caption": "Figure 9: Learning rate sensitivity for different parameterizations of r and \u03b8 for RTUs with 80 hidden units.", "description": "This figure shows the impact of different parameterizations of r and \u03b8 on the learning rate sensitivity of RTUs with 80 hidden units. Three different parameterizations are compared: r = exp(-exp(vlog)), \u03b8 = exp(\u03b8log); r = exp(-exp(vlog)), \u03b8; and r = exp(-\u03bd), \u03b8. The y-axis represents the Mean Squared Return Error (MSRE), a measure of prediction accuracy. The x-axis represents the step size used during training. The figure shows that different parameterizations lead to different levels of sensitivity to the learning rate, highlighting the importance of careful parameterization in achieving optimal performance.", "section": "E.1 Empirical Analysis for different r and \u03b8 parameterizations"}, {"figure_path": "4UvMOnZMam/figures/figures_20_1.jpg", "caption": "Figure 9: Learning rate sensitivity for different parameterizations of r and \u03b8 for RTUs with 80 hidden units.", "description": "The figure shows the mean squared return error (MSRE) for different learning rates when training a recurrent trace unit (RTU) network with 80 hidden units.  Three different parameterizations of the complex-valued diagonal recurrence weights (r and \u03b8) are compared:  \n1. r = exp(-exp(vlog)), \u03b8 = exp(\u03b8log)\n2. r = exp(-exp(vlog)), \u03b8\n3. r = exp(-v), \u03b8\nThe plot illustrates the impact of different parameterizations on learning stability and optimal learning rate selection.  The x-axis shows the learning rate while the y-axis represents the MSRE.", "section": "E.1 Empirical Analysis for different r and \u03b8 parameterizations"}, {"figure_path": "4UvMOnZMam/figures/figures_25_1.jpg", "caption": "Figure 11: Comparison to a block diagonal RNN.", "description": "This figure compares the performance of RTUs to other RTRL-based approaches with similar architectures: an online version of LRU and a vanilla block diagonal RNN. The results indicate that seemingly small differences between the diagonal RNNs can result in significantly different behavior. RTUs outperform both online LRUs and block diagonal RNNs. The better performance of RTUs highlights the benefits of using the proposed parameterization and incorporating nonlinearities in the recurrence for achieving better performance in online RL.", "section": "4.1 Ablation Study on Architectural Choices for RTUs and LRUs"}, {"figure_path": "4UvMOnZMam/figures/figures_25_2.jpg", "caption": "Figure 12: Evaluating RTUs with BPTT.", "description": "This figure shows the performance comparison between RTUs trained with RTRL and RTUs trained with T-BPTT in the trace conditioning task. The x-axis represents different truncation lengths used for T-BPTT, and the y-axis represents the mean squared return error (MSRE). The results demonstrate that the performance of RTUs trained with T-BPTT approaches that of RTUs trained with RTRL as the truncation length increases, suggesting that using a longer history improves accuracy with T-BPTT.", "section": "4.2 Learning under resources constraints"}, {"figure_path": "4UvMOnZMam/figures/figures_26_1.jpg", "caption": "Figure 2: Learning under resources constraints in Trace Conditioning. Each of the four subplots shows how each algorithm's performance varies as a function of resources. (a) LRU and GRU with T-TBTT is not competitive with RTUs even as T is increased while restricting the number of hidden units in LRU and GRU so that all algorithms use about the same computation per step. (b) If we allow GRU and LRU's computation to increase (fixed network size) while increasing T, the performance gap remains. (c) Fixing T to a large value to solve the task, we can increase the number of parameters, holding the computation equal for all methods. (d) If we do not require compute to be equal across methods as we scale parameters, then the LRU can eventually match the error of RTU, but GRU cannot. The black dashed line represents the near perfect prediction performance.", "description": "This figure displays a comprehensive analysis of how different recurrent neural network architectures (RTUs, LRUs, GRUs) perform under varying computational resource constraints in a trace conditioning task.  It shows the trade-off between computational budget (measured in FLOPS), truncation length (for T-BPTT algorithms), and the number of parameters.  The results demonstrate RTUs' superior performance and scalability compared to LRUs and GRUs, particularly when computational resources are limited.", "section": "4.2 Learning under resources constraints"}, {"figure_path": "4UvMOnZMam/figures/figures_26_2.jpg", "caption": "Figure 2: Learning under resources constraints in Trace Conditioning. Each of the four subplots shows how each algorithm's performance varies as a function of resources. (a) LRU and GRU with T-TBPTT is not competitive with RTUs even as T is increased while restricting the number of hidden units in LRU and GRU so that all algorithms use about the same computation per step. (b) If we allow GRU and LRU's computation to increase (fixed network size) while increasing T, the performance gap remains. (c) Fixing T to a large value to solve the task, we can increase the number of parameters, holding the computation equal for all methods. (d) If we do not require compute to be equal across methods as we scale parameters, then the LRU can eventually match the error of RTU, but GRU cannot. The black dashed line represents the near perfect prediction performance.", "description": "This figure shows an ablation study on the performance of different recurrent neural network architectures (RTUs, LRUs, and GRUs) under different computational constraints in a trace conditioning task. It demonstrates that RTUs consistently outperform LRUs and GRUs, particularly when computational resources are limited. The figure also illustrates how performance varies when resources are allocated to increasing either truncation length or the number of parameters.", "section": "4.2 Learning under resources constraints"}, {"figure_path": "4UvMOnZMam/figures/figures_26_3.jpg", "caption": "Figure 1: Ablation over different architectural choices for RTUs and LRUs. The RTU variants are blue, and the LRU variants are orange. In each subplot, we restrict both architectures in a particular way, reporting prediction error (MSRE) as a function of hidden state size. Across variations, RTUs are often better and, at worst, tie LRU. Here, both architectures were using RTRL.", "description": "This figure presents an ablation study comparing different architectural variants of RTUs and LRUs on a multi-step prediction task.  Five different architectural variations are tested for both RTUs and LRUs, focusing on where non-linearity is applied in the network.  The mean squared return error (MSRE) is reported for different sizes of the hidden state, demonstrating the impact of architectural choices on prediction accuracy. RTUs consistently perform as well as, or better than, LRUs across all variations.  Note that all models used RTRL in this experiment.", "section": "4.1 Ablation Study on Architectural Choices for RTUs and LRUs"}, {"figure_path": "4UvMOnZMam/figures/figures_27_1.jpg", "caption": "Figure 16: RTUs used in the animal learning benchmark.", "description": "This figure displays the learning rate sensitivity curves for linear and nonlinear RTUs with various hidden unit counts (160, 800, 450, 230, 6100, 3200, 1650) in the animal learning benchmark. The plots show how the mean squared return error (MSRE) changes with different learning rates, providing insights into the optimal learning rate range for RTUs of various sizes and types.", "section": "4.2 Learning under resources constraints"}, {"figure_path": "4UvMOnZMam/figures/figures_27_2.jpg", "caption": "Figure 17: RTUs used in the animal learning benchmark.", "description": "This figure shows the learning rate sensitivity curves for linear and non-linear RTUs in the animal learning prediction benchmark.  It displays how the mean squared return error (MSRE), a measure of prediction accuracy, changes with different learning rates and varying numbers of hidden units (h) within the RTUs. The different colors and shades represent different numbers of hidden units.  The graph helps to identify the optimal learning rate range for each RTU configuration to achieve the best prediction performance.", "section": "4.2 Learning under resources constraints"}, {"figure_path": "4UvMOnZMam/figures/figures_27_3.jpg", "caption": "Figure 18: The impact of stale gradients and stale targets when combining RTRL and PPO on Ant.", "description": "This figure shows the results of an ablation study investigating the effects of stale gradients and stale targets when using RTRL with PPO on the Ant environment from the Mujoco POMDP benchmark.  The experiment compares four conditions: (1) both the gradient and target are computed using the latest data; (2) the gradient is stale but the target is fresh; (3) the gradient is fresh but the target is stale; and (4) both the gradient and target are stale.  The results indicate that using stale gradients leads to better performance than using fresh gradients, and suggest that stale gradients might help PPO maintain the trust region. A rightmost subplot shows the approximate KL divergence between the two policies, illustrating how agents with stale gradients move away from the old policy more slowly than agents with fresh gradients, possibly suggesting that stale gradients might help with maintaining the trust region.", "section": "Integrating Linear RTRL Methods with PPO"}, {"figure_path": "4UvMOnZMam/figures/figures_28_1.jpg", "caption": "Figure 18: The impact of stale gradients and stale targets when combining RTRL and PPO on Ant.", "description": "This figure shows an ablation study on the effects of stale gradients and stale targets when using RTUs with PPO on the Ant environment. It compares four scenarios: using true gradients and true targets, true gradients and false targets, false gradients and true targets, and false gradients and false targets.  The results indicate that using stale gradients leads to better performance compared to recomputing the gradient, suggesting that stale gradients may help PPO maintain the trust region.  The impact of stale value targets and advantage estimates is minimal.", "section": "Experiments in Memory-Based Control"}, {"figure_path": "4UvMOnZMam/figures/figures_28_2.jpg", "caption": "Figure 18: The impact of stale gradients and stale targets when combining RTRL and PPO on Ant.", "description": "This figure shows an ablation study on the effects of stale gradients and targets when using RTRL with PPO on the Ant environment.  It compares four scenarios: using fresh gradients and targets, using fresh gradients with stale targets, using stale gradients with fresh targets, and using stale gradients and stale targets. The results show that using stale gradients consistently leads to better performance than recomputing the gradients, regardless of whether the targets are stale or fresh. This suggests a possible benefit from using stale gradients in this specific setting.", "section": "Experiments in Memory-Based Control"}, {"figure_path": "4UvMOnZMam/figures/figures_29_1.jpg", "caption": "Figure 21: Agents architectures used in our control experiments.", "description": "This figure shows the architecture of the agents used in the control experiments.  The observation is first passed through an MLP, which then feeds into a memory model (either a Recurrent Trace Unit (RTU), Linear Recurrent Unit (LRU), or Gated Recurrent Unit (GRU)). The output of the memory model is then passed to two separate MLPs: one for the actor head that outputs the action, and one for the critic head that outputs the state value.", "section": "6 Experiments in Memory-Based Control"}, {"figure_path": "4UvMOnZMam/figures/figures_29_2.jpg", "caption": "Figure 4: Learning curves on the Mujoco POMDP benchmark. Environments with -P mean that velocity components are occluded from the observations, while -V means that the positions and angles are occluded. All architectures have the same number of recurrent parameters (24k parameter). For each architecture, we show the performance of its best-tuned variant.", "description": "This figure displays the learning curves for several different reinforcement learning agents on a set of MuJoCo environments.  The environments are partially observable, meaning some information about the state of the environment is hidden from the agent.  The \"P\" versions of the environments hide velocity information, while the \"V\" versions hide position and angle information. Each line represents a different RL algorithm (NonLinear RTU, Linear RTU, GRU, Online LRU, LRU) which are each attempting to learn an optimal control policy for their respective environments. The x-axis represents training steps, and the y-axis represents the total discounted reward.  All algorithms used the same number of parameters, allowing for a comparison based on architectural differences rather than simply computational resources.", "section": "Experiments in Memory-Based Control"}, {"figure_path": "4UvMOnZMam/figures/figures_30_1.jpg", "caption": "Figure 4: Learning curves on the Mujoco POMDP benchmark. Environments with -P mean that velocity components are occluded from the observations, while -V means that the positions and angles are occluded. All architectures have the same number of recurrent parameters (24k parameter). For each architecture, we show the performance of its best-tuned variant.", "description": "This figure displays the learning curves for different recurrent neural network architectures (NonLinear RTU, Linear RTU, GRU, Online LRU, and LRU) on various Mujoco POMDP benchmark tasks.  The benchmark tasks involve partially observable environments where either velocity or positional information is hidden. The results show the undiscounted return over the course of 1 million environment steps.  All architectures are constrained to have the same number of recurrent parameters. The best performing variant of each architecture is presented in the plot.", "section": "Experiments in Memory-Based Control"}, {"figure_path": "4UvMOnZMam/figures/figures_30_2.jpg", "caption": "Figure 2: Learning under resources constraints in Trace Conditioning. Each of the four subplots shows how each algorithm's performance varies as a function of resources. (a) LRU and GRU with T-TBPTT is not competitive with RTUs even as T is increased while restricting the number of hidden units in LRU and GRU so that all algorithms use about the same computation per step. (b) If we allow GRU and LRU's computation to increase (fixed network size) while increasing T, the performance gap remains. (c) Fixing T to a large value to solve the task, we can increase the number of parameters, holding the computation equal for all methods. (d) If we do not require compute to be equal across methods as we scale parameters, then the LRU can eventually match the error of RTU, but GRU cannot. The black dashed line represents the near perfect prediction performance.", "description": "This figure compares the performance of RTUs, GRUs, and LRUs under various resource constraints in the Trace Conditioning task. It demonstrates that RTUs outperform other methods across different resource settings.  Subplots show performance changes with varying truncation length (T), compute (FLOPS), and number of parameters.", "section": "4.2 Learning under resources constraints"}, {"figure_path": "4UvMOnZMam/figures/figures_30_3.jpg", "caption": "Figure 1: Ablation over different architectural choices for RTUs and LRUs. The RTU variants are blue, and the LRU variants are orange. In each subplot, we restrict both architectures in a particular way, reporting prediction error (MSRE) as a function of hidden state size. Across variations, RTUs are often better and, at worst, tie LRU. Here, both architectures were using RTRL.", "description": "This figure presents an ablation study comparing the performance of Recurrent Trace Units (RTUs) and Linear Recurrent Units (LRUs) across different architectural variations.  Each subplot shows a comparison with a specific architectural constraint (e.g., linear recurrence with nonlinear output, nonlinear recurrence), showing mean squared return error (MSRE) plotted against the number of hidden units.  The results indicate that RTUs generally outperform or match the performance of LRUs across various architectures when both use the Real-Time Recurrent Learning (RTRL) algorithm.", "section": "4.1 Ablation Study on Architectural Choices for RTUs and LRUs"}, {"figure_path": "4UvMOnZMam/figures/figures_30_4.jpg", "caption": "Figure 2: Learning under resources constraints in Trace Conditioning. Each of the four subplots shows how each algorithm's performance varies as a function of resources. (a) LRU and GRU with T-TBPTT is not competitive with RTUs even as T is increased while restricting the number of hidden units in LRU and GRU so that all algorithms use about the same computation per step. (b) If we allow GRU and LRU's computation to increase (fixed network size) while increasing T, the performance gap remains. (c) Fixing T to a large value to solve the task, we can increase the number of parameters, holding the computation equal for all methods. (d) If we do not require compute to be equal across methods as we scale parameters, then the LRU can eventually match the error of RTU, but GRU cannot. The black dashed line represents the near perfect prediction performance.", "description": "This figure presents an ablation study comparing the performance of RTUs, GRUs, and LRUs under different resource constraints in a trace conditioning task.  Subplots (a), (b), and (c) show how performance varies with different levels of computational budget, while subplot (d) focuses on scaling performance with the number of parameters. The results highlight that RTUs are more computationally efficient and achieve better or comparable performance to the other methods.", "section": "4.2 Learning under resources constraints"}, {"figure_path": "4UvMOnZMam/figures/figures_31_1.jpg", "caption": "Figure 4: Learning curves on the Mujoco POMDP benchmark. Environments with -P mean that velocity components are occluded from the observations, while -V means that the positions and angles are occluded. All architectures have the same number of recurrent parameters (24k parameter). For each architecture, we show the performance of its best-tuned variant.", "description": "This figure displays the learning curves for different RL agents on various MuJoCo environments.  The \"P\" and \"V\" suffixes indicate whether position or velocity information, respectively, was hidden from the agent's observations.  All agents had the same number of recurrent parameters. The results showcase the performance of each agent's best hyperparameter configuration.", "section": "6 Experiments in Memory-Based Control"}]