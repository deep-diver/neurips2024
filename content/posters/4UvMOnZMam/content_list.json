[{"type": "text", "text": "Real-Time Recurrent Learning using Trace Units in Reinforcement Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Esraa Elelimy, Adam White\u2217, Michael Bowling\u2217, Martha White   \nUniversity of Alberta, Alberta Machine Intelligence Institute (Amii) \u2217Canada CIFAR AI Chair elelimy,amw8,mbowling,whitem@ualberta.ca ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recurrent Neural Networks (RNNs) are used to learn representations in partially observable environments. For agents that learn online and continually interact with the environment, it is desirable to train RNNs with real-time recurrent learning (RTRL); unfortunately, RTRL is prohibitively expensive for standard RNNs. A promising direction is to use linear recurrent architectures (LRUs), where dense recurrent weights are replaced with a complex-valued diagonal, making RTRL efficient. In this work, we build on these insights to provide a lightweight but effective approach for training RNNs in online RL. We introduce Recurrent Trace Units (RTUs), a small modification on LRUs that we nonetheless find to have significant performance benefits over LRUs when trained with RTRL. We find RTUs significantly outperform other recurrent architectures across several partially observable environments while using significantly less computation.1 ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Agents, animals, and people perceive their surrounding environment through imperfect sensory observations. When the state of the environment is partially observable, agents construct and maintain their own state from the stream of observations. The constructed agent state summarizes past environment-agent interactions in a form that is useful to predict and control future interactions [41]. Recurrent Neural Networks (RNNs) provide a flexible architecture for constructing agent state [19, 21, 13, 6, 10]. ", "page_idx": 0}, {"type": "text", "text": "While standard RNN architectures have been mainly supplanted by Transformers [44], in online reinforcement learning settings where the agent learns while interacting with the environment, RNNs remain a promising direction to pursue [17, 11]. There are two main issues that limit the use of selfattention mechanisms from Transformers in online learning. First, calculating the similarity between each pair of points results in a computational complexity that is a function of $k^{2}$ , where $k$ is the sequence length. Moreover, calculating the similarity between all pairs ignores the temporal order of the data points, which limits the usefulness of self-attention when the data is temporally correlated [47]. Second, we need access to the whole sequence of observations before taking an action or updating the learnable parameters, which is impractical in continual learning. While recent works have reduced the complexity of transformers from quadratic in the sequence length to linear [15, 16, 37], the entire sequence length is still needed to train such architectures. Gated Transformer-XL attempts to overcome this issue by keeping a moving window of previous observations [35]. A window of past observations does not scale well to long sequences\u2014the computation is quadratic in the sequence length\u2014and a window is one particular fixed function of history. The simpler recursive form in RNNs, on the other hand, can learn a great variety of functions of history from the data and is well suited for updating the state online from a sequential data stream and have been shown to outperform transformers in such settings [23]. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "A key open question is how to efficiently train RNNs in online RL. We can divide the literature into methods that approximate Real-Time Recurrent Learning (RTRL) and those that restrict the recurrent architecture. RTRL [46] exploits the recursive nature of the gradient for RNNs, carrying forward the needed gradient information instead of unrolling the recurrent dynamics back in time like Truncated Backpropagation Through Time (T-BPTT) [45]. RTRL avoids storing past data but is so computationally expensive and is intractable for even moderately sized networks. Several methods approximate the RTRL gradient update, including NoBackTrack [33], Unbiased Online Recurrent Optimization [43, 5], Sparse N-Step Approximation (SnAp) [28]. All of these methods produce a biased gradient estimate. Other works have tried to approximate an unbiased gradient estimate of BPTT specifically for the case of policy gradient updates in RL. However, the approximation resulted in a high variance due to added stochasticity to the policy [1]. ", "page_idx": 1}, {"type": "text", "text": "Methods in the second category usually restrict the RNN architecture to a diagonal RNN [9, 30], including Columnar Networks [18], the element-wise LSTM [17], and Independently Recurrent Neural Networks (IndRNNs) [20]. The RTRL algorithm is computationally efficient for such architectures. However, this approach sacrifices representation power and can perform poorly [18]. Recent work suggests overcoming the poor performance of diagonal RNNs with a small modification: having a complex-valued recurrent state instead of restricting it to real values [34]. In fact, as we will show in section 3.1, there exists an equivalence between using a dense linear recurrent layer and a diagonal recurrent layer with complex values, indicating no loss of representational capacity. LRUs have been combined with RTRL [48], though only empirically explored for supervised learning datasets. ", "page_idx": 1}, {"type": "text", "text": "In this work, we extend the insights from LRUs into the online RL setting. Our primary contribution are our experiments showing that such a lightweight recurrent architecture can outperform standard approaches like Gated Recurrent Units (GRUs) [4] in RL, with significantly less computation. To obtain this result, we propose a small extension on LRUs, which we call Recurrent Trace Units (RTUs). RTUs incorporate nonlinearity into the recurrence and use a slightly different parameterization than LRUs, but one we find is more amenable to the use of RTRL in online RL than LRUs. We extend Proximal Policy Optimization (PPO) [40] to use RTRL, ablating the decision choices we propose. We provide an in-depth study in an animal-learning prediction benchmark, showing that RTUs scale better than GRUs with increasing compute and number of parameters and that RTUs outperform alternative diagonal recurrent architectures trained with RTRL. We then show across numerous control environments that RTUs have comparable or better performance, compared to GRUs and LRUs. ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We formalize the problem setting as a Partially Observable Markov Decision Process (POMDP). At each time step $t=0,1,2,\\ldots$ , the agent perceives an observation $\\mathbf{x}_{t}$ , a limited view of the state $\\mathbf{s}_{t}\\in\\mathcal{S}$ , and takes an action $A_{t}\\in\\mathcal A({\\bf s}_{t})$ . Depending on the action taken, the agent finds itself in a new state $\\mathbf{s}_{t+1}\\in\\mathcal{S}$ , observes the corresponding observation $\\mathbf{x}_{t+1}$ and a reward $R_{t+1}\\in\\mathbb{R}$ . In the online control setting, the agent\u2019s goal is to maximize the discounted sum of the received rewards. It may also make predictions about its environment, such as future observations\u2019 outcomes. ", "page_idx": 1}, {"type": "text", "text": "For prediction and control in a partially observable environment, the agent should use the stream of observations to construct its agent state. The agent state summarizes information from the history of the agent-environment interactions that are useful for prediction and control [41]. We could use the whole history up to $t$ , namely $({\\bf x}_{0},A_{1},R_{1},{\\bf x}_{1},A_{2},R_{2},...\\,{\\bf x}_{t})$ , as the agent state. Though the history preserves all the information, it is not feasible to use directly. We want the agent to have constant memory and computation per time step and storing the whole history causes the memory and the computation to grow with time. Instead, the agent needs to compress this history into a concise representation. We refer to the agent\u2019s internal representation of the history at time $t$ as its agent state or its hidden state $\\mathbf{h}_{t}$ . The agent constructs its current agent state $\\mathbf{h}_{\\mathbf{t}}\\in\\mathbb{R}^{n}$ from its previous agent state $\\mathbf{h}_{t-1}\\in\\mathbb{R}^{n}$ and the recent observation $\\mathbf x_{t}\\in\\mathbb R^{d}$ using a state-update function $\\mathbf{g}:\\mathbb{R}^{n}\\times\\mathbb{R}^{d}\\to\\mathbb{R}^{n}$ : $\\mathbf h_{t}=\\mathbf g(\\mathbf h_{t-1},\\mathbf x_{t})$ . ", "page_idx": 1}, {"type": "text", "text": "One way to learn this state-update function $\\mathbf{g}$ is with a recurrent neural network (RNN). A simple form is a linear recurrent layer, where $\\mathbf{g}(\\mathbf{h}_{t-1},\\mathbf{x}_{t})=\\mathbf{W}_{x}\\mathbf{x}_{t}+\\mathbf{W}_{h}\\mathbf{h}_{t-1}$ for weight matrices $\\mathbf{W}_{x}$ and $\\mathbf{W}_{h}$ . We can also add a nonlinear activation, such as ReLU. ", "page_idx": 2}, {"type": "text", "text": "In general, we will write ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{h}_{t}=\\mathbf{g}(\\mathbf{h}_{t-1},\\mathbf{x}_{t},\\psi),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\psi$ are the learnable parameters in the network. The agent maps the agent state $\\mathbf{h}_{t}$ to an output $\\hat{y}_{t}$ and then receives a loss $\\bar{\\mathcal{L}}_{t}\\doteq\\mathcal{L}(\\hat{y}_{t},y_{t})$ indicating how far the output is from a target $y_{t}$ . The agent updates $\\pmb{\\psi}$ to minimize this loss over time. ", "page_idx": 2}, {"type": "text", "text": "Two main gradient-based algorithms are widely used to train RNNs: Truncated Backpropagation Through Time (TBPTT) and Real-Time Recurrent Learning (RTRL). T-BPTT specifies a truncation length $T$ , which controls the number of steps considered when calculating the gradient [45]. As a result, the computation and memory complexities of T-BPTT are functions of the truncation length. Learning with T-BPTT involves a trade-off between the network\u2019s ability to look further back in time and its compute and memory requirements. Picking a large $T$ can be expensive, or require us to limit the network size, but picking too small of a $T$ can cause the agent to miss important relationships and so result in poor performance. ", "page_idx": 2}, {"type": "text", "text": "Williams and Zipser (1989) introduced the Real-time Recurrent Learning algorithm (RTRL) as a learning algorithm for continual recurrent learning. Instead of unrolling the recurrent dynamics back in time, RTRL computes the gradient using the most recent observation, and the gradient is calculated and carried from the last step [46]. Assuming the network parameters have not changed, this recursive form gives the exact gradient and does not suffer from the truncation bias inherent to T-BPTT. We provide a more detailed background on the BPTT and RTRL in Appendix A. In reality, the agent updates its parameters frequently, so the gradient information saved from previous time steps is stale, i.e., calculated w.r.t old parameters; yet, under the assumption of small learning rates, RTRL is known to converge [46]. These properties make RTRL ideal for online learning, but unfortunately, there is a catch: its computational complexity is quartic, of fourth order, in the size of $\\mathbf{h}_{t}$ , which can be prohibitively expensive. For this reason, we pursue a restricted diagonal form in this work, for which RTRL is efficient and linear in $\\mathbf{h}_{t}$ . ", "page_idx": 2}, {"type": "text", "text": "3 Recurrent Trace Units ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we introduce Recurrent Trace Units (RTUs). We start by revisiting why complexvalued diagonals represent dense recurrent layers, and why using real-valued diagonals is insufficient. We then introduce the specific form for RTUs that leverages this relationship. We then provide the RTRL update for RTUs, highlighting that it is simple to implement and linear in the hidden dimension. We finally contrast RTUs to LRUs and motivate why this small extension beyond LRUs is worthwhile. ", "page_idx": 2}, {"type": "text", "text": "3.1 Revisiting Complex-valued Diagonal Recurrence ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Assume we have the recurrence relationship, with learnable parameters $\\mathbf{W}_{h}\\in\\mathbb{R}^{n\\times n}$ and $\\mathbf{W}_{x}\\in$ $\\mathbb{R}^{n\\times d}$ , $\\mathbf{h}_{t}\\,\\doteq\\,\\,\\mathbf{W}_{h}\\mathbf{h}_{t-1}+\\mathbf{W}_{x}\\mathbf{u}(\\mathbf{x}_{t})$ , where u can be any transformation of the inputs $\\mathbf{x}_{t}$ before they are inputted into the recurrent layer. We can rewrite the square matrix $\\mathbf{W}_{h}$ using an eigenvalue decomposition ${\\bf W}_{h}\\;=\\;{\\bf P}\\;{\\\\\\Lambda}\\;{\\bf P}^{-1}$ , where $\\mathbf{P}$ contains the $n$ linearly independent eigenvectors and $\\mathbf{A}\\in\\mathbb{C}^{\\bar{n}\\times n}$ is a diagonal matrix with the corresponding eigenvalues. Then we have that ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{h}_{t}=\\mathbf{P}(\\mathbf{\\boldsymbol{\\Lambda}}\\,\\mathbf{P}^{-1}\\,\\mathbf{h}_{t-1}\\,+\\mathbf{P}^{-1}\\mathbf{W}_{x}\\,\\mathbf{u}(\\mathbf{x}_{t}))\\implies\\mathbf{P}^{-1}\\mathbf{h}_{t}=\\mathbf{\\boldsymbol{\\Lambda}}\\mathbf{P}^{-1}\\,\\mathbf{h}_{t-1}\\,+\\,\\mathbf{P}^{-1}\\,\\mathbf{W}_{x}\\,\\mathbf{u}(\\mathbf{x}_{t})\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "By defining $\\overline{{\\mathbf{h}}}_{t}\\;\\doteq\\;\\mathbf{P}^{-1}\\;\\mathbf{h}_{t}\\;\\in\\;\\mathbb{C}^{n}$ and $\\begin{array}{r l r}{\\overline{{\\mathbf{W}}}_{x}}&{\\doteq}&{\\mathbf{P}^{-1}\\mathbf{W}_{x}\\;\\in\\;\\mathbb{C}^{n\\times d}}\\end{array}$ , we get a new recurrence $\\overline{{\\mathbf{h}}}_{t}^{\\cdot}=\\,\\mathbf{A}\\overline{{\\mathbf{h}}}_{t-1}^{\\;\\;\\;-}\\mathbf{\\overline{{W}}}_{x}\\mathbf{u}(\\mathbf{x}_{t})$ . ", "page_idx": 2}, {"type": "text", "text": "We can see $\\overline{{\\mathbf{h}}}_{t}$ and $\\mathbf{h}_{t}$ are representationally equivalent: they are linearly weighted for downstream predictions, and so the linear transformation on $\\overline{{\\mathbf{h}}}_{t}$ can fold into this downstream linear weighting. But it is more computationally efficient to use $\\overline{{\\mathbf{h}}}_{t}$ with a diagonal weight matrix $\\Lambda$ , meaning each hidden unit only has one recurrent relation instead of $\\mathbf{n}$ . LRUs precisely leverage this equivalence [34]. Specifically, they learn a complex-valued $\\overline{{\\mathbf{h}}}_{t}$ , and use $\\mathbf{Re}(\\overline{{\\mathbf{W}}}\\ \\overline{{\\mathbf{h}}}_{t})$ as an input to an MLP for downstream nonlinearity. ", "page_idx": 2}, {"type": "text", "text": "Since we did not impose constraints on the matrix $\\mathbf{W}_{h}$ , other than being diagonalizable, the eigenvalues of $\\mathbf{W}_{h}$ can be complex or real numbers. Previous diagonal RNNs such as eLSTM [17], Columnar networks [18], and IndRNN [20] use only real-valued diagonal matrices. Having only real-valued diagonals assumes that the matrix $\\mathbf{W}_{h}$ is symmetric. We provide a small experiment in Appendix B.2 showing that this assumption does not hold even in the simplest setting and that complex eigenvalues do arise. We also investigate whether this result can be extended beyond linear recurrence, and largely obtain a negative theortical result (see Appendix B.1 and $\\mathbf{C}$ ). ", "page_idx": 3}, {"type": "text", "text": "3.2 The RTU Parameterization ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "A complex number can be represented in three ways: $a+b i$ (the real representation), $r\\exp(i\\theta)$ (the exponential representation), and $r(\\cos(\\theta)+i\\sin(\\theta))$ (the cosine representation). Mathematically, these three representations are equivalent, but do they affect learning differently? Orvieto et al. [34] empirically showed that using the exponential representation resulted in a better-behaved loss function than the real representation on a simple task; we provide some discussion in Appendix D.1 further motivating why the real representation is less stable. We chose instead to pursue the cosine representation, because it allows us to represent the complex hidden vector as two real-valued vectors. The remainder of this section outlines RTUs, with and without nonlinearity in the recurrence. ", "page_idx": 3}, {"type": "text", "text": "Our goal is to learn a complex-valued diagonal matrix with weights $\\lambda_{k}=r_{k}(\\cos(\\theta_{k})+i\\sin(\\theta_{k}))$ on the diagonal, for $k=1,\\hdots,n$ . Multiplying by a complex number is equivalent to multiplying by a $2\\mathtt{x}2$ block matrix with a rescaling. We can use this rotational form to avoid explicitly using complex numbers, and instead use two real-values for each complex-valued hidden node. We write this real-valued matrix $\\mathbf{A}\\in\\mathbb{R}^{2n\\times2n}$ as blocks of rotation matrices2 ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{A}=\\left[\\begin{array}{l l l l}{\\mathbf{c}_{1}}&{}&{}&{}\\\\ {}&{}&{\\cdot\\cdot{\\mathrm{~\\boldmath~\\kappa~}}}&{}\\\\ {}&{}&{\\mathbf{c}_{n}}\\end{array}\\right]\\qquad\\mathrm{where}\\quad\\mathbf{c}_{k}=\\mathbf{r}_{k}\\left[\\cos(\\theta_{k})\\quad-\\sin(\\theta_{k})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Each element of $\\mathbf{h}_{t}=\\mathbf{A}\\mathbf{h}_{t-1}+\\mathbf{W}_{x}\\;\\mathbf{x}_{t}\\in\\mathbb{R}^{2n}$ has two components ${\\mathbf{h}}_{t}^{c_{1}},{\\mathbf{h}}_{t}^{c_{2}}$ , updated recursively: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{h}_{t}^{c_{1}}=\\mathbf{r}\\cos(\\pmb{\\theta})\\odot\\mathbf{h}_{t-1}^{c_{1}}-\\mathbf{r}\\sin(\\pmb{\\theta})\\odot\\mathbf{h}_{t-1}^{c_{2}}+\\mathbf{W}_{x}^{c_{1}}\\mathbf{x}_{t},}\\\\ &{\\mathbf{h}_{t}^{c_{2}}=\\mathbf{r}\\cos(\\pmb{\\theta})\\odot\\mathbf{h}_{t-1}^{c_{2}}+\\mathbf{r}\\sin(\\pmb{\\theta})\\odot\\mathbf{h}_{t-1}^{c_{1}}+\\mathbf{W}_{x}^{c_{2}}\\mathbf{\\Deltax}_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We finally combine the new recurrent states into one state $\\mathbf{h}_{t}\\doteq[\\mathbf{f}(\\mathbf{h}_{t}^{c_{1}});\\mathbf{f}(\\mathbf{h}_{t}^{c_{2}})]$ , potentially using a non-linearity $f$ after the recurrence. ", "page_idx": 3}, {"type": "text", "text": "We also adopt two parameterization choices made in LRUs that showed improved performance. The first is learning logarithmic representations of the parameters rather than learning them directly: instead of learning $\\mathbf{r}$ and $\\pmb{\\theta}$ , the network learns $\\pmb{\\nu}^{\\mathrm{log}}$ and $\\bar{\\pmb{\\theta}}^{\\mathrm{log}}$ , where $\\mathbf{r}\\doteq\\exp(-\\nu),\\nu=\\mathbf{\\bar{\\exp}}(\\nu^{\\mathrm{log}})$ , and $\\pmb{\\theta}^{\\mathrm{log}}\\doteq\\log(\\pmb{\\theta})$ . This re-parametrization restricts the $\\mathbf{r}$ to be $\\in(0,1]$ , required for stability. We found these modifications to improve stability of RTUs (see Appendix E). The second parameterization choice we adopt from LRUs is to multiply the input $(\\mathbf{W}_{x}\\mathbf{x}_{t})_{k}$ by a normalization factor of $\\gamma_{k}=$ $\\left(1-r_{k}^{2}\\right)^{1/2}$ . Putting this all together, the final formulation of RTUs is: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{h}_{t}^{c_{1}}=\\mathbf{g}(\\nu^{\\mathrm{log}},\\theta^{\\mathrm{log}})\\odot\\mathbf{h}_{t-1}^{c_{1}}-\\phi(\\nu^{\\mathrm{log}},\\theta^{\\mathrm{log}})\\odot\\mathbf{h}_{t-1}^{c_{2}}+\\gamma\\odot\\mathbf{W}_{x}^{c_{1}}\\mathbf{x}_{t},}\\\\ &{\\mathbf{h}_{t}^{c_{2}}=\\mathbf{g}(\\nu^{\\mathrm{log}},\\theta^{\\mathrm{log}})\\odot\\mathbf{h}_{t-1}^{c_{2}}+\\phi(\\nu^{\\mathrm{log}},\\theta^{\\mathrm{log}})\\odot\\mathbf{h}_{t-1}^{c_{1}}+\\gamma\\odot\\mathbf{W}_{x}^{c_{2}}\\mathbf{x}_{t},}\\\\ &{\\mathbf{h}_{t}=[\\mathbf{f}(\\mathbf{h}_{t}^{c_{1}});\\mathbf{f}(\\mathbf{h}_{t}^{c_{2}})],}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\gamma\\in\\mathbb{R}^{n}$ is the vector composed of $\\gamma_{k}=(1-\\exp(-\\exp(\\nu_{k}^{\\mathrm{log}}))^{2})^{1/2}$ and ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{g(\\nu_{k},\\theta_{k})\\doteq\\exp(-\\exp(\\nu_{k}^{\\mathrm{log}}))\\cos(\\exp(\\theta_{k}^{\\mathrm{log}})),}\\\\ {\\phi(\\nu_{k},\\theta_{k})\\doteq\\exp(-\\exp(\\nu_{k}^{\\mathrm{log}}))\\sin(\\exp(\\theta_{k}^{\\mathrm{log}})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Note that $\\gamma$ can be absorbed by $\\mathbf{W}$ , and so does not change representation capacity. ", "page_idx": 3}, {"type": "text", "text": "There are two ways to incorporate non-linearity into RTUs: inside the recurrence or after the recurrence. In the above, in Equation (2), the non-linearity is after the recurrence. These RTUs maintain the equivalence to a dense linear RNN, because the recurrence itself remains linear. We refer to this definition of RTUs as Linear RTUs, because the recurrence is linear, even though we have the ability to represent nonlinear functions by allowing for any nonlinear activation after the recurrence. We also evaluated a different variation of RTUs where the non-linearity is added to the recurrence directly. These Nonlinear RTUs are written as: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{h}_{t}^{c_{1}}=\\mathbf{f}(\\mathbf{g}(\\nu^{\\mathrm{log}},\\theta^{\\mathrm{log}})\\odot\\mathbf{h}_{t-1}^{c_{1}}-\\phi(\\nu^{\\mathrm{log}},\\theta^{\\mathrm{log}})\\odot\\mathbf{h}_{t-1}^{c_{2}}+\\boldsymbol{\\gamma}\\odot\\mathbf{W}_{x}^{c_{1}}\\mathbf{x}_{t}),}\\\\ &{\\mathbf{h}_{t}^{c_{2}}=\\mathbf{f}(\\mathbf{g}(\\nu^{\\mathrm{log}},\\theta^{\\mathrm{log}})\\odot\\mathbf{h}_{t-1}^{c_{2}}+\\phi(\\nu^{\\mathrm{log}},\\theta^{\\mathrm{log}})\\odot\\mathbf{h}_{t-1}^{c_{1}}+\\boldsymbol{\\gamma}\\odot\\mathbf{W}_{x}^{c_{2}}\\mathbf{x}_{t}),}\\\\ &{\\mathbf{h}_{t}=[\\mathbf{h}_{t}^{c_{1}};\\mathbf{h}_{t}^{c_{2}}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Notice now $f$ \u2014a nonlinear activation like ReLU\u2014is used in the update to $\\mathbf{h}_{t}^{c_{1}}$ and $\\mathbf{h}_{t}^{c_{2}}$ , and the final $\\mathbf{h}_{t}$ simply stacks these two components. Nonlinear RTUs lose the equivalence to dense RNNs, though in our experiments, we find they perform as well or better than Linear RTUs. ", "page_idx": 4}, {"type": "text", "text": "3.3 The RTRL Update for RTUs ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "This section shows the RTRL updates for RTUs with more in-depth derivations in Appendix E. To keep notation simpler, we write the updates as if we are directly updating $r$ and $\\theta$ ; the updates for $\\pmb{\\nu}^{\\mathrm{log}}$ and $\\pmb{\\theta}^{\\mathrm{log}}$ are easily obtained then using the chain rule. The full derivation is in Appendix E.2. ", "page_idx": 4}, {"type": "text", "text": "Consider the partial derivative with respect to $r_{1}$ for the first RTU with input $\\bar{x}_{1}\\doteq(\\mathbf{W}_{x}^{c_{1}}\\mathbf{x}_{t})_{1}$ : ", "page_idx": 4}, {"type": "text", "text": "Then ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle h_{t,1}^{c_{1}}=r_{1}\\cos(\\theta_{1})h_{t-1,1}^{c_{1}}-r_{1}\\sin(\\theta_{1})h_{t-1,1}^{c_{2}}+\\sqrt{(1-r_{1}^{2})}\\bar{x}_{1}.}\\\\ {\\displaystyle\\frac{\\partial\\mathcal{L}_{t}}{\\partial r_{1}}=\\frac{\\partial\\mathcal{L}_{t}}{\\partial h_{t,1}^{c_{1}}}\\frac{\\partial h_{t,1}^{c_{1}}}{\\partial r_{1}}+\\frac{\\partial\\mathcal{L}_{t}}{\\partial h_{t,1}^{c_{2}}}\\frac{\\partial h_{t,1}^{c_{2}}}{\\partial r_{1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Since $r_{1}$ only impacts the two units in the first RTU, and derivatives w.r.t. the remaining hidden units are zero. Therefore, we just need to keep track of the vector of partial derivatives for these two components, $\\mathbf{e}_{t}^{r,c_{1}}\\doteq[\\frac{\\partial h_{t,1}^{c_{1}}}{\\partial r_{1}},\\dots,\\frac{\\partial h_{t,n}^{c_{1}}}{\\partial r_{n}}]$ and $\\mathbf{e}_{t}^{\\bar{r},c_{2}}\\doteq[\\frac{\\partial h_{t,1}^{c_{2}}}{\\partial r_{1}},\\dots,\\frac{\\partial h_{t,n}^{c_{2}}}{\\partial r_{n}}]$ with recursive formulas: $\\begin{array}{r}{\\mathbf{e}_{t}^{r,c_{1}}\\!=\\!\\cos(\\theta)\\odot\\mathbf{h}_{t-1}^{c_{1}}\\!+\\!\\mathbf{r}\\odot\\cos(\\theta)\\odot\\mathbf{e}_{t-1}^{r,c_{1}}-\\sin(\\theta)\\odot\\mathbf{h}_{t-1}^{c_{2}}-\\mathbf{r}\\odot\\sin(\\theta)\\odot\\mathbf{e}_{t-1}^{r,c_{2}}-\\frac{\\mathbf{r}}{\\sqrt{1-\\mathbf{r}^{2}}}\\odot\\mathbf{W}_{x}^{c_{1}}\\!\\le\\!\\mathbf{r}_{x}^{c_{2}},}\\\\ {\\mathbf{e}_{t}^{r,c_{2}}\\!=\\!\\cos(\\theta)\\odot\\mathbf{h}_{t-1}^{c_{2}}\\!+\\!\\mathbf{r}\\odot\\cos(\\theta)\\odot\\mathbf{e}_{t-1}^{r,c_{2}}+\\sin(\\theta)\\odot\\mathbf{h}_{t-1}^{c_{1}}+\\mathbf{r}\\odot\\sin(\\theta)\\odot\\mathbf{e}_{t-1}^{r,c_{1}}-\\frac{\\mathbf{r}}{\\sqrt{1-\\mathbf{r}^{2}}}\\odot\\mathbf{W}_{x}^{c_{2}}\\!>\\!\\mathbf{r}_{x}^{c_{2}}.}\\end{array}$ t We can similarly derive such traces for $\\theta$ . The update to $\\mathbf{r}$ involves first computing $\\frac{\\partial{\\mathcal{L}}_{t}}{\\partial h_{t}^{c_{1}}}$ , using backpropagation to compute gradients back from the output layer to the hidden layer; this step involves no gradients back-in-time. Then $\\mathbf{r}$ is updated using the gradient $\\begin{array}{r}{\\frac{\\partial\\mathcal{L}_{t}}{\\partial\\mathbf{h}_{t}^{c_{1}}}\\odot\\mathbf{e}_{t}^{r,c_{1}}+\\frac{\\partial\\mathcal{L}_{t}}{\\partial\\mathbf{h}_{t}^{c_{2}}}\\odot\\mathbf{e}_{t}^{r,c_{2}}}\\end{array}$ , which is linear in the size of $\\mathbf{r}\\in\\mathbb{R}^{n}$ , as the vectors ${\\bf e}_{t}^{r,c_{1}},{\\bf e}_{t}^{r,c_{2}}\\in\\mathbb{R}^{n}$ can be updated with linear computation in the above recursion. This update is the RTRL update, with no approximation. ", "page_idx": 4}, {"type": "text", "text": "3.4 Contrasting to LRUs ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "RTUs are similar to LRUs, with two small differences. First, RTUs have real-valued hidden units, because the cosine representation is used instead of the exponential representation. Second, RTUs use nonlinear activations in the recurrence, making them no longer linear. Though again a minor difference, we find that incorporating nonlinearity in the recurrence can be beneficial. RTUs can be seen as a small generalization of LRUs, moving away from strict linearity\u2014and thus motivating the name change\u2014but nonetheless a generalization we find performs notably better in practice. ", "page_idx": 4}, {"type": "text", "text": "Let us now motivate the utility of moving to a cosine representation and real-valued traces. LRUs parameterize each hidden unit with $\\lambda_{k}=r_{k}\\exp(i\\theta_{k})=\\exp(-\\exp(\\nu_{k}^{\\mathrm{log}})+i\\exp(\\theta_{k}^{\\mathrm{log}}))$ and directly work with complex numbers. Consequently, the hidden layer cannot be directly used to predict realvalues. It would be biased to take $\\mathbf{Re}(\\overline{{\\mathbf{h}}}_{t})$ (see Appendix D.2), and instead an additional weight matrix $\\overline{{\\mathbf{W}}}\\in\\mathbb{C}^{n\\times n}$ must be learned, to get $\\mathbf{Re}(\\overline{{\\mathbf{W}}}\\;\\overline{{\\mathbf{h}}}_{t})$ . To understand why this works, assume that we took the original $\\mathbf{h}_{t}$ from the dense NN, and handed it to an MLP. This would involve multiplying ${\\mathbf W}{\\mathbf h}_{t}$ for some W. If we set $\\overline{{\\mathbf{W}}}=\\mathbf{W}\\mathbf{P}$ , then $\\overline{{\\mathbf{W}}}\\;\\overline{{\\mathbf{h}}}_{t}=\\mathbf{W}\\mathbf{P}\\mathbf{P}^{-1}\\mathbf{h}_{t}=\\mathbf{W}\\mathbf{h}_{t}$ and we did not introduce any bias. In fact, if $\\overline{{\\mathbf{W}}}$ is set this way, we do not need to take the real-valued part, because the output of $\\overline{{\\mathbf{Wh}}}_{t}$ is real-valued. Of course, learning does not force this equivalence\u2014in fact this parameterization is more flexible than the original\u2014and so it is necessary to take the real-part. ", "page_idx": 4}, {"type": "text", "text": "RTUs avoid some of these complications by explicitly writing the recurrence and updates with realvalued hidden states. Implicitly, the relationship between the two real-valued hidden vectors forces them to behave like complex numbers (as rotations), but all equations and learning stay in real-valued space. RTUs consequently avoid the need to post multiply by the matrix, removing a small number of learnable parameters, allowing the use of a nonlinear activation directly on the output, and allowing the hidden state to be immediately passed to a downstream MLP. We acknowledge that others may argue that working directly with complex numbers is preferable. The preference for real-valued hidden layers may simply be our own limitations, but we suspect much of the reinforcement learning community is similarly more comfortable to work in real-valued space. We found small choices in our implementation for LRU did not always behave as expected, partially due to how auto-differentiation is implemented in packages for complex numbers3.In the end, our goal is to make these simple recurrent traces easy to use, and providing updates with real numbers may remove some barriers. ", "page_idx": 5}, {"type": "text", "text": "4 Online Prediction Learning ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we explore different architectural variants of RTUs and LRUs in a online prediction task and then move on to study the tradeoffs between computational resources and performance when using RTUs with RTRL compared to GRUs and LRUs with T-BPTT. ", "page_idx": 5}, {"type": "text", "text": "4.1 Ablation Study on Architectural Choices for RTUs and LRUs ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this first experiment, we investigate the impact of several architectural choices on the performance of RTUs and LRUs varying where nonlinearity is applied. We use a simple multi-step prediction task called Trace conditioning [39] inspired by experiments in animal learning. The agent\u2019s objective is to predict a signal\u2014called the Unconditional Stimulus (US)\u2014conditioned on an earlier signal\u2014the Conditional Stimulus (CS). The prediction is formulated as a return, Gt =.  k\u221e=0 \u03b3k USt+k+1, where the agent\u2019s goal is to estimate the value function for this return. More detai ls on this environment and experimental settings are in Appendix F. Figure 1 summarizes the results. ", "page_idx": 5}, {"type": "image", "img_path": "4UvMOnZMam/tmp/b210d2bcad818c0afe408b66a8df0a3fb6f9cdeffa0ba95c6825d6d876a88b9b.jpg", "img_caption": ["Figure 1: Ablation over different architectural choices for RTUs and LRUs. The RTU variants are blue, and the LRU variants are orange. In each subplot, we restrict both architectures in a particular way, reporting prediction error (MSRE) as a function of hidden state size. Across variations, RTUs are often better and, at worst, tie LRU. Here, both architectures were using RTRL. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "4.2 Learning under resources constraints ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we investigate the tradeoffs between computational resources and performance when using RTUs with RTRL compared to GRUs and LRUs with T-BPTT. ", "page_idx": 5}, {"type": "text", "text": "In the following experiments, all agents consist of a recurrent layer followed by a linear layer generating the prediction. We measure performance of the agents online by calculating the Mean Square Return Errors (MSRE), which is the mean squared error between the agent\u2019s prediction at time $t$ and $G_{t}$ . In all the experiments, we used the Adam optimizer. We first ran each agent with different step sizes for 5 runs and 2 million steps. We then averaged the MSRE over the 2 million steps and selected each agent\u2019s best step size value. Finally, we ran the agents with the best step size value for 10 runs, which we report here. We also report all agents\u2019 step size sensitivity curves in Appendix F.3. ", "page_idx": 5}, {"type": "image", "img_path": "4UvMOnZMam/tmp/3af3fdb54f8fd0b1bbf0fcf65c472e88fadef29166095b9589e14ba7f054c8a9.jpg", "img_caption": ["Figure 2: Learning under resources constraints in Trace Conditioning. Each of the four subplots shows how each algorithm\u2019s performance varies as a function of resources. (a) LRU and GRU with T-TBTT is not competitive with RTUs even as $T$ is increased while restricting the number of hidden units in LRU and GRU so that all algorithms use about the same computation per step. (b) If we allow GRU and LRU\u2019s computation to increase (fixed network size) while increasing $T$ , the performance gap remains. (c) Fixing $T$ to a large value to solve the task, we can increase the number of parameters, holding the computation equal for all methods. (d) If we do not require compute to be equal across methods as we scale parameters, then the LRU can eventually match the error of RTU, but GRU cannot. The black dashed line represents the near perfect prediction performance. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Learning under computational constraints: We first investigate: how well do different agents exploit the available computational resources? We specified a fixed computational budget of 15000 FLOPs. Since RTUs are learning using RTRL and have a linear computational complexity, the computational budget only determines the number of hidden units in the architecture. For GRU and LRU, both the truncation length and the hidden dimension contribute to the budget. We tested several configurations of truncation lengths and parameters such that the overall computations fit the computational budget. Figure 2.a shows the results of this experiment. As we move along the horizontal axis, the number of parameters for GRU and LRU decreases as $T$ increases to fit the computational constraints. However, the RTU agents do not depend on $T$ , so their performance and computation is constant. ", "page_idx": 6}, {"type": "text", "text": "Scaling with computation: The computational complexity of T-BPTT depends on the truncation length and the number of parameters in the neural network. Thus, the agent can use the additional resources in two ways: (1) Increasing the truncation length, and (2) Increasing the number of parameters. On the other hand, RTUs use all the computations to have more parameters. ", "page_idx": 6}, {"type": "text", "text": "Now, we move to our second question: how well do different methods scale with increasing the computational budget? We answer this question in two stages: Firstly, we study T-BPTT with increasing $T$ and a fixed number of parameters. For RTU, the computation increases by adding more parameters such that all corresponding points from GRU and RTU use the same amount of computation. Secondly, we fixed the truncation length for GRU to 45, which is more than the maximum distance between the CS and the US, and increased the computation by increasing the number of parameters for GRU. Again, for RTU, we increased the computation by increasing the number of parameters. ", "page_idx": 6}, {"type": "text", "text": "Figure of 2.b shows the first experiment\u2019s results. While GRU\u2019s performance improved as the truncation length increased, RTU outperformed GRU across all different computational budgets. Figure 2.c shows the results of the second experiment. The RTU agent\u2019s performance consistently improves as we increase the computation available. However, the performance improvement for the GRU agent is inconsistent. The inconsistency of GRU performance could be associated with the trade-off between the truncation length and the number of parameters. ", "page_idx": 6}, {"type": "text", "text": "Scaling With Parameters: Finally, we study the performance of RTU and GRU when given the same number of parameters and allow the GRU agent to use more computation. We fixed the truncation length for GRU to 45 as before and used the same number of parameters for both agents. Figure 2.d shows the results of this experiment. For RTU, we see the same consistent performance improvement as we increase the number of parameters. For GRU, the performance improvement is also consistent, though it degrades slightly towards the end. The RTU agent outperforms the GRU agent even though the GRU uses more computation. ", "page_idx": 6}, {"type": "text", "text": "We provide additional experiments comparing RTUs to two other approaches that use RTRL: online LRUs and a real-valued diagonal RNN in Appendix F.1. ", "page_idx": 6}, {"type": "text", "text": "5 Real-Time Recurrent Policy Gradient ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "This section first highlights some differences in using linear RTRL methods, i.e., RTRL with linear complexity, in incremental and batch settings. We then investigate different ways of integrating linear RTRL methods with policy gradient approaches, and we use PPO as a case study for this investigation. Finally, we compare the performance of RTRL methods with T-BPTT methods and other baselines. ", "page_idx": 7}, {"type": "text", "text": "5.1 Linear RTRL Methods in Incremental and Batch Settings ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The benefits of linear RTRL methods over T-BPTT are more evident in the incremental rather than the batch setting. In the incremental learning setting, where the agent updates its parameters after each interaction step, linear RTRL methods have a constant computational complexity per update that depends only on the number of parameters. In contrast, T-BPTT methods have a complexity proportional to the truncation length T since T-BPTT methods require storing a sequence of past T activations to perform one gradient update. Figure 3 shows the time it takes to make one update with linear RTRL and T-BPTT given the same number of parameters. For T-BPTT, the time to make one update scales with the truncation length T, while for linear RTRL, it is constant. ", "page_idx": 7}, {"type": "image", "img_path": "4UvMOnZMam/tmp/1e08cc3cfeee4d5c8fda2c45d0a76349bfc82da285ac37cb46081228faa76c3c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 3: Contrasting runtime in incremental and batch settings. In the incremental setting, evaluated in the animal-learning prediction task, T-BPTT updates scale with truncation length, whereas linear RTRL is constant. With batch updates, evaluated in Ant-P with PPO, linear RTRL remains linear and T-BPTT is slightly more efficient. ", "page_idx": 7}, {"type": "text", "text": "The computational analysis for the batch setting is different than the incremental setting. In the batch setting, linear RTRL still have a constant cost per update and provide an untruncated yet stale gradient for all the samples. When using T-BPTT in the batch setting, there are two possibilities for the gradient updates. The first option, the typical strategy, is to divide the batch into non-overlapping sequences, each of length T, and perform T-BPTT on each sequence. In this case, the cost of one gradient update per sequence is a function of T, resulting in an effective update cost per sample independent of T. As a result, T-BPTT is computationally efficient in this case, albeit at the expense of a worse gradient estimate; in each sequence, only the last sample has a gradient estimate with T steps [25]. Figure 3 shows the time it takes to make one batch update with linear RTRL and T-BPTT given the same number of parameters. In this case, both methods use similar time per update. The second option is to divide the batch into overlapping sequences, where each gradient uses a sequence of T steps [25]. This approach increases the cost of updates per sample to be proportional to T, as in the incremental setting, with the benefit of better gradient estimates. However, all standard implementations of RL methods with T-BPTT use the computationally efficient option [38, 14, 24]. ", "page_idx": 7}, {"type": "text", "text": "Integrating Linear RTRL Methods with PPO When performing batch updates, as with PPO, the RTRL gradients used to update the recurrent parameters will be stale, as they were calculated during the interaction with the environment w.r.t old policy and value parameters. One solution to mitigate the gradient staleness is to go through the whole trajectory after each epoch update and re-compute the gradient traces. However, this can be computationally expensive. In Appendix G, Algorithm 1, we provide the pseudocode for integrating RTRL methods with PPO with optional steps for re-running the network to update the RTRL gradient traces, the value targets, and the advantage estimates. We also performed an ablation study to investigate the effect of the gradient staleness in RTRL when combined with PPO, Appendix G. The results from the ablation study show that using a stale gradient results in better performance with RTUs and suggests that the staleness might help PPO maintain the trust region. ", "page_idx": 7}, {"type": "text", "text": "6 Experiments in Memory-Based Control ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we evaluate the memory capabilities of RTUs when solving challenging RL control problems. We divide the problems in this section based on the source of partial observability. 1) Missing sensory data, where we mask out parts of the agent\u2019s observation. The agent must accumulate and integrate the sensory observations over time to account for the missing information. 2) Remembering important cues, where the agent must remember an essential cue about the environment that happened many steps in advance. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Integrating Sensory Observations: We use the standard Mujoco POMDP benchmark widely used in prior work for evaluating memory-based RL agents [31, 12, 27, 32]. The benchmark consists of several challenging tasks where the agent controls a multi-joint dynamical body while only observing the joints\u2019 positional (Mujoco-P) or velocity information (Mujoco-V). To increase experiment throughput, we use the Jax implementation of Mujoco from the Brax library [7] and implemented wrappers to mask either the velocity (Mujoco-P) or positional information (Mujoco-V). ", "page_idx": 8}, {"type": "text", "text": "We evaluated our Linear and Non-linear RTUs against GRU, LRU, and Online LRU. All agents use PPO [40] as the control algorithm, and the difference between the agents is the recurrent component. For all agents, we fixed the number of parameters for the recurrent part to be $\\sim24\\mathrm{{k}}$ . We tuned the learning rate for all agents in all environments and selected the best learning rate for each agent per environment. We also included a GPT2-transformer baseline. We followed the implementation details in previous work [32], and used a GPT2 variant with 200k parameters. We provide the results for GPT2 in Appendix H. ", "page_idx": 8}, {"type": "image", "img_path": "4UvMOnZMam/tmp/f5ec8df82e29a358d4457a95db9810062650a2de07cca27029db7dafba3dc92d.jpg", "img_caption": ["Figure 4: Learning curves on the Mujoco POMDP benchmark. Environments with $-\\mathrm{P}$ mean that velocity components are occluded from the observations, while -V means that the positions and angles are occluded. All architectures have the same number of recurrent parameters ( 24k parameter). For each architecture, we show the performance of its best-tuned variant. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "When given the same number of parameters, RTU agents outperform other baselines in all environments in Figure 4. Furthermore, we show in Appendix H that even when increasing the truncation length of both GRU and LRU agents to use significantly longer history, they outperform RTUs in only one task. Of particular note is again that RTUs outperform online LRUs, highlighting again that our simple modifications have a large impact on performance in this online RL setting. ", "page_idx": 8}, {"type": "text", "text": "Remembering Important Cues: ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Next, we test the agents\u2019 ability to remember essential environmental cues. We use several tasks from the POPGym benchmark [29] in addition to the Reacher POMDP task, a modified version of Mujoco Reacher where the agent observes the target position only at the beginning of the episode. ", "page_idx": 8}, {"type": "image", "img_path": "4UvMOnZMam/tmp/85022226ebd0c9fa1b8ec9ecd42fe48b90690071d13aa5c6edbe78ea25103f26.jpg", "img_caption": ["Figure 5: Reacher, 30 runs with standard errors. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "The POPGym tasks we consider along with the Reacher POMDP are all long-term memory tasks [32] as the agent must remember and carry the information for a long time. Figure 5 summarizes the results for the reacher POMDP task and the POPGym results can be found in Figure 6. In both cases, we can see that RTUs outperform the other approaches. Non-linear RTUs achieve a better performance than linear RTUs in reacher POMDP, and both achieve a better performance in all tasks than online LRUs. In ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Reacher POMDP, GRU was able to achieve a similar performance to that of linear RTUs. ", "page_idx": 8}, {"type": "image", "img_path": "4UvMOnZMam/tmp/cfac45d21afb9538cc7e98670f27c10c9518a5edd90609573edc512f16f61068.jpg", "img_caption": ["Figure 6: Results across several tasks from the POPGym benchmark. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "7 Conclusion and Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we investigated using complex-valued diagonal RNNs for online RL. We built on LRUs, to provide a small modification (RTUs) that we found performed significantly better in online RL across various partially observable prediction and control settings. We also found RTUs performed better than the more computationally intensive GRUs. Overall, RTUs are a promising, lightweight approach to online learning in partially observable RL environments. ", "page_idx": 9}, {"type": "text", "text": "A primary limitation of RTUs is the extension to multilayer recurrence. This limitation is inherent to all RTRL approaches; with multilayers, we need to save the gradient traces of the hidden state w.r.t the weights from all the preceding layers [17]. Previous work [17, 48] showed that using stop gradient operations between the layers and not tracing the time dependencies across layers is a viable choice. However, we need a more principled approach for tracing the gradient across layers. ", "page_idx": 9}, {"type": "text", "text": "One advantage of the linearity restriction in LRUs is that it allows the use of parallel scans for training [26]. However, recent works have shown the possibility of employing parallel scans to non-linear RNNs [8, 22]. A future direction is to investigate the use of parallel scans for training RTUs. ", "page_idx": 9}, {"type": "text", "text": "8 Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We would like to thank Nicolas Zucchet for advice about the online LRU implementation, and Subhojeet Pramanik for many discussions on transformers and RNNs. We would like to thank NSERC, CIFAR and Amii for research funding and the Digital Research Alliance of Canada for the computational resources. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Firas Al-Hafez, Guoping Zhao, Jan Peters, and Davide Tateo. Time-efficient reinforcement learning with stochastic stateful policies. In The Twelfth International Conference on Learning Representations.   \n[2] Atilim Gunes Baydin, Barak A Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. Automatic differentiation in machine learning: a survey. Journal of machine learning research, 2018.   \n[3] James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python $^+$ NumPy programs, 2018.   \n[4] Kyunghyun Cho, Bart Van Merri\u00ebnboer, Dzmitry Bahdanau, and Yoshua Bengio. On the properties of neural machine translation: Encoder-decoder approaches. arXiv preprint arXiv:1409.1259, 2014.   \n[5] Tim Cooijmans and James Martens. On the variance of unbiased online recurrent optimization. arXiv preprint arXiv:1902.02405, 2019.   \n[6] Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Vlad Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, et al. Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures. In International conference on machine learning, 2018.   \n[7] C. Daniel Freeman, Erik Frey, Anton Raichuk, Sertan Girgin, Igor Mordatch, and Olivier Bachem. Brax - a differentiable physics engine for large scale rigid body simulation, 2021.   \n[8] Xavier Gonzalez, Andrew Warrington, Jimmy TH Smith, and Scott W Linderman. Towards scalable and stable parallelization of nonlinear rnns. arXiv e-prints, pages arXiv\u20132407, 2024.   \n[9] Gori, Bengio, and De Mori. Bps: a learning algorithm for capturing the dynamic nature of speech. In International Joint Conference on Neural Networks, 1989.   \n[10] Audrunas Gruslys, Will Dabney, Mohammad Gheshlaghi Azar, Bilal Piot, Marc Bellemare, and Remi Munos. The reactor: A fast and sample-efficient actor-critic agent for reinforcement learning. In International Conference on Learning Representations, 2018.   \n[11] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains through world models. arXiv preprint arXiv:2301.04104, 2023.   \n[12] Dongqi Han, Kenji Doya, and Jun Tani. Variational recurrent models for solving partially observable control tasks. In International Conference on Learning Representations, 2019.   \n[13] Matthew Hausknecht and Peter Stone. Deep recurrent q-learning for partially observable mdps. In 2015 aaai fall symposium series, 2015.   \n[14] Shengyi Huang, Rousslan Fernand Julien Dossa, Chang Ye, Jeff Braga, Dipam Chakraborty, Kinal Mehta, and Jo\u00c3G, o GM Ara\u00c3\u0161jo. Cleanrl: High-quality single-flie implementations of deep reinforcement learning algorithms. Journal of Machine Learning Research, 2022.   \n[15] Kazuki Irie, Imanol Schlag, R\u00f3bert Csord\u00e1s, and J\u00fcrgen Schmidhuber. Going beyond linear transformers with recurrent fast weight programmers. Advances in neural information processing systems, 2021.   \n[16] Kazuki Irie, Imanol Schlag, R\u00f3bert Csord\u00e1s, and J\u00fcrgen Schmidhuber. A modern self-referential weight matrix that learns to modify itself. In International Conference on Machine Learning, 2022.   \n[17] Kazuki Irie, Anand Gopalakrishnan, and J\u00fcrgen Schmidhuber. Exploring the promise and limits of real-time recurrent learning, 2023.   \n[18] Khurram Javed, Haseeb Shah, Rich Sutton, and Martha White. Online real-time recurrent learning using sparse connections and selective learning. Journal of Machine Learning Research, 2023.   \n[19] Steven Kapturowski, Georg Ostrovski, Will Dabney, John Quan, and Remi Munos. Recurrent experience replay in distributed reinforcement learning. In International Conference on Learning Representations, 2019.   \n[20] Shuai Li, Wanqing Li, Chris Cook, Ce Zhu, and Yanbo Gao. Independently Recurrent Neural Network (IndRNN): Building A Longer and Deeper RNN. In Conference on Computer Vision and Pattern Recognition, 2018.   \n[21] Xiujun Li, Lihong Li, Jianfeng Gao, Xiaodong He, Jianshu Chen, Li Deng, and Ji He. Recurrent reinforcement learning: a hybrid approach. arXiv preprint arXiv:1509.03044, 2015.   \n[22] Yi Heng Lim, Qi Zhu, Joshua Selfridge, and Muhammad Firmansyah Kasim. Parallelizing non-linear sequential models over the sequence length. In The Twelfth International Conference on Learning Representations, 2023.   \n[23] Chenhao Lu, Ruizhe Shi, Yuyao Liu, Kaizhe Hu, Simon Shaolei Du, and Huazhe Xu. Rethinking transformers in solving pomdps. In Forty-first International Conference on Machine Learning.   \n[24] Chris Lu, Jakub Kuba, Alistair Letcher, Luke Metz, Christian Schroeder de Witt, and Jakob Foerster. Discovered policy optimisation. Advances in Neural Information Processing Systems, 2022.   \n[25] Owen Marschall, Kyunghyun Cho, and Cristina Savin. A unified framework of online learning algorithms for training recurrent neural networks. Journal of Machine Learning Research, 2020.   \n[26] Eric Martin and Chris Cundy. Parallelizing linear recurrent neural nets over sequence length. In International Conference on Learning Representations, 2018.   \n[27] Lingheng Meng, Rob Gorbet, and Dana Kulic\u00b4. Memory-based deep reinforcement learning for pomdps. In International Conference on Intelligent Robots and Systems (IROS), 2021.   \n[28] Jacob Menick, Erich Elsen, Utku Evci, Simon Osindero, Karen Simonyan, and Alex Graves. Practical real time recurrent learning with a sparse approximation. In International Conference on Learning Representations, 2021.   \n[29] Steven Morad, Ryan Kortvelesy, Matteo Bettini, Stephan Liwicki, and Amanda Prorok. POPGym: Benchmarking partially observable reinforcement learning. In International Conference on Learning Representations, 2023.   \n[30] Michael Mozer. A focused backpropagation algorithm for temporal pattern recognition. Complex Systems, 1989.   \n[31] Tianwei Ni, Benjamin Eysenbach, and Ruslan Salakhutdinov. Recurrent model-free rl can be a strong baseline for many pomdps. In International Conference on Machine Learning, 2022.   \n[32] Tianwei Ni, Michel Ma, Benjamin Eysenbach, and Pierre-Luc Bacon. When do transformers shine in rl? decoupling memory from credit assignment. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[33] Yann Ollivier, Corentin Tallec, and Guillaume Charpiat. Training recurrent networks online without backtracking, 2015.   \n[34] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences, 2023.   \n[35] Emilio Parisotto, Francis Song, Jack Rae, Razvan Pascanu, Caglar Gulcehre, Siddhant Jayakumar, Max Jaderberg, Raphael Lopez Kaufman, Aidan Clark, Seb Noury, et al. Stabilizing transformers for reinforcement learning. In International conference on machine learning, 2020.   \n[36] P Ivan Pavlov. Conditioned reflexes: an investigation of the physiological activity of the cerebral cortex, 1927.   \n[37] Subhojeet Pramanik, Esraa Elelimy, Marlos C Machado, and Adam White. Recurrent linear transformers. arXiv preprint arXiv:2310.15719, 2023.   \n[38] Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah Dormann. Stable-baselines3: Reliable reinforcement learning implementations. Journal of Machine Learning Research, 2021.   \n[39] Banafsheh Rafiee, Zaheer Abbas, Sina Ghiassian, Raksha Kumaraswamy, Richard S Sutton, Elliot A Ludvig, and Adam White. From eye-blinks to state construction: Diagnostic benchmarks for online representation learning. Adaptive Behavior, 2020.   \n[40] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017.   \n[41] Sutton. Toward a new approach to model-based reinforcement learning, 2020.   \n[42] Sutton. Markov and agent state, 2020.   \n[43] Corentin Tallec and Yann Ollivier. Unbiased online recurrent optimization, 2017.   \n[44] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 2017.   \n[45] Williams and Peng. An efficient gradient-based algorithm for on-line training of recurrent network trajectories. Neural computation, 1990.   \n[46] Williams and Zipser. A learning algorithm for continually running fully recurrent neural networks. Neural computation, 1989.   \n[47] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers effective for time series forecasting?, 2022.   \n[48] Nicolas Zucchet, Robert Meier, Simon Schug, Asier Mujika, and Jo\u00e3o Sacramento. Online learning of long range dependencies. In Advances in Neural Information Processing Systems, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Background on BackPropagation Through Time and Real-Time Recurrent Learning ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "This section provides a brief background on BackPropagation Through Time (BPTT) and Real-Time Recurrent Learning (RTRL) algorithms. ", "page_idx": 12}, {"type": "text", "text": "A.1 BackPropagation Through Time ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "BPTT calculates the gradient, $\\nabla_{\\psi}\\mathcal{L}$ , by unfolding the recurrent dynamics through time and incorporating the impact of the parameters on the loss from all observed time steps. Formally, we can write $\\nabla_{\\psi}\\mathcal{L}$ as: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\nabla_{\\psi}\\mathcal{L}=\\frac{1}{t}\\sum_{i=0}^{t-1}\\nabla_{\\psi}\\mathcal{L}_{i}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Applying the chain rule, we re-write Eq.5 as: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\nabla_{\\psi}\\mathcal{L}=\\frac{1}{t}\\sum_{i=0}^{t-1}\\nabla_{\\psi}\\mathcal{L}_{i}}}\\\\ {{\\displaystyle=\\frac{1}{t}\\sum_{i=0}^{t-1}\\frac{\\partial\\mathcal{L}_{i}}{\\partial{\\bf h}_{i}}\\frac{\\partial{\\bf h}_{i}}{\\partial\\psi}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "When calculating $\\frac{\\partial\\mathbf{h}_{i}}{\\partial\\pmb{\\psi}}$ , we need to consider the effect of $\\psi$ from all the time steps. To illustrate this effect, consider unrolling the last 2 steps of the RNN dynamics: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{h}_{t}=\\mathbf{f}(\\mathbf{h}_{t-1},\\mathbf{x}_{t},\\psi)}\\\\ &{\\quad\\mathrm{Re-write~}\\mathbf{h}_{t-1}\\mathrm{~as~}\\mathbf{f}(\\mathbf{h}_{t-2},\\mathbf{x}_{t-1},\\psi)}\\\\ &{\\quad=\\mathbf{f}(\\mathbf{f}(\\mathbf{h}_{t-2},\\mathbf{x}_{t-1},\\psi),\\mathbf{x}_{t},\\psi)}\\\\ &{\\quad\\mathrm{Re-write~}\\mathbf{h}_{t-2}\\mathrm{~as~}\\mathbf{f}(\\mathbf{h}_{t-3},\\mathbf{x}_{t-2},\\psi)}\\\\ &{\\quad=\\mathbf{f}(\\mathbf{f}(\\mathbf{f}(\\mathbf{h}_{t-3},\\mathbf{x}_{t-2},\\psi),\\mathbf{x}_{t-1},\\psi),\\mathbf{x}_{t},\\psi).}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Equation 7 shows that the network parameters $\\pmb{\\psi}$ affect the construction of the recurrent state $\\mathbf{h}_{t}$ through two pathways: a direct pathway, i.e., using $\\psi$ to evaluate f $\\mathbf{:}(\\mathbf{h}_{t-1},\\mathbf{x}_{t},\\psi)$ , and an implicit pathway, i.e., $\\pmb{\\psi}$ affected constructing all previous recurrent states, $\\mathbf h_{t-1},\\ldots,\\mathbf h_{1}$ , and all those recurrent states affected $\\mathbf{h}_{t}$ construction. Thus, to calculate $\\frac{\\partial\\mathbf{h}_{t}}{\\partial\\psi}$ , we need to consider those two pathways: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\frac{\\partial\\mathbf{h}_{t}}{\\partial\\psi}=\\frac{\\partial\\mathbf{f}(\\mathbf{h}_{t-1},\\mathbf{x}_{t},\\psi)}{\\partial\\psi}+\\frac{\\partial\\mathbf{f}(\\mathbf{h}_{t-1},\\mathbf{x}_{t},\\psi)}{\\partial\\mathbf{h}_{t-1}}\\frac{\\partial\\mathbf{h}_{t-1}}{\\partial\\psi}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Once again, we need to consider the two pathways when evaluating \u2202h\u2202t\u03c8\u03c8\u03c8\u22121 in 8. For simplicity, let $\\begin{array}{r}{\\mathbf{J}_{t}\\doteq\\frac{\\partial\\mathbf{h}_{t}}{\\partial\\psi}}\\end{array}$ , $\\begin{array}{r}{\\mathbf{B}_{t}=\\frac{\\partial\\mathbf{f}\\left(\\mathbf{h}_{t-1},\\mathbf{x}_{t},\\psi\\right)}{\\partial\\psi}}\\end{array}$ , $\\begin{array}{r}{\\mathbf{C}_{t}=\\frac{\\partial\\mathbf{f}\\left(\\mathbf{h}_{t-1},\\mathbf{x}_{t},\\psi\\right)}{\\partial\\mathbf{h}_{t-1}}}\\end{array}$ , and re-write 8: ", "page_idx": 12}, {"type": "text", "text": "$\\begin{array}{r l}&{\\mathbb{J}_{t}=\\mathbf{B}_{t}+\\mathbf{C}_{t}\\mathbf{J}_{t-1}}\\\\ &{\\quad=\\mathbf{B}_{t}+\\mathbf{C}_{t}\\left(\\mathbf{B}_{t-1}+\\mathbf{C}_{t-1}\\mathbf{J}_{t-2}\\right)\\qquad\\mathrm{Unrolling~}\\mathbf{J}_{t-1}}\\\\ &{\\quad=\\mathbf{B}_{t}+\\mathbf{C}_{t}\\mathbf{B}_{t-1}+\\mathbf{C}_{t}\\mathbf{C}_{t-1}\\mathbf{J}_{t-2}}\\\\ &{\\quad=\\mathbf{B}_{t}+\\mathbf{C}_{t}\\mathbf{B}_{t-1}+\\mathbf{C}_{t}\\mathbf{C}_{t-1}\\left(\\mathbf{B}_{t-2}+\\mathbf{C}_{t-2}\\mathbf{J}_{t-3}\\right)\\qquad\\mathrm{Unrolling~}\\mathbf{J}_{t-2}}\\\\ &{\\quad=\\mathbf{B}_{t}+\\mathbf{C}_{t}\\mathbf{B}_{t-1}+\\mathbf{C}_{t}\\mathbf{C}_{t-1}\\mathbf{B}_{t-2}+\\mathbf{C}_{t}\\mathbf{C}_{t-1}\\mathbf{C}_{t-2}\\mathbf{J}_{t-3}}\\\\ &{\\quad=\\mathbf{B}_{t}+\\mathbf{C}_{t}\\mathbf{B}_{t-1}+\\mathbf{C}_{t}\\mathbf{C}_{t-1}\\mathbf{B}_{t-2}+\\cdots+\\mathbf{C}_{t}\\mathbf{C}_{t-1}\\mathbf{C}_{t-2}\\cdot\\mathbf{\\Lambda}_{\\cdot}\\mathbf{C}_{2}\\mathbf{B}_{1}+\\mathbf{C}_{t}\\mathbf{C}_{t-1}\\mathbf{C}_{t-2}\\cdot\\ldots\\mathbf{C}_{1}\\mathbf{J}_{0}}\\\\ &{\\quad=\\sum_{k=1}^{t}\\left(\\prod_{i=k+1}^{t}\\mathbf{C}_{i}\\right)\\mathbf{B}_{k}+\\left(\\prod_{i=1}^{t}\\mathbf{C}_{i}\\right)\\mathbf{J}_{0}.}\\end{array}$ Keep unrolling ", "page_idx": 12}, {"type": "text", "text": "Writing \u2202\u2202h\u03c8\u03c8\u03c8t using the results from 9: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\partial{\\bf h}_{t}}{\\partial\\psi}=\\frac{\\partial{\\bf f}({\\bf h}_{t-1},{\\bf x}_{t},\\psi)}{\\partial\\psi}+\\frac{\\partial{\\bf f}({\\bf h}_{t-1},{\\bf x}_{t},\\psi)}{\\partial{\\bf h}_{t-1}}\\frac{\\partial{\\bf h}_{t-1}}{\\partial\\psi}}\\\\ {\\displaystyle\\qquad=\\sum_{k=1}^{t}\\left(\\prod_{i=k+1}^{t}\\frac{\\partial{\\bf f}({\\bf h}_{i-1},{\\bf x}_{i},\\psi)}{\\partial{\\bf h}_{i-1}}\\right)\\frac{\\partial{\\bf f}({\\bf h}_{k-1},{\\bf x}_{k},\\psi)}{\\partial\\psi}+\\left(\\prod_{i=1}^{t}\\frac{\\partial{\\bf f}({\\bf h}_{i-1},{\\bf x}_{i},\\psi)}{\\partial{\\bf h}_{i-1}}\\right)\\frac{\\partial{\\bf h}_{0}}{\\partial\\psi}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "According to Eq. 10, the agent needs to store all the previous inputs to calculate $\\frac{\\partial\\mathbf{h}_{t}}{\\partial\\psi}$ which is impractical; the computation and memory complexity will be increasing with $t$ . ", "page_idx": 13}, {"type": "text", "text": "A.1.1 Truncated-BackPropagation Through Time ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Williams and Peng (1990) introduced Truncated-BackPropagation Through Time (T-BPTT) which solves the issue of increasing memory and computational complexities of BPTT. In T-BPTT, we specify a truncation length $T$ , which controls the number of steps considered when calculating the gradient in 10. We now write the truncated version of 9 which takes into consideration the gradient from the last $T$ steps only: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbf{J}_{t}=\\sum_{k=t-T}^{t}\\left(\\prod_{i=k+1}^{t}\\mathbf{C}_{i}\\right)\\mathbf{B}_{k}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Using results from 11, we then write the approximated gradient of the loss w.r.t the learnable parameters: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{7_{\\psi}\\mathcal{L}=\\displaystyle\\sum_{j=t-T}^{t}\\frac{\\partial\\mathcal{L}_{j}}{\\partial\\mathbf{h}_{j}}\\frac{\\partial\\mathbf{h}_{j}}{\\partial\\psi}}\\\\ &{\\qquad=\\displaystyle\\sum_{j=t-T}^{t}\\frac{\\partial\\mathcal{L}_{j}}{\\partial\\mathbf{h}_{j}}\\sum_{k=j-T}^{j}\\left(\\prod_{i=k+1}^{t}\\frac{\\partial\\mathbf{f}(\\mathbf{h}_{i-1},\\mathbf{x}_{i},\\psi)}{\\partial\\mathbf{h}_{i-1}}\\right)\\frac{\\partial\\mathbf{f}(\\mathbf{h}_{k-1},\\mathbf{x}_{k},\\psi)}{\\partial\\psi}+\\left(\\prod_{i=1}^{t}\\frac{\\partial\\mathbf{f}(\\mathbf{h}_{i-1},\\mathbf{x}_{i},\\psi)}{\\partial\\mathbf{h}_{i-1}}\\right)\\frac{\\partial\\mathbf{h}_{i}}{\\partial\\psi}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "A.2 Real-Time Recurrent Learning ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Williams and Zipser (1989) introduced the Real-time Recurrent Learning algorithm (RTRL) as a learning algorithm for continual recurrent learning. RTRL employs the recurrent formulation of the gradient in 8; instead of unrolling \u2202h\u2202t\u03c8\u03c8\u03c8\u2212 1 further back in time, RTRL saves its calculated value from the previous time step and use it later when needed. It is worth emphasizing that after the agent updates its parameters, the gradient information saved from previous time steps would be stale, i.e., calculated w.r.t old parameters, however, under the assumption of small learning rates, RTRL is known to converge. The gradient formulation of RTRL can be written as: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\psi}\\mathcal{L}=\\displaystyle\\sum_{i=0}^{t}\\frac{\\partial\\mathcal{L}_{i}}{\\partial\\mathbf{h}_{i}}\\frac{\\partial\\mathbf{h}_{i}}{\\partial\\psi}}\\\\ &{\\qquad=\\displaystyle\\sum_{i=0}^{t}\\frac{\\partial\\mathcal{L}_{i}}{\\partial\\mathbf{h}_{i}}}\\\\ &{\\qquad\\left(\\frac{\\partial\\mathbf{f}\\left(\\mathbf{h}_{i-1},\\mathbf{x}_{i},\\psi\\right)}{\\partial\\psi}+\\frac{\\partial\\mathbf{f}\\left(\\mathbf{h}_{i-1},\\mathbf{x}_{i},\\psi\\right)}{\\partial\\mathbf{h}_{i-1}}\\frac{\\partial\\mathbf{h}_{i-1}}{\\partial\\psi}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "B More Details on Representability with Complex-valued Diagonal Recurrence ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "This section explains why we need complex-valued diagonals to represent dense recurrent layers. We first show when it is equivalent to use complex-valued diagonal and a dense recurrent layer. We highlight that using a real-valued diagonal is like restricting the weights to be symmetric\u2014because the (complex) diagonal corresponds to the eigenvalues of the weight matrix\u2014which can severely limit representability. We provide a small experiment to show that complex eigenvalues naturally arise when training both a dense linear and nonlinear RNN, further motivating the utility of moving towards complex-valued diagonals. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "B.1 Representability with Complex-valued Diagonals ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Let us first consider when we can perfectly represent a dense, linear recurrent layer with a complexvalued diagonal recurrent layer. Assume we have the recurrence relationship, with learnable parameters $\\mathbf{W}_{h}\\in\\mathbb{R}^{n\\times n}$ and $\\mathbf{W}_{x}\\in\\mathbb{R}^{n\\times d}$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbf h_{t}\\doteq\\ f(\\mathbf W_{h}\\mathbf h_{t-1}+\\mathbf W_{x}\\mathbf u(\\mathbf x_{t}))\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $f$ is a potentially nonlinear function that inputs a vector and outputs the same-sized vector and $\\mathbf{u}$ can be any transformation of the inputs $\\mathbf{x}_{t}$ before they are inputted into the recurrent layer. The following equivalence result is straightforward but worthwhile formalizing. ", "page_idx": 14}, {"type": "text", "text": "Proposition B.1. Assume $f\\circ\\mathbf{P}=\\mathbf{P}\\circ f$ for any full rank, potentially complex-valued $\\mathbf{P}\\in\\mathbb{C}^{n\\times n}$ with unit-length column vectors. Then given any $\\mathbf{W}_{h}$ and $\\mathbf{W}_{x}$ for Equation (14), there is a corresponding complex-valued diagonal weight matrix \u039b \u2208Cn\u00d7n and Wx \u2208Cn\u00d7d ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\overline{{\\mathbf{h}}}_{t}=f(\\mathbf{\\Lambda}\\mathbf{\\overline{{h}}}_{t-1}+\\overline{{\\mathbf{W}}}_{x}\\ \\mathbf{u}(\\mathbf{x}_{t})).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\overline{{\\mathbf{h}}}_{t}\\in\\mathbb{C}^{n}$ is a linear transformation of $\\mathbf{h}_{t}\\in\\mathbb{R}^{n}$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. We can rewrite the square matrix $\\mathbf{W}_{h}$ using an eigenvalue decomposition ${\\bf W}_{h}\\;=\\;{\\bf P}\\;{\\\\\\Lambda}\\,{\\bf P}^{-1}$ , where $\\mathbf{P}$ contains the $n$ linearly independent eigenvectors and $\\Lambda$ is a diagonal matrix with the corresponding eigenvalues. Then, we can re-write (14) as: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{h}_{t}=f(\\mathbf{P}\\boldsymbol{\\Lambda}\\,\\mathbf{P}^{-1}\\,\\mathbf{h}_{t-1}\\,+\\,\\mathbf{P}\\mathbf{P}^{-1}\\mathbf{W}_{x}\\,\\mathbf{u}(\\mathbf{x}_{t}))}\\\\ &{\\qquad\\qquad=\\mathbf{P}f(\\boldsymbol{\\Lambda}\\,\\mathbf{P}^{-1}\\,\\mathbf{h}_{t-1}\\,+\\,\\mathbf{P}^{-1}\\mathbf{W}_{x}\\,\\mathbf{u}(\\mathbf{x}_{t}))}\\\\ &{\\mathbf{P}^{-1}\\mathbf{h}_{t}=f(\\boldsymbol{\\Lambda}\\mathbf{P}^{-1}\\,\\mathbf{h}_{t-1}\\,+\\,\\mathbf{P}^{-1}\\,\\mathbf{W}_{x}\\,\\mathbf{u}(\\mathbf{x}_{t}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\mathbf{P}$ came outside of $f$ under our assumption that it commutes with such matrices of eigenvectors. By defining $\\overline{{\\mathbf{h}}}_{t}\\doteq\\mathbf{P}^{-1}\\,\\mathbf{h}_{t}$ and $\\overline{{\\mathbf{W}}}_{x}\\,\\doteq\\,\\mathbf{P}^{-1}\\mathbf{W}_{x}$ , we get Eq. (15). \u53e3 ", "page_idx": 14}, {"type": "text", "text": "We can see $\\overline{{\\mathbf{h}}}_{t}$ and $\\mathbf{h}_{t}$ are representationally equivalent: they are linearly weighted for downstream predictions, and so the linear transformation on $\\overline{{\\mathbf{h}}}_{t}$ can fold into this downstream linear weighting. But it is more computationally efficient to use $\\overline{{\\mathbf{h}}}_{t}$ with a diagonal weight matrix $\\Lambda$ , meaning each hidden unit only has one recurrent relation instead of $\\mathbf{n}$ . ", "page_idx": 14}, {"type": "text", "text": "Since we did not impose constraints on the matrix $\\mathbf{W}_{h}$ , other than being diagonalizable, the eigenvalues of $\\mathbf{W}_{h}$ can be complex or real numbers. Previous diagonal RNNs such as eLSTM [17], Columnar networks [18], and IndRNN [20] use only real-valued diagonal matrices. Having only real-valued diagonals implicitly assumes that the matrix $\\mathbf{W}_{h}$ is a symmetric matrix. [34] suggested using complex-valued diagonal matrices for better performance. ", "page_idx": 14}, {"type": "text", "text": "The above equivalence has only been used without any activation, namely in linear-recurrent units (LRUs) [34]. A natural question is if only the identity $f(\\mathbf{x})\\,=\\,\\mathbf{x}$ (and linear functions) satisfy this property of commuting with eigenvector matrices. Intuitively, this seems like the only option, as imagining a nonlinearity that commutes is hard. Surprisingly, for the more restricted case of symmetric $\\mathbf{W}_{h}$ , we can show a slightly more general class of activations can be used, proving an if-and-only-if relationship (see Appendix C). However, even for this restricted setting, this generalized class is limited and such activations unlikely to be preferable to a linear recurrence. We see this as a negative result, that suggests this equivalence only holds for the linear setting. ", "page_idx": 14}, {"type": "text", "text": "B.2 Complex Eigenvalues in Vanilla RNNs ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We empirically investigate whether complex eigenvalues appear when training dense RNNs in a simple task. The goal is to show that the weight matrix, $\\mathbf{W}_{h}$ , is not a symmetric matrix, even in the simplest tasks. Hence, having only real-valued diagonals is too restrictive. ", "page_idx": 14}, {"type": "text", "text": "We used the Three State POMDP [42], depicted in Figure 7, for this experiment. In this task, the agent needs to remember one cue from the previous time-step ago, to make a prediction about the next time-step. The MDP has three states, $s_{1},\\,s_{2}$ , and $s_{3}$ , and no actions. If the agent is in either $s_{1}$ or $s_{2}$ , it transitions to any of the three states with equal probability. However, if the agent is in $s_{3}$ , it transitions to the state preceded by $s_{3}$ . A sequence of observations would look like $1,3,1,2,2,3,2,\\cdot\\cdot\\cdot$ , and we ask the agent to predict the next observation. ", "page_idx": 15}, {"type": "image", "img_path": "4UvMOnZMam/tmp/17f52fd1a6ad6c82d5cabfce5edfff7f769b89479eeaeded26b6012d5b0de5cb.jpg", "img_caption": ["Figure 7: Illustration of the Three State MDP. We used dashed lines for the transitions starting in $s_{3}$ to make them more visible. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "We trained a vanilla RNN with 3 hidden states with T-BPTT with truncation length 2, which is a sufficient history length in this problem to predict the next observation. Since we have 3 hidden states, the matrix $\\mathbf{W}_{h}$ is $\\in\\bar{\\mathbb{R}}^{3\\times3}$ and could have at most 2 complex eigenvalues. ", "page_idx": 15}, {"type": "text", "text": "We measured the performance in terms of the percentage of correct predictions made in $S_{3}$ . We recorded the number of complex eigenvalues of ${\\mathbf W}_{h}$ after each parameter update, shown in Figure 8. This agent reaches $100\\%$ accuracy in this problem relatively quickly. We can also see that the agent oscillates between having two complex eigenvalues and zero eigenvalues. The average number of complex eigenvalues across $30\\;\\mathrm{run}$ is above 1.5, which means that on more than $\\frac{3}{4}$ of the steps, the RNN has two complex eigenvalues. The primary point is that we see complex eigenvalues appear frequently. ", "page_idx": 15}, {"type": "image", "img_path": "4UvMOnZMam/tmp/450ea4d5e75eea45c293e0dad963f1d680214430e33d93bef9f61f5d436d10b4.jpg", "img_caption": ["Figure 8: Left: The percentage of correct predictions when training an RNN in the Three State MDP. Right: Number of complex eigenvalues when training an RNN in the Three States MDP. The solid line is the mean over 30 runs, the shaded region area is the standard error, and the lines are individual runs. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "C More on the Equivalence of Non-Linear RTUs and Dense RNNs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "As discussed in the main body, we likely only have an equivalence between using a full dense weight matrix and a complex-valued diagonal matrix for linear recurrent layers. However, we can obtain a slightly more general equivalence in the restricted setting where the weight matrix for the recurrence is symmetric. This restricted setting is not of general interest, but we include the result here because it could be of interest to a few. ", "page_idx": 15}, {"type": "text", "text": "For the case where we have a symmetric weight matrix, we need the activation to commute with orthonormal matrices. Consider again the form ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{h}_{t}\\doteq\\ f(\\mathbf{W}_{h}\\mathbf{h}_{t-1}+\\mathbf{W}_{x}\\mathbf{x}_{t})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $f$ is a potentially nonlinear function that inputs a vector and outputs the same-sized vector. To obtain the equivalence, assume that for any orthonormal matrix A, $f\\circ\\mathbf{A}=\\mathbf{A}\\circ f$ . We can rewrite the square and symmetric matrix $\\mathbf{W}_{h}$ using an eigenvalue decomposition $\\mathbf{W}_{h}\\;=\\;\\mathbf{A}\\:\\mathbf{A}\\:\\mathbf{A}^{\\top}$ , where A contains the $n$ linearly independent eigenvectors and is an orthonormal matrix and $\\Lambda$ is a diagonal matrix with the corresponding eigenvalues. Then, we can re-write (14) as: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{h}_{t}=f(\\mathbf{A}\\,\\mathbf{A}\\,\\mathbf{A}^{\\top}\\,\\mathbf{h}_{t-1}\\,+\\,\\mathbf{A}\\mathbf{A}^{\\top}\\mathbf{W}_{x}\\,\\mathbf{x}_{t})}\\\\ &{\\qquad=\\mathbf{A}f(\\mathbf{A}\\,\\mathbf{A}^{\\top}\\,\\mathbf{h}_{t-1}\\,+\\,\\mathbf{A}^{\\top}\\mathbf{W}_{x}\\,\\mathbf{x}_{t})}\\\\ &{\\mathbf{A}^{\\top}\\mathbf{h}_{t}=\\mathbf{A}\\mathbf{A}^{\\top}\\,\\mathbf{h}_{t-1}\\,+\\,\\mathbf{A}^{\\top}\\,\\mathbf{W}_{x}\\,\\mathbf{x}_{t}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where A came outside of $f$ because commutes with orthonormal matrices. By defining $\\overline{{\\mathbf{h}}}_{t}\\doteq\\mathbf{A}^{\\top}\\mathbf{\\Lambda}\\mathbf{h}_{t}$ and $\\overline{{\\mathbf{W}}}_{x}\\,\\doteq\\,\\mathbf{A}^{\\top}\\mathbf{W}_{x}$ , we get: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\overline{{\\mathbf{h}}}_{t}=f(\\mathbf{\\Lambda}\\overline{{\\mathbf{h}}}_{t-1}+\\overline{{\\mathbf{W}}}_{x}\\;\\mathbf{x}_{t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Each hidden unit now has one recurrent relation instead of $\\mathbf{n}$ , because our weight matrix $\\Lambda$ is diagonal. ", "page_idx": 16}, {"type": "text", "text": "A natural question is if only the identity $f(\\mathbf{x})=\\mathbf{x}$ \u2014namely linear recurrence\u2014satisfies this property of commuting with orthonormal matrices. We show below that it holds for a slightly more general class of recurrent layers, proving an if-and-only-if relationship. We see the below result as a negative result, highlighting that this equivalence largely only holds for the linear setting and does not generalize to other activations of interest. It provides even further evidence that likely the only setting of interest for the general non-symmetric case is also with a linear recurrence. ", "page_idx": 16}, {"type": "text", "text": "Nonetheless, let us obtain the if-and-only-if for completeness. A simple extension that continues to satisfies this property is $f(\\mathbf{x})=\\mathbf{x}c(||\\mathbf{x}||_{2})$ for any $c:\\mathbb{R}\\rightarrow\\mathbb{R}$ . We can see that for any orthonormal $\\mathbf{A}$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\nf(\\mathbf{Ax})=\\mathbf{Ax}c(||\\mathbf{Ax}||_{2})=\\mathbf{Ax}c(||\\mathbf{x}||_{2})=\\mathbf{A}f(\\mathbf{x}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This means that we can have activations that rescale the input $\\mathbf{x}$ depending on the norm of that input. More generally, the activation can involve matrices and rotations. We can define $\\pmb\\theta(\\mathbf x)=[\\mathbf x\\,\\mathbf U(\\mathbf x)]^{\\top}$ where $\\mathbf{\\dot{U}}(\\mathbf{x})\\in\\mathbb{R}^{n\\times n-1}$ is a matrix where the columns are orthogonal vectors to each other and to $\\mathbf{x}$ . Then for any vector-valued $\\mathbf{g}:\\mathbb{R}\\to\\mathbb{R}^{n}$ , we have that $f(\\mathbf{x})=\\pmb{\\theta}(\\mathbf{x})^{\\top}\\pmb{g}(||\\mathbf{x}||_{2})$ also satisfies this property: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f(\\mathbf{Ax})=\\theta(\\mathbf{Ax})^{\\top}g(||\\mathbf{Ax}||_{2})=[\\mathbf{Ax};U(\\mathbf{Ax})]g(||\\mathbf{x}||_{2})}\\\\ {=\\mathbf{A}[\\mathbf{x};\\mathbf{U}(\\mathbf{x})]g(||\\mathbf{x}||_{2})=\\mathbf{A}f(\\mathbf{x}).\\qquad\\qquad\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The last line follows because $\\mathbf{AU}(\\mathbf{x})=\\mathbf{U}(\\mathbf{Ax})$ . Namely, for any orthogonal vector $\\mathbf{u}$ with $\\mathbf{u}^{\\top}\\mathbf{x}=0$ , we have that $\\tilde{\\mathbf{u}}\\doteq\\mathbf{A}\\mathbf{u}$ satisfies $\\tilde{\\mathbf{u}}^{\\top}\\mathbf{A}\\mathbf{x}=\\mathbf{u}^{\\top}\\mathbf{A}^{\\top}\\mathbf{A}\\mathbf{x}=\\mathbf{u}^{\\top}\\mathbf{x}=0$ because $\\mathbf{A}^{\\top}\\mathbf{A}=\\mathbf{I}$ . ", "page_idx": 16}, {"type": "text", "text": "Now we show this formally. Denote $O^{n}\\subset\\mathbb{R}^{n\\times n}:A\\in O^{n}\\iff A^{T}A=A A^{T}=I$ . Denote ", "page_idx": 16}, {"type": "equation", "text": "$$\ne_{i}=\\left[\\begin{array}{l}{0}\\\\ {\\vdots}\\\\ {1}\\\\ {\\vdots}\\\\ {0}\\end{array}\\right],\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Definition C.1. $\\pmb\\theta(\\mathbf x)$ is a matrix such that: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\pmb{\\theta}(\\mathbf{x})\\in O^{n}}\\\\ {\\pmb{\\theta}(\\mathbf{x})\\mathbf{x}=\\|\\mathbf{x}\\|_{2}^{2}e_{1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Lemma C.2. $\\pmb\\theta(\\mathbf x)$ exits and $\\forall\\mathbf{A}\\in O^{n},\\pmb{\\theta}(\\mathbf{Ax})=\\pmb{\\theta}(\\mathbf{x})\\mathbf{A}^{T}$ ", "page_idx": 16}, {"type": "text", "text": "Proof. Let $\\pmb\\theta(\\mathbf x)$ be $\\in O^{n}$ . Then, by definition of $O^{n}$ , $\\pmb\\theta(\\mathbf x)^{T}\\pmb\\theta(\\mathbf x)=\\pmb\\theta(\\mathbf x)\\pmb\\theta(\\mathbf x)^{T}=I$ . Let $\\pmb{\\theta}(\\mathbf{x})[1]=$ $\\mathbf{x}$ , and $\\pmb{\\theta}(\\mathbf{x})[2:]$ be any orthogonal vectors, also orthogonal to $\\mathbf{x}$ . Where $\\pmb\\theta(\\mathbf x)[i]$ be the $i^{t h}$ row of $\\pmb\\theta(\\mathbf x)$ . Then, for $\\mathbf{x}\\in\\mathbb{R}^{n}$ , $\\pmb{\\theta}(\\mathbf{x})\\bar{\\mathbf{x}}=\\|\\mathbf{x}\\|_{2}e_{1}$ . ", "page_idx": 17}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}{\\theta(\\mathbf{Ax})\\mathbf{Ax}=\\|\\mathbf{Ax}\\|_{2}^{2}e_{1}=\\|\\mathbf{x}\\|_{2}^{2}e_{1}=\\theta(\\mathbf{x})\\mathbf{x}}&{}\\\\ {\\theta(\\mathbf{Ax})^{T}\\theta(\\mathbf{Ax})\\mathbf{Ax}=\\theta(\\mathbf{Ax})^{T}\\theta(\\mathbf{x})\\mathbf{x}}&{}\\\\ {\\mathbf{Ax}=\\theta(\\mathbf{Ax})^{T}\\theta(\\mathbf{x})\\mathbf{x}}&{}\\\\ {\\mathbf{A}^{T}\\mathbf{Ax}=\\mathbf{A}^{T}\\theta(\\mathbf{Ax})^{T}\\theta(\\mathbf{x})\\mathbf{x}}&{}\\\\ {\\mathbf{x}=\\mathbf{A}^{T}\\theta(\\mathbf{Ax})^{T}\\theta(\\mathbf{x})\\mathbf{x}}&{}\\\\ {\\mathbf{A}^{T}\\theta(\\mathbf{Ax})^{T}\\theta(\\mathbf{x})=I}&{}\\\\ {\\theta(\\mathbf{Ax})^{T}=\\theta(\\mathbf{x})\\mathbf{A}^{T}}&{}\\end{array}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Theorem C.3. Let $f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}^{n}$ and $A\\in O^{n}$ . Then, $f\\circ A=A\\circ f\\iff\\exists g:\\mathbb{R}\\to\\mathbb{R}^{n},f(x)=$ $\\theta(x)^{T}g(\\|x\\|_{2})$ ", "page_idx": 17}, {"type": "text", "text": "Proof. Define $g(\\alpha)=f(\\alpha^{2}e_{1})$ . ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{f}(\\mathbf{x})=\\pmb{\\theta}(\\mathbf{x})^{T}\\pmb{\\theta}(\\mathbf{x})\\mathbf{f}(\\mathbf{x})}\\\\ &{\\quad\\quad=\\pmb{\\theta}(\\mathbf{x})^{T}\\mathbf{f}(\\pmb{\\theta}(\\mathbf{x})\\mathbf{x})}\\\\ &{\\quad\\quad=\\pmb{\\theta}(\\mathbf{x})^{T}\\mathbf{f}\\big(\\lVert\\mathbf{x}\\rVert_{2}^{2}\\mathbf{e}_{1}\\big)}\\\\ &{\\quad\\quad=\\pmb{\\theta}(\\mathbf{x})^{T}\\mathbf{g}(\\lVert\\mathbf{x}\\rVert_{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\mathbf{f}\\circ\\mathbf{A})(\\mathbf{x})=\\mathbf{f}(\\mathbf{Ax})}\\\\ &{\\qquad\\qquad=\\pmb{\\theta}(\\mathbf{Ax})^{T}\\mathbf{g}(\\|\\mathbf{Ax}\\|_{2})}\\\\ &{\\qquad\\quad=\\pmb{\\theta}(\\mathbf{Ax})^{T}\\mathbf{g}(\\|\\mathbf{x}\\|_{2})}\\\\ &{\\qquad\\quad=\\mathbf{A}\\pmb{\\theta}(\\mathbf{x})^{T}\\mathbf{g}(\\|\\mathbf{x}\\|_{2})}\\\\ &{\\qquad\\quad=\\mathbf{(A\\circf)}(\\mathbf{x})}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "D Issues with Two Alternative Parameterizations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section we provide a few additional insights on alternative ways to handle complex numbers within an RNN, and why they are not preferable. ", "page_idx": 17}, {"type": "text", "text": "D.1 Stability ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We look at the gradient when using each complex representation to understand how different representations affect learning stability. Since each hidden unit has only one recurrent relation in diagonal RNNs, it is sufficient to consider one unit, in isolation. To keep the below intuition simple, we also omit the input of $\\mathbf{x}$ , and consider ", "page_idx": 17}, {"type": "equation", "text": "$$\nh_{t}=\\lambda h_{t-1}=.\\ldots=\\lambda^{t}h_{0}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $h_{0}$ is the initial hidden state. ", "page_idx": 17}, {"type": "text", "text": "Real Representation $a+b i$ : Substituting $\\lambda$ in (22) with the real representation, we get: ", "page_idx": 17}, {"type": "equation", "text": "$$\nh_{t}=(a+b i)^{t}h_{0}=h_{0}\\sum_{k=0}^{t}{\\binom{t}{k}}a^{t-k}b^{k}i^{k}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then it follows that the gradient w.r.t the learnable parameters $a$ and $b$ is: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\partial h_{t}}{\\partial a}=h_{0}\\sum_{k=0}^{t}\\binom{t}{k}(t-k)a^{t-k-1}b^{k}i^{k}}\\\\ {\\displaystyle\\frac{\\partial h_{t}}{\\partial b}=h_{0}\\sum_{k=0}^{t}\\binom{t}{k}k a^{t-k}b^{k-1}i^{k}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "To prevent the gradient from vanishing/exploding, we need to restrict both $|a|$ and $|b|$ to be $\\in(0,1]$ . ", "page_idx": 18}, {"type": "text", "text": "Exponential Representation $r\\exp(i\\theta)$ : Substituting $\\lambda$ in (22) with the exponential representation, we get: ", "page_idx": 18}, {"type": "equation", "text": "$$\nh_{t}=r^{t}\\mathrm{exp}(i t\\theta)h_{0}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and the gradient w.r.t the learnable parameters $r$ and $\\theta$ is: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{\\partial h_{t}}{\\partial r}=t r^{t-1}\\mathrm{exp}(i t\\theta)h_{0},\\;\\;\\;\\frac{\\partial h_{t}}{\\partial\\theta}=r^{t}\\mathrm{exp}(i t\\theta)i t h_{0}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "To prevent the gradient from vanishing/exploding, we need to restrict $r\\in(0,1]$ . ", "page_idx": 18}, {"type": "text", "text": "Cosine Representation $r(\\cos(\\theta)+i\\sin(\\theta));$ : Substituting $\\lambda$ in (22) with the cosine representation, we get: ", "page_idx": 18}, {"type": "equation", "text": "$$\nh_{t}=r^{t}(\\cos(t\\theta)+i\\sin(t\\theta))h_{0}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and the gradient w.r.t the learnable parameters $r$ and $\\theta$ is: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\partial h_{t}}{\\partial r}=t r^{t-1}(\\cos(t\\theta)+i\\sin(t\\theta))h_{0}}\\\\ {\\displaystyle\\frac{\\partial h_{t}}{\\partial\\theta}=r^{t}(i t\\cos(t\\theta)-t\\sin t\\theta)h_{0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "To prevent the gradient from vanishing/exploding, we need to restrict $r\\,\\in\\,(0,1]$ . It is simpler to maintain stability with the exponential and cosine representations, since we only need to constrain $r\\,\\in\\,(0,1]$ . , whereas the real representation requires us to restrict both the complex number\u2019s magnitude and phase. ", "page_idx": 18}, {"type": "text", "text": "D.2 Biased gradient when only using the real part of the hidden state ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We can attempt to get the benefits of having a real-valued hidden state by simply converting the complex-valued state to a real-valued one within the LRU. However, taking only the real part results in a biased gradient, as we show in this section. ", "page_idx": 18}, {"type": "text", "text": "Consider again the one recurrent unit example, but now also consider the output obtained by taking only the real part of the recurrent state: $\\bar{y_{t}}\\,=\\,w R e\\{h_{t}\\}$ , where $w$ is a learnable parameter. The gradient w.r.t $r$ and $\\theta$ is: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\partial y_{t}}{\\partial r}=w\\Big(\\cos(\\theta)h_{t-1}+r\\cos(\\theta)\\frac{\\partial h_{t-1}}{\\partial r}\\Big)}\\\\ {\\displaystyle\\frac{\\partial y_{t}}{\\partial\\theta}=w\\Big(-r\\sin(\\theta)h_{t-1}+r\\cos(\\theta)\\frac{\\partial h_{t-1}}{\\partial\\theta}\\Big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Multiplying by a complex number $z$ is equivalent to a rotation by the matrix $\\left[{\\begin{array}{c c}{R e\\{z\\}}&{-I m g\\{z\\}}\\\\ {I m g\\{z\\}}&{R e\\{z\\}}\\end{array}}\\right]$ and a scale by $\\sqrt{R e{\\{z\\}}^{2}+I m g{\\{z\\}}^{2}}$ . We re-write the recurrent unit using this property as: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{h_{t}^{c_{1}}=r\\cos(\\theta)h_{t-1}^{c_{1}}-r\\sin(\\theta)h_{t-1}^{c_{2}}}\\\\ &{h_{t}^{c_{2}}=r\\cos(\\theta)h_{t-1}^{c2}+r\\sin(\\theta)h_{t-1}^{c_{1}}}\\\\ &{~y_{t}=w(h_{t}^{c_{1}}+h_{t}^{c_{2}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Notice that we don\u2019t need to take the real part of the recurrent state in this formulation. We now write the gradient using this new formulation: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\frac{\\partial{\\it y}_{t}}{\\partial r}=w(r\\cos(\\theta)(h_{t-1}^{c_{1}}+h_{t-1}^{c_{2}})+r\\cos(\\theta)(\\frac{\\partial h_{t-1}^{c_{1}}}{\\partial r}+\\frac{\\partial h_{t-1}^{c_{2}}}{\\partial r})}}\\\\ {{\\displaystyle\\qquad+r\\sin(\\theta)(h_{t-1}^{c_{1}}-h_{t-1}^{c_{2}})+r\\sin(\\theta)(\\frac{\\partial h_{t-1}^{c_{1}}}{\\partial r}-\\frac{\\partial h_{t-1}^{c_{2}}}{\\partial r}))}}\\\\ {{\\displaystyle\\frac{\\partial{\\it y}_{t}}{\\partial\\theta}=w(-r\\sin(\\theta)(h_{t-1}^{c_{1}}+h_{t-1}^{c_{2}})+r\\cos(\\theta)(\\frac{\\partial h_{t-1}^{c_{1}}}{\\partial\\theta}+\\frac{\\partial h_{t-1}^{c_{2}}}{\\partial\\theta})}}\\\\ {{\\displaystyle\\qquad+r\\cos(\\theta)(h_{t-1}^{c_{1}}-h_{t-1}^{c_{2}})+r\\sin(\\theta)(\\frac{\\partial h_{t-1}^{c_{1}}}{\\partial\\theta}-\\frac{\\partial h_{t-1}^{c_{2}}}{\\partial\\theta}))}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Comparing Eq. 25 and Eq. 23, we can see that using only the real part of the recurrent state leads to a loss of information in the gradient. ", "page_idx": 19}, {"type": "text", "text": "E Recurrent Trace Units ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "This appendix details the parametrization used for RTUs, the derivation of the RTRL update rules, and the extension of RTUs to multi-layers. ", "page_idx": 19}, {"type": "text", "text": "E.1 Empirical Analysis for different $r$ and $\\theta$ parameterizations: ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Since $r$ represents the magnitude of a complex number, then $r\\in\\mathbb{R}^{+}$ , it is preferred to have $r\\in(0,1]$ to avoid vanishing/exploding gradients as discussed in the previous section. Let $w_{r}$ be a learnable parameter which could directly represent $r$ or represent a function of $r$ , we can enforce the constraints on $r$ in several ways: ", "page_idx": 19}, {"type": "text", "text": "1. Direct learning: Learn $r$ directly, and clip it after each parameter\u2019s update to be in $(0,1]$ . 2. Enforcing $r\\in\\mathbb{R}^{+}$ : Learn $\\nu$ such that $r\\doteq\\exp(-\\nu)$ . This parameterization enforces $r$ to be positive. However, additional clipping is needed to enforce $r\\in(0,1]$ . 3. Enforcing stability on $r$ : We can enforce $r$ to be $\\in\\ (0,1]$ by using a positive non-linear function. For example, learn $\\nu^{\\mathrm{log}}$ such that $r\\doteq\\exp(-\\exp(\\nu^{\\mathrm{log}}))$ , this parameterization ensures that $r\\in(0,1]$ and is suggested by Orvieto et al. [34]. Another example is learning $r=\\sigma(\\nu)$ , which also ensures $r\\in(0,1]$ . ", "page_idx": 19}, {"type": "text", "text": "Finally, we can also enforce stability on $\\theta$ by learning $\\theta^{\\mathrm{log}}$ such that $\\theta\\doteq\\exp(\\theta^{\\log})$ to ensure that $\\theta$ is always positive. ", "page_idx": 19}, {"type": "image", "img_path": "4UvMOnZMam/tmp/31ae4f3654285f732640f62805454ccfe125451d5fa20e6134533761a429922a.jpg", "img_caption": ["Figure 9: Learning rate sensitivity for different parameterizations of $r$ and $\\theta$ for RTUs with 80 hidden units. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "We empirically compare the different parameterizations of $r$ and $\\theta$ . In our experiments, learning $r$ directly resulted in unstable training where the MSRE diverges. We plot the learning rate sensitivity ", "page_idx": 19}, {"type": "image", "img_path": "4UvMOnZMam/tmp/0ddb3d47f18cd3c3837b532c10817fe6fded1917b5c737ee3839b7fc533628b7.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Figure 10: Learning rate sensitivity for different parameterizations of $r$ and $\\theta$ for RTUs with 512 hidden units. ", "page_idx": 20}, {"type": "text", "text": "for the other parameterization for two different sizes of RTUs in Figures 9 and 10. While all the parametrizations produce similar performance, we notice that learning $\\theta^{\\mathrm{log}}$ and $\\nu^{\\mathrm{log}}$ has better learning rate sensitivity. ", "page_idx": 20}, {"type": "text", "text": "E.2 Real-Time Recurrent Learning for a Single Layer Linear RTUs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We now outline the details for the RTRL update rules for RTUs. The set of learnable parameters for RTUs is $\\psi\\doteq\\{\\pmb{\\nu}^{\\mathrm{log}},\\pmb{\\theta}^{\\mathrm{log}},\\mathbf{W}_{x}^{c_{1}},\\mathbf{W}_{x}^{c_{2}}\\}$ . At each time step $t$ , the learner receives a loss $\\mathcal{L}_{t}(\\hat{y}_{t},y_{t};\\pmb{\\psi})$ where $y_{t}$ is the network output at time $t$ , then the gradient of the loss w.r.t the parameters is: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{\\partial\\mathcal{L}_{t}}{\\partial\\psi}=\\frac{\\partial\\mathcal{L}_{t}}{\\partial\\mathbf{h}_{t}}\\frac{\\partial\\mathbf{h}_{t}}{\\partial\\mathbf{h}_{t}^{c_{1}}}\\frac{\\partial\\mathbf{h}_{t}^{c_{1}}}{\\partial\\psi}+\\frac{\\partial\\mathcal{L}_{t}}{\\partial\\mathbf{h}_{t}}\\frac{\\partial\\mathbf{h}_{t}}{\\partial\\mathbf{h}_{t}^{c_{2}}}\\frac{\\partial\\mathbf{h}_{t}^{c_{2}}}{\\partial\\psi},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We can derive the following gradients: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{\\displaystyle\\mathbf{\\hat{p}}\\mathbf{h}_{t}^{c_{1}}}{\\displaystyle\\frac{\\partial\\mathbf{b}_{t}^{c_{1}}}{\\partial\\nu^{\\mathrm{log}}}=\\frac{\\partial\\mathbf{g}(\\pmb{\\nu},\\pmb{\\theta})}{\\partial\\nu^{\\mathrm{log}}}\\odot\\mathbf{h}_{t-1}^{c_{1}}+\\mathbf{g}(\\pmb{\\nu},\\pmb{\\theta})\\frac{\\partial\\mathbf{h}_{t-1}^{c_{1}}}{\\partial\\nu^{\\mathrm{log}}}-\\frac{\\partial\\phi(\\pmb{\\nu},\\pmb{\\theta})}{\\partial\\nu^{\\mathrm{log}}}\\odot\\mathbf{h}_{t-1}^{c_{2}}-\\phi(\\pmb{\\nu},\\pmb{\\theta})\\frac{\\partial\\mathbf{h}_{t-1}^{c_{2}}}{\\partial\\nu^{\\mathrm{log}}}+\\frac{\\partial\\boldsymbol{\\gamma}}{\\partial\\nu_{\\mathrm{log}}}\\odot\\mathbf{W}_{x}^{c_{1}}\\mathbf{x}_{t},}\\\\ {\\frac{\\displaystyle\\mathbf{\\hat{p}}\\mathbf{h}_{t}^{c_{2}}}{\\displaystyle\\frac{\\partial\\mathbf{h}_{t}^{c_{2}}}{\\partial\\nu^{\\mathrm{log}}}=\\frac{\\partial\\mathbf{g}(\\pmb{\\nu},\\pmb{\\theta})}{\\partial\\nu^{\\mathrm{log}}}\\odot\\mathbf{h}_{t-1}^{c_{2}}+\\mathbf{g}(\\pmb{\\nu},\\pmb{\\theta})\\frac{\\partial\\mathbf{h}_{t-1}^{c_{2}}}{\\partial\\nu^{\\mathrm{log}}}+\\frac{\\partial\\phi(\\pmb{\\nu},\\pmb{\\theta})}{\\partial\\nu^{\\mathrm{log}}}\\odot\\mathbf{h}_{t-1}^{c_{1}}+\\phi(\\pmb{\\nu},\\pmb{\\theta})\\frac{\\partial\\mathbf{h}_{t-1}^{c_{1}}}{\\partial\\nu^{\\mathrm{log}}}+\\frac{\\partial\\boldsymbol{\\gamma}}{\\partial\\nu_{\\mathrm{log}}}\\odot\\mathbf{W}_{x}^{c_{2}}\\mathbf{x}_{t}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\displaystyle\\frac{\\partial\\mathbf{h}_{t}^{c_{1}}}{\\partial\\theta^{\\mathrm{log}}}=\\frac{\\partial\\mathbf{g}(\\pmb{\\nu},\\pmb{\\theta})}{\\partial\\theta^{\\mathrm{log}}}\\odot\\mathbf{h}_{t-1}^{c_{1}}+\\mathbf{g}(\\pmb{\\nu},\\pmb{\\theta})\\frac{\\partial\\mathbf{h}_{t-1}^{c_{1}}}{\\partial\\theta^{\\mathrm{log}}}-\\frac{\\partial\\phi(\\pmb{\\nu},\\pmb{\\theta})}{\\partial\\theta^{\\mathrm{log}}}\\odot\\mathbf{h}_{t-1}^{c_{2}}-\\phi(\\pmb{\\nu},\\pmb{\\theta})\\frac{\\partial\\mathbf{h}_{t-1}^{c_{2}}}{\\partial\\theta^{\\mathrm{log}}}}\\\\ {\\displaystyle\\frac{\\partial\\mathbf{h}_{t}^{c_{2}}}{\\partial\\theta^{\\mathrm{log}}}=\\frac{\\partial\\mathbf{g}(\\pmb{\\nu},\\pmb{\\theta})}{\\partial\\theta^{\\mathrm{log}}}\\odot\\mathbf{h}_{t-1}^{c_{2}}+\\mathbf{g}(\\pmb{\\nu},\\pmb{\\theta})\\frac{\\partial\\mathbf{h}_{t-1}^{c_{2}}}{\\partial\\theta^{\\mathrm{log}}}+\\frac{\\partial\\phi(\\pmb{\\nu},\\pmb{\\theta})}{\\partial\\theta^{\\mathrm{log}}}\\odot\\mathbf{h}_{t-1}^{c_{1}}+\\phi(\\pmb{\\nu},\\pmb{\\theta})\\frac{\\partial\\mathbf{h}_{t-1}^{c_{1}}}{\\partial\\theta^{\\mathrm{log}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\partial\\mathrm{g}(\\nu,\\theta)}{\\partial\\nu^{\\mathrm{log}}}=-\\mathrm{g}(\\nu,\\theta)\\exp(\\nu^{\\mathrm{log}})}\\\\ {\\displaystyle\\frac{\\partial\\mathrm{g}(\\nu,\\theta)}{\\partial\\theta^{\\mathrm{log}}}=-\\phi(\\nu,\\theta)\\exp(\\theta^{\\mathrm{log}})}\\\\ {\\displaystyle\\frac{\\partial\\phi(\\nu,\\theta)}{\\partial\\nu^{\\mathrm{log}}}=-\\phi(\\nu,\\theta)\\exp(\\nu^{\\mathrm{log}})}\\\\ {\\displaystyle\\frac{\\partial\\phi(\\nu,\\theta)}{\\partial\\theta^{\\mathrm{log}}}=\\mathbf{g}(\\nu,\\theta)\\exp(\\theta^{\\mathrm{log}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "To efficiently compute the gradient w.r.t $\\mathbf{W}_{x}^{c_{1}}$ and $\\mathbf{W}_{x}^{c_{2}}$ , we look at the influence of each when considering a single element from each recurrent state, $\\mathbf{h}_{t}^{c_{1}}$ and $\\mathbf{h}_{t}^{c_{2}}$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{h_{t,i}^{c_{1}}=g(\\nu_{i},\\theta_{i})h_{t-1,i}^{c_{1}}-\\phi(\\nu_{i},\\theta_{i})h_{t-1,i}^{c_{2}}+\\gamma_{i}\\displaystyle\\sum_{j=0}^{d}w_{x,(i,j)}^{c_{1}}{}^{}x_{t,j}}\\\\ &{h_{t,i}^{c_{2}}=g(\\nu_{i},\\theta_{i})h_{t-1,i}^{c_{2}}+\\phi(\\nu_{i},\\theta_{i})h_{t-1,i}^{c_{1}}+\\gamma_{i}\\displaystyle\\sum_{j=0}^{d}w_{x,(i,j)}^{c_{2}}{}^{}x_{t,j}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We then get: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial h_{t,i}^{c_{1}}}{\\partial W_{x,(i,j)}^{c_{1}}}=g(\\nu_{i},\\theta_{i})\\frac{\\partial h_{t-1,i}^{c_{1}}}{\\partial W_{x,(i,j)}^{c_{1}}}-\\phi(\\nu_{i},\\theta_{i})\\frac{\\partial h_{t-1,i}^{c_{2}}}{\\partial W_{x,(i,j)}^{c_{1}}}+\\gamma_{i}x_{t,j}}\\\\ &{\\frac{\\partial h_{t,i}^{c_{2}}}{\\partial W_{x,(i,j)}^{c_{1}}}=g(\\nu_{i},\\theta_{i})\\frac{\\partial h_{t-1,i}^{c_{2}}}{\\partial W_{x,(i,j)}^{c_{1}}}+\\phi(\\nu_{i},\\theta_{i})\\frac{\\partial h_{t-1,i}^{c_{1}}}{\\partial W_{x,(i,j)}^{c_{1}}}}\\\\ &{\\frac{\\partial h_{t,i}^{c_{1}}}{\\partial W_{x,(i,j)}^{c_{2}}}=g(\\nu_{i},\\theta_{i})\\frac{\\partial h_{t-1,i}^{c_{1}}}{\\partial W_{x,(i,j)}^{c_{2}}}-\\phi(\\nu_{i},\\theta_{i})\\frac{\\partial h_{t-1,i}^{c_{2}}}{\\partial W_{x,(i,j)}^{c_{2}}}}\\\\ &{\\frac{\\partial h_{t,i}^{c_{2}}}{\\partial W_{x,(i,j)}^{c_{2}}}=g(\\nu_{i},\\theta_{i})\\frac{\\partial h_{t-1,i}^{c_{2}}}{\\partial W_{x,(i,j)}^{c_{2}}}+\\phi(\\nu_{i},\\theta_{i})\\frac{\\partial h_{t-1,i}^{c_{1}}}{\\partial W_{x,(i,j)}^{c_{2}}}+\\gamma_{i}x_{t,j}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We see that each $h_{t,i}^{c_{1}}$ gets affected by weights from only one row of $\\mathbf{W}_{x}^{c_{1}}$ , thus, $\\frac{\\partial\\mathbf{h}_{t}^{c_{1}}}{\\mathbf{W}_{x}^{c_{1}}}$ can be written as a matrix of the same dimension as $\\mathbf{W}_{x}^{c_{1}}$ . The same is true for Wcx2 , Wcx2 , and $\\frac{\\partial\\mathbf{h}_{t}^{c_{2}}}{\\mathbf{W}_{x}^{c_{2}}},\\frac{\\partial\\mathbf{h}_{t}^{c_{1}}}{\\mathbf{W}_{x}^{c_{2}}}$ $\\frac{\\partial{\\mathbf{h}}_{t}^{c_{2}}}{\\mathbf{W}_{x}^{c_{1}}}$ . ", "page_idx": 21}, {"type": "text", "text": "E.3 Single Layer Non-Linear RTUs Formulation: ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We extend the linear RTUs to non-linear RTUs by adding a non-linear activation function f to the recurrent states. We can write the non-linear RTUs as follows: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{h}_{t}^{c_{1}}=\\mathbf{f}(\\mathbf{g}(\\pmb{\\nu},\\pmb{\\theta})\\odot\\mathbf{h}_{t-1}^{c_{1}}-\\phi(\\pmb{\\nu},\\pmb{\\theta})\\odot\\mathbf{h}_{t-1}^{c_{2}}+\\pmb{\\gamma}\\odot\\mathbf{W}_{x}^{c_{1}}\\mathbf{x}_{t})}\\\\ &{\\mathbf{h}_{t}^{c_{2}}=\\mathbf{f}(\\mathbf{g}(\\pmb{\\nu},\\pmb{\\theta})\\odot\\mathbf{h}_{t-1}^{c_{2}}+\\phi(\\pmb{\\nu},\\pmb{\\theta})\\odot\\mathbf{h}_{t-1}^{c_{1}}+\\gamma\\odot\\mathbf{W}_{x}^{c_{2}}\\mathbf{x}_{t})}\\\\ &{\\mathbf{\\quadh}_{t}=[\\mathbf{h}_{t}^{c_{1}};\\mathbf{h}_{t}^{c_{2}}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where f : $\\mathbb{R}^{n}\\to\\mathbb{R}^{n}$ . Following the same procedure as in the linear case, we can derive RTRL update rules for the non-linear RTUs. ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{r l}&{\\displaystyle\\frac{\\partial\\mathbf{h}_{t}^{c_{1}}}{\\partial\\nu^{\\log}}=\\mathbf{f}^{\\prime}(\\cdot)(\\frac{\\partial\\mathbf{g}(\\pmb{\\nu},\\pmb{\\theta})}{\\partial\\nu^{\\log}}\\odot\\mathbf{h}_{t-1}^{c_{1}}+\\mathbf{g}(\\pmb{\\nu},\\pmb{\\theta})\\frac{\\partial\\mathbf{h}_{t-1}^{c_{1}}}{\\partial\\nu^{\\log}}-\\frac{\\partial\\phi(\\pmb{\\nu},\\pmb{\\theta})}{\\partial\\nu^{\\log}}\\odot\\mathbf{h}_{t-1}^{c_{2}}-\\phi(\\pmb{\\nu},\\pmb{\\theta})\\frac{\\partial\\mathbf{h}_{t-1}^{c_{2}}}{\\partial\\nu^{\\log}})}\\\\ &{\\displaystyle\\frac{\\partial\\mathbf{h}_{t}^{c_{2}}}{\\partial\\nu^{\\log}}=\\mathbf{f}^{\\prime}(\\cdot)(\\frac{\\partial\\mathbf{g}(\\pmb{\\nu},\\pmb{\\theta})}{\\partial\\nu^{\\log}}\\odot\\mathbf{h}_{t-1}^{c_{2}}+\\mathbf{g}(\\pmb{\\nu},\\pmb{\\theta})\\frac{\\partial\\mathbf{h}_{t-1}^{c_{2}}}{\\partial\\nu^{\\log}}+\\frac{\\partial\\phi(\\pmb{\\nu},\\pmb{\\theta})}{\\partial\\nu^{\\log}}\\odot\\mathbf{h}_{t-1}^{c_{1}}+\\pmb{\\phi}(\\pmb{\\nu},\\pmb{\\theta})\\frac{\\partial\\mathbf{h}_{t-1}^{c_{1}}}{\\partial\\nu^{\\log}})}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{\\partial\\mathbf{h}_{t}^{c_{1}}}{\\partial\\theta^{\\mathrm{log}}}=\\mathbf{f}^{\\prime}(\\cdot)(\\frac{\\partial\\mathbf{g}(\\pmb{\\nu},\\theta)}{\\partial\\theta^{\\mathrm{log}}}\\odot\\mathbf{h}_{t-1}^{c_{1}}+\\mathbf{g}(\\pmb{\\nu},\\theta)\\frac{\\partial\\mathbf{h}_{t-1}^{c_{1}}}{\\partial\\theta^{\\mathrm{log}}}-\\frac{\\partial\\phi(\\pmb{\\nu},\\theta)}{\\partial\\theta^{\\mathrm{log}}}\\odot\\mathbf{h}_{t-1}^{c_{2}}-\\phi(\\pmb{\\nu},\\theta)\\frac{\\partial\\mathbf{h}_{t-1}^{c_{2}}}{\\partial\\theta^{\\mathrm{log}}})}\\\\ &{\\displaystyle\\frac{\\partial\\mathbf{h}_{t}^{c_{2}}}{\\partial\\theta^{\\mathrm{log}}}=\\mathbf{f}^{\\prime}(\\cdot)(\\frac{\\partial\\mathbf{g}(\\pmb{\\nu},\\theta)}{\\partial\\theta^{\\mathrm{log}}}\\odot\\mathbf{h}_{t-1}^{c_{2}}+\\mathbf{g}(\\pmb{\\nu},\\theta)\\frac{\\partial\\mathbf{h}_{t-1}^{c_{2}}}{\\partial\\theta^{\\mathrm{log}}}+\\frac{\\partial\\phi(\\pmb{\\nu},\\theta)}{\\partial\\theta^{\\mathrm{log}}}\\odot\\mathbf{h}_{t-1}^{c_{1}}+\\phi(\\pmb{\\nu},\\theta)\\frac{\\partial\\mathbf{h}_{t-1}^{c_{1}}}{\\partial\\theta^{\\mathrm{log}}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{h_{t,i}^{c_{1}}=f(g(\\nu_{i},\\theta_{i})h_{t-1,i}^{c_{1}}-\\phi(\\nu_{i},\\theta_{i})h_{t-1,i}^{c_{2}}+\\gamma_{i}\\displaystyle\\sum_{j=0}^{d}w_{x,(i,j)}^{c_{1}}x_{t,j})}\\\\ &{h_{t,i}^{c_{2}}=f(g(\\nu_{i},\\theta_{i})h_{t-1,i}^{c_{2}}+\\phi(\\nu_{i},\\theta_{i})h_{t-1,i}^{c_{1}}+\\gamma_{i}\\displaystyle\\sum_{j=0}^{d}w_{x,(i,j)}^{c_{2}}x_{t,j})}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We then get: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial h_{t,i}^{c_{1}}}{\\partial W_{x,(i,j)}^{c_{1}}}=f^{\\prime}(\\cdot)(g(\\nu_{i},\\theta_{i})\\frac{\\partial h_{t-1,i}^{c_{1}}}{\\partial W_{x,(i,j)}^{c_{1}}}-\\phi(\\nu_{i},\\theta_{i})\\frac{\\partial h_{t-1,i}^{c_{2}}}{\\partial W_{x,(i,j)}^{c_{1}}}+\\gamma_{i}x_{t,j})}\\\\ &{\\frac{\\partial h_{t,i}^{c_{2}}}{\\partial W_{x,(i,j)}^{c_{1}}}=f^{\\prime}(\\cdot)(g(\\nu_{i},\\theta_{i})\\frac{\\partial h_{t-1,i}^{c_{2}}}{\\partial W_{x,(i,j)}^{c_{1}}}+\\phi(\\nu_{i},\\theta_{i})\\frac{\\partial h_{t-1,i}^{c_{1}}}{\\partial W_{x,(i,j)}^{c_{1}}})}\\\\ &{\\frac{\\partial h_{t,i}^{c_{1}}}{\\partial W_{x,(i,j)}^{c_{1}}}=f^{\\prime}(\\cdot)(g(\\nu_{i},\\theta_{i})\\frac{\\partial h_{t-1,i}^{c_{1}}}{\\partial W_{x,(i,j)}^{c_{2}}}-\\phi(\\nu_{i},\\theta_{i})\\frac{\\partial h_{t-1,i}^{c_{2}}}{\\partial W_{x,(i,j)}^{c_{2}}})}\\\\ &{\\frac{\\partial h_{t,i}^{c_{2}}}{\\partial W_{x,(i,j)}^{c_{1}}}=f^{\\prime}(\\cdot)(g(\\nu_{i},\\theta_{i})\\frac{\\partial h_{t-1,i}^{c_{2}}}{\\partial W_{x,(i,j)}^{c_{2}}}+\\phi(\\nu_{i},\\theta_{i})\\frac{\\partial h_{t-1,i}^{c_{1}}}{\\partial W_{x,(i,j)}^{c_{2}}}+\\gamma_{i}x_{t,j})}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "E.4 Complexity Analysis of RTUs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We now move to calculate the computation and memory complexity of RTUs when learning using the RTRL rules introduced in the previous section. ", "page_idx": 22}, {"type": "text", "text": "For an input $\\mathbf{x_{t}}\\in\\mathbb{R}^{d}$ and hidden states $\\mathbf{h}_{t}=[\\mathbf{f}(\\mathbf{h}_{t}^{c_{1}});\\mathbf{f}(\\mathbf{h}_{t}^{c_{2}})]\\in\\mathbb{R}^{2n}$ ,we have $\\mathbf{g}(\\nu,\\theta),\\phi(\\nu,\\theta),\\gamma\\in$ $\\mathbb{R}^{n}$ and $\\mathbf{W}_{x}^{c1},\\mathbf{W}_{x}^{c2}\\in\\mathbb{R}^{d\\times n}$ . ", "page_idx": 22}, {"type": "text", "text": "An agent using the RTU with RTRL needs to store the gradient information,\u2202h\u2202t\u03c8\u03c8\u03c8\u22121 and $\\frac{\\partial\\mathbf{h}_{t-1}^{c_{2}}}{\\partial\\pmb{\\psi}}$ , from one step to the next. We denote the set of saved gradient information as: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\pmb{\\nu}^{t-1}}\\doteq\\left\\{\\cfrac{\\partial\\mathbf{h}_{t-1}^{c_{1}}}{\\partial\\b{\\nu}^{\\log}},\\cfrac{\\partial\\mathbf{h}_{t-1}^{c_{2}}}{\\partial\\b{\\nu}^{\\log}}\\right\\}}\\\\ &{\\nabla_{\\pmb{\\theta}^{t-1}}\\doteq\\left\\{\\cfrac{\\partial\\mathbf{h}_{t-1}^{c_{1}}}{\\partial\\b{\\theta}^{\\log}},\\cfrac{\\partial\\mathbf{h}_{t-1}^{c_{2}}}{\\partial\\b{\\theta}^{\\log}}\\right\\}}\\\\ &{\\nabla_{\\pmb{W}_{\\pmb{x}}^{t-1}}\\doteq\\left\\{\\cfrac{\\partial\\mathbf{h}_{t-1}^{c_{1}}}{\\partial\\mathbf{W}_{\\pmb{x}}^{c_{1}}},\\cfrac{\\partial\\mathbf{h}_{t-1}^{c_{2}}}{\\partial\\mathbf{W}_{\\pmb{x}}^{c_{1}}},\\cfrac{\\partial\\mathbf{h}_{t-1}^{c_{1}}}{\\partial\\mathbf{W}_{\\pmb{x}}^{c_{2}}},\\cfrac{\\partial\\mathbf{h}_{t-1}^{c_{2}}}{\\partial\\mathbf{W}_{\\pmb{x}}^{c_{2}}}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The saved gradient information has the following dimensions: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\pmb{\\nu}^{t-1}}\\in\\mathbb{R}^{2n}}\\\\ &{\\nabla_{\\pmb{\\theta}^{t-1}}\\in\\mathbb{R}^{2n}}\\\\ &{\\nabla_{\\pmb{W}_{x}^{t-1}}\\in\\mathbb{R}^{4(d\\times n)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Then, it follows that memory complexity for RTU with RTRL is $O(n+n d)$ . i.e., linear in the number of parameters. ", "page_idx": 22}, {"type": "text", "text": "For the computational complexity, a forward pass according to 2 has a computational complexity of $O(n+n d)$ . Additionally, after doing the forward pass, the learner needs to update the saved gradient information according to equations 27 through 31 which has a computational complexity of $O(n+n d)$ . To summarize, using Real-Time Recurrent Learning with RTUs has linear computational and memory complexities. ", "page_idx": 22}, {"type": "text", "text": "E.5 Multi-Layers Recurrent Trace Units ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We now extend RTUs to a multilayer setting. We show that in the multilayer case, we lose the computational advantages. However, prior work suggested that treating each recurrent layer independently is a sensible choice and allows us to gain a computational advantage[18]. Consider an RTU with $n$ layers, we refer to the hidden dimension of a layer $i$ where $0<i\\leq n$ as $d_{i}$ . The network has the following set of parameters: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{\\psi}\\doteq\\{\\psi_{1},\\pmb{\\psi}_{2},\\psi_{3},\\ldots,\\pmb{\\psi}_{n}\\},}\\\\ {\\pmb{\\psi}_{i}\\doteq\\{\\pmb{\\nu}^{\\mathrm{log},i},\\pmb{\\theta}^{\\mathrm{log},i},\\mathbf{W}_{x}^{c_{1},i},\\mathbf{W}_{x}^{c_{2},i}\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "To update the network parameters, we need to calculate the gradient of the loss w.r.t the parameters from all the layers: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\frac{\\partial{\\mathcal{L}}_{t}}{\\partial\\psi}}={\\frac{\\partial{\\mathcal{L}}_{t}}{\\partial\\mathbf{h}_{t}^{n}}}{\\frac{\\partial\\mathbf{h}_{t}^{n}}{\\partial\\psi}}}\\ ~}\\\\ {{\\displaystyle{\\frac{\\partial\\mathbf{h}_{t}^{n}}{\\partial\\psi}}=\\left\\{{\\frac{\\partial\\mathbf{h}_{t}^{n}}{\\partial\\psi_{1}}},{\\frac{\\partial\\mathbf{h}_{t}^{n}}{\\partial\\psi_{2}}},{\\frac{\\partial\\mathbf{h}_{t}^{n}}{\\partial\\psi_{3}}},\\cdot\\cdot\\cdot,{\\frac{\\partial\\mathbf{h}_{t}^{n}}{\\partial\\psi_{n}}}\\right\\}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbf{h}_{t}^{n}=\\left[\\mathbf{f}\\left(\\mathbf{h}_{t}^{c1,n}\\right)\\right]\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Take as an example the gradient of $\\mathbf{h}_{t}^{c1,n}\\mathrm{~w.r.t~}\\pmb{\\nu}^{\\mathrm{log},n},\\pmb{\\nu}^{\\mathrm{log},n-1},\\dots,\\pmb{\\nu}^{\\mathrm{log},1}$ . Unrolling the last two layers of the network, we get: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{h}_{t}^{c_{1},n}=g(\\pmb{\\nu}^{n},\\pmb{\\theta}^{n})\\odot\\mathbf{h}_{t-1}^{c_{1},n}-\\phi(\\pmb{\\nu}^{n},\\pmb{\\theta}^{n})\\odot\\mathbf{h}_{t-1}^{c_{2},n}+\\gamma^{n}\\odot\\mathbf{W}_{x}^{c_{1},n}\\mathbf{h}_{t}^{n-1}}\\\\ &{\\qquad=g(\\pmb{\\nu}^{n},\\pmb{\\theta}^{n})\\odot\\mathbf{h}_{t-1}^{c_{1},n}-\\phi(\\pmb{\\nu}^{n},\\pmb{\\theta}^{n})\\odot\\mathbf{h}_{t-1}^{c_{2},n}+\\gamma^{n}\\odot\\mathbf{W}_{x}^{c_{1},n}\\left[\\mathbf{f}(\\mathbf{h}_{t}^{c_{1},n-1})\\right]}\\\\ &{\\qquad=g(\\pmb{\\nu}^{n},\\pmb{\\theta}^{n})\\odot\\mathbf{h}_{t-1}^{c_{1},n}-\\phi(\\pmb{\\nu}^{n},\\pmb{\\theta}^{n})\\odot\\mathbf{h}_{t-1}^{c_{2},n}+\\gamma^{n}\\odot\\mathbf{W}_{x}^{c_{1},n}}\\\\ &{\\qquad\\times\\left[\\mathbf{f}(g(\\pmb{\\nu}^{n-1},\\pmb{\\theta}^{n-1})\\odot\\mathbf{h}_{t-1}^{c_{1},n-1}-\\pmb{\\phi}(\\pmb{\\nu}^{n-1},\\pmb{\\theta}^{n-1})\\odot\\mathbf{h}_{t-1}^{c_{2},n-1}+\\gamma^{n-1}\\odot\\mathbf{W}_{x}^{c_{1},n-1}\\mathbf{h}_{t}^{n-2})\\right]}\\\\ &{\\qquad\\qquad\\times\\left[\\mathbf{f}(g(\\pmb{\\nu}^{n-1},\\pmb{\\theta}^{n-1})\\odot\\mathbf{h}_{t-1}^{c_{2},n-1}+\\phi(\\pmb{\\nu}^{n-1},\\pmb{\\theta}^{n-1})\\odot\\mathbf{h}_{t-1}^{c_{1},n-1}+\\gamma^{n-1}\\odot\\mathbf{W}_{x}^{c_{2},n-1}\\mathbf{h}_{t}^{n-2})\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The gradient of $\\mathbf{h}_{t}^{c1,n}\\mathrm{~w.r.t~}\\nu^{\\mathrm{log},n}$ can be calculated in linear complexity as indicated in the previous section. ", "page_idx": 23}, {"type": "text", "text": "Calculating the gradient w.r.t the parameters from the earlier layer: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial{\\bf h}_{t}^{c_{1},n}}{\\partial\\nu^{\\mathrm{log},n-1}}={\\bf g}(\\nu^{n},\\theta^{n})\\frac{\\partial{\\bf h}_{t-1}^{c_{1},n}}{\\partial\\nu^{\\mathrm{log},n-1}}-\\phi(\\nu^{n},\\theta^{n})\\frac{\\partial{\\bf h}_{t-1}^{c_{2},n}}{\\partial\\nu^{\\mathrm{log},n-1}}+\\gamma^{n}\\odot\\bf W_{x}^{c_{1},n}\\frac{\\partial{\\bf h}_{t}^{n-1}}{\\partial\\nu^{\\mathrm{log},n-1}}}\\\\ &{\\qquad\\qquad\\frac{\\partial{\\bf h}_{t}^{n-1}}{\\partial\\nu^{\\mathrm{log},n-1}}\\in{\\bf R}^{2_{n-1}}\\mathrm{~Can~be~calulated~with~linear~complexity.}}\\\\ &{\\qquad\\qquad\\frac{\\partial{\\bf h}_{t-1}^{c_{1},n}}{\\partial\\nu^{\\mathrm{log},n-1}}\\in{\\bf R}^{d_{n}\\times d_{n-1}}\\mathrm{~Saved~from~previous~timestep.}}\\\\ &{\\qquad\\qquad\\frac{\\partial{\\bf h}_{t-1}^{c_{2},n}}{\\partial\\nu^{\\mathrm{log},n-1}}\\in{\\bf R}^{d_{n}\\times d_{n-1}}\\mathrm{~Saved~from~previous~timestep.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\frac{\\partial N_{k}^{(1,n)}}{\\partial N^{(2,n)}}=}&{\\!\\!\\!\\{W^{\\prime\\prime},\\theta^{11},\\frac{\\partial N_{k}^{(1,n)}}{\\partial N^{(2,n)}}-\\phi(\\nu^{\\prime},\\theta^{11},\\frac{\\partial N_{k}^{(2,n)}}{\\partial N^{(2,n)}}+\\gamma^{n}\\odot W_{x}^{n,\\ n},\\frac{\\partial N_{k}^{(1,n)}}{\\partial N^{(2,n)}}}\\\\ &{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Let\u2019s define Ji,i\u22121 = $\\begin{array}{r}{J_{i,i-1}=\\{\\frac{\\partial\\mathbf{h}_{t-1}^{c_{1},i}}{\\partial\\pmb{\\nu}^{\\mathrm{log},i-1}},\\frac{\\partial\\mathbf{h}_{t-1}^{c_{2},i}}{\\partial\\pmb{\\nu}^{\\mathrm{log},i-1}}\\}}\\end{array}$ {\u2202\u03bd\u03bd\u03bd\u2202lhotg,\u2212i1\u22121 ,\u2202\u03bd\u03bd\u03bd\u2202lhotg,\u2212i1\u22121 }. Then, to calculate the gradient of the hidden units from layer $n$ w.r.t the parameters from layer $n-1$ , we need save $J_{n,n-1}$ , and to calculate the gradient of the hidden units from layer $n$ w.r.t the parameters of layer $n-2$ , we need to save $J_{n,n-2}$ and $J_{n-1,n-2}$ . If we keep going, to calculate the gradient of the hidden units of layer $n$ w.r.t the parameters of the first layer, we need to save $J_{n,1},J_{n-1,1},J_{n-2,1},\\ldots,J_{2,1}$ . ", "page_idx": 24}, {"type": "text", "text": "E.6 Implementing RTRL within the reverse-mode automatic differentiation ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "For a function $f:\\mathbb{R}^{n}\\,\\rightarrow\\,\\mathbb{R}^{m}$ , we have the Jacobian $\\partial f(x)\\,\\in\\,\\mathbb{R}^{m\\times n}$ and we can calculate this Jacobian in two ways: forward-mode or reverse-mode differentiation. In the forward-mode differentiation, the chain rule is applied to each operation while traversing the computational graph in the forward pass [2]. While computing the derivatives during the forward pass is appealing, we need to do $n$ forward passes to get the full Jacobian, where each forward pass would allow us to compute the derivative w.r.t only one of the inputs. i.e., with forward mode differentiation, we evaluate the Jacobian one column at a time. As a result, forward-mode differentiation is inefficient for neural networks; neural networks map from learnable parameters, which can be in millions, to a loss function, hence, have very wide jacobians, $n\\gg m$ . ", "page_idx": 24}, {"type": "text", "text": "Reverse-mode differentiation, on the other hand, offers a more efficient approach. It allows us to evaluate the Jacobian one row at a time, which is particularly advantageous for neural networks. This efficiency comes at the cost of two passes through the network: a forward pass for function evaluation and a backward pass for derivative evaluation [2]. ", "page_idx": 24}, {"type": "text", "text": "RTRL is an instance of forward-mode differentiation; during the forward pass, the gradient information is evaluated along with the recurrent function computation. As a result, there is no need to perform a backward pass for the recurrent component. To efficiently use a recurrent layer with RTRL within a larger neural network, we combine RTRL for the recurrent layer with the reverse mode for the rest of the network. We use a stop gradient operation on the recurrent layer hidden state and do a normal reverse-mode differentiation. Due to the stop gradient operation, the gradient from the reverse mode assumes no time dependencies between the recurrent states. We then use the gradient traces calculated during the forward pass of the recurrent layer to correct the gradient from the reverse mode and account for the time dependencies between the recurrent states [3]. 4 ", "page_idx": 24}, {"type": "text", "text": "F Additional Details on Trace Conditioning Experiments ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In animal learning, Trace conditioning is a type of experiment where animals predict the occurrence of a stimulus (e.g., food), based on the occurrence of another stimulus like a tone. There is no prior connection between the two stimuli. However, after enough repetitions of pairing them together\u2014 playing the tone and then serving the food\u2014the animal learns to anticipate food arrival when it hears the tone [36]. We use an open-source trace conditioning benchmark introduced in prior work [39]. Two signals appear sequentially: the Conditional Stimulus (CS) and the Unconditional Stimulus (US). The CS is the trigger signal, similar to the tone, and the US is the signal of interest and appears several time steps after the CS, similar to the food. The agent also observes several distractor signals which are uncorrelated with the CS and US; the agent must learn ignore them and focus only on predicting the US. ", "page_idx": 24}, {"type": "text", "text": "The agent\u2019s objective is to predict the onset of the US, which we model as a prediction of the discounted sum of the future US, $G_{t}$ : ", "page_idx": 24}, {"type": "equation", "text": "$$\nG_{t}\\doteq\\sum_{k=0}^{\\infty}\\gamma^{k}\\mathbf{U}\\mathbf{S}_{t+k+1},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\gamma$ is a discount factor determining the prediction horizon. This problem is challenging because the CS appears, then disappears, and sometime later the US appears; the agent must construct an internal state that represents the time period between the two signals. ", "page_idx": 24}, {"type": "image", "img_path": "4UvMOnZMam/tmp/8b1fb50444a7b08297e479640fac798dff966d8c141c5c4dcc25c87d0a3809c0.jpg", "img_caption": ["Figure 11: Comparison to a block diagonal RNN. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "F.1 Additional Trace Conditioning Experiments ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Comparison to Other RTRL-based Architectures: We now compare RTUs to other RTRL-based approaches with similar architectures: an online version of LRU [48] and a vanilla block diagonal RNN. The block diagonal is a recurrent formulation similar to RTU but ignores the relation between the learnable parameters. i.e., replaces ck in 3.2 with ck = cakk $\\begin{array}{r}{\\mathbf{c}_{k}=\\left[\\begin{array}{l l}{a_{k}}&{b_{k}}\\\\ {c_{k}}&{d_{k}}\\end{array}\\right]}\\end{array}$ ", "page_idx": 25}, {"type": "text", "text": "The results in figure 11 indicate that these seemingly small differences between the diagonal RNNs can result in significantly different behavior. RTUs outperform online LRUs, with the differences discussed in-depth in Section 3.4. RTUs also outperform the block diagonal RNN. We emphasized using real-valued diagonals implicitly assumes symmetric matrices, but that is for a single real-value. This block diagonal has more representational capacity than the RTU. This result suggests it is beneficial for learning to enforce these constraints on the learnable parameters, that they correspond to the rotational representation of complex numbers. ", "page_idx": 25}, {"type": "text", "text": "On the role of RTRL in RTUs: To highlight the role of RTRL in RTUs, we evaluated the performance of both linear and non-linear RTUs with T-BPTT. In this experiment, we all agents use the same number of parameters; the only difference is whether they use RTRL or BPTT. ", "page_idx": 25}, {"type": "text", "text": "Figure 12 summarises the results of this experiment. We can see that the performance of T-BPTT approaches the performance of RTRL as the truncation length increases to cover the whole context of the task. ", "page_idx": 25}, {"type": "image", "img_path": "4UvMOnZMam/tmp/088c8dd44465753cd754a8eda57e020a2e99f126d75aa395faf7117427e3f46b.jpg", "img_caption": ["Figure 12: Evaluating RTUs with BPTT. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "F.2 Trace Conditioning Experiments Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "All agents have one recurrent layer, either an RTU or a GRU, and one linear layer. At each time step $t$ , the agent passes the observation $\\mathbf{o}_{t}$ to the recurrent layer, which outputs the recurrent state, the agent state. The recurrent state is then passed to the linear layer generating the prediction. For each agent, we swept over the learning rate $\\alpha$ used to update the network parameters, $\\alpha\\in$ $\\{10^{-1},10^{-\\tilde{2}},10^{-3},10^{-\\dot{4}},10^{-5},10^{-6}\\}$ , and averaged the performance for each learning rate over 5 independent runs. We then selected the best-performing learning rate for each agent and ran 30 independent runs using it. For all the experiments, we ran the agents for 2 million steps, and the performance was the mean squared prediction error averaged over the 2 million steps. ", "page_idx": 26}, {"type": "text", "text": "F.3 Learning Rate Sensitivity ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We show the learning rate sensitivity for all agents in the animal learning benchmark. ", "page_idx": 26}, {"type": "image", "img_path": "4UvMOnZMam/tmp/65be161f4f3f206c938b1f8c4bc55144609dd94fce7c9fa5648dd8211fba13ef.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Figure 13: GRUs used in the animal learning benchmark. The $(H;T)$ in the label refers to the (hidden dimension: truncation length) for the GRU. ", "page_idx": 26}, {"type": "image", "img_path": "4UvMOnZMam/tmp/140e53c082da77ea56ff622269df95aff09be87090c0b1cc7decf4dc0df3a3e9.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Figure 14: LRUs used in the animal learning benchmark. The $(H;T)$ in the label refers to the (hidden dimension: truncation length) for the GRU. ", "page_idx": 26}, {"type": "image", "img_path": "4UvMOnZMam/tmp/a47639876a64509d5b45537ff68e590d85ffd70b0d49b74b2597cd4caf179340.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Figure 15: RTUs used in the animal learning benchmark. The number in the label refers to the number of hidden units in the RTU. ", "page_idx": 26}, {"type": "text", "text": "G Integrating Linear RTRL Methods with PPO ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "When performing batch updates, as with PPO, the RTRL gradients used to update the recurrent parameters will be stale, as they were calculated during the interaction with the environment w.r.t old policy and value parameters. One solution to mitigate the gradient staleness is to go through the whole trajectory after each epoch update and re-compute the gradient traces. However, this can be computationally expensive. In Algorithm 1, we provide the pseudocode for integrating RTRL methods with PPO with optional steps for re-running the network to update the RTRL gradient traces, the value targets, and the advantage estimates. ", "page_idx": 26}, {"type": "image", "img_path": "4UvMOnZMam/tmp/dbb4ffba48c4c60aa4f882f92d8e938dce9bc2d21ac690f7d319aa29c3ec8ec7.jpg", "img_caption": ["Figure 16: RTUs used in the animal learning benchmark. "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "4UvMOnZMam/tmp/1230c717114589c62a2f8f3b3ffdf11904234317a9f36f62cb6e5f4abf5f5b64.jpg", "img_caption": ["Figure 17: RTUs used in the animal learning benchmark. "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "", "page_idx": 27}, {"type": "image", "img_path": "4UvMOnZMam/tmp/c65c12004e9e69de660d14420c13a89c16153e953c25d57a776e275eaf84d85b.jpg", "img_caption": ["Figure 18: The impact of stale gradients and stale targets when combining RTRL and PPO on Ant. "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "In the next experiment, we investigate the effect of the gradient staleness in RTRL when combined with PPO and how this staleness interacts with the targets and advantage estimates. To understand this interaction, we evaluate all combinations of stale gradient and stale targets with increasing the number of epoch updates. We perform this analysis on the Ant-P environment from the Mujoco POMDP benchmark [31, 12, 27, 32]. Surprisingly, Figure 18 shows that using a stale gradient results in better performance with RTUs than re-computing the gradient traces. This performance improvement is also consistent when we increase the number of epochs from 4 to 8. It also shows that re-computing the value targets and advantage estimates has a minimal effect on the performance. We repeated the same experiments for NonLinear RTUs and Online LRU with consistent results in figures 19 and 20. ", "page_idx": 27}, {"type": "text", "text": "One hypothesis for the superior performance of stale gradients is that the staleness is helping PPO maintain the trust region. We investigate this hypothesis by measuring the KL divergence between the policy used to collect the trajectory and the most recent policy. We use the following estimate for the KL divergence between the two policies as $(r-1)-\\log(r)$ , where $r=(\\pi_{\\theta_{n e w}})/(\\pi_{\\theta_{o l d}})$ . The rightmost subplot of Figure 18 shows that at the beginning of learning, agents with stale gradients move away from the old policy more slowly than agents with fresh gradients; perhaps stale gradients help with maintaining the trust region. However, this hypothesis still needs more investigation in future work. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "image", "img_path": "4UvMOnZMam/tmp/944c8a5d7a694d16aecec2a3f5601ea3258c0d3c026cbb46ebaa8cf455798606.jpg", "img_caption": [], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "Figure 19: (a) Approximate KL divergence for NonLinear RTU with 4 epochs.(b) Approximate KL divergence for NonLinear RTU with 8 epochs. ", "page_idx": 28}, {"type": "image", "img_path": "4UvMOnZMam/tmp/eb853cf1803e6d5b3eec5ea1645fb6cab029b8fd0b79341a2fe8f07edee1473c.jpg", "img_caption": ["Figure 20: (a) Approximate KL divergence for LRU with 4 epochs.(b) Approximate KL divergence for LRU with 8 epochs. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "H More Details on the Memory-Based Control Experiments ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In our implementations, we use a shared representation learning network followed by two MLPs for the actor and the critic heads, as illustrated in Figure 21. The shared representations consist of a feedforward layer with 64 hidden units and memory components: an RTU, LRU, or a GRU. The actor and the critic\u2019s heads consist of two feedforward layers with tanh activation function. ", "page_idx": 28}, {"type": "text", "text": "Additional Mujoco Results: ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In Figures 22 and 23, we set the truncation length for GRU and LRU to be 64, which is larger than needed to solve the Mujoco POMDP tasks. These results show that even when the truncation length is larger than needed, RTUs still outperform T-BPTT baselines. We also show that the transformerbased models, GPT2, perform worse than RNN-based models. This is consistent with previous work suggesting that transformers might not be suitable for RL tasks [32]. ", "page_idx": 28}, {"type": "text", "text": "Inputs: a differentiable policy parametrization $\\pi(a|s,\\mathbf{W}_{p})$ .   \nInputs: a differentiable state-value function parametrization $\\hat{v}(s,\\mathbf{W}_{v})$ .   \nloop Generate a trajectory using the current policy $\\mathbf{O}_{0},A_{0},R_{1},\\ldots,\\mathbf{O}_{M},A_{M},R_{M}.$ , Store the transition and the gradient traces for the recurrent components along the way for $i=0,\\dots,M$ Compute the advantage estimates and the target value for each timestep $t$ for epoch $=1$ , ..., k do Divide the trajectory into minibatches and shuffle them. for minibatch $=1$ , ..., m do Calculate PPO loss Perform a gradient step with AutoDiff and correct it with the RTRL saved gradient as discussed in E.6. end for [Optional] Re-run network to update hidden states and the gradient traces for the trajectory. [Optional] Update value targets and advantages estimates for the trajectory. end for ", "page_idx": 29}, {"type": "image", "img_path": "4UvMOnZMam/tmp/2c02c9d9cd3bf6cdc8e6a1055f4d448777ef593f8b5e9075d12afe903cf6407a.jpg", "img_caption": ["Figure 21: Agents architectures used in our control experiments. "], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "4UvMOnZMam/tmp/c3e351e494cff43363d5746d25d20e78360f50f07c619e917049fdc536f83623.jpg", "img_caption": [], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "Figure 22: Additional results on Mujoco-P, where we allow GRU and LRU to use a larger truncation length than needed to solve these tasks. We also show results for GPT2. ", "page_idx": 29}, {"type": "text", "text": "Learning Rate Sensitivity: Figures 24, 25, 26, and 27 show the learning rate sensitivity for all agents in the Mujoco POMDP benchmark. Finally, we used the default hyper-parameters for PPO [40] indicated in Table 1 for all agents. ", "page_idx": 29}, {"type": "image", "img_path": "4UvMOnZMam/tmp/c00f94ab6044b2bdaa77da63d57d76a8821732c31aec98ecbbc56242c2aa8731.jpg", "img_caption": [], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "Figure 23: Additional results on Mujoco-P, where we allow GRU and LRU to use a larger truncation length than needed to solve these tasks. ", "page_idx": 30}, {"type": "image", "img_path": "4UvMOnZMam/tmp/4ed1ada94da8c2e562a61300b95896a2ce3696510a8d665445fca2a2c4936745.jpg", "img_caption": ["Figure 24: Learning rate sweep for LRU in the control experiments. "], "img_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "4UvMOnZMam/tmp/e68cd13110707605d2d48018a68f8a8314375dbead070cbea1671086709a036f.jpg", "img_caption": ["Figure 25: Learning rate sweep for RTUs in the control experiments. "], "img_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "4UvMOnZMam/tmp/7c07aab9376a520649aeda35ed8938aa73de2dff9153e6f18a737f6aea4a89c5.jpg", "img_caption": ["Figure 26: Learning rate sweep for GRUs in the control experiments. "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "I Compute resources ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "We ran the Mujoco-P, Mujoco-V on NVIDIA P100 GPU. Each of the Mujoco-P and Mujoco-V trials took around 30 minutes to complete on a single GPU. For the POPGym experiments and animal learning experiments, we used a large CPU cluster. Each trial of the POPGym experiments took around 2 hours to complete. While each run of animal learning took around 15 minutes to complete on a single CPU with memory less than 1 GB. ", "page_idx": 30}, {"type": "image", "img_path": "4UvMOnZMam/tmp/3834e2926b3d12ae52a3ccd898e022d05953ea5308ebfe6bce853932473d3753.jpg", "img_caption": [], "img_footnote": [], "page_idx": 31}, {"type": "table", "img_path": "4UvMOnZMam/tmp/ce05ecd8beec1615ffb985788cfb205dbf724b87a378b2de42353edd594e9680.jpg", "table_caption": ["Figure 27: Learning rate sweep for GPT in the control experiments. ", "Table 1: Hyper Parameters for PPO. "], "table_footnote": [], "page_idx": 31}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "1. Claims ", "page_idx": 31}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We provide detailed theoretical and experimental analysis that supports our claims. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 31}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Limitations are discussed in the conclusion section. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper. ", "page_idx": 31}, {"type": "text", "text": "\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 32}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Details of all proofs are provided in the appendix and referenced in the main paper. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 32}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: all hyper-parameters and experiment details are shared in the appendix and referenced in the main paper. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. ", "page_idx": 32}, {"type": "text", "text": "\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 33}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: A link for a public repo is provided. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 33}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: all hyper-parameters and experiment details are shared in the appendix and referenced in the main paper. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 34}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: we report error bars/shaded standard error regions in all our experiments. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 34}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: Compute details were provided in the appendix. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. ", "page_idx": 34}, {"type": "text", "text": "\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 35}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: The research conducted follows the NeurIPS code of ethics. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 35}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 35}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: The research conducted here doesn\u2019t include large models or scraped datasets. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 35}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 36}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: all related work has been properly cited. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 36}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: The paper does not introduce new assets. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 36}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: The research conducted here doesn\u2019t include human subjects. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 37}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: The research conducted here doesn\u2019t include human subjects. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 37}]