{"references": [{"fullname_first_author": "Williams and Zipser", "paper_title": "A learning algorithm for continually running fully recurrent neural networks", "publication_date": "1989", "reason": "This paper introduced the Real-Time Recurrent Learning (RTRL) algorithm, a core method used and analyzed in the current research."}, {"fullname_first_author": "Williams and Peng", "paper_title": "An efficient gradient-based algorithm for on-line training of recurrent network trajectories", "publication_date": "1990", "reason": "This paper introduced the Truncated-BackPropagation Through Time (T-BPTT) algorithm, a crucial baseline method compared against RTRL in the current paper."}, {"fullname_first_author": "Ollivier", "paper_title": "Training recurrent networks online without backtracking", "publication_date": "2015", "reason": "This paper introduced methods that approximate RTRL, a critical area of comparison for the proposed method."}, {"fullname_first_author": "Schulman", "paper_title": "Proximal policy optimization algorithms", "publication_date": "2017", "reason": "This paper describes the Proximal Policy Optimization (PPO) algorithm, a reinforcement learning method used in conjunction with the proposed recurrent neural network approach."}, {"fullname_first_author": "Orvieto", "paper_title": "Resurrecting recurrent neural networks for long sequences", "publication_date": "2023", "reason": "This paper explored complex-valued diagonal recurrences in RNNs, providing foundational insights for the current paper's architecture."}]}