{"references": [{"fullname_first_author": "Alessandro Achille", "paper_title": "Critical learning periods in deep neural networks", "publication_date": "2017-11-01", "reason": "This paper introduces the concept of critical learning periods, which is relevant to the paper's investigation of training dynamics."}, {"fullname_first_author": "Alessio Ansuini", "paper_title": "Intrinsic dimension of data representations in deep neural networks", "publication_date": "2019-12-01", "reason": "This paper explores the concept of intrinsic dimension, which is related to the paper's analysis of layer width sufficiency and the relationship between channel weight norms."}, {"fullname_first_author": "Kaiming He", "paper_title": "Deep residual learning for image recognition", "publication_date": "2016-07-01", "reason": "This is a highly influential paper in the field of deep learning, and its ResNet architecture is used in the paper's empirical evaluations."}, {"fullname_first_author": "Zhou Lu", "paper_title": "The expressive power of neural networks: A view from the width", "publication_date": "2017-12-01", "reason": "This paper provides a theoretical lower bound on layer width for approximating functions, which is directly relevant to the paper's investigation of layer width sufficiency."}, {"fullname_first_author": "Preetum Nakkiran", "paper_title": "Deep double descent: Where bigger models and more data hurt", "publication_date": "2020-04-01", "reason": "This paper introduces the concept of deep double descent, which helps explain the relationship between model size, training data, and generalization performance, a factor considered in the paper's analysis."}]}