[{"figure_path": "yTTomSJsSW/tables/tables_6_1.jpg", "caption": "Table 1: Performance comparison between RE-CONTROL and other test-time alignment approaches. The win rate is evaluated by GPT-4 as the rate at which the model's response is rated better than the preferred response in the dataset. Note that CD [31] requires the base model to have the same tokenization strategy as the reward model.", "description": "This table presents a quantitative comparison of the proposed RE-CONTROL method against several baseline test-time alignment methods across three different large language models (LLMs) and two datasets (HH-RLHF and SHP).  The metrics used for comparison include diversity, coherence, average reward, win rate (evaluated by GPT-4), and inference time.  The results show RE-CONTROL's superior performance in terms of win rate while maintaining comparable generation quality and requiring significantly less inference time than many of the baselines. Note that a limitation of the CD (Controlled Decoding) method is highlighted: it requires the base model to share the same tokenization strategy as the reward model.", "section": "5 Experiment"}, {"figure_path": "yTTomSJsSW/tables/tables_16_1.jpg", "caption": "Table 3: Summary of the hyperparameters used in training the value function of RE-CONTROL on HH-RLHF.", "description": "This table presents the hyperparameters used for training the value function in the RE-CONTROL model.  It shows the values used for parameters such as the number of epochs, learning rate, batch size, floating point format, number of layers, and hidden dimension for the Vicuna-7B and Falcon-7B language models.", "section": "5.1 Experimental Setup"}, {"figure_path": "yTTomSJsSW/tables/tables_16_2.jpg", "caption": "Table 1: Performance comparison between RE-CONTROL and other test-time alignment approaches. The win rate is evaluated by GPT-4 as the rate at which the model's response is rated better than the preferred response in the dataset. Note that CD [31] requires the base model to have the same tokenization strategy as the reward model.", "description": "This table compares the performance of the proposed RE-CONTROL method against several other test-time alignment methods (Static RE, CD, CD prefix, Prompting, CD prefix + Prompting) across two datasets (HH-RLHF and SHP) and three different base models (Vicuna-7B, Falcon-7B, Llama3-8B).  The metrics used for comparison include diversity, coherence, average reward, win rate (evaluated by GPT-4), and inference time. The table highlights that RE-CONTROL generally outperforms other methods in terms of win rate while maintaining reasonable inference times and acceptable diversity and coherence.", "section": "5 Experiment"}, {"figure_path": "yTTomSJsSW/tables/tables_17_1.jpg", "caption": "Table 1: Performance comparison between RE-CONTROL and other test-time alignment approaches. The win rate is evaluated by GPT-4 as the rate at which the model's response is rated better than the preferred response in the dataset. Note that CD [31] requires the base model to have the same tokenization strategy as the reward model.", "description": "This table compares the performance of the proposed method, RE-CONTROL, against several other test-time alignment methods on two datasets: HH-RLHF and SHP. The metrics used for comparison include diversity, coherence, average reward, win rate (as judged by GPT-4), and inference time.  The results show that RE-CONTROL outperforms other methods in terms of win rate while maintaining comparable or better performance on diversity and coherence metrics.", "section": "5 Experiment"}, {"figure_path": "yTTomSJsSW/tables/tables_17_2.jpg", "caption": "Table 6: Summary of training hyperparameters for proximal policy optimization (PPO)", "description": "This table lists the hyperparameters used for training the proximal policy optimization (PPO) model.  The hyperparameters include settings related to the number of updates steps, batch size, mini-batch size, Lora rank, learning rate, gradient accumulation steps, input and output maximum lengths, and weight decay.  These parameters are specific to the Vicuna-7B model.", "section": "5.1 Experimental Setup"}, {"figure_path": "yTTomSJsSW/tables/tables_18_1.jpg", "caption": "Table 7: Summary of training hyperparameters for Direct Policy Optimization (DPO)", "description": "This table presents the hyperparameters used for training the Direct Policy Optimization (DPO) model, a baseline method compared against in the paper.  It shows the values for parameters such as the maximum number of training steps, learning rate, Lora rank, warmup steps, batch size, gradient accumulation steps, maximum sequence length, weight decay, and regularization parameter \u03b2. These settings are specific to the Vicuna-7B model used in the experiments.", "section": "5.1 Experimental Setup"}, {"figure_path": "yTTomSJsSW/tables/tables_19_1.jpg", "caption": "Table 8: Summary of the hyperparameters used in training the value function of RE-CONTROL on SHP.", "description": "This table shows the hyperparameters used for training the value function in the RE-CONTROL model when using the Stanford SHP dataset.  It specifies values for the number of epochs, learning rate, batch size, floating point format, number of layers, and hidden dimension for both the Vicuna-7B and Llama3-8B backbones. These hyperparameters are crucial for optimizing the value function, which is a core component of the RE-CONTROL alignment approach.", "section": "5.1 Experimental Setup"}, {"figure_path": "yTTomSJsSW/tables/tables_19_2.jpg", "caption": "Table 9: Summary of hyperparameters of RE-CONTROL at test time on SHP.", "description": "This table shows the hyperparameter settings used during the test phase of the RE-CONTROL model on the Stanford SHP dataset.  It specifies values for parameters such as step size, number of updates, batch size, floating point format, maximum prompt length, and maximum generated continuation length, broken down by the backbone models used (Vicuna-7B and Llama3-8B). These settings are crucial for controlling the model's behavior at inference time and balancing the trade-off between accuracy and computational cost.", "section": "5.1 Experimental Setup"}, {"figure_path": "yTTomSJsSW/tables/tables_20_1.jpg", "caption": "Table 10: Summary of hyperparameters of static representation editing on SHP", "description": "This table shows the hyperparameters used for static representation editing on the Stanford SHP dataset.  It lists the values used for training and testing for both the Vicuna-7B and Llama3-8B models. Parameters include the number of epochs, learning rate, training and testing batch sizes, and intervention strength.", "section": "5.1 Experimental Setup"}]