[{"figure_path": "yTTomSJsSW/figures/figures_3_1.jpg", "caption": "Figure 1: Overview of RE-CONTROL: A value function is trained on the hidden space of an LLM to predict the expected reward. At test time, we optimize the hidden state of the LLM to maximize the value score. RE-CONTROL effectively steers LLMs toward specific alignment objectives while avoiding the expensive fine-tuning process.", "description": "This figure illustrates the RE-CONTROL method.  It shows three scenarios: an unaligned LLM, a fine-tuned LLM, and the RE-CONTROL approach.  In each scenario, a prompt is given to the LLM, and the generated text is shown.  The key difference is that RE-CONTROL uses a trained value model to optimize the hidden states of the LLM at test time, allowing it to steer the generation towards a desired outcome without the need for expensive fine-tuning. This is visualized using color-coded nodes for hidden states and arrows to indicate the optimization process.", "section": "4 Aligning Large Language Models from a Control Perspective"}, {"figure_path": "yTTomSJsSW/figures/figures_4_1.jpg", "caption": "Figure 2: At test time, we perform gradient-based optimization to determine the control signals added to the language dynamical system for alignment. The color represents the value score on the state space, with darker colors indicating higher scores. Our goal is not to update the state to the global optimum but to control the state to achieve a better value score while remaining close to the original state.", "description": "This figure illustrates the test-time optimization process of the RE-CONTROL model.  The contours represent the value function learned over the hidden state space of the language model. A red circle shows the initial hidden state, and a green circle shows the state after gradient-based optimization using control signals.  The black arrow indicates the direction and magnitude of the state update, demonstrating how RE-CONTROL subtly adjusts the model's representation to improve its alignment with the desired objective without significantly altering the original state.", "section": "4.2 Adding Control Signals to Large Language Models with Representation Editing"}, {"figure_path": "yTTomSJsSW/figures/figures_8_1.jpg", "caption": "Figure 1: Overview of RE-CONTROL: A value function is trained on the hidden space of an LLM to predict the expected reward. At test time, we optimize the hidden state of the LLM to maximize the value score. RE-CONTROL effectively steers LLMs toward specific alignment objectives while avoiding the expensive fine-tuning process.", "description": "This figure illustrates the architecture of the RE-CONTROL model.  A value function is trained to estimate the expected reward based on the hidden states of a pre-trained Large Language Model (LLM). During inference, this value function is used to guide the optimization of the LLM's hidden states, thereby steering its output towards desired alignment objectives without requiring any model weight updates (fine-tuning). The figure shows how the model operates differently from both unaligned and fine-tuned LLMs, highlighting its unique strengths in achieving alignment efficiently.", "section": "4 Aligning Large Language Models from a Control Perspective"}, {"figure_path": "yTTomSJsSW/figures/figures_8_2.jpg", "caption": "Figure 1: Overview of RE-CONTROL: A value function is trained on the hidden space of an LLM to predict the expected reward. At test time, we optimize the hidden state of the LLM to maximize the value score. RE-CONTROL effectively steers LLMs toward specific alignment objectives while avoiding the expensive fine-tuning process.", "description": "This figure illustrates the RE-CONTROL method. A value function is trained on the hidden states of a pre-trained language model (LLM) to predict the expected reward for a given task.  During inference, this value function is used to guide the optimization of the LLM's hidden state. By iteratively adjusting these hidden states using gradient ascent, the model's output aligns with specific objectives without requiring full model fine-tuning. The figure visually depicts the flow of information and the interaction between the value model, the LLM, and the generated text.", "section": "4 Aligning Large Language Models from a Control Perspective"}, {"figure_path": "yTTomSJsSW/figures/figures_9_1.jpg", "caption": "Figure 5: The influence of step size \u03b1 and the number of updates n at test time on diversity, coherence, and average reward. We use Vicuna-7B as the base model.", "description": "This figure shows the impact of two hyperparameters (step size and number of updates) used in the RE-CONTROL model on three evaluation metrics (diversity, coherence, and average reward).  The left subplot shows how changing the step size affects these metrics while keeping the number of updates constant. The right subplot shows the effect of altering the number of updates on the metrics while keeping the step size fixed. The results indicate a complex relationship between hyperparameter settings and model performance. Optimal values for these parameters need to be carefully tuned to balance reward maximization and maintaining good generation quality.", "section": "Hyperparameter Study"}, {"figure_path": "yTTomSJsSW/figures/figures_9_2.jpg", "caption": "Figure 6: Inference time comparison under different batch sizes. Note that CD does not support batch generation.", "description": "This figure compares the inference time of three different methods for aligning large language models: the baseline model, the proposed RE-CONTROL method, and the Controlled Decoding (CD) method.  The comparison is made across varying batch sizes (1, 8, 16, and 32).  The key takeaway is that RE-CONTROL demonstrates significantly faster inference times than CD, especially as the batch size increases. CD's inability to support batch generation further contributes to this significant performance gap.", "section": "Experiment"}, {"figure_path": "yTTomSJsSW/figures/figures_9_3.jpg", "caption": "Figure 7: Compute-performance tradeoff at test time on HH-RLHF using Vicuna-7B as the backbone. Note that CD does not support batch generation.", "description": "This figure shows the trade-off between inference time and performance for RE-Control and Controlled Decoding (CD).  RE-Control demonstrates a much faster inference time than CD. The figure showcases that as the number of iterations increases in RE-Control, the GPT-4 win rate improves, but only up to a certain point after which the win rate plateaus or slightly decreases due to overfitting. This graph highlights RE-Control's efficiency and its ability to achieve high performance even with limited computational resources.", "section": "6.4 Inference Time Analysis"}]