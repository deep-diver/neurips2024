[{"figure_path": "F9mNL6vR27/figures/figures_4_1.jpg", "caption": "Figure 1: The sketch of proof for Theorem 1.", "description": "This figure is a sketch of the proof for Theorem 1. It illustrates the steps involved in approximating the Newton operator using a neural network.  The process begins with an encoding step, followed by functional approximation using a neural network.  Finally, a reconstruction step produces the desired output.  The diagram highlights the core components and flow of the proof, which involves combining classical numerical methods with neural networks.", "section": "3 Newton Informed Neural Operator"}, {"figure_path": "F9mNL6vR27/figures/figures_7_1.jpg", "caption": "Figure 2: Training and testing performance of DeepONet under different conditions.", "description": "This figure shows the training and testing performance comparison of two different methods for training the DeepONet model to solve a convex problem. Method 1 uses only Mean Squared Error (MSE) loss with 500 supervised data samples, while Method 2 integrates MSE loss with Newton's loss, incorporating 5000 unsupervised data samples. The results demonstrate that Method 2, which leverages Newton's loss and additional unsupervised data, achieves significantly better generalization, with lower L2 and H1 test errors, than Method 1. This illustrates the advantage of using Newton's method for improved accuracy and generalization in solving nonlinear PDEs.", "section": "4.2 Case 1: Convex problem"}, {"figure_path": "F9mNL6vR27/figures/figures_8_1.jpg", "caption": "Figure 3: Solutions of 2D Non-convex problem (11)", "description": "This figure shows the solutions of the 2D Non-convex problem (11) and the training/testing performance comparison between two different training methods: Method 1 (MSE loss only) and Method 2 (MSE loss + Newton's loss). Method 2 demonstrates significantly better generalization ability and lower testing errors (L2 and H1) compared to Method 1, highlighting the effectiveness of incorporating Newton's loss in training the neural network.", "section": "4.3 Case 2: Non-convex problem with multiple solutions"}, {"figure_path": "F9mNL6vR27/figures/figures_9_1.jpg", "caption": "Figure 4: The convergence behavior of the Neural Operator-based solver.", "description": "This figure demonstrates the convergence behavior of the Neural Operator-based solver. Subfigure (a) shows an example of how the neural operator iteratively maps an initial state to a steady state. Subfigure (b) illustrates the average convergence rate of the L2 error during the iterative process. Subfigure (c) presents the training loss and test error over epochs, indicating that unsupervised data and training with Newton's loss contribute to improved model performance.", "section": "4 Experiments"}, {"figure_path": "F9mNL6vR27/figures/figures_9_2.jpg", "caption": "Figure 4: The convergence behavior of the Neural Operator-based solver.", "description": "This figure demonstrates the convergence behavior of the neural operator-based solver for the Gray-Scott model. Subfigure (a) shows an example of how the neural operator iteratively maps an initial state (a ring-like pattern not present in the training data) to its corresponding steady state. Subfigure (b) displays the average convergence rate of the L2 error over a test dataset, highlighting the efficiency of the method. Subfigure (c) presents the training loss and L2 test error during the training process using method 2 (which incorporates both supervised and unsupervised data). This figure visually illustrates the effectiveness of the proposed method in solving the Gray-Scott model, especially when dealing with limited supervised data.", "section": "4.4 Case 3: The Gray-Scott model"}, {"figure_path": "F9mNL6vR27/figures/figures_14_1.jpg", "caption": "Figure 5: Examples of steady states of the Gray Scott model", "description": "This figure displays various steady-state patterns produced by the Gray-Scott model under different initial conditions.  The Gray-Scott model is a reaction-diffusion system known for its sensitivity to initial conditions, resulting in a wide variety of complex patterns. The images showcase the diversity of these patterns, highlighting the model's complexity and the challenge of predicting its behavior with only a limited amount of initial data. These patterns illustrate the range of steady-state solutions obtainable by varying initial conditions.", "section": "4.4 Case 3: The Gray-Scott model"}, {"figure_path": "F9mNL6vR27/figures/figures_19_1.jpg", "caption": "Figure 7: Argyris method", "description": "This figure illustrates the Argyris finite element method for approximating functions and their derivatives.  It shows a triangular element with nodes at its vertices (z1, z2, z3) and midpoints (m1, m2, m3).  The inner and outer circles at each node represent evaluations of the function and its derivatives, respectively. The arrows indicate the evaluation of normal derivatives at the midpoints.  This method provides a high-order approximation of the function by incorporating multiple degrees of freedom at each node. This is used as an example of the embedding operator P in assumption 1 (iv).", "section": "3.2 Neural Operator Structures"}]