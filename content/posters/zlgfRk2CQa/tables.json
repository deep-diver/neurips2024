[{"figure_path": "zlgfRk2CQa/tables/tables_8_1.jpg", "caption": "Table 1: Results for TSP runs on symmetric and asymmetric instances. The results are the mean for 12,800 random instances. For DT-L, M = 45 was used for problem size n = 15 and M = 120 for n = 30, where tour lengths are shown in column 2. Column 3 (Random Tours) gives the tour length for random tours, while columns 4 and 5 give the tour lengths for greedy nearest neighbor tours starting from a random point (NN) or choosing the lowest NN tour from all starting points (BNN) respectively.", "description": "This table presents the results of the Traveling Salesperson Problem (TSP) experiments. It compares the average tour length obtained by the DT-L model against three baselines: random tours, nearest neighbor (NN) tours starting from a random point, and the best nearest neighbor (BNN) tour among all starting points. The comparison is done for both symmetric and asymmetric TSP instances with different numbers of cities (n=15 and n=30). For DT-L, different numbers of iterations (M=45 for n=15, M=120 for n=30) were used. The results show that DT-L significantly outperforms random tours and achieves tour lengths comparable to the NN and BNN baselines, demonstrating the model's ability to learn effective TSP solving strategies.", "section": "TSP Results"}, {"figure_path": "zlgfRk2CQa/tables/tables_14_1.jpg", "caption": "Table E1: Representative training times and peak memory usage for different models. Training time is given as hours:minutes.", "description": "This table presents the training time and peak memory usage for different models used in the experiments. The models are trained on different problems (Prefix Sums, Mazes, Chess Puzzles) with varying widths (w). Training times are provided in hours:minutes, and peak memory usage is given in GB. The hardware used for training is also specified.", "section": "E Running Times and Peak Memory Usage"}, {"figure_path": "zlgfRk2CQa/tables/tables_14_2.jpg", "caption": "Table E2: Representative training times and peak memory usage for different models (using spectral normalized weight caching). Training time is given as hours:minutes.", "description": "This table shows the training time and peak memory usage for different models used in the paper. The spectral normalized weights are cached to improve training time. The models are trained for different problems such as prefix sums, mazes, and chess puzzles, and the training time and memory usage are measured for each model.", "section": "E Running Times and Peak Memory Usage"}, {"figure_path": "zlgfRk2CQa/tables/tables_14_3.jpg", "caption": "Table E2: Representative training times and peak memory usage for different models (using spectral normalized weight caching). Training time is given as hours:minutes.", "description": "This table shows the training time and peak memory usage for different models trained using spectral normalized weight caching. The models were trained for three different problems (Prefix Sums, Mazes, and Chess Puzzles).  The table includes the model's width (w), the problem type, the batch size, the number of epochs, the hardware used, the training time (in hours and minutes), and the peak memory usage (in GB).", "section": "E Running Times and Peak Memory Usage"}, {"figure_path": "zlgfRk2CQa/tables/tables_18_1.jpg", "caption": "Table E1: Representative training times and peak memory usage for different models. Training time is given as hours: minutes.", "description": "This table presents the training time and peak memory usage for different models trained on various problems (Prefix Sums, Mazes, Chess Puzzles).  The models are categorized by type (DT-R and DT-L) and width (w).  Training times are given in hours and minutes, while memory usage is in GB.  The table helps to compare the computational efficiency of the different models and their ability to scale to more complex problems.", "section": "E Running Times and Peak Memory Usage"}, {"figure_path": "zlgfRk2CQa/tables/tables_18_2.jpg", "caption": "Table E2: Representative training times and peak memory usage for different models (using spectral normalized weight caching). Training time is given as hours:minutes.", "description": "This table presents the training time and peak memory usage for different models using the Deep Thinking with Lipschitz Constraints (DT-L) model, with spectral normalized weight caching. The results are categorized by model (width w), problem type (Prefix Sums, Mazes, Chess Puzzles), batch size, number of epochs, hardware used (RTX8000), training time (in hours:minutes), and peak memory usage (in GB).", "section": "E Running Times and Peak Memory Usage"}]