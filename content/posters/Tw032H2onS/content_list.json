[{"type": "text", "text": "Boosted Conformal Prediction Intervals ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ran Xie   \nDepartment of Statistics   \nStanford University   \nranxie@stanford.edu ", "page_idx": 0}, {"type": "text", "text": "Rina Foygel Barber Department of Statistics University of Chicago rina@uchicago.edu ", "page_idx": 0}, {"type": "text", "text": "Emmanuel J. Cande\\`s   \nDepartment of Statistics   \nDepartment of Mathematics   \nStanford University   \ncandes@stanford.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "This paper introduces a boosted conformal procedure designed to tailor conformalized prediction intervals toward specific desired properties, such as enhanced conditional coverage or reduced interval length. We employ machine learning techniques, notably gradient boosting, to systematically improve upon a predefined conformity score function. This process is guided by carefully constructed loss functions that measure the deviation of prediction intervals from the targeted properties. The procedure operates post-training, relying solely on model predictions and without modifying the trained model (e.g., the deep network). Systematic experiments demonstrate that starting from conventional conformal methods, our boosted procedure achieves substantial improvements in reducing interval length and decreasing deviation from target conditional coverage. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Black-box machine learning algorithms have been increasingly employed to inform decision-making in sensitive applications. For instance, deep convolutional neural networks have been applied to diagnose skin cancer [14], and AlphaFold has been utilized in the development of malaria vaccines [24, 25]; here, scientists have employed AlphaFold to predict the structure of a key protein in the malaria parasite, facilitating the identification of potential binding sites for antibodies that could prevent the transmission of the parasite [25]. These instances highlight the critical need for understanding prediction accuracy, and one popular approach to quantify the uncertainty associated with general predictions relies on the construction of prediction sets guaranteed to contain the target label or response with high probability. Ideally, we would like the coverage to be valid conditional on the values taken by the features of the predictive model (e.g., patient demographics). ", "page_idx": 0}, {"type": "text", "text": "Conformal prediction [3] stands out as a flexible calibration procedure that provides a wrapper around any black-box prediction model to produce valid prediction intervals. Imagine we have a data set $\\{(X_{i},Y_{i})\\}_{i=1}^{n}$ and a test point $(X_{n+1},Y_{n+1})$ drawn exchangeably from an unknown, arbitrary distribution $P$ (e.g. the pairs $(X_{i},Y_{i})$ may be i.i.d.). Taking the data set and the observed features $X_{n+1}$ as inputs, conformal prediction forms a prediction interval $C_{n}(X_{n+1})$ for $Y_{n+1}$ with valid marginal coverage, i.e. such that $\\mathbb{P}(Y_{n+1}\\in C_{n}(X_{n+1}))=0.95$ or any nominal level specified by the user ahead of time. This is achieved by means of a conformity score $E(x,y;f)$ , where $(x,y)$ represents a data point while $f$ represents any aspects of the distribution that we have estimated. For instance, the score may be given by the magnitude of the prediction error $|y-\\hat{\\mu}(x)|$ , where ${\\hat{\\mu}}(x)$ represents the model prediction of the expected outcome, in which case $f$ is simply $\\hat{\\mu}$ . Roughly, we ", "page_idx": 0}, {"type": "image", "img_path": "Tw032H2onS/tmp/052853f0c603b90b401427a36c1973143ec796303b5f9db4685eba609c34bebc.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Illustration of the boosted conformal prediction procedure. We introduce a boosting stage between training and calibration, where we boost $\\tau$ rounds on the conformity score function $E(\\cdot,\\cdot)$ and obtain the boosted score $E^{(\\tau)}(\\cdot,\\cdot)$ . The number of boosting rounds $\\tau$ is selected via cross validation. A detailed description of the procedure is presented in Algorithm 1. ", "page_idx": 1}, {"type": "text", "text": "would include $y$ in the prediction interval if $E(X_{n+1},y;f)$ does not take on an atypical value when compared with $\\{E(X_{i},Y_{i};f)\\}$ , $i=1,\\dots,n$ . Selecting an appropriate conformity score is akin to choosing a test statistic in statistical testing, where two statistics may yield the same Type I error rate yet differ substantially in other aspects of performance. ", "page_idx": 1}, {"type": "text", "text": "One central issue is that while the conformal procedure guarantees marginal coverage, it does not extend similar guarantees to other desirable inferential properties without additional assumptions. In response, researchers have introduced a variety of conformity scores, including the locally adaptive (Local) conformity score [16], the conformalized quantile regression (CQR) conformity score [18], and its variants, CQR-m [22] and CQR- ${\\bf\\nabla}\\cdot{\\bf{r}}$ [23]. Among these, CQR has often demonstrated superior empirical performance in terms of both interval length and conditional coverage [18]. ", "page_idx": 1}, {"type": "text", "text": "This paper introduces a boosting procedure aimed at enhancing an arbitrary score function.1 By employing machine learning techniques, namely, gradient boosting, our objective is to modify the Local or CQR score functions (or other baselines) to reduce the average length of prediction intervals or improve conditional coverage while maintaining marginal coverage. While this paper focuses primarily on length and conditional coverage, our methods can be tuned to optimize other criteria; we elaborate on this in Section 7. ", "page_idx": 1}, {"type": "text", "text": "Our boosted conformal procedure searches within a family of generalized scores for a score achieving a low value of a loss function adapted to the task at hand. Specifically, to evaluate the conditional coverage of prediction intervals, we build a loss function that maximizes deviation from the target coverage rate in the leaves of a shallow contrast tree [21]. Searching within a strategically designed family of score functions, rather than directly retraining or fine-tuning the fitted model under the task-specific loss function, yields greater flexibility and avoids the costs associated with retraining or fine-tuning. Further, this boosting process is executed post-model training, requiring only the model predictions and no direct access to the training algorithm. ", "page_idx": 1}, {"type": "text", "text": "Source code for implementing the boosted conformal procedure is available online at https:// github.com/ran-xie/boosted-conformal. Details regarding the acquisition and preprocessing of the real datasets are also provided in the GitHub repository. ", "page_idx": 1}, {"type": "text", "text": "2 The split conformal procedure ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We begin by outlining the key steps of the split conformal procedure applied to a family $\\{(X_{i},\\bar{Y_{i}})\\}_{i=1}^{n}$ of exchangeable samples (e.g., i.i.d.). ", "page_idx": 1}, {"type": "text", "text": "\u2022 Training. Randomly partition $[n]$ into a training set $I_{1}$ and a calibration set $I_{2}$ . On the training set, train a model by means of an algorithm $\\mathrm{\\AA}$ to produce a conformity score function $E(\\cdot,\\cdot;f)$ . The structure of this score function is predetermined, whereas the model $f$ is learned from A. An example of a conformity score is $E(x,y;f)=|y-{\\hat{\\mu}}(x)|$ , where $\\hat{\\mu}(x)$ is a learned regression function so that $f$ is here simply $\\hat{\\mu}$ . \u2022 Calibration. Evaluate the function $E(\\cdot,\\cdot;f)$ on each instance in the calibration set and obtain scores $\\{E_{i}\\}_{i\\in I_{2}}$ ,2 with each $E_{i}=E(X_{i},Y_{i};f)$ . The $(1-\\alpha)$ th empirical quantile ", "page_idx": 1}, {"type": "text", "text": "of the score, $Q_{1-\\alpha}(E,I_{2})$ , is calculated as ", "page_idx": 2}, {"type": "equation", "text": "$$\nQ_{1-\\alpha}(E,I_{2})=\\operatorname*{inf}\\{z:\\mathbb{P}\\left(Z\\leq z\\right)\\geq1-\\alpha\\},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $Z$ follows the distribution $\\frac{1}{|I_{2}|+1}\\big(\\delta_{\\infty}+\\sum\\delta_{E_{i}}\\big)$ , and $\\delta_{a}$ is a point mass at $a$ . ", "page_idx": 2}, {"type": "text", "text": "\u2022 Testing. For a new observation $X_{n+1}$ , output the conformalized prediction interval ", "page_idx": 2}, {"type": "equation", "text": "$$\nC_{n}(X_{n+1})=\\{y\\in\\mathbb{R}:E(X_{n+1},y;f)\\leq Q_{1-\\alpha}(E,I_{2})\\}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "If ties between $\\{E_{i}\\}_{i\\in I_{2}}$ occur with probability zero, it holds that ", "page_idx": 2}, {"type": "equation", "text": "$$\n1-\\alpha\\leq\\mathbb{P}(Y_{n+1}\\in C_{n}(X_{n+1}))\\leq1-\\alpha+{\\frac{2}{|I_{2}|+2}},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "see [16]. By introducing additional randomization during the calibration step, the prediction interval can be tuned to obey $\\mathbb{P}(Y_{n+1}\\in C_{n}(X_{n+1}))=1-\\alpha,$ , see [4]. This adjustment is not critical here and we omit the details. ", "page_idx": 2}, {"type": "text", "text": "Locally adaptive conformal prediction (Local for short) [16] introduces a score function that aims to make conformal prediction adapt to situations where the spread of the distribution of $Y$ varies significantly with the observed features $X$ . On the training set, run an algorithm $\\mathrm{\\AA}$ to fit two functions $\\mu_{0}(\\cdot)$ and $\\sigma_{0}(\\cdot)$ , where $\\mu_{0}(X)$ estimates the conditional mean $\\mathbb{E}[Y\\mid X]$ , and $\\sigma_{0}(X)$ the dispersion around the conditional mean, frequently chosen as the conditional mean absolute deviation (MAD), $\\mathbb{E}[|Y-\\mu_{0}(X)|\\mid X]$ . With $f=\\left(\\mu_{0},\\sigma_{0}\\right)$ , the locally adaptive (Local) score function is: ", "page_idx": 2}, {"type": "equation", "text": "$$\nE(x,y;f)=|y-\\mu_{0}(x)|/\\sigma_{0}(x).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "For a new observation $X_{n+1}$ , the conformalized prediction interval (1) takes on the simplified expression $[\\mu_{0}(X_{n+1})-Q_{1-\\alpha}(E,I_{2})\\sigma_{0}(X_{n+1}),\\bar{\\mu_{0}(X_{n+1})}+Q_{1-\\alpha}(E,I_{2})\\sigma_{0}(X_{n+1})].$ ", "page_idx": 2}, {"type": "text", "text": "Conformalized quantile regression (CQR) [17] also aims to adapt to heteroskedasticity by calibrating conditional quantiles, which often results in shorter prediction intervals. Apply quantile regression to produce a pair of estimated quantiles $(\\hat{q}_{\\alpha/2}(x),\\bar{\\hat{q}}_{1-\\alpha/2}(x))$ , where ${\\hat{q}}_{\\beta}(X)$ is the estimated $\\beta$ th quantile of the conditional distribution of $Y$ . The CQR score function is defined as ", "page_idx": 2}, {"type": "equation", "text": "$$\nE(x,y;f)=\\operatorname*{max}\\{{\\hat{q}}_{\\alpha/2}(x)-y,y-{\\hat{q}}_{1-\\alpha/2}(x)\\},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $f=(\\hat{q}_{\\alpha/2},\\hat{q}_{1-\\alpha/2})$ . For a new observation $X_{n+1}$ , following (1) yields the prediction interval ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\left[\\hat{q}_{\\alpha/2}(X_{n+1})-Q_{1-\\alpha}(E,I_{2}),\\hat{q}_{1-\\alpha/2}(X_{n+1})+Q_{1-\\alpha}(E,I_{2})\\right].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Generalized conformity score families. To construct a Local conformity score, we estimate two functions $\\mu_{0}(\\cdot)$ and $\\sigma_{0}(\\cdot)$ to plug into (3). Since these components are constructed without looking at performance downstream, it is reasonable to imagine that other choices may enjoy enhanced properties. How then should we systematically select $\\mu(\\cdot)$ and $\\sigma(\\cdot)?$ To address this, we define a generalized Local score family $\\mathcal{F}$ containing all potential score functions of the form ", "page_idx": 2}, {"type": "equation", "text": "$$\n{\\mathcal F}:=\\{E(\\cdot,\\cdot;f):E(x,y;f)=|y-\\mu(x)|/\\sigma(x),\\sigma(\\cdot)>0\\},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $f=(\\mu,\\sigma)$ . For each $E(\\cdot,\\cdot;f)\\in\\mathcal{F}$ , the conformalized prediction interval is given by ", "page_idx": 2}, {"type": "equation", "text": "$$\n[\\mu(X)-Q_{1-\\alpha}(E,I_{2})\\sigma(X),\\mu(X)+Q_{1-\\alpha}(E,I_{2})\\sigma(X)]\\,.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Turning to CQR, one notable limitation is the uniform adjustment of prediction intervals by the constant factor $Q_{1-\\alpha}(E,I_{2})$ , as shown in (5). This approach is suboptimal in the presence of heteroskedasticity, as it applies an identical correction to prediction intervals of varying widths for each $X=x$ . Thus, simply updating the fitted quantiles $(\\hat{q}_{\\alpha},\\hat{q}_{1-\\alpha/2})$ and plugging them into the original score function would be inadequate, as the structure of the original score imposes significant limitations on the effectiveness of conformalized prediction intervals. To address this, several variants including CQR- $\\cdot\\mathbf{m}$ [22] and CQR-r [23] have been proposed. Focusing on CQR-r, it employs a flexible score function, defined as $\\begin{array}{r}{E(x,y;f)=\\operatorname*{max}\\{\\hat{q}_{\\alpha{'}2}(\\bar{x})-y,y-\\hat{q}_{1-\\alpha{'}2}(x)\\}/(\\hat{q}_{1-\\alpha{'}2}(\\bar{x})-\\hat{q}_{\\alpha{/}2}(x)),}\\end{array}$ , with $f=(\\hat{q}_{\\alpha/2},\\hat{q}_{1-\\alpha/2},\\hat{q}_{1-\\alpha/2}-\\hat{q}_{\\alpha/2})$ . Following (1), conformalized prediction intervals become ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\left[\\hat{q}_{\\alpha/2}(X)-\\hat{\\sigma}(X)Q_{1-\\alpha}(E,I_{2}),\\hat{q}_{1-\\alpha/2}(X)+\\hat{\\sigma}(X)Q_{1-\\alpha}(E,I_{2})\\right],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\hat{\\sigma}=\\hat{q}_{1-\\alpha/2}-\\hat{q}_{\\alpha/2}$ . Intuitively, the adjusted score function allows prediction bands to adjust in proportion to their width, instead of adding a constant shift as in CQR. However, despite the intuitive appeal of adjusted scores as a seemingly more reasonable \u201callocation\u201d of the conformal correction, empirical studies reveal that they do not result in narrower prediction intervals when compared to CQR [23]. This phenomenon is largely due to the uniform direction of the conformal adjustment, represented by $Q_{1-\\alpha}(E,I_{2})$ , across all observations. In particular, if $Q_{1-\\alpha}(E,I_{2})<0$ , indicating that the true target $y$ predominantly lies within the estimated quantile range $[\\hat{q}_{\\alpha/2},\\hat{q}_{1-\\alpha/2}]$ , there is a uniform narrowing of the predicted interval across all samples. ", "page_idx": 3}, {"type": "text", "text": "In light of these insights, we propose a novel score family, $\\mathcal{H}$ , designed to augment the flexibility of the conformity score functions: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{H}:=\\big\\{E(\\cdot,\\cdot;f):E(x,y;f)=\\operatorname*{max}\\big\\{\\mu_{1}(x)-y,y-\\mu_{2}(x)\\big\\}/\\sigma(x),\\mu_{1}(\\cdot)\\le\\mu_{2}(\\cdot),\\sigma(\\cdot)>0\\big\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $f=(\\mu_{1},\\mu_{2},\\sigma)$ , which leads to conformalized prediction intervals of the form ", "page_idx": 3}, {"type": "equation", "text": "$$\n[\\mu_{1}(X)-\\sigma(X)Q_{1-\\alpha}(E,I_{2}),\\mu_{2}(X)+\\sigma(X)Q_{1-\\alpha}(E,I_{2})].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Notably, $\\mathcal{H}$ includes the Local, CQR, and CQR-r scores as special cases. ", "page_idx": 3}, {"type": "text", "text": "3 Boosted conformal procedure ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "It is clear from above that a model is trained to produce a conformity score $E(\\cdot,\\cdot;f)$ ; e.g., we may learn a regression function ${\\hat{\\mu}}(\\cdot)$ to plug it into a score function $|y\\mathrm{~-~}\\hat{\\mu}(x)|$ . To overcome the limitation of working with an arbitrarily selected score function, we introduce a boosting step before calibration, see Figure 1. In a nutshell, we use gradient boosting to iteratively improve upon a predefined score $E(\\cdot,\\cdot;f)$ now denoted as $E^{(0)}(\\cdot,\\cdot)$ , where the superscript indicates the 0th iteration. To achieve this, we construct a task-specific loss function $\\ell$ , which takes a dataset $\\mathcal{D}$ and a score function $E(\\cdot,\\cdot;f)$ as inputs, and outputs $\\ell(E(\\cdot,\\cdot;f);D)$ measuring how closely the conformalized prediction interval aligns with the analyst\u2019s objective. This loss function $\\ell$ is designed to be differentiable with respect to each of the model components produced by the training algorithm. Importantly, it does not require knowledge of the gradient of $f(x)$ with respect to $x$ . In the example above, taking the labels as fixed, this means that for each feature $x_{i}\\in\\mathcal{D}$ , $i=1,\\hdots,n$ , if we set ${\\hat{y}}_{i}={\\hat{\\mu}}(x_{i})$ , then the loss $\\ell(E(\\cdot,\\cdot);\\mathcal{D})$ is a function of $\\{\\hat{y}_{i}\\}_{i=1}^{n}$ , and the derivative $\\partial\\ell(E(\\cdot,\\cdot);\\mathcal{D})/\\partial\\hat{y}_{i}$ is well defined. In Sections 5.1 and 6.1, we present examples of such derivatives. ", "page_idx": 3}, {"type": "text", "text": "Each boosting iteration updates the score function sequentially, employing a gradient boosting algorithm such as XGBoost [12] or LightGBM [15]. These algorithms accept as input a dataset $\\mathcal{D}$ , a base score function $E(\\cdot,\\cdot;f)$ , a custom loss function $\\ell$ , gradients of $\\ell$ with respect to $f$ (denoted $\\nabla_{f}\\ell)$ , and a number of boosting rounds $\\tau$ . We may write the boosting procedure as ", "page_idx": 3}, {"type": "equation", "text": "$$\n(E^{(0)}(\\cdot,\\cdot),\\cdot\\,.\\,.\\,,E^{(\\tau)}(\\cdot,\\cdot))=\\mathrm{GradientBoosting}({\\mathcal D},E(\\cdot,\\cdot\\,;f),\\ell,\\nabla_{f}\\ell,\\tau).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "This yields a boosted score function $E^{(\\tau)}(\\cdot,\\cdot)$ , which is then used for calibration and for constructing prediction intervals. The number $\\tau$ is calculated using $k$ -fold cross-validation on the training dataset, selecting $\\tau$ from potential values up to a predefined maximum $T$ (e.g., 500). We partition the dataset into $k$ folds and for each $j\\,=\\,1,\\ldots,k$ , we hold out fold $j$ for sub-calibration and the remaining $k-1$ folds for sub-training. We apply $T$ rounds of gradient boosting (11) on the sub-training data, generating $T+1$ candidate score functions $E_{j}^{(0)}(\\cdot,\\cdot),\\cdot\\cdot\\cdot,E_{j}^{(T)}(\\cdot,\\cdot)$ . Each score function is then evaluated on sub-calibration data, using the loss function $\\ell$ to compute losses at all epochs, i.e., for each fold $j=1,\\dots k$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\{L_{j}^{(t)}\\}_{t=0}^{T}=\\{\\ell(E_{j}^{(t)};\\mathrm{fold}_{j})\\}_{t=0}^{T}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Last, $\\tau$ is selected as the round that minimizes the average loss across all $k$ folds: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tau=\\arg\\operatorname*{min}_{0\\leq t\\leq T}\\sum_{j=1}^{k}L_{j}^{(t)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "see Figure 2. This cross-validation step simulates the calibration step in conformal prediction and effectively prevents the overfitting of the score function. ", "page_idx": 3}, {"type": "text", "text": "Since boosting is conducted on the training data, the boosted procedure satisfies the same marginal coverage guarantee as the split conformal procedure, as formalized below. ", "page_idx": 3}, {"type": "image", "img_path": "Tw032H2onS/tmp/d273e2406c4a32e313f0d12eb176bbae5edbe7cec3cca73c2a5f561fb2014099.jpg", "img_caption": ["Figure 2: Schematic drawing showing the selection of the number of boosting rounds via crossvalidation. Left: we hold out fold $j$ , and use the remaining $k-1$ folds to generate candidate scores $E_{j}^{(t)}$ , $t\\,=\\,0,\\ldots,$ max-round. The performance of each score is evaluated on fold $j$ using the loss function $\\ell$ . Right: best-round minimizes the average loss across all $k$ folds. A detailed description of the procedure is presented in Algorithm 1. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Proposition 3.1. Let $\\{(X_{i},Y_{i})\\}_{i=1}^{m}$ be the held out calibration set, and $(X_{m+1},Y_{m+1})$ be a pair of new observation. If the $m+1$ samples are exchangeable, and ties between $\\{E^{(\\tau)}(X_{i},Y_{i})\\}_{i=1}^{m}$ occur with probability zero, the confromalized prediction interval $(I)$ computed from score function $E^{(\\tau)}(\\cdot,\\cdot)$ satisfies the coverage guarantee (2). ", "page_idx": 4}, {"type": "text", "text": "Searching within generalized conformity score families. To update the Local score function (3), we search within the generalized score family $\\mathcal{F}$ (6). First, we initialize $\\mu^{(0)}=\\mu_{0}$ and $\\sigma^{(0)}=\\sigma_{0}$ . After completing $\\tau$ iterations of boosting on the training set, we obtain the boosted score function $E^{(\\tau)}(x,y)\\,=\\,|\\bar{y}-\\mu^{(\\tau)}(x)|/\\sigma^{(\\tau)}(x)$ . Notably, we can update any score function within $\\mathcal{F}$ . For instance, to update $E(x,y;f)=|y-{\\hat{\\mu}}(x)|$ , we simply initialize $\\boldsymbol{\\mu}^{(0)}=\\hat{\\boldsymbol{\\mu}}$ , and take $\\sigma^{(0)}$ to be the constant function equal to one. Similarly, to update the CQR score function (4), we search within the score family $\\mathcal{H}$ (9). First, we initialize a triple $\\mu_{1}^{(0)}=\\hat{q}_{\\alpha/2}$ , $\\mu_{2}^{(0)}=\\hat{q}_{1-\\alpha/2}$ , $\\sigma^{(0)}=\\hat{q}_{1-\\alpha/2}-\\hat{q}_{\\alpha/2}$ . After $\\tau$ boosting rounds, we obtain the boosted score function $E^{(\\tau)}(x,y)=\\operatorname*{max}\\{\\mu_{1}^{(\\tau)}(x)-y,y-$ $\\mu_{2}{}^{(\\tau)}(x)\\}/\\sigma^{(\\tau)}(x)$ . ", "page_idx": 4}, {"type": "text", "text": "Algorithm 1 Boosting stage ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Input: ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Training data $(X_{i},Y_{i})\\in\\mathbb{R}^{p}\\times\\mathbb{R}$ , $i=1,...,n$ ; base conformity score function $E^{(0)}(\\cdot,\\cdot)$ Loss function $\\ell$ ; target mis-coverage level $\\alpha\\in(0,1)$ Number $k$ of cross-validation folds; maximum boosting rounds $T$ ", "page_idx": 4}, {"type": "text", "text": "Procedure: ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Randomly divide $\\{1,...,n\\}$ into $k$ folds   \nfor $j\\leftarrow1$ to $k$ do Set fold $j$ as sub-calibration set, and the remaining $k-1$ folds as sub-training set On the sub-training set, call GradientBoosting (11) to obtain candidate scores $\\{E_{j}^{(t)}\\}_{t=0}^{T}$ On the sub-calibration set, evaluate $L_{j}^{(t)}=\\ell(E_{j}^{(t)})$ , $t=0,\\dots,T$   \neSnetd  bfooorsting rounds $\\begin{array}{r}{\\tau\\gets\\arg\\operatorname*{min}_{t}\\frac{1}{k}\\sum_{j=1}^{k}L_{j}^{(t)}}\\end{array}$ as in (12)   \nOn the training set, call GradientBoosting (11) to obtain boosted functions $\\{E^{(t)}\\}_{t=0}^{\\tau}$ ", "page_idx": 4}, {"type": "text", "text": "Output: ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Boosted conformity score function $E^{(\\tau)}(\\cdot,\\cdot)$ ", "page_idx": 4}, {"type": "text", "text": "4 Related Works ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Adapting the classical conformal procedure to improve properties of the conformalized intervals has been one of the primary focuses of recent literature. Noteworthy contributions\u2014including CFGNN [28] and ConTr [26]\u2014approach this problem by introducing modifications to the training stage of the procedure. As outlined in Section 2, a model is trained to produce a score function $E(\\cdot,\\cdot;f)$ . The model $f$ usually depends on a set of model parameters, e.g., neural network parameters $\\theta$ . Denote the trained model $f$ by $f_{\\theta}$ . CF-GNN and ConTr retrain or fine-tune the model by using a carefully constructed loss function, which may aim to produce narrower prediction intervals or prediction sets of reduced cardinality in classification problems. This process generates a new set of model parameters $\\theta^{\\prime}$ . The new model $f_{\\theta^{\\prime}}$ is then plugged into the same predefined conformity score function\u2014namely CQR [28] or the adaptive prediction set score (APS) [26]\u2014to produce $E(\\cdot,\\cdot,f_{\\theta^{\\prime}})$ . ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "There are two primary limitations. First, the score function imposes constraints on the properties of conformalized intervals as explained in Section 2. Our approach introduces more flexibility by constructing a family of generalized score functions that is a superset of $\\{E(\\cdot,\\cdot;f_{\\theta}):\\theta\\in\\Theta\\}$ , where $\\Theta$ is the parameter space of the training model. This family is strategically designed to contain an oracle conformity score ideally suited to the task at hand, e.g., achieving exact conditional coverage. Second, current methodologies necessitate fine-tuning or retraining models from scratch, requiring both access to the training model and significant computational resources. In contrast, our boosted conformal method operates directly on model predictions and circumvents these issues. ", "page_idx": 5}, {"type": "text", "text": "Conditional coverage of conformalized prediction intervals has also attracted significant interest, characterized by efforts to establish theoretical guarantees and achieve numerical improvements. Prior work established an impossibility result [8, 20], which states that exact conditional coverage in finite samples cannot be guaranteed without making assumptions about the data distribution. Subsequently, Gibbs et al. [27] developed a modified conformal procedure that guarantees conditional coverage for predefined protected sub-groups, i.e. subsets of the feature space. Our approach differs from the previous works by introducing a numerical method directly aimed at improving the conditional coverage, $\\mathbb{P}(Y\\in\\dot{C_{n}}(X)|X=\\bar{x})$ , across all potential values of $x$ . ", "page_idx": 5}, {"type": "text", "text": "5 Boosting for conditional coverage ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Maintaining valid marginal coverage, our goal is to produce a prediction interval $C_{n}$ obeying ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{P}(Y\\in C_{n}(X_{n+1})|X_{n+1}=x)\\approx1-\\alpha\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "for all possible values of $x$ . To this end, we present a loss function that quantifies the conditional coverage rate of any prediction interval. Requiring merely a dataset $\\mathcal{D}$ and a prediction interval $C_{n}(\\cdot)$ as inputs, it also serves as an effective evaluation metric, which may be of independent interest. ", "page_idx": 5}, {"type": "text", "text": "5.1 A measure for deviation from target conditional coverage ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "From now on, we let $E$ be the score function $E(\\cdot,\\cdot;f)$ . Set $D=\\{(X_{i},Y_{i})\\}_{i=1}^{n}$ and denote by $C_{n}(\\cdot)$ the conformalized prediction interval constructed from $E$ . We shall assess the deviation of $C_{n}(\\cdot)$ from the target conditional coverage by means of Contrast Trees [21]. As background, a contrast tree iteratively identifies splits within the feature space $\\mathcal{X}$ in a greedy fashion, aiming to maximize absolute within-group deviations from the target conditional coverage rate $(1\\!-\\!\\alpha)$ . For a subset $R$ of the data point indices $[n]$ , let $\\boldsymbol{\\mathcal{D}}_{R}=\\{X_{j},Y_{j}\\}_{j\\in R}^{\\circ}$ . The absolute within-group deviation is computed as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{d\\left(C_{n}(\\cdot);\\mathcal{D}_{R}\\right)=\\left||R|^{-1}{\\sum_{j\\in R}}\\mathbb{1}(Y_{j}\\in C_{n}(X_{j}))-(1-\\alpha)\\right|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The overall empirical maximum deviation is then defined as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\ell_{M}\\left(E;\\mathcal{D}\\right)=\\operatorname*{max}_{1\\leq m\\leq M}{d\\left(C_{n}(\\cdot);\\mathcal{D}_{\\hat{R}_{m}}\\right)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\hat{R}_{1}\\cup\\cdot\\cdot\\cdot\\cup\\hat{R}_{M}$ is a partition of $[n]$ , which itself depends on $E$ and $\\mathcal{D}$ . Specifically, it is computed by running a contrast tree for $M$ iterations. At each iteration, the algorithm not only seeks to isolate regions with large deviations but also discourages splits where any subset $\\hat{R}_{m}$ is too small. ", "page_idx": 5}, {"type": "text", "text": "To update score functions via gradient boosting as described in (11), we would need a differentiable approximation of the maximum deviation. To this end, we construct approximations for the following three components of the loss function. With an abuse of notation, in subsequent discussions, we shall employ the same notations to denote these differentiable approximations. ", "page_idx": 5}, {"type": "text", "text": "1. Approximation for the prediction interval $C_{n}(\\cdot)$ in (14): the prediction interval is formulated as (7) for the generalized Local score, and as (10) for the generalized CQR score. ", "page_idx": 5}, {"type": "text", "text": "Denote the upper and lower limits of $C_{n}(\\cdot)$ by $u(\\cdot)$ and $l(\\cdot)$ . We approximate the empirical quantile $Q_{1-\\alpha}(E,I_{2})$ in $u(\\cdot)$ and $l(\\cdot)$ with a smooth quantile estimator $Q_{1-\\alpha}^{s}$ . Given $r$ scalars $\\{z_{i}\\}_{i=1}^{r}$ , $Q_{1-\\alpha}^{s}$ is constructed as: ", "page_idx": 6}, {"type": "equation", "text": "$$\nQ_{1-\\alpha}^{s}(\\{z_{i}\\}_{i=1}^{r}):=\\langle\\mathrm{HD}(r),s(\\mathbf{z})\\rangle,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\langle\\cdot,\\cdot\\rangle$ represents the dot product. Here, $\\mathrm{HD}(r)\\ =\\ [W_{r,1},...,W_{r,r}]$ is the weight vector corresponding to the Harrel-Davis distribution-free empirical quantile estimator [1], and $s(\\mathbf{z})$ is a differentiable ordering $\\{\\tilde{z}_{(i)}\\}_{i=1}^{r}$ , arranged in the ascending order. In practice, the derivative of $s(\\mathbf{z})$ with respect to each $z_{i}$ is given by the package developed in [19]. This approach is a smooth approximation of the Harrel-Davis quantile estimator $Q_{\\mathrm{1-}\\alpha}^{\\mathrm{HD}}$ , constrructed as a linear combination of the order statistics, $Q_{1-\\alpha}^{\\mathrm{\\bar{HD}}}\\;=\\;$ $\\begin{array}{r}{\\langle\\mathrm{HD}(r),\\{z_{(i)}\\}\\rangle=\\sum_{i=1}^{r}W_{r,i}z_{(i)}}\\end{array}$ , where $W_{r,i}$ takes the value $I_{(1-\\alpha)(r+1),\\alpha(r+1)}(i/r)-$ $I_{(1-\\alpha)(r+1),\\alpha(r+1)}((i-1)/r)$ and $I_{a,b}(\\boldsymbol{x})$ represents the incomplete beta function. ", "page_idx": 6}, {"type": "text", "text": "2. Approximation for absolute deviation $d_{i}$ (14): the indicator function in (14) can be approximated by the product of two sigmoid functions, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{1}\\big(Y_{j}\\in C_{n}(X_{j})\\big)=\\mathbb{1}\\big(u(X_{j})-Y_{j}\\ge0\\big)\\mathbb{1}\\big(Y_{j}-l(X_{j})\\ge0\\big)}\\\\ &{\\qquad\\qquad\\qquad\\approx S_{\\tau_{1}}\\big(u(X_{j})-Y_{j}\\big)S_{\\tau_{1}}\\big(Y_{j}-l(X_{j})\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\tau_{1}$ is a parameter, trading off smoothness and quality of the approximation. The sigmoid function $S_{\\tau_{1}}(x)$ is defined as $S_{\\tau_{1}}(x)=(1+e^{\\b{\\dot{-}}\\tau_{1}x})^{\\b{\\dot{-}}1}$ . ", "page_idx": 6}, {"type": "text", "text": "3. Approximation for maximum deviation: we employ a log-sum-exp function [2] to derive the differentiable approximation of $\\ell_{M}$ as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\ell_{M}\\left(E;\\mathcal{D}\\right):={\\tau_{2}}^{-1}\\log\\sum_{m=1}^{M}\\exp\\left(\\tau_{2}d_{m}(C_{n}(\\cdot);\\mathcal{D}_{m})\\right)\\!,}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\tau_{2}$ is a parameter, serving the same purpose as $\\tau_{1}$ . ", "page_idx": 6}, {"type": "text", "text": "Here, we demonstrate calculating the derivative of the smooth approximation (17) with respect to each component of the generalized Local score, expanding it as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\langle\\boldsymbol{\\mathit{A}};\\mathcal{D}\\rangle={\\tau_{2}}^{-1}\\log\\sum_{m=1}^{M}\\exp\\Big(\\tau_{2}\\left|\\left|R_{m}\\right|^{-1}\\!\\sum_{j\\in R_{m}}S_{\\tau_{1}}(u(X_{j})-Y_{j})S_{\\tau_{1}}(Y_{j}-l(X_{j}))-(1-\\alpha)\\Big|\\Big),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{S_{\\tau_{1}}(u(X_{j})-Y_{j})=\\left(1+\\exp\\left[-\\tau_{1}(\\mu_{j}+Q_{1-\\alpha}^{s}(\\{E_{i}\\}_{i=1}^{n})\\sigma_{j}-Y_{j})\\right]\\right)^{-1},}\\\\ {S_{\\tau_{1}}(Y_{j}-l(X_{j}))=\\left(1+\\exp\\left[-\\tau_{1}(Y_{j}-\\mu_{j}+Q_{1-\\alpha}^{s}(\\{E_{i}\\}_{i=1}^{n})\\sigma_{j})\\right]\\right)^{-1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "with $\\mu_{i}=\\mu(X_{i})$ , $\\sigma_{i}=\\sigma(X_{i})$ , $E_{i}=|Y_{i}-\\mu_{i}|/\\sigma_{i}$ . As a result, for each feature $X_{i}$ within $\\mathcal{D}$ , we can evaluate $\\partial\\ell_{M}\\left(E;\\boldsymbol{\\mathcal{D}}\\right)/\\partial\\mu_{i}$ and $\\partial\\ell_{M}\\left(E;D\\right)/\\partial\\sigma_{i}$ via the chain rule. ", "page_idx": 6}, {"type": "text", "text": "5.2 Boosting score functions for conditional coverage ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Since the empirical maximum deviation $\\ell_{M}$ (15) is non-differentiable, we opt for the differentiable approximation during the gradient boosting step (11). Nonetheless, we utilize the original $\\ell_{M}$ to select the number of boosting rounds as in step (12) and to evaluate the conditional coverage of the conformalized prediction interval on the test set. ", "page_idx": 6}, {"type": "text", "text": "5.2.1 Theoretical guarantees ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The oracle score function achieving conditional coverage as defined in (13) belongs to both proposed generalized score families. ", "page_idx": 6}, {"type": "text", "text": "Proposition 5.1 (Asymptotic expressiveness). Let $\\{X_{i},Y_{i}\\}_{i=1}^{n}$ be i.i.d. with continuous joint probability density distribution. Under the split conformal procedure, for any target coverage rate $1-\\alpha$ , as $n\\rightarrow\\infty,$ , there exists $(\\mu^{*},\\sigma^{*})$ and $(\\mu_{1}^{*},\\mu_{2}^{*},\\sigma^{*})$ such that the corresponding generalized Local (6) and CQR (9) score functions recover conditional coverage at rate $1-\\alpha$ , as defined in (13). ", "page_idx": 6}, {"type": "text", "text": "It goes without saying that there is no reason to assume that the optimal $\\mu^{*}$ corresponds to the conditional mean, median or any quantile of $Y$ given $X$ , or that the optimal $\\sigma^{*}$ corresponds to the standard deviation or the mean absolute deviation of $Y$ given $X$ , as in the original Local score (3). That said, our greedy strategy has no guarantee on global optimality and this is why the choice of the starting point\u2014whether it is the Local or CQR score function\u2014plays a role in the performance. ", "page_idx": 6}, {"type": "image", "img_path": "Tw032H2onS/tmp/53a777c631c673d0180abd41faf6e44e5ddab343030c0692524446acb72b7bf0.jpg", "img_caption": ["Figure 3: Comparison of test set conditional coverage evaluated on the dataset meps-19: (a) shows the classical Local-type conformal procedure and (b) our boosted Local-type conformal procedure. The target miscoverage rate is set to $\\alpha=10\\%$ (red). Miscoverage rate is computed at each leaf of the contrast tree, constructed to detect deviation from the target rate. Each leaf node is labeled with its size, namely, the fraction of the test set it represents. "], "img_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "Tw032H2onS/tmp/4aff1d6cbf2bc437a6177ade22339f994a9b49711d306c350c1af131de897bda.jpg", "table_caption": ["Table 1: Test set maximum deviation loss $\\ell_{M}$ evaluated on various conformalized intervals. The best result achieved for each dataset is highlighted in bold. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "5.2.2 Empirical results on real data ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We apply our boosted conformal procedure to the 11 datasets previously analyzed in [23, 18, 22]. Details on the datasets are provided in Section A.6 in the Appendix. In each dataset, we randomly hold out $20\\%$ as test data. All experiments are repeated 10 times, starting from the data splitting. We refer to Section A.7 for details on the models and hyper-parameters we employ for the training and boosting stages. ", "page_idx": 7}, {"type": "text", "text": "We evaluate the conditional coverage of the prediction intervals as the maximum within-group deviations across a partitioned test set (15). This partition is obtained through a contrast tree algorithm described in Section 5.1. Figure 3 illustrates the comparison between miscoverage rates of prediction intervals at each leaf of the contrast tree. These intervals are derived under the classical Local conformal procedure and our boosted conformal procedure. Notably, the conditional coverage of the boosted prediction interval more closely aligns with the target rate $1-\\alpha$ . ", "page_idx": 7}, {"type": "text", "text": "The experiment results summarized in Table 1 indicate that applying boosting significantly enhances the performance of the baseline Local procedure. In contrast, boosting on CQR does not yield significant improvements\u2014a sign that CQR already targets conditional coverage. (Before boosting, the prediction intervals generated by the baseline Local procedure exhibit conditional coverage deviations up to three times greater than those of the baseline CQR procedure.) It is noteworthy, however, that after boosting, the conditional coverage of the Local procedure improves to a level comparable to that of the boosted CQR procedure. While generally slightly less effective, nevertheless surpasses the performance of the boosted CQR procedure in two cases. Results on the remaining datasets are deferred to Tables A2 and A3. ", "page_idx": 7}, {"type": "text", "text": "6 Boosting for length ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We begin by specifying the oracle prediction interval with minimum length. For a random variable $Z$ , the High Density Region (HDR) at a specified significance level $\\alpha$ , denoted as $\\mathrm{HDR}_{\\alpha}(Z)$ , is defined as the shortest deterministic interval that covers $Z$ with probability at least $1\\mathrm{~-~}\\alpha$ . The boundaries of $\\mathrm{HDR}_{\\alpha}(Z)$ , the lower limit $Q_{l(\\alpha)}$ and the upper limit $Q_{u(\\alpha)}$ , obey the condition $\\mathbb{P}(Z\\in$ $[Q_{l(\\alpha)},Q_{u(\\alpha)}])\\geq1-\\alpha$ . For a pair of $(X,Y)$ drawn from $P$ , for every value of $x\\in\\mathbb{R}^{p}$ , the oracle prediction interval at that point is expressed as ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathrm{HDR}_{\\alpha}(Y|X=x)=\\left[Q_{l(\\alpha)}(Y|X=x),Q_{u(\\alpha)}(Y|X=x)\\right].\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Before introducing the boosting strategy, we present a word of caution against optimizing exclusively for this objective. Importantly, to maintain valid marginal coverage, the shortest prediction interval is prone to overcover when the spread of $Y|X$ (the conditional distribution of $Y$ given $X$ ) is small, and undercover when the spread of $Y|X$ is large. This may be undesirable. ", "page_idx": 8}, {"type": "text", "text": "Similar to Proposition 5.1, we can show that the generalized score families exhibit the necessary expressiveness to contain the oracle conformity score, achieving optimal length while ensuring valid marginal coverage. The formal proof is deferred to Section A.3. ", "page_idx": 8}, {"type": "text", "text": "6.1 A measure for length ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Consider a dataset $D\\,=\\,\\{(X_{i},Y_{i})\\}_{i=1}^{n}$ and a score function $E$ . Denote the corresponding conformalized prediction interval by $C_{n}(\\cdot)$ , with its quality measured by the average length: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\ell_{L}(E;\\mathcal{D})=n^{-1}{\\sum_{i=1}^{n}}|C_{n}(X_{i})|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "To derive a differentiable approximation of $\\ell_{L}$ , we approximate the empirical quantile $Q_{1-\\alpha}$ in the conformalized intervals (7) and (10) with the smooth quantile estimator $Q_{1-\\alpha}^{s}$ constructed in (16). Here, we demonstrate calculating the derivative of the smooth approximation of $\\ell_{L}$ with respect to each component of the generalized Local score, expanding it as follows based on the previously outlined approximation steps: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\ell_{L}\\left(E;\\mathcal{D}\\right)=2\\bar{\\sigma}Q_{1-\\alpha}^{s}(\\{E_{i}\\}_{i=1}^{n}),\\qquad E_{i}=|Y_{i}-\\mu_{i}|/\\sigma_{i},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "with $\\mu_{i}\\,=\\,\\mu(X_{i})$ $\\begin{array}{r}{\\mu(X_{i}),\\,\\sigma_{i}=\\sigma(X_{i}),\\,\\bar{\\sigma}=n^{-1}\\sum_{i=1}^{n}\\sigma_{i}}\\end{array}$ . As a result, for each feature $X_{i}$ within $\\mathcal{D}$ , we can evaluate $\\partial\\ell_{L}\\left(E;\\boldsymbol{\\mathcal{D}}\\right)/\\partial\\mu_{i}$ and $\\partial\\ell_{L}\\left(E;\\boldsymbol{\\mathcal{D}}\\right)/\\partial\\sigma_{i}$ via the chain rule. For instance, ", "page_idx": 8}, {"type": "equation", "text": "$$\n{\\frac{\\partial\\ell_{L}\\left(E;{\\mathcal{D}}\\right)}{\\partial\\mu_{i}}}=-2{\\bar{\\sigma}}{\\frac{\\partial Q_{1-\\alpha}^{s}(\\{E_{j}\\}_{j=1}^{n})}{\\partial E_{i}}}{\\frac{\\mathrm{sign}(Y_{i}-\\mu_{i})}{\\sigma_{i}}}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "6.2 Empirical results on real data ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We apply our boosted conformal procedure to the same datasets described in Section 5.2.2. Detailed information on the models and hyperparameters used during the training and boosting stages can be found in Section A.7. Partial experiment results are summarized in Table 2. Notably, the boosting performance highlighted in bold exhibits significant improvement compared to previously documented results [17, 23]. We see a pronounced enhancement with the blog dataset; before boosting, the Local prediction intervals are on average $42\\%$ longer than those generated by CQR. After boosting, these intervals outperform the boosted CQR intervals by $32\\%$ . Using CQR as the baseline also yields substantial improvements, a decrease in averaged length exceeding $10\\%$ in six out of the eleven datasets. The meps-21 dataset, in particular, shows an improvement of up to $18\\%$ relative to the baseline. Results on the remaining datasets can be found in Tables A4 and A5. Figure 4 compares the conformalized prediction intervals derived from baseline Local and CQR scores with those obtained from the boosted scores. To effectively visualize the impact of boosting, we conduct a regression tree analysis on the training set to predict the label $Y$ , setting the maximum number of tree nodes to four. This regression tree is then applied to the test set, allowing for a detailed comparison of the prediction intervals across each of the four distinct leaves. ", "page_idx": 8}, {"type": "text", "text": "7 Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We introduced a post-training conformity score boosting scheme aiming to optimize for conditional coverage or length of the conformalized prediction interval. An intriguing avenue for future exploration involves simultaneously optimizing both length and conditional coverage, potentially trading ", "page_idx": 8}, {"type": "image", "img_path": "Tw032H2onS/tmp/75911a8c72defd78613b87fe9e042d489a1aa766d14cca4762a01fd34183b536.jpg", "img_caption": ["regression tree leaf "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Figure 4: Comparison of test set average interval length evaluated on the meps-19 and blog datasets: classical Local and CQR conformal procedure versus the boosted procedures (abbreviated as \u2018Localb\u2019 and \u2018CQRb\u2019) compared in each of the 4 leaves of a regression tree trained on the training set to predict the label $Y$ . A positive log ratio value between the regular and boosted interval lengths indicates improvement from boosting. The target miscoverage rate is set at $\\alpha=10\\%$ . ", "page_idx": 9}, {"type": "table", "img_path": "Tw032H2onS/tmp/4a6f312374cff8d923d211e0c0983bd2d1c9497f62170bad09826d9982f28d12.jpg", "table_caption": ["Table 2: Test set average interval length $\\ell_{L}$ evaluated on various conformalized prediction intervals. The best result achieved for each dataset is highlighted in bold. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "off these objectives by incorporating user-specified weights [29]. Additionally, we can readily adapt our procedure to meet various application-specific objectives. For instance, we can optimize for conditional coverage on predefined feature groups, a common task in enhancing fairness in distributing social resources across different demographic groups [27]. Similarly, we can modify our procedure to reduce the length of prediction intervals for predefined label groups, which can be seen as reallocating resources to decrease uncertainty for certain groups at the expense of higher uncertainty for other groups [26]. Candidate loss functions tailored to these objectives are detailed in Section A.1. Lastly, the primary emphasis of this paper centers on the design of the conformity score boosting scheme and formalizing the optimization of conditional coverage in mathematical terms, leaving room for computational optimization to enhance performance and runtime efficiency. In essence, the gradient boosting algorithm in our procedure can be replaced with any gradient-based machine learning model. Thus, another interesting future direction would be to explore whether alternative algorithms could enhance performance. ", "page_idx": 9}, {"type": "text", "text": "Acknowlegements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "E.C. was supported by the Office of Naval Research under Grant No. N00014-24-1-2305, the National Science Foundation under Grant No. DMS2032014, and the Simons Foundation under Award 814641. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Frank E. Harrell and C. E. Davis. \u201cA New Distribution-Free Quantile Estimator\u201d. In: Biometrika 69.3 (1982), pp. 635\u2013640. ISSN: 00063444. URL: http://www.jstor.org/ stable/2335999.   \n[2] Stephen P Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.   \n[3] \u201cClassification with conformal predictors\u201d. In: Algorithmic Learning in a Random World. Boston, MA: Springer US, 2005, pp. 53\u201396. ISBN: 978-0-387-25061-8. DOI: 10.1007/0- 387-25061-1_3. URL: https://doi.org/10.1007/0-387-25061-1_3.   \n[4] Vladimir Vovk, Alexander Gammerman, and Glenn Shafer. Algorithmic learning in a random world. Vol. 29. Springer, 2005. [5] I-Cheng Yeh. Concrete Compressive Strength. UCI Machine Learning Repository. DOI: https://doi.org/10.24432/C5PK67. 2007.   \n[6] C.M. Achilles et al. Tennessee\u2019s student teacher achievement ratio (STAR) project. 2008.   \n[7] Michael Redmond. Communities and Crime. UCI Machine Learning Repository. DOI: https://doi.org/10.24432/C53W3X. 2009.   \n[8] Vladimir Vovk. \u201cConditional validity of inductive conformal predictors\u201d. In: Asian conference on machine learning. PMLR. 2012, pp. 475\u2013490.   \n[9] Hadi Fanaee-T. Bike Sharing Dataset. UCI Machine Learning Repository. DOI: https://doi.org/10.24432/C5W894. 2013.   \n[10] Prashant Rana. Physicochemical Properties of Protein Tertiary Structure. UCI Machine Learning Repository. DOI: https://doi.org/10.24432/C5QW3H. 2013.   \n[11] Krisztian Buza. BlogFeedback. UCI Machine Learning Repository. DOI: https://doi.org/10.24432/C58S3F. 2014.   \n[12] Tianqi Chen and Carlos Guestrin. \u201cXgboost: A scalable tree boosting system\u201d. In: Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining. 2016, pp. 785\u2013794.   \n[13] Kamaljot Singh. Facebook Comment Volume Dataset. UCI Machine Learning Repository. DOI: https://doi.org/10.24432/C5Q886. 2016.   \n[14] Andre Esteva et al. \u201cDermatologist-level classification of skin cancer with deep neural networks\u201d. In: nature 542.7639 (2017), pp. 115\u2013118.   \n[15] Guolin Ke et al. \u201cLightgbm: A highly efficient gradient boosting decision tree\u201d. In: Advances in neural information processing systems 30 (2017).   \n[16] Jing Lei et al. \u201cDistribution-Free Predictive Inference for Regression\u201d. In: Journal of the American Statistical Association 113.523 (2018), pp. 1094\u20131111. DOI: 10.1080/01621459. 2017.1307116. eprint: https://doi.org/10.1080/01621459.2017.1307116. URL: https://doi.org/10.1080/01621459.2017.1307116.   \n[17] Yaniv Romano, Evan Patterson, and Emmanuel Candes. \u201cConformalized quantile regression\u201d. In: Advances in neural information processing systems 32 (2019).   \n[18] Yaniv Romano, Evan Patterson, and Emmanuel J. Cande\\`s. \u201cConformalized Quantile Regression\u201d. In: Proceedings of the 33rd International Conference on Neural Information Processing Systems. Red Hook, NY, USA: Curran Associates Inc., 2019.   \n[19] Mathieu Blondel et al. \u201cFast differentiable sorting and ranking\u201d. In: International Conference on Machine Learning. PMLR. 2020, pp. 950\u2013959.   \n[20] Rina Foygel Barber et al. \u201cThe limits of distribution-free conditional predictive inference\u201d. In: Information and Inference: A Journal of the IMA 10.2 (Aug. 2020), pp. 455\u2013482. ISSN: 2049-8772. DOI: 10.1093/imaiai/iaaa017. eprint: https://academic.oup.com/ imaiai/article-pdf/10/2/455/38549621/iaaa017.pdf. URL: https://doi.org/ 10.1093/imaiai/iaaa017.   \n[21] Jerome H. Friedman. \u201cContrast trees and distribution boosting\u201d. In: Proceedings of the National Academy of Sciences 117.35 (2020), pp. 21175\u201321184. DOI: 10 . 1073 / pnas . 1921562117. eprint: https://www.pnas.org/doi/pdf/10.1073/pnas.1921562117. URL: https://www.pnas.org/doi/abs/10.1073/pnas.1921562117.   \n[22] Danijel Kivaranovic, Kory D. Johnson, and Hannes Leeb. Adaptive, Distribution-Free Prediction Intervals for Deep Networks. 2020. arXiv: 1905.10634 [stat.ML].   \n[23] Matteo Sesia and Emmanuel J Cande\\`s. \u201cA comparison of some conformal quantile regression methods\u201d. In: Stat 9.1 (2020), e261.   \n[24] John Jumper et al. \u201cHighly accurate protein structure prediction with AlphaFold\u201d. In: Nature 596.7873 (2021), pp. 583\u2013589.   \n[25] Kuang-Ting Ko et al. \u201cStructure of the malaria vaccine candidate Pfs48/45 and its recognition by transmission blocking antibodies\u201d. In: Nature Communications 13.1 (2022), p. 5603.   \n[26] David Stutz et al. \u201cLearning Optimal Conformal Classifiers\u201d. In: International Conference on Learning Representations. 2022. URL: https://openreview.net/forum?id=t8O4LKFVx.   \n[27] Isaac Gibbs, John J. Cherian, and Emmanuel J. Cande\\`s. Conformal Prediction With Conditional Guarantees. 2023. arXiv: 2305.12616 [stat.ME].   \n[28] Kexin Huang et al. Uncertainty Quantification over Graph with Conformalized Graph Neural Networks. 2023. arXiv: 2305.14535 [cs.LG].   \n[29] Lahav Dabah and Tom Tirer. On Temperature Scaling and Conformal Prediction of Deep Classifiers. 2024. arXiv: 2402.05806 [cs.LG]. URL: https://arxiv.org/abs/2402. 05806.   \n[30] Medical expenditure panel survey, panel 19. URL: https://meps.ahrq.gov/mepsweb/ data_stats/download_data_files_detail.jsp?cboPufNumber $\\fallingdotseq$ HC-181.   \n[31] Medical expenditure panel survey, panel 20. URL: https://meps.ahrq.gov/mepsweb/ data_stats/download_data_files_detail.jsp?cboPufNumber=HC-181.   \n[32] Medical expenditure panel survey, panel 21. URL: https://meps.ahrq.gov/mepsweb/ data_stats/download_data_files_detail.jsp?cboPufNumber $=$ HC-192. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1 Candidate loss functions for additional application-specific objectives ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Conditional coverage on predefined feature groups: this task can be viewed as a specialized application within our broader strategy of boosting for conditional coverage, as detailed in Section 5. There, the primary challenge was to develop a loss function that accurately measures deviations from the target conditional coverage rate. We achieved this by using contrast trees to identify partitions in the feature space that maximize these deviations, effectively identifying subgroups in need of protection. This process is simplified when the partitions correspond to prespecified groups, allowing us to continue using the empirical maximum deviation as a candidate loss function. ", "page_idx": 12}, {"type": "text", "text": "Consider a dataset $D\\,=\\,\\{(X_{i},Y_{i})\\}_{i=1}^{n}$ and a score function $E$ . Denote by $C_{n}(\\cdot)$ the conformalized prediction interval constructed from $E$ . Let $G_{1},\\allowbreak\\dots,G_{M}$ be prespecified feature index groups. Within each set $D_{i}=\\{(X_{j},Y_{j})\\}_{j\\in G_{i}}$ , compute the absolute deviation $d_{i}$ as ", "page_idx": 12}, {"type": "equation", "text": "$$\nd_{i}\\left(C_{n}(\\cdot);\\mathcal{D}_{i}\\right)=\\left|\\frac{1}{|G_{i}|}{\\sum_{j\\in G_{i}}}\\mathbb{1}(Y_{j}\\in C_{n}(X_{j}))-(1-\\alpha)\\right|.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "The overall empirical maximum deviation is then defined as ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\ell\\left(E;\\mathcal{D}\\right)=\\operatorname*{max}_{1\\leq i\\leq M}\\!d_{i}\\left(C_{n}(\\cdot);\\mathcal{D}_{i}\\right)\\!.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Interval length conditional on predefined label groups: for a dataset $D\\,=\\,\\{(X_{i},Y_{i})\\}_{i=1}^{n}$ and a score function $E$ , let $\\mathcal{D}_{1},\\ldots,\\mathcal{D}_{M}$ be the prespecified label groups. A natural minimization objective for balancing uncertainty among these groups is defined as: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\ell(E;\\mathcal{D})=\\sum_{i=1}^{M}w_{i}\\frac{1}{\\sum_{j=1}^{n}\\mathbb{1}(Y_{j}\\in\\mathcal{Y}_{i})}\\sum_{j=1}^{n}\\mathbb{1}(Y_{j}\\in\\mathcal{Y}_{i})|C_{n}(X_{j})|,\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $\\left(w_{1},\\dots,w_{M}\\right)$ represents a set of user-specified weights. ", "page_idx": 12}, {"type": "text", "text": "A.2 Proof of Proposition 5.1 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Our proof relies on the following lemma. ", "page_idx": 12}, {"type": "text", "text": "Lemma A.1 (Expressiveness). Given any sample pair $X$ and $Y$ with a continuous joint probability density distribution, and a prediction interval $[c_{l}(\\cdot),c_{u}(\\cdot)]$ with marginal coverage equal to $1-\\alpha,$ there exist specific function sets: $(\\mu(\\cdot),\\sigma(\\cdot))$ for the Local type, and $(\\mu_{1}(\\cdot),\\mu_{2}(\\cdot),\\tilde{\\sigma}(\\cdot))$ for the CQR type, such that asymptotically: ", "page_idx": 12}, {"type": "text", "text": "1. The conformalized prediction interval (7), derived using the generalized Local type conformity score $f_{\\mu,\\sigma}$ , accurately recovers $[c_{l}(\\cdot),c_{u}(\\cdot)]$ .   \n2. Similarly, the conformalized prediction interval $(I O)$ , based on the generalized CQR type conformity score $E_{\\mu_{1},\\mu_{2},\\tilde{\\sigma}}$ , also recovers $[c_{l}(\\cdot),c_{u}(\\cdot)]$ . ", "page_idx": 12}, {"type": "text", "text": "Proof of Lemma A.1. Recall that the generalized Local score (6) characterized by $(\\mu,\\sigma)$ takes the form ", "page_idx": 12}, {"type": "equation", "text": "$$\nE_{\\mu,\\sigma}(x,y)=\\frac{|y-\\mu(x)|}{\\sigma(x)}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Asymptotically, the conformalized prediction interval is given by ", "page_idx": 12}, {"type": "equation", "text": "$$\n[\\mu(X)-Q_{1-\\alpha}(E_{\\mu,\\sigma})\\sigma(X),\\mu(X)+Q_{1-\\alpha}(E_{\\mu,\\sigma})\\sigma(X)]\\,.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Here, $Q_{1-\\alpha}$ represents the population quantile. Set ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mu(x)=\\frac{c_{l}(x)+c_{u}(x)}{2},\\quad\\sigma(x)=\\frac{c_{u}(x)-c_{l}(x)}{2}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "By assumption, we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbb{P}(Y\\in[c_{l}(X),c_{u}(X)])=1-\\alpha.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "With a simple change of variables, the above inequality is equivalent to ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left|\\frac{Y-\\mu(X)}{\\sigma(X)}\\right|\\leq1\\right)=1-\\alpha.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "In other words, this is equivalent to ", "page_idx": 13}, {"type": "equation", "text": "$$\nQ_{1-\\alpha}(E_{\\mu,\\sigma})=1.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We have thus proved the result for the generalized Local type conformity score $E_{\\mu,\\sigma}$ . In the same spirit, we can prove the result for the generalized CQR type conformity score $E_{\\mu_{1},\\mu_{2},\\tilde{\\sigma}}$ by taking ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mu_{1}=c_{l}+\\displaystyle\\frac{c_{u}(x)-c_{l}(x)}{2},\\quad\\mu_{2}=c_{u}-\\displaystyle\\frac{c_{u}(x)-c_{l}(x)}{2},\\quad\\tilde{\\sigma}=\\displaystyle\\frac{c_{u}(x)-c_{l}(x)}{2}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Recall that a generalized CQR score function (9) characterized by $(\\mu_{1},\\,\\mu_{2},\\,\\sigma)$ is defined as: ", "page_idx": 13}, {"type": "equation", "text": "$$\nE_{\\mu_{1},\\mu_{2},\\sigma}(x,y)=\\operatorname*{max}\\left\\lbrace\\mu_{1}(x)-y,y-\\mu_{2}(x)\\right\\rbrace/\\sigma(x),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "which leads to the asymptotic conformalized prediction intervals of the form ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\bigl[\\mu_{1}(X)-\\sigma(X)Q_{1-\\alpha}(E_{\\mu_{1},\\mu_{2},\\sigma}),\\mu_{2}(X)+\\sigma(X)Q_{1-\\alpha}(E_{\\mu_{1},\\mu_{2},\\sigma})\\bigr].\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Plugging in $\\mu_{1},\\mu_{2},\\tilde{\\sigma}$ defined above, we immediately have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{P}(Y\\in[c_{l}(X),c_{u}(X)])=1-\\alpha}\\\\ &{\\Longleftrightarrow\\mathbb{P}(c_{l}(X)-Y\\leq0,Y-c_{u}(X)\\leq0)=1-\\alpha}\\\\ &{\\Longleftrightarrow\\mathbb{P}\\left(\\frac{c_{l}(X)-Y+\\tilde{\\sigma}(X)}{\\tilde{\\sigma}(X)}\\leq1,\\frac{Y-c_{u}(X)+\\tilde{\\sigma}(X)}{\\tilde{\\sigma}(X)}\\leq1\\right)=1-\\alpha}\\\\ &{\\Longleftrightarrow\\ Q_{1-\\alpha}(E_{\\mu_{1},\\mu_{2},\\sigma}(x,y))=1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof of Proposition 5.1. It suffices to take $c_{l}(x)=Q_{\\alpha/2}(Y|X=x),c_{u}(x)=Q_{1-\\alpha/2}(Y|X=x)$ and apply Lemma A.1. ", "page_idx": 13}, {"type": "text", "text": "A.3 Boosting for length: theoretical guarantees ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Similar to Proposition 5.1, we show in Proposition A.2 below that the generalized Local and CQR score families exhibit the necessary expressiveness to contain the oracle score, achieving optimal length while ensuring valid marginal coverage. ", "page_idx": 13}, {"type": "text", "text": "Proposition A.2 (Asymptotic expressiveness). Under the assumptions of Proposition 5.1, for any target coverage rate $1-\\alpha$ , as $n\\to\\infty$ , the following statements hold true: ", "page_idx": 13}, {"type": "text", "text": "1. There exists $(\\mu^{*},\\sigma^{*})$ such that the corresponding generalized Local score function (6) recovers the shortest oracle prediction interval (18).   \n2. There exists $(\\mu_{1}^{*},\\mu_{2}^{*},\\sigma^{*})$ such that the corresponding generalized CQR score function (9) recovers the shortest oracle prediction interval (18). ", "page_idx": 13}, {"type": "text", "text": "Proof of Proposition $A.2$ . It suffices to take $c_{l}(x)=Q_{l(\\alpha)}(Y|X=x)$ , $c_{u}(x)=Q_{u(\\alpha)}(Y|X=x)$ and apply Lemma A.1, where $Q_{l(\\alpha)}$ and $Q_{u(\\alpha)}$ are the lower and upper limits of the High Density Region defined in (18). \u53e3 ", "page_idx": 13}, {"type": "text", "text": "A.4 CQR type conformity score boosting ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A generalized CQR score function (9) is uniquely defined by a triple $(\\mu_{1}(\\cdot),\\mu_{2}(\\cdot),\\sigma(\\cdot))$ . We will show how searching for a generalized CQR score can be reduced to searching for a Local generalized score. To begin with, we shall say that score functions are equivalent if they recover identical conformalized prediction intervals. ", "page_idx": 13}, {"type": "text", "text": "Definition A.3. Let $\\{X_{i},Y_{i}\\}_{i=1}^{n}$ , $(X_{n+1},Y_{n+1})$ be i.i.d. with continuous joint probability density distribution, and let $[n]$ be partitioned into a training set $I_{1}$ and a calibration set $I_{2}$ . Consider two conformity score functions, $E_{1}$ and $E_{2}$ , which produce conformalized prediction intervals $C_{1}(\\cdot)$ and $C_{2}(\\cdot)$ , respectively. For any target coverage rate $1-\\alpha,\\,E_{1}$ and $E_{2}$ are equivalent if $C_{1}(\\cdot)=C_{2}(\\cdot)$ when marginal coverage rates $\\mathbb{P}(Y_{n+1}\\in C_{1}(X_{n+1}))$ and $\\mathbb{P}(Y_{n+1}\\in C_{2}(X_{n+1}))$ match. ", "page_idx": 13}, {"type": "text", "text": "Building on this definition, we are now equipped to establish the following equivalences: ", "page_idx": 14}, {"type": "text", "text": "Lemma A.4. Under the assumptions of Definition A.3, the following statements hold: ", "page_idx": 14}, {"type": "text", "text": "1. For the CQR- $r$ score function defined in Section 2, there is an equivalent generalized Local score function characterized by a pair $(\\mu(\\cdot),\\sigma(\\cdot))$ , where $\\mu=(\\mu_{1}+\\mu_{2})/2$ , $\\sigma\\,=\\,(\\mu_{2}\\mathrm{~-~}$ $\\mu_{1})/2$ . 2. For any generalized Local score function characterized by the pair $(\\mu(\\cdot),\\sigma(\\cdot))$ , there is an equivalent generalized CQR score function characterized by a triple $(\\mu(\\cdot),\\mu(\\cdot),\\sigma(\\cdot))$ . ", "page_idx": 14}, {"type": "text", "text": "The proof of the above Lemma is deferred to Section A.5. Leveraging these equivalences, we carry out the boosted conformal procedure as follows: first, we initialize a triple $\\bar{\\mu}_{1}^{(0)}\\,=\\,\\hat{q}_{\\alpha/2}$ = q\u02c6\u03b1/2, \u00b52 $\\mu_{2}^{(0)}\\,=$ $\\hat{q}_{1-\\alpha/2}$ , $\\sigma_{1}^{(0)}\\,=\\,\\hat{q}_{1-\\alpha/2}\\,-\\,\\hat{q}_{\\alpha/2}$ , which characterizes the CQR-r score function. Next, we find an equivalent generalized Local score function characterized by a pair $(\\mu^{(0)},\\sigma^{(0)})$ chosen according to Lemma A.4. After $\\tau$ boosting rounds, we obtain the boosted pair $(\\mu^{(\\tau)},\\sigma^{(\\tau)})$ and the corresponding score function. Finally, we recover an equivalent generalized CQR score function ", "page_idx": 14}, {"type": "equation", "text": "$$\n{\\cal E}^{(\\tau)}(x,y)=\\mathrm{max}\\left\\{\\mu_{1}^{(\\tau)}(x)-y,{y}-{\\mu_{2}}^{(\\tau)}(x)\\right\\}/\\sigma_{1}^{(\\tau)}(x),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "characterized by the triple $(\\mu_{1}{}^{(\\tau)},\\mu_{2}{}^{(\\tau)},\\sigma_{1}{}^{(\\tau)})$ chosen according to Lemma A.4. ", "page_idx": 14}, {"type": "text", "text": "A.5 Proof of Lemma A.4 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Recall that the generalized Local score (6) characterized by $(\\mu,\\sigma)$ takes the form ", "page_idx": 14}, {"type": "equation", "text": "$$\nE_{\\mu,\\sigma}(x,y)=\\frac{|y-\\mu(x)|}{\\sigma(x)}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The conformalized prediction interval is given by ", "page_idx": 14}, {"type": "equation", "text": "$$\n[\\mu(X)-Q_{1-\\alpha}(E_{\\mu,\\sigma},I_{2})\\sigma(X),\\mu(X)+Q_{1-\\alpha}(E_{\\mu,\\sigma},I_{2})\\sigma(X)]\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "A generalized CQR score function (9) characterized by $(\\mu_{1},\\,\\mu_{2},\\,\\sigma)$ is defined as: ", "page_idx": 14}, {"type": "equation", "text": "$$\nE_{\\mu_{1},\\mu_{2},\\sigma}(x,y)=\\operatorname*{max}\\left\\lbrace\\mu_{1}(x)-y,y-\\mu_{2}(x)\\right\\rbrace/\\sigma(x),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which leads to conformalized prediction intervals of the form ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\lbrack\\mu_{1}(X)-\\sigma(X)Q_{1-\\alpha}(E_{\\mu_{1},\\mu_{2},\\sigma},I_{2}),\\mu_{2}(X)+\\sigma(X)Q_{1-\\alpha}(E_{\\mu_{1},\\mu_{2},\\sigma},I_{2})].\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Plugging in the triple $\\mu_{1}(x)\\,=\\,\\hat{q}_{\\alpha/2}$ , $\\mu_{2}(x)\\,=\\,\\hat{q}_{1-\\alpha/2}$ , $\\sigma_{1}(x)\\,=\\,\\hat{q}_{1-\\alpha/2}\\,-\\,\\hat{q}_{\\alpha/2}$ , which characterize the CQR-r score function, we have the conformalized prediction interval ", "page_idx": 14}, {"type": "equation", "text": "$$\n[\\mu_{1}(X)-\\sigma_{1}(X)Q_{1-\\alpha}(E_{\\mu_{1},\\mu_{2},\\sigma_{1}},I_{2}),\\mu_{2}(X)+\\sigma_{1}(X)Q_{1-\\alpha}(E_{\\mu_{1},\\mu_{2},\\sigma_{1}},I_{2})].\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Set ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mu(X)={\\frac{\\mu_{1}(X)+\\mu_{2}(X)}{2}},\\sigma(X)={\\frac{\\mu_{2}(X)-\\mu_{1}(X)}{2}},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "then the generalized Local conformity score $E_{\\mu,\\sigma}(x,y)=|y-\\mu(x)|/\\sigma(x)$ recovers conformalized prediction intervals of the form ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~~[\\mu(X)-\\sigma(X)Q(E_{\\mu,\\sigma},I_{2}),\\mu(X)+\\sigma(X)Q(E_{\\mu,\\sigma},I_{2})]}\\\\ &{=\\left[\\frac{\\mu_{1}(X)+\\mu_{2}(X)}{2}-\\frac{\\mu_{2}(X)-\\mu_{1}(X)}{2}Q(E_{\\mu,\\sigma},I_{2}),\\right.}\\\\ &{~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\left.\\frac{\\mu_{1}(X)+\\mu_{2}(X)}{2}+\\frac{\\mu_{2}(X)-\\mu_{1}(X)}{2}Q(E_{\\mu,\\sigma},I_{2})\\right]}\\\\ &{=\\left[\\mu_{2}(X)-(\\mu_{2}(X)-\\mu_{1}(X))\\frac{Q(E_{\\mu,\\sigma},I_{2})-1}{2},\\right.}\\\\ &{~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\left.\\mu_{1}(X)+(\\mu_{2}(X)-\\mu_{1}(X))\\frac{Q(E_{\\mu,\\sigma},I_{2})-1}{2}\\right]}\\\\ &{=\\left[\\mu_{1}(X)-\\sigma_{1}(X)\\frac{Q(E_{\\mu,\\sigma},I_{2})-1}{2},\\mu_{2}(X)+\\sigma_{1}(X)\\frac{Q(E_{\\mu,\\sigma},I_{2})-1}{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "From the monotonicity of the interval lengths with respect to the empirical quantiles, we have that the two score functions are equivalent by Definition A.3. ", "page_idx": 14}, {"type": "text", "text": "2. Let a generalized Local score function be $E_{\\mu,\\sigma}(x,y)=|y-\\mu(x)|/\\sigma(x)$ . Then it suffices to observe that ", "page_idx": 15}, {"type": "equation", "text": "$$\n|y-\\mu(x)|=\\operatorname*{max}\\{y-\\mu(z),\\mu(x)-y\\}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "A.6 Additional information on real datasets ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In Table A1, we provide the predicted label, dimensions, and source for each dataset. Data cleaning and preprocessing are in accordance with the methods described by Romano et al. [17]. ", "page_idx": 15}, {"type": "table", "img_path": "Tw032H2onS/tmp/0623f262cf8c5a7837b12207adde39e4f7b89922a82dc148f1305a8d2abf583b.jpg", "table_caption": ["Table A1: Datasets for our empirical analyses, with the predicted label, number of samples $(n)$ , and features $(d)$ . "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "All datasets, except for the meps and star data sets, are licensed under CC-BY 4.0. The Medical Expenditure Panel Survey (meps) data is subject to copyright and usage rules. The licensing status of the star dataset could not be determined. ", "page_idx": 15}, {"type": "text", "text": "A.7 Experimental Setup ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In each dataset, we randomly hold out $20\\%$ as test data. The remaining data is divided into a training set and a calibration set, each taking up a proportion of $\\gamma$ and $1-\\gamma$ . We explore training ratios $\\gamma$ ranging from $10\\%$ to $90\\%$ . Results corresponding to the optimal value of the hyperparameter $\\gamma$ are recorded in Table 1, following the practice of Sesia er al. [23]. ", "page_idx": 15}, {"type": "text", "text": "In the training stage, we employ the random forest regressor from Python\u2019s scikit-learn package to learn the baseline Local score function. The hyperparameters are the package defaults, except for the total number of trees, which we set to 1000, and the minimum number of samples required at a leaf node, which we set to 40, as recommended by Romano et al. [17]. For the baseline CQR score function, we adopt a black-box neural network quantile regressor with three fully connected layers and ReLU non-linearities, following the practice of Sesia et al. [23]. In the boosting stage, we set the hyper-parameters $\\tau_{1}$ , $\\tau_{2}$ in the approximated loss (17) to 50. The approximated loss is then passed to the Gradient Boosting Machine from Python\u2019s XGBoost package along with a base conformity score. We set the maximum tree depth to 1 to avoid overfitting and perform cross-validation for the number of boosting rounds, as outlined in Section 3. All other hyperparameters are set to package defaults. ", "page_idx": 15}, {"type": "text", "text": "All experiments were conducted on a dual-socket AMD EPYC 7502 32-Core Processor system, utilizing 8 of its 128 CPUs each time. The runtime for each dataset and random seed varies by dataset size, ranging from 10 minutes to 5 hours. ", "page_idx": 15}, {"type": "text", "text": "A.8 Additional results and error bars ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In Tables A2 to A5, we present additional results on marginal coverage, maximum conditional coverage deviation $(\\ell_{M})$ , and average interval length $(\\ell_{P})$ for each real dataset (including those not reported in Tables 1 and 2), both before and after boosting. Notably, in each case, boosting is applied to optimize either conditional coverage or average interval length. As a result, the nontargeted characteristic may or may not improve after boosting. ", "page_idx": 15}, {"type": "text", "text": "Table A2: Additional information on conformalized intervals obtained before and after boosting for conditional coverage with the Local conformity score as baseline. The target miscoverage rate is set to $\\alpha=10\\%$ . ", "page_idx": 16}, {"type": "table", "img_path": "Tw032H2onS/tmp/b9c4f471a7b346a9faf82b85b7643e4417641b4fcfebe00f3c072429aa9e43bd.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "Tw032H2onS/tmp/3cff0a78e2f0baddf3707b1142c90bb69912a54461837517ad49ac22f20054b3.jpg", "table_caption": ["Table A3: Additional information on conformalized intervals obtained before and after boosting for conditional coverage with the CQR conformity score as baseline. The target miscoverage rate is set to $\\alpha=10\\%$ . "], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "Tw032H2onS/tmp/210936354280ee92a7c2aa1d7446a509a1cc75825fe61661cd4ee00fc73d7612.jpg", "table_caption": ["Table A4: Additional information on conformalized intervals obtained before and after boosting for length with the Local conformity score as baseline. The target miscoverage rate is set to $\\alpha=1\\bar{0}\\%$ . "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Table A5: Additional information on conformalized intervals obtained before and after boosting for length with the CQR conformity score as baseline. The target miscoverage rate is set to $\\alpha=10\\%$ . ", "page_idx": 17}, {"type": "table", "img_path": "Tw032H2onS/tmp/dbbadc175f981427bb61e330cfeb958f8665b4d8342481bdba684a7c2abb4d25.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "We have previously reported the evaluated losses $\\ell_{M}$ and $\\ell_{L}$ for each dataset, averaged over ten random seeds. Tables A6 and A7 below detail the distribution of these evaluations, providing the mean, $10\\%$ quantile, and $90\\%$ quantile for the test set deviations in conditional coverage $(\\ell_{M})$ and average interval length $(\\ell_{L})$ . These statistics are derived from 110 test set evaluations across 11 datasets and 10 random training-test splits. We opt to report empirical quantiles instead of standard deviations due to the asymmetric and non-Gaussian nature of the data. ", "page_idx": 17}, {"type": "table", "img_path": "Tw032H2onS/tmp/bcefa3480f3b9b0108d6bf892cedea5943909c0d45c991f83e929d51e61aa1a9.jpg", "table_caption": ["Table A6: Distribution of the test set conditional coverage deviation $\\ell_{M}$ evaluated on various conformalized prediction intervals across 11 datasets and 10 random training-test splits. "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "Tw032H2onS/tmp/f821e42626a3dd8c339ddf4d204bc5969572262eabab04cbaf06fcf8a2a6df73.jpg", "table_caption": ["Table A7: Distribution of the test set average interval length $\\ell_{L}$ evaluated on various conformalized prediction intervals across 11 datasets and 10 random training-test splits. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "A.9 Experiments under different miscoverage rates ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In Tables A8 and A9, we illustrate the performance of boosting for length with the Local conformity score as baseline with miscoverage rates set to $5\\%$ and $20\\%$ , respectively. ", "page_idx": 17}, {"type": "table", "img_path": "Tw032H2onS/tmp/062bd1863c5d71646172d7e9b112a1d8ead34e9b814737e94a6be41935f3780a.jpg", "table_caption": ["Table A8: Additional information on conformalized intervals obtained before and after boosting for length with the Local conformity score as baseline. The target miscoverage rate is set to $\\alpha=5\\%$ . "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "Tw032H2onS/tmp/e14ba6dffc4bcb5c355de91a6300953feb5299f47cf40740d774b5f0d24a07d7.jpg", "table_caption": ["Table A9: Additional information on conformalized intervals obtained before and after boosting for length with the Local conformity score as baseline. The target miscoverage rate is set to $\\alpha=2\\bar{0}\\%$ . "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "A.10 Training a gradient boosting algorithm with our custom loss functions ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Our proposed boosted conformal procedure serves as a post-training step designed to refine the conformity score $E(\\cdot,\\cdot;f)$ obtained during model training. This procedure can leverage pre-trained models when available. In the absence of pre-trained models, we can alternatively train a gradient boosting algorithm directly using our custom loss functions. For example, in the context of the local score, we may initialize with $\\mu^{(\\bar{0})}=0$ , $\\sigma^{(0)}=1$ and then apply our boosted conformal procedure. As discussed in Section 7, this approach is flexible enough to replace gradient boosting with any gradient-based algorithm, such as neural networks, trained under our custom loss functions. This framework aligns with that of [26], which introduces a neural network trained with a custom loss function to minimize the average prediction set size in classification tasks. ", "page_idx": 18}, {"type": "text", "text": "In Figure A1, we compare the performance of the two approaches optimizing for average interval length, searching within the generalized Local score family $\\mathcal{F}$ defined in (6). The primary distinction between the two procedures lies in the initialization: the first approach employs $\\mu^{(0)}=0$ , $\\sigma^{(0)}=1$ , while the second derives $\\mu^{(0)}$ and $\\sigma^{(0)}$ from a trained random forests model. We run the experiments on the meps-19 dataset and compare the performance across different splits of the training and calibration data. In this context, the percentage of training data refers to the proportion of training data within the combined training and calibration datasets. Our results indicate that cross-validation selects a greater number of boosting iterations when we directly train the gradient boosting algorithm, resulting in longer runtime. However, the average interval length and maximum conditional coverage deviation after boosting are notably smaller for the boosted conformal procedure we introduced in this paper. ", "page_idx": 18}, {"type": "image", "img_path": "Tw032H2onS/tmp/d47b2fac3480967b7fb3b0af7935aa871d5a721db55dcddce9e193eb061de370.jpg", "img_caption": ["Figure A1: Comparison of boosted interval length, marginal coverage, maximum conditional coverage deviation $(\\ell_{M})$ , and runtime between direct training of a gradient-based algorithm (red) and boosting on a pre-trained conformity score (blue). "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "Tw032H2onS/tmp/29cdf94da25c18c24ad396c959a1c1d8ca6fee886e18a9e4257095f74dfb438f.jpg", "img_caption": ["Figure A2: Comparison of the average boosted length, standard deviation of boosted length, and average runtime of the boosting procedure when selecting the optimal number of boosting rounds using 5-fold cross-validation versus a hold-out validation set of varying sizes. Experiments are conducted on the bike dataset, with a target miscoverage rate of $\\alpha=1\\dot{0}\\%$ . "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "A.11 Selecting optimal boosting rounds via hold-out validation set ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In our boosted conformal procedure, we use cross-validation on the training set to determine the optimal number of boosting rounds, a process that can be time-consuming. An alternative approach is to hold out a fraction of the training set for validation. While more computationally efficient, this method introduces a trade-off: a smaller validation set can lead to greater variability in prediction intervals and model performance, whereas a larger validation set may reduce the effective training set size, potentially limiting the model\u2019s performance. To explore this trade-off, we conduct experiments on the bike dataset, optimizing for prediction interval length. We compare performance across two settings: 5-fold cross-validation and a hold-out validation set, with the training-to-validation set ratio ranging from 1:1 to 8:1. For each setting, we run 100 experiments, recording the average boosted length, the standard deviation of boosted lengths, and the average runtime. The results are shown in Figure A2. ", "page_idx": 19}, {"type": "text", "text": "A.12 Additional figures on individual datasets ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we present a series of supplementary figures. First, we showcase the improvements in conditional coverage achieved through the boosted procedure for each benchmark dataset. Figure A3 details results for datasets meps-20 and meps-21. Figure A4 details results for datasets community, bike, and concrete. ", "page_idx": 19}, {"type": "text", "text": "Next, we illustrate enhanced interval lengths. Figure A5 details results for datasets meps-20, meps21, and bike. Figure A6 details results for datasets facebook-1, facebook-2, and concrete. Finally, we demonstrate in Figure A7 how cross-validating the number of boosting rounds effectively prevents the gradient boosting algorithm from overfitting. ", "page_idx": 20}, {"type": "image", "img_path": "Tw032H2onS/tmp/c66e904c6d07ea6315683be256b49d924ca06f0cc7ce914826da9a1ed1f1c073.jpg", "img_caption": ["(b) meps-21 dataset. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Figure A3: See the caption of Figure 3 for details. ", "page_idx": 20}, {"type": "image", "img_path": "Tw032H2onS/tmp/9de0ac73bd97420afce5b9a9b5b8cdfdee627ea3f2aa6f06dcc63451cb53007b.jpg", "img_caption": ["Figure A4: See the caption of Figure 3 for details. ", "(c) concrete dataset. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "Tw032H2onS/tmp/95bb7c90db55fcf758b70b41c8aef4d75a5fc2d25189aae2c3d754f46e9e3c41.jpg", "img_caption": ["(a) meps-20 dateset. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "Tw032H2onS/tmp/38b5ed3d1c9110bd63e70408ecc1dec1b1f43a49324c302dcdd24fa345eba67a.jpg", "img_caption": ["(b) meps-21 dateset. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "Tw032H2onS/tmp/361e1c82fbd36e0e7532969569a1019980af77f9f7518ab40b05c3da960e69d1.jpg", "img_caption": ["(c) bike dateset. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Figure A5: See the caption of Figure 4 for details. ", "page_idx": 22}, {"type": "image", "img_path": "Tw032H2onS/tmp/a88faf8fa58b4d858ebfe2bb9fd3d3e06529c2343923eb54b03cb9a464e2e628.jpg", "img_caption": ["(a) facebook-1 dateset. "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "Tw032H2onS/tmp/c322cb1860a217ced4cd82624b12fd074dfcaec9ec6c994e9304cdb647028a4f.jpg", "img_caption": ["(b) facebook-2 dateset. "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "Tw032H2onS/tmp/f43143642b86a8cf2e93b75ac34cb4216275e47fa4c4f42233f9604265d5938e.jpg", "img_caption": ["(c) concrete dateset. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Figure A6: See the caption of Figure 4 for details. ", "page_idx": 23}, {"type": "image", "img_path": "Tw032H2onS/tmp/e36ec8fead77489ec878c262166fcf33250755d01361a3a4ded2f8de7941e0c1.jpg", "img_caption": ["Figure A7: Empirical maximum deviation $\\ell_{M}$ across $T=500$ boosting rounds evaluated on dataset concrete under random seeds 7, 8, 9, train-calibration ratio $60\\%$ : The left panel illustrates the crossvalidated loss, computed as the average across $k=3$ sub-calibration folds. The right panel displays the test loss. The optimal number of boosting rounds $\\tau$ , determined through cross-validation as specified in (12), is highlighted in red. ", "(c) seed 7. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and precede the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 25}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 25}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 25}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 25}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \u201d[Yes] \u201d is generally preferable to \u201d[No] \u201d, it is perfectly acceptable to answer \u201d[No] \u201d provided a proper justification is given (e.g., \u201derror bars are not reported because it would be too computationally expensive\u201d or \u201dwe were unable to find the license for the dataset we used\u201d). In general, answering \u201d[No] \u201d or \u201d[NA] \u201d is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. IMPORTANT, please: ", "page_idx": 25}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\u201d, \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 25}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper. ", "page_idx": 25}, {"type": "text", "text": "\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. \u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 25}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Assumptions necessary for establishing the theoretical guarantees are detailed in each result presented in the paper. Additional limitations are discussed in Section 7. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \u201dLimitations\u201d section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 26}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: See Sections A.4, A.2 and A.3. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Justification: See Sections 5.2.2 and A.7. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 26}, {"type": "text", "text": "\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 27}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 27}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: Real data sets for our empirical analyses are publicly accessible and described in Section A.6. Currently, the code is accessible upon request. Although the code has not been released yet, we have provided detailed documentation for reproducibility in Sections 5.2.2 and A.7. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. ", "page_idx": 27}, {"type": "text", "text": "\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). \u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 28}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: See Section A.7. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 28}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: See Section A.8. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \u201dYes\u201d if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: See Section A.7. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 28}, {"type": "text", "text": "\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 29}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 29}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: See Introduction. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] Justification: The paper poses no such risks. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 30}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: See Section A.7. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 30}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: See Sections 5.2.2, A.7, and A.6. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 30}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 31}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 31}]