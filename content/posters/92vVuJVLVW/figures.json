[{"figure_path": "92vVuJVLVW/figures/figures_1_1.jpg", "caption": "Figure 1: (a) The ancestry of biological organisms condenses evolutionary information into information-dense genes to initialize their diverse descendants [62, 17]. (b) The Learngene framework condenses the significant knowledge from an ancestry model into a more compact part termed learngene and then inherited to initialize the descendant models of elastic scales. (c) The density of attention heads across the different layers of the ancestry model, which employs the DeiT-B [46]. (d) An illustration of our idea.", "description": "This figure illustrates the core concept of the Cluster-Learngene framework. (a) uses a biological analogy to show how genes condense evolutionary information to initialize diverse descendants. (b) shows the proposed framework, where the learngene is condensed from an ancestry model and then used to initialize descendant models of various sizes. (c) shows the density of attention heads across different layers of a DeiT-B model, highlighting the varying density across layers. (d) provides a simplified illustration of the clustering of attention heads.", "section": "1 Introduction"}, {"figure_path": "92vVuJVLVW/figures/figures_3_1.jpg", "caption": "Figure 1: (a) The ancestry of biological organisms condenses evolutionary information into information-dense genes to initialize their diverse descendants [62, 17]. (b) The Learngene framework condenses the significant knowledge from an ancestry model into a more compact part termed learngene and then inherited to initialize the descendant models of elastic scales. (c) The density of attention heads across the different layers of the ancestry model, which employs the DeiT-B [46]. (d) An illustration of our idea.", "description": "This figure illustrates the core concepts of the Cluster-Learngene method. (a) uses a biological analogy to explain how evolutionary information is condensed into genes to create diverse descendants. (b) shows the Learngene framework, which condenses knowledge from a large model (ancestry model) into a smaller, more efficient part (learngene) that can be used to initialize smaller models (descendant models). (c) displays the density of attention heads across different layers of a DeiT-B model, highlighting the varying density and potential for redundancy. (d) visually represents the concept of clustering attention heads to create a learngene.", "section": "1 Introduction"}, {"figure_path": "92vVuJVLVW/figures/figures_5_1.jpg", "caption": "Figure 2: Illustration of priority weight-sharing. The darker the color, the larger the cluster size associated with the head centroid.", "description": "This figure illustrates the process of priority weight-sharing used in Cluster-Learngene to initialize descendant models with varying numbers of attention heads.  Head centroids, obtained from clustering attention heads in the ancestry model, are sorted in descending order by cluster size (darker color indicates larger cluster). These centroids are then distributed to initialize the attention heads in the descendant model layers. If the number of attention heads in a layer aligns with the number of centroids, they are evenly shared.  If not, the remaining centroids are shared based on the remainder, prioritizing those representing larger clusters.", "section": "3.3 Learngene Inheriting"}, {"figure_path": "92vVuJVLVW/figures/figures_7_1.jpg", "caption": "Figure 3: Initializing descendant models of elastic scales. \"L6/9/12\" denote descendant models with 6, 9, and 12 layers, respectively. For a fair comparison, the downstream models in Pretraining-Finetuning inherit parameters from 12 layers of the pre-trained model, with the inherited number of attention heads matching those in Cluster-Learngene. We fine-tune 50 epochs for all models. In (a), the hyperparameter w takes values ranging from 1 to \u221e (i.e., the number of attention heads in descendant models is eight times that of the ancestry model). In (b), w ranges from 2 to 1. Continuing this pattern, in (c), w ranges from a maximum of 4 to a minimum of 3.", "description": "This figure shows the performance of initializing descendant models with different numbers of layers and attention heads.  It compares Cluster-Learngene to Pretraining-Finetuning, demonstrating Cluster-Learngene's ability to adapt to varying resource constraints by efficiently initializing models of different scales.  The hyperparameter 'w' controls the scaling factor for the number of attention heads.", "section": "4.2.1 Initializing Descendant Models of Elastic Scales"}, {"figure_path": "92vVuJVLVW/figures/figures_7_2.jpg", "caption": "Figure 3: Initializing descendant models of elastic scales. \"L6/9/12\" denote descendant models with 6, 9, and 12 layers, respectively. For a fair comparison, the downstream models in Pretraining-Finetuning inherit parameters from 12 layers of the pre-trained model, with the inherited number of attention heads matching those in Cluster-Learngene. We fine-tune 50 epochs for all models. In (a), the hyperparameter w takes values ranging from 1 to \u221e (i.e., the number of attention heads in descendant models is eight times that of the ancestry model). In (b), w ranges from 2 to 1. Continuing this pattern, in (c), w ranges from a maximum of 4 to a minimum of 3.", "description": "This figure visualizes the performance of Cluster-Learngene in initializing descendant models with varying numbers of layers and attention heads, comparing it to the Pretraining-Finetuning method.  It shows that Cluster-Learngene adapts well to different model scales, unlike the Pretraining-Finetuning method which requires separate training for each model variant.  The hyperparameter 'w' controls the scaling factor between the number of attention heads in the descendant and ancestry models.", "section": "4.2.1 Initializing Descendant Models of Elastic Scales"}, {"figure_path": "92vVuJVLVW/figures/figures_7_3.jpg", "caption": "Figure 5: Faster convergence. Different points represent results for varying epochs and the hyperparameter w is set to 1.0 for our method.", "description": "This figure compares the training efficiency of Cluster-Learngene against two other methods (From-Scratch and Pretraining-Finetuning) across four different model architectures: DeiT-Tiny, DeiT-Base, Swin-Tiny, and Swin-Base.  The x-axis represents the number of training epochs, and the y-axis shows the achieved accuracy.  The figure demonstrates that Cluster-Learngene achieves faster convergence than the other methods, reaching similar accuracy levels in significantly fewer training epochs. The speedup factor achieved by Cluster-Learngene is indicated for each architecture.", "section": "4.2 Main Results of Model Initialization"}, {"figure_path": "92vVuJVLVW/figures/figures_9_1.jpg", "caption": "Figure 6: Visualization of attention representations (197 \u00d7 197). We perform the following normalization operation on all attention heads A of the ancestry model and descendant model: 255A. The descendant model is trained for 50 epochs, and w is set to 1.", "description": "This figure visualizes attention representations from the first and last layers of both the ancestry and descendant models.  The color intensity represents the attention weight, with darker colors indicating stronger attention. The figure shows how the Cluster-Learngene method condenses similar semantic attention patterns from the ancestry model into fewer, more representative patterns in the descendant model, thereby reducing redundancy and improving efficiency.", "section": "4.3.3 Qualitative Visualization"}, {"figure_path": "92vVuJVLVW/figures/figures_14_1.jpg", "caption": "Figure 3: Initializing descendant models of elastic scales. \"L6/9/12\" denote descendant models with 6, 9, and 12 layers, respectively. For a fair comparison, the downstream models in Pretraining-Finetuning inherit parameters from 12 layers of the pre-trained model, with the inherited number of attention heads matching those in Cluster-Learngene. We fine-tune 50 epochs for all models. In (a), the hyperparameter w takes values ranging from 1 to \u221e (i.e., the number of attention heads in descendant models is eight times that of the ancestry model). In (b), w ranges from 2 to 1. Continuing this pattern, in (c), w ranges from a maximum of 4 to a minimum of 3.", "description": "This figure visualizes the performance of initializing descendant models with different numbers of layers and attention heads using Cluster-Learngene and compares it to the traditional Pretraining-Finetuning method.  It showcases Cluster-Learngene's ability to adapt to different resource constraints by adjusting the number of parameters in the descendant model.  The hyperparameter 'w' controls the scaling factor for the number of attention heads.", "section": "4.2 Main Results of Model Initialization"}, {"figure_path": "92vVuJVLVW/figures/figures_15_1.jpg", "caption": "Figure 8: Faster convergence. Different points represent results for varying epochs and the hyperparameter w is set to 1.0 for our method.", "description": "This figure demonstrates the faster convergence achieved by Cluster-Learngene compared to From-Scratch and Pretraining-Finetuning methods.  The x-axis represents the number of training epochs, and the y-axis shows the accuracy achieved.  The figure shows that Cluster-Learngene reaches a similar level of accuracy to other methods with significantly fewer training epochs.  Two subfigures are shown, one for DeiT-Tiny and one for Swin-Tiny architectures, highlighting that the speedup is consistent across different architectures.", "section": "4.2.4 Faster Convergence"}]