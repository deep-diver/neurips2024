{"importance": "This paper is crucial for researchers working on efficient model initialization and transfer learning, particularly in the context of Vision Transformers.  It offers a novel approach to reduce computational costs and improve model performance in various downstream tasks, which is highly relevant given the increasing resource demands of large-scale models. The adaptive clustering and learnable parameter transformation techniques introduced are significant contributions that open new avenues for research in model customization and resource-efficient AI.", "summary": "Cluster-Learngene efficiently initializes elastic-scale Vision Transformers by adaptively clustering and inheriting key modules from a large ancestry model, saving resources and boosting downstream task performance.", "takeaways": ["Cluster-Learngene adaptively clusters attention heads and FFNs based on density to identify and inherit key modules (learngene).", "Priority weight-sharing and learnable parameter transformations enable efficient initialization of descendant models with varying scales.", "Extensive experiments demonstrate improved efficiency and performance compared to existing methods, particularly for smaller models and diverse downstream tasks."], "tldr": "Large pre-trained vision models are computationally expensive, and their applicability is often overgeneralized.  This leads to inefficient resource utilization, especially for tasks with limited resources.  Existing methods for efficiently adapting these models often struggle with manual stacking or lack adaptability to different model scales.\nCluster-Learngene tackles this issue by **adaptively clustering crucial internal modules (attention heads and FFNs) from a large pre-trained model** to create a compact \"learngene\". This learngene is then used to initialize smaller, specialized models suited for downstream tasks. The method incorporates **priority weight sharing and learnable parameter transformations** to adapt the learngene to various model scales, thereby addressing the issue of model size mismatch. The extensive experiments show that Cluster-Learngene is both more efficient and achieves better performance than other initialization methods.", "affiliation": "School of Computer Science and Engineering, Southeast University", "categories": {"main_category": "Computer Vision", "sub_category": "Vision-Language Models"}, "podcast_path": "92vVuJVLVW/podcast.wav"}