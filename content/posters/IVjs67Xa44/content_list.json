[{"type": "text", "text": "Putting Gale & Shapley to Work: Guaranteeing Stability Through Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Hadi Hosseini Sanjukta Roy Duohan Zhang\\* Penn State University, USA University of Leeds, UK Penn State University, USA hadi@psu.edu s.roy@leeds.ac.uk dqz5235@psu.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Two-sided matching markets describe a large class of problems wherein participants from one side of the market must be matched to those from the other side according to their preferences. In many real-world applications (e.g. content matching or online labor markets), the knowledge about preferences may not be readily available and must be learned, i.e., one side of the market (aka agents) may not know their preferences over the other side (aka arms). Recent research on online settings has focused primarily on welfare optimization aspects (i.e. minimizing the overall regret) while paying little attention to the game-theoretic properties such as the stability of the final matching. In this paper, we exploit the structure of stable solutions to devise algorithms that improve the likelihood of finding stable solutions. We initiate the study of the sample complexity of finding a stable matching, and provide theoretical bounds on the number of samples needed to reach a stable matching with high probability. Finally, our empirical results demonstrate intriguing tradeoffs between stability and optimality of the proposed algorithms, further complementing our theoretical findings. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Two-sided markets provide a framework for a large class of problems that deal with matching two disjoint sets (colloquially agents and arms) according to their preferences. These markets have been extensively studied in the past decades and formed the foundation of matching theory\u2014a prominent subfield of economics that deals with designing markets without money. They have had profound impact on numerous practical applications such as school choice [Abdulkadiro\u02d8glu et al., 2005a,b], entry-level labor markets [Roth and Peranson, 1999], and medical residency [Roth, 1984]. The primary objective is to find a stable matching between the two sets such that no pair prefers each other to their matched partners. ", "page_idx": 0}, {"type": "text", "text": "The advent of digital marketplaces and online systems has given rise to novel applications of twosided markets such as matching riders and drivers [Banerjee and Johari, 2019], electric vehicle to charging stations [Gerding et al., 2013], and matching freelancers (or flexworkers) to job requester in a gig economy. In contrast to traditional markets that consider preferences to be readily available (e.g. by direct reporting or elicitation), in these new applications preferences may be uncertain or unavailable due to limited access or simply eliciting may not be feasible. Thus, a recent line of work has utilized bandit learning to learn preferences by modeling matching problems as multi-arm bandit problems where the preferences of agents are unknown while the preferences of arms are known. The goal is to devise learning algorithms such that a matching based on the learned preferences minimize the regret for each agent (see, for example, Liu et al. [2020, 2021], Sankararaman et al. [2021], Basu et al. [2021], Maheshwari et al. [2022], Kong et al. [2022], Zhang et al. [2022], Kong and Li [2023], Wang et al. [2022]). ", "page_idx": 0}, {"type": "table", "img_path": "IVjs67Xa44/tmp/46d3c74e5c9dced89a10566be829f0c8fdab394efc312affcd057814bcb275c3.jpg", "table_caption": [], "table_footnote": ["Table 1: Comparison of bounds on probability of unstable matchings and the sample complexity to find a stable matching. $\\begin{array}{r}{\\gamma=\\exp\\left(-\\frac{\\Delta^{2}T}{8K}\\right)}\\end{array}$ . "], "page_idx": 1}, {"type": "text", "text": "Despite tremendous success in improving the regret bound in this setting, the study of stability of the final matching has not received sufficient attention. The following stylized example illustrates how an optimal matching (with zero regret) across all agents may remain unstable. ", "page_idx": 1}, {"type": "text", "text": "Example 1. Consider three agents $\\{a_{1},a_{2},a_{3}\\}$ and three arms $\\{b_{1},b_{2},b_{3}\\}$ . Let us assume true preferences are given as strict linear orderings1as follows: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{c}{a_{1}:b_{1}^{*}\\succ b_{2}\\succ b_{3}}\\\\ {a_{2}:b_{2}^{*}\\succ\\underline{{b_{1}}}\\succ b_{3}}\\\\ {a_{3}:b_{1}\\succ b_{2}\\succ b_{3}^{*}}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{c}{b_{1}:a_{2}\\succ a_{3}\\succ a_{1}^{*}}\\\\ {b_{2}:a_{1}\\succ a_{3}\\succ a_{2}^{*}}\\\\ {b_{3}:a_{1}\\succ a_{2}\\succ a_{3}^{*}}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "The underlined matching is the only stable solution in this instance. The matching denoted by \u2217 is a regret minimizing matching: agents $a_{1}$ and $a_{2}$ have a negative regret (compared to the stable matching), and $a_{3}$ has zero regret. However, this matching is not stable because $a_{3}$ and $b_{1}$ form $a$ blocking pair. Thus, $a_{3}$ would deviate from the matching. ", "page_idx": 1}, {"type": "text", "text": "Note that stability is a desirable property that eliminates the incentives for agents to participate in secondary markets, and is the essential predictor of the long-term reliability of many real-world matching markets [Roth, 2002]. Though some work (e.g. Liu et al. [2021], Pokharel and Das [2023]) did stability analysis, it is insufficient as we discuss later in Section 3 and Section 4. ", "page_idx": 1}, {"type": "text", "text": "Our contributions. We propose bandit-learning algorithms that utilize structural properties of the Deferred Acceptance algorithm (DA)\u2014a seminal algorithm proposed by Gale and Shapley [1962] that has played an essential role in designing stable matching markets. Contrary to previous works [Liu et al., 2020, Kong and Li, 2023], we show that by exploiting an arm-proposing variant of the DA algorithm, the probability of finding a stable matching improves compared to those used in many previous work (an agent-proposing variant, such as Liu et al. [2020], Basu et al. [2021], Kong and Li [2023]). We demonstrate that for a class of profiles (i.e. profiles satisfying $\\alpha$ -condition or those with a masterlist), the arm-proposing DA is more likely to produce a stable matching compared to the agent-proposing DA for any sampling method (Corollary 1). For the commonly studied uniform sampling strategy, we show the probability bounds for two variants of DA for general preference profiles (Theorem 2). We initiate the study of sample complexity in the Probably Approximately Correct (PAC) framework. We propose a non-uniform sampling strategy which is based on armproposing DA and Action Elimination algorithm (AE) [Even-Dar et al., 2006], and show that it has a lower sample complexity as compared to uniform sampling (Theorem 3 and Theorem 5). Lastly, we validate our theoretical findings using empirical simulations (Section 6). ", "page_idx": 1}, {"type": "text", "text": "Table 1 shows the main theoretical results for uniform agent-DA algorithm, uniform arm-DA algorithm, and AE arm-DA algorithm. We note that the novel AE arm-DA algorithm achieves smaller sample complexity for finding a stable matching. Our bounds depend on structure of the stable solution $m$ , parameterized by the \u2018amount\u2019 of justified envy $E S(m)$ (see Definition 4.1). ", "page_idx": 1}, {"type": "text", "text": "1.1 Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "The two-sided matching problem is one of the most prominent success story of the field of game theory, and in particular, mechanism design, with a profound practical impact in applications ranging from organ exchange and labor market to modern markets involving allocation of compute, server, or content. The framework was formalized by Gale and Shapley [1962]\u2019s seminal work, where they, along with a long list of subsequent works focused primarily on game-theoretical aspects such as stability and incentives [Roth and Sotomayor, 1992, Roth, 1986, Dubins and Freedman, 1981]. While the DA algorithm is strategyproof for the proposing side [Gale and Shapley, 1962], no stable mechanism can guarantee that agents from both sides have incentives to report preferences truthfully in a dominant strategy Nash equilibrium [Roth, 1982]. A series of works focused on strategic aspects of stable matchings [Huang, 2006, Teo et al., 2001, Vaish and Garg, 2017, Hosseini et al., 2021]. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Stable matchings under uncertain linear or pairwise preferences were studied by Aziz et al. [2020, 2022]. When preferences are unknown, the problem of learning preferences can be modeled as a multi-agent multi-arm bandit problem. Recent work has shown a variety of learning approaches using Explore-Then-Commit (ETC), Thompson sampling [Kong et al., 2022], in centralized [Liu et al., 2020, Pokharel and Das, 2023] or decentralized [Sankararaman et al., 2021, Kong and Li, 2023] matching markets. Subsequent works focused on domains with restricted preferences (as we also study in this paper) wherein a unique stable matching exists under true preferences [Sankararaman et al., 2021, Basu et al., 2021, Maheshwari et al., 2022, Wang and Li, 2024] or those that generalize to many-to-one markets [Wang et al., 2022, Kong and Li, 2024, Li et al., 2024]. An extensive related work with details on upper bounds is provided in Appendix A. ", "page_idx": 2}, {"type": "text", "text": "2 Preliminary ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Let $[k]=\\{1,2,\\dots,k\\}$ , and $\\begin{array}{r}{\\zeta(\\beta)=\\sum_{n=1}^{\\infty}\\frac{1}{n^{\\beta}}}\\end{array}$ denotes the Riemann Zeta function, and $2>\\zeta(\\beta)>1$ if $\\beta\\geq2$ . ", "page_idx": 2}, {"type": "text", "text": "Problem setup. An instance of a two-sided matching market is specified by a set of $N$ agents, ${\\mathcal N}\\,=\\,\\{a_{1},a_{2},\\dots,a_{N}\\}$ on one side, and a set of $K$ arms, $K\\,=\\,\\{b_{1},b_{2},\\ldots,b_{K}\\}$ , on the other side. The preference of an agent $a_{i}$ , denoted by $\\succ_{a_{i}}$ , is a strict total ordering over the arms. Each agent $a_{i}$ is additionally endowed with a utility $\\mu_{i,j}$ over arm $b_{j}$ . Thus, we say an agent $a_{i}$ prefers arm $b_{j}$ to $b_{k}$ , i.e. $b_{j}\\succ_{a_{i}}b_{k}$ , if and only if $\\mu_{i,j}\\,>\\,\\mu_{i,k}$ .2 We use $\\mu$ to indicate the utility profile of all agents, where $\\mu\\,=\\,(\\mu_{i,j})_{i\\in[N],j\\in[K]}$ . The preferences of arms are denoted by a strict total ordering over the agents, i.e. an arm $b_{i}$ has preference $\\succ_{b_{i}}$ . The minimum preference gap is defined as $\\begin{array}{r}{\\Delta=\\operatorname*{min}_{i\\in[N]}\\operatorname*{min}_{j,k\\in[K],j\\neq k}|\\mu_{i,j}-\\mu_{i,k}|}\\end{array}$ . It captures the difficulty of a learning problem in matching markets, i.e., the mechanism needs more samples to estimate the preference proflie if $\\Delta$ is small. ", "page_idx": 2}, {"type": "text", "text": "Stable matching. A matching is a mapping $m:\\mathcal{N}\\cup\\mathcal{K}\\rightarrow\\mathcal{N}\\cup\\mathcal{K}\\cup\\{\\emptyset\\}$ such that $m(a_{i})\\in K$ for all $i\\,\\in\\,[N]$ , and $m(b_{j})\\in\\bar{\\mathcal{N}}\\cup\\{\\emptyset\\}$ for all $j\\,\\in\\,[K]$ , $m(a_{i})\\,=\\,b_{j}$ if and only if $m(b_{j})\\,=\\,a_{i}$ Additionally, $\\bar{m_{}}(b_{j})=\\emptyset$ if $b_{j}$ is not matched. Sometimes we abuse the notation and use $a_{i}$ to denote $i$ if agent is clear from context, and similarly use $b_{j}$ to denote $j$ if arm is clear from context. Given a matching $m$ , an agent-arm pair $(a_{i},b_{j})$ is called a blocking pair if they prefer each other than their assigned partners, i.e., $b_{j}\\succ_{a_{i}}m(a_{i})$ and $a_{i}\\succ_{b_{j}}m(b_{j})$ . Note that for any arm $b_{j}$ , getting matched is always better than being not matched, i.e., $a_{i}\\succ_{b_{j}}\\emptyset$ . A matching is stable if there is no blocking pair. ", "page_idx": 2}, {"type": "text", "text": "The Deferred Acceptance $(D A)$ algorithm [Gale and Shapley, 1962] finds a stable matching in two sided market as follows: the participants from the proposing side make proposals to the other side according to their preferences. The other side tentatively accepts the most favorable proposals and rejects the rest. The process continues until everyone from the proposing side either holds an accepted proposal (i.e., matched to the one who has accepted its proposal), or has already proposed to everyone on its preference list (i.e., remains unmatched). We consider two variants of the DA algorithm, namely, agent-proposing and arm-proposing. The matching computed by the DA algorithm is optimal for the proposing side [Gale and Shapley, 1962], i.e. proposing side receives their best match among all stable matchings. It is simultaneously pessimal for the receiving side [McVitie and Wilson, 1971]. We denote the agent-optimal (arm-pessimal) stable matching by $\\overline{m}$ and the agent-pessimal (arm-optimal) stable matching by $\\underline{m}$ . ", "page_idx": 2}, {"type": "text", "text": "Rewards and preferences. Agents receive stochastic rewards by pulling arms. If an agent $a_{i}$ pulls an arm $b_{j}$ , she gets a stochastic reward drawn from a 1-subgaussian distribution3 with mean value $\\mu_{i,j}$ . We denote the sample average of agent $a_{i}$ over arm $b_{j}$ as $\\hat{\\mu}_{i,j}$ . The agent-optimal stable regret is defined as the difference between the expected reward from the agent\u2019s most preferred stable match and the expected reward from the arm that the agent is matched to. Formally, we have $\\overline{{R}}_{i}(m)=\\mu_{i,\\overline{{m}}(a_{i})}-\\mu_{i,m(a_{i})}$ for agent $a_{i}$ and matching $m$ . Similarly, the arm-optimal stable regret as Ri(m) = \u00b5i,m(ai) \u2212\u00b5i,m(ai). ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Preference profiles. Restriction on preferences has been heavily studied by previous papers (see Sankararaman et al. [2021], Basu et al. [2021], Maheshwari et al. [2022]) as they capture natural structures where, for example, riders all rank drivers according to a common masterlist, but drivers may have different preferences according to, e.g., distance to riders. If the true preference proflies are known and there exists a unique stable matching, then the agent-proposing DA algorithm and the arm-proposing DA algorithm lead to the same matching, namely ${\\overline{{m}}}={\\underline{{m}}}$ . A natural property of the preference profile that leads to unique stable matching is called uniqueness consistency where not only there exists a unique stable matching, but also any subset of the preference proflie that contains the stable partner of each agent/arm in the subset, there exists a unique stable matching. Karpov [2019] provided a necessary and sufficient condition $\\alpha$ -condition) to characterize preference proflies that satisfy uniqueness consistency. A preference proflie satisfies the $\\alpha$ -condition if and only if there is a stable matching $m$ and an order of agents and arms such that $\\forall i\\in[N],\\forall j>i,m(a_{i})\\succ_{a_{i}}b_{j}$ , and a possibly different order of agents and arms such that $\\forall j\\!\\in\\![K],\\forall i>j,m(b_{j})\\succ_{b_{j}}a_{i}$ . ", "page_idx": 3}, {"type": "text", "text": "3 Unique Stable Matching: Agents vs. Arms ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To warm up, we start by analyzing instances of a matching market where a unique stable solution exists. As we discussed in the preliminaries, these markets are common and can be characterized by a property called uniqueness consistency. We show that for any sampling algorithm, the arm-proposing DA algorithm is more likely to generate a stable matching compared to the agent-proposing DA algorithm. All missing proofs and additional results are relegated to the full version Hosseini et al. [2024]. ", "page_idx": 3}, {"type": "text", "text": "Theorem 1. Assume that the true preferences satisfy uniqueness consistency condition. For any estimated utility $\\hat{\\mu},$ , if the agent-proposing DA algorithm produces a stable matching, then the arm-proposing DA algorithm produces a stable matching. ", "page_idx": 3}, {"type": "text", "text": "Theorem 1 states that when preferences satisfy the uniqueness consistency condition, for any estimated utility, the stability of agent-proposing DA matching implies the stability of arm-proposing DA matching. For any fixed sampling algorithm, each estimation occurs with some probability, so we immediately have the following corollary. ", "page_idx": 3}, {"type": "text", "text": "Corollary 1. For any sampling algorithm, the arm-proposing DA algorithm has a higher probability of being stable than the agent-proposing DA algorithm if the true preferences satisfy uniqueness consistency condition. ", "page_idx": 3}, {"type": "text", "text": "The following example further shows that the arm-proposing DA could generate a stable matching even if the estimation is incorrect, while the agent-proposing DA generates an unstable matching. In Section 6, we provide empirical evaluations on stability and regret of variants of the DA algorithm. ", "page_idx": 3}, {"type": "text", "text": "Example 2 (The stability of arm vs. agent proposing DA when estimation is wrong.). Consider two agents $\\{a_{1},a_{2}\\}$ and two arms $\\{b_{1},b_{2}\\}$ . Assume the true preferences are as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{c}{a_{1}:b_{1}^{*}\\succ\\underline{{b_{2}}}}\\\\ {a_{2}:b_{2}\\succ b_{2}^{*}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{c}{b_{1}:a_{1}^{*}\\succ\\underline{{a_{2}}}}\\\\ {b_{2}:a_{2}^{*}\\succ\\underline{{a_{1}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The matching denoted by \u2217is the only stable solution in this instance. Assume that after sampling data, agent $a_{1}$ has a wrong estimation: $b_{2}\\succ b_{1}$ and agent $a_{2}$ has the correct estimation. Under the wrong estimation, arm-proposing $D A$ algorithm returns the matching \u2217while agent-proposing $D A$ algorithm returns the underlined matching, which is unstable with respect to true preferences. Note that algorithms that rely on agent-proposing $D A$ (e.g. Liu et al. [2021], Kong and Li [2023]) may similarly fail to find a stable matching as they do not exploit the known arms preferences effectively. ", "page_idx": 3}, {"type": "table", "img_path": "IVjs67Xa44/tmp/f2ba8029c8730a70af1e2c65f751a73c27d8c20bd8ca2635f19070b676d88a24.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "4 Uniform Sampling DA Algorithms ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we compare the stability performance for two types of DA combined with uniform sampling algorithm when the preferences could be arbitrary. Compared with Section 3, we note that the theory in this section does not constrain preferences. We provide probability bounds for finding an unstable matching in Section 4.1 and analyze sample complexity for reaching a stable matching in Section 4.2. ", "page_idx": 4}, {"type": "text", "text": "Uniform sampling is a technique in bandit literature [Garivier et al., 2016] (usually termed as exploration-then-commit algorithm, ETC). Kong and Li [2023] utilized UCB to construct a confidence interval (CI) for each agent-arm pair, where each agent samples arms uniformly. The exploration phase stops when every agent\u2019s CIs for each pair of arms have no overlap, i.e. agents are confident that arms are ordered by the estimation correctly. Then, in the commit phase agents form a matching through agent-proposing DA, and keep pulling the same arms. ", "page_idx": 4}, {"type": "text", "text": "Agents construct CIs based on the collected data by utilizing the upper confidence bound (UCB) and lower confidence bound (LCB). Given a parameter $\\beta$ , if arm $b_{j}$ is sampled $t$ times by agent $a_{i}$ , we define the UCB and LCB as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{U C B_{i,j}=\\hat{\\mu}_{i,j}(t)+\\sqrt{2\\beta\\log(K t)/t},\\,\\,L C B_{i,j}=\\hat{\\mu}_{i,j}(t)-\\sqrt{2\\beta\\log(K t)/t},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\hat{\\mu}_{i,j}(t)$ is the average of the collected samples. ", "page_idx": 4}, {"type": "text", "text": "Uniform sampling algorithm (Algorithm 1) Agents explore the arms uniformly. Suppose that agent $a_{i}$ has disjoint confidence intervals over all arms, i.e., there exists a permutation $\\sigma_{i}~:=$ $(\\sigma_{i}(1),\\sigma_{i}(2),\\ldots,\\sigma_{i}(K))$ over arms such that $L C B_{i,\\sigma_{i}(k)}>U C B_{i,\\sigma_{i}(k+1)}$ for each $k\\in[K-1]$ Then, agent $a_{i}$ can reasonably infer the accuracy of the estimated preference proflie. The parameter $\\beta$ is used to control the confidence length, where a larger $\\beta$ implies that the agent needs more samples to differentiate the utility for a pair of arms. ", "page_idx": 4}, {"type": "text", "text": "After the sampling stage, agents can consider to form a matching either through agent-proposing DA or arm-proposing DA, as is discussed in Section 3. For simplicity, we refer uniform sampling ( Algorithm 1) with agent-proposing DA as uniform agent-DA algorithm, and uniform sampling with arm-proposing DA as uniform arm-DA algorithm. ", "page_idx": 4}, {"type": "text", "text": "4.1 Probability Bounds for Stability ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We provide theoretical analysis on probability bounds for stability for the uniform agent-DA algorithm and the uniform arm-DA algorithm. We show probability bounds of learning a stable matching using the properties of stable solutions and the structure of the proflie. We first define the following notions of local and global envy-sets. ", "page_idx": 4}, {"type": "text", "text": "Definition 4.1. The local envy-set for agent $a_{i}$ for a matching $m$ is defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\nE S_{i}(m)=\\left\\{\\!\\!\\!\\begin{array}{l l}{{\\!\\!0}}&{{i f\\{b_{j}:a_{i}\\sim b_{j}\\ m(b_{j})\\}\\ i s\\ e m p t y}}\\\\ {{\\!\\!\\{b_{j}:a_{i}\\succ b_{j}\\ m(b_{j})\\}\\bigcup\\{m(a_{i})\\}}}&{{o t h e r w i s e.}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The global envy-set of a matching $m$ is defined as the union of local envy-sets over all agents: ", "page_idx": 4}, {"type": "equation", "text": "$$\nE S(m)=\\bigcup_{i\\in[N]}\\{(a_{i},b_{j})\\colon b_{j}\\in E S_{i}(m))\\}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "By the definition of stability, a matching $m$ is stable if and only if the global envy-set is justified, i.e., agents truly prefer their current matched arm to the arms in the envy-set $E S(m)$ . Formally, $\\mu_{i,m(a_{i})}\\geq\\mu_{i,j}$ , for all $(a_{i},b_{j})\\in E S(m)$ if and only if $m$ is a stable matching. This observation is key in establishing theoretical results. ", "page_idx": 5}, {"type": "text", "text": "The following lemma provides a condition for finding a stable matching using the envy-set of the estimated matching. The detailed proof of all the results can be found in the full version Hosseini et al. [2024]. ", "page_idx": 5}, {"type": "text", "text": "Lemma 1. Assume $\\hat{\\mu}$ is the sample average, and matching m\u02c6 is stable with respect to $\\hat{\\mu}$ . Define a \u2018good\u2019 event for agent $a_{i}$ and arm $b_{j}$ as $\\mathcal{F}_{i,j}=\\{|\\mu_{i,j}-\\hat{\\mu}_{i,j}|\\leq\\Delta/2\\}$ , and define the intersection of the good events over envy-set as $\\mathcal{F}(E S(\\hat{m}))=\\cap_{(a_{i},b_{j})\\in E S(\\hat{m})}\\mathcal{F}_{i,j}$ . Then if the event $\\mathcal{F}(E S(\\hat{m}))$ occurs, matching $\\hat{m}$ is guaranteed to be stable (with respect to $\\mu$ ). ", "page_idx": 5}, {"type": "text", "text": "Now we prove the following bounds for two types of uniform sampling algorithms. The probability bound depends on the size of the envy-set. ", "page_idx": 5}, {"type": "text", "text": "Theorem 2. By uniform sampling (Algorithm $^{\\,I}$ ), each agent samples each arm $T/K$ times, and $\\hat{m}_{1}$ and $\\hat{m}_{2}$ are matchings generated by agent-proposing $D A$ and arm-proposing $D A$ , respectively. Then, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(i)~P(\\hat{m}_{1}~i s~u n s t a b l e)=O(|E S(\\overline{{m}})|\\exp(-\\frac{\\Delta^{2}T}{8K})),}\\\\ {i i)~P(\\hat{m}_{2}~i s~u n s t a b l e)=O(|E S(\\underline{{m}})|\\exp(-\\frac{\\Delta^{2}T}{8K})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where m is the agent-pessimal stable matching and $\\overline{{m}}$ is the agent-optimal stable matching. ", "page_idx": 5}, {"type": "text", "text": "Proof sketch. Since $\\hat{m}_{1}$ and $\\hat{m}_{2}$ are produced by DA based on $\\hat{\\mu}$ , both matchings are stable with respect to $\\hat{\\mu}$ . By Lemma 1, both matchings are guaranteed to be stable with respect to $\\mu$ conditioned on $\\mathcal{F}(E S(\\hat{m}_{1}))$ (or $\\mathcal{F}(E S(\\hat{m}_{2}))$ ). Thus, it follows ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{^{\\mathrm{>}}(\\hat{m}_{1}\\mathrm{~is~unstable})\\leq1-P(\\mathcal{F}(E S(\\hat{m}_{1})))}\\\\ &{=\\mathbb{E}[\\cup_{(a_{i},b_{j})\\in E S(\\hat{m}_{1})}P(|\\mu_{i,j}-\\hat{\\mu}_{i,j}|\\geq\\Delta/2)]}&{[\\mathrm{definition~of~}\\mathcal{F}]}\\\\ &{\\leq\\mathbb{E}[\\sum_{(a_{i},b_{j})\\in E S(\\hat{m}_{1})}P(|\\mu_{i,j}-\\hat{\\mu}_{i,j}|\\geq\\Delta/2)]}&{[\\mathrm{union~bound}]}\\\\ &{\\qquad\\qquad(a_{i},b_{j})\\in E S(\\hat{m}_{1})}&\\\\ &{\\leq2\\mathbb{E}[|E S(\\hat{m}_{1})|]\\exp(-\\frac{\\Delta^{2}T}{8K})}&{[\\sqrt{\\frac{K}{T}}\\mathrm{-subgausian~with~}0\\mathrm{~mean}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "To complete the proof for $\\hat{m}_{1}$ , we demonstrate an upper bound of $\\mathbb{E}[|E S(\\hat{m}_{1})|]$ . We show that $\\begin{array}{r}{|\\mathbb{E}[|E S(\\hat{m}_{1})|]-|E S(\\overline{{m}})||\\leq N^{2}K^{3}e x p\\big({-}\\frac{\\Delta^{2}T}{4K}\\big)}\\end{array}$ .l eTteh ep rdoifoffe irse ndceef eirs rneed gtloi giAbplep ewnhdeixn $T$ .is sufficiently ", "page_idx": 5}, {"type": "text", "text": "In the next lemma, we show the relation between the size of the envy-sets for the agent-optimal and agent-pessimal matchings. Then, combining Theorem 2 and Lemma 2, we prove the next corollary. ", "page_idx": 5}, {"type": "text", "text": "Lemma 2. Given any instance of a matching problem, we have the following relationship between the size of the two envy sets: $|E\\dot{S}(\\underline{{m}})|\\leq|E\\dot{S}(\\overline{{m}})|$ . ", "page_idx": 5}, {"type": "text", "text": "Proof. Agent-pessimal stable matching $\\underline{m}$ is the arm-optimal stable matching, and agent-optimal stable matching $\\overline{{m}}$ is the arm-pessimal stable matching, so we have that $\\underline{{m}}(b_{j})\\succ\\!\\!b_{j}\\;\\overline{{m}}(b_{j})$ or $\\overline{{m}}\\bar{(}b_{j})=$ $\\underline{{m}}(b_{j})$ for each arm $b_{j}$ . From the definition of the envy-set, we have $|E S(\\underline{{m}})|\\leq|E S(\\overline{{m}})|$ . \u518f\u53e3 ", "page_idx": 5}, {"type": "text", "text": "Corollary 2. The uniform arm-DA algorithm has a smaller probability bound of being unstable than the uniform agent-DA algorithm. ", "page_idx": 5}, {"type": "text", "text": "Remark 1. Liu et al. [2020] showed the probability bound of $\\exp(-\\frac{\\Delta^{2}T}{2K})$ for finding an invalid ranking by the ETC algorithm, where a valid ranking is defined as the estimated ranking such that the estimated pairwise comparison is correct for a subset of agent-arm pairs. However, their result did not relate the probability bound with the structure of the instance, whereas the bound in Theorem 2 crucially uses the envy set to improve the probability of finding a stable solution. Liu et al. $I2O2I J$ provided an upper bound on the sum of the probabilities of being unstable for the Confilct-Avoiding UCB algorithm (CA-UCB). Under CA-UCB algorithm, $O(\\log^{2}(T))$ out of $T$ matchings are unstable in expectation. ", "page_idx": 5}, {"type": "table", "img_path": "IVjs67Xa44/tmp/2b26f1d06ba9398c8bad4de905ed7ebcd523b41220c75833fd1bb35387e52a27.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "4.2 Sample Complexity ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We turn to analyze the sample complexity to learn a stable matching under the probably approximately correct (PAC) framework. In particular we ask: given a probability budget $\\alpha$ , how many samples $T$ are needed to find a stable matching? Formally, an algorithm has sample complexity $T$ with probability budget $\\alpha$ if with probability at least $1-\\alpha$ , the algorithm guarantees that it would find a stable matching with the total number of samples over all agent-arm pairs upper bounded by $T$ . ", "page_idx": 6}, {"type": "text", "text": "Theorem 3. [Sample complexity for uniform sampling algorithm] With probability at least $1-\\alpha$ , both the uniform agent- $.D A$ and the uniform arm- $\\cdot D A$ algorithms find a stable matching with the same sample complexity $\\begin{array}{r}{\\tilde{O}(\\frac{N K}{\\Delta^{2}}\\log(\\alpha^{-1}))^{4}}\\end{array}$ . ", "page_idx": 6}, {"type": "text", "text": "Note that uniform agent-DA algorithm finds the stable matching $\\overline{{m}}$ , and uniform arm-DA algorithm finds the stable matching $\\underline{m}$ . Uniform sampling (Algorithm 1) suffers from sub-optimal sample complexity for finding stable matchings since agents sample each arm uniformly. Thus, in the next section we devise an exploration algorithm that exploits the structure of stable matchings by utilizing arms\u2019 known preferences. ", "page_idx": 6}, {"type": "text", "text": "5 An Arm Elimination DA Algorithm ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The proposed algorithm (Algorithm 3) combines the arm-proposing DA and Action Elimination (AE) algorithm [Audibert and Bubeck, 2010, Even-Dar et al., 2006, Jamieson and Nowak, 2014]. The AE algorithm eliminates an arm (i.e. no longer sampling the arm) when confidence bound indicates that the arm is sub-optimal (i.e. the upper confidence bound is smaller than another arm\u2019s lower confidence bound), and outputs the best arm when there is only one arm that hasn\u2019t been eliminated. Note that Algorithm 3 differs from the vanilla arm-proposing DA in Line 8, when an agent has been proposed by two arms. Agents utilize the arm elimination algorithm (see Algorithm 2) until the agent eliminates the sub-optimal arm. Note that at every round, each agent chooses an arm with fewer samples thus far (see Line 4 in Algorithm 2). One significant observation is that if Algorithm 2 outputs winners correctly whenever an agent is proposed, Algorithm 3 terminates with the arm-optimal matching $\\underline{m}$ . ", "page_idx": 6}, {"type": "text", "text": "5.1 Probability Bounds for Stability ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We compute the probability bound for learning an unstable matching for Algorithm 3. Contrary to uniform sampling, here we compute the bound on given sample size. ", "page_idx": 6}, {"type": "text", "text": "Theorem 4. By Algorithm $^3$ , assume that agent $a_{i}$ samples arm $b_{j}$ for $T_{i,j}$ and $\\hat{m}$ is returned by the algorithm. We define $\\begin{array}{r}{T_{m i n}=\\operatorname*{min}_{(a_{i},b_{j})\\in E S(\\hat{m})}T_{i,j}}\\end{array}$ as the minimum sample size for agent-arm pairs. Then, we have ", "page_idx": 6}, {"type": "equation", "text": "$$\nP(\\hat{m}\\;i s\\;u n s t a b l e)\\leq O(|E S(\\underline{{m}})|\\exp(-\\frac{\\Delta^{2}T_{m i n}}{8})).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Remark 2. Theorem $^{4}$ provides stability bound for Algorithm $^3$ that depends on $T_{i,j}$ , which is unknown apriori. If the total sample budget is $N T$ and we set $\\begin{array}{r}{T_{i,j}=\\frac{N T}{|E S(\\underline{{m}})|}}\\end{array}$ , the stability bound becomes $O(|E S(\\underline{{{m}}})|e x p(-\\frac{\\Delta^{2}N T}{8|E S(\\underline{{{m}}})|}))$ , which is smaller than uniform arm-DA\u2019s stability bound $O(|E S(\\underline{{m}})|e x p(-\\frac{\\Delta^{2}T}{8K}))$ , as stated in Theorem 2. Even though the upper bound could be larger than that of Algorithm $^{\\,l}$ , simulated experiments show that AE arm-DA significantly improves stability guarantees compared to the uniform sampling variants (Algorithm 1). ", "page_idx": 6}, {"type": "image", "img_path": "IVjs67Xa44/tmp/0a57a1c0df156cb68a5b13f7a0644502732be5545414e182e292f40f02fc451a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "5.2 Sample Complexity ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We compute the sample complexity to learn a stable matching for Algorithm 3. Note that agents only sample pairs in the envy-set, while in Algorithm 1 agents explore all arms uniformly. The following analysis shows that Algorithm 3 has smaller sample complexity compared to Algorithm 1. ", "page_idx": 7}, {"type": "text", "text": "Theorem 5. [Sample complexity for $A E$ arm- $.D A$ algorithm] With probability at least $1-\\alpha,A l g o\\cdot$ rithm 3 terminates and returns a stable matching, $\\underline{{m}},$ , with sample complexity of ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\tilde{O}(\\frac{E S(\\underline{{m}})}{\\Delta^{2}}\\log(\\alpha^{-1})).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Proof sketch. We begin by defining a good event $|\\hat{\\mu}_{i,j}(t)-\\mu_{i,j}|\\leq\\sqrt{2\\beta\\log(K t)/t}$ only for agentarm pairs in the envy-set $|E S(\\underline{{m}})|$ . Conditioned on such events for all time, we demonstrate that the algorithm terminates with true preferences on the envy-set $E S(\\underline{{m}})$ , and thus, the algorithm executes the arm-proposing DA when agents have known preferences and produces $\\underline{m}$ . ", "page_idx": 7}, {"type": "text", "text": "Then we show the upper bound of sample complexity for each agent-arm pair: $\\begin{array}{r}{T=O(\\frac{\\beta}{\\Delta^{2}}\\log(\\frac{\\beta K}{\\Delta^{2}}))}\\end{array}$ . We prove it by induction on the number of proposals. The base case is when arms $b_{j}$ and $b_{j^{\\prime}}$ propose to agent $a_{i}$ for the first time. Then, we show the number of samples for each pair is bounded by $T$ . In the inductive step, say $b_{j}$ is the winner in the last round and has sampled $t_{i,j}\\leq T$ times, and $b_{j^{\\prime}}$ proposes to $a_{i}$ in this round. Then if $a_{i}$ samples $b_{j}$ for $T-t_{i,j}$ more times and $a_{i}$ samples $b_{j^{\\prime}}$ for $T$ times, by the same computation as the base case, we have that the number of samples for each pair is bounded by $T$ . Since Algorithm 3 only samples the agent-arm pairs in the envy set $E S(\\underline{{m}})$ , we get that the total sample complexity is $\\begin{array}{r}{|E S(\\underline{{m}})|T=O(\\frac{\\beta(|E S(\\underline{{m}})|)}{\\Delta^{2}}\\log(\\frac{\\beta K}{\\Delta^{2}}))}\\end{array}$ ( \u03b2(|E\u2206S2(m)|)log( \u03b2\u2206K2 )). By setting the probability budget = 4|EKS(\u03b2m)|, we have that with probability at least 1 \u2212\u03b1, the AE arm-DA algorithm has sample complexity $\\begin{array}{r}{{\\tilde{O}}(\\frac{|E S(m)|}{\\Delta^{2}}\\log(\\alpha^{-1}))}\\end{array}$ . The complete proof appears in Appendix D. \u53e3 ", "page_idx": 7}, {"type": "text", "text": "It is worth noting that a large $\\beta$ implies a small $\\alpha$ , which implies that the algorithm needs more samples to guarantee finding a stable matching. We show bounds on the envy-sets in the next lemma. ", "page_idx": 7}, {"type": "text", "text": "Lemma 3. Considering any true preference $\\mu_{\\cdot}$ , we have the following bounds for envy-set: ", "page_idx": 7}, {"type": "text", "text": "(i) Size of the envy-set for $\\overline{{m}}$ : $(m a x\\{N,K\\}-N)N\\leq|E S(\\overline{{m}})|\\leq N K.$ (ii) Size of the envy-set for m: $(m a x\\{N,K\\}-N)N\\leq|E S(\\underline{{m}})|\\leq N K-N+1.$ ", "page_idx": 7}, {"type": "image", "img_path": "IVjs67Xa44/tmp/4447c379bb6301bd63f15ec13cd43f185654ce59565f0c591aa091f073ecb2d7.jpg", "img_caption": ["Figure 1: $95\\%$ confidence interval of stability and regret for 200 randomized general preference profiles. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Remark 3. By comparing Theorem 5 to Theorem 3, the sample complexity ratio between the $A E$   \narm-DA (Algorithm 3) and uniform arm selection (Algorithm 1) is |ENS(Km)|, which further shows   \nworse-case ratios as $E S({\\underline{{m}}})$ . Thus, Algorithm 3 $^3$ ictly improves the sample $\\frac{m a x\\{N,K\\}-N}{X}$ $\\begin{array}{r}{1-\\frac{N-1}{N K}<1}\\end{array}$   \ncomplexity of finding a stable matching. ", "page_idx": 8}, {"type": "text", "text": "Remark 4. One can illustrate the magnitude of $|E S(\\underline{{m}})|$ through the lens of arm-proposing $D A$ algorithm. Observe that $|E S(\\underline{{m}})|$ is the number of proposals made by arms and rejections made by agents in the arm-proposing $D A$ algorithm. In a highly competitive environment for arms, e.g. when there are much more arms than agents so that many arms are not matched, the magnitude of $|E S(\\underline{{m}})|$ is large. In a less competitive environment, e.g. when arms put different agents as their top choices, $|E S(\\underline{{m}})|$ has much smaller magnitude. ", "page_idx": 8}, {"type": "text", "text": "6 Experimental Results ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we experimentally validate our theoretical results. For this, we consider $N=K=20$ and randomly generate preferences. In particular, we follow a similar experiment setting in Liu et al. [2021]: for each $i$ , the true utilities $\\{\\mu_{i,1},\\mu_{i,2},\\dots,\\mu_{i,20}\\}$ are randomized permutations of the sequence $\\{1,2,\\ldots,20\\}$ so that the minimum preference gap is fixed $\\left.\\Delta\\right.=1)$ ) and algorithm performance exhibits relatively low variability. Arms\u2019 preferences are generated the same way. We conduct 200 independent simulations, with each simulation featuring a randomized true preference proflie. We compare average stability, i.e., the proportion of stable matchings over 200 experiments, average regrets, and maximum regrets over agents between four algorithms: uniform agent- $\\mathrm{\\cdotDA^{5}}$ , uniform arm-DA, AE arm-DA, and CA-UCB [Liu et al., 2021]. ", "page_idx": 8}, {"type": "text", "text": "In terms of stability, our experiments show that the AE arm-DA algorithm significantly enhances the likelihood of achieving stability compared to both types of uniform sampling algorithm and CA-UCB algorithm (Figure 1). On the other hand, the regret gap between uniform agent-DA and other two arm-proposing types of algorithms illustrates the utility difference of agent-optimal matching $\\overline{{m}}$ and arm-optimal matching $\\underline{m}$ . We note that when preferences are restricted to have unique stable matching, AE arm-DA algorithm\u2019s regret converges faster to 0, compared to uniform algorithms, while still keeping faster stability convergence (Figure 2). Additional experiments with other preference domains (e.g. masterlist) in provided in Appendix E. ", "page_idx": 8}, {"type": "text", "text": "At the first glance, Figure 1 (the center and the right plots) seems to suggest that the the uniform arm-DA is outperforming the AE arm-DA algorithm. However, note that the regret here is with respect to the agent-optimal solution (i.e. $\\overline{{R}}$ ); and thus, the AE arm-DA algorithm by design is not optimized to reach that solution. Upon further investigation, however, we see that when comparing the two algorithms using the agent-pessimal regret $(\\underline{{R}})$ then the AE arm-DA converges with fewer samples both in terms of average and maximum regrets, as illustrated in Figure 3. ", "page_idx": 8}, {"type": "text", "text": "7 Conclusion and Future Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The game-theoretical properties such as stability in two-sided matching problems are critical indicators of success and sustenance of matching markets; without stability agents may \u2018scramble\u2019 to participate in secondary markets even when all preferences are known [Kojima et al., 2013]. We demonstrated key techniques in learning preferences that rely on the structure of stable solutions. In particular, exploiting the \u2018known\u2019 preferences of arms in the arm-proposing variant of DA and eliminating arms early on, provably reduces the sample complexity of finding stable matchings while experimentally having little impact on optimality (measured by regret). Findings of this paper can have substantial impact in designing new labor markets, school admissions, or healthcare where decisions must be made as preferences are revealed [Rastegari et al., 2014]. ", "page_idx": 8}, {"type": "image", "img_path": "IVjs67Xa44/tmp/8227b2e6bf801ebe26c0ee136502f787f86eb2bd3c3fecc281df98981c5cbf3a.jpg", "img_caption": ["Figure 2: $95\\%$ confidence interval of stability and regrets for 200 randomized SPC preference proflies. Please see the definition of SPC in Appendix E. An SPC preference profile has a unique stable matching. "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "IVjs67Xa44/tmp/d7931e14417cc92a9291ac9eb7fc7817447aaf7a6f0b012efad6f388b9af1a48.jpg", "img_caption": ["Figure 3: $95\\%$ confidence interval of agent-pessimal stable regrets for 200 randomized general preference profiles. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "We conclude by discussing some limitations and open questions. First, extending this framework to settings with incomplete preferences, ties, or those that go beyond subgaussian utility assumptions are interesting directions for future research. We opted to avoid these nuances, for example ties, as such variations often introduce computational complexity with known preferences. In addition, given that the number of stable solutions could raise exponentially [Knuth, 1976], designing learning algorithms that could converge to stable solutions while satisfying some fairness notions (e.g. egalitarian or regret-minimizing) is an intriguing future direction. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Hadi Hosseini acknowledges support from National Science Foundation (NSF) IIS grants #2144413 and #2107173. We thank Shraddha Pathak for her feedback. We also thank the anonymous reviewers for their comments and constructive criticisms. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Atila Abdulkadiro\u02d8glu, Parag A Pathak, and Alvin E Roth. The New York City High School Match. American Economic Review, 95(2):364\u2013367, 2005a. ", "page_idx": 9}, {"type": "text", "text": "Atila Abdulkadirog\u02d8lu, Parag A Pathak, Alvin E Roth, and Tayfun S\u00f6nmez. The Boston Public School Match. American Economic Review, 95(2):368\u2013371, 2005b. ", "page_idx": 9}, {"type": "text", "text": "Jean-Yves Audibert and S\u00e9bastien Bubeck. Best arm identification in multi-armed bandits. In COLT-23th Conference on learning theory-2010, pages 13\u2013p, 2010. ", "page_idx": 9}, {"type": "text", "text": "Haris Aziz, P\u00e9ter Bir\u00f3, Serge Gaspers, Ronald de Haan, Nicholas Mattei, and Baharak Rastegari. Stable matching with uncertain linear preferences. Algorithmica, 82:1410\u20131433, 2020.   \nHaris Aziz, P\u00e9ter Bir\u00f3, Tam\u00e1s Fleiner, Serge Gaspers, Ronald De Haan, Nicholas Mattei, and Baharak Rastegari. Stable matching with uncertain pairwise preferences. Theoretical Computer Science, 909:1\u201311, 2022.   \nSiddhartha Banerjee and Ramesh Johari. Ride Sharing. In Sharing Economy, pages 73\u201397. Springer, 2019.   \nSoumya Basu, Karthik Abinav Sankararaman, and Abishek Sankararaman. Beyond $l o g^{2}(t)$ regret for decentralized bandits in matching markets. In Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 705\u2013715. PMLR, 18\u201324 Jul 2021.   \nSanmay Das and Emir Kamenica. Two-sided bandits and the dating market. In IJCAI, volume 5, page 19. Citeseer, 2005.   \nLester E Dubins and David A Freedman. Machiavelli and the gale-shapley algorithm. The American Mathematical Monthly, 88(7):485\u2013494, 1981.   \nEyal Even-Dar, Shie Mannor, Yishay Mansour, and Sridhar Mahadevan. Action elimination and stopping conditions for the multi-armed bandit and reinforcement learning problems. Journal of machine learning research, 7(6), 2006.   \nDavid Gale and Lloyd S Shapley. College admissions and the stability of marriage. The American Mathematical Monthly, 69(1):9\u201315, 1962.   \nAur\u00e9lien Garivier, Tor Lattimore, and Emilie Kaufmann. On explore-then-commit strategies. Advances in Neural Information Processing Systems, 29, 2016.   \nEnrico H. Gerding, Sebastian Stein, Valentin Robu, Dengji Zhao, and Nicholas R. Jennings. Two-sided online markets for electric vehicle charging. In Proceedings of the 2013 International Conference on Autonomous Agents and Multi-Agent Systems, AAMAS \u201913, page 989\u2013996. International Foundation for Autonomous Agents and Multiagent Systems, 2013. ISBN 9781450319935.   \nHadi Hosseini, Fatima Umar, and Rohit Vaish. Accomplice manipulation of the deferred acceptance algorithm. In Zhi-Hua Zhou, editor, Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21, pages 231\u2013237, 8 2021. doi: 10.24963/ijcai.2021/33. URL https://doi.org/10.24963/ijcai.2021/33. Main Track.   \nHadi Hosseini, Sanjukta Roy, and Duohan Zhang. Putting gale & shapley to work: Guaranteeing stability through learning. arXiv preprint arXiv:2410.04376, 2024.   \nChien-Chung Huang. Cheating by men in the gale-shapley stable matching algorithm. In European Symposium on Algorithms, pages 418\u2013431. Springer, 2006.   \nRobert W Irving. Stable marriage and indifference. Discrete Applied Mathematics, 48(3):261\u2013272, 1994.   \nMeena Jagadeesan, Alexander Wei, Yixin Wang, Michael Jordan, and Jacob Steinhardt. Learning equilibria in matching markets from bandit feedback. Advances in Neural Information Processing Systems, 34:3323\u20133335, 2021.   \nKevin Jamieson and Robert Nowak. Best-arm identification algorithms for multi-armed bandits in the fixed confidence setting. In 2014 48th Annual Conference on Information Sciences and Systems (CISS), pages 1\u20136. IEEE, 2014.   \nAlexander Karpov. A necessary and sufficient condition for uniqueness consistency in the stable marriage matching problem. Economics Letters, 178:63\u201365, 2019.   \nDonald Ervin Knuth. Mariages stables et leurs relations avec d\u2019autres probl\u00e8mes combinatoires: introduction \u00e0 l\u2019analyse math\u00e9matique des algorithmes. (No Title), 1976.   \nFuhito Kojima, Parag A Pathak, and Alvin E Roth. Matching with couples: Stability and incentives in large markets. The Quarterly Journal of Economics, 128(4):1585\u20131632, 2013.   \nFang Kong and Shuai Li. Player-optimal stable regret for bandit learning in matching markets. In Proceedings of the 2023 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 1512\u20131522. SIAM, 2023.   \nFang Kong and Shuai Li. Improved bandits in many-to-one matching markets with incentive compatibility. Proceedings of the AAAI Conference on Artificial Intelligence, 38(12):13256\u201313264, Mar. 2024. doi: 10.1609/aaai.v38i12.29226. URL https://ojs.aaai.org/index.php/AAAI/ article/view/29226.   \nFang Kong, Junming Yin, and Shuai Li. Thompson sampling for bandit learning in matching markets. In Lud De Raedt, editor, Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22, pages 3164\u20133170. International Joint Conferences on Artificial Intelligence Organization, 7 2022. doi: 10.24963/ijcai.2022/439. URL https://doi.org/10.24963/ijcai. 2022/439. Main Track.   \nTor Lattimore and Csaba Szepesv\u00e1ri. Bandit algorithms. Cambridge University Press, 2020.   \nYuantong Li, Guang Cheng, and Xiaowu Dai. Two-sided competing matching recommendation markets with quota and complementary preferences constraints. In Forty-first International Conference on Machine Learning, 2024.   \nLydia T. Liu, Horia Mania, and Michael Jordan. Competing bandits in matching markets. In Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics, volume 108 of Proceedings of Machine Learning Research, pages 1618\u20131628. PMLR, 26\u201328 Aug 2020.   \nLydia T Liu, Feng Ruan, Horia Mania, and Michael I Jordan. Bandit learning in decentralized matching markets. The Journal of Machine Learning Research, 22(1):9612\u20139645, 2021.   \nChinmay Maheshwari, Shankar Sastry, and Eric Mazumdar. Decentralized, communication- and coordination-free learning in structured matching markets. In Advances in Neural Information Processing Systems, volume 35, pages 15081\u201315092. Curran Associates, Inc., 2022.   \nDavid F Manlove. The structure of stable marriage with indifference. Discrete Applied Mathematics, 122(1-3):167\u2013181, 2002.   \nDavid G McVitie and Leslie B Wilson. The stable marriage problem. Communications of the ACM, 14(7):486\u2013490, 1971.   \nYifei Min, Tianhao Wang, Ruitu Xu, Zhaoran Wang, Michael Jordan, and Zhuoran Yang. Learn to match with no regret: Reinforcement learning in markov matching markets. Advances in Neural Information Processing Systems, 35:19956\u201319970, 2022.   \nGaurab Pokharel and Sanmay Das. Converging to stability in two-sided bandits: The case of unknown preferences on both sides of a matching market. arXiv preprint arXiv:2302.06176, 2023.   \nBaharak Rastegari, Anne Condon, Nicole Immorlica, Robert Irving, and Kevin Leyton-Brown. Reasoning about optimal stable matchings under partial information. In Proceedings of the ffiteenth ACM conference on Economics and computation, pages 431\u2013448, 2014.   \nAlvin E Roth. The economics of matching: Stability and incentives. Mathematics of operations research, 7(4):617\u2013628, 1982.   \nAlvin E Roth. The Evolution of the Labor Market for Medical Interns and Residents: A Case Study in Game Theory. Journal of Political Economy, 92(6):991\u20131016, 1984.   \nAlvin E Roth. On the allocation of residents to rural hospitals: a general property of two-sided matching markets. Econometrica: Journal of the Econometric Society, pages 425\u2013427, 1986.   \nAlvin E Roth. The economist as engineer: Game theory, experimentation, and computation as tools for design economics. Econometrica, 70(4):1341\u20131378, 2002.   \nAlvin E Roth and Elliott Peranson. The Redesign of the Matching Market for American Physicians: Some Engineering Aspects of Economic Design. American Economic Review, 89(4):748\u2013780, 1999.   \nAlvin E Roth and Marilda Sotomayor. Two-sided matching. Handbook of game theory with economic applications, 1:485\u2013541, 1992.   \nAbishek Sankararaman, Soumya Basu, and Karthik Abinav Sankararaman. Dominate or delete: Decentralized competing bandits in serial dictatorship. In Proceedings of The 24th International Conference on Artificial Intelligence and Statistics, volume 130 of Proceedings of Machine Learning Research, pages 1252\u20131260. PMLR, 13\u201315 Apr 2021.   \nChung-Piaw Teo, Jay Sethuraman, and Wee-Peng Tan. Gale-shapley stable marriage problem revisited: Strategic issues and applications. Management Science, 47(9):1252\u20131267, 2001.   \nRohit Vaish and Dinesh Garg. Manipulating gale-shapley algorithm: Preserving stability and remaining inconspicuous. In IJCAI, pages 437\u2013443, 2017.   \nZilong Wang and Shuai Li. Optimal analysis for bandit learning in matching markets with serial dictatorship. Theoretical Computer Science, page 114703, 2024.   \nZilong Wang, Liya Guo, Junming Yin, and Shuai Li. Bandit learning in many-to-one matching markets. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management, pages 2088\u20132097, 2022.   \nYiRui Zhang and Zhixuan Fang. Decentralized two-sided bandit learning in matching market. In The 40th Conference on Uncertainty in Artificial Intelligence, 2024.   \nYiRui Zhang, Siwei Wang, and Zhixuan Fang. Matching in multi-arm bandit with collision. In Advances in Neural Information Processing Systems, volume 35, pages 9552\u20139563. Curran Associates, Inc., 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A Related work (Extended) ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Stable matching markets. The two-sided matching market problem has been used to analyze many markets, such as the residency assignment problem [Gale and Shapley, 1962, Roth and Sotomayor, 1992]. The deferred-acceptance (DA) algorithm is an elegant procedure that guarantees a stable solution and can be computed in polynomial time [Gale and Shapley, 1962]. The DA algorithm is strategy proof for proposers Roth [1982], i.e., no single proposer can be matched to a better partner by misrepresenting the preference. However, proposers can form a coalition to misrepresent their preferences and some proposers are better off Huang [2006], Dubins and Freedman [1981]. Research on stable matching with uncertain preferences [Aziz et al., 2020, 2022] investigated to find a stable matching with the highest probability when both sides are unsure about their preferences. ", "page_idx": 13}, {"type": "text", "text": "Bandit learning in matching markets. Liu et al. [2020] formalized the centralized and decentralized bandit learning problem in matching markets. In this problem, one side of participants (agents) have unknown preferences over the other side (arms), while arms have known preferences over agents. They introduced a centralized uniform sampling algorithm that achieves $\\bar{O(\\frac{K\\log(T)}{\\Delta^{2}})}$ agentoptimal stable regret for each agent, considering the time horizon $T$ and the minimum preference gap $\\Delta$ . For decentralized matching markets, Sankararaman et al. [2021] showed that there exists an instance such that the regret for agent $a_{i}$ is lower bounded as $\\begin{array}{r l}{\\Omega(\\operatorname*{max}\\{\\frac{(i-1)\\log(T)}{\\Delta^{2}},\\frac{K\\log(T)}{\\Delta}\\})}&{{}}\\end{array}$ . Some research [Sankararaman et al., 2021, Basu et al., 2021, Maheshwari et al., 2022] focused on special preference proflies where a unique stable matching exists. Recently, Kong and Li [2023], Zhang et al. [2022] both unveiled decentralized algorithms to achieve a near-optimal regret bound $\\begin{array}{r}{O(\\frac{K l o g T}{\\Delta^{2}})}\\end{array}$ , embodying the exploration-exploitation trade-off central to reinforcement learning and multi-armed bandit problems. ", "page_idx": 13}, {"type": "text", "text": "Other works focused on different variants of the bandit matching market problem. Das and Kamenica [2005], Zhang and Fang [2024] studied the bandit problem in matching markets, where both sides of participants have unknown preferences. Wang et al. [2022], Kong and Li [2024] generalized the one-to-one setting to many-to-one matching markets under the bandit framework. Kong and Li [2024] proposed an ODA algorithm that utilized a similar idea of arm-proposing DA variant compared to the AE arm-DA algorithm in this paper, however, the ODA algorithm achieved $\\begin{array}{r}{O\\big(\\frac{N K}{\\Delta^{2}}\\log(T)\\big)}\\end{array}$ regret bound in the one-to-one setting, which is worse than the state-of-the-art algorithms in one-to-one matching (e.g. Kong and Li [2023], Zhang et al. [2022]). The performance of the ODA algorithm is hindered by unnecessary agent pulls. [Jagadeesan et al., 2021] studied stability of bandit problem in matching markets with monetary transfer. [Min et al., 2022] studied Markov matching markets by considering unknown transition functions. ", "page_idx": 13}, {"type": "text", "text": "B Omitted proofs from Section 3 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Theorem 1. Assume that the true preferences satisfy uniqueness consistency condition. For any estimated utility $\\hat{\\mu},$ , if the agent-proposing DA algorithm produces a stable matching, then the arm-proposing $D A$ algorithm produces a stable matching. ", "page_idx": 13}, {"type": "text", "text": "Proof. We denote the matching generated based on the estimated utility $\\hat{\\mu}$ by the agent-proposing DA and arm-proposing DA as $\\hat{m}_{1}$ and $\\hat{m}_{2}$ , respectively. By the definition of uniqueness consistency there is only one stable matching, denote it by $m^{*}$ . Since $\\hat{m}_{1}$ is assumed to be stable, $\\hat{m}_{1}=m^{*}$ , we show that the arm-proposing DA algorithm also returns the same matching, i.e., $\\hat{m}_{2}=m^{*}$ . ", "page_idx": 13}, {"type": "text", "text": "By using the Rural-Hospital theorem [Roth, 1986] on the estimated preferences $\\hat{\\mu}$ , we have that the same subset of agents and arms are matched in both $\\hat{m}_{1}$ and $\\hat{m}_{2}$ , so we can reduce the case to $N=K$ by only considering the subset of matched agents and arms. ", "page_idx": 13}, {"type": "text", "text": "Since the true preferences satisfy uniqueness consistency, then it must satisfy the $\\alpha$ -condition. Using the definition of $\\alpha$ -condition, we have an ordering of agents and arms such that ", "page_idx": 13}, {"type": "equation", "text": "$$\na_{i}\\succ_{b_{i}}a_{j},\\forall j>i,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $a_{i}=\\hat{m}_{1}(b_{i})$ . ", "page_idx": 13}, {"type": "text", "text": "Suppose for contradiction that $\\hat{m}_{2}\\neq m^{*}$ . Then there must exist some $k>l$ , such that $\\hat{m}_{2}(b_{l})=a_{k}$ . However, since both $\\hat{m}_{1}$ and $\\hat{m}_{2}$ are stable with respect to $\\hat{\\mu}$ and $\\hat{m}_{2}$ is arm optimal, the partner of arm $b_{l}$ in $\\hat{m}_{2}$ is at least as good as its partner in $\\hat{m}_{1}$ . Thus, ", "page_idx": 14}, {"type": "equation", "text": "$$\na_{k}=\\hat{m}_{2}(b_{l})\\succ_{b_{l}}\\hat{m}_{1}(b_{l})=a_{l},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which is a contradiction to equation (2). Thus, we prove that $\\hat{m}_{2}$ is also stable (with respect to true utility). \u53e3 ", "page_idx": 14}, {"type": "text", "text": "C Omitted proofs from Section 4 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The following lemmas are useful to prove the stability bounds. ", "page_idx": 14}, {"type": "text", "text": "Lemma 4 (Property of independent subgaussian, Lemma 5.4 in Lattimore and Szepesv\u00e1ri [2020]). Suppose that $X$ is $d$ -subgaussian and $X_{1}$ and $X_{2}$ are independent and $d_{1}$ and $d_{2}$ subgaussian, respectively, then we have the following property:   \n$(l)\\;V a r[X]\\leq d^{2}$ .   \n(2) $c X$ is |c|d-subgaussian for all $c\\in\\mathbb{R}$ .   \n( $3)\\,X_{1}+X_{2}$ is $\\sqrt{d_{1}^{2}+d_{2}^{2}}$ -subgaussian. ", "page_idx": 14}, {"type": "text", "text": "By the property of independent subgaussian random variables, we have the following lemma that bounds the probability of ranking two arms wrongly. ", "page_idx": 14}, {"type": "text", "text": "Lemma 5. Sample $h_{1}$ data $\\{X_{1},X_{2},\\ldots,X_{h_{1}}\\}$ i.i.d. from $^{\\,l}$ -subgaussian with mean $\\mu_{1}$ , and $h_{2}$ data $\\{Y_{1},Y_{2},\\ldots,Y_{h_{2}}\\}$ i.i.d. from $^{\\,l}$ -subgaussian with mean $\\mu_{2}$ , where $\\mu_{1}<\\mu_{2}$ . Two datasets are independent. Define $\\hat{\\mu}_{1}$ and $\\hat{\\mu}_{2}$ as the sample mean for two datasets. Then we have ", "page_idx": 14}, {"type": "equation", "text": "$$\nP(\\hat{\\mu}_{2}<\\hat{\\mu}_{1})\\leq e x p(-\\frac{(\\mu_{2}-\\mu_{1})^{2}}{2(\\frac{1}{h_{1}}+\\frac{1}{h_{2}})}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. By Lemma 4, $\\hat{\\mu}_{2}-\\hat{\\mu}_{1}$ is $\\begin{array}{r}{\\sqrt{\\frac{1}{h_{1}}+\\frac{1}{h_{2}}}}\\end{array}$ -subgaussian with mean $\\mu_{2}-\\mu_{1}$ . Thus by the definition of subgaussian ", "page_idx": 14}, {"type": "equation", "text": "$$\nP(\\hat{\\mu}_{2}<\\hat{\\mu}_{1})=P((\\hat{\\mu}_{2}-\\hat{\\mu}_{1})-(\\mu_{2}-\\mu_{1})<-(\\mu_{2}-\\mu_{1}))\\leq e x p(-\\frac{(\\mu_{2}-\\mu_{1})^{2}}{2(\\frac{1}{h_{1}}+\\frac{1}{h_{2}})}),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and the proof is complete. ", "page_idx": 14}, {"type": "text", "text": "Lemma 1. Assume $\\hat{\\mu}$ is the sample average, and matching m\u02c6 is stable with respect to $\\hat{\\mu}$ . Define $a$ \u2018good\u2019 event for agent $a_{i}$ and arm $b_{j}$ as $\\bar{\\mathcal{F}_{i,j}}=\\{|\\mu_{i,j}-\\hat{\\mu_{i,j}}|\\leq\\Delta/2\\}$ , and define the intersection of the good events over envy-set as $\\mathcal{F}(E S(\\hat{m}))=\\cap_{(a_{i},b_{j})\\in E S(\\hat{m})}\\mathcal{F}_{i,j}$ . Then if the event $\\mathcal{F}(E S(\\hat{m}))$ occurs, matching $\\hat{m}$ is guaranteed to be stable (with respect to $\\mu_{,$ ). ", "page_idx": 14}, {"type": "text", "text": "Proof. We show that no agent-arm pair forms a blocking pair in $\\hat{m}$ . We prove it by contradiction. ", "page_idx": 14}, {"type": "text", "text": "Assume that there exists an agent $a_{i}$ and an arm $b_{j}$ in the local envy-set $E S_{i}(\\hat{m})$ that blocks $\\hat{m}$ , which means $\\mu_{i,\\hat{m}(a_{i})}<\\mu_{i,j}$ , and more concretely, $\\mu_{i,j}-\\mu_{i,\\hat{m}(a_{i})}\\geq\\Delta$ . From stability of $\\hat{m}$ with respect to the preference $\\hat{\\mu}$ we have that $\\hat{\\mu}_{i,\\hat{m}(a_{i})}>\\hat{\\mu}_{i,j}$ , otherwise, $(a_{i},b_{j})$ blocks $\\hat{m}$ according to $\\hat{\\mu}$ . Thus, under the event $\\mathcal{F}(E S(\\hat{m}))$ , it holds that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\hat{\\mu}_{i,j}\\geq\\mu_{i,j}-\\frac{\\Delta}{2}\\geq\\mu_{i,\\hat{m}(a_{i})}+\\frac{\\Delta}{2}\\geq\\hat{\\mu}_{i,\\hat{m}(a_{i})},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which is a contradiction. Therefore, $\\hat{m}$ is stable with respect to $\\mu$ . ", "page_idx": 14}, {"type": "text", "text": "Theorem 2. By uniform sampling (Algorithm $^{\\,I}$ ), each agent samples each arm $T/K$ times, and $\\hat{m}_{1}$ and $\\hat{m}_{2}$ are matchings generated by agent-proposing $D A$ and arm-proposing $D A$ , respectively. Then, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(i)~P(\\hat{m}_{1}~i s~u n s t a b l e)=O(|E S(\\overline{{m}})|\\exp(-\\frac{\\Delta^{2}T}{8K})),}\\\\ {~i i)~P(\\hat{m}_{2}~i s~u n s t a b l e)=O(|E S(\\underline{{m}})|\\exp(-\\frac{\\Delta^{2}T}{8K})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where m is the agent-pessimal stable matching and m is the agent-optimal stable matching. ", "page_idx": 15}, {"type": "text", "text": "Proof. Since $\\hat{m}_{1}$ and $\\hat{m}_{2}$ are produced by DA based on $\\hat{\\mu}$ , both matchings are stable with respect to $\\hat{\\mu}$ . By Lemma 1, both matchings are guaranteed to be stable with respect to $\\mu$ conditioned on $\\mathcal{F}(E S(\\hat{m}_{1}))$ (or $\\mathcal{F}(E S(\\hat{m}_{2}))$ . Thus, it follows ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P(\\hat{m}_{1}\\mathrm{~is~unstable})\\leq1-P(\\mathcal{F}(E S(\\hat{m}_{1})))}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\mathbb{E}[\\cup_{(a_{i},b_{j})\\in E S(\\hat{m}_{1})}P(|\\mu_{i,j}-\\hat{\\mu}_{i,j}|\\geq\\Delta/2)]}\\\\ &{\\leq\\mathbb{E}[\\sum_{(a_{i},b_{j})\\in E S(\\hat{m}_{1})}P(|\\mu_{i,j}-\\hat{\\mu}_{i,j}|\\geq\\Delta/2)]}\\\\ &{\\qquad\\qquad\\qquad\\quad(a_{i},b_{j})\\!\\in\\!E S(\\hat{m}_{1})}\\\\ &{\\leq2\\mathbb{E}[|E S(\\hat{m}_{1})|]\\exp(-\\frac{\\Delta^{2}T}{8K}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the second line follows from the definition of the event $\\mathcal{F}$ , the third line is by union bound, and the last line follows from Lemma 4 that $\\mu_{i,j}-\\hat{\\mu}_{i,j}$ is $\\sqrt{\\frac{K}{T}}$ -subgaussian with 0 mean. ", "page_idx": 15}, {"type": "text", "text": "To complete the proof for $\\hat{m}_{1}$ , we demonstrate an upper bound of $\\mathbb{E}[|E S(\\hat{m}_{1})|]$ . Then we have that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{|\\mathbb{E}[|E S(\\hat{m}_{1})|]-|E S(\\overline{{m}})||\\leq N K\\cdot P(\\hat{m}_{1}\\neq\\overline{{m}})}&{}\\\\ {\\leq N K\\cdot P(\\exists i,j,j^{\\prime}\\;s u c h\\;t h a t\\;\\mu_{i,j}>\\mu_{i,j^{\\prime}},\\hat{\\mu}_{i,j}<\\hat{\\mu}_{i,j^{\\prime}})}\\\\ {\\leq N^{2}K^{3}P(\\mu_{i,j}>\\mu_{i,j^{\\prime}},\\hat{\\mu}_{i,j}<\\hat{\\mu}_{i,j^{\\prime}})}&{}\\\\ {\\leq N^{2}K^{3}e x p(-\\frac{\\Delta^{2}T}{4K}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the second line comes from the fact that if $\\hat{m}_{1}\\neq\\overline{{m}}$ , then there exists an agent $a_{i}$ and two arms $b_{j}$ and $b_{j^{\\prime}}$ that are learned incorrectly since a correct estimated proflie produces $\\overline{{m}}$ by agent-proposing DA. The last inequality follows from Lemma 5 since $(\\mu_{i,j}-\\mu_{i,j^{\\prime}})\\geq\\Delta$ and number of samples is $T/K$ . ", "page_idx": 15}, {"type": "text", "text": "Note that the difference is negligible when $T$ is sufficiently large compared with $N$ and $K$ . The same computation applies to $\\hat{m}_{2}$ and $\\underline{m}$ . Thus the second statement follows. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "A useful technical lemma for bounding the number of samples agents need to find a stable matching is as follows. ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{ma}6.\\mathrm{~}F o r\\,\\nu a r i a b l e s\\mathrm{~}K\\in\\mathbb{N}\\,a n d\\,d>0,\\operatorname*{min}\\{t\\in\\mathbb{N}:l o g(K t)/t\\leq d\\}=\\Theta(\\frac{1}{d}\\log(\\frac{K}{d})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. Define $T_{\\operatorname*{min}}:=\\operatorname*{min}\\{t\\in\\mathbb{N}:l o g(K t)/t\\leq d\\}$ and $T_{\\mathrm{max}}:=\\operatorname*{max}\\{t\\in\\mathbb{N}:l o g(K t)/t\\geq d\\}$ . We observe that $T_{\\mathrm{min}}\\leq T_{\\mathrm{max}}+1$ and thus, we compute the upper bound of $T_{\\mathrm{min}}$ through $T_{\\mathrm{max}}$ . By the definition of $T_{\\mathrm{max}}$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\nT_{\\mathrm{max}}\\leq\\frac{1}{d}\\log(K T_{\\mathrm{max}})\\leq\\frac{1}{d}\\log(\\frac{K}{d}\\log(K T_{\\mathrm{max}})).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Again by the definition of $T_{\\mathrm{max}}$ and the fact that $x\\geq l o g^{2}(x)$ for any $x>0$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{T_{\\operatorname*{max}}}{K}d^{2}\\leq\\frac{\\log^{2}(K T_{\\operatorname*{max}})}{K T_{\\operatorname*{max}}}\\leq1.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Equation (3) and Equation (4) give ", "page_idx": 15}, {"type": "equation", "text": "$$\nT_{\\mathrm{max}}\\leq\\frac{1}{d}\\log(\\frac{2K}{d}\\log(\\frac{K}{d}))=\\frac{1}{d}\\log(\\frac{2K}{d})+\\frac{1}{d}\\log(\\log(\\frac{K}{d}))\\leq\\frac{2}{d}\\log(\\frac{2K}{d}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "On the other hand, we compute the lower bound of $T_{\\mathrm{min}}$ by the definition of $T_{\\mathrm{min}}$ ", "page_idx": 15}, {"type": "equation", "text": "$$\nT_{\\mathrm{min}}\\ge\\frac{1}{d}l o g(K T_{\\mathrm{min}})\\ge\\frac{1}{d}\\log(\\frac{K}{d}\\log(K T_{\\mathrm{min}})).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since $T_{\\mathrm{min}}$ is a positive integer, we have that ", "page_idx": 15}, {"type": "equation", "text": "$$\nT_{\\mathrm{min}}\\geq\\frac{1}{d}\\log(\\frac{K}{d}\\log(K))\\geq\\frac{1}{d}\\log(\\frac{K}{d})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "when $K>2$ . Thus the proof is complete. ", "page_idx": 15}, {"type": "text", "text": "Theorem 3. [Sample complexity for uniform sampling algorithm] With probability at least $1-\\alpha$ , both the uniform agent- $.D A$ and the uniform arm- $\\cdot D A$ algorithms find a stable matching with the same sample complexity $\\begin{array}{r}{\\tilde{O}(\\frac{N K}{\\Delta^{2}}\\log(\\alpha^{-1}))^{6}}\\end{array}$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. We first show that Algorithm 1 finds the stable matching $\\overline{{m}}$ with a high probability. Then we analyze the total number of samples. If agent $a_{i}$ samples arm $b_{j}$ for $t$ times, by Lemma 4 and the definition of subgaussian, with probability at least $1-\\frac{2}{(K t)^{\\beta}}$ (K2t)\u03b2 such that |\u00b5\u02c6i,j(t) \u2212\u00b5i,j| \u2264 $\\sqrt{2\\beta\\log(K t)/t}$ , where $\\hat{\\mu}_{i,j}(t)$ is the sample average of agent $a_{i}$ over arm $b_{j}$ when $a_{i}$ samples $b_{j}$ for $t$ times. Taking a union bound for all $i,j,t$ gives that with probability at least $\\begin{array}{r}{1-\\frac{2N}{K^{\\beta-1}}\\zeta(\\beta)}\\end{array}$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n|\\hat{\\mu}_{i,j}(t)-\\mu_{i,j}|\\leq\\sqrt{2\\beta\\log(K t)/t},\\forall t\\in\\mathbb{N},\\forall i\\in[N],\\forall j\\in[K].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Conditioned on this, we first show that Algorithm 1 terminates with true preference proflies. Assume that the mechanism stops with sample size $T$ for each agent-arm pair. For any $i$ and $j\\neq k$ , if $\\hat{\\mu}_{i,j}>\\hat{\\mu}_{i,k}$ , we have a stopping condition ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\mu}_{i,j}-\\hat{\\mu}_{i,k}\\geq2\\sqrt{2\\beta\\log(K T)/T},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "since by Equation (1) and line 6 of Algorithm 1 ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{L C B_{i,j}=\\hat{\\mu}_{i,j}-\\sqrt{2\\beta\\log(K T)/T}\\geq\\hat{\\mu}_{i,k}+\\sqrt{2\\beta\\log(K T)/T}=U C B_{i,k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then by Equation (5) ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mu_{i,j}\\geq L C B_{i,j}\\geq U C B_{i,k}\\geq\\mu_{i,k}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Hence, uniform agent-DA algorithm produces the stable matching $\\overline{{m}}$ . ", "page_idx": 16}, {"type": "text", "text": "Then we compute the sample complexity. By Equation (5) ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\mu}_{i,j}-\\hat{\\mu}_{i,k}\\geq-2\\sqrt{2\\beta\\log(K T)/T}+\\mu_{i,j}-\\mu_{i,k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore if we set ", "page_idx": 16}, {"type": "equation", "text": "$$\nT=\\operatorname*{min}\\{t\\in\\mathbb{N}:\\sqrt{2\\beta\\log(K t)/t}\\le\\Delta/4\\},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "we have that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\mu}_{i,j}-\\hat{\\mu}_{i,k}\\geq-2\\sqrt{2\\beta\\log(K T)/T}+\\mu_{i,j}-\\mu_{i,k}}\\\\ &{\\qquad\\qquad\\geq-2\\sqrt{2\\beta\\log(K T)/T}+\\Delta}\\\\ &{\\qquad\\qquad\\geq2\\sqrt{2\\beta\\log(K T)/T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and thus the stopping condition 6 is satisfied. By Lemma 6 we have $T\\ =\\ \\operatorname*{min}\\{t\\ \\in\\ \\mathbb{N}\\ :$ $\\begin{array}{r}{\\sqrt{2\\beta\\log(K t)/t}\\,\\le\\,\\Delta/4\\}\\;=\\;O(\\frac{\\beta}{\\Delta^{2}}\\log(\\frac{\\beta K}{\\Delta^{2}}))}\\end{array}$ . Note that $T$ is the sample complexity for each agent-arm pair. Thus, the total number of samples are bounded by $\\begin{array}{r}{N K T=O(\\frac{\\beta N K}{\\Delta^{2}}\\log(\\frac{\\beta K}{\\Delta^{2}}))}\\end{array}$ . ", "page_idx": 16}, {"type": "text", "text": "By setting a probability budget $\\begin{array}{r}{\\alpha=\\frac{4N}{K^{\\beta-1}}}\\end{array}$ K4\u03b2N\u22121 , we have that with probability at least 1 \u2212\u03b1, the uniform sampling algorithm terminates with sample complexity O( \u03b2\u2206N2K log( \u03b2\u2206K2 )), where \u03b2 = 1+ logl(o4gN(K\u03b1)\u22121). Therefore, the sample complexity for uniform agent-DA algorithm is $\\begin{array}{r}{\\tilde{O}(\\frac{N K}{\\Delta^{2}}\\log(\\alpha^{-1}))}\\end{array}$ . ", "page_idx": 16}, {"type": "text", "text": "For uniform arm-DA, the only difference is that it produces a matching by arm-proposing DA algorithm. Therefore, uniform arm-DA finds the stable matching $\\underline{m}$ with the same sample complexity. ", "page_idx": 16}, {"type": "text", "text": "D Omitted proofs from Section 5 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Theorem 4. By Algorithm $^3$ , assume that agent $a_{i}$ samples arm $b_{j}$ for $T_{i,j}$ and $\\hat{m}$ is returned by the algorithm. We define $\\begin{array}{r}{T_{m i n}=\\operatorname*{min}_{(a_{i},b_{j})\\in E S(\\hat{m})}T_{i,j}}\\end{array}$ as the minimum sample size for agent-arm pairs. Then, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\nP(\\hat{m}\\;i s\\;u n s t a b l e)\\leq O(|E S(\\underline{{m}})|\\exp(-\\frac{\\Delta^{2}T_{m i n}}{8})).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "$^6\\tilde{O}$ denotes the upper bound that omits terms logarithmic in the input. ", "page_idx": 16}, {"type": "text", "text": "Proof. When the sampling phase of Algorithm 3 ends, there is no unmatched agent and the matching $\\hat{m}$ is the same matching proposed by arm-proposing DA according to estimated utility $\\hat{\\mu}$ . Thus, $\\hat{m}$ is stable with respect to $\\hat{\\mu}$ . By Lemma 1, we know that $\\hat{m}$ is stable under the event $\\mathcal{F}(E S(\\hat{m}))$ . Then ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P(\\hat{m}\\,i s\\,u n s t a b l e)\\leq1-P(\\mathcal{F}(E S(\\hat{m})))}\\\\ &{=\\mathbb{E}[\\cup_{(a_{i},b_{j})\\in E S(\\hat{m})}P(\\vert\\mu_{i,j}-\\hat{\\mu}_{i,j}\\vert\\geq\\Delta/2)]}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\leq\\mathbb{E}[\\underbrace{1}_{(a_{i},b_{j})\\in E S(\\hat{m})}P(\\vert\\mu_{i,j}-\\hat{\\mu}_{i,j}\\vert\\geq\\Delta/2)]}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\leq2\\mathbb{E}[\\underbrace{1}_{(a_{i},b_{j})\\in E S(\\hat{m})}\\exp(-\\frac{\\Delta^{2}T_{i,j}}{8})]}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad(a_{i},b_{j})\\in E S(\\hat{m})}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\leq2\\mathbb{E}[\\vert E S(\\hat{m})\\vert\\exp(-\\frac{\\Delta^{2}T_{m i n}}{8})].}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The third line comes from union bound, and the fourth line comes from Lemma 4 and that $\\mu_{i,j}-\\hat{\\mu}_{i,j}$ is $\\sqrt{\\frac{1}{T_{i,j}}}$ -subgaussian. Observe that correct estimated profile on $E S(\\hat{m})$ outputs $\\underline{m}$ by Algorithm 3 since the algorithm follows arm-DA algorithm. Then we have the computation ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P(\\Hat{m}\\neq\\underline{{m}})\\leq P(\\exists(a_{i},b_{j})\\in E S(\\Hat{m}),(a_{i},b_{j^{\\prime}})\\in E S(\\Hat{m}),\\,a n d\\,\\mu}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\sum_{(a_{i},b_{j}),(a_{i},b_{j^{\\prime}})\\in E S(\\Hat{m})}P(\\mu_{i,j}>\\mu_{i,j^{\\prime}},\\Hat{\\mu}_{i,j}<\\Hat{\\mu}_{i,j^{\\prime}})}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\sum_{(a_{i},b_{j}),(a_{i},b_{j^{\\prime}})\\in E S(\\Hat{m})}e x p(-\\frac{\\Delta^{2}}{2(\\frac{1}{T_{i,j}}+\\frac{1}{T_{i,j^{\\prime}}})})}\\\\ &{\\qquad\\qquad\\leq N K^{2}e x p(-\\frac{\\Delta^{2}T_{m i n}}{4}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the third line follows from Lemma 5. Therefore, we have that $\\hat{m}$ converges to $\\underline{m}$ and so the probability of not being $\\underline{m}$ is negligible when $T_{m i n}$ is sufficiently large. Thus, we complete the proof. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "Theorem 5. [Sample complexity for $A E$ arm- $.D A$ algorithm] With probability at least $1-\\alpha$ , Algorithm 3 terminates and returns a stable matching, $\\underline{m},$ , with sample complexity of ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\tilde{O}(\\frac{E S(\\underline{{m}})}{\\Delta^{2}}\\log(\\alpha^{-1})).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. The proof has similar structure to the proof of Theorem 3; however, the proof is different because Algorithm 3 only sample the pairs in the envy set $E S(\\underline{{m}})$ while Algorithm 1 samples all arms uniformly. ", "page_idx": 17}, {"type": "text", "text": "We first show that with a high probability, the algorithm terminates with $\\underline{m}$ . If agent $a_{i}$ samples arm $b_{j}$ for $t$ times, by Lemma 4 and the definition of subgaussian, we have that $|\\hat{\\mu}_{i,j}(t)-\\mu_{i,j}|\\leq$ $\\sqrt{2\\beta\\log(K t)/t}$ with probability at least (K2t)\u03b2 , here \u00b5i,j(t) is the sample average of agent ai over arm $b_{j}$ when $a_{i}$ samples $b_{j}$ for $t$ times. Taking a union bound over all $t$ and all pairs $a_{i},b_{j}$ in the envy set $\\bar{E}S(\\underline{{m}})$ , we have that ", "page_idx": 17}, {"type": "equation", "text": "$$\n|\\hat{\\mu}_{i,j}(t)-\\mu_{i,j}|\\leq\\sqrt{2\\beta\\log(K t)/t},\\forall t\\geq1,\\forall(a_{i},b_{j})\\in E S(\\underline{{m}})\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "with probability at least $\\begin{array}{r}{1-\\frac{2|E S(m)|}{K^{\\beta}}\\zeta(\\beta)}\\end{array}$ . Conditioned on Equation (8), we have that Algorithm 3 terminates with correct estimated proflie on pairs of the envy-set $E S(\\underline{{m}})$ as the same logic in the first part of the proof in Theorem 3. Thus, since Algorithm 3 follows arm-proposing DA algorithm, the algorithm outputs $\\underline{m}$ . ", "page_idx": 17}, {"type": "text", "text": "Next, we claim that for every agent-arm pair in the envy-set $E S(\\underline{{m}})$ , the sample complexity is $\\begin{array}{r}{T=\\operatorname*{min}\\{t\\in\\mathbb{N}:\\sqrt{2\\beta\\log(K t)/t}\\leq\\Delta/4\\}=O(\\frac{\\beta}{\\Delta^{2}}\\log(\\frac{\\beta K}{\\Delta^{2}}))}\\end{array}$ . We prove this for each agent-arm pair by induction on the number of iterations of while loop of Algorithm 3. We will show that in each invocation of Algorithm 2, the number of samples for any agent-arm pair in the envy-set $E S(\\underline{{m}})$ is at most $T$ . Thus, since the while loop in Algorithm 3 checks whether the sample number is bounded by $T$ , we get the desired bound. ", "page_idx": 17}, {"type": "text", "text": "Base case. Let arms $b_{j}$ and $b_{j^{\\prime}}$ are the first to propose to agent $a_{i}$ . Let $T_{i,j}$ and $T_{i,j^{\\prime}}$ be the number of samples for agent $a_{i}$ over arm $b_{j}$ and $b_{j^{\\prime}}$ by Algorithm 2. Without loss of generality, we assume that $\\mu_{i,j}\\;>\\;\\mu_{i,j^{\\prime}}$ . Since both arms have not been sampled before, by Algorithm 2 the stopping condition Equation (6) is satisfied when $T_{i,j}=T_{i,j^{\\prime}}=T$ since ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\mu}_{i,j}-\\hat{\\mu}_{i,j^{\\prime}}\\geq-2\\sqrt{2\\beta\\log(K T)/T}+\\mu_{i,j}-\\mu_{i,j^{\\prime}}}\\\\ &{\\qquad\\qquad\\geq-2\\sqrt{2\\beta\\log(K T)/T}+\\Delta}\\\\ &{\\qquad\\qquad\\geq2\\sqrt{2\\beta\\log(K T)/T},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the first line comes from Equation (8), and the third line comes from the definition of $T$ . ", "page_idx": 18}, {"type": "text", "text": "Inductive steps. By induction hypothesis, in Algorithm 3, agent $a_{i}$ has already sampled $b_{j}$ for $t_{i,j}\\leq T$ times. Let $b_{j}$ be the winner in the last round and $b_{j^{\\prime}}$ proposes to $a_{i}$ in this round. In Line 4 of Algorithm 2, agent $a_{i}$ samples $t_{i,j}$ times of arm $b_{j^{\\prime}}$ before sampling $b_{j}$ . Then if $a_{i}$ samples $b_{j}$ for $T-t_{i,j}$ more times and $a_{i}$ samples $b_{j^{\\prime}}$ $T$ times, by the same computation as the base case, we have that the stopping condition Equation (6) is satisfied when $T_{i,j}=T_{i,j^{\\prime}}=T$ . Thus, we show that the number of samples for any agent-arm pair is bounded by $T$ . ", "page_idx": 18}, {"type": "text", "text": "sSainmcpel eA clogomrpitlhexmit y3  ios n $\\begin{array}{r}{|\\dot{E}S(\\underline{{m}})|T=O(\\frac{\\^}{\\beta(|E S(\\underline{{m}})|)}\\log(\\frac{\\beta K}{\\Delta^{2}}))}\\end{array}$ . envy set $E S(\\underline{{m}})$ , we get that the total ", "page_idx": 18}, {"type": "text", "text": "By setting the probability budget $\\begin{array}{r}{\\alpha=\\frac{4|E S(\\underline{{m}})|}{K^{\\beta}}}\\end{array}$ , we have that with probability at least $1-\\alpha$ , the .m  Thhaesr esfaomrep,l iet  icso omf ptlheex iotryd ecro $\\frac{\\beta|E S(\\underline{{m}})|}{\\Delta^{2}}\\log(\\frac{\\beta K}{\\Delta^{2}})$ , where $\\begin{array}{r}{\\beta=\\frac{\\log(4|E S(m)|\\alpha^{-1})}{\\log(K)}}\\end{array}$ $\\begin{array}{r}{\\tilde{O}(\\frac{|E S(m)|}{\\Delta^{2}}\\log(\\alpha^{-1}))}\\end{array}$ ", "page_idx": 18}, {"type": "text", "text": "Lemma 3. Considering any true preference $\\mu$ , we have the following bounds for envy-set: ", "page_idx": 18}, {"type": "text", "text": "(i) Size of the envy-set for $\\overline{{m}}$ : $(m a x\\{N,K\\}-N)N\\leq|E S(\\overline{{m}})|\\leq N K.$ (ii) Size of the envy-set for $\\underline{m}$ : $(m a x\\{N,K\\}-N)N\\leq|E S(\\underline{{m}})|\\leq N K-N+1.$ ", "page_idx": 18}, {"type": "text", "text": "Proof. First we consider the best case and $N\\geq K$ . Suppose that for any $i\\in[K]$ , agent $a_{i}$ prefers arm $b_{i}$ the most; also for any $j\\in[K]$ , arm $b_{j}$ prefers agent $a_{j}$ the most. Under this preference proflie, we immediately have $\\overline{{m}}=\\underline{{m}}=\\{(a_{1},b_{1}),(a_{2},b_{2}),\\ldots,(a_{K},b_{K})\\}$ and, since all arms match their first choice, we have $|E S(\\overline{{{m}}})|\\,=\\,|E S(\\underline{{{m}}})|\\,=\\,0$ . if $K>N$ , we construct the preference profile similarly for the first $N$ agents and $N$ arms, then the remaining $K-N$ arms are not matched, and so $|E S(\\overline{{{m}}})|=|E S(\\underline{{{m}}})|=\\bar{(}K-N)N$ . ", "page_idx": 18}, {"type": "text", "text": "Now consider the worst case for $\\overline{m}$ . We keep agents\u2019 preferences the same as stated above; however, agent $a_{j}$ is the worst agent according to $b_{j}$ for each $j\\;\\in\\;[m i n\\{N,K\\}]$ . Then $\\overline{{m}}\\,=$ $\\{(a_{1},b_{1}),(a_{2},b_{2}),\\dot{}\\dots,(a_{m i n\\{N,K\\}},b_{m i n\\{N,K\\}})\\}$ . Thus, for each $j\\in[K]$ , arm $b_{j}$ is in the envy-set $E S_{i}(\\overline{{m}})$ of agent $a_{i}$ for all $i\\in[N]$ . Then, envy-set $E S({\\overline{{m}}})$ contains all agent-arm pairs and so in the worst case $|E S({\\overline{{m}}})|=N K$ . ", "page_idx": 18}, {"type": "text", "text": "Lastly, we show the worst case for $\\underline{m}$ . We claim that for all matched arms, at most one arm can get matched to the worst choice. In their seminal work, Gale and Shapley [1962] observed that when one arm gets matched to the worst agent, it proposes to all other agents and get rejected by them. This observation implies all other agents are tentatively matched and thus no agent is available. Therefore, we have that the worst case is $|E S(\\underline{{m}})|=N K-N+1$ . \u53e3 ", "page_idx": 18}, {"type": "text", "text": "E Omitted details from Section 6 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We define the sequence preference condition (SPC) that we use for our experiments. A preference profile satisfies SPC if and only if there is an order of agents and arms such that $\\forall i\\in\\mathbf{\\bar{[}N]},\\forall j>$ $i,\\mu_{i,i}>\\mu_{i,j},a n d\\,\\forall i\\in[K],\\forall j>i,a_{i}>_{b_{i}}a_{j}$ . If each participant of one side (agents or arms) has the same preference (also known as a masterlist ) over the other, the preference proflie satisfies SPC. Clearly, a preference profile that is SPC satisfies $\\alpha$ -condition. ", "page_idx": 18}, {"type": "text", "text": "Figure 4 shows the stability and regrets for all four algorithms under the constraint of the agent masterlist preference proflies. When agents share identical utilities for each arm, the average rewards remain the same regardless of the matching. Consequently, the average regrets (showed in the middle figure) are always zero. ", "page_idx": 18}, {"type": "image", "img_path": "IVjs67Xa44/tmp/0637ccc357e0c7f7cbe4ea9281fc84c44219f220123a84b96ad9988d968f3b65.jpg", "img_caption": ["Figure 4: $95\\%$ confidence interval of stability and regrets for 200 randomized agent masterlist preference profiles. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "We also explain why the uniform arm-DA algorithm surpasses the AE arm-DA algorithm in terms of $\\overline{{R}}$ , but underperforms compared to the AE-arm DA algorithm in terms of $R$ under general preference profiles. Note that the uniform arm-DA algorithm samples arms uniformly, and thus it may end up matching agents to arms that are between the agent-optimal stable match and the agent-pessimal stable match. Thus, when compared with the agent-pessimal regret, it may seem better. However, in comparison, agents are incrementally matched to increasingly preferable arms throughout the procedure of Algorithm 3, and therefore agents are matched to arms that are no better than agentpessimal stable match as shown in Figure 1 and Figure 3. ", "page_idx": 19}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: Claims in the abstract and introduction match the theoretical analysis in Section 3, Section 4, and Section 5, and the simulated experiments in Section 6. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 20}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: We mentioned several limitations and future work in Section 7 ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 20}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We provide detailed proofs in appendix. Assumptions are clearly stated in the statement of theorems. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 21}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We clearly state the experimental details in Section 6. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 21}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 21}, {"type": "text", "text": "Answer:[Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We provide codes in the supplemental material. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 22}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We present the experimental setting in Section 6. Additional details are provided in Appendix E. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 22}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: We show error bars for figures in Section 6 and Appendix E. The error bars are calculated as average $\\pm\\,1.96*$ standard error. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Experiments use bandit domain and algorithms can be run on a typical personal computer. Minimal compute resources are required to reproduce experiments in the paper. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 23}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: Yes, we follow the code of ethics. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 23}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: Yes, we discussed the societal impacts of our work in Section 7. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: Our paper has no such risks. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 24}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: We don\u2019t use any existing codes or data. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 24}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 25}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: Our paper does not release new assets. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 25}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 25}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 25}]