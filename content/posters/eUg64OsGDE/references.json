{"references": [{"fullname_first_author": "N. Amini-Naieni", "paper_title": "Open-world text-specified object counting", "publication_date": "2023", "reason": "This paper introduces a foundational model for open-world object counting using only text, which COUNTGD extends to include visual exemplars."}, {"fullname_first_author": "S. Liu", "paper_title": "GroundingDINO: Marrying dino with grounded pre-training for open-set object detection", "publication_date": "2024", "reason": "COUNTGD builds directly on this model, leveraging its pre-trained vision-language capabilities for improved accuracy and generality."}, {"fullname_first_author": "C. Liu", "paper_title": "CountR: Transformer-based generalised visual counting", "publication_date": "2022", "reason": "This paper introduces a state-of-the-art visual exemplar-based object counting model that COUNTGD improves upon by incorporating text prompts."}, {"fullname_first_author": "V. Ranjan", "paper_title": "Learning to count everything", "publication_date": "2021", "reason": "This paper introduces the FSC-147 dataset, which is the primary dataset used for training and evaluation in this work."}, {"fullname_first_author": "R. Paiss", "paper_title": "Teaching clip to count to ten", "publication_date": "2023", "reason": "This paper introduces the CountBench dataset, a secondary dataset used for evaluating COUNTGD's zero-shot performance."}]}