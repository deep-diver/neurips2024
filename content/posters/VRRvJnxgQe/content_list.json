[{"type": "text", "text": "NoiseGPT: Label Noise Detection and Rectification through Probability Curvature ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Haoyu Wang ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Zhuo Huang ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "School of Automation Beijing Institute of Technology haoyu.wang@bit.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Sydney AI Centre University of Sydney zhuohuang.ai@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Zhiwei Lin\u2217", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "School of Automation Beijing Institute of Technology linzhiwei@bit.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Tongliang Liu\u2020 Sydney AI Centre University of Sydney tongliang.liu@sydney.edu.au ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Machine learning craves high-quality data which is a major bottleneck during realistic deployment, as it takes abundant resources and massive human labor to collect and label data. Unfortunately, label noise where image data mismatches with incorrect label exists ubiquitously in all kinds of datasets, significantly degrading the learning performance of deep networks. Learning with Label Noise (LNL) has been a common strategy for mitigating the influence of noisy labels. However, existing LNL methods either require pertaining using the memorization effect to separate clean data from noisy ones or rely on dataset assumptions that cannot extend to various scenarios. Thanks to the development of Multimodal Large Language Models (MLLMs) which possess massive knowledge and hold In-Context Learning (ICL) ability, this paper proposes NoiseGPT to effectively leverage MLLMs as a knowledge expert for conducting label noise detection and rectification. Specifically, we observe a probability curvature effect of MLLMs where clean and noisy examples reside on curvatures with different smoothness, further enabling the detection of label noise. By designing a token-wise Mix-of-Feature (MoF) technique to produce the curvature, we propose an In-Context Discrepancy (ICD) measure to determine the authenticity of an image-label pair. Subsequently, we repeat such a process to find the best matching pairs to complete our label rectification. Through extensive experiments, we carefully demonstrate the effectiveness of NoiseGPT on detecting and cleansing dataset noise, especially on ILSVRC12, the AUROC of NoiseGPT reached over 0.92. And by integrating with existing methods, the classification performance can be significantly improved on noisy datasets, typically by $22.8\\%$ on $80\\%$ symmetric CIFAR-10 with M-correction. Source code: https://github.com/drunkerWang/NoiseGPT_demo.git ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Contemporary machine learning is greedy for high-quality datasets. However, large-scale datasets such as human-annotated ones like ImageNet [1] and COCO [2] or internet-downloaded ones like WebVision [3] and Instagram Datasets [4] are either resource-consuming or untrustworthy. As a result, practitioners often have to spend substantial time to conduct prolonged labeling process and the results could still be undesirable. Noise still ubiquitously exist in almost all kinds of datasets. More importantly, dataset noise has been demonstrated to be significantly harmful for training of Deep Learning Models [5]. Consequently, it is urgent to discover efficient and transferable methodology to identify and rectify dataset noise. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To solve this problem, Learning with Noisy Labels (LNL) aims to improve the robustness of Deep Neural Networks (DNNs) through bridging the noise distribution and clean distribution. Existing methodologies [6, 7] commonly leverage loss correction and loss reweighting techniques, where transition matrix [8, 9], for example, is used to capture the noise generation process which enables endto-end optimization without ftiting to noisy labels. Meanwhile, based on the observation that DNNs converge faster on clean examples than noisy ones, sample-selection-based methods [10, 11, 12] divide samples into clean and noisy during training in order to learn from confident examples while exploiting noisy samples [13, 14, 15, 16, 17]. However, it is challenging to estimate accurate noise transition matrix due to the complexity of real-world noise generation process. And the performance of approximation struggles under high-level noise rate without strong dataset assumptions. Consequently, existing methods are largely limited and require to be improved or assisted. ", "page_idx": 1}, {"type": "text", "text": "Thanks to the development of MLLMs [18, 19, 20, 21, 22] which have been trained on massive data to effectively fit various real-world data distribution, we propose to leverage MLLMs as knowledge experts to help reduce dataset noise. Based on the empirical findings that noisy data are distributed on different MLLMs probability curvature from clean data, we propose an intuitive hypothesis that MLLMs are inherently optimized for the matching of image-text pairs. Such hypothesis motivates us to propose NoiseGPT which leverages a novel In-Context Discrepancy (ICD) criteria to identify and rectify noisy examples. Particularly, given an example which is in context with its label from dataset, if they match with each other, the MLLM output is stable under perturbation. Conversely, if the image and label are unmatched, the MLLM output will be sensitive to input perturbations. In circumstances where a sample is regarded as noisy, we further leverage CLIP [23] as zero-shot classifier to generate candidate labels. ICD is also applied on these candidates to elect a corrected label with best score. Through extensive studies on datasets such as conventional corrupted versions of CIFAR-10, CIFAR-100 [24], ImageNet ISCVRC2012, as well as real-world grounded datasets like CIFAR-N [25], WebVision [3], the efficacy of NoiseGPT is rigorously validated. As a zero-shot data cleansing method, NoiseGPT demonstrates its powerful ability to scalably distill significantly cleaner versions of noisy datasets than their original ones. Furthermore, our algorithm can be embedded into other LNL methods to further enhance the noisy learning performance under various LNL scenarios. To conclude, our contributions in this paper are as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We introduce MLLMs as machine experts to cope with noisy labels for the first time, potentially mitigating the reliance on human labor.   \n\u2022 We propose NoiseGPT to tackle the challenge of label noise by leveraging zero-shot capability of MLLMs. We employ a novel In-Context Discrepancy (ICD) criteria combined with the token-wise Mixture-of-Feature (MoF) technique to quantify the possibility discrepancy and identify noisy samples. Additionally, these noisy samples are recycled after label rectification by comparing ICD scores among candidate labels generated by zero-shot classifier CLIP.   \n\u2022 We conduct intricate investigations to evaluate the effectiveness of NoiseGPT. Furthermore, we integrate NoiseGPT as an auxiliary data cleansing method alongside existing LNL algorithms to validate its performance improvement. Additionally, we conduct performance analysis comprehensively understand the details of NoiseGPT. ", "page_idx": 1}, {"type": "text", "text": "2 Related works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Learning with noisy labels ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Existing LNL methods can be categorized into three types, data cleaning, loss-adjustment based approaches and sample-selection based approaches. Data cleaning endeavors to filter out examples whose labels are likely to be corrupted [26, 27]. Previous works in this branch leverages various methods [28, 29, 30] such as bagging, boosting, K-nearest neighbor, anomaly detection to exclude falselabeled instances. However, these methods tend to over-clean samples that are even true-labeled, resulting in aggravation of shortage of data in many cases. Tendencies of probability curvatures of DNNs [31, 32, 33] during training are also utilized to filter noisy examples. However, their robustness is strongly correlative to the training setting. Loss-adjustment based approaches focus on modifying the loss items before updating the DNNs, including loss correction and loss reweighting. Based on the fact that DNNs using Cross Entropy(CE) loss are prone to overfit noisy data [5], substantial researches have been conducted to design a robust corrected loss [34, 35] by leveraging transfer learning [36, 37, 38, 39], where noise transition matrix [8, 40, 41, 42] has been utilized. However, the performance of loss correction is highly dependent on the precisely-estimated transition matrix, which is hard under heavy noise and large number of classes. And the correction errors will be accumulated during training. Loss reweighting, on the other hand, aims at attributing weighted importance to examples in a designed training scheme to separate clean and noisy examples [43, 44, 45, 46]. Rectifying vectors [47] are also used to guide the classification network through leveraging information from input of the logits and labels. Nonetheless, this approach requires particular reweighting functions and hyperparameters for different noise type and datasets, limiting its practical implementation. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Another line of work, sample-selection, endeavors to identify true-labeled examples from noisy training datasets during training. Researches [48, 49, 13, 50] distinguish clean examples by their early-stage loss in DNNS utilizing memorization effect [51, 52]. Multi-network learning [53, 54, 11] simultaneously trains an additional network to guide the sample-selection. Mix-up [15] employs a semi-supervised mixture model [55] to separate clean and noisy examples, which is integrated into a multi-network framework in DivideMix [13]. SELF [56] leverages unsupervised loss from unlabeled examples while maintaining a running average model called mean-teacher [57, 58]. ProMix [59] leverages the utilities of clean examples by training balanced and unbiased classifiers in a self-supervised framework on separated sub-datasets. Recently, RoCL [60] combines supervised and semi-supervised learning in a two-stage training strategy to exploit both selected clean examples and relabeled examples. Despite their achievement, existing sample-selection methods are intrinsically linked to specific classification tasks, incapable to explicitly provide cleaned versions of datasets in various scenarios. To cope with this, our work aims to provide a transferable noise reduction paradigm to detect and rectify noisy labels by leveraging the zero-shot capability of MLLMs. ", "page_idx": 2}, {"type": "text", "text": "2.2 Multi-modal models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Recent years, Large Language Models [61, 62, 63, 64, 65, 66, 67] have been successfully applied across different tasks of natural language processing. Concurrently, the arising of Vision Transformers (ViTs) [68] has significantly advanced the development of visual models. In order to align the representations of image and text, CLIP [23] utilizes unsupervised learning by training separate image and text encoders with a contrastive loss on substantial image-text pairs. Besides, researches also endeavor to augment LLM with pretrained visual models to obtain multi-modal LLMs, known as MLLMs [69, 70, 71]. InstructBLIP [72] focuses on equipping MLLMs with the capacity to follow human-instructions on a natural-language interface. The mPLUG-Owl [73] introduces a modularized multi-modal pretraining paradigm to boost its transferability. MMICL [21] proposes a novel context training scheme, allowing the insertion of image features at any position among input text tokens. Based on the powerful zero-shot in-context capability of MLLMs, we propose NoiseGPT, leveraging extensive knowledge acquired from vast multi-modal training examples, to detect label noise. Subsequently, we utilize the zero-shot classification capability of CLIP for candidate labels and compare their matching rates with the input image to derive a rectified label. ", "page_idx": 2}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we propose NoiseGPT for label noise detection and rectification, as shown in Figure 1. Given a classifier and an MLLM model parameterized by $\\psi$ and $\\theta=\\{\\theta^{e n c},\\theta^{d e c}\\}$ , respectively, and a small clean exemplar dataset $\\mathcal{D}^{e x}$ with several examples per category to provide prompt for MLLMs, we take advantage of an intriguing Probability Curvature effect of MLLM where clean examples $x^{c l e a n}$ and noisy examples $x^{n o i s y}$ lead to different prediction discrepancies under perturbation, e.g., $\\mathbb{E}_{\\tilde{x}_{i}^{n o i s y}\\sim p(\\tilde{x}^{n o i s y}|x^{n o i s y})}d(x^{n o i s y};\\tilde{x}_{i}^{n o i s y})\\,<\\,\\mathbb{E}_{\\tilde{x}_{i}^{c l e a n}\\sim p(\\tilde{x}^{c l e a n}|x^{c l e a n})}d(x^{c l e a n};\\tilde{x}_{i}^{c l e a n}),$ where $d(\\cdot;\\cdot)$ denotes a novel In-Context Discrepancy (ICD) measure and $\\tilde{x}$ stands for the perturbation of an example under distribution $p(\\tilde{x}|x)$ . Based on such an effect, we can successfully detect whether a given image-label example pair $\\boldsymbol{x}\\,=\\,\\{\\mathbf{x},\\tilde{y}\\}$ has clean labels, i.e., whether $\\tilde{y}$ matches with $\\mathbf{x}$ . By further exploring the probable class candidates of the classifier, we can effectively find the ground truth label $y$ by choosing the best-matching category. Hence, our NoiseGPT can assist as a dataset cleanser without human intervention. ", "page_idx": 2}, {"type": "image", "img_path": "VRRvJnxgQe/tmp/3bfa1d8b20437c1492fcc32669cf666a1e3f9b998795c37a43bc6473ef28fdd1.jpg", "img_caption": ["Figure 1: We leverage the zero-shot ability of MLLM to examine whether an example pair is noisy. To identify the potential noise, NoiseGPT first perturbs a given example $x$ and produces a set of augmented versions $\\tilde{x}$ via using a novel token-wise Mixture-of-Feature (MoF) technique. Then by comparing the Softmax probabilities $q_{\\theta}$ between $x$ and $\\tilde{x}$ , we can calculate an In-Context Discrepancy (ICD) measure to further decide the authenticity of the given label $\\tilde{y}$ . "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Generally, NoiseGPT includes two stages: 1) Noise detection and 2) Label rectification. Next, we first demonstrate the noise detection process in Section 3.1, and then we carefully elucidate the details of label rectification in Section 3.2. ", "page_idx": 3}, {"type": "text", "text": "3.1 Noise Detection ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we propose to conduct noise detection based on the probability curvature effect. Specifically, we observe that clean examples lie on a smooth and convex probability curvature, and noisy examples fall on fluctuated and non-convex curvature, as shown in Figure 2. A similar effect has also been found in Mitchell et al. [74]. As a result, under slight perturbation, the probability value would change differently, which allows us to identify dataset noise. To utilize such an effect, we first conduct a novel token-wise Mixture-ofFeature (MoF) to perturb each example, then, the In-Context Discrepancy (ICD) measure can be calculated to assist detection. ", "page_idx": 3}, {"type": "image", "img_path": "VRRvJnxgQe/tmp/28bc5fcc9ead2877775e84fb60f01d82e288962e847f69ba24e57f0daaf57335.jpg", "img_caption": ["Figure 2: We demonstrate the distinctive curvatures of clean and noisy examples through an experiment. Ten perturbed exemplars are generated for a clean and a noisy sample from CIFAR-100 respectively. The MLLM output Softmax probability $q_{\\theta}(\\tilde{\\mathbf{x}}|\\tilde{y})$ of perturbed clean exemplars $\\tilde{\\mathbf{x}}^{c l e a n}\\sim p(\\cdot)$ (left) reside within a convex region on the curvature; While those of noisy exemplars $\\tilde{\\mathbf{x}}^{n o i s y}\\tilde{\\sim}p(\\cdot)$ (right) tend to cluster around the original point, posing lower or higher probability. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Mixture-of-Feature aims to perturbate or augment examples at the feature level through interpolation, segmentation, and partial substitution, which avoids changing in the input space as it might cause a mismatch between modalities [75]. Since dataset noise owes to the image example possessing some confusing features that resemble other noisy classes, we propose to mix the features between a given query example $x$ and exemplars $x^{e x}\\in\\mathcal{D}^{e x}$ from other noisy classes to inject noisy signals. Specifically, our MoF process is formulated as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\widetilde{\\mathbf{z}}=p(\\mathbf{z}|\\mathbf{z}^{e x})\\triangleq\\mathbf{z}[\\mathbb{I}(\\mathbf{m}=1)]\\oplus\\mathbf{z}^{e x}[\\mathbb{I}(\\mathbf{m}=0)],\\mathrm{where~}p(\\mathbf{m}=1)=w,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "image", "img_path": "VRRvJnxgQe/tmp/40ca92813a545b045f6fa786f8e10925012c3c61a0b8ee35129398e5f75f8ac7.jpg", "img_caption": ["ICD score under perturbation produced by MLLM "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 3: We conduct validation experiments on six noisy datasets using ICD score, namely CIFAR$10\\;\\mathrm{{Sym}}$ . $50\\%$ \u201e CIFAR- $100\\;\\mathrm{{Sym}}$ . $50\\%$ \u201e CIFAR-10N with worse labels, CIFAR-100N with noisy labels, Webvision Sym. $40\\%$ \u201e ISLVRC12 Sym. $40\\%$ ,. We collect scores of 10,000 clean samples and 10,000 noisy samples from each dataset, and each sample is augmented by 10 perturbed exemplars with a MoF weight of 0.5. ", "page_idx": 4}, {"type": "text", "text": "where $\\mathbf{z}$ and ${\\bf z}^{e x}$ are the latent representations of input images $\\mathbf{x}\\in x$ and ${\\bf x}^{e x}\\in x^{e x}$ correspondingly extracted by the vision encoders of MLLMs, $\\mathbf{z}[\\mathbb{I}(\\cdot)]$ denotes the indexing operation that selects the elements from ${\\bf z}$ where an indexing function $\\mathbb{I}(\\cdot)$ holds true, $\\oplus$ stands for element-wise addition, $p(\\cdot)$ stands for the MoF function, and $\\mathbf{m}$ indicates a binary mask to select feature elements, which is controlled by probability $w$ to trade off between $\\mathbf{z}$ and ${\\bf z}^{e x}$ , larger $w$ reserves more information from the original input in the mixed feature $\\tilde{\\mathbf{z}}$ . ", "page_idx": 4}, {"type": "text", "text": "Different from the MoF process conducted using intermediate features in Tong et al. [75] which might break the image patterns further causing misalignment with the subsequent LLM decoder, we propose a token-wise MoF operation that mixes the features by replacing tokens. Intuitively, the latent embeddings $\\mathbf{z}$ of MLLMs are sequentialized tokens, each containing specific semantic information. Thus, we conduct MoF from the token level, which can correspondingly perturbate semantic parts across different example pairs. In this way, the mixed feature $\\tilde{\\mathbf{z}}$ can successfully inherit noisy information and still be aligned with the latent MLLM feature space. After producing mixed features $\\tilde{\\mathbf{z}}$ as the perturbed versions of original ones $\\mathbf{z}$ , we combine them with the embeddings of both prompt $s$ and label $\\tilde{y}$ and feed them to the MLLM decoder module to obtain the final prediction as $q_{\\theta^{d e c}}\\{s,\\mathbf{z},\\tilde{y}\\}$ . Next, we leverage the In-Context Discrepancy (ICD) criteria to detect dataset noise. ", "page_idx": 4}, {"type": "text", "text": "In-Context Discrepancy is based on the probability curvature effect where perturbation affects the prediction probabilities of clean and noisy examples differently. Formally, our ICD criteria is calculated as ", "page_idx": 4}, {"type": "equation", "text": "$$\nd(s,\\mathbf{z},\\tilde{y};s,\\tilde{\\mathbf{z}},\\tilde{y})\\triangleq q_{\\theta^{d e c}}(s,\\mathbf{z},\\tilde{y})-\\mathbb{E}_{\\tilde{\\mathbf{z}}\\sim p(\\tilde{\\mathbf{z}}|\\mathbf{z})}q_{\\theta^{d e c}}(s,\\tilde{\\mathbf{z}},\\tilde{y}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $d(\\cdot;\\cdot)$ denotes the ICD function calculated between two entries, and $p(\\tilde{\\mathbf{z}}|\\mathbf{z})$ is the perturbation distribution for generating $\\tilde{\\mathbf{z}}$ . Intuitively, MLLMs are convexly optimized to associate visual features with corresponding text labels. If an example pair $x$ is clean, $i.e.$ , image $\\mathbf{x}$ and label $\\tilde{y}$ are matched, it will reside in an extrema where its local probability curvature is smooth and convex. On the other hand, if $x$ is a noisy point where the $\\mathbf{x}$ and $\\tilde{y}$ are mismatched, it might fall on unstable and non-convex curvature. ", "page_idx": 4}, {"type": "text", "text": "By injecting a little perturbation as done by the previous MoF process, we observe that the MLLM prediction $q_{\\theta}$ of clean examples will gradually decrease, but for noisy examples, $q_{\\theta}$ would seriously oscillate to a random value around its original one, as depicted in Figure 2. Therefore, our ICD results for clean examples are always positive and relatively larger than noisy ones, further effectively validating the authenticity of input example pairs. To rescale the ICD score of different examples, we further conduct normalization by dividing the results of Eq. 2 by a standard deviation of $q_{\\theta^{d e c}}(s,\\tilde{\\mathbf{z}},\\tilde{y})$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\bar{d}(s,\\mathbf{z},\\tilde{y};s,\\tilde{\\mathbf{z}},\\tilde{y})\\triangleq\\frac{q_{\\theta^{d e c}}(s,\\mathbf{z},\\tilde{y})-\\mathbb{E}_{\\tilde{\\mathbf{z}}\\sim p(\\tilde{\\mathbf{z}}|\\mathbf{z})}q_{\\theta^{d e c}}(s,\\tilde{\\mathbf{z}},\\tilde{y})}{\\sqrt{\\mathbb{E}_{\\tilde{\\mathbf{z}}\\sim p(\\tilde{\\mathbf{z}}|\\mathbf{z})}\\left[q_{\\theta^{d e c}}(s,\\tilde{\\mathbf{z}},\\tilde{y})-\\mathbb{E}_{\\tilde{\\mathbf{z}}\\sim p(\\tilde{\\mathbf{z}}|\\mathbf{z})}q_{\\theta^{d e c}}(s,\\tilde{\\mathbf{z}},\\tilde{y})\\right]^{2}}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "As a result, we can effectively divide clean examples and noisy ones based on the distribution of $\\bar{d}(z,\\tilde{z})$ as illustrated in Figure 3. Moreover, the effectiveness of such an effect is carefully validated on various datasets under MMICL [76], a state-of-the-art MLLM that supports effective image-text in-context learning (ICL) in Section 4. In practice, we set a threshold $\\tau$ to decide whether an example pair is noisy or not: those with discrepancy scores larger than $\\tau$ are considered as clean, otherwise, we further conduct label rectification. ", "page_idx": 4}, {"type": "text", "text": "Thanks to the previous noise detection process, we can effectively validate the authenticity of a given example pair $x=\\{\\mathbf{x},\\tilde{y}\\}$ . Moreover, we hope to find potentially correct labels $y$ to rectify the noisy ones $\\tilde{y}$ Particularly, we repeat the noise detection process for $C$ probable noisy categories to find the most likely label $y$ , formally ", "page_idx": 5}, {"type": "equation", "text": "$$\ny\\triangleq\\arg\\operatorname*{max}\\{\\bar{d}(s,\\mathbf{z},\\tilde{y};s,\\tilde{\\mathbf{z}},\\mathbf{y}_{j}^{p r e d})\\}_{j=0}^{C},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "(4) where $\\mathbf{y}^{p r e d}$ indicates the most probable candidate labels of $\\mathbf{x}$ selected by the classifier $\\psi$ where the subscript denotes the $j$ -th entry. Since various datasets have different numbers of classes, it is infeasible to traverse all classes. Hence, we leverage the predictions of classifier models such as CLIP [23] and select top- $C$ classes as the candidate labels i.e., $\\mathbf{y}^{\\bar{p}r e d}\\mathbf{\\Phi}=\\,[\\mathrm{argsort}(g_{\\psi}(\\mathbf{x}))]_{0:C}$ where $[\\mathrm{argsort}(\\cdot)]_{0:C}$ finds the index of top- $C$ elements. ", "page_idx": 5}, {"type": "text", "text": "Additionally, to ensure the MLLM output is mapped to a certain probability space in order to provide unified measurement for ", "page_idx": 5}, {"type": "text", "text": "Algorithm 1 NoiseGPT: noise identification and rectification. ", "page_idx": 5}, {"type": "text", "text": "Input: sample $_x$ and label $y$ from dataset $\\mathcal{D}^{s e t}$ , MLLM $q_{\\theta}$ , pretrained multi-classifier $q_{c v}$ , perturbation function $p$ , weight of MoF $w$ , number of perturbations $_n$ , number of candidate labels $C$ , threshold $\\tau$   \n1: Uniformly sample $\\rho C$ examplers with ground truth labels from $\\mathcal{D}^{s e t}$ to construct a tiny support set $\\mathcal{D}^{e x}$ ;   \n2: for $_x$ in $\\mathcal{D}^{s e t}$ do   \n3: $\\mathbf{x}_{i}\\sim p(\\mathbf{x}|\\tilde{\\mathbf{x}}_{i}^{e x}),i\\in[1,n]$ $\\triangleright$ token-wise MoF   \n4: \u00b5\u02dc \u2190 1n q\u03b8(xi, y\u02dc) \u25b7approximate expectation in Eq. 2   \n5: $\\begin{array}{l}{{d_{x}}\\gets q_{\\theta}(\\mathbf{x}_{i},\\tilde{y})-\\tilde{\\mu}}\\\\ {{\\tilde{\\sigma}_{x}}^{2}\\gets\\frac{1}{n}\\sum_{i=1}^{n}(q_{\\theta}(\\tilde{\\mathbf{x}}_{i},\\tilde{y})-\\tilde{\\mu})^{2}}\\\\ {{\\bar{d}_{x}}\\gets\\frac{{d_{x}}}{\\sqrt{\\tilde{\\sigma}_{x}^{2}}}}\\\\ {{.}}\\end{array}$   \n6:   \n7: \u25b7normalization   \n8: if $\\bar{d}_{x}>\\tau$ then   \n9: Accept current $\\tilde{y}$ as correct label   \n10: else   \n11: $\\begin{array}{r l}&{\\{y_{j}^{p r e d}\\}^{C}\\leftarrow q_{c v}(x)\\quad\\mathsf{>a c q u i r e~c a n d i d a t}}\\\\ &{\\mathbf{\\dot{or}}\\:j\\in0,1,\\cdots\\,,c\\:\\mathbf{do}}\\\\ &{\\quad\\,\\bar{d}_{j}\\sim\\bar{d}(\\mathbf{x},\\tilde{y};\\tilde{\\mathbf{x}},y_{j}^{p r e d}),j\\in[1,C]}\\\\ &{\\quad\\,y\\leftarrow\\arg\\operatorname*{max}\\{\\bar{d}(\\mathbf{z},\\tilde{y};\\tilde{\\mathbf{z}},\\mathbf{y}_{j}^{p r e d})\\}_{j=0}^{C}}\\\\ &{\\quad\\,\\bar{\\mathbf{z}}\\cdots\\:\\mathbf{arg}\\operatorname*{max}\\{\\bar{d}(\\mathbf{z},\\tilde{y};\\tilde{\\mathbf{z}},\\mathbf{y}_{j}^{p r e d})\\}_{j=0}^{C}}\\end{array}$ e labels   \n12:   \n13:   \n14:   \n15: end for   \n16: end if   \n17: end for ", "page_idx": 5}, {"type": "text", "text": "every example, we conduct prompting to restrict the output to only binary words, i.e., True or False. Moreover, our prompt leverages the ICL ability of MLLMs by providing both correct and incorrect matching exemplars. Our prompt is shown below: ", "page_idx": 5}, {"type": "text", "text": "Question: This image <IMG_label# $i>$ shows a photo of $\\;{\\mathrm{<label}}\\#i>$ , True or False? Answer: True; Question: This image <IMG_label# $j>$ shows a photo of $<\\!\\mathrm{label}\\#i>$ , True or False? Answer: False; Question: This image <IMG_query $>$ shows a photo of $\\mathtt{<l a b e l\\#i>}$ , True or False? Answer: ", "page_idx": 5}, {"type": "text", "text": "Specifically, for a query example $x=\\{{\\mathrm{IMG}}_{-}{\\mathrm{query}},{\\mathrm{label}}\\#i\\}$ whose label prediction from classifier is $i:=[\\mathrm{argsort}(g_{\\psi}(\\mathbf{x}))]_{0}$ , we choose one image IMG_label# $i\\in\\mathcal{D}^{e x}$ to match with label label#i as a True exemplar. Moreover, we choose another image IMG_label $\\neq j\\in\\mathcal{D}^{e x}$ where $j\\in\\{0,\\cdots,C\\}$ and $j\\neq i$ as a False exemplar. As shown in Huang et al. [77], such prompt design can effectively teach MLLMs what kind of image-label combination is True or False. As a result, the prediction $q_{\\theta}$ for the query example is based on both the inherent knowledge of MLLMs and the demonstration provided by the prompt. Furthermore, our NoiseGPT restricts $q_{\\theta}$ to binary values which significantly stabilizes the MLLM outputs. As revealed by existing studies [77, 78], providing a set of class candidates and asking which one is the ground truth shows sub-optimal performance. The reason is that current MLLMs cannot effectively conduct multi-class classification and it gets easily confused when facing various choices. Therefore, we only require MLLMs to output binary prediction under demonstrative exemplars which unleashes the power of ICL and benefits making trustworthy inferences. We summarize our methodology in Algorithm 1. Next, we empirically validate the proposed NoiseGPT. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we first introduce the specifics of our experiment setup. Then we validate the efficacy of our method through an investigation with regard to noise identification and rectification. Furthermore, we undertake a quantitative analysis to demonstrate the enhancing effects of NoiseGPT as data cleansing method through comparing contemporary state-of-the-art LNL models with their combined counterparts with our methodology. Finally, we conduct ablation studies to fully explore the performance of our approach. ", "page_idx": 5}, {"type": "text", "text": "4.1 Experiments setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Datasets In our experiments, we leverages re-annotated noisy datasets CIFAR-10N and CIFAR100N [25] which contain real-world human annotation errors. We also generate noisy versions of CIFAR-10, CIFAR-100 [24], WebVision [3] and ImageNet ILSVRC2012 for our studies. The details are as follows: ", "page_idx": 6}, {"type": "text", "text": "CIFAR-10N and CIFAR-100N [25]: We adopt Aggregate, Rand1 and Worst versions of CIFAR10N whose noise rates are $9.03\\%$ , $17.23\\%$ and $40.21\\%$ respectively and Noisy-Fine version of CIFAR-100N whose noise rate is $40.20\\%$ . ", "page_idx": 6}, {"type": "text", "text": "CIFAR-10 and CIFAR-100 [24]: The CIFAR-10 contains 50,000 labeled images of 10 different classes, while CIFAR-100 contains 100 classes, each with 500 images. We mannually inject $20\\%$ , $50\\%$ , $80\\%$ , $90\\%$ symmetric and $40\\%$ asymmetric noise into CIFAR-10 and CIFAR-100 respectively. ", "page_idx": 6}, {"type": "text", "text": "WebVision [3]: We utilize its validation subset which contains 50,000 images for the $40\\%$ symmetric noise condition. Moreover, in order to verify the capacity of NoiseGPT under the real-world circumstance where samples are collected without careful annotation, we utilize mini-Webvision, a subset of Webvision, for noise detection and rectification experiments and test the classification performance on the validation set of Webvision. ", "page_idx": 6}, {"type": "text", "text": "ImageNet ILSVRC2012: We utilizes the validation subset which contains 50,000 images and generate symmetric noise for $50\\%$ examples in it. ", "page_idx": 6}, {"type": "text", "text": "Models Primarily, we leverage original CLIP models [23] as our multi-classifier. For MLLM backbone, we employ MMICL [76] which adopts vision encoder of BLIP-2 [70] and FLAN-T5-XXL [79] as the LLM. For comparison with previous LNL works, we consider two methods Pro-Mix [59] and M-correction [35] which train an 18-layer PreAct Resnet for classification tasks. ", "page_idx": 6}, {"type": "text", "text": "Evaluation settings For exemplars that are used in the in-context learning process, we select 3 images per category to construct a tiny groundtruth support set $\\left\\{x_{e}\\right\\}$ to simulate the scarceness of examples in real-world condition. examples are also selected from this support set to generate perturbed sample features. Specifically, for each query sample, we construct $n\\,=\\,10$ perturbed features with different perturbing resources from $\\{x_{e}\\}$ . For pseudo labels, we employ the top $C=3$ prediction of CLIP to conduct label rectification process. ", "page_idx": 6}, {"type": "image", "img_path": "VRRvJnxgQe/tmp/9f37c3cf36cbf4bfd6c191e186493feeedd8a7ad67c63b47b079d95f07f9a80d.jpg", "img_caption": ["Figure 4: The noise detection ROC curves. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "4.2 Performance analysis ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we aim to demonstrate the radical capability of NoiseGPT in detecting and rectifying label noise. Our experiments are conducted on 6 datasets: CIFAR-10N Aggregate, Rand1, Worst, CIFAR-100N Noisy, Webvisin Sym. $40\\%$ and ILSVRC12 Sym. $40\\%$ . The results of classification and rectification are recorded to evaluate the performance. ", "page_idx": 6}, {"type": "table", "img_path": "VRRvJnxgQe/tmp/ab04cc97d8132ea1e7c78a41887ab223dc95f945efb973d467a99128e16c1992.jpg", "table_caption": ["Table 1: Noise detection and rectification performance. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Effectiveness of noise detection For evaluation metrics, we utilize Area Under the Receiver Operating Characteristic curve (AUROC) to reflect the noise detection performance. AUROC is commonly used to evaluate the performance of binary classification models. Generally, a higher AUROC score (closer to 1) indicates better discrimination ability, while 0.5 suggests random guessing. For label rectification performance, we compare corrected labels of query examples with their true labels to obtain a rectification accuracy. ", "page_idx": 6}, {"type": "text", "text": "Table 1 shows the performance of NoiseGPT in noise detection and rectification, the first column shows AUROC scores and the second shows the label correction accuracy. Figure 4 shows the AUROC curves. Our NoiseGPT achieves over 0.83 and 0.78 AUROC score on CIFAR-10N and CIFAR-100N datasets. Especially for Webvision and ILSVRC12, their score reaches over 0.89, which demonstrates the efficacy of flitering out noisy examples. In terms of rectification, NoiseGPT achieves over $80\\%$ accuracy on CIFAR-10N datasets. To sum up, NoiseGPT is evident to detect label noise and further recycle most of the noisy examples by rectifying noisy labels to ensure the dataset quantity and quality. ", "page_idx": 7}, {"type": "text", "text": "Comparison with baselines We compare the detection performance with the baselines in a binary-classification manner with evaluation metrics such as Precision, Recall rate and F1 score with baseline methods. Experiments are conducted on CIFAR-10 sym. $80\\%$ dataset, where instances are categorized into 4 types: ", "page_idx": 7}, {"type": "text", "text": "true-clean, false-clean, true-noisy, and false", "page_idx": 7}, {"type": "table", "img_path": "VRRvJnxgQe/tmp/590d47e056a698cd7a86c4eccb50a7b57c8f55879d2c27cd8b374812ce3f4bd3.jpg", "table_caption": ["Table 2: Detection performance comparison. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "noisy. Table 2 shows the results. Note that the detection scores of baselines are from their own detection modules. And the hyperparameters of NoiseGPT are fine-tuned to get the best scores. ", "page_idx": 7}, {"type": "text", "text": "4.3 Quantitative comparison ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "While NoiseGPT primarily focuses on zero-shot noise detection and rectification, existing research in Learning with Noisy Labels (LNL) has delved into training noise-robust Deep Neural Networks (DNNs) for classification tasks. To effectively showcase the advantages of our approach, we integrate NoiseGPT as a data cleansing method with two LNL baselines, namely Pro-Mix and M-correction, constituting \"NoiseGPT $^+$ Pro-Mix\" and \"NoiseGPT $\\bf\\Gamma+\\bf M$ -correction\", respectively. The classification models are trained under the same settings of as specified in papers of baselines. ", "page_idx": 7}, {"type": "table", "img_path": "VRRvJnxgQe/tmp/fb266512526be1097bf3c0c2d589d75ab78cee36d4f539c7743fb4e1c44c84cd.jpg", "table_caption": ["Table 3: Noise rectification results. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Comparison with classic noisy labels Our experiments are conducted on the CIFAR-10 and CIFAR-100 datasets, considering varying levels of symmetric and asymmetric noise. Table 3 shows the noise reduction effects of NoiseGPT, which is capable of improving the clean proportion within datasets. Note that the last row shows the number of clean examples after rectification. The improvement is particularly substantial for CIFAR-10 datasets with high noise rate. And for more challenging datasets of CIFAR-100, NoiseGPT still keeps its effectiveness across varing noise conditions. ", "page_idx": 7}, {"type": "text", "text": "Subsequently, we transfer cleaned datasets into classification training. We compare the performance of \"NoiseGPT $^{\\cdot}+$ \" with their baseline counterparts and other representative works in this field. Note that we re-produce the aforementioned Proto-Mix and M-correction. Due to the lack of detailed training setting information proposed in their papers, some of the re-produced results are not as fine-tuned as what in papers. Nonetheless, we conduct experiments of \"NoiseGPT+\" with same hyperparameters with their baseline counterparts on each dataset. Thus they objectively demonstrate the effect of NoiseGPT. Table 4 shows experimental results. NoiseGPT poses enhancing effects to LNL works in most noise conditions. Especially in high noise levels, the improvement of classification accuracy is increased by over $20\\%$ and $14\\%$ respectively for M-correction and Pro-Mix in CIFAR-10 Sym. 0.9. ", "page_idx": 7}, {"type": "table", "img_path": "VRRvJnxgQe/tmp/2a4ee0c92fa9b7e1f453aa957e8e4e9e06dcaeab6fac697b79b082a3d4732423.jpg", "table_caption": ["Table 5: Classification on Webvision. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "table", "img_path": "VRRvJnxgQe/tmp/09bdbad84d39060a9de7c938e1e504a44cd1ff0488714adff639f9e52ae8d49b.jpg", "table_caption": ["Table 4: Classification accuracy comparisons. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Comparison with real-world noisy labels In the real-world situation, some datasets prevailing recently are collected from the Internet, such as Webvision. Thus, they contain label noises that represent different patterns from the symmetric. In this paper, we utilize mini-Webvision, a smaller subset of Webvision to verify the capacity of NoiseGPT to combat label noises of this kind. Due to the lack of ground-truth annotations, we directly compare the classification performance with baselines. Following previous works [13], we use the Inception-ResNet v2 [80] as the classification backbone.The results in Table 5 demonstrate that NoiseGPT remains effective on real-world noisy datasets like Webvision. The \u201cNoiseGPT+\u201d denotes that the training set is first cleaned by NoiseGPT. ", "page_idx": 8}, {"type": "text", "text": "4.4 Sensitivity study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Effect of perturbation number Since we approximate the expectation in Eq. 2 with a sequence of perturbations, theoretically increasing the perturbation number $n$ will make the normalized ICD score more robust and effective in distinguishing clean and noisy examples. However, there is a marginal effect when $n$ increases to an extent and becomes computationally unworthy. Figure 5 shows the trend of noise rectification performance under a changing hyperparameter $n$ in the attachment. In our experiments, we select the number of perturbations in order to balance the computational cost and performance. ", "page_idx": 8}, {"type": "image", "img_path": "VRRvJnxgQe/tmp/65a3ef2134216c2ecc4cd1d3e477f8741d37a7384cf7da6e6c4a329754980373.jpg", "img_caption": ["Figure 5: Trend of performance under changing perturbation number. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.5 Ablation study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The existence of curvatures on different datasets We further demonstrate the quality of possibility curvature by conducting experiments where query examples from noisy dataset are augmented by a series of perturbed exemplars with varying perturbation strength. According to Section 3.1, we can control the perturbation strength by adjusting token-wise MoF weight $w$ , a larger $w$ indicates higher proportion of information coming from query example. We conduct this experiment on four datasets: CIFAR-10 Sym. $50\\%$ , CIFAR-100 Sym. $50\\%$ , Webvision Sym. $40\\%$ , and ILSVRC Sym. $40\\%$ . ", "page_idx": 8}, {"type": "text", "text": "Figure 6 shows the curvatures of output Softmax probability under changing MoF weights. We calculate the averaged $q_{\\theta}(x)$ of clean and noisy examples in each dataset respectively. It is investigated that on for clean examples, the output Softmax probability $q_{\\theta}^{c l e a n}(x)$ tends to descend as the MoF weight $w$ decreases. Conversely however, the curvature of noisy examples fluctuates optionally unconcerned with the change of weight $w$ . This phenomenon is confirmed on all datasets, firmly backing up our method, NoiseGPT, which utilizes the discrepancy of output Softmax probability $q_{\\theta}(x)$ under perturbation to distinguish between clean and noisy examples. ", "page_idx": 8}, {"type": "image", "img_path": "VRRvJnxgQe/tmp/9627c0cf4dec8bcb7692f5fa4a968b3b1931cd1a70f935646760319d9823a1c3.jpg", "img_caption": ["Figure 6: Output possibility curvatures of clean and noisy examples under different perturbation strength. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Clean classes that are easier to be considered noisy Attributing to the fact that clean and noisy examples have different ICD score distributions, our NoiseGPT is capable to detect and rectify noise. However, the ICD score distribution does not remain unchanged among different categories. Some categories of clean examples tend to have relatively higher ICD scores than others, which, in other words, are easier to be mistaken as noisy ", "page_idx": 9}, {"type": "image", "img_path": "VRRvJnxgQe/tmp/ba6a55fb44d5272fb14c793c32513313efa0ed38880aa343791620a0e7aeb115.jpg", "img_caption": [], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "during the process of NoiseGPT. Figure 7: Clean categories that are easier to be mistaken as noisy. We investigate such categories ", "page_idx": 9}, {"type": "text", "text": "over different datasets, their average ICD scores are recorded as a indication of to what extent they tend to be mistaken. Note that for clean examples, lower score indicates easier to be mistaken. Figure 7 shows which classes are easy to be mistaken for noisy. Further exploration of detection biases are provided in Section 5 in our Appendix. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Contribution In this work, we propose a novel label noise solution via leveraging MLLMs as experts to reduce and recycle noisy instances in datasets. Specifically, we investigate the probability curvature of MLLMs under input perturbation. Through a token-wise Miture-of-Feature technique, we can calculate ICD scores of input examples and divide them into clean and noisy. By conducting extensive quantitative and qualitative experiments on different datasets, our method is validated to sustain effective over varying noise conditions. Moreover, it surpasses previous LNL methods in noise detection and poses substantial potential to cope with other deep learning models to improve their performance. In the future, our method can be further explored for insights into more probelms like OOD detection, weakly-supervised learning, etc. ", "page_idx": 9}, {"type": "text", "text": "Limitation Despite its adaptability to various datasets and noise levels, the performance of NoiseGPT is constrained by the capabilities of the underlying machine expert it relies on. Research [75] has highlighted the bottleneck effect in vision models within MLLMs. Furthermore, the instruction-following capability of MLLMs significantly influences the distributions of ICD scores, which are closely tied to the confidence of MLLM answers. Enhancing the proficiency of these machine experts can lead to improved performance of NoiseGPT in its tasks. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115:211\u2013252, 2015. [2] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740\u2013755. Springer, 2014.   \n[3] Wen Li, Limin Wang, Wei Li, Eirikur Agustsson, and Luc Van Gool. Webvision database: Visual learning and understanding from web data. arXiv preprint arXiv:1708.02862, 2017. [4] Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, and Laurens Van Der Maaten. Exploring the limits of weakly supervised pretraining. In Proceedings of the European conference on computer vision (ECCV), pages 181\u2013196, 2018.   \n[5] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning (still) requires rethinking generalization. Communications of the ACM, 64(3):107\u2013115, 2021.   \n[6] Jacob Goldberger and Ehud Ben-Reuven. Training deep neural-networks using a noise adaptation layer. In International conference on learning representations, 2016.   \n[7] Daiki Tanaka, Daiki Ikami, Toshihiko Yamasaki, and Kiyoharu Aizawa. Joint optimization framework for learning with noisy labels. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5552\u20135560, 2018.   \n[8] Giorgio Patrini, Alessandro Rozza, Aditya Krishna Menon, Richard Nock, and Lizhen Qu. Making deep neural networks robust to label noise: A loss correction approach. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1944\u20131952, 2017.   \n[9] Hongxin Wei, Huiping Zhuang, Renchunzi Xie, Lei Feng, Gang Niu, Bo An, and Yixuan Li. Logit clipping for robust learning against label noise. 2022.   \n[10] Hwanjun Song, Minseok Kim, and Jae-Gil Lee. Selfie: Refurbishing unclean samples for robust deep learning. In International conference on machine learning, pages 5907\u20135915. PMLR, 2019.   \n[11] Eran Malach and Shai Shalev-Shwartz. Decoupling\" when to update\" from\" how to update\". Advances in neural information processing systems, 30, 2017.   \n[12] Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei. Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels. In International conference on machine learning, pages 2304\u20132313. PMLR, 2018.   \n[13] Junnan Li, Richard Socher, and Steven CH Hoi. Dividemix: Learning with noisy labels as semi-supervised learning. arXiv preprint arXiv:2002.07394, 2020.   \n[14] Haobo Wang, Ruixuan Xiao, Yiwen Dong, Lei Feng, and Junbo Zhao. Promix: Combating label noise via maximizing clean sample utility. arXiv preprint arXiv:2207.10276, 2022.   \n[15] David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A Raffel. Mixmatch: A holistic approach to semi-supervised learning. Advances in neural information processing systems, 32, 2019.   \n[16] Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels. Advances in neural information processing systems, 31, 2018.   \n[17] Xingrui Yu, Bo Han, Jiangchao Yao, Gang Niu, Ivor Tsang, and Masashi Sugiyama. How does disagreement help generalization against label corruption? In International conference on machine learning, pages 7164\u20137173. PMLR, 2019.   \n[18] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International conference on machine learning, pages 12888\u201312900. PMLR, 2022.   \n[19] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale N Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. Advances in Neural Information Processing Systems, 36, 2024.   \n[20] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024.   \n[21] Haozhe Zhao, Zefan Cai, Shuzheng Si, Xiaojian Ma, Kaikai An, Liang Chen, Zixuan Liu, Sheng Wang, Wenjuan Han, and Baobao Chang. Mmicl: Empowering vision-language model with multi-modal incontext learning. arXiv preprint arXiv:2309.07915, 2023.   \n[22] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023.   \n[23] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[24] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \n[25] Jiaheng Wei, Zhaowei Zhu, Hao Cheng, Tongliang Liu, Gang Niu, and Yang Liu. Learning with noisy labels revisited: A study using real-world human annotations. arXiv preprint arXiv:2110.12088, 2021.   \n[26] Virginia Wheway. Using boosting to detect noisy data. In Advances in Artificial Intelligence. PRICAI 2000 Workshop Reader: FourWorkshops held at PRICAI 2000 Melbourne, Australia, August 28-September 1, 2000 Revised Papers 6, pages 123\u2013130. Springer, 2001.   \n[27] Borut Sluban, Dragan Gamberger, and Nada Lavrac\u02c7. Ensemble-based noise detection: noise ranking and visual performance evaluation. Data mining and knowledge discovery, 28:265\u2013303, 2014.   \n[28] Sarah Jane Delany, Nicola Segata, and Brian Mac Namee. Profliing instances in noise reduction. KnowledgeBased Systems, 31:28\u201340, 2012.   \n[29] Dragan Gamberger, Nada Lavrac, and Saso Dzeroski. Noise detection and elimination in data preprocessing: experiments in medical domains. Applied artificial intelligence, 14(2):205\u2013223, 2000.   \n[30] Jaree Thongkam, Guandong Xu, Yanchun Zhang, and Fuchun Huang. Support vector machine for outlier detection in breast cancer survivability prediction. In Advanced Web and Network Technologies, and Applications: APWeb 2008 International Workshops: BIDM, IWHDM, and DeWeb Shenyang, China, April 26-28, 2008. Revised Selected Papers 10, pages 99\u2013109. Springer, 2008.   \n[31] Deepak Ravikumar, Efstathia Soufleri, Abolfazl Hashemi, and Kaushik Roy. Unveiling privacy, memorization, and input curvature links. arXiv preprint arXiv:2402.18726, 2024.   \n[32] Isha Garg, Deepak Ravikumar, and Kaushik Roy. Memorization through the lens of curvature of loss function around samples. arXiv preprint arXiv:2307.05831, 2023.   \n[33] Sheng Liu, Jonathan Niles-Weed, Narges Razavian, and Carlos Fernandez-Granda. Early-learning regularization prevents memorization of noisy labels. Advances in neural information processing systems, 33:20331\u201320342, 2020.   \n[34] Scott Reed, Honglak Lee, Dragomir Anguelov, Christian Szegedy, Dumitru Erhan, and Andrew Rabinovich. Training deep neural networks on noisy labels with bootstrapping. arXiv preprint arXiv:1412.6596, 2014.   \n[35] Eric Arazo, Diego Ortego, Paul Albert, Noel O\u2019Connor, and Kevin McGuinness. Unsupervised label noise modeling and loss correction. In International conference on machine learning, pages 312\u2013321. PMLR, 2019.   \n[36] Ziming Hong, Zhenyi Wang, Li Shen, Yu Yao, Zhuo Huang, Shiming Chen, Chuanwu Yang, Mingming Gong, and Tongliang Liu. Improving non-transferable representation learning by harnessing content and style. In The Twelfth International Conference on Learning Representations, 2024.   \n[37] Zhuo Huang, Muyang Li, Li Shen, Jun Yu, Chen Gong, Bo Han, and Tongliang Liu. Winning prize comes from losing tickets: Improve invariant learning by exploring variant parameters for out-of-distribution generalization. International Journal of Computer Vision, pages 1\u201319, 2024.   \n[38] Yexiong Lin, Yu Yao, Xiaolong Shi, Mingming Gong, Xu Shen, Dong Xu, and Tongliang Liu. Cs-isolate: Extracting hard confident examples by content and style isolation. Advances in Neural Information Processing Systems, 36, 2024.   \n[39] Tongliang Liu and Dacheng Tao. Classification with noisy labels by importance reweighting. IEEE Transactions on pattern analysis and machine intelligence, 38(3):447\u2013461, 2015.   \n[40] Dan Hendrycks, Mantas Mazeika, Duncan Wilson, and Kevin Gimpel. Using trusted data to train deep networks on labels corrupted by severe noise. Advances in neural information processing systems, 31, 2018.   \n[41] Xiaobo Xia, Tongliang Liu, Nannan Wang, Bo Han, Chen Gong, Gang Niu, and Masashi Sugiyama. Are anchor points really indispensable in label-noise learning? Advances in neural information processing systems, 32, 2019.   \n[42] Xiaobo Xia, Tongliang Liu, Bo Han, Nannan Wang, Mingming Gong, Haifeng Liu, Gang Niu, Dacheng Tao, and Masashi Sugiyama. Part-dependent label noise: Towards instance-dependent label noise. Advances in Neural Information Processing Systems, 33:7597\u20137610, 2020.   \n[43] Haw-Shiuan Chang, Erik Learned-Miller, and Andrew McCallum. Active bias: Training more accurate neural networks by emphasizing high variance samples. Advances in Neural Information Processing Systems, 30, 2017.   \n[44] Nikola Konstantinov and Christoph Lampert. Robust learning from untrusted sources. In International conference on machine learning, pages 3488\u20133498. PMLR, 2019.   \n[45] Sunil Thulasidasan, Tanmoy Bhattacharya, Jeff Bilmes, Gopinath Chennupati, and Jamal Mohd-Yusof. Combating label noise in deep learning using abstention. arXiv preprint arXiv:1905.10964, 2019.   \n[46] Guoqing Zheng, Ahmed Hassan Awadallah, and Susan Dumais. Meta label correction for noisy label learning. In Proceedings of the AAAI conference on artificial intelligence, volume 35, pages 11053\u201311061, 2021.   \n[47] Haoliang Sun, Chenhui Guo, Qi Wei, Zhongyi Han, and Yilong Yin. Learning to rectify for robust learning with noisy labels. Pattern Recognition, 124:108467, 2022.   \n[48] Yanyao Shen and Sujay Sanghavi. Learning with bad training data via iterative trimmed loss minimization. In International Conference on Machine Learning, pages 5739\u20135748. PMLR, 2019.   \n[49] Pengfei Chen, Ben Ben Liao, Guangyong Chen, and Shengyu Zhang. Understanding and utilizing deep neural networks trained with noisy labels. In International conference on machine learning, pages 1062\u20131070. PMLR, 2019.   \n[50] Xiaobo Xia, Tongliang Liu, Bo Han, Mingming Gong, Jun Yu, Gang Niu, and Masashi Sugiyama. Sample selection with uncertainty of losses for learning with noisy labels. arXiv preprint arXiv:2106.00445, 2021.   \n[51] Devansh Arpit, Stanis\u0142aw Jastrze\u02dbbski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al. A closer look at memorization in deep networks. In International conference on machine learning, pages 233\u2013242. PMLR, 2017.   \n[52] Hwanjun Song, Minseok Kim, Dongmin Park, and Jae-Gil Lee. How does early stopping help generalization against label noise? arXiv preprint arXiv:1911.08059, 2019.   \n[53] Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels. Advances in neural information processing systems, 31, 2018.   \n[54] Xingrui Yu, Bo Han, Jiangchao Yao, Gang Niu, Ivor Tsang, and Masashi Sugiyama. How does disagreement help generalization against label corruption? In International conference on machine learning, pages 7164\u20137173. PMLR, 2019.   \n[55] Zhuo Huang, Chao Xue, Bo Han, Jian Yang, and Chen Gong. Universal semi-supervised learning. Advances in Neural Information Processing Systems, 34:26714\u201326725, 2021.   \n[56] Duc Tam Nguyen, Chaithanya Kumar Mummadi, Thi Phuong Nhung Ngo, Thi Hoai Phuong Nguyen, Laura Beggel, and Thomas Brox. Self: Learning to fliter noisy labels with self-ensembling. arXiv preprint arXiv:1910.01842, 2019.   \n[57] Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems, 30, 2017.   \n[58] Mingyu Li, Tao Zhou, Zhuo Huang, Jian Yang, Jie Yang, and Chen Gong. Dynamic weighted adversarial learning for semi-supervised classification under intersectional class mismatch. ACM Transactions on Multimedia Computing, Communications and Applications, 20(4):1\u201324, 2024.   \n[59] Haobo Wang, Ruixuan Xiao, Yiwen Dong, Lei Feng, and Junbo Zhao. Promix: Combating label noise via maximizing clean sample utility. arXiv preprint arXiv:2207.10276, 2022.   \n[60] Tianyi Zhou, Shengjie Wang, and Jeff Bilmes. Robust curriculum learning: from clean label detection to noisy label self-correction. In International Conference on Learning Representations, 2020.   \n[61] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.   \n[62] Luciano Floridi and Massimo Chiriatti. Gpt-3: Its nature, scope, limits, and consequences. Minds and Machines, 30:681\u2013694, 2020.   \n[63] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.   \n[64] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.   \n[65] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \n[66] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023.   \n[67] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in neural information processing systems, 35:23716\u201323736, 2022.   \n[68] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.   \n[69] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International conference on machine learning, pages 12888\u201312900. PMLR, 2022.   \n[70] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pages 19730\u201319742. PMLR, 2023.   \n[71] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024.   \n[72] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale N Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. Advances in Neural Information Processing Systems, 36, 2024.   \n[73] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023.   \n[74] Eric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher D Manning, and Chelsea Finn. Detectgpt: Zero-shot machine-generated text detection using probability curvature. In International Conference on Machine Learning, pages 24950\u201324962. PMLR, 2023.   \n[75] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. arXiv preprint arXiv:2401.06209, 2024.   \n[76] Haozhe Zhao, Zefan Cai, Shuzheng Si, Xiaojian Ma, Kaikai An, Liang Chen, Zixuan Liu, Sheng Wang, Wenjuan Han, and Baobao Chang. Mmicl: Empowering vision-language model with multi-modal incontext learning. arXiv preprint arXiv:2309.07915, 2023.   \n[77] Zhuo Huang, Chang Liu, Yinpeng Dong, Hang Su, Shibao Zheng, and Tongliang Liu. Machine vision therapy: Multimodal large language models can enhance visual robustness via denoising in-context learning. arXiv preprint arXiv:2312.02546, 2023.   \n[78] Yuexiang Zhai, Shengbang Tong, Xiao Li, Mu Cai, Qing Qu, Yong Jae Lee, and Yi Ma. Investigating the catastrophic forgetting in multimodal large language models. arXiv preprint arXiv:2309.10313, 2023.   \n[79] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. Journal of Machine Learning Research, 25(70):1\u201353, 2024.   \n[80] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander Alemi. Inception-v4, inception-resnet and the impact of residual connections on learning. In Proceedings of the AAAI conference on artificial intelligence, volume 31, 2017. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we first introduce implementations details and computational usage of our experiment.   \nThen, we demonstrate more supplementary experiments to fully explore the qualities of NoiseGPT. ", "page_idx": 15}, {"type": "text", "text": "Implementation details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Among all hyperparameters, MoF weight $w$ , number of perturbation exemplars $n$ and number of candidate classes $C$ are of most significance. Theoretically, a larger $n$ avails the approximation in Eq. 2, and for datasets with more complicated classes a larger $C$ is suitable to search for appropriate label. Table 6 shows hyperparameters ", "page_idx": 15}, {"type": "table", "img_path": "VRRvJnxgQe/tmp/e2b9de27108b1b4597be8ec493f8493cfb06edcf2d35978ae5bfd6ea406cc3e8.jpg", "table_caption": ["Table 6: NoiseGPT hyperparameters. "], "table_footnote": ["pf our NoiseGPT experiments. Note that in some of the experiments, we fine-tune the threshold for better noise detection performance. "], "page_idx": 15}, {"type": "text", "text": "Compute resources ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Our noise detection and rectification experiments of NoiseGPT are powered by GeForce RTX 4090, taking up about 28.5 GiB memory in total for CIFAR datasets. The runtime (hours) of experiments are recorded under $n=10$ , $C=3$ , $w=0.5$ , and illustrated in Table 7. ", "page_idx": 15}, {"type": "table", "img_path": "VRRvJnxgQe/tmp/7383f7d17aa72017ffbd121236acaa0d0631acdc35dd6f99109fd7add2018a0e.jpg", "table_caption": ["Table 7: Runtime of NoiseGPT. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "More classification training performance ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In Section 4.3, we have compared the classification performance of \"NoiseGPT+\" and baselines on CIFAR-10/100 Sym./Asym. datasets. Here we expand this experiment on more datasets: CIFAR-10N Worst and CIFAR-100N Noisy. Table 8 shows the effects of NoiseGPT as a dataset cleansing method. And Table 9 shows the results of classification accuracy. ", "page_idx": 15}, {"type": "table", "img_path": "VRRvJnxgQe/tmp/9a98fc172f6be09efb6013a1b4e2775747a742c3681467a30eab3b3470c4ab0b.jpg", "table_caption": ["Table 8: Noise rectification results of CIFAR-N datasets. "], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "VRRvJnxgQe/tmp/2a0b637ac6ed4fc1a8d8e927c98318dbc4a1cf1d8cfc7b99327ec60081fd3d6f.jpg", "table_caption": ["Table 9: Classification accuracy on CIFAR-N datasets. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "More noise detection and rectification performance ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In section 4.2 we introduce AUROC and Label Correction Accuracy as the evaluating metrics for NoiseGPT noise detection and rectification performance. In order to better understand the improving impacts NoiseGPT poses to classification tasks, we further exhibit the evaluating results of NoiseGPT on CIFAR-10/100 Sym./Asym. ", "page_idx": 16}, {"type": "table", "img_path": "VRRvJnxgQe/tmp/aa7e8533a118b4ca5914bf73ffd7d2fb3574bb8d5bf180b058b898ef9fe24c27.jpg", "table_caption": ["Table 10: NoiseGPT performance on CIFAR datasets. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "datasets. Table 10 shows the noise detection and rectification results with AUROC score and correction accuracy. Figure 8 shows the ROC curves of noise identification. ", "page_idx": 16}, {"type": "image", "img_path": "VRRvJnxgQe/tmp/15905eb6108facb1bfa10c14798d307db0cc05a4027d003dbedd24e499f01b3a.jpg", "img_caption": ["Figure 8: The noise detection ROC curves of CIFAR datasets. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Noisy classes that are easier to be considered noisy ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In Section 4.5, we illustrate classes that are clean but easier to be mistaken for noisy. Similarly, some categories of noisy examples are easier to be considered clean, obtaining lower ICD scores than their fellows. Contrary to clean classes, higher score for noisy examples indicate that they are easy to be mistaken. ", "page_idx": 16}, {"type": "table", "img_path": "VRRvJnxgQe/tmp/38d66aefb3b412fb3a8bca94e59f77baa113773dae1e73cfeb21d784f03ef44c.jpg", "table_caption": ["Table 11: Comparison of detection bias. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Comparisons on the detection biases with the baseline ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The experiments in Section 4.5 indicate that there are essentially biases in the noise detection stage of NoiseGPT, which will lead to unbalanced example quantities of different classes after rectification. Similar phenomenon also appears in the baselines. Thus, here we compare the biases in noise detection between NoiseGPT and Proto-Mix, and calculate the proportions of 10 classes in selected clean data on CIFAR- $\\cdot10\\;\\mathrm{sym}$ . $90\\%$ . Table 11 shows the results. Our NoiseGPT exhibits significantly lower variance in the example distributions across different classes, indicating reduced bias in noise detection and rectification compared to Proto-Mix. ", "page_idx": 16}, {"type": "image", "img_path": "VRRvJnxgQe/tmp/f5f1af0c5086c61e7f28faf163fe74318ec9daf320997cbdfcc92bef1251e9d5.jpg", "img_caption": ["Figure 9: Noisy categories that are easier to be mistaken as clean. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: In Abstract and Section 1, we discuss about previous LNL methods, and the utilization of MLLMs in our work to detect and rectify noisy labels. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 18}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: In Section 5, we point out that our method is constrained by the performance of visual models, as their capability to extract prioritized information limit our NoiseGPT to work on complicated large-revolution datasets. Besides, the effects of prompt settings is also mentioned. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 18}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: We mainly propose the assumption of MLLM output probability curvature in Section 3, guiding the definition of ICD criteria, which is further proved with experiments in Section 3.1. The results of experiments in Section 4.5 also back up this theory. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 19}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: Our method is based on the zero-shot inference of backbone models. Following the experiment settings and hyperparameters mention in Section 4.1, all results of NoiseGPT can be reproduced. As for comparison baseline works, we follow the settings in their original codes and papers, and re-produce some of the results. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 19}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 20}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 20}, {"type": "text", "text": "Answer: [No] ", "page_idx": 20}, {"type": "text", "text": "Justification: So far we have not got ready for the open of our codes, due to time constraint.   \nWe will consider it latter. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 20}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: We introduce the effects of hyperparameters in Section 3 and make it clearly demonstrated in our experiment settings in Section 4.1. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 20}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [No] ", "page_idx": 20}, {"type": "text", "text": "Justification: Due to the lack of computational resources, we are not able to conduct statistical researches. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 21}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We introduce the running environment of our experiments and reference execution time of NoiseGPT on different datasets in the Appendix Section 5. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 21}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We fully understand the NeurIPS codes of ethics provided. And we will make it ensured from harmful consequences. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 21}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: In Section 5 we discuss the positive impacts our work may have on the future researches in the fields of noisy labels and machine learning. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 22}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: There is no risk our released work could have posed. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 22}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We state the reference of all the models and data clearly in our paper, which can be seen as cites in the main content and detailed in Reference. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: Our work does not contain such new assets that includes datasets or model frameworks. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 23}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: Our work involves no crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 23}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: Our work involves no crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}]