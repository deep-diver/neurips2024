[{"figure_path": "h15RyEj151/tables/tables_6_1.jpg", "caption": "Table 1: In accordance with our theory, the analyzed real SSM (see Section 2.2) cannot practically learn to closely approximate  (\u00b7) up to time t under important choices of  (\u00b7), even when t is moderate. This table reports the approximation error attained by the real SSM, i.e. the minimum  with which a mapping learned by the real SSM  -approximates  (\u00b7) up to time t (see Section 2.3), when t = 32 and  (\u00b7) varies over the following possibilities: the canonical copy (delay) mapping from Corollary 1; the random (generic) mapping from Corollary 2; and the basic oscillatory mapping from Corollary 3. Learning was implemented by applying one of three possible gradient-based optimizers-Adam [29], AdamW [36] or RAdam [33]-to a loss as in our theory (see Appendix C). For each choice of  (\u00b7), reported approximation errors are normalized (scaled) such that a value of one is attained by the trivial zero mapping. Each configuration was evaluated with five random seeds, and its reported approximation error is the minimum (best) that was attained. The dimension of the real SSM (NR) was set to 1024; other choices of dimension led to qualitatively identical results. For further implementation details see Appendix F.1.", "description": "This table shows the approximation error of a real SSM in approximating three different mappings (copy, random, oscillatory) up to time t=32.  Three different optimizers were used, and the minimum error across five random seeds is reported. The dimension of the SSM was 1024.", "section": "4.1 Theoretically Analyzed Setting"}, {"figure_path": "h15RyEj151/tables/tables_7_1.jpg", "caption": "Table 2: In contrast to the analyzed real SSM, and in alignment with our theory, the analyzed complex SSM (see Section 2.2) can practically learn to closely approximate  (\u00b7) up to time t under important choices of (\u00b7) and various choices of t. This table reports approximation errors attained by the complex SSM. It adheres to the description of Table 1, with the following exceptions (all designed to stress the superiority of the complex SSM over the real SSM): (i) only Adam optimizer was used; (ii) in addition to 32, t also took the values 64, 128 and 256; (iii) for each configuration, the reported approximation error is the maximum (worst) that was achieved across the random seeds; and (iv) the dimension of the complex SSM (nc) was set to t (higher dimensions led to qualitatively identical results). For further implementation details see Appendix F.1.", "description": "This table presents the approximation errors achieved by the complex SSM when learning to approximate various mappings up to time t.  It contrasts the results with those from the real SSM (Table 1) by using a single optimizer, testing at different times t, reporting the maximum errors instead of minimum, and setting the dimension to t (for better demonstration of complex SSM's superiority).", "section": "4.1 Theoretically Analyzed Setting"}, {"figure_path": "h15RyEj151/tables/tables_8_1.jpg", "caption": "Table 3: Ablation experiment demonstrating that real parameterizations can compare (favorably) to complex parameterizations for SSMs with selectivity, but complex parameterizations become superior when selectivity is fully or partially removed. This table reports test accuracies attained by a Mamba neural network [20] on a synthetic induction-head task regarded as canonical in the SSM literature [27, 20]. Evaluation included multiple configurations for the SSMs underlying the neural network. Each configuration corresponds to either real or complex parameterization, and to a specific partial version of selectivity\u2014i.e., to a specific combination of parameters that are selective (replaced by functions of the input), where the parameters that may be selective are: the input matrix B; the output matrix C; and a discretization parameter \u0394. For each configuration, the highest and lowest accuracies attained across three random seeds are reported. Notice that when both B and C are selective, the real parameterization compares (favorably) to the complex parameterization, whereas otherwise, the complex parameterization is superior. For further implementation details see Appendix F.3.", "description": "This table shows the results of an ablation study on the impact of complex parameterizations in SSMs with selectivity.  It tests a Mamba neural network on a synthetic induction-head task, varying which parameters (input matrix B, output matrix C, and discretization parameter \u0394) utilize selectivity. The table compares the accuracy of real and complex parameterizations under different selectivity configurations.", "section": "4.3 Selectivity"}]