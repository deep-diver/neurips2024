{"references": [{"fullname_first_author": "Albert Gu", "paper_title": "Mamba: Linear-time sequence modeling with selective state spaces", "publication_date": "2023-XX-XX", "reason": "This paper introduces Mamba, a state-of-the-art neural network architecture that uses complex parameterizations and selectivity, which are key concepts in the current paper."}, {"fullname_first_author": "Albert Gu", "paper_title": "Efficiently modeling long sequences with structured state spaces", "publication_date": "2022-XX-XX", "reason": "This paper introduces the S4 architecture, a prominent example of a structured state space model (SSM) that uses complex parameterizations, which are central to the current study."}, {"fullname_first_author": "Ankit Gupta", "paper_title": "On the parameterization and initialization of diagonal state space models", "publication_date": "2022-XX-XX", "reason": "This paper investigates the parameterization and initialization of diagonal SSMs, which provides context and background for the current paper's analysis of complex parameterizations."}, {"fullname_first_author": "Antonio Orvieto", "paper_title": "Transformers are ssms: Generalized models and efficient algorithms through structured state space duality", "publication_date": "2024-XX-XX", "reason": "This paper establishes a connection between transformers and SSMs, which helps to contextualize the importance of SSMs and their properties."}, {"fullname_first_author": "Joshua Hanson", "paper_title": "Universal approximation of input-output maps by temporal convolutional nets", "publication_date": "2019-XX-XX", "reason": "This paper provides a theoretical foundation for the use of recurrent neural networks to approximate functions, which is relevant to the current paper's exploration of SSMs as universal approximators."}]}