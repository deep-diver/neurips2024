[{"figure_path": "pW9Jwim918/tables/tables_1_1.jpg", "caption": "Table 1: AUROC (%) of LLM-generated text detection methods on WritingPrompts from the Fast-DetectGPT benchmark, where GPT4 is used for text generation. 'Reward model' indicates the detection using the reward score of the pre-trained reward model. The bold denotes the best result.", "description": "This table presents the Area Under the Receiver Operating Characteristic curve (AUROC) scores, expressed as percentages, for various LLM-generated text (LGT) detection methods.  The methods are evaluated on the WritingPrompts dataset from the Fast-DetectGPT benchmark.  GPT-4 was used to generate the LGTs for this evaluation. One method shown is a baseline using a pre-trained reward model to assess the quality of the text. The highest AUROC score is achieved by the ReMoDetect method, highlighting its superior performance compared to existing methods.", "section": "1 Introduction"}, {"figure_path": "pW9Jwim918/tables/tables_5_1.jpg", "caption": "Table 2: AUROC (%) of multiple LGT detection methods, including log-likelihood (Loglik.) [20], Rank [20], DetectGPT (D-GPT) [11], LRR [21], NPR [21], Fast-DetectGPT (FD-GPT) [13], OpenAI-Detector (Open-D) [20], ChatGPT-Detector (Chat-D) [6], and ReMoDetect (Ours). We consider two major LGT detection benchmarks from (a) Fast-DetectGPT [13] and (b) MGTBench [15]. The bold indicates the best result within the group.", "description": "This table presents the Area Under the Receiver Operating Characteristic Curve (AUROC) scores for several LLM-generated text (LGT) detection methods.  It compares the performance of these methods across two benchmark datasets: Fast-DetectGPT and MGTBench.  The results are broken down by LLM model used to generate the text (GPT3.5 Turbo, GPT4, etc.) and by the specific text domain (PubMed, XSum, etc.). The bold values represent the best AUROC score for each combination.", "section": "4 Experiments"}, {"figure_path": "pW9Jwim918/tables/tables_5_2.jpg", "caption": "Table 2: AUROC (%) of multiple LGT detection methods, including log-likelihood (Loglik.) [20], Rank [20], DetectGPT (D-GPT) [11], LRR [21], NPR [21], Fast-DetectGPT (FD-GPT) [13], OpenAI-Detector (Open-D) [20], ChatGPT-Detector (Chat-D) [6], and ReMoDetect (Ours). We consider two major LGT detection benchmarks from (a) Fast-DetectGPT [13] and (b) MGTBench [15]. The bold indicates the best result within the group.", "description": "This table presents the AUROC scores achieved by various LLM-generated text detection methods on two benchmark datasets: Fast-DetectGPT and MGTBench.  It compares the performance of several existing methods against the proposed ReMoDetect method across multiple text domains and LLMs.  The results highlight ReMoDetect's superior performance compared to existing techniques.", "section": "4 Experiments"}, {"figure_path": "pW9Jwim918/tables/tables_6_1.jpg", "caption": "Table 3: Comparison with ReMoDetect (Ours) and GPTZero [39], a commercial black-box LGT detection API. We report the average AUROC (%) on the Fast-DetectGPT benchmark, including PubMed, XSum, and WritingPrompts. The bold indicates the best results.", "description": "This table compares the performance of ReMoDetect with GPTZero, a commercial LLM detection API, across three datasets from the Fast-DetectGPT benchmark (PubMed, XSum, and WritingPrompts).  The average AUROC (Area Under the Receiver Operating Characteristic curve) is presented for each model and dataset. ReMoDetect demonstrates superior performance.", "section": "4 Experiments"}, {"figure_path": "pW9Jwim918/tables/tables_7_1.jpg", "caption": "Table 4: Robustness against rephrasing attacks. We report the average AUROC (%) before ('Original') and after ('Attacked') the rephrasing attack with T5-3B on the Fast-DetectGPT benchmark, including XSum, PubMed, and small-sized WritingPrompts. Values in the parenthesis indicate the relative performance drop after the rephrasing attack. The bold indicates the best result.", "description": "This table presents the results of an experiment evaluating the robustness of several LLM-generated text detection methods against rephrasing attacks.  The experiment used the Fast-DetectGPT benchmark, and focused on three text domains (XSum, PubMed, and WritingPrompts-small).  The methods' AUROC scores are reported for both original texts and texts that have undergone a rephrasing attack using the T5-3B model.  The relative decrease in performance after the attack is shown in parentheses, illustrating the impact of rephrasing on each method's detection accuracy.", "section": "4 Experiments"}, {"figure_path": "pW9Jwim918/tables/tables_7_2.jpg", "caption": "Table 5: AUROC (%) of ChatGPT-D and ReMoDetect (ours), on datasets and models that are seen (S) or unseen (U) during training time. The bold denotes the best results.", "description": "This table presents the results of a robustness test performed on the ChatGPT-Detector and ReMoDetect models. The goal was to evaluate how well the models generalize to unseen data and models by comparing the Area Under the Receiver Operating Characteristic curve (AUROC) across several scenarios where either the data, the model, or both are unseen during the training phase. The results indicate that ReMoDetect demonstrates superior robustness compared to ChatGPT-Detector, especially in unseen scenarios.", "section": "4.3 Additional Analysis"}, {"figure_path": "pW9Jwim918/tables/tables_8_1.jpg", "caption": "Table 6: LGT Detection results on non-RLHF trained LLMs. We report AUROC (%) of multiple LGT detection methods, including log-likelihood (Loglik.), Rank, Fast-DetectGPT (FD-GPT), OpenAI-Detector (Open-D), ChatGPT-Detector (Chat-D), and ReMoDetect (Ours). We consider LGT detection benchmarks from Fast-DetectGPT: PubMed, XSum, and WritingPrompts-small (WP-s). Here, Phi-3 medium is DPO trained and OLMo-7B-SFT is SFT-only trained. The bold indicates the best result within the group.", "description": "This table presents the Area Under the ROC Curve (AUROC) scores for several Large Language Model (LLM) generated text detection methods on three different datasets (PubMed, XSum, and WritingPrompts-small).  It compares the performance of these methods on LLMs that were *not* trained using reinforcement learning from human feedback (RLHF), specifically using either direct preference optimization (DPO) or supervised fine-tuning (SFT). The table highlights ReMoDetect's performance against various baselines, showcasing its effectiveness even on LLMs trained with methods other than RLHF.", "section": "4 Experiments"}, {"figure_path": "pW9Jwim918/tables/tables_9_1.jpg", "caption": "Table 7: Contribution of each proposed component of ReMoDetect on detecting aligned LGTs from human-written texts. We report the average detection performance of GPT4 under text domains in the Fast-DetectGPT benchmark. All values are percentages, and the best results are indicated in bold.", "description": "This table shows the ablation study of the proposed ReMoDetect model by gradually adding the components (Continual Preference Tuning, Replay Buffers, and Reward Modeling with Mixed Responses).  It demonstrates the incremental improvements in AUROC, AUPR, and TPR@FPR1% metrics when each component is added, highlighting their individual contributions to the overall performance.  The results are based on GPT-4 generated text across several domains.", "section": "4 Experiments"}, {"figure_path": "pW9Jwim918/tables/tables_9_2.jpg", "caption": "Table 8: Comparison of detection time, model parameters, and average AUROC (%) of Fast-DetectGPT benchmark for various LGT detection methods. Detection time was measured in an A6000 GPU, and the overall detection time was measured for 300 XSum dataset samples.", "description": "This table compares the detection time, model parameters, and AUROC of several LLM-generated text detection methods on the Fast-DetectGPT benchmark.  The detection time is measured using an A6000 GPU and is based on 300 samples from the XSum dataset.  The table highlights the efficiency and performance of the proposed ReMoDetect method compared to existing techniques.", "section": "4 Experiments"}, {"figure_path": "pW9Jwim918/tables/tables_15_1.jpg", "caption": "Table 2: AUROC (%) of multiple LGT detection methods, including log-likelihood (Loglik.) [20], Rank [20], DetectGPT (D-GPT) [11], LRR [21], NPR [21], Fast-DetectGPT (FD-GPT) [13], OpenAI-Detector (Open-D) [20], ChatGPT-Detector (Chat-D) [6], and ReMoDetect (Ours). We consider two major LGT detection benchmarks from (a) Fast-DetectGPT [13] and (b) MGTBench [15]. The bold indicates the best result within the group.", "description": "This table presents the Area Under the Receiver Operating Characteristic Curve (AUROC) scores achieved by several LLM-generated text (LGT) detection methods.  It compares the performance of ReMoDetect against several baselines across two benchmark datasets: Fast-DetectGPT and MGTBench. The results are broken down by LLM model (GPT3.5 Turbo, GPT4, GPT4 Turbo, Llama 3 70B, Gemini Pro, Claude 3 Opus) and text domain (PubMed, XSum, WritingPrompts-small, Essay, Reuters, WritingPrompts).  The bold values highlight the best-performing method in each row.", "section": "4 Experiments"}, {"figure_path": "pW9Jwim918/tables/tables_16_1.jpg", "caption": "Table 10: AUROC (%) on Fast-DetectGPT benchmark [13] for different models: Claude3 Haiku [5] and Sonnet [5]. The bold indicates the best result.", "description": "This table presents the Area Under the Receiver Operating Characteristic Curve (AUROC) scores for different Large Language Models (LLMs) on the Fast-DetectGPT benchmark.  The models evaluated include Claude3 Haiku, Claude3 Sonnet, and several baselines.  The AUROC is a measure of the LLM's ability to distinguish between human-written text and machine-generated text.  Higher AUROC scores indicate better performance.  The table is broken down by domain (PubMed, XSum, WritingPrompts-small) and shows the AUROC for each model within each domain. The best-performing model in each domain is highlighted in bold.", "section": "4 Experiments"}, {"figure_path": "pW9Jwim918/tables/tables_16_2.jpg", "caption": "Table 11: TPR(%) at FPR 1% and AUPR (%) of multiple LLM-generated text detection methods, including log-likelihood (Loglik.) [20], Rank [20], DetectGPT (D-GPT) [11], LRR [21], NPR [21], Fast-DetectGPT (FD-GPT) [13], OpenAI-Detector (Open-D) [20], ChatGPT-Detector (Chat-D) [6], and ReMoDetect (Ours). We consider LLM-generated text detection benchmarks from Fast-DetectGPT [13]. The bold indicates the best result within the group.", "description": "This table presents the True Positive Rate (TPR) at a 1% False Positive Rate (FPR) and the Area Under the Precision-Recall Curve (AUPR) for several LLM-generated text detection methods.  It compares the performance of ReMoDetect against baselines across different LLMs and text domains from the Fast-DetectGPT benchmark.", "section": "4 Experiments"}, {"figure_path": "pW9Jwim918/tables/tables_16_3.jpg", "caption": "Table 2: AUROC (%) of multiple LGT detection methods, including log-likelihood (Loglik.) [20], Rank [20], DetectGPT (D-GPT) [11], LRR [21], NPR [21], Fast-DetectGPT (FD-GPT) [13], OpenAI Detector (Open-D) [20], ChatGPT-Detector (Chat-D) [6], and ReMoDetect (Ours). We consider two major LGT detection benchmarks from (a) Fast-DetectGPT [13] and (b) MGTBench [15]. The bold indicates the best result within the group.", "description": "This table presents the Area Under the Receiver Operating Characteristic curve (AUROC) scores achieved by several LLM-generated text (LGT) detection methods on two benchmark datasets: Fast-DetectGPT and MGTBench.  The methods compared include various zero-shot and supervised techniques.  The results are broken down by LLM used for text generation (GPT3.5 Turbo, GPT4, etc.) and by the specific dataset and its sub-domains (PubMed, XSum, WritingPrompts). The table highlights the superior performance of ReMoDetect across diverse models and datasets.", "section": "4 Experiments"}, {"figure_path": "pW9Jwim918/tables/tables_17_1.jpg", "caption": "Table 2: AUROC (%) of multiple LGT detection methods, including log-likelihood (Loglik.) [20], Rank [20], DetectGPT (D-GPT) [11], LRR [21], NPR [21], Fast-DetectGPT (FD-GPT) [13], OpenAI-Detector (Open-D) [20], ChatGPT-Detector (Chat-D) [6], and ReMoDetect (Ours). We consider two major LGT detection benchmarks from (a) Fast-DetectGPT [13] and (b) MGTBench [15]. The bold indicates the best result within the group.", "description": "This table presents the Area Under the ROC Curve (AUROC) scores achieved by various LLM-generated text (LGT) detection methods on two benchmark datasets: Fast-DetectGPT and MGTBench.  The methods compared include several zero-shot and supervised approaches, along with the proposed ReMoDetect method.  The AUROC scores are presented for different LLMs and across multiple text domains within each benchmark, allowing for a comprehensive comparison of performance.", "section": "4 Experiments"}, {"figure_path": "pW9Jwim918/tables/tables_19_1.jpg", "caption": "Table 4: Robustness against rephrasing attacks. We report the average AUROC (%) before ('Original') and after ('Attacked') the rephrasing attack with T5-3B on the Fast-DetectGPT benchmark, including XSum, PubMed, and small-sized WritingPrompts. Values in the parenthesis indicate the relative performance drop after the rephrasing attack. The bold indicates the best result.", "description": "This table demonstrates the robustness of different LLM-generated text detection methods against rephrasing attacks. It shows the AUROC scores (Area Under the Receiver Operating Characteristic Curve) before and after a rephrasing attack using the T5-3B model.  The results are broken down by model (GPT3.5 Turbo, GPT4, and Claude3 Opus) and dataset (XSum, PubMed, and WritingPrompts-small). The values in parentheses show the percentage drop in AUROC after the attack, highlighting the resilience of each method.", "section": "4 Experiments"}, {"figure_path": "pW9Jwim918/tables/tables_19_2.jpg", "caption": "Table 2: AUROC (%) of multiple LGT detection methods, including log-likelihood (Loglik.) [20], Rank [20], DetectGPT (D-GPT) [11], LRR [21], NPR [21], Fast-DetectGPT (FD-GPT) [13], OpenAI-Detector (Open-D) [20], ChatGPT-Detector (Chat-D) [6], and ReMoDetect (Ours). We consider two major LGT detection benchmarks from (a) Fast-DetectGPT [13] and (b) MGTBench [15]. The bold indicates the best result within the group.", "description": "This table presents the Area Under the Receiver Operating Characteristic curve (AUROC) for several LLM-generated text (LGT) detection methods.  It compares the performance of these methods across different LLMs (GPT3.5 Turbo, GPT4, GPT4 Turbo, Llama 3 70B, Gemini pro, Claude3 Opus) and datasets (PubMed, XSum, WritingPrompts, Essay, Reuters, and WritingPrompts-small) from two benchmark datasets: Fast-DetectGPT and MGTBench.  The best performing method in each category is highlighted in bold.", "section": "4 Experiments"}, {"figure_path": "pW9Jwim918/tables/tables_20_1.jpg", "caption": "Table 2: AUROC (%) of multiple LGT detection methods, including log-likelihood (Loglik.) [20], Rank [20], DetectGPT (D-GPT) [11], LRR [21], NPR [21], Fast-DetectGPT (FD-GPT) [13], OpenAI-Detector (Open-D) [20], ChatGPT-Detector (Chat-D) [6], and ReMoDetect (Ours). We consider two major LGT detection benchmarks from (a) Fast-DetectGPT [13] and (b) MGTBench [15]. The bold indicates the best result within the group.", "description": "This table compares the performance of ReMoDetect against other LLM-generated text detection methods on two benchmark datasets: Fast-DetectGPT and MGTBench.  The results are presented as AUROC (Area Under the Receiver Operating Characteristic curve) scores, a metric that measures the ability of a classifier to distinguish between human-written and LLM-generated text. The table shows the performance across multiple domains (PubMed, XSum, WritingPrompts) and different LLMs (GPT3.5 Turbo, GPT4, etc.).", "section": "4 Experiments"}, {"figure_path": "pW9Jwim918/tables/tables_20_2.jpg", "caption": "Table 2: AUROC (%) of multiple LGT detection methods, including log-likelihood (Loglik.) [20], Rank [20], DetectGPT (D-GPT) [11], LRR [21], NPR [21], Fast-DetectGPT (FD-GPT) [13], OpenAI-Detector (Open-D) [20], ChatGPT-Detector (Chat-D) [6], and ReMoDetect (Ours). We consider two major LGT detection benchmarks from (a) Fast-DetectGPT [13] and (b) MGTBench [15]. The bold indicates the best result within the group.", "description": "This table presents the Area Under the Receiver Operating Characteristic curve (AUROC) scores achieved by several LLM-generated text (LGT) detection methods on two benchmark datasets: Fast-DetectGPT and MGTBench.  The methods compared include various zero-shot and supervised approaches, along with the authors' proposed ReMoDetect method.  The results are broken down by LLM used for text generation and by the text domain (e.g., PubMed, XSum, WritingPrompts).  The table highlights the superior performance of ReMoDetect across different LLMs and domains.", "section": "4 Experiments"}]