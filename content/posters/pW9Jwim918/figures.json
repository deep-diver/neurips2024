[{"figure_path": "pW9Jwim918/figures/figures_1_1.jpg", "caption": "Figure 1: Motivation: Aligned LGTs and human-written texts are easily distinguishable by using the reward model. We visualize the (a) t-SNE of the reward model's final feature and the (b) histogram of the predicted reward score. Here, 'Machine' indicates the text generated by GPT3.5/GPT4 Turbo, Llama3-70B, and Claude on the Reuters domain.", "description": "This figure demonstrates that aligned Large Language Models (LLMs) generate texts with higher reward scores (as predicted by a reward model trained on human preferences) than human-written texts.  Panel (a) shows a t-SNE visualization of the reward model's feature embeddings, clearly separating machine-generated and human-written text. Panel (b) presents histograms of the predicted reward scores, further emphasizing the distinct distributions.", "section": "1 Introduction"}, {"figure_path": "pW9Jwim918/figures/figures_3_1.jpg", "caption": "Figure 2: Overview of Reward Model based LLM Generated Text Detection (ReMoDetect): We continually fine-tune the reward model r\u03c6 to prefer aligned LLM-generated responses YLM even further while preventing the overfitting by using the replay technique: (xbuf, ybuf) is the replay buffer and r\u03c60 is the initial reward model. Moreover, we generate a human/LLM mixed text YMIX by partially rephrasing the human response YHU using the aligned LLM, which serves as a median preference data compared to YLM and YHU, i.e., YLM > YMIX > YHU | x, to improve the reward model's detection ability.", "description": "This figure illustrates the ReMoDetect framework.  It shows how the reward model is continually fine-tuned to distinguish between human-written text and LLM-generated text, using a replay buffer to avoid overfitting and incorporating human/LLM mixed text for improved boundary learning.", "section": "3 ReMoDetect: Detecting Aligned LLM's Generations using Reward Models"}, {"figure_path": "pW9Jwim918/figures/figures_3_2.jpg", "caption": "Figure 2: Overview of Reward Model based LLM Generated Text Detection (ReMoDetect): We continually fine-tune the reward model r\u03b8 to prefer aligned LLM-generated responses YLM even further while preventing the overfitting by using the replay technique: (xbuf, ybuf) is the replay buffer and r\u03b80 is the initial reward model. Moreover, we generate a human/LLM mixed text YMIX by partially rephrasing the human response YHU using the aligned LLM, which serves as a median preference data compared to YLM and YHU, i.e., YLM > YMIX > YHU | x, to improve the reward model's detection ability.", "description": "This figure illustrates the ReMoDetect framework.  It shows how the reward model is continually fine-tuned to better distinguish between human-written text and LLM-generated text.  A key aspect is the use of a \"replay buffer\" to prevent overfitting, and the generation of mixed human/LLM texts to help the model learn the decision boundary more effectively. The process starts with an initial reward model and then iteratively refines it using two training components: continual preference tuning with replay and reward modeling with mixed responses.", "section": "3 ReMoDetect: Detecting Aligned LLM's Generations using Reward Models"}, {"figure_path": "pW9Jwim918/figures/figures_6_1.jpg", "caption": "Figure 1: Motivation: Aligned LGTs and human-written texts are easily distinguishable by using the reward model. We visualize the (a) t-SNE of the reward model's final feature and the (b) histogram of the predicted reward score. Here, 'Machine' indicates the text generated by GPT3.5/GPT4 Turbo, Llama3-70B, and Claude on the Reuters domain.", "description": "This figure demonstrates that aligned Large Language Model (LLM) generated texts (LGTs) are easily distinguishable from human-written texts using a reward model.  Panel (a) shows a t-distributed stochastic neighbor embedding (t-SNE) plot of the reward model's final feature vector, clearly separating LLM-generated and human-written texts. Panel (b) presents a histogram of the reward scores, highlighting the distinct distributions between the two text types.  The results suggest that the reward model effectively captures a difference in text characteristics introduced by alignment training in LLMs.", "section": "1 Introduction"}, {"figure_path": "pW9Jwim918/figures/figures_6_2.jpg", "caption": "Figure 4: Predicted reward distribution of human-written texts and LGTs on three different reward models (RMs), including (a) Gemma 2B (b) Gemma 7B, and (c) Llama3 8B. 'Machine' denotes GPT4 Turbo and Claude3 Opus generated texts. We use WritingPrompts-small as the text domain.", "description": "This figure visualizes the predicted reward scores given by three different reward models (Gemma 2B, Gemma 7B, and Llama3 8B) for both human-written texts and machine-generated texts (GPT4 Turbo and Claude3 Opus) on the WritingPrompts-small dataset.  It shows how the reward models distinguish between human and machine-generated text based on the predicted reward score distribution.", "section": "4.2 Reward Model Analysis"}, {"figure_path": "pW9Jwim918/figures/figures_7_1.jpg", "caption": "Figure 5: Predicted reward distribution of human-written texts and LGTs (a) 'Before' and (b) 'After' training the reward model with Eq (2). \u201cMachine' denotes GPT4-Turbo generated texts on Eassy domain.", "description": "This figure visualizes the reward distribution predicted by the reward model before and after training with the proposed continual preference tuning method.  The 'before' plot shows the initial distribution where LLM-generated texts (Machine) have slightly higher reward scores than human-written texts.  The 'after' plot illustrates how the training shifts the distribution, significantly separating LLM-generated texts with substantially higher reward scores compared to human-written texts.  This separation is crucial for accurate LLM-generated text detection.", "section": "3.2 Continual Preference Tuning: Increasing the Separation Gap of the Predicted Reward"}, {"figure_path": "pW9Jwim918/figures/figures_8_1.jpg", "caption": "Figure 6: Average AUROC (%) of various LGT detection methods on various input response lengths by monotonically increasing 30 words each. We consider three text domains from the Fast-DetectGPT benchmark and two aligned LLM, including (a) GPT4 Turbo and (b) Claude3 Opus.", "description": "This figure shows the average AUROC scores for different LLM-generated text (LGT) detection methods across varying lengths of input text.  The x-axis represents the number of words in the input text, increasing in increments of 30. The y-axis represents the AUROC score, a measure of the detector's performance.  Two different LLMs are tested: GPT4 Turbo and Claude3 Opus. The figure visually demonstrates the robustness of the proposed method (Ours) compared to other methods like Log-likelihood, LRR, DetectGPT, and Fast-DetectGPT, especially with shorter response lengths.", "section": "4 Experiments"}, {"figure_path": "pW9Jwim918/figures/figures_13_1.jpg", "caption": "Figure 1: Motivation: Aligned LGTs and human-written texts are easily distinguishable by using the reward model. We visualize the (a) t-SNE of the reward model's final feature and the (b) histogram of the predicted reward score. Here, 'Machine' indicates the text generated by GPT3.5/GPT4 Turbo, Llama3-70B, and Claude on the Reuters domain.", "description": "This figure shows that reward models can easily distinguish between texts generated by large language models (LLMs) and human-written texts.  Panel (a) uses t-SNE to visualize the separation of aligned LLM-generated texts from human-written texts based on reward model features. Panel (b) shows a histogram illustrating the difference in the reward score distributions between human and machine-generated texts.", "section": "1 Introduction"}, {"figure_path": "pW9Jwim918/figures/figures_18_1.jpg", "caption": "Figure 1: Motivation: Aligned LGTs and human-written texts are easily distinguishable by using the reward model. We visualize the (a) t-SNE of the reward model\u2019s final feature and the (b) histogram of the predicted reward score. Here, \u2018Machine\u2019 indicates the text generated by GPT3.5/GPT4 Turbo, Llama3-70B, and Claude on the Reuters domain.", "description": "This figure demonstrates the effectiveness of reward models in distinguishing between human-written text and text generated by large language models (LLMs).  Panel (a) shows a t-distributed stochastic neighbor embedding (t-SNE) plot visualizing the separation of human-written text and LLM-generated text based on their reward scores. Panel (b) presents a histogram comparing the distribution of predicted reward scores for both human-written and LLM-generated text, showcasing a clear distinction between the two distributions. The results support the main finding that aligned LLMs (trained to maximize human preferences) produce text with reward scores even higher than human-written texts, making them easier to detect using the reward model.", "section": "1 Introduction"}, {"figure_path": "pW9Jwim918/figures/figures_18_2.jpg", "caption": "Figure 1: Motivation: Aligned LGTs and human-written texts are easily distinguishable by using the reward model. We visualize the (a) t-SNE of the reward model's final feature and the (b) histogram of the predicted reward score. Here, 'Machine' indicates the text generated by GPT3.5/GPT4 Turbo, Llama3-70B, and Claude on the Reuters domain.", "description": "This figure shows that reward models can easily distinguish between texts generated by large language models (LLMs) and human-written texts.  The t-SNE plot (a) visualizes the separation of LLM-generated and human-written texts in a feature space learned by a reward model, while the histogram (b) shows the distribution of predicted reward scores for each type of text, clearly indicating a difference in their distributions.", "section": "1 Introduction"}, {"figure_path": "pW9Jwim918/figures/figures_18_3.jpg", "caption": "Figure 1: Motivation: Aligned LGTs and human-written texts are easily distinguishable by using the reward model. We visualize the (a) t-SNE of the reward model's final feature and the (b) histogram of the predicted reward score. Here, 'Machine' indicates the text generated by GPT3.5/GPT4 Turbo, Llama3-70B, and Claude on the Reuters domain.", "description": "This figure demonstrates the effectiveness of reward models in distinguishing between aligned LLM-generated texts (LGTs) and human-written texts.  Panel (a) shows a t-SNE visualization of the reward model's feature embeddings, clearly separating machine-generated text from human-written text. Panel (b) displays a histogram of the predicted reward scores, further highlighting the distinct distributions of these two text types. This separation is crucial for the LGT detection method proposed in the paper.", "section": "Motivation"}, {"figure_path": "pW9Jwim918/figures/figures_18_4.jpg", "caption": "Figure 1: Motivation: Aligned LGTs and human-written texts are easily distinguishable by using the reward model. We visualize the (a) t-SNE of the reward model's final feature and the (b) histogram of the predicted reward score. Here, 'Machine' indicates the text generated by GPT3.5/GPT4 Turbo, Llama3-70B, and Claude on the Reuters domain.", "description": "The figure demonstrates that aligned Large Language Model (LLM) generated texts (LGTs) and human-written texts have different reward scores, as predicted by a reward model trained to assess human preferences.  The t-SNE plot (a) visually separates LLM generated texts from human-written ones in a 2-dimensional space, while the histogram (b) shows the reward scores for both LLM-generated and human-written texts, exhibiting a clear separation in the distribution. This motivates the use of the reward model for detecting LLM-generated texts.", "section": "1 Introduction"}]