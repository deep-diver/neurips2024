[{"figure_path": "Rsb32EBmbj/tables/tables_3_1.jpg", "caption": "Table 1: Comparisons among the test accuracy (%) of different SSM structures with different Training types on MNIST and CIFAR-10 test set. \u2018Best\u2019 and \u2018Last\u2019 mean the test performance on the best and last checkpoint, respectively. \u2018Diff\u2019 denotes the accuracy gap between the \u2018Best\u2019 and \u2018Last\u2019. The best checkpoint is selected based on the highest RA on the test set under PGD-10.", "description": "This table presents a comparison of the test accuracy of various SSM structures under different training methods (Standard Training, PGD-AT, TRADES, FreeAT, YOPO) on MNIST and CIFAR-10 datasets.  It shows the best and last checkpoint's accuracy, along with the difference between them. The best checkpoint is determined by the highest robust accuracy (RA) achieved under the PGD-10 attack.", "section": "3 Empirical Evaluation: Component Contributions to AT Gains"}, {"figure_path": "Rsb32EBmbj/tables/tables_8_1.jpg", "caption": "Table 1: Comparisons among the test accuracy (%) of different SSM structures with different Training types on MNIST and CIFAR-10 test set. \u2018Best\u2019 and \u2018Last\u2019 mean the test performance on the best and last checkpoint, respectively. \u2018Diff\u2019 denotes the accuracy gap between the \u2018Best\u2019 and \u2018Last\u2019. The best checkpoint is selected based on the highest RA on the test set under PGD-10.", "description": "This table presents a comparison of the test accuracy of various SSM structures under different training methods (Standard Training, PGD-AT, TRADES, FreeAT, and YOPO) on MNIST and CIFAR-10 datasets.  It shows the clean accuracy, accuracy under PGD-10 adversarial attacks, and accuracy under AutoAttack.  'Best' and 'Last' represent the test performance at the best and last checkpoints, respectively, with 'Diff' showing the difference. The best checkpoint is chosen based on the highest Robust Accuracy (RA) achieved under PGD-10 attacks.", "section": "3 Empirical Evaluation: Component Contributions to AT Gains"}, {"figure_path": "Rsb32EBmbj/tables/tables_15_1.jpg", "caption": "Table 1: Comparisons among the test accuracy (%) of different SSM structures with different Training types on MNIST and CIFAR-10 test set. \u2018Best\u2019 and \u2018Last\u2019 mean the test performance on the best and last checkpoint, respectively. \u2018Diff\u2019 denotes the accuracy gap between the \u2018Best\u2019 and \u2018Last\u2019. The best checkpoint is selected based on the highest RA on the test set under PGD-10.", "description": "This table compares the test accuracy of various SSM structures under different training methods (standard training, PGD-AT, TRADES, FreeAT, and YOPO) on the MNIST and CIFAR-10 datasets.  It shows the best and last checkpoint's performance, highlighting the difference between them. The 'best' checkpoint is selected based on the highest Robust Accuracy (RA) achieved under PGD-10 adversarial attacks.", "section": "3 Empirical Evaluation: Component Contributions to AT Gains"}, {"figure_path": "Rsb32EBmbj/tables/tables_15_2.jpg", "caption": "Table 4: Model and Training parameters on MNIST, CIFAR-10 and Tiny-Imagenet datasets.", "description": "This table details the model architecture and training hyperparameters used in the experiments.  It shows the input dimensions, number of SSM layers, model dimensions, state dimensions, output dimensions, and reduction method before the output head for each dataset (MNIST, CIFAR-10, and Tiny-Imagenet).  Training parameters such as the optimizer, batch size, learning rate, scheduler, weight decay, number of epochs, and adversarial attack parameters (for adversarial training) are also specified for each dataset.", "section": "3 Empirical Evaluation: Component Contributions to AT Gains"}, {"figure_path": "Rsb32EBmbj/tables/tables_16_1.jpg", "caption": "Table 1: Comparisons among the test accuracy (%) of different SSM structures with different Training types on MNIST and CIFAR-10 test set. \u2018Best\u2019 and \u2018Last\u2019 mean the test performance on the best and last checkpoint, respectively. \u2018Diff\u2019 denotes the accuracy gap between the \u2018Best\u2019 and \u2018Last\u2019. The best checkpoint is selected based on the highest RA on the test set under PGD-10.", "description": "This table compares the test accuracy of various SSM structures under different training methods (Standard Training, PGD-AT, TRADES, FreeAT, and YOPO) on MNIST and CIFAR-10 datasets.  It shows the performance at both the best and last checkpoints, indicating the robustness-generalization trade-off and the level of robust overfitting.  The best checkpoint is chosen based on the highest robust accuracy under PGD-10 attacks.", "section": "3 Empirical Evaluation: Component Contributions to AT Gains"}, {"figure_path": "Rsb32EBmbj/tables/tables_17_1.jpg", "caption": "Table 6: Comparisons among the test accuracy (%) of S4 and DSS with different AdS modules and different AT types on Tiny-Imagenet test set. \u2018Best\u2019 and \u2018Last\u2019 mean the test performance at the best and last epoch, respectively. \u2018Diff\u2019 denotes the accuracy gap between the \u2018Best\u2019 and \u2018Last\u2019, \u2018\u2013\u2019 indicates that the results of AdSS are not used. The best checkpoint is selected based on the highest RA on the test set under PGD-10.", "description": "This table presents a comparison of the test accuracy of two different SSM structures (S4 and DSS) on the Tiny-Imagenet dataset under various adversarial training (AT) methods (PGD-AT, TRADES, FreeAT, YOPO).  It also explores the impact of different adaptive scaling (AdS) modules (ReLU, Sigmoid, Tanh) on the model's performance.  The table shows the best and last epoch's clean accuracy (CA), robust accuracy (RA) under PGD-10, and robust accuracy under AutoAttack (AA). The 'Diff' column shows the difference in accuracy between the best and last epoch. The best epoch is determined by the highest RA under PGD-10.", "section": "4.2 Experimental Validation and Further Insights"}]