[{"figure_path": "a1wf2N967T/figures/figures_1_1.jpg", "caption": "Figure 1: The comparison of typical DRL frameworks with our GEM. The limitations of conventional DRL methods are presented on the left. Conversely, the right-hand side illustrates the advantages of our framework, which benefited from the integration of the B-VAE and MLLMs.", "description": "This figure compares traditional disentangled representation learning (DRL) frameworks with the proposed GEM framework. The left side shows limitations of conventional methods, such as being impractical for real-world scenarios due to the neglect of logical relations between factors. The right side highlights GEM's advantages, including its unsupervised nature, logical and practical approach using bidirectional and weighted relations, and improved interpretability and generalizability by integrating a bidirectional weighted graph and Multimodal Large Language Model (MLLM).", "section": "1 Introduction"}, {"figure_path": "a1wf2N967T/figures/figures_4_1.jpg", "caption": "Figure 1: The comparison of typical DRL frameworks with our GEM. The limitations of conventional DRL methods are presented on the left. Conversely, the right-hand side illustrates the advantages of our framework, which benefited from the integration of the B-VAE and MLLMs.", "description": "This figure compares traditional disentangled representation learning (DRL) frameworks with the proposed GEM framework. The left side highlights limitations of existing methods, such as their impracticality in real scenarios, negligence of logical relations between attributes, and reliance on priors and supervisions. In contrast, the right side showcases GEM's advantages: unsupervised learning, consideration of bidirectional and weighted relations between attributes, and enhanced interpretability and generalizability due to the integration of \u03b2-VAE and multimodal large language models (MLLMs).", "section": "1 Introduction"}, {"figure_path": "a1wf2N967T/figures/figures_5_1.jpg", "caption": "Figure 4: Our aim is using the commonsense knowledge behind MLLMs to equip GEM with ability of interrelations discovery, where a certain degree of fluctuations on absolute scores are acceptable.", "description": "This figure illustrates the concept of using MLLMs (large language models) to learn the statistical relationships between attributes, rather than the absolute scores themselves.  The example shows scores from MLLMs for \"age\" and \"baldness\" fluctuating across different images. The authors emphasize that this fluctuation is acceptable because their proposed GEM model focuses on learning the relationships, not the exact numerical scores provided by the MLLM.", "section": "3.2 MLLM-based Interrelation Discovery Branch"}, {"figure_path": "a1wf2N967T/figures/figures_5_2.jpg", "caption": "Figure 2: Pipeline of our GEM. The model consists of two complementary branches, termed as a B-VAE branch (blue) and a MLLM branch (brown). The former utilizes B-VAE based semantic encoder Esem to disentangle underlying factors, while the latter employs prompt engineering to discover and rank interrelations. The bidirectional weighted DisGragh G is further proposed to embed relation-aware representations, with its parameters optimized constantly by a GNN network Egnn. ", "description": "This figure illustrates the GEM framework's architecture. It shows two main branches: a B-VAE branch for disentangling factors and a MLLM branch for discovering and ranking inter-factor relations.  The output of both branches feeds into a DisGraph (a bidirectional weighted graph) which is dynamically updated by a GNN (Graph Neural Network).", "section": "3 Methodology"}, {"figure_path": "a1wf2N967T/figures/figures_6_1.jpg", "caption": "Figure 6: Qualitative comparisons between GEM and typical DRL Methods. Each row in facial images corresponds to the traversal results on a specific attribute, as indicated adjacent to the images (i.e., Bangs, Bald, Gender, Beard, Blond, and Makeup). GEM exhibits superior ability in fine-grained disentanglement with discovered practical and bidirectional relations (illustrated by the heatmap).", "description": "This figure compares the performance of GEM against FactorVAE and DEAR in terms of disentanglement quality for six fine-grained facial attributes (Bangs, Bald, Gender, Beard, Blond, and Makeup).  It shows traversals across the latent dimensions for each method.  The heatmap illustrates the bidirectional relationships between attributes, as determined by the MLLM in GEM, demonstrating GEM's ability to capture fine-grained details and practical interrelations.", "section": "4.1 Qualitative Results"}, {"figure_path": "a1wf2N967T/figures/figures_8_1.jpg", "caption": "Figure 7: Relation-aware disentanglement results on LSUN and the attributes beyond CelebA. Paired fine-grained attributes with inconsistent bidirectional relations are chosen to indicate effectiveness.", "description": "This figure shows qualitative results demonstrating GEM's ability to perform relation-aware disentanglement on complex scenes from the LSUN dataset (bedroom and horse). It showcases examples of fine-grained attributes with inconsistent bidirectional relations, highlighting GEM's capacity to capture complex inter-attribute relationships, even those that are not strictly causal or symmetric.", "section": "4.1 Qualitative Results"}, {"figure_path": "a1wf2N967T/figures/figures_8_2.jpg", "caption": "Figure 8: Evaluation experiments on the various MLLMs for attributes scoring.", "description": "This figure shows the evaluation results of different MLLMs (GLM-4, GPT-4V, GPT-40) on attribute scoring accuracy for various attributes (Young, Gender, Hairline, Makeup, Chubby, Eyebag, Mustache, Beard, Hat, Bald, Glasses, Blond, Lipstick, Sideburn, Bangs, Eyebrows).  The average accuracy and zero-scoring rate (percentage of attributes with zero scores) are shown for each MLLM, indicating their performance in attribute scoring. GPT-40 shows the best performance in terms of both average accuracy (88.4%) and a minimal zero-scoring rate (0.025%).", "section": "4.4 Evaluations of MLLMS"}, {"figure_path": "a1wf2N967T/figures/figures_9_1.jpg", "caption": "Figure 9: Ablation on replacing B-VAE with VAE, w/o graph learner, and w/o adversarial strategy.", "description": "This ablation study investigates the effects of removing or replacing key components in the GEM model.  The top two rows demonstrate the impact of removing the GNN-based graph learner and replacing the B-VAE with the vanilla VAE. The bottom two rows compare the full GEM model with a version that excludes the adversarial training strategy. The results illustrate the contributions of each component in achieving fine-grained and relation-aware disentanglement.", "section": "4.5 Ablation Study"}, {"figure_path": "a1wf2N967T/figures/figures_16_1.jpg", "caption": "Figure 10: 68-points landmark pre-processing for data from the CelebA.", "description": "This figure shows an example of the 68-point landmark detection pre-processing step used in the GEM model. The image displays a face with 68 key points identified and numbered, marking locations such as jawline, eyebrows, nose, eyes, and lips. This pre-processing step helps to refine and crop the input image before feeding it into the B-VAE based attribute determining branch.", "section": "A.4 Face landmark results"}, {"figure_path": "a1wf2N967T/figures/figures_16_2.jpg", "caption": "Figure 1: The comparison of typical DRL frameworks with our GEM. The limitations of conventional DRL methods are presented on the left. Conversely, the right-hand side illustrates the advantages of our framework, which benefited from the integration of the B-VAE and MLLMs.", "description": "This figure compares traditional disentangled representation learning (DRL) frameworks with the proposed GEM framework. The left side highlights limitations of conventional methods, such as ignoring logical relations between factors and relying on unrealistic assumptions. The right side showcases GEM's advantages, particularly its use of bidirectional weighted graphs and multimodal large language models (MLLMs) to capture complex data relationships and improve interpretability.", "section": "1 Introduction"}, {"figure_path": "a1wf2N967T/figures/figures_19_1.jpg", "caption": "Figure 6: Qualitative comparisons between GEM and typical DRL Methods. Each row in facial images corresponds to the traversal results on a specific attribute, as indicated adjacent to the images (i.e., Bangs, Bald, Gender, Beard, Blond, and Makeup). GEM exhibits superior ability in fine-grained disentanglement with discovered practical and bidirectional relations (illustrated by the heatmap).", "description": "This figure compares the qualitative disentanglement results of GEM with FactorVAE and DEAR on the CelebA dataset.  It shows traversals across different latent dimensions for six facial attributes (Bangs, Bald, Gender, Beard, Blond, Makeup), visualizing how changes in each latent dimension affect the corresponding attribute.  The heatmap illustrates the bidirectional relations between attributes discovered by GEM using MLLMs, highlighting GEM's superior performance in fine-grained disentanglement and capturing practical inter-attribute relationships.", "section": "4.1 Qualitative Results"}, {"figure_path": "a1wf2N967T/figures/figures_20_1.jpg", "caption": "Figure 6: Qualitative comparisons between GEM and typical DRL Methods. Each row in facial images corresponds to the traversal results on a specific attribute, as indicated adjacent to the images (i.e., Bangs, Bald, Gender, Beard, Blond, and Makeup). GEM exhibits superior ability in fine-grained disentanglement with discovered practical and bidirectional relations (illustrated by the heatmap).", "description": "This figure compares the qualitative results of GEM with FactorVAE and DEAR on the CelebA dataset. It shows traversals across various latent dimensions for six fine-grained facial attributes (Bangs, Bald, Gender, Beard, Blond, Makeup).  Each row represents a traversal along a specific attribute's latent dimension.  The results illustrate GEM's superior ability to achieve fine-grained disentanglement and capture practical bidirectional relations between attributes, as visualized in the heatmap showing correlation coefficients.", "section": "4.1 Qualitative Results"}, {"figure_path": "a1wf2N967T/figures/figures_21_1.jpg", "caption": "Figure 14: Reconstruction results of the LSUN-bedroom.", "description": "This figure shows the reconstruction results of the LSUN-bedroom dataset using the GEM model.  It visually demonstrates the model's ability to reconstruct images from the LSUN-bedroom dataset, showcasing its performance on a real-world, complex dataset with diverse and varied bedroom scenes. The image grid displays a selection of the input images alongside their corresponding reconstructions generated by GEM.", "section": "4.1 Qualitative Results"}, {"figure_path": "a1wf2N967T/figures/figures_22_1.jpg", "caption": "Figure 15: Reconstruction results of the LSUN-horse.", "description": "This figure shows the reconstruction results of the LSUN-horse dataset using the GEM model.  It visually demonstrates the model's ability to reconstruct images from the LSUN-horse dataset, showcasing its performance on a different dataset than CelebA, highlighting its generalizability. The images are arranged in a grid, allowing for a visual comparison of the original and reconstructed images.", "section": "4.1 Qualitative Results"}, {"figure_path": "a1wf2N967T/figures/figures_22_2.jpg", "caption": "Figure 6: Qualitative comparisons between GEM and typical DRL Methods. Each row in facial images corresponds to the traversal results on a specific attribute, as indicated adjacent to the images (i.e., Bangs, Bald, Gender, Beard, Blond, and Makeup). GEM exhibits superior ability in fine-grained disentanglement with discovered practical and bidirectional relations (illustrated by the heatmap).", "description": "This figure compares the qualitative results of GEM with FactorVAE and DEAR on CelebA dataset.  It shows traversals across different latent dimensions for six facial attributes: bangs, bald, gender, beard, blond, and makeup.  The results demonstrate GEM's superior ability to achieve fine-grained disentanglement, capturing both the individual attributes and their bidirectional relationships (shown in the heatmap). FactorVAE and DEAR show limitations in capturing fine-grained detail and relationships.", "section": "4.1 Qualitative Results"}]