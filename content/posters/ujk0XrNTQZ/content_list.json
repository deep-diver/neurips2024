[{"type": "text", "text": "DRAGO: Primal-Dual Coupled Variance Reduction for Faster Distributionally Robust Optimization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ronak Mehta1 Jelena Diakonikolas2 Zaid Harchaoui1 ", "page_idx": 0}, {"type": "text", "text": "1University of Washington, Seattle 2University of Wisconsin, Madison ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We consider the penalized distributionally robust optimization (DRO) problem with a closed, convex uncertainty set, a setting that encompasses learning using $f$ -DRO and spectral/ $\\mathcal{L}$ -risk minimization. We present DRAGO, a stochastic primal-dual algorithm that combines cyclic and randomized components with a carefully regularized primal update to achieve dual variance reduction. Owing to its design, DRAGO enjoys a state-of-the-art linear convergence rate on strongly convex-strongly concave DRO problems with a fine-grained dependency on primal and dual condition numbers. The theoretical results are supported by numerical benchmarks on regression and classification tasks. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Contemporary machine learning research is increasingly exploring the phenomenon of distribution shift, in which predictive models encounter differing data-generating distributions in training versus deployment [Wiles et al., 2022]. A popular approach to learn under potential distribution shift is distributionally robust optimization (DRO) of an empirical risk-type objective ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{w\\in\\mathcal{W}}{\\operatorname*{min}}\\;\\underset{q\\in\\mathcal{Q}}{\\operatorname*{max}}\\;\\Big[\\mathcal{L}_{0}(w,q):=\\sum_{i=1}^{n}q_{i}\\ell_{i}(w)\\Big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "where $\\ell_{i}:\\,\\mathbb{R}^{d}\\;\\rightarrow\\;\\mathbb{R}$ denotes the loss on training instance $i\\;\\in\\;[n]\\;:=\\;\\{1,\\ldots,n\\}$ , and $q\\ =$ $(q_{1},\\ldots,q_{n})\\ \\in\\ Q$ is a vector of $n$ weights for each example. The feasible set $\\mathcal{Q}$ , often called the uncertainty set, is a collection of possible instance-level reweightings arising from distributional shifts between train and evaluation data, and is often chosen as a ball about the uniform vector $\\mathbf{1}/n=(1/n,\\ldots,1/n)$ in $f$ -divergence [Namkoong and Duchi, 2016, Carmon and Hausler, 2022, Levy et al., 2020] or a spectral/ $\\cal L$ -risk-based uncertainty set [Mehta et al., 2023]. ", "page_idx": 0}, {"type": "text", "text": "We consider here the penalized version of (1), stated as ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\mathcal{L}(w,q):=\\sum_{i=1}^{n}q_{i}\\ell_{i}(w)-\\nu D(q\\|\\mathbf{1}/n)+\\frac{\\mu}{2}\\left\\|w\\right\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "where $\\mu,\\nu\\geq0$ are regularization parameters and $D(q||\\mathbf{1}/n)$ denotes some statistical divergence (such as the Kullback-Leibler (KL) or $\\chi^{2}$ -divergence) between the original weights ${\\mathbf{1}}/n$ and shifted weights $q$ . For clarity, we focus on the cases of $\\mu,\\nu>0$ , but also describe the modifications to the methods, results, and proofs for cases in which $\\mu=0$ or $\\nu=0$ , in Appx. C.4. See Fig. 1 for intuition on the relationship between the uncertainty set, divergence $D$ , and hyperparameter $\\nu$ . ", "page_idx": 0}, {"type": "text", "text": "Standard (1) and penalized (2) DRO objectives have seen an outpour of recent use in reinforcement learning and control [Lotidis et al., 2023, Yang et al., 2023, Wang et al., 2023a, Yu et al., 2023, Kallus et al., 2022, Liu et al., 2022] as well as creative applications in robotics [Sharma et al., 2020], language modeling [Liu et al., 2021], sparse neural network training [Sapkota et al., 2023], and defense against model extraction [Wang et al., 2023b]. However, even in a classical supervised learning setup, current optimization algorithms for DRO have limitations in both theory and practice. ", "page_idx": 0}, {"type": "image", "img_path": "ujk0XrNTQZ/tmp/08cfcd057057e6f5c5284ad91021bccb0d097f73436cbcb355bb63e74c87d415.jpg", "img_caption": ["Figure 1: Visualization of Uncertainty Sets and Penalties. Each plot is a probability simplex in $n=3$ dimensions with the uncertainty set as the colored portion. The black dots are optimal dual variables $\\begin{array}{r}{q_{\\nu}^{\\star}:=\\arg\\operatorname*{max}_{q\\in\\mathcal{Q}}\\sum_{i=1}^{n}q_{i}\\dot{\\ell}_{i}(w)-\\nu D(q\\|\\mathbf{1}/n)}\\end{array}$ for a fixed $w\\in\\mathcal{W}$ . As $\\nu$ decreases, $q_{\\nu}^{\\star}$ may shift toward the boundary of the uncertainty set. The combination of $\\nu$ and $D$ determines an \u201ceffective\u201d uncertainty set, whose shape is given by the level sets of $D$ . Our methods apply to both. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "For context, we consider the large-scale setting in which the sample size $n$ is high, and the training loss on each example is accessed via a collection of $n$ primal first-order oracles $\\{(\\boldsymbol{\\ell}_{i},\\nabla\\boldsymbol{\\ell}_{i})\\}_{i=1}^{n}$ . Quantitatively, we measure the performance of algorithms by the runtime or global complexity of elementary operations to reach within $\\varepsilon$ of the minimum of $\\bar{\\mathcal{L}}(w)\\;=\\;\\operatorname*{max}_{q\\in\\mathcal{Q}}\\bar{\\mathcal{L}}(w,q)$ , whereas qualitatively, we consider the types of uncertainty sets that can be handled by the algorithm and convergence analysis. Under standard assumptions, $\\bar{\\mathcal{L}}$ is differentiable with gradient computed via ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r}{q^{\\star}(w)=\\underset{q\\in\\mathbb{Q}}{\\arg\\operatorname*{max}}\\,\\mathcal{L}(w,q),\\;\\mathrm{followed}\\;\\mathrm{by}\\;\\nabla\\bar{\\mathcal{L}}(w)=\\sum_{i=1}^{n}q_{i}^{\\star}(w)\\nabla\\ell_{i}(w)+\\mu w.}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "In the learning setting, we are interested in stochastic algorithms that can approximate this gradient with $b<n$ calls to the oracles. For uniformly randomly sampled $i\\in[n],\\bar{n}q_{i}^{\\star}(w)\\nabla\\ell_{i}(w)\\bar{+}\\mu w$ is an unbiased estimator of $\\nabla\\bar{\\mathcal{L}}(w)$ . However, computing $q_{i}^{\\star}(w)$ depends on the first step in (3) which itself requires calling all $n$ oracles (see (2)), i.e., is no different than the cost of full batch gradient descent. A direct minibatch stochastic gradient descent approach would approximate $\\bar{\\mathcal{L}}(\\boldsymbol{w},\\bar{\\cdot})$ in the first step of (3) with only $b$ calls to generate approximate weights $\\hat{q}(w)$ . Because $\\hat{q}(w)\\neq q^{\\star}(w)$ in general for $b<n$ , these methods have non-vanishing bias, i.e., do not converge [Levy et al., 2020]. ", "page_idx": 1}, {"type": "text", "text": "This motivated research in DRO-specific stochastic algorithms with theoretical convergence guarantees under particular assumptions (see Tab. 1 and Appx. B for details) [Namkoong and Duchi, 2016, Levy et al., 2020, Carmon and Hausler, 2022]. While we highlight the dependence on sample size $n$ and suboptimality $\\varepsilon$ , the dependence on all constants is given in Tab. 1. For $f$ -divergence-based uncertainty sets in the standard oracle framework, several methods achieve a $O\\bar{(\\varepsilon^{-2})}$ complexity. Levy et al. [2020] do so by proving uniform bias bounds, so that if $b$ scales as $O\\!\\left(\\varepsilon^{-2}\\right)$ , the convergence guarantee is achieved. However, if the required batch size $b$ exceeds the training set size $n$ , then the method reduces to the sub-gradient method, as we can see in Tab. 1. These sublinear rates typically stem from two causes. The first is the adoption of a \u201dfully stochastic\u201d perspective on the oracles, wherein each oracle\u2019s output is treated as an independent random sample drawn from a probability distribution. The second is the non-smoothness of the objective, as we shall see below. ", "page_idx": 1}, {"type": "text", "text": "Variance reduction techniques, on the other hand, exploit the fact that the optimization algorithm takes multiple passes through the same dataset, and achieve linear rates of the form $O((n+\\kappa_{\\ell})\\log(\\varepsilon^{-1}))$ in empirical risk minimization when the objective is both smooth (i.e., has Lipschitz continuous gradient) and strongly convex and $\\kappa_{\\ell}$ is an associated condition number [Johnson and Zhang, 2013, Defazio et al., 2014]. Assuming access to stronger oracles involving constrained minimization and applying a variance reduction scheme, Carmon and Hausler [2022] achieve $O(n\\varepsilon^{-2/3}+n^{3/4}\\varepsilon^{-1})$ for $f$ -divergences as well, but do not obtain linear convergence due to the second type of cause: the objective $\\bar{\\nabla}\\bar{\\mathcal{L}}$ is non-smooth when $\\nu=0$ . ", "page_idx": 1}, {"type": "text", "text": "Recently, Mehta et al. [2024] handled the $\\nu\\,>\\,0$ case for spectral risk uncertainty sets, and their variance-reduced algorithm achieves a linear $O((n+\\kappa_{\\mathcal{Q}}\\kappa_{\\ell})\\ln(1/\\varepsilon))$ convergence guarantee (where $\\kappa_{\\mathcal{Q}}\\geq1$ measures the \u201csize\u201d of the uncertainty set), but only with a lower bound of order $\\Omega(n)$ on the problem parameter $\\nu$ . The challenge of this problem, considered from a general optimization viewpoint beyond DRO, stems from the non-bilinearity of the coupled term $\\check{\\sum}_{i=1}^{n}\\,q_{i}\\ell_{i}(\\boldsymbol{w})$ and the constraint that $\\textstyle\\sum_{i=1}^{n}q_{i}=1$ for probability vectors. If the coupled term was bilinear (i.e., of the form $q^{\\intercal}A w$ for $A\\,\\in\\,\\mathbb{R}^{n\\times d};$ and the constraints applied separately to each $q_{i}$ , then dual decomposition techniques could be used. Qualitatively, algorithms and analyses often rely on particular uncertainty sets; for example, Kumar et al. [2024] use duality arguments specific to the Kullback-Leibler uncertainty set to create a primal-only minimization problem. See Appx. B for a detailed discussion of related work from the ML and the optimization lenses. Given the interest from both communities, we address whether a stochastic DRO algorithm can simultaneously 1) achieve a linear convergence rate for any $\\nu>0$ and 2) apply to many common uncertainty sets. ", "page_idx": 2}, {"type": "text", "text": "Contributions We propose DRAGO, a minibatch primal-dual algorithm for the penalized DRO problem (2) that achieves $\\varepsilon$ -suboptimality in ", "page_idx": 2}, {"type": "equation", "text": "$$\nO\\left(\\left[\\frac{n}{b}+\\frac{\\kappa_{Q}L}{\\mu}+\\frac{n}{b}\\sqrt{\\frac{n G^{2}}{\\mu\\nu}}\\right]\\ln\\left(\\frac{1}{\\varepsilon}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "iterations, where $b\\,\\in\\,\\{1,\\ldots,n\\}$ is the minibatch size, $\\kappa_{\\mathcal{Q}}\\,=\\,n q_{\\mathrm{max}}\\,:=\\,n\\,\\mathrm{max}_{q\\in\\mathcal{Q},i\\in[n]}\\,q_{i}$ measures the size of the uncertainty set, and $G$ and $L$ are the Lipschitz continuity parameters of $\\ell_{i}$ and $\\nabla\\ell_{i}$ , respectively. For commonly used parameters of uncertainty sets, $n q_{\\mathrm{max}}$ is bounded above by an absolute constant independent in $n$ (see Prop. 3), so for $d\\,<\\,n$ and $b=n/d$ , we maintain an $O(n)$ per-iteration complexity (the dual dimensionality) while reducing the number of iterations to $O((d+n q_{\\operatorname*{max}}L/\\mu+d\\sqrt{n G^{2}/(\\mu\\nu)}))\\ln\\left(1/\\varepsilon\\right))$ . Theoretically, the complexity bound we achieve in (4) is the best one among current penalized DRO algorithms, delineating a clear dependence on smoothness constants of the coupled term and strong convexity constants of the individual terms in (2). Practically, DRAGO has a single hyperparameter and operates on any closed, convex uncertainty set for which the map $l\\,\\mapsto\\,{\\mathrm{arg}}\\,{\\mathrm{max}}_{q\\in{\\mathcal{Q}}}^{\\,\\bullet}\\left\\{\\,q^{\\top}l-\\nu D({\\dot{q}}\\|{\\bf1}/n)\\right\\}$ is efficiently computable. DRAGO is also of general conceptual interest as a stochastic variance-reduced primal-dual algorithm for min-max problems. It delicately combines randomized and cyclic components, which effectively address the varying dimensions of the two problems (see Sec. 2). The theoretical guarantees of the algorithm are explained in Sec. 3. Numerical performance benchmarks are shown in Sec. 4. ", "page_idx": 2}, {"type": "text", "text": "2 The DRAGO Algorithm ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We present here the Distributionally Robust Annular Gradient Optimizer (DRAGO). While similar in spirit to a primal-dual proximal gradient method with a stochastic flavor, there are several innovations that allow the algorithm to achieve its superior complexity guarantee. These include using 1) minibatch stochastic gradient estimates to improve the trade-off between the per-iteration complexity and required number of iterations (especially when $n\\gg d,$ ), 2) a combination of randomized and cyclically updated components in the primal and dual gradient estimates, and 3) a novel regularization term in the primal update which reduces variance in the gradient estimate (i.e., coupled variance reduction). Here, we describe the algorithm in a manner that helps elucidate the upcoming theoretical analysis (Sec. 3). On the other hand, in Appx. D, we present an alternate description of DRAGO that is amenable to direct implementation in code. ", "page_idx": 2}, {"type": "text", "text": "Notation $\\pmb{\\&}$ Terminology Let $\\psi\\;:\\;\\mathbb{R}^{n}\\;\\rightarrow\\;\\mathbb{R}\\cup\\{+\\infty\\}$ be a proper, convex function such that $\\mathcal{Q}\\subseteq\\mathrm{dom}(\\psi):=\\{q\\in\\mathbb{R}^{n}:\\psi(q)<+\\infty\\}$ . Let $\\psi$ have a non-empty subdifferential for each $q\\in\\mathcal{Q}$ , and denote by $\\nabla\\psi$ a map from $q\\in\\mathcal{Q}$ to an arbitrary but consistently chosen subgradient in $\\partial\\psi(q)$ . We denote the Bregman divergence generated by $\\psi$ as $\\Delta_{\\psi}(q,\\bar{q})=\\psi(q)-\\psi(\\bar{q})-\\langle\\nabla\\psi(\\bar{q}),q-\\bar{q}\\rangle$ . We define the Bregman divergence in this way for purely technical reasons, to gracefully account for cases such as computing $\\bar{\\Delta}_{\\psi}(q,\\bar{q})$ when $\\psi$ is the negative entropy function and $\\bar{q}$ lies on the boundary of the $n$ -dimensional probability simplex. Finding a minimizer of (2) is equivalent to finding a saddle-point $(w_{\\star},q_{\\star})\\in\\mathcal{W}\\times\\mathcal{Q}$ which satisfies ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{q\\in\\mathcal{Q}}\\mathcal{L}(w_{\\star},q)=\\mathcal{L}(w_{\\star},q_{\\star})=\\operatorname*{min}_{w\\in\\mathcal{W}}\\mathcal{L}(w,q_{\\star}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "table", "img_path": "ujk0XrNTQZ/tmp/34e9ec47910ad2cb70593fb289e1291429c8d5cff9c356d909792c6f76f32435.jpg", "table_caption": [], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "Table 1: Complexity Bounds of DRO Methods. Runtime or global complexity (i.e., the total number of elementary operations required to compute $w$ satisfying $\\begin{array}{r}{\\operatorname*{max}_{q\\in\\mathcal{Q}}\\mathcal{L}(w,q)-\\mathcal{L}(w_{\\star},q_{\\star})\\le\\varepsilon}\\end{array}$ . Throughout, we assume that each $\\ell_{i}$ is convex and $\\mu,\\nu\\geq0$ . The \u201cSupport Constrained\u201d uncertainty set refers to all closed, convex sets of probability mass vectors and any 1-strongly convex penalty. This includes $f$ -divergences and spectral risk measures, but not general Wasserstein balls. $^{*}\\mathbf{B}$ ounds hold in high probability.  Complexity is measured in our framework; see Appx. B for details. ", "page_idx": 3}, {"type": "text", "text": "We denote the Jacobian of $\\ell:=(\\ell_{1},\\ldots,\\ell_{n})$ at $w$ as $\\nabla\\ell(w)\\,\\in\\,\\mathbb{R}^{n\\times d}$ . We refer to the gradient of the coupled term $q^{\\top}\\ell(w)$ of (2) with respect to $w$ and $q$ (that is, ${\\nabla}\\ell(w)^{\\top}q$ and $\\ell(w))$ as the primal and dual gradients, respectively. In both the definition of the algorithm as well as the analysis, we will consider a sequence of positive constants $(a_{t})_{t\\geq1}$ with the additional values $a_{0}=0$ . The partial tshuem cs oonfv tehrigse snecqeu reantec e wwilill l bbe e pdreonpootretido $\\begin{array}{r}{A_{t}=\\sum_{\\tau=0}^{t}a_{\\tau}}\\end{array}$ .  SHeec.r e3, $t$ rseop rwees ewnitss ht hfeo ri ttehrea tisoeqn uceonucnet etro  wgrhoilwe $A_{t}^{-1}$ as fast as possible. As mentioned in Sec. 1, $d$ is the primal dimension, $n$ is the dual dimension as well as the sample size, and $b$ denotes a batch size that divides $n$ for ease of presentation. ", "page_idx": 3}, {"type": "text", "text": "Algorithm Description We specify the algorithm by recursively defining a sequence of primal-dual iterates $\\{(w_{t},q_{t})_{t\\ge1}\\}$ that achieve a particular convergence guarantee. First, we may assume that $\\textbf{0}\\in\\mathcal{W}$ without loss of generality, so fix $w_{0}\\;=\\;{\\bf0}$ and $q_{0}\\,=\\,{\\bf1}/n$ . For any $t\\,\\geq\\,1$ , we introduce $v_{t-1}^{\\mathrm{P}}$ and $v_{t}^{\\mathrm{D}}$ as to-be-specified stochastic gradient estimates of the quantities $\\nabla\\ell(w_{t-1})^{\\top}q_{t-1}$ and $\\ell(w_{t})$ , respectively. We choose to update $w_{t}$ before $q_{t}$ , so that $v_{t-1}^{\\mathrm{P}}$ may depend only on the history $\\{(w_{\\tau},q_{\\tau}\\}_{\\tau=0}^{t-1}$ , whereas $v_{t}^{\\mathrm{D}}$ may depend additionally on $w_{t}$ as well. Letting $(C_{t})_{t\\geq1}$ and $(c_{t})_{t\\geq1}$ be to-be-specified sequences of positive constants with $C_{0}=c_{0}=0$ , we employ primal-dual proximal approach guided by the updates ", "page_idx": 3}, {"type": "equation", "text": "$$\nw_{t}:=\\underset{w\\in\\mathcal{W}}{\\mathrm{arg}\\,\\mathrm{min}}\\,\\left\\lbrace a_{t}\\left\\langle v_{t}^{\\mathrm{P}},w\\right\\rangle+\\frac{a_{t}\\mu}{2}\\left\\|w\\right\\|_{2}^{2}+\\frac{C_{t-1}\\mu}{2}\\left\\|w-w_{t-1}\\right\\|_{2}^{2}\\right.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "For the primal update, despite the non-standard term (6), $w_{t}$ can be computed easily in closed form when $\\dot{\\mathcal{W}}=\\mathbb{R}^{d}$ . Otherwise, retrieving $w_{t}$ relies on computing an $\\ell_{2}$ -norm projection onto $\\mathcal{W}$ . On the other hand, the maximization (7) can often be solved exactly or to high accuracy using methods specific to each uncertainty set; we describe these in detail in Appx. D.2. The randomized and cyclic coordinate-wise components mentioned above are contained within the upcoming formulas for $v_{t-1}^{\\mathrm{P}}$ and $v_{t}^{\\mathrm{D}}$ . After specifying these vectors, the constants $(C_{t},c_{t})$ will be chosen in the analysis to recover the final algorithm. ", "page_idx": 3}, {"type": "text", "text": "Next, we describe the computation of $v_{t-1}^{\\mathrm{P}}$ and $v_{t}^{\\mathrm{D}}$ , which will rely on quantities that are stored by the algorithm along its iterations. We store three tables of values $\\hat{\\ell}_{t}\\,\\mathrm{\\Omega}\\in\\,\\mathbb{R}^{n}$ , $\\hat{q}_{t}\\ \\in\\ \\mathbb{R}^{n}$ , and $\\bar{g}^{\\^}\\in\\ \\mathbb{R}^{n\\times\\breve{d}}$ , which are approximations of $\\ell(w_{t})$ , $q_{t}$ , and $\\nabla\\ell(w_{t})$ , respectively. Before explaining how these tables are updated, consider the data indices $\\{1,\\ldots,n\\}$ to be partitioned into $n/b$ blocks, written $(B_{1},\\bar{\\dots},B_{n/b})$ for $B_{K}=(K-b+1,\\ldots,K\\dot{b})$ . On each iteration $t$ , we randomly sample independent block indices $I_{t},J_{t}\\sim\\mathrm{Unif}[n/b]$ and define ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{v_{t-1}^{\\mathrm{P}}=\\hat{g}_{t-1}^{\\top}\\hat{q}_{t-1}+\\frac{a_{t-1}}{a_{t}}\\cdot\\frac{n}{b}\\displaystyle\\sum_{i\\in B_{I_{t}}}\\big(q_{t-1,i}\\nabla\\ell_{i}(w_{t-1})-\\hat{q}_{t-2,i}\\hat{g}_{t-2,i}\\big)}\\\\ &{\\quad v_{t}^{\\mathrm{D}}=\\hat{\\ell}_{t}+\\frac{a_{t-1}}{a_{t}}\\cdot\\frac{n}{b}\\displaystyle\\sum_{j\\in B_{J_{t}}}\\big(\\ell_{j}(w_{t})-\\hat{\\ell}_{t-1,j}\\big)e_{j}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "As for the tables of approximations, we update them on each iteration without suffering the $O(n d)$ computational cost of querying every first-order oracle $(\\ell_{1},\\nabla\\ell_{1}),\\dots,(\\ell_{n},\\nabla\\ell_{n})$ . We set $\\left(\\hat{\\ell}_{0},\\hat{q}_{0},\\hat{g}_{0}\\right)=\\left(\\ell(w_{0}),q_{0},\\nabla\\ell(w_{0})\\right)$ , and for iteration $t\\geq1$ update block $K_{t}:=(n/b)$ mod $t+1$ via ", "page_idx": 4}, {"type": "equation", "text": "$$\n(\\widehat{\\ell}_{t,k},\\widehat{q}_{t,k},\\widehat{g}_{t,k})=\\left\\{\\begin{array}{l l}{(\\ell_{k}(w_{t}),q_{t,k},\\nabla\\ell_{k}(w_{t}))}&{\\mathrm{~if~}k\\in B_{K_{t}}}\\\\ {(\\widehat{\\ell}_{t-1,k},\\widehat{q}_{t-1,k},\\widehat{g}_{t-1,k})}&{\\mathrm{~if~}k\\notin B_{K_{t}}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "While $q_{t}$ in particular is always known by the algorithm, we use the approximation $\\hat{q}_{t}$ which is possibly dual infeasible for every $t\\geq1$ , but will approach $q_{t}$ as $t$ grows large. Using this approximation is essential to controlling the per-iteration time complexity, as described below. In addition, the design of (8) and (9) is grounded in the long line of work on incremental methods (both deterministic and randomized). The table of past gradients updated cyclically resembles IAG [Blatt et al., 2007], whereas the randomized component resembles methods such as SAGA [Defazio et al., 2014] and stochastic PDHG [Chambolle et al., 2018]. The full algorithm description is given in Algorithm 1. In the next section, we show that $(a_{t},C_{t},c_{t})_{t\\geq0}$ can be determined by a single hyperparameter. ", "page_idx": 4}, {"type": "text", "text": "Computational Complexity We also discuss the per-iteration time complexity and the global space complexity of Algorithm 1, whereas the number of required iterations for $\\varepsilon$ -suboptimality is given in the next section. We see that the per-iteration time complexity is $O(n+b{\\bar{d}})$ , as we query $b$ first-order oracles in both the primal and dual updates and all other operations occur on $n$ -length or $d\\!.$ -length vectors. While we need to employ $O(n d)$ operations to compute $\\hat{g}_{t-1}^{\\top}\\hat{q}_{t-1}$ in (8) when $t=1$ , this quantity can be maintained with $O(b d)$ operations in every subsequent iteration as only $b$ rows of $\\hat{q}_{t}$ and ${\\hat{g}}_{t}$ are editted in each iteration. For space complexity, we may consider storing the entire gradient table $\\hat{g}_{t}$ in memory, resulting in an $O(n d)$ complexity. However, due to our time complexity calculation, we may reduce the space complexity to $O(\\dot{n}+b d)$ by storing only $w_{t-1},\\ldots,w_{t-n/b}$ and recomputing the relevant values of ${\\hat{g}}_{t}$ in (8) in every iteration. Therefore, the use of block-cyclic updates in the historical tables may significantly reduce the space complexity as compared to randomized updates (as in Defazio et al. [2014]). ", "page_idx": 4}, {"type": "text", "text": "3 Theoretical Analysis ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We provide the theoretical convergence rate and global complexity of DRAGO along with technical highlights of the proof which may be of independent interest. ", "page_idx": 4}, {"type": "text", "text": "Convergence Analysis We measure suboptimality using the primal-dual gap: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\gamma_{t}:=\\mathcal{L}(w_{t},q_{\\star})-\\mathcal{L}(w_{\\star},q_{t})-\\frac{\\mu}{2}\\left\\|w_{t}-w_{\\star}\\right\\|_{2}^{2}-\\frac{\\nu}{2}\\left\\|q_{t}-q_{\\star}\\right\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the saddle point $(w^{\\star},q^{\\star})$ exists under Asm. 1. Note that $\\gamma_{t}~\\geq~0$ , as it is the sum of nonnegative quantities ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}(w_{t},q_{\\star})-\\mathcal{L}(w_{\\star},q_{\\star})-\\frac{\\mu}{2}\\left\\lVert w_{t}-w_{\\star}\\right\\rVert_{2}^{2}\\geq0\\mathrm{~and~}\\mathcal{L}(w_{\\star},q_{\\star})-\\mathcal{L}(w_{\\star},q_{t})-\\frac{\\nu}{2}\\left\\lVert q_{t}-q_{\\star}\\right\\rVert_{2}^{2}\\geq0.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "To state the main result, define $\\mathbb{E}_{t}$ as the conditional expectation over $(I_{t},J_{t})$ given $(w_{t-1},q_{t-1})$ and $\\mathbb{E}_{1}$ as the marginal expectation over the optimization trajectory. Consider the following assumptions. ", "page_idx": 4}, {"type": "text", "text": "Algorithm 1 Distributionally Robust Annular Gradient Optimizer (DRAGO) ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Input: Sequence of constants $(a_{t},C_{t},c_{t})_{t\\geq0}$ , block size $b\\in\\{1,\\ldots,n\\}$ , number of iterations $T$ .   \nInitialize $w_{0}=0_{d}$ , $q_{0}=\\mathbf{1}/n$ , $\\hat{\\ell}_{0}=\\ell(w_{0})$ , $\\hat{g}_{0}=\\nabla\\ell(w_{0})$ , and $\\hat{q}_{0}=q_{0}$ . ", "page_idx": 5}, {"type": "text", "text": "for $t=1$ to $T$ do ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Sample blocks $I_{t}$ and $J_{t}$ uniformly on $[n/b]$ , and define $K_{t}=t\\ \\ \\mathrm{mod}\\ (n/b)+1.$ ", "page_idx": 5}, {"type": "text", "text": "Primal Update: ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Set $\\begin{array}{r}{v_{t-1}^{\\mathrm{P}}=\\hat{g}_{t-1}^{\\top}\\hat{q}_{t-1}+\\frac{a_{t-1}n}{a_{t}b}\\sum_{i\\in B_{I_{t}}}(q_{t-1,i}\\nabla\\ell_{i}(w_{t-1})-\\hat{q}_{t-2,i}\\hat{g}_{t-2,i})}\\end{array}$ , update $w_{t}$ using (5).   \nUpdate table $(\\widehat{\\ell}_{t,k},\\widehat{g}_{t,k})$ to $(\\ell_{k}(w_{t}),\\nabla\\ell_{k}(w_{t}))$ if $k\\in B_{K_{t}}$ or $(\\hat{\\ell}_{t-1,k},\\hat{g}_{t-1,k})$ if $k\\notin B_{K_{t}}$ . ", "page_idx": 5}, {"type": "text", "text": "Dual Update: ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Set $\\begin{array}{r}{v_{t}^{\\mathrm{D}}=\\hat{\\ell}_{t}+\\frac{a_{t-1}n}{a_{t}b}\\sum_{j\\in B_{J_{t}}}(\\ell_{j}(w_{t})-\\hat{\\ell}_{t-1,j})e_{j}}\\end{array}$ , update $q_{t}$ using (7).   \nUpdate table $\\hat{q}_{t,k}$ to $q_{t,k}$ if $k\\in B_{K_{t}}$ or $\\hat{q}_{t-1,k}$ if $k\\notin B_{K_{t}}$ . ", "page_idx": 5}, {"type": "text", "text": "end for ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "return $(w_{T},q_{T})$ . ", "page_idx": 5}, {"type": "text", "text": "Assumption 1. Let $\\ell_{1},\\ldots,\\ell_{n}$ be $G$ -Lipschitz continuous and $L$ -smooth, in that for all $i\\in[n]$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\ell_{i}(w)-\\ell_{i}(w^{\\prime})\\|_{2}\\le G\\,\\|w-w^{\\prime}\\|_{2}\\;\\,a n d\\;\\,\\|\\nabla\\ell_{i}(w)-\\nabla\\ell_{i}(w^{\\prime})\\|_{2}\\le L\\,\\|w-w^{\\prime}\\|_{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "i.e., each $\\ell_{i}$ is $G$ -Lipschitz continuous and $L$ -smooth with respect to $\\lVert\\cdot\\rVert_{2}$ . Let $\\mu>0$ and $\\nu\\,>\\,0$ , and let $q\\mapsto D(q\\|\\mathbf{1}_{n}^{-}/n)$ be 1-strongly convex with respect to $\\lVert\\cdot\\rVert_{2}$ . Finally, $\\mathcal{Q}$ is closed, convex, and contains ${\\mathbf{1}}/n$ . ", "page_idx": 5}, {"type": "text", "text": "Regarding Asm. 1, an example of a loss function satisfying both Lipschitzness and smoothness is given by the Huber loss used in robust statistics [Huber, 1981]. Another setting in which both assumptions are satisfied is when the domain $\\mathcal{W}$ is compact, as smoothness will imply Lipschitz continuity. Compactness is a common assumption when pursuing statistical guarantees such as uniform convergence. As for the assumption of strong convexity, we describe modifications of the algorithm when $\\mu=0$ or $\\nu=0$ in Appx. C.4 along with corresponding changes in the analysis. ", "page_idx": 5}, {"type": "text", "text": "Theorem 2. For a constant $\\alpha>0$ , define the sequence ", "page_idx": 5}, {"type": "equation", "text": "$$\na_{1}=1,a_{2}=4\\alpha,\\;a n d\\,a_{t}=\\left(1+\\alpha\\right)a_{t-1}\\,f o r\\,t>2,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "along with its partial sum $\\begin{array}{r}{A_{t}=\\sum_{\\tau=1}^{t}a_{\\tau}}\\end{array}$ . Under Asm. 1, there is an absolute constant $C$ such that using the parameter ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\alpha=C\\operatorname*{min}\\left\\{\\frac{b}{n},\\frac{\\mu}{L\\kappa_{\\mathscr{Q}}},\\frac{b}{n}\\sqrt{\\frac{\\mu\\nu}{n G^{2}}}\\right\\},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "the iterates of Algorithm $^{\\,l}$ satisfy: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}a_{t}\\mathbb{E}_{1}[\\gamma_{t}]+\\frac{A_{T}\\mu}{4}\\mathbb{E}_{1}\\left\\Vert w_{T}-w_{\\star}\\right\\Vert_{2}^{2}+\\frac{A_{T}\\nu}{4}\\mathbb{E}_{1}\\left\\Vert q_{T}-q_{\\star}\\right\\Vert_{2}^{2}\\leq\\frac{n G^{2}}{\\nu}\\left\\Vert w_{0}-w_{1}\\right\\Vert_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We can compute a point $(w_{T},q_{T})$ achieving an expected gap no more than $\\varepsilon$ with big- $O$ complexity ", "page_idx": 5}, {"type": "equation", "text": "$$\n(n+b d)\\cdot\\left({\\frac{n}{b}}+{\\frac{L\\kappa_{Q}}{\\mu}}+{\\frac{n}{b}}{\\sqrt{\\frac{n G^{2}}{\\mu\\nu}}}\\right)\\cdot\\ln\\left({\\frac{1}{\\varepsilon}}\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "By dividing the result of Thm. 2 by $A_{T}$ , we see that both the expected gap and expected distanceto-optimum in the primal and dual sequences decay geometrically in $T$ . By plugging in $b=n/d$ we get the following runtime for Algorithm 1: ", "page_idx": 5}, {"type": "equation", "text": "$$\nO\\left(n d+\\frac{n d L\\kappa_{\\mathcal{Q}}}{\\mu}+n^{3/2}d\\sqrt{\\frac{G^{2}}{\\mu\\nu}}\\right)\\ln\\left(\\frac{1}{\\varepsilon}\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Note in particular the individual dependence on the condition numbers $L/\\mu$ and $n G^{2}/(\\mu\\nu)$ , as opposed to the max-over-min-type condition numbers such as those achievable by generic primaldual or variational inequality methods (see Appx. B.3). The proof of Thm. 2 is provided in Appx. C, along with a high-level overview in Appx. C.1. Our analysis relies on controlling $a_{t}\\gamma_{t}$ by bounding $a_{t}\\mathcal{L}(w_{\\star},q_{t})$ above and bounding $a_{t}\\mathcal{L}(w_{t},q_{\\star})$ below. Key technical steps occur in the lower bound. We first apply that $w\\,\\mapsto\\,a_{t}q_{t}^{\\top}\\ell(w)$ is convex to produce a linear underestimator of the function supported at $w_{t}$ , and use that $w_{t}$ is the minimizer of a strongly convex function, so ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{a_{t}\\mathcal{L}\\big(w_{\\star},q_{t}\\big)\\geq a_{t}q_{t}^{\\top}\\ell\\big(w_{t}\\big)+\\mathrm{telescoping~and~non\\mathrm{-}p~}}}\\\\ {{\\displaystyle~~~~~~~~~~~~~~~+{a_{t}\\left\\langle\\nabla\\ell(w_{t})^{\\top}q_{t}-v_{t}^{\\mathrm{P}},w_{\\star}-w_{t}\\right\\rangle}}}\\\\ {{\\displaystyle~~~~~~~~~~~~~+\\frac{c_{t}\\mu}{2}\\sum_{\\tau=t-n/b}^{t-2}\\left\\|w_{t}-w_{\\tau\\vee0}\\right\\|_{2}^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Note that the terms above will be negated when combining the upper and lower bounds. The labeled terms are highly relevant in the analysis, with the second being non-standard. By expanding the definition of $\\bar{v}_{t}^{\\mathrm{P}}$ and adding and subtracting $w_{t-1}$ , we write ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{a_{t}\\left\\langle\\nabla\\ell(w_{t})^{\\top}q_{t}-v_{t}^{\\mathrm{P}},w_{\\star}-w_{t}\\right\\rangle=a_{t}\\left\\langle\\nabla\\ell(w_{t})^{\\top}q_{t}-\\hat{g}_{t-1}^{\\top}\\hat{q}_{t-1},w_{\\star}-w_{t}\\right\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\qquad-\\displaystyle\\frac{n a_{t-1}}{b(1+\\eta)}\\sum_{i\\in I_{t}}\\left\\langle\\nabla\\ell_{i}(w_{t-1})q_{t-1}-\\hat{g}_{t-2,i}\\hat{q}_{t-2,i},w_{\\star}-w_{t-1}\\right\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad-\\displaystyle\\frac{n a_{t-1}}{b(1+\\eta)}\\sum_{i\\in I_{t}}\\left\\langle\\nabla\\ell_{i}(w_{t-1})q_{t-1}-\\hat{g}_{t-2,i}\\hat{q}_{t-2,i},w_{t-1}-w_{t}\\right\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "When choosing the learning rate $\\eta$ correctly, the first two terms will telescope in expectation (see Lem. 11). The third term, after applying Young\u2019s inequality, requires controlling $\\begin{array}{r}{\\frac{1}{b}\\sum_{i\\in I_{t}}\\|\\nabla\\ell_{i}(w_{t-1})q_{t-1}-\\hat{g}_{t-2,i}\\hat{q}_{t-2,i}\\|_{2}^{2}}\\end{array}$ in expectation, which we dub the \u201cprimal noise bound\u201d (Lem. 5 ). When we combine the upper and lower bounds, we get a similar inner product term $a_{t}\\left\\langle q_{\\star}-q_{t},\\ell(w_{t})-v_{t}^{\\mathrm{D}}\\right\\rangle$ , and mirroring the arguments above, we encounter the term $\\begin{array}{r}{\\frac{1}{b}\\sum_{j\\in J_{t}}(\\ell_{j}(w_{t})-\\hat{\\ell}_{t-1,j})_{2}^{2}}\\end{array}$ which also requires a \u201cdual noise bound\u201d (Lem. 6). Without loss of generality, assume that the blocks are ordered such that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\hat{\\ell}_{t-1,i}=\\ell_{i}(w_{t-1-K\\vee0})\\;\\mathrm{for}\\;i\\in B_{K}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "By computing the conditional expectation $\\mathbb{E}_{t}[\\cdot]:=\\mathbb{E}[\\cdot|w_{t-1}]$ , we have that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{t}\\left[\\displaystyle\\frac1b\\sum_{j\\in J_{t}}(\\ell_{j}(w_{t})-\\hat{\\ell}_{t-1,j})_{2}^{2}\\right]=\\displaystyle\\frac1n\\sum_{i=1}^{n}(\\ell_{i}(w_{t})-\\hat{\\ell}_{t-1,i})_{2}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\frac1n\\sum_{K=1}^{n/b}\\sum_{i\\in B_{K}}(\\ell_{i}(w_{t})-\\ell_{i}(w_{t-1-K\\vee0}))_{2}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\displaystyle\\frac{G^{2}b}{n}\\sum_{\\tau=t-n/b}^{t-2}\\|w_{t-1}-w_{\\tau\\vee0}\\|_{2}^{2}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where the second line follows from (14) and the third from $G$ -Lipschitzness. This will telescope with the second term introduced in (13), showing the importance of the regularization. The argument follows similarly even when (14) does not hold, as the blocks can simply be \u201crenamed\u201d to achieve the final bound. While the proof is technical, this core idea guides the analysis and the algorithm design. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we provide numerical benchmarks to measure DRAGO against baselines in terms of evaluations of each component $\\{(\\boldsymbol{\\ell}_{i},\\nabla\\boldsymbol{\\ell}_{i})\\}_{i=1}^{n}$ and wall clock time. We consider regression and classification tasks. Letting $(x_{i},y_{i})$ denote a feature-label pair, we have that each $\\ell_{i}$ represents the ", "page_idx": 6}, {"type": "image", "img_path": "ujk0XrNTQZ/tmp/418a640082e630485bd15d8bfd924dcf811456ec3e099f00dde852b02a00f1e2.jpg", "img_caption": ["Figure 2: Regression Benchmarks. In both panels, the $y$ -axis measures the primal suboptimality gap (15). Individual plots correspond to particular datasets. Left: The $x_{\\mathrm{~\\,~}}$ -axis displays the number of individual first-order oracle queries to $\\{(\\boldsymbol{\\ell}_{i},\\nabla\\boldsymbol{\\ell}_{i})\\}_{i=1}^{n}$ . Right: The $x$ -axis displays wall-clock time. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "squared error loss or multinomial cross-entropy loss, given by ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\ell_{i}(w):=\\frac{1}{2}\\left(y_{i}-x_{i}^{\\top}w\\right)^{2}\\mathrm{~and~}\\ell_{i}(w):=-x_{i}^{\\top}w_{y_{i}}+\\log\\sum_{y\\in\\mathbb{Y}}\\exp\\left(x_{i}^{\\top}w_{y}\\right),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "respectively. In the latter case, we denote $\\boldsymbol{w}\\,=\\,(w_{1},\\dots,w_{C})\\,\\in\\,\\mathbb{R}^{C\\times d}$ , indicating multiclass classification with label set $\\mathcal{Y}:=\\{1,\\dots,C\\}$ . We show results for the conditional value-at-risk (CVaR) [Rockafellar and Royset, 2013] in this section, exploring the effect of sample size $n$ , dimensionality $d$ , and regularization parameter $\\nu$ on optimization performance. The parameter $\\nu$ in particular has interpretations both as a conditioning device (as it is inversely related to the smoothness constant of $w\\mapsto\\operatorname*{max}_{q\\in\\mathcal{Q}}\\mathcal{L}(w,q)$ and as a robustness parameter, as it controls the essential size of the uncertainty set. Detailed experimental settings are contained in Appx. E, including additional experiments with the $\\chi^{2}$ -divergence ball uncertainty set. The code to reproduce these experiments can be found at https://github.com/ronakdm/drago. ", "page_idx": 7}, {"type": "text", "text": "We compare against baselines that can be used on the CVaR uncertainty set: distributionally robust stochastic gradient descent (SGD) [Levy et al., 2020] and LSVRG [Mehta et al., 2023]. For SGD, we use a batch size of 64 and for LSVRG we use the default epoch length of $n$ . For DRAGO, we investigate the variants in which $b$ is set to 1 and $b=n/d$ a priori, as well as cases when $b$ is a tuned hyperparameter. On the $y$ -axis, we plot the primal gap ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\!\\!\\!\\!\\!\\!\\frac{\\operatorname*{max}_{q\\in\\mathcal{Q}}\\mathcal{L}\\!\\left(w_{t},q\\right)-\\mathcal{L}\\!\\left(w_{\\star},q_{\\star}\\right)}{\\operatorname*{max}_{q\\in\\mathcal{Q}}\\mathcal{L}\\!\\left(w_{0},q\\right)-\\mathcal{L}\\!\\left(w_{\\star},q_{\\star}\\right)}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where we approximate $\\mathcal{L}(w_{\\star},q_{\\star})$ by running LBFGS [Nocedal and Wright, 1999] on the primal objective until convergence. On the $x$ -axis, we display either the exact number of calls to the firstorder oracles of the form $(\\ell_{i},\\nabla\\ell_{i})$ or the wall clock time in seconds. We fix $\\mu=1$ but vary $\\nu$ to study its role as a conditioning parameter, which is especially important as prior work establishes different convergence rates for different values of $\\nu$ (see Tab. 1). ", "page_idx": 7}, {"type": "text", "text": "4.1 Regression with Large Block Sizes ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this experiment, we consider six regression datasets, named yacht $n=244$ , $d=6$ ) [Tsanas and Xifara, 2012], energy $(n=614,d=8)$ [Baressi Segota et al., 2020], concrete $(n=824,d=8)$ [Yeh, 2006], acsincome $(n\\,=\\,4000,d\\,=\\,202)$ [Ding et al., 2021], $\\mathrm{kin}8\\mathrm{nm}$ $(n\\,=\\,6553,d\\,=\\,8)$ [Akujuobi and Zhang, 2017], and power $n=7654$ , $d=4$ ) [Tu\u00a8fekci, 2014]. In each case, there is a univariate, real-valued output. Notice that most datasets, besides acsincome, are low-dimensional as compared to their sample size. Thus, the default block size $n/d$ becomes relatively large, imposing an expensive per-iteration cost in terms of oracle queries. However, when the block size is high, the stochastic gradient estimates in each iteration have lower variance and the table components are updated more frequently, which could improve convergence in principle. The main question of this section is whether DRAGO efficiently manages this trade-off via the block size parameter $b$ . Results for gradient evaluations and wall clock time are on the left and right panels of Fig. 2, respectively. ", "page_idx": 7}, {"type": "image", "img_path": "ujk0XrNTQZ/tmp/23fa2cc229117dc1996b45b1e4e6671998db6fbeb2781a2fd2c70d014bde9eac.jpg", "img_caption": ["Figure 3: Text Classification Benchmarks. In all plots, the $y$ -axis measures the normalized primal (i.e., DRO risk) suboptimality gap, defined in (15). Columns represent a varying dual regularization parameter $\\nu$ . On the first three columns the $x$ -axis measures the number of individual first-order oracle queries to $\\{(\\boldsymbol{\\ell}_{i},\\nabla\\boldsymbol{\\ell}_{i})\\}_{i=1}^{n}$ and the remaining three the $x$ -axis displays wall-clock time. The objective becomes ill-conditioned as $\\nu$ decreases. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Results The DRAGO variant for $b=n/d$ is not included on the left plot, as the number of queries (almost 2,000 in the case of power) penalizes its performance heavily. Still, the same variant performs best or near best on all datasets in terms of wall clock time (right plot). Thus, if the computation of the queries is inexpensive enough, DRAGO can achieve the lowest suboptimality within a fixed time budget. This is most striking in the case of $\\mathrm{kin}8\\mathrm{nm}$ , in which DRAGO achieves $10^{-7}$ primal gap within 1 second, versus LSVRG which is only able to reach within $10^{-2}$ of the minimum in the same amount of time. We also experiment with tuning $b$ to reach a balance between the cost of queries and distance to optimum in the left plot with the $b=16$ variant. In the datasets with $n\\leq1,000$ , DRAGO can match the performance of baselines with only $b=1$ , whereas in the larger datasets, a batch size of 16 is needed to be comparable. ", "page_idx": 8}, {"type": "text", "text": "4.2 Text Classification Under Ill-Conditioning ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We consider a natural language processing example using the emotion dataset [Saravia et al., 2018], which is a classification task consisting of six sentiment categories: sadness, anger, love, fear, joy, and surprise. To featurize the text, we fine-tune a pre-trained BERT network [Devlin et al., 2019] on a held-out set of 8,000 training examples to learn a vectorial representation. We then use a disjoint subset of 8,000 training points and apply PCA to reduce them to 45-dimensional vectors. Because of the six classes this results in $d\\:=\\:270$ parameters to learn. To study the effect of dual regularization, we consider $\\nu\\,\\in\\,\\{1.0,0.01,0.001\\}$ . As $\\nu$ decreases, the dual solution may shift further from uniformity, and potentially increase the distributional robustness of the learned minimizer. However, the objective can also become poorly conditioned, introducing a key trade-off between optimization and statistical considerations when selecting $\\nu$ . The results are in Fig. 3. ", "page_idx": 8}, {"type": "text", "text": "Results The run time required for LSVRG to make 500K gradient evaluations is too large to be considered. We also observe that LSVRG is vulnerable to ill-conditioned objectives, as it is outperformed by SGD for smaller values of $\\nu$ in terms of wall clock time. Within 4 seconds, DRAGO can achieve close to a $10^{-5}$ primal suboptimality gap while the gaps of SGD and LSVRG are 2 to 3 orders of magnitude larger in the same amount of time. We hypothesize that because the dual variables in LSVRG are updated once every $n$ iterations, the primal gradient estimates may accrue excessive bias. DRAGO with $b=n/d$ , making $\\sim30$ individual first-order queries per iteration, is performant in terms of oracle queries and wall clock time even as $\\nu$ drops by 3 orders of magnitude. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We proposed DRAGO, a stochastic primal-dual algorithm for solving a host of distributionally robust optimization (DRO) problems. The method achieves linear convergence without placing conditions on the dual regularizer, and its empirical performance remains strong across varying settings of the sample size $n$ , dimension $d$ , and the dual regularization parameter $\\nu$ . The method combines ideas of variance reduction, minibatching, and cyclic coordinate-style updates even though the dual feasible set (a.k.a. the uncertainty set) is non-separable. Opportunities for future work include extensions to non-convex settings and applications to min-max problems beyond distributional robustness, such as missing data imputation and fully composite optimization. ", "page_idx": 8}, {"type": "text", "text": "Acknowledgements This work was supported by NSF DMS-2023166, CCF-2019844, DMS2134012, NIH, IARPA 2022-22072200003, U. S. Office of Naval Research under award number N00014-22-1-2348. Part of this work was done while R. Mehta and Z. Harchaoui were visiting the Simons Institute for the Theory of Computing. ", "page_idx": 9}, {"type": "text", "text": "Broader Impact Distributionally robust optimization (DRO) within machine learning is heavily motivated by problems in artificial intelligence (AI) safety, such as mitigating catastrophic performance of models on minority groups or end-users. While this work is focused on theoretical and algorithmic aspects of the problem, we intend to increase accessibility and scalability for downstream applications as well. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "U. Akujuobi and X. Zhang. Delve: A Dataset-Driven Scholarly Search and Analysis System. SIGKDD Explor. Newsl., 19, 2017.   \nA. Alacaoglu and Y. Malitsky. Stochastic variance reduction for variational inequality methods. In Conference on Learning Theory, pages 778\u2013816. PMLR, 2022.   \nA. Alacaoglu, V. Cevher, and S. J. Wright. On the complexity of a practical primal-dual coordinate method. arXiv preprint arXiv:2201.07684, 2022.   \nS. Baressi Segota, N. Andelic, J. Kudlacek, and R. Cep. Artificial Neural Network for Predicting Values of Residuary Resistance per Unit Weight of Displacement. Journal of Maritime & Transportation Science, 57, 2020.   \nJ. Blanchet, Y. Kang, and K. Murthy. Robust Wasserstein Profile Inference and Applications to Machine Learning. Journal of Applied Probability, 56, 2019.   \nD. Blatt, A. O. Hero, and H. Gauchman. A Convergent Incremental Gradient Method with a Constant Step Size. SIAM Journal on Optimization, 2007.   \nX. Cai, C. Song, S. Wright, and J. Diakonikolas. Cyclic Block Coordinate Descent With Variance Reduction for Composite Nonconvex Optimization. In ICML, 2023.   \nX. Cai, A. Alacaoglu, and J. Diakonikolas. Variance reduced Halpern iteration for finite-sum monotone inclusions. In ICLR, 2024.   \nY. Carmon and D. Hausler. Distributionally Robust Optimization via Ball Oracle Acceleration. In NeurIPS, 2022.   \nA. Chambolle, M. J. Ehrhardt, P. Richta\u00b4rik, and C.-B. Schonlieb. Stochastic Primal-Dual Hybrid Gradient Algorithm with Arbitrary Sampling and Imaging Applications. SIAM Journal on Optimization, 28(4):2783\u20132808, 2018.   \nL. Condat. Fast projection onto the simplex and the $\\ell_{1}$ -ball. Mathematical Programming, 2016.   \nA. Defazio, F. Bach, and S. Lacoste-Julien. SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives. NeurIPS, 27, 2014.   \nJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2019.   \nF. Ding, M. Hardt, J. Miller, and L. Schmidt. Retiring Adult: New Datasets for Fair Machine Learning. In NeurIPS, volume 34. Curran Associates, Inc., 2021.   \nS. S. Du, G. Gidel, M. I. Jordan, and C. J. Li. Optimal Extragradient-Based Bilinearly-Coupled Saddle-Point Optimization, 2022.   \nN. He, Z. Harchaoui, Y. Wang, and L. Song. Point Process Estimation with Mirror Prox Algorithms. Applied Mathematics & Optimization, 2020. ", "page_idx": 9}, {"type": "text", "text": "P. J. Huber. Robust statistics. Wiley New York, 1981. ", "page_idx": 10}, {"type": "text", "text": "R. Johnson and T. Zhang. Accelerating Stochastic Gradient Descent using Predictive Variance Reduction. In NeurIPS, volume 26, 2013.   \nN. Kallus, X. Mao, K. Wang, and Z. Zhou. Doubly Robust Distributionally Robust Off-Policy Evaluation and Learning. In ICML, 2022.   \nD. Kovalev, A. Gasnikov, and P. Richta\u00b4rik. Accelerated Primal-Dual Gradient Method for Smooth and Convex-Concave Saddle-Point Problems with Bilinear Coupling. In NeurIPS, 2022.   \nD. Kuhn, P. M. Esfahani, V. A. Nguyen, and S. Shafieezadeh-Abadeh. Wasserstein Distributionally Robust optimization: Theory and Applications in Machine Learning. Operations research & management science in the age of analytics, 2019.   \nR. Kumar, K. A. Majmundar, D. M. Nagaraj, and A. Suggala. Stochastic Re-weighted Gradient Descent via Distributionally Robust Optimization. TMLR, 2024.   \nD. Levy, Y. Carmon, J. Duchi, and A. Sidford. Large-Scale Methods for Distributionally Robust Optimization. In NeurIPS, 2020.   \nC. J. Li, A. Yuan, G. Gidel, Q. Gu, and M. I. Jordan. Nesterov meets optimism: rate-optimal separable minimax optimization. In ICML, 2023.   \nT. Lin, C. Jin, and M. I. Jordan. Near-Optimal Algorithms for Minimax Optimization. In COLT, 2020.   \nE. Z. Liu, B. Haghgoo, A. S. Chen, A. Raghunathan, P. W. Koh, S. Sagawa, P. Liang, and C. Finn. Just Train Twice: Improving Group Robustness without Training Group Information. In ICML, 2021.   \nZ. Liu, Q. Bai, J. Blanchet, P. Dong, W. Xu, Z. Zhou, and Z. Zhou. Distributionally Robust $Q$ - Learning. In ICML, 2022.   \nK. Lotidis, N. Bambos, J. Blanchet, and J. Li. Wasserstein Distributionally Robust Linear-Quadratic Estimation under Martingale Constraints. In AISTATS, 2023.   \nR. Mehta, V. Roulet, K. Pillutla, L. Liu, and Z. Harchaoui. Stochastic Optimization for Spectral Risk Measures. In AISTATS, 2023.   \nR. Mehta, V. Roulet, K. Pillutla, and Z. Harchaoui. Distributionally Robust Optimization with Bias and Variance Reduction. In ICLR, 2024.   \nH. Namkoong and J. C. Duchi. Stochastic Gradient Methods for Distributionally Robust Optimization with f-divergences. In NeurIPS, 2016.   \nH. Namkoong and J. C. Duchi. Variance-based Regularization with Convex Objectives. In NeurIPS, 2017.   \nJ. Nocedal and S. J. Wright. Numerical Optimization. Springer, 1999.   \nB. Palaniappan and F. Bach. Stochastic Variance Reduction Methods for Saddle-Point Problems. NeurIPS, 29, 2016.   \nR. T. Rockafellar and J. O. Royset. Superquantiles and Their Applications to Risk, Random Variables, and Regression. In Theory Driven by Influential Applications. Informs, 2013.   \nH. Sapkota, D. Wang, Z. Tao, and Q. Yu. Distributionally Robust Ensemble of Lottery Tickets Towards Calibrated Sparse Network Training. In NeurIPS, 2023.   \nE. Saravia, H.-C. T. Liu, Y.-H. Huang, J. Wu, and Y.-S. Chen. CARER: Contextualized Affect Representations for Emotion Recognition. In EMNLP, 2018.   \nV. D. Sharma, M. Toubeh, L. Zhou, and P. Tokekar. Risk-Aware Planning and Assignment for Ground Vehicles using Uncertain Perception from Aerial Vehicles. In 2020 IEEE/RSJ IROS, 2020.   \nC. Song, S. J. Wright, and J. Diakonikolas. Variance reduction via primal-dual accelerated dual averaging for nonsmooth convex finite-sums. In ICML, 2021.   \nC. Song, C. Y. Lin, S. Wright, and J. Diakonikolas. Coordinate Linear Variance Reduction for Generalized Linear Programming. In NeurIPS, 2022.   \nA. Tsanas and A. Xifara. Accurate Quantitative Estimation of Energy Performance of Residential Buildings Using Statistical Machine Learning Tools. Energy and Buildings, 49, 2012.   \nP. Tu\u00a8fekci. Prediction of Full Load Electrical Power Output of a Base Load Operated Combined Cycle Power Plant using Machine Learning Methods. International Journal of Electrical Power & Energy Systems, 60, 2014.   \nS. Wang, N. Si, J. Blanchet, and Z. Zhou. A Finite Sample Complexity Bound for Distributionally Robust Q-learning. In AISTATS, 2023a.   \nZ. Wang, L. Shen, T. Liu, T. Duan, Y. Zhu, D. Zhan, D. Doermann, and M. Gao. Defending against Data-Free Model Extraction by Distributionally Robust Defensive Training. In NeurIPS, 2023b.   \nO. Wiles, S. Gowal, F. Stimberg, S.-A. Rebuff,i I. Ktena, K. D. Dvijotham, and A. T. Cemgil. A fine-grained analysis on distribution shift. In ICLR, 2022.   \nG. Xie, L. Luo, Y. Lian, and Z. Zhang. Lower Complexity Bounds for Finite-Sum Convex-Concave Minimax Optimization Problems. In ICML, 2020.   \nJ. Yang, S. Zhang, N. Kiyavash, and N. He. A Catalyst Framework for Minimax Optimization. In NeurIPS, 2020.   \nZ. Yang, Y. Guo, P. Xu, A. Liu, and A. Anandkumar. Distributionally Robust Policy Gradient for Offline Contextual Bandits. In AISTATS, 2023.   \nI. Yeh. Analysis of Strength of Concrete Using Design of Experiments and Neural Networks. Journal of Materials in Civil Engineering, 18, 2006.   \nY. Yu, T. Lin, E. V. Mazumdar, and M. Jordan. Fast Distributionally Robust Learning with VarianceReduced Min-Max Optimization. In AISTATS, 2022.   \nZ. Yu, L. Dai, S. Xu, S. Gao, and C. P. Ho. Fast Bellman Updates for Wasserstein Distributionally Robust MDPs. In NeurIPS, 2023.   \nJ. Zhang, M. Hong, and S. Zhang. On lower iteration complexity bounds for the convex concave saddle point problems. Mathematical Programming, 194, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Appx. A contains all notation introduced throughout the paper. Appx. B discusses convergence rate comparisons in detail with contemporary work. The full convergence analysis of DRAGO is given in Appx. C. A description of the algorithm amenable to implementation is given in Appx. D, whereas experimental settings are described in detail within Appx. E. ", "page_idx": 12}, {"type": "text", "text": "Table of Contents ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A Notation 14   \nB Comparisons to Existing Literature 15   \nB.1 Directly Using Gradient Descent 15   \nB.2 Distributionally Robust Optimization (DRO) 15   \nB.3 Primal-Dual Saddle Point Algorithms . . 17   \nC Convergence Analysis of DRAGO 21   \nC.1 Overview 21   \nC.2 Technical Lemmas 23   \nC.3 Proof of Main Result 29   \nC.4 Modification for Unregularized Objectives 40   \nD Implementation Details 44   \nD.1 Algorithm Description . . 44   \nD.2 Solving the Maximization Problem 44   \nE Experimental Details 48   \nE.1 Datasets 48   \nE.2 Hyperparameter Selection 48   \nE.3 Compute Environment . 49   \nE.4 Additional Experiments 49 ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "F NeurIPS Paper Checklist 51 ", "page_idx": 12}, {"type": "table", "img_path": "ujk0XrNTQZ/tmp/5764d8c081b74405426e1c3c11f6dd67e6f6269fcdaba73341ed4a8ddfda664b.jpg", "table_caption": ["Table 2: Notation used throughout the paper. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "B Comparisons to Existing Literature ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this appendix, we compare our work to existing literature along two axes: 1) distributional robust optimization (DRO), and 2) primal-dual algorithms for saddle-point problems. In the first category, we are primarily concerned with questions of practical and statistical interest, such as which uncertainty sets can be used, how the size of the uncertainty set affects the convergence rate, and what assumptions are needed on the distribution of losses. In the second category, we discuss computational complexity under various assumptions such as smoothness and strong convexity of the objective. ", "page_idx": 14}, {"type": "text", "text": "B.1 Directly Using Gradient Descent ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In the penalized case, we add that the objective $w\\mapsto\\operatorname*{max}_{q\\in\\mathcal{Q}}\\mathcal{L}(w,q)$ is $\\begin{array}{r}{(L+\\mu+\\frac{n G^{2}}{\\nu})}\\end{array}$ -smooth when the losses $\\ell_{i}$ are $G$ -Lipschitz continuous and $L$ -smooth. For this reason, we may consider simply applying full-batch gradient descent to this objective, which is included in our comparisons. To see why this smoothness condition holds, define $\\begin{array}{r}{\\check{h(l)}:=\\operatorname*{max}_{q\\in\\mathcal{Q}}\\left\\{q^{\\top}l-\\nu D(q\\|\\mathbf{1}/n)\\right\\}}\\end{array}$ , so that when $q\\mapsto D(q\\|\\mathbf{1}/n)$ is 1-strongly convex with respect to $\\left\\|\\cdot\\right\\|_{2}^{2}$ , it holds that $\\nabla h$ is $(1/\\nu)$ -Lipschitz continuous with respect to $\\left\\|\\cdot\\right\\|_{2}^{2}$ , and that $\\nabla h(l)$ is non-negative and sums to one (i.e. is a probability mass function on $[n]!$ ). Then, by the chain rule, for any $w_{1},w_{2}\\in\\mathcal{W}$ , we have that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\nabla\\ell(w_{1})^{\\top}\\nabla h(\\ell(w_{1}))-\\nabla\\ell(w_{2})^{\\top}\\nabla h(\\ell(w_{2}))\\right\\|_{2}}\\\\ &{\\leq\\left\\|\\nabla\\ell(w_{1})^{\\top}(\\nabla h(\\ell(w_{1}))-\\nabla h(\\ell(w_{2})))\\right\\|_{2}+\\left\\|(\\nabla\\ell(w_{1})-\\nabla\\ell(w_{2}))^{\\top}\\nabla h(\\ell(w_{2}))\\right\\|_{2}]}\\\\ &{\\leq\\frac{n G}{\\nu}\\left\\|\\ell(w_{1})-\\ell(w_{2})\\right\\|_{2}+L\\left\\|w_{1}-w_{2}\\right\\|_{2}}\\\\ &{\\leq\\left(\\frac{n G^{2}}{\\nu}+L\\right)\\left\\|w_{1}-w_{2}\\right\\|_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Thus, when referring to the gradient descent on $\\begin{array}{r}{w\\mapsto\\operatorname*{max}_{q\\in{\\mathcal{Q}}}{\\mathcal{L}}(w,q)=h(\\ell(w))+\\frac{\\mu}{2}\\left\\|w\\right\\|_{2}^{2}}\\end{array}$ , we reference the smoothness constant $\\begin{array}{r}{\\left(L+\\mu+\\frac{n G^{2}}{\\nu}\\right)}\\end{array}$ ", "page_idx": 14}, {"type": "text", "text": "B.2 Distributionally Robust Optimization (DRO) ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Examples of DRO Problems Our problem ", "text_level": 1, "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{w\\in\\mathcal{W}}\\operatorname*{max}_{q\\in\\mathcal{Q}}\\Big[\\mathcal{L}(w,q):=q^{\\top}\\ell(w)-\\nu D(q\\|\\mathbf{1}/n)+\\frac{\\mu}{2}\\left\\|w\\right\\|_{2}^{2}\\Big]\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "accommodates several settings of interest across machine learning. For example, $f$ -DRO with parameter $\\rho$ [Namkoong and Duchi, 2016, Carmon and Hausler, 2022] results by defining the uncertainty set and penalty as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{Q}=\\mathcal{Q}(\\rho):=\\left\\{q:D_{f}(q\\|\\mathbf{1}/n)\\leq\\frac{\\rho}{n}\\right\\}\\,\\mathrm{and}\\;D(q\\|\\mathbf{1}/n)=D_{f}(q\\|\\mathbf{1}/n),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\begin{array}{r}{D_{f}(q\\|p)\\;=\\;\\sum_{i=1}^{n}p_{i}f(q_{i}/p_{i})}\\end{array}$ denotes an $f$ -divergence generated by $f$ (which is always well-defined when $p_{i}={\\bf1}/n)$ . Common examples include the Kullback-Leibler (KL) divergence, generated by $f_{\\mathrm{KL}}(x)=-x\\log x$ and the $\\chi^{2}$ -divergence generated by $f_{\\chi^{2}}(x)=(x-1)^{2}$ . Spectral risk measures [Mehta et al., 2023] are parametrized by an $n$ -length vector $\\sigma=(\\sigma_{1},\\ldots,\\sigma_{n})$ such that $0\\leq\\sigma_{1}\\leq...\\leq\\sigma_{n}$ and $\\textstyle\\sum_{i=1}^{n}\\sigma_{i}=1$ . The penalty in that setting may also be in the form of an $f$ -divergence [Mehta et al., 2024], so that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{Q}=\\mathcal{Q}(\\sigma):=\\mathrm{conv}\\left(\\left\\{\\mathrm{permutations~of~}\\sigma\\right\\}\\right)\\mathrm{~and~}D(q||\\mathbf{1}/n)=D_{f}(q||\\mathbf{1}/n).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where conv $(\\cdot)$ denotes the convex hull. The most notable example of such an uncertainty set is the $\\theta$ -conditional value-at-risk, or CVaR [Rockafellar and Royset, 2013], wherein the largest $\\theta n$ values of $\\sigma$ are set to $1/(n\\theta)$ and the remaining are set to zero (with a fractional component if $\\theta n$ is not an integer). Finally, Wasserstein-DRO [Kuhn et al., 2019, Blanchet et al., 2019, Yu et al., 2022] with parameter $\\delta$ typically sets the uncertainty set to be a $\\delta$ -ball $\\{Q\\,:\\,W_{c}(Q,P_{n})\\,\\leq\\,\\delta\\}$ in Wasserstein distance, where $Q$ is a probability measure, $P_{n}$ is the empirical measure of the training data, and ", "page_idx": 14}, {"type": "text", "text": "$W_{c}$ is the Wasserstein distance associated to cost function $c$ . This differs from $f$ -DRO and spectral risk minimization because in the latter settings, the \u201cshifted\u201d distribution $Q$ is assumed to remain on the same $n$ atoms as before, so that it may simply be specified by a probability vector $q\\,\\in\\,[0,1]^{n}$ (resp. $P_{n}$ by ${\\mathbf{1}}/n$ ). However, as shown by $\\mathrm{Yu}$ et al. [2022], Wasserstein-DRO can be reformulated into a problem of the form (16) if the following conditions are satisfied: 1) the loss is of a generalized linear model $\\ell_{i}(w)=\\Psi(\\left\\langle w,x_{i}\\right\\rangle,y_{i})$ with feature-label pair $\\left({x_{i},y_{i}}\\right)$ and discrepancy function $\\Psi,2,$ ) the cost function $c((x,y),(x^{\\prime},y^{\\prime}))$ is of the form $\\left\\|\\boldsymbol{x}-\\boldsymbol{x}^{\\prime}\\right\\|_{2}+\\beta\\left|\\boldsymbol{y}-\\boldsymbol{y}^{\\prime}\\right|$ for $\\beta~>~0$ , and 3) the function $\\Psi$ is Lipschitz continuous with known constant. ", "page_idx": 15}, {"type": "text", "text": "Comparison of DRO Approaches The performance of classical and recent algorithms designed for the problems above is detailed in Tab. 3. The rightmost column displays the oracle complexity, meaning the number of queries to individual/component loss functions $\\{(\\ell_{i},\\nabla\\ell_{i})\\}$ as a function of the desired suboptimality $\\varepsilon$ . The desiderata in the large-scale setting is to decouple the contribution of the sample size $n$ and the condition number in smooth, strongly convex settings $\\dot{\\mu}\\,>\\,0$ and $L<\\infty)$ or the quantity $1/\\varepsilon$ in non-smooth or non-strongly convex settings. For readability, we encode dependence on $n$ in red and dependence on $1/\\varepsilon$ in blue within Tab. 3. In certain cases, such as in the sub-gradient method, the dependence on $n$ is understood as part of a smoothness constant. Similarly, because $q_{\\mathrm{max}}$ is of order $1/n$ in the best case and 1 in the worst case, we interpret $\\kappa_{\\mathcal{Q}}$ to play the role of a condition number that measures the size of the uncertainty set. That being said, $\\kappa_{\\mathcal{Q}}$ is upper bounded by a constant independent of $n$ in common cases. We collect examples of these results below. ", "page_idx": 15}, {"type": "text", "text": "Proposition 3. In (16) $i f\\,\\mathcal{Q}$ is chosen to be the CVaR uncertainty set with tail probability $\\theta_{i}$ , then $\\kappa_{\\mathcal{Q}}\\stackrel{\\bullet}{\\leq}\\frac{1}{\\theta}$ . If $\\mathcal{Q}$ is chosen to be the $\\chi^{2}$ -DRO uncertainty set with radius $\\rho_{;}$ , then $\\bar{\\kappa_{\\mathcal{Q}}}\\leq\\sqrt{1+\\rho}$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. For the $\\theta$ -CVAR, we have that $q_{i}\\in[0,1/(n\\theta)]$ for all $i\\in[n]$ . Thus, ", "page_idx": 15}, {"type": "equation", "text": "$$\nn q_{i}\\leq{\\frac{1}{\\theta}}{\\mathrm{~for~all~}}i=1,\\ldots,n.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "For the $\\chi^{2}$ -DRO uncertainty set, we have by direct computation that ", "page_idx": 15}, {"type": "equation", "text": "$$\nn\\left\\|q-\\mathbf{1}/n\\right\\|_{2}^{2}\\leq\\frac{\\rho}{n}\\iff\\left\\|q\\right\\|_{2}^{2}\\leq\\frac{1+\\rho}{n^{2}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This implies that for any $i\\in[n]$ , we have that $q_{i}^{2}\\leq(1+\\rho)/n^{2}$ which implies that ", "page_idx": 15}, {"type": "equation", "text": "$$\nn q_{i}\\le\\sqrt{1+\\rho},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "the result as desired. ", "page_idx": 15}, {"type": "text", "text": "The closest comparison to our setting is that of LSVRG [Mehta et al., 2023] and Prospect [Mehta et al., 2024]. Including DRAGO, these methods all achieve linear convergence on (16), under the assumption of strongly convex regularization in the primal objective. However, both LSVRG and Prospect demand a stringent lower bound of $\\Omega(n G^{2}{\\bar{/}}\\mu)$ on the dual regularization parameter $\\nu$ to achieve this rate, which essentially matches ours in this regime (as $\\sqrt{n G^{2}/(\\mu\\nu)}$ reduces to a constant). When this condition does not hold, however, LSVRG does not have a convergence guarantee while Prospect underperforms against the sub-gradient method. Furthermore, while Propsect has a single hyperparameter, LSVRG must search over both a learning rate and an epoch length. DRAGO, on the other hand, is the only method that achieves unconditional linear convergence and fully captures the dependence on the dual regularization parameter $\\nu$ , which is a smoothness measure of the objective $\\operatorname*{max}_{q}\\mathcal{L}(\\cdot,q)$ . ", "page_idx": 15}, {"type": "text", "text": "Moving to methods that are not linearly convergent, Levy et al. [2020] consider a variant of minibatch SGD that solves the maximization problem within the mini-batch itself. In Tab. 3, we include a term $\\operatorname*{min}\\left\\{n,b(\\varepsilon)\\right\\}$ , where $b(\\varepsilon)$ denotes a required batch size. This is because Levy et al. [2020] measures complexities in terms of calls to first-order oracles for a loss summed across an arbitrary batch size. Thus, our comparison relies on multiplying the required batch size with the number of iterations required for a desired suboptimality (see Levy et al. [2020, Theorem 2], for example). In the setting in which there is a fixed training set of size $n$ , we incorporate this requirement on the batch size in the complexity bound, to account for the case in which the theoretically required batch size grows so large that it exceeds the full batch size itself. Indeed, as commented by Carmon and Hausler [2022], when the uncertainty set is large (i.e. $\\theta$ is small for CVaR and $\\rho$ is large for $\\chi^{2}$ -divergence), the method may underperform against the sub-gradient method. This is true of methods such as in Mehta et al. [2024] and ours that depend on $\\kappa_{\\mathcal{Q}}\\leq n$ . This indicates that across DRO settings, increased uncertainty set sizes can bring the performance of incremental methods arbitrarily close to that of the full-batch sub-gradient method. Finally, notice that DRAGO also has a dependence on the batch size $b$ in Tab. 3. While it would appear that $b=1$ would minimize oracle complexity, we describe in the next section how $b$ can be chosen to minimize global complexity, which includes the cost of each oracle call as a function of the primal dimension $d$ . Because of the diversity of settings, a coarser measure such as first-order oracle evaluations may be sufficient to compare methods along the DRO axis; we use the finer-grained global complexity to compare methods along the axis of saddle-point algorithms. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "B.3 Primal-Dual Saddle Point Algorithms ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "For context, both (1) and (2) are coupled min-max problems with primal-dual variables $(w,q)$ . Observe that the coupled term in the objective (depending on both $w$ and $q$ ) is not bilinear in general. Such objectives have received much less attention in the optimization literature on primal-dual methods than their bilinearly-coupled counterparts. For the bilinear setting, methods such as stochastic Chambolle-Pock-style algorithms [He et al., 2020, Song et al., 2021] use stochastic variance-reduced updates, and in particular, employ coordinate-style updates of a single dual variable per iteration. To illustrate the advantage of these updates, notice that the primal and dual dimensions are $d$ and $n$ , respectively. When $n$ is significantly larger than $d$ , and assuming that each $(\\ell_{i},\\nabla\\ell_{i})$ call comes at an $O(d)$ cost, updating the dual variables at $O(n)$ cost every iteration becomes the primary computational bottleneck. Coordinate-style updates can eliminate this dependence, but apply only when the dual feasible set $\\mathcal{Q}$ decomposes as $\\mathcal{Q}_{1}\\times...\\times\\mathcal{Q}_{n}$ , i.e., is separable. ", "page_idx": 16}, {"type": "text", "text": "There is a long line of work on variance-reduced algorithms for general stochastic optimization problems [Johnson and Zhang, 2013, Defazio et al., 2014, Palaniappan and Bach, 2016, Cai et al., 2023] and structured and/or bilinearly coupled min-max problems [Yang et al., 2020, Song et al., 2021, Du et al., 2022, Kovalev et al., 2022, Alacaoglu et al., 2022]. However, few of these results directly apply to (2) due to non-bilinearity and non-separability of the dual feasible set. These issues motivated work on reformulating common DRO problems as bilinearly coupled min-max problems in Song et al. [2022]. However, the guarantees obtained in Song et al. [2022] are of the order $O(\\varepsilon^{-1})$ . ", "page_idx": 16}, {"type": "text", "text": "Another viewpoint is that (2) can be written as a monotone variational inequality (VI) problem for the operator $\\dot{F}(w,q)=(\\nabla_{w}\\ell(w)^{\\top}q+\\mu w,-\\ell(w)+\\nu\\nabla_{q}D(q\\|\\mathbf{1}/n))\\in\\mathbb{R}^{\\dot{d}+n}$ , where we assumed 1-smoothness of $D$ for ease of presentation. Under our assumptions, this operator is $\\operatorname*{min}\\left\\{\\mu,\\nu\\right\\}$ - strongly monotone and max $\\{n G^{2},L\\}$ -Lipschitz continuous, and VI algorithms such as mirrorprox, Popov\u2019s method, and variance-reduced methods like Alacaoglu and Malitsky [2022], Cai et al. [2024] can be used. However, the max-over-min dependence on the individual Lipschitz constants and strong convexity constants is unfavorable compared to the individual condition numbers observed in (4). If $F$ is called directly, the global complexity will be $n^{2}$ because $q$ is $n$ -dimensional along with $n$ in the condition number. A finite sum approach could improve dependence on the number of oracle calls, but the global complexity would still be $n^{2}$ due to the non-separability of the dual feasible set and again the $n$ -dimensional dual variables. ", "page_idx": 16}, {"type": "text", "text": "To summarize the points above and in order to make comparisons among methods for general minmax problems, we first collect aspects of (16) that make it a highly specialized problem in this regard. The major points include: ", "page_idx": 16}, {"type": "text", "text": "1. The objective has a finite-sum structure, in that it can be written as $\\textstyle\\sum_{i=1}^{n}f_{i}(w,q)$ , where $\\begin{array}{r}{f_{i}(w,q):=q_{i}\\ell_{i}(w)-\\nu D(q\\|\\mathbf{1}/n)+\\frac{\\mu}{2}\\left\\|w\\right\\|_{2}^{2}}\\end{array}$ .   \n2. The dimension of the dual variable $q$ is equal to $n$ , the number of functions in the sum (i.e. the sample size), and could be much larger than $d$ .   \n3. The dual regularizer $q\\mapsto D(q\\|\\mathbf{1}/n)$ is not necessarily smooth. This encompasses common statistical divergences such as the Kullback Leibler (KL).   \n4. The dual feasible set $\\mathcal{Q}$ is non-separable, as any feasible dual iterate must sum to 1. ", "page_idx": 16}, {"type": "text", "text": "For discussions in which linear convergence is a pre-requisite, it is typically assumed that $(w,q)\\mapsto$ $\\mathcal{L}(w,q)$ is a smooth map, and finer-grained results depend on the individual Lipschitz continuity parameters of $w\\mapsto\\nabla_{w}\\mathcal{L}(w,q)$ , $q\\mapsto\\nabla_{q}\\mathcal{L}(w,q)$ , $w\\mapsto\\nabla_{q}\\mathcal{L}(w,q)$ , and $q\\mapsto\\nabla_{w}\\mathcal{L}(w,q)$ . To make the regularized DRO setting of (16) comparable to classical and contemporary saddle-point methods, we make the additional assumption in this section that the (rescaled) $\\chi^{\\dot{2}}$ -divergence penalty is used, so that $\\begin{array}{r}{D(q\\|\\mathbf{1}/n)=\\frac{1}{2}\\left\\|q-\\mathbf{1}/n\\right\\|_{2}^{2}}\\end{array}$ and we may compute the smoothness constants as ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla_{w}\\mathcal{L}(w,q)-\\nabla_{w}\\mathcal{L}(w^{\\prime},q)\\|_{2}\\leq(L+\\mu)\\,\\|w-w^{\\prime}\\|_{2}}\\\\ &{\\|\\nabla_{w}\\mathcal{L}(w,q)-\\nabla_{w}\\mathcal{L}(w,q^{\\prime})\\|_{2}=\\left\\|\\nabla\\ell(w)^{\\top}(q-q^{\\prime})\\right\\|_{2}\\leq\\sqrt{n}G\\,\\|q-q^{\\prime}\\|_{2}}\\\\ &{\\|\\nabla_{q}\\mathcal{L}(w,q)-\\nabla_{q}\\mathcal{L}(w,q^{\\prime})\\|_{2}\\leq\\nu\\,\\|q-q^{\\prime}\\|_{2}}\\\\ &{\\|\\nabla_{q}\\mathcal{L}(w,q)-\\nabla_{q}\\mathcal{L}(w^{\\prime},q)\\|_{2}=\\|\\ell(w)-\\ell(w^{\\prime})\\|_{2}\\leq\\sqrt{n}G\\,\\|w-w^{\\prime}\\|_{2}}\\\\ &{\\|\\nabla_{(w,q)}\\mathcal{L}(w,q)-\\nabla_{(w,q)}\\mathcal{L}(w^{\\prime},q^{\\prime})\\|_{2}\\leq\\left((L+\\mu)\\vee\\nu+\\sqrt{n}G\\right)\\!\\sqrt{2}\\,\\|(w,q)-(w^{\\prime},q^{\\prime})\\|_{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "In making this assumption, however, we emphasize that our results do not depend on (17), (18), and (19) being true. We assume here that calls to the oracles $(\\ell_{i},\\nabla_{w}\\ell_{i})$ cost $O(d)$ operations while calls to $\\nabla_{q}\\mathcal{L}$ cost $O(n)$ operations, making the total $O(n+d)$ . This per-iteration complexity is a subtlety of DRO which is essential to recognize when comparing methods. With DRAGO, we also have a batch size parameter $b$ , setting the complexity to $O(n+b d)$ . By tuning $b$ , we may achieve improvements in terms of global complexity over standard primal-dual methods. Results comparing linearly convergent methods in terms of the global complexity of elementary operations are given in Tab. 4. The table contains a \u201chalf-life\u201d column, which is the constant $\\tau$ multiplied with $\\log(1/\\varepsilon)$ when describing the number of iterations needed to reach $\\varepsilon$ -suboptimality as $\\bar{\\tau}\\log(1/\\varepsilon)$ . Before comparisons, observe that the optimal batch size for DRAGO is $b=n/d$ , in the sense that the global number of arithmetic operations is of order ", "page_idx": 17}, {"type": "equation", "text": "$$\nn\\frac{L\\kappa_{\\mathcal{Q}}}{\\mu}+n d\\sqrt{\\frac{n G^{2}}{\\mu\\nu}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This is an improvement over setting $b=1$ , in which case the same number is ", "page_idx": 17}, {"type": "equation", "text": "$$\n(n+d)\\frac{L\\kappa_{\\mathcal{Q}}}{\\mu}+n(n+d)\\sqrt{\\frac{n G^{2}}{\\mu\\nu}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Next, the most comparable and recent setting is that of Li et al. [2023]. Notice that DRAGO is able to improve by a factor of $d$ on the $\\frac{L}{\\mu}$ term, as long as $\\kappa_{\\mathcal{Q}}=O(1)$ . ", "page_idx": 17}, {"type": "text", "text": "While less comparable, we mention known lower bounds for completeness. In terms of lower bounds, since (16) enjoys a particular structure, such as decomposability into so-called marginal terms $\\nu D(q\\|\\mathbf{1}/n)$ and $\\frac{\\bar{\\mu}}{2}\\left|\\right|w\\bar{\\|}_{2}^{2}$ and a coupled term $q^{\\top}\\ell(w)$ , we are not necessarily constrained by the more general lower bounds of Zhang et al. [2022] and Xie et al. [2020]. For example, the proximal incremental first-order (PIFO) model of Xie et al. [2020] assumes that we observe first-order information from a single component in the sum; in DRAGO, using a batch size of $n/d$ is required to achieve the desired improvement. Zhang et al. [2022], on the other hand, do not treat the finite sum class of problems considered here. Furthermore, we still list the per-iteration cost as $O(n+d)$ because a single PIFO call to the dual gradient is of size $n$ . ", "page_idx": 17}, {"type": "table", "img_path": "ujk0XrNTQZ/tmp/9f959ce5dce773e5cab7924e702c6595fda5c1ea0cbb434d418a571305457db3.jpg", "table_caption": [], "table_footnote": [""], "page_idx": 18}, {"type": "table", "img_path": "ujk0XrNTQZ/tmp/6aa0af92dfbb462da8129e0c4d420781225b07435a730a5ef8bc70bb32496bc0.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "Table 4: Complexity of Primal-Dual Saddle Point Methods. Half-life (defined as $\\tau$ such that a linearly convergent method requires ${\\cal O}(\\tau\\log(1/\\varepsilon))$ iterations to achieve $\\varepsilon$ -suboptimality) and periteration cost of linearly convergent optimization algorithms. The global number of arithmetic operations (under the assumption that $(\\ell_{i},\\nabla_{w}\\ell_{i})$ costs $O(d)$ operations and $\\nabla_{q}\\mathcal{L}$ costs $O(n)$ operations) required to achieve a point $(w,q)$ satisfying $\\mathcal{L}(w,q_{\\star})-\\mathcal{L}(w_{\\star},q)\\leq\\varepsilon$ can be computed by multiplying the last two columns. The \u201cAssumptions\u201d column contains changes to the assumptions that each $\\ell_{i}$ is $L$ -smooth and that $D(\\cdot\\|\\mathbf{1}/n)$ is $\\nu$ -smooth. ", "page_idx": 19}, {"type": "text", "text": "C Convergence Analysis of DRAGO ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "This section provides the convergence analysis for DRAGO. We first recall the quantities of interest and provide an alternate description of the algorithm that is useful for understanding the analysis. A high-level overview is given in Appx. C.1, and the remaining subsections comprise steps of the proof. ", "page_idx": 20}, {"type": "text", "text": "C.1 Overview ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "See Tab. 2 for a reference on notation used throughout the proof, which will also be introduced as it appears. Define $q_{0}=\\mathbf{1}/n$ and recall the objective ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\boldsymbol{w},\\boldsymbol{q}):=\\boldsymbol{q}^{\\top}\\ell(\\boldsymbol{w})-\\nu D(\\boldsymbol{q}\\|\\boldsymbol{q}_{0})+\\frac{\\mu}{2}\\left\\|\\boldsymbol{w}\\right\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "By strong convexity of $w\\;\\mapsto\\;\\operatorname*{max}_{q}\\mathcal{L}(w,q)$ with respect to $\\lVert\\cdot\\rVert_{2}$ and strong concavity of $q\\mapsto$ $\\bar{\\operatorname*{min}}_{w}\\,\\mathcal{L}(\\bar{\\boldsymbol{w}},\\boldsymbol{q})$ with respect to $\\lVert\\cdot\\rVert_{2}$ , a primal-dual solution pair $(w^{\\star},q^{\\star})$ is guaranteed to exist and is unique, and thus we define ", "page_idx": 20}, {"type": "equation", "text": "$$\nw_{\\star}=\\underset{w\\in\\mathcal{W}}{\\arg\\operatorname*{min}}\\,\\underset{q\\in\\mathcal{Q}}{\\operatorname*{max}}\\,\\mathcal{L}(w,q)\\mathrm{~and~}q_{\\star}=\\underset{q\\in\\mathcal{Q}}{\\arg\\operatorname*{max}}\\,\\underset{w\\in\\mathcal{W}}{\\operatorname*{min}}\\,\\mathcal{L}(w,q).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "In other words, we have that $\\begin{array}{r}{\\operatorname*{max}_{q\\in\\mathcal{Q}}\\mathcal{L}(w_{\\star},q)=\\mathcal{L}(w_{\\star},q_{\\star})=\\operatorname*{min}_{w\\in\\mathcal{W}}\\mathcal{L}(w,q_{\\star})}\\end{array}$ . We proceed to describe the algorithm, the optimality criterion, and the proof outline. ", "page_idx": 20}, {"type": "text", "text": "Algorithm Description First, we describe two sequences of parameters that are used to weigh various terms in the primal and dual updates. The parameters are set in the analysis, and the version of the algorithm in Appx. D is a description with these values plugged in. However, we keep them as variables in this section to better describe the logic of the proof. ", "page_idx": 20}, {"type": "text", "text": "Specifically, let $(a_{t})_{t\\geq1}$ be a sequence of positive numbers and define $a_{0}\\,=\\,0$ in addition. Denote $\\textstyle A_{t}\\;=\\;\\sum_{s=1}^{T}a_{s}$ . These will become the averaging sequence that will aggregate successive gaps \u03b3t (see (27)) into the return value tT= $\\textstyle\\sum_{t=1}^{T}a_{t}\\gamma_{t}$ , which will be upper bounded by a constant in $T$ (in expectation). We also define anoth er similar sequence $(c_{t})_{t\\geq1}$ , and define $C_{t}=\\dot{A}_{t}-(n/b-1)c_{t}$ for batch size $b$ . We assume for simplicity that $b$ divides $n$ . When all constants are set, the algorithm will reduce to that given in Algorithm 1. We have one hyperparameter $\\alpha>0$ , which may be interpreted as a learning rate. ", "page_idx": 20}, {"type": "text", "text": "Using initial values $w_{0}=0$ and $q_{0}=\\mathbf{1}/n$ , initialize the tables $\\hat{\\ell}_{0}=\\ell(w_{0})\\in\\mathbb{R}^{n}$ , $\\hat{g}_{0}=\\nabla\\ell(w_{0})\\in$ $\\mathbb{R}^{n\\times d}$ , and $\\hat{q}_{0}\\,=\\,q_{0}\\,\\in\\,\\mathbb{R}^{n}$ . In addition, partition the $[n]$ sample indices into $M:=n/b$ blocks of size $b$ , or $B_{1},\\ldots,B_{M}$ with $B_{K}:=((K-1)b+1,\\ldots,\\bar{K}b)$ for $K\\in[M]$ . We can set the averaging sequence according to the following scheme: ", "page_idx": 20}, {"type": "equation", "text": "$$\na_{1}=1,a_{2}=4\\alpha,\\;\\mathrm{and}\\;a_{t}=\\left(1+\\alpha\\right)a_{t-1}\\;\\mathrm{for}\\;t>2.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The initial value $a_{2}\\,=\\,4\\alpha$ is a slight modification for theoretical convenience, and the algorithm operates exactly as in Appx. D in practice. In order to retrieve the Appx. D version, we simply replace the condition above with $a_{t}=\\left(1+\\alpha\\right)a_{t-1}$ for $t\\geq2$ . ", "page_idx": 20}, {"type": "text", "text": "Consider iterate $t\\in\\{1,\\ldots,T\\}$ . We sample a random block $I_{t}$ uniformly from $[M]$ and compute the primal update. ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\delta_{t}^{\\mathrm{P}}:=\\displaystyle\\frac{1}{b}\\sum_{i\\in B_{t}}\\left(q_{t-1,i}\\nabla\\ell_{i}(w_{t-1})-\\hat{q}_{t-2,i}\\hat{g}_{t-2,i}\\right)}\\\\ &{v_{t}^{\\mathrm{P}}:=\\displaystyle\\hat{g}_{t-1}^{\\top}\\hat{q}_{t-1}+\\frac{n a_{t-1}}{a_{t}}\\delta_{t}^{\\mathrm{P}}}\\\\ &{w_{t}:=\\arg\\operatorname*{min}_{u}\\,\\,a_{t}\\left\\langle v_{t}^{\\mathrm{P}},w\\right\\rangle+\\frac{a_{t}\\mu}{2}\\left\\|w\\right\\|_{2}^{2}+\\displaystyle\\frac{C_{t-1}\\mu}{2}\\left\\|w-w_{t-1}\\right\\|_{2}^{2}+\\frac{c_{t-1}\\mu}{2}\\sum_{s=t-n/b}^{t-2}\\left\\|w-w_{s\\vee0}\\right\\|_{2}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We see that $C_{t-1}+c_{t-1}(n/b-1)\\;=\\;A_{t-1}$ , so the inner objective of the update (23) is $A_{t}\\mu\\cdot$ - strongly convex (as the one in (26) is $A_{t}\\nu$ -strongly concave). Note that when $n/b<1$ , we simply treat the method as not including the additional regularization term ct\u221221\u00b5 ts\u2212=2t\u2212 $\\begin{array}{r l}{\\frac{c_{t-1}\\mu}{2}\\sum_{s=t-n/b}^{t-2}\\left\\|w-w_{s\\vee0}\\right\\|_{2}^{2}}\\end{array}$ . Proceeding, we then modify the loss and gradient table. The loss update has to occur between the primal and dual updates to achieve control over the variation in the dual update (see Appx. C.2). Define $K_{t}=t$ mod $M+1$ as the (deterministic) block to be updated, and set ", "page_idx": 21}, {"type": "equation", "text": "$$\n(\\hat{\\ell}_{t,k},\\hat{g}_{t,k})=\\left\\{\\binom{\\ell_{k}(w_{t}),\\nabla\\ell_{k}(w_{t}))}{(\\hat{\\ell}_{t-1,k},\\hat{g}_{t-1,k})}~~~\\mathrm{if}\\ k\\in B_{K_{t}}\\right..\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Define $e_{j}$ to be the $j$ -th standard basis vector. Then, sample a random block $J_{t}$ uniformly from $[M]$ compute ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\delta_{t}^{\\mathrm{D}}:=\\displaystyle\\frac1b\\sum_{j\\in B_{J_{t}}}(\\ell_{j}(w_{t})-\\hat{\\ell}_{t-1,j})e_{j}}\\\\ &{v_{t}^{\\mathrm{D}}:=\\hat{\\ell}_{t}+\\frac{n a_{t-1}}{a_{t}}\\delta_{t}^{\\mathrm{D}}}\\\\ &{\\;\\;q_{t}:=\\arg\\operatorname*{max}_{q\\in\\mathcal{Q}}\\,(v_{t}^{\\mathrm{D}},q)-a_{t}\\nu D(q\\|q_{0})-A_{t-1}\\nu\\Delta_{D}(q,q_{t-1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Notice the change in indices between (21) and (25), which accounts for the update in the loss table that occurs in between. Finally, we must update the remaining table. Set ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\hat{q}_{t,k}=\\left\\{\\!\\!\\begin{array}{l l}{q_{t,k}\\ }&{\\mathrm{if}\\ k\\in B_{K_{t}}}\\\\ {\\hat{q}_{t-1,k}\\ }&{\\mathrm{otherwise}}\\end{array}\\!\\!.\\right.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We define the random variable $\\mathcal{H}_{t}:=\\{(I_{s},J_{s})\\}_{s=1}^{t}$ as the history of blocks selected at all times up to and including $t$ , and define $\\mathbb{E}_{t}\\left[\\cdot\\right]$ to be the conditional expectation operator given $\\mathcal{H}_{t-1}$ . In other words, $\\mathbb{E}_{t}$ integrates the randomness $\\{(I_{s},J_{s})\\}_{s=t}^{T}$ . Accordingly $\\mathbb{E}_{1}\\left[\\cdot\\right]$ is the marginal expectation of the entire random process. We may now describe the optimality criterion. ", "page_idx": 21}, {"type": "text", "text": "Proof Outline Construct the gap function ", "text_level": 1, "page_idx": 21}, {"type": "equation", "text": "$$\n\\gamma_{t}=a_{t}\\left(\\mathcal{L}(w_{t},q_{\\star})-\\mathcal{L}(w_{\\star},q_{t})-\\frac{\\mu}{2}\\left\\|w_{t}-w_{\\star}\\right\\|_{2}^{2}-\\frac{\\nu}{2}\\left\\|q_{t}-q_{\\star}\\right\\|_{2}^{2}\\right)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and aim to bound $\\mathbb{E}_{1}\\left[\\sum_{s=1}^{t}\\gamma_{s}\\right]$ by a constant. Throughout the proof, we will use a free parameter $\\alpha$ , with update rule ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{a_{t}\\leq\\operatorname*{min}\\left\\lbrace\\left(1+\\frac{\\alpha}{4}\\right)a_{t-1},\\alpha A_{t-1},4\\alpha(n/b-1)^{2}C_{t-1}\\right\\rbrace.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Because we will search for $\\alpha$ down to an absolute constant, we will often swap $\\textstyle{\\left(1+{\\frac{\\alpha}{4}}\\right)}$ for $(1+\\alpha)$ for readability. We assume the right-hand side of (28) holds in Step 1 and Step 2. We then select $\\alpha$ to satisfy this condition (and all others) in Step 3 below. The proof occurs in five steps total. ", "page_idx": 21}, {"type": "text", "text": "1. Lower bound the dual suboptimality $a_{t}\\mathcal{L}(w_{\\star},q_{t})$ . ", "page_idx": 21}, {"type": "text", "text": "2. Upper bound the primal suboptimality $a_{t}\\mathcal{L}(w_{t},q_{\\star})$ , and combine both to derive a bound on   \nthe gap function for $t\\geq2$ .   \n3. Derive all conditions on the learning rate constant $\\alpha$ and batch size $b$ .   \n4. Bound $\\gamma_{1}$ and sum $\\gamma_{t}$ over $t$ for a $T$ -step progress bound.   \n5. Bound the remaining non-telescoping terms to complete the analysis. ", "page_idx": 21}, {"type": "text", "text": "We begin with a section of technical lemmas that will not only be useful in various areas of the proof but also capture the main insights that allow the method to achieve the given rate. Given these lemmas, the main proof occurs in Appx. C.3 and otherwise follows standard structure. ", "page_idx": 21}, {"type": "text", "text": "C.2 Technical Lemmas ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "This section contains a number of lemmas that describe common structures in the analysis of quantities in the primal and dual. Lem. 4 bounds cross terms that arise when there are inner products between the primal-dual iterates and their gradient estimates. Lem. 5 and Lem. 6 are respectively the primal and dual noise bounds, constructed to control the variation of the terms $\\delta_{t}^{\\mathrm{P}}$ and $\\delta_{t}^{\\mathrm{D}}$ appearing in (21). Finally, Lem. 10 exploits the cyclic style updates of the $\\hat{q}_{t}$ table to bound the term $\\left|\\left|\\hat{q}_{t-1}-q_{t-2}\\right|\\right|_{2}^{2}$ which is used in the primal noise bound. ", "page_idx": 22}, {"type": "text", "text": "Cross Term Bound Both of the estimates of the gradient of the coupled term $q^{\\top}\\ell(w)$ with respect to the primal and dual variables share a similar structure (see (22) and compare to (29)). They are designed to achieve a particular form of telescoping, with a remaining squared term that can be controlled by case-specific techniques. This can be observed within Lem. 4. In the sequel, we will refer to a sequence of random vectors $(u_{t})_{t\\geq1}$ as adapted to $\\mathcal{H}_{t}$ , where $\\mathcal{H}_{t}\\,=\\,\\{(I_{s},\\bar{J}_{s})\\}_{s=1}^{t}$ is the history of random blocks. This simply means that $u_{t}$ , when conditioned on $\\mathcal{H}_{t-1}$ , is only a function of the current random block $\\left(I_{t},J_{t}\\right)$ . Similarly, conditioned on $\\mathcal{H}_{t-1}$ , we have that $u_{t-1}$ is not random. In the language of probability theory, $\\{\\sigma(\\mathcal{H}_{t})\\}_{t\\geq1}$ forms a filtration and $u_{t}$ is $\\sigma(\\mathcal{H}_{t})$ - measurable, but this terminology is not necessary for understanding the results. ", "page_idx": 22}, {"type": "text", "text": "Lemma 4 (Cross Term Bound). $L e t\\left(x_{t}\\right)_{t\\geq1},\\,\\left(y_{t}\\right)_{t\\geq1}$ , and $(\\hat{y}_{t})_{t\\geq1}$ denote random sequences of $\\mathbb{R}^{m}$ - valued vectors, and let $x_{\\star}\\in\\mathbb{R}^{m}$ be a vector. Denote by $I_{t}$ be the index of a uniform random block $[M]$ . ", "page_idx": 22}, {"type": "equation", "text": "$$\nv_{t}:=\\hat{y}_{t-1}+\\frac{n a_{t-1}}{a_{t}}\\frac{1}{b}\\sum_{i\\in B_{I_{t}}}\\left(y_{t-1,i}-\\hat{y}_{t-2,i}\\right)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Finally, let $(x_{t},y_{t},\\hat{y}_{t})_{t\\geq1}$ adapted to $\\mathcal{H}_{t}$ (as defined above). Then, for any positive constant $\\gamma>0$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{a_{t}\\mathbb{E}_{t}\\big[\\langle y_{t}-v_{t},x_{\\star}-x_{t}\\rangle\\big]\\leq a_{t}\\mathbb{E}_{t}\\left[\\langle y_{t}-\\hat{y}_{t-1},x_{\\star}-x_{t}\\rangle\\right]-a_{t-1}\\left\\langle y_{t-1}-\\hat{y}_{t-2},x_{\\star}-x_{t-1}\\right\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\left.\\frac{n^{2}a_{t-1}^{2}}{2\\gamma}\\mathbb{E}_{t}\\left\\|\\frac{1}{b}\\sum_{i\\in B_{I_{t}}}\\left(y_{t-1,i}-\\hat{y}_{t-2,i}\\right)\\right\\|_{2}^{2}+\\frac{\\gamma}{2}\\mathbb{E}_{t}\\left\\|x_{t}-x_{t-1}\\right\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. By plugging in the value of $v_{t}$ and using $x_{\\star}-x_{t}=x_{\\star}-x_{t-1}+x_{t-1}-x_{t}$ , we have that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathrm{\\Sigma}_{\\mathcal{U}}\\mathbb{E}_{t}\\big[\\langle y_{t}-v_{t},x_{\\star}-x_{t}\\rangle\\big]=a_{t}\\mathbb{E}_{t}\\big[\\langle y_{t}-\\hat{y}_{t-1},x_{\\star}-x_{t}\\rangle\\big]-n a_{t-1}\\mathbb{E}_{t}\\left[\\bigg\\langle\\frac{1}{b}\\sum_{i\\in B_{t}}(y_{t-1}-\\hat{y}_{t-2}),x_{\\star}-x_{t}\\big.\\right.}\\\\ &{\\quad\\null\\quad\\quad\\quad=a_{t}\\mathbb{E}_{t}\\big[\\langle y_{t}-\\hat{y}_{t-1},x_{\\star}-x_{t}\\rangle\\big]-a_{t-1}\\,\\langle y_{t-1}-\\hat{y}_{t-2},x_{\\star}-x_{t-1}\\rangle}\\\\ &{\\quad\\null\\quad\\quad+{n a}_{t-1}\\mathbb{E}_{t}\\left[\\bigg\\langle\\frac{1}{b}\\sum_{i\\in B_{t}}(y_{t-1}-\\hat{y}_{t-2}),x_{t}-x_{t-1}\\bigg\\rangle\\right]}\\\\ &{\\leq a_{t}\\mathbb{E}_{t}\\left[\\langle y_{t}-\\hat{y}_{t-1},x_{\\star}-x_{t}\\rangle\\right]-a_{t-1}\\,\\langle y_{t-1}-\\hat{y}_{t-2},x_{\\star}-x_{t-1}\\rangle}\\\\ &{\\quad\\null\\quad+{\\frac{n^{2}a_{t-1}^{2}}{2\\gamma}}\\mathbb{E}_{t}\\left\\|\\frac{1}{b}\\sum_{i\\in B_{t}}\\big(y_{t-1,i}-\\hat{y}_{t-2,i}\\big)\\right\\|_{2}^{2}+\\frac{\\gamma}{2}\\mathbb{E}_{t}\\left\\|x_{t}-x_{t-1}\\right\\|_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the final step follows from Young\u2019s inequality with parameter $\\gamma$ . ", "page_idx": 22}, {"type": "text", "text": "In the primal case, we have that $v_{t}=v_{t}^{\\mathrm{P}}$ , $y_{t}=\\nabla\\ell(w_{t})^{\\top}q_{t}$ , $\\hat{y}_{t}=\\hat{g}_{t}^{\\top}\\hat{q}_{t}$ , and $x_{t}=w_{t}$ . In the dual case, we have that $v_{t}=v_{t}^{\\mathrm{D}}$ , $y_{t}=\\ell(w_{t})$ , $\\hat{y}_{t}=\\hat{\\ell}_{t+1}$ , and $x_{t}=q_{t}$ . The next few lemmas control the third term appearing in Lem. 4 for the specific case of the primal and dual sequences. ", "page_idx": 22}, {"type": "text", "text": "Noise Term Bounds Next, we proceed to control the $\\delta_{t}^{\\mathrm{P}}$ and $\\delta_{t}^{\\mathrm{D}}$ by way of Lem. 5 and Lem. 6. As discussed in Sec. 3, a key step in the convergence proof is establishing control over these terms. Define $\\pi(t,i)$ to satisfy $\\hat{q}_{t,i}\\,=\\,q_{\\pi(t,i),i}$ and $\\hat{g}_{t,i}\\,=\\,\\nabla\\ell_{i}\\bigl(w_{\\pi(t,i)}\\bigr)$ , that is, the time index of the last update of table element $i$ on or before time $t$ . This notation is used to write the table values such as $\\hat{q}_{t}$ in terms of past values of the iterates (e.g., $q_{t}$ ). ", "page_idx": 22}, {"type": "text", "text": "Lemma 5 (Primal Noise Bound). When $t\\geq2$ , we have that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{t}\\left\\|\\delta_{t}^{\\mathrm{P}}\\right\\|_{2}^{2}\\leq\\displaystyle\\frac{3q_{\\mathrm{max}}}{n}\\sum_{i=1}^{n}q_{t-1,i}\\left\\|\\nabla\\ell_{i}(w_{t-1})-\\nabla\\ell_{i}(w^{\\star})\\right\\|_{2}^{2}}\\\\ &{\\qquad\\qquad+\\displaystyle\\frac{3q_{\\mathrm{max}}}{n}\\sum_{i=1}^{n}q_{\\pi(t-2,i),i}\\left\\|\\nabla\\ell_{i}(w_{\\pi(t-2,i)})-\\nabla\\ell_{i}(w^{\\star})\\right\\|_{2}^{2}}\\\\ &{\\qquad\\qquad+\\displaystyle\\frac{3G^{2}}{n}\\left\\|q_{t-1}-\\hat{q}_{t-2}\\right\\|_{2}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. By definition, we have that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathbb{E}_{t}\\left\\|\\delta_{t}^{\\mathrm{P}}\\right\\|_{2}^{2}=\\mathbb{E}_{t}\\left[\\left\\|\\frac{1}{b}\\sum_{i\\in B_{I_{t}}}\\nabla\\ell_{i}(w_{t-1})q_{t-1,i}-\\hat{g}_{t-2,i}\\hat{q}_{t-2,i}\\right\\|_{2}^{2}\\right]}\\\\ {\\quad\\quad\\quad=\\frac{1}{b^{2}}\\mathbb{E}_{t}\\left[\\left\\|\\sum_{i\\in B_{I_{t}}}\\nabla\\ell_{i}(w_{t-1})q_{t-1,i}-\\hat{g}_{t-2,i}\\hat{q}_{t-2,i}\\right\\|_{2}^{2}\\right]}\\\\ {\\quad\\quad\\quad\\leq\\frac{1}{b}\\mathbb{E}_{t}\\left[\\sum_{i\\in B_{I_{t}}}\\|\\nabla\\ell_{i}(w_{t-1})q_{t-1,i}-\\hat{g}_{t-2,i}\\hat{q}_{t-2,i}\\|_{2}^{2}\\right]}\\\\ {\\quad\\quad\\quad=\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\|\\nabla\\ell_{i}(w_{t-1})q_{t-1,i}-\\hat{g}_{t-2,i}\\hat{q}_{t-2,i}\\|_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where we use that $I_{t}$ is drawn uniformly over $n/b$ . Continuing again with the term above, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\|\\nabla\\ell_{i}(w_{t-1})q_{t-1,i}-\\hat{g}_{t-2,i}\\hat{q}_{t-2,i}\\|_{2}^{2}}\\\\ &{=\\displaystyle\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\|(\\nabla\\ell_{i}(w_{t-1})-\\nabla\\ell_{i}(w_{*}))q_{t-1,i}-(\\hat{g}_{t-2,i}-\\nabla\\ell_{i}(w_{*}))\\hat{q}_{t-2,i}+(q_{t-1,i}-\\hat{q}_{t-2,i})\\nabla\\ell_{i}(w_{*})}\\\\ &{\\le\\displaystyle\\frac{3}{n}\\displaystyle\\sum_{i=1}^{n}\\Big(q_{t-1,i}^{-}\\|\\nabla\\ell_{i}(w_{t-1})-\\nabla\\ell_{i}(w_{*})\\|_{2}^{2}+\\hat{q}_{t-2,i}^{2}\\|\\hat{g}_{t-2,i}-\\nabla\\ell_{i}(w_{*})\\|_{2}^{2}}\\\\ &{\\quad+(q_{t-1,i}-\\hat{q}_{t-2,i})^{2}\\|\\nabla\\ell_{i}(w_{*})\\|_{2}^{2}\\Big)}\\\\ &{\\le\\displaystyle\\frac{3}{n}\\displaystyle\\sum_{i=1}^{n}\\Big(q_{m\\setminus\\Omega}q_{t-1,i}\\Big\\|\\nabla\\ell_{i}(w_{t-1})-\\nabla\\ell_{i}(w_{*})\\Big\\|_{2}^{2}+q_{m\\setminus\\Omega_{i}}\\hat{q}_{t-2,i}\\ \\|\\hat{g}_{t-2,i}-\\nabla\\ell_{i}(w_{*})\\|_{2}^{2}}\\\\ &{\\quad+(q_{t-1,i}-\\hat{q}_{t-2,i})^{2}G^{2}\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where we use that $\\|\\nabla\\ell_{i}(w_{\\star})\\|_{2}^{2}\\leq G^{2}$ because every $\\ell_{i}$ is $G$ -Lipschitz with respect to $\\left\\|\\cdot\\right\\|_{2}^{2}$ , and that $q_{i}\\leq q_{\\mathrm{max}}\\,=\\mathrm{max}_{q\\in\\mathcal{Q}}\\left\\|q\\right\\|_{\\infty}.$ . This completes the proof. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "The corresponding dual noise bound in Lem. 6 follows similarly, using cyclic updates in the loss table. ", "page_idx": 23}, {"type": "text", "text": "Lemma 6 (Dual Noise Bound). For $t\\geq2$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}_{t}\\left\\|\\delta_{t}^{\\mathrm{D}}\\right\\|_{2}^{2}\\leq\\frac{G^{2}}{n}\\sum_{\\tau=t-n/b}^{t-2}\\left\\|w_{t-1}-w_{\\tau\\vee0}\\right\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. Then, we may exploit the coordinate structure of the noise term to write ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{\\tau}\\big\\lVert\\bar{\\theta}_{t}^{(\\theta)}\\big\\rVert_{2}^{2}=\\mathbb{E}_{\\tau}\\Bigg\\lVert\\frac{1}{b_{t}^{2}\\theta}\\sum_{i=1,\\ t_{i}}^{(j,(\\varepsilon),\\epsilon_{i}-1)-\\bar{\\theta}_{i-1,i}}\\bar{\\theta}_{i,t_{i}}^{0}\\Bigg\\rVert_{2}^{2}}&{}\\\\ &{\\mathrm{~-~}\\frac{1}{b_{t}^{2}}\\Bigg(\\mathbb{E}_{\\tau}\\sum_{i=1,\\ t_{i}}^{\\tau}\\Big\\lVert\\bar{\\theta}_{i,t_{i}}(w_{t-1})-\\bar{\\theta}_{i-1,i-\\cdot}\\bar{\\theta}_{i-1,i}\\Big)\\bar{\\theta}_{i,t_{i}}^{0}\\Bigg\\rVert_{2}^{2}}\\\\ &{\\quad+\\mathbb{E}_{\\tau}\\Bigg[\\sum_{i,t_{i}}^{\\tau}\\big(\\theta(w_{t-1})-\\bar{\\theta}_{i-1,i}\\big)\\big(\\bar{\\theta}_{i,t_{i}}(w_{t-1})-\\bar{\\theta}_{i-1,i-\\cdot}\\bar{\\theta}_{i,t_{i}}\\big)\\Big]\\Bigg)}\\\\ &{\\quad-\\frac{1}{b_{t}^{2}}\\mathbb{E}_{\\tau}\\Bigg[\\bar{\\theta}_{i,t_{i}}\\big(\\theta(w_{t-1})-\\bar{\\theta}_{i-1,i-\\cdot}\\bar{\\theta}_{i,t_{i}}\\big)\\bar{\\theta}_{i,t_{i}}^{0}\\Bigg]}\\\\ &{\\quad-\\frac{1}{b_{t}^{2}}\\mathbb{E}_{\\tau}\\Bigg[\\bar{\\theta}_{i,t_{i}}\\big(\\theta(w_{t-1})-\\bar{\\theta}_{i-1,i}\\big)\\bar{\\theta}_{i,t_{i}}^{0}\\Bigg]}\\\\ &{\\quad+\\frac{1}{b_{t}^{2}}\\mathbb{E}_{\\tau}\\Bigg[\\bar{\\theta}_{i,t_{i}}\\big(w_{t-1}\\big)-\\bar{\\theta}_{i-1,i}\\big]^{2}}\\\\ &{\\quad\\le\\frac{1}{b_{t}^{2}}\\frac{1}{b_{t}^{2}}\\Bigg[\\bar{\\theta}_{i,t_{i}}\\big(w_{t-1}\\big)-\\bar{\\theta}_{i-1,i}\\big]^{2}}\\\\\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The red term is zero because $j\\neq k$ . The sum in the last line has $n/b-1$ terms because our order of updates forces one of the blocks of the $\\hat{\\ell}_{t-1}$ vector to have values equal to $w_{t-1}$ before defining $\\delta_{t}^{\\mathrm{D}}$ . \u53e3 ", "page_idx": 24}, {"type": "text", "text": "Controlling the Recency of the Loss Table In this section, we bound the $\\left|\\left|q_{t-1}-\\hat{q}_{t-2}\\right|\\right|_{2}^{2}$ term appearing in the primal noise bound Lem. 5. Controlling this term is essential to achieving the correct rate, as we comment toward the end of this section. ", "page_idx": 24}, {"type": "text", "text": "Recall that the $[n]$ indices are partitioned into blocks $(B_{1},\\hdots,B_{M})$ for $M\\,:=\\,n/b$ , where $b$ is assumed to divide $n$ . For any $t\\geq1$ , we first decompose ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}(q_{t,i}-\\hat{q}_{t-1,i})^{2}=\\sum_{K=1}^{M}\\sum_{i\\in B_{K}}(q_{t,i}-\\hat{q}_{t-1,i})^{2},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and analyze block-by-block. Our goal is to be able to count this quantity in terms of $\\left|\\left|q_{t}-q_{t-1}\\right|\\right|_{2}^{2}$ terms. The main result is given in Lem. 10, which is built up in the following lemmas. Consider a block index $K\\in[M]$ . Define the number $t_{K}=M\\left\\lfloor(t-\\bar{K})/M\\right\\rfloor+K$ when $t-1\\geq K$ and and $t_{K}=0$ otherwise. ", "page_idx": 24}, {"type": "text", "text": "Lemma 7. It holds that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{i\\in B_{K}}(q_{t,i}-\\hat{q}_{t-1,i})^{2}\\leq\\sum_{i\\in B_{K}}(t-t_{K})\\sum_{s=t_{K}+1}^{t}(q_{s,i}-q_{s-1,i})^{2}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. We define $t_{K}$ to be the earliest time index $\\tau$ on or before $t-1$ when block $K$ of $q_{\\tau}$ was used to update ${\\hat{q}}_{\\tau}$ . When $t-1<K$ , then $t_{K}\\,=\\,0$ . When $t-1\\geq K$ , we can compute this number ", "page_idx": 24}, {"type": "text", "text": "$t_{K}=M\\left\\lfloor[(t-1)-(K-1)]/M\\right\\rfloor+K=M\\left\\lfloor(t-K)/M\\right\\rfloor+K.$ . Then, write ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{i\\in B_{K}}(q_{t,i}-\\hat{q}_{t-1,i})^{2}=\\displaystyle\\sum_{i\\in B_{K}}(q_{t,i}-q_{t_{K},i})^{2}}&{}\\\\ {\\displaystyle=\\sum_{i\\in B_{K}}\\left(\\sum_{s=t_{K}+1}^{t}q_{s,i}-q_{s-1,i}\\right)^{2}}&{}\\\\ &{\\leq\\displaystyle\\sum_{i\\in B_{K}}(t-t_{K})\\sum_{s=t_{K}+1}^{t}(q_{s,i}-q_{s-1,i})^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the last line follows by Young\u2019s inequality. ", "page_idx": 25}, {"type": "text", "text": "While we will not be able to cancel these terms on every iterate, we will be able to when aggregating over time and then redistributing them. Recall $(a_{t})_{t\\geq1}$ as described in Appx. C.1. Indeed, by summing across iterations, we see that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}a_{t}\\sum_{i=1}^{n}(q_{t,i}-\\hat{q}_{t-1,i})^{2}\\leq\\sum_{t=1}^{T}a_{t}\\sum_{K=1}^{M}\\sum_{i\\in B_{K}}(t-t_{K})\\sum_{s=t_{K}+1}^{t}(q_{s,i}-q_{s-1,i})^{2}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We can start by swapping the first two sums and only considering the values of $t$ that are greater than or equal to $K$ . ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{c l}{\\displaystyle\\sum_{\\succ=1}^{T}a_{t}\\displaystyle\\sum_{K=1}^{M}\\sum_{i\\in B_{K}}(t-t_{K})\\displaystyle\\sum_{s=t_{K}+1}^{t}\\left(q_{s,i}-q_{s-1,i}\\right)^{2}}\\\\ {\\displaystyle=\\sum_{K=1}^{M}\\sum_{t=1}^{K-1}a_{t}\\displaystyle\\sum_{i\\in B_{K}}(t-t_{K})\\displaystyle\\sum_{s=t_{K}+1}^{t}\\left(q_{s,i}-q_{s-1,i}\\right)^{2}+\\displaystyle\\sum_{K=1}^{M}\\sum_{t=K}^{T}a_{t}\\displaystyle\\sum_{i\\in B_{K}}(t-t_{K})\\displaystyle\\sum_{s=t_{K}+1}^{t}(q_{s,i}-q_{s-1,i})}\\\\ {\\displaystyle=\\sum_{K=1}^{M}\\sum_{t=1}^{K-1}a_{t}\\displaystyle\\sum_{i\\in B_{K}}t\\displaystyle\\sum_{s=1}^{t}(q_{s,i}-q_{s-1,i})^{2}+\\displaystyle\\sum_{K=1}^{M}\\sum_{t=K}^{T}a_{t}\\displaystyle\\sum_{i\\in B_{K}}(t-t_{K})\\displaystyle\\sum_{s=t_{K}+1}^{t}(q_{s,i}-q_{s-1,i})^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where we use in the last line that $t_{K}=0$ when $t<K$ . We handle the terms $S_{0}$ and $S_{1}$ separately. In either case, we have to match the sums over $K$ and over $i$ in order to create complete vectors, as opposed to differences between coordinates. We also maintain the update rules of the sequence $(a_{t})_{t\\geq1}^{-}$ that will be used in the proof. ", "page_idx": 25}, {"type": "text", "text": "Lemma 8. Assume that $\\alpha\\leq\\frac{1}{M}$ and $a_{t}\\leq(1+\\alpha)a_{t-1}$ . It holds that $S_{0}$ as defined in (31) satisfies ", "page_idx": 25}, {"type": "equation", "text": "$$\nS_{0}\\leq\\frac{e M(M-1)}{2}\\sum_{t=1}^{M-1}a_{t}\\left\\Vert q_{t}-q_{t-1}\\right\\Vert_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. Write ", "page_idx": 26}, {"type": "text", "text": "$\\begin{array}{r l}{{\\Phi}_{i}=\\frac{{\\cal N}}{2}\\sum_{k=1}^{n}{\\sum_{k=1}^{n}{\\sum_{k=1}^{n}{\\sum_{k=1}^{n}{\\alpha_{i}}{\\alpha_{k}\\cdot q_{i}\\cdot q_{k-1-k-1}}^{2}}}}}\\\\ {~~}\\\\ {~~}\\\\ {~~-\\sum_{k=1}^{n}{\\alpha_{i}\\sum_{k=1}^{n}{\\sum_{l=1}^{n}{\\alpha_{l}\\left(\\alpha_{l}-\\alpha_{l-1-k}\\right)^{2}}}}}\\\\ {~~}\\\\ {~~}\\\\ {~~-\\sum_{k=1}^{n}{\\mu_{k}\\sum_{i=1}^{n}{\\sum_{k=1}^{n}{\\sum_{p=0}^{n-1}{\\alpha_{k}\\left(\\alpha_{k}-\\alpha_{l-1-k}\\right)^{p}}}}}}\\\\ {~~}\\\\ {~~}\\\\ {~~~-\\sum_{k=1}^{n}{\\sum_{i=1}^{n}{\\sum_{p=0}^{n-1}{\\alpha_{p}\\left(\\alpha_{k}-\\alpha_{l-k-1}\\right)^{p}}}}}\\\\ {~~}\\\\ {~~~}\\\\ {~~~-\\sum_{k=1}^{n}{\\mu_{k}\\sum_{i=1}^{n}{\\sum_{p=0}^{n-1}{\\alpha_{p}\\left(\\alpha_{k}-\\alpha_{l-k-1}\\right)^{2p}}}}}\\\\ {~~}\\\\ {~~~}\\\\ {~~~-\\sum_{k=1}^{n}{\\mu_{k}\\sum_{i=1}^{n}{\\alpha_{i}\\cdot q_{k-1-k}^{2}}}}\\\\ {~~}\\\\ {~~}\\\\ {~~~-\\sum_{k=1}^{n}{\\left(\\sum_{p=0}^{n-1}{\\alpha_{p}\\left(1-\\alpha_{p}\\right)}\\right)\\left\\{\\alpha_{k}-\\alpha_{l-k-1}\\right\\}^{2}}}\\\\ {~~}\\\\ {~~~}\\\\ {~~~~\\leq\\sum_{k=1}^{n}{\\left(\\sum_{p=0}^{n-1}{\\alpha_{p}\\left(1+\\alpha_{p}\\right)^{p}}\\right)\\left\\{\\alpha_{k}-\\alpha_{l-1}\\right\\}^{2}}}\\\\ {~~}\\\\ {~~~}\\\\ {~~~~\\leq\\sum_{k=1}^{n}{\\alpha_{i}\\cdot q_{k}\\left(1+\\alpha_{p}\\right)^ $ by definition swap sums over $K$ and $t$ move sum over $s$ $\\begin{array}{r}{\\displaystyle\\sum_{K=t+1}^{M}(\\cdot)\\le\\displaystyle\\sum_{K=1}^{M}(\\cdot)}\\\\ {\\displaystyle\\sum_{K=1}^{M}\\sum_{i\\in B_{K}}(\\cdot)=\\displaystyle\\sum_{i=1}^{n}(\\cdot)}\\end{array}$ swap sums over $s$ and $t$ $a_{t}\\leq(1+\\alpha)a_{t-1}$ $t-s\\leq M$ \u2264 $\\begin{array}{r}{\\alpha\\leq\\displaystyle\\frac{1}{M}\\implies(1+\\alpha)^{M}\\leq e}\\\\ {\\displaystyle\\sum_{t=s}^{M-1}(\\cdot)\\leq\\sum_{t=1}^{M-1}(\\cdot)}\\end{array}$ s=1 M\u22121 M < eas \u2225qs\u2212qs\u22121\u222522 s=1 M\u22121 eM(M 1) as\u2225qs\u2212qs\u22121\u222522 , 2 s=1 ", "page_idx": 26}, {"type": "text", "text": "the result as desired. ", "page_idx": 26}, {"type": "text", "text": "Thus, $S_{0}$ contributes about $M^{3}$ of such terms over the entire trajectory, which can be viewed as an initialization cost. Next, we control $S_{1}$ . ", "page_idx": 26}, {"type": "text", "text": "Lemma 9. Assume that $\\alpha\\leq\\frac{1}{M}$ and $a_{t}\\leq(1+\\alpha)a_{t-1}$ . It holds that $S_{1}$ as defined in (31) satisfies ", "page_idx": 26}, {"type": "equation", "text": "$$\nS_{1}\\leq2e^{2}M^{2}\\sum_{t=1}^{T}a_{t}\\left\\Vert q_{t}-q_{t-1}\\right\\Vert_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. We can reparametrize $t$ in terms of $r=t-K$ , which will help in reasoning with $t_{K}$ . Define $r_{K}=M\\left\\lfloor r/M\\right\\rfloor\\mp K$ , and let ", "page_idx": 27}, {"type": "text", "text": "$\\begin{array}{r l r}{S_{1}=}&{\\frac{\\mathcal{C}_{2}}{K_{1}\\sum_{i=0}^{I-1}\\omega_{i}}\\mathrm{Le}^{-\\mathrm{i}k\\cdot\\Gamma}\\left(t_{1}+K_{2}\\right)\\frac{\\sqrt{\\omega_{1}}}{\\sin\\alpha_{i}}\\left(\\mathrm{for}_{i}-\\mathrm{for}_{-1}\\right)^{2}}&{\\mathrm{by~ofoltinion~out~\\alpha~}t={t}-k\\cdot\\Gamma}\\\\ &{\\quad\\sin\\alpha_{i}\\cos\\beta_{i}}&{\\sin\\alpha_{i}\\cos\\gamma_{i}}&{t_{1}^{\\prime}\\sin\\alpha_{i}}\\\\ &{\\leq M\\frac{\\sqrt{\\omega_{1}}}{K_{2}}\\sum_{i,j\\in\\neq1,i\\neq j}^{I}\\mathrm{e}^{-\\mathrm{i}k\\cdot\\Gamma}\\sum_{s_{i}=0}^{I-1}\\mathrm{(for-e_{i})}\\mathrm{Le}^{\\mathrm{i}k\\cdot\\Gamma}\\left(t_{1}-\\mathrm{d}_{1}\\right)^{2}}&{\\mathrm{(f}-{M})\\sqrt{M})\\leq k\\cdot\\Gamma}\\\\ &{\\quad\\sin\\alpha_{i}\\cos\\beta_{i}}&{\\sin\\alpha_{j}\\sin\\gamma_{i}}&{\\sin\\alpha_{i}\\cos\\gamma_{i}}\\\\ &{\\leq M\\frac{\\sqrt{M}}{K_{1}}\\sum_{i=0}^{I-1}\\mathrm{Le}^{\\mathrm{i}k\\cdot\\Gamma}\\left(1+\\alpha_{1}\\omega_{1}\\cos\\gamma_{i}\\right)\\frac{\\sqrt{M}}{K_{2}}\\left(\\mathrm{for-e_{i}}_{-1}\\right)^{2}}&{\\mathrm{c}_{0}<1+\\alpha_{1}\\omega_{1}-\\mathrm{d}_{1}}\\\\ &{\\quad\\sin\\alpha_{i}\\cos\\beta_{i}}&{\\sin\\alpha_{j}\\sin\\gamma_{i}}&{\\sin\\alpha_{i}\\cos\\gamma_{i}}\\\\ &{\\leq M\\frac{\\sqrt{M}}{K_{2}}\\sum_{i,j\\in\\neq1,i\\neq j}^{I}\\mathrm{e}^{-\\mathrm{i}k\\cdot\\Gamma}\\frac{\\sqrt{M}}{K_{3}}\\left(\\mathrm{for-e_{i}}_{-1}\\right)^{2}}&{K_{3}\\leq M\\mathrm{add}_{0}\\leq\\frac{M}{\\Gamma}}\\\\ &{\\quad\\sin\\alpha_{i}\\cos\\beta_{i}}&{\\cos\\gamma_{i}}&{\\sin\\alpha_{i}\\cos\\gamma_{i}}\\\\ &{=M\\frac$ $\\begin{array}{r l}&{\\mathbb{E}\\left(\\frac{\\displaystyle\\sum_{j=1}^{N}\\sum_{i=1}^{N}\\sum_{j=1\\neq i,j=1\\neq j}^{N}\\sum_{l=1\\neq i,j=1\\neq i,j\\neq i}^{N}\\left(\\sum_{l=k}^{m-1}\\sum_{i=1}^{N}\\sum_{l=k}^{i-1}(\\alpha_{l}-\\alpha_{l+k-1,j})^{l}\\right)}\\\\ &{=-i\\alpha\\sum_{j=1}^{N}\\sum_{i=1}^{N}\\sum_{l=N}^{N}\\sum_{l=N}^{N}\\sum_{l=N}^{N}\\sum_{i,j=1\\neq i,j=1\\neq j}^{N}}\\\\ &{\\le\\alpha\\sum_{j=1}^{N}\\sum_{i=1}^{N}\\sum_{l=N}^{N}\\sum_{l=N}^{N-1}\\sum_{l=N}^{N-1}(\\alpha_{l+k-1,j}+\\alpha_{l+k-1,j})^{l}}\\\\ &{\\le\\alpha\\sum_{j=1}^{N}\\sum_{i=1}^{N}\\sum_{l=N}^{N}\\sum_{l=N}^{N-1}\\sum_{l=N}^{N-1}\\sum_{l=N}^{N-1}(\\alpha_{l+k-1,j}+\\alpha_{l+k-1,j})^{l}}\\\\ &{\\le\\alpha\\sum_{j=1}^{N}\\sum_{i=1}^{N}\\sum_{l=N}^{N}\\sum_{l=N-1}^{N}\\sum_{l=N}^{N-1}\\sum_{l=N}^{N}\\bigg(\\alpha_{l+k-1,j}+\\alpha_{l+k-1,j}\\bigg)}\\\\ &{\\le\\alpha\\sum_{j=1}^{N}\\sum_{i=1}^{N}\\sum_{l=N}^{N}\\sum_{l=N}^{N}\\sum_{l=N}^{N}\\sum_{l=N}^{N}\\bigg(\\alpha_{l+k-1,j}+\\alpha_{l+k-1,j}\\bigg)}\\\\ &{\\le\\alpha\\sum_{j=1}^{N}\\sum_{i=1}^{N}\\alpha_{l+k-1,j}+\\alpha_{l+k-1,j}\\ln\\sum_{l=N}^{N}}\\\\ &{\\le\\alpha\\sum_{j=1}^{N}\\sum_{i=1}^{N}\\sum_{l=N}^{N}\\sum_{l=N}^{N-1}\\sum_{l=N}^{N-1}\\bigg(\\alpha_{l+k-1,j}+\\alpha_{l+k-1,j}\\bigg)^{l}}\\\\ &{\\le\\alpha\\sum_{j=1}^{N}$ $\\begin{array}{l}{\\displaystyle\\sum_{K=1}^{\\mathrm{{M}}}\\(\\cdot)\\le\\sum_{K=1}^{M}(\\cdot)}\\\\ {\\displaystyle\\sum_{K=1}^{M}\\sum_{i\\in B_{K}}(\\cdot)=\\sum_{i=1}^{n}(\\cdot)}\\end{array}$ $a_{t}\\leq(1+\\alpha)a_{t-1}$ $r-s\\leq M{\\mathrm{~and~}}\\alpha\\leq{\\frac{1}{M}}$ s=1 ", "page_idx": 27}, {"type": "text", "text": "The last line follows because t nner sum $\\begin{array}{r}{\\sum_{s=M\\lfloor r/M\\rfloor}^{\\operatorname*{min}\\{r+M,T\\}}a_{s}\\left\\Vert q_{s}-q_{s-1}\\right\\Vert_{2}^{2}}\\end{array}$ can be thought of as a $2M$ $r$ $2M$ times at most. \u53e3 ", "page_idx": 27}, {"type": "text", "text": "In either case, the contribution over the entire trajectory is order $M^{2}=(n/b)^{2}$ terms. Combining the bounds for S0 and S1 yields the following. ", "page_idx": 27}, {"type": "text", "text": "Lemma 10. Letting $M=n/b$ be the number of blocks. Assume that $\\alpha\\leq\\frac{1}{M}$ and $a_{t}\\leq(1\\!+\\!\\alpha)a_{t-1}$ . We have that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}a_{t}\\sum_{i=1}^{n}(q_{t,i}-\\hat{q}_{t-1,i})^{2}\\leq3e^{2}M^{2}\\sum_{t=1}^{T}a_{t}\\left\\Vert q_{t}-q_{t-1}\\right\\Vert_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. Use Lem. 7 to achieve (31) and apply Lem. 8 and Lem. 9 on each term to get ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{t=1}^{T}a_{t}\\sum_{i=1}^{n}(q_{t,i}-\\hat{q}_{t-1,i})^{2}\\leq\\frac{e M(M-1)}{2}\\sum_{t=1}^{M-1}a_{t}\\left\\Vert q_{t}-q_{t-1}\\right\\Vert_{2}^{2}+2e^{2}M^{2}\\sum_{t=1}^{T}a_{t}\\left\\Vert q_{t}-q_{t-1}\\right\\Vert_{2}^{2}}\\\\ &{\\leq3e^{2}M^{2}\\displaystyle\\sum_{t=1}^{T}a_{t}\\left\\Vert q_{t}-q_{t-1}\\right\\Vert_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "completing the proof. ", "page_idx": 28}, {"type": "text", "text": "We close this subsection with comments on how Lem. 10 can be used. The term $\\mathbb{E}_{t}\\left\\Vert\\delta_{t}^{\\mathrm{D}}\\right\\Vert_{2}^{2}$ is multiplied in (36) by a factor $\\frac{n a_{t-1}^{2}G^{2}}{\\mu C_{t-1}}$ . Thus, if we apply Lem. 10 when redistributing over time a term $\\left|\\left|q_{t-1}-q_{t-2}\\right|\\right|_{2}^{2}$ which will be multiplied by a factor (ignoring absolute constants) of ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\frac{n a_{t-1}^{2}G^{2}}{\\mu C_{t-1}}\\cdot M^{2}=\\frac{n^{3}a_{t-1}^{2}G^{2}}{b^{2}\\mu C_{t-1}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "In order to cancel such a term, we require the use of $\\left.-\\frac{A_{t-1}\\nu}{2}\\right\\|q_{t}-q_{t-1}\\bigr\\|_{2}^{2}$ . We can reserve half to be used up by (49), and be left with the condition ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\frac{n^{3}a_{t-1}^{2}G^{2}}{b^{2}\\mu C_{t-1}}\\leq A_{t-1}\\nu\\,\\Longleftrightarrow\\,\\alpha^{2}\\leq\\frac{b^{2}}{n^{2}}\\frac{\\mu\\nu}{2n G^{2}},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "as we have applied that $a_{t-1}\\leq2\\alpha C_{t-1}$ and $a_{t-1}\\leq\\alpha A_{t-1}$ . Thus, this introduces a dependence of order b \u00b5\u03bd on $\\alpha$ , which propagates to the learning rate $\\alpha$ . We now proceed to the main logic of the convergence analysis. ", "page_idx": 28}, {"type": "text", "text": "C.3 Proof of Main Result ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "C.3.1 Lower Bound on Dual Objective ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We first quantify the gap between $\\mathcal{L}(w_{\\star},q_{t})$ and $\\mathcal{L}(w_{\\star},q_{\\star})$ by providing a lower bound in expectation on $\\mathcal{L}(w_{\\star},q_{t})$ , given in Lem. 11. As in Lem. 5, recall the notation $\\pi(t,i)$ to satisfy $\\hat{q}_{t,i}=q_{\\pi(t,i),i}$ and $g_{t,i}=\\nabla\\ell_{i}(w_{\\pi(t,i)})$ , that is, the time index of the last update of table element $i$ on or before time $t$ , with $\\pi(t,i)=0$ for $t\\leq0$ . ", "page_idx": 28}, {"type": "text", "text": "Lemma 11. Assume that $\\alpha\\leq\\mu/(24e L\\kappa_{\\mathcal{Q}})$ . Then, for $t\\geq2$ , we have that: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad_{\\mathrm{suff}}\\exp_{t}\\omega_{t+\\tau,\\tau+\\tau,\\eta^{\\prime}}}\\\\ &{\\le-a_{\\mathrm{E}}[q_{1}q_{1}^{T}t^{\\prime}(\\tau_{t})]-\\frac{a_{\\mathrm{E}}}{2}\\sum_{i=1}^{n}\\mathbb{E}_{q_{i}}[\\nabla\\ell_{i}(w_{i})-\\nabla\\ell_{i}(w_{i})]_{2}^{2}}\\\\ &{\\quad+\\frac{a_{\\mathrm{E}}}{4L}\\sum_{i=1}^{n}\\frac{1}{\\eta}q_{i-1,i}\\Big\\|\\nabla\\ell_{i}(w_{i-1})-\\nabla\\ell_{i}(w_{i-1})\\Big\\|_{2}^{2}+\\displaystyle\\sum_{i=1}^{n}\\frac{a_{\\mathrm{E}}(\\tau_{i-2,i})}{4L}q_{\\tau(t-2,i),i}\\Big\\|\\nabla\\ell_{i}(w_{\\tau(t-2,i)}-\\nabla\\ell_{i}(w_{i-1})}\\\\ &{\\quad+\\underbrace{\\theta n\\alpha_{t-1}\\zeta^{2}}_{t=1}\\Big\\|q_{t-1}-\\hat{q}_{t-2}\\Big\\|_{2}^{2}}\\\\ &{\\quad-\\alpha_{\\mathrm{E}}[\\zeta(\\tau(w_{i})^{T}q_{t}-\\hat{\\tau}_{t}^{2}\\hat{\\tau}_{+1}\\mathbb{,\\psi}_{s}-w_{t})]+a_{t-1}\\left<\\nabla(\\ell(w_{t-1})^{T}q_{t-1}-\\hat{y}_{t-2}^{T}\\hat{q}_{t-2},w_{*}-w_{t-1})\\right>}\\\\ &{\\quad-\\frac{a_{\\mathrm{E}}\\mu_{t}}{2}\\mathbb{E}_{q_{i}}[\\|w_{1}\\|_{2}^{2}+a_{\\mathrm{E}}\\psi_{s}\\mathbb{E}_{[D(q_{1}\\|q_{1})]}-\\frac{\\mu A_{\\mathrm{E}}}{2}\\mathbb{E}_{q_{i}}[w_{\\Psi_{i}}-w_{t}]_{2}^{2}}\\\\ &{\\quad-\\frac{\\mu A_{\\mathrm{E}}}{2}\\|w_{i}-w_{t-1}\\|_{2}^{2}+\\frac{\\mu C_{1}-1}{2}\\|w_{s}-w_{t-1}\\|_{2}^{2}}\\\\ &{\\quad-\\frac{\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. We use convexity and smoothness (1), then add and subtract (2) elements from the primal update, and finally use the definition of the proximal operator (3) with the optimality of $w_{t}$ for the problem that defines it. ", "page_idx": 29}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{\\Delta t}\\int_{\\Omega}\\mathrm{e}^{\\phi_{1}}(x_{1})+\\mathrm{a}_{1}\\,\\mathrm{e}^{\\phi_{1}}(y_{1})\\,\\mathrm{e}^{-\\phi_{2}}\\mathrm{e}^{-x_{2}}\\mathrm{e}^{-x_{3}}\\mathrm{e}^{-y_{4}}+\\mathrm{c}_{1}^{\\mathrm{a}}\\mathrm{e}^{-y_{5}}\\mathrm{e}^{-x_{6}}\\mathrm{e}^{-y_{7}}}\\\\ &{\\quad+\\mathrm{a}_{2}\\,\\mathrm{e}^{-y_{1}}\\mathrm{e}^{-y_{7}}\\mathrm{e}^{-y_{8}}\\mathrm{e}^{-y_{9}}}\\\\ &{\\quad\\mathrm{a}_{3}\\,\\mathrm{e}^{-z_{9}}\\mathrm{e}^{-y_{1}}+\\mathrm{c}_{2}\\,\\mathrm{e}^{-y_{9}}\\mathrm{e}^{-y_{10}}\\mathrm{e}^{-y_{11}}+\\mathrm{c}_{2}\\,\\mathrm{e}^{-y_{12}}\\mathrm{e}^{-y_{13}}}\\\\ &{\\quad+\\mathrm{a}_{3}\\,\\mathrm{e}^{-z_{9}}\\mathrm{e}^{-y_{13}}+\\mathrm{c}_{2}\\,\\mathrm{e}^{-y_{14}}\\mathrm{e}^{-y_{23}}+\\mathrm{c}_{2}^{\\mathrm{b}}\\mathrm{e}^{y_{14}}\\mathrm{e}^{-y_{25}}\\mathrm{e}^{-y_{26}}}\\\\ &{\\quad\\mathrm{a}_{4}\\,\\mathrm{e}^{-y_{2}}\\mathrm{e}^{-y_{12}}+\\mathrm{c}_{3}\\,\\mathrm{e}^{y_{15}}\\mathrm{e}^{-y_{21}}+\\mathrm{c}_{3}\\,\\mathrm{e}^{-y_{22}}\\mathrm{e}^{-y_{23}}}\\\\ &{\\quad+\\mathrm{c}_{4}^{\\mathrm{a}}\\,\\mathrm{e}^{-y_{16}}\\mathrm{e}^{-y_{17}}+\\mathrm{c}_{4}\\,\\mathrm{e}^{-y_{23}}\\mathrm{e}^{-y_{24}}}\\\\ &{\\quad\\mathrm{a}_{5}\\,\\mathrm{e}^{-z_{9}}\\mathrm{e}^{-y_{18}}-\\mathrm{e}^{-y_{19}}\\mathrm{e}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Next, we are able to use Lem. 4 with $\\boldsymbol{v}_{t}=\\boldsymbol{v}_{t}^{\\mathrm{P}},\\boldsymbol{y}_{t}=\\nabla\\ell(\\boldsymbol{w}_{t})^{\\top}\\boldsymbol{q}_{t},\\hat{y}_{t}=\\hat{g}_{t}$ , $x_{t}=w_{t}$ , $x_{\\star}=w_{\\star}$ , and $\\gamma=\\mu C_{t-1}/2$ which yields that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\imath_{t}\\mathbb{E}_{t}\\left[\\left\\langle\\nabla\\ell(w_{t})^{\\top}q_{t}-v_{t}^{\\mathrm{P}},w_{\\star}-w_{t}\\right\\rangle\\right]}\\\\ &{\\leq a_{t}\\mathbb{E}_{t}\\left[\\left\\langle\\nabla\\ell(w_{t})^{\\top}q_{t}-\\hat{g}_{t-1}^{\\top}\\hat{q}_{t-1},w_{\\star}-w_{t}\\right\\rangle\\right]-a_{t-1}\\left\\langle\\nabla\\ell(w_{t-1})^{\\top}q_{t-1}-\\hat{g}_{t-2}^{\\top}\\hat{q}_{t-2},w_{\\star}-w_{t-1}\\right\\rangle}\\\\ &{\\quad+\\frac{n^{2}a_{t-1}^{2}}{\\mu C_{t-1}}\\underbrace{\\mathbb{E}_{t}\\left[\\left\\|\\frac{1}{b}\\sum_{i\\in I_{t}}\\nabla\\ell_{i}(w_{t-1})q_{t-1,i}-g_{t-2,i}\\hat{q}_{t-2,i}\\right\\|_{2}^{2}\\right]}_{\\mathbb{E}_{t}\\left\\|\\delta_{t}^{\\mathrm{P}}\\right\\|_{2}^{2}}+\\frac{\\mu C_{t-1}}{4}\\mathbb{E}_{t}\\left\\|w_{t}-w_{t-1}\\right\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Then, apply Lem. 5 to achieve ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{t}\\left\\|\\delta_{t}^{\\mathrm{P}}\\right\\|_{2}^{2}\\leq\\displaystyle\\frac{3q_{\\mathrm{max}}}{n}\\sum_{i=1}^{n}q_{t-1,i}\\left\\|\\nabla\\ell_{i}(w_{t-1})-\\nabla\\ell_{i}(w^{\\star})\\right\\|_{2}^{2}}\\\\ &{\\qquad\\qquad+\\displaystyle\\frac{3q_{\\mathrm{max}}}{n}\\sum_{i=1}^{n}q_{\\pi(t-2,i),i}\\left\\|\\nabla\\ell_{i}(w_{\\pi(t-2,i)})-\\nabla\\ell_{i}(w^{\\star})\\right\\|_{2}^{2}}\\\\ &{\\qquad\\qquad+\\displaystyle\\frac{3G^{2}}{n}\\left\\|q_{t-1}-\\hat{q}_{t-2}\\right\\|_{2}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "In the following, the blue terms indicate what changes from line to line. Combine the previous two steps to collect all terms for the lower bound. That is, apply (34) to write ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\mathbb{E}_{t}[a_{t}\\zeta(w_{*},q_{t})]}}\\\\ {~~}\\\\ {{\\displaystyle:=a_{t}q_{t}^{\\top}\\ell(w_{*})+a_{t}\\frac{\\mu}{2}\\|w_{*}\\|_{2}^{2}-a_{t}\\nu D(q_{t}||q_{0})}}\\\\ {~~}\\\\ {{\\displaystyle\\geq a_{t}q_{t}^{\\top}\\ell(w_{t})+\\mathbb{E}_{t}[a_{t}\\left\\langle\\nabla\\ell(w_{t})^{\\top}q_{t}-v_{t}^{\\mathrm{p}},w_{*}-w_{t}\\right\\rangle]+\\frac{a_{t}}{2L}\\sum_{i=1}^{n}q_{t,i}\\left\\|\\nabla\\ell_{i}(w_{t})-\\nabla\\ell_{i}(w_{*})\\right\\|_{2}^{2}}}\\\\ {~~}\\\\ {{\\displaystyle~~~+\\frac{a_{t}\\mu}{2}\\left\\|w_{t}\\right\\|_{2}^{2}-a_{t}\\nu D(q_{t}||q_{0})+\\frac{\\mu A_{t}}{2}\\left\\|w_{*}-w_{t}\\right\\|_{2}^{2}}}\\\\ {~~}\\\\ {{\\displaystyle~~~+\\frac{\\mu C_{t-1}}{2}\\left\\|w_{t}-w_{t-1}\\right\\|_{2}^{2}-\\frac{\\mu C_{t-1}}{2}\\|w_{*}-w_{t-1}\\|_{2}^{2}}}\\\\ {~~}\\\\ {{\\displaystyle~~~+\\frac{c_{t-1}\\mu}{2}\\sum_{\\tau=t-n/b}^{t-2}\\|w_{t}-w_{\\tau\\forall0}\\|_{2}^{2}-\\frac{c_{t-1}\\mu}{2}\\sum_{\\tau=t-n/b}^{t-2}\\left\\|w_{*}-w_{\\tau\\forall0}\\right\\|_{2}^{2},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "then apply (35) to the blue term above to write ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{t}[a_{t}C(w_{*},q_{t})]}\\\\ &{\\geq a_{t}q_{t}^{\\top}\\ell(w_{t})+\\frac{a_{t}}{2L}\\sum_{i=1}^{n}\\|\\nabla\\ell_{i}(w_{t})-\\nabla\\ell_{i}(w_{*})\\|_{2}^{2}}\\\\ &{\\quad-\\frac{n^{2}a_{t-1}^{2}}{\\mu C_{t-1}}\\mathbb{E}_{t}\\left\\|\\nabla\\ell_{i-1}(w_{t-1})q_{t-1,i-1}-g_{t-2,i_{t-1}}\\hat{q}_{t-2,i_{t-1}}\\right\\|_{2}^{2}}\\\\ &{\\quad+a_{t}\\mathbb{E}_{t}\\left[\\langle\\nabla\\ell(w_{t})^{\\top}q_{t}-\\hat{g}_{t-1}^{\\top}\\hat{q}_{t-1},w_{*}-w_{t}\\rangle\\right]-a_{t-1}\\left\\langle\\nabla\\ell(w_{t-1})^{\\top}q_{t-1}-\\hat{g}_{t-2}^{\\top}\\hat{q}_{t-2},w_{*}-w_{t-1}\\right\\rangle}\\\\ &{\\quad+\\frac{a_{t}\\mu}{2}\\left\\|w_{t}\\right\\|_{2}^{2}-a_{t}\\nu D(q_{t})\\|q_{0}+\\frac{\\mu A_{t}}{2}\\left\\|w_{*}-w_{t}\\right\\|_{2}^{2}}\\\\ &{\\quad+\\frac{\\mu C_{t-1}}{4}\\left\\|w_{t}-w_{t-1}\\right\\|_{2}^{2}-\\frac{\\mu C_{t-1}}{2}\\|w_{*}-w_{t-1}\\|_{2}^{2}}\\\\ &{\\quad+\\frac{\\varepsilon_{t-1}\\mu}{2}\\sum_{i=1}^{n}\\frac{\\Gamma-2}{\\nu\\tau(w_{*}-w_{t})\\|_{2}^{2}}-\\frac{\\tau_{t-1}\\mu}{2}\\sum_{\\tau=t-n/b}^{\\ell-2}\\|w_{*}-w_{\\tau\\vee0}\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Finally, apply (36) to the term (37) to achieve ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\geq\\alpha_{t}q_{t}^{\\top}\\ell(w_{t})+\\frac{\\alpha_{t}}{2}\\sum_{i=1}^{N}q_{i}\\,,\\|\\nabla\\ell_{i}(w_{t})-\\nabla\\ell_{i}(w_{t})\\|_{2}^{2}}\\\\ {\\displaystyle-\\frac{3\\kappa_{t}\\alpha_{t-1}^{2}}{\\mu\\,C_{t-1}}\\sum_{i=1}^{N}q_{i-1,i}\\,\\|\\nabla\\ell_{i}(w_{t-1})-\\nabla\\ell_{i}(w_{s})\\|_{2}^{2}-\\frac{3\\kappa_{t}\\alpha_{t-1}^{2}}{\\mu\\,C_{t-1}}\\sum_{i=1}^{n}q_{\\pi(t-2,i),i}\\,\\Big\\|\\nabla\\ell_{i}(w_{\\pi(t-2,i)}-\\nabla\\ell_{i})}\\\\ {\\displaystyle-\\frac{3\\kappa_{t}\\alpha_{t-1}^{2}}{\\mu\\,C_{t-1}}\\|q_{t-1}-\\hat{q}_{t-2}\\|_{2}^{2}}\\\\ {\\displaystyle+\\alpha_{t}\\mathbb{E}_{t}\\Big[\\left\\langle\\nabla\\ell(w_{t})\\nabla_{q}-\\hat{y}_{t-1}^{2}\\hat{\\eta}_{t-1},w_{*}-w_{t}\\right\\rangle\\Big]-\\alpha_{t-1}\\left\\langle\\nabla\\ell(w_{t-1})^{\\top}q_{t-1}-\\hat{y}_{t-2}^{\\top}\\hat{\\eta}_{t-2},w_{*}-w_{t-1}\\right\\rangle}\\\\ {\\displaystyle+\\frac{\\alpha_{t}\\mu}{2}\\left\\|w_{t}\\right\\|_{2}^{2}-a_{t}\\nu D(q_{t}||q_{0})+\\frac{\\mu A_{t}}{2}\\left\\|w_{s}-w_{t}\\right\\|_{2}^{2}}\\\\ {\\displaystyle+\\frac{\\mu C_{t}}{4}\\|w_{t}-w_{t-1}\\|_{2}^{2}-\\frac{\\mu C_{t-1}}{2}\\|w_{s}-w_{t-1}\\|_{2}^{2}}\\\\ {\\displaystyle+\\frac{c_{t-1}\\mu}{2}\\sum_{i=1}^{k}\\|w_{t}-w_{\\tau\\tau\\tau}\\|_{2}^{2}-\\frac{c_{t-1}\\mu}{2}\\underbrace{\\tau_{\\tau\\tau-1}^{\\top-2}}_{\\tau\\tau\\rightarrow t-\\nu_{\\tau\\tau}}\\|w\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Next, we apply at\u22121 \u22642\u03b1Ct\u22121 and \u03b1 \u226424e\u00b5L\u03baQ to achieve: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\frac{3n G^{2}a_{t-1}^{2}}{\\mu C_{t-1}}\\leq\\frac{6n G^{2}\\alpha a_{t-1}}{\\mu},}&\\\\ &{\\frac{3\\kappa_{Q}a_{t-1}^{2}}{\\mu C_{t-1}}\\leq\\frac{a_{t-1}}{4L},}&\\\\ &{\\frac{3\\kappa_{Q}a_{t-1}^{2}}{\\mu C_{t-1}}\\leq\\frac{a_{\\pi(t-2,i)}}{4L},\\ \\forall i\\in[n]}&\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "For terms that contain $\\pi(t-2,i)$ , we recall that $\\pi(t-2,i)$ can be at most $n/b$ timesteps behind $t-2$ , so we have that ", "page_idx": 31}, {"type": "equation", "text": "$$\na_{t-1}\\leq\\left(1+\\frac{1}{n/b}\\right)^{n/b}a_{t-n/b}\\leq e a_{t-n/b}\\leq e a_{\\pi(t-2,i)}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We use here that $\\begin{array}{r}{a_{t}\\leq(1+\\frac{\\alpha}{4})a_{t-1}}\\end{array}$ and impose the condition $\\textstyle{\\frac{\\alpha}{4}}\\leq{\\frac{b}{n}}$ in the rate. Substituting this back into the lower bound achieves the desired claim. \u53e3 ", "page_idx": 31}, {"type": "text", "text": "C.3.2 Upper Bound on Gap Criterion ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Next, we quantify the gap between $\\mathcal{L}(w_{t},q_{\\star})$ and $\\mathcal{L}(w_{\\star},q_{\\star})$ by upper bounding $a_{t}\\mathcal{L}(w_{t},q_{\\star})$ , as given in Lem. 12. When combined with the lower bound Lem. 11, we may then control the gap. ", "page_idx": 31}, {"type": "text", "text": "Lemma 12. For $t\\geq2$ , we have that: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{a_{t}\\mathcal{L}(w_{t},q_{\\star})\\leq a_{t}q_{\\star}^{\\mathrm{\\tiny~\\top}}(\\ell(w_{t})-v_{t}^{\\mathrm{D}})+\\frac{a_{t}\\mu}{2}\\left\\|w_{t}\\right\\|_{2}^{2}+A_{t-1}\\nu\\Delta_{D}(q_{\\star},q_{t-1})}\\\\ &{\\qquad\\qquad\\qquad+\\left.a_{t}q_{t}^{\\mathrm{\\tiny~\\top}}v_{t}^{\\mathrm{\\tiny~D}}-a_{t}\\nu D(q_{t}||q_{0})-A_{t-1}\\nu\\Delta_{D}(q_{t},q_{t-1})-A_{t}\\nu\\Delta_{D}(q_{\\star},q_{t}).\\right\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Proof. We add and subtract (1) terms in the dual update step and apply the definition of the proximal operator (2) with Bregman divergence, and the optimality of $q_{t}$ for the maximization problem that defines it. ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{a_{t}\\mathcal{L}(w_{t},q_{\\star}):=a_{t}{q_{\\star}}^{\\top}\\ell(w_{t})-a_{t}\\nu D(q_{\\star}||q_{0})+\\frac{a_{t}\\mu}{2}\\left\\|w_{t}\\right\\|_{2}^{2}}\\\\ &{\\overset{(1)}{=}a_{t}{q_{\\star}}^{\\top}(\\ell(w_{t})\\!-\\!v_{t}^{\\mathrm{D}})+\\frac{a_{t}\\mu}{2}\\left\\|w_{t}\\right\\|_{2}^{2}\\!+\\!A_{t-1}\\nu\\Delta_{D}(q_{\\star},q_{t-1}))}\\\\ &{\\quad\\quad\\quad\\quad+a_{t}{q_{\\star}}^{\\top}v_{t}^{\\mathrm{D}}-a_{t}\\nu D(q_{\\star}||q_{0})\\!-\\!A_{t-1}\\nu\\Delta_{D}(q_{\\star},q_{t-1}))}\\\\ &{\\overset{(2)}{\\leq}a_{t}{q_{\\star}}^{\\top}(\\ell(w_{t})-v_{t}^{\\mathrm{D}})+\\frac{a_{t}\\mu}{2}\\left\\|w_{t}\\right\\|_{2}^{2}+A_{t-1}\\nu\\Delta_{D}(q_{\\star},q_{t-1}))}\\\\ &{\\quad\\quad\\quad\\quad+a_{t}{q_{t}}^{\\top}v_{t}^{\\mathrm{D}}-a_{t}\\nu D(q_{t}||q_{0})-A_{t-1}\\nu\\Delta_{D}(q_{t},q_{t-1})-A_{t}\\nu\\Delta_{D}(q_{\\star},q_{t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We can combine the upper bound from Lem. 12 and lower bound from Lem. 11 in Lem. 13. We identify telescoping terms in blue and non-positive terms in red. The green term is canceled after aggregation across time $t$ . This bound, like before applies for $t\\geq2$ . ", "page_idx": 31}, {"type": "text", "text": "Lemma 13. Assume that $\\alpha\\leq\\mu/(24e L\\kappa_{\\mathcal{Q}})$ . For $t>2$ , we have that: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{\\mathbb{E}_{\\left[{\\mathbf{X}}\\right]}=\\sum_{i,j=1}^{{\\infty}}\\sum_{\\ell=1}^{\\infty}\\mathcal{L}_{\\left({\\ell+\\ell_{i}}\\right),\\ldots,\\ell}-\\sum_{\\ell=1}^{{\\infty}}\\sum_{\\ell=1}^{{\\infty}}w_{i,j}-w_{\\ell,j}^{-}\\frac{w_{\\ell}^{-}}{2}w_{\\ell}^{-}\\,\\ln\\ell_{i}+\\ell_{j}\\Big\\}}\\\\ &{\\le\\alpha\\sum_{\\ell=1}^{{\\infty}}\\sum_{\\ell=1}^{{\\infty}}\\sum_{\\ell=1}^{{\\ell}}w_{i,j}-k_{\\ell}\\Big(\\ell_{i}-\\ell_{j}\\Big)\\,w_{i,j}\\Big[\\ell\\Big(\\ell_{1,1}-\\ell_{j}\\Big)-\\ell_{j-1}\\Big]}&{\\le\\alpha\\,\\ln\\ell_{i}}\\\\ &{\\le\\alpha\\,\\ln\\left(\\frac{\\ell_{i}-1}{2}\\right)\\,\\sum_{\\ell=1}^{{\\infty}}w_{i,j}\\Big[\\ell_{1,2}-\\ell_{j}\\Big(\\ell_{i+1,j}\\Big)+\\frac{1}{2}\\,\\ell_{j-1,j}-\\ell_{j-1}\\Big]}&{\\quad{\\scriptstyle(\\mathcal{X})}}\\\\ &{\\quad-\\frac{2}{\\sqrt{3}}\\,\\ell_{j-1}^{2}\\left[\\ell_{1,1}\\ell_{1}\\Big(\\ell_{1,2}\\Big)-\\ell_{j-1}\\Big(\\ell_{1,1}\\Big)\\right]}&{\\quad{\\scriptstyle(\\mathcal{X})}}\\\\ &{\\quad+\\frac{\\alpha_{\\ell}-1}{2}\\sum_{\\ell=1}^{\\infty}\\sqrt{\\ell_{1,1}\\ell_{1}\\left(\\ell_{1,2}\\Big)-\\ell_{j}\\Big(\\ell_{2,1}\\Big)}+\\frac{\\alpha_{\\ell}-1}{2}\\,\\ell_{j-1}\\,\\ln\\ell_{i+1,j}\\prod_{\\ell=1,j+1}\\dots\\nabla_{\\ell}\\ell_{j}}\\\\ &{\\quad-\\alpha\\,\\ell_{i,j}\\,\\Big[\\ell\\Big(\\ell_{1,1}\\Big)\\,w_{i,j}-\\ell_{j-1}\\,\\ln\\ell_{i-1,j}\\Big]\\Big)+\\frac{\\alpha_{\\ell}}{2}\\,\\ell_{j-1}\\,\\ln\\ell_{i+1,j}\\,\\Big[\\ell\\Big(\\ell_{1\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": ") ", "page_idx": 32}, {"type": "text", "text": ") ", "page_idx": 32}, {"type": "text", "text": ") ", "page_idx": 32}, {"type": "text", "text": ") ", "page_idx": 32}, {"type": "text", "text": "For $t=2$ , the above holds with the addition of the term $\\begin{array}{r}{\\frac{\\nu}{4}\\left\\|q_{1}-q_{0}\\right\\|_{2}^{2}+\\frac{n G^{2}}{\\nu}\\left\\|w_{0}-w_{1}\\right\\|_{2}^{2}}\\end{array}$ . ", "page_idx": 32}, {"type": "text", "text": "Proof. First, combine Lem. 11 and Lem. 12 to write: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left\\{\\left|-\\sum_{j=1}^{K}\\mathbb{E}\\left[\\alpha_{j}(E(w_{1},q)-\\zeta(w_{1},q)-\\zeta(w_{1},q))\\right]\\right|_{{\\mathbb{R}}^{n}}-\\alpha_{2}\\frac{|w_{1}|^{2}}{2}\\frac{|w_{1}-z_{0}|^{2}}{|w_{1}-z_{0}|^{2}}\\left|\\Phi_{1}-\\Phi_{2}\\right|_{{\\mathbb{R}}^{n}}^{2}\\right\\}}\\\\ &{\\mathbb{E}\\left\\{\\frac{1}{n w_{1}}\\mathbb{E}\\Bigg[\\alpha_{3}(q-\\zeta)^{\\top}(\\Phi_{1})-\\frac{\\zeta(w_{1})}{2}\\Bigg]\\right\\}}\\\\ &{+\\mathbb{E}\\left\\{\\left|A_{1}-\\lambda_{2}\\Delta_{3}(w_{1},q)-\\lambda_{1}w_{2}\\Delta_{3}(q-\\phi_{1})\\right|_{{\\mathbb{R}}^{n}}\\right\\}}\\\\ &{-\\mathbb{E}\\Bigg[\\frac{1}{n\\tilde{\\mu}_{2}}\\sum_{j=1}^{K}\\mathbb{E}\\|\\nabla_{z}(w_{1})-\\nabla_{z}(w_{1},q)\\|_{{\\mathbb{R}}^{n}}^{2}\\Bigg]}\\\\ &{+\\frac{n_{2}\\Delta_{3}}{n\\tilde{\\mu}_{2}}\\sum_{j=1}^{K}\\mathbb{E}\\|\\nabla_{z}(w_{1})-\\nabla_{z}(w_{1},q)\\|_{{\\mathbb{R}}^{n}}^{2}+\\frac{\\zeta(w_{1}-z_{0})}{n\\tilde{\\mu}_{2}}\\mathbb{E}_{\\|\\Phi_{1}-z_{0}\\|_{{\\mathbb{R}}^{n}}}\\Bigg\\{\\nabla f_{i}(w_{1},q-\\zeta_{2},q)-\\nabla_{z}(w_{1},q)}\\\\ &{+\\frac{n_{3}\\Delta_{2}}{n\\tilde{\\mu}_{2}}\\sum_{j=1}^{K}\\mathbb{E}\\|\\nabla_{z}(w_{1})-\\nabla_{z}(z_{1},q)\\|_{{\\mathbb{R}}^{n}}^{2}}\\\\ &{+\\frac{n_{4}\\Delta_{3}}{n\\tilde{\\mu}_{2}}\\Bigg[\\nabla_{z}(\\theta_{1})^{\\top}\\nabla_{z}(\\theta_{1}) \n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Bound the cross term identified above. In the case that $t=2$ , use Lem. 4 with $v_{t}=v_{t}^{\\mathrm{D}}$ , $y_{t}=\\ell(w_{t})$ , $\\hat{y}_{t+1}=\\hat{\\ell}_{t},x_{t}=q_{t},x_{\\star}=q_{\\star}$ , and $\\gamma=\\nu A_{t-1}=\\nu$ which yields that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{a_{t}\\mathbb{E}_{t}\\big[(q_{\\star}-q_{2})^{\\top}(\\ell(w_{2})-v_{2}^{\\mathrm{D}})\\big]}\\\\ &{\\le a_{2}\\mathbb{E}_{2}\\big[(q_{\\star}-q_{2})^{\\top}(\\ell(w_{t})-\\hat{\\ell}_{2})\\big]-a_{1}(q_{\\star}-q_{1})^{\\top}(\\ell(w_{1})-\\hat{\\ell}_{1})}\\\\ &{\\phantom{a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a}+\\frac{\\nu}{4}\\mathbb{E}_{2}\\big[\\|q_{1}-q_{0}\\|_{2}^{2}\\big]+\\frac{n^{2}}{\\nu}\\mathbb{E}_{2}\\left\\|\\frac{1}{b}\\sum_{j\\in J_{t}}(\\ell_{j}(w_{1})-\\hat{\\ell}_{1,j})e_{j}\\right\\|_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where the fourth term above can be bounded as ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{n^{2}}{\\nu}\\mathbb{E}_{2}\\left\\|\\frac{1}{b}\\sum_{j\\in J_{t}}(\\ell_{j}(w_{1})-\\hat{\\ell}_{1,j})e_{j}\\right\\|_{2}^{2}\\leq\\frac{n b}{\\nu}\\left\\|\\frac{1}{b}\\sum_{j=1}^{b}(\\ell_{j}(w_{1})-\\hat{\\ell}_{1,j})e_{j}\\right\\|_{2}^{2}\\leq\\frac{n G^{2}}{\\nu}\\left\\|w_{1}-w_{0}\\right\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Using the definition of the update, we have that $\\left\\|w_{1}-w_{0}\\right\\|_{2}^{2}=(1/\\mu_{.}^{2})\\left\\|\\nabla\\ell(w_{0})^{\\top}q_{0}\\right\\|_{2}^{2}$ . In the case that $t>2$ , use Lem. 4 as above but instead with $\\gamma=\\nu A_{t-2}$ which yields that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{a_{t}\\mathbb{E}_{t}\\big[(q_{*}-q_{t})^{\\top}(\\ell(w_{t})-v_{t}^{\\mathrm{D}})\\big]}\\\\ &{\\leq a_{t}\\mathbb{E}_{t}\\big[(q_{*}-q_{t})^{\\top}(\\ell(w_{t})-\\hat{\\ell}_{t})\\big]-a_{t-1}(q_{*}-q_{t-1})^{\\top}(\\ell(w_{t-1})-\\hat{\\ell}_{t-1})}\\\\ &{\\quad+\\,\\frac{A_{t-2}\\nu}{4}\\mathbb{E}_{t}\\big[\\,\\|q_{t}-q_{t-1}\\|_{2}^{2}\\,\\big]+\\frac{n^{2}a_{t-1}^{2}}{A_{t-2}\\nu}\\underbrace{\\mathbb{E}_{t}\\left\\|\\frac{1}{b}\\sum_{j\\in J_{t}}(\\ell_{j}(w_{t-1})-\\hat{\\ell}_{t-1,j})e_{j}\\right\\|_{2}^{2}}_{\\mathbb{E}_{t}\\big\\|\\delta_{t}^{\\mathrm{D}}\\big\\|_{2}^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "We may then apply Lem. 6 and $a_{t-1}\\leq\\alpha A_{t-2}$ to get that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\frac{n^{2}a_{t-1}^{2}}{A_{t-2}\\nu}\\mathbb{E}_{t}\\left\\|\\delta_{t}^{\\mathrm{D}}\\right\\|_{2}^{2}\\leq\\frac{n a_{t-1}\\alpha G^{2}}{\\nu}\\sum_{\\tau=t-n/b}^{t-2}\\left\\|w_{t-1}-w_{\\tau\\vee0}\\right\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "We may use strong convexity to get the $\\begin{array}{r}{\\frac{A_{t-2}\\nu}{4}\\mathbb{E}_{t}\\left[\\left|\\left|q_{t}-q_{t-1}\\right|\\right|_{2}^{2}\\right]}\\end{array}$ term to cancel with $-\\frac{A_{t-1}\\nu}{2}\\Delta_{D}\\big(q_{t},q_{t-1}\\big)$ and that $A_{t-2}\\leq A_{t-1}$ to complete the proof. \u53e3 ", "page_idx": 33}, {"type": "text", "text": "In this section, we provide derivations that determine the values of the constant $c_{t}$ that allow for cancellation of errors. We slightly adjust the notation in this subsection, in that we assume that for some $\\eta>0$ ", "page_idx": 34}, {"type": "equation", "text": "$$\na_{t}\\leq(1+\\eta)a_{t-1}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "and determine $\\eta$ such that (28) is satisfied. We will see that $\\eta$ is simply a constant factor away from $\\alpha$ , so the resulting condition we actually be on $\\alpha$ . The latter is given formally in Lem. 14. We assume here that $\\bar{n}/b\\ge2$ , which is taken as an assumption of Thm. 2. ", "page_idx": 34}, {"type": "text", "text": "In the statement of Lem. 13, the lines above (43) will telescope without additional conditions. For (44), we set $c_{t}\\:=\\:a_{t}/m$ for some parameter $m$ . Note that this condition does not need to be checked when $n/b<1$ , as the additional sum term over $\\tau$ will not be included in the update. Counting all the terms that will appear when matched on the index $t-1$ , we have the condition that ", "page_idx": 34}, {"type": "equation", "text": "$$\n-{\\frac{a_{t-1}}{4}}-A_{t-1}+C_{t-1}+\\sum_{s=t+1}^{t+n/b-1}a_{s}/m\\leq0.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "The first term result from the \u201cgood term\u201d $-\\frac{a_{t-1}\\mu}{4}\\left\\|w_{t-1}-w_{\\star}\\right\\|_{2}^{2}$ from the bottom. The rightmost term above results because $a_{s}\\left\\lVert\\boldsymbol{w}_{\\star}-\\boldsymbol{w}_{\\tau\\vee0}\\right\\rVert_{2}^{2}$ will have $\\tau=t-1$ when $s\\in\\{t\\!+\\!1,\\dots,t\\!+\\!n/b\\!-\\!1\\}$ . We will begin by requiring that require that $a_{t}\\leq(1+\\beta)a_{t-1}$ for all $t$ and some $\\beta>0$ , and then determine $\\beta$ below. The condition reads as ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\frac{4n/b-4+m}{4m}a_{t-1}\\geq\\frac{(1+\\beta)^{2}}{m}\\sum_{s=0}^{n/b-2}\\left(1+\\beta\\right)^{s}a_{t-1},\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "which can be summed and represented as ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\frac{(4n/b-4+m)}{4}\\geq(1+\\beta)^{2}\\frac{(1+\\beta)^{n/b-1}-1}{\\beta}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Rearranging and taking a logarithm on both sides, this is the same as ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\ln\\Big(\\frac{\\beta(4n/b-4+m)}{4(1+\\beta)^{2}}+1\\Big)\\geq(n/b-1)\\log(1+\\beta).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Next, using the inequality $\\textstyle{\\frac{2x}{2+x}}\\leq\\ln(1+x)$ with $\\begin{array}{r}{x=\\frac{\\beta(4n/b-4+m)}{4(1+\\beta)^{2}}}\\end{array}$ which holds for all $x\\geq0$ , we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\ln\\left({\\frac{\\beta(4n/b-4+m)}{4(1+\\beta)^{2}}}+1\\right)\\geq{\\frac{2{\\frac{\\beta(4n/b-4+m)}{4(1+\\beta)^{2}}}}{2+{\\frac{\\beta(4n/b-4+m)}{4(1+\\beta)^{2}}}}}={\\frac{2\\beta(4n/b-4+m)}{8(1+\\beta)^{2}+\\beta(4n/b-4+m)}}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We can also apply the upper bound $\\ln(x+1)\\leq x$ with $x=\\beta$ (which also holds for any $x\\geq0$ ) to write ", "page_idx": 34}, {"type": "equation", "text": "$$\n(n/b-1)\\log(1+\\beta)\\leq(n/b-1)\\beta,\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "which means that (51) will be satisfied (using (52)) if ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\frac{2\\beta(4n/b-4+m)}{8(1+\\beta)^{2}+\\beta(4n/b-4+m)}\\geq(n/b-1)\\beta}\\\\ &{}&{\\iff\\frac{n/b-1+m/4}{n/b-1}\\geq(1+\\beta)^{2}+(\\beta/2)(n/b-1+m/4).}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "In order to satisfy the inequality, substitute $m=4c\\beta(n/b-1)^{2}$ for some $c>0$ to be determined and assume that $\\begin{array}{r}{\\bar{\\beta}\\leq\\frac{1}{n/b-1}}\\end{array}$ , so that $\\beta(n/b-1)\\leq1$ . The LHS reads as ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\frac{n/b-1+m/2}{n/b-1}=1+c\\beta(n/b-1).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "The RHS can be upper-bounded as ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(1+\\beta)^{2}+(\\beta/2)(n/b-1+m/4)=(1+\\beta)^{2}+(\\beta/2)(n/b-1+c\\beta(n/b-1)^{2})}\\\\ {\\leq1+\\beta(2+\\beta)+\\beta(n/b-1)\\frac{1+c}{2},\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "which makes the inequality satisfied when ", "page_idx": 35}, {"type": "equation", "text": "$$\nc\\geq2\\left(\\frac{4+2\\beta}{n/b-1}+1\\right),\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "so we can set $c\\,=\\,16$ . We now have the flexibility to control $\\beta$ , and the telescoping of (44) is achieved. For (45), set $\\begin{array}{r}{\\beta=\\frac{\\alpha}{4}}\\end{array}$ and pass the condition of $\\beta\\leq1/(n/b-1)$ onto $\\alpha$ , which maintains the rate (and is already satisfied when $\\alpha\\leq{\\frac{b}{n}}$ and $n\\geq2$ ). Then, we can achieve ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\frac{n a_{t}\\alpha G^{2}}{\\nu}\\leq\\frac{\\mu a_{t-1}}{m}=\\frac{\\mu a_{t-1}}{4\\alpha(n/b-1)^{2}}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "by requiring that $\\alpha\\leq\\frac{b\\sqrt{\\mu\\nu}}{4n^{3/2}G}$ , which achieves the telescoping of (45). Finally, to address (46), we may satisfy it if ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\frac{n a_{t}\\alpha G^{2}}{\\nu}\\leq\\frac{\\mu C_{t-1}}{4}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "which we can achieve by incorporating the condition $a_{t}\\,\\leq\\,4\\alpha(n/b-1)^{2}C_{t-1}$ into the update of at, because nat\u03bd\u03b1G $\\begin{array}{r}{\\frac{n a_{t}\\alpha G^{2}}{\\nu}\\leq\\frac{\\mu a_{t}}{m}}\\end{array}$ by the previous condition on $\\alpha$ . Having chosen $c_{t}$ , we are prepared to produce a learning rate parameter $\\eta$ to capture all conditions on $\\alpha$ in one, as given in Lem. 14. ", "page_idx": 35}, {"type": "text", "text": "Lemma 14. For all $t\\geq1$ , we have the following. ", "page_idx": 35}, {"type": "text", "text": "\u2022 Using the update scheme ", "page_idx": 35}, {"type": "equation", "text": "$$\na_{2}=4\\eta a_{1},\\;a n d\\;a_{t}=\\left(1+\\eta\\right)a_{t-1}\\,f o r\\,t>2\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "when ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\eta=\\frac{1}{4}\\operatorname*{min}\\left\\{\\frac{b}{32n},\\frac{\\mu}{24e L\\kappa_{\\mathscr{Q}}},\\frac{b}{n}\\sqrt{\\frac{n G^{2}}{\\mu\\nu}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "we have that (28) holds. ", "page_idx": 35}, {"type": "text", "text": "Proof. Noting that $m=16\\alpha(n/b-1)^{2}$ we confirm that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{C_{t}\\geq A_{t-1}/2}\\\\ &{}&{\\iff A_{t}-\\frac{1}{16\\alpha(n/b-1)}a_{t}\\geq A_{t-1}/2}\\\\ &{}&{\\iff\\left(1-\\frac{1}{16\\alpha(n/b-1)}\\right)a_{t}\\geq-A_{t-1}/2}\\\\ &{}&{\\iff2\\left(\\frac{1}{16\\alpha(n/b-1)}-1\\right)a_{t}\\leq A_{t-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "This condition is satisfied when $\\begin{array}{r}{\\alpha\\leq\\frac{1}{32(n/b-1)}}\\end{array}$ , so we incorporate $\\textstyle\\alpha\\leq{\\frac{b}{32n}}$ into the rate, implying that $a_{t}\\leq2\\alpha C_{t}$ . This concludes the proof of the first bullet point. ", "page_idx": 35}, {"type": "text", "text": "Next, we show that $a_{t}\\leq(1{+}\\alpha/4)a_{t-1}$ will imply that $a_{t}\\leq\\alpha A_{t-1}$ , which is the second part of (28). First, we define $a_{2}=\\alpha a_{1}$ as an initialization (which also satisfies $a_{2}\\leq(1{+}\\alpha/4)a_{1}$ when $\\alpha\\leq4/3)$ , and show inductively that if $a_{t-1}\\leq\\alpha A_{t-2}$ , then $a_{t}\\,\\leq\\,(1+\\alpha/4)a_{t-1}\\;\\implies a_{t}\\,\\leq\\,\\alpha A_{t-1}$ . In the base case, $A_{1}=a_{1}$ , so the condition $a_{2}=\\alpha a_{1}$ satisfies $a_{2}\\leq\\alpha A_{1}$ . Next, fixing $t$ and assuming that 1) $a_{t-1}\\leq\\alpha A_{t-2}$ and 2) that $a_{t}\\leq(1+\\alpha/4)a_{t-1}$ , we have that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\alpha A_{t-1}=\\alpha(a_{t-1}+A_{t-2})\\geq(\\alpha+1)a_{t-1}\\geq a_{t},\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "the desired result. Finally, we consider the condition $a_{t}\\leq4\\alpha(n/b-1)^{2}C_{t-1}$ , the third part of (28). We wish to show that the following inequality holds: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle a_{t}\\leq4\\alpha(n/b-1)^{2}C_{t-1}=4\\alpha(n/b-1)^{2}\\left(A_{t-1}-\\frac{1}{4\\alpha(n/b-1)}a_{t-1}\\right)}\\\\ {\\displaystyle=4\\alpha(n/b-1)^{2}\\left(a_{t-1}+A_{t-2}-\\frac{1}{4\\alpha(n/b-1)}a_{t-1}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "which is implied by the inequality ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{a_{t}\\leq4\\alpha(n/b-1)^{2}\\left(a_{t-1}+(1/\\alpha)a_{t-1}-\\displaystyle\\frac{1}{4\\alpha(n/b-1)}a_{t-1}\\right)}}\\\\ {{\\phantom{a}=4(n/b-1)^{2}\\left(\\alpha+1-\\displaystyle\\frac{1}{4(n/b-1)}\\right)a_{t-1}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "When $n/b\\ge2$ , we have that $\\begin{array}{r}{(1+\\alpha/4)\\,\\le\\,4(n/b-1)^{2}\\,\\Big(\\alpha+1-\\frac{1}{4(n/b-1)}\\Big)}\\end{array}$ , so we require that $b\\leq n/2$ . Thus, our final updates are given by ", "page_idx": 36}, {"type": "equation", "text": "$a_{2}=4\\eta a_{1}\\leq\\left(1+\\eta\\right)a_{1}$ ", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Because each condition was satisfied when using $a_{t}=(1\\!+\\!\\alpha/4)a_{t-1}$ , we define $\\eta=\\alpha/4$ to achieve the claimed result. \u53e3 ", "page_idx": 36}, {"type": "text", "text": "C.3.4 Bound on Sum of Successive Gaps ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Lem. 15 is an upper estimate for the expected sum of the gap function over $T$ iterates. Recall that $\\mathbb{E}_{1}\\left[\\cdot\\right]$ the full expectation over $\\{(I_{t},\\bar{J}_{t})\\}_{t=1}^{T}$ . The green term is a quantity that remain as an initialization term, whereas the blue terms have to be bounded from above. The terms directly below the blue terms account for all of the \u201cnegative $\\pi(t-1,i)^{*}$ terms are not yet used up by the telescoping in lines (39), (40), and (41), and there are in fact between 1 and $n$ copies of those terms in each iteration, even though we will use only 1. ", "page_idx": 36}, {"type": "text", "text": "Lemma 15 (Progress Bound). Assume that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\alpha\\leq\\operatorname*{min}\\left\\{{\\frac{b}{32n}},{\\frac{\\mu}{24e L\\kappa_{Q}}},{\\frac{b}{36e^{2}n}}{\\sqrt{\\frac{\\mu\\nu}{n G^{2}}}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "For any $T\\geq1$ , we have that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{I}_{0}\\left[\\displaystyle\\sum_{t=1}^{T}\\gamma_{t}\\right]\\leq\\displaystyle\\frac{n G^{2}}{\\nu\\mu^{2}}\\left\\lVert\\nabla\\ell(w_{0})^{\\top}q_{0}\\right\\rVert_{2}^{2}}\\\\ &{\\phantom{\\mathcal{I}_{0}\\left[\\displaystyle\\sum_{t=1}^{T}\\left[(\\ell_{k}-q_{T})^{\\top}(\\ell(w_{T})-\\hat{\\ell}_{T})\\right]\\right.}+a_{T}\\mathbb{E}_{0}\\left[(\\ell_{k}\\mathfrak{s})\\right.}\\\\ &{\\phantom{\\left.{\\frac{...}{...}}\\!\\!-a_{T}\\mathbb{E}_{0}\\left[\\langle\\nabla\\ell(w_{T})^{\\top}q_{T}-\\hat{g}_{T-1}^{\\top}\\hat{q}_{T-1},w_{\\star}-w_{T}\\rangle\\right]}\\left.\\right.}\\\\ &{\\phantom{\\left.{\\frac{....}{...}}\\!\\!-\\left.-\\sum_{i=1}^{n}\\!(n-T+\\pi(T-1,i))\\frac{a_{\\pi(T-1,i)}}{4L}\\mathbb{E}_{0}\\left[q_{\\pi(T-1,i)}\\left\\lVert\\nabla\\ell_{i}(w_{\\pi(T-1,i)})-\\nabla\\ell_{i}(w_{\\star})\\right\\rVert_{2}^{2}\\right]\\right]}}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad+\\frac{6n G^{2}\\alpha}{\\mu}\\mathbb{E}_{0}\\underset{t=1}{\\overset{r}{\\prod}}\\alpha_{t-1}\\lVert q_{t-1}-\\hat{q}_{t-2}\\rVert_{2}^{2}-\\frac{T}{t=1}\\frac{A_{t-1}\\nu}{2}\\mathbb{E}_{0}\\left[D(q_{t}\\lVert q_{t-1})\\right]}\\\\ &{\\quad-\\frac{\\alpha_{T}}{2L}\\underset{t=1}{\\overset{n}{\\prod}}\\mathbb{E}_{0}\\left[\\mathcal{T}_{t}_{*}(w_{T})-\\nabla\\ell_{i}(w_{*})\\right]\\rvert_{2}^{2}\\Big]}\\\\ &{\\quad-\\frac{\\mu A_{T}}{2}\\mathbb{E}_{0}\\left[\\left\\|w_{*}-w_{T}\\right\\|_{2}^{2}\\right]}\\\\ &{\\quad-\\frac{\\mu a_{T-1}}{2}\\underset{t=1}{\\overset{n}{\\prod}}\\underset{t=1}{\\overset{r}{\\prod}}\\underset{t=T-\\left\\lceil\\eta,h\\right\\rceil}{\\sum}\\mathbb{E}_{0}\\left[\\left\\|w_{T}-w_{T\\setminus0}\\right\\|_{2}^{2}\\right]}\\\\ &{\\quad-A_{T}\\succeq\\mathbb{E}_{0}\\left[D(q_{*}\\lVert q_{T})\\right]}\\\\ &{\\quad-\\underset{t=1}{\\overset{T}{\\sum}}\\frac{a_{t}\\mu_{t}}{4}\\lVert w_{t}-w_{*}\\rVert_{2}^{2}-\\underset{r=1}{\\overset{r}{\\sum}}\\frac{a_{t}\\nu}{2}\\mathbb{E}_{0}\\left\\lVert q_{t}-q_{*}\\right\\rVert_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Proof. We proceed by first deriving an upper bound on $\\gamma_{1}$ , the gap function for $t=1$ . Note that $w_{1}$ is non-random, as $a_{0}=0$ implies that $v_{0}=\\nabla\\ell(w_{0})$ . Using that $w_{1}$ is the optimum for the proximal operator that defines it, the upper bound can be written as ", "page_idx": 37}, {"type": "equation", "text": "$$\na_{1}\\mathcal{L}(w_{1},q_{\\star})\\leq a_{1}q_{\\star}^{\\ \\top}(\\ell(w_{1})-\\hat{\\ell}_{1})+\\frac{a_{1}\\mu}{2}\\left\\|w_{1}\\right\\|_{2}^{2}+a_{1}q_{1}^{\\ \\top}\\hat{\\ell}_{1}-a_{1}\\nu D(q_{1}||q_{0})-A_{1}\\nu\\Delta_{D}(q_{\\star},q_{1}),\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where we use that $\\tilde{\\ell}_{1}=\\hat{\\ell}_{1}$ . For the lower bound, use a similar argument to Lem. 11 to achieve ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{a_{1}\\mathcal{L}(w_{\\star},q_{1})\\geq a_{1}q_{1}^{\\top}\\ell(w_{1})+a_{1}\\left\\langle\\nabla\\ell(w_{1})^{\\top}q_{1}-\\nabla\\ell(w_{0})^{\\top}q_{0},w_{\\star}-w_{1}\\right\\rangle+\\frac{a_{1}\\mu}{2}\\left\\|w_{1}\\right\\|_{2}^{2}}\\\\ &{\\qquad\\qquad\\qquad-\\left.a_{1}\\nu D(q_{1}||q_{0})+\\frac{a_{1}}{2L}\\sum_{i=1}^{n}q_{1,i}\\left\\|\\nabla\\ell_{i}(w_{1})-\\nabla\\ell_{i}(w_{\\star})\\right\\|_{2}^{2}+\\frac{\\mu A_{1}}{2}\\left\\|w_{\\star}-w_{1}\\right\\|_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where we use that $v_{0}^{\\mathrm{P}}=\\nabla\\ell(w_{0})^{\\top}q_{0}$ . We combine them to get ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\gamma_{1}\\leq a_{1}({q_{\\star}}-{q_{1}})^{\\top}(\\ell(w_{1})-\\hat{\\ell}_{1})-a_{1}\\left\\langle\\nabla\\ell(w_{1})^{\\top}{q_{1}}-\\nabla\\ell(w_{0})^{\\top}{q_{0}},w_{\\star}-w_{1}\\right\\rangle}\\\\ {\\quad\\quad-\\displaystyle\\frac{a_{1}}{2L}\\displaystyle\\sum_{i=1}^{n}q_{1,i}\\left\\|\\nabla\\ell_{i}(w_{1})-\\nabla\\ell_{i}(w_{\\star})\\right\\|_{2}^{2}-\\displaystyle\\frac{\\mu A_{1}}{2}\\left\\|w_{\\star}-w_{1}\\right\\|_{2}^{2}-A_{1}\\nu\\Delta_{D}(q_{\\star},q_{1})}\\\\ {\\quad\\quad-\\displaystyle\\frac{a_{1}\\mu}{2}\\left\\|w_{1}-w_{\\star}\\right\\|_{2}^{2}-\\displaystyle\\frac{a_{1}\\nu}{2}\\left\\|q_{1}-q_{\\star}\\right\\|_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where the last two terms are the result of the additional quadratic slack terms in $\\gamma_{1}$ . All of the terms from the display above will be telescoped. Thus, we apply Lem. 13 and collect the unmatched terms from the $t~\\ge~2$ one-step bound (using that $A_{0}~=~0)$ ). The term (57) can be viewed as counting the remainder of (40) after it has telescoped some but not all terms $\\begin{array}{r l}{\\frac{a_{\\pi(T-1,i)}}{4L}\\mathbb{E}_{0}\\left[q_{\\pi(T-1,i)}\\left|\\left|\\nabla\\bar{\\ell}_{i}\\big(w_{\\pi(T-1,i)}\\big)-\\nabla\\ell_{i}(w_{\\star})\\right|\\right|_{2}^{2}\\right]}\\end{array}$ across iterations. ", "page_idx": 37}, {"type": "text", "text": "C.3.5 Completing the Proof ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "We use similar techniques as before to bound the remaining terms from the $T$ -step progress bound given in Lem. 15. We may now prove the main result. ", "page_idx": 38}, {"type": "text", "text": "Theorem 2. For a constant $\\alpha>0$ , define the sequence ", "page_idx": 38}, {"type": "equation", "text": "$$\na_{1}=1,a_{2}=4\\alpha,\\;a n d\\,a_{t}=\\left(1+\\alpha\\right)a_{t-1}\\,f o r\\,t>2,\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "along with its partial sum $\\begin{array}{r}{A_{t}=\\sum_{\\tau=1}^{t}a_{\\tau}}\\end{array}$ . Under Asm. 1, there is an absolute constant $C$ such that using the parameter ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\alpha=C\\operatorname*{min}\\left\\{\\frac{b}{n},\\frac{\\mu}{L\\kappa_{\\mathscr{Q}}},\\frac{b}{n}\\sqrt{\\frac{\\mu\\nu}{n G^{2}}}\\right\\},\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "the iterates of Algorithm $^{\\,l}$ satisfy: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}a_{t}\\mathbb{E}_{1}[\\gamma_{t}]+\\frac{A_{T}\\mu}{4}\\mathbb{E}_{1}\\left\\Vert w_{T}-w_{\\star}\\right\\Vert_{2}^{2}+\\frac{A_{T}\\nu}{4}\\mathbb{E}_{1}\\left\\Vert q_{T}-q_{\\star}\\right\\Vert_{2}^{2}\\leq\\frac{n G^{2}}{\\nu}\\left\\Vert w_{0}-w_{1}\\right\\Vert_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "We can compute a point $(w_{T},q_{T})$ achieving an expected gap no more than $\\varepsilon$ with big- $O$ complexity ", "page_idx": 38}, {"type": "equation", "text": "$$\n(n+b d)\\cdot\\left({\\frac{n}{b}}+{\\frac{L\\kappa_{Q}}{\\mu}}+{\\frac{n}{b}}{\\sqrt{\\frac{n G^{2}}{\\mu\\nu}}}\\right)\\cdot\\ln\\left({\\frac{1}{\\varepsilon}}\\right).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Proof. We first apply Lem. 15, and proceed to bound the inner product terms (55) and (56). Apply Young\u2019s inequality with parameter $\\nu A_{T-1}/2$ to get ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nu_{T}\\mathbb{E}_{0}\\left[(q_{\\star}-q_{T})^{\\top}(\\ell(w_{T})-\\hat{\\ell}_{T})\\right]\\leq\\displaystyle\\frac{\\nu A_{T-1}}{4}\\mathbb{E}_{0}\\left\\lVert q_{\\star}-q_{T}\\right\\rVert_{2}^{2}+\\displaystyle\\frac{a_{T}^{2}}{\\nu A_{T-1}}\\mathbb{E}_{0}\\}|\\ell(w_{T})-\\hat{\\ell}_{T}||_{2}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\displaystyle\\frac{\\nu A_{T-1}}{4}\\mathbb{E}_{0}\\left\\lVert q_{\\star}-q_{T}\\right\\rVert_{2}^{2}+\\displaystyle\\frac{a_{T}^{2}G^{2}}{\\nu A_{T-1}}\\sum_{\\tau=T-n/b}^{T-2}\\mathbb{E}_{0}\\left\\lVert w_{T}-w_{\\tau\\vee0}\\right\\rVert_{2}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\displaystyle\\frac{\\nu A_{T-1}}{4}\\mathbb{E}_{0}\\left\\lVert q_{\\star}-q_{T}\\right\\rVert_{2}^{2}+\\displaystyle\\frac{a_{T}\\alpha G^{2}}{\\nu}\\sum_{\\tau=T-n/b}^{T-2}\\mathbb{E}_{0}\\left\\lVert w_{T}-w_{\\tau\\vee0}\\right\\rVert_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "The left-hand term will be canceled by (61) by applying strong concavity (le\u221aaving behind \u03bdA4T \u22121E0 \u2225q\u22c6\u2212qT \u222522) and the right-hand term (because of the condition \u03b1 \u2264 $\\begin{array}{r}{\\alpha\\ \\leq\\ \\frac{\\sqrt{\\mu\\nu}}{4n^{3/2}G})}\\end{array}$ 4n3/2G) will be canceled by (60). Next, consider (56). By Young\u2019s inequality with parameter $\\mu A_{T-1}/2$ , we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\;a_{T}\\mathbb{E}_{0}\\left[\\big\\langle\\nabla\\ell(w_{T})^{\\top}q_{T}-\\hat{g}_{T-1}^{\\top}\\hat{q}_{T-1},w_{\\star}-w_{T}\\big\\rangle\\right]}\\\\ &{\\leq\\displaystyle\\frac{a_{T}^{2}}{\\mu A_{T-1}}\\mathbb{E}_{0}\\left\\|\\nabla\\ell(w_{T})^{\\top}q_{T}-\\hat{g}_{T-1}^{\\top}\\hat{q}_{T-1}\\right\\|_{2}^{2}+\\displaystyle\\frac{\\mu A_{T-1}}{4}\\mathbb{E}_{0}\\left\\|w_{\\star}-w_{T}\\right\\|_{2}^{2}}\\\\ &{\\leq\\displaystyle\\frac{a_{T}\\alpha}{\\mu}\\mathbb{E}_{0}\\left\\|\\nabla\\ell(w_{T})^{\\top}q_{T}-\\hat{g}_{T-1}^{\\top}\\hat{q}_{T-1}\\right\\|_{2}^{2}+\\displaystyle\\frac{\\mu A_{T-1}}{4}\\mathbb{E}_{0}\\left\\|w_{\\star}-w_{T}\\right\\|_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where the second term will be canceled by (59). For the remaining term, ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{5_{0}}{50}\\left\\|\\nabla\\ell(w_{T})^{\\top}q_{T}-\\hat{g}_{T-1}^{\\top}\\hat{q}_{T-1}\\right\\|_{2}^{2}}\\\\ &{\\leq\\mathbb{E}_{0}\\left\\|(\\nabla\\ell(w_{T})-\\ell(w_{\\star}))^{\\top}q_{T}+\\nabla\\ell(w_{\\star})^{\\top}(q_{T}-\\hat{q}_{T-1})+(\\nabla\\ell(w_{\\star})-\\hat{g}_{T-1})^{\\top}\\hat{q}_{T-1}\\right\\|_{2}^{2}}\\\\ &{\\leq3\\mathbb{E}_{0}\\left\\|(\\nabla\\ell(w_{T})-\\nabla\\ell(w_{\\star}))^{\\top}q_{T}\\right\\|_{2}^{2}+3\\mathbb{E}_{0}\\left\\|\\nabla\\ell(w_{\\star})^{\\top}(q_{T}-\\hat{q}_{T-1})\\right\\|_{2}^{2}+3\\mathbb{E}_{0}\\left\\|(\\nabla\\ell(w_{\\star})-\\hat{g}_{T-1})^{\\top}\\hat{q}_{T}\\right\\|_{2}^{2}}\\\\ &{\\leq3\\sigma_{n}\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}_{0}\\left[q_{T,i}\\left\\|\\nabla\\ell_{i}(w_{T})-\\nabla\\ell_{i}(w_{\\star})\\right\\|_{2}^{2}\\right]+3n G^{2}\\mathbb{E}_{0}\\left\\|q_{T}-\\hat{q}_{T-1}\\right\\|_{2}^{2}+3\\sigma_{n}\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}_{0}\\left[\\hat{q}_{T-1,i}\\left\\|\\nabla\\ell_{i}(w_{\\star})-\\nabla\\ell_{i}(w_{\\star})\\right\\|_{2}^{2}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "We may add the middle term above to (58), so that the remaining term to bound is ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\frac{6n G^{2}\\alpha}{\\mu}\\sum_{t=1}^{T+1}a_{t-1}\\mathbb{E}_{0}\\left\\|q_{t-1}-\\hat{q}_{t-2}\\right\\|_{2}^{2}-\\sum_{t=1}^{T}\\frac{A_{t-1}\\nu}{2}\\mathbb{E}_{0}\\left[\\Delta_{D}\\bigl(q_{t},q_{t-1}\\bigr)\\right].\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "To show that this quantity is non-negative, we use that $a_{0}=0$ and Lem. 10 (recalling that $M=n/b$ to see that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\frac{\\mathfrak{j}n G^{2}\\alpha}{\\mu}\\mathbb{E}_{0}\\sum_{t=1}^{T+1}a_{t}\\left\\Vert q_{t}-\\hat{q}_{t-1}\\right\\Vert_{2}^{2}\\leq\\frac{18e^{2}n^{3}G^{2}\\alpha}{b^{2}\\mu}\\mathbb{E}_{0}\\sum_{t=1}^{T+1}a_{t-1}\\left\\Vert q_{t}-q_{t-1}\\right\\Vert_{2}^{2}\\leq\\frac{18e^{2}n^{3}G^{2}\\alpha^{2}}{b^{2}\\mu}\\mathbb{E}_{0}\\sum_{t=1}^{T}A_{t-1}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "which will cancel with the rightmost term in (58) provided that $\\begin{array}{r}{\\alpha\\,\\leq\\,\\frac{b}{36e^{2}n}\\sqrt{\\frac{\\mu\\nu}{n G^{2}}}}\\end{array}$ . Thus, plugging the previous displays into Lem. 15, we have that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathsf{s}_{\\theta}\\left[\\displaystyle\\sum_{t=1}^{T}\\gamma_{t}\\right]\\le\\frac{n^{2}G^{2}}{\\nu\\mu^{2}}\\left\\|\\nabla\\ell(w_{0})^{\\top}\\boldsymbol{\\varphi}_{\\theta}\\right\\|_{2}^{2}}&{}\\\\ &{\\quad+\\frac{3\\alpha\\tau_{0}\\alpha_{0}}{2\\mu}\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}_{\\theta}\\left[\\langle\\boldsymbol{r}_{i}|\\,\\nabla\\ell_{i}(w_{T})-\\nabla\\ell_{i}(w_{s})\\boldsymbol{\\|}_{2}^{2}\\right]-\\frac{\\alpha_{T}}{2L}\\sum_{i=1}^{n}\\mathbb{E}_{\\theta}\\left[\\langle\\boldsymbol{r}_{i},\\|\\nabla\\ell_{i}(w_{T})-\\nabla\\cdot\\boldsymbol{\\mathsf{t}}_{\\theta}\\|_{2}^{2}\\right]}\\\\ &{\\quad+\\frac{3\\alpha_{0}\\alpha_{T}\\alpha_{0}}{2\\mu}\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}_{\\theta}\\left[\\langle\\boldsymbol{q}_{\\ell}(\\boldsymbol{r}_{-1,i}),\\boldsymbol{i}\\rvert\\,\\nabla\\ell_{i}(w_{s})-\\nabla\\ell_{i}(w_{s}(\\boldsymbol{r}_{-1,i}))\\boldsymbol{\\|}_{2}^{2}\\right]}\\\\ &{\\quad-\\displaystyle\\sum_{t=1}^{T}(n-T+\\pi(T-1,i))\\frac{\\alpha_{0}\\tau(T-1,i)}{4L}\\mathbb{E}_{\\theta}\\left[\\mu_{\\tau}(\\boldsymbol{r}_{-1,i})\\left\\|\\nabla\\ell_{i}(w_{\\tau}(\\boldsymbol{r}_{-1,i})-\\nabla\\ell_{i}(w_{s})\\right\\|_{2}^{2}\\right]}\\\\ &{\\quad-\\displaystyle\\sum_{t=1}^{T}\\frac{\\alpha_{0}\\mu}{4}\\mathbb{E}_{\\theta}\\left[\\|w_{t}-w_{*}\\|_{2}^{2}-\\displaystyle\\sum_{t=1}^{T}\\frac{\\alpha_{t}\\nu}{4}\\mathbb{E}_{\\theta}\\left\\|\\mu_{t}-q_{t}\\right\\|_{2}^{2}\\right.}\\\\ &{\\quad\\left.-\\frac{4T-\\mu}{2\\nu\\mu}\\mathbb{E}_{\\theta}\\left\\|\\boldsymbol{w}_{T}-\\boldsymbol{w}_{*}\\right\\|_{2}^{2}-\\frac{4T-1\\nu}{4L}\\mathbb{E}_{\\theta}\\left\\|\\|\\\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "The black lines will cancel given our conditions on $\\alpha$ . Substituting the definition of $\\gamma_{t}$ and moving the final non-positive terms on the last line, that is, (AT \u22121+aT )\u00b5E0\u2225wT\u2212w\u22c6\u222522 and (AT \u221214+aT )\u03bdE0 \u2225qT \u2212q\u22c6\u222522 to the left-hand side achieves the claim. \u53e3 ", "page_idx": 39}, {"type": "text", "text": "C.4 Modification for Unregularized Objectives ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "For completeness, we describe a modification of DRAGO for unregularized objectives, or (2) when $\\mu\\geq0$ and $\\nu\\geq0$ . The analysis follows similarly to the previous subsections (regarding the $\\mu,\\nu>0$ case), and we highlight the steps that differ in this subsection by presenting a slightly different upper bound on the gap criterion based on the modified primal and dual updates. This will result a different update for the sequence $(a_{t})_{t\\geq1}$ , subsequently affecting the rate. ", "page_idx": 39}, {"type": "text", "text": "C.4.1 Overview ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "The modified algorithm is nearly identical to Algorithm 1, except that the dual and primal updates can be written as ", "page_idx": 39}, {"type": "equation", "text": "$$\nq_{t}:=\\operatorname*{arg\\,max}_{q\\in\\mathcal{Q}}\\;a_{t}\\left\\langle v_{t}^{\\mathrm{D}},q\\right\\rangle-a_{t}\\nu D(q\\|q_{0})-(\\nu A_{t-1}+\\nu_{1})\\Delta_{D}(q,q_{t-1})\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "and ", "page_idx": 39}, {"type": "equation", "text": "$$\nv_{t}:=\\underset{w\\in\\mathcal{W}}{\\mathrm{arg}\\,\\mathrm{min}}\\;\\,a_{t}\\left<v_{t}^{\\mathrm{P}},w\\right>+\\frac{a_{t}\\mu}{2}\\left<|w|\\right|_{2}^{2}+\\frac{C_{t-1}\\mu+\\mu_{1}}{2}\\left\\|w-w_{t-1}\\right\\|_{2}^{2}+\\frac{c_{t-1}\\mu+\\mu_{2}}{2}\\sum_{s=t-n/b}^{t-2}\\left\\|w-w_{t-1}\\right\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "respectively, and $\\mu_{1},\\mu_{2},\\nu_{1}\\geq0$ are to-be-set hyperparameters. When $\\nu>0$ , we may set $\\nu_{1}=0$ , and when $\\mu>0$ , we may set $\\mu_{1}=\\mu_{2}=0$ , which recover the Algorithm 1 updates exactly. While ", "page_idx": 39}, {"type": "text", "text": "we may set $\\nu_{1}=1$ when it is positive (and similarly for $\\mu_{1}$ and $\\mu_{2}$ ), they may be set to different values in order to balance the terms appearing in the rate below. As in Appx. C.1, we wish to upper bound the expectation of the quantity ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\gamma_{t}=a_{t}\\left(\\mathcal{L}(w_{t},q_{\\star})-\\mathcal{L}(w_{\\star},q_{t})-\\frac{\\mu}{2}\\left\\|w_{t}-w_{\\star}\\right\\|_{2}^{2}-\\frac{\\nu}{2}\\left\\|q_{t}-q_{\\star}\\right\\|_{2}^{2}\\right)\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "which is still non-negative in the case of $\\mu\\:=\\:0$ or $\\nu\\,=\\,0$ . By using an appropriate averaging sequence $(a_{t})_{t\\geq1}$ and defining $\\textstyle A_{T}\\,=\\,\\sum_{t=1}^{T}a_{t}$ , we upper bound $\\sum_{t=1}^{T}a_{t}\\mathbb{E}_{0}[\\gamma_{t}]$ (see Thm. 2) by a constant value independent of $T$ . R ecall that the batch size is  denoted by $b$ . As we derive in Appx. C.4.3, our final update on the $\\left(a_{t}\\right)$ sequence is ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\iota_{t}=\\operatorname*{min}\\left\\{\\frac{C_{t-1}\\mu+\\mu_{1}}{12e n q_{\\mathrm{max}}\\,L},\\left(1+\\frac{b}{n}\\right)a_{t-1},\\frac{b}{32n}\\frac{\\sqrt{\\left(A_{t-1}\\nu+\\nu_{1}\\right)\\operatorname*{min}\\left\\{C_{t-1}\\mu+\\mu_{1},c_{t-1}\\mu+\\mu_{2}\\right\\}}}{\\sqrt{n}G}\\right\\}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Observe that when $\\mu=0$ , we set $\\begin{array}{r}{a_{t}=\\frac{\\mu_{1}}{12e n q_{\\mathrm{max}}\\,L}}\\end{array}$ to achieve a $O(1/t)$ rate. We omit proofs in this subsection as they follow with the exact same steps as the corresponding lemmas in the strongly convex-strongly concave setting (which we point to for each result). ", "page_idx": 40}, {"type": "text", "text": "C.4.2 Upper Bound on Gap Criterion ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Following the steps of Appx. C.3.1 and Appx. C.3.2, we will first derive lower and upper bounds on $\\mathbb{E}_{t}[a_{t}\\mathcal{L}(w_{\\star},q_{t})]$ and $a_{t}\\mathcal{L}(w_{t},q_{\\star})$ and combine them to upper bound $a_{t}\\mathbb{E}_{t}\\left[\\gamma_{t}\\right]$ . Recalling that $\\mathbb{E}_{t}\\left[\\cdot\\right]$ denotes the condition expectation given $\\left(w_{t-1},q_{t-1}\\right)$ , and we can then take the marginal expectation to upper bound $a_{t}\\mathbb{E}_{0}\\left[\\gamma_{t}\\right]$ . The following lower bound is analogous to Lem. 11 and follows the exact same proof technique. ", "page_idx": 40}, {"type": "text", "text": "Lemma 16. For $t\\geq2$ , assuming that $\\begin{array}{r}{a_{t}\\leq\\frac{C_{t-1}\\mu+\\mu_{1}}{12e n q_{\\mathrm{max}}\\,L}}\\end{array}$ and $a_{t}\\leq(1+b/n)a_{t-1}$ , we have that: ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\mathbb{E}_{t}|a_{t}\\zeta(w_{t},\\phi,q)|}\\\\ &{\\leq-a_{t}R_{t}[\\eta^{T}(t(w_{t}))]-\\frac{a_{t}}{2}\\displaystyle\\sum_{i=1}^{n}q_{i},\\mathbb{E}_{t}|\\nabla\\ell_{i}(w_{t})-\\nabla\\ell_{i}(w_{t})|_{2}^{2}+\\frac{a_{t}\\mu}{2}\\|w_{t}\\|_{2}^{2}}\\\\ &{~+\\frac{a_{t-1}}{4L}\\displaystyle\\sum_{i=1}^{n}q_{i-1}|\\,\\nabla\\ell_{i}(w_{t-1})-\\nabla\\ell_{i}(w_{t})|_{2}^{2}+\\sum_{i=1}^{n}\\frac{a_{t}(\\nu_{i}-2_{2})}{4L}q_{i-1,i}\\,\\|\\nabla\\ell_{i}(w_{t-2,i})-\\nabla\\ell_{i}(w_{t},\\phi-\\phi_{t})\\|_{2}}\\\\ &{+\\frac{3a_{t}\\tilde{Q}^{2}\\tilde{Q}^{2}(t_{1-1})}{L}\\|q_{i-1}-\\hat{q}_{t-2}\\|_{2}^{2}}\\\\ &{-\\alpha_{t-1}\\mu_{t}^{-1}(\\gamma+\\ell_{t})\\tau_{q}-\\hat{y}_{t-1}^{-1}\\hat{w}_{t-1},w_{t}-w_{t}\\rangle\\big\\|+a_{t-1}\\,\\big\\langle\\nabla\\ell(w_{t-1})^{\\top}q_{t-1}-\\hat{y}_{t-2}^{\\top}\\hat{w}_{t-2},w_{t}-w_{t-1}\\big\\rangle}\\\\ &{-\\alpha_{t}\\xi_{\\xi}\\big[\\langle\\nabla\\ell(w_{t})^{\\top}q_{i}\\rangle-\\frac{A_{t}\\mu}{2L}+\\beta_{t}+\\mu_{2}}\\\\ &{+a_{t}\\nu_{\\xi_{1}}[\\partial(q_{i}|\\phi|)]-\\frac{\\lambda\\mu_{t}}{2}\\displaystyle\\sum_{i=1}^{n}\\hat{Q}_{t}\\,\\|w_{t}-w_{t}\\|_{2}^{2}}\\\\ &{-\\frac{C_{t-1}\\mu_{t}+\\mu_{t}}{2}\\,\\|w_{t}-w_{t-1}\\|_{2}^{2}+\\frac{C_{t-1}\\mu_{t}+\\mu\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Similarly, following the same steps as Lem. 12, one can derive the following upper bound. ", "page_idx": 40}, {"type": "text", "text": "Lemma 17. For $t\\geq2$ , we have that: ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\iota_{t}\\mathcal{L}(w_{t},q_{\\star})\\leq a_{t}q_{\\star}^{\\textsf{T}}(\\ell(w_{t})-v_{t}^{\\mathrm{D}})+(A_{t-1}\\nu+\\nu_{1})\\Delta_{D}\\big(q_{\\star},q_{t-1}\\big)}\\\\ &{\\qquad\\qquad\\qquad+\\,a_{t}q_{t}^{\\textsf{T}}v_{t}^{\\mathrm{D}}-a_{t}\\nu D\\big(q_{t}||q_{0}\\big)-\\big(A_{t-1}\\nu+\\nu_{1}\\big)\\Delta_{D}\\big(q_{t},q_{t-1}\\big)-\\big(A_{t}\\nu+\\nu_{1}\\big)\\Delta_{D}\\big(q_{\\star},q_{t}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "By combining Lem. 17 and Lem. 16, we can upper bound the quantity $\\mathbb{E}_{t}\\left[a_{t}(\\mathcal{L}(w_{t},q_{\\star})-\\mathcal{L}(w_{\\star},q_{t})\\right]$ . Consequently, the following result follows the same steps as Lem. 13. As before, we identify telescoping terms in blue, non-positive terms in red, where green term is bounded after aggregation across time $t$ . ", "page_idx": 40}, {"type": "text", "text": "Lemma 18. Assume that $\\begin{array}{r}{a_{t-1}\\leq\\frac{C_{t-1}\\mu+\\mu_{1}}{12e L n q_{\\mathrm{max}}}}\\end{array}$ and $a_{t}\\leq(1+b/n)a_{t-1}$ . For $t>2$ , we have that: ", "page_idx": 41}, {"type": "text", "text": "$\\begin{array}{r l r}&{}&{\\mathbb{E}_{\\|\\gamma\\|}=\\mathbb{E}_{\\|\\gamma\\|}\\sum_{i,j}[\\sigma_{i}(C(\\eta_{1},\\eta_{2})-C(\\eta_{1},\\eta_{3})-\\sigma_{i}(\\eta_{2})\\Big\\lVert\\eta_{1}-\\eta_{1}\\Big\\rVert_{2}^{2}-\\frac{\\alpha_{j}\\nu}{\\mu_{1}+\\eta_{2}}\\Big\\lVert\\eta_{1}-\\eta_{1}\\Big\\rVert_{2}^{2}]}\\\\ &{}&{\\le_{\\sigma_{i}}\\mathbb{E}_{\\|\\gamma\\|}[(\\eta_{1}-\\eta_{1})^{\\top}(\\ell(w_{1})-\\ell_{i})]-\\alpha_{i-1}(\\eta_{2}-\\eta_{1})^{\\top}(\\ell(w_{1})-\\hat{\\lambda})}\\\\ &{}&{+(\\lambda_{1}-\\nu+\\eta_{1})\\Delta_{\\mathcal{D}}(\\eta_{1},\\eta_{2})-(\\lambda_{1}\\nu+\\eta_{1})\\mathbb{E}_{\\|\\gamma\\|}\\sum_{i,j}[\\Delta_{\\mathcal{D}}(\\eta_{1},\\eta_{2})]}&{\\quad\\mathrm{(for~})}\\\\ &{}&{-\\frac{\\alpha_{i}}{2\\sum_{j}}\\sum_{i,j}\\Big[\\sum_{i=1,1}^{\\infty}\\mathbb{E}_{\\|\\gamma\\|}[\\eta_{1}(w_{1})-\\nabla\\ell_{i}(w_{1})]\\Big]^{2}\\Bigg]}\\\\ &{}&{+\\frac{\\alpha_{i-1}}{4L}\\displaystyle\\frac{\\sum_{j}\\eta_{1}(\\ell(w_{1})-\\eta_{1})}{\\mu_{1}}\\mathbb{E}_{\\|\\gamma\\|}[\\ell_{i}(w_{1})-\\nabla\\ell_{i}(w_{1})]_{2}^{2}+\\displaystyle\\sum_{i=1}^{n}\\frac{\\alpha_{i}(\\eta_{2}-2\\eta_{2})}{4L}\\mathbb{E}_{\\|\\gamma\\|}[\\nabla\\ell_{i}(w_{1}-2\\eta_{2})-\\nabla\\ell_{i}(\\ell_{i})}\\\\ &{}&{+\\theta}\\\\ &{}&{-\\theta_{i}\\frac{\\gamma_{j}}{\\mu_{1}}\\Big[\\nabla\\ell(w_{1})^{\\top}q_{i}-\\hat{\\lambda})\\Big\\{\\bar{\\eta}_{1}^{$ ) ) ) \u22c6)) ) ) \u22280 (72)   \n$\\begin{array}{r l}&{+\\displaystyle\\frac{n a_{t-1}^{2}G^{2}}{A_{t-2}\\nu+\\nu_{1}}\\sum_{\\tau=t-n/b}^{t-3}\\|w_{t-1}-w_{\\tau\\vee0}\\|_{2}^{2}-\\displaystyle\\frac{c_{t-1}\\mu+\\mu_{2}}{2}\\sum_{\\tau=t-n/b}^{t-2}\\mathbb{E}_{t}\\left\\|w_{t}-w_{\\tau\\vee0}\\right\\|_{2}^{2}}\\\\ &{+\\displaystyle\\frac{n a_{t-1}^{2}G^{2}}{A_{t-2}\\nu+\\nu_{1}}\\left\\|w_{t-1}-w_{t-2}\\right\\|_{2}^{2}-\\displaystyle\\frac{C_{t-1}\\mu+\\mu_{1}}{4}\\mathbb{E}_{t}\\left\\|w_{t}-w_{t-1}\\right\\|_{2}^{2}}\\\\ &{-\\displaystyle\\frac{a_{t}\\mu}{2}\\left\\|w_{t}-w_{\\star}\\right\\|_{2}^{2}-\\displaystyle\\frac{a_{t}\\nu}{2}\\mathbb{E}_{t}\\left\\|q_{t}-q_{\\star}\\right\\|_{2}^{2}-\\displaystyle\\frac{A_{t-1}\\nu+\\nu_{1}}{2}\\mathbb{E}_{t}\\left[\\Delta_{D}(q_{t},q_{t-1})\\right].}\\end{array}$ (73) (74) (75) ", "page_idx": 41}, {"type": "text", "text": "For $t\\,=\\,2$ , the above holds with the addition of the term $\\frac{n G^{2}}{\\nu+\\nu_{1}}\\left\\|\\boldsymbol{w}_{1}-\\boldsymbol{w}_{0}\\right\\|_{2}^{2}$ and without any term including $A_{t-2}$ in the denominator. ", "page_idx": 41}, {"type": "text", "text": "We then select constants to achieve the desired telescoping in each line of Lem. 18. ", "page_idx": 41}, {"type": "text", "text": "C.4.3 Determining Constants ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "We now select the constants $(\\mu_{1},\\mu_{2},\\nu_{1})$ , and the sequences $\\left(a_{t}\\right)$ and $\\left(c_{t}\\right)$ to complete the main part of the analysis. ", "page_idx": 41}, {"type": "text", "text": "In the statement of Lem. 18, the lines above (71) will telescope without additional conditions. For (72), we set $c_{t}\\,=\\,a_{t}/m$ for some parameter $m$ (just as in Appx. C.3.3). Note that this condition does not need to be checked when $n/b<1$ , as the additional sum term over $\\tau$ will not be included in the update. Counting all the terms that will appear when matched on the index $t-1$ , we have the condition that ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\underbrace{u\\left[-\\frac{a_{t-1}}{4}-A_{t-1}+C_{t-1}+\\sum_{s=t+1}^{t+n/b-1}a_{s}/m\\right]}_{\\le0}+\\underbrace{-(\\mu_{1}+\\mu_{2}(n/b-1))+\\mu_{1}+(n/b-1)\\mu_{2}}_{=0}\\le0,\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where the first underbrace is non-positive based on the choice of $m$ selected in Appx. C, and the equality is satisfied for all values of $\\mu_{1}$ and $\\mu_{2}$ . Lines (73) and (74) yield the conditions ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\frac{n a_{t}^{2}G^{2}}{A_{t-1}\\nu+\\nu_{1}}\\leq\\frac{C_{t-1}\\mu+\\mu_{1}}{4}\\;\\mathrm{and}\\;\\frac{n a_{t}^{2}G^{2}}{A_{t-1}\\nu+\\nu_{1}}\\leq\\frac{c_{t-1}\\mu+\\mu_{2}}{2}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "for the telescoping to be achieved, which can equivalently be rewritten as ", "page_idx": 42}, {"type": "equation", "text": "$$\na_{t}\\leq\\sqrt{\\frac{(A_{t-1}\\nu+\\nu_{1})(C_{t-1}\\mu+\\mu_{1})}{4n G^{2}}}\\mathrm{~and~}a_{t}\\leq\\sqrt{\\frac{(A_{t-1}\\nu+\\nu_{1})(c_{t-1}\\mu+\\mu_{2})}{2n G^{2}}}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "These can be accomplished by setting ", "page_idx": 42}, {"type": "equation", "text": "$$\na_{t}\\leq\\frac{\\sqrt{\\left(A_{t-1}\\nu+\\nu_{1}\\right)\\operatorname*{min}{\\left\\{C_{t-1}\\mu+\\mu_{1},c_{t-1}\\mu+\\mu_{2}\\right\\}}}}{2\\sqrt{n}G}.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "We must also handle the term C3tn\u2212G12\u00b5a+t2\u2212\u00b511 \u2225qt\u22121 \u2212q\u02c6t\u22122\u222522. By the argument of Lem. 10 using at2 instead of $a_{t}$ , we have that when summing over $t$ , we have that ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{t=1}^{T}\\frac{3n G^{2}a_{t}^{2}}{\\mu_{t}}\\left\\|q_{t}-\\hat{q}_{t-1}\\right\\|_{2}^{2}\\leq\\frac{3n G^{2}}{C_{t-1}\\mu+\\mu_{1}}\\cdot3e^{4}M^{2}\\sum_{t=1}^{T}a_{t}^{2}\\left\\|q_{t}-q_{t-1}\\right\\|_{2}^{2}}}\\\\ &{}&{\\leq\\frac{512n G^{2}M^{2}}{C_{t-1}\\mu+\\mu_{1}}\\sum_{t=1}^{T}a_{t}^{2}\\left\\|q_{t}-q_{t-1}\\right\\|_{2}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where $M=n/b$ is the number of blocks, and the $e^{4}$ term appears from the squared constants (as opposed to $e^{2}$ ). Using that we will sum the non-positive terms $\\begin{array}{r}{\\sum_{t=1}^{T}A_{t-1}\\nu\\mathbb{E}_{t}[\\Delta_{D}(q_{t},q_{t-1},])}\\end{array}$ , so that the final condition needed to cancel this term is ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\frac{512n G^{2}M^{2}}{C_{t-1}\\mu+\\mu_{1}}a_{t}^{2}\\leq\\frac{A_{t-1}\\nu+\\nu_{1}}{2}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "which can also be rewritten as ", "page_idx": 42}, {"type": "equation", "text": "$$\na_{t}\\le\\frac{1}{32M}\\sqrt{\\frac{(A_{t-1}\\nu+\\nu_{1})(C_{t-1}\\mu+\\mu_{1})}{n G^{2}}}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Thus, combining the previous conditions, our final update on the $\\left(a_{t}\\right)$ sequence is ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\iota_{t}=\\operatorname*{min}\\left\\{\\frac{C_{t-1}\\mu+\\mu_{1}}{12e n q_{\\mathrm{max}}\\,L},\\left(1+\\frac{b}{n}\\right)a_{t-1},\\frac{b}{32n}\\frac{\\sqrt{(A_{t-1}\\nu+\\nu_{1})\\operatorname*{min}\\left\\{C_{t-1}\\mu+\\mu_{1},c_{t-1}\\mu+\\mu_{2}\\right\\}}}{\\sqrt{n}G}\\right\\},\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "as alluded to in Appx. C.4.1. ", "page_idx": 42}, {"type": "text", "text": "D Implementation Details ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "In this section, we provide additional background on implementing DRAGO in practice. This involves a description of the algorithm amenable for direct translation into code and procedures for computing the dual proximal mapping for common uncertainty sets and penalties. We assume in this section that $\\mathcal{W}=\\mathbb{R}^{d}$ and provide multiple options for the uncertainty set $\\mathcal{Q}$ . ", "page_idx": 43}, {"type": "text", "text": "D.1 Algorithm Description ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "The full algorithm is given in Algorithm 2. We first describe the notation. Recall that $M=n/b$ , or the number of blocks. We partition $[n]$ into $(B_{1},\\hdots,B_{M})$ , where each $B_{K}$ denotes a $b$ -length list of contiguous indices. For any matrix $u\\in\\mathbb{R}^{n\\times m}$ (including $m=1,$ ), we denote by $u[B_{K}]\\in\\mathbb{R}^{b\\times m}$ the rows of $u$ corresponding to the indices in $B_{K}$ . Finally, for a vector $s\\in\\mathbb{R}^{b}$ , we denote by $s e_{B_{K}}$ the vector that contains $s_{k}$ in indices $k\\,\\in\\,B_{K}$ , and has zeros elsewhere. Next, we comment on particular aspects regarding the implementation version as compared to Algorithm 1 (Sec. 2). ", "page_idx": 43}, {"type": "text", "text": "\u2022 We store two versions of each table, specifically $\\hat{\\ell},\\hat{\\ell}_{1}\\in\\mathbb{R}^{n}$ , $\\hat{g}_{1},\\hat{g}_{2}\\in\\mathbb{R}^{n\\times d}$ , and $\\hat{q}_{1},\\hat{q}_{2}\\in$ $\\mathbb{R}^{n}$ . For any iterate $t$ , these variables are meant to store $\\hat{\\ell}_{t},\\hat{\\ell}_{t-1}\\in\\mathbb{R}^{n}$ , $\\hat{g}_{t-1},\\hat{g}_{t-2}\\in\\mathbb{R}^{n\\times d}$ , and $\\hat{q}_{t-1},\\hat{q}_{t-2}\\in\\mathbb{R}^{n}$ .   \n\u2022 The quantities $\\hat{g}_{\\mathrm{agg}}\\,\\in\\,\\mathbb{R}^{d}$ and $\\hat{w}_{\\mathrm{agg}}\\,\\in\\,\\mathbb{R}^{d}$ are introduced as to not recompute the sums in the primal update on each iteration (which would cost $O(n d)$ operations). Instead, these aggregates are updated using $O(b d)$ operations.   \n\u2022 The loss and gradient tables are not updated immediately after the primal update. However, the values that fill the tables are computed, and the update occurs at the end of the loop. This is because $\\hat{\\ell}[B_{K_{t}}]$ is used to fill $\\hat{\\ell}_{1}[B_{K_{t}}]$ at the end of the loop, we we must maintain knowledge of $\\hat{\\ell}[B_{K_{t}}]$ temporarily.   \n\u2022 While the proximal operator is specified for the primal in the case of $\\mathcal{W}=\\mathbb{R}^{d}$ , the proximal operator for the dual os computed by a subroutine DualProx, which we describe in the next subsection. ", "page_idx": 43}, {"type": "text", "text": "D.2 Solving the Maximization Problem ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "As discussed in Appx. B, the primary examples of DRO uncertainty sets $\\mathcal{Q}$ are balls in $f$ -divergence (specifically, KL and $\\chi^{2}$ ) and spectral risk measure sets. For the penalty $D$ , it is also common to use $f$ -divergences. We review these concepts in this section and provide recipes for computing the maximization problem. ", "page_idx": 43}, {"type": "text", "text": "$f$ -Divergences We first recall the definition of $f$ -divergences used throughout this section. ", "page_idx": 43}, {"type": "text", "text": "Definition 19. Let $f:[0,\\infty)\\mapsto\\mathbb{R}\\cup\\{+\\infty\\}$ be a convex function such that $f(1)=0$ , $f(x)$ is finite for $x\\,>\\,0$ , and $\\textstyle\\operatorname*{lim}_{x\\to0^{+}}f(x)\\,=\\,0$ . Let $q$ and $\\bar{q}$ be two probability mass functions defined on $n$ atoms. The $f$ -divergence from $q$ to $\\bar{q}$ generated by this function $f$ is given by ", "page_idx": 43}, {"type": "equation", "text": "$$\nD_{f}(q\\|\\bar{q}):=\\sum_{i=1}^{n}f\\left(\\frac{q_{i}}{\\bar{q}_{i}}\\right)\\bar{q}_{i},\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where we define $0f\\left(0/0\\right):=0$ . For any $i$ such that $\\bar{q}_{i}=0$ but $q_{i}>0$ , we define $D_{f}(q\\|\\bar{q})=:+\\infty$ . ", "page_idx": 43}, {"type": "text", "text": "The two running examples we use are the $\\chi^{2}$ -divergence generated by $f_{\\chi^{2}}(x)\\,=\\,x^{2}\\,-\\,1$ and the KL divergence generated by $f_{\\mathrm{KL}}(x)=x\\ln{\\bar{x}}$ on $(0,\\infty)$ and define $0\\ln0=0$ . For any convex set $\\mathcal{X}\\subseteq\\mathbb{R}^{k}$ , we also introduce the convex indicator function ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\iota_{\\mathcal{X}}(x):=\\left\\{\\!\\!\\begin{array}{l l}{0}&{\\mathrm{~if~}x\\in\\mathcal{X}}\\\\ {1}&{\\mathrm{~otherwise~}}\\end{array}\\!\\!.\\right.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "In either of the two cases below, we select the penalty $D(q\\|\\mathbf{1}/n)\\ =\\ D_{f}(q\\|\\mathbf{1}/n)$ to be an $f$ - divergence. Denote in addition $f^{*}$ as the Fenchel conjugate of $f$ . ", "page_idx": 43}, {"type": "text", "text": "Input: Learning rate parameter $\\alpha>0$ , batch size $b\\in\\{1,\\ldots,n\\}$ , number of iterations $T$ . Initialization: ", "page_idx": 44}, {"type": "text", "text": "$w\\leftarrow0_{d}$ and $q\\gets\\mathbf{1}/n$   \n$\\begin{array}{r}{\\hat{\\ell}\\gets\\ell(w),\\hat{\\ell}_{1}\\gets\\ell(w),\\hat{g}_{1}\\gets\\nabla\\ell(w),\\hat{g}_{2}\\gets\\nabla\\ell(w),\\hat{q}_{1}\\gets q\\mathrm{~and~}\\hat{q}_{2}\\gets q}\\end{array}$   \n$\\hat{w}_{K}\\gets w$ for $K\\in\\{1,\\ldots,M\\}$ for $M=n/b$   \n$\\hat{g}_{\\mathrm{agg}}\\leftarrow\\hat{g}_{1}^{\\top}\\hat{q}_{1}$ and $\\begin{array}{r}{\\hat{w}_{\\mathrm{agg}}\\gets\\sum_{K=1}^{M}\\hat{w}_{K}}\\end{array}$   \n$\\bar{\\beta}=1/[16\\alpha(1+\\alpha)(n/b-1)^{2}]$ if $n/b>1$ and 0 otherwise   \nfor $t=1$ to $T$ do Sample blocks $I_{t}$ and $J_{t}$ uniformly on $[n/b]$ and compute $K_{t}=t\\;\\;\\mathrm{mod}\\;\\left(n/b\\right)+1$ $\\beta_{t}\\leftarrow(1-(1+\\alpha)^{1-t})/(\\alpha(1+\\alpha))$ Primal Update: $g\\gets[\\nabla\\ell_{i}(w)]_{i\\in B_{I_{t}}}\\in\\mathbb{R}^{b\\times d}$ and $\\begin{array}{r}{v^{\\mathrm{P}}\\leftarrow\\hat{g}_{\\mathrm{agg}}+\\frac{1}{1+\\alpha}\\delta^{\\mathrm{P}}}\\end{array}$ . $\\begin{array}{r}{w\\leftarrow\\frac{1}{(1+\\beta_{t})}\\left((\\beta_{t}-\\bar{\\beta}(M-1))w+\\bar{\\beta}(\\hat{w}_{\\mathrm{agg}}-\\hat{w_{K_{t}}})-v^{\\mathrm{P}}/\\mu\\right)}\\end{array}$ $\\hat{w}_{\\mathrm{agg}}\\leftarrow\\hat{w}_{\\mathrm{agg}}+w-\\hat{w}_{K_{t}}$ and $\\hat{w}_{K_{t}}\\gets w$ Compute Loss and Gradient Table Updates: $\\left(l_{t},g_{t}\\right)\\gets[\\ell_{k}(w),\\nabla\\ell_{k}(w)]_{k\\in B_{K_{t}}}\\in\\mathbb{R}^{b}\\times\\mathbb{R}^{b\\times d}$ Dual Update: $l\\leftarrow[\\ell_{j}(w)]_{j\\in B_{J_{t}}}\\in\\mathbb{R}^{b}$ $\\delta^{\\mathrm{D}}\\gets M(l-\\hat{\\ell}_{1}[J_{t}])$ and $\\begin{array}{r}{v^{\\mathrm{D}}\\leftarrow\\hat{\\ell}+(l-\\hat{\\ell}[B_{K_{t}}])e_{B_{K_{t}}}+\\frac{1}{1+\\alpha}\\delta^{\\mathrm{D}}e_{B_{J_{t}}}}\\end{array}$ $q\\gets\\mathrm{DualProx}(q,v^{\\mathrm{D}},\\beta_{t})=\\arg\\operatorname*{max}_{\\bar{q}\\in\\mathcal{Q}}\\big\\{\\left\\langle v^{\\mathrm{D}},\\bar{q}\\right\\rangle-\\nu D(\\bar{q}\\|\\mathbf{1}_{n}/n)-\\beta_{t}\\nu\\Delta_{D}(\\bar{q},q)\\big\\}$ Update All Tables: $\\hat{g}_{2}[B_{K_{t}}]\\gets\\hat{g}_{1}[B_{K_{t}}]$ and $\\hat{g}_{1}[B_{K_{t}}]\\gets g_{t}$ $\\hat{\\ell}_{1}[B_{K_{t}}]\\gets\\hat{\\ell}[B_{K_{t}}]$ and $\\hat{\\ell}[B_{K_{t}}]\\gets l_{t}$ $\\hat{q}_{2}[B_{K_{t}}]\\gets\\hat{q}_{1}[B_{K_{t}}]$ and $\\hat{q}_{1}[B_{K_{t}}]\\gets q[B_{K_{t}}]$ $\\hat{g}_{\\mathrm{agg}}\\gets\\hat{g}_{\\mathrm{agg}}+\\hat{g}_{1}[B_{K_{t}}]^{\\top}\\hat{q}_{1}[B_{K_{t}}]-\\hat{g}_{2}[B_{K_{t}}]^{\\top}\\hat{q}_{2}[B_{K_{t}}]$   \nend for ", "page_idx": 44}, {"type": "text", "text": "return $(w,q)$ . ", "page_idx": 44}, {"type": "text", "text": "D.2.1 Spectral Risk Measure Uncertainty Sets ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "As in Appx. B, the spectral risk measure uncertainty set is defined by a set of non-decreasing, nonnegative weights $\\sigma=(\\sigma_{1},\\ldots,\\sigma_{n})$ that sum to one. Our uncertainty set is given by ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\mathcal{Q}=\\mathcal{Q}(\\sigma):=\\mathrm{conv}\\left(\\{\\mathrm{permutations~of~}\\sigma)\\}\\right),\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "and we use $D_{f}$ has the penalty for either $f_{\\chi^{2}}$ or $f_{\\mathrm{KL}}$ . The set $\\mathcal{Q}(\\sigma)$ is referred to the permutahedron on $\\sigma$ . In this case, the maximization problem can be dualized and solved via the following result. ", "page_idx": 44}, {"type": "text", "text": "Proposition 20. [Mehta et al., 2024, Proposition $3J$ Let $l\\in\\mathbb{R}^{n}$ be a vector and $\\pi$ be a permutation that sorts its entries in non-decreasing order, i.e., $l_{\\pi(1)}\\le\\ldots\\le l_{\\pi(n)}$ . Consider a function $f$ strictly convex with strictly convex conjugate defining a divergence $D_{f}$ . Then, the maximization over the permutahedron subject to the shift penalty can be expressed as ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{q\\in\\mathcal{Q}(\\sigma)}\\left\\{q^{\\top}l-\\nu D_{f}(q\\|\\mathbf{1}_{n}/n)\\right\\}=\\operatorname*{min}_{z\\in\\mathbb{R}^{n}\\atop z_{1}\\leq\\ldots\\leq z_{n}}\\sum_{i=1}^{n}g_{i}(z\\,;l),\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where we define $\\begin{array}{r}{g_{i}(z\\,;\\,l):=\\sigma_{i}z+\\frac{\\nu}{n}\\,\\,f^{*}\\left((l_{\\pi(i)}-z)/\\nu\\right)}\\end{array}$ . The optima of both problems, denoted ", "page_idx": 44}, {"type": "equation", "text": "$$\nz^{\\mathrm{opt}}(l)=\\underset{z_{1}\\leq...\\leq z_{n}}{\\arg\\operatorname*{min}}\\sum_{i=1}^{n}g_{i}(z;l),\\;q^{\\mathrm{opt}}=\\underset{q\\in\\mathcal{Q}(\\sigma)}{\\arg\\operatorname*{max}}\\,q^{\\top}l-\\nu D_{f}(q\\|\\mathbf{1}_{n}/n),\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "are related as $q^{\\mathrm{opt}}(l)=\\nabla(\\nu D_{f}(\\cdot\\|\\mathbf{1}_{n}/n))^{*}(l-z_{\\pi^{-1}}^{\\mathrm{opt}}(l))$ , that is, ", "page_idx": 45}, {"type": "equation", "text": "$$\nq_{i}^{\\mathrm{opt}}(l)=\\frac{1}{n}[f^{*}]^{\\prime}\\left(\\frac{1}{\\nu}(l_{i}-z_{\\pi^{-1}(i)}^{\\mathrm{opt}}(l))\\right).\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "As described in Mehta et al. [2024, Appendix C], the minimization problem (78) is an exact instance of isotonic regression and can be solved efficiently with the pool adjacent violators (PAV) algorithm. ", "page_idx": 45}, {"type": "text", "text": "D.2.2 Divergence-Ball Uncertainty Sets ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Another common uncertainty set format is a ball in $f$ -divergence, or ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\mathcal{Q}=\\mathcal{Q}(\\rho):=\\left\\{q\\in\\mathbb{R}^{n}:D_{f}(q\\|\\mathbf{1}/n)\\leq\\rho,q\\geq0,\\mathrm{~and~}\\mathbf{1}^{\\top}q=1\\right\\}.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "We describe the case of the rescaled $\\chi^{2}$ -divergence in particular, in which the feasible set is an $\\ell_{2}$ -ball intersected with the probability simplex. Given a vector $l\\in\\mathbb{R}^{n}$ , we aim to compute the mapping ", "page_idx": 45}, {"type": "equation", "text": "$$\nl\\mapsto\\operatorname*{arg\\,max}_{q\\in\\mathcal{P}_{n}}\\;\\;\\;\\langle l,q\\rangle-\\frac{\\nu}{2}\\left\\|q-\\mathbf{1}/n\\right\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where $\\mathcal{P}_{n}:=\\left\\{q\\in\\mathbb{R}^{n}:q\\geq0,\\mathbf{1}^{\\top}q=1\\right\\}$ denotes the $n$ -dimensional probability simplex. We apply a similar approach to Namkoong and Duchi [2017], in which we take a partial dual of the problem above. Indeed, note first that for any $q\\in\\mathcal{P}_{n}$ , we have that $\\begin{array}{r}{\\frac{1}{2}\\left\\|{q-\\mathbf{1}_{n}}/{\\bar{n}}\\right\\|_{2}^{2}=\\frac{1}{2}\\left\\|{q}\\right\\|_{2}^{2}-\\frac{1}{2n}}\\end{array}$ . Thus, the optimal solution to (80) can be computed by solving ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{q\\in\\mathcal{P}_{n}}\\operatorname*{min}_{\\lambda\\geq0}\\left\\langle l,q\\right\\rangle-\\frac{\\nu}{2}\\left\\|q\\right\\|_{2}^{2}-\\lambda\\left(\\frac{1}{2}\\left\\|q\\right\\|_{2}^{2}-\\rho-\\frac{1}{2n}\\right),\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "or equivalently, by strong duality via Slater\u2019s condition, solving ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\lambda\\geq0}\\left[f(\\lambda):=(\\nu+\\lambda)\\operatorname*{min}_{q\\in\\mathcal{P}_{n}}\\frac{1}{2}\\left\\|q-l/(\\nu+\\lambda)\\right\\|_{2}^{2}-\\lambda\\left(\\rho+\\frac{1}{2n}\\right)-\\frac{1}{2(\\nu+\\lambda)}\\left\\|l\\right\\|_{2}^{2}\\right].\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Notice that evaluation of the outer objective itself requires Euclidean projection onto the probability simplex as a subroutine, after which the maximization problem can be computed via the bisection method, as it is a univariate concave maximization problem over a convex set. In order to determine which half to remove in the bisection search, we also compute the derivative of $\\lambda\\mapsto f(\\lambda)$ , which is given by ", "page_idx": 45}, {"type": "equation", "text": "$$\nf^{\\prime}(\\lambda):=\\frac12\\left\\|q^{\\mathrm{opt}}(\\lambda)\\right\\|_{2}^{2}-\\rho-\\frac1{2n},\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where $q^{\\mathrm{opt}}(\\lambda)$ achieves the minimum in $\\begin{array}{r}{\\operatorname*{min}_{q\\in\\mathcal{P}_{n}}\\frac{1}{2}\\left\\|q-l/(\\nu+\\lambda)\\right\\|_{2}^{2}}\\end{array}$ for a fixed $\\lambda\\geq0$ . For projection onto the probability simplex, we apply Algorithm 1 from Condat [2016], which is a solution relying on sorting the projected vector. The overall method consists of three steps. ", "page_idx": 45}, {"type": "text", "text": "1. Sorting: Projection onto the simplex relies on sorting the vector $l/(\\nu+\\lambda)$ on each evaluation. However, because $l/(\\nu+\\lambda)$ varies from evaluation to evaluation simply by multiplying by a positive scalar, we may pre-sort $l$ and use the same sorted indices on each evaluation of $(f(\\lambda),f^{\\prime}(\\lambda))$ listed below. ", "page_idx": 45}, {"type": "text", "text": "2. Two-Pointer Search: We find the upper and lower limits for $\\lambda$ by initializing $\\lambda_{\\operatorname*{min}}\\,=\\,0$ and $\\lambda_{\\operatorname*{max}}\\,=1$ , and repeatedly making the replacement $\\left(\\lambda_{\\operatorname*{min}}\\,,\\lambda_{\\operatorname*{max}}\\right)\\gets\\left(\\lambda_{\\operatorname*{max}}\\,,2\\lambda_{\\operatorname*{max}}\\right)$ until $f^{\\prime}(\\lambda_{\\mathrm{max}}\\,)<-\\varepsilon$ for some tolerance $\\varepsilon>0$ . This, along with $f^{\\prime}(\\lambda_{\\operatorname*{min}})>\\varepsilon$ indicates that the optimal value of $\\lambda$ lies within $(\\lambda_{\\operatorname*{min}}\\,,\\lambda_{\\operatorname*{max}}\\,)$ . For any $\\lambda$ with $|f^{\\prime}(\\lambda)|<\\varepsilon$ , we return the associated $q^{\\mathrm{opt}}(\\lambda)$ as the solution. ", "page_idx": 45}, {"type": "text", "text": "3. Binary Search: Finally, we repeatedly evaluate $f^{\\prime}(\\lambda)$ for $\\lambda\\,=\\,(\\lambda_{\\operatorname*{min}}\\,+\\,\\lambda_{\\operatorname*{max}}\\,)/2$ . If $f^{\\prime}(\\lambda)\\;>\\;\\varepsilon$ , we set $\\lambda_{\\operatorname*{min}}~\\leftarrow~\\lambda$ , whereas if $f^{\\prime}(\\lambda)\\;<\\;-\\varepsilon$ , then we set $\\lambda_{\\operatorname*{max}}~\\gets~\\lambda$ . We terminate when $\\lambda_{\\operatorname*{max}{}}-\\lambda_{\\operatorname*{min}{}}<\\varepsilon$ or $|f^{\\prime}(\\lambda)|<\\varepsilon$ . ", "page_idx": 45}, {"type": "text", "text": "The parameter $\\varepsilon$ is set to $10^{-10}$ in our experiments. Note that the same procedure can be used to compute the dual proximal operator in Algorithm 1. In particular, when $\\begin{array}{r}{\\bar{\\Delta_{D}}((\\r,q),q_{t-1})=\\frac{1}{2}\\left\\Vert q-q_{t}\\right\\Vert_{2}^{2}}\\end{array}$ , which is true when $D$ is the $\\chi^{2}$ -divergence, then ", "page_idx": 46}, {"type": "equation", "text": "$$\nq_{t}=\\operatorname*{arg\\,max}_{\\boldsymbol{q}\\in\\mathcal{P}_{n}}\\quad\\langle\\boldsymbol{l}+\\nu\\beta_{t}\\boldsymbol{q}_{t},\\boldsymbol{q}\\rangle-\\frac{\\nu(1+\\beta_{t})}{2}\\left\\|\\boldsymbol{q}-\\mathbf{1}/n\\right\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "which is a particular case of (80), and hence can be solved using the exact same procedure. The runtime of this subroutine is $O(n\\log n+n\\log(1/\\varepsilon))$ , accounting for both the initial sorting at $O(n\\log n)$ cost, and the $O(\\log(1/\\varepsilon))$ iterations of the exponential and binary searches. Each iteration requires a linear scan of $n$ elements at cost $O(n)$ . ", "page_idx": 46}, {"type": "text", "text": "Hardware Acceleration Finally, note that the computations in Appx. D.2 and Appx. D.2.2 involve primitives such as sorting, linear scanning through vectors, and binary search. Due to their serial nature (as opposed to algorithms that rely on highly parallelizable operations such as matrix multiplication), we also utilize just-in-time compilation on the CPU via the Numba package for increased efficiency. ", "page_idx": 46}, {"type": "table", "img_path": "ujk0XrNTQZ/tmp/48ae45720ed0c3ce719e598975b1b9d02bfec602445934ee0d24ceccf9536f5c.jpg", "table_caption": [], "table_footnote": ["Table 5: Dataset attributes such as sample size $n$ , parameter dimension $d$ , and sources. "], "page_idx": 47}, {"type": "text", "text": "E Experimental Details ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "We describe details of the experimental setup, including datasets, compute environment, and hyperparamater tuning. We largely maintain the benchmarks of Mehta et al. [2023]. ", "page_idx": 47}, {"type": "text", "text": "E.1 Datasets ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "The sample sizes, dimensions, and source of the datasets are summarized in Tab. 5. The tasks associated with each dataset are listed below. ", "page_idx": 47}, {"type": "text", "text": "(a) yacht: predicting the residuary resistance of a sailing yacht based on its physical attributes Tsanas and Xifara [2012].   \n(b) energy: predicting the cooling load of a building based on its physical attributes Baressi Segota et al. [2020].   \n(c) concrete: predicting the compressive strength of a concrete type based on its physical and chemical attributes Yeh [2006].   \n(d) kin8nm: predicting the distance of an 8 link all-revolute robot arm to a spatial endpoint [Akujuobi and Zhang, 2017].   \n(e) power: predicting net hourly electrical energy output of a power plant given environmental factors [Tu\u00a8fekci, 2014].   \n(f) acsincome: predicting income of US adults given features compiled from the American Community Survey (ACS) Public Use Microdata Sample (PUMS) [Ding et al., 2021].   \n(g) emotion: predicting the sentiment of sentence in the form of six emotions. Each input is a segment of text and we use a BERT neural network Devlin et al. [2019] as an initial feature map. This representation is fine-tuned using 2 epochs on a random half (8,000 examples) of the original emotion dataset, and then applied to the remaining half. We then apply principle components analysis (PCA) to reduce the dimension of each vector to 45. ", "page_idx": 47}, {"type": "text", "text": "E.2 Hyperparameter Selection ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "We fix a minibatch size of $64~\\mathrm{SGD}$ and an epoch length of $N\\,=\\,n$ for LSVRG. In practice, the regularization parameter $\\mu$ and shift cost $\\nu$ are tuned by a statistical metric, i.e. generalization error as measured on a validation set. ", "page_idx": 47}, {"type": "text", "text": "For the tuned hyperparameters, we use the following method. Let $k\\,\\in\\,\\{1,\\ldots,K\\}$ be a seed that determines algorithmic randomness. This corresponds to sampling a minibatch without replacement for SGD and SRDA and a single sampled index for LSVRG. Letting $\\mathcal{L}_{k}(\\eta)$ denote the average value of the training loss of the last ten passes using learning rate $\\eta$ and seed $k$ , the quantity ${\\mathcal{L}}(\\eta)=$ $\\textstyle\\frac{1}{K}\\sum_{k=1}^{K}{\\mathcal{L}}_{k}(\\eta)$ was minimized to select $\\eta$ . The learning rate $\\eta$ is chosen in the set $\\{1\\times10^{-4},3\\times$ $10^{-4}$ , $1\\times10^{-3}$ , $3\\times10^{-3}$ , $1\\times10^{-2}$ , $3\\!\\times\\!10^{-2},1\\!\\times\\!10^{-1},3\\!\\times\\!10^{-1},1\\!\\times\\!10^{0},3\\!\\times\\!10^{0}\\!\\}$ , with two orders of magnitude lower numbers used in acsincome due to its sparsity. We discard any learning rates that cause the optimizer to diverge for any seed. ", "page_idx": 47}, {"type": "image", "img_path": "ujk0XrNTQZ/tmp/23e600e7f7ac2cf70f7f524fa19e408a2e1609dbc5186b37c099f882e5179441.jpg", "img_caption": ["Figure 4: Benchmarks on the $\\chi^{2}$ Uncertainty Set. In both panels, the $y$ -axis measure the primal suboptimality gap, defined in (15). Individual plots correspond to particular datasets. Left: The $x$ -axis displays the number of individual first-order oracle queries to $\\{(\\boldsymbol{\\ell}_{i},\\nabla\\boldsymbol{\\ell}_{i})\\}_{i=1}^{n}$ . Right: The $x$ -axis displays wall-clock time. "], "img_footnote": [], "page_idx": 48}, {"type": "text", "text": "E.3 Compute Environment ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Experiments were run on a CPU workstation with an Intel i9 processor, a clock speed of 2.80GHz, 32 virtual cores, and 126G of memory. The code used in this project was written in Python 3 using the Numba packages for just-in-time compilation. Run-time experiments were conducted without CPU parallelism. The algorithms are primarily written in PyTorch and support automatic differentiation. ", "page_idx": 48}, {"type": "text", "text": "E.4 Additional Experiments ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "We explore the sensitivity of the results to alterations of the objective and algorithm hyperparameters. ", "page_idx": 48}, {"type": "text", "text": "Sensitivity to Uncertainty Set Choice In Sec. 4, we mainly show performance on spectral riskbased uncertainty sets, in particular the conditional value-at-risk (CVaR). In this section, we also consider $f$ -divergence ball-based uncertainty sets, with the procedure described in Appx. D.2.2. As in Namkoong and Duchi [2017], we use a radius that is inversely proportional to the sample size, namely $\\rho\\ {\\overset{=}{=}}\\ {\\frac{1}{n}}$ , and the strong convexity-strong concavity parameter $\\mu\\,=\\,\\nu\\,=\\,1$ . In Fig. 4, we demonstrate the performance of DRAGO with $b\\,=\\,1$ , $\\textit{b}=\\textsubscript{16}$ (as chosen heuristically), and $b=n/d$ . We compare against the biased stochastic gradient descent, which can be defined using oracle to compute the optimal dual variables given a vector of losses; however, note that LSVRG is designed only for spectral risk measures, so the method does not apply in the divergence ball setting. We observe that the optimization performance across both regression and multi-class classification tasks are qualitatively similar to that seen in Fig. 2 nad Fig. 3. The $b=1$ variant performs well on smaller datasets $(n\\leq1,000)$ , whereas the $b=16$ heuristic generally does not dominate in terms of gradient evaluations or wall time. While the number of gradient evaluations is significantly larger for the $b=n/d$ variant, implementation techniques such as just-in-time complication (see Appx. D) allow for efficient computation, resulting in better overall optimization performance as a function of wall time. ", "page_idx": 48}, {"type": "text", "text": "Sensitivity to Batch Size In Fig. 5, we consider the datasets with the largest ratio of $n$ to $d$ (hence the largest theoretically prescribed batch size) and assess the performance of DRAGO with smaller batch sizes. For both datasets, we have that in magnitude, $n/d\\approx1000$ . Intuitively, the smaller batch size methods would perform better in terms of oracle queries but the large batch methods would be more performant in terms of wall time. With only a batch size of $b=64$ , this variant of DRAGO generally matches the best-performing setting when viewed from either oracle calls or direct wall time. This is approximately $16\\times$ smaller than the $n/d$ benchmark, indicating that tuning the batch size can significantly reduce the memory overhead of the algorithm while increasing speed. ", "page_idx": 48}, {"type": "image", "img_path": "ujk0XrNTQZ/tmp/88ddef4ac905259d159af9184975e61148151fa67a3cf7b9b66548d7a6b1534f.jpg", "img_caption": ["Figure 5: DRAGO on varying batch sizes and strong convexity parameters. Each row indicates a dataset, where as each column denotes the $\\mathrm{CVaR}$ objective with the given regularization parameters. Top Rows: The $x_{\\mathrm{~\\,~}}$ -axis displays the number of individual first-order oracle queries to $\\{(\\bar{\\ell_{i}},\\bar{\\nabla}\\ell_{i})\\}_{i=1}^{n}$ . Bottom Rows: The $x$ -axis displays wall-clock time. "], "img_footnote": [], "page_idx": 49}, {"type": "text", "text": "F NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 50}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Justification: Sec. 3 for direct theoretical claims and Appx. B for detailed comparisons to other work. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 50}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 50}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Justification: See Sec. 1 and Sec. 5 for general points, whereas Sec. 4 mentions specific cases of algorithm performance. The assumptions made are otherwise standard. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \u201dLimitations\u201d section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 50}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 50}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 51}, {"type": "text", "text": "Justification: This is done for every theoretical statement. Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 51}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 51}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 51}, {"type": "text", "text": "Justification: Appx. D contains detailed descriptions of experimental details and code with an associated environment and quickstart guide is provided. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 51}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 52}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 52}, {"type": "text", "text": "Justification: The code is made public at https://github.com/ronakdm/drago. ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 52}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 52}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 52}, {"type": "text", "text": "Justification: See Appx. D and Appx. E for implementation and experiment details. Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 52}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 52}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 52}, {"type": "text", "text": "Justification: Training curves and hyperparameter selection experiments are averaged over multiple seeds, but error bars are not shown to make the plots visible. ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \u201dYes\u201d if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 52}, {"type": "text", "text": "", "page_idx": 53}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 53}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 53}, {"type": "text", "text": "Justification: Yes, as written in Appx. E. ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 53}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 53}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 53}, {"type": "text", "text": "Justification: NA ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 53}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 53}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 53}, {"type": "text", "text": "Justification: It is written after the main text of the paper. ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. \u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. ", "page_idx": 53}, {"type": "text", "text": "\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 54}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 54}, {"type": "text", "text": "Answer: [No] ", "page_idx": 54}, {"type": "text", "text": "Justification: Generative and related large-scale models are not used in this work. ", "page_idx": 54}, {"type": "text", "text": "Guidelines: ", "page_idx": 54}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 54}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 54}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 54}, {"type": "text", "text": "Justification: NA ", "page_idx": 54}, {"type": "text", "text": "Guidelines: ", "page_idx": 54}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/ datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 54}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 55}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 55}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 55}, {"type": "text", "text": "Justification: Documented code is provided. ", "page_idx": 55}, {"type": "text", "text": "Guidelines: ", "page_idx": 55}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 55}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 55}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 55}, {"type": "text", "text": "Justification: Human participants are not used. ", "page_idx": 55}, {"type": "text", "text": "Guidelines: ", "page_idx": 55}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 55}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 55}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 55}, {"type": "text", "text": "Justification: No IRM approvals were needed for this work. ", "page_idx": 55}, {"type": "text", "text": "Guidelines: ", "page_idx": 55}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 55}]