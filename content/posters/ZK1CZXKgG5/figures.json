[{"figure_path": "ZK1CZXKgG5/figures/figures_1_1.jpg", "caption": "Figure 1: (a) Illustration of a video sequence. Given the initial prompts, we respectively plot consistency curves between prompts of two modalities and the subsequent searched target. (b) Framework of previous vision-language trackers (VLTs). They primarily obtain tracking results by matching the search image with the initial prompts based on similarity. (c) Framework of our proposed MemVLT (left) by modeling Complementary Learning Systems (CLS) Theory (right). MemVLT effectively models the storage and interaction of long-term and short-term memories, resulting in prompts that adapt to the search image.", "description": "This figure illustrates the limitations of existing vision-language tracking (VLT) methods and introduces the proposed MemVLT approach. Subfigure (a) shows a video sequence and plots showing the decreasing consistency between initial prompts and the actual target over time. This highlights the problem that VLT methods rely on fixed initial multimodal prompts, which are ineffective for dynamically changing targets. Subfigure (b) illustrates the framework of previous VLT methods, which primarily use similarity matching between the search image and the initial prompts. Subfigure (c) shows the MemVLT framework, which incorporates memory modeling based on the Complementary Learning Systems (CLS) theory to generate adaptive prompts for tracking, addressing the shortcomings of previous methods. The adaptive prompts effectively guide the tracking process by leveraging the storage and interaction of short-term and long-term memories.", "section": "1 Introduction"}, {"figure_path": "ZK1CZXKgG5/figures/figures_3_1.jpg", "caption": "Figure 2: (a) Framework of our proposed MemVLT. Given a language description and a template patch as references, MemVLT tracks the target in search images at time t. The input is first encoded using text and vision encoders. Subsequently, the memory interaction module processes static prompt features based on stored memory, generating adaptive prompts. After incorporating these prompts, the search features are fed into the prediction head to obtain the tracking results. Additionally, the memory storage module utilizes processed data to represent and store memory information. (b) Diagram of memory interaction. To illustrate the process of memory interaction, we organize the memory information from the perspectives of long-term and short-term memories. Through the interaction between these memories, adaptive visual and textual prompts are obtained.", "description": "This figure shows the overall architecture of the proposed MemVLT model. It consists of two main parts: the memory interaction module and the memory storage module.  The memory interaction module takes encoded visual and textual features, along with previously stored memory, and generates adaptive prompts for the tracker. These adaptive prompts are then integrated with the current search features to produce a prediction. The memory storage module is responsible for storing and managing short-term and long-term memory representations to allow for adaptation to changes in the target over time.  Subfigure (b) provides a closer look at how the memory interaction module works, highlighting the interplay between short-term and long-term memories.", "section": "3 Methodology"}, {"figure_path": "ZK1CZXKgG5/figures/figures_8_1.jpg", "caption": "Figure 3: (a)-(c): Heatmaps obtained during the forward process of MemVLT, integrating various adaptive prompts. (d): Heatmap guided solely by the initial fixed prompts. The above process diagrams depict the types and sequence of feature integration. We also illustrate the tracked result bbox and groundtruth bbox. Better viewed with zoom-in. Figure 4: Comparison between confidence prediction and IoU values (taking the \"advSamp_INF_bus6\" sequence as an Example).", "description": "This figure shows the heatmaps generated during the tracking process by MemVLT.  Subfigures (a)-(c) illustrate the heatmaps when adaptive prompts (generated by the memory interaction module) are integrated.  Subfigure (d) shows the heatmap when only the initial fixed prompts are used.  The process diagrams illustrate the feature integration sequence. Bounding boxes of the tracked results and ground truth are also shown. Figure 4 shows the comparison of the confidence score generated by the prediction head and the Intersection over Union (IoU) values of the ground truth and tracked results in a sample video sequence, showing a close relationship between the two.", "section": "Qualitative Analysis"}, {"figure_path": "ZK1CZXKgG5/figures/figures_9_1.jpg", "caption": "Figure 5: Qualitative comparison results of our tracker with other two latest VLTs (i.e., JointNLT [14] and MMTrack [16]) on three challenging sequences from TNL2K [26] benchmark. The first column indicates the provided initial template information. Better viewed in color with zoom-in.", "description": "This figure shows a qualitative comparison of the proposed MemVLT tracker with two other state-of-the-art trackers (JointNLT and MMTrack) on three challenging video sequences from the TNL2K benchmark. Each row represents a different sequence, and each column shows the tracking results at different frames.  The first column displays the initial template provided to the trackers. The remaining columns illustrate the tracking results of MemVLT, JointNLT, and MMTrack, highlighting the performance differences in handling challenging scenarios. The ground truth bounding boxes are shown in green.  MemVLT demonstrates better performance in adapting to target variations and dealing with distractions.", "section": "Qualitative Analysis"}, {"figure_path": "ZK1CZXKgG5/figures/figures_16_1.jpg", "caption": "Figure 2: (a) Framework of our proposed MemVLT. Given a language description and a template patch as references, MemVLT tracks the target in search images at time t. The input is first encoded using text and vision encoders. Subsequently, the memory interaction module processes static prompt features based on stored memory, generating adaptive prompts. After incorporating these prompts, the search features are fed into the prediction head to obtain the tracking results. Additionally, the memory storage module utilizes processed data to represent and store memory information. (b) Diagram of memory interaction. To illustrate the process of memory interaction, we organize the memory information from the perspectives of long-term and short-term memories. Through the interaction between these memories, adaptive visual and textual prompts are obtained.", "description": "This figure illustrates the architecture of MemVLT, a memory-based vision-language tracker.  Panel (a) shows the overall framework: the input (search image, template, language description) is processed by encoders, then a Memory Interaction Module (MIM) generates adaptive prompts based on short-term and long-term memory. Finally, a prediction head outputs the tracking result. Panel (b) details the MIM, showing how long-term and short-term memories interact to generate the adaptive prompts.", "section": "3 Methodology"}, {"figure_path": "ZK1CZXKgG5/figures/figures_16_2.jpg", "caption": "Figure A1: (a) The diagram of the short-term memory generation (SMG) Layer. Depending on the input modality data, this module is capable of generating short-term memories for either visual or textual inputs. (b) The diagram of the transformer decoder layer. It primarily consists of a cross-attention operation and a fully connected operation.", "description": "This figure shows the architecture of two important modules in the MemVLT model: the short-term memory generation layer and the transformer decoder layer. The short-term memory generation layer takes as input either visual or textual data and generates short-term memories. The transformer decoder layer is a standard transformer decoder that uses cross-attention and feed-forward networks to process the input data. Together, these modules allow the MemVLT model to effectively interact with both long-term and short-term memories.", "section": "A More Details on the MemVLT Model"}, {"figure_path": "ZK1CZXKgG5/figures/figures_19_1.jpg", "caption": "Figure 2: (a) Framework of our proposed MemVLT. Given a language description and a template patch as references, MemVLT tracks the target in search images at time t. The input is first encoded using text and vision encoders. Subsequently, the memory interaction module processes static prompt features based on stored memory, generating adaptive prompts. After incorporating these prompts, the search features are fed into the prediction head to obtain the tracking results. Additionally, the memory storage module utilizes processed data to represent and store memory information. (b) Diagram of memory interaction. To illustrate the process of memory interaction, we organize the memory information from the perspectives of long-term and short-term memories. Through the interaction between these memories, adaptive visual and textual prompts are obtained.", "description": "This figure shows the architecture of MemVLT, a memory-based vision-language tracker.  Panel (a) illustrates the overall framework, showing how text and vision encoders process inputs, a memory interaction module generates adaptive prompts, and a prediction head produces tracking results.  Panel (b) zooms in on the memory interaction module, illustrating the interplay between short-term and long-term memories to create these adaptive prompts. ", "section": "3 Methodology"}]