[{"figure_path": "ZsxZ65YqL1/figures/figures_3_1.jpg", "caption": "Figure 1: Cases of four critique dimensions. Scalar-valued critiques are scores and preference labels.", "description": "This figure shows four examples of different critique dimensions in the CRITICEVAL benchmark. Each example includes a task input, an LLM response, and different types of critiques. The four critique dimensions are: feedback (Fs), correction (CR), comparison (Fc), and meta-feedback (F4(Fs)). Scalar-valued critiques are represented by scores and preference labels, while textual critiques are in natural language format. This figure helps visualize the different types of critiques used for a comprehensive evaluation of LLMs' critique ability.", "section": "4 CRITICEVAL Construction"}, {"figure_path": "ZsxZ65YqL1/figures/figures_4_1.jpg", "caption": "Figure 2: The data construction pipeline for our proposed CRITICEVAL. Step 1: 9 tasks and numerous LLMs are prepared. Step 2: LLMs are employed to generate responses, which are then meticulously reviewed by human experts. Step 3: Critiques are generated by LLMs with strong critique ability, and human experts annotate them.", "description": "This figure illustrates the three-step data construction pipeline used to create the CRITICEVAL benchmark.  Step 1 involves preparing 9 diverse tasks and a large number of LLMs. Step 2 shows the process of generating responses using these LLMs, followed by human expert review to assess response quality. Finally, Step 3 depicts the generation of critiques using high-performing LLMs, which are then reviewed and annotated by human experts to ensure high quality and consistency.", "section": "4 CRITICEVAL Construction"}, {"figure_path": "ZsxZ65YqL1/figures/figures_9_1.jpg", "caption": "Figure 3: Distribution of failure modes in each critique dimension.", "description": "This figure presents a detailed analysis of the common failure modes observed in model-generated critiques across three critique dimensions (feedback, comparison, and correction).  The leftmost panel displays the distribution of these failure modes for each dimension, while the rightmost panels show the average subjective scores corresponding to each failure mode within its respective dimension. This analysis is further discussed in Section 6.8 of the paper, which explores fine-grained failure modes to gain a deeper understanding of the weaknesses of model-generated critiques.", "section": "6.8 Fine-grained Failure Modes in Model-Generated Critiques"}, {"figure_path": "ZsxZ65YqL1/figures/figures_20_1.jpg", "caption": "Figure 5: Human annotated Likert scores (1-7).", "description": "This figure shows the distribution of human-annotated Likert scores (1-7) for the response quality of different tasks in CRITICEVAL.  The scores represent the quality of the generated responses in nine different tasks, categorized into low, medium, high, and correct response quality. The x-axis represents the Likert score, and the y-axis represents the number of responses. The figure allows for a visual comparison of the response quality distribution across the different tasks. The different colors represent the different categories of response quality.", "section": "4.2 Response and Critique to be Evaluated"}, {"figure_path": "ZsxZ65YqL1/figures/figures_27_1.jpg", "caption": "Figure 7: Each dot represents one LLM's performance on the subjective evaluation.", "description": "This figure displays the relationship between the average number of unique tokens and the average Likert score given by GPT-4 for three different critique dimensions (feedback, correction, and comparison-based feedback).  Each point represents a different Large Language Model (LLM). The plot helps visualize whether there's a correlation between the length of critiques generated by LLMs and the subjective quality assessment of those critiques by GPT-4.", "section": "J Analysis about Length Bias in Subjective Evaluation"}, {"figure_path": "ZsxZ65YqL1/figures/figures_30_1.jpg", "caption": "Figure 1: Cases of four critique dimensions. Scalar-valued critiques are scores and preference labels.", "description": "This figure illustrates the four critique dimensions evaluated in CRITICEVAL: feedback, correction, comparison, and meta-feedback.  It shows examples of both scalar-valued (scores and preference labels) and textual critiques for each dimension.  The image helps to clarify the different types of critiques and how they are used to evaluate the critique capability of LLMs.", "section": "4 CRITICEVAL Construction"}, {"figure_path": "ZsxZ65YqL1/figures/figures_31_1.jpg", "caption": "Figure 11: One case of objective evaluation on translation task. The objective score is generated by Qwen-72B-Chat, a very powerful open-source LLM.", "description": "This figure shows a single example of the objective evaluation process in CRITICEVAL, focusing on the translation task.  It displays the source sentence in English, the machine-generated translation, and the Likert score (1-7) assigned by the Qwen-72B-Chat large language model.  This score reflects the model's assessment of the translation quality, providing a quantitative measure of its accuracy and fluency.", "section": "5.1 Objective Evaluation"}, {"figure_path": "ZsxZ65YqL1/figures/figures_31_2.jpg", "caption": "Figure 12: One case of generated comparison critique on translation task. The preference label is generated by Qwen-72B-Chat, a very powerful open-source LLM.", "description": "This figure shows an example of a comparison-based critique generated by the Qwen-72B-Chat large language model.  The task is to compare two translations of the same English sentence into Chinese and determine which translation is of higher quality.  The figure displays the original English sentence, the two different Chinese translations (A and B), and the Qwen-72B-Chat's assessment of which translation is superior, along with a brief explanation of its reasoning.", "section": "I Case Study"}, {"figure_path": "ZsxZ65YqL1/figures/figures_43_1.jpg", "caption": "Figure 13: Interpretable analysis of the LLM\u2019s critique ability on subjective evaluation.", "description": "This figure provides a more detailed, interpretable analysis of the performance of different LLMs in CRITICEVAL across various critique dimensions and response qualities.  It breaks down the results by response quality (low, medium, high) showing the overall performance of each LLM for feedback, comparison-based feedback, and correction tasks. The visualization allows for a clearer understanding of the relative strengths and weaknesses of various LLMs in the subjective evaluation of critiques. ", "section": "6.3 More Effective Critiques Consistently Lead to Superior Corrections"}, {"figure_path": "ZsxZ65YqL1/figures/figures_44_1.jpg", "caption": "Figure 13: Interpretable analysis of the LLM's critique ability on subjective evaluation.", "description": "This figure provides a more detailed, interpretable analysis of how different LLMs perform on the subjective evaluation of their critique abilities across various tasks and levels of response quality.  The graph showcases performance across three critique dimensions: feedback, comparison-based feedback, and correction.  It compares several LLMs, including GPT-4-turbo, GPT-3.5-turbo, Qwen-72B-Chat, Llama-2-70B-Chat, and InternLM2-20B-Chat, highlighting their strengths and weaknesses in providing different types of critiques.", "section": "6.3 More Effective Critiques Consistently Lead to Superior Corrections"}]