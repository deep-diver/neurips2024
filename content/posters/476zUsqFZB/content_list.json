[{"type": "text", "text": "PMechRP: Interpretable Deep Learning for Polar Reaction Prediction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 In recent years, deep learning methods have been widely applied to chemical   \n2 reaction prediction due to the time consuming and resource intensive nature of   \n3 designing synthetic pathways. However, with the majority of models being trained   \n4 on the US Patent Office dataset, many proposed architectures lack interpretability   \n5 by modeling chemical reactions as overall transformations. These models map   \n6 directly from reactants to products, and provide minimal insight into the underlying   \n7 driving forces of a reaction. In order to improve interpretrability and provide   \n8 insight into the causality of a chemical reaction, we train various machine learning   \n9 frameworks on the PMechDB dataset. This dataset contains polar elementary   \n10 steps, which model chemical reactions as a sequence of steps associated with   \n11 movements of electrons. Through training on PMechDB, we have created a new   \n12 system for polar mechanistic reaction prediction: PMechRP. Our findings indicate   \n13 that PMechRP is able to provide both accurate and interpretrable predictions, with   \n4 a novel two-step transformer based method achieving the highest top-5 accuracy at   \n15 $89.9\\%$ . ", "page_idx": 0}, {"type": "text", "text": "16 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "17 Two main approaches exist for the prediction of chemical reactions: machine learning based methods,   \n18 and quantum chemistry based methods [1, 13, 5, 8]. While quantum chemistry models offer detailed   \n19 prediction of chemical properties, their computational demands render them feasible only for a   \n20 limited scope of reaction systems, precluding their use for broad-spectrum, high-throughput reaction   \n21 prediction. Conversely, ML models offer computational efficiency and scalability, making them   \n22 well-suited for application across larger chemical systems and datasets. Countless ML models have   \n23 been devised for tasks such as reaction yield prediction [16], reaction classification [14], chemical   \n24 property prediction [4, 2], and both forward and reverse reaction prediction [6, 20, 3, 10].   \n26 Although ML models offer a high-throughput and highly adaptable chemical prediction, a significant   \n27 drawback lies in their lack of interpretability when compared to quantum chemistry or simulation   \n28 based methods. The predominant approach of predicting reactions as overall transformations results   \n29 in a black-box scenario, where predicted products emerge directly from reactants without insight into   \n30 intermediate transition states. Although these models may achieve high accuracy on datasets like   \n31 the US Patent Office dataset [11], their outputs pose challenges for organic chemists, who typically   \n32 reason through chemical synthesis via arrow-pushing mechanisms rather than overall transformations.   \n33 An example of a overall transformation vs a mechanistic elementary step approach can be seen in   \n34 Figure 1. The elementary step approach breaks the overall transformation down into a sequence of   \n35 arrow pushing steps, which illustrate the flow of electrons and the shifting of atoms. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "OVERALL TRANSFORMATION ", "page_idx": 1}, {"type": "image", "img_path": "476zUsqFZB/tmp/9419f5901248cf5c87ec76b3602afeeb849238e8798c657d253fb4a88238096f.jpg", "img_caption": ["Figure 1: Example of an overall transformation vs an elementary step approach. This is a the final reaction step in the synthesis of enzalutamide, a drug used to treat prostate cancer that generates over $\\mathbb{S}6$ billion a year in revenue [21]. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "36 By thinking about reactions as occurring through many elementary steps, organic chemists are   \n37 able to reason about the underlying driving forces of a reaction. When training ML models to   \n38 forecast elementary step reactions, we effectively guide them to emulate an organic chemists\u2019 thought   \n39 processes, thereby generating predictions that are readily interpretable and serve as practical aids for   \n40 organic synthesis design. ", "page_idx": 1}, {"type": "text", "text": "41 2 Data ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "42 To develop predictive models for polar reaction mechanisms, we undertook training on the recently   \n43 introduced PMechDB dataset. This dataset comprises more than 12,700 polar elementary steps,   \n44 each balanced, partially atom mapped, and manually verified by a team of organic chemists. Each   \n45 reaction represents a single elementary step polar reaction. These entries have been collected through   \n46 manual curation from a diverse array of chemistry literature and textbooks [19]. These reactions   \n47 are stored as smiles strings, and notably, the reactions contain arrow pushing information, providing   \n48 insights into the reactivity of individual atoms within each reaction. Leveraging the manually curated   \n49 reactions within the dataset, we conducted an 80/10/10 train/val/test split via random sampling from   \n50 the \"manually_curated_all.csv\" flie. For models which perform cross-validation, the validation data   \n51 was combined with the training data. ", "page_idx": 1}, {"type": "text", "text": "52 3 Methods ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "53 Here we describe two different machine learning approaches for predicting polar elementary step   \n54 mechanisms. Namely, we describe the reactive atom two-step approach, the single-step seq-to-seq   \n55 prediction methods, and a spectator focused two-step transformer method. ", "page_idx": 1}, {"type": "text", "text": "56 3.1 Two-Step Prediction ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "57 The two-step prediction model comprises distinct phases. Initially, the model undertakes the task   \n58 of predicting reactive atoms within the given reaction. Subsequently, these identified reactive sites   \n59 are paired to formulate potential reaction mechanisms, followed by the application of a ranker   \n60 model to rank the plausibility of these proposed mechanisms. This architectural design yields   \n61 highly interpretable predictions, enabling a granular understanding of the model\u2019s rationale. When   \n62 generating predictions, users can discern precisely which atoms are deemed reactive, and they can   \n63 view the precise arrow-pushing mechanism predicted by the model. From the view point of organic   \n64 chemists, the two-step architecture offers greater transparency compared to single-step approaches,   \n65 as the arrow pushing mechanism provides justification for why the final products were predicted. ", "page_idx": 2}, {"type": "text", "text": "66 3.1.1 Siamese Architecture ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "67 The two-step siamese architecture [6] comprises three distinct models, each serving a specific function.   \n68 Initially, two separate reactive atom predictor models are instantiated. One model is specifically   \n69 trained for predicting source atoms, while the other is trained for predicting sink atoms. To train   \n70 the source and sink models, the electron-donating atom from the intermolecular arrow is labeled   \n71 as the source atom, while the electron-accepting atom is labeled as the sink atom. This labeling   \n72 process employs the reactive sites identification method as detailed in [6]. Atoms are represented   \n73 by continuous vectors derived from predefined atomic and graph-topological features, utilizing a   \n74 neighborhood of size 3. Subsequently, both source and sink classifiers are trained to categorize these   \n75 feature vectors accordingly. After the trained reactive atom classifiers predict source and sink atoms,   \n76 these atoms are paired together to enumerate possible arrow pushing mechanisms. Afterwards, a   \n77 siamese architecture is used as a plasubility ranker model, which then ranks the plausibility of each   \n78 potential mechanism to generate a final set of predictions. A visual representation of the source and   \n79 sink pair is provided in Figure 2. ", "page_idx": 2}, {"type": "text", "text": "80 3.1.2 OrbChain ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "81 A polar elementary step reaction Rxn can be modeled as the following: a set of reactant molecules   \n82 $\\bar{R^{'}}\\!\\!=\\,\\left\\{r_{0},r_{1},\\ldots,\\bar{r}_{n}\\right\\}$ , a set of product molecules $P=\\{p_{0},p_{1},\\ldots,\\bar{p_{n}}\\}$ , and a set of arrows $\\alpha=$   \n83 $\\{a_{0},a_{1},\\ldots,a_{m}\\}$ , which transforms $\\mathbf{R}$ into $\\mathbf{P}.$ We consider a molecular orbital (MO) $m_{i}^{(*)}$ to be   \n84 associated with four parameters: $\\mathrm{m}=(\\mathrm{a},\\mathrm{e},\\mathrm{n},\\mathrm{c})$ , where a represents the atom corresponding to the   \n85 molecular orbital, e denotes the number of electrons contained in the MO, n corresponds to the atom   \n86 adjacent to atom a in the case of a bond orbital, and c represents a possible chain of fliled or unfliled   \n87 MOs. Based on the methods described in [6, 9, 18], we model a polar mechanism as an interaction   \n88 between two reactive molecular orbitals $(m_{1}^{(\\ast)},m_{2}^{(\\ast)})$ , where one orbital is the \"source\" orbital and   \n89 acts as a nucleophile, while the other orbital is the \"sink\" orbital and acts as the electrophile. Given   \n90 atom mapped reactants and products, and A, we can uniquely determine the reactive pair of orbitals   \n91 in R used to create P. Conversely, given the reactive pair of orbitals $(m_{1}^{(*)},m_{2}^{(*)})$ and the reactants R,   \n92 we can generate $\\mathbf{P}$ given R. ", "page_idx": 2}, {"type": "text", "text": "93 3.1.3 Reactive Atom Prediction and Plausibility Ranking ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "94 We enumerate over all molecular orbitals found in reactants R, and divide orbitals into reactive   \n95 and non-reactive orbitals. These positive and negative examples are used to train the source and   \n96 sink identification models. Rather than directly predicting the reactive MOs, we perform a binary   \n97 classification prediction on the label of atom a, which is associated with the molecular orbital. We   \n98 adopt the reactive sites identification method from [6] and represent atoms using continuous vectors   \n99 becased on predefined graph-topological and physiochemical features. We train two models: a source   \n100 model and a sink model. The source model predicts a binary classification label for whether or not an   \n101 atom is a source, while the sink predicts a binary classification for whether or not an atom is a sink.   \n102 The training data was constructed by extracting the labeled source, and the labeled sink atom from   \n103 each reaction as positive examples, and then randomly sampling non-source or non-sink examples to   \n104 use as negative examples. ", "page_idx": 2}, {"type": "image", "img_path": "476zUsqFZB/tmp/59b16bf7f42d0247493febfd44e59a7e0bfa13a52e834432a7a803227d640a7a.jpg", "img_caption": ["C[CH+:20]CCOC.[Br-:10]>>C[CH:20](CCOC)[Br:10] 10=20 "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 2: An example of a simple polar elementary step. The electron pushing arrows can be seen in blue, while the source and sink sites are seen in red. The bromine atom labeled 10 is the source atom. The carbon atom labeled 20 is the sink atom. The corresponding SMILES string and arrow codes can be seen below. ", "page_idx": 3}, {"type": "text", "text": "105 3.2 Plausibility Ranking ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "106 Once a set of source atoms and sink atoms are predicted, these two sets are paired together to generate   \n107 pairs of molecular orbitals. A siamese network is used to rank the resulting molecular orbital pairs to   \n108 generate the final reaction mechanism predictions. ", "page_idx": 3}, {"type": "text", "text": "109 3.3 Seq-to-seq Prediction ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "110 In addition to exploring two-step models, we also explore the performance of text-based models. An   \n111 exceedingly common representation of chemical reactions is in the form of SMILES strings (simplified   \n112 molecular-input line-entry system), which is a text-based representation. This representation lends   \n113 itself towards NLP models such as transformers. These architectures model reaction prediction as a   \n114 translation problem, wherein they are translating from reactant SMILES to product SMILES. These   \n115 models have achieved state-of-the-art accuracies when predicting overall chemical transformations.   \n116 However, these models possess several drawbacks in that they are more difficult to interpret and do   \n117 not explicitly encode important molecular information such as invariance to atom permutations. This   \n118 means that the same reaction can be represented by a large number of different SMILES strings, and   \n119 additional strategies such as data augmentation may be needed to prevent a transformer model from   \n120 making different predictions for identical sets of reactants. ", "page_idx": 3}, {"type": "text", "text": "121 3.3.1 Molecular Transformer ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "122 We utilize the innovative text-based reaction predictor, Molecular Transformer [15], which employs a   \n123 bidirectional encoder and autoregressive decoder coupled with a fully connected network to generate   \n124 probability distributions over potential tokens. The pre-trained Molecular Transformers underwent   \n125 training using various versions of the USPTO dataset. We did not separate reactants and reagents,   \n126 so the model pre-trained using the USPTO_MIT_mixed dataset was selected and subsequently fine   \n127 tuned on the PMechDB dataset. ", "page_idx": 3}, {"type": "text", "text": "128 3.3.2 Chemformer ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "129 In addition to the molecular transformer, we also adopt the Chemformer model [7], which is another   \n130 transformer-based reaction predictor. The Chemformer model also employs a bidirectional encoder   \n131 and autoregressive decoder with a fully connected network to generate probability distributions   \n132 over potential tokens. The Chemformer model was pre-trained on molecular reconstruction and   \n133 classification tasks using a dataset of 100M SMILES strings from the ZINC-15 [17] dataset. Af  \n134 terwards, the model was fine-tuned on various downstream tasks including forward prediction and   \n135 retrosynthesis. The pre-training substantially improved the model\u2019s generalizability and convergence   \n136 times on downstream tasks, such as USPTO forward prediction, compared to randomly initialized   \n137 models. We chose to start from the model fine-tuned on USPTO-mixed since reactants and reagents   \n138 are not separated in the PMechDB dataset. This model was subsequently fine-tuned on the PMechDB   \n139 dataset for mechanistic-level predictions. The vocabulary of the model was expanded by 66 tokens to   \n140 account for unseen atoms in the PMechDB dataset. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "141 3.3.3 Two-Step Transformer Architecture ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "142 During experiments, all models were observed to exhibit a significant decrease in performance   \n143 in reaction prediction as the size of the reactants grows. A quantitative analysis of the effects   \n144 of spectators, and the number of atoms can be found in Figure 5 and Figure 6 respectively. To   \n145 combat this, we propose a novel two-step architecture for transformers. Firstly, we use the source   \n146 and sink reactive atom models from the siamese architecture to predict top-2 reactive atoms of   \n147 the model. Reactant molecules which contain the predicted reactive atoms are considered to be   \n148 non-spectator molecules. Since we take top-2 predictions from the source and sink models, we   \n149 predict at most 2 sink molecules, and at most 2 source molecules. Pairing the sinks and sources   \n150 together, we can have at most 4-unique source-sink combinations. After the combinations are   \n151 generated, we run a top-5 prediction using our best performing transformer on each combination.   \n152 Hence a fine-tuned chemformer model was used on each combination, as well as on the original   \n153 reactants. After generating predictions for the source-sink combinations, the molecules which were   \n154 deemed as spectators and removed are added back into the predicted products. If there are fewer than   \n155 4-unique source-sink combinations, more predictions are made on the original reactants until 5 total   \n56 predictions are generated. For each reaction, we take the output predictions, canonicalize them, and   \n157 then perform a simple majority vote with ties being broken randomly.   \n158   \n160 This architecture takes inspiration from common practices in organic chemistry. Often times when   \n161 an organic chemist aims to predict the outcome of a set of reactants, they quickly look through all   \n162 reactant molecules, and fliter away molecules which are likely to be spectators or non-reactive, before   \n163 focusing on a few molecules of interest. By performing a two-step prediction, we are able to first   \n164 filter away potential spectator ions, then predict the reaction mechanism after reducing the space of   \n165 possible reactions exponentially. A considerable performance increase was observed after performing   \n166 this method of ensembling. The results can be seen in Table 3 and Table 4. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "167 3.4 Multi-task learning ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "168 Due to the highly related nature of many chemistry prediction tasks, multitask learning can be used to   \n169 develop robust models which may demonstrate improved learning efficiency and prediction accuracy.   \n170 T5Chem is one such model, which leverages multitask learning on a transformer architecture to   \n171 perform 5 different tasks. The T5Chem multi-task transformer architecture is able to perform for  \n172 ward/backwards prediction, reaction yield prediction, reaction classification, and reagents prediction   \n173 [12]. This architecture was first pretrained with a BERT-like MLM objective on 97 million PubChem   \n174 molecules. Then, the model was further fine-tuned on 5 different tasks using the USPTO_500_MT   \n175 dataset. We selected this model, and fine-tuned it using the 80/10/10 split of the manually curated   \n176 PMechDB reactions. ", "page_idx": 4}, {"type": "text", "text": "177 4 Results and Discussion ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "178 4.1 Performance on PMechDB Dataset ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "179 We assess the performance of the two-step prediction method, comprising reactive sites identification   \n180 and plausibility ranking. The top-N accuracy of the reactive sites identification on PMechDB is   \n181 presented in Table 7. Reactive site identification is considered correct if both the source and sink   \n182 atom were correctly identified within the top-N predictions of each model.   \n183 The source and sink ranking models are able to predict the reactive atoms with relatively high   \n184 accuracy. Although the reactive atom models are able to fliter down the number of potentially reactive   \n185 atoms significantly, due to the large number of atoms and aromatic structures contained in the polar   \n186 reactions, enumerating all possible molecular orbital pairs leads to a large number of possible reaction   \n187 mechanisms fed into the ranker model. Several reaction fingerprints were used for plausibility ranking.   \n188 The results can be found in Table 2.   \n189 In order to perform two-step prediction, both reactive site identification and plausibility ranking must   \n190 be performed. Thus for the best performing two-step model, we use the reactionFP fingerprint for   \n191 plausibility ranking. Therefore in Table 3, we consider this as the best two-step siamese model. For   \n192 the Chemformer, MolTransformer, and T5Chem models, we fine-tuned the pretrained models on the   \n193 PMechDB datset. The results comparing all the trained models can be seen in Table 3   \n194 Although the Siamese two-step model allows for improved interpretability due to its direct prediction   \n195 of arrows, the models based on Chemformer yield the most accurate predictions, with the two-step   \n196 transformer model outperforming all other models significantly. The effects of various ensemble   \n197 sizes can be seen in Table 4. ", "page_idx": 4}, {"type": "table", "img_path": "476zUsqFZB/tmp/8730182701313b566f6ed6e60e19e1aff72421b25f5a7c65d8fa1ce89cf8c3a6.jpg", "table_caption": ["Table 1: Reactive Atom Classification for Siamese Architecture "], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "table", "img_path": "476zUsqFZB/tmp/1c9bef42d7ebfef8e2c2b25b2ed8231be97338aba3f9c460413d38b206963b5b.jpg", "table_caption": ["Table 2: Plausibility Ranking for Two-Step Architecture "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "table", "img_path": "476zUsqFZB/tmp/fb61e7268886b83b604d1c17b35c9eb85a408194f625e0e9f33d1555d62fe256.jpg", "table_caption": ["Table 3: Top-N Accuracy of Trained Models "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "table", "img_path": "476zUsqFZB/tmp/c72f4104c2e758465438f1355dd057d38898ea2dc7f2a4a1f35239a3aef03c1f.jpg", "table_caption": ["Table 4: Effects of Ensemble Size on Top-N Accuracy "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "198 4.1.1 Pretraining ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "199 Pretraining the Chemformer models made a large difference in performance, the effects of pretraining   \n200 can be seen in Table 5.   \n201 The large increase in performance from the pretraining, indicates overlap between the USPTO   \n202 dataset and the PMechDB dataset. This is in stark contrast to radical mechanisms, which exhibited   \n203 lower performance when using a pretrained model [18]. This suggests that radical reactions are   \n204 underrepresented in USPTO datasets compared to polar reactions, and that pre-trained transformer   \n205 models would be expected to have higher performance on polar reactions. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "206 4.2 Pathway Search ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "207 In addition to predicting single-step elementary reactions, further work is being done to evaluate and   \n208 improve the model\u2019s performance on predicting polar mechanistic pathways. This involves chaining   \n209 several elementary steps together to transform a list of starting reactants to a list of target products.   \n210 An example of a simple two-step mechanism correctly predicted by the ensemble transformer model   \n211 can be seen in Figure 3.   \n212 Although the transformer architectures outperform all other models in single step predictions on   \n213 the test dataset, the reactions contained in PMechDB are mostly 1-2 reactant reactions, and contain   \n214 very limited spectator ions. This results in the transformer models having a strong performance on   \n215 reactions which contain 1-2 reactants, but inconsistent performance on reactions with one or more   \n216 spectator ions. An example of this can be seen in the following elementary step which contains a   \n217 spectator benzene ring. 4   \n218 When the chemformer model is asked to predict on Step A, it does not recover the correct products,   \n219 while on Step B with spectators removed, it ranks the products as the top-1 prediction. Interestingly,   \n220 the two-step transformer model is able to correctly predict this step. Comparing the various methods   \n221 numerically, the two-step models appear to demonstrate significantly less performance degradation in   \n222 predicting elementary steps with spectator molecules. Figure 5 demonstrates the top-5 accuracies of   \n223 the various models as the number of reactant molecules is varied, while Figure 6 demonstrates the   \n224 top-5 accuracies as the number of atoms contained in the reactants is varied.   \n225 The two-step transformer model can be seen to outperform both the chemformer and siamese   \n226 architectures. When comparing the models, it seems that the number of reactant atoms has a much   \n227 smaller effect on the prediction accuracy of the transformer models when compared to the siamese   \n228 architecture. Perhaps this indicates that the transformer models are able to implicitly learn which   \n229 reactive atoms it should pay attention to without being distracted by large unreactive substructures.   \n230 ", "page_idx": 5}, {"type": "table", "img_path": "476zUsqFZB/tmp/19d0fb63141134617058aa7744c1ebdfa40382479aba945bf87be548941e92c2.jpg", "table_caption": ["Table 5: Top-N Accuracy of Chemformer Models "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "image", "img_path": "476zUsqFZB/tmp/66f11b34976fb03e140f474acc4f5b7f13e6a3763ed4acfe380381f8b8fa6cfd.jpg", "img_caption": ["Figure 3: A simple 2-step mechanism correctly predicted by ensemble transformer model. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "image", "img_path": "476zUsqFZB/tmp/98e20146e4d57fd7be8db8e2b6cfc6c8afc0b0bffc29cca20ca0e1e3a1fdcd18.jpg", "img_caption": ["Figure 4: Step A represents the elementary step with the spectator molecule benzene included. Step B represents the elementary step with the benzene ring excluded. "], "img_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "476zUsqFZB/tmp/c421b2e1a1d6c58e629a903db8f31073d619e68b68c973d99894a380b553ca1d.jpg", "img_caption": ["Top-5 Accuracy vs Number of Molecules "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 5: Comparing the top-5 accuracies of both the transformer and two-step models as number of reactant molecules is varied. ", "page_idx": 7}, {"type": "image", "img_path": "476zUsqFZB/tmp/93aaa043ba0b0c7f3991c82a84d88c22ad2e89d9b2b614414c8aaf8785dda663.jpg", "img_caption": ["Top-5 Accuracy vs Number of Reactant Atoms "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 6: Comparing the top-5 accuracies of both the transformer and two-step models as number of reactant atoms is varied. ", "page_idx": 7}, {"type": "text", "text": "231 Notably, the two-step transformer model strongly outperforms the chemformer model when   \n232 it views reactions which contain more than 2 reactant molecules. This suggests the first step   \n233 manages to fliter away the spectator ions to some extent and makes the prediction task easier for the   \n234 transformer model. ", "page_idx": 8}, {"type": "text", "text": "235 5 Limitations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "236 Lastly, we note there are several limitations with the current state of the PMechRP polar reaction   \n237 system. Firsly, the PMechDB dataset includes less than 13,000 steps. This means the dataset is   \n238 relatively small for training large architectures, and it may be difficult for these models to generalize   \n239 well to all forms of experimental chemistry. Secondly, the transformer models directly translate from   \n240 reactants to products, without generating the arrow pushing mechanisms. Although the elementary   \n241 step predictions still offer significant interpretability, the two-step siamese method offers greater   \n242 insight into the causality of a reaction by directly showing the flow of electrons. Additional methods   \n243 could be developed to predict arrow codes or reactive orbitals using a transformer architecture in   \n244 order to offer predictions with arrow pushing mechanisms. ", "page_idx": 8}, {"type": "text", "text": "245 6 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "246 We developed and compared several reaction prediction systems for polar reaction mechanisms.   \n247 Through our analysis, we have created the reaction prediction system, PMechRP. This predictor offers   \n248 a fresh perspective on reaction prediction by specifically targeting polar reactions and operating at   \n249 the mechanistic reaction level. From the viewpoint of organic chemists, mechanistic level reaction   \n250 prediction offers immense interpretabiltiy beneftis, and has a lot of potential to aid in the prediction of   \n251 synthetic pathways. We utilized PMechDB datasets to train and develop a wide range of architectures.   \n252 Our findings demonstrate that the most accurate models are based on a two-step process, where   \n253 spectators are filtered out to generate a variety of reactants before they are fed into an ensemble   \n254 transformer architecture. Leveraging PMechDB datasets, our polar predictor marks a significant step   \n255 towards interpretable reaction prediction. ", "page_idx": 8}, {"type": "text", "text": "256 References ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "257 [1] Roman M Balabin and Ekaterina I Lomakina. Neural network approach to quantum-chemistry   \n258 data: Accurate prediction of density functional theory energies. The journal of chemical physics,   \n259 131(7), 2009.   \n260 [2] Connor W Coley, Regina Barzilay, William H Green, Tommi S Jaakkola, and Klavs F Jensen.   \n261 Convolutional embedding of attributed molecular graphs for physical property prediction.   \n262 Journal of chemical information and modeling, 57(8):1757\u20131772, 2017.   \n263 [3] Connor W Coley, Regina Barzilay, Tommi S Jaakkola, William H Green, and Klavs F Jensen.   \n264 Prediction of organic reaction outcomes using machine learning. ACS central science, 3(5):434\u2013   \n265 443, 2017.   \n266 [4] Connor W Coley, Wengong Jin, Luke Rogers, Timothy F Jamison, Tommi S Jaakkola, William H   \n267 Green, Regina Barzilay, and Klavs F Jensen. A graph-convolutional neural network model for   \n268 the prediction of chemical reactivity. Chemical science, 10(2):370\u2013377, 2019.   \n269 [5] Larry A Curtiss, Paul C Redfern, and Krishnan Raghavachari. Gaussian-4 theory. The Journal   \n270 of chemical physics, 126(8), 2007.   \n271 [6] David Fooshee, Aaron Mood, Eugene Gutman, Mohammadamin Tavakoli, Gregor Urban,   \n272 Frances Liu, Nancy Huynh, David Van Vranken, and Pierre Baldi. Deep learning for chemical   \n273 reaction prediction. Molecular Systems Design & Engineering, 3(3):442\u2013452, 2018.   \n274 [7] Ross Irwin, Spyridon Dimitriadis, Jiazhen He, and Esben Jannik Bjerrum. Chemformer: a pre  \n275 trained transformer for computational chemistry. Machine Learning: Science and Technology,   \n276 3(1):015022, 2022.   \n277 [8] Dora Kadish, Aaron D Mood, Mohammadamin Tavakoli, Eugene S Gutman, Pierre Baldi, and   \n278 David L Van Vranken. Methyl cation affinities of canonical organic functional groups. The   \n279 Journal of Organic Chemistry, 86(5):3721\u20133729, 2021.   \n280 [9] Matthew A Kayala, Chlo\u00e9-Agathe Azencott, Jonathan H Chen, and Pierre Baldi. Learning to   \n281 predict chemical reactions. Journal of chemical information and modeling, 51(9):2209\u20132222,   \n282 2011.   \n283 [10] Matthew A Kayala and Pierre Baldi. Reactionpredictor: prediction of complex chemical   \n284 reactions at the mechanistic level using machine learning. Journal of chemical information and   \n285 modeling, 52(10):2526\u20132540, 2012.   \n286 [11] Daniel Mark Lowe. Extraction of chemical structures and reactions from the literature. PhD   \n287 thesis, 2012.   \n288 [12] Jieyu Lu and Yingkai Zhang. Unified deep learning model for multitask reaction predictions   \n289 with explanation. Journal of chemical information and modeling, 62(6):1376\u20131387, 2022.   \n290 [13] Gabriel A Pinheiro, Johnatan Mucelini, Marinalva D Soares, Ronaldo C Prati, Juarez LF   \n291 Da Silva, and Marcos G Quiles. Machine learning prediction of nine molecular properties based   \n292 on the smiles representation of the qm9 quantum-chemistry dataset. The Journal of Physical   \n293 Chemistry A, 124(47):9854\u20139866, 2020.   \n294 [14] Daniel Probst, Philippe Schwaller, and Jean-Louis Reymond. Reaction classification and yield   \n295 prediction using the differential reaction fingerprint drfp. Digital discovery, 1(2):91\u201397, 2022.   \n296 [15] Philippe Schwaller, Teodoro Laino, Th\u00e9ophile Gaudin, Peter Bolgar, Christopher A Hunter,   \n297 Costas Bekas, and Alpha A Lee. Molecular transformer: a model for uncertainty-calibrated   \n298 chemical reaction prediction. ACS central science, 5(9):1572\u20131583, 2019.   \n299 [16] Philippe Schwaller, Alain C Vaucher, Teodoro Laino, and Jean-Louis Reymond. Prediction   \n300 of chemical reaction yields using deep learning. Machine learning: science and technology,   \n301 2(1):015016, 2021.   \n302 [17] Teague Sterling and John J Irwin. Zinc 15\u2013ligand discovery for everyone. Journal of chemical   \n303 information and modeling, 55(11):2324\u20132337, 2015.   \n304 [18] Mohammadamin Tavakoli, Pierre Baldi, Ann Marie Carlton, Yin Ting Chiu, Alexander Shmakov,   \n305 and David Van Vranken. Ai for interpretable chemistry: Predicting radical mechanistic pathways   \n306 via contrastive learning. Advances in Neural Information Processing Systems, 36, 2024.   \n307 [19] Mohammadamin Tavakoli, Ryan J Miller, Mirana Claire Angel, Michael A Pfeiffer, Eugene S   \n308 Gutman, Aaron D Mood, David Van Vranken, and Pierre Baldi. Pmechdb: A public database of   \n309 elementary polar reaction steps. Journal of Chemical Information and Modeling, 2024.   \n310 [20] Shuangjia Zheng, Jiahua Rao, Zhongyue Zhang, Jun Xu, and Yuedong Yang. Predicting   \n311 retrosynthetic reactions using self-corrected transformer neural networks. Journal of chemical   \n312 information and modeling, 60(1):47\u201355, 2019.   \n313 [21] Ai-Nan Zhou, Bonan Li, Lejun Ruan, Yeting Wang, Gengli Duan, and Jianqi Li. An improved   \n314 and practical route for the synthesis of enzalutamide and potential impurities study. Chinese   \n315 Chemical Letters, 28(2):426\u2013430, 2017. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "316 A Appendix / supplemental material ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "317 In this appendix, we provide additional details about the experiments and models trained. ", "page_idx": 9}, {"type": "text", "text": "318 A.1 Compute Resources ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "319 All models were trained using a single NVidia Titan X GPU. ", "page_idx": 9}, {"type": "text", "text": "321 Here we provide some Figures 7, 8 displaying the the number of atoms and atom types found in the   \n322 PMechDB dataset [19] ", "page_idx": 10}, {"type": "image", "img_path": "476zUsqFZB/tmp/f8635987dcbda20f8c66cc9a77c911bda769a33dbb3fce8bdbe5af8319e94fd6.jpg", "img_caption": ["Figure 7: The distribution of the total number of atoms contained in each reaction for the manually curated training dataset. "], "img_footnote": [], "page_idx": 10}, {"type": "image", "img_path": "476zUsqFZB/tmp/67e6491bad3eacc376cf7d3a0c71ec4bb9a96e8f1141a29c64817c7d8c25c792.jpg", "img_caption": ["Figure 8: The distribution of atoms for the reactions in the manually curated training dataset. "], "img_footnote": [], "page_idx": 10}, {"type": "text", "text": "323 A.3 Reactive Atom Prediction ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "324 A fingerprint of length 800 is constructed for each atom. This fingerprint includes 700 graph  \n325 topological features. These features are extracted using a neighborhood of size 3 with the method   \n326 described in [6]. The remaining features consist of physiochemical properties such as valence number,   \n327 electronegativity, etc.   \n328 The source and sink prediction models are trained using the \"manually_curated_all.csv\" file, where   \n329 a 90/10 train/test split was performed. Each training reaction is processed to extract the atom   \n330 fingerprints, the atom is given a label 1 if it is reactive, and 0 if it is non-reactive. The final output   \n331 layer performs a binary classification on a reactive atom. The parameters of the source and sink   \n332 prediction models can be seen below: ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 10}, {"type": "table", "img_path": "476zUsqFZB/tmp/f0d77976bbdb0b6132f9be3b3822bbdad55e713aaf4ae081ec860cbae6e4e573.jpg", "table_caption": ["Table 6: Source and Sink Model Parameters "], "table_footnote": [], "page_idx": 11}, {"type": "text", "text": "333 A.4 Plausibility Ranking ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "334 We tested 3 fingerprints. The reactionFP fingerprint is extracted using the features explained in [6]   \n335 to create a fingerprint of length 3200. For the rxnfp fingerprint, we use the default configuration to   \n336 create a fingerprint of size 256. We use the DRFP fingerprint with a size of 2048 with the default   \n337 configuration. ", "page_idx": 11}, {"type": "text", "text": "338 The parameters of the ranker models can be seen below: ", "page_idx": 11}, {"type": "table", "img_path": "476zUsqFZB/tmp/c6ea71da93efa00918f8c728fb07c37ddbd66ce864d07de70b28717eed7bc3b8.jpg", "table_caption": ["Table 7: Source and Sink Model Parameters "], "table_footnote": [], "page_idx": 11}, {"type": "text", "text": "340 The checklist is designed to encourage best practices for responsible machine learning research,   \n341 addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove   \n342 the checklist: The papers not including the checklist will be desk rejected. The checklist should   \n343 follow the references and follow the (optional) supplemental material. The checklist does NOT count   \n344 towards the page limit.   \n345 Please read the checklist guidelines carefully for information on how to answer these questions. For   \n346 each question in the checklist:   \n347 \u2022 You should answer [Yes] , [No] , or [NA] .   \n348 \u2022 [NA] means either that the question is Not Applicable for that particular paper or the   \n349 relevant information is Not Available.   \n350 \u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA).   \n351 The checklist answers are an integral part of your paper submission. They are visible to the   \n352 reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it   \n353 (after eventual revisions) with the final version of your paper, and its final version will be published   \n354 with the paper.   \n355 The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation.   \n356 While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a   \n357 proper justification is given (e.g., \"error bars are not reported because it would be too computationally   \n358 expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering   \n359 \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we   \n360 acknowledge that the true answer is often more nuanced, so please just use your best judgment and   \n361 write a justification to elaborate. All supporting evidence can appear either in the main paper or the   \n362 supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification   \n363 please point to the section(s) where related material for the question can be found. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "364 IMPORTANT, please: ", "page_idx": 12}, {"type": "text", "text": "365 \u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\",   \n366 \u2022 Keep the checklist subsection headings, questions/answers and guidelines below.   \n367 \u2022 Do not modify the questions and only use the provided macros for your answers.   \n368 1. Claims   \n369 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n370 paper\u2019s contributions and scope?   \n371 Answer: [Yes]   \n372 Justification: The paper clearly outlines its contributions and scope. All contributions are   \n373 backed by evaluating the accuracy of the models, and providing tables and plots for the   \n374 performance.   \n375 Guidelines:   \n376 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n377 made in the paper.   \n378 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n379 contributions made in the paper and important assumptions and limitations. A No or   \n380 NA answer to this question will not be perceived well by the reviewers.   \n381 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n382 much the results can be expected to generalize to other settings.   \n383 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n384 are not attained by the paper.   \n385 2. Limitations   \n386 Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 12}, {"type": "text", "text": "87 Answer: [Yes] ", "page_idx": 12}, {"type": "text", "text": "88 Justification: The paper does discuss the limitations of the work. We analyze and address   \n389 situations where the model performs poorly, such as on reactions with a large number of   \n390 spectator ions or atoms.   \n391 Guidelines:   \n92 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n93 the paper has limitations, but those are not discussed in the paper.   \n94 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n95 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n96 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n97 model well-specification, asymptotic approximations only holding locally). The authors   \n398 should reflect on how these assumptions might be violated in practice and what the   \n99 implications would be.   \n00 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n01 only tested on a few datasets or with a few runs. In general, empirical results often   \n02 depend on implicit assumptions, which should be articulated.   \n03 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n04 For example, a facial recognition algorithm may perform poorly when image resolution   \n05 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n06 used reliably to provide closed captions for online lectures because it fails to handle   \n07 technical jargon.   \n408 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n409 and how they scale with dataset size.   \n410 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n411 address problems of privacy and fairness.   \n412 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n413 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n414 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n415 judgment and recognize that individual actions in favor of transparency play an impor  \n416 tant role in developing norms that preserve the integrity of the community. Reviewers   \n417 will be specifically instructed to not penalize honesty concerning limitations.   \n418 3. Theory Assumptions and Proofs   \n419 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n420 a complete (and correct) proof?   \n421 Answer: [NA]   \n422 Justification: The paper does not include theoretical results.   \n423 Guidelines:   \n424 \u2022 The answer NA means that the paper does not include theoretical results.   \n425 \u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross  \n426 referenced.   \n427 \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n428 \u2022 The proofs can either appear in the main paper or the supplemental material, but if   \n429 they appear in the supplemental material, the authors are encouraged to provide a short   \n430 proof sketch to provide intuition.   \n431 \u2022 Inversely, any informal proof provided in the core of the paper should be complemented   \n432 by formal proofs provided in appendix or supplemental material.   \n433 \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced.   \n434 4. Experimental Result Reproducibility   \n435 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n436 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n437 of the paper (regardless of whether the code and data are provided or not)?   \n438 Answer: [Yes]   \n439 Justification: We clearly describe the architectures used, as well as any modifications made   \n440 to them. The hyperparameters and dimensions of the models can be found in the appendix.   \nThe PMechDB dataset is publically available and can be accessed by any user. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 14}, {"type": "text", "text": "442   \n443   \n444   \n445   \n446   \n447   \n448   \n449   \n450   \n451   \n452   \n453   \n454   \n455   \n456   \n457   \n458   \n459   \n460   \n461   \n462   \n463   \n464   \n465   \n466   \n467   \n468   \n469   \n470   \n471   \n472   \n473   \n474   \n475   \n476   \n477   \n478   \n479   \n480   \n481   \n482   \n483   \n484   \n485   \n486   \n487   \n488   \n489   \n490   \n491   \n492   \n493   \n494   \n495 ", "page_idx": 14}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 14}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Justification: For the existing models, their codes can be found online at their respective git repositories. For the two-step models which predict reactive atoms, the codes use openeye software, which is a commercial library to do most of the chemoinformatics processing and thus this code cannot be released. Everything else from the paper is publically available including the PMechDB dataset. ", "page_idx": 14}, {"type": "text", "text": "Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "504 6. Experimental Setting/Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "05 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n06 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n07 results? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: The paper specifies the data splits and hyperparameters necessary to reproduce the results. ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 15}, {"type": "text", "text": "17 7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 15}, {"type": "text", "text": "Justification: There are not error bars to report, the models were assessed based on their reaction prediction accuracy. They were evaluated once on the test set, so there are no error bars. ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 15}, {"type": "text", "text": "545 8. Experiments Compute Resources ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "46 Question: For each experiment, does the paper provide sufficient information on the com  \n47 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n48 the experiments?   \n550 Justification: This information can be found in the appendix.   \n551 Guidelines: ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 16}, {"type": "text", "text": "560 9. Code Of Ethics ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "561 Question: Does the research conducted in the paper conform, in every respect, with the   \n562 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "564 Justification: To our knowledge the paper conforms with the code of ethics.   \n65 Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 16}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Justification: The paper discussed the ability of the model to be applied to synthetic pathway prediction, which is a very important challenge of chemistry, and the ability of the models to provide interpretable predictions for chemistry. ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 16}, {"type": "text", "text": "601 11. Safeguards   \n602 Question: Does the paper describe safeguards that have been put in place for responsible   \n603 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n604 image generators, or scraped datasets)?   \n605 Answer: [NA]   \n606 Justification: The paper does not have risk for misuse. It simply describes architectures   \n607 which are useful for specifically predicting elementary step reactions. The models currently   \n608 have no ability to design synthetic pathways for a target molecule, they must be first provided   \n609 with a list of reactants to produce a set of products.   \n610 Guidelines:   \n611 \u2022 The answer NA means that the paper poses no such risks.   \n612 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n613 necessary safeguards to allow for controlled use of the model, for example by requiring   \n614 that users adhere to usage guidelines or restrictions to access the model or implementing   \n615 safety filters.   \n616 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n617 should describe how they avoided releasing unsafe images.   \n618 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n619 not require this, but we encourage authors to take this into account and make a best   \n620 faith effort.   \n621 12. Licenses for existing assets   \n622 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n623 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n624 properly respected?   \n625 Answer: [Yes]   \n626 Justification: Papers are cited. The models used have public access git repos. The PMechDB   \n627 dataset is governed by the Creative Commons Attribution-NonCommercial-NoDerivs (CC  \n628 BY-NC-ND) license.   \n629 Guidelines:   \n630 \u2022 The answer NA means that the paper does not use existing assets.   \n631 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n632 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n633 URL.   \n634 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n635 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n636 service of that source should be provided.   \n637 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n638 package should be provided. For popular datasets, paperswithcode.com/datasets   \n639 has curated licenses for some datasets. Their licensing guide can help determine the   \n640 license of a dataset.   \n641 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n642 the derived asset (if it has changed) should be provided.   \n643 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n644 the asset\u2019s creators.   \n645 13. New Assets   \n646 Question: Are new assets introduced in the paper well documented and is the documentation   \n647 provided alongside the assets?   \n648 Answer: [Yes]   \n649 Justification: We have provided descriptions of the various methods and experiments, as   \n650 well as their limitations.   \n651 Guidelines:   \n652 \u2022 The answer NA means that the paper does not release new assets. ", "page_idx": 17}, {"type": "text", "text": "\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 18}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: This does not apply. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 18}, {"type": "text", "text": "653   \n654   \n655   \n656   \n657   \n658   \n659   \n660   \n661   \n662   \n663   \n664   \n665   \n666   \n667   \n668   \n669   \n670   \n671   \n672   \n673   \n674   \n675   \n676   \n677   \n678   \n679   \n680   \n681   \n682   \n683   \n684   \n685   \n686   \n687   \n688   \n689   \n690   \n691   \n692   \n693 ", "page_idx": 18}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] Justification: This does not apply. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 18}]