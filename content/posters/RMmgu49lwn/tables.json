[{"figure_path": "RMmgu49lwn/tables/tables_4_1.jpg", "caption": "Table 1: Comparison between image tokenizers on IN-1k.", "description": "This table compares the performance of different image tokenizers on the ImageNet-1k dataset.  It shows key metrics for each tokenizer, including codebook usage, reconstruction FID (rFID), perplexity (PPL), Fr\u00e9chet Inception Distance for Autoregressive proposal networks (FID<sub>AR</sub>), Inception Score for Autoregressive proposal networks (IS<sub>AR</sub>), Fr\u00e9chet Inception Distance for Non-Autoregressive proposal networks (FID<sub>NAR</sub>), and Inception Score for Non-Autoregressive proposal networks (IS<sub>NAR</sub>).  The results highlight the superior performance of VQ-KD tokenizers compared to VQGAN and FSQ across various metrics. Lower FID and rFID values indicate better image generation quality, while higher IS scores reflect improved diversity. Lower PPL suggests that the proposal network models the token sequence more effectively.", "section": "3.4 Main Observation"}, {"figure_path": "RMmgu49lwn/tables/tables_5_1.jpg", "caption": "Table 2: System level comparison on IN-1k.", "description": "This table compares the performance of various image generation models on the ImageNet-1k dataset.  The models are categorized by their architecture (Autoregressive (AR), Noise-to-Noise (NAR), or Diffusion), the number of parameters, and their Fr\u00e9chet Inception Distance (FID) score, a lower score indicating better performance.  The table highlights that VQ-KDCLIP, an autoregressive model using a feature reconstruction objective, significantly outperforms other state-of-the-art models, achieving a substantially lower FID score. ", "section": "3.4 Main Observation"}, {"figure_path": "RMmgu49lwn/tables/tables_5_2.jpg", "caption": "Table 3: Comparison between image tokenizers on MS-COCO. T2I experiments are conducted on the MS-COCO Captions dataset.", "description": "This table presents a comparison of different image tokenizers' performance on the MS-COCO Captions dataset for text-to-image (T2I) tasks.  It shows the codebook usage (percentage), reconstruction FID (rFID), perplexity (PPL) of the AR proposal network, FID for text-to-image tasks (FIDT2I), and FID for AR proposal network (FIDAR). The results illustrate the relative performance of different tokenizers in terms of various metrics, highlighting their strengths and weaknesses.", "section": "3.5 Further Verification"}, {"figure_path": "RMmgu49lwn/tables/tables_6_1.jpg", "caption": "Table 4: Performance of cluster-based tokenizers.", "description": "This table presents a comparison of the performance of cluster-based tokenizers using different pretrained models (CLIP, ViT, DINO, MAE) as encoders. The performance is evaluated using rFID, FIDAR, FIDNAR, FID, FIDAR, and FIDT21 metrics on both IN-1k and MS-COCO datasets.  The results show that the ViT encoder yields the best performance, highlighting the potential of using pretrained models for image tokenization in image generation tasks.", "section": "4.2 Clustering Pretrained Models as Tokenizers"}, {"figure_path": "RMmgu49lwn/tables/tables_7_1.jpg", "caption": "Table 5: AR modeling with a large-scale proposal network or strong data augmentation.", "description": "This table presents the results of image generation using different tokenizers (VQGAN, FSQ, VQ-KD variants, and ClusterCLIP) with two different proposal networks: GPT-2 XL (a large-scale proposal network) and a standard proposal network with strong data augmentation.  The metrics used to evaluate the performance are FIDAR (Fr\u00e9chet Inception Distance for Autoregressive models) and ISAR (Inception Score for Autoregressive models). Lower FIDAR indicates better image generation quality. Higher ISAR suggests a better balance between diversity and quality of generated images.", "section": "4.3 Scaling Up the Proposal Network"}, {"figure_path": "RMmgu49lwn/tables/tables_7_2.jpg", "caption": "Table 6: Effect of different teachers in VQ-KD.", "description": "This table presents the results of using different sized OpenCLIP models as teachers in the VQ-KD training process.  The table shows that larger models (ViT-H/14 and ViT-G/14) generally result in lower FIDAR (Fr\u00e9chet Inception Distance for autoregressive generation) scores and improved ISAR (Inception Score for autoregressive generation) scores, suggesting that better image understanding (IU) capabilities of the teacher models lead to better image generation performance.  The rFID (reconstruction FID) and PPL (perplexity) are also shown, providing further insights into the quality of the tokenization and the ease of modeling the token sequences.", "section": "4.5 Large Teacher Models in VQ-KD"}, {"figure_path": "RMmgu49lwn/tables/tables_7_3.jpg", "caption": "Table 7: Effect of codebook size and dimension. Experiments are conducted on VQ-KDCLIP.", "description": "This table presents the results of experiments conducted on the VQ-KDCLIP model, investigating the impact of codebook size and dimension on the model's performance.  It shows the rFID, FID_AR, and IS_AR metrics for different codebook sizes (in powers of 2) and dimensions, along with the codebook usage percentage.  The results highlight the trade-off between codebook expressiveness and the ability of the model to effectively utilize the larger codebook. ", "section": "4.6 Codebook Size and Dimension"}]