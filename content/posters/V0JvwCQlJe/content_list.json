[{"type": "text", "text": "FairWire: Fair Graph Generation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "O. Deniz Kose Department of Electrical Engineering and Computer Science University of California Irvine Irvine, CA, USA okose@uci.edu ", "page_idx": 0}, {"type": "text", "text": "Yanning Shen\u2217 Department of Electrical Engineering and Computer Science University of California Irvine Irvine, CA, USA yannings@uci.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Machine learning over graphs has recently attracted growing attention due to its ability to analyze and learn complex relations within critical interconnected systems. However, the disparate impact that is amplified by the use of biased graph structures in these algorithms has raised significant concerns for their deployment in realworld decision systems. In addition, while synthetic graph generation has become pivotal for privacy and scalability considerations, the impact of generative learning algorithms on structural bias has not yet been investigated. Motivated by this, this work focuses on the analysis and mitigation of structural bias for both real and synthetic graphs. Specifically, we first theoretically analyze the sources of structural bias that result in disparity for the predictions of dyadic relations. To alleviate the identified bias factors, we design a novel fairness regularizer that offers a versatile use. Faced with the bias amplification in graph generation models brought to light in this work, we further propose a fair graph generation framework, FairWire, by leveraging our fair regularizer design in a generative model. Experimental results on real-world networks validate that the proposed tools herein deliver effective structural bias mitigation for both real and synthetic graphs. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The volume of graph-structured data has been explosively growing due to the advancement in interconnected systems. In this context, machine learning (ML) over graphs attracts increasing attention (1), where specifically graph neural networks (GNNs) (2; 3; 4) have been proven to handle complex learning tasks over graphs, such as social recommendation (5), traffic flow forecasting (6). ", "page_idx": 0}, {"type": "text", "text": "Despite the increasing research focus on graph ML, the deployment of these algorithms in real-world decision systems requires guarantees preventing disparate impacts. Here, algorithmic disparity refers to the performance gap incurred by ML algorithms with respect to certain sensitive attributes protected by anti-discrimination laws or social norms (e.g., ethnicity, religion). While algorithmic bias is a concern over tabular data (7; 8), such bias becomes more critical for learning over graphs, as the use of graph structure in the algorithm design has been demonstrated to amplify the already existing bias (9). Motivated by this, in this work, we specifically focus on structural bias and consequently the disparity in the predictions of dyadic relationships among nodes. Note that since the link predictions are informed by the proximity principle (nodes connect to other nodes that are similar to themselves), the bias in graph topology is directly reflected in link prediction. For example, in a social network, the denser connectivity within people from the same ethnic group leads to higher recommendation rates within these groups and may cause segregation in social relations (10). Hence, the development of a fair link prediction algorithm is of crucial importance to prevent potential segregation. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Fairness-aware algorithms typically require the knowledge of the sensitive attributes, the sharing of which can potentially create privacy concerns (11). From a scalability perspective, sharing real graphs is also accompanied by difficulties due to the ever-increasing size of graphs. All these factors contribute to the value of synthetic graph generation for a number of applications, such as recommendation systems (12), anomaly detection (13). For graph generation, data-driven models are shown to achieve state-of-the-art results (14; 15; 16), however, the fairness aspect of these models is under-explored. Recent works demonstrate that, in general, generative models tend to amplify the already existing bias in real data (17; 18), which is a potential issue for graph generation as well. ", "page_idx": 1}, {"type": "text", "text": "Faced with the aforementioned structural bias issues in graphs, in this work, we first carry out a theoretical analysis investigating the sources of such structural bias. Specifically, we deduce the factors that affect a commonly used bias metric, namely statistical parity (19), for link prediction. Guided by the theoretical findings, a novel fairness regularizer, $\\mathcal{L}_{\\mathrm{FairWire}}$ is designed, which can be utilized for various graph-related problems, including link prediction and graph generation. In addition, an empirical analysis for a graph generation model is carried out, which reveals that the use of generative algorithms amplifies the already existing structural bias in real graph data. To resolve this issue, we design a new diffusion-based fair graph generation framework, FairWire, which leverages the proposed regularizer $\\mathcal{L}_{\\mathrm{FairWire}}$ . The training of diffusion model in FairWire is specifically designed to capture the correlations between the synthetic sensitive attributes and the graph connectivity, which enables fair model training with the existing techniques without revealing the real sensitive information. Overall, the contributions of this work can be summarized as follows: c1) A theoretical analysis that reveals the causes of disparity in the predictions of dyadic relations between nodes is derived. Differing from the existing analyses regarding the statistical parity in link prediction, our analysis considers a more general setting where sensitive attributes can be non-binary. c2) Based on the theoretical findings, we design a novel fairness regularizer, $\\mathcal{L}_{\\mathrm{FairWire}}$ , which can be directly utilized for link prediction, as well as for graph generation models to alleviate the structural bias in a task-agnostic way. ", "page_idx": 1}, {"type": "text", "text": "c3) We conduct an empirical analysis for the effect of graph generation models on the structural bias, which reveals the possible bias amplification related to these models. ", "page_idx": 1}, {"type": "text", "text": "c4) FairWire, a novel fair graph generation framework, is developed by leveraging $\\mathcal{L}_{\\mathrm{FairWire}}$ within a diffusion model. The diffusion model is trained to capture the relations between the sensitive attributes and the graph topology, facilitating fair model training without private information leakage. c5) Comprehensive experimental results over real-world networks show that the proposed framework can effectively mitigate structural bias and create fair synthetic graphs. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Fairness-aware learning over graphs. Fairness-aware graph ML has attracted increasing attention in recent years (20; 21; 22). Existing works mainly focus on: 1) Group fairness (9; 23; 24; 25), 2) Individual fairness (26; 27), and 3) Counterfactual fairness (28; 29; 30). To mitigate bias in graph ML, different strategies are leveraged, including but not limited to adversarial regularization (9; 24; 31; 32), Bayesian debiasing (33), and graph editing (28; 34; 35; 36; 37). With a specific focus on link prediction, (19; 38) propose fairness-aware strategies to alter the adjacency matrix, while (39) designs a fairness-aware regularizer. Differing from the majority of existing strategies, our proposed design herein is guided and supported by theoretical results. Specifically, we rigorously analyze the factors in graph topology leading to disparity for link prediction considering non-binary sensitive attributes. Furthermore, the developed bias mitigation tool can be employed in a versatile manner for training the link prediction models, as well as for training the generative models to create synthetic, fair graphs. ", "page_idx": 1}, {"type": "text", "text": "Synthetic Graph Generation. Generating synthetic graphs that simulate the existing ones has been a topic of interest for a long time (40; 41), for which the success of deep neural networks has been demonstrated (12; 42; 43). Recently, the use of diffusion-based graph generative models has been increasing, due to their success in reflecting several important statistics of real graphs in the synthetic ones (44; 45; 46; 16; 47; 48).To the best of our knowledge, the only existing work that considers fair graph generation is (49) which only outputs a graph structure without nodal features, sensitive attributes and node labels, and also requires class labels as input. Furthermore, it focuses on the disparities in generation quality for different sensitive groups as the fairness metric, which may not be predictive for the fairness performance in downstream tasks. In contrast, our scheme herein does not require any class labels or training of a particular downstream task. In addition, differing from the existing diffusion models, our generation of graph topology and nodal features is guided by the sensitive attributes, which enables us to capture the correlations between the synthetic sensitive attributes and synthetic graph structure/nodal features. To the best of our knowledge, this work provides the first fairness-aware diffusion-based graph generation framework. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Given an input graph $\\mathcal{G}:=(\\mathcal{V},\\mathcal{E})$ , the focus of this study is investigating and mitigating the structural bias that may lead to unfair results for learning algorithms. Here, $\\bar{\\mathcal{V}}:=\\bar{\\{}v_{1},v_{2},\\bar{\\cdot\\cdot\\cdot}\\,,v_{N}^{\\bar{}}\\}$ denotes the set of nodes and $\\mathcal{E}\\subseteq\\mathcal{V}\\times\\mathcal{V}$ stands for the set of edges. Nodal features and the adjacency matrix of the input graph $\\mathcal{G}$ are represented by $\\mathbf{X}\\in\\mathbb{R}^{N\\times F}$ and $\\mathbf{A}\\in\\{0,1\\}^{N\\times N}$ , respectively, where $\\mathbf{A}_{i j}=1$ if and only if $(v_{i},v_{j})\\in\\mathcal{E}$ . This work considers a single, potentially non-binary sensitive attribute for each node denoted by $\\mathbf{s}\\in\\{1,\\cdots\\,,K\\}^{N}$ . In addition, $\\mathbf{S}\\in\\{0,1\\}^{N\\times K}$ represents the one-hot encoding of the sensitive attributes. For the graphs with class information, $\\mathbf{y}\\in\\dot{\\mathbb{R}}^{N}$ denotes the class labels. Node representations output by the lth GNN layer are $\\mathbf{H}^{l+1}$ , with $\\mathbf{h}_{i}^{l+1}\\in\\mathbb{R}^{F^{l+1}}$ denoting the learned hidden representations for node $v_{i}$ . $\\mathbf{x}_{i}\\in\\mathbb{R}^{F}$ , and $s_{i}$ represent the feature vector, and the sensitive attribute of node $v_{i}$ , respectively. Furthermore, $\\scriptstyle S_{k}$ denotes the set of nodes whose sensitive attributes are equal to $k$ . We define inter-edge set $\\mathcal{E}^{\\chi}:=\\left\\{e_{i j}|v_{i}\\in S_{a},v_{j}\\in S_{b},a\\ne b\\right\\}$ , and intra-edge set $\\mathcal{E}^{\\omega}:=\\{e_{i j}|v_{i}\\in S_{a},v_{j}\\in S_{b},a=b\\}$ . Similarly, $\\begin{array}{r}{d_{i}^{\\chi}:=\\sum_{v_{j}\\in\\mathcal{V}-S_{a}}A_{i j},\\forall v_{i}\\in S_{a}}\\end{array}$ and $\\begin{array}{r}{d_{i}^{\\omega}:=\\sum_{v_{j}\\in S_{a}}A_{i j},\\forall v_{i}\\in S_{a}}\\end{array}$ are the inter- and intra-degrees of node $v_{i}$ , respectively. Finally, $U_{A}$ represents the discrete uniform distribution over the elements of set $\\boldsymbol{\\mathcal{A}}$ . ", "page_idx": 2}, {"type": "text", "text": "4 Inspection and Mitigation of Structural Bias ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "This section first derives the conditions for a graph topology that leads to optimal statistical parity for link prediction. Guided by the obtained conditions, a fairness regularizer will then be presented. Statistical parity for link prediction is defined as $\\Delta_{\\mathrm{SP}}:=|\\mathbb{E}_{(v_{i},v_{j})\\sim U_{\\mathcal{V}}\\times U_{\\mathcal{V}}}[g(v_{i},v_{j})\\mid s_{i}^{-}=s_{j}]-$ $\\mathbb{E}_{(v_{i},v_{j})\\sim U_{\\mathcal{V}}\\times U_{\\mathcal{V}}}[g(v_{i},v_{j})\\mid s_{i}\\neq s_{j}]|$ (19), where $g(v_{i},v_{j})$ denotes the predicted probability for an edge between the nodes $i$ and $j$ . To the best of our knowledge, our analysis is the first theoretical investigation for the relation between $\\Delta_{\\mathrm{SP}}$ and the graph topology considering multi-valued sensitive attributes, thus it generalizes previous findings with binary sensitive attributes (19). ", "page_idx": 2}, {"type": "text", "text": "4.1 Bias Analysis ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "This subsection derives the conditions for a fair graph topology that achieves optimal statistical parity in the ensuing link prediction task. First, we will introduce the GNN model considered in this work. ", "page_idx": 2}, {"type": "text", "text": "GNN model: Throughout the analysis, a stochastic graph view, $\\tilde{\\mathbf A}$ , is adopted, i.e., $\\tilde{A}_{i j}$ denotes the probability of an edge between the nodes $v_{i}$ and $v_{j}$ , and $\\tilde{A}_{i j}\\,=\\,\\tilde{A}_{j i}$ . Let $\\mathbf{Z}^{l+1}$ represent the aggregated representations by the lth GNN layer with $i$ th row $\\begin{array}{r}{\\mathbb{E}_{\\hat{\\mathbf{A}}}[\\mathbf{z}_{i}^{l+1}]:=\\sum_{v_{j}\\in\\mathcal{V}}\\tilde{A}_{i j}\\mathbf{c}_{j}^{l+1}}\\end{array}$ , where ${\\bf c}_{i}^{l+1}:={\\bf W}^{l}{\\bf h}_{i}^{l}$ . Then, the hidden representation output by lth GNN layer for node $v_{i}$ can be written as $\\begin{array}{r}{\\mathbb{E}_{\\tilde{\\mathbf{A}}}[\\mathbf{h}_{i}^{l+1}]\\stackrel{}{=}\\sigma(\\sum_{v_{j}\\in\\mathcal{V}}\\tilde{A}_{i j}\\mathbf{W}^{l}\\mathbf{h}_{j}^{l})=\\sigma(\\mathbb{E}_{\\tilde{\\mathbf{A}}}[\\mathbf{z}_{i}^{l+1}])}\\end{array}$ , where $\\mathbf{W}^{l}$ is the weight matrix and $\\sigma(\\cdot)$ is the non-linear activation employed in the $l$ th GNN layer. ", "page_idx": 2}, {"type": "text", "text": "The following assumptions are made for Theorem 1 that will be presented in this subsection:   \nA2: $\\|\\mathbf{c}_{i}\\|_{\\infty}\\overset{,}{\\leq}\\delta,\\forall v_{i}\\in\\mathcal{V}$ . $\\begin{array}{r}{\\frac{\\mathbb{E}_{\\widehat{\\mathbf{A}}}[d_{i}^{\\omega}]}{|S_{k}|}\\geq\\frac{\\mathbb{E}_{\\widehat{\\mathbf{A}}}[d_{i}^{x}]}{N-|S_{k}|},\\forall v_{i}\\in\\mathcal{S}_{k},\\forall k\\in\\{1,\\cdots,K\\}.}\\end{array}$   \nA3: $\\begin{array}{r}{\\sum_{v_{i},v_{j}\\in\\mathcal{V}}\\tilde{A}_{i j}\\gg\\mathbb{E}_{\\tilde{\\mathbf{A}}}[d_{i}^{\\chi}],\\forall v_{i}\\in\\mathcal{V}}\\end{array}$ . ", "page_idx": 2}, {"type": "text", "text": "These assumptions are naturally satisfied by most of the real-world graphs. Assumption A1 implies that the representations, $\\mathbf{c}_{i}$ \u2019s, are finite. For A2, note that most of the real-world social networks have considerably more intra-edges than inter-edges (50), i.e., $\\mathbb{E}_{\\tilde{\\mathbf{A}}}[d_{i}^{\\omega}]\\,\\geq\\,\\mathbb{E}_{\\tilde{\\mathbf{A}}}[d_{i}^{\\chi}]$ . Thus, unless $|S_{i}||\\gg\\|S_{j}|,i\\neq j$ (extremely unbalanced sensitive group sizes), A2 holds. Finally, A3 holds with high probability as $\\begin{array}{r}{\\mathbb{E}_{\\tilde{\\mathbf{A}}}[d_{i}^{\\chi}]=\\sum_{v_{i}\\in S_{s_{i}},v_{j}\\in\\mathcal{V}-S_{s_{i}}}\\tilde{A}_{i j}}\\end{array}$ . We also demonstrate that these assumptions are valid for the real-world networks we are using in Appendix A in order to further justify them. Building upon these assumptions, Theorem 1 reveals the factors leading to the disparity between the representations of different sensitive groups obtained at any GNN layer. Specifically, it upper bounds the term $\\begin{array}{r}{\\delta_{k}^{(l+1)}:=\\|\\mathbb{E}_{\\tilde{\\mathbf{A}},v_{i}\\sim U_{S_{k}}}[\\mathbf{h}_{i}^{l+1}\\mid s_{i}=k]-\\mathbb{E}_{\\tilde{\\mathbf{A}},v_{i}\\sim U_{(V-S_{k})}}[\\mathbf{h}_{i}^{l+1}\\mid s_{i}\\neq k]\\|_{2}.}\\end{array}$ . The proof of the theorem is presented in Appendix B. ", "page_idx": 3}, {"type": "text", "text": "Theorem 1. The disparity between the representations of nodes in a sensitive group $\\ensuremath{\\mathcal{S}}_{k}$ and the representations of the remaining nodes output by the lth GNN layer, $\\delta_{k}^{(l+1)}$ , can be upper bounded by: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\overset{,(l+1)}{\\underset{k}{\\mathop{)}}}\\leq L\\Big(\\delta\\sqrt{F^{(l+1)}}\\Big(\\left|\\frac{p_{k}^{\\omega}}{\\vert S_{k}\\vert}-\\frac{p_{k}^{x}}{N-\\vert S_{k}\\vert}\\right|+\\left|\\frac{\\sum_{v_{i},v_{j}\\in\\mathcal{V}}\\tilde{A}_{i j}-p_{k}^{\\omega}-2p_{k}^{x}}{N-\\vert S_{k}\\vert}-\\frac{p_{k}^{x}}{\\vert S_{k}\\vert}\\right|\\Big)+2\\sqrt{N}\\Delta_{z}\\Big),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $L$ is the Lipschitz constant of the activation function $\\sigma(\\cdot)$ , $\\left\\|\\mathbf{z}_{i}^{l+1}-\\mathrm{mean}(\\mathbf{z}_{j}^{l+1}\\mid v_{j}\\in\\mathcal{V})\\right\\|_{\\infty}\\leq$ $\\Delta_{z}$ $,\\forall v_{i}\\in\\mathcal{V}_{i}$ , and $\\begin{array}{r}{p_{k}^{\\chi}:=\\sum_{v_{i}\\in S_{k},v_{j}\\notin S_{k}}\\tilde{A}_{i,j},p_{k}^{\\omega}:=\\sum_{v_{i}\\in S_{k},v_{j}\\in S_{k}}\\tilde{A}_{i,j}.}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "Representation disparity resulting from the aforementioned GNN-based aggregation is examined and explained by Theorem 1. The commonly used fairness measures, such as statistical parity (19), are naturally a function of the representation disparity. Herein, we further investigate the said relation between the representation disparity and $\\Delta_{\\mathrm{SP}}$ mathematically. Specifically, for a link prediction model described by a function $\\bar{g}(v_{i},\\bar{v}_{j}):=\\mathbf{h}_{i}^{\\top}\\Sigma\\mathbf{h}_{j}$ , Proposition 1 directly upper bounds $\\Delta_{\\mathrm{SP}}$ . Here, $\\mathbf{h}_{i}$ denotes the representation for node $v_{i}$ that is employed for the link prediction task, i.e., the hidden representations in the final layer. The proof of Proposition 1 is presented in Appendix C. ", "page_idx": 3}, {"type": "text", "text": "Proposition 1. For a link prediction model described by $g(v_{i},v_{j}):=\\mathbf{h}_{i}^{\\top}\\Sigma\\mathbf{h}_{j}$ , $\\Delta_{\\mathrm{SP}}$ can be upper bounded by: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\displaystyle\\Delta_{\\mathrm{SP}}\\leq\\sum_{k=1}^{K}\\frac{|{\\cal S}_{k}|}{N}q\\|\\Sigma\\|_{2}\\delta^{m a x},\\displaystyle\\qquad\\qquad\\qquad(2)}\\\\ {\\displaystyle\\cdot\\|{\\bf h}_{i}\\|_{2}\\leq q,\\forall v_{i},\\,\\delta^{m a x}:=\\operatorname*{max}_{k}(\\|{\\bf E}_{\\tilde{\\mathbf{A}},v_{i}\\sim U_{\\mathcal{S}_{k}}}[{\\bf h}_{i}\\mid s_{i}=k]-{\\mathbb{E}_{\\tilde{\\mathbf{A}},v_{j}\\sim U_{(\\nu-s_{k})}}}[{\\bf h}_{j}\\mid s_{j}\\neq k]\\|_{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Combining the findings of Theorem 1 and Proposition 1, Corollary 1 further demonstrates the factors (including the topological ones) that affect the resulting statistical parity in the link prediction task. ", "page_idx": 3}, {"type": "text", "text": "Corollary 1. For a link prediction model $g(v_{i},v_{j}):=(\\mathbf{h}_{i}^{L+1})^{\\top}\\Sigma\\mathbf{h}_{j}^{L+1}$ , where $\\mathbf{h}_{j}^{L+1}$ is the representation created by Lth (final) GNN layer, $\\Delta_{\\mathrm{SP}}$ can be upper bounded by: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\Delta_{\\mathrm{SP}}\\leq\\sum_{k=1}^{K}\\frac{|S_{k}|}{N}q\\|\\Sigma\\|_{2}L\\big(\\delta\\sqrt{F^{(L+1)}}\\big(\\alpha_{1}+\\alpha_{2}\\big)+2\\sqrt{N}\\Delta_{z}\\big),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\begin{array}{r}{\\alpha_{1}:=\\left|\\frac{p_{k}^{\\omega}}{|S_{k}|}-\\frac{p_{k}^{\\chi}}{N-|S_{k}|}\\right|}\\end{array}$ and $\\begin{array}{r}{\\alpha_{2}:=\\Big|\\frac{\\sum_{v_{i},v_{j}\\in\\mathcal{V}}\\tilde{A}_{i j}-p_{k}^{\\omega}-2p_{k}^{\\chi}}{N-|S_{k}|}-\\frac{p_{k}^{\\chi}}{|S_{k}|}\\Big|.}\\end{array}$ ", "page_idx": 3}, {"type": "text", "text": "4.2 A Regularizer for Fair Connections ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The bias analysis in Subsection 4.1 brings to light the factors resulting in topological bias for a probabilistic graph connectivity. Corollary 1 shows that the topological bias can be minimized if $\\alpha_{1}~=~0$ and $\\alpha_{2}~=~0$ . One can obtain $\\alpha_{1}~=~0$ by ensuring $\\begin{array}{r}{\\frac{p_{k}^{\\omega}}{p_{k}^{\\chi}}~=~\\frac{|S_{k}|}{N-|S_{k}|},\\forall k}\\end{array}$ N|\u2212S|kS||, \u2200k. Meanwhile, $\\alpha_{2}=0$ if $\\begin{array}{r}{p_{k}^{\\omega}=\\sum_{v_{i},v_{j}\\in\\mathcal{V}}(\\tilde{\\mathbf{A}})-c|S_{k}|-c N}\\end{array}$ and $p_{k}^{\\chi}=c|S_{k}|$ for any constant $c\\in\\mathbb R$ . Overall, the optimal values of $p_{k}^{\\omega}$ and $p_{k}^{\\chi}$ that minimize both $\\alpha_{1}$ and $\\alpha_{2}$ follow as $\\begin{array}{r}{(p_{k}^{\\omega})^{*}=\\frac{\\sum_{v_{i},v_{j}\\in\\nu}\\tilde{A}_{i j}|S_{k}|^{2}}{N^{2}}}\\end{array}$ and $\\begin{array}{r}{(p_{k}^{\\chi})^{*}=\\frac{(\\sum_{v_{i},v_{j}\\in\\mathcal{V}}\\tilde{A}_{i j})(N|S_{k}|-|S_{k}|^{2})}{N^{2}}}\\end{array}$ . Therefore, in order to mitigate structural bias, we can design a regularizer that pushes the expected number of inter-edges and intra-edges towards $(p_{k}^{\\chi})^{*}$ and $(p_{k}^{\\omega})^{*}$ : $\\begin{array}{r}{\\bar{\\mathsf{\\Pi}}:=\\sum_{k=1}^{K}|\\sum_{v_{i},v_{j}\\in\\mathcal{V}}(\\tilde{\\mathbf{A}}\\odot(\\mathbf{S}\\mathbf{e}_{k})(\\mathbf{S}\\mathbf{e}_{k})^{\\top})_{i,j}-(p_{k}^{\\omega})^{*}|+|\\sum_{v_{i},v_{j}\\in\\mathcal{V}}(\\tilde{\\mathbf{A}}\\odot(\\mathbf{S}\\mathbf{e}_{k})(\\mathbf{1}-(\\mathbf{S}\\mathbf{e}_{k}))^{\\top})_{i,j}-\\bar{\\mathsf{\\Pi}}_{\\partial,v_{i},v_{j}}(\\mathbf{\\tilde{A}}\\odot(\\mathbf{S}\\mathbf{e}_{k})(\\mathbf{1}-(\\mathbf{S}\\mathbf{e}_{k}))^{\\top})_{i,j}|}\\end{array}$ $(p_{k}^{\\chi})^{\\ast}\\vert$ . Here, ${\\bf e}_{k}\\,\\in\\,\\mathbb{R}^{K}$ is the basis vector with only non-zero entry 1, at the $k$ th element, and $\\odot$ denotes the Hadamard product. Note that such a regularizer is compatible with any learning algorithm that outputs probabilities of all possible edges in the graph, e.g., topology inference algorithms. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Although $\\mathcal{L}$ can be applied to several graph ML algorithms and its theory-guided design can promise effective topological bias mitigation, its design requires a single-batch learning setting due to the definitions of $p_{k}^{\\bar{\\chi}}$ and $p_{k}^{\\omega}$ (resulting in a complexity growing exponentially with $N$ ). Specifically, $p_{k}^{\\chi}$ and $p_{k}^{\\omega}$ are calculated based on all edge probabilities related to the all nodes in $\\ensuremath{\\boldsymbol{S}}_{k}$ . Therefore, regularizing the values of $p_{k}^{\\chi}$ and $p_{k}^{\\omega}$ will lead to scalability issues for large graphs. To tackle this challenge, we only focus on the optimal ratio between the expected number of intra- and inter-edges, i.e., k\u03c7 =N|\u2212S|kS|k|, \u2200k \u2208{1, \u00b7 \u00b7 \u00b7 , K}, which is governed by \u03b11. The idea is to manipulate the ratio between the expected number of intra- and inter-edges in each mini-batch of nodes for a better scalability. We call the corresponding batch-wise fairness regularizer $\\mathcal{L}_{\\mathrm{FairWire}}$ , which follows as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\cdot\\mathrm{Fairwir}(\\tilde{\\mathbf{A}},\\mathcal{B}):=\\sum_{k=0}^{K}\\left|\\frac{\\sum_{v_{i},v_{j}\\in\\mathcal{B}}(\\tilde{\\mathbf{A}}\\odot(\\mathbf{S}\\mathbf{e}_{k})(\\mathbf{S}\\mathbf{e}_{k})^{\\top})_{i j}}{|\\mathcal{S}_{k}|}-\\frac{\\sum_{v_{i},v_{j}\\in\\mathcal{B}}(\\tilde{\\mathbf{A}}\\odot(\\mathbf{S}\\mathbf{e}_{k})(\\mathbf{1}-(\\mathbf{S}\\mathbf{e}_{k}))^{\\top})_{i j}}{N-|\\mathcal{S}_{k}|}\\right|,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ denotes the set of nodes within the utilized minibatch. Note that the aforementioned versatile use of $\\mathcal{L}$ also applies to $\\mathcal{L}_{\\mathrm{FairWire}}$ , which can directly be used in topology inference tasks. Specifically, for link prediction, the following loss function can be employed in training to combat bias: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{l p}=\\sum_{v_{i},v_{j}\\in\\mathcal{B}}\\mathcal{L}_{C E}\\big(\\tilde{\\mathbf{A}}_{i j},\\mathbf{A}_{i j}\\big)+\\lambda\\mathcal{L}_{\\mathrm{FairWire}}(\\tilde{\\mathbf{A}},\\mathcal{B}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\tilde{\\mathbf{A}}_{i j}$ denotes the predicted probability by the algorithm for an edge between $v_{i}$ and $v_{j}$ , and $\\mathcal{L}_{C E}$ is cross-entropy loss. The hyperparameter $\\lambda$ is used to adjust the weight of fairness in training. ", "page_idx": 4}, {"type": "text", "text": "5 Fair Graph Generation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Generating synthetic graphs that capture the structural characteristics in real data attracts increasing attention as a promising remedy for scalability (ever-increasing size of real-world graphs) and privacy issues. Especially, sharing real sensitive attributes for fair model training exacerbates the privacy concerns due to the sensitive attribute leakage problem (11). Thus, creating synthetic graphs with generative models becomes instrumental in applications over interconnected systems. In this work, we focus on diffusion models whose success in capturing the original data distribution has been shown for various types of networks (45; 46; 16; 47). Despite the growing interest in these models, their effects on fairness have not yet been investigated, which limits their use in critical real-world decision systems. Motivated by this, in Subsection 5.1, we first empirically analyze the impact of diffusion models on the algorithmic bias by comparing the original and synthetic graphs in terms of different fairness metrics for link prediction. This empirical investigation reveals that the algorithmic bias is amplified while using generative models for graph creation. To resolve this critical issue, we develop FairWire in Subsection 5.2, a fair graph generation framework, which leverages our proposed regularizer $\\mathcal{L}_{\\mathrm{FairWire}}$ during the training of a diffusion model. ", "page_idx": 4}, {"type": "text", "text": "5.1 Diffusion Models and Structural Bias ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To evaluate the effect of synthetic graph generation on bias, we first sample 10 different synthetic graphs for each of the 4 real-world networks (see Table 7 in Appendix E and Subsection 6.1 for more details on the datasets). Synthetic graphs are sampled using a diffusion model that is trained following the setup in (47), which is a state-of-the-art algorithm for diffusion-based graph generation. Upon creating graphs, we evaluate them for the link prediction task on the same test set (generated from the real data) and report the corresponding utility (AUC) and fairness performance. Fairness performance is measured via two widely used bias metrics, statistical parity $(\\Delta_{\\mathrm{SP}})$ and equal opportunity $\\left(\\Delta_{\\mathrm{EO}}\\right)$ (19) for which lower values indicate better fairness (see Subsection 6.1 for more details on the link prediction model and evaluation metrics). The obtained results are presented in Table 1. ", "page_idx": 4}, {"type": "text", "text": "In Table 1, $\\mathcal{G}$ denotes the original graphs, and the synthetic graphs are represented by $\\tilde{\\mathcal{G}}$ . Overall, Table 1 shows that graph generation via diffusion models indeed amplifies the already existing bias in the original graphs consistently for all the considered datasets. This brings the potential bias-related issues in synthetic graph creation to light and calls for robust bias mitigation solutions. ", "page_idx": 4}, {"type": "table", "img_path": "V0JvwCQlJe/tmp/6c2d1c5d2bfda504469f66a89a6743e2183bcc1b57697ad9858d3da238026a31.jpg", "table_caption": ["Table 1: Comparative results "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "5.2 FairWire: A Fair Graph Generation Framework ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The proposed fairness regularizer in Subsection 4.2, $\\mathcal{L}_{\\mathrm{FairWire}}$ , can be utilized in two different settings: $i$ ) during model training for link prediction, $i i$ ) for training a graph generation model in a taskagnostic way. Note that for both cases, a model is trained to predict a probabilistic graph adjacency matrix, A\u02dc, upon which $\\mathcal{L}_{\\mathrm{FairWire}}$ can be employed. Both use cases can facilitate several fairness-aware graph-based applications. That said, the bias amplification issue in generative models (also observed from Table 1) makes creating fair graphs via graph generation models of particular interest. ", "page_idx": 5}, {"type": "text", "text": "The proposed fair graph generation framework, FairWire, is built upon structured denoising diffusion models for discrete data (51). In the forward diffusion, FairWire employs a Markov process to create noisy graph data samples by independently adding or deleting edges. For denoising, a messagepassing neural network (MPNN) is trained to predict the clean graph based on noisy samples by using the guidance of sensitive attributes (and node labels if available). Finally, we sample synthetic graphs with the guidance of synthetic sensitive attributes that are initialized based on their distribution in the original data. If input graph has also node labels, during graph generation, these node labels are sampled based on their distribution conditioned on the sensitive attributes in the original graph. In the sequel, as our main novelty lies in the denoising process, we discuss the training process of FairWire (reverse diffusion process) in more detail, while the forward diffusion and sampling processes are explained in Appendix D. Note that the diffusion process is presented for attributed graphs, where synthetic nodal features $\\tilde{\\mathbf{X}}$ are also generated. However, the proposed approach can be readily adapted to graphs without nodal features. ", "page_idx": 5}, {"type": "text", "text": "Reverse diffusion process: For denoising, we train an MPNN, $\\phi_{\\theta}$ parametrized by $\\theta$ , which is shown to be a scalable solution for the generation of large, attributed graphs (47). Specifically, $\\phi_{\\theta}$ inputs a noisy version of the input graph and the original sensitive attributes described by $\\mathbf{X}^{t},\\mathbf{A}^{t}$ , S and aims to recover the original nodal features $\\mathbf{X}^{0}$ and graph topology $\\mathbf{A}^{0}$ . Here, $\\mathbf{A}^{0}\\in\\mathbb{R}^{N\\times N\\times2}$ denotes the one-hot representations for the edge labels. Note that the sensitive attributes are used to guide the MPNN to capture the relations between them and graph topology. Therefore, the sensitive attributes are initialized and kept the same during both training and sampling (the original distribution of sensitive attributes is used to initialize them during sampling). For a node $v$ , the message passing at the $l$ th layer can be described as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{h}_{v}^{(t,l+1)}=\\sigma\\bigg(\\mathbf{W}_{T\\rightarrow H}^{(l)}\\mathbf{h}_{t}+\\mathbf{b}_{H}^{(l)}+\\sum_{u\\in\\mathcal{N}^{(t)}(v)}\\frac{1}{\\big|\\mathcal{N}^{(t)}(v)\\big|}\\left[\\mathbf{h}_{u}^{(t,l)}\\|\\mathbf{S}_{u}^{(l)}\\right]\\mathbf{W}_{[H,S]\\rightarrow H}^{(l)}\\right),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{S}_{v}^{(l+1)}=\\sigma\\bigg(\\mathbf{b}_{S}^{(l)}+\\sum_{u\\in\\mathcal{N}^{(t)}(v)}\\frac{1}{\\Big|\\mathcal{N}^{(t)}(v)\\Big|}\\mathbf{S}_{u}^{(l)}\\mathbf{W}_{S\\rightarrow S}^{(l)}\\bigg),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\parallel$ stands for the concatenation operator. In this aggregation, $\\mathbf{W}_{T\\rightarrow H}^{(l)},\\mathbf{W}_{[H,S]\\rightarrow H}^{(l)},\\mathbf{W}_{S\\rightarrow S}^{(l)},\\mathbf{b}_{H}^{(l)}$ and $\\mathbf{b}_{S}^{(l)}$ are all learnable parameters, while $\\sigma(\\cdot)$ consists of ReLU (52) and LayerNorm (53) layers. In addition, $\\mathbf{H}^{(t,0)}$ and $\\mathbf{h}_{t}$ are initialized as hidden representations created for $\\mathbf{X}^{t}$ and time step $t$ via multi-layer perceptrons (MLP), respectively, and $\\mathbf{S}^{(0)}=\\mathbf{S}$ . After creating hidden representations for nodes and their sensitive attributes, final representation for a node $v$ is generated via $\\mathbf{h}_{v}=\\mathbf{h}_{v}^{(t,0)}\\lVert\\mathbf{h}_{v}^{(t,1)}\\rVert\\cdot\\cdot\\cdot\\lVert\\mathbf{S}_{v}^{(0)}\\rVert\\mathbf{S}_{v}^{(1)}\\rVert\\cdot\\cdot\\cdot\\lVert\\mathbf{h}_{t}$ . Note that when node labels are available, their one-hot representations $\\mathbf{Y}$ , are also employed in this MPNN in the same way as $\\mathbf{S}$ are utilized. Based on these final representations, node attributes, and edge labels are predicted. To create fair graph connections in the synthetic graphs, we regularize the predicted edge probabilities, A\u02dc, via the designed fairness regularizer $\\mathcal{L}_{\\mathrm{FairWire}}$ . Overall, the training loss of the MPNN follows as: ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sum_{v_{i}\\in B}\\mathcal{L}_{C E}\\big(\\tilde{\\mathbf{X}}_{i:},\\mathbf{X}_{i:}^{0}\\big)+\\sum_{v_{i},v_{j}\\in B}\\mathcal{L}_{C E}\\big(\\tilde{\\mathbf{A}}_{i j:},\\mathbf{A}_{i j:}^{0}\\big)+\\lambda\\mathcal{L}_{\\mathrm{FairWire}}\\big(\\tilde{\\mathbf{A}},\\boldsymbol{B}\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\lambda$ adjusts the focus on the fairness regularizer. ", "page_idx": 6}, {"type": "text", "text": "Remark (Applicability to general generative models): Although the designed regularizer in Subsection 4.2 is embodied in a diffusion-based graph generation framework in Subsection 5.2, $\\mathcal{L}_{\\mathrm{FairWire}}$ can be utilized in any generative model outputting synthetic graph topologies as a fairness regularizer on the connections, including but not limited to graph autoencoder-based or random walk-based graph generation models. ", "page_idx": 6}, {"type": "text", "text": "Remark (Creation of synthetic sensitive attributes): We design a generative framework in Subsection 5.2 that outputs synthetic sensitive attributes whose effect on the connections is reflected by inputting them in the training of MPNN. We emphasize that the creation of these synthetic sensitive attributes also enables the use of existing fairness-aware schemes on the created graphs without leaking the real sensitive attributes. ", "page_idx": 6}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "6.1 Datasets and Experimental Setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Datasets. In the experiments, four attributed networks are employed, namely Cora, Citeseer, Amazon Photo and Amazon Computer for link prediction. Cora and Citeseer are widely utilized citation networks, where the articles are nodes and the network topology depicts the citation relationships between these articles (54). Amazon Photo and Amazon Computer are product co-purchase networks, where the nodes are the products and the links are created if two products are often bought together (55). In addition to link prediction, we also evaluate the synthetic graphs on node classification, where the German credit (56) and Pokec-n (9) graphs are employed. For more details on the datasets and their statistics, please see Appendix E. ", "page_idx": 6}, {"type": "text", "text": "Experimental Setup. In this section, we first report the performance of $\\mathcal{L}_{\\mathrm{FairWire}}$ for link prediction. For this task, the area under the curve (AUC) is employed as the utility metric. As fairness metrics, statistical parity and equal opportunity definitions in (19; 37) are used, where $\\Delta_{\\mathrm{SP}}:=|\\mathbb{E}_{(v_{i},v_{j})\\sim U_{V}\\times U_{V}}[\\Tilde{A}_{i j}=1\\ |\\ s_{i}=s_{j}]-\\mathbb{E}_{(v_{i},v_{j})\\sim U_{V}\\times U_{V}}[\\Tilde{A}_{i j}=1\\ |\\ s_{i}\\neq s_{j}]|$ and $\\Delta_{\\mathrm{EO}}:=$ $\\begin{array}{r}{|\\mathbb{E}_{(v_{i},v_{j})\\sim U_{V}\\times U_{V}}[\\tilde{A}_{i j}=1\\ |\\ A_{i j}=1,s_{i}=s_{j}]-\\mathbb{E}_{(v_{i},v_{j})\\sim U_{V}\\times U_{V}}[\\tilde{A}_{i j}=1\\ |\\ A_{i j}=1,1=2,\\tilde{A}_{i j}=1]\\ |\\tilde{A}_{i j}=1\\ |\\tilde{A}_{i j}=1.}\\end{array}$ 1si, \u0338= sj]|. Lower values for $\\Delta_{\\mathrm{SP}}$ and $\\Delta_{\\mathrm{EO}}$ indicate better fairness performance. ", "page_idx": 6}, {"type": "text", "text": "To evaluate the generated synthetic graphs, we use both the link prediction and node classification tasks. Herein, we sample 10 synthetic graphs for each dataset with the trained diffusion models. Afterward, we train link prediction/node classification models (for more details on these models, please see Appendix G) on the sampled graphs, and test these models on the real graphs $\\mathcal{G}$ (the test set is the same for all baselines and FairWire). Here, we consider the scenario where there is no access to the real graphs due to privacy concerns, and the models are trained on the synthetic graphs for downstream tasks. To evaluate these synthetic graphs on link prediction, the same utility and fairness metrics as in the link prediction task are used. For node classification, accuracy is employed as the utility measure with $\\Delta_{S P}:=|P(\\hat{y}_{j}=1\\mid s_{j}=0)-P(\\hat{y}_{j}=1\\mid s_{j}=1)|$ , and $\\Delta_{E O}:=|P(\\hat{y}_{j}=1\\mid\\dot{y}_{j}=1,s_{j}=0)-P(\\hat{y}_{j}=1\\mid\\ddot{y}_{j}=1,\\stackrel{\\cdot}{s_{j}}=1)|$ being the fairness metrics. ", "page_idx": 6}, {"type": "text", "text": "For more details on the training of link prediction, node classification, diffusion models, and the hyperparameter selection for FairWire and baselines, see Appendix G. A sensitivity analysis is also provided in Appendix $\\mathrm{H}$ for the effect of hyperparameter $\\lambda$ in (4) and in (7). Note that the performance of the generative algorithms is generally reported in terms of the distances between the statistics of real data and the synthetic ones, instead of the fairness performance. For completeness, we report the distance metrics for node degree distribution and clustering coefficient distribution in Appendix F. ", "page_idx": 6}, {"type": "text", "text": "Baselines. For link prediction, fairness-aware baselines include adversarial regularization (9), FairDrop (37), and FairAdj (19). For graph generation, FairGen (49), is the only existing fairnessaware baseline designed for node classification. For a comprehensive evaluation, we also employ adversarial regularization (9) and FairAdj (19) as in-processing and post-processing fairness-aware strategies within the generative model. For more details on the baselines, please see Appendix G. ", "page_idx": 6}, {"type": "table", "img_path": "V0JvwCQlJe/tmp/c53b810b5d147c594506cd8e3d1b7e400696d8c28777edc6719ecf48b562a914.jpg", "table_caption": ["Table 2: Comparative link prediction results. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "V0JvwCQlJe/tmp/0651455b3f641b9c9eb8f88d3c02eac5a0291d9930ffca0dcadacc12cdf9c069.jpg", "table_caption": ["Table 3: Comparative results for graph generation on Link Prediction. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "6.2 Link Prediction Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Comparative results for the link prediction task are presented in Table 2, where we consider the setting $\\mathcal{L}_{\\mathrm{FairWire}}$ is employed as a fairness regularizer while training a GNN model for link prediction. The natural baseline here is to employ the same GNN model without any fairness interventions, which is denoted by GNN in Table 2. Note that in Table 2, $c$ equals to 0.01 for Amazon Photo and the results are presented for $c=0.1$ for Amazon Computer. ", "page_idx": 7}, {"type": "text", "text": "The results in Table 2 demonstrate that employing $\\mathcal{L}_{\\mathrm{FairWire}}$ as a fairness regularizer leads to better fairness measures compared to the naive baseline, while also providing similar utility. Specifically, the proposed regularizer is observed to improve both fairness metrics, $\\Delta_{S P}$ and $\\Delta_{E O}$ , with improvements ranging from $20\\%$ to $80\\%$ for every evaluated dataset compared to the natural baseline, GNN. Furthermore, the obtained results show that $\\mathcal{L}_{\\mathrm{FairWire}}$ also outperforms the fairness-aware baselines Adversarial (9), FairDrop (37), and FairAdj (19) in both fairness metrics. For certain datasets (e.g., Amazon Photo, Amazon Computer), we report the results of FairWire for different values of $\\lambda$ to illustrate the trade-off between fairness and utility and to show that FairWire leads to a better trade-off compared to the other fairness-aware baselines. Note that we could not include the results of FairAdj over the product networks (i.e., Amazon Photo, Amazon Computer) due to its substantial memory use during its optimization process, which led to out-of-memory errors for the infrastructure we use. In addition to the improved fairness performance, it can be observed that the employment of $\\mathcal{L}_{\\mathrm{FairWire}}$ generally results in the lowest standard deviation values for fairness metrics, which demonstrates the stability of the proposed strategy for bias mitigation. Overall, the results corroborate the effectiveness of $\\mathcal{L}_{\\mathrm{FairWire}}$ in enhancing fairness while also providing similar utility compared to the state-of-the-art fairness-aware baselines. ", "page_idx": 7}, {"type": "text", "text": "6.3 Results for Graph Generation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Comparative results for graph generation are presented in Tables 3 and 4, where the link prediction and node classification tasks are used to evaluate the synthetic graphs, respectively. In these tables, $\\mathcal{G}$ represents the original data, and G\u02dc stands for the synthetic graphs generated by the fairness-agnostic GraphMaker (47). Overall, Tables 3 and 4 show that FairWire improves fairness metrics compared to $\\bar{\\mathcal{G}}$ , fairness-agnostic synthetic graphs created via diffusion, without a significant utility loss for both link prediction and node classification. Specifically, FairWire can achieve improvements in both $\\Delta_{S P}$ and $\\Delta_{E O}$ ranging from $25\\%$ to $90\\%$ for all datasets compared to $\\tilde{\\mathcal{G}}$ with similar utility. ", "page_idx": 7}, {"type": "table", "img_path": "V0JvwCQlJe/tmp/0338ce0017706a161389deb97b737df90320779781437ee1754b73b7b2555ad2.jpg", "table_caption": ["Table 4: Comparative results for graph generation on Node Classification. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Note that, for link prediction, the fairness improvement reported for FairAdj in Table 3 is accompanied by a significant utility drop. Specifically, for a larger $\\lambda$ value (i.e., $\\lambda=1$ ), FairWire can provide better fairness measures $\\langle\\Delta_{S P}=7.01\\pm6.25$ and $\\Delta_{E O}=3.06\\pm2.95)$ on Citeseer with a similar accuracy $(83.47\\pm7.79)$ to FairAdj. Thus, the results in Table 3 demonstrate that FairWire provides a better utility/fairness trade-off compared to fairness-aware baselines on all evaluated datasets. ", "page_idx": 8}, {"type": "text", "text": "In Table 4, similar to the link prediction experiments (Table 2), the results of FairAdj for the Pokec-n network could not be obtained due to computational limitations. For FairGen (49), we directly input the synthetic graph output by the algorithm to the node classification model we train, thus the results are obtained for a single synthetic graph. A possible explanation for the better accuracy of FairGen on the German dataset is that the algorithm is observed to output a denser synthetic network, which might be useful for the utility. It is observed that the synthetic graph output by FairGen for Pokec-n was not informative enough for the node classification task (we provide the codes for the FairGen algorithm in our supplementary material for the reproducibility of these results.) All in all, the results in Table 4 signify that the superior performance of FairWire in terms of fairness/utility trade-off can also be observed for node classification, which validates the efficacy of FairWire in creating fair synthetic graphs that also capture the real data distribution. ", "page_idx": 8}, {"type": "text", "text": "6.4 Visualization of Synthetic Graphs ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Our analysis in Subsection 4.1 reveals that the ratio of intra(edges connecting the same sensitive group) and interedges (edges between different sensitive groups) is a factor contributing to the structural bias. Specifically, the bias factor $\\alpha_{1}$ is minimized when =N|\u2212S|kS|k|, \u2200k, where pk\u03c9 and $p_{k}^{\\chi}$ are the expected number of intra-and inter-edges for the nodes in $\\mathcal{S}_{k}$ . This finding suggests that for a graph with multiple $(>2)$ sensitive groups, given the sizes of sensitive groups are not catastrophically unbalanced, the number of inter-edges (related to $p_{k}^{\\chi}$ ) should be larger than the number of intra-edges (related to $p_{k}^{\\omega}$ ) to alleviate structural bias (i.e., $|S_{k}|\\leq\\bar{N}-|S_{k}|)$ . However, for graphs encountered in several domains, the number of intra-edges is significantly larger than the number of inter-edges, due to the homophily principle (10). Motivated by this, in Figure 1, we visualize the distributions of intra- and inter-edges in synthetic graphs created by i) a fairness-agnostic strategy, GraphMaker (47), and ii) FairWire, for Cora. In Figure 1, intra- and inter-edges are colored with blue and red, respectively. Figure 1 reveals that the graph created by GraphMaker (47) predominantly consists of intra-edges, leading to the structural bias reflected in Table 3. In contrast, FairWire exhibits a remarkable balancing effect, which provides a potential explanation for the improvement in fairness. ", "page_idx": 8}, {"type": "image", "img_path": "V0JvwCQlJe/tmp/90c3dbf1093bf2df9a8a6ae162476af0e8a97460d96de1f20a3cf5973ffbab21.jpg", "img_caption": ["Figure 1: Distribution of the intra-edges (blue) and inter-edges (red) in the synthetic graphs created for Cora dataset by GraphMaker (47) (left) and FairWire (right). "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This study focuses on the investigation and mitigation of structural bias for both real and synthetic graphs, where a novel fairness regularizer, $\\mathcal{L}_{\\mathrm{FairWire}}$ , is designed to alleviate the effects of bias factors identified in a developed theoretical bias analysis. Furthermore, the proposed fairness regularizer is leveraged in a fair graph generation framework, FairWire, which alleviates the bias amplification observed in graph generative models. Experimental results corroborate the effectiveness of the proposed tools in bias mitigation for both real and synthetic graphs. ", "page_idx": 9}, {"type": "text", "text": "Limitations: This paper considers the setting where sensitive attributes are available during model training, which might limit its use for certain real-world applications. Thus, one future direction of this work would be to consider the partial availability of these sensitive attributes in the input graph data. Furthermore, although we showed that real-world graphs typically satisfy the assumptions in Subsection 4.1, another possible future work we consider is deriving a theoretical bias analysis without the dependency on these assumptions. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Work in this paper is supported by NSF ECCS 2412484. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] I. Chami, S. Abu-El-Haija, B. Perozzi, C. R\u00e9, and K. Murphy, \u201cMachine learning on graphs: A model and comprehensive taxonomy,\u201d The Journal of Machine Learning Research, vol. 23, no. 1, pp. 3840\u20133903, 2022.   \n[2] T. N. Kipf and M. Welling, \u201cSemi-supervised classification with graph convolutional networks,\u201d in Proc. International Conference on Learning Representations (ICLR), Apr. 2017.   \n[3] P. Veli\u02c7ckovi\u00b4c, G. Cucurull, A. Casanova, A. Romero, P. Li\u00f2, and Y. Bengio, \u201cGraph attention networks,\u201d in International Conference on Learning Representations, 2018.   \n[4] K. Xu, W. Hu, J. Leskovec, and S. Jegelka, \u201cHow powerful are graph neural networks?\u201d in International Conference on Learning Representations, 2018.   \n[5] J. Yu, H. Yin, J. Li, Q. Wang, N. Q. V. Hung, and X. Zhang, \u201cSelf-supervised multi-channel hypergraph convolutional network for social recommendation,\u201d in Proceedings of the web conference 2021, 2021, pp. 413\u2013424.   \n[6] M. Li and Z. Zhu, \u201cSpatial-temporal fusion graph neural networks for traffic flow forecasting,\u201d in Proceedings of the AAAI conference on artificial intelligence, vol. 35, no. 5, 2021, pp. 4189\u20134196.   \n[7] N. Mehrabi, F. Morstatter, N. Saxena, K. Lerman, and A. Galstyan, \u201cA survey on bias and fairness in machine learning,\u201d ACM computing surveys (CSUR), vol. 54, no. 6, pp. 1\u201335, 2021.   \n[8] D. Pessach and E. Shmueli, \u201cA review on fairness in machine learning,\u201d ACM Computing Surveys (CSUR), vol. 55, no. 3, pp. 1\u201344, 2022.   \n[9] E. Dai and S. Wang, \u201cSay no to the discrimination: Learning fair graph neural networks with limited sensitive attribute information,\u201d in Proceedings of the 14th ACM International Conference on Web Search and Data Mining, 2021, pp. 680\u2013688.   \n[10] B. Hofstra, R. Corten, F. Van Tubergen, and N. B. Ellison, \u201cSources of segregation in social networks: A novel approach using facebook,\u201d American Sociological Review, vol. 82, no. 3, pp. 625\u2013656, 2017.   \n[11] E. Dai and S. Wang, \u201cLearning fair graph neural networks with limited and private sensitive attribute information,\u201d IEEE Transactions on Knowledge and Data Engineering, 2022.   \n[12] A. Bojchevski, O. Shchur, D. Z\u00fcgner, and S. G\u00fcnnemann, \u201cNetgan: Generating graphs via random walks,\u201d in International conference on machine learning. PMLR, 2018, pp. 610\u2013619.   \n[13] L. Akoglu, M. McGlohon, and C. Faloutsos, \u201cRtm: Laws and a recursive generator for weighted time-evolving graphs,\u201d in 2008 Eighth IEEE International Conference on Data Mining. IEEE, 2008, pp. 701\u2013706.   \n[14] N. Goyal, H. V. Jain, and S. Ranu, \u201cGraphgen: A scalable approach to domain-agnostic labeled graph generation,\u201d in Proceedings of The Web Conference 2020, 2020, pp. 1253\u20131263.   \n[15] Y. Li, O. Vinyals, C. Dyer, R. Pascanu, and P. W. Battaglia, \u201cLearning deep generative models of graphs. corr abs/1803.03324 (2018),\u201d 1803.   \n[16] C. Vignac, I. Krawczuk, A. Siraudin, B. Wang, V. Cevher, and P. Frossard, \u201cDigress: Discrete denoising diffusion for graph generation,\u201d in The Eleventh International Conference on Learning Representations, 2022.   \n[17] K. Schwarz, Y. Liao, and A. Geiger, \u201cOn the frequency bias of generative models,\u201d Advances in Neural Information Processing Systems, vol. 34, pp. 18 126\u201318 136, 2021.   \n[18] S. Zhao, H. Ren, A. Yuan, J. Song, N. Goodman, and S. Ermon, \u201cBias and generalization in deep generative models: An empirical study,\u201d Advances in Neural Information Processing Systems, vol. 31, 2018.   \n[19] P. Li, Y. Wang, H. Zhao, P. Hong, and H. Liu, \u201cOn dyadic fairness: Exploring and mitigating bias in graph connections,\u201d in International Conference on Learning Representations, 2021.   \n[20] Y. Dong, J. Ma, S. Wang, C. Chen, and J. Li, \u201cFairness in graph mining: A survey,\u201d IEEE Transactions on Knowledge and Data Engineering, 2023.   \n[21] M. Choudhary, C. Laclau, and C. Largeron, \u201cA survey on fairness for machine learning on graphs,\u201d arXiv preprint arXiv:2205.05396, 2022.   \n[22] Y. Dong, O. D. Kose, Y. Shen, and J. Li, \u201cFairness in graph machine learning: Recent advances and future prospectives,\u201d in Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2023, pp. 5794\u20135795.   \n[23] T. Rahman, B. Surma, M. Backes, and Y. Zhang, \u201cFairwalk: towards fair graph embedding,\u201d in Proceedings of the 28th International Joint Conference on Artificial Intelligence, 2019, pp. 3289\u20133295.   \n[24] A. Bose and W. Hamilton, \u201cCompositional fairness constraints for graph embeddings,\u201d in International Conference on Machine Learning. PMLR, 2019, pp. 715\u2013724.   \n[25] J. Palowitch and B. Perozzi, \u201cDebiasing graph representations via metadata-orthogonal training,\u201d in IEEE International Conference on Advances in Social Networks Analysis and Mining (ASONAM), 2020, pp. 435\u2013442.   \n[26] Y. Dong, J. Kang, H. Tong, and J. Li, \u201cIndividual fairness for graph neural networks: A ranking based approach,\u201d in Proc ACM Conference on Knowledge Discovery & Data Mining (SIGKDD), 2021, pp. 300\u2013310.   \n[27] W. Song, Y. Dong, N. Liu, and J. Li, \u201cGuide: Group equality informed individual fairness in graph neural networks,\u201d in Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2022, pp. 1625\u20131634.   \n[28] C. Agarwal, H. Lakkaraju, and M. Zitnik, \u201cTowards a unified framework for fair and stable graph representation learning,\u201d in Uncertainty in Artificial Intelligence. PMLR, 2021, pp. 2114\u20132124.   \n[29] J. Ma, R. Guo, M. Wan, L. Yang, A. Zhang, and J. Li, \u201cLearning fair node representations with graph counterfactual fairness,\u201d in Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining, 2022, pp. 695\u2013703.   \n[30] Z. Guo, J. Li, T. Xiao, Y. Ma, and S. Wang, \u201cTowards fair graph neural networks via graph counterfactual,\u201d in Proceedings of the 32nd ACM International Conference on Information and Knowledge Management, 2023, pp. 669\u2013678.   \n[31] J. Fisher, A. Mittal, D. Palfrey, and C. Christodoulopoulos, \u201cDebiasing knowledge graph embeddings,\u201d in Proc. Conference on Empirical Methods in Natural Language Processing (EMNLP), 2020, pp. 7332\u20137345.   \n[32] D. Guo, C. Wang, B. Wang, and H. Zha, \u201cLearning fair representations via distance correlation minimization,\u201d IEEE Transactions on Neural Networks and Learning Systems (TNNLS), 2022.   \n[33] M. Buyl and T. De Bie, \u201cDebayes: a bayesian method for debiasing network embeddings,\u201d in International Conference on Machine Learning (ICML). PMLR, 2020, pp. 1220\u20131229.   \n[34] Y. Dong, N. Liu, B. Jalaian, and J. Li, \u201cEdits: Modeling and mitigating data bias for graph neural networks,\u201d in Proceedings of the ACM Web Conference 2022, 2022, pp. 1259\u20131269.   \n[35] O. D. Kose and Y. Shen, \u201cFair contrastive learning on graphs,\u201d IEEE Transactions on Signal and Processing over Networks, vol. 8, pp. 475\u2013488, 2022.   \n[36] \u2014\u2014, \u201cDemystifying and mitigating bias for node representation learning,\u201d IEEE Transactions on Neural Networks and Learning Systems, 2023.   \n[37] I. Spinelli, S. Scardapane, A. Hussain, and A. Uncini, \u201cFairdrop: Biased edge dropout for enhancing fairness in graph representation learning,\u201d IEEE Transactions on Artificial Intelligence, vol. 3, no. 3, pp. 344\u2013354, 2021.   \n[38] C. Laclau, I. Redko, M. Choudhary, and C. Largeron, \u201cAll of the fairness for edge prediction with optimal transport,\u201d in International Conference on Artificial Intelligence and Statistics. PMLR, 2021, pp. 1774\u20131782.   \n[39] M. Buyl and T. D. Bie, \u201cThe KL-divergence between a graph model and its fair I-projection as a fairness regularizer,\u201d in Joint European Conf. on Machine Learning and Knowledge Discovery in Databases. Springer, 2021, pp. 351\u2013366.   \n[40] X. Ying and X. Wu, \u201cGraph generation with prescribed feature constraints,\u201d in Proceedings of the 2009 SIAM International Conference on Data Mining. SIAM, 2009, pp. 966\u2013977.   \n[41] D. Chakrabarti and C. Faloutsos, \u201cGraph mining: Laws, generators, and algorithms,\u201d ACM computing surveys (CSUR), vol. 38, no. 1, pp. 2\u2013es, 2006.   \n[42] L. Rendsburg, H. Heidrich, and U. Von Luxburg, \u201cNetgan without gan: From random walks to low-rank approximations,\u201d in International Conference on Machine Learning. PMLR, 2020, pp. 8073\u20138082.   \n[43] M. Simonovsky and N. Komodakis, \u201cGraphvae: Towards generation of small graphs using variational autoencoders,\u201d in Artificial Neural Networks and Machine Learning\u2013ICANN 2018: 27th International Conference on Artificial Neural Networks, Rhodes, Greece, October 4-7, 2018, Proceedings, Part I 27. Springer, 2018, pp. 412\u2013422.   \n[44] J. Liu, A. Kumar, J. Ba, J. Kiros, and K. Swersky, \u201cGraph normalizing flows,\u201d Advances in Neural Information Processing Systems, vol. 32, 2019.   \n[45] C. Niu, Y. Song, J. Song, S. Zhao, A. Grover, and S. Ermon, \u201cPermutation invariant graph generation via score-based generative modeling,\u201d in International Conference on Artificial Intelligence and Statistics. PMLR, 2020, pp. 4474\u20134484.   \n[46] J. Jo, S. Lee, and S. J. Hwang, \u201cScore-based generative modeling of graphs via the system of stochastic differential equations,\u201d in International Conference on Machine Learning. PMLR, 2022, pp. 10 362\u201310 383.   \n[47] M. Li, E. Kreac\u02c7ic\u00b4, V. K. Potluru, and P. Li, \u201cGraphmaker: Can diffusion models generate large attributed graphs?\u201d arXiv preprint arXiv:2310.13833, 2023.   \n[48] X. Chen, J. He, X. Han, and L.-P. Liu, \u201cEfficient and degree-guided graph generation via discrete diffusion modeling,\u201d arXiv preprint arXiv:2305.04111, 2023.   \n[49] L. Zheng, D. Zhou, H. Tong, J. Xu, Y. Zhu, and J. He, \u201cFairgen: Towards fair graph generation,\u201d arXiv preprint arXiv:2303.17743, 2023.   \n[50] B. Hofstra, R. Corten, F. Van Tubergen, and N. B. Ellison, \u201cSources of segregation in social networks: A novel approach using facebook,\u201d American Sociological Review, vol. 82, no. 3, pp. 625\u2013656, May 2017.   \n[51] J. Austin, D. D. Johnson, J. Ho, D. Tarlow, and R. Van Den Berg, \u201cStructured denoising diffusion models in discrete state-spaces,\u201d Advances in Neural Information Processing Systems, vol. 34, pp. 17 981\u201317 993, 2021.   \n[52] K. Jarrett, K. Kavukcuoglu, M. Ranzato, and Y. LeCun, \u201cWhat is the best multi-stage architecture for object recognition?\u201d in 2009 IEEE 12th international conference on computer vision. IEEE, 2009, pp. 2146\u20132153.   \n[53] J. L. Ba, J. R. Kiros, and G. E. Hinton, \u201cLayer normalization,\u201d arXiv preprint arXiv:1607.06450, 2016.   \n[54] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi-Rad, \u201cCollective classification in network data,\u201d AI magazine, vol. 29, no. 3, pp. 93\u201393, 2008.   \n[55] O. Shchur, M. Mumme, A. Bojchevski, and S. G\u00fcnnemann, \u201cPitfalls of graph neural network evaluation,\u201d arXiv preprint arXiv:1811.05868, 2018.   \n[56] D. Dua, C. Graff et al., \u201cUci machine learning repository,\u201d 2017.   \n[57] O. D. Kose and Y. Shen, \u201cFairgat: Fairness-aware graph attention networks,\u201d arXiv preprint arXiv:2303.14591, 2023.   \n[58] J. Ho, C. Saharia, W. Chan, D. J. Fleet, M. Norouzi, and T. Salimans, \u201cCascaded diffusion models for high fidelity image generation,\u201d The Journal of Machine Learning Research, vol. 23, no. 1, pp. 2249\u20132281, 2022.   \n[59] C. Vignac, I. Krawczuk, A. Siraudin, B. Wang, V. Cevher, and P. Frossard, \u201cDigress: Discrete denoising diffusion for graph generation,\u201d in Proceedings of the 11th International Conference on Learning Representations, 2023.   \n[60] P. Erdos and A. Renyi, \u201cOn random graphs i,\u201d Publ. math. debrecen, vol. 6, no. 290-297, p. 18, 1959.   \n[61] P. W. Holland, K. B. Laskey, and S. Leinhardt, \u201cStochastic blockmodels: First steps,\u201d Social networks, vol. 5, no. 2, pp. 109\u2013137, 1983.   \n[62] T. Chen, W. Zhang, Q. Lu, K. Chen, Z. Zheng, and Y. Yu, \u201cSvdfeature: a toolkit for featurebased collaborative filtering,\u201d The Journal of Machine Learning Research, vol. 13, no. 1, pp. 3619\u20133622, 2012.   \n[63] T. N. Kipf and M. Welling, \u201cVariational graph auto-encoders,\u201d arXiv preprint arXiv:1611.07308, 2016.   \n[64] X. Glorot and Y. Bengio, \u201cUnderstanding the difficulty of training deep feedforward neural networks,\u201d in Proc. International Conference on Artificial Intelligence and Statistics (AISTATS), May 2010, pp. 249\u2013256.   \n[65] D. P. Kingma and J. Ba, \u201cAdam: A method for stochastic optimization,\u201d arXiv preprint arXiv:1412.6980, 2014.   \n[66] J. Gasteiger, A. Bojchevski, and S. G\u00fcnnemann, \u201cPredict then propagate: Graph neural networks meet personalized pagerank,\u201d in International Conference on Learning Representations, 2018.   \n[67] M. Y. Wang, \u201cDeep graph library: Towards efficient and scalable deep learning on graphs,\u201d in ICLR workshop on representation learning on graphs and manifolds, 2019.   \n[68] L. Takac and M. Zabovsky, \u201cData analysis in public social networks,\u201d in International Scientific Conference and International Workshop. \u2019Present Day Trends of Innovations\u2019, vol. 1, no. 6, May 2012. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Assumptions and Real-World Graphs ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In order to show that the assumptions made in Subsection 4.1 are typically valid for real-world graphs, here we present the exact values of the terms within Assumptions 2 and 3 for the datasets we use. Specifically, we make the following assumptions: ", "page_idx": 13}, {"type": "text", "text": "First, for Assumption 2, we obtained the real values of the terms $\\begin{array}{r}{l_{k}\\::=\\:\\frac{\\mathbb{E}_{\\tilde{\\mathbf{A}},v_{i}\\sim U_{S_{k}}}[d_{i}^{\\omega}]}{|S_{k}|}}\\end{array}$ and $r_{k}:=$ $\\frac{\\mathbb{E}_{\\tilde{\\mathbf{A}},v_{i}\\sim U_{S_{k}}}[d_{i}^{\\chi}]}{N\\mathrm{-}|S_{k}|}$ , where we want $l_{k}\\ge r_{k}\\forall k\\in\\{1,\\cdots,K\\}$ . For all different sensitive groups, these values are presented in Table 5 for all the used datasets. ", "page_idx": 13}, {"type": "table", "img_path": "V0JvwCQlJe/tmp/36b26fbdf010ca7b38dc7737ecff56aac06d8887cb3fd2d8cff45d22b39c38c6.jpg", "table_caption": ["Table 5: Validity of Assumption 2 for real-world graphs. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "For Assumption 3, we present the real values of the terms $\\sum_{v_{i},v_{j}\\in\\mathcal{V}}\\tilde{A}_{i j}$ and $\\mathbb{E}_{\\tilde{\\mathbf{A}},v_{i}\\sim U_{\\nu}}[d_{i}^{\\chi}]$ , where we want $\\begin{array}{r}{\\sum_{v_{i},v_{j}\\in\\mathcal{V}}\\tilde{A}_{i j}\\gg\\mathbb{E}_{\\tilde{\\mathbf{A}},v_{i}\\sim U_{\\mathcal{V}}}[d_{i}^{\\chi}]}\\end{array}$ . For all real-world datasets we use, these values are reported in Table 6. ", "page_idx": 13}, {"type": "table", "img_path": "V0JvwCQlJe/tmp/2019afd779f837c59a6aaab5dfd2a9be9a76d5139555950d0bdc425044977b16.jpg", "table_caption": ["Table 6: Validity of Assumption 3 for real-world graphs. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "Overall, the results presented in both Tables 5 and 6 demonstrate that the assumptions made for the theoretical bias analysis in Section 4.1 are valid for the real-world graphs we are using. This supports that our analysis is applicable to most of the real-world data and settings. ", "page_idx": 13}, {"type": "text", "text": "B Proof of Theorem 1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Here, without loss of generality, we will focus on the $l$ th GNN layer, where the input representations are represented by $\\mathbf{H}^{l}$ and output representations are denoted $\\mathbf{H}^{l+\\breve{1}}$ . The considered disparity measure follows as: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\delta_{k}^{(l+1)}:=\\left\\|\\mathbb{E}_{\\tilde{\\mathbf{A}},v_{i}\\sim U_{S_{k}}}[\\mathbf{h}_{i}^{l+1}\\mid s_{i}=k]-\\mathbb{E}_{\\tilde{\\mathbf{A}},v_{i}\\sim U_{(V-S_{k})}}[\\mathbf{h}_{i}^{l+1}\\mid s_{i}\\neq k]\\right\\|_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Let\u2019s re-write the disparity measure $\\delta_{k}^{(l+1)}$ by using definitions $\\mathbf{c}_{i}^{l+1}:=\\mathbf{W}^{l}\\mathbf{h}_{i}^{l}$ , and $\\mathbb{E}_{\\tilde{\\mathbf{A}}}[\\mathbf{h}_{i}^{l+1}]=$ $\\sigma(\\sum_{v_{j}\\in\\nu}\\tilde{A}_{i j}\\mathbf{c}_{j}^{l+1})$ . ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\delta_{k}^{(l+1)}:=\\left\\|\\mathbb{E}_{\\tilde{\\mathbf{A}},v_{i}\\sim U_{\\mathcal{S}_{k}}}[\\mathbf{h}_{i}^{l+1}\\mid s_{i}=k]-\\mathbb{E}_{\\tilde{\\mathbf{A}},v_{i}\\sim U_{(\\mathcal{V}-\\mathcal{S}_{k})}}[\\mathbf{h}_{i}^{l+1}\\mid s_{i}\\neq k]\\right\\|_{2},}\\\\ &{\\qquad=\\left\\|\\frac{1}{|S_{k}|}\\sum_{v_{i}\\in S_{k}}\\sigma\\left(\\displaystyle\\sum_{v_{j}\\in\\mathcal{V}}\\tilde{A}_{i j}\\mathbf{c}_{j}\\right)-\\frac{1}{N-|S_{k}|}\\sum_{v_{i}\\notin S_{k}}\\sigma\\left(\\displaystyle\\sum_{v_{j}\\in\\mathcal{V}}\\tilde{A}_{i j}\\mathbf{c}_{j}\\right)\\right\\|_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Using Lemma A.1. in (57), it can be shown that $\\delta_{k}^{(l+1)}$ can be upper-bounded as: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\delta_{k}^{(l+1)}=\\left\\|\\frac{1}{|S_{k}|}\\sum_{v_{i}\\in S_{k}}\\sigma\\left(\\sum_{v_{j}\\in\\mathcal{V}}\\tilde{A}_{i j}\\mathbf{c}_{j}\\right)-\\frac{1}{N-|S_{k}|}\\sum_{v_{i}\\notin S_{k}}\\sigma\\left(\\sum_{v_{j}\\in\\mathcal{V}}\\tilde{A}_{i j}\\mathbf{c}_{j}\\right)\\right\\|_{2},}\\\\ {\\displaystyle\\leq L\\left(\\left\\|\\frac{1}{|S_{k}|}\\sum_{v_{i}\\in S_{k}}\\sum_{v_{j}\\in\\mathcal{V}}\\tilde{A}_{i j}\\mathbf{c}_{j}-\\frac{1}{N-|S_{k}|}\\sum_{v_{i}\\notin S_{k}}\\sum_{v_{j}\\in\\mathcal{V}}\\tilde{A}_{i j}\\mathbf{c}_{j}\\right\\|_{2}+2\\sqrt{N}\\Delta_{z}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Then, we can divide the sums over all nodes into two: nodes in $\\ensuremath{\\mathcal{S}}_{k}$ and the remaining ones. ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\delta_{k}^{(t+1)}\\leq L\\left(\\left\\|\\displaystyle\\frac{1}{S^{\\varepsilon}}\\displaystyle\\sum_{v\\in\\mathcal E_{k}}\\sum_{\\varphi\\in\\mathcal E_{k}}\\lambda_{\\theta,\\mathbf{f}}e_{j}-\\frac{1}{N-|\\mathcal E_{k}|}\\displaystyle\\sum_{v\\in\\mathcal E_{k}}\\sum_{v\\in\\mathcal E_{k}}\\tilde\\lambda_{\\theta,\\mathbf{r}}e_{j}\\right\\|_{2}+2\\sqrt{N}\\Delta_{x}\\right),}\\\\ &{\\qquad=L\\left(\\left\\|\\displaystyle\\left(\\frac{1}{|S|_{\\mathcal E}|}\\displaystyle\\sum_{v\\in\\mathcal E_{k}}\\sum_{v\\in\\mathcal E_{k}}\\lambda_{\\theta,\\mathbf{f}}e_{j}+\\frac{1}{|S|_{\\mathcal E}|}\\displaystyle\\sum_{v\\in\\mathcal E_{k}}\\sum_{v\\in\\mathcal E_{k}}\\lambda_{\\theta,\\mathbf{f}}e_{j}\\right)\\right.}\\\\ &{\\qquad-\\left.\\left(\\frac{1}{N-|\\mathcal E_{k}|}\\displaystyle\\sum_{v\\in\\mathcal E_{k}}\\sum_{v\\in\\mathcal E_{k}}\\lambda_{\\theta,\\mathbf{f}}e_{j}+\\frac{1}{N-|\\mathcal E_{k}|}\\displaystyle\\sum_{v\\in\\mathcal E_{k}}\\sum_{v\\in\\mathcal E_{k}}\\lambda_{\\theta,\\mathbf{f}}e_{j}\\right)\\right\\|_{2}+2\\sqrt{N}\\Delta_{x}\\right)}\\\\ &{\\qquad=L\\left(\\left\\|\\displaystyle\\sum_{v\\in\\mathcal E_{k}}\\left(\\frac{1}{|S|_{\\mathcal E}|}\\displaystyle\\sum_{v\\in\\mathcal E_{k}}\\tilde\\lambda_{\\theta,t}-\\frac{1}{N-|\\mathcal E_{k}|}\\displaystyle\\sum_{v\\in\\mathcal E_{k}}\\tilde\\lambda_{\\theta,t}\\right)e_{j}\\right.}\\\\ &{\\qquad+\\left.\\displaystyle\\sum_{v\\in\\mathcal E_{k}}\\left(\\frac{1}{|S|_{\\mathcal E}|}\\displaystyle\\sum_{v\\in\\mathcal E_{k}}\\tilde\\lambda_{\\theta,t}-\\frac{1}{N-|\\mathcal E_{k}|}\\displaystyle\\sum_{v\\in\\mathcal E_{k}}\\tilde\\lambda_{\\theta,t}\\right)e_{j}\\right\\|_{2}+2\\sqrt{N}\\Delta_{x}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Then, by triangle inequality, it follows that: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\delta_{k}^{(l+1)}\\leq L\\Bigg(\\left\\|\\displaystyle\\sum_{v_{j}\\in S_{k}}\\left(\\frac{1}{|S_{k}|}\\displaystyle\\sum_{v_{i}\\in S_{k}}\\tilde{A}_{i j}-\\frac{1}{N-|S_{k}|}\\displaystyle\\sum_{v_{i}\\notin S_{k}}\\tilde{A}_{i j}\\right)\\mathbf{c}_{j}\\right\\|_{2}}\\\\ &{\\qquad\\qquad+\\left\\|\\displaystyle\\sum_{v_{j}\\notin S_{k}}\\left(\\frac{1}{|S_{k}|}\\displaystyle\\sum_{v_{i}\\in S_{k}}\\tilde{A}_{i j}-\\frac{1}{N-|S_{k}|}\\displaystyle\\sum_{v_{i}\\notin S_{k}}\\tilde{A}_{i j}\\right)\\mathbf{c}_{j}\\right\\|_{2}+2\\sqrt{N}\\Delta_{z}\\Bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Assumption 2 in Subsection 4.1 ensures that $\\begin{array}{r l}{\\left(\\frac{1}{|S_{k}|}\\sum_{v_{i}\\in S_{k}}\\tilde{A}_{i j}-\\frac{1}{N-|S_{k}|}\\sum_{v_{i}\\notin S_{k}}\\tilde{A}_{i j}\\right)}&{\\ge}\\end{array}$ $0,\\forall v_{j}\\ \\ \\in\\ \\ S_{k}$ . Furthermore, third assumption presented in Subsection 4.1 guarantees that $\\begin{array}{r}{\\left(\\frac{1}{|S_{k}|}\\sum_{v_{i}\\in S_{k}}\\tilde{A}_{i j}-\\frac{1}{N-|S_{k}|}\\sum_{v_{i}\\notin S_{k}}\\tilde{A}_{i j}\\right)\\le0,\\forall v_{j}\\notin S_{k}}\\end{array}$ . Utilizing Assumption 1 in Subsection 4.1, $\\|\\mathbf{c}_{i}\\|_{\\infty}\\le\\delta,\\forall v_{i}\\in\\mathcal{V}$ , the following upper bound can be derived. ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\delta_{k}^{l+1}\\leq L\\Bigg(\\Bigg\\|\\displaystyle\\sum_{v_{j}\\in S_{k}}\\left(\\frac{1}{|S_{k}|}\\displaystyle\\sum_{v_{i}\\in S_{k}}\\tilde{A}_{i j}-\\frac{1}{N-|S_{k}|}\\displaystyle\\sum_{v_{i}\\notin S_{k}}\\tilde{A}_{i j}\\right)\\delta_{F^{(l+1)}}\\Bigg\\|_{2}}\\\\ &{\\qquad\\qquad+\\left\\|\\displaystyle\\sum_{v_{j}\\notin S_{k}}\\left(\\frac{1}{|S_{k}|}\\displaystyle\\sum_{v_{i}\\in S_{k}}\\tilde{A}_{i j}-\\frac{1}{N-|S_{k}|}\\displaystyle\\sum_{v_{i}\\notin S_{k}}\\tilde{A}_{i j}\\right)\\delta_{F^{(l+1)}}\\Bigg\\|_{2}+2\\sqrt{N}\\Delta_{z}\\Bigg),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\delta_{F^{(l+1)}}$ stands for an $F^{(l+1)}$ dimensional vector with all elements being equal to $\\delta$ . Then, by utilizing the definitions $\\begin{array}{r}{p_{k}^{\\chi}:=\\sum_{v_{i}\\in S_{k},v_{j}\\notin S_{k}}\\tilde{A}_{i,j},p_{k}^{\\omega}:=\\sum_{v_{i}\\in S_{k},v_{j}\\in S_{k}}\\tilde{A}_{i,j}}\\end{array}$ , the upper bound in (13) can be rewritten as: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\frac{\\langle l+1\\rangle}{k}\\leq L\\Bigg(\\left\\|\\displaystyle\\sum_{v_{j}\\in\\mathcal{S}_{k}}\\left(\\frac{1}{|S_{k}|}\\displaystyle\\sum_{v_{i}\\in\\mathcal{S}_{k}}\\bar{A}_{i j}-\\frac{1}{N-|S_{k}|}\\displaystyle\\sum_{v_{i}\\notin\\mathcal{S}_{k}}\\bar{A}_{i j}\\right)\\delta_{F^{(i+1)}}\\right\\|_{2}}\\\\ &{\\qquad+\\left\\|\\displaystyle\\sum_{v_{j}\\notin\\mathcal{S}_{k}}\\left(\\frac{1}{|S_{k}|}\\displaystyle\\sum_{v_{i}\\in\\mathcal{S}_{k}}\\bar{A}_{i j}-\\frac{1}{N-|S_{k}|}\\displaystyle\\sum_{v_{i}\\notin\\mathcal{S}_{k}}\\bar{A}_{i j}\\right)\\delta_{F^{(i+1)}}\\right\\|_{2}+2\\sqrt{N}\\Delta_{\\bar{z}}\\Bigg),}\\\\ &{\\qquad=L\\left(\\left\\|\\left(\\frac{p_{k}^{\\prime}}{|S_{k}|}-\\frac{p_{k}^{\\prime}}{N-|S_{k}|}\\right)\\delta_{F^{(i+1)}}\\right\\|_{2}+\\left\\|\\left(\\frac{\\sum_{v_{i},v_{j}\\in\\mathcal{V}}\\bar{A}_{i j}-p_{k}^{\\prime\\prime}-2p_{k}^{\\mathcal{X}}}{N-|S_{k}|}-\\frac{p_{k}^{\\mathcal{X}}}{|S_{k}|}\\right)\\delta_{F^{(i+1)}}\\right\\|_{2}\\right.}\\\\ &{\\qquad=L\\left(\\left|\\frac{p_{k}^{\\prime\\prime}}{|S_{k}|}-\\frac{p_{k}^{\\mathcal{X}}}{N-|S_{k}|}\\right|\\|\\delta_{F^{(i+1)}}\\|_{2}+\\left|\\frac{\\sum_{v_{i},v_{j}\\in\\mathcal{V}}\\bar{A}_{i j}-p_{k}^{\\prime\\prime}-2p_{k}^{\\mathcal{X}}}{N-|S_{k}|}-\\frac{p_{k}^{\\mathcal{X}}}{|S_{k}|}\\right|\\|\\delta_{F^{(i+1)}}\\|_{2}+2\\sqrt{\\frac{\\mathcal{Y}}{\\delta}}\\right)\\left(\\frac{\\zeta_{1}}{|S_{k}|}\\right)\\delta_{F^{(i+1)}}\\right\\|_{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\left.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The final result follows from the inequality, $\\|\\pmb{\\delta}_{F^{(l+1)}}\\|_{2}\\leq\\delta\\sqrt{F^{(l+1)}}$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf\\Lambda_{k}^{\\langle l+1\\rangle}:=\\left\\|\\mathbb{E}_{\\hat{\\mathbf A},v_{i}\\sim U_{S_{k}}}[\\mathbf h_{i}^{l+1}\\mid s_{i}=k]-\\mathbb{E}_{\\hat{\\mathbf A},v_{i}\\sim U_{(V-S_{k})}}[\\mathbf h_{i}^{l+1}\\mid s_{i}\\neq k]\\right\\|_{2},}\\\\ &{\\qquad\\le L\\left(\\delta\\sqrt{F^{(l+1)}}\\left(\\left|\\frac{p_{k}^{\\omega}}{|S_{k}|}-\\frac{p_{k}^{x}}{N-|S_{k}|}\\right|+\\left|\\frac{\\sum_{v_{i},v_{j}\\in V}\\tilde{A}_{i j}-p_{k}^{\\omega}-2p_{k}^{x}}{N-|S_{k}|}-\\frac{p_{k}^{x}}{|S_{k}|}\\right|\\right)+2\\sqrt{N}\\Delta_{z}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which concludes the proof. ", "page_idx": 15}, {"type": "text", "text": "C Proof of Proposition 1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Statistical parity for link prediction is defined as (19): ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\Delta_{\\mathrm{SP}}:=\\big|\\mathbb{E}_{(v_{i},v_{j})\\sim U_{V}\\times U_{V}}\\big[g(v_{i},v_{j})\\mid s_{i}=s_{j}\\big]-\\mathbb{E}_{(v_{i},v_{j})\\sim U_{V}\\times U_{V}}\\big[g(v_{i},v_{j})\\mid s_{i}\\neq s_{j}\\big]\\big|.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By considering each sensitive group explicitly, statistical parity can also be written as: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\Delta_{\\mathrm{SP}}:=\\Big\\lvert\\sum_{k=1}^{K}\\frac{\\lvert S_{k}\\rvert}{N}\\big(\\mathbb{E}_{\\hat{\\mathbf{A}}}[g(v_{i},v_{j})\\mid s_{i}=s_{j},s_{i}=k]-\\mathbb{E}_{\\Tilde{\\mathbf{A}}}[g(v_{i},v_{j})\\mid s_{i}\\neq s_{j},s_{i}=k]\\big)\\Big\\rvert.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Define $\\mathbb{E}_{\\tilde{\\mathbf{A}},v_{i}\\sim U_{S_{k}}}[\\mathbf{h}_{i}\\mid s_{i}=k]:=\\mathbf{p}_{k}$ and $\\mathbb{E}_{\\tilde{\\mathbf{A}},v_{i}\\sim U_{(\\nu-s_{k})}}[\\mathbf{h}_{i}\\mid s_{i}\\neq k]:=\\mathbf{q}_{k}$ . We further assume that $\\|\\mathbf{h}_{i}\\|_{2}\\,\\leq\\,q,\\forall v_{i}\\,\\in\\,\\mathcal{V}$ and it holds that $\\big\\|\\mathbb{E}_{\\tilde{\\mathbf{A}},v_{i}\\sim U_{S_{k}}}\\big[\\mathbf{\\tilde{h}}_{i}\\ |\\ s_{i}=k\\big]-\\mathbb{E}_{\\tilde{\\mathbf{A}},v_{j}\\sim U_{(\\nu-S_{k})}}[\\mathbf{h}_{j}\\ |\\ s_{j}\\ \\neq$ $k]\\|_{2}\\leq\\delta_{k}^{m a x}$ . Using the definitions for $\\mathbf{p}_{k}$ and $\\mathbf{q}_{k}$ and link prediction model $g(v_{i},v_{j}):=\\mathbf{h}_{i}^{\\top}\\Sigma\\mathbf{h}_{j}$ , $\\Delta_{\\mathrm{SP}}$ can be reformulated as: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta_{\\mathrm{SP}}:=\\bigg|\\displaystyle\\sum_{k=1}^{K}\\frac{|\\boldsymbol{S}_{k}|}{N}\\big(\\mathbf{p}_{k}^{\\top}\\boldsymbol{\\Sigma}\\mathbf{p}_{k}-\\mathbf{p}_{k}^{\\top}\\boldsymbol{\\Sigma}\\mathbf{q}_{k}\\big)\\bigg|,}\\\\ &{\\qquad=\\bigg|\\displaystyle\\sum_{k=1}^{K}\\frac{|\\boldsymbol{S}_{k}|}{N}\\big(\\mathbf{p}_{k}^{\\top}\\boldsymbol{\\Sigma}(\\mathbf{p}_{k}-\\mathbf{q}_{k})\\big)\\bigg|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By triangle inequality, it follows that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\Delta_{\\mathrm{SP}}\\leq\\sum_{k=1}^{K}{\\frac{|S_{k}|}{N}}{\\bigg|}\\big(\\mathbf{p}_{k}^{\\top}\\Sigma(\\mathbf{p}_{k}-\\mathbf{q}_{k})\\big)\\,{\\bigg|}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Finally, by using Cauchy-Schwarz inequality and the assumption $\\|\\mathbf{h}_{i}\\|_{2}\\;\\leq\\;q,\\forall v_{i}\\;\\in\\;\\mathcal{V}$ , we can conclude that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Delta_{\\mathrm{SP}}\\leq\\displaystyle\\sum_{k=1}^{K}\\frac{|\\mathcal{S}_{k}|}{N}q\\|\\Sigma\\|_{2}\\Bigg\\|\\big(\\mathbf{p}_{k}-\\mathbf{q}_{k}\\big)\\Bigg\\|_{2},}\\\\ {\\Delta_{\\mathrm{SP}}\\leq\\displaystyle\\sum_{k=1}^{K}\\frac{|\\mathcal{S}_{k}|}{N}q\\|\\Sigma\\|_{2}\\delta_{k}^{m a x},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the final inequality follows from the assumption that $\\|\\mathbb{E}_{{\\tilde{\\mathbf{A}}},v_{i}\\sim U_{S_{k}}}[\\mathbf{h}_{i}\\quad|\\quad s_{i}\\ =\\ k]\\ -$ $\\mathbb{E}_{\\tilde{\\mathbf{A}},v_{j}\\sim U_{(\\nu-s_{k})}}[\\mathbf{h}_{j}\\mid s_{j}\\neq k]\\|_{2}\\leq\\delta_{k}^{m a x}$ . ", "page_idx": 16}, {"type": "text", "text": "D Diffusion Model ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Diffusion Models for Graph Generation. Briefly, diffusion models are composed of two main elements: a noise model $q$ , and a denoising neural network $\\phi_{\\theta}$ . The noise model $q$ progressively corrupts data to create a sequence of increasingly noisy data points. Inspired by the success of Gaussian noise for diffusion-based image generation (58), the earlier diffusion-based graph generation models employ Gaussian noise to create noisy graph data (45; 46). However, such a noise model cannot properly capture the structural properties of discrete graph connections. Motivated by this, a discrete noise model is introduced in (51). The discrete noise model for graph structure is typically applied in the form of edge deletion and additions (16; 47). After creating noisy data, a denoising network $\\phi_{\\theta}$ is trained to invert this process by predicting the original graph structure A from the noisy samples. While different neural network structures are examined as candidates for the denoising network, message-passing neural networks are shown to be a scalable solution for the creation of medium- to large-scale graphs (47). ", "page_idx": 16}, {"type": "text", "text": "Forward diffusion process: introduced in (51), we employ a Markov process herein to add noise to the input graph structure in the form of edge additions or deletions. These edge modifications can be executed by modeling the existence/non-existence of an edge as the edge class labels, where we have 2 classes, and applying a transition matrix that switches the labels with a certain probability. Then, given $\\mathbf{A}^{0}\\in\\mathbb{R}^{N\\times N\\times2}$ denotes the one-hot representations for the edge labels, the noise model can be described by the transition matrices $\\mathbf{Q}_{A}^{t}\\in\\mathbb{R}^{\\dot{2}\\times2}$ for $t=1,\\cdot\\cdot\\cdot,T$ , where $q\\bigl(\\mathbf{A}^{t}\\mid\\mathbf{A}^{t-1}\\bigr)=\\mathbf{A}^{t-1}\\mathbf{Q}_{A}^{t}$ , $q(\\mathbf{A}^{t}\\mid\\mathbf{A}^{0})\\,=\\,\\mathbf{A}^{0}\\bar{\\mathbf{Q}}_{A}^{t}$ , and $\\bar{\\mathbf{Q}}_{A}^{t}\\,=\\,\\mathbf{Q}_{A}^{1}\\mathbf{Q}_{A}^{2}\\cdot\\cdot\\cdot\\mathbf{Q}_{A}^{t}$ . For a uniform transition model, (59) proves that the empirical data distribution (the probability for the existence of an edge) is the optimal prior distribution. Following this finding, we specifically employ the transition matrix: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{Q}_{A}^{t}=\\alpha^{t}\\mathbf{I}+(1-\\alpha^{t})\\mathbf{1}\\mathbf{m}_{E}^{\\top},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\mathbf{I}\\in\\mathbb{R}^{2\\times2}$ is the identity matrix, $\\mathbf{1}\\in\\mathbb{R}^{2}$ is a vector of ones, and $\\mathbf{m}_{E}\\,\\in\\,\\mathbb{R}^{2}$ describes the distribution of edge labels in the original graph. For the assignment of $\\alpha^{t}$ , cosine schedule is utilized, where $\\bar{\\alpha}^{t}:=\\cos(0.5\\pi(t/T+s)/(\\bar{1}+\\bar{s}))^{2}$ with a small s value for $\\bar{\\alpha}^{t}=\\Pi_{\\tau=1}^{t}\\alpha^{\\tau}$ . Note that for categorical nodal features, the forward diffusion process follows the same procedure. ", "page_idx": 16}, {"type": "text", "text": "Sampling: Using the trained MPNN, synthetic graphs can be sampled iteratively. During sampling, we first sample the sensitive attributes of the nodes, $\\tilde{\\mathbf{S}}$ , based on its original distribution in the real graph. As the next step, we need to estimate the reverse diffusion iterations $p_{\\theta}\\left(\\mathcal{G}^{t-1}=(\\mathbf{A}^{t-1},\\mathbf{\\check{X}}^{\\cdot}\\mathbf{1}^{\\cdot})\\ |\\ \\mathcal{G}^{t}=(\\mathbf{A}^{t},\\mathbf{X}^{t})\\right)$ , which is modeled as a product over nodes and edges (59): ", "page_idx": 16}, {"type": "equation", "text": "$$\np_{\\theta}\\left(\\boldsymbol{\\mathcal{G}}^{t-1}\\mid\\boldsymbol{\\mathcal{G}}^{t},\\tilde{\\mathbf{S}},t\\right)=\\prod_{i=1}^{N}\\prod_{f=1}^{F}p_{\\theta}\\left(\\mathbf{X}_{i f}^{t-1}\\mid\\boldsymbol{\\mathcal{G}}^{t},\\tilde{\\mathbf{S}},t\\right)}\\\\ {\\prod_{1\\leq i<j\\leq N}p_{\\theta}\\left(\\mathbf{A}_{i j}^{t-1}\\mid\\boldsymbol{\\mathcal{G}}^{t},\\tilde{\\mathbf{S}},t\\right).}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "To compute each independent term, we marginalize over the predictions of the MPNN: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p_{\\theta}\\left(\\mathbf{A}_{i j}^{t-1}\\mid\\mathcal{G}^{t},\\tilde{\\mathbf{S}},t\\right)=\\displaystyle\\sum_{e\\in\\mathcal{E}^{c}}p_{\\theta}\\left(\\mathbf{A}_{i j}^{t-1}\\mid\\mathbf{A}_{i j}=e,\\mathcal{G}^{t}\\right)p_{\\theta}\\left(\\mathbf{A}_{i j}=e\\mid\\mathcal{G}^{t},\\tilde{\\mathbf{S}},t\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\approx\\displaystyle\\sum_{e\\in\\mathcal{E}^{c}}p_{\\theta}\\left(\\mathbf{A}_{i j}^{t-1}\\mid\\mathbf{A}_{i j}=e,\\mathcal{G}^{t}\\right)\\tilde{\\mathbf{A}}_{i j e},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\mathcal{E}^{c}$ denotes all possible labels for edges, which are $\\{0,1\\}$ for an unweighted graph. Then, we can use our Markovian noise to model $p_{\\theta}$ $\\bar{(\\mathbf{A}_{i j}^{t-1}\\mid\\mathbf{A}_{i j}=\\overset{\\cdot}{e},\\mathcal{G}^{t})}$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\np_{\\theta}\\left(\\mathbf{A}_{i j}^{t-1}\\mid\\mathbf{A}_{i j}=e,\\boldsymbol{\\mathcal{G}}^{t}\\right)=\\left\\{\\_{0}^{q\\left(\\mathbf{A}_{i j}^{t-1}\\mid\\mathbf{A}_{i j}=e,\\mathbf{A}_{i j}^{t}\\right)}\\right.\\quad\\mathrm{if~}q\\left(\\mathbf{A}_{i j}^{t}\\mid\\mathbf{A}_{i j}=e\\right)>0\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Note that posterior $q\\left(\\mathbf{A}_{i j}^{t-1}\\mid\\mathbf{A}_{i j}=e,\\mathbf{A}_{i j}^{t}\\right)$ can be computed in closed-form using Bayes rule. Leveraging this model, $G^{t-1}$ can be sampled, which becomes the input of the MPNN at the next time step. ", "page_idx": 17}, {"type": "text", "text": "E Additional Details on Datasets and Datasets Statistics ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "For citation networks (Cora and Citeseer), the one-hot encoding representations of the words in the article descriptions constitute the binary nodal attributes. In these networks, similar to the setups in (19; 37), the category of the articles is employed as the sensitive attribute. Furthermore, for product co-purchase networks, nodal attributes are again the one-hot encodings of the words in the product reviews and the product categories are utilized as the sensitive attributes. For the evaluation of node classification, we employ German (56) and Pokec-n (9) networks. Specifically, the German credit graph has 1,000 nodes representing the clients in a German bank, where the links are created based on the similarity of credit accounts. For this graph, the node labels classify clients into good vs. bad credit risks, where the clients\u2019 gender are employed as the sensitive attribute (28). In addition, Pokec-n is sampled from an anonymized version of the Pokec network of 2012 (a social network from Slovakia), where nodes correspond to users who live in two major regions, and the region information is utilized as the sensitive attribute (9). The working field of the users is binarized and utilized as the labels to be predicted in node classification. ", "page_idx": 17}, {"type": "table", "img_path": "V0JvwCQlJe/tmp/97c399b7809950e0401aab16a04639d8a1e648ca0519f2056947ae4a19b7f081.jpg", "table_caption": ["Table 7: Dataset statistics. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Statistical information for the utilized datasets are presented in Table 7, where $F$ is the total number of nodal features and $K$ represents the number of sensitive groups. ", "page_idx": 17}, {"type": "text", "text": "F Evaluation with Statistics ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We evaluate the created synthetic graphs by FairWire via link prediction in Subsection 6.2. In order to provide a more traditional evaluation scheme, here we also report the 1-Wasserstein distance between the node degree distribution and clustering coefficient distribution of original graph and the synthetic ones. Table 8 presents the corresponding distance measures, where lower values for all metrics signify better performance. In Table 8, ER and SBM stand for traditional baselines Erdos\u2013R\u00e9nyi model (60) anf SBM stochastic block model (61), respectively. Furthermore, as deep learning based baselines, Feature-based MF represents feature-based matrix factorization (62), GAE and VGAE stand for graph autoencoder and variational graph autoencoder (63), respectively. Finally, GraphMaker in Table 8 corresponds to a diffusion-based graph generation baseline (47). Note that all these baselines are fairness-agnostic. Overall, results in Table 8 signify that FairWire can create synthetic graphs that follow a similar distribution to the original data while also improving the fairness metrics. ", "page_idx": 17}, {"type": "text", "text": "G Implementation Details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Link Prediction Model. For link prediction, we train a one-layer graph convolutional network (GCN), where the inner product between the output node representations signifies the corresponding edge probability. For training, $80\\%$ of the edges are used, where the remaining edges are split equally into two for the validation and test sets. For link prediction experiments, results are obtained for five random data splits, and their average along with the standard deviations are reported. The weights of the GNN model for link prediction are initialized utilizing Glorot initialization (64), where it is trained for 1000 epochs by employing Adam optimizer (65). The learning rate, the dimension of hidden representations, and the dropout rate are selected via grid search for the proposed scheme and all baselines, where the value leading to the best validation set performance is selected. For learning rate the, the dimension of hidden representations, and the dropout rate, the corresponding hyperparameter spaces are $\\left\\{1e-1,1e-2,3e-3,1e-3\\right\\}$ , $\\{32,128,5\\bar{1}2\\}$ , and $\\{0.0,0.1,\\bar{0}.2\\}$ , respectively. ", "page_idx": 17}, {"type": "table", "img_path": "V0JvwCQlJe/tmp/e1ee5b4a28868a24ee39af376d850bb8a1777afa5c8300b5ce62dc9d54bcd13e.jpg", "table_caption": [], "table_footnote": ["Table 8: Distances of statistical measures between the real graph and synthetic ones. "], "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "Diffusion Model. Diffusion models are trained for 10000 epochs by employing Adam optimizer (65), where the number of diffusion steps is 3. In the MPNN described in Subsection 5.2, hidden representation size for time step $t$ is 32 for Cora and Citeseer and 16 for the Amazon Photo, German credit, and Pokec-n networks. In addition, hidden representation sizes for the nodal attributes and sensitive attributes are 512 and 64, respectively, for all datasets. The MPNN consists of two layers. ", "page_idx": 18}, {"type": "text", "text": "Node Classification Model. For node classification, we employ one-layer and two-layers APPNP (66) networks for German credit and Pokec-n graphs, respectively. For training, $50\\%$ of the nodes are used, where the remaining nodes are split equally into two for the validation and test sets. The weights of the GNN model for node classification are initialized utilizing Glorot initialization (64), where it is trained for 1000 epochs by employing Adam optimizer (65). The learning rate, the dimension of hidden representations, and the dropout rate are selected via grid search for the proposed scheme and all baselines, where the value leading to the best validation set performance is selected. For learning rate the, the dimension of hidden representations, and the dropout rate, the corresponding hyperparameter spaces are $\\{3e-2,1e-2,3e-3\\}$ , $\\{32,128,512\\}$ , and $\\{0.0,0.1\\}$ , respectively. ", "page_idx": 18}, {"type": "text", "text": "Hyperparameter Selection. For the link prediction task, we select the multiplier of $\\mathcal{L}_{\\mathrm{FairWire}}$ among the values $\\left\\lbrace0.01,0.05,0.1,0.5\\right\\rbrace$ via grid search (the multiplier of the cross-entropy loss is 1). The results for $\\mathcal{L}_{\\mathrm{FairWire}}$ in Table 2 are obtained for the $\\lambda$ values $0.05,0.1,0.01/0.05,0.1$ on Cora, Citeseer, Amazon Photo, and Amazon Computer, respectively. For adversarial regularization (9), the multiplier of the regularizer is selected via a grid search among the values $\\{0.1,1,10\\}$ (the multiplier of link prediction loss is again 1). The multiplers of the adversarial regularization for the results in Table 2 are $\\{10,1,1,1\\}$ on Cora, Citeseer, Amazon Photo, and Amazon Computer, respectively. Furthermore, the hyperparameter $\\delta$ in FairDrop algorithm is tuned among the values $\\{0.16,0.25\\}$ (0.16 is the default value in their codes), where 0.16 led to the best fairness/utility trade-off on each dataset. For FairAdj (19), we use the hyperparameter values suggested by (19) directly for the citation networks. ", "page_idx": 18}, {"type": "text", "text": "For the generative models, we select the multiplier of $\\mathcal{L}_{\\mathrm{FairWire}}$ ( $\\lambda$ in (7)) among the values $\\{0.05,0.1,1.0,10.0\\}$ via grid search. The results for FairWire in Table 3 are reported for the $\\lambda$ values 10.0, 0.1 on Cora, and Citeseer, respectively $\\lambda\\:=\\:0.05$ for Amazon photo in Table 11). For the results in Table 4, $\\lambda$ equals to 10 and 1 for German credit and Pokec-n, respectively. For adversarial regularization (9), the multiplier of the regularizer (again in the training loss of the MPNN) is selected via a grid search among the values $\\{0.001,0.01,0.1\\}$ . The multiplers of the adversarial regularization for the results in Table 3 are $\\{0.01,0.01,0.01\\}$ on Cora, Citeseer, and Amazon Photo, respectively. Furthermore, the hyperparameter $\\eta$ in FairAdj (19) algorithm is tuned among the values $\\{0.{\\bar{0}}01,0.{\\dot{0}}05,0.01\\}$ , where 0.001 led to the best fairness/utility trade-off on each dataset. ", "page_idx": 18}, {"type": "text", "text": "Baselines. Fairness-aware baselines in the experiments include adversarial regularization (9), FairDrop (37), FairAdj (19), and FairGen (49). Adversarial regularization refers to the bias mitigation technique where an adversary is trained to predict the sensitive attributes. In link prediction, the adversary is trained to predict the sensitive attributes of the nodes that are incident to the edges whose labels are of interest. Furthermore, FairDrop (37) is an edge dropout strategy where the dropout probabilities vary for intra- or inter-edges so as to create a more balanced graph topology. FairAdj (19) optimizes a fair adjacency with certain structural constraints for link prediction in an iterative manner considering both fairness and utility. Finally, FairGen (35) focuses on the disparities in generation quality (distances between different graph statistics) for different sensitive groups and employs fairness-aware regularizers during graph generation via a transformer-based model. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "For graph generation experiments, GraphMaker (47) is utilized to create synthetic graphs in a fairnessagnostic way. While an existing work that considers fair link prediction for synthetic graphs is not available to the best of our knowledge, we employ adversarial regularization (9) as an in-processing bias mitigation strategy during the training of the MPNN described in Subsection 5.2. Furthermore, we use FairAdj (19) as a post-processing bias mitigation strategy on the synthetic graphs generated via GraphMaker, and the processed synthetic graphs are then evaluated for the link prediction and node classification tasks. ", "page_idx": 19}, {"type": "text", "text": "H Sensitivity Analysis ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In order to examine the impact of hyperparameter selection on fairness improvements, the sensitivity analyses for the proposed tools are executed with respect to the hyperparameter $\\lambda$ .The results are obtained for changing $\\lambda$ values for both the link prediction (see (4)) and graph generation (see (7)) experiments and reported in Tables 9, 10. Overall, the results signify that both $\\mathcal{L}_{\\mathrm{FairWire}}$ in the link prediction and FairWire, lead to better fairness measures compared to the natural baselines within a wide range of hyperparameter selection. ", "page_idx": 19}, {"type": "table", "img_path": "V0JvwCQlJe/tmp/f2504461f93f1f03a5f0e246204f0bbccf685f25255bc5e3be96991740368e21.jpg", "table_caption": ["Table 9: Sensitivity Analyses for Different Tasks "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "V0JvwCQlJe/tmp/1a38695bdaaff0c3ab76536d5ae58a9bf58499a40f9e65131608b74df5f7bdb3.jpg", "table_caption": ["Table 10: Sensitivity Analysis for Graph Generation on Cora "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "I Additional Graph Generation Results ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Due to limited space, we present the comparative results for graph generation on Amazon Photo in Table 11 for link prediction. ", "page_idx": 20}, {"type": "table", "img_path": "V0JvwCQlJe/tmp/df7273ae2b671626c784f1e998247f5f63f45323d71f4bbe9a075f49a025912d.jpg", "table_caption": ["Table 11: Graph generation results on Amazon-Photo "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Similar to the link prediction experiments (Table 2), the results of FairAdj for the Amazon Photo network could not be obtained due to computational limitations. Overall, the results reported in Table 11 conclude again that FairWire can provide the best utility/fairness trade-off compared to other fairness-aware baselines. ", "page_idx": 20}, {"type": "text", "text": "J Computational Resources ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Experiments are carried over on 4 NVIDIA RTX A4000 GPUs. ", "page_idx": 20}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: For c1, please see Subsection 4.1. The contribution in c2 is presented in 4.2. For c3 and c4, please check Subsections 5.1 and 5.2, respoectively. Finally, the experimental results emphasized in c5 are presented in Section 6. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: Please see Limitations in Section 7. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Our assumptions are explicitly presented in Subsection 4.1. Furthermore, all proofs are available in Appendices B and C. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The proposed regularizer is presented with its derivation in Subsections 4.1 and 4.2. Furthermore, for reproducibility of the empirical evaluation, our codes are provided within the supplementary material of this submission. All experimental settings are also described in Subsection 6.1, and Appendix G. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 22}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Codes to reproduce all the results in Section 6 are provided in the supplementary material to this submission. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 23}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Please see Subsection 6.1 and Appendix G. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 23}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: All experimental results are obtained for multiple random seeds and their average together with standard deviations are reported. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 23}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 24}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Computational resources used for the experiments are clarified in Appendix J. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: To the best of our knowledge, our research is completely compliant with the NeurIPS Code of Ethics. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 24}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: This work addresses possible bias propagated by ML algorithms. Hence, it in fact helps mitigate a crucial negative impact of ML. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 24}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 25}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 25}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Citation and Amazon networks are provided by the DGL library (67), which is explained and the corresponding license is explicitly provided in the README file in our supplementary material. Furthermore, the Pokec dataset, used in this study, was created and presented in (68), which is again mentioned in the README flie in our supplementary material. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. \u2022 The authors should cite the original paper that produced the code package or dataset. \u2022 The authors should state which version of the asset is used and, if possible, include a URL. ", "page_idx": 25}, {"type": "text", "text": "\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 26}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 26}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not release new assets. \u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. \u2022 The paper should discuss whether and how consent was obtained from people whose asset is used. \u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 26}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 26}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA]   \nJustification:   \nGuidelines: \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}]