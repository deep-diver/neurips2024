[{"type": "text", "text": "Generating Origin-Destination Matrices in Neural Spatial Interaction Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ioannis Zachos1\u2217 Mark Girolami1,2 Theodoros Damoulas2,3 ", "page_idx": 0}, {"type": "text", "text": "1Department of Engineering, Cambridge University, Cambridge, CB2 1PZ. 2The Alan Turing Institute, London, NW1 2DB. 3Departments of Statistics & Computer Science, University of Warwick, Coventry, CV4 7AL. iz230@cam.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Agent-based models (ABMs) are proliferating as decision-making tools across policy areas in transportation, economics, and epidemiology. In these models, a central object of interest is the discrete origin-destination matrix which captures spatial interactions and agent trip counts between locations. Existing approaches resort to continuous approximations of this matrix and subsequent ad-hoc discretisations in order to perform ABM simulation and calibration. This impedes conditioning on partially observed summary statistics, fails to explore the multimodal matrix distribution over a discrete combinatorial support, and incurs discretisation errors. To address these challenges, we introduce a computationally efficient framework that scales linearly with the number of origin-destination pairs, operates directly on the discrete combinatorial space, and learns the agents\u2019 trip intensity through a neural differential equation that embeds spatial interactions. Our approach outperforms the prior art in terms of reconstruction error and ground truth matrix coverage, at a fraction of the computational cost. We demonstrate these benefits in large-scale spatial mobility ABMs in Cambridge, UK and Washington, DC, USA. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "High-resolution complex simulators such as agent-based models (ABMs) are increasingly deployed to assist policymaking in transportation [10, 20], social sciences [3, 8, 14, 33], and epidemiology [16, 21]. They simulate individual agent interactions governed by stochastic dynamic systems, giving rise to an aggregate, in a mean field sense, continuous emergent structure. This is achieved by computationally expensive forward simulations, which hinders ABM parameter calibration and large-scale testing of multiple policy scenarios [24]. Considering ABMs for the COVID-19 pandemic [16] as an example, the continuous mean field process corresponds to the spatial intensity of the infections which is noisily observed at some spatial aggregation level, while the individual and discrete human contact interactions that give rise to that intensity are at best partially observed or fully latent. In transportation and mobility, running examples in this work, the continuous mean field process corresponds to the spatial intensity of trips arising from unobserved individual agent trips between discrete sets of origin and destination locations [10]. ", "page_idx": 0}, {"type": "text", "text": "The formal object of interest that describes the discrete count of these spatial interactions, e.g. agent trips between locations, is the origin-destination matrix (ODM). It is an $I\\times J$ (two-way) contingency table $\\mathbf{T}$ with elements $T_{i,j}\\,\\in\\,\\mathbb{N}$ counting the interactions of two spatial categorical variables $i,j\\in\\mathbb{N}_{>0}$ , see Fig. 1. It is typically sparse due to infeasible links between origin and destination locations, and partially observed through summary statistics \u2013 such as table row and/or column marginals \u2013 due to privacy concerns, data availability, and data collection costs. Operating at ", "page_idx": 0}, {"type": "image", "img_path": "eUcyIe1AzY/tmp/0c003ab037ec9e76a6f8d443ceca084cea49c0376d3911b25d73302a97438c30.jpg", "img_caption": ["$R_{i}$ :Region i $\\mathcal{C}$ :Summary statistic constraint Cij: Travel cost from i to $j$ :Origin/Destination :Discrete ${T}_{i j}$ :Continuous $\\Lambda_{i j}$ $\\bigcirc$ :Observed log destination attraction $y_{j}$ $\\propto$ marker size) "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: The ground truth discrete ODM (two-way contingency table) can be reconstructed through either multiple expensive ABM simulations [2] (ABM rectangle) or approximated by a continuous representation $\\pmb{\\Lambda}$ coupled with the Harris-Wilson SDE (SIM rectangle). In the latter, the ground truth can be reconstructed by sampling in the discrete combinatorial space of constrained ODMs conditioned on $\\pmb{\\Lambda}$ (GENSIT rectangle). ABM simulations scale with ${\\bar{O}}(M\\log(M))$ compared to GENSIT which scales with $\\mathcal{O}(I J)$ , where $M\\gg I+J$ is the size of the agent interaction graph. ", "page_idx": 1}, {"type": "text", "text": "the discrete ODM level and learning this latent contingency table from summary statistics is vital for handling high-resolution spatial constraints and partial observations such as the total number of agents interacting between a pair of locations. It is also necessary for population synthesis in ABMs [15], which is performed prior to simulation in order to reduce the size of the ABM\u2019s parameter space. Moreover, it avoids errors and biases due to ad-hoc discretisation required when working with continuous approximations of the underlying discrete ODM $\\mathbf{T}^{*}$ . ", "page_idx": 1}, {"type": "text", "text": "Traditional ABMs, Fig. 1 (Left), simulate individual-level and spatially granular discrete ODMs at a high computational cost [2], which scales at least with ${\\mathcal{O}}({\\bar{M}}\\log({\\bar{M}}))$ , where $M\\gg I+J$ is the size of the agent interaction graph. These ODMs are then aggregated using a sum pooling operation to regional ones, $\\mathbf{T}^{(s)}$ , whose summary statistics $\\mathcal{C}$ correspond to observed data. ", "page_idx": 1}, {"type": "text", "text": "However, the lower-dimensional subspace of contingency tables $\\mathbf{T}$ satisfying constraints $\\mathcal{C}$ (e.g. row and column marginals), denoted by $\\tau_{c}$ , is known to be combinatorially large [11] and therefore sampling and optimisation in that space is notoriously hard since it requires enumerating all elements of $\\tau_{c}$ . This underlying challenge is the reason why prior art [34, 13, 40, 31, 26, 17] has been imposing a continuous relaxation of the ODM estimation problem, where the continuous approximation of the discrete contingency table restricts inference at the agent trip intensity level $\\Lambda$ . This leads to quantisations and inefficient rejection sampling, see Fig. 2. ", "page_idx": 1}, {"type": "text", "text": "Such intensity-level Spatial Interaction Models (SIMs) [44, 30], Fig. 1 (Right), are derived from entropy maximisation arguments, Sec. 2, with summary statistics constraints. These models are embedded in the Harris-Wilson (HW) system of differential equations [19] that describe the time evolution of location attraction and reflects the utility gained from reaching a destination. Coupling SIMs with the HW model introduces an inductive bias that regularises the continuous ODM and facilitates a mean-field ABM approximation. This approximation effectively acts as a cheap ABM surrogate or emulator, facilit", "page_idx": 1}, {"type": "image", "img_path": "eUcyIe1AzY/tmp/e09473975bb5c30778ce767c85877d57f4457a0dabba5ecf280d29df57d591c4.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 2: The space $\\tau_{c}$ of $3\\times3$ discrete ODMs with summary statistics $\\mathcal{C}_{T}$ . Sampling on the continuous relaxation of $\\tau_{c}$ ( $\\Lambda$ level) with quantisation can lead to either large rejection rates, or poor exploration of the distribution over $\\tau_{c}$ . ", "page_idx": 1}, {"type": "text", "text": "ating faster forward simulations to be run by practitioners [4]. This has tangible benefits to ABM calibration allowing faster exploration of the parameter space. Our enhanced ODM reconstruction error demonstrates GeNSIT\u2019s ability to sufficiently approximate the ABM simulator at a fraction of the computational cost. However, if both the continuous ODM row and column marginals are fixed, the HW model becomes redundant and the SIM can either be greedily approximated through iterative proportional fitting [44] which is sensitive to initialisation or become unidentifiable as the number of parameters scales with both the number of origins and destinations. Furthermore, conditioning on individual continuous ODM cells creates discontinuities as the SIM becomes a piecewise function, also preventing its coupling into the HW model. ", "page_idx": 1}, {"type": "text", "text": "Competing approaches that operate directly on the discrete ODM space and are motivated by econometric arguments are discrete choice models [43]. However, these cannot encode summary statistic constraints without introducing large rejection rates. The work of [9] leverages SIMs to sample discrete ODMs but removes intensity constraints through log-linearity assumptions and does not exploit the physics structure, effectively stripping SIMs of their advantages over other choice models. Markov Chain Monte Carlo routines have been devised to address these issues by learning the SIM parameters [13] and the associated discrete ODM table over its entire support [46]. Such routines incur a computational overhead in the order of at least $I$ or $J$ due to the intractability of the Harris-Wilson model prior, rendering them prohibitive for large-scale applications. Neural Network (NN) parameter calibration of SIMs has been empirically shown to achieve up to ten-fold speed-ups during training [17] by using a numerical discretisation scheme for the Harris-Wilson model as a forward solver. Despite the significant advantages offered by NNs, they operate strictly in the continuous intensity level and cannot generate discrete agent-level ODMs. ", "page_idx": 2}, {"type": "text", "text": "In this paper, we introduce a computationally scalable framework named Generating Neural Spatial Interaction Tables (GENSIT) for exploring the constrained discrete ODM space using closed-form or Gibbs Markov Basis sampling while neurally calibrating the underlying physics-driven SIM parameters of its continuous representation, as shown in Fig. 3. Our framework scales linearly with the number of origin-destination pairs $I J$ , which is at least $I$ or $J$ times faster than MCMC [13, 46]. We offer faster ODM reconstruction and enhanced uncertainty quantification compared to continuous approaches in seminal works [13, 17] and hybrid (discrete and continuous) approaches [46] in terms of the number of iterations required. It is the first framework that jointly explores the constrained continuous and discrete combinatorial ODM spaces in linear in the number of origin-destination pairs time. It does so while outperforming the prior art in terms of reconstruction error and ground truth table coverage while enabling the integration of a broader set of constraints, if these are available. ", "page_idx": 2}, {"type": "text", "text": "Our framework has merit beyond ODM sampling in ABMs. The challenge of learning discrete contingency tables constrained by their summary statistics extends to other fields. Contingency tables have been widely studied in multiple instance learning [32, 12, 47] and ecological inference [36, 35, 38]. In Neuroscience one estimates the efficiency, cost and resilience (equivalent to $T_{i j}$ ) of neural pathways between pairs of brain regions $(i,j)$ to understand communication and information processing [37],[18]. Epidemiology also investigates social contact matrices quantifying the number of contacts $(T_{i j})$ between types of individuals $(\\bar{i},j)$ stratified by demographics, such as age [28]. ", "page_idx": 2}, {"type": "text", "text": "2 Spatial Interaction Intensities and Contingency Tables ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Consider $A$ agents travelling from $I$ residences (origins) to $J$ workplaces (destinations). The expected number of trips (intensity) between origin $i$ and destination $j$ is $\\Lambda_{i j}$ and is unobserved. The average number of agents starting (ending) their journey from each origin $i$ (to each destination $j$ ) is: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\Lambda_{i+}=\\sum_{j=1}^{J}\\Lambda_{i j},\\quad i=1,\\ldots,I,\\qquad\\Lambda_{+j}=\\sum_{i=1}^{I}\\Lambda_{i j},\\quad j=1,\\ldots,J.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The expected total number of agents travelling is assumed conserved: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\Lambda_{++}=\\sum_{i=1}^{I}\\Lambda_{i+}=\\sum_{j=1}^{J}\\Lambda_{+j}=A.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The family of models for intensities $\\Lambda$ that assimilate any collection of the above constraints are called Spatial Interaction Models [44]. The demand for each destination depends on its attractiveness denoted by $\\mathbf{z}:=(z_{1},\\dots,z_{J})\\in\\mathbb{R}_{>0}^{J}$ . In our example, this is the number of jobs available at each destination. Let the log-attraction be $\\mathbf{x}:=\\log(\\mathbf{z})$ . Between two destinations of similar attractiveness, iangterontdsu caered  taos sreufmleecdt  ttroa vperl eifmerp endeaanrcbey.  dTehsetsien aatsisounsm.p tiTohnesr eafroe rjeu,s tai fcieods tb ym eactroinxo $\\mathbf{C}\\,=\\,(c_{i,j})_{i,j=1}^{I,J}$ 3 0is] and establish the basis for the agents\u2019 utility function. The maximum entropy distribution of agent trips subject to $\\Lambda_{++}=A$ yields a totally constrained SIM intensity: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\Lambda_{i j}=\\frac{\\Lambda_{++}\\exp(\\alpha x_{j}-\\beta_{i j})}{\\sum_{k,m}^{I,J}\\exp(\\alpha x_{m}-\\beta c_{k m})},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathcal{C}_{\\Lambda}=\\{\\Lambda_{++}\\}$ is the set of summary statistic constraints on $\\pmb{\\Lambda}$ . Henceforth, we set $\\Lambda_{++}\\in\\mathcal{C}_{\\Lambda}$ unless otherwise stated. The vector $\\pmb{\\theta}=(\\dot{\\alpha},\\beta)$ contains the agents\u2019 two utility parameters controlling the effects of attractiveness and deterrence on the expected number of trips $\\Lambda$ . If $\\alpha$ grows larger relative to $\\beta$ then agents gravitate towards destinations with higher job availability regardless of the travel cost incurred, and vice versa. Further, if we also fix the expected origin demand $\\Lambda_{\\cdot+}$ , then we obtain the following singly (also known as production) constrained SIM intensity: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\Lambda_{i j}=\\frac{\\Lambda_{i+}\\exp(\\alpha x_{j}-\\beta c_{i j})}{\\sum_{m}^{J}\\exp(\\alpha x_{m}-\\beta c_{i m})},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathcal{C}_{\\Lambda}$ is expanded to include $\\Lambda_{\\cdot+}$ in this case. Moreover, setting $\\mathcal{C}_{\\Lambda}=\\{\\Lambda_{++},\\Lambda_{++},\\Lambda_{+}.\\}$ yields a doubly constrained SIM ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\Lambda_{i j}=\\Lambda_{i+}\\Lambda_{+j}\\exp(\\alpha x_{j}-\\beta c_{i j})O(i)D(j),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $O(i),D(j)$ are called balancing factors that ensure that $\\mathcal{C}_{\\Lambda}$ are satisfied. The balancing factors introduce $I+J$ unknown parameters, rendering the intensity model unidentifiable. Alternatively, these factors are approximately recursively using iterative proportional fitting [44], which is sensitive to initialisation. Including individual cell constraints in $\\mathcal{C}_{\\Lambda}$ breaks the continuity of $\\pmb{\\Lambda}$ as a function of its parameters. For these reasons, the doubly and/or cell-constrained SIMs are prohibitive for use in statistical inference. See App. B.1.1 for more information on SIMs as a modelling choice. ", "page_idx": 3}, {"type": "text", "text": "We note that additional data at the origin, destination and origin-destination level can be assimilated into SIMs. This can be achieved by incorporating them as terms in the maximum entropy argument used to derive the $\\Lambda$ functional forms in equations (3), (4), and (5). We note that the SIM\u2019s $\\Lambda$ is equivalent to the multinomial logit [29], which generalises our $\\Lambda$ construction to accommodate for more data. See App. B.1.2 for a guide on eliciting agent utility functions. ", "page_idx": 3}, {"type": "text", "text": "It has been shown that the destination attractiveness $\\mathbf{z}=\\exp(\\mathbf{x})$ in families of SIMs is governed by the Harris-Wilson system of $J$ coupled stochastic differential equations (SDEs) [19, 13]: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}z_{j}}{\\mathrm{d}t}=\\epsilon z_{j}(\\Lambda_{+j}-\\kappa z_{j}+\\delta)+\\sigma z_{j}\\circ B_{j,t},\\,\\,\\,{\\bf z}(0)={\\bf z}^{\\prime}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\epsilon\\,>\\,0$ is a responsiveness parameter, $\\kappa\\,>\\,0$ is the number of agents competing for one job, $\\delta\\geq0$ is a parameter related to the smallest number of jobs a destination can have, $\\sigma\\,>\\,0$ is the standard deviation of SDE\u2019s noise, and $\\mathbf{B}_{t}$ is a $J$ -dimensional Wiener process. The term $\\Lambda_{+j}-\\kappa z_{j}+\\delta$ in (6) reflects the net job capacity at destination $j$ . If more agents travel to $j$ than there are jobs there (positive capacity), this may signify a boost in $j$ \u2019s local economy, which would trigger a rise in job availability, and vice versa. The diffusion term stochastically perturbs this trend to account for unobserved events, such as local government interventions affecting employment. By the HW-SDE, the SIM intensity is a stochastic and physics-driven quantity reflecting the agents\u2019 average number of trips $\\Lambda$ between their residences and workplaces. However, the SIM intensity differs from the realised number of trips $\\mathbf{T}$ agents make. The two notions are connected as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\nT_{i j}|\\Lambda_{i j}\\sim\\mathrm{Poisson}(\\Lambda_{i j}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "$\\mathbf{T}$ is the $I\\times J$ discrete contingency table summarising the number of agents travelling from $i$ to $j$ , and $T_{i j}\\,\\perp\\,T_{i^{\\prime}j^{\\prime}}|\\Lambda_{i j},\\Lambda_{i^{\\prime}j^{\\prime}}\\ \\bar{\\forall}\\ i\\neq\\,i^{\\prime},j\\neq\\,j^{\\prime}$ . Any choice of model for $T_{i j}\\vert\\Lambda_{i j}$ (say Poisson or Binomial) becomes equivalent upon conditioning on sufficient summary statistics $\\mathcal{C}_{T}$ [1]. We note that the conditional independence of the $T_{i j}$ \u2019s given the $\\Lambda_{i j}$ \u2019s and that $\\mathbf{T}$ inherits all constraints from $\\Lambda$ such that every summary statistic constraint in $\\Lambda$ -space is also applied in $\\mathbf{T}$ -space, i.e. we always set $\\Lambda_{++}=\\mathbb{E}[T_{++}|\\mathcal{C}_{T}]$ and $\\Lambda_{i+}=\\mathbb{E}[T_{i+}|\\mathcal{C}_{T}]$ . The hard-coded constraints $\\mathcal{C}_{T}$ are realisations of the Poisson random variables $T_{++}|\\mathbf{A},T_{i+}|\\mathbf{A},T_{+j}|\\mathbf{A}$ , and therefore are no longer random. They can be thought of as noise-free data on the discrete table space. Following the notational convention for $\\pmb{\\Lambda}$ , we define $\\begin{array}{r}{T_{i+}\\,=\\,\\sum_{j=1}^{J}T_{i j}}\\end{array}$ , $\\begin{array}{r}{T_{+j}\\,=\\,\\sum_{i=1}^{I}T_{i j}}\\end{array}$ and $\\begin{array}{r}{T_{++}=\\sum_{i,j=1}^{\\bar{I},J}T_{i j}}\\end{array}$ , respectively. The aforementioned summar y statistics become  Dirac random variables  upon conditioning on . The union of table and intensity constraints is summarised by $\\mathcal{C}$ . We sometimes drop subscripts from $\\mathcal{C}$ for clarity. See App. A for more information on the notation used. ", "page_idx": 3}, {"type": "text", "text": "3 Neural Calibration of Spatial Interaction Models ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We now introduce our framework1 (GENSIT). We estimate the parameters of the continuous SIM intensity by employing an ensemble of Neural Networks $\\psi_{N N}:\\vec{\\mathbb{R}}^{J}\\rightarrow\\mathbb{R}^{2}$ . This allows us to bypass the computational challenges of solving the HW-SDE inverse problem within a Bayesian framework [13, 46]. Conditioned on those estimates and for every ensemble member $e=1,\\ldots,E$ , we solve the HW-SDE to get estimates of the time-evolved log destination attraction $\\hat{\\bf x}$ after $\\tau$ time steps using an Euler-Maruyama numerical solver $\\phi_{H W}:\\mathbb{R}^{\\overline{{J}}}\\to\\mathbb{R}^{J}$ [27]. This allows us to incorporate the HW physics model into our parameter estimation without sampling from the SDE\u2019s intractable steady-state Boltzmann-Gibbs distribution, which was the case in [13, 46]. ", "page_idx": 3}, {"type": "image", "img_path": "eUcyIe1AzY/tmp/37c7812393d5c571c7e92e46a88d4c57d7d0b75a6a81e8eb0b566e9f21bf0974.jpg", "img_caption": ["Figure 3: GENSIT: (a) successive iterations of $\\mathrm{Alg}$ . 1 for a given ensemble member, (b) plate diagram for every iteration, ensemble member. We propose two schemes: a Joint and a Disjoint (see App. B.3.1 for details). Contrary to the latter, the former passes table $\\mathbf{T}$ information to the loss $\\mathcal{L}$ (see $--\\sqrt{-}$ in (b)). We perform an optimisation step in the intensity $\\Lambda$ space and a sampling step in $\\mathbf{T}$ space, with associated complexities $\\mathcal{O}(\\tau J+I J)$ and $\\mathcal{O}(I J)$ . The $\\pmb{\\Lambda}$ arises by the well-known family of SIMs (3),(4) coupled with the HW-SDE (6). The $\\mathbf{T}$ sampling step generates discrete $\\mathcal{C}_{T}$ -constrained ODMs contrary to [13, 17], which only operate on the continuous mean-field level $\\Lambda$ . "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Instead of a log-destination attraction data (y) likelihood, we compute a loss operator $\\mathcal{L}(\\cdot\\,;\\,\\mathcal{D},\\pmb{\\nu})$ that can assimilate data $\\mathcal{D}$ from multiple sources on any transformation of $\\hat{\\mathbf{T}},\\hat{\\mathbf{\\Lambda}},\\hat{\\mathbf{x}}$ . We note that $\\pmb{\\nu}$ is used to denote the loss hyperparameters (see Tab. 5). Then, the NN parameters (weights and biases) W are updated using back-propagation using a suite of optimisation algorithms [22]. This step requires derivatives of the loss with respect to the NN parameters $\\nabla_{\\mathbf{W}}\\mathcal{L}(\\cdot\\,;\\,\\mathcal{D},\\nu)$ to be computed, which is achieved using off-the-shelf auto-differentiation libraries [25]. This gradient is informed by $\\hat{\\bf x}$ estimates and therefore by the dynamics of the HW-SDE in (6). Hence, the resulting SIM intensity is both stochastic and physics-driven. These steps are depicted in Fig. 3b by following the arrows from right to left and in steps 6 to 12 of Alg. 1. We note that our framework is divided into two sampling schemes: a Joint and a Disjoint. The former passes $\\mathbf{T}$ information to the loss operator $\\mathcal{L}$ , whereas the latter does not. The Joint scheme corresponds to a Gibbs sampler on the full posterior marginals $\\theta|(\\mathbf{x},\\mathbf{T},\\mathcal{C},\\mathcal{D})$ , ${\\bf x}|(\\pmb{\\theta},{\\bf T},\\mathcal{C},\\mathcal{D})$ and $\\mathbf{T}|(\\pmb\\theta,\\mathbf x,\\mathcal{C},\\mathcal{D})$ . The Disjoint scheme corresponds to a collapsed Gibbs sampler where we sample from $\\pmb{\\theta}|(\\mathbf{x},\\mathcal{C},\\mathcal{D})$ , ${\\bf x}|(\\pmb\\theta,\\mathcal{C},\\mathcal{D})$ and then from $\\mathbf{T}|(\\pmb{\\theta},\\mathbf{x},\\mathcal{C},\\bar{D})$ by integrating out $\\mathbf{T}$ (see App. B.3.1). ", "page_idx": 4}, {"type": "text", "text": "The described optimisation routine yields a point estimate for $\\Lambda$ for any given ensemble member $e$ and iteration $n$ . We either increase the ensemble size $E$ to obtain a distribution over $\\Lambda$ , or we can treat $\\Lambda$ as a deterministic mapping of realisations of random variables $\\mathbf{x},\\pmb{\\theta}$ whose generalised posterior [7, 23] is: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{p(\\mathbf{x},\\pmb{\\theta}|\\mathcal{D})\\propto\\exp(-\\mathcal{L}(\\mathbf{x},\\pmb{\\theta}\\,;\\,\\mathcal{D},\\pmb{\\nu}))p(\\mathbf{x}|\\pmb{\\theta})p(\\pmb{\\theta}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\pmb{\\nu}$ are loss-related hyperparameters. Each loss evaluation can be treated as a sample from a generalised likelihood (first term), while samples from the $\\mathbf{x}$ prior (second term) are obtained by forward solving $\\phi_{H W}$ . Finally, we can enforce a prior over $\\pmb{\\theta}$ by appropriately initialising the NN parameters $\\mathbf{W}$ . ", "page_idx": 4}, {"type": "text", "text": "The continuous agent trip intensity $\\Lambda$ (continuous ODM) samples need to be mapped to discrete agent trips $\\mathbf{T}$ (discrete ODM), which are required for agent population synthesis. We proceed by introducing the necessary machinery to achieve this. We wish to sample from the target measure $\\mu$ evaluated as $\\mu(\\mathbf{T}|\\mathbf{A},{\\mathcal{C}})$ . ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "3.1 Constrained Table Sampling ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Let the table cells be indexed by $\\mathcal{X}=\\{(i,j):1\\leq i\\leq I,1\\leq j\\leq J\\}$ such that $T(x)=T_{i j}$ is the table value of cell $x=(i,j)\\in\\dot{\\boldsymbol{\\chi}}$ . All non-negative two-way contingency tables $\\mathbf{T}$ are assumed to be members of a discrete space $\\tau$ . The $k$ -th basis operator $\\dot{\\mathbb{1}_{k}}:\\mathcal{X}_{k}\\overset{=}{\\rightarrow}\\{0,1\\}^{I+J}$ is defined to be ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{1}_{k}(x)=(0,\\ldots,0,\\underbrace{\\underbrace{1}_{\\mathrm{entry~}i},0,\\ldots,0,\\underbrace{1}_{\\mathrm{entry~}I+j}}_{I+J\\mathrm{\\entries}},0)\\,.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Hence, we can define summary statistic operators $S_{k}:\\mathcal{T}\\rightarrow\\mathbb{N}^{I+J}$ as linear combinations of the basis operator, that is $\\begin{array}{r}{S_{k}(\\mathbf{T})=\\sum_{x\\in\\mathcal{X}_{k}}\\mathbf{T}(x)\\mathbb{1}_{k}(x)}\\end{array}$ . A collection of such summary statistic operators is abbreviated by ${\\pmb S}({\\bf T})$ . ", "page_idx": 5}, {"type": "table", "img_path": "eUcyIe1AzY/tmp/8efde1984283a755e75616e01ff40699ee4d5d5985e2671e12adb43525c9674c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "Constraints on $\\mathbf{T}$ can be expressed as fixed summary statistics $\\mathcal{C}_{T}=\\left\\{\\mathbf{s}_{1},\\ldots,\\mathbf{s}_{K}\\right\\}$ , where each $\\mathbf{s}_{k}$ is a fixed evaluation of $S_{k}(\\mathbf{T})$ with respect to the basis operator. For example, constraint $\\{\\mathbf{T}._{+},\\mathbf{T}_{+}.\\}$ can be expressed in terms of the basis operator (9) over the entire cell set $\\mathcal{X}$ . ", "page_idx": 5}, {"type": "text", "text": "Definition 3.1. Let $\\mathcal{C}_{T}=\\left\\{\\mathbf{s}_{1},\\ldots,\\mathbf{s}_{K}\\right\\}$ be a set of constraints based on summary statistics operators $_s$ . A table $\\mathbf{T}$ is $\\mathcal{C}_{T}$ -admissible if and only if $\\pmb{S}(\\mathbf{T})=\\{\\mathbf{s}_{k}\\}_{k=1}^{K}\\in\\mathcal{C}_{T}$ . ", "page_idx": 5}, {"type": "text", "text": "The subspace of $\\tau$ containing all $\\mathcal{C}_{T}$ -admissible $I\\times J$ contingency tables is $\\mathcal{T}_{\\mathcal{C}}=\\{\\mathbf{T}\\in\\mathcal{T}:\\mathcal{S}(\\mathbf{T})=$ $\\mathcal{C}_{T}\\}$ . This space contains all discrete ODMs consistent with the aggregate summary statistics $\\mathcal{C}_{T}$ . Our goal is to efficiently sample from a measure $\\mu$ on $\\tau_{c}$ subject to arbitrary $\\mathcal{C}_{T}$ . ", "page_idx": 5}, {"type": "text", "text": "The target distribution is tractable, i.e. the entire table can be sampled directly in closed-form, if and only if the universe of summary statistic constraints contains at most one table marginal (row or col. sum). This covers the cases where $\\mathcal{C}_{T}$ tractable $\\subseteq\\,{\\mathcal{P}}(\\{T_{++},\\mathbf{T}..+,\\{\\mathbf{T}_{\\mathcal{X}_{l}}|{\\mathcal{X}}_{l}\\subseteq\\mathcal{X},l\\ \\in\\}\\!\\!)$ $\\mathbb{N}\\backslash\\})\\cup\\mathcal{P}(\\{T_{++},\\mathbf{T}_{+}.,\\{\\mathbf{T}_{\\mathcal{X}_{l}}|\\mathcal{X}_{l}\\subseteq\\mathcal{X},l\\in\\mathbb{N}\\}\\})$ ). Note that the unconstrained case of $\\mathcal{C}_{T}\\,=\\,\\emptyset$ is handled by the construction in (7). This facilitates sampling $\\mathbf{T}^{(1:N)}$ in closed-form and in parallel as shown in step 22 of Alg. 1. The most notable cases of tractable distributions are (see App. B.2.1): ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\bf T}|{\\cal\\Lambda},T_{++}\\sim\\mathrm{Multinomial}(T_{++},{\\cal{\\Lambda}}/\\Lambda_{++});}}\\\\ {{\\displaystyle{\\bf T}|{\\cal\\Lambda},{\\bf T}_{+}\\sim\\prod_{i=1}^{I}\\mathrm{Multinomial}(T_{i+},{\\cal{\\Lambda}}/\\Lambda_{i+});}}\\\\ {{\\displaystyle{\\bf T}|{\\cal\\Lambda},{\\bf T}_{+\\cdot}\\sim\\prod_{j=1}^{J}\\mathrm{Multinomial}(T_{+j},{\\cal{\\Lambda}}/\\Lambda_{+j}).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Each of the distributions above can assimilate cell constraints of the form $\\mathcal{C}_{T}=\\{\\mathbf{T}_{\\mathcal{X}_{l}}|\\mathcal{X}_{l}\\subseteq\\mathcal{X},l\\in\\mathbb{N}\\}$ without violating $\\mu$ \u2019s tractability by limiting the support of $\\mathbf{T}|\\mathbf{A},\\mathcal{C}$ . If the constraint set $\\mathcal{C}_{T}$ contains at least both marginals, that is $\\mathcal{C}_{T}^{\\mathrm{\\scriptsize~intractable}}\\subseteq\\mathcal{P}(\\{\\mathbf{T}_{+},\\mathbf{T}_{+}.,\\{\\mathbf{T}_{\\mathcal{X}_{l}}|\\mathcal{X}_{l}\\subseteq\\mathcal{X},l\\in\\mathbb{N}\\}\\})\\setminus\\mathcal{C}_{T}^{\\mathrm{\\scriptsize~tractable}}$ , then the target distribution becomes Fisher\u2019s non-central multivariate hypergeometric [1, 42]: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\frac{\\prod_{i=1}^{I}T_{i+}!\\prod_{j=1}^{J}T_{+j}!}{T_{++}!\\prod_{i,j=1}^{I,J}T_{i j}!}\\prod_{i,j=1}^{I,J}\\left(\\frac{\\Lambda_{i j}\\Lambda_{++}}{\\Lambda_{i+}\\Lambda_{+j}}\\right)^{T_{i j}}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Direct sampling without rejection from this distribution for arbitrary $I,J$ is infeasible [1]. ", "page_idx": 6}, {"type": "text", "text": "3.1.1 Markov Basis MCMC ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Therefore, we devise an MCMC proposal on $\\tau_{c}$ . Using a suite of greedy deterministic Algorithms [6], we can initialise our MCMC with a $\\mathbf{T}^{(0)}\\in\\mathcal{T}_{\\mathcal{C}}$ . By virtue of definition 3.2 we guarantee that no proposed moves modify the summary statistics in $\\mathcal{C}_{k}$ (Condition 1) and that there exists a path between any two tables such that any table member of the path is $\\mathcal{C}_{k}$ -admissible (Condition 2). The collection $\\mathcal{C}$ of $K$ constraints generates $K$ sets of Markov bases $\\mathcal{M}_{1},\\...\\,,\\mathcal{M}_{K}$ . Our proposal mechanism consists of the universe $\\begin{array}{r}{\\mathcal{M}=\\bigcap_{k=1}^{K}\\mathcal{M}_{k}}\\end{array}$ . See App. B.2.2 for info on Markov Bases. ", "page_idx": 6}, {"type": "text", "text": "Definition 3.2. : A Markov basis $\\mathcal{M}_{k}$ is a set of moves $\\mathbf{f}_{1},\\ldots,\\mathbf{f}_{L}:\\mathcal{X}\\to\\mathbb{Z}$ satisfying: ", "page_idx": 6}, {"type": "text", "text": "In the case of $I\\times J$ ODMs constrained by $\\mathcal{C}_{T}$ intractable, $\\mathcal{M}$ consists of functions $\\mathbf{f}_{1},\\ldots,\\mathbf{f}_{L}$ such that $\\forall\\;x=(i_{1},j_{1}),x^{\\prime}=(i_{2},j_{2})\\in\\mathcal{X}$ with $i_{1}\\neq i_{2},j_{1}\\neq j_{2}$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbf{f}_{l}(x)={\\left\\{\\begin{array}{l l}{\\eta}&{{\\mathrm{if~}}x=(i_{1},j_{1}){\\mathrm{~or~}}x=(i_{2},j_{2});}\\\\ {-\\eta}&{{\\mathrm{if~}}x=(i_{1},j_{2}){\\mathrm{~or~}}x=(i_{2},j_{1});}\\\\ {0}&{{\\mathrm{otherwise.}}}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "A Gibbs Markov Basis (GMB) sampler can now be constructed (see steps 20 - 22 in Alg. 1). ", "page_idx": 6}, {"type": "text", "text": "Proposition 3.1. (Adapted from [11]): Let $\\mu$ be a probability measure on $\\tau_{c}$ . Given a Markov basis $\\mathcal{M}$ that satisfies 3.2, generate a Markov chain in $\\tau_{c}$ by sampling $l$ uniformly at random from $\\{1,\\ldots,L\\}$ . Let $\\eta\\in\\mathbb{Z}$ . If the chain is at $\\mathbf{T}\\in{\\mathcal{T}}_{\\mathcal{C}}$ , determine $\\mathrm{supp}\\left(\\eta\\right)$ such that $\\mathbf{T}+\\eta\\mathbf{f}_{l}\\ge0$ . Choose ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\eta)\\propto\\prod_{x\\in\\mathcal{X}:\\mathbf{f}_{l}(x)\\neq0}(\\mu(T(x)+\\eta\\mathbf{f}_{l}(x)))^{-1}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "and move to $\\mathbf{T}^{\\prime}=\\mathbf{T}+\\eta\\mathbf{f}_{l}$ for the choice of $\\eta$ . An aperiodic, reversible, connected Markov chain in $\\tau_{c}$ is constructed with stationary distribution proportional to $\\mu(\\mathbf{T})$ . Proof is found in [11]. ", "page_idx": 6}, {"type": "text", "text": "We note that when individual cells are fixed $\\mathbf{\\nabla}_{\\mathbf{T}_{\\mathcal{X}_{l}}}\\in\\mathcal{C})$ , the summary statistics operator is applied over $\\scriptstyle{\\mathcal{X}}_{l}$ instead of $\\mathcal{X}$ . Therefore, the size of $\\mathcal{M}$ is reduced to comply with Definition 3.2. This may shorten the diameter of the Markov Chain\u2019s state space, leading to better mixing times [41]. ", "page_idx": 6}, {"type": "text", "text": "4 Experimental Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We empirically test our framework on both synthetic and real-world data from Cambridge, UK and Washington, DC, USA. We compare GENSIT against SIM-MCMC [13], SIM-NN [17], SIT-MCMC [46] and the Geo-contextual Multitask Embedding Learner (GMEL) [26]. See App. C.1 for the computational complexities of these methods. GMEL is trained on a larger set of data that includes, besides the cost matrix C, destination-level urban indicators $\\mathbf{Y}$ , where the log destination attraction y is a column vector of $\\mathbf{Y}$ . We validate the generated $\\mathbf{T}$ and \u039b samples from all methods against the ground truth ODM $\\mathbf{T}^{*}$ using the Standardised Root Mean Square Error (SRMSE) and the $99\\%$ high probability region cell coverage probability (CP) (see App. C.2 for definitions). See App. D,E,F for experimental protocols, implementation details, a comprehensive sensitivity study on our framework, and reproducibility using our package gensit. ", "page_idx": 7}, {"type": "image", "img_path": "eUcyIe1AzY/tmp/a1d28b40cbfd2495d67a90d40cbd1fd44697191cc10672272e9f678e6eb48f29.jpg", "img_caption": ["4.1 Synthetic ODM Scalability "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 4: Total computation time (left) of GENSIT and SIM-NN [17] versus the number of origin-destination pairs $I J$ with $(I\\times J)$ equal to $100\\times100,200\\times200,\\dotsc,1000\\times1000$ . The two algorithms are run for $N=10^{3}$ iterations with $\\mathcal{C}_{T}=\\{\\mathbf{T}_{++},\\mathbf{T}_{+}.,\\mathbf{T}_{\\mathcal{X}_{50}\\mathcal{Y}_{0}}\\}$ . Constraint $\\mathbf{T}_{\\mathcal{X}_{50\\%}}$ means that $50\\%$ of table cells chosen uniformly at random are fixed. Total computation time is the sum of the $\\mathbf{T}$ sampling (middle) and $\\pmb{\\Lambda}$ learning (right) times. Intensity learning and table sampling times are computed for lines 6-12 and lines 19-24 of Alg. 1, respectively. Our framework scales linearly with $I J$ , which is much faster than SIM-MCMC and SIT-MCMC. SIM-NN does not operate at all in the discrete table space, which explains its faster computational speed. ", "page_idx": 7}, {"type": "text", "text": "We begin by offering synthetic experiments for varying ODM dimensions $(I\\times J)$ and number of agents $A$ , comparing computation time and ground truth reconstruction error (SRMSE) in Figs. 4 and 5. For a fixed $(I\\times J)$ dimension we sample an intensity uniformly at random and subsequently generate a ground truth table $\\mathbf{T}^{*}$ with fixed $A=T_{++}$ by sampling from the Multinomial distribution in (10). For Figs. 4 and 5b we set $T_{++}\\,=\\,10^{6}$ while for Fig. 5a we vary this for $0.01,0.1,0.25,0.5$ and $1\\times\\dot{10}^{6}$ . ", "page_idx": 7}, {"type": "image", "img_path": "eUcyIe1AzY/tmp/9402a0e9c12c887a6ca58f2b21487036c0f87df0857485d05403228a32ad2435.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 5: SRMSE by total number of agents $A$ (left) and ODM dimension $(I\\times J)$ (right) of GENSIT\u2019s discrete ODM sampling (line 24 of Alg. 1) for $N=10^{4}$ iterations, a fixed intensity $\\pmb{\\Lambda}$ and $\\mathcal{C}_{T}\\,=\\,\\{\\mathbf{T}_{+},\\mathbf{T}_{\\mathcal{X}_{50\\mathcal{Y}_{0}}}\\}$ . On the left we set $(I\\times J)=150\\times150$ . The reconstruction error (SRMSE) scales linearly in $I J$ and exponentially in $A=T_{++}$ . ", "page_idx": 7}, {"type": "text", "text": "Our framework achieves a speed-up of ${\\mathcal{O}}(J^{2})$ compared to previous methods such as SIM-MCMC, SIT-MCMC. ", "page_idx": 7}, {"type": "text", "text": "Reconstruction errors grow linearly in dimension $(I\\times J)$ and exponentially in number of agents $A$ . ", "page_idx": 7}, {"type": "text", "text": "4.2 Real-world ODM Reconstructions and Predictions ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We now assess our framework\u2019s capacity to estimate agent home-to-work trips in real-world ODM prediction problems in Cambridge, UK [46] and Washington DC, USA [26]. ", "page_idx": 7}, {"type": "text", "text": "4.2.1 Cambridge, UK ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In the Cambridge dataset, the ground truth ODM is a $69\\times13$ contingency table with 33, 704 agents. Tab. 1 shows that reconstruction error (SRMSE) and the $\\%$ of ground truth cells covered by the $99\\%$ high probability region of the ODM samples (CP) are significantly improved when operating in $\\mathbf{T}$ level using our Joint scheme compared to SIM-MCMCand SIM-NN, which only operate on the $\\Lambda$ level. We also outperform the competitive SIT-MCMC approach [46], which can operate in the discrete ODM level, since in the limit of $\\mathcal{C}_{T}$ our method can move to high $\\mathbf{T}$ probability regions much faster than MCMC due to the optimisation of the SIM $\\pmb{\\Lambda}$ parameters. This effectively deflates the $\\hat{\\bf A}$ estimator variance relative to the variance of the $\\Lambda$ samples in MCMC. Only in the absence of rich $\\mathcal{C}_{T}$ data does SIT-MCMC outperform our method (total constrained case) as it limits the support of $\\pmb{\\theta}$ to $[0,2]^{2}$ which acts as regularisation. Letting the support of $\\pmb{\\theta}$ span the entire $\\mathbb{R}^{2}$ allows information carried by stronger $\\pmb{\\Lambda}$ constraints (such as $\\Lambda_{+}.$ \u00b7) to permeate to the $\\mathbf{T}$ space much faster in GeNSIT compared to SIT-MCMC. The plethora of sources of uncertainty ranging from the HW-SDE (6) to the combinatorial nature of $\\tau_{c}$ suggest that GENSIT is faster in reconstructing ground truth ODMs compared to a fully Bayesian approach such as SIT-MCMC. ", "page_idx": 8}, {"type": "table", "img_path": "eUcyIe1AzY/tmp/cc719e420b55dbaf6feef61dfd0bac95db2ac6d504ac06c5d44cbf68f20639f6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "eUcyIe1AzY/tmp/816ec719cb76b4fb9504d8b191d0a6445365c67cafb06b69bbde27af000b0dab.jpg", "table_caption": ["(a) ODMs with closed-form (tractable) T distributions (10). ", "$-$ : This case is not handled by the approach mentioned in the column. ", "(b) ODMs with intractable $\\mathbf{T}$ distribution (13), where conditioning $\\pmb{\\Lambda}$ on $\\mathcal{C}$ is problematic. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 1: Ground truth $\\mathbf{T}^{*}$ validation metrics comparing our method against [13, 17, 46] in the $\\Lambda$ and $\\mathbf{T}$ levels across constraint sets $\\mathcal{C}$ and $\\sigma=0.141$ (best) for the Cambridge dataset. On a $\\mathcal{C}$ basis the best metric in $\\mathbf{T},\\pmb{\\Lambda}$ spaces is emphasised for each of the two and highlighted between the two. Inference on the discrete table space offers lower SRMSE and higher CP compared to inference in the continuous space. On an ODM basis we obtain the best reconstruction error (SRMSE) and ground truth coverage (CP) in $\\mathbf{T}$ -space in all but the totally constrained ODM. This is due to SIT-MCMC forcing $\\theta\\in[0,2]^{2}$ instead of $\\mathbb{R}^{2}$ , which has a regularisation effect. In the absence of substantial $\\mathcal{C}$ data, this effect is more pronounced. See Tab. 6 (App. E.1) for full table across multiple $\\sigma$ regimes. ", "page_idx": 8}, {"type": "text", "text": "Fig. 6 supports this claim by shedding light on the convergence rate of running mean estimates of the ground truth $\\mathbf{T}^{*}$ . The Joint GeNSIT scheme converges to a mean $\\mathbf{T}$ estimate much earlier than SIT-MCMC across all $\\mathcal{C}$ regimes. Mean $\\mathbf{T}$ estimates are also improved in the Joint GeNSIT compared to SIT-MCMC in confined $\\mathbf{T}$ spaces (doubly, doubly and $10\\%$ cell, doubly and $20\\%$ cell constrained ODMs). Under the same $\\mathcal{C}$ regimes CP does not improve significantly as $N$ grows large, which suggests that the variance of $\\mathbf{T}$ samples appears stable as early as $N=\\mathrm{i}0^{4}$ . In the Disjoint GeNSIT the information encoded in larger $\\mathcal{C}_{T}$ is not propagated to the $\\pmb{\\Lambda}$ updates. As a result, no SRMSE or CP improvements are detected in the course of $N$ . ", "page_idx": 8}, {"type": "text", "text": "4.2.2 Washington, DC, USA ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We apply our method to the Washington dataset, where the ground truth ODM is a $179\\times179$ contingency table with 200, 029 agents. Tab. 2 reports the reconstruction error (SRMSE) and the $\\%$ coverage of the ground truth cells (CP) in $\\pmb{\\Lambda}$ and $\\mathbf{T}$ . Comparisons against SIM-MCMCand SIT-MCMCwere infeasible due to their high computational complexity (300 hours to obtain 500 samples on a 32-core NVIDIA GPU). Instead, we leveraged GMEL[26] which operates only in the continuous $\\pmb{\\Lambda}$ space by learning a mapping between a large feature space $\\mathcal{D}$ and $\\mathbf{T}_{\\mathcal{X}\\mathrm{train}}$ . Tab. 2 shows that we outperform both GMEL and SIM-NN in terms of reconstruction error and coverage. ", "page_idx": 8}, {"type": "image", "img_path": "eUcyIe1AzY/tmp/84f9d7485f5029c5e2c2289167b93311da2af05cfd3d76636dcb75b6db30954b.jpg", "img_caption": ["Figure 6: SRMSE and CP for $\\mathcal{C}=\\{\\mathbf{T}.+\\}\\,(\\bullet),\\,\\{\\mathbf{T}.+,\\mathbf{T}_{+}.,\\mathbf{T}_{\\mathcal{X}_{2}}\\}\\,(\\bullet)$ computed cumulatively along $N$ iterations of Alg. 1 for $\\mathbf{T}|\\mathcal{D},\\mathcal{C}$ samples for GENSIT (Joint) and SIT-MCMC for the Cambridge dataset. The Joint GENSIT converges to a lower SRMSE faster than SIT-MCMC while achieving better $\\mathbf{T}^{*}$ coverage. The singly constrained ODM $\\mathcal{C}=\\{\\mathbf{T}._{+}\\})$ has very large support $\\tau_{c}$ given that CP is decreasing with N. See App. E for all cases of $\\mathcal{C}$ and frameworks. "], "img_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "eUcyIe1AzY/tmp/f4db96d293cdbe9be0bea59353e49413d6e348822c8a9c24de5c3667a4d7ecc0.jpg", "table_caption": [], "table_footnote": ["Table 2: Ground truth $\\mathbf{T}^{*}$ validation metrics (mean $\\pm$ std. for $E=10$ ensemble size) comparing our method against [26, 17] in the $\\Lambda$ and $\\mathbf{T}$ levels for $\\mathcal{C}=\\{T_{++},\\mathbf{T}_{\\mathcal{X}\\tan}\\}$ and $\\sigma=0.141$ (best) for the Washington dataset. Arrow $\\uparrow$ indicates higher values are better, and vice versa. We achieve the best error (SRMSE) and ground truth coverage (CP) overall (see bold cells). The observed data $\\mathcal{D}$ leveraged to train GENSIT, SIM-NN is a small subset of the data required to train GMEL $\\textbf{y}$ is a column vector of $\\mathbf{Y}$ ). See Tab. 7 (App. E.2) for full table across multiple $\\sigma$ regimes. "], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "5 Concluding Remarks ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We introduced GeNSIT, an efficient framework for jointly sampling the discrete combinatorial space of agent trips $\\mathbf{\\rho}(\\mathbf{T})$ subject to summary statistic data and its continuous mean-field limit $\\Lambda$ . We surmount the limitations of methods which operate strictly on $\\Lambda$ space [13, 17, 26] and of methods that incur a large computational cost [46]. We accomplish this by introducing the first framework operating on both T, \u039b that scales linearly with the number of origin-destination pairs $I J$ . We offer enhanced reconstruction error and coverage of the ground truth ODMs in Cambridge, UK and Washington, DC. Although NNs require a much larger number of internal parameters to be calibrated relative to MCMC, their embedding of physics models regularises this parameter space and prevents over-fitting. A remaining open problem on this front is the assimilation of more complex $\\mathcal{C}$ structures in agent population synthesis and simulation (see App. B.2.3), since ground truth data is typically partially observed. Our work also relies on the SIM\u2019s assumptions about the agents\u2019 decision-making process, which in practise is unobserved. An examination of different agent utility models could benefit the applicability of our framework. In terms of our work\u2019s social impact, policy decisions made from ABMs of social systems could negatively affect individuals, necessitating expert review and ethics oversight. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "IZ was supported by UK Research and Innovation (UKRI) Research Council and Arup Group PhD studentship. MG was supported by a Royal Academy of Engineering Research Chair grant RCSRF1718/6/34 and EPSRC grants EP/T000414/1, EP/W005816/1, EP/V056441/1, EP/V056522/1, EP/R018413/2, EP/R034710/1, EP/R004889/1. TD was supported by the UKRI Turing AI acceleration Fellowship EP/V02678X/1. For the purpose of open access, the authors have applied a Creative Commons Attribution (CC-BY) licence to any Author Accepted Manuscript version arising from this submission. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Alan Agresti. Categorical Data Analysis. Wiley Series in Probability and Statistics. John Wiley & Sons, Inc., Hoboken, NJ, USA, July 2002. ISBN 978-0-471-36093-3 978-0-471-24968-9. doi: 10.1002/0471249688.   \n[2] Rushil Anirudh, Jayaraman J. Thiagarajan, Peer-Timo Bremer, Timothy Germann, Sara Del Valle, and Frederick Streitz. Accurate Calibration of Agent-based Epidemiological Models with Neural Network Surrogates. In Proceedings of the 1st Workshop on Healthcare AI and COVID-19, ICML 2022, pages 54\u201362. PMLR, July 2022. [3] Steven C. Bankes. Agent-based modeling: A revolution? Proceedings of the National Academy of Sciences, 99(suppl_3):7199\u20137200, May 2002. doi: 10.1073/pnas.072081299.   \n[4] David L. Banks and Mevin B. Hooten. Statistical Challenges in Agent-Based Modeling. The American Statistician, 75(3):235\u2013242, April 2021. doi: /10.1080/00031305.2021.1900914.   \n[5] Hac\u00e8ne Belbachir. A combinatorial contribution to the multinomial Chu-Vandermonde convolution. Les Annales RECITS, 1:27\u201332, January 2014.   \n[6] Yvonne M. M. Bishop, Stephen E. Fienberg, and Paul W. Holland. Discrete Multivariate Analysis. Springer, New York, 2007. ISBN 978-0-387-72805-6.   \n[7] Pier Giovanni Bissiri, Chris Holmes, and Stephen Walker. A General Framework for Updating Belief Distributions. Journal of the Royal Statistical Society Series B: Statistical Methodology, 78(5):1103\u20131130, November 2016. ISSN 1369-7412, 1467-9868. doi: 10.1111/rssb.12158.   \n[8] Eric Bonabeau. Agent-based modeling: Methods and techniques for simulating human systems. Proceedings of the National Academy of Sciences, 99(suppl_3):7280\u20137287, May 2002. ISSN 0027-8424, 1091-6490. doi: 10.1073/pnas.082080899. [9] Luis Carvalho. A Bayesian Statistical Approach for Inference on Static Origin\u2013Destination Matrices in Transportation Studies. Technometrics, 56(2):225\u2013237, April 2014. ISSN 0040- 1706, 1537-2723. doi: 10.1080/00401706.2013.826144.   \n[10] Andrew Crooks, Christian Castle, and Michael Batty. Key challenges in agent-based modelling for geo-spatial simulation. Computers, Environment and Urban Systems, 32(6):417\u2013430, November 2008. ISSN 01989715. doi: 10.1016/j.compenvurbsys.2008.09.004.   \n[11] Persi Diaconis and Bernd Sturmfels. Algebraic algorithms for sampling from conditional distributions. The Annals of Statistics, 26(1), February 1998. ISSN 0090-5364. doi: 10.1214/ aos/1030563990.   \n[12] Daniel R. Dooly, Qi Zhang, Sally A. Goldman, and Robert A. Amar. Multiple-Instance Learning of Real-Valued Data. Journal of Machine Learning Research, 3(Dec):651\u2013678, 2002. ISSN ISSN 1533-7928.   \n[13] L. Ellam, M. Girolami, G. A. Pavliotis, and A. Wilson. Stochastic modelling of urban structure. Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences, 474 (2213):20170700, May 2018. ISSN 1364-5021, 1471-2946. doi: 10.1098/rspa.2017.0700.   \n[14] J. Doyne Farmer and Duncan Foley. The economy needs agent-based modelling. Nature, 460 (7256):685\u2013686, August 2009. ISSN 1476-4687. doi: 10.1038/460685a.   \n[15] Bilal Farooq, Michel Bierlaire, Ricardo Hurtubia, and Gunnar Fl\u00f6tter\u00f6d. Simulation based population synthesis. Transportation Research Part B: Methodological, 58:243\u2013263, December 2013. ISSN 0191-2615. doi: 10.1016/j.trb.2013.09.012.   \n[16] Neil M. Ferguson, Derek A. T. Cummings, Christophe Fraser, James C. Cajka, Philip C. Cooley, and Donald S. Burke. Strategies for mitigating an influenza pandemic. Nature, 442(7101): 448\u2013452, July 2006. ISSN 1476-4687. doi: 10.1038/nature04795.   \n[17] Thomas Gaskin, Grigorios A. Pavliotis, and Mark Girolami. Neural parameter calibration for large-scale multiagent models. Proceedings of the National Academy of Sciences, 120(7): e2216415120, February 2023. ISSN 0027-8424, 1091-6490. doi: 10.1073/pnas.2216415120.   \n[18] Joaqu\u00edn Go\u00f1i, Martijn P. van den Heuvel, Andrea Avena-Koenigsberger, Nieves Velez de Mendizabal, Richard F. Betzel, Alessandra Griffa, Patric Hagmann, Bernat Corominas-Murtra, Jean-Philippe Thiran, and Olaf Sporns. Resting-brain functional connectivity predicted by analytic measures of network communication. Proceedings of the National Academy of Sciences, 111(2):833\u2013838, January 2014. doi: 10.1073/pnas.1315529111.   \n[19] B Harris and A G Wilson. Equilibrium Values and Dynamics of Attractiveness Terms in Production-Constrained Spatial-Interaction Models. Environment and Planning A: Economy and Space, 10(4):371\u2013388, April 1978. ISSN 0308-518X, 1472-3409. doi: 10.1068/a100371.   \n[20] Andreas Horni, Kai Nagel, and Kay W. Axhausen. The Multi-Agent Transport Simulation MATSim. Ubiquity Press, August 2016. ISBN 978-1-909188-75-4. doi: 10.5334/baw.   \n[21] Cliff C. Kerr, Robyn M. Stuart, Dina Mistry, Romesh G. Abeysuriya, Katherine Rosenfeld, Gregory R. Hart, Rafael C. N\u00fa\u00f1ez, Jamie A. Cohen, Prashanth Selvaraj, Brittany Hagedorn, Lauren George, Michal Jastrzebski, Amanda S. Izzo, Greer Fowler, Anna Palmer, Dominic Delport, Nick Scott, Sherrie L. Kelly, Caroline S. Bennette, Bradley G. Wagner, Stewart T. Chang, Assaf P. Oron, Edward A. Wenger, Jasmina Panovska-Griffiths, Michael Famulare, and Daniel J. Klein. Covasim: An agent-based model of COVID-19 dynamics and interventions. PLoS Computational Biology, 17(7):e1009149, July 2021. ISSN 1553-734X. doi: 10.1371/ journal.pcbi.1009149.   \n[22] Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization, January 2017.   \n[23] Jeremias Knoblauch, Jack Jewson, and Theodoros Damoulas. An Optimization-centric View on Bayes\u2019 Rule: Reviewing and Generalizing Variational Inference. Journal of Machine Learning Research, 23(132):1\u2013109, 2022. ISSN 1533-7928.   \n[24] Robert Lempert. Agent-based modeling as organizational and public policy simulators. Proceedings of the National Academy of Sciences, 99(suppl_3):7195\u20137196, May 2002. doi: 10.1073/pnas.072079399.   \n[25] Shen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar, Pieter Noordhuis, Teng Li, Adam Paszke, Jeff Smith, Brian Vaughan, Pritam Damania, and Soumith Chintala. PyTorch distributed. Proceedings of the VLDB Endowment, 13(12):3005\u20133018, August 2020. ISSN 2150-8097. doi: 10.14778/3415478.3415530.   \n[26] Zhicheng Liu, Fabio Miranda, Weiting Xiong, Junyan Yang, Qiao Wang, and Claudio Silva. Learning Geo-Contextual Embeddings for Commuting Flow Prediction. Proceedings of the AAAI Conference on Artificial Intelligence, 34(01):808\u2013816, April 2020. ISSN 2374-3468. doi: 10.1609/aaai.v34i01.5425.   \n[27] Gisiro Maruyama. Continuous Markov processes and stochastic equations. Rendiconti del Circolo Matematico di Palermo, 4(1):48\u201390, January 1955. ISSN 1973-4409. doi: 10.1007/ BF02846028.   \n[28] Dina Mistry, Maria Litvinova, Ana Pastore y Piontti, Matteo Chinazzi, Laura Fumanelli, Marcelo F. C. Gomes, Syed A. Haque, Quan-Hui Liu, Kunpeng Mu, Xinyue Xiong, M. Elizabeth Halloran, Ira M. Longini, Stefano Merler, Marco Ajelli, and Alessandro Vespignani. Inferring high-resolution human mixing patterns for disease modeling. Nature Communications, 12(1): 323, January 2021. ISSN 2041-1723. doi: 10.1038/s41467-020-20544-y.   \n[29] Peter Nijkamp and Aura Reggiani. Entropy, spatial interaction models and discrete choice analysis: Static and dynamic analogies. European Journal of Operational Research, 36(2): 186\u2013196, August 1988. ISSN 03772217. doi: 10.1016/0377-2217(88)90424-9.   \n[30] Jim Pooler. An extended family of spatial interaction models. Progress in Human Geography, 18(1):17\u201339, March 1994. ISSN 0309-1325, 1477-0288. doi: 10.1177/030913259401800102.   \n[31] Nastaran Pourebrahim, Selima Sultana, Amirreza Niakanlahiji, and Jean-Claude Thill. Trip distribution modeling with Twitter data. Computers, Environment and Urban Systems, 77: 101354, September 2019. ISSN 0198-9715. doi: 10.1016/j.compenvurbsys.2019.101354.   \n[32] Soumya Ray and David Page. Multiple Instance Regression. In Proceedings of the Eighteenth International Conference on Machine Learning, ICML \u201901, pages 425\u2013432, San Francisco, CA, USA, June 2001. Morgan Kaufmann Publishers Inc. ISBN 978-1-55860-778-1.   \n[33] Carl Orge Retzlaff, Martina Ziefle, and Andr\u00e9 Calero Valdez. The History of Agent-Based Modeling in the Social Sciences. In Vincent G. Duffy, editor, Digital Human Modeling and Applications in Health, Safety, Ergonomics and Risk Management. Human Body, Motion and Behavior, pages 304\u2013319, Cham, 2021. Springer International Publishing. ISBN 978-3-030- 77817-0. doi: 10.1007/978-3-030-77817-0_22.   \n[34] Caleb Robinson and Bistra Dilkina. A machine learning approach to modeling human migration. In Proceedings of the 1st ACM SIGCAS Conference on Computing and Sustainable Societies, Compass \u201918, New York, NY, USA, 2018. Association for Computing Machinery. ISBN 978-1-4503-5816-3. doi: 10.1145/3209811.3209868.   \n[35] Ori Rosen, Martin Tanner, Gary King, Ori Rosen, and Martin Tanner. Ecological Inference: New Methodological Strategies. Cambridge University Press, New York, 2004.   \n[36] Alexander A. Schuessler. Ecological inference. Proceedings of the National Academy of Sciences, 96(19):10578\u201310581, September 1999. doi: 10.1073/pnas.96.19.10578.   \n[37] Caio Seguin, Olaf Sporns, and Andrew Zalesky. Brain network communication: Concepts, models and applications. Nature Reviews Neuroscience, 24(9):557\u2013574, September 2023. ISSN 1471-0048. doi: 10.1038/s41583-023-00718-5.   \n[38] Daniel R Sheldon and Thomas Dietterich. Collective Graphical Models. In Advances in Neural Information Processing Systems, volume 24. Curran Associates, Inc., 2011.   \n[39] Stephen G. Simpson. Ordinal Numbers and the Hilbert Basis Theorem. The Journal of Symbolic Logic, 53(3):961\u2013974, 1988. ISSN 0022-4812. doi: 10.2307/2274585.   \n[40] Gabriel Spadon, Andre C. P. L. F. de Carvalho, Jose F. Rodrigues-Jr, and Luiz G. A. Alves. Reconstructing commuters network using machine learning and urban indicators. Scientific Reports, 9(1):11801, August 2019. ISSN 2045-2322. doi: 10.1038/s41598-019-48295-x.   \n[41] Caprice Stanley and Tobias Windisch. Heat-bath random walks with Markov bases. Advances in Applied Mathematics, 92:122\u2013143, January 2018. ISSN 01968858. doi: 10.1016/j.aam.2017. 08.002.   \n[42] Thomas M. Sutter, Laura Manduchi, Alain Ryser, and Julia E. Vogt. Continuous Relaxation For The Multivariate Noncentral Hypergeometric Distribution. In ICLR2022 Workshop on the Elements of Reasoning: Objects, Structure and Causality, March 2022.   \n[43] Kenneth Train. Discrete Choice Methods with Simulation. Cambridge University Press, Cambridge, 2 edition, 2009. ISBN 978-0-521-74738-7.   \n[44] A G Wilson. A Family of Spatial Interaction Models, and Associated Developments. Environment and Planning A: Economy and Space, 3(1):1\u201332, March 1971. ISSN 0308-518X, 1472-3409. doi: 10.1068/a030001.   \n[45] Tobias Windisch. Rapid Mixing and Markov Bases. SIAM Journal on Discrete Mathematics, 30(4):2130\u20132145, January 2016. ISSN 0895-4801, 1095-7146. doi: 10.1137/15M1022045.   \n[46] Ioannis Zachos, Theodoros Damoulas, and Mark Girolami. Table inference for combinatorial origin-destination choices in agent-based population synthesis. Stat, 13(1):e656, 2024. ISSN 2049-1573. doi: 10.1002/sta4.656.   \n[47] Yivan Zhang, Nontawat Charoenphakdee, Zhenguo Wu, and Masashi Sugiyama. Learning from Aggregate Observations. In Advances in Neural Information Processing Systems, volume 33, pages 7993\u20138005. Curran Associates, Inc., 2020. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendices ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Nomenclature 16 ", "page_idx": 14}, {"type": "text", "text": "B Theory and Methodology Foundations 17 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Continuous ODM . 17   \nB.1.1 Spatial Interaction Modelling Choice 17   \nB.1.2 Eliciting Agent Utility Functions . . . 17   \nB.2 Discrete ODM . . . 17   \nB.2.1 Target Table Distributions 17   \nB.2.2 Markov Bases Preliminaries . 18   \nB.2.3 Incorporating Additional Constraints 18   \nB.3 GENSIT 19   \nB.3.1 Disjoint versus Joint scheme . 19 ", "page_idx": 14}, {"type": "text", "text": "C Method Comparisons 19 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "C.1 Computational Complexities 19   \nC.2 Validation Metrics . 19 ", "page_idx": 14}, {"type": "text", "text": "D Experimental Settings 20 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "D.1 Implementation Details 20   \nD.2 Experimental Protocols . 21 ", "page_idx": 14}, {"type": "text", "text": "E Auxiliary Experimental Results 22 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "E.1 Cambridge, UK 22   \nE.2 Washington, DC, USA 23 ", "page_idx": 14}, {"type": "text", "text": "F Reproducibility 23 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "F.1 Cambridge, UK 24   \nF.2 Washington, DC, USA 26 ", "page_idx": 14}, {"type": "text", "text": "A Nomenclature ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We provide a Tab. of notation (see Tab. 3) used throughout the paper. ", "page_idx": 15}, {"type": "table", "img_path": "eUcyIe1AzY/tmp/6e4e357a29361bfc9c756d1c852c1822b7622949700c958df5745d7a8a5facd1.jpg", "table_caption": ["Table 3: Notation used in the paper. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "B Theory and Methodology Foundations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "B.1 Continuous ODM ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "B.1.1 Spatial Interaction Modelling Choice ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "A fundamental assumption is that the continuous agent trip intensity $\\Lambda$ is governed by a SIM. Therefore, agents are discouraged from travelling long distances due to high travelling costs incurred and encouraged to travel to locations that offer high utility, such as areas with high job availability. This means that highly irregular or non-uniform trip data might not be captured well by $\\Lambda$ . However, operating at the discrete ODM level allows us to assimilate trip data as constraints. This shrinks the ODM support and enhances spatial imputation of missing data much better than doing so at the continuous $\\Lambda$ level. In the limit of trip data constraints, we are guaranteed to improve our estimated of missing trip data regardless of their spatial distribution and presence of outliers. This is theoretically founded [11] and empirically shown in Tabs. 1,2 of our Experimental Section. In poorly constrained settings, we rely more on $\\pmb{\\Lambda}$ for spatial imputation, which increases reconstruction error. Outlier presence will be reflected in the row/column sum constraints facilitating table sampling in that region of the ODM support. If no row/column sum constraints are imposed, exploring such regions of the support would be significantly hindered. ", "page_idx": 16}, {"type": "text", "text": "B.1.2 Eliciting Agent Utility Functions ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In the ODMs considered we have constructed agent utility functions that leverage the log destination attraction at each destination location $\\mathbf{x}$ and the total distance (cost) of travelling between any origin and destination $\\mathbf{C}$ . Additional observations at the origin, destination and origin-destination level can be assimilated by incorporating them as terms in the maximum entropy argument used to derive the functional forms of $\\Lambda$ in equations (3), (4), and (5). The SIM intensity is equivalent to the multinomial logit [29], which allows us to define an arbitrary utility model inside the exp in the numerator of these equations. For example, one might want to include data $\\mathbf{o}\\in\\mathbb{R}^{I}$ in addition to the existing $\\mathbf{x},\\mathbf{C}$ in the totally constrained SIM (t $\\mathrm{otal}=\\Lambda_{++}^{}$ ). Then, we need to maximise: ", "page_idx": 16}, {"type": "equation", "text": "$$\n-\\sum_{i,j}^{I,J}\\Lambda_{i j}(\\log(\\Lambda_{i j})-o_{i}-\\alpha x_{j}+\\beta c_{i j})-\\mu(\\sum_{i j}^{I,J}\\Lambda_{i j}-\\Lambda_{++}),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\mu$ is the Lagrange multiplier. This yields ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\Lambda_{i j}=\\frac{\\Lambda_{++}(\\exp(o_{i}+\\alpha x_{j}-\\beta c_{i j}))}{\\sum_{i,j}^{I,J}\\exp(o_{i}+\\alpha x_{j}-\\beta c_{i j})}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "B.2 Discrete ODM ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "B.2.1 Target Table Distributions ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The target distributions $\\mu\\left(\\mathbf{T}\\mid\\Lambda,\\mathcal{C}_{T}\\right)$ we sample from in Alg. 1 are listed below in Tab. 4. ", "page_idx": 16}, {"type": "text", "text": "We note that the quantity $\\begin{array}{r}{\\omega_{i j}\\,=\\,\\frac{\\Lambda_{i j}\\Lambda_{++}}{\\Lambda_{i+}\\Lambda_{+j}}}\\end{array}$ \u039b\u039biij+\u039b\u039b+++j in the last three rows of Tab. 4 is called the odds ratio and encodes the dependence between rows (origins) and columns (destinations). SIMs incorporate this dependence spatially in the cost matrix $\\mathbf{C}$ . The case for spatial independence is achieved when $\\beta=0$ in (3), (4). This translates to the travel cost having no effect on the agents\u2019 choice of destination. Fisher\u2019s non-central multivariate hypergeometric reduces to its central version if and only if $\\omega_{i j}=1$ [1]. The central version corresponds to a uniform distribution over $\\tau_{c}$ . Additionally, the normalising constant of Fisher\u2019s non-central multivariate hypergeometric is a partition function involving a sum over all elements of $\\tau_{c}$ satisfying the conditioned margins. By virtue of the extension to Chu-Vandermonde\u2019s theorem [5] proved in [46], this normalising constant can be computed in constant time using ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\prod_{j}^{J}\\frac{T_{+j}!T_{+j}!}{T_{++}!T_{i j}!}\\prod_{i}^{I}\\left(\\frac{\\omega_{i j}}{\\omega_{+j}}\\right)^{T_{i j}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "table", "img_path": "eUcyIe1AzY/tmp/93e47685c25ab90e0963badbdfbced6b98ad176be7233a1da86ebada837eb674.jpg", "table_caption": ["Table 4: List of target $\\mathbf{T}$ distributions with their associated constraints $\\mathcal{C}_{T}$ . "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "B.2.2 Markov Bases Preliminaries ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Theoretically analysing the full joint framework is challenging future work, however we offer some additional theoretical insight. We encourage the reader to follow the references of this section for more technical details. ", "page_idx": 17}, {"type": "text", "text": "The fundamental theorem of Markov Bases introduced in the seminal work of Diaconis and Sturmfels [11] establishes an equivalence between a Markov Basis and the generator of an ideal of a polynomial ring. By virtue of the Hilbert basis theorem [39], any ideal in a polynomial ring has a finite generating set. Therefore, there exists a finite MB for the class of inference problems we are dealing with. This MB connects the fiber, i.e. the support of all discrete ODMs satisfying summary statistics in the form of row, column sums and/or cell constraints. By Proposition 3.1, we can construct a Gibbs MB sampler that will converge to Fisher\u2019s non-central hypergeometric distribution on the fiber in finite time. ", "page_idx": 17}, {"type": "text", "text": "Moreover, an MB sampler converges to its stationary distribution in at most $A^{2}$ steps, where $A$ is the total number of agents [45]. Although this result is not directly applicable to our framework in its given form, we conjecture that it can be extended to $\\eta=\\pm1$ and Fisher\u2019s central hypergeometric distribution as evidenced by our experimental results. An MB connects tables (elements) of the fiber graph (discrete state-space of tables). The fastest convergence of an MB sampler is achieved by an MB that has the smallest possible diameter, i.e. the longest path between two elements of the fiber graph. ", "page_idx": 17}, {"type": "text", "text": "B.2.3 Incorporating Additional Constraints ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Incorporating additional structural constraints remains an open problem. The case for sparse matrices can be handled through the cell constraints $\\mathbf{T}_{\\mathcal{X}^{\\prime}}$ for $\\mathcal{X}^{\\prime}\\subseteq\\mathcal{X}$ . Symmetric ODMs (e.g. an adjacency matrix of a bipartite graph) can be explored as follows. The subspace of $\\tau_{c}$ consists of square matrices $I\\times I$ with equal row and column sums. We construct an MB on $\\tau_{c}$ by applying the following modification to equation (14): ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbf{f}_{l}(x)=\\left\\{{\\overset{+\\eta}{-\\eta}}\\right.\\,\\,\\,{\\mathrm{if}}\\;x\\in\\{(i_{1},j_{1}),(i_{2},j_{2}),(j_{1},i_{1}),(j_{2},i_{2})\\}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "First, this guarantees that every $2\\times2$ move in the original $\\mathcal{M}$ that modifies a cell along the diagonal is by definition symmetric as to the other cell it modifies, meaning if cell $(i,j)$ is modified then so is $(j,i)$ . Second, without loss of generality assume that a move modifies a $2\\times2$ section in the upper triangular section of the ODM. This ensures that the same move is applied symmetrically to the lower triangular section of the ODM. In both cases, any move will guarantee that the ODM will be symmetric after removing any duplicate Markov Bases generated in the process. ", "page_idx": 17}, {"type": "text", "text": "B.3 GENSIT ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "B.3.1 Disjoint versus Joint scheme ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The Disjoint scheme consists only of loss terms that depend directly on fully observed data $\\mathcal{D}$ (either log-destination attraction $\\mathbf{y}$ or total distance agents travelled by origin location $\\mathbf{D.}_{+},$ ). In contrast, the Joint scheme consists of loss terms that either depend on the same fully observed data and on the partially observed table $\\mathbf{T}$ through the table marginal $\\mathbf{T}|\\pmb{\\theta},\\mathbf{x},\\mathcal{C},\\mathcal{D}$ . ", "page_idx": 18}, {"type": "text", "text": "The Joint scheme is an instance of a Gibbs sampler on the full posterior marginals $\\theta|(\\mathbf{x},\\mathbf{T},\\mathcal{C},\\mathcal{D})$ , ${\\bf x}|(\\pmb\\theta,{\\bf T},\\mathcal{C},\\mathcal{D})$ and $\\mathbf{T}|(\\pmb{\\theta},\\mathbf{x},\\mathcal{C},\\mathcal{D})$ . The Disjoint scheme is an instance of a collapsed Gibbs sampler where we sample from $\\pmb{\\theta}|(\\mathbf{x},\\mathcal{C},\\mathcal{D})$ , ${\\bf x}|(\\pmb\\theta,\\mathcal{C},\\mathcal{D})$ and then from $\\mathbf{T}|(\\pmb\\theta,\\mathbf x,\\mathcal{C},\\mathcal{D})$ . This means we integrate out $\\mathbf{T}$ by means of $\\begin{array}{r}{p(\\pmb{\\theta}|\\mathbf{x},\\mathcal{C},\\mathcal{D})=\\sum_{\\mathbf{T}}p(\\pmb{\\theta}|\\mathbf{T},\\mathbf{x},\\mathcal{C},\\mathcal{D})P(\\mathbf{\\tilde{T}}|\\mathbf{x},\\mathcal{C},\\mathcal{D})}\\end{array}$ and $p(\\mathbf{x}|\\pmb{\\theta},\\mathcal{C},\\mathcal{D})=$ $\\begin{array}{r}{\\sum_{\\mathbf{T}}p(\\mathbf{x}|\\mathbf{T},\\pmb{\\theta},\\mathcal{C},\\mathcal{D})P(\\mathbf{T}|\\pmb{\\theta},\\mathcal{C},\\mathcal{D})}\\end{array}$ . Therefo re, we use the Joint scheme when we have reason to believe that the covariance between $\\theta,\\mathbf{x}$ and $\\mathbf{T}$ is small. This would be the case when the agent trip intensity is influenced by both the Harris-Wilson SDE and the realised number of agent trips. In contrast, we use the Disjoint scheme to accelerate convergence by reducing the covariance between $\\theta,\\mathbf{x}$ and $\\mathbf{T}$ . This would be the case when the agent trip intensity is governed only by the Harris-Wilson SDE and not by the realised number of agent trips. ", "page_idx": 18}, {"type": "text", "text": "C Method Comparisons ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section, we compare our method\u2019s computational complexity to competitive approaches and list the validation metrics employed for empirical comparisons in the main paper. ", "page_idx": 18}, {"type": "text", "text": "C.1 Computational Complexities ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Let $N$ be the number of iterations, and $I,J$ be the number of origins, destinations. ", "page_idx": 18}, {"type": "text", "text": "1. GENSIT: $\\mathcal{O}(N E(\\tau J+I J))$ , where $\\tau$ is the number of time steps in the Euler-Maruyama solver, $E$ is the ensemble size. We set $\\tau=1$ , and $E=1$ .   \n2. SIM-MCMC [13]: $\\mathcal{O}(N J(L I+J^{2}))$ (low $\\sigma$ regime) and $\\mathcal{O}(N I J L K n_{p}n_{t})$ (high $\\sigma$ regime), where $L$ is the number of leapfrog steps in Hamiltonian Monte Carlo, $1<K<N$ is the number of stopping times, and $n_{p},n_{t}$ are the number of particles and temperatures used in Annealed Importance Sampling, respectively. Typical ranges are $n_{p}\\in[10,100]$ , $n_{t}\\in[10,30]$ , and $L\\in[1,10]$ and $E\\in[1,10]$ .   \n3. SIT-MCMC [46]: $\\mathcal{O}(N J(L I+J^{2}))$ (low noise regime) $\\mathcal{O}(N I J L K n_{p}n_{t})$ (high noise regime). See items 1,2 for details.   \n4. SIM-NN [17]: $\\mathcal{O}(N E(\\tau J))$ . See item 1 for details.   \n5. GMEL [26]: $\\mathcal{O}(n_{n}n_{l}((I+J)D+I J))$ where $\\mathbf{Y}\\in\\mathbb{R}^{(I+J)\\times D}$ is the feature matrix for all locations, $D$ is the number of features per location, $n_{n}<D$ is the number of nodes per layer, and $n_{l}$ is the number of hidden layers. In the DC dataset, we have $I+J=179$ since every origin is also a destination (we do not count them twice). Typical ranges include $n_{l}\\in[1,10]$ and $n_{n}\\in[1,D]$ . ", "page_idx": 18}, {"type": "text", "text": "C.2 Validation Metrics ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We leverage the following three validations metrics employed throughout the literature [26, 46, 34, 40, 31]. The Standardised Root Mean Square Error (SRMSE) between the target ODM M (table T or intensity $\\Lambda$ ) samples and the ground truth $\\mathbf{T}^{*}$ is equal to ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathrm{SRMSE}(\\mathbf{M}^{(1:N)},\\mathbf{T}^{*})=\\sqrt{\\frac{\\sum_{i,j=1}^{I,J}\\left(\\mathbb{E}[M_{i j}^{(1:N)}|\\mathcal{C},\\mathcal{D}]-T_{i j}^{*}\\right)^{2}}{I J}}\\left(\\frac{\\sum_{i,j=1}^{I J}\\mathbb{E}[M_{i j}^{(1:N)}|\\mathcal{C},\\mathcal{D}]}{I J}\\right)^{-1},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and the Sorensen Similarity Index (SSI) is equal to ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{SSI}(\\mathbf{M}^{(1:N)},\\mathbf{T}^{*})=\\frac{1}{I J}\\sum_{i,j=1}^{I,J}\\frac{2\\operatorname*{min}\\left(\\mathbb{E}[M_{i j}^{(1:N)}|\\mathcal{C},\\mathcal{D}],T_{i j}^{*}\\right)}{\\mathbb{E}[M_{i j}^{(1:N)}|\\mathcal{C},\\mathcal{D}]+T_{i j}^{*}},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "whose domain is [0, 1] and values closer to 1 imply a better ODM fit. Finally, the coverage probability of the ground truth table $\\mathbf{T}^{*}$ is calculated by first identifying the lower $L_{q}\\left(\\mathbf{T}_{i j}^{(1:N)}\\right)$ and upper $U_{q}\\left(\\mathbf{T}_{i j}^{(1:N)}\\right)$ boundaries of the high probability region containing $q\\%$ of the total mass for each table cell $\\boldsymbol{x}=(i,j)\\in\\boldsymbol{\\mathcal{X}}$ . Then, the $q\\%$ cell coverage probability is equal to ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{CP}_{q}\\big(\\mathbf{M}^{(1:N)},\\mathbf{T}^{*}\\big)=\\frac{1}{I J}\\sum_{i,j=1}^{I J}\\mathbb{1}\\Bigg\\{L_{q}\\left(M_{i j}^{(1:N)}|\\mathcal{C},\\mathcal{D}\\right)\\leq T_{i j}^{*}\\leq U_{q}\\left(M_{i j}^{(1:N)}|\\mathcal{C},\\mathcal{D}\\right)\\Bigg\\}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "D Experimental Settings ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we detail the implementation details and experimental protocols used to obtain the results in the main paper. ", "page_idx": 19}, {"type": "text", "text": "D.1 Implementation Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Regarding the HW-SDE, an Euler-Maruyama numerical solver $\\phi_{H W}$ is employed throughout the paper with a time discretisation step of $\\Delta t=0.01$ and number of steps $\\tau=1$ . The low and high SDE noise levels correspond to $\\sigma=0.014$ and $\\sigma=0.141$ , respectively, to establish a comparison against [13, 17, 46]. Following [13, 17, 46], we set the responsiveness parameter $\\epsilon=1$ , and the parameter $\\delta$ relating to the job availability of a destination where no agents travel to 0, as in [17]. The job competition parameter \u03ba is uniquely determined by jJ\u039b=+1 +ex+p\u03b4(Jxj). Moreover, the same transportation network distance-based cost matrix $\\mathbf{C}$ as [46] is used. ", "page_idx": 19}, {"type": "text", "text": "The set of observation data $\\mathcal{D}$ may include the observed log destination attraction $\\dot{\\textbf{y}}\\in\\mathbb{R}^{J}$ and the total distance travelled from each origin $\\mathbf{D.}_{+}\\;=\\;$ $(\\mathbf{T}^{*}\\odot\\mathbf{C})\\in\\mathbb{R}_{>0}^{I}$ . For Cambridge, both y, $\\mathbf{D}_{\\cdot+}$ have been sourced from the UK\u2019s population census dataset provided by the Office of National Statistics. Their spatial resolution is regional: middle and lower super output areas for y, $\\mathbf{D}_{+}$ , respectively. For DC, we only have access to a feature matrix Y \u2208R(I+J)\u00d7D from which we extract a column to use as y. ", "page_idx": 19}, {"type": "text", "text": "Our NN is a multi-layer perceptron with one hidden layer, implemented in PyTorch [25] and depicted in Fig. 7. The input layer is set to the observed log-destination attractions $\\textbf{y}\\in\\mathbb{R}^{J}$ since we are learning the $\\pmb{\\theta}$ that generates the observed physics $\\mathbf{y}$ . The output layer is two-dimensional due to the parameter vector $\\pmb{\\theta}\\in\\mathbb{R}^{2}$ . For both datasets we set the number of hidden layers to one and number of nodes to 20. The hidden, output layers have a linear and absolute activation functions, respectively. The latter is important in guaranteeing that our $\\pmb{\\theta}$ estimates are always positive. The NN parameters W are initialised by sampling uniformly over the region $[0,4]^{(J+1)\\times20+21\\times2}$ ", "page_idx": 19}, {"type": "text", "text": "We use the Adam optimizer [22] with 0.002 learning rate. Bias is initialised uniformly at [0, 4]. We follow [13, 46] in fixing $\\sigma_{d}=0.03$ and $\\sigma_{T},\\sigma_{\\Lambda}$ to 0.07 to reflect a $1\\%$ and $3\\%$ noise levels, i.e. $\\sigma/\\log(J)\\approx3\\%$ . We assume y are observations from the SDE\u2019s stationary distribution, hence our batch size is one. We initialise the Euler-Maruyama solver at $\\mathbf{y}$ and run for $\\tau=1$ and step size $\\Delta t=0.001$ . At equilibrium the only change in the log-destination attractions is attributed to the SDE\u2019s diffusion. ", "page_idx": 19}, {"type": "image", "img_path": "eUcyIe1AzY/tmp/43480c6aa524ea405d14988bb1c9a81f74de85bedc195765dc23da8b3db8253e.jpg", "img_caption": ["Figure 7: Visual depiction of the Neural Network architecture used in GENSIT for both real-world datasets. The size of the weights $|\\mathbf{W}|$ scales linearly in the number of destinations $J$ . "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "The SDE is encoded in the NN as follows. The Euler-Maruyama solver $\\phi_{H W}$ provides forward solutions from $\\pmb{\\theta}$ to $\\mathbf{x}$ at different time steps. The loss function computes the discrepancy between the forward solution $\\mathbf{x}$ after $\\tau$ steps and the observed data y at the stationary equilibrium of the SDE, allowing us to encode the physics directly into the NN. A more complicated architecture with a larger number of weights would potentially lead to overfitting the SDE parameters $\\pmb{\\theta}$ to y compromising our framework\u2019s generalisability. ", "page_idx": 20}, {"type": "table", "img_path": "eUcyIe1AzY/tmp/3c9b73dfbb44951cd27dbd440c86ba5baca686d783c1d575863a2b9a2712374d.jpg", "table_caption": [], "table_footnote": ["Table 5: List of loss function operators $\\mathcal{L}$ and their observation data requirements $\\mathcal{D}$ . "], "page_idx": 20}, {"type": "text", "text": "A list of all loss functions used to train the NN is provided in Tab. 5. The Joint and Disjoint GeNSIT schemes in Figs. 8 and 9 use the loss operators $\\mathcal{L}\\left(\\mathbf{x}\\;;\\;\\mathcal{D},\\pmb{\\nu}\\right)$ , and $\\mathcal{L}\\left(\\mathbf{x},\\mathbf{T},\\mathbf{A}\\,;\\,\\mathcal{D},\\pmb{\\nu}\\right)$ . The observed total distance travelled is assumed to be more noisy than the observed employment data since the latter is a more volatile quantity than the latter and is more prone to errors. We note that SIM-NN in [17] was run under the same NN configuration except for the destination attraction loss $\\mathcal{L}\\left(\\mathbf{x}\\;;\\;\\mathcal{D},\\pmb{\\nu}\\right)$ for which the authors used the $L_{2}$ error. This is equivalent to our loss in Tab. 5 in the limit of $\\sigma_{d}\\rightarrow0$ . Regarding, table sampling, the observed cells in the Doubly Constrained with $10\\%$ cells fixed and Doubly Constrained with $20\\%$ cells fixed cases are chosen uniformly at random over the discrete cell support $[1,I]\\times[1,J]$ . All intractable distributions in Tab. 4 are sampled using Gibbs Markov Basis described in Proposition 3.1. A variant of maximum entropy iterative proportional fitting [6] is employed to initialise tables in Alg. 1. ", "page_idx": 20}, {"type": "text", "text": "D.2 Experimental Protocols ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Figures based on synthetic experiments are produced as follows. We generate ground truth tables $(100\\times100)$ , $(200\\times200)$ , $(300\\times300)$ , $(400\\times400)$ , $(500\\times500)$ , $(600\\times600)$ , $(700\\times700)$ , $800\\times$ 800), $(900\\times900)$ , $7000\\times1000)$ as explained in the main paper. For each of these tables, Algorithm 1 and SIM-NN are run for $N=10^{3}$ , $\\overset{\\cdot}{A}=10^{6}$ , and $\\mathcal{C}_{T}=\\mathrm{~\\bar{~}{\\left\\{\\mathbf{T}_{++},T_{+},T_{\\mathcal{X}_{50\\mathcal{Y}_{0}}}\\right\\}}~}$ while monitoring the computation times to produce Fig. 4. Additionally, we generate ground truth tables of sizes $(50\\times50)$ , $(100\\times100)$ , $(150\\times150)$ , $(200\\times200)$ , $(250\\times250)$ with total number of agents $A=10^{6}$ , $\\mathcal{C}_{T}=\\{\\mathbf{T}_{+},\\mathbf{T}_{\\mathcal{X}_{50\\%}}\\}$ and run line 24 of Alg. 1 to produce Fig. 5b. Finally, we generate $150\\times150$ ground truth tables with $A=0.01,0.1,0.25,0.5,1\\times10^{6}$ and run the same line of Alg. 1 using $\\bar{\\mathcal{C}}_{T}=\\{\\mathbf{T}_{+},\\mathbf{T}_{\\mathcal{X}_{50\\%}}\\}$ and $N=10^{4}$ to create Fig. 5a. ", "page_idx": 20}, {"type": "text", "text": "Alg. 1 is run for $N=10^{5}$ iterations and $E=1$ ensemble size to produce the data in Figs. 8,10. In Fig. 9 a computational budget of $N\\times E=10^{4}$ samples is fixed and the following schedule $(N,E)$ is used: (10, 1000), (50, 200), (100, 100), (500, 20), (1000, 10), (5000, 2), (10000, 1). In this case, estimators of the likes of $\\mathbb{E}\\left[\\cdot\\mid\\mathbf{T}^{*}\\right]$ are computed over both $N$ and $E$ by coupling all samples across members of the ensemble. The Standardised Root Mean Square Error and $R\\%$ High probability region cell Coverage Probability metrics are computed in the same fashion as in [46]. Finally, in Tab. 1 the latent samples $\\mathbf{A}^{(1:N)},\\mathbf{T}^{\\dot{(1:N)}}$ have been trimmed by applying a burning and thinning of 100 and 100 samples across every member of the ensemble. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "We follow the setup of [13, 46] to initialise SIMMCMC , SIT-MCMC and ensure comparability with our models. The computational budget in Tab. 1 and Figs. 10,8 is set to $N=10^{5}$ . SIM model parameters are fixed to $\\epsilon=1$ , $\\kappa=1.025$ , $\\delta=0.0128$ and $\\sigma_{d}=3\\%\\times\\log(J)$ . For x updates the acceptance is monitored to be at least $90\\%$ , while the $\\pmb{\\theta}$ -update\u2019s acceptance ranges from $30\\%$ to $70\\%$ depending on the size of the constraint data $\\mathcal{C}$ . In the high-noise sampling scheme\u2019s importance sampling of the normalizing constant, the number of particles is set to 100 and a uniform temperature schedule of 50 inverse temperatures is employed. The percentage of positive signs is maintained around $75\\%$ and above. ", "page_idx": 21}, {"type": "text", "text": "In the case of the Washington DC data, we employ the same train/test/validation test split as in [26]. In terms of hyperparameter optimisation for GENSIT , we leverage the same architecture as in the Cambridge data and optimise the learning rate on the validation set. To ensure a meaningful comparison, we also optimise the learning rate and multitask weights of the GMEL framework on the validation set. We run each method appearing in Tab. 2 for an ensemble of size $E=10$ obtain error bars of this ensemble. ", "page_idx": 21}, {"type": "text", "text": "All experiments were run using a 32-core CPU machine with 128GB memory. SIM-MCMC and SIT-MCMC took approximately 0.5-1 second per iteration and were for 100 hours in the Cambridge data. SIM-NN took approximately 0.1 seconds per iteration and was run for 50 and 100 hours in the Cambridge, and DC data, respectively. Finally, our framework (GENSIT) took on average 0.2 seconds per iteration and ", "page_idx": 21}, {"type": "image", "img_path": "eUcyIe1AzY/tmp/d0962072842def4ff48a6e9db17def6aa67a7b97178343148d3c490078407f4d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure 8: SRMSE (y-axis) and CP ( $\\propto$ marker size) across $\\mathcal{C}$ (marker colour) computed cumulatively along $N$ iterations of Alg. 1 for $\\mathbf{T}|\\mathcal{D},\\mathcal{C}$ samples for GENSIT (Joint and Disjoint) and SITMCMC for the Cambridge dataset. The Joint GENSIT converges to a lower SRMSE faster than SIT-MCMC while achieving better $\\mathbf{T}^{*}$ coverage in all but the totally constrained $\\mathcal{C}$ cases. ", "page_idx": 21}, {"type": "text", "text": "was run for 70 and 120 hours in the Cambridge, and DC data, respectively. ", "page_idx": 21}, {"type": "text", "text": "E Auxiliary Experimental Results ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this section we provide auxiliary experimental results for the Cambridge and Washington datasets. ", "page_idx": 21}, {"type": "text", "text": "E.1 Cambridge, UK ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "ODM\u2019s reconstruction sensitivity to different ensembles sizes $E$ of Alg. 1 and number of steps $N$ is examined in Fig. 9. A strong preference towards $N\\gg E$ is evidenced by significant enhancements in both SRMSE and CP except for the Totally and Singly constrained ODMs (see Tab. 4). All other ODMs employ the GMB sampler 3.1 since the target distribution $\\mu$ is intractable. GMB mixes slower than closed-form sampling and therefore needs to be run for a larger number of steps $N$ . Despite the fact that a larger ensemble size $E$ facilitates exploration of the non-convex loss landscape, this is not materialised in T-space. The SRMSE\u2019s invariance under different $N,E$ configurations detected in the Totally and Singly ODMs suggests that the conditional independence of successive $\\mathbf{T}$ samples compensates for potentially inadequately navigating the loss landscape. Besides, any type of $\\mathbf{T}$ sampler used is invariant under initialisation. Therefore, leveraging both optimisation and sampling offers complementary benefits in the estimation of both $\\pmb{\\Lambda}$ and $\\mathbf{T}$ . ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "The information captured in different $\\mathcal{L}$ and $\\mathcal{D}$ is portrayed in Fig. 10. Our choice of $\\mathcal{L}(\\mathbf{x},\\mathbf{T},\\mathbf{A}\\,\\,;\\,\\,\\mathcal{D},\\pmb{\\nu})$ in the Joint scheme (see 3rd ${\\bf X}$ value from the left) achieves one of the lowest SRMSEs and highest CPs across $\\sigma,\\mathcal{C}$ . Tuning of $\\pmb{\\nu}$ becomes essential when loss terms assimilating multiple data sources may yield conflicting metrics (lower SRMSE and low CP, and vice versa). ", "page_idx": 22}, {"type": "table", "img_path": "eUcyIe1AzY/tmp/8c7f56bea13f14bf34b7af527ca34e9280204bbf3bd40d1a979dcd860e173078.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "eUcyIe1AzY/tmp/74b2a9c9bc9a4a6c26dae2604e1b2ace01b6ed9b072c01069a53d927ef291c81.jpg", "table_caption": ["(a) ODMs with closed-form (tractable) T distributions (10). "], "table_footnote": ["(b) ODMs with intractable $\\mathbf{T}$ distribution (13), where conditioning $\\pmb{\\Lambda}$ on $\\mathcal{C}$ is problematic. "], "page_idx": 22}, {"type": "text", "text": "Table 6: Table 1 expanded to multiple SDE noise $\\sigma$ regimes. ", "page_idx": 22}, {"type": "text", "text": "Fig. 8 sheds light on the effect of such information propagation on the convergence rate of running estimates of $\\mathbb{E}[\\cdot~\\vert~\\mathcal{D}]$ to the ground truth $\\mathbf{T}^{*}$ . The Joint GeNSIT scheme converges to a mean $\\mathbf{T}$ estimate much earlier than SIT-MCMC across all $\\mathcal{C}$ regimes. Mean $\\mathbf{T}$ estimates are improved in the Joint GeNSIT compared to SIT-MCMC in confined $\\mathbf{T}$ spaces (Doubly, Doubly and $10\\%$ cell, Doubly and $20\\%$ cell constrained ODMs). Under the same $\\mathcal{C}$ regimes CP does not improve significantly as $N$ grows large, which suggests that the variance of $\\mathbf{T}$ samples appears stable as early as $N=\\mathrm{i}0^{4}$ . In the Disjoint GeNSIT the information encoded in larger $\\mathcal{C}_{T}$ is not propagated to the $\\Lambda$ updates. As a result, no SRMSE or CP improvements are detected in the course of $N$ . ", "page_idx": 22}, {"type": "text", "text": "E.2 Washington, DC, USA ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We append the expanded Tab. 2 of results for the Washington dataset in Tab. 7. ", "page_idx": 22}, {"type": "text", "text": "F Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Our codebase and the real-world data we used are accessible from the Supplementary Material. Trip and employment data for Cambridge, UK are obtained from the Office of National Statistics and used in [46]. The entire feature space for the Washington DC data is accessible through this repository. All data assets leveraged are under the CC-BY 4.0 licence. Once the repository is cloned and a Python virtual environment needs to be created the gensit package can be installed. Depending on the machine capabilities, the number of workers and threads per worker can be set as follows: ", "page_idx": 22}, {"type": "image", "img_path": "eUcyIe1AzY/tmp/f94ad735d183f4483a3175e2553b666da6def0bc2a38d88add8536c0577ab7ab.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "", "img_caption": ["size) under different combinations of the number of iterations $N$ and ensemble size $E$ used in Alg. 1 for a fixed budget of $N\\times E\\,=\\,10^{5}$ When $N\\gg E$ , local $\\Lambda$ , $\\mathbf{T}$ information is greedily leveraged compared to global information accumulated over the NN ensemble of $\\mathbf W^{(0)}$ initialisations. Closed-form $\\mathbf{T}$ sampling eliminates the dependence between successive $\\mathbf{T}$ samples. In contrast, Gibbs Markov Basis sampling requires a larger number of steps $N$ to converge to the stationary target distribution $\\mu$ at the expense of higher sample autocorrelation. ", "Figure 10: T-space SRMSE and $99\\%$ HMR cell CP ( ${\\displaystyle\\propto}$ marker size) achieved for different choices of $\\mathcal{L}$ (see Tab. 5 for definitions). Both a low SRMSE and a high CP cannot be achieved for these $\\mathcal{L}$ choices. This pattern is more prevalent in $\\mathcal{L}$ \u2019s that assimilate both y and $\\mathbf{D}_{\\cdot+}$ , suggesting the possibility of contradictory information in the two datasets. Losses that include $\\mathcal{L}\\left(\\mathbf{T},\\Lambda\\right)$ achieve lower SRMSEs and are not significantly reduced in light of more $\\mathcal{C}$ data. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "export \\$N_WORKERS [input_here] export \\$N_THREADS [input_here] ", "page_idx": 23}, {"type": "text", "text": "F.1 Cambridge, UK ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "export \\$CAMBRIDGE cambridge_work_commuter_lsoas_to_msoas ", "page_idx": 23}, {"type": "text", "text": "The experimental protocols used in the paper can be executed as follows. Experiment 1 can be reproduced by running ", "page_idx": 23}, {"type": "table", "img_path": "eUcyIe1AzY/tmp/deeafcd2db5c5e06ab51577fa3602bf5abe11f8a259695b24d63a188a0a35fee.jpg", "table_caption": [], "table_footnote": ["Table 7: Table 2 expanded to multiple SDE noise $\\sigma$ regimes. "], "page_idx": 24}, {"type": "text", "text": "gensit run ./data/inputs/configs/\\$CAMBRIDGE/experiment1_disjoint.toml   \n-sm \\   \n-et SIM_MCMC -et SIM_NN -et NonJointTableSIM_NN \\   \n-nt \\$N_THREADS -nw \\$N_WORKERS ", "page_idx": 24}, {"type": "text", "text": "and ", "page_idx": 24}, {"type": "text", "text": "gensit run ./data/inputs/configs/\\$CAMBRIDGE/experiment1_joint.toml   \n-sm \\   \n-et JointTableSIM_NN \\   \n-nt \\$N_THREADS -nw \\$N_WORKERS ", "page_idx": 24}, {"type": "text", "text": "The exploration-exploitation experiment can be run through commands ", "page_idx": 24}, {"type": "text", "text": "gensit run ./data/inputs/configs/\\$CAMBRIDGE/experiment2_disjoint.toml   \n\\   \n-sm -et SIM_MCMC -et SIM_NN -et NonJointTableSIM_NN \\   \n-nt \\$N_THREADS -nw \\$N_WORKERS ", "page_idx": 24}, {"type": "text", "text": "and ", "page_idx": 24}, {"type": "text", "text": "gensit run ./data/inputs/configs/\\$CAMBRIDGE/experiment2_joint.toml   \n-sm -et JointTableSIM_NN \\   \n-nt \\$N_THREADS -nw \\$N_WORKERS ", "page_idx": 24}, {"type": "text", "text": "Finally, the comparison of loss functions in experiment 3 can be performed using ", "page_idx": 24}, {"type": "text", "text": "gensit run ./data/inputs/configs/\\$CAMBRIDGE/experiment3_joint.toml   \n-sm -et JointTableSIM_NN \\   \n-nt \\$N_THREADS -nw \\$N_WORKERS ", "page_idx": 24}, {"type": "text", "text": "F.2 Washington, DC, USA ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "The GENSIT Disjoint scheme is run using ", "page_idx": 25}, {"type": "table", "img_path": "eUcyIe1AzY/tmp/0987624889c6ea95488e385db79715fd4dfdf09e0e8379edd4e3f20e72bf9985.jpg", "table_caption": [], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "The GENSIT Joint scheme is run using ", "page_idx": 25}, {"type": "text", "text": "gensit run ./data/inputs/configs/DC/experiment1_nn_joint.toml \\   \n-sm -et JointTableSIM_NN \\   \n-nt \\$N_THREADS -nw \\$N_WORKERS ", "page_idx": 25}, {"type": "text", "text": "The SIM-NN comparison is run using ", "page_idx": 25}, {"type": "text", "text": "gensit run ./data/inputs/configs/DC/experiment1_nn_disjoint.toml \\   \n-sm -et SIM_NN \\   \n-nt \\$N_THREADS -nw \\$N_WORKERS ", "page_idx": 25}, {"type": "text", "text": "The GMEL comparison is run using ", "page_idx": 25}, {"type": "text", "text": "gensit run ./data/inputs/configs/DC/vanilla_comparisons.toml -sm -et GraphAttentionNetworkModel_Comparison \\ -nt \\$N_THREADS -nw \\$N_WORKERS ", "page_idx": 25}, {"type": "text", "text": "For a detailed explanation of how to reproduce the Tabs. and Figs. we refer the reader to the README.md file in root directory of our codebase, which can be found in the Supplementary Material. ", "page_idx": 25}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Scalability claims are backed by computational complexities, two large empirical studies, and convergence studies. Performance claims backed by empirical studies and appropriate metrics. ", "page_idx": 26}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Limitations in terms of handling structured cell constraints discussed in concluding remarks. Limitations in terms of performance under low-constraint regimes discussed in main experimental section. ", "page_idx": 26}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: Citation of proven results provided. ", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: All necessary information needed to reproduce experimental findings is provided in the main text and Appendix. Code to reproduce computational experiments, as well as the real-world datasets are provided in the supplementary material. ", "page_idx": 26}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: As mentioned previously, all code and data are provided in the supplementary material and will become open access at the end of the reviewing period. ", "page_idx": 26}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Hyperparameter tuning and train/test/validation splits are provided either in the main text or in the appendix. ", "page_idx": 26}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We cite all error bars where computationally feasible and discuss the statistical significance of our empirical claims in the main text. ", "page_idx": 26}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: All computational complexities are cited in the main paper, while compute resources are referenced in both the main text and Appendix. ", "page_idx": 27}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: All datasets and frameworks used conform to the Code of Ethics. Spatial datasets are provided at regional levels where individual anonymity is preserved. ", "page_idx": 27}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: This work can assist practitioners working with multi-agent systems in the development of agent-based models. There are potential positive impacts of applying this work to agent-based modelling allowing decision-makers to test different policy scenariors. ", "page_idx": 27}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] Justification: No risks posed. ", "page_idx": 27}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: All data assets leveraged are under the CC-BY 4.0 license ", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: All new assets are properly documented and released under the CC-BY 4.0 license. ", "page_idx": 27}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [No] ", "page_idx": 27}, {"type": "text", "text": "Justification: All datasets used are publicly accessible and no primary data collection has been undertaken. ", "page_idx": 27}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: No primary data collection has been undertaken. ", "page_idx": 27}]