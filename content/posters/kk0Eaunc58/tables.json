[{"figure_path": "kk0Eaunc58/tables/tables_1_1.jpg", "caption": "Table 1: ViT Configurations", "description": "This table shows the configurations of three different sizes of Vision Transformers (ViTs): ViT-Ti, ViT-S, and ViT-B.  It lists the number of layers, the embedding dimension (\"Dim\"), the number of attention heads, the dimension of each head, and the total number of parameters in each model. The table highlights the key differences in architecture and model size among these ViT variations.", "section": "1 Introduction"}, {"figure_path": "kk0Eaunc58/tables/tables_5_1.jpg", "caption": "Table 2: The accuracy of HydraViT with our different design choices. \"3 Heads\" corresponds to a subnetwork that has the same architecture as DeiT-tiny, \"6 Heads\" corresponds to DeiT-small, and \"12 Heads\" corresponds to DeiT-base.", "description": "This table presents the accuracy results of HydraViT under various design choices.  It investigates the impact of three factors on the model's performance: weighted sampling of subnetworks during training, the use of separate classifier heads for each subnetwork, and the number of training epochs. The results are shown for three different subnetwork sizes, corresponding to the DeiT-tiny, DeiT-small, and DeiT-base models. Each row represents a specific configuration of these three factors, and the accuracies achieved for each subnetwork size are reported. The final row shows the accuracies of the original DeiT models for comparison.", "section": "4 Evaluation"}, {"figure_path": "kk0Eaunc58/tables/tables_7_1.jpg", "caption": "Table 3: Comparison of HydraViT with the baselines MatFormer, DynaBERT, and SortedNet. The table shows for selected subnetworks the RAM usage, MACs, model parameters, throughput and their corresponding accuracy when trained from scratch (when applicable) and from the initial DeiT-tiny checkpoint. Note that DynaBERT relies on Knowledge Distillation in every block, which is why it reaches less than 1% accuracy when trained from scratch.", "description": "This table compares HydraViT against three baseline models (MatFormer, DynaBERT, and SortedNet) across different subnetwork sizes (defined by embedding dimension).  It shows the RAM usage, number of Multiply-Accumulate operations (MACs), model parameters, throughput, and accuracy for each model, both when trained from scratch and initialized with a DeiT-tiny checkpoint.  The table highlights HydraViT's efficiency and performance relative to the baselines, especially when considering that DynaBERT uses knowledge distillation which gives it an unfair advantage when initialized with a checkpoint.", "section": "4.2 HydraViT vs. Baselines"}, {"figure_path": "kk0Eaunc58/tables/tables_13_1.jpg", "caption": "Table 3: Comparison of HydraViT with the baselines MatFormer, DynaBERT, and SortedNet. The table shows for selected subnetworks the RAM usage, MACs, model parameters, throughput and their corresponding accuracy when trained from scratch (when applicable) and from the initial DeiT-tiny checkpoint. Note that DynaBERT relies on Knowledge Distillation in every block, which is why it reaches less than 1% accuracy when trained from scratch.", "description": "This table compares the performance of HydraViT against three baseline models (MatFormer, DynaBERT, and SortedNet) across various subnetworks.  It provides key metrics for each model including RAM usage, number of Multiply-Accumulate operations (MACs), model parameters, throughput, and accuracy.  The accuracy is shown for two training scenarios: training from scratch and training initialized with a DeiT-tiny checkpoint. A note highlights that DynaBERT's reliance on Knowledge Distillation affects its accuracy when training from scratch.", "section": "4.2 HydraViT vs. Baselines"}, {"figure_path": "kk0Eaunc58/tables/tables_14_1.jpg", "caption": "Table 3: Comparison of HydraViT with the baselines MatFormer, DynaBERT, and SortedNet. The table shows for selected subnetworks the RAM usage, MACs, model parameters, throughput and their corresponding accuracy when trained from scratch (when applicable) and from the initial DeiT-tiny checkpoint. Note that DynaBERT relies on Knowledge Distillation in every block, which is why it reaches less than 1% accuracy when trained from scratch.", "description": "This table compares the performance of HydraViT against three other models (MatFormer, DynaBERT, and SortedNet) across various metrics.  It shows RAM usage, number of multiply-accumulate operations (MACs), model parameters, throughput, and accuracy for different subnetwork sizes. The comparison is made for models trained both from scratch and initialized using DeiT-tiny weights. A key observation is that DynaBERT's performance suffers significantly when trained from scratch due to its reliance on knowledge distillation.", "section": "4.2 HydraViT vs. Baselines"}, {"figure_path": "kk0Eaunc58/tables/tables_14_2.jpg", "caption": "Table 6: HydraViT loading times, each model was loaded six times.", "description": "This table presents the latency results of loading different HydraViT models with varying numbers of heads and embedding dimensions.  Each model was loaded six times to measure the latency, providing an average loading time and standard deviation for each configuration. This data helps demonstrate the model loading time of HydraViT, highlighting its efficiency and suitability for scenarios where switching models at runtime is necessary.", "section": "D Model Loading Time"}, {"figure_path": "kk0Eaunc58/tables/tables_15_1.jpg", "caption": "Table 7: The accuracy of HydraViT when initialized with DeiT-tiny vs DeiT-base. While the accuracy at the 12 heads is higher with DeiT-base initialization the average accuracy is lower than with DeiT-tiny initialization.", "description": "This table compares the accuracy of HydraViT when initialized with either DeiT-tiny or DeiT-base.  It shows the accuracy achieved with 3, 6, and 12 heads after 400 epochs of training. While DeiT-base initialization leads to slightly better accuracy with 12 heads, the average accuracy across all three head configurations is superior when using DeiT-tiny initialization.", "section": "4. Evaluation"}]