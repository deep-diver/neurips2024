{"references": [{"fullname_first_author": "Alexey Dosovitskiy", "paper_title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale", "publication_date": "2021-00-00", "reason": "This paper introduced the Vision Transformer (ViT), the foundation upon which HydraViT is built and a core topic of the paper."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Training data-efficient image transformers & distillation through attention", "publication_date": "2021-00-00", "reason": "This paper details the training procedure used for HydraViT, a crucial aspect of its methodology and a key comparison point for evaluating performance."}, {"fullname_first_author": "Mojtaba Valipour", "paper_title": "Sortednet, a place for every network and every network in its place: Towards a generalized solution for training many-in-one neural networks", "publication_date": "2023-00-00", "reason": "This paper is a key baseline for comparison, representing a state-of-the-art approach to scalable network architectures that is directly compared against HydraViT."}, {"fullname_first_author": "Sneha Kudugunta", "paper_title": "MatFormer: Nested Transformer for Elastic Inference", "publication_date": "2023-00-00", "reason": "Another significant baseline for comparison, this paper offers a different approach to scalable transformers that directly competes with HydraViT's method."}, {"fullname_first_author": "Lu Hou", "paper_title": "Dynabert: Dynamic bert with adaptive width and depth", "publication_date": "2020-00-00", "reason": "This paper provides a further baseline for comparison, illustrating another existing strategy for creating adaptable neural network architectures that is contrasted with HydraViT"}]}