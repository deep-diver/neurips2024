[{"figure_path": "3HpgVs22UJ/figures/figures_0_1.jpg", "caption": "Figure 1: Conceptual idea of QCS: Follow RCSL when learning from optimal trajectories where it predicts actions confidently but the Q-function may stitch incorrectly. Conversely, refer to the Q-function when learning from sub-optimal trajectories where RCSL is less certain but the Q-function is likely accurate.", "description": "This figure illustrates the core concept of Q-Aided Conditional Supervised Learning (QCS).  QCS adaptively combines Return-Conditioned Supervised Learning (RCSL) and Q-functions. When the agent learns from optimal trajectories, RCSL is prioritized due to its inherent stability and accuracy in predicting confident actions. However, RCSL struggles with stitching suboptimal trajectory segments to improve overall performance.  Therefore, when the agent encounters suboptimal trajectories, the Q-function's stitching ability is leveraged to create a more accurate loss function. The weighting between RCSL and the Q-function is dynamically adjusted based on the trajectory return; higher returns favor RCSL, lower returns favor the Q-function.", "section": "1 Introduction"}, {"figure_path": "3HpgVs22UJ/figures/figures_1_1.jpg", "caption": "Figure 2: Mean normalized return in MuJoCo medium, medium-replay, medium-expert, and AntMaze large. The scores of RCSL, the value-based methods, and the combined methods represent the maximum mean performances within their respective groups. The full scores are in Section 6.2.", "description": "This figure compares the performance of QCS against other state-of-the-art methods across four different environments: MuJoCo medium, MuJoCo medium-replay, MuJoCo medium-expert, and AntMaze large.  The results are presented as mean normalized returns, grouped by method type (RCSL, Value-Based, Combined RCSL-Value, and QCS).  The maximum mean return achieved within each group is highlighted. The figure shows that QCS consistently outperforms other approaches in most scenarios, often exceeding the maximum return achieved by other methods. Detailed scores for each environment and method are provided in Section 6.2 of the paper.", "section": "6 Experiments"}, {"figure_path": "3HpgVs22UJ/figures/figures_3_1.jpg", "caption": "Figure 3: An example demonstrating the limit of RCSL: The dataset consists of two trajectories, with a time limit of T = 3 and a discount factor \u03b3 = 1. The black dashed arrow represents the optimal policy yielding a maximum return of 7.", "description": "This figure illustrates a simple Markov Decision Process (MDP) with a start state and a goal state.  Two trajectories are shown: one successful trajectory with a return of 7 and one unsuccessful trajectory with a return of 6.  The optimal policy (dashed arrows) is to go from the start state to the goal state with a return of 7, but because RCSL only considers returns from the provided trajectories, it might choose the suboptimal path with the highest return from the provided trajectories, demonstrating its lack of stitching ability.", "section": "3.1 How Can Max-Q Policy Surpass RCSL in Suboptimal Datasets?"}, {"figure_path": "3HpgVs22UJ/figures/figures_4_1.jpg", "caption": "Figure 4: (a) the view of the environment and true Q calculated through value iteration, (b) training datasets with color representing the true Q for each sample, (c) Qe learned through regression with a medium dataset (upper) and an expert dataset (bottom), (d) Qe learned through IQL with a medium dataset (upper) and an expert dataset (bottom).", "description": "This figure shows a visualization of the learned Q-function (Qe) in a simple environment with discrete states and actions.  Panel (a) displays the environment and the true Q-values obtained through value iteration.  Panels (b) show the training datasets, where color intensity represents the true Q-value for each data point. Panels (c) and (d) compare the learned Qe using regression and IQL, respectively, trained on both a medium-quality dataset (containing a mix of near-optimal and suboptimal actions) and an expert dataset (containing mostly near-optimal actions). The figure highlights the over-generalization effect observed when the Q-function is trained on the expert dataset, showing a flattened Qe,  which contrasts with the more accurate Qe learned from the medium dataset.", "section": "3 When Is Q-Aid Beneficial for RCSL?"}, {"figure_path": "3HpgVs22UJ/figures/figures_4_2.jpg", "caption": "Figure 5: We present the estimated Qe(s,\u0101) for \u0101 = A and the normalized NTK ke(s,\u0101, s, aref)/||\u2207eQe(s, Aref)||2 across four datasets with a 1D action space for Inverted Double Pendulum and a 3D action space for Hopper. In these figures, we fix the state s and the fixed reference action aref at zero (marked as ), and sweep over all actions \u0101 \u2208 A. For Hopper, we use axes for action dimensions and color to represent Q-values in 3D plots. Additionally, in the NTK plot, we only include the high-NTK regions for values over 0.9. Refer to Appendix E for details.", "description": "This figure visualizes the learned Q-function (Qe) and the Neural Tangent Kernel (NTK) for four different datasets: Inverted Double Pendulum Medium, Inverted Double Pendulum Expert, Hopper Medium-Replay, and Hopper Expert.  It shows how the Q-function and NTK vary across the action space, illustrating the over-generalization phenomenon observed in the expert dataset where the Q-function becomes nearly flat due to limited action diversity.", "section": "3 When Is Q-Aid Beneficial for RCSL?"}, {"figure_path": "3HpgVs22UJ/figures/figures_7_1.jpg", "caption": "Figure 6: Views of tasks used in our experiments.", "description": "This figure shows screenshots of the seven different tasks used in the paper's experiments.  The tasks represent a variety of challenges in offline reinforcement learning, including continuous control (Halfcheetah, Hopper, Walker2d), sparse rewards (AntMaze), and complex manipulation (Adroit).  The AntMaze environments showcase different map complexities and sizes.  The variety of tasks demonstrates the broad applicability and effectiveness of the proposed QCS algorithm across diverse offline RL benchmarks.", "section": "6 Experiments"}, {"figure_path": "3HpgVs22UJ/figures/figures_9_1.jpg", "caption": "Figure 7: t-SNE [40] analysis of states visited by policies trained with RCSL, max-Q (argmaxa\u2208AQIL(s,a)), and QCS losses during evaluation, alongside dataset's states in walker2d-medium.", "description": "This figure compares the state distributions explored by RCSL, max-Q, and QCS policies during evaluation. RCSL and max-Q represent the extremes of QCS. RCSL's adherence to dataset states is shown, contrasting with the state distribution shift of max-Q. QCS inherits RCSL's stability but surpasses its performance, indicating a blend of transition recombination without straying from the state distribution.", "section": "Test Time State Distribution Shift"}, {"figure_path": "3HpgVs22UJ/figures/figures_15_1.jpg", "caption": "Figure 8: Distribution of trajectory returns in the MuJoCo datasets, including the dataset's maximum trajectory return and the QCS score.", "description": "This figure shows the distribution of trajectory returns for different datasets in the MuJoCo environment.  Each dataset (medium, medium-replay, medium-expert for each of the three tasks: Halfcheetah, Hopper, Walker2d) is represented by a histogram showing the frequency of different trajectory returns.  Vertical lines indicate the maximum trajectory return observed in each dataset, and the score achieved by the QCS algorithm.  The figure helps illustrate the differences in dataset quality and how QCS performs relative to the best possible outcome.", "section": "6 Experiments"}, {"figure_path": "3HpgVs22UJ/figures/figures_17_1.jpg", "caption": "Figure 4: (a) the view of the environment and true Q calculated through value iteration, (b) training datasets with color representing the true Q for each sample, (c) Qe learned through regression with a medium dataset (upper) and an expert dataset (bottom), (d) Qe learned through IQL with a medium dataset (upper) and an expert dataset (bottom).", "description": "This figure visualizes the results of an experiment designed to demonstrate the over-generalization of the Q-function when trained on optimal trajectories.  Panel (a) shows the environment and the true Q-values calculated using value iteration. Panel (b) displays the training datasets used, with color-coding representing the true Q-value for each sample. Panels (c) and (d) present the learned Q-functions (Qe) obtained using regression and IQL, respectively. The upper row shows results using a medium dataset, and the bottom row displays results from an expert dataset. This comparison highlights how the Q-function trained on the expert dataset exhibits over-generalization, resulting in a flat Q-value across the action space.", "section": "3 When Is Q-Aid Beneficial for RCSL?"}, {"figure_path": "3HpgVs22UJ/figures/figures_17_2.jpg", "caption": "Figure 5: We present the estimated Qe(s,\u0101) for \u0101 = A and the normalized NTK ke(s,\u0101, s, aref)/||\u2207eQe(s, aref)||2 across four datasets with a 1D action space for Inverted Double Pendulum and a 3D action space for Hopper. In these figures, we fix the state s and the fixed reference action aref at zero (marked as ), and sweep over all actions \u0101 \u2208 A. For Hopper, we use axes for action dimensions and color to represent Q-values in 3D plots. Additionally, in the NTK plot, we only include the high-NTK regions for values over 0.9. Refer to Appendix E for details.", "description": "This figure shows the learned Q-function (Qe) and its Neural Tangent Kernel (NTK) for different datasets.  The 1D action space plots (Inverted Double Pendulum) show Qe values as a function of actions (\u0101) while fixing the state (s) and a reference action (aref). The 3D action space plots (Hopper) visualize Qe values using color for the three dimensions of action.  The NTK plots demonstrate the influence of updating the Q-function for one action-state pair on other pairs. High NTK values indicate strong overgeneralization of the Q-function, particularly visible in the expert dataset.", "section": "3 When Is Q-Aid Beneficial for RCSL?"}, {"figure_path": "3HpgVs22UJ/figures/figures_18_1.jpg", "caption": "Figure 11: The average L2 distance between different actions within each quantized state in the Inverted Double Pendulum and MuJoCo Hopper environments. All histograms are plotted with 50 bins.", "description": "This figure shows the distribution of the L2 distances between actions within each quantized state for the Inverted Double Pendulum and Hopper environments. Separate histograms are shown for the expert, medium, and medium-replay datasets.  The expert datasets show a more concentrated distribution of actions, indicating that optimal policies tend to select actions within a narrower range. In contrast, suboptimal datasets show a wider spread of actions.", "section": "E.4 Action Distributions"}, {"figure_path": "3HpgVs22UJ/figures/figures_22_1.jpg", "caption": "Figure 12: Training curves of QCS-R and QCS-G in the MuJoCo and AntMaze domains.", "description": "This figure presents the training curves for both QCS-R (return-maximizing tasks) and QCS-G (goal-reaching tasks) across various MuJoCo and AntMaze environments.  The x-axis represents the gradient steps (in powers of 10), and the y-axis displays the normalized return. The curves show the performance trends for each algorithm during training across different datasets. The shaded regions likely represent confidence intervals or standard deviations.  This helps visualize the stability and convergence of the QCS algorithms in different environments and tasks, showcasing their learning performance.", "section": "I Training Curves"}]