[{"figure_path": "8abNCVJs2j/figures/figures_2_1.jpg", "caption": "Figure 1: Scatter plot of AF\u2081 with AF\u2082 and their distributions on GPT-2 small 124M for iteration k\u2208 [1,6000].", "description": "This figure shows a scatter plot of the change in objective function (\u0394F\u2081) when updating both weights and masks versus the change in objective function (\u0394F\u2082) when only updating weights.  The data is from training a GPT-2 small 124M model for 6000 iterations.  The marginal distributions of \u0394F\u2081 and \u0394F\u2082 are also shown as histograms.  This figure is used to illustrate the phenomenon of incorrect descending direction in sparse training with hard thresholding.", "section": "3 The curse of discontinuity"}, {"figure_path": "8abNCVJs2j/figures/figures_3_1.jpg", "caption": "Figure 2: (a)-(c) shows scatter plots of the predicted and actual loss reduction of dense, hard-thresholding and S-STE with GPT-2 large 774M model for iteration k \u2208 [1, 3000]. The diagonal line is for reference. (d) shows empirical cumulative distribution of their actual AoD for k \u2208 [1, 6000].", "description": "This figure compares the predicted and actual loss reduction for different training methods (dense, hard-thresholding, and S-STE) using the GPT-2 large 774M model. Subfigures (a), (b), and (c) show scatter plots illustrating the relationship between predicted and actual loss reduction for each method.  The diagonal line represents perfect prediction.  Subfigure (d) displays the cumulative distribution of the actual amount of descent (AoD) for each method, highlighting the differences in their optimization behavior.", "section": "The curse of discontinuity"}, {"figure_path": "8abNCVJs2j/figures/figures_4_1.jpg", "caption": "Figure 2: (a)-(c) shows scatter plots of the predicted and actual loss reduction of dense, hard-thresholding and S-STE with GPT-2 large 774M model for iteration k \u2208 [1, 3000]. The diagonal line is for reference. (d) shows empirical cumulative distribution of their actual AoD for k \u2208 [1, 6000].", "description": "This figure shows a comparison of the predicted and actual loss reduction for three different methods: dense training, hard-thresholding, and S-STE (smooth straight-through estimator).  Subfigures (a), (b), and (c) are scatter plots illustrating the relationship between predicted and actual loss reduction for each method. The diagonal line represents the ideal scenario where predicted and actual loss reduction match perfectly.  Subfigure (d) displays the cumulative distribution of the actual amount of descent (AoD) for each method, illustrating the performance variation over a larger number of iterations. The figure highlights the inconsistencies and issues with hard thresholding, which are addressed by the proposed S-STE method.", "section": "The curse of discontinuity"}, {"figure_path": "8abNCVJs2j/figures/figures_5_1.jpg", "caption": "Figure 4: (a) Flip rate curve over the training process with different \u03b2 on Transformer-base. (b) Dynamically recalculated \u03b2 at each layer on different epochs. Results show that frequently updating \u03b2 will cause it to be unexpectedly large. (c) Flip rate curve over the training process with fixed and dynamic \u03b2 on Transformer-base. (d) Flip rate of dense, SR-STE and S-STE algorithm on Transformer-base.", "description": "This figure visualizes the impact of different beta (\u03b2) values and updating strategies on the flip rate during the training process.  Subfigure (a) shows how different constant \u03b2 values affect the flip rate's trajectory. Subfigure (b) demonstrates the effect of dynamically recalculating \u03b2 at each layer throughout various epochs, revealing that frequent updates lead to unexpectedly high \u03b2 values. Subfigure (c) compares the flip rate when using a fixed \u03b2 versus a dynamic \u03b2, highlighting the stability benefits of a fixed \u03b2. Finally, subfigure (d) provides a direct comparison of the flip rate for dense models, models trained using SR-STE, and models trained using the proposed S-STE method.", "section": "4.2 Fixed weight rescaling \u03b2"}]