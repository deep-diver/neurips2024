[{"type": "text", "text": "A Boosting-Type Convergence Result for ADABOOST.MH with Factorized Multi-Class Classifiers ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xin Zou\u2217 Zhengyu Zhou\u2217 Jingyuan Xu Weiwei Liu\u2020 ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "School of Computer Science, Wuhan University National Engineering Research Center for Multimedia Software, Wuhan University Institute of Artificial Intelligence, Wuhan University Hubei Key Laboratory of Multimedia and Network Communication Engineering, Wuhan University {zouxin2021, zzysince1999, jingyuanxu777, liuweiwei863} $@$ gmail.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "ADABOOST is a well-known algorithm in boosting. Schapire and Singer propose, an extension of ADABOOST, named ADABOOST.MH, for multi-class classification problems. K\u00e9gl shows empirically that ADABOOST.MH works better when the classical one-against-all base classifiers are replaced by factorized base classifiers containing a binary classifier and a vote (or code) vector. However, the factorization makes it much more difficult to provide a convergence result for the factorized version of ADABOOST.MH. Then, K\u00e9gl raises an open problem in COLT 2014 to look for a convergence result for the factorized ADABOOST.MH. In this work, we resolve this open problem by presenting a convergence result for ADABOOST.MH with factorized multi-class classifiers. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Boosting is an approach to machine learning based on the idea of creating a highly accurate prediction rule by combining many relatively weak and inaccurate rules [19] and has inspired a lot on theoretical analysis and algorithm design in supervised learning [11, 17]. The seminal algorithm in boosting, ADABOOST [9], requires no knowledge of the upper bound of the edge, which makes it convenient in practice. ", "page_idx": 0}, {"type": "text", "text": "In addition to to binary ADABOOST, [9] also proposes two multi-class extensions, named ADABOOST.M1 and ADABOOST.M2. Then, Schapire and Singer\u2019s seminal paper [20] proposes another extension named ADABOOST.MH. The main idea of ADABOOST.MH is to use vector-valued base classifiers to build a multi-class discriminant function of $K$ outputs when there are $K$ classes, and then replace the weight vector in ADABOOST with a weight matrix over instances and labels. ", "page_idx": 0}, {"type": "text", "text": "The simplest implementation of the concept in ADABOOST.MH is to use $K$ independent one-againstall classifiers in which base classifiers are only loosely connected through the common normalization of the weight matrix. However, [15] points out that such an implement is suboptimal in most of the practical problems since it is limited to only decision stumps weak learners. To solve this problem, [15] proposes another base learner named multi-class Hamming trees, which optimizes the multi-class edge without reducing the problem to $K$ binary classifications. The key idea in [15] is to factorize general vector-valued classifiers h into an input-independent code vector of length $K$ , i.e., $\\mathbf{v}\\in\\{-\\overline{{1}},+1\\}^{K}$ , and label-independent scalar classifier $\\varphi$ . However, [15] gets in trouble when proving the convergence rate of the proposed implement of ADABOOST.MH due to the factorization step. So [14] raises an open problem in COLT 2014, looking for a convergence rate of the factorized ADABOOST.MH in [15], with limited dependence on the sample size $n$ . ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Our contributions can be concluded as follows: ", "page_idx": 1}, {"type": "text", "text": "1. We provide a convergence result (Theorem 3.3) of factorized ADABOOST.MH, where the step $T^{*}$ which guarantees the training error to be 0 is of order $O\\left(n^{2}\\ln n\\right)$ .   \n2. According to the requirement of [14], we improve the dependence on $n$ and resolve the open problem by providing a convergence result (Theorem 3.4) where $T^{*}$ is of order $O\\left(K\\ln(n K)\\right)$ . This result greatly improves when $n$ is much larger than $K$ . ", "page_idx": 1}, {"type": "text", "text": "More related works are deferred to Appendix B. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We consider a multi-class classification problem where the input space is $\\mathcal{X}=\\mathbb{R}^{d}$ and $\\mathscr{L}=[K]$ is the label space, where $K$ is the number of classes and $[K]^{*}:=\\{1,\\dots,K\\}$ . Assume we attain the training data $\\mathcal{D}_{L}=\\left\\{(\\mathbf{x}_{1},\\ell(\\mathbf{x}_{1})),\\dots,(\\mathbf{x_{n}},\\ell(\\mathbf{x}_{n}))\\right\\}$ , where $\\ell(\\mathbf{x}_{i})\\in\\mathcal{L}$ is the label of $\\mathbf{x}_{i}$ . Since we want to use vector-valued classifiers, it is convenient to use the one-hot labels $\\mathbf{y}_{i}\\in\\{-1,+1\\}^{K}$ for $\\mathbf{x}_{i}$ , where $\\mathbf{y}_{i}(\\ell(\\mathbf{x}_{i}))\\,=\\,1$ and all the other elements are $-1$ . We use the new dataset $\\mathcal{D}=$ $\\{(\\mathbf{x}_{1},\\mathbf{y}_{1}),\\ldots,(\\mathbf{x}_{n},\\mathbf{y}_{n})\\}$ as the input data of ADABOOST.MH and define an observation matrix $\\mathbf{X}:=(\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{n})^{T}\\in\\mathbb{R}^{n\\times d}$ , a label matrix $\\mathbf{Y}:=(\\mathbf{y}_{1},\\dots,\\mathbf{y}_{n})^{T}\\in\\{-1,+1\\}^{n\\times K}$ . We call $\\mathbf{y}$ and $\\ell$ the label and the index of $\\mathbf{x}$ respectively, as in [14]. ", "page_idx": 1}, {"type": "text", "text": "[14] considers a special case of ADABOOST.MH, where each weak classifier has a specialized structure. ADABOOST.MH returns a vector-valued discriminant function $\\mathbf{f}\\,:\\,\\boldsymbol{\\chi}\\,\\rightarrow\\,\\dot{\\mathbb{R}}^{K}$ with a combined predictor $\\mathbf{F_{f}}:\\mathcal{X}\\to\\{-1,+1\\}^{K}$ where $\\mathbf{F}_{\\mathbf{f}}(\\mathbf{x})_{l}=\\mathrm{sign}(\\mathbf{f}(\\mathbf{x})_{l})$ for $l=1,\\ldots,K$ . Here and in this paper, we define ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathrm{sign}(x)=\\left\\{\\begin{array}{l l}{{+1\\;\\;\\;}}&{{x\\geq0}}\\\\ {{-1\\;\\;\\;}}&{{x<0}}\\end{array}\\right..\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "The goal of the ADABOOST.MH algorithm [20] is to return f such that the Hamming loss of $\\mathbf{F_{f}}$ , ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\widehat{R}_{\\mathrm{H}}(\\mathbf{F_{f}},\\mathbf{W}):=\\sum_{i=1}^{n}\\sum_{l=1}^{K}w_{i,l}\\mathbb{I}\\{\\mathbf{F_{f}}(\\mathbf{x}_{i})_{l}\\neq y_{i,l}\\},\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "is as small as possible, where $\\mathbb{I}(\\cdot)$ is the indicator function and $\\mathbf{W}\\,=\\,[w_{i,l}]\\,\\,\\in\\,\\,[0,1]^{n\\times K}$ is a distribution over the data points and the labels. W can be chosen as any distribution over $[n]\\times[K]$ and is different in different papers. In [20], the authors set $\\begin{array}{r}{w_{i,l}=\\frac{1}{n K}}\\end{array}$ for any $i\\in[n],l\\in[\\bar{K}]$ . Here, we follow [14] and set ", "page_idx": 1}, {"type": "equation", "text": "$$\nw_{i,l}=\\left\\{\\begin{array}{l l}{{\\frac{1}{2n}}}&{{\\quad\\mathrm{if}\\;y_{i,l}=+1}}\\\\ {{\\frac{1}{2n(K-1)}}}&{{\\quad\\mathrm{if}\\;y_{i,l}=-1}}\\end{array}\\right..\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "We define the weighted multi-class exponential margin-based error ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\widehat{R}_{\\mathrm{EXP}}(\\mathbf{f},\\mathbf{W}):=\\sum_{i=1}^{n}\\sum_{l=1}^{K}w_{i,l}\\exp{(-\\mathbf{f}(\\mathbf{x}_{i})_{l}\\cdot y_{i,l})}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "as a surrogate for $\\widehat{R}_{\\mathrm{H}}(\\mathbf{F_{f}},\\mathbf{W})$ . Since $\\mathbb{I}\\{\\mathbf{F_{f}}(\\mathbf{x}_{i})_{l}\\neq y_{i,l}\\}=\\mathbb{I}\\{\\mathbf{f}(\\mathbf{x}_{i})_{l}\\cdot y_{i,l}\\leq0\\}\\leq\\exp{(-\\mathbf{f}(\\mathbf{x}_{i})_{l}\\cdot y_{i,l})},$ we can get that $\\widehat{R}_{\\mathrm{H}}(\\mathbf{F_{f}},\\mathbf{W})\\leq\\widehat{R}_{\\mathrm{EXP}}(\\mathbf{f},\\mathbf{W})$ . ", "page_idx": 1}, {"type": "text", "text": "It\u2019s well-known that ADABOOST directly minimizes the exponential loss [19, Chapter 7], then, we can apply the ADABOOST algorithm to the extended binary training set $\\cup_{i=1}^{n}\\bar{\\{(\\mathbf{x}_{i},y_{i,l})\\}_{l=1}^{K}}$ , yielding the ADABOOST.MH algorithm, which directly minimizes $\\widehat{R}_{\\mathrm{EXP}}(\\mathbf{f},\\mathbf{W})$ and output the final discriminant function $\\mathbf{f}^{(T)}(\\cdot)$ , where $\\begin{array}{r}{\\mathbf{f}^{(T)}(\\mathbf{x})=\\sum_{t=1}^{T}\\mathbf{h}^{(T)}(\\mathbf{x})}\\end{array}$ is a sum of $T$ base classifiers $\\mathbf{h}^{(t)}:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{K}$ returned by a base learner algorithm $\\mathrm{BASE}(\\mathbf{X},\\mathbf{Y},\\mathbf{W}^{(t)})$ in each iteration $t$ . ", "page_idx": 1}, {"type": "text", "text": "Define ", "page_idx": 1}, {"type": "equation", "text": "$$\nZ(\\mathbf{h},\\mathbf{W})=\\sum_{i=1}^{n}\\sum_{l=1}^{K}w_{i,l}\\exp\\left(-\\mathbf{h}(\\mathbf{x}_{i})_{l}\\cdot y_{i,l}\\right),\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "by a similar calculation in [19, Proof of Theorem 3.1], we can obtain that: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\widehat{R}_{\\mathrm{EXP}}({\\bf f}^{(T)},{\\bf W})=\\prod_{t=1}^{T}Z({\\bf h}^{(t)},{\\bf W}^{(t)}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "According to the above discussion, we know that to minimize $\\widehat{R}_{\\mathrm{EXP}}(\\mathbf{f}^{(T)},\\mathbf{W})$ , the base learner needs to find a $\\mathbf{h}^{(t)}$ that minimizes $Z(\\mathbf{h}^{(t)},\\mathbf{W}^{(t)})$ at the $t$ -th iteration. In the following, we introduce two choices of $\\mathbf{h}$ in [20] and [15], the corresponding convergence rate of $\\widehat{R}_{\\mathrm{EXP}}(\\mathbf{f}^{(T)},\\mathbf{W})$ , and problems when trying to get a convergence rate of $\\widehat{R}_{\\mathrm{EXP}}(\\mathbf{f}^{(T)},\\mathbf{W})$ for factorized ADABOOST.MH. ", "page_idx": 2}, {"type": "text", "text": "2.1 Unfactorized Choice ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "[20] considers using h with the form $\\mathbf{h}(\\mathbf{x})=\\alpha\\varphi(\\mathbf{x})$ , where $\\alpha\\in\\mathbb{R}$ and $\\varphi:\\mathbb{R}^{d}\\rightarrow\\{-1,+1\\}^{K}$ can be seen as the vector consists of $K$ binary classifiers $\\varphi_{1},\\ldots,\\varphi_{K}$ . ", "page_idx": 2}, {"type": "text", "text": "We consider the $t$ -th iteration, and to simplify the notations, we omit the superscript $t$ and use ${\\mathbf W},{\\mathbf h},\\varphi,\\alpha$ to represent $\\mathbf{W}^{(t)},\\mathbf{h}^{(t)},\\varphi^{(t)},\\alpha^{(\\hat{t})}$ respectively. According to [20], if we define ", "page_idx": 2}, {"type": "equation", "text": "$$\nr=\\sum_{i=1}^{n}\\sum_{l=1}^{K}w_{i,l}\\cdot y_{i,l}\\cdot\\varphi(\\mathbf{x}_{i})_{l}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "as the edge, then we have ", "page_idx": 2}, {"type": "equation", "text": "$$\nZ(\\mathbf{h},\\mathbf{W})=\\sum_{i=1}^{n}\\sum_{l=1}^{K}w_{i,l}\\exp{(-\\mathbf{h}(\\mathbf{x}_{i})_{l}\\cdot y_{i,l})}=\\sum_{i=1}^{n}\\sum_{l=1}^{K}w_{i,l}\\exp{(-\\alpha\\varphi(\\mathbf{x}_{i})_{l}\\cdot y_{i,l})}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Since $\\begin{array}{r l r}{\\sum_{i,l:\\varphi(\\mathbf{x}_{i})_{l}\\cdot y_{i,l}=1}w_{i,l}\\ +\\ \\sum_{i,l:\\varphi(\\mathbf{x}_{i})_{l}\\cdot y_{i,l}=-1}w_{i,l}}&{{}=}&{1}\\end{array}$ and $\\begin{array}{r}{\\sum_{i,l:\\varphi(\\mathbf{x}_{i})_{l}\\cdot y_{i,l}=1}w_{i,l}}\\end{array}$ \u2212 $\\begin{array}{r}{\\sum_{i,l:\\varphi(\\mathbf{x}_{i})_{l}\\cdot y_{i,l}=-1}w_{i,l}=r}\\end{array}$ , we can get that ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\sum_{\\substack{i,l:\\varphi(\\mathbf{x}_{i})_{l}\\cdot y_{i,l}=1}}w_{i,l}=\\frac{1+r}{2},\\sum_{\\substack{i,l:\\varphi(\\mathbf{x}_{i})_{l}\\cdot y_{i,l}=-1}}w_{i,l}=\\frac{1-r}{2}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "So we have: ", "page_idx": 2}, {"type": "equation", "text": "$$\nZ(\\mathbf{h},\\mathbf{W})=\\frac{1+r}{2}\\cdot e^{-\\alpha}+\\frac{1-r}{2}\\cdot e^{\\alpha}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Fix $\\varphi$ first, minimizing $Z(\\mathbf{h},\\mathbf{W})$ over $\\alpha$ yields that: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\alpha=\\frac{1}{2}\\ln\\left(\\frac{1+r}{1-r}\\right).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "This gives ", "page_idx": 2}, {"type": "equation", "text": "$$\nZ(\\mathbf{h},\\mathbf{W})={\\sqrt{1-r^{2}}}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Then choose $\\varphi$ to minimize $\\sqrt{1-r^{2}}$ , i.e., maximize $|r|$ . If we have $r^{(t)}\\ge\\delta>0$ for all $t$ , then we can get: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\widehat{R}_{\\mathrm{EXP}}\\big(\\mathbf{f}^{(T)},\\mathbf{W}\\big)=\\prod_{t=1}^{T}\\sqrt{1-\\left(r^{(t)}\\right)^{2}}\\leq\\left(\\sqrt{1-\\delta^{2}}\\right)^{T}\\leq\\exp\\left(-\\frac{\\delta^{2}}{2}T\\right),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "which means that the weighted exponential error goes to error exponentially fast. Let $\\mathrm{exp}\\left(-{\\frac{\\delta^{2}}{2}}T\\right)<$ $\\textstyle{\\frac{1}{n K}}$ , we know that the weighted Hamming error becomes zero after ", "page_idx": 2}, {"type": "equation", "text": "$$\nT^{*}=\\left\\lceil\\frac{2\\ln(n K)}{\\delta^{2}}\\right\\rceil+1\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "iterations. The condition $r^{(t)}\\,\\geq\\,\\delta\\,>\\,0$ for all $t$ is satisfied when the empirically weak learning condition on the classifier $\\varphi$ holds for the extended binary training set $\\cup_{i=1}^{n}\\mathbf{\\dot{\\bar{\\{\\{(x_{i}},y_{i,l})\\}}}}_{l=1}^{K}$ . ", "page_idx": 2}, {"type": "text", "text": "Definition 2.1 (empirically $\\delta$ -weak learning condition). For a given binary dataset $\\left\\{\\left(\\mathbf{x}_{1},y_{1}\\right),\\ldots,\\left(\\mathbf{x}_{m},\\bar{y_{m}}\\right)\\right\\}$ where $y_{i}\\in\\{-1,+1\\}$ , we say that the empirically $\\delta$ -weak learning condition holds for some $\\delta>0$ if for any distribution $\\mathbf{w}\\in\\bar{\\Delta}^{m-1}$ over $[m]$ , we can always find a binary classifier $\\varphi:\\mathcal{X}\\to\\{-1,+1\\}$ such that: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\gamma=\\sum_{i=1}^{m}\\mathbf{w}_{i}\\cdot y_{i}\\cdot\\varphi(\\mathbf{x}_{i})\\geq\\delta,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\Delta^{m-1}=\\left\\{\\pmb{\\lambda}\\in\\mathbb{R}^{m}\\bigg|\\pmb{\\lambda}_{i}\\geq0\\,\\forall i\\in[m],\\sum_{i=1}^{m}\\pmb{\\lambda}_{i}=1\\right\\}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "is the $(m-1)$ -dimensional probability simplex. ", "page_idx": 3}, {"type": "text", "text": "2.2 Factorized Choice ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The original ADABOOST.MH [20] reduces the multi-class problem into $K$ binary one-against-all classifications. [15] avoids such a reduction by factorizing the vector-valued classifier $\\mathbf{h}$ into an input-independent vector of length $K$ and a label-independent scalar classifier. Formally, [15] sets ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{h}(\\mathbf{x})=\\alpha\\mathbf{v}\\varphi(\\mathbf{x}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\alpha\\in\\mathbb{R}^{+}$ is a positive real-valued base coefficient, $\\mathbf{v}\\in\\{-1,+1\\}^{K}$ is an input-independent vote (or code) vector of length $K$ , and $\\varphi:\\mathbb{R}^{d}\\rightarrow\\{-1,+1\\}$ is a label-independent binary classifier. For more details about the factorized ADABOOST.MH, please refer to Algorithm 1 in Appendix A. ", "page_idx": 3}, {"type": "text", "text": "We consider the $t$ -th iteration, and to simplify the notations, we omit the superscript $t$ and use $\\mathbf{W},\\mathbf{h},\\varphi,\\alpha,\\mathbf{v}$ to represent $\\mathbf{W}^{(t)},\\mathbf{h}^{(t)},\\varphi^{(t)},\\bar{\\alpha}^{(t)},\\mathbf{v}^{(t)}$ respectively. [15] shows that ", "page_idx": 3}, {"type": "equation", "text": "$$\nZ(\\mathbf{h},\\mathbf{W})=\\frac{e^{\\alpha}+e^{-\\alpha}}{2}-\\frac{e^{\\alpha}-e^{-\\alpha}}{2}\\cdot\\sum_{l=1}^{K}v_{l}\\left(\\mu_{l+}-\\mu_{l-}\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mu_{l-}=\\sum_{i=1}^{n}w_{i,l}\\mathbb{I}\\{\\varphi(\\mathbf{x}_{i})\\neq y_{i,l}\\}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "is the weighted per-class error rate and ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mu_{l+}=\\sum_{i=1}^{n}w_{i,l}\\mathbb{I}\\{\\varphi(\\mathbf{x}_{i})=y_{i,l}\\}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "is the weighted per-class correct classification rate for each class $l=1,\\ldots,K$ . Similar to Equation (5), we define the multi-class edge of the classifier $\\mathbf{h}$ as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\gamma=\\gamma({\\mathbf v},\\varphi,{\\mathbf W})=\\sum_{l=1}^{K}\\gamma_{l}=\\sum_{l=1}^{K}v_{l}\\left(\\mu_{l+}-\\mu_{l-}\\right)=\\sum_{i=1}^{n}\\varphi({\\mathbf x}_{i})\\sum_{l=1}^{K}w_{i,l}\\cdot v_{l}\\cdot y_{i,l},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\gamma_{l}=v_{l}\\left(\\mu_{l+}-\\mu_{l-}\\right)=\\sum_{i=1}^{n}w_{i,l}\\cdot v_{l}\\cdot\\varphi(\\mathbf{x}_{i})\\cdot y_{i,l}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "is the classwise edge of $\\mathbf{h}$ . By a similar calculation as in Section 2.1, we know that $Z(\\mathbf{h},\\mathbf{W})$ is minimized when we set ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\alpha=\\frac{1}{2}\\ln\\left(\\frac{1+\\gamma}{1-\\gamma}\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "which gives ", "page_idx": 3}, {"type": "equation", "text": "$$\nZ(\\mathbf{h},\\mathbf{W})={\\sqrt{1-\\gamma^{2}}}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "So in order to minimize $Z(\\mathbf{h},\\mathbf{W})$ , we need to choose $\\mathbf{v}$ and $\\varphi$ to maximize $|\\gamma|$ . From the equation $\\begin{array}{r}{\\gamma(\\mathbf{v},\\varphi,\\mathbf{W})\\;=\\;\\sum_{l=1}^{K}v_{l}\\left(\\mu_{l+}-\\mu_{l-}\\right)}\\end{array}$ , we know that if $\\gamma(\\mathbf{v},\\varphi,\\mathbf{W})\\;\\leq\\;0$ , then ${\\boldsymbol\\gamma}(-\\mathbf{v},\\boldsymbol\\varphi,\\mathbf{W})\\,=$ ", "page_idx": 3}, {"type": "text", "text": "$-\\gamma(\\mathbf{v},\\varphi,\\mathbf{W})\\ge0$ . So the problem reduces to finding $\\mathbf{v},\\varphi$ that maximize $\\gamma$ . From Equation (6) we know that for fixed $\\varphi,\\gamma$ is maximized when we choose $\\mathbf{v}$ as ", "page_idx": 4}, {"type": "equation", "text": "$$\nv_{l}=\\left\\{\\begin{array}{l l}{+1\\quad}&{\\mu_{l+}\\geq\\mu_{l-}}\\\\ {-1\\quad}&{\\mu_{l+}<\\mu_{l-}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "for all classes $l=1,\\ldots,K$ . ", "page_idx": 4}, {"type": "text", "text": "Similar to Section 2.1, if there exists a number $\\delta\\,>\\,0$ such that $\\gamma\\left(\\mathbf{v}^{(t)},\\varphi^{(t)},\\mathbf{W}^{(t)}\\right)\\,\\geq\\,\\delta$ for all $t=1,\\dots,T$ , then we can get an upper bound for $\\widehat{R}_{\\mathrm{EXP}}(\\mathbf{f}^{(T)},\\mathbf{W})$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\widehat{R}_{\\mathrm{EXP}}\\big(\\mathbf{f}^{(T)},\\mathbf{W}\\big)=\\prod_{t=1}^{T}\\sqrt{1-\\gamma\\left(\\mathbf{v}^{(t)},\\boldsymbol{\\varphi}^{(t)},\\mathbf{W}^{(t)}\\right)^{2}}\\leq\\left(\\sqrt{1-\\delta^{2}}\\right)^{T}\\leq\\exp\\left(-\\frac{\\delta^{2}}{2}T\\right),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "which means that the weighted exponential error goes to error exponentially fast. Let $\\mathrm{exp}\\left(-{\\frac{\\delta^{2}}{2}}T\\right)<$ 2n(K1\u22121), we know that the weighted Hamming error becomes zero after ", "page_idx": 4}, {"type": "equation", "text": "$$\nT^{*}=\\left\\lceil\\frac{2\\ln(2n(K-1))}{\\delta^{2}}\\right\\rceil+1\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "iterations. To get the exponential convergence rate, the question now is whether there exists a number $\\delta>0$ such that $\\gamma\\left(\\mathbf{v}^{(t)},\\varphi^{(t)},\\mathbf{W}^{(t)}\\right)\\geq\\bar{\\delta}$ for all $t=1,\\dots,T$ . ", "page_idx": 4}, {"type": "text", "text": "2.3 Conditions for the Two Choices ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "For the condition in the unfactorized choice, if the empirically $\\delta^{\\prime}$ -weak learning condition holds, then for a fixed weight matrix $\\mathbf{W}$ , let $I=\\left\\{l\\in\\left[K\\right]\\right|\\sum_{i=1}^{n}w_{i,l}\\'>0\\right\\}$ , then for all $l\\in I$ , there exists a binary classifier $\\varphi_{l}$ such that ", "page_idx": 4}, {"type": "equation", "text": "$$\nr_{l}=\\sum_{i=1}^{n}\\frac{w_{i,l}}{\\sum_{i=1}^{n}w_{i,l}}\\varphi_{l}(\\mathbf{x}_{i})y_{i,l}\\geq\\delta^{\\prime},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "then we can find a $\\varphi$ such that $\\varphi_{l}=\\varphi_{l}$ for $l\\in I$ so that ", "page_idx": 4}, {"type": "equation", "text": "$$\nr=\\sum_{i=1}^{n}\\sum_{l=1}^{K}w_{i,l}\\cdot\\varphi_{l}(\\mathbf{x}_{i})\\cdot y_{i,l}=\\sum_{l\\in I}\\sum_{i=1}^{n}w_{i,l}\\cdot\\varphi_{l}(\\mathbf{x}_{i})\\cdot y_{i,l}\\geq\\sum_{l\\in I}\\sum_{i=1}^{n}w_{i,l}\\cdot\\delta^{\\prime}=\\delta^{\\prime}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "So the empirically $\\delta^{\\prime}$ -weak learning condition is sufficient for an exponential convergence rate for the ADABOOST.MH algorithm in [20]. ", "page_idx": 4}, {"type": "text", "text": "For the factorized choice proposed in [15], we can not use the above argument since $\\mathbf{h}$ is factorized and we need to find a binary classifier $\\varphi$ for all $l=1,\\ldots,K$ , while for the unfactorized choice, we can find $K$ binary classifiers $\\varphi_{1},\\ldots,\\varphi_{K}$ separately for each class. In [14], the author tries to solve this problem by constructing pseudo-weights and pseudo-labels and then applying the empirically $\\delta^{\\prime}$ -weak learning condition to the constructed dataset $\\left\\{(\\mathbf{x}_{1},y_{1}^{\\prime}),\\ldots,(\\mathbf{x}_{n},y_{n}^{\\prime})\\right\\}$ . ", "page_idx": 4}, {"type": "text", "text": "[14] rewrites $\\gamma$ as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\gamma=\\sum_{i=1}^{n}\\varphi({\\bf x}_{i})\\sum_{l=1}^{K}w_{i,l}\\cdot v_{l}\\cdot y_{i,l}=\\sum_{i=1}^{n}\\varphi({\\bf x}_{i})\\sum_{l=1}^{K}w_{i,l}\\left[\\mathbb{I}\\{v_{l}\\cdot y_{i,l}=+1\\}-\\mathbb{I}\\{v_{l}\\cdot y_{i,l}=-1\\}\\right]}}\\\\ {{\\displaystyle\\quad=\\sum_{i=1}^{n}\\varphi({\\bf x}_{i})(w_{i}^{+}-w_{i}^{-})=\\sum_{i=1}^{n}\\varphi({\\bf x}_{i})\\mathrm{sign}(w_{i}^{+}-w_{i}^{-})|w_{i}^{+}-w_{i}^{-}|},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where we define ", "page_idx": 4}, {"type": "equation", "text": "$$\nw_{i}^{+}=\\sum_{l=1}^{K}w_{i,l}\\mathbb{I}\\{v_{l}\\cdot y_{i,l}=+1\\},\\;\\;w_{i}^{-}=\\sum_{l=1}^{K}w_{i,l}\\mathbb{I}\\{v_{l}\\cdot y_{i,l}=-1\\}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "for simplicity. Then we define $y_{i}^{\\prime}=\\mathrm{sign}(w_{i}^{+}-w_{i}^{-})$ as the $i$ -th pseudo-label and $w_{i}^{\\prime}=|w_{i}^{+}-w_{i}^{-}|$ as the $i$ -th pseudo-weight, then ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\gamma=\\sum_{i=1}^{n}w_{i}^{\\prime}\\cdot y_{i}^{\\prime}\\cdot\\varphi(\\mathbf{x}_{i}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "However, since $\\begin{array}{r}{\\sum_{i=1}^{n}w_{i}^{\\prime}=\\sum_{i=1}^{n}|w_{i}^{+}-w_{i}^{-}|\\leq\\sum_{i=1}^{n}(w_{i}^{+}+w_{i}^{-})\\mathop{=}1}\\end{array}$ , $\\mathbf{w}^{\\prime}=(w_{1}^{\\prime},\\ldots,w_{n}^{\\prime})$ is not necessarily a distribution on $[n]$ . To make use of the empirically $\\delta^{\\prime}$ -weak learning condition, we define ", "page_idx": 5}, {"type": "equation", "text": "$$\nw_{\\Sigma}^{\\prime}:=\\sum_{i=1}^{n}w_{i}^{\\prime}\\leq1.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "If we can get a lower bound $\\omega>0$ such that $w_{\\Sigma}^{\\prime}\\geq\\omega$ , then we have: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\gamma=\\sum_{i=1}^{n}w_{i}^{\\prime}\\cdot y_{i}^{\\prime}\\cdot\\varphi(\\mathbf{x}_{i})=w_{\\Sigma}^{\\prime}\\sum_{i=1}^{n}{\\frac{w_{i}^{\\prime}}{w_{\\Sigma}^{\\prime}}}\\cdot y_{i}^{\\prime}\\cdot\\varphi(\\mathbf{x}_{i})\\geq w_{\\Sigma}^{\\prime}\\cdot\\delta^{\\prime}\\geq\\omega\\cdot\\delta^{\\prime},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the first inequality is from the empirically $\\delta^{\\prime}$ -weak learning condition. Since the number of examples $n$ may be very large, we wish the lower bound $\\omega$ to be independent of $n$ , but it can depend on the number of classes $K$ . ", "page_idx": 5}, {"type": "text", "text": "Then [14] raises an open problem: ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Whether there exists a setup $(\\mathbf{X},\\mathbf{W},\\mathbf{Y}$ , and function class) in which all of the $2^{K}$ different vote vectors $\\mathbf{v}\\in\\{-1,+1\\}^{K}$ lead to arbitrarily small (or zero) $w_{\\Sigma}^{\\prime}$ , or we can find a constant (independent of $n$ ) lower bound $\\omega$ such that with at least one vote vector and classifier $\\varphi$ , $w_{\\Sigma}^{\\prime}\\geq\\omega$ holds? ", "page_idx": 5}, {"type": "text", "text": "We resolve this open problem by showing that: ", "page_idx": 5}, {"type": "text", "text": "There exists a constant $\\omega\\ =\\ {\\frac{1}{\\sqrt{2K}}}$ such that: for any $\\mathbf{X},\\mathbf{W},\\mathbf{Y}$ and function class, there always exists a vote vector v s.t. $w_{\\Sigma}^{\\prime}~\\geq~\\omega$ holds. With this result, if the empirically $\\delta^{\\prime}$ -weak learning condition holds, then for any $\\mathbf{X},\\mathbf{W},\\mathbf{Y}$ , there always exists a vote vector v and a binary classifier $\\varphi$ such that $\\begin{array}{r}{\\gamma\\ =\\ \\sum_{i=1}^{n}\\varphi(\\mathbf{x}_{i})\\sum_{l=1}^{K}w_{i,l}\\ \\cdot\\ v_{l}\\ \\cdot\\ y_{i,l}\\ \\geq\\ \\frac{\\delta^{\\prime}}{\\sqrt{2K}}}\\end{array}$ . So if we run the ADABOOST.MH algorithm with factorized h, $\\widehat{R}_{E X P}(\\mathbf{f}^{(T)},\\mathbf{W})$ becomes zero after at most $\\begin{array}{r}{T^{*}=\\left\\lceil\\frac{4K\\ln(2n(K-1))}{(\\delta^{\\prime})^{2}}\\right\\rceil+1}\\end{array}$ iterations. ", "page_idx": 5}, {"type": "text", "text": "3 Our Solution ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we provide formal theorems for our above answer to the open problem and further discussions. ", "page_idx": 5}, {"type": "text", "text": "Because the training set size $n$ may be very large, [15] requires the lower bound to be independent of the training set size $n$ (but can be dependent on the number of classes $K$ ), which is much more difficult than finding a lower bound depends on $n$ . To consider this problem more holistically, we provide two lower bounds, one depends on $n$ and another depends on $K$ . ", "page_idx": 5}, {"type": "text", "text": "To solve this problem, we first formulate the problem of \u201cfinding a constant $\\omega$ such that for any training set and weight matrix, there exists a code vector $\\mathbf{v}$ such that $w_{\\Sigma}^{\\prime}\\geq\\omega\\;(w_{\\Sigma}^{\\prime}$ depends on the training set, weight matrix, and the code vector)\" into \u201cfinding the lower bound of a constrained minimax problem\". We then provide a $n$ -dependent lower bound by the fact $\\|\\mathbf{x}\\|_{1}\\,\\geq\\,\\|\\mathbf{x}\\|_{\\infty}$ and the fact that the maximum is not smaller than the average, where $\\|\\cdot\\|_{p}$ is the $\\ell_{p}$ -norm of a vector. For the $n$ -independent lower bound, we choose to lower bound the expected value of $w_{\\Sigma}^{\\prime}$ when the code vector $\\mathbf{v}$ is drawn from some distribution $\\mathcal{D}$ on $\\{-1,+1\\}^{K}$ . To eliminate the trouble caused by the labels, we choose $\\mathbf{v}$ to be a Rademacher random vector with independent elements, i.e., $\\mathbf{v}=(\\varepsilon_{1},\\dots,\\varepsilon_{K})$ where $\\begin{array}{r}{\\mathbb{P}\\left[\\varepsilon_{i}=1\\right]=\\mathbb{P}\\left[\\varepsilon_{i}=-1\\right]=\\frac{1}{2}\\;}\\end{array}$ for $i=1,\\ldots,K$ . We then provide the lower bound with the help of Khintchine inequality [10]. ", "page_idx": 5}, {"type": "text", "text": "We define ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{W}:=\\left\\{\\mathbf{W}\\in\\mathbb{R}^{n\\times K}\\bigg|\\mathbf{W}_{i,l}\\geq0\\mathrm{~for~all~}i\\in[n],l\\in[K];\\sum_{i=1}^{n}\\sum_{l=1}^{K}\\mathbf{W}_{i,l}=1\\right\\}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "as the set of all possible $\\mathbf{W}$ . Let $\\mathbf{e}(\\cdot):[K]\\to\\{-1,+1\\}^{K}$ be ", "page_idx": 6}, {"type": "equation", "text": "$$\n{\\mathbf e}(l)_{i}=\\left\\{\\begin{array}{l l}{+1\\quad}&{i=l}\\\\ {-1\\quad}&{i\\neq l}\\end{array}\\right.,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "we the define $\\mathcal{V}:=\\left\\{(\\mathbf{e}(l_{1}),\\cdot\\cdot\\cdot,\\mathbf{e}(l_{n}))^{T}\\in\\{-1,+1\\}^{n\\times K}|l_{1},\\cdot\\cdot\\cdot,l_{n}\\in\\mathcal{L}=[K]\\right\\}$ as the set of all possible $\\mathbf{Y}$ , and define $\\mathcal{V}=\\{-1,+1\\}^{K}$ as the set of all possible $\\mathbf{v}$ . We then have: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{w_{\\Sigma}^{\\prime}(\\mathbf{W},\\mathbf{Y},\\mathbf{v})=\\displaystyle\\sum_{i=1}^{n}|w_{i}^{+}-w_{i}^{-}|}\\\\ &{\\phantom{=}\\displaystyle\\sum_{i=1}^{n}\\left|\\sum_{l=1}^{K}w_{i,l}\\left\\{\\mathbb{I}[v_{l}y_{i,l}=+1]-\\mathbb{I}[v_{l}y_{i,l}=-1]\\right\\}\\right|}\\\\ &{\\phantom{=\\displaystyle\\sum_{i=1}^{n}\\left|\\sum_{l=1}^{K}w_{i,l}\\cdot v_{l}\\cdot y_{i,l}\\right|}}\\\\ &{\\phantom{=\\displaystyle\\sum_{i=1}^{n}}|\\langle(\\mathbf{W}\\odot\\mathbf{Y})_{i}^{T},\\mathbf{v}\\rangle|=\\|(\\mathbf{W}\\odot\\mathbf{Y})\\cdot\\mathbf{v}\\|_{1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\odot$ is the Schur product and $\\textstyle\\|\\mathbf{x}\\|_{1}=\\sum_{i=1}^{n}\\left|x_{i}\\right|$ is the $\\ell_{1}$ -norm of the vector $\\mathbf{x}$ . ", "page_idx": 6}, {"type": "text", "text": "The following two facts translate the problem that we are concerned with into a minimax problem. Fact 3.1. The following two statements are equivalent: ", "page_idx": 6}, {"type": "text", "text": "(1) There exists a setup $(\\mathbf{X},\\mathbf{W},\\mathbf{Y})$ in which all of the $2^{K}$ different vote vectors $\\textbf{v}\\in\\mathcal{V}$ lead to arbitrarily small (or zero) $w_{\\Sigma}^{\\prime}$ . ", "page_idx": 6}, {"type": "text", "text": "(2) $\\operatorname*{min}_{\\mathbf{w}\\in\\mathcal{W},\\mathbf{Y}\\in\\mathcal{Y}\\,\\mathbf{v}\\in\\mathcal{V}}\\lVert(\\mathbf{W}\\odot\\mathbf{Y})\\cdot\\mathbf{v}\\rVert_{1}$ is arbitrarily small (or zero). ", "page_idx": 6}, {"type": "text", "text": "Fact 3.2. The following two statements are equivalent: ", "page_idx": 6}, {"type": "text", "text": "(1) We can find a constant (independent of $n$ ) lower bound $\\omega$ such that for any setup $(\\mathbf{X},\\mathbf{W},\\mathbf{Y})$ , there exists at least one vote vector and classifier $\\varphi$ such that $w_{\\Sigma}^{\\prime}\\geq\\omega$ holds. ", "page_idx": 6}, {"type": "text", "text": "(2) we can find a constant (independent of $n$ ) lower bound $\\omega$ such that $\\operatorname*{min}_{\\mathbf{w}\\in\\mathcal{W},\\mathbf{Y}\\in\\mathcal{Y}\\mathbf{v}\\in\\mathcal{V}}\\lVert(\\mathbf{W}\\odot\\mathbf{Y})\\cdot\\mathbf{\\Lambda}$ $\\mathbf{v}\\|_{1}\\geq\\omega$ . ", "page_idx": 6}, {"type": "text", "text": "So, to find the lower bound $\\omega$ , we need to prove that $\\operatorname*{min}_{\\mathbf{w}\\in{\\mathcal{W}},\\mathbf{Y}\\in{\\mathcal{Y}}\\,\\mathbf{v}\\in{\\mathcal{V}}}\\lVert(\\mathbf{W}\\odot\\mathbf{Y})\\cdot\\mathbf{v}\\rVert_{1}\\geq\\omega$ . Let\u2019s begin with a simple $n$ -dependent lower bound. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3.3 (An $n$ -dependent lower bound). $\\operatorname*{min}_{\\mathbf{w}\\in\\mathcal{W},\\mathbf{Y}\\in\\mathcal{Y}}\\operatorname*{max}_{\\mathbf{v}\\in\\mathcal{V}}\\lVert\\left(\\mathbf{W}\\odot\\mathbf{Y}\\right)\\cdot\\mathbf{v}\\rVert_{1}\\ge\\frac{1}{n}.$ ", "page_idx": 6}, {"type": "text", "text": "Proof of Theorem 3.3. ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\mathbf{w}\\in\\mathcal{W},\\mathbf{Y}\\in\\mathcal{Y}}{\\operatorname*{min}}\\underset{\\mathbf{v}\\in\\mathcal{Y}}{\\operatorname*{max}}\\|(\\mathbf{W}\\odot\\mathbf{Y})\\cdot\\mathbf{v}\\|_{1}\\triangleq\\underset{\\mathbf{w}\\in\\mathcal{W},\\mathbf{Y}\\in\\mathcal{Y}}{\\operatorname*{min}}\\underset{\\mathbf{v}\\in\\mathcal{V}}{\\operatorname*{max}}\\|(\\mathbf{W}\\odot\\mathbf{Y})\\cdot\\mathbf{v}\\|_{\\infty}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\underset{\\mathbf{w}\\in\\mathcal{W},\\mathbf{Y}\\in\\mathcal{Y}_{\\mathbf{v}}\\in\\mathcal{V},i\\in[n]}{\\operatorname*{min}}\\left|\\displaystyle\\sum_{l=1}^{K}w_{i,l}\\cdot y_{i,l}\\cdot v_{l}\\right|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\geq\\underset{\\mathbf{w}\\in\\mathcal{W}}{\\operatorname*{min}}\\underset{i\\in[n]}{\\operatorname*{max}}\\sum_{l=1}^{K}w_{i,l}\\stackrel{c}{=}\\frac{1}{n},}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $a$ is from the fact that $\\|\\mathbf{x}\\|_{1}\\geq\\|\\mathbf{x}\\|_{\\infty}$ where $\\|\\mathbf{x}\\|_{\\infty}=\\max_{1\\leq i\\leq n}|\\mathbf{x}_{i}|$ is the $\\ell_{\\infty}$ -norm of $\\mathbf{x}$ ; $b$ comes from choosing $v_{l}=y_{i,l}$ for $l=1,\\ldots,K$ when $i$ is fixed; $c$ is from the fact that $\\operatorname*{max}_{i\\in[n]}\\sum_{l=1}^{K}w_{i,l}\\geq$ $\\begin{array}{r}{\\frac{1}{n}\\sum_{i=1}^{n}\\sum_{l=1}^{K}w_{i,l}=\\frac{1}{n}}\\end{array}$ and the equation can be attained. \u53e3 ", "page_idx": 6}, {"type": "text", "text": "Remark 1. The lower bound in Theorem 3.3 depends on $n$ , and if we use $\\scriptstyle{\\frac{1}{n}}$ as the lower bound of $w_{\\Sigma}^{\\prime}$ , then we need at most $\\begin{array}{r}{T^{*}=\\left\\lceil\\frac{2n^{2}\\ln(2n(K-1))}{(\\delta^{\\prime})^{2}}\\right\\rceil+1}\\end{array}$ iterations to make the exponential error become zero, which quadratically increases as $n$ . When the training set is large, $T^{*}$ becomes very large, which is one of the reasons that $[I4J$ wants to get a lower bound independent of $n$ . ", "page_idx": 7}, {"type": "text", "text": "Next, we introduce how we solve the open problem to get a lower bound independent of $n$ . ", "page_idx": 7}, {"type": "text", "text": "Theorem 3.4 (An $n$ -independent lower bound). $\\begin{array}{r}{\\underset{\\mathbf{w}\\in\\mathcal{W},\\mathbf{Y}\\in\\mathcal{Y}_{\\mathbf{v}\\in\\mathcal{V}}}{\\operatorname*{min}}\\|(\\mathbf{W}\\odot\\mathbf{Y})\\cdot\\mathbf{v}\\|_{1}\\ge\\frac{1}{\\sqrt{2K}}.}\\end{array}$ ", "page_idx": 7}, {"type": "text", "text": "Remark 2. Theorem 3.4 shows that there is a constant $\\begin{array}{r}{\\omega=\\frac{1}{\\sqrt{2K}}}\\end{array}$ such that for any setup $\\mathbf{W},\\mathbf{Y}_{:}$ there always exists a code vector v such that $w_{\\Sigma}^{\\prime}\\geq\\omega$ . This solves the open problem proposed by [14]. So we need at most $\\begin{array}{r}{T^{*}=\\left\\lceil\\frac{4K\\ln(2n(K-1))}{(\\delta^{\\prime})^{2}}\\right\\rceil+1}\\end{array}$ iterations (see Corollary 3.6) to make the exponential error become zero. ", "page_idx": 7}, {"type": "text", "text": "To prove Theorem 3.4, we use the well-known Khintchine inequality [10] Lemma 3.5. ", "page_idx": 7}, {"type": "text", "text": "Lemma 3.5 (10, Khintchine inequality). Let $\\{\\varepsilon_{n}\\}_{n=1}^{N}$ be i.i.d. random variables with $\\mathbb{P}(\\varepsilon_{n}\\mathrm{~=~}$ $\\pm1)={\\textstyle{\\frac{1}{2}}}$ for ${n=1,\\ldots,N},$ , i.e., a sequence with Rademacher distribution. Let $0<p<\\infty$ and let $x_{1},\\ldots,{\\bar{x}}_{n}\\in\\mathbb{C}$ . Then ", "page_idx": 7}, {"type": "equation", "text": "$$\nA_{p}\\left(\\sum_{n=1}^{N}|x_{n}|^{2}\\right)^{1/2}\\leq\\left(\\underset{\\varepsilon_{1},\\ldots,\\varepsilon_{N}}{\\mathbb{E}}\\left|\\sum_{n=1}^{N}\\varepsilon_{n}x_{n}\\right|\\right)^{1/p}\\leq B_{p}\\left(\\sum_{n=1}^{N}|x_{n}|^{2}\\right)^{1/2}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "for some constants $A_{p},B_{p}>0$ depending only on $p$ , where ", "page_idx": 7}, {"type": "equation", "text": "$$\nA_{p}=\\left\\{\\begin{array}{l l}{2^{1/2-1/p}\\quad}&{0<p\\leq p_{0}}\\\\ {2^{1/2}(\\Gamma((p+1)/2)/\\sqrt{\\pi})^{1/p}\\quad}&{p_{0}<p<2}\\\\ {1\\quad}&{2\\leq p<\\infty}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "and ", "page_idx": 7}, {"type": "equation", "text": "$$\nB_{p}=\\left\\{\\begin{array}{l l}{1}&{\\quad0<p\\leq2}\\\\ {2^{1/2}(\\Gamma((p+1)/2)/\\sqrt{\\pi})^{1/p}}&{\\quad2<p<\\infty}\\end{array}\\right.,\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $p_{0}\\approx1.847$ and $\\Gamma$ is the Gamma function ", "page_idx": 7}, {"type": "text", "text": "Proof of Theorem 3.4. The basic idea of our proof is to consider the average performance of different code vectors for fixed choices of $\\mathbf{W},\\mathbf{Y}$ , i.e., use the fact that the maximum is not less than the average, which gives: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\mathbf{w}\\in\\mathcal{W},\\mathbf{Y}\\in\\mathcal{Y}_{\\mathbf{v}\\in\\mathcal{V}}}{\\operatorname*{min}}\\lVert(\\mathbf{W}\\odot\\mathbf{Y})\\cdot\\mathbf{v}\\rVert_{1}\\ge\\underset{\\mathbf{w}\\in\\mathcal{W},\\mathbf{Y}\\in\\mathcal{Y}_{\\mathbf{v}\\sim D}}{\\operatorname*{min}}\\mathbb{E}_{\\mathbf{\\theta}}\\lVert(\\mathbf{W}\\odot\\mathbf{Y})\\cdot\\mathbf{v}\\rVert_{1}}\\\\ &{\\qquad\\qquad\\qquad=\\underset{\\mathbf{w}\\in\\mathcal{W},\\mathbf{Y}\\in\\mathcal{Y}_{\\mathbf{v}\\sim D}}{\\operatorname*{min}}\\mathbb{E}_{\\mathbf{\\theta}}\\left[\\displaystyle\\sum_{i=1}^{n}\\left|\\sum_{l=1}^{K}w_{i,l}\\cdot v_{l}\\cdot y_{i,l}\\right|\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "for any distribution $D$ on $\\nu$ . ", "page_idx": 7}, {"type": "text", "text": "We take $v_{1},\\ldots,v_{K}$ be independent Rademacher random variables and then get: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{\\mathbf{W}\\in\\mathbb{W},\\mathbf{Y}\\in\\mathbb{W}_{-D}}{\\operatorname*{min}}\\mathbb{E}_{\\left[\\displaystyle\\sum_{i=1}^{K}\\left|\\sum_{l=1}^{K}w_{i,l}\\cdot v_{l}\\cdot y_{l,i}\\right|\\right]}=\\underset{\\mathbf{W}\\in\\mathbb{W},\\mathbf{Y}\\in\\mathbb{H}_{+1}}{\\operatorname*{min}}\\mathbb{E}_{\\left[\\displaystyle\\sum_{i=1}^{B}\\left|\\sum_{l=1}^{K}w_{i,l}\\cdot\\varepsilon_{l}\\cdot y_{l,i}\\right|\\right]}}&{}\\\\ {\\overset{\\mathrm{a}}{\\geq}A_{1}\\underset{\\mathbf{W}\\in\\mathbb{W}_{+1}}{\\operatorname*{min}}\\sum_{i=1}^{n}\\left(\\sum_{l=1}^{K}w_{i,l}^{2}\\right)^{1/2}}&{}\\\\ {\\frac{\\delta}{2}\\sqrt{\\frac{K}{2}}\\underset{\\mathbf{W}\\in\\mathbb{W}_{+1}}{\\operatorname*{min}}\\sum_{i=1}^{n}\\left(\\frac{1}{K}\\sum_{l=1}^{K}w_{i,l}^{2}\\right)^{1/2}}&{}\\\\ {\\frac{\\xi}{2}\\sqrt{\\frac{K}{2}}\\underset{\\mathbf{W}\\in\\mathbb{W}_{+1}}{\\operatorname*{min}}\\frac{1}{K}\\sum_{i=1}^{N}w_{i,l}}&{}\\\\ {=\\frac{1}{\\sqrt{2K}},}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $a$ applies Lemma \u221a3.5 with $p=1$ and the fact that $y_{i,l}^{2}=1$ for all $i,l;b$ puts in the value of $A_{1}$ ; $c$ uses the concavity of $\\sqrt{\\cdot}$ and Jensen\u2019s inequality. \u53e3 ", "page_idx": 8}, {"type": "text", "text": "With the lower bound of $w_{\\Sigma}^{\\prime}$ , we can now provide a lower bound of the edge $\\gamma$ and convergence guarantee for the version of ADABOOST.MH proposed by [15] conditioned on the empirically $\\delta^{\\prime}$ -weak learning condition. ", "page_idx": 8}, {"type": "text", "text": "Corollary 3.6 (Lower bound for $\\gamma$ ). If the empirically $\\delta^{\\prime}$ -weak learning condition holds, then for any $\\mathbf{X},\\mathbf{W}\\in\\mathcal{W},\\mathbf{Y}\\in\\mathcal{Y}$ , there always exists a binary classifier $\\varphi^{*}$ and code vector $\\mathbf{v}^{\\mathrm{max}}$ such that ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\gamma({\\mathbf{v}}^{\\mathrm{max}},\\varphi^{*},{\\mathbf{W}})\\geq\\frac{\\delta^{\\prime}}{\\sqrt{2K}}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "If we run ADABOOST.MH with factorized h, then we have ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\widehat{R}_{E X P}(\\mathbf{f}^{(T)},\\mathbf{W})\\leq\\exp\\left(-\\frac{\\delta^{\\prime}}{4K}T\\right)\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "and we need at most ", "page_idx": 8}, {"type": "equation", "text": "$$\nT^{*}=\\left\\lceil\\frac{4K\\ln(2n(K-1))}{(\\delta^{\\prime})^{2}}\\right\\rceil+1\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "to make the exponential error $\\widehat{R}_{E X P}(\\mathbf{f}^{(T)},\\mathbf{W})$ become zero. ", "page_idx": 8}, {"type": "text", "text": "Proof of Corollary 3.6. For any $\\mathbf{W},\\mathbf{X},\\mathbf{Y}$ , let $\\mathbf{v}^{\\operatorname*{max}}=\\underset{\\mathbf{v}\\in\\mathcal{V}}{\\arg\\operatorname*{max}}\\|(\\mathbf{W}\\odot\\mathbf{Y})\\cdot\\mathbf{v}\\|_{1}$ . Let $w_{i}^{\\prime},y_{i}^{\\prime},w_{\\Sigma}^{\\prime}$ be defined as before, where we replace $\\mathbf{v}$ there by ${\\mathbf v}^{\\mathrm{max}}$ . By Theorem 3.4, $w_{\\Sigma}^{\\prime}\\geq\\frac{1}{\\sqrt{2K}}>0$ . ", "page_idx": 8}, {"type": "text", "text": "By the empirically $\\delta^{\\prime}$ -weak learning condition, there exists a binary classifier $\\varphi^{*}$ such that ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}{\\frac{w_{i}^{\\prime}}{w_{\\Sigma}^{\\prime}}}\\cdot y_{i}^{\\prime}\\cdot\\varphi^{*}(\\mathbf{x}_{i})\\geq\\delta^{\\prime},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "which means that ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\gamma(\\mathbf{v}^{\\mathrm{max}},\\boldsymbol{\\varphi}^{*},\\mathbf{W})\\geq w_{\\Sigma}^{\\prime}\\delta^{\\prime}\\geq\\frac{\\delta^{\\prime}}{\\sqrt{2K}}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "For fixed $\\mathbf{W},\\mathbf{X},\\mathbf{Y}$ , let $\\mathbf{v}^{*}(\\varphi)$ be the code vector depending on $\\varphi$ that is defined in Equation (7). Since the choice $\\mathbf{v}^{*}(\\varphi)$ maximizes $\\gamma$ when $\\varphi$ is fixed, we have that: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\gamma(\\mathbf{v}^{*}(\\boldsymbol{\\varphi}^{*}),\\boldsymbol{\\varphi}^{*},\\mathbf{W})\\geq\\gamma(\\mathbf{v}^{\\operatorname*{max}},\\boldsymbol{\\varphi}^{*},\\mathbf{W})\\geq\\frac{\\delta^{\\prime}}{\\sqrt{2K}}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Combining the arguments in Sections 2.1 and 2.2 shows $\\begin{array}{r}{\\widehat{R}_{\\mathrm{EXP}}({\\bf f}^{(T)},{\\bf W})\\leq\\exp\\left(-\\frac{\\delta^{\\prime}}{4K}T\\right)}\\end{array}$ and that when we run ADABOOST.MH with factorized $\\mathbf{h}$ , which returns $\\varphi^{*},\\mathbf{v}^{*}\\big(\\varphi^{*}\\big)$ at each iteration, ", "page_idx": 8}, {"type": "text", "text": "after at most ", "page_idx": 8}, {"type": "equation", "text": "$$\nT^{*}=\\left\\lceil\\frac{4K\\ln(2n(K-1))}{(\\delta^{\\prime})^{2}}\\right\\rceil+1\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "iterations. ", "page_idx": 8}, {"type": "text", "text": "The previous discussions are based on fixing the training set size $n$ and the number of classes $K$ . Here we consider the case when they can tend to infinity. We think the reason [14] looks for a lower bound of $w_{\\Sigma}^{\\prime}$ that is independent of $n$ is that the author thinks the number of examples $n$ can be arbitrarily large in some cases, which may make the lower bound of $w_{\\Sigma}^{\\prime}$ arbitrarily small. ", "page_idx": 8}, {"type": "text", "text": "Combine our two lower bounds in Theorems 3.3 and 3.4, for any $\\mathbf{X},\\mathbf{W},\\mathbf{Y}$ , we can always find a $\\mathbf{v}\\in\\{-1,+1\\}^{K}$ such that: ", "page_idx": 8}, {"type": "equation", "text": "$$\nw_{\\Sigma}^{\\prime}\\geq\\operatorname*{max}\\left\\{\\frac{1}{n},\\frac{1}{\\sqrt{2K}}\\right\\},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "so the lower bound can become arbitrarily small only when $n$ and $K$ tend to infinity together. ", "page_idx": 8}, {"type": "text", "text": "4 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this section, we discuss the importance of solving this problem. ", "page_idx": 9}, {"type": "text", "text": "In statistical learning theory, algorithms can be divided into proper and improper learning algorithms. For proper learning, the most famous algorithms are ERM [22] and its variants [26, 16, 24]. For improper learning, boosting algorithms are usually used to construct improper algorithms [2, 3, 17, 23]. Furthermore, the convergence rate of the boosting algorithm usually affects the sample complexity of the constructed algorithm, i.e., the sample complexity of the constructed algorithms usually depends on the value $T^{*}$ where the training error becomes zero. So boosting algorithms are basic but important tools in statistical learning theory. ", "page_idx": 9}, {"type": "text", "text": "In binary classification, ADABOOST [9] is one of the most famous and influential algorithms among all the binary boosting algorithms. Since the proposal of ADABOOST, many works have tried to extend the boosting framework to multi-class classification problems. Most multi-class boosting algorithms have been restricted to reducing the multi-class classification problem to multiple two-class problems, among which the most famous and influential one is ADABOOST.MH [20]. Moreover, ADABOOST.MH has inspired the proposal of many other multi-class boosting algorithms. For example, inspired by the characteristics of ADABOOST.MH that reduces the multi-class classification problem to multiple two-class problems, [13] chooses another line of thought to develop an algorithm that directly extends the ADABOOST.MH algorithm to the multi-class case without reducing it to multiple two-class problems; [1] demonstrates how to improve the efficiency and effectiveness of ADABOOST.MH and proposes the algorithm LDA-ADABOOST.MH; [18] proposes an efficient multi-class fault diagnosis approach based on the ADABOOST.MH algorithm; [7] proposes a method for ranking based on ADABOOST.MH. There are also many other works based on ADABOOST.MH [21, 12, 8]. Furthermore, many works (for example, [13, 8, 25, 27]) use ADABOOST.MH as the baseline, which further shows the importance of ADABOOST.MH. For example, the only baseline used in [13] is ADABOOST.MH. In summary, ADABOOST.MH serves as a link between binary classification boosting algorithms and multi-class classication boosting algorithms, the cornerstone of multi-class boosting, and has a big influence on the multi-class boosting field. Our work is important because it shows that K\u00e9gl\u2019s work [15], which solves the computational problem (at the level of the strong learner at least) of ADABOOST.MH, does indeed work in theory and works essentially as fast as binary ADABOOST. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we resolve the open problem raised by [14] by presenting a $n$ -independent lower bound for $w_{\\Sigma}^{\\prime}$ . In addition to that, we also provide a $n$ -dependent lower bound for $w_{\\Sigma}^{\\prime}$ to show that $w_{\\Sigma}^{\\prime}$ may be arbitrarily small only when $n$ and $K$ tend to infinity together. Based on the lower bounds for $w_{\\Sigma}^{\\prime}$ and the empirically $\\delta^{\\prime}$ -weak learning condition, we provide an upper bound for the weighted exponential error and a number $T^{*}$ where the weighted exponential error becomes zero after at most $T^{*}$ iterations. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work is supported by the Key R&D Program of Hubei Province under Grant 2024BAB038, National Key R&D Program of China under Grant 2023YFC3604702, and the Fundamental Research Fund Program of LIESMARS. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Bassam Al-Salemi, Mohd Juzaiddin Ab Aziz, and Shahrul Azman Noah. Lda-adaboost.mh: Accelerated adaboost.mh based on latent dirichlet allocation for text categorization. J. Inf. Sci., 41(1):27\u201340, 2015.   \n[2] Noga Alon, Steve Hanneke, Ron Holzman, and Shay Moran. A theory of PAC learnability of partial concept classes. In FOCS, pages 658\u2013671, 2021.   \n[3] Nataly Brukhim, Daniel Carmon, Irit Dinur, Shay Moran, and Amir Yehudayoff. A characterization of multiclass learnability. In FOCS, pages 943\u2013955, 2022. [4] Nataly Brukhim, Amit Daniely, Yishay Mansour, and Shay Moran. Multiclass boosting: Simple and intuitive weak learning criteria. In NeurIPS, 2023.   \n[5] Nataly Brukhim, Steve Hanneke, and Shay Moran. Improper multiclass boosting. In COLT, pages 5433\u20135452, 2023.   \n[6] Nataly Brukhim, Elad Hazan, Shay Moran, Indraneel Mukherjee, and Robert E. Schapire. Multiclass boosting and the cost of weak learning. In NeurIPS, pages 3057\u20133067, 2021.   \n[7] R\u00f3bert Busa-Fekete, Bal\u00e1zs K\u00e9gl, Tam\u00e1s \u00c9ltet\u00f6, and Gy\u00f6rgy Szarvas. Ranking by calibrated adaboost. In Olivier Chapelle, Yi Chang, and Tie-Yan Liu, editors, Yahoo! Learning to Rank Challenge, volume 14, pages 37\u201348, 2011.   \n[8] Andrea Esuli, Tiziano Fagni, and Fabrizio Sebastiani. Mp-boost: A multiple-pivot boosting algorithm and its application to text categorization. In Fabio Crestani, Paolo Ferragina, and Mark Sanderson, editors, SPIRE, volume 4209, pages 1\u201312, 2006. [9] Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. J. Comput. Syst. Sci., 55(1):119\u2013139, 1997.   \n[10] Uffe Haagerup. The best constants in the khintchine inequality. Studia Mathematica, 70(3):231\u2013 283, 1981.   \n[11] Steve Hanneke. The optimal sample complexity of PAC learning. J. Mach. Learn. Res., 17:38:1\u201338:15, 2016.   \n[12] Wei Hao and Jiebo Luo. Generalized multiclass adaboost and its applications to multimedia classification. In CVPR Workshops, page 113, 2006.   \n[13] Trevor Hastie, Saharon Rosset, Ji Zhu, and Hui Zou. Multi-class adaboost. Statistics and its Interface, 2(3):349\u2013360, 2009.   \n[14] Bal\u00e1zs K\u00e9gl. Open problem: A (missing) boosting-type convergence result for adaboost.mh with factorized multi-class classifiers. In Maria-Florina Balcan, Vitaly Feldman, and Csaba Szepesv\u00e1ri, editors, COLT, volume 35, pages 1268\u20131275, 2014.   \n[15] Bal\u00e1zs K\u00e9gl. The return of adaboost.mh: multi-class hamming trees. In ICLR, 2014.   \n[16] Jaeho Lee and Maxim Raginsky. Minimax statistical learning with wasserstein distances. In NeurIPS, pages 2692\u20132701, 2018.   \n[17] Omar Montasser, Steve Hanneke, and Nathan Srebro. VC classes are adversarially robustly learnable, but only improperly. In Alina Beygelzimer and Daniel Hsu, editors, COLT, pages 2512\u20132530, 2019.   \n[18] Peng Peng, Yi Zhang, Yinan Wu, and Heming Zhang. An effective fault diagnosis approach based on gentle adaboost and adaboost.mh. In 2018 IEEE International Conference on Automation, Electronics and Electrical Engineering (AUTEEE), pages 8\u201312, 2018.   \n[19] Robert E. Schapire and Yoav Freund. Boosting: Foundations and Algorithms. The MIT Press, 05 2012.   \n[20] Robert E. Schapire and Yoram Singer. Improved boosting algorithms using confidence-rated predictions. Mach. Learn., 37(3):297\u2013336, 1999.   \n[21] Fabrizio Sebastiani, Alessandro Sperduti, and Nicola Valdambrini. An improved boosting algorithm and its application to text categorization. In CIKM, pages 78\u201385, 2000.   \n[22] Vladimir Vapnik. Principles of risk minimization for learning theory. In NeurIPS, pages 831\u2013838, 1991.   \n[23] Jingyuan Xu and Weiwei Liu. On robust multiclass learnability. In NeurIPS, 2022.   \n[24] Dong Yin, Kannan Ramchandran, and Peter L. Bartlett. Rademacher complexity for adversarially robust generalization. In ICML, pages 7085\u20137094, 2019.   \n[25] Arman Zharmagambetov, Magzhan Gabidolla, and Miguel \u00c1. Carreira-Perpi\u00f1\u00e1n. Improved multiclass adaboost for image classification: The role of tree optimization. In ICIP, pages 424\u2013428, 2021.   \n[26] Zhengyu Zhou and Wei Liu. Sample complexity for distributionally robust learning under chi-square divergence. J. Mach. Learn. Res., 24:230:1\u2013230:27, 2023.   \n[27] Ji Zhu, Saharon Rosset, and Trevor Hastie. A new multiclass generalization of adaboost. Ann Arbor, 1001:48109. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A The pseudocode of the factorized ADABOOST.MH ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In this section, we adapt the pseudocode of the factorized ADABOOST.MH from [14]. $\\mathbf{X}$ is the $n\\times d$ observation matrix, $\\mathbf{Y}$ is the $n\\times d$ label matrix, W is the user-defined weight matrix used in the definition of the weighted Hamming error (1). Let $\\mathrm{{BASE}}(\\cdot,\\cdot,\\cdot)$ be the base learner algorithm, and $T$ be the number of iterations. Let $\\alpha^{(t)}$ be the base coefficient $\\mathbf{v}^{(t)}$ be the vote vector, $\\varphi^{(t)}(\\cdot)$ be the scalar base (weak) classifier, $\\mathbf{h}^{(t)}(\\cdot)$ be the vector-based classifier, and $\\mathbf{f}^{(t)}(\\cdot)$ be the final (strong) discriminant function. ", "page_idx": 12}, {"type": "image", "img_path": "7Lv8zHQWwS/tmp/1a1367ac99c8f3791166af3c0e22c9d5dc485f56f2755ae0dcc9d51a2a89f4dc.jpg", "img_caption": [], "img_footnote": [], "page_idx": 12}, {"type": "text", "text": "B Related Works ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In addition to ADABOOST.M1, ADABOOST.M2, ADABOOST.MH, and factorized ADABOOST.MH, there are also some works on multi-class boosting. ", "page_idx": 12}, {"type": "text", "text": "To circumvent the hardness result for a large class of natural boosting, [5] utilizes the technique of list learning and proposes an efficient improper multi-class boosting algorithm with sample and oracle complexity bounds that are entirely independent of the number of classes. ", "page_idx": 12}, {"type": "text", "text": "[6] studies the resources required for boosting, especially how they depend on the number of classes $K$ . [6] presents results on the sample complexity, oracle complexity, and finds a trade-off between number of oracle calls and the resources required of the weak learner. ", "page_idx": 12}, {"type": "text", "text": "[4] proposes an efficient multi-class boosting algorithm with the help of list learning, the success of the proposed algorithm is guaranteed by the relaxed $\\gamma$ -BRG condition. ", "page_idx": 12}, {"type": "text", "text": "In this paper, we solve the open problem proposed in [14] and provide a bound for the oracle complexity of the factorized ADABOOST.MH algorithm. The algorithm that we consider is different from those in [5, 6, 4], and the conditions are also different. We find a missing convergence result for factorized ADABOOST.MH, so we think our work is a complementary of the related works [5, 6, 4]. ", "page_idx": 12}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 13}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 13}, {"type": "text", "text": "Justification: We highly summarize what we do in the abstract. The introduction clearly introduces the concepts and issues that are related to our main results. ", "page_idx": 13}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 13}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 13}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 13}, {"type": "text", "text": "Guidelines: ", "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 13}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 13}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 13}, {"type": "text", "text": "Justification: We provide the assumptions of the theorems and all our theorems are followed by their proofs. ", "page_idx": 14}, {"type": "text", "text": "Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 14}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 14}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 14}, {"type": "text", "text": "Justification: Our paper is purely theoretical, with no experiments. ", "page_idx": 14}, {"type": "text", "text": "Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 14}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 14}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 15}, {"type": "text", "text": "Justification: Our paper is purely theoretical, with no experiments. Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 15}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 15}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 15}, {"type": "text", "text": "Justification: Our paper is purely theoretical, with no experiments. Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 15}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 15}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 15}, {"type": "text", "text": "Justification: Our paper is purely theoretical, with no experiments. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 16}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 16}, {"type": "text", "text": "Justification: Our paper is purely theoretical, with no experiments. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 16}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: This paper is about a theoretical result for a multi-class boosting algorithm, it clearly conforms with the NeurIPS Code of Ethics. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 16}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 16}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 16}, {"type": "text", "text": "Justification: Our paper is purely theoretical, and has no societal impact. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 17}, {"type": "text", "text": "Justification: Our paper is purely theoretical, with no experiments, so it clearly poses no such risks. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 17}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 17}, {"type": "text", "text": "Justification: Our paper is purely theoretical, with no experiments, which needs no assets. Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 17}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 18}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: Our paper is purely theoretical, with no experiments, and we do not release new assets. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 18}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: Our paper is purely theoretical, with no experiments, so it does not involve crowdsourcing nor research with human subjects. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 18}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: Our paper is purely theoretical, with no experiments, so it does not involve crowdsourcing nor research with human subjects. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 18}]