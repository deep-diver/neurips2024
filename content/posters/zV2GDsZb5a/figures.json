[{"figure_path": "zV2GDsZb5a/figures/figures_1_1.jpg", "caption": "Figure 1: Single-image relighting results on real data. Neural Gaffer supports single-image relighting for various input images under diverse lighting conditions, using either image-conditioned input (i.e., an environment map) or text-conditioned input (i.e., a description of the target lighting). These results demonstrate our model's capability to adapt to diverse lighting scenarios while preserving the visual fidelity of the original objects. Our relighting results remain consistent with the lighting rotating. Please see the supplementary webpage for additional video results produced from real input images.", "description": "This figure showcases the Neural Gaffer model's ability to relight single images of various objects under different lighting conditions.  It demonstrates the model's ability to handle both image-based (environment map) and text-based (description of lighting) conditions, consistently producing high-quality relit images that maintain visual fidelity and react to lighting rotation.  The supplementary materials contain additional video results.", "section": "1 Introduction"}, {"figure_path": "zV2GDsZb5a/figures/figures_3_1.jpg", "caption": "Figure 2: Model architecture. Neural Gaffer is an img2img latent diffusion model conditioned on the input image and rotated lighting maps.", "description": "This figure illustrates the architecture of the Neural Gaffer model, a 2D image-based relighting diffusion model.  It shows how the input image and lighting information are processed.  The input image is encoded using a CLIP encoder to extract visual features.  The target environment map is processed through two separate encoders, generating LDR (low dynamic range) and normalized HDR (high dynamic range) map representations.  These encodings, along with the CLIP embedding of the input image, are concatenated and fed into the denoiser, a U-Net architecture which also processes noisy latent vectors.  The denoiser then generates refined latent vectors that are decoded to produce the final relit image. The process is conditioned by the lighting information extracted from the environment map.", "section": "3.2 Lighting-conditioned 2D Relighting Diffusion Model"}, {"figure_path": "zV2GDsZb5a/figures/figures_4_1.jpg", "caption": "Figure 3: Relighting a 3D neural radiance field. Given an input NeRF and a target environmental lighting, in Stage 1, we use Neural Gaffer to predict relit images at each predefined camera viewpoint. We then tune the appearance field to overfit the multi-view relighting predictions with a reconstruction loss. In Stage 2, we further refine the appearance of the coarsely relit radiance field via the diffusion guidance loss. Using this pipeline, we can relight a NeRF model in minutes.", "description": "This figure illustrates a two-stage pipeline for relighting a 3D neural radiance field (NeRF).  Stage 1 uses Neural Gaffer to generate relit images from multiple viewpoints, which are then used to refine the NeRF's appearance via a reconstruction loss. Stage 2 further refines the appearance using a diffusion guidance loss, resulting in a high-quality, relit NeRF. The entire process is computationally efficient, taking only minutes to complete.", "section": "3.3 Relighting a 3D radiance field with diffusion prior guidance"}, {"figure_path": "zV2GDsZb5a/figures/figures_6_1.jpg", "caption": "Figure 4: Single-image relighting comparison with DiLightNet [79] under diverse lighting. Our method demonstrates superior fidelity to the target lighting, maintains more consistent color and detail, and can generate more accurate highlights, shadows, and high-frequent reflections.", "description": "This figure compares the single-image relighting results of the proposed Neural Gaffer model against DiLightNet [79] under various lighting conditions.  For several different objects, it shows the input image, the relighting results from DiLightNet, the relighting results from Neural Gaffer, the ground truth image, and the target lighting environment map. The comparison highlights Neural Gaffer's superior performance in accurately reproducing highlights, shadows, and reflections while maintaining color consistency and detail.", "section": "4.3 Results analysis"}, {"figure_path": "zV2GDsZb5a/figures/figures_7_1.jpg", "caption": "Figure 5: Object insertion. Our diffusion model can be applied to object insertion. Compared with Any-Door [14], our method better preserves the identity of the inserted object and achieves higher-quality results.", "description": "This figure demonstrates the object insertion capability of the Neural Gaffer model.  It shows examples where a foreground object (a vase and a BB-8 droid) is seamlessly integrated into a background scene with different lighting conditions.  The results are compared to a simple copy-paste approach and the results from AnyDoor, highlighting Neural Gaffer's superior ability to preserve object identity and generate more realistic lighting and shadows.", "section": "4.3 Results analysis"}, {"figure_path": "zV2GDsZb5a/figures/figures_7_2.jpg", "caption": "Figure 6: Relighting 3D objects. We take multi-view images and a target lighting condition as input and compare our method's relighting results to NVDIFFREC-MC [25] and TensoIR [30]. (The shown images are channel-aligned results.) Lacking strong priors, the baselines tend to miss specular highlights (NVDIFFREC-MC), bake the original highlights (TensoIR), or produce artifacts (NVDIFFREC-MC and TensoIR). In contrast, our method implicitly learns to model the complex interplay of light and materials from the training process of our relighting diffusion model, resulting in more accurate relighting results with fewer artifacts. Video comparisons are available on our project webpage.", "description": "This figure compares the 3D relighting results of the proposed method against two baselines, namely NVDIFFREC-MC and TensoIR, across three different objects. The results show that the proposed method more accurately reproduces the lighting effects, unlike the baselines, which tend to miss or overemphasize certain aspects of lighting such as specular highlights and shadows, sometimes leading to artifacts.  Videos showing more details are available on the project website.", "section": "3.3 Relighting a 3D radiance field with diffusion prior guidance"}, {"figure_path": "zV2GDsZb5a/figures/figures_8_1.jpg", "caption": "Figure 13: More results of relighting 3D objects. We present additional 3D relighting comparison results, which include all of our testing objects. The results show that our methods achieve better qualitative relighting outcomes for all the tested objects.", "description": "This figure shows additional results of relighting 3D objects using the proposed Neural Gaffer model. It presents a comparison of the results obtained with Neural Gaffer against two baselines (NVDIFFREC-MC and TensoIR) and ground truth relighting.  The figure demonstrates the superior qualitative performance of Neural Gaffer in accurately capturing specular highlights and shadows, which were more challenging for the baseline methods.", "section": "4.3 Results analysis"}, {"figure_path": "zV2GDsZb5a/figures/figures_8_2.jpg", "caption": "Figure 8: Ablations on methods for relighting 3D radiance fields. Our full model achieves the most accurate and visually realistic relighting results among all variants. Please zoom in to see detailed differences across ablation variants.", "description": "This figure shows the ablation study results for relighting 3D radiance fields using the proposed two-stage pipeline. It compares the results of the full model with several ablated versions: using only the SDS loss (Score Distillation Sampling) for refinement, omitting stage 1 or stage 2, and using only the first or the second stage.  The results visually demonstrate the importance of both stages in achieving accurate and realistic highlights and reflections.", "section": "Ablation Study"}, {"figure_path": "zV2GDsZb5a/figures/figures_9_1.jpg", "caption": "Figure 1: Single-image relighting results on real data. Neural Gaffer supports single-image relighting for various input images under diverse lighting conditions, using either image-conditioned input (i.e., an environment map) or text-conditioned input (i.e., a description of the target lighting). These results demonstrate our model's capability to adapt to diverse lighting scenarios while preserving the visual fidelity of the original objects. Our relighting results remain consistent with the lighting rotating. Please see the supplementary webpage for additional video results produced from real input images.", "description": "This figure shows several examples of single-image relighting results obtained using the Neural Gaffer model.  The input images depict various objects (a helmet, a rubber duck, etc.) under various lighting conditions. The model then relights these images using image-conditioned (environment maps) or text-conditioned inputs (descriptions of lighting conditions). The results demonstrate that the model can successfully relight the objects while preserving their visual fidelity, even with significant changes in the lighting conditions. This highlights the model's ability to handle diverse lighting scenarios and its robustness.", "section": "1 Introduction"}, {"figure_path": "zV2GDsZb5a/figures/figures_15_1.jpg", "caption": "Figure 10: Comparison of single-image relighting with IC-Light [85]. In this experiment, we compare relighting results and evaluate the consistency of specular highlights against IC-Light [85]. Each column represents a uniformly rotated version of the same environment map image, progressing from left to right. Our method consistently adjusts highlights, faithfully reproducing the object under novel lighting conditions. In contrast, IC-Light [85] exhibits inconsistent highlight movement, with certain regions changing just slightly. Further comparisons are available in videos provided on the supplemental webpage.", "description": "This figure compares the single-image relighting results of the proposed method against IC-Light [85], focusing on the consistency of specular highlights under different lighting conditions.  The results show that the proposed method consistently adjusts highlights according to lighting rotation, while IC-Light shows inconsistent highlight movement.", "section": "A Comparison with IC-Light"}, {"figure_path": "zV2GDsZb5a/figures/figures_15_2.jpg", "caption": "Figure 1: Single-image relighting results on real data. Neural Gaffer supports single-image relighting for various input images under diverse lighting conditions, using either image-conditioned input (i.e., an environment map) or text-conditioned input (i.e., a description of the target lighting). These results demonstrate our model's capability to adapt to diverse lighting scenarios while preserving the visual fidelity of the original objects. Our relighting results remain consistent with the lighting rotating. Please see the supplementary webpage for additional video results produced from real input images.", "description": "This figure shows the results of single-image relighting on real data using the Neural Gaffer model.  The model successfully changes the lighting conditions of various input images, using either an environment map or a text description as input.  The results show high-quality relighting across diverse lighting scenarios, while maintaining the visual fidelity of the original objects.  The lighting changes are consistent, even when the light source is rotated.  Additional video results are available on the supplementary webpage.", "section": "1 Introduction"}, {"figure_path": "zV2GDsZb5a/figures/figures_16_1.jpg", "caption": "Figure 12: Color ambiguity analysis. We found that our data-driven model can handle the inherent color ambiguity to some extent, especially when the objects' materials in the input image are relatively specular. Because the diffusion model can infer the lighting of the input image through the reflections or specular highlights", "description": "This figure demonstrates the model's ability to handle inherent color ambiguity in single-image relighting, particularly when dealing with specular materials.  The model infers lighting information from reflections and highlights, allowing it to generate accurate relit images even when the input image has ambiguous coloring. Two examples are shown, each comparing the input image, the model's prediction, and the ground truth.", "section": "4.3 Results analysis"}, {"figure_path": "zV2GDsZb5a/figures/figures_17_1.jpg", "caption": "Figure 13: More results of relighting 3D objects. We present additional 3D relighting comparison results, which include all of our testing objects. The results show that our methods achieve better qualitative relighting outcomes for all the tested objects.", "description": "This figure shows additional qualitative results comparing the relighting performance of the proposed Neural Gaffer method against two baseline methods, TensoIR and NVDIFFREC-MC, across various 3D objects.  The results demonstrate Neural Gaffer's superior ability to produce high-quality, visually realistic relighting results compared to existing techniques.", "section": "4.3 Results analysis"}]