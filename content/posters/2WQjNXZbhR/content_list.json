[{"type": "text", "text": "Dendritic Integration Inspired Artificial Neural Networks Capture Data Correlation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Chongming Liua,b, Jingyang $\\mathrm{Ma^{a,b}}$ , Songting Lia,b,c,1, and Douglas Zhoua,b,c,d,1 ", "page_idx": 0}, {"type": "text", "text": "aSchool of Mathematical Sciences, Shanghai Jiao Tong University, Shanghai, China bInstitute of Natural Sciences, Shanghai Jiao Tong University, Shanghai, China   \ncMinistry of Education Key Laboratory of Scientific and Engineering Computing, Shanghai Jiao Tong University, Shanghai, China   \ndShanghai Frontier Science Center of Modern Analysis, Shanghai Jiao Tong University, Shanghai, China ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Incorporating biological neuronal properties into Artificial Neural Networks (ANNs) to enhance computational capabilities is under active investigation in the field of deep learning. Inspired by recent findings indicating that dendrites adhere to quadratic integration rule for synaptic inputs, this study explores the computational benefits of quadratic neurons. We theoretically demonstrate that quadratic neurons inherently capture correlation within structured data, a feature that grants them superior generalization abilities over traditional neurons. This is substantiated by few-shot learning experiments. Furthermore, we integrate the quadratic rule into Convolutional Neural Networks (CNNs) using a biologically plausible approach, resulting in innovative architectures\u2014Dendritic integration inspired CNNs (Dit-CNNs). Our Dit-CNNs compete favorably with state-of-the-art models across multiple classification benchmarks, e.g., ImageNet-1K, while retaining the simplicity and efficiency of traditional CNNs. All source code are available at https://github.com/liuchongming1999/ Dendritic-integration-inspired-CNN-NeurIPS-2024. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "While the Artificial Neural Network (ANN) framework has made significant advancements towards solving complex tasks, it still faces problems that are rudimentary to real brains [6]. A notable distinction between the modern ANN framework and the human brain is that the former relies on a significant number of training samples, which consumes large amounts of energy, whereas the latter runs on extremely low power $\\cdot20$ watts) and possesses a strong generalization capability based on few-shot learning. Studies have demonstrated that incorporating dendritic features in ANNs can alleviate these issues and enhance overall performance [52, 38, 30]. However, it is difficult to quantify the nonlinear integration of dendrites, which is an essential property that allows individual neurons to perform complex computations [43, 49]. As a result, dendritic-inspired models often employ linear integration with nonlinear activation functions such as ReLU and Sigmoid [21, 37]. ", "page_idx": 0}, {"type": "text", "text": "In light of recent studies revealing that the somatic responses of biological neurons to multiple synaptic inputs on dendrites follow a quadratic integration rule [15, 28], we explore the computational benefti of the quadratic neuron model [41]. This model substitutes the linear integration and nonlinear activation function of traditional point neurons with quadratic integration as follows1: ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "equation", "text": "$$\nf(x)=\\sigma(w\\cdot x+b)\\to f(x)=x^{T}A x+w\\cdot x+b.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "The expected value of a quadratic term $E_{x\\sim\\mathcal{D}}[a_{i j}x_{i}x_{j}]$ closely relates to the covariance between variables $x_{i}$ and $x_{j}$ $x_{i}$ and $x_{j}$ are the $i$ -th and the $j$ -th components of vector $x$ , respectively), suggesting an inherent capability for capturing input correlation. This attribute allows biological neurons to exhibit remarkable performance in correlation detection tasks [1, 29]. Here, we theoretically demonstrate that quadratic neurons indeed naturally capture correlation within structured data, which is pivotal in numerous machine learning applications [7], such as language generation [5] and video understanding [23]. This intrinsic property endows quadratic neurons with superior generalization capabilities compared to traditional neurons, as evidenced by few-shot learning experiments. We further propose a biologically plausible method to integrate them into Convolutional Neural Networks (CNNs). Applying this approach to ResNet [16] and ConvNeXt [34] results in novel CNN models\u2014Dendritic integration inspired ResNet (Dit-ResNet) and Dendritic integration inspired ConvNeXt (Dit-ConvNeXt), respectively. Evaluations on CIFAR-10 and CIFAR-100 datasets demonstrate that Dit-ResNets significantly enhance test accuracy by merely replacing a single layer of point neurons with quadratic neurons. On the ImageNet-1K dataset [10], Dit-ConvNeXts demonstrate significant enhancements in top-1 accuracy with only a one percent increase in parameters, exhibiting competitive performance compared to state-of-the-art models. Ablation studies further substantiate the effectiveness of quadratic neurons and their capability for capturing data correlation. ", "page_idx": 1}, {"type": "text", "text": "This paper is structured as follows: Section 2 reviews previous works related to dendritic-inspired models and high-order interactive operations in neural networks. Section 3 presents a theoretical analysis of the computational benefits derived from quadratic neurons, as demonstrated through few-shot learning experiments. Section 4 describes the architecture of Dit-CNNs and evaluates their performance on several computer vision benchmark datasets. Section 5 concludes the paper. All numerical experiments in this paper are conducted using Python and executed on 8 Tesla A100 computing cards with a 7nm GA100 GPU, featuring 6,912 CUDA cores and 432 tensor cores. ", "page_idx": 1}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Dendritic-inspired computational models. The significant role of dendrites in the nonlinear integration within neural systems is well-recognized, enabling neurons to perform intricate tasks [43, 49, 2]. Recently, numerous studies have explored the implementation of dendritic features in ANNs from different aspects, yielding encouraging results. In [52, 21, 37], the integration of dendritic morphology into ANNs has led to higher test accuracies on simple tasks compared to traditional ANNs. Furthermore, dendritic plasticity rules have been employed to develop learning algorithms in [38, 36], effectively replacing the non-biological backpropagation algorithm and achieving enhanced performance on classification tasks with small data sizes. Additionally, in [30], dendritic nonlinearity is considered by adding input to each layer with the Hadamard product rule, which has shown improvements in approximation and classification experiments. ", "page_idx": 1}, {"type": "text", "text": "Neural networks with high-order interactions. Recent advancements in the design of deep learning architectures have been predominantly driven by the ability to capture high-order statistics among inputs and features. Over recent decades, there has been a pivotal shift in the foundational neural networks used for computer vision tasks: moving from convolution-based architectures [16, 24, 56] to Transformer-based models [11, 33, 9]. This transition has been driven by the development of the self-attention mechanism, which facilitates quadratic interactions among inputs. Additionally, a novel category of neural networks, referred to as Mamba [14], has demonstrated exceptional performance in tasks that require the processing of extensive high-order statistical information, such as video understanding [32, 59, 27]. The backbone of Mamba\u2019s architecture is a state space model that enables high-order interactions among inputs. Moreover, the integration of high-order spatial interactions into CNNs has also been shown to enhance performance significantly [40]. These developments underscore the significance of high-order statistical information in tackling large-scale complex tasks and suggest that incorporating high-order interactions as a foundational aspect of model design is an effective strategy. ", "page_idx": 1}, {"type": "table", "img_path": "2WQjNXZbhR/tmp/a143fca5b0949438bed4fb5d96f5b7663eadc36a6a426c709d55aeb1fe4291b3.jpg", "table_caption": ["Table 1: The overview of current works with quadratic format. "], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "Previous works have explored the concept of quadratic integration as a sophisticated alternative to traditional linear summation. Table 1 summarizes various neuron models formulated in quadratic formats, along with their relevant references. The one-rank format of quadratic neurons, when applied across entire networks, is documented in [54, 12, 4, 53], demonstrating enhanced performance in classification tasks. Studies such as [48, 20, 35, 60] have implemented quadratic neurons on a pixel-wise basis within convolution layers, yielding modest improvements in test accuracy. In contrast, Table 1 illustrates how our Dit-CNNs approach departs from these models by integrating a channel-wise quadratic operations with biological interpretation and providing a theoretical analysis of the computational advantages offered by quadratic neurons. ", "page_idx": 2}, {"type": "text", "text": "3 Quadratic neurons possess enhanced generalization capabilities ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "This section provides a theoretical analysis demonstrating how quadratic neurons enhance generalization capabilities by effectively capturing correlation within structured data. Additional numerical experiments corroborate this assertion. The detailed proofs can be found in Appendix A. ", "page_idx": 2}, {"type": "text", "text": "3.1 Binary classification for normal distributions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Assume that the data points of two classes are equally sampled from two different non-degenerate normal distributions: $c l a s s_{1}\\sim N(\\mu_{1},\\Sigma_{1})$ , $c l a s s_{2}\\sim N(\\mu_{2},\\Sigma_{2})$ , where $\\mu_{j}\\in\\mathbb{R}^{d}$ , $\\Sigma_{j}\\in\\mathbb{R}^{\\breve{d}\\times{d}}$ (j = $1,2)$ . Then the optimal classifier $y_{o p t}(x):\\mathbb{R}^{d}\\to\\{1,2\\}$ can be defined according to the sampling probability: ", "page_idx": 2}, {"type": "equation", "text": "$$\ny_{o p t}(x)=\\arg\\operatorname*{max}_{j\\in\\{1,2\\}}p_{j}(x),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $p_{j}(x)$ is the probability (density function) of sampling point $x$ from distribution $N(\\mu_{j},\\Sigma_{j})$ . ", "page_idx": 2}, {"type": "text", "text": "If a single quadratic neuron, as described in Equation (1) $(f(x)=x^{T}A x+w\\cdot x+b$ , where $A$ is a symmetric matrix), is used to solve the above binary classification task, we can prove the following theorems: ", "page_idx": 2}, {"type": "text", "text": "Theorem 1. (Existence) The critical points with respect to the cross-entropy loss $L(A,w,b)$ are given as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\nA^{*}=\\Sigma_{1}^{-1}-\\Sigma_{2}^{-1},\\,\\,w^{*}=-2(\\Sigma_{1}^{-1}\\mu_{1}-\\Sigma_{2}^{-1}\\mu_{2}),\\,\\,b^{*}=\\mu_{1}^{T}\\Sigma_{1}^{-1}\\mu_{1}-\\mu_{2}^{T}\\Sigma_{2}^{-1}\\mu_{2}+\\log\\left(\\frac{|\\Sigma_{1}|}{|\\Sigma_{2}|}\\right).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "i.e. ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\frac{\\partial L}{\\partial A}\\mid_{A^{*},w^{*},b^{*}}=0,\\;\\;\\;\\frac{\\partial L}{\\partial w}\\mid_{A^{*},w^{*},b^{*}}=0,\\;\\;\\;\\frac{\\partial L}{\\partial b}\\mid_{A^{*},w^{*},b^{*}}=0,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where ", "page_idx": 2}, {"type": "equation", "text": "$$\nL(A,w,b)=\\frac{1}{2}\\left[E_{x\\sim c l a s s_{1}}\\left(\\log(1+e^{f(x)})\\right)+E_{x\\sim c l a s s_{2}}\\left(\\log(1+e^{-f(x)})\\right)\\right].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Moreover, the corresponding classifier generated by this formula is the same as the theoretically optimal classifier in Equation (2). ", "page_idx": 2}, {"type": "text", "text": "Theorem 2. (Uniqueness) Consider the conditional cross-entropy loss defined on a set $\\Omega\\in\\mathcal{M}(\\mathbb{R}^{d})$ , where $\\mathcal{M}(\\mathbb{R}^{d})$ denotes the Lebesgue $\\sigma$ -algebra on $\\mathbb{R}^{d}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\nL(A,w,b\\mid\\Omega)=\\frac{1}{2}\\left[E_{x\\sim c l a s s_{1},x\\in\\Omega}\\left(\\log(1+e^{f(x)})\\right)+E_{x\\sim c l a s s_{2},x\\in\\Omega}\\left(\\log(1+e^{-f(x)})\\right)\\right],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where ", "page_idx": 3}, {"type": "equation", "text": "$$\nE_{x\\sim c l a s s_{j},x\\in\\Omega}\\left(g(x)\\right)=\\int_{\\Omega}g(x)p_{j}(x)\\,d\\mu,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "then the unique solution satisfying ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\frac{\\partial L(A,w,b\\mid\\Omega)}{\\partial A}\\mid_{A^{*},w^{*},b^{*}}=0,\\;\\frac{\\partial L(A,w,b\\mid\\Omega)}{\\partial w}\\mid_{A^{*},w^{*},b^{*}}=0,\\;\\frac{\\partial L(A,w,b\\mid\\Omega)}{\\partial b}\\mid_{A^{*},w^{*},b^{*}}=0\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "for every $\\Omega\\in\\mathcal{M}(\\mathbb{R}^{d})$ is given by: ", "page_idx": 3}, {"type": "equation", "text": "$$\nA^{*}=\\Sigma_{1}^{-1}-\\Sigma_{2}^{-1},\\,\\,w^{*}=-2(\\Sigma_{1}^{-1}\\mu_{1}-\\Sigma_{2}^{-1}\\mu_{2}),\\,\\,b^{*}=\\mu_{1}^{T}\\Sigma_{1}^{-1}\\mu_{1}-\\mu_{2}^{T}\\Sigma_{2}^{-1}\\mu_{2}+\\log\\left(\\frac{|\\Sigma_{1}|}{|\\Sigma_{2}|}\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "image", "img_path": "2WQjNXZbhR/tmp/fd18b58c7a310b31c3403af91d2c0f36a7d2f8a3c79799ef999ade09836501b0.jpg", "img_caption": ["Figure 1: Visualization of classifier boundaries for two models (a single quadratic neuron with 2-dimensional inputs and a two-layer MLP comprising 2-ReLU(10)-1) across two distinct tasks. For each task, the impact of varying training sample sizes is examined. Model comparisons are performed under uniform conditions, utilizing identical random seeds and hyperparameters for fairness. Training is executed using gradient descent with a learning rate of 0.1 over 10,000 epochs. The term \"theoretical\" denotes the optimal classifier\u2019s boundary as specified in Equation (2), while \"numerical\" represents the empirical classification boundary obtained from the model. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Theorem 1 identifies an analytical critical point for a quadratic neuron with cross-entropy loss, which emerges as the optimal classifier. On the other hand, Theorem 2 establishes the uniqueness of this critical point under conditional cross-entropy loss, where $\\Omega$ denotes the regions occupied by distinct batches of data points. Under these conditions, the quadratic parameters will converge to this unique critical point when using the stochastic gradient descent (SGD) algorithm. It is demonstrated that, unlike traditional neuron, quadratic neuron inherently possesses the ability to converge to the optimal classification solution by directly including the covariance matrix. This finding is supported by simulated data in Task 1 (identical covariance) and Task 2 (non-identical covariance) shown in Figure 1, which indicates that quadratic neuron consistently achieves the theoretically optimal classification outcome. Moreover, in comparison to a multi-layer perceptron (MLP) with two layers, quadratic neuron surpass traditional neuron in Task 2 which require capturing correlation information from data distributions (require fewer training samples to converge to the optimal solution). This highlights the unique capability of quadratic neurons to identify internal correlations within structured data, offering a possible explanation for their superior generalization capability compared to traditional neurons, a topic that will be further explored in numerical experiments below. ", "page_idx": 3}, {"type": "text", "text": "From the above classification tasks for normal distributions as depicted in Figure 1, it is suggested that quadratic neurons may require fewer training samples in comparison to traditional neurons. We next examine this few-shot learning capability of quadratic neurons on the MNIST [25] and Arabic MNIST datasets [19]. The results, presented in Figure 2, indicate that quadratic neurons outperform MLP with two layers when trained with a limited number of samples on both datasets. This evidence shows the enhanced generalization ability of quadratic neurons in applications. Further insights into the mechanism through which quadratic neurons capture data correlation in the MNIST dataset, and a related theorem concerning multi-class classification for normal distributions, are detailed in Appendix B. ", "page_idx": 4}, {"type": "image", "img_path": "2WQjNXZbhR/tmp/430785b1b1a7261189219c6ac86624e707a105119496b33bef67912a60cc7c74.jpg", "img_caption": ["Figure 2: Performance of two models on few-shot learning tasks with MNIST and Arabic MNIST datasets. The first model consists of 10 quadratic neurons with a 784-dimensional input, while the second model is a MLP with the configuration of 784-ReLU(8000)-10. Both models are evaluated under identical conditions using the same training protocol, which includes SGD with a learning rate of 0.1 and a batch size of 100 across 20 epochs. The term \u2019Sample size\u2019 refers to the ratio of the number of training samples to the full training set. Experiments are conducted for each model and sample ratio across ten runs, and the resulting test accuracy is depicted through an error bar plot (error bar represents the upper and lower bound for test accuracy). "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "4 Integrating quadratic neurons into CNNs with biological plausibility ", "text_level": 1, "page_idx": 4}, {"type": "image", "img_path": "2WQjNXZbhR/tmp/4a4bb15cc0b93e07f76820ae4613d12865f91a51fbf6c6bf1512570f29f10065.jpg", "img_caption": ["Figure 3: Schematic of the biological interpretation of Dit-CNNs. Dit-CNNs is inspired by neural networks in the visual system. For example, different types of cone cells encode various color (channel) information, and retinal ganglion cells receive inputs from multiple types of cone cells [22], the responses can be modeled as having receptive fields (convolutional kernels) related to different color channels $(w_{1}*x_{1},w_{2}*x_{2},w_{3}*x_{3})$ . When multiple channel inputs are present, traditional CNNs simply linearly sum the corresponding responses. In contrast, dendritic neurons integrate these inputs with an additional quadratic term based on the dendritic quadratic integration rule. This approach leads to the formulation of Dit-CNNs after simplification. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "The concept of convolution [13], inspired by the discovery of the receptive field in the cat visual cortex [18], laid the foundation for CNNs [26] which have been designed to efficiently tackle large-scale vision tasks. The underlying biological interpretation of convolution is that each neuron exhibits varying responses at different image locations based on its receptive field (convolution kernel). Traditionally, CNNs have focused solely on the linear integration of inputs from presynaptic neurons. Specifically, for an input $X\\ \\in\\ \\mathbb{R}^{C_{i n}^{\\bullet}\\times H_{i n}\\times W_{i n}}$ , where $C_{i n}$ represents the number of input layer neurons (channels), and $H_{i n}$ and $W_{i n}$ denote the height and width of the input feature, respectively. Given convolution kernels $\\boldsymbol{w}\\;\\in\\;\\mathbb{R}^{C_{i n}\\times C_{o u t}\\times\\left(2l+1\\right)\\times\\left(2l+1\\right)}$ , the convolution output, $Y\\stackrel{\\cdot}{=}C o n v(X)\\in\\mathbb{R}^{C_{o u t}\\times H_{o u t}\\times W_{o u t}}$ , represents the response of neuron $i$ at location $[j,k]$ to inputs from locations $[j-l:j+l,k-l:k+l]$ across all input layer neurons: ", "page_idx": 5}, {"type": "equation", "text": "$$\nY[i,j,k]=\\sum_{m=1}^{C_{i n}}w[m,i,:,:]*X[m,j-l:j+l,k-l:k+l].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "However, acknowledging that neurons with dendrites obey a quadratic integration rule, we propose a novel approach that incorporates quadratic neurons into CNNs as depicted in Figure 3. This adaptation incorporates a quadratic integration term among inputs from different neurons (channels). Specifically, with a quadratic integration coefficient $A\\in\\mathbb{R}^{\\check{C}_{o u t}\\,\\star\\,C_{i n}\\,\\times\\,C_{i n}}$ , the output neuron\u2019s response in Equation (3) is modified as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\nY[i,j,k]=\\sum_{m=1}^{C_{i n}}w[m,i,:,:]*X[m,j-l:j+l,k-l:k+l]+\\underbrace{X^{T}[:,j,k]A[i,:,:]X[:,j,k]}_{\\cap_{u,u\\in i\\=l}:=i u:a_{u}\\neq i_{u}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "4.1 Evaluations on CIFAR-10 and CIFAR-100 ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Dataset and models. Our initial experiments utilize the CIFAR dataset, which includes 50,000 training images and 10,000 testing images. The experimental setup follows the ResNet configurations as outlined in [16], comprising models such as ResNet-20, ResNet-32, ResNet-56 and ResNet-110. To reduce computational demands, we incorporate quadratic neurons specifically into one layer\u2014namely, the second layer of the first block in the second stage of the ResNet architecture [16]. Additionally, to address issues related to gradient dynamics, such as gradient explosion, a Layer Normalization (LN) layer [3] is implemented immediately before the layer equipped with quadratic neurons. ", "page_idx": 5}, {"type": "table", "img_path": "2WQjNXZbhR/tmp/5817d0b52b6f72c3a6ffff0799e1a4405c1c2e24df21286fc43305c30c51cf38.jpg", "table_caption": ["Table 2: Comparative performance of Dit-ResNets and structurally similar models on CIFAR. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "Experimental settings. The training process incorporates SGD with a weight decay of 0.0001, a batch size of 128, and a momentum of 0.9. Following the recommendations from [12], quadratic integration parameters are initialized to zero, and a distinct learning rate is adopted for these parameters. Initially, the learning rate for the quadratic integration matrix is set at 1, and 0.1 for all other parameters. These rates is reduced by a factor of ten at 80 and 120 epochs, with training concluding at 160 epochs. Data augmentation procedures mirror the original protocol: each image is padded by 4 pixels on each side, follows by random cropping of a $32\\times32$ section from the padded image or its horizontal flip. For testing, a single view of the original $32\\times32$ image is evaluated. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Results. Table 2 presents a comparative analysis of CIFAR performance between our Dit-ResNets and the original ResNet model [16], as well as two adaptations that incorporate quadratic neurons [12, 54], which have shown superior outcomes among existing approaches with quadratic formats [48, 20, 35, 60, 4]. For the CIFAR-10 dataset, given the extensive benchmarking in prior studies, we conduct ten runs of our Dit-ResNets, reporting the optimal outcome for comparative analysis. For CIFAR-100, we independently execute all models, documenting average test accuracy and variance (mean\u00b1std). Our experiments indicate significant performance enhancements on both CIFAR-10 and CIFAR-100 datasets after integrating quadratic neurons into a single layer of ResNet, confirming their efficacy. Furthermore, when comparing a deeper ResNet model with our Dit-ResNet (e.g., ResNet-110 vs. Dit-ResNet-56), our approach not only boosts performance but also reduces training overhead. This suggests that augmenting the model with quadratic integration parameters is more effective than merely increasing network depth\u2014a traditionally favored strategy. In comparison to other models employing quadratic neurons [12, 54], our Dit-ResNets consistently achieve the highest test accuracy with the fewest parameters. This underscores the efficacy of our approach in leveraging the computational advantages of quadratic neurons. These neurons, as discussed in Section 3, demonstrate enhanced generalization capabilities. ", "page_idx": 6}, {"type": "image", "img_path": "2WQjNXZbhR/tmp/748b9bdf332f21d4c71c7e79128235f4791b6a6659b84a9b9f49427a6918ace9.jpg", "img_caption": ["Figure 4: Visualization of some performance results presented in Table 2 (left) and Table 3 (right). "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "4.2 Evaluations on ImageNet-1K ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Dataset and settings. We extend our investigations to the ImageNet-1K dataset [10], which comprises 1.28 million training images and 50,000 validation images across 1,000 categories. Our analysis primarily focuses on top-1 accuracy on the validation set. Training is conducted at a resolution of $224\\times224$ , supplemented by a comprehensive suite of data augmentation and regularization strategies. These include RandAugment [8], mixup [57], cutmix [55], label smoothing [44], layer scale [47], random erasing [58], exponential moving average (EMA) [39], and stochastic depth [17]. These techniques are inspired by the timm library [50] and methodologies from Touvron et al. [45]. Detailed descriptions of these hyperparameters are provided in Appendix C. ", "page_idx": 6}, {"type": "text", "text": "Model description. Mirroring our approach with the CIFAR dataset, we integrate quadratic neurons into a single layer of the ConvNeXt model [34] to conserve computational resources. This adaptation results in the creation of three distinct models: Dit-ConvNeXt-T, Dit-ConvNeXt-S, and Dit-ConvNeXt-B. Specific details about the layer replacement are discussed in the ablation study. ", "page_idx": 6}, {"type": "text", "text": "Results. Table 3 presents a comparative analysis of Dit-ConvNeXts against state-of-the-art models from the existing literature. Compared to the original ConvNeXt counterparts, our Dit-ConvNeXts exhibit an average increase of $0.5\\%$ in top-1 accuracy, with only a marginal average increase of $1\\%$ rise in parameter count. Moreover, our models maintain competitive performance against Transformerbased, SSM-based, and CNN-based architectures, highlighting the efficacy and adaptability of Dit-ConvNeXts. Figure 4 further shows that our model consistently improves in accuracy as the model size increases, while other methods tend to saturate, indicating superior scaling property for our model. ", "page_idx": 6}, {"type": "table", "img_path": "2WQjNXZbhR/tmp/dc855d3be615eef3c0f40236f4d618637ddccc8c75987904ef3474accf963036.jpg", "table_caption": ["Table 3: Dit-ConvNeXts versus state-of-the-art (SOTA) models on ImageNet-1K. All models listed in the table are trained and validated at a resolution of $224\\times224$ . "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "4.3 Ablation study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "4.3.1 Dit-CNNs capture data correlation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The expectation of the quadratic term for Gaussian variables can be expressed as follows: ", "page_idx": 7}, {"type": "equation", "text": "$$\nE_{x\\sim N(\\mu,\\Sigma)}\\left[x^{T}A x\\right]=\\mu^{T}A\\mu+\\operatorname{tr}(A\\Sigma).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "This equation highlights that the term $\\operatorname{tr}(A{\\Sigma})$ encapsulates the information derived from data correlation. Consequently, Table 4 examines the impact of this term on the performance of Dit-CNNs in tackling complex tasks. The data presented in Table 4 show a notable decline in accuracy for Dit-CNNs when this term is omitted. This underscores the significance of the quadratic neurons within Dit-CNNs, demonstrating their pivotal role in effectively capturing data correlation, which is essential for task performance. ", "page_idx": 7}, {"type": "table", "img_path": "2WQjNXZbhR/tmp/147c8e584ef78b77662630b8d82395723797988db877bf7a6f7d54b8da48bbf7.jpg", "table_caption": ["Table 4: The performance of Dit-CNNs and their counterparts, from which the covariance term $\\operatorname{tr}(A{\\Sigma})$ and the quadratic term $x^{T}A x$ are omitted in quadratic neurons $\\Sigma$ is estimated from training samples). "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.3.2 Incorporate quadratic neurons with minimal computational overhead ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To manage computational costs effectively, only three layers within the ConvNeXt architecture have been identified as suitable candidates for the integration of quadratic neurons, as depicted in Figure 5. We explore the most effective layer for the integration of quadratic neurons and elucidate the rationale behind this choice. Figure 5 indicates that substituting traditional neurons with quadratic neurons in the first layer of the block 3 yields the most significant performance improvement. Additionally, a negative correlation is observed between the model\u2019s final accuracy and the accuracy after omitting the quadratic term from Dit-ConvNeXt-T post-training. This correlation underscores the critical influence of quadratic neurons on the model\u2019s efficacy. These findings suggest that Dit-CNNs demonstrating enhanced performance are those where quadratic neurons play a crucial role, highlighting the superior generalization capabilities of quadratic neurons compared to traditional neurons. ", "page_idx": 8}, {"type": "image", "img_path": "2WQjNXZbhR/tmp/b2469a8a0442046e52ae52a4faba2e793fbd95627b0247e90591cf891bbed621.jpg", "img_caption": ["Figure 5: Left: Structure of ConvNeXt highlighting three candidate layers (in red) for integrating quadratic neurons. Right: ImageNet-1k performance on different Dit-ConvNeXt-T, blue dots indicates top-1 accuracy (left blue vertical axis) while green dots indicates the accuracy post-removal of the quadratic term from Dit-ConvNeXt-T after training (right green vertical axis). "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.3.3 Channel/Pixel-wise quadratic neuron utilizations on ImageNet-1K ", "text_level": 1, "page_idx": 8}, {"type": "table", "img_path": "2WQjNXZbhR/tmp/b984ce5843c07be43b71dd9e36ad3a190af367e4445497ca296d124f8901452b.jpg", "table_caption": ["Table 5: Comparison of quadratic neurons between channel-wise and pixel-wise "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "As previously discussed, our Dit-CNNs employ quadratic neurons in a channel-wise manner, which provides a clear biological interpretation, as detailed in Section 4. Meanwhile, employing quadratic neurons on a pixel-wise basis, akin to the quadratic convolution outlined in [20], [35], and [60], leads to a scenario where dendritic integration occurs only when neurons receive synaptic inputs from the same neuron. This setup simplifies interactions between different neurons to a linear approximation, a method that diverges from biological plausibility within the brain. The superior performance achieved through the channel-wise application of quadratic neurons, as evidenced in Table 5, underscores the efficacy of our model. This result supports the hypothesis that models which more closely mirror brain-like mechanisms tend to exhibit enhanced performance. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we first provide a theoretical demonstration of the computational advantages of quadratic neurons in capturing internal correlation within structured data. These neurons model dendritic integration rules observed in biological neurons. Our empirical evaluations using the MNIST and Arabic MNIST datasets for few-shot learning validate our theoretical assertions. Drawing from the biological interpretation of CNNs, we introduce a biologically plausible method to integrate quadratic neurons into CNN architectures, resulting in Dit-CNNs. These Dit-CNNs not only exhibit significant performance enhancements with minimal modifications to their original counterparts but also compete favorably with state-of-the-art models. The potential applicability of our approach to other architectures, such as Deep-MAD [42], hints at further performance improvements. The promising results of this research underscore the vast potential of brain-inspired models. Given that the quadratic integration rule of neurons could be confined to specific brain areas [15, 28], future electrophysiological experiments could reveal other neuronal integration rules. Our findings could guide the development of brain-inspired deep neural networks by incorporating various integration rules corresponding to different brain areas in distinct layers. Additionally, how to theoretically analyze these new brain-inspired models will be an important issue. It is our aim to extend our analysis concerning high-order statistical information to explore the generalization error of these innovative brain-inspired models. We hope that our work will stimulate further investigations into brain-inspired models, ultimately contributing to the quest for artificial general intelligence (AGI). ", "page_idx": 9}, {"type": "text", "text": "Limitations and Discussions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Theoretical results for quadratic neurons. Our theoretical framework is established on the assumption that training samples are normally distributed, a simplification that might not fully encapsulate the complexity inherent in real-world datasets. Despite this assumption, our models have demonstrated exceptional performance on various tasks including ImageNet-1K. This underscores the potential of quadratic neurons in capturing correlation within more intricate data distributions. Future efforts will focus on developing a more comprehensive theoretical foundation to better understand the mechanisms through which quadratic neurons achieve this capability. ", "page_idx": 9}, {"type": "text", "text": "Computational cost of quadratic neurons. While our Dendritic integration inspired Convolutional Neural Networks (Dit-CNNs) strategically limit the increase in the number of learnable parameters by selectively incorporating quadratic neurons in a singular layer, this modification undeniably raises the computational complexity, as evidenced by an increase in Floating Point Operations (FLOPs). Nevertheless, given the substantial enhancements our Dit-CNNs contribute to model performance, it warrants further investigation into optimizing the efficiency of quadratic neuron deployment. For instance, drawing inspiration from the inherent sparsity observed in the quadratic coefficients of biological neurons\u2014specifically, the pronounced quadratic interactions among synaptic inputs on the same dendritic branch [29]\u2014it is conceivable to predefine a sparse configuration for the quadratic coefficients within our Dit-CNNs. This approach could potentially reduce computational demands while maintaining performance gains. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We thank Ruoyu Sun for his helpful discussions and Zhi-Qin John Xu for his comments. This work was supported by Science and Technology Innovation 2030 - Brain Science and Brain-Inspired Intelligence Project with Grant No. 2021ZD0200204; National Natural Science Foundation of China Grant 12271361, 12250710674 (S.L.); National Natural Science Foundation of China with Grant No. 12225109, 12071287 (D.Z.) and the Student Innovation Center at Shanghai Jiao Tong University (S.L., D.Z.). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Hagai Agmon-Snir, Catherine E Carr, and John Rinzel. The role of dendrites in auditory coincidence detection. Nature, 393(6682):268\u2013272, 1998.   \n[2] Gal Ariav, Alon Polsky, and Jackie Schiller. Submillisecond precision of the input-output transformation function mediated by fast sodium dendritic spikes in basal dendrites of ca1 pyramidal neurons. Journal of Neuroscience, 23(21):7750\u20137758, 2003.   \n[3] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.   \n[4] Jie Bu and Anuj Karpatne. Quadratic residual networks: A new class of neural networks for solving forward and inverse problems in physics involving pdes. In Proceedings of the 2021 SIAM International Conference on Data Mining (SDM), pages 675\u2013683. SIAM, 2021.   \n[5] Pawe\u0142 Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, Inigo Casanueva, Stefan Ultes, Osman Ramadan, and Milica Ga\u0161ic\u00b4. Multiwoz\u2013a large-scale multi-domain wizard-of-oz dataset for task-oriented dialogue modelling. arXiv preprint arXiv:1810.00278, 2018.   \n[6] Spyridon Chavlis and Panayiota Poirazi. Drawing inspiration from biological dendrites to empower artificial neural networks. Current Opinion in Neurobiology, 70:1\u201310, 2021.   \n[7] Xin Chen, Bin Yan, Jiawen Zhu, Dong Wang, Xiaoyun Yang, and Huchuan Lu. Transformer tracking. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8126\u20138135, 2021.   \n[8] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data augmentation with a reduced search space. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops, pages 702\u2013703, 2020.   \n[9] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Scaling vision transformers to 22 billion parameters. In International Conference on Machine Learning, pages 7480\u20137512. PMLR, 2023.   \n[10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A largescale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009.   \n[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.   \n[12] Fenglei Fan, Wenxiang Cong, and Ge Wang. A new type of neurons for machine learning. International journal for numerical methods in biomedical engineering, 34(2):e2920, 2018.   \n[13] Kunihiko Fukushima. Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position. Biological cybernetics, 36(4):193\u2013202, 1980.   \n[14] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023.   \n[15] Jiang Hao, Xu-dong Wang, Yang Dan, Mu-ming Poo, and Xiao-hui Zhang. An arithmetic rule for spatial summation of excitatory and inhibitory inputs in pyramidal neurons. Proceedings of the National Academy of Sciences, 106(51):21906\u201321911, 2009.   \n[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.   \n[17] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with stochastic depth. In Computer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11\u201314, 2016, Proceedings, Part IV 14, pages 646\u2013661. Springer, 2016.   \n[18] David H Hubel and Torsten N Wiesel. Receptive fields, binocular interaction and functional architecture in the cat\u2019s visual cortex. The Journal of physiology, 160(1):106, 1962.   \n[19] Weiwei Jiang. Mnist-mix: a multi-language handwritten digit recognition dataset. IOP SciNotes, 1(2):025002, 2020.   \n[20] Yiyang Jiang, Fan Yang, Hengliang Zhu, Dian Zhou, and Xuan Zeng. Nonlinear cnn: improving cnns with quadratic convolutions. Neural Computing and Applications, 32:8507\u20138516, 2020.   \n[21] Ilenna Simone Jones and Konrad Paul Kording. Might a single neuron solve interesting machine learning problems through successive computations on its dendritic tree? Neural Computation, 33(6):1554\u20131571, 2021.   \n[22] Eric R Kandel, James H Schwartz, Thomas M Jessell, Steven Siegelbaum, A James Hudspeth, Sarah Mack, et al. Principles of neural science, volume 4. McGraw-hill New York, 2000.   \n[23] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. arXiv preprint arXiv:1705.06950, 2017.   \n[24] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. Communications of the ACM, 60(6):84\u201390, 2017.   \n[25] Yann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998.   \n[26] Yann LeCun, Lawrence D Jackel, L\u00e9on Bottou, Corinna Cortes, John S Denker, Harris Drucker, Isabelle Guyon, Urs A Muller, Eduard Sackinger, Patrice Simard, et al. Learning algorithms for classification: A comparison on handwritten digit recognition. Neural networks: the statistical mechanics perspective, 261(276):2, 1995.   \n[27] Kunchang Li, Xinhao Li, Yi Wang, Yinan He, Yali Wang, Limin Wang, and Yu Qiao. Videomamba: State space model for efficient video understanding. arXiv preprint arXiv:2403.06977, 2024.   \n[28] Songting Li, Nan Liu, Xiao-hui Zhang, Douglas Zhou, and David Cai. Bilinearity in spatiotemporal integration of synaptic inputs. PLoS computational biology, 10(12):e1004014, 2014.   \n[29] Songting Li, Nan Liu, Xiaohui Zhang, David W McLaughlin, Douglas Zhou, and David Cai. Dendritic computations captured by an effective point neuron model. Proceedings of the National Academy of Sciences, 116(30):15244\u201315252, 2019.   \n[30] Gang Liu and Jing Wang. Dendrite net: A white-box module for classification, regression, and system identification. IEEE Transactions on Cybernetics, 52(12):13774\u201313787, 2021.   \n[31] Shiwei Liu, Tianlong Chen, Xiaohan Chen, Xuxi Chen, Qiao Xiao, Boqian Wu, Tommi K\u00e4rkk\u00e4inen, Mykola Pechenizkiy, Decebal Mocanu, and Zhangyang Wang. More convnets in the 2020s: Scaling up kernels beyond 51x51 using sparsity. arXiv preprint arXiv:2207.03620, 2022.   \n[32] Yue Liu, Yunjie Tian, Yuzhong Zhao, Hongtian Yu, Lingxi Xie, Yaowei Wang, Qixiang Ye, and Yunfan Liu. Vmamba: Visual state space model. arXiv preprint arXiv:2401.10166, 2024.   \n[33] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 10012\u201310022, 2021.   \n[34] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11976\u201311986, 2022.   \n[35] Pranav Mantini and Shishr K Shah. Cqnn: Convolutional quadratic neural networks. In 2020 25th International Conference on Pattern Recognition (ICPR), pages 9819\u20139826. IEEE, 2021.   \n[36] Toviah Moldwin, Menachem Kalmenson, and Idan Segev. The gradient clusteron: A model neuron that learns to solve classification tasks via dendritic nonlinearities, structural plasticity, and gradient descent. PLoS computational biology, 17(5):e1009015, 2021.   \n[37] Varun Ojha and Giuseppe Nicosia. Backpropagation neural tree. Neural Networks, 149:66\u201383, 2022.   \n[38] Alexandre Payeur, Jordan Guerguiev, Friedemann Zenke, Blake A Richards, and Richard Naud. Burst-dependent synaptic plasticity can coordinate learning in hierarchical circuits. Nature neuroscience, 24(7):1010\u20131019, 2021.   \n[39] Boris T Polyak and Anatoli B Juditsky. Acceleration of stochastic approximation by averaging. SIAM journal on control and optimization, 30(4):838\u2013855, 1992.   \n[40] Yongming Rao, Wenliang Zhao, Yansong Tang, Jie Zhou, Ser Nam Lim, and Jiwen Lu. Hornet: Efficient high-order spatial interactions with recursive gated convolutions. Advances in Neural Information Processing Systems, 35:10353\u201310366, 2022.   \n[41] Sanjeevakumar Redlapalli, Madan M Gupta, and K-Y Song. Development of quadratic neural unit with applications to pattern classification. In Fourth International Symposium on Uncertainty Modeling and Analysis, 2003. ISUMA 2003., pages 141\u2013146. IEEE, 2003.   \n[42] Xuan Shen, Yaohua Wang, Ming Lin, Yilun Huang, Hao Tang, Xiuyu Sun, and Yanzhi Wang. Deepmad: Mathematical architecture design for deep convolutional neural network. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6163\u20136173, 2023.   \n[43] Nelson Spruston. Pyramidal neurons: dendritic structure and synaptic integration. Nature Reviews Neuroscience, 9(3):206\u2013221, 2008.   \n[44] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2818\u20132826, 2016.   \n[45] Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herv\u00e9 J\u00e9gou. Fixing the train-test resolution discrepancy. Advances in neural information processing systems, 32, 2019.   \n[46] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through attention. In International conference on machine learning, pages 10347\u201310357. PMLR, 2021.   \n[47] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Herv\u00e9 J\u00e9gou. Going deeper with image transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 32\u201342, 2021.   \n[48] Nikolaos Tsapanos, Anastasios Tefas, Nikolaos Nikolaidis, and Ioannis Pitas. Neurons with paraboloid decision boundaries for improved neural network classification performance. IEEE transactions on neural networks and learning systems, 30(1):284\u2013294, 2018.   \n[49] Bal\u00e1zs B Ujfalussy, Judit K Makara, M\u00e1t\u00e9 Lengyel, and Tiago Branco. Global and multiplexed dendritic computations under in vivo-like conditions. Neuron, 100(3):579\u2013592, 2018.   \n[50] Ross Wightman. Pytorch image models. https://github.com/rwightman/ pytorch-image-models, 2019.   \n[51] Ross Wightman, Hugo Touvron, and Herv\u00e9 J\u00e9gou. Resnet strikes back: An improved training procedure in timm. arXiv preprint arXiv:2110.00476, 2021.   \n[52] Xundong Wu, Xiangwen Liu, Wei Li, and Qing Wu. Improved expressivity through dendritic neural networks. Advances in neural information processing systems, 31, 2018.   \n[53] Chenhui Xu, Fuxun Yu, Zirui Xu, Chenchen Liu, Jinjun Xiong, and Xiang Chen. Quadranet: Improving high-order neural interaction efficiency with hardware-aware quadratic neural networks. In 2024 29th Asia and South Pacific Design Automation Conference (ASP-DAC), pages 19\u201325. IEEE, 2024.   \n[54] Zirui Xu, Fuxun Yu, Jinjun Xiong, and Xiang Chen. Quadralib: A performant quadratic neural network library for architecture optimization and design exploration. Proceedings of Machine Learning and Systems, 4:503\u2013514, 2022.   \n[55] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In Proceedings of the IEEE/CVF international conference on computer vision, pages 6023\u20136032, 2019.   \n[56] Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part I 13, pages 818\u2013833. Springer, 2014.   \n[57] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412, 2017.   \n[58] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data augmentation. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 13001\u201313008, 2020.   \n[59] Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, and Xinggang Wang. Vision mamba: Efficient visual representation learning with bidirectional state space model. arXiv preprint arXiv:2401.09417, 2024.   \n[60] Georgios Zoumpourlis, Alexandros Doumanoglou, Nicholas Vretos, and Petros Daras. Nonlinear convolution filters for cnn-based learning. In Proceedings of the IEEE international conference on computer vision, pages 4761\u20134769, 2017. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Proofs of theorems for binary classification task ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Assume that the data points of two classes are equally sampled from two distinct non-degenerate normal distributions: $\\begin{array}{r}{c\\bar{l}a s s_{1}\\sim N(\\mu_{1},\\Sigma_{1})}\\end{array}$ , $c l a s s_{2}\\sim N(\\mu_{2},\\Sigma_{2})$ , where $\\mu_{j}\\in\\mathbb{R}^{d}$ , $\\Sigma_{j}\\in\\mathbb{R}^{\\breve{d}\\times d}$ ( ${\\boldsymbol{j}}=$ $1,2)$ . The optimal classifier $y_{o p t}(x)\\,:\\,\\mathbb{R}^{d}\\,\\to\\,\\{1,2\\}$ is then defined based on the probability of sampling any point $x$ from these distributions. Specifically, the classifier can be expressed as: ", "page_idx": 14}, {"type": "equation", "text": "$$\ny_{o p t}(x)=\\arg\\operatorname*{max}_{j\\in\\{1,2\\}}p_{j}(x),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $p_{j}(x)$ denotes the probability (density function) of sampling point $x$ from the distribution $N(\\mu_{j},\\dot{\\Sigma_{j}})$ . The decision rule effectively assigns $x$ to the class with the highest probability density, reflecting the most likely class membership based on the normal distribution parameters. ", "page_idx": 14}, {"type": "text", "text": "If a single quadratic neuron, as described in Equation (1) $(f(x)=x^{T}A x+w\\cdot x+b$ , where $A$ is a symmetric matrix), is employed to solve the binary classification task described above, we can prove the following theorems. ", "page_idx": 14}, {"type": "text", "text": "A.1 Existence of the optimal solution ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Theorem 3. (Existence) The critical points with respect to the cross-entropy loss $L(A,w,b)$ are given as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\nA^{*}=\\Sigma_{1}^{-1}-\\Sigma_{2}^{-1},\\,\\,w^{*}=-2(\\Sigma_{1}^{-1}\\mu_{1}-\\Sigma_{2}^{-1}\\mu_{2}),\\,\\,b^{*}=\\mu_{1}^{T}\\Sigma_{1}^{-1}\\mu_{1}-\\mu_{2}^{T}\\Sigma_{2}^{-1}\\mu_{2}+\\log\\left(\\frac{|\\Sigma_{1}|}{|\\Sigma_{2}|}\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "i.e. ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{\\partial L}{\\partial A}\\mid_{A^{*},w^{*},b^{*}}=0,\\;\\;\\;\\frac{\\partial L}{\\partial w}\\mid_{A^{*},w^{*},b^{*}}=0,\\;\\;\\;\\frac{\\partial L}{\\partial b}\\mid_{A^{*},w^{*},b^{*}}=0,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where ", "page_idx": 14}, {"type": "equation", "text": "$$\nL(A,w,b)=\\frac{1}{2}\\left[E_{x\\sim c l a s s_{1}}\\left(\\log(1+e^{f(x)})\\right)+E_{x\\sim c l a s s_{2}}\\left(\\log(1+e^{-f(x)})\\right)\\right].\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Moreover, the corresponding classifier generated by this formula is the same as the theoretically optimal classifier in Equation (5). ", "page_idx": 14}, {"type": "text", "text": "Proof. We express the generalized cross-entropy loss $L(A,w,b)$ as: ", "page_idx": 14}, {"type": "equation", "text": "$$\nL(A,w,b)=\\frac{1}{2}\\int_{\\mathbb{R}^{d}}\\left(\\log(1+e^{f(x)})p_{1}(x)+\\log(1+e^{-f(x)})p_{2}(x)\\right)\\,d\\mu.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The gradients are computed explicitly as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\n{\\frac{\\partial L}{\\partial A}}={\\frac{1}{2}}\\int_{\\mathbb{R}^{d}}\\left({\\frac{e^{f(x)}}{1+e^{f(x)}}}p_{1}(x)-{\\frac{1}{1+e^{f(x)}}}p_{2}(x)\\right){\\frac{\\partial f}{\\partial A}}\\,d\\mu,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and similarly for $\\frac{\\partial L}{\\partial w}$ and $\\frac{\\partial L}{\\partial b}$ . Given the probability density function of the multivariate normal distribution: ", "page_idx": 14}, {"type": "equation", "text": "$$\np_{j}(x)=(2\\pi)^{-d/2}|\\Sigma_{j}|^{-1/2}\\exp\\left(-\\frac{1}{2}(x-\\mu_{j})^{T}\\Sigma_{j}^{-1}(x-\\mu_{j})\\right)\\quad j\\in\\{1,2\\}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "If the parameters $A,w,b$ are set as: ", "page_idx": 14}, {"type": "equation", "text": "$$\nA=\\Sigma_{1}^{-1}-\\Sigma_{2}^{-1},\\,w=-2(\\Sigma_{1}^{-1}\\mu_{1}-\\Sigma_{2}^{-1}\\mu_{2}),\\,b=\\mu_{1}^{T}\\Sigma_{1}^{-1}\\mu_{1}-\\mu_{2}^{T}\\Sigma_{2}^{-1}\\mu_{2}+\\log\\left(\\frac{|\\Sigma_{1}|}{|\\Sigma_{2}|}\\right),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "one can obtain ", "page_idx": 14}, {"type": "equation", "text": "$$\nf(x)=x^{T}(\\Sigma_{1}^{-1}-\\Sigma_{2}^{-1})x-2(\\mu_{1}^{T}\\Sigma_{1}^{-1}-\\mu_{2}^{T}\\Sigma_{2}^{-1})x+\\mu_{1}^{T}\\Sigma_{1}^{-1}\\mu_{1}-\\mu_{2}^{T}\\Sigma_{2}^{-1}\\mu_{2}+\\log\\left(\\frac{|\\Sigma_{1}|}{|\\Sigma_{2}|}\\right)x,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which ensures that ", "page_idx": 14}, {"type": "equation", "text": "$$\ne^{f(x)}={\\frac{p_{2}(x)}{p_{1}(x)}},\\,\\forall x\\in\\mathbb{R}^{d}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Consequently, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{e^{f(x)}}{1+e^{f(x)}}p_{1}(x)-\\frac{1}{1+e^{f(x)}}p_{2}(x)=0,\\,\\forall x\\in\\mathbb{R}^{d}\\Rightarrow\\frac{\\partial L}{\\partial A}=0,\\frac{\\partial L}{\\partial w}=0,\\frac{\\partial L}{\\partial b}=0.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The classifier generated by the quadratic neuron is defined as: ", "page_idx": 15}, {"type": "equation", "text": "$$\ny_{m o d e l}(x)={\\left\\{\\begin{array}{l l}{1}&{{\\mathrm{if~}}f(x)<0,}\\\\ {2}&{{\\mathrm{if~}}f(x)>0.}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore, ", "page_idx": 15}, {"type": "equation", "text": "$$\ny_{m o d e l}(x)=1\\Leftrightarrow y_{o p t}(x)=\\arg\\operatorname*{max}_{j\\in\\{1,2\\}}p_{j}(x)=1.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Similarly, $y_{m o d e l}(x)\\;=\\;2$ implies $y_{o p t}(x)\\;=\\;2$ . Hence, $y_{m o d e l}(x)\\;=\\;y_{o p t}(x)$ for all $x$ in $\\mathbb{R}^{d}$ , demonstrating the consistency between these two classifiers. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "A.2 Uniqueness of the optimal solution ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "To prove the theorem for uniqueness, we first introduce the following preliminary lemmas: ", "page_idx": 15}, {"type": "text", "text": "Lemma 1. Let $\\mathcal{M}(\\mathbb{R}^{d})$ denote the Lebesgue $\\sigma$ -algebra on $\\mathbb{R}^{d}$ . Consider $f(x)$ , a Lebesgue measurable function on $\\mathbb{R}^{d}$ , satisfying: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\int_{\\Omega}f(x)\\,d\\mu=0,\\,\\forall\\Omega\\in\\mathcal{M}(\\mathbb{R}^{d}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "It follows that $f(x)=0$ almost everywhere on $\\mathbb{R}^{d}$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. To demonstrate this via contradiction, let us assume, without loss of generality, that: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mu(\\{x\\mid f(x)>0\\})>0.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since ", "page_idx": 15}, {"type": "equation", "text": "$$\n0<\\mu(\\{x\\mid f(x)>0\\})=\\mu(\\bigcup_{n=1}^{+\\infty}\\{x\\mid f(x)>\\frac{1}{n}\\})\\leq\\sum_{n=1}^{+\\infty}\\mu(\\{x\\mid f(x)>\\frac{1}{n}\\}),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "then we know there exist $n_{0}\\in\\mathbb{Z}^{*}$ such that $\\begin{array}{r}{\\mu(\\{x\\mid f(x)>\\frac{1}{n_{0}}\\})>0}\\end{array}$ . If we set $\\Omega_{0}=\\{x\\mid f(x)>$ $\\frac{1}{n_{0}}\\}\\in\\mathcal{M}(\\mathbb{R}^{d})$ , then by the property of $f(x)$ we can derive: ", "page_idx": 15}, {"type": "equation", "text": "$$\n0=\\int_{\\Omega_{0}}{f(x)\\,d\\mu}>{\\frac{\\mu(\\{x\\mid f(x)>{\\frac{1}{n_{0}}}\\})}{n_{0}}}>0.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Hence, we have reached a contradiction. ", "page_idx": 15}, {"type": "text", "text": "Lemma 2. Assume $f(x)=x^{T}A x+w\\cdot x+b,$ , a Lebesgue measurable function, equals zero almost everywhere $(f(x)=0\\,a.e.,$ , with $A$ being a symmetric matrix. It then follows that $A=0,\\,w=0$ , and $b=0$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. Since $f(x)$ is continuous, we know $f(x)=0$ for $\\forall x\\in\\mathbb{R}^{d}$ . Then we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\nb=f(0)=0.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Setting $x=\\varepsilon w$ for some $\\varepsilon>0$ , we obtain: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\|\\boldsymbol{w}\\|_{2}^{2}+\\varepsilon\\boldsymbol{w}^{T}A\\boldsymbol{w}=0,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "letting $\\varepsilon$ go to zero, we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\|w\\|_{2}^{2}=0\\Rightarrow w=0.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Having established that $f(x)=x^{T}A x$ , consider $x=e_{i}$ , where $e_{i}$ is the unit vector with a value of 1 at the $i$ -th position and 0s elsewhere. Evaluating the function at this vector, we have $f(e_{i})=$ $e_{i}^{T}A e_{i}=a_{i i}=0$ . Subsequently, by letting $x=e_{i}\\!+\\!e_{j}$ , where $e_{j}$ is another unit vector corresponding to the $j$ -th position, we obtain the following equation: ", "page_idx": 15}, {"type": "equation", "text": "$$\na_{i i}+a_{i j}+a_{j i}+a_{j j}=0.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since $a_{i i}=a_{j j}=0$ and $a_{i j}=a_{j i}$ $A$ is a symmetric matrix), we know: ", "page_idx": 15}, {"type": "equation", "text": "$$\na_{i j}=0,\\;\\forall i,j\\in\\{1,2,\\ldots,d\\}\\Rightarrow A=0.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Theorem 4. (Uniqueness) Consider the conditional cross-entropy loss defined on a set $\\Omega\\in\\mathcal{M}(\\mathbb{R}^{d})$ , where $\\mathcal{M}(\\mathbb{R}^{d})$ denotes the Lebesgue $\\sigma$ -algebra on $\\mathbb{R}^{d}$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\nL(A,w,b\\mid\\Omega)=\\frac{1}{2}\\left[E_{x\\sim c l a s s_{1},x\\in\\Omega}\\left(\\log(1+e^{f(x)})\\right)+E_{x\\sim c l a s s_{2},x\\in\\Omega}\\left(\\log(1+e^{-f(x)})\\right)\\right],\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "with ", "page_idx": 16}, {"type": "equation", "text": "$$\nE_{x\\sim c l a s s_{j},x\\in\\Omega}\\left(g(x)\\right)=\\int_{\\Omega}g(x)p_{j}(x)\\,d\\mu,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "then the unique solution satisfies ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{\\partial L(A,w,b\\mid\\Omega)}{\\partial A}\\mid_{A^{*},w^{*},b^{*}}=0,\\;\\frac{\\partial L(A,w,b\\mid\\Omega)}{\\partial w}\\mid_{A^{*},w^{*},b^{*}}=0,\\;\\frac{\\partial L(A,w,b\\mid\\Omega)}{\\partial b}\\mid_{A^{*},w^{*},b^{*}}=0\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "for every $\\Omega\\in\\mathcal{M}(\\mathbb{R}^{d})$ given by: ", "page_idx": 16}, {"type": "equation", "text": "$$\nA^{*}=\\Sigma_{1}^{-1}-\\Sigma_{2}^{-1},\\,\\,w^{*}=-2(\\Sigma_{1}^{-1}\\mu_{1}-\\Sigma_{2}^{-1}\\mu_{2}),\\,\\,b^{*}=\\mu_{1}^{T}\\Sigma_{1}^{-1}\\mu_{1}-\\mu_{2}^{T}\\Sigma_{2}^{-1}\\mu_{2}+\\log\\left(\\frac{|\\Sigma_{1}|}{|\\Sigma_{2}|}\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. Following the derivation outlined in the proof of Theorem 3, we obtain: ", "page_idx": 16}, {"type": "equation", "text": "$$\n{\\frac{\\partial L(A,w,b\\mid\\Omega)}{\\partial A}}={\\frac{1}{2}}\\int_{\\Omega}\\left({\\frac{e^{f(x)}}{1+e^{f(x)}}}p_{1}(x)-{\\frac{1}{1+e^{f(x)}}}p_{2}(x)\\right)x\\cdot x^{T}\\,d\\mu,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n{\\frac{\\partial L(A,w,b\\mid\\Omega)}{\\partial w}}={\\frac{1}{2}}\\int_{\\Omega}\\left({\\frac{e^{f(x)}}{1+e^{f(x)}}}p_{1}(x)-{\\frac{1}{1+e^{f(x)}}}p_{2}(x)\\right)x\\,d\\mu,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n{\\frac{\\partial L(A,w,b\\mid\\Omega)}{\\partial b}}={\\frac{1}{2}}\\int_{\\Omega}\\left({\\frac{e^{f(x)}}{1+e^{f(x)}}}p_{1}(x)-{\\frac{1}{1+e^{f(x)}}}p_{2}(x)\\right)\\,d\\mu.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Hence, if $A^{*},w^{*}$ , and $b^{*}$ satisfy: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{\\partial L(A,w,b\\mid\\Omega)}{\\partial A}\\mid_{A^{*},w^{*},b^{*}}=0,\\;\\frac{\\partial L(A,w,b\\mid\\Omega)}{\\partial w}\\mid_{A^{*},w^{*},b^{*}}=0,\\;\\frac{\\partial L(A,w,b\\mid\\Omega)}{\\partial b}\\mid_{A^{*},w^{*},b^{*}}=0\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "for every $\\Omega\\in\\mathcal{M}(\\mathbb{R}^{d})$ . Subsequently, invoking Lemma 1 yields: ", "page_idx": 16}, {"type": "equation", "text": "$$\ne^{x^{T}A^{*}x+(w^{*})^{T}x+b^{*}}={\\frac{p_{2}(x)}{p_{1}(x)}},\\ a.e.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Simplifying this equation, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\scriptscriptstyle{\\mathcal{I}}^{T}\\big(\\Sigma_{1}^{-1}-\\Sigma_{2}^{-1}-A^{*}\\big)x-\\left[2\\big(\\mu_{1}^{T}\\Sigma_{1}^{-1}-\\mu_{2}^{T}\\Sigma_{2}^{-1}\\big)+(w^{*})^{T}\\big]\\,x+\\mu_{1}^{T}\\Sigma_{1}^{-1}\\mu_{1}-\\mu_{2}^{T}\\Sigma_{2}^{-1}\\mu_{2}+\\log\\left(\\frac{|\\Sigma_{1}|}{|\\Sigma_{2}|}\\right)-b^{*}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which holds almost everywhere. Applying Lemma 2, we obtain: ", "page_idx": 16}, {"type": "equation", "text": "$$\nA^{*}=\\Sigma_{1}^{-1}-\\Sigma_{2}^{-1},\\,\\,w^{*}=-2(\\Sigma_{1}^{-1}\\mu_{1}-\\Sigma_{2}^{-1}\\mu_{2}),\\,\\,b^{*}=\\mu_{1}^{T}\\Sigma_{1}^{-1}\\mu_{1}-\\mu_{2}^{T}\\Sigma_{2}^{-1}\\mu_{2}+\\log\\left(\\frac{|\\Sigma_{1}|}{|\\Sigma_{2}|}\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "B Quadratic neurons capture correlation on multi-class classification task ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "For a classification task with $k$ classes, assume the training samples for these classes are equally drawn from $k$ normal distributions: $c l a s s_{j}\\sim N(\\mu_{j},\\Sigma_{j}),\\bar{j}\\in[\\bar{k}]$ , the optimal classifier $y_{o p t}(x):$ $\\mathbb{R}^{d}\\rightarrow\\{1,2,\\dots,k\\}$ is defined by: ", "page_idx": 16}, {"type": "equation", "text": "$$\ny_{o p t}(x)=\\arg\\operatorname*{max}_{j\\in[k]}p_{j}(x),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $p_{j}(x)$ represents the probability (density function) of sampling point $x$ from the normal distribution $N(\\mu_{j},\\Sigma_{j})$ . Employing $k$ quadratic neurons for this multi-class classification task allows for the derivation of a theorem analogous to the one discussed in the previous section: ", "page_idx": 16}, {"type": "text", "text": "Theorem 5. (Existence) The critical points with respect to the cross-entropy loss $L(\\theta)$ are given as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\nA_{j}^{*}=\\Sigma_{j}^{-1},\\;w_{j}^{*}=-2\\Sigma_{j}^{-1}\\mu_{j},\\;b^{*}=\\mu_{j}^{T}\\Sigma_{j}^{-1}\\mu_{j}+\\log(|\\Sigma_{j}|),\\;j\\in[k],\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where ", "page_idx": 17}, {"type": "equation", "text": "$$\nL(\\theta)=\\frac{1}{k}\\sum_{j=1}^{k}E_{x\\sim c l a s s_{j}}\\left[\\log\\left(1+\\sum_{i=1,i\\neq j}^{k}e^{f_{i}(x)-f_{j}(x)}\\right)\\right].\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Moreover, the corresponding classier generated by this formula is the same as the theoretically optimal classifier as defined in Equation (6). ", "page_idx": 17}, {"type": "text", "text": "Proof. The gradients can be computed explicitly as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{\\partial L}{\\partial A_{j}}=\\frac{1}{k}\\int_{\\mathbb{R}^{d}}\\left(\\sum_{i=1,i\\neq j}^{k}\\frac{e^{f_{j}(x)}}{\\sum_{i=1}^{k}e^{f_{i}(x)}}p_{i}(x)-\\sum_{i=1,i\\neq j}^{k}\\frac{e^{f_{i}(x)}}{\\sum_{i=1}^{k}e^{f_{i}(x)}}p_{j}(x)\\right)x\\cdot x^{T}\\,d\\mu,\\;j\\in[k],\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and similarly for\u2202\u2202wLj and $\\frac{\\partial L}{\\partial b_{j}}$ that can be calculated. From the derivation outlined in the proof of Theorem 3 and the choice for critical points as follows ", "page_idx": 17}, {"type": "equation", "text": "$$\nA_{j}^{*}=\\Sigma_{j}^{-1},\\;w_{j}^{*}=-2\\Sigma_{j}^{-1}\\mu_{j},\\;b^{*}=\\mu_{j}^{T}\\Sigma_{j}^{-1}\\mu_{j}+\\log(|\\Sigma_{j}|),\\;j\\in[k],\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "then we have ", "page_idx": 17}, {"type": "equation", "text": "$$\ne^{f_{j}(x)-f_{i}(x)}=\\frac{p_{j}(x)}{p_{i}(x)},\\;\\forall x\\in\\mathbb{R}^{d},\\;\\forall i,j\\in[k],\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which is equivalent to ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{e^{f_{j}(x)}}{\\sum_{i=1}^{k}e^{f_{i}(x)}}p_{i}(x)=\\frac{e^{f_{i}(x)}}{\\sum_{i=1}^{k}e^{f_{i}(x)}}p_{j}(x),\\;\\forall x\\in\\mathbb{R}^{d},\\;\\forall i,j\\in[k].\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Thus, we can derive: ", "page_idx": 17}, {"type": "equation", "text": "$$\n{\\frac{\\partial L}{\\partial A_{j}}}=0,\\;{\\frac{\\partial L}{\\partial w_{j}}}=0,\\;{\\frac{\\partial L}{\\partial b_{j}}}=0,\\;\\forall j\\in[k].\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The classifier generated by quadratic neurons is defined as: ", "page_idx": 17}, {"type": "equation", "text": "$$\ny_{m o d e l}(x)=\\underset{j\\in[k]}{\\arg\\operatorname*{max}}~f_{j}(x),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, ", "page_idx": 17}, {"type": "equation", "text": "$$\ny_{m o d e l}(x)=j\\Leftrightarrow f_{j}(x)>f_{i}(x),\\;\\forall i\\in[k]/\\{j\\}\\Leftrightarrow y_{o p t}(x)=j.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Hence, $y_{m o d e l}(x)\\,=\\,y_{o p t}(x)$ for all $x$ in $\\mathbb{R}^{d}$ , demonstrating the consistency between these two classifiers. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "Theorem 5 demonstrates that in multi-class classification tasks, each neuron captures data correlation by directly relating to the covariance matrix of their respective class. This concept is empirically validated through numerical experiments on the MNIST dataset, as depicted in Figure 6. The eigenvectors associated with the largest eigenvalues of the covariance matrices, representing the principal components of each class\u2019s distribution, reveal the spatial correlation within that class\u2019s data. The marked similarity between these eigenvectors and the eigenvectors of the quadratic integration matrices of neurons confirms that quadratic neurons effectively capture correlation from the data distribution. This intrinsic ability of quadratic neurons to capture correlations provides a possible explanation for the superior generalization capability of quadratic neurons. ", "page_idx": 17}, {"type": "text", "text": "C Experimental settings of ImageNet training ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The training settings for our Dit-ConvNeXt models on ImageNet-1K are detailed in Table 6. While all Dit-ConvNeXt variants adhere to a unified configuration, certain parameters\u2014namely, the stochastic depth rate, learning rate, and weight decay\u2014are individually customized for each model variant. These hyperparameters were optimized based on performance outcomes, utilizing a grid search over potential learning rates from the set $\\{0.004,0.006,0.008\\}$ and weight decay values options within $\\left\\{0.04,0.06,0.08\\right\\}$ . ", "page_idx": 17}, {"type": "image", "img_path": "2WQjNXZbhR/tmp/c00dbd8b5aa33c6bab9f73f0246dd98863f66ec59f2c1b6af404c9e6e5633b3f.jpg", "img_caption": ["Figure 6: Comparison of Eigenvectors between covariance matrices $\\Sigma_{j}$ and quadratic weights $A_{j}$ for $j\\in\\{0,1,2,3,4,5,6,7,8,9\\}$ . Left: Visualization of eigenvectors corresponding to the largest eigenvalue of $\\Sigma_{j}$ alongside the most similar eigenvectors of $A_{j}$ . Right: Cosine similarity metrics for ten eigenvector pairs depicted on the left. "], "img_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "2WQjNXZbhR/tmp/8344e86cc7263aec1fd9354f43f150e4c8881eb0fde406fbe2e5f5cd49bc23c1.jpg", "table_caption": ["Table 6: ImageNet-1K training settings. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: See Section 1. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: See the Limitations and Discussions part. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 19}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: See Section 2 and Appendix A. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 20}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The main text, along with Appendix C, contains comprehensive details necessary for replicating the study\u2019s findings, including hyperparameters, data augmentation techniques, etc. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 20}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] Justification: Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 21}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The main text, along with Appendix C, contains comprehensive details. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 21}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Several experiments, including the comparison of top-1 accuracy on ImageNet1K, do not feature error bars due to adherence to the convention established in prior studies, which omitted such reporting. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: See the last paragraph in Section 1. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 22}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 22}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: This paper focuses exclusively on fundamental scientific research and does not address societal impacts. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 22}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 23}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 23}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 23}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: See Reference section. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 24}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 24}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] Justification: ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}]