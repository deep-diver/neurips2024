[{"type": "text", "text": "Medformer: A Multi-Granularity Patching Transformer for Medical Time-Series Classification ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yihe Wang\u2217, Nan Huang\u2217, Taida Li\u2217 University of North Carolina - Charlotte {ywang145,nhuang1,tli14}@charlotte.edu ", "page_idx": 0}, {"type": "text", "text": "Yujun Yan Dartmouth College yujun.yan@dartmouth.edu ", "page_idx": 0}, {"type": "text", "text": "Xiang Zhang University of North Carolina - Charlotte xiang.zhang@charlotte.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Medical time series (MedTS) data, such as Electroencephalography (EEG) and Electrocardiography (ECG), play a crucial role in healthcare, such as diagnosing brain and heart diseases. Existing methods for MedTS classification primarily rely on handcrafted biomarkers extraction and CNN-based models, with limited exploration of transformer-based models. In this paper, we introduce Medformer, a multi-granularity patching transformer tailored specifically for MedTS classification. Our method incorporates three novel mechanisms to leverage the unique characteristics of MedTS: cross-channel patching to leverage inter-channel correlations, multi-granularity embedding for capturing features at different scales, and two-stage (intra- and inter-granularity) multi-granularity self-attention for learning features and correlations within and among granularities. We conduct extensive experiments on five public datasets under both subject-dependent and challenging subject-independent setups. Results demonstrate Medformer\u2019s superiority over 10 baselines, achieving top averaged ranking across five datasets on all six evaluation metrics. These findings underscore the significant impact of our method on healthcare applications, such as diagnosing Myocardial Infarction, Alzheimer\u2019s, and Parkinson\u2019s disease. We release the source code at https://github.com/DL4mHealth/Medformer. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Medical time series refers to sequences of health-related data points recorded at successive times, tracking various physiological signals over time [1, 2]. Effective classification of MedTS data enables continuous monitoring and real-time analysis of a subject\u2019s physiological state, supporting early abnormality detection, accurate diagnosis, timely intervention, and personalized treatment\u2014ultimately enhancing patient outcomes and healthcare efficiency [3, 4]. For instance, Electroencephalography (EEG) provides insights into a subject\u2019s neurological status [5, 6], while Electrocardiography (ECG) aids in diagnosing heart conditions [7, 8, 9]. Most current MedTS classification approaches rely on handcrafted biomarker extraction [10, 11, 12], convolutional neural networks (CNN)-based models [13, 14, 15, 16], graph convolutional networks (GNNs)-based models[17, 18], or combinations of CNNs and self-attention modules[19, 20]. Notably, effective transformer-based models for MedTS classification remain underexplored. ", "page_idx": 0}, {"type": "text", "text": "Transformers have demonstrated strong performance in time series representation learning across tasks such as forecasting [21, 22, 23], classification [24, 25], and anomaly detection [26, 27], with a predominant focus on forecasting. While these methods are applicable to MedTS classification, their design motivations and mechanisms may not fully align with the unique requirements of this domain. For example, as shown in Figure 1, models like Autoformer [28] and Informer [29] adopt the token embedding approach from the vanilla transformer [30], using a single cross-channel timestamp as an input token. This strategy struggles to capture coarse-grained temporal features. In contrast, iTransformer [31] encodes the entire series from one channel as an input token, which can overlook fine-grained temporal features while focusing on multi-channel correlations. Additionally, PatchTST [32] embeds a sequence of timestamps from one channel as a patch for self-attention, limiting the model\u2019s capacity to learn cross-channel relationships. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "These existing methods fail to fully exploit the distinctive characteristics of MedTS data, such as local temporal dynamics, inter-channel correlations, and multi-scale feature analysis. First, effective capturing temporal dynamics demands multi-timestamp inputs to capture local temporal patterns, as highlighted in approaches like PatchTST [32] and Crossformer [33]. Second, leveraging cross-channel information is critical; for example, multi-channel EEG data recorded following the International 10\u201320 system [34] monitors the brain activities, with each electrode/channel corresponding to specific brain regions. Since brain functions are integrated, inter-channel correlations (e.g., brain connectome) are crucial in EEG analysis [35, 36, 37]. Third, representation learning across multiple temporal scales and periods is vital to uncover a broad range of health patterns, as certain disease indicators may only appear within specific frequency bands [10, 12]. ", "page_idx": 1}, {"type": "text", "text": "To bridge this gap, we propose Medformer\\*, a multi-granularity patching transformer designed explicitly for MedTS classification. Our approach introduces three mechanisms to enhance learning capacity. First, we propose a novel token embedding method using cross-channel patching, effectively capturing both multitimestamp and cross-channel features. To the best of our knowledge, this is the first application of cross-channel patching for transformer embedding in time series analysis. Second, rather than using fixed-length patches, we employ multigranularity patching with a list of patch lengths, enabling the model to capture features in different scales. This multigranularity approach could simulate different frequency bands, capturing bandspecific features without relying on handcrafted up/downsampling and band filters. Third, We introduce a two-stage (intra- and ", "page_idx": 1}, {"type": "image", "img_path": "jfkid2HwNr/tmp/9bdab3d780698b7a149ad599706690a6755d9bb027dcab9b7cd24da3e5e33dd0.jpg", "img_caption": ["Figure 1: Token embedding methods. Vanilla transformer, Autoformer, and Informer [30, 28, 29] employ a single crosschannel timestamp as a token; iTransformer [31] utilizes an entire channel as a token; and PatchTST and Crossformer [32, 33] adopt a patch of timestamps from one channel as a token. For MedTS classification, we propose Medformer considering inter-channel dependencies (cross-channel), temporal properties (multi-timestamp), and multifaceted scale of temporal patterns (multi-granularity). "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "inter-granularity) self-attention mechanism to capture features within individual granularities and correlations across granularities, enabling complementary information integration across scales. ", "page_idx": 1}, {"type": "text", "text": "We conduct extensive experiments using ten baselines across five public datasets, including three EEG datasets and two ECG datasets, focused on detecting diseases such as Alzheimer\u2019s and cardiovascular conditions under both subject-dependent and subject-independent setups (Figure 2). Results show that Medformer achieves the highest average ranking across all six evaluation metrics and five datasets (Figure 4), highlighting its superior effectiveness, stability, and potential for real-world applications. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Medical Time Series. Medical time series refers to specialized time series data collected from the human body, commonly used for disease diagnosis [3, 7], health monitoring [6, 1], and brain-computer interfaces (BCIs) [39, 2]. Different MedTS modalities include EEG [40, 41, 42], ECG [7, 8, 9], EMG [43, 44], and EOG [45, 46], each offering distinct capabilities for various medical applications. ", "page_idx": 1}, {"type": "text", "text": "For example, EEG and ECG data are instrumental in assessing brain and heart health [40, 7]. Recent BCI research explores using EEG to control objects, providing functional support to individuals with disabilities [2, 47]. Unlike general time series research, which predominantly focuses on forecasting tasks [48, 49], MedTS research is centered around signal decoding, which involves classifying hidden information within MedTS sequences. Current approaches often rely on biomarker identification and deep-learning models utilizing CNNs, GNNs, or hybrid models combining CNNs with self-attention modules. For instance, band features such as relative band power and inter-band correlations [11, 50] have proven effective in EEG-based Alzheimer\u2019s disease diagnosis. Deep-learning models like EEGNet [14], EEG-Conformer [20], and TCN [51, 13] have also shown strong performance across various MedTS classification tasks. ", "page_idx": 2}, {"type": "text", "text": "Transformers for Time Series. Transformer has demonstrated its strong learning and scaling-up ability in many domains, including natural language processing [30, 57] and computer vision [58, 59]. Existing transformerbased methods for time series analysis can be categorized into two main directions: modifying token embedding methods and selfattention mechanisms, or both. For example, PatchTST [32] uses a sequence of singlechannel timestamps as a patch for token embedding. Methods like Autoformer [28], Informer [29], Nonformer [54], and FEDformer [52] develop new self-attention mech", "page_idx": 2}, {"type": "table", "img_path": "jfkid2HwNr/tmp/efe5ce0e7cb9665f82bb17946230f079dd03059585922cd770f13faa831f63ea.jpg", "table_caption": ["Table 1: Existing methods do not fully utilize all potential characteristics in MedTS. "], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "anisms or replace the self-attention module to improve learning ability and reduce complexity. Crossformer [33] and iTransformer [31] modify both token embedding methods and self-attention mechanisms. Patching. Patch embedding has been widely used in time series transformers since the proposal of PatchTST [32]. Existing methods of patching, such as Crossformer [33], CARD [23], and MTST [53], inherit from PatchTST [32] and utilize a sequence of single-channel timestamps for patching. This channel-independent patching might benefti learning ability in time series forecasting but may not be as effective in MedTS classification. Multi-Granularity. Existing methods such as Pyraformer [21], MTST [53], Pathformer [55], and Scaleformer [60], utilize multi-granularity embedding to capture features at different scales, allowing models to learn both fine-grained and coarse-grained patterns. We discuss the differences between our method and existing multi-granularity approaches in Appendix G.1. ", "page_idx": 2}, {"type": "text", "text": "Medformer includes both novel token embedding and self-attention mechanisms. Figure 1 and Table 1 present a comparison of token embedding methods and feature utilization between our method and existing methods. The components of our method can be easily incorporated into existing methods to improve classification learning ability. For example, cross-channel multi-granularity patching can be integrated with methods that modify self-attention mechanisms, such as Autoformer [28] and Informer [29], for token embedding. Similarly, the two-stage multi-granularity self-attention can be combined with existing multi-granularity methods, like MTST [53] and Pathformer [55], to enhance the learning of inter-granularity features. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries and Problem Formulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Disease Diagnosis with MedTS. Medical time series data typically exhibit multiple hierarchical levels, including subject, session, trial, and sample levels [13]. In disease diagnosis tasks using MedTS, each subject is usually assigned a single label, such as indicating the presence or absence of Alzheimer\u2019s disease. However, multi-labeling may be necessary when a subject has co-occurring conditions [61]. Notably, a subject\u2019s medical or physiological state remains relatively stable over time (or within short periods without significant change). For instance, a subject diagnosed with Alzheimer\u2019s disease is expected to retain that diagnosis for many years. If the subject also has Parkinson\u2019s disease, a multi-label learning approach is required, which essentially conducts classification tasks for each disease independently if they are not mutually exclusive. ", "page_idx": 2}, {"type": "text", "text": "Since long sequences of time series data (e.g., trials or sessions) are often segmented into shorter samples for deep learning training, all samples from a single subject should ideally retain the same medical condition label. Thus, each MedTS sample typically includes a class label indicating a specific disease type and a subject ID indicating its original subject. Given the ultimate goal of diagnosing whether a subject has a particular disease, experimental setups must be carefully designed to reflect real-world clinical applications. Diverse setups can yield markedly different outcomes, potentially leading to misleading conclusions. Here, we introduce two widely used setups in MedTS classification and clarify their distinctions. Figure 2 provides a simple illustration of these two setups. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Subject-Dependent. In this setup, the division into training, validation, and test sets is based on time series samples. All samples from various subjects are randomly shuffled and then allocated into the respective sets. Consequently, samples with identical subject IDs may be present in the training, validation, and test sets. This scenario potentially introduces \u201cinformation leakage,\" wherein the model could inadvertently learn the distribution specific to certain subjects during the training phase. This setup is typically employed to assess whether a dataset exhibits cross-subject features and has limited applications under real-world MedTS-based disease diagnosis scenarios. The reason is simple: we cannot know the label of unseen subjects and their corresponding samples during training. Generally, the results of the subject-dependent setup tend to be notably higher than those from the subject-independent setup, often showing the upper limit of a dataset\u2019s learning capability. ", "page_idx": 3}, {"type": "text", "text": "Subject-Independent. In this setup, the division into training, validation, and test sets is based on subjects. Each subject and their corresponding samples are exclusively distributed into one of the training, validation, or test sets. Consequently, samples with identical subject IDs can only be present in one of these sets. This setup holds significant importance in disease diagnosis tasks as it closely simulates real-world scenarios. It enables us to train a model on subjects with known labels and subsequently evaluate its performance on unseen subjects; in other words, evaluate if a subject has a specific disease. However, this setup poses significant challenges in MedTS classification tasks. Due to the variability in data distribution and the potential presence of unknown noise within each subject\u2019s data, capturing general task-related features across subjects becomes challenging [62, 13, 63, 64]. Even if subjects share the same label, the personal noise inherent in each subject\u2019s data may obscure these com", "page_idx": 3}, {"type": "image", "img_path": "jfkid2HwNr/tmp/dc9ff3451c823557c17f395e1a474de60475efe9266beac4feab10fab7566ed1.jpg", "img_caption": ["Figure 2: Subject-dependent/independent setups (figure adopted from our previous work [13]). In the subject-dependent setup, samples from the same subject can appear in both the training and test sets, causing information leakage. In a subject-independent setup, samples from the same subject are exclusively in either the training or test set, which is more challenging and practically meaningful but less studied. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "mon features. Developing a method that effectively captures common features among subjects while disregarding individual noise remains an unsolved problem. ", "page_idx": 3}, {"type": "text", "text": "In this work, we evaluate our model mainly in the subject-independent setup to better align with real-world applications, aiming to draw attention within the time series research community to the substantial challenges posed by this setup. While our model is not specifically tailored to address subject-independent problems, it integrates multi-timestamp, cross-channel, and multi-granularity features in MedTS, enhancing its capacity to capture subject-invariant representations. Consequently, our model is well-equipped to tackle the subject-independent challenge to a certain extent, and our results (Section 5) confirm such capability of Medformer. ", "page_idx": 3}, {"type": "text", "text": "We next present the problem formulation for multivariate medical time series classification in the context of disease diagnosis. ", "page_idx": 3}, {"type": "text", "text": "Problem (MedTS Classification). Consider an input MedTS sample $\\mathbf{\\boldsymbol{x}}_{i n}\\in\\mathbb{R}^{T\\times C}$ , where $T$ denotes the number of timestamps and $C$ represents the number of channels. Our objective is to learn an encoder that can generate a representation $^h$ , which can be used to predict the corresponding label $\\pmb{y}\\in\\mathbb{R}^{K}$ of the input sample. Here, $K$ denotes the number of medically relevant classes, such as various disease types or different stages of one disease. ", "page_idx": 3}, {"type": "image", "img_path": "jfkid2HwNr/tmp/56fd57accc1f74a02713970949575df6e58bc71b6b36bb4b4f9eb1265e7b5e2a.jpg", "img_caption": ["Figure 3: Overview of Medformer. a) Workflow. b) For the input sample $x_{\\mathrm{in}}$ , we apply $n$ distinct patch lengths in parallel to create patched features $\\pmb{x}_{p}^{(i)}$ , where $i$ ranges from 1 to $_n$ . Each patch length represents a unique granularity. These patches are then projected into x(ei)and subsequently augmented to formx(ei ). c) We obtain the final embedding $\\pmb{x}^{(i)}$ by combining the augmented $\\widetilde{\\pmb{x}}_{e}^{(i)}$ with both the positional embedding $W_{\\mathrm{pos}}$ and the granularity embedding W g(ri ). Additionally, a granularity-specific router $\\pmb{u}^{(i)}$ is designed to capture integrated information for each respective granularity. We then perform intra-granularity self-attention, focusing on individual granularities, and inter-granularity self-attention, using the routers to facilitate communication across different granularities. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "4 Method ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we first describe the cross-channel multi-granularity patching mechanism for learning spatial-temporal features in different scales. Next, we analyze the two-stage multi-granularity selfattention mechanism, which leverages features within the same granularity and correlations among different granularities. The architecture of the proposed Medformer is illustrated in Figure 3. ", "page_idx": 4}, {"type": "text", "text": "Cross-Channel Multi-Granularity Patch Embedding. From the medical perspective, the brain or heart functions as a cohesive unit, suggesting a naive assumption that there are inherent correlations among different channels in MedTS [35, 36, 37], as each channel represents the activities of distinct brain or heart regions. Motivated by the above assumption, we reasonably propose multi-channel patching for token embedding, which is different from existing patch embedding methods that embed patches in a channel-independent manner and fail to capture inter-channel correlations [32, 33, 23]. Figure 1 provides an overview comparison of existing token embedding methods and ours. Additionally, existing research on EEG biomarker extraction has shown that certain features are linked to different frequency bands, such as $\\alpha,\\beta$ , and $\\gamma$ bands [10, 12]. This motivates us to embed patch tokens in a multi-granularity way. Instead of using traditional methods like up/downsampling or handcrafted band flitering, multi-granularity patching automatically corresponds to different sampling frequencies, which can simulate different frequency bands and capture band-related features. ", "page_idx": 4}, {"type": "text", "text": "Given the above rationales, we propose a novel token embedding approach: cross-channel multigranularity patching. Given an input multivariate MedTS sample $\\bar{\\mathbf{x}_{\\mathrm{in}}}\\in\\bar{\\mathbb{R}}^{T\\times C}$ , and a list of different patch lengths $\\{L_{1},L_{2},\\ldots,L_{n}\\}$ . For the $i$ -th patch length $L_{i}$ denoting granularity $i$ , we segment the input sample into $N_{i}$ cross-channel non-overlapping patches $\\mathbf{x}_{\\mathrm{p}}^{(i)}\\in\\mathbb{R}^{N_{i}\\times(L_{i}\\cdot C)}$ . Zero padding is applied to ensure that the number of timestamps $T$ is divisible by $L_{i}$ , making $N_{i}=\\lceil T/\\bar{L}_{i}\\rceil$ . ", "page_idx": 5}, {"type": "text", "text": "The patches are mapped into latent embeddings space using a linear projection: $\\pmb{x}_{\\mathrm{e}}^{(i)}=\\pmb{x}_{\\mathrm{p}}^{(i)}\\pmb{W}^{(i)}$ , where $\\pmb{x}_{\\mathrm{e}}^{(i)}\\in\\mathbb{R}^{N_{i}\\times D}$ and $\\boldsymbol{W}^{(i)}\\in\\mathbb{R}^{(L_{i}\\cdot C)\\times D}$ . Inspired by the augmented views contrasting in the contrastive learning framework [65, 13, 66], we further apply data augmentations such as masking and jittering on $\\pmb{x}_{\\mathrm{e}}^{(i)}$ to obtain augmented embeddings $\\widetilde{\\pmb{x}}_{\\mathrm{e}}^{(i)}\\in\\mathbb{R}^{N_{i}\\times D}$ . We assume the augmentation can improve the learning ability in the following inter-granularity self-attention stage by forcing different granularities to learn and complement information from each other. ", "page_idx": 5}, {"type": "text", "text": "A fixed positional embedding $W_{\\mathrm{pos}}\\in\\mathbb{R}^{G\\times D}$ is generated for positional encoding [30], where $G$ is a very large number. We add $W_{\\mathrm{pos}}[1:N_{i}]\\in\\mathbb{R}^{N_{i}\\times D}$ , the first $N_{i}$ rows of the positional embedding $W_{\\mathrm{pos}}$ , and a learnable granularity embedding $W_{\\mathrm{gr}}^{(i)}\\in\\mathbb{R}^{1\\times D}$ , to obtain the final patch embedding for the $i$ -th granularity with patch length $L_{i}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\pmb{x}^{(i)}=\\widetilde{\\pmb{x}}_{\\mathrm{e}}^{(i)}+W_{\\mathrm{pos}}[1:N_{i}]+W_{\\mathrm{gr}}^{(i)},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\pmb{x}^{(i)}\\ \\in\\ \\mathbb{R}^{N_{i}\\times D}$ . Note that the granularity embedding $W_{\\mathrm{gr}}^{(i)}$ aims to distinguish among granularities and is broadcasted to all embedding rows with $D$ dimension during addition. ", "page_idx": 5}, {"type": "text", "text": "To reduce time and space complexity, we initialize a router to be used in the multi-granularity self-attention (as described later) for each granularity: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{u}^{(i)}=\\pmb{W}_{\\mathrm{pos}}[N_{i}+1]+\\pmb{W}_{\\mathrm{gr}}^{(i)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\pmb{u}^{(i)},W_{\\mathrm{pos}}[N_{i}+1],W_{\\mathrm{gr}}^{(i)}\\in\\mathbb{R}^{1\\times D}$ . Here, $W_{\\mathrm{pos}}[N_{i}+1]$ is not used for positional embedding but to inform the router about the number of patches with the current $L_{i}$ granularity, and $W_{\\mathrm{gr}}^{(i)}$ contains the granularity information. Both components help distinguish the routers from one another. Finally, we obtain a list of final patch embeddings $\\{\\pmb{x}^{(1)},\\pmb{x}^{(2)},\\dots,\\pmb{x}^{(n)}\\}$ and router embeddings $\\left\\{\\pmb{u}^{(1)},\\pmb{u}^{(2)},\\cdot\\cdot\\cdot,\\pmb{u}^{(n)}\\right\\}$ for different granularities with patch lengths $\\{L_{1},L_{2},\\ldots,L_{n}\\}$ . We feed the embeddings to the two-stage multi-granularity self-attention. ", "page_idx": 5}, {"type": "text", "text": "Multi-Granularity Self-Attention. Our goal is to learn multi-granularity features and granularity interactions during self-attention. A naive approach to achieve this goal is to concatenate all the patch embeddings $\\{\\pmb{x}^{(\\bar{1})},\\pmb{x}^{(2)},\\cdot\\cdot\\cdot,\\pmb{x}^{(n)}\\}$ into a large patch embedding $\\pmb{X}\\in\\mathbb{R}^{(N_{1}+N_{2}+\\ldots+N_{n})\\times\\bar{D}}$ and perform self-attention on this new embedding, where $n$ denotes the number of different granularities. However, this results in a time complexity of $O\\left(\\left(\\textstyle{\\sum_{i=1}^{n}N_{i}}\\right)^{2}\\right)$ , which is impractical for a large $n$ . ", "page_idx": 5}, {"type": "text", "text": "To reduce the time complexity, we propose a router mechanism and split the self-attention module into two stages: a) intra-granularity self-attention and b) inter-granularity self-attention. The intragranularity stage performs self-attention within the same granularity to capture the distinctive features of each granularity. The inter-granularity stage performs self-attention across different granularities to capture their correlations. ", "page_idx": 5}, {"type": "text", "text": "aaaaaaaaaaaaaaaaa)))))))))))))))))                 IIIIIIIIIIIIIIIIInnnnnnnnnnnnnnnnntttttttttttttttttrrrrrrrrrrrrrrrrraaaaaaaaaaaaaaaaa-----------------GGGGGGGGGGGGGGGGGrrrrrrrrrrrrrrrrraaaaaaaaaaaaaaaaannnnnnnnnnnnnnnnnuuuuuuuuuuuuuuuuulllllllllllllllllaaaaaaaaaaaaaaaaarrrrrrrrrrrrrrrrriiiiiiiiiiiiiiiiitttttttttttttttttyyyyyyyyyyyyyyyyy                 SSSSSSSSSSSSSSSSSeeeeeeeeeeeeeeeeelllllllllllllllllfffffffffffffffff-----------------AAAAAAAAAAAAAAAAAtttttttttttttttttttttttttttttttttteeeeeeeeeeeeeeeeennnnnnnnnnnnnnnnntttttttttttttttttiiiiiiiiiiiiiiiiiooooooooooooooooonnnnnnnnnnnnnnnnn. For the $i$ -th patch length $L_{i}$ denoting granularity $i$ , we vertically concatenate the patch embedding $\\pmb{x}^{(i)}\\in\\mathbb{R}^{N_{i}\\times D}$ and router embedding $\\pmb{u}^{(i)}\\in\\mathbb{R}^{1\\times D}$ to form an intermediate sequence of embeddings z(i) \u2208R(Ni+1)\u00d7D: ", "page_idx": 5}, {"type": "equation", "text": "$$\nz^{(i)}=\\left[{\\pmb x}^{(i)}\\|{\\pmb u}^{(i)}\\right]\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $[\\cdot\\|\\cdot]$ denotes concatenation. We perform self-attention on the new $\\boldsymbol{z}^{(i)}$ for both the patch embedding $\\pmb{x}^{(i)}$ and the router embedding $\\pmb{u}^{(i)}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{x}^{(i)}\\leftarrow\\mathrm{Attn}^{\\mathrm{Intra}}\\left(\\pmb{x}^{(i)},\\pmb{z}^{(i)},\\pmb{z}^{(i)}\\right)}\\\\ {\\pmb{u}^{(i)}\\leftarrow\\mathrm{Attn}^{\\mathrm{Intra}}\\left(\\pmb{u}^{(i)},\\pmb{z}^{(i)},\\pmb{z}^{(i)}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where At $\\operatorname{n}\\left(Q,K,V\\right)$ denotes the scaled dot-product self-attention mechanism in [30]. Note that the router embedding $\\pmb{u}^{(i)}$ is updated concurrently with the patch embedding $\\pmb{x}^{(i)}$ to maintain consistency, ensuring that the router effectively summarizes each granularity\u2019s features in the current training step and is ready for the subsequent inter-granularity self-attention. The intra-granularity self-attention mechanism allows the model to capture temporal features within a single granularity, facilitating the extraction of local features and correlations among timestamps at the same scale. ", "page_idx": 6}, {"type": "text", "text": "bbbbbbbbbbbbbbbbb)))))))))))))))))                 IIIIIIIIIIIIIIIIInnnnnnnnnnnnnnnnnttttttttttttttttteeeeeeeeeeeeeeeeerrrrrrrrrrrrrrrrr-----------------GGGGGGGGGGGGGGGGGrrrrrrrrrrrrrrrrraaaaaaaaaaaaaaaaannnnnnnnnnnnnnnnnuuuuuuuuuuuuuuuuulllllllllllllllllaaaaaaaaaaaaaaaaarrrrrrrrrrrrrrrrriiiiiiiiiiiiiiiiitttttttttttttttttyyyyyyyyyyyyyyyyy                 SSSSSSSSSSSSSSSSSeeeeeeeeeeeeeeeeelllllllllllllllllfffffffffffffffff-----------------AAAAAAAAAAAAAAAAAtttttttttttttttttttttttttttttttttteeeeeeeeeeeeeeeeennnnnnnnnnnnnnnnntttttttttttttttttiiiiiiiiiiiiiiiiiooooooooooooooooonnnnnnnnnnnnnnnnn. We concatenate all router embeddings $\\left\\{\\pmb{u}^{(1)},\\pmb{u}^{(2)},\\cdot\\cdot\\cdot,\\pmb{u}^{(n)}\\right\\}$ to form a sequence of routers $U\\in\\mathbb{R}^{n\\times D}$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\nU=\\left[\\pmb{u}^{(1)}||\\pmb{u}^{(2)}||\\pmb{\\cdot}\\pmb{\\cdot}\\cdot||\\pmb{u}^{(n)}\\right]\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $n$ is the number of different granularities. For granularity $i$ with patch length $L_{i}$ , we apply self-attention to the router embedding $\\pmb{u}^{(i)}\\in\\mathbb{R}^{1\\times D}$ with all the routers $U$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\pmb{u}^{(i)}\\leftarrow\\mathrm{Attn}^{\\mathrm{Inter}}\\left(\\pmb{u}^{(i)},\\pmb{U},\\pmb{U}\\right)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Each router contains global information specific to one granularity by doing intra-granularity selfattention. By performing self-attention among routers, information can be exchanged and learned across different granularities, effectively capturing features across various scales. Additionally, the use of the router mechanism successfully reduces the time complexity of the naive approach from $O\\left(\\left(\\sum_{i=1}^{n}N_{i}\\right)^{2}\\right)$ to $O\\left(\\sum_{i=1}^{n}N_{i}^{2}+n^{2}\\right)$ . Given that $N_{i}\\leq T$ , the worst-case time complexity for our self-attention mechanism is $O\\left(n T^{2}+n^{2}\\right)$ . However, a reasonable choice of patch lengths as a power series, i.e., $L_{i}=2^{i}$ , leads to a time complexity of $O(T^{2})$ . To further reduce complexity and memory consumption, we apply shared layer normalization and feed-forward layers across all granularities. See appendix F for more details about complexity analysis. ", "page_idx": 6}, {"type": "text", "text": "Summary. Our method utilizes the standard transformer architecture shown in Figure 3. For given sample $x_{\\mathrm{in}}$ , after $M$ layers of self-attention learning, we obtain a list of updated patch embeddings $\\left\\{\\pmb{x}^{(1)},\\pmb{x}^{(2)},\\cdot\\cdot\\cdot,\\pmb{x}^{(n)}\\right\\}$ , which we concatenate them to form a final representation $^h$ that can be used to predict label $y\\in\\mathbb{R}^{K}$ in a downstream classification task. Note that although we discuss multigranularity here, our method is flexible and can be easily adapted to variants such as single-granularity or even repetitive same granularities. See Appendix D.2 for more details. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We compare our Medformer with 10 baselines across 5 datasets, including 3 EEG datasets and 2 ECG datasets. Our method is evaluated under two setups (Section 3): subject-dependent and subject-independent. In the subject-dependent setup, training, validation, and test sets are split based on samples, while in the subject-independent setup, they are split based on subjects. ", "page_idx": 6}, {"type": "table", "img_path": "jfkid2HwNr/tmp/36014c0e4746d4b9f53edc08f992ad5ed6f02652bf0062d219c4f3dd8d629bba.jpg", "table_caption": ["Table 2: The information of processed datasets. The table shows the number of subjects, samples, classes, channels, sampling rate, sample timestamps, modality of MedTS, and file size. Here, #-Timestamps indicates the number of timestamps per sample. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Datasets. (1) APAVA [67] is an EEG dataset where each sample is assigned a binary label indicating whether the subject has Alzheimer\u2019s disease. (2) TDBRAIN [68] is an EEG dataset with a binary label assigned to each sample, indicating whether the subject has Parkinson\u2019s disease. (3) ADFTD [69, 19] is an EEG dataset with a three-class label for each sample, categorizing the subject as Healthy, having Frontotemporal Dementia, or Alzheimer\u2019s disease. (4) PTB [70] is an ECG dataset where each sample is labeled with a binary indicator of Myocardial Infarction. (5) PTB-XL [71] is an ECG dataset with a five-class label for each sample, representing various heart conditions. Table 2 provides information on the processed datasets. For additional details on data characteristics, train-validation-test splits under different setups, and data preprocessing, please see Appendix B. ", "page_idx": 6}, {"type": "table", "img_path": "jfkid2HwNr/tmp/ffcd9f8a2797dcd7b1d87f37d3c685b27b391c8d94f3044a75a8e3828a646fd4.jpg", "table_caption": ["Table 3: Results of Subject-Dependent Setup. The training, validation, and test sets are split based on samples according to a predetermined ratio. Results of the ADFTD dataset under this setup are presented here. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Baselines. We compare with 10 state-of-the-art time series transformer methods: Autoformer [28], Crossformer [33], FEDformer [52], Informer [29], iTransformer [31], MTST [53], Nonformer [54], PatchTST [32], Reformer [56], and vanilla Transformer [30]. ", "page_idx": 7}, {"type": "text", "text": "Implementation. We employ six evaluation metrics: accuracy, precision (macro-averaged), recall (macro-averaged), F1 score (macro-averaged), AUROC (macro-averaged), and AUPRC (macroaveraged). The training process is conducted with five random seeds (41-45) on fixed training, validation, and test sets to compute the mean and standard deviation of the models. All experiments are run on an NVIDIA RTX 4090 GPU and a server with 4 RTX A5000 GPUs. ", "page_idx": 7}, {"type": "text", "text": "For data augmentation methods, we provide six widely used methods in time series augmentation [72, 66, 73, 61]. For more details about these six methods, see Appendix A. For the parameter tuning in our method and all baselines, we employ 6 layers for the encoder, set the dimension $D$ to 128, and the hidden dimension of feed-forward networks to 256. We utilize the Adam optimizer with a learning rate of 1e-4. The batch size is set to {32,32,128,128,128} for datasets APAVA, TDBrain, ADFTD, PTB, and PTB-XL, respectively. The training epoch is set to 100, with early stopping triggered after 10 epochs without improvement in the F1 score on the validation set. We save the model with the best F1 score on the validation set and evaluate it on the test set. See Appendix $\\mathrm{C}$ for any additional implementation details of our method and all baselines. ", "page_idx": 7}, {"type": "text", "text": "5.1 Results of Subject-Dependent ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Setup. In this setup, the training, validation, and test sets are split based on samples. All samples from all subjects are randomly shuffled and distributed into the training, validation, and test sets according to a predetermined ratio, allowing samples from the same subject to appear in three sets simultaneously. As discussed in the Preliminaries section 3, this setup has limited applicability for MedTS-based disease diagnosis in real-world scenarios. It is usually used to evaluate whether the dataset exhibits cross-subject features quickly. The results obtained from this setup are typically much higher than those from the subject-independent setup, showing a dataset\u2019s upper limit of learnability. ", "page_idx": 7}, {"type": "text", "text": "Results. We evaluate the EEG dataset ADFTD using this setup to provide a direct comparison of results with the subject-independent setup. The results are presented in Table 3. Our method outperforms all the baselines, achieving the top-1 results in all six evaluations, with an impressive F1 score of $97.50\\%$ . Notably, baseline methods like Informer, Nonformer, Reformer, and Transformer also demonstrate strong performance, achieving F1 scores exceeding $90\\%$ . The overall results indicate the presence of discernible and learnable features related to Alzheimer\u2019s Disease within this dataset. ", "page_idx": 7}, {"type": "text", "text": "5.2 Results of Subject-Independent ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Setup. In this setup, the training, validation, and test sets are split based on subjects. All subjects and their corresponding samples are distributed into the training, validation, and test sets according to a predetermined ratio or subject IDs. Samples from the same subjects should exclusively appear in one of these three sets. This setup simulates real-world MedTS-based disease diagnosis, aiming to train a model on subjects with known labels and then test it on unseen subjects to determine if they have a specific disease. The challenges associated with this setup are discussed in section 3. All five datasets are evaluated using this setup. ", "page_idx": 7}, {"type": "table", "img_path": "jfkid2HwNr/tmp/a3b05c37b30c732cd5efa14e61bd5fa90104d330354c392a2e19dfb516c734af.jpg", "table_caption": ["Table 4: Results of Subject-Independent Setup. The training, validation, and test sets are distributed based on subjects according to a predetermined ratio/IDs. Results of the APAVA, TDBrain, ADFTD, PTB, and PTB-XL datasets under this setup are presented here. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Results. Table 4 presents the results of the subject-independent setup. Our method achieves the top-1 F1 scores on 4 out of 5 datasets. Overall, our method achieves 15 top-1 and 30 top-3 rankings out of 30 evaluations conducted across 5 datasets and 10 baselines, considering 6 different metrics. Figure 4 provides an overview heatmap table of average rank across 5 datasets on 6 metrics for all methods. Lower rank numbers indicate better results, with rank 1 representing the best performance among all methods. Our method demonstrates the best average rank among all methods across the 6 metrics. Additionally, it is notable that the result for ADFTD is a $50.65\\%$ F1 score under the subject-independent setup, which is significantly lower than the $97.50\\%$ F1 score achieved under the subject-dependent setup. This comparison highlights the challenge of the subject-independent setup, which better simulates real-world scenarios. ", "page_idx": 9}, {"type": "image", "img_path": "jfkid2HwNr/tmp/a9b7479351a72996c5c9eb77a5ed8f491c07267b3d2ec2a616c7f18b9a5aa2cc.jpg", "img_caption": ["Figure 4: Average Rank of Subject-Independent Setup. The heatmap table shows the average rank of Medformer and 10 baselines across 5 datasets using the subjectindependent setup. A lower number indicates better results. The average rank is calculated across the 5 datasets to obtain the overall average rank. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "5.3 Ablation Study and Additional Experiments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Ablation Study. 1) Module Study: We conduct a module study to evaluate the effectiveness of each proposed mechanism in our method (Appendix D.1). 2) Patch Length Study: We perform parameter tuning on the list of patch lengths to evaluate the effectiveness of multi-granularities (Appendix D.2). ", "page_idx": 9}, {"type": "text", "text": "Additional Experiments. We also perform experiments on two human activities recognition datasets [74, 75] to demonstrate the learning ability of our model on general time series with potential channel correlations (Appendix E). ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion and Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Conclusion This paper presents Medformer, a multi-granularity patching transformer tailored for MedTS classification. We introduce three novel mechanisms that leverage the distinctive features of MedTS, including channel correlations and frequency band biomarkers. These mechanisms include cross-channel patching to capture multi-timestamp and cross-channel features, multi-granularity embedding to learn features at various scales, and a two-stage multi-granularity self-attention mechanism to extract features both within and across granularities. Experimental results across five datasets, evaluated against ten baselines under the subject-independent setup, demonstrate the effectiveness and robustness of our approach, underscoring its potential for real-world applications. ", "page_idx": 9}, {"type": "text", "text": "Limitations and Future Work The design of Medformer allows for inputting various patch lengths, offering both opportunities and challenges. While variable patch lengths provide flexibility and have been shown to outperform uniform lengths in some cases (see Appendix D.2 and Appendix C), not all patch length configurations yield optimal results. Some combinations may perform worse than uniform patch lengths, necessitating careful tuning of patch lengths as hyperparameters. Future work could explore mechanisms for automatically selecting the most effective patch lengths and optimizing the model\u2019s ability to capture relevant granularities. Additionally, while our method outperforms baselines and performs well on certain datasets under the subject-independent setup, it does not include mechanisms specifically designed for this setup. Developing modules that decompose subject-specific features from task-related features could further enhance learning under the subject-independent setup, presenting an intriguing direction for future research. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Yara Badr, Usman Tariq, Fares Al-Shargie, Fabio Babiloni, Fadwa Al Mughairbi, and Hasan Al-Nashash. A review on evaluating mental stress by deep learning using eeg signals. Neural Computing and Applications, pages 1\u201326, 2024. [2] Hamdi Altaheri, Ghulam Muhammad, Mansour Alsulaiman, Syed Umar Amin, Ghadir Ali Altuwaijri, Wadood Abdul, Mohamed A Bencherif, and Mohammed Faisal. Deep learning techniques for classification of electroencephalogram (eeg) motor imagery (mi) signals: A review. Neural Computing and Applications, 35(20):14681\u201314722, 2023.   \n[3] Xinwen Liu, Huan Wang, Zongjin Li, and Lang Qin. Deep learning in ecg diagnosis: A review. Knowledge-Based Systems, 227:107187, 2021.   \n[4] Fatma Murat, Ozal Yildirim, Muhammed Talo, Ulas Baran Baloglu, Yakup Demir, and U Rajendra Acharya. Application of deep learning techniques for heartbeats detection using ecg signals-analysis and review. Computers in biology and medicine, 120:103726, 2020. [5] Aniqa Arif, Yihe Wang, Rui Yin, Xiang Zhang, and Ahmed Helmy. Ef-net: Mental state recognition by analyzing multimodal eeg-fnirs via cnn. Sensors, 24(6):1889, 2024. [6] Mahboobeh Jafari, Afshin Shoeibi, Marjane Khodatars, Sara Bagherzadeh, Ahmad Shalbaf, David L\u00f3pez Garc\u00eda, Juan M Gorriz, and U Rajendra Acharya. Emotion recognition in eeg signals using deep learning methods: A review. Computers in Biology and Medicine, page 107450, 2023.   \n[7] Qiao Xiao, Khuan Lee, Siti Aisah Mokhtar, Iskasymar Ismail, Ahmad Luqman bin Md Pauzi, Qiuxia Zhang, and Poh Ying Lim. Deep learning-based ecg arrhythmia classification: A systematic review. Applied Sciences, 13(8):4964, 2023.   \n[8] Zekai Wang, Stavros Stavrakis, and Bing Yao. Hierarchical deep learning with generative adversarial network for automatic cardiac diagnosis from ecg signals. Computers in Biology and Medicine, 155:106641, 2023.   \n[9] Dani Kiyasseh, Tingting Zhu, and David A Clifton. Clocs: Contrastive learning of cardiac signals across space, time, and patients. In International Conference on Machine Learning, pages 5606\u20135615. PMLR, 2021.   \n[10] Katerina D Tzimourta, Vasileios Christou, Alexandros T Tzallas, Nikolaos Giannakeas, Loukas G Astrakas, Pantelis Angelidis, Dimitrios Tsalikakis, and Markos G Tsipouras. Machine learning algorithms and statistical approaches for alzheimer\u2019s disease analysis based on resting-state eeg recordings: A systematic review. International journal of neural systems, 31(05):2130002, 2021.   \n[11] Golshan Fahimi, Seyed Mahmoud Tabatabaei, Elnaz Fahimi, and Hamid Rajebi. Index of theta/alpha ratio of the quantitative electroencephalogram in alzheimer\u2019s disease: a case-control study. Acta Medica Iranica, pages 502\u2013506, 2017.   \n[12] Salah S Al-Zaiti, Christian Martin-Gill, Jessica K Z\u00e8gre-Hemsey, Zeineb Bouzid, Ziad Faramand, Mohammad O Alrawashdeh, Richard E Gregg, Stephanie Helman, Nathan T Riek, Karina Kraevsky-Phillips, et al. Machine learning for ecg diagnosis and risk stratification of occlusion myocardial infarction. Nature Medicine, 29(7):1804\u20131813, 2023.   \n[13] Yihe Wang, Yu Han, Haishuai Wang, and Xiang Zhang. Contrast everything: A hierarchical contrastive framework for medical time-series. Advances in Neural Information Processing Systems, 36, 2024.   \n[14] Vernon J Lawhern, Amelia J Solon, Nicholas R Waytowich, Stephen M Gordon, Chou P Hung, and Brent J Lance. Eegnet: a compact convolutional neural network for eeg-based brain\u2013computer interfaces. Journal of neural engineering, 15(5):056013, 2018.   \n[15] Hisaki Makimoto, Moritz H\u00f6ckmann, Tina Lin, David Gl\u00f6ckner, Shqipe Gerguri, Lukas Clasen, Jan Schmidt, Athena Assadi-Schmidt, Alexandru Bejinariu, Patrick M\u00fcller, et al. Performance of a convolutional neural network derived from an ecg database in recognizing myocardial infarction. Scientific reports, 10(1):8445, 2020.   \n[16] Donghao Luo and Xue Wang. Moderntcn: A modern pure convolution structure for general time series analysis. In The Twelfth International Conference on Learning Representations, 2024.   \n[17] Xiaocai Shan, Jun Cao, Shoudong Huo, Liangyu Chen, Ptolemaios Georgios Sarrigiannis, and Yifan Zhao. Spatial\u2013temporal graph convolutional network for alzheimer classification based on brain functional connectivity imaging of electroencephalogram. Human Brain Mapping, 43(17):5194\u20135209, 2022.   \n[18] Dominik Klepl, Fei He, Min Wu, Daniel J Blackburn, and Ptolemaios Sarrigiannis. Adaptive gated graph convolutional network for explainable diagnosis of alzheimer\u2019s disease using eeg data. IEEE Transactions on Neural Systems and Rehabilitation Engineering, 2023.   \n[19] Andreas Miltiadous, Emmanouil Gionanidis, Katerina D Tzimourta, Nikolaos Giannakeas, and Alexandros T Tzallas. Dice-net: a novel convolution-transformer architecture for alzheimer detection in eeg signals. IEEE Access, 2023.   \n[20] Yonghao Song, Qingqing Zheng, Bingchuan Liu, and Xiaorong Gao. Eeg conformer: Convolutional transformer for eeg decoding and visualization. IEEE Transactions on Neural Systems and Rehabilitation Engineering, 31:710\u2013719, 2022.   \n[21] Shizhan Liu, Hang Yu, Cong Liao, Jianguo Li, Weiyao Lin, Alex X Liu, and Schahram Dustdar. Pyraformer: Low-complexity pyramidal attention for long-range time series modeling and forecasting. In International conference on learning representations, 2021.   \n[22] Gerald Woo, Chenghao Liu, Doyen Sahoo, Akshat Kumar, and Steven Hoi. Etsformer: Exponential smoothing transformers for time-series forecasting. arXiv preprint arXiv:2202.01381, 2022.   \n[23] Xue Wang, Tian Zhou, Qingsong Wen, Jinyang Gao, Bolin Ding, and Rong Jin. Card: Channel aligned robust blend transformer for time series forecasting. In International Conference on Learning Representations, 2023.   \n[24] Minghao Liu, Shengqi Ren, Siyuan Ma, Jiahui Jiao, Yizhou Chen, Zhiguang Wang, and Wei Song. Gated transformer networks for multivariate time series classification. arXiv preprint arXiv:2103.14438, 2021.   \n[25] Zekun Li, Shiyang Li, and Xifeng Yan. Time series as images: Vision transformer for irregularly sampled time series. Advances in Neural Information Processing Systems, 36, 2024.   \n[26] Jiehui Xu, Haixu Wu, Jianmin Wang, and Mingsheng Long. Anomaly transformer: Time series anomaly detection with association discrepancy. International Conference on Learning Representations, 2021.   \n[27] Junho Song, Keonwoo Kim, Jeonglyul Oh, and Sungzoon Cho. Memto: Memory-guided transformer for multivariate time series anomaly detection. Advances in Neural Information Processing Systems, 36, 2024.   \n[28] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting. Advances in Neural Information Processing Systems, 34:22419\u201322430, 2021.   \n[29] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In Proceedings of the AAAI conference on artificial intelligence, volume 35, pages 11106\u201311115, 2021.   \n[30] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \n[31] Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, and Mingsheng Long. itransformer: Inverted transformers are effective for time series forecasting. International Conference on Learning Representations, 2024.   \n[32] Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. A time series is worth 64 words: Long-term forecasting with transformers. ICLR, 2023.   \n[33] Yunhao Zhang and Junchi Yan. Crossformer: Transformer utilizing cross-dimension dependency for multivariate time series forecasting. In The Eleventh International Conference on Learning Representations, 2022.   \n[34] Uwe Herwig, Peyman Satrapi, and Carlos Sch\u00f6nfeldt-Lecuona. Using the international 10-20 eeg system for positioning of transcranial magnetic stimulation. Brain topography, 16:95\u201399, 2003.   \n[35] Vincent Bazinet, Justine Y Hansen, and Bratislav Misic. Towards a biologically annotated brain connectome. Nature reviews neuroscience, 24(12):747\u2013760, 2023.   \n[36] Saeid Sanei and Jonathon A Chambers. EEG signal processing. John Wiley & Sons, 2013.   \n[37] Selcan Kaplan Berkaya, Alper Kursat Uysal, Efnan Sora Gunal, Semih Ergin, Serkan Gunal, and M Bilginer Gulmezoglu. A survey on ecg analysis. Biomedical Signal Processing and Control, 43:216\u2013235, 2018.   \n[38] Yunhe Gao, Mu Zhou, Di Liu, Zhennan Yan, Shaoting Zhang, and Dimitris N Metaxas. A data-scalable transformer for medical image segmentation: architecture, model efficiency, and benchmark. arXiv preprint arXiv:2203.00131, 2022.   \n[39] Elon Musk et al. An integrated brain-machine interface platform with thousands of channels. Journal of medical Internet research, 21(10):e16194, 2019.   \n[40] Siyi Tang, Jared Dunnmon, Khaled Kamal Saab, Xuan Zhang, Qianying Huang, Florian Dubost, Daniel Rubin, and Christopher Lee-Messer. Self-supervised graph neural networks for improved electroencephalographic seizure analysis. In International Conference on Learning Representations, 2021.   \n[41] Chaoqi Yang, M Brandon Westover, and Jimeng Sun. Manydg: Many-domain generalization for healthcare applications. In The Eleventh International Conference on Learning Representations, 2023.   \n[42] Xiaodong Qu, Zepeng Hu, Zhaonan Li, and Timothy J Hickey. Ensemble methods and lstm outperformed other eight machine learning classifiers in an eeg-based bci experiment. In International Conference on Learning Representations, 2020.   \n[43] Dezhen Xiong, Daohui Zhang, Xingang Zhao, and Yiwen Zhao. Deep learning for emg-based human-machine interaction: A review. IEEE/CAA Journal of Automatica Sinica, 8(3):512\u2013533, 2021.   \n[44] Yuanchao Dai, Jing Wu, Yuanzhao Fan, Jin Wang, Jianwei Niu, Fei Gu, and Shigen Shen. Mseva: A musculoskeletal rehabilitation evaluation system based on emg signals. ACM Transactions on Sensor Networks, 19(1):1\u201323, 2022.   \n[45] Yingying Jiao, Yini Deng, Yun Luo, and Bao-Liang Lu. Driver sleepiness detection from eeg and eog signals using gan and lstm networks. Neurocomputing, 408:100\u2013111, 2020.   \n[46] Jiahao Fan, Chenglu Sun, Meng Long, Chen Chen, and Wei Chen. Eognet: a novel deep learning model for sleep stage classification based on single-channel eog signal. Frontiers in Neuroscience, 15:573194, 2021.   \n[47] Saifuddin Mahmud, Xiangxu Lin, and Jong-Hoon Kim. Interface for human machine interaction for assistant devices: A review. In 2020 10th Annual computing and communication workshop and conference (CCWC), pages 0768\u20130773. IEEE, 2020.   \n[48] Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. Timesnet: Temporal 2d-variation modeling for general time series analysis. 2023.   \n[49] Jiecheng Lu, Xu Han, Yan Sun, and Shihao Yang. Cats: Enhancing multivariate time series forecasting by constructing auxiliary time series as exogenous variables. arXiv preprint arXiv:2403.01673, 2024.   \n[50] Magali T Schmidt, Paulo AM Kanda, Luis FH Basile, Helder Frederico da Silva Lopes, Regina Baratho, Jose LC Demario, Mario S Jorge, Antonio E Nardi, Sergio Machado, J\u00e9ssica N Ianof, et al. Index of alpha/theta ratio of the electroencephalogram: a new marker for alzheimer\u2019s disease. Frontiers in aging neuroscience, 5:60, 2013.   \n[51] Shaojie Bai, J Zico Kolter, and Vladlen Koltun. An empirical evaluation of generic convolutional and recurrent networks for sequence modeling. arXiv preprint arXiv:1803.01271, 2018.   \n[52] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. Fedformer: Frequency enhanced decomposed transformer for long-term series forecasting. In International Conference on Machine Learning, pages 27268\u201327286. PMLR, 2022.   \n[53] Yitian Zhang, Liheng Ma, Soumyasundar Pal, Yingxue Zhang, and Mark Coates. Multiresolution time-series transformer for long-term forecasting. In International Conference on Artificial Intelligence and Statistics, pages 4222\u20134230. PMLR, 2024.   \n[54] Yong Liu, Haixu Wu, Jianmin Wang, and Mingsheng Long. Non-stationary transformers: Exploring the stationarity in time series forecasting. Advances in Neural Information Processing Systems, 35:9881\u20139893, 2022.   \n[55] Peng Chen, Yingying ZHANG, Yunyao Cheng, Yang Shu, Yihang Wang, Qingsong Wen, Bin Yang, and Chenjuan Guo. Pathformer: Multi-scale transformers with adaptive pathways for time series forecasting. In The Twelfth International Conference on Learning Representations, 2024.   \n[56] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In International Conference on Learning Representations, 2019.   \n[57] Yifan Yao, Jinhao Duan, Kaidi Xu, Yuanfang Cai, Zhibo Sun, and Yue Zhang. A survey on large language model (llm) security and privacy: The good, the bad, and the ugly. High-Confidence Computing, page 100211, 2024.   \n[58] Alexey Dosovitskiy. An image is worth 16x16 words: Transformers for image recognition at scale. ICLR, 2021.   \n[59] Fudong Lin, Summer Crawford, Kaleb Guillot, Yihe Zhang, Yan Chen, Xu Yuan, Li Chen, Shelby Williams, Robert Minvielle, Xiangming Xiao, et al. Mmst-vit: Climate change-aware crop yield prediction via multi-modal spatial-temporal vision transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5774\u20135784, 2023.   \n[60] Amin Shabani, Amir Abdi, Lili Meng, and Tristan Sylvain. Scaleformer: Iterative multi-scale refining transformers for time series forecasting. arXiv preprint arXiv:2206.04038, 2022.   \n[61] Rushuang Zhou, Lei Lu, Zijun Liu, Ting Xiang, Zhen Liang, David A Clifton, Yining Dong, and Yuan-Ting Zhang. Semi-supervised learning for multi-label cardiovascular diseases prediction: a multi-dataset study. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023.   \n[62] Joseph Y Cheng, Hanlin Goh, Kaan Dogrusoz, Oncel Tuzel, and Erdrin Azemi. Subject-aware contrastive learning for biosignals. arXiv preprint arXiv:2007.04871, 2020.   \n[63] Isabela Albuquerque, Jo\u00e3o Monteiro, Olivier Rosanne, Abhishek Tiwari, Jean-Fran\u00e7ois Gagnon, and Tiago H Falk. Cross-subject statistical shift estimation for generalized electroencephalography-based mental workload assessment. In 2019 IEEE International Conference on Systems, Man and Cybernetics (SMC), pages 3647\u20133653. IEEE, 2019.   \n[64] Chaoqi Yang, M Brandon Westover, and Jimeng Sun. Manydg: Many-domain generalization for healthcare applications. In The Eleventh International Conference on Learning Representations, 2022.   \n[65] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pages 1597\u20131607. PMLR, 2020.   \n[66] Zhihan Yue, Yujing Wang, Juanyong Duan, Tianmeng Yang, Congrui Huang, Yunhai Tong, and Bixiong Xu. Ts2vec: Towards universal representation of time series. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 8980\u20138987, 2022.   \n[67] J Escudero, Daniel Ab\u00e1solo, Roberto Hornero, Pedro Espino, and Miguel L\u00f3pez. Analysis of electroencephalograms in alzheimer\u2019s disease patients with multiscale entropy. Physiological measurement, 27(11):1091, 2006.   \n[68] Hanneke van Dijk, Guido van Wingen, Damiaan Denys, Sebastian Olbrich, Rosalinde van Ruth, and Martijn Arns. The two decades brainclinics research archive for insights in neurophysiology (tdbrain) database. Scientific data, 9(1):333, 2022.   \n[69] Andreas Miltiadous, Katerina D Tzimourta, Theodora Afrantou, Panagiotis Ioannidis, Nikolaos Grigoriadis, Dimitrios G Tsalikakis, Pantelis Angelidis, Markos G Tsipouras, Euripidis Glavas, Nikolaos Giannakeas, et al. A dataset of scalp eeg recordings of alzheimer\u2019s disease, frontotemporal dementia and healthy subjects from routine eeg. Data, 8(6):95, 2023.   \n[70] PhysioToolkit PhysioBank. Physionet: components of a new research resource for complex physiologic signals. Circulation, 101(23):e215\u2013e220, 2000.   \n[71] Patrick Wagner, Nils Strodthoff, Ralf-Dieter Bousseljot, Dieter Kreiseler, Fatima I Lunze, Wojciech Samek, and Tobias Schaeffter. Ptb-xl, a large publicly available electrocardiography dataset. Scientific data, 7(1):1\u201315, 2020.   \n[72] Johannes P\u00f6ppelbaum, Gavneet Singh Chadha, and Andreas Schwung. Contrastive learning based self-supervised time-series analysis. Applied Soft Computing, 117:108397, 2022.   \n[73] Xiang Zhang, Ziyuan Zhao, Theodoros Tsiligkaridis, and Marinka Zitnik. Self-supervised contrastive pre-training for time series via time-frequency consistency. Advances in Neural Information Processing Systems, 35:3988\u20134003, 2022.   \n[74] Prabhat Kumar and S Suresh. Flaap: An open human activity recognition (har) dataset for learning and finding the associated activity patterns. Procedia Computer Science, 212:64\u201373, 2022.   \n[75] Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra, Jorge Luis Reyes-Ortiz, et al. A public domain dataset for human activity recognition using smartphones. In Esann, volume 3, page 3, 2013.   \n[76] Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. Timesnet: Temporal 2d-variation modeling for general time series analysis. In International Conference on Learning Representations, 2023.   \n[77] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Appendix A Data Augmentation Banks ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In the embedding stage, we apply data augmentation to the patch embeddings. We utilize a bank of data augmentation techniques to enhance the model\u2019s robustness and generalization. During the forward pass in training, each patch will pick one augmentation from available augmentation options with equal probability. The data augmentation methods include temporal filpping, channel shuffling, temporal masking, frequency masking, jittering, and dropout, and can be further expanded to more choices. We provide a detailed description of each technique below. ", "page_idx": 15}, {"type": "text", "text": "Temporal Flippling We reverse the MedTS data along the temporal dimension. The probability of applying this augmentation is controlled by a parameter prob, with a default value of 0.5. ", "page_idx": 15}, {"type": "text", "text": "Channel Shuffilng We randomly shuffle the order of MedTS channels. The probability of applying channel shuffling is controlled by the parameter prob, also set by default to 0.5. ", "page_idx": 15}, {"type": "text", "text": "temporal masking We randomly mask some timestamps across all channels. The proportion of timestamps masked is controlled by the parameter ratio, with a default value of 0.1. ", "page_idx": 15}, {"type": "text", "text": "Frequency Masking First introduced in [73] for contrastive learning, this method involves converting the MedTS data into the frequency domain, randomly masking some frequency bands, and then converting it back. The proportion of frequency bands masked is controlled by the parameter ratio, with a default value of 0.1. ", "page_idx": 15}, {"type": "text", "text": "Jittering Random noise, ranging from 0 to 1, is added to the raw data. The intensity of the noise is adjusted by the parameter scale, which is set by default to 0.1. ", "page_idx": 15}, {"type": "text", "text": "Dropout Similar to the dropout layer in neural networks, this method randomly drops some values.   \nThe proportion of values dropped is controlled by the parameter ratio, with a default setting of 0.1. ", "page_idx": 15}, {"type": "text", "text": "Appendix B Data Preprocessing ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "B.1 APAVA Preprocessing ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The Alzheimer\u2019s Patients\u2019 Relatives Association of Valladolid (APAVA) dataset\\*, referenced in the paper [67], is a public EEG time series dataset with 2 classes and 23 subjects, including 12 Alzheimer\u2019s disease patients and 11 healthy control subjects. On average, each subject has $30.0\\,\\pm$ 12.5 trials, with each trial being a 5-second time sequence consisting of 1280 timestamps across 16 channels. Before further preprocessing, each trial is scaled using the standard scaler. Subsequently, we segment each trial into 9 half-overlapping samples, where each sample is a 1-second time sequence comprising 256 timestamps. This process results in 5,967 samples. Each sample has a subject ID to indicate its originating subject. For the training, validation, and test set splits, we employ the subject-independent setup. Samples with subject IDs {15,16,19,20} and {1,2,17,18} are assigned to the validation and test sets, respectively. The remaining samples are allocated to the training set. ", "page_idx": 15}, {"type": "text", "text": "B.2 TDBrain Preprocessing ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The TDBrain dataset\\*, referenced in the paper [68], is a large permission-accessible EEG time series dataset recording brain activities of 1274 subjects with 33 channels. Each subject has two trials: one under eye open and one under eye closed setup. The dataset includes a total of 60 labels, with each subject potentially having multiple labels indicating multiple diseases simultaneously. In this paper, we utilize a subset of this dataset containing 25 subjects with Parkinson\u2019s disease and 25 healthy controls, all under the eye-closed task condition. Each eye-closed trial is segmented into non-overlapping 1-second samples with 256 timestamps, and any samples shorter than 1-second are discarded. This process results in 6,240 samples. Each sample is assigned a subject ID to indicate its originating subject. For the training, validation, and test set splits, we employ the subject-independent setup. Samples with subject IDs {18,19,20,21,46,47,48,49} are assigned to the validation set, while samples with subject IDs {22,23,24,25,50,51,52,53} are assigned to the test set. The remaining samples are allocated to the training set. ", "page_idx": 15}, {"type": "text", "text": "B.3 ADFTD Preprocessing ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The Alzheimer\u2019s Disease and FronTotemporal Dementia (ADFTD) dataset\\*, referenced in the papers [69, 19], is a public EEG time series dataset with 3 classes, including 36 Alzheimer\u2019s disease (AD) patients, 23 Frontotemporal Dementia (FTD) patients, and 29 healthy control (HC) subjects. The dataset has 19 channels, and the raw sampling rate is $500\\mathrm{Hz}$ . Each subject has a trial, with trial durations of approximately 13.5 minutes for AD subjects $_\\mathrm{min}{=}5.1$ , max $=\\!21.3$ ), 12 minutes for FD subjects $\\mathrm{min}{=}7.9$ , max $_{=16.9}$ ), and 13.8 minutes for HC subjects $(\\mathrm{min}{=}12.5$ , $\\scriptstyle\\mathtt{m a x=16.5}$ ). A bandpass filter between $0.5{-}45\\mathrm{Hz}$ is applied to each trial. We downsample each trial to $256\\mathrm{Hz}$ and segment them into non-overlapping 1-second samples with 256 timestamps, discarding any samples shorter than 1 second. This process results in 69,752 samples. For the training, validation, and test set splits, we employ both the subject-dependent and subject-independent setups. For the subject-dependent setup, we allocate $60\\%$ , $20\\%$ , and $20\\%$ of total samples into the training, validation, and test sets, respectively. For the subject-independent setup, we allocate $60\\%$ , $20\\%$ , and $20\\%$ of total subjects with their corresponding samples into the training, validation, and test sets, respectively. ", "page_idx": 16}, {"type": "text", "text": "B.4 PTB Preprocessing ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The PTB dataset\\*, referenced in the paper [70], is a public ECG time series recording from 290 subjects, with 15 channels and a total of 8 labels representing 7 heart diseases and 1 health control. The raw sampling rate is $1000\\mathrm{Hz}$ . For this paper, we utilize a subset of 198 subjects, including patients with Myocardial infarction and healthy control subjects. We first downsample the sampling frequency to $250\\mathrm{Hz}$ and normalize the ECG signals using standard scalers. Subsequently, we process the data into single heartbeats through several steps. We identify the R-Peak intervals across all channels and remove any outliers. Each heartbeat is then sampled from its R-Peak position, and we ensure all samples have the same length by applying zero padding to shorter samples, with the maximum duration across all channels serving as the reference. This process results in 64,356 samples. For the training, validation, and test set splits, we employ the subject-independent setup. Specifically, we allocate $60\\%$ , $20\\%$ , and $20\\%$ of the total subjects, along with their corresponding samples, into the training, validation, and test sets, respectively. ", "page_idx": 16}, {"type": "text", "text": "B.5 PTB-XL Preprocessing ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The PTB-XL dataset\\*, referenced in the paper [71], is a large public ECG time series dataset recorded from 18,869 subjects, with 12 channels and 5 labels representing 4 heart diseases and 1 healthy control category. Each subject may have one or more trials. To ensure consistency, we discard subjects with varying diagnosis results across different trials, resulting in 17,596 subjects remaining. The raw trials consist of 10-second time intervals, with sampling frequencies of $100\\mathrm{Hz}$ and $500\\mathrm{Hz}$ versions. For our paper, we utilize the $500\\mathrm{Hz}$ version, then we downsample to $250\\mathrm{Hz}$ and normalize using standard scalers. Subsequently, each trial is segmented into non-overlapping 1-second samples with 250 timestamps, discarding any samples shorter than 1 second. This process results in 191,400 samples. For the training, validation, and test set splits, we employ the subject-independent setup. Specifically, we allocate $60\\%$ , $20\\%$ , and $20\\%$ of the total subjects, along with their corresponding samples, into the training, validation, and test sets, respectively. ", "page_idx": 16}, {"type": "text", "text": "Appendix C Implementation Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We implement our method and all the baselines based on the Time-Series-Library project\\* from Tsinghua University [76], which integrates all methods under the same framework and training techniques to ensure a relatively fair comparison. The 10 baseline time series transformer methods are Autoformer [28], Crossformer [33], FEDformer [52], Informer [29], iTransformer [31], MTST [53], Nonformer [54], PatchTST [32], Reformer [56], and vanilla Transformer [30]. ", "page_idx": 16}, {"type": "text", "text": "For all methods, we employ 6 layers for the encoder, with the self-attention dimension $D$ set to 128 and the hidden dimension of the feed-forward networks set to 256. The optimizer used is Adam, with a learning rate of 1e-4. The batch size is set to {32,32,128,128,128} for the datasets APAVA, TDBrain, ADFD, PTB, and PTB-XL, respectively. Training is conducted for 100 epochs, with early stopping triggered after 10 epochs without improvement in the F1 score on the validation set. We save the model with the best F1 score on the validation set and evaluate it on the test set. We employ six evaluation metrics: accuracy, precision (macro-averaged), recall (macro-averaged), F1 score (macroaveraged), AUROC (macro-averaged), and AUPRC (macro-averaged). Both subject-dependent and subject-independent setups are implemented for different datasets. Each experiment is run with 5 random seeds (41-45) and fixed training, validation, and test sets to compute the average results and standard deviations. ", "page_idx": 17}, {"type": "text", "text": "Medformer (Our Method) We use a list of patch lengths in patch embedding to generate patches with different granularities. Instead of flattening the patches and mapping them to dimension $D$ during patch embedding, we use a conv2d network to directly map patches into a 1-D representation with dimension $D$ . These patch lengths can vary, including different numbers of patch lengths such as $\\{2,4,8,16\\}$ , repetitive numbers such as $\\{8,8,8,8\\}$ , or a mix of different and repetitive lengths such as $\\{8,8,8,16,16,16\\}$ . It is also possible to use only one patch length, such as $\\{8\\}$ , which indicates a single granularity. The patch lists used for the datasets APAVA, TDBrain, ADFD, PTB, and PTB-XL are $\\{2,2,2,4,4,4,16,16,16,32,32,32,32,32\\},$ , $\\{8,8,8,16,16,16\\}$ , $\\{2,4,8,8,16,16,16,16,32,32,32,32,32,32,32,32\\}$ , $\\{2,4,8,8,16,16,16,32,32,32,32,32\\}$ , and $\\{2,4,8,8,16,16,16,16,32,32,32,32,32,32,32,32\\}$ , respectively. The data augmentations are randomly chosen from a list of four possible options: none, jitter, scale, and mask. The number following each augmentation method indicates the degree of augmentation. Detailed descriptions of these methods can be found in Appendix A. The augmentation methods used for the datasets APAVA, TDBrain, ADFD, PTB, and PTB-XL are {none, drop0.35}, {none, drop0.25}, {drop0.5}, {drop0.5}, and {jitter0.2, scale0.2, drop0.5}, respectively. ", "page_idx": 17}, {"type": "text", "text": "Autoformer Autoformer [28] employs an auto-correlation mechanism to replace self-attention for time series forecasting. Additionally, they use a time series decomposition block to separate the time series into trend-cyclical and seasonal components for improved learning. The raw source code is available at https://github.com/thuml/Autoformer. ", "page_idx": 17}, {"type": "text", "text": "Crossformer Crossformer [33] designs a single-channel patching approach for token embedding. They utilize two-stage self-attention to leverage both temporal features and channel correlations. A router mechanism is proposed to reduce time and space complexity during the cross-dimension stage. The raw code is available at https://github.com/Thinklab-SJTU/Crossformer. ", "page_idx": 17}, {"type": "text", "text": "FEDformer FEDformer [52] leverages frequency domain information using the Fourier transform. They introduce frequency-enhanced blocks and frequency-enhanced attention, which are computed in the frequency domain. A novel time series decomposition method replaces the layer norm module in the transformer architecture to improve learning. The raw code is available at https: //github.com/MAZiqing/FEDformer. ", "page_idx": 17}, {"type": "text", "text": "Informer Informer [29] is the first paper to employ a one-forward procedure instead of an autoregressive method in time series forecasting tasks. They introduce ProbSparse self-attention to reduce complexity and memory usage. The raw code is available at https://github.com/zhouhaoyi/Informer2020. ", "page_idx": 17}, {"type": "text", "text": "iTransformer iTransformer [31] questions the conventional approach of embedding attention tokens in time series forecasting tasks and proposes an inverted approach by embedding the whole series of channels into a token. They also invert the dimension of other transformer modules, such as the layer norm and feed-forward networks. The raw code is available at https://github.com/thuml/iTransformer. ", "page_idx": 17}, {"type": "text", "text": "MTST MTST [53] uses the same token embedding method as Crossformer and PatchTST. It highlights the importance of different patching lengths in forecasting tasks and designs a method that can take different sizes of patch tokens as input simultaneously. The raw code is available at https://github.com/networkslab/MTST. ", "page_idx": 17}, {"type": "text", "text": "Nonformer Nonformer [54] analyzes the impact of non-stationarity in time series forecasting tasks and its significant effect on results. They design a de-stationary attention module and incorporate normalization and denormalization steps before and after training to alleviate the over-stationarization problem. The raw code is available at https://github.com/thuml/Nonstationary_Transformers. ", "page_idx": 17}, {"type": "table", "img_path": "jfkid2HwNr/tmp/3d86affb66b64b41be488cdf2bf492a6cb3380c7c94bb14d159c37a78d73672c.jpg", "table_caption": ["Table 5: Module Study. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "PatchTST PatchTST [32] embeds a sequence of single-channel timestamps as a patch token to replace the attention token used in the vanilla transformer. This approach enlarges the receptive field and enhances forecasting ability. The raw code is available at https://github.com/yuqinie98/PatchTST. ", "page_idx": 18}, {"type": "text", "text": "Reformer Reformer [56] replaces dot-product attention with locality-sensitive hashing. They also use a reversible residual layer instead of standard residuals. The raw code is available at https: //github.com/lucidrains/reformer-pytorch. ", "page_idx": 18}, {"type": "text", "text": "Transformer Transformer [30], commonly known as the vanilla transformer, is introduced in the well-known paper \"Attention is All You Need.\" It can also be applied to time series by embedding each timestamp of all channels as an attention token. The PyTorch version of the code is available at https://github.com/jadore801120/attention-is-all-you-need-pytorch. ", "page_idx": 18}, {"type": "text", "text": "Appendix D Ablation Study ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "D.1 Module Study ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "To assess the efficacy of our proposed mechanisms\u2014inter-granularity self-attention, embedding augmentation, and multi-channel patching\u2014we conduct ablation studies on five datasets across three distinct settings: without inter-granularity attention, without embedding augmentation, and with single-channel patching. We maintain the other two modules intact in each setting and fix all hyperparameters as described in the implementation details C. Table 5 presents a comparison between our full Medformer model and these three variants. The complete Medformer model secures 28 top-1 and 30 top-2 rankings across 30 evaluations, demonstrating robust performance. We observe that each module significantly enhances performance: on average, across the datasets, inter-granularity attention contributes to a $3.64\\%$ improvement in F1 score, embedding augmentation leads to a $4.46\\%$ increase and multi-channel patching results in a $6.10\\%$ enhancement in F1 score. We find multi-channel patching particularly beneficial for results, especially in EEG data. Overall, these results underscore the critical role of each component in our design. ", "page_idx": 18}, {"type": "text", "text": "D.2 Patch Length Study ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "To investigate the effects of multi-granularity and computational complexity, we conduct an empirical analysis using various patch lengths on the APAVA dataset. Table 6 presents the evaluation results for different combinations of patch lengths. Initially, we compare the performance of models using a single patch length against models using five identical patch lengths (e.g., {8} vs $\\{8,8,8,8,8\\};$ ). Our findings indicate that using repetitive patch lengths generally enhances performance, except when ", "page_idx": 18}, {"type": "table", "img_path": "jfkid2HwNr/tmp/55242e619b28b9032afb3b02a4c4bb2cf61274981a09a7b706892736b9c39b4c.jpg", "table_caption": ["Table 6: Patch Length Study "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "$L=2$ , suggesting that additional identical patch lengths can capture more information, analogous to multi-head attention mechanisms. ", "page_idx": 19}, {"type": "text", "text": "Furthermore, we assess the performance of a manually selected combination of varying patch lengths, specifically $\\{2,2,4,16,32\\}$ . This configuration achieves the highest performance across all evaluated metrics, underscoring the effectiveness of our designed attention module in accommodating multigranularity patches. However, it is worth noting that mixing different patch lengths does not guarantee improved performance. See G for more detailed discussion. ", "page_idx": 19}, {"type": "text", "text": "Appendix E Additional Experiments ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Table 7: Additional Datasets and Methods We selected three old baselines in the previous experiments that showed strong performance: Crossformer, Reformer, and Transformer. Additionally, we introduce three new baselines: TCN, ModernTCN, and Mamba. These six baselines are evaluated on one old dataset in the previous experiments, TDBrain(6,240 samples, 2 classes), and two new HAR datasets: FLAAP (13,123 samples, 10 classes) and UCI-HAR (10,299 samples, 6 classes). The bold number denotes the best result, and the underlined number denotes the second best. ", "page_idx": 19}, {"type": "table", "img_path": "jfkid2HwNr/tmp/f7fc3fc65d6b5c4e204389707d08dada8fbbf5fc65f1dbc06a436c94176ab119.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "To evaluate the performance of our method on general time series, we test it on two human activity recognition (HAR) datasets: FLAAP [74] and UCI-HAR [75], which exhibit potential channel correlations inherently. Additionally, we compare our method with three other approaches: TCN [51], ModernTCN [16], and Mamba [77]. Our method achieves the highest top-1 accuracy and F1 score on TDBrain and FLAAP, and ranks second-best on UCI-HAR. ", "page_idx": 19}, {"type": "text", "text": "Appendix F Complexity Analysis ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Let the number of timestamps $T$ , and patch list $\\{L_{1},L_{2},\\ldots,L_{n}\\}$ be given, where the $i$ -th patch length $L_{i}$ produce $N_{i}=\\lceil T/L_{i}\\rceil$ number of patches. During intra-granularity attention, we perform self-attention among the patch embeddings within the same granularity. The total complexity is ", "page_idx": 19}, {"type": "text", "text": "$O\\left(\\sum_{i=1}^{n}N_{i}^{2}\\right)$ . During intra-granularity attention, we perform self-attention among $n$ routers, with a time complexity of $O(n^{2})$ . Therefore, the total time complexity is $\\begin{array}{r}{O\\left(n^{2}+\\sum_{i=1}^{n}N_{i}^{2}\\right)}\\end{array}$ . ", "page_idx": 20}, {"type": "text", "text": "One potentially useful patch list is the power series $\\{2^{1},2^{2},\\ldots2^{n}\\}$ , where $2^{n}<T$ . In this case, the complexity of intra-granularity attention reduces as follows: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\cal O}\\left(\\displaystyle\\sum_{i=1}^{n}N_{i}^{2}\\right)={\\cal O}\\left(\\displaystyle\\sum_{i=1}^{n}\\left\\lceil\\frac{T}{2^{i}}\\right\\rceil^{2}\\right)\\le{\\cal O}\\left(\\displaystyle\\sum_{i=1}^{n}\\left(\\displaystyle\\frac{T}{2^{i}}+1\\right)^{2}\\right)}\\\\ &{={\\cal O}\\left(\\displaystyle\\sum_{i=1}^{n}\\left(\\displaystyle\\frac{T^{2}}{2^{2i}}+2\\displaystyle\\frac{T}{2^{i}}+1\\right)\\right)={\\cal O}\\left(T^{2}\\displaystyle\\sum_{i=1}^{n}\\displaystyle\\frac{1}{2^{2i}}+2T\\displaystyle\\sum_{i=1}^{n}\\displaystyle\\frac{1}{2^{i}}+n\\right)}\\\\ &{\\le{\\cal O}\\left(\\displaystyle\\frac{1}{3}T^{2}+2T+\\log T\\right)={\\cal O}(T^{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The complexity of inter-granularity attention is $O(n^{2})\\,\\leq\\,O(\\log^{2}T)$ . Therefore, the total time complexity of the two-stage multi-granularity self-attention module is $O(T^{2})$ , which is the same complexity as the vanilla transformer. This analysis demonstrates our model\u2019s ability to incorporate different granularities without significantly increasing computational overhead. ", "page_idx": 20}, {"type": "text", "text": "Appendix G Discussion ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "G.1 Comparision with Other Multi-Granularity Methods ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "MTST [53] and Pathformer [55] differ from our Medformer in three significant aspects: (1) Patching & Embedding MTST and Pathformer utilize single-channel patching, presupposing channel independence. In contrast, Medformer employs multi-channel patching to capture potential channel correlations. (2) Granularity Interactions MTST assimilates multi-granularity information by concatenating outputs from different branches, while Pathformer uses adaptive pathways for weighted aggregation of these outputs without any inter-granularity interactions within the attention modules. In contrast, Medformer introduces a novel inter-granularity attention mechanism specifically designed for granularity interaction, thereby effectively integrating multi-granularity information. ", "page_idx": 20}, {"type": "text", "text": "Scaleformer [60] operates as a model-agnostic structural framework that employs variable downsampling and upsampling rates on embeddings outside of attention modules. Although it integrates seamlessly with non-patching methods like Autoformer and FEDformer, its incorporation into patching methods is not straightforward and may result in sub-optimal patch representations [53]. Consequently, the design objectives of Scaleformer are largely orthogonal to ours, which concentrate on multi-granularity patching and attention mechanisms. ", "page_idx": 20}, {"type": "text", "text": "Appendix H Broader Impacts ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Our proposed model demonstrates performance comparable to or surpassing state-of-the-art baselines on medical time series classification tasks. The model\u2019s design, which includes specialized patching and self-attention mechanisms, specifically targets channel correlations and multi-granularity information. We anticipate our findings will encourage further research into effective strategies for capturing multi-scale information in medical time series data. Additionally, this work could broaden interest in medical time series classification, an area that remains less explored compared to time series forecasting. ", "page_idx": 20}, {"type": "text", "text": "Besides, different experiment setups based on medical perspectives, such as subject-dependent and subject-independent, are evaluated to simulate real-world applications. On a societal level, our model has potential applications in healthcare, such as facilitating the diagnosis of diseases using medical time series data. For instance, it could be employed to detect neurological disorders through EEG data. However, practitioners should be cognizant of the model\u2019s limitations. ", "page_idx": 20}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The claims made in abstract and introduction are supported by the results in Section 5. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: See Appendix G. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: This paper does not include theoretical results. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: See Section 5 and Appendix C. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 22}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We include an anonymous link in the Abstract providing source codes with full implementation details for our methods and all baselines. All five datasets used for evaluation are publicly available. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 23}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: See Section 5 and Appendix C. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 23}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: We run experiments over five random seeds and report the average value with the standard deviation. See Table 2-6. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 23}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 24}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: See Section C. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The authors follow the NeurIPS Code of Ethics during the conduct of this research. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 24}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: See Appendix H. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 24}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 25}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: This paper does not pose a high risk of misuse. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 25}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We properly cite baseline models and the code library used for implementation.   \nSee Appendix C. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: This paper does not publish new assets. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 26}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: This paper does not include crowdsourcing experiments nor research with human subjects. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 26}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: This paper does not include crowdsourcing experiments nor research with human subjects. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 26}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}]