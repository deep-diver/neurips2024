[{"figure_path": "jfkid2HwNr/figures/figures_1_1.jpg", "caption": "Figure 1: Token embedding methods. Vanilla transformer, Autoformer, and Informer [30, 28, 29] employ a single cross-channel timestamp as a token; iTransformer [31] utilizes an entire channel as a token; and PatchTST and Crossformer [32, 33] adopt a patch of timestamps from one channel as a token. For MedTS classification, we propose Medformer considering inter-channel dependencies (cross-channel), temporal properties (multi-timestamp), and multifaceted scale of temporal patterns (multi-granularity).", "description": "This figure compares different token embedding methods used in time series transformers. Vanilla Transformer, Autoformer, and Informer use a single timestamp from all channels. iTransformer uses all timestamps from a single channel. PatchTST and Crossformer use multiple timestamps from a single channel.  The authors propose Medformer which uses multiple timestamps from all channels, incorporating cross-channel information and multiple temporal scales.", "section": "1 Introduction"}, {"figure_path": "jfkid2HwNr/figures/figures_3_1.jpg", "caption": "Figure 2: Subject-dependent/independent setups (figure adopted from our previous work [13]). In the subject-dependent setup, samples from the same subject can appear in both the training and test sets, causing information leakage. In a subject-independent setup, samples from the same subject are exclusively in either the training or test set, which is more challenging and practically meaningful but less studied.", "description": "This figure illustrates the difference between subject-dependent and subject-independent experimental setups for medical time series classification. In subject-dependent setup, data from the same subject may appear in both training and testing sets, leading to potential information leakage. The subject-independent setup is more realistic as it simulates real-world scenarios where the model is tested on unseen subjects, making it more challenging but also more meaningful.", "section": "3 Preliminaries and Problem Formulation"}, {"figure_path": "jfkid2HwNr/figures/figures_4_1.jpg", "caption": "Figure 3: Overview of Medformer. a) Workflow. b) For the input sample xin, we apply n distinct patch lengths in parallel to create patched features \u00e6\u00ba), where i ranges from 1 to n. Each patch length represents a unique granularity. These patches are then projected into \u00e6 and subsequently augmented to form (?). c) We obtain the final embedding \u00e6(i) by combining the augmented with both the positional embedding Wpos and the granularity embedding W\u00b2). Additionally, a granularity-specific router u(\u00b9) is designed to capture integrated information for each respective granularity. We then perform intra-granularity self-attention, focusing on individual granularities, and inter-granularity self-attention, using the routers to facilitate communication across different granularities.", "description": "This figure illustrates the architecture of Medformer, a multi-granularity patching transformer for medical time-series classification. It shows the workflow of the model, the cross-channel multi-granularity patch embedding mechanism, and the multi-granularity self-attention mechanism.  The workflow diagram details the processing steps from input time series to output. The patch embedding section highlights how Medformer handles multiple granularities of temporal information simultaneously, across different channels. Finally, the self-attention section depicts how the model learns intra-granularity and inter-granularity relationships via a two-stage attention mechanism.", "section": "4 Method"}, {"figure_path": "jfkid2HwNr/figures/figures_9_1.jpg", "caption": "Figure 1: Token embedding methods. Vanilla transformer, Autoformer, and Informer [30, 28, 29] employ a single cross-channel timestamp as a token; iTransformer [31] utilizes an entire channel as a token; and PatchTST and Crossformer [32, 33] adopt a patch of timestamps from one channel as a token. For MedTS classification, we propose Medformer considering inter-channel dependencies (cross-channel), temporal properties (multi-timestamp), and multifaceted scale of temporal patterns (multi-granularity).", "description": "This figure compares different token embedding methods used in various transformer-based time series models.  It highlights the limitations of existing methods in capturing the unique characteristics of medical time series data, such as cross-channel correlations and multi-scale temporal patterns. The figure shows how Medformer addresses these limitations by incorporating cross-channel patching, multi-timestamp embedding, and multi-granularity to learn features more effectively for medical time series classification.", "section": "2 Related Work"}]