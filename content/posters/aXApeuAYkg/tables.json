[{"figure_path": "aXApeuAYkg/tables/tables_6_1.jpg", "caption": "Table 1: Evaluation of adapted XLSR models on the 10-min ML-SUPERB and VoxCeleb dataset for LID, ASR, and SV tasks. These evaluations test the encoder's generalizability across different tasks, demonstrating effectiveness without further task-specific tuning.", "description": "This table presents the results of evaluating adapted XLSR models on three different tasks: Language Identification (LID), Automatic Speech Recognition (ASR), and Speaker Verification (SV).  The models were initially adapted for either LID or ASR and then evaluated on both the adapted task and a task they weren't trained for.  The purpose is to demonstrate how well the adapted encoder generalizes to unseen tasks without any further fine-tuning.  The table shows LID accuracy and ASR Character Error Rate (CER) for both normal and few-shot conditions (for LID and ASR) and Speaker Verification Equal Error Rate (EER) for SV. Bottleneck dimensions of the adapted models are also included.", "section": "5.1 Generalization Ability on Unseen Tasks"}, {"figure_path": "aXApeuAYkg/tables/tables_6_2.jpg", "caption": "Table 1: Evaluation of adapted XLSR models on the 10-min ML-SUPERB and VoxCeleb dataset for LID, ASR, and SV tasks. These evaluations test the encoder's generalizability across different tasks, demonstrating effectiveness without further task-specific tuning.", "description": "This table presents the results of experiments evaluating the generalization ability of adapted XLSR models on three speech processing tasks: Language Identification (LID), Automatic Speech Recognition (ASR), and Speaker Verification (SV).  The models were adapted for either LID or ASR and then evaluated on both the adapted task and an unseen task. The table shows the performance metrics for each task (LID accuracy, ASR Character Error Rate (CER), and SV Equal Error Rate (EER) and Detection Cost Function (DCF)) for both \"normal\" (10 minutes of data per language) and few-shot (five utterances per language) conditions. The \"Bottleneck Dims\" column shows the dimensionality of the bottleneck layers used in the adaptation methods. The results highlight the effectiveness of the proposed CA-XLSR model in achieving strong generalization across different tasks.", "section": "5 Experiments and Results"}, {"figure_path": "aXApeuAYkg/tables/tables_7_1.jpg", "caption": "Table 2: ASR CER(%) and LID Acc (%) in ML-SUPERB 10min. and 1h. sets, comparing different layers to generate the language embedding to condition the following layers. We adapt the XLSR model for LID and ASR tasks.", "description": "This table presents the results of experiments comparing different configurations of the CA-SSLR model on the ML-SUPERB benchmark.  Specifically, it shows the impact of varying the number of layers used to generate the language embedding that is used to condition subsequent layers. The experiment uses the XLSR model adapted for LID and ASR tasks. The table reports the Real-Time Factor (RTF), the relative improvement in RTF, LID accuracy (ACC), and ASR Character Error Rate (CER) for both 10-minute and 1-hour configurations of the dataset, broken down further by \"Normal\" and \"Few-shots\" language subsets. This table demonstrates the effectiveness of using hierarchical conditioning to improve both LID and ASR performance.", "section": "5.2 Condition-Aware SSLR Model"}, {"figure_path": "aXApeuAYkg/tables/tables_7_2.jpg", "caption": "Table 1: Evaluation of adapted XLSR models on the 10-min ML-SUPERB and VoxCeleb dataset for LID, ASR, and SV tasks. These evaluations test the encoder's generalizability across different tasks, demonstrating effectiveness without further task-specific tuning.", "description": "This table presents the results of experiments evaluating the generalization ability of adapted XLSR models on three different speech processing tasks: Language Identification (LID), Automatic Speech Recognition (ASR), and Speaker Verification (SV).  The models were adapted for a single task (either LID or ASR) and then evaluated on both the task they were adapted for and an unseen task. This setup helps assess the model's ability to generalize to new tasks without needing additional task-specific training. The table shows the performance metrics (accuracy, character error rate, equal error rate, and detection cost function) for both normal and few-shot conditions of the datasets.", "section": "5.1 Generalization Ability on Unseen Tasks"}, {"figure_path": "aXApeuAYkg/tables/tables_9_1.jpg", "caption": "Table 4: Ablation study of condition-aware settings for ASR-adapted XLSR models on 10-min ML-SUPERB dataset, using CC or TCAC. Conditioning is based on predicted language labels or LID embeddings, except in the ground truth (G.T.) experiment.", "description": "This table presents the results of an ablation study that investigates the impact of different condition-aware settings on the performance of ASR-adapted XLSR models. The study uses the 10-minute ML-SUPERB dataset and compares the performance of models using different conditioning methods (G.T. CC, Hard CC, Soft CC, Embed CC, and Embed TCAC). The results are reported in terms of Character Error Rate (CER) for both normal and few-shot languages.", "section": "5.4 Analysis of the TCA Conditioner"}, {"figure_path": "aXApeuAYkg/tables/tables_12_1.jpg", "caption": "Table 5: Hyper-parameters used for training ASR, LID, and SV decoder models", "description": "This table details the hyperparameters used for training the decoder models for Automatic Speech Recognition (ASR), Language Identification (LID), and Speaker Verification (SV). It specifies parameters such as feature projection dimensions, the number of decoder layers, hidden channel counts, dropout rates, loss functions, learning rates, warmup steps, effective batch sizes, iterations per epoch, and the total number of epochs for each task. These settings are crucial for optimizing the performance of each decoder model.", "section": "A.1 Decoder Models for ASR, LID, and SV"}, {"figure_path": "aXApeuAYkg/tables/tables_12_2.jpg", "caption": "Table 6: Training hyper-parameters for CA-SSLR models. The superscripts \u201cDec\u201d and \u201cFeat\u201d represent the decoder and the feature projection layer, respectively.", "description": "This table details the hyperparameters used for training the CA-SSLR models.  It shows the training data used (ML-SUPERB and/or VoxCeleb), the dimensionality of the language and speaker condition embeddings, the dropout rate for these embeddings, the initialization method for the model components (ASR, LID, and SV decoders), the trainable model parts (decoders, feature projection layers, and adapters), the loss functions used, the learning rate, batch size, number of iterations per epoch, and the number of epochs trained.", "section": "A.2 CA-SSLR Hierarchical Models"}, {"figure_path": "aXApeuAYkg/tables/tables_13_1.jpg", "caption": "Table 7: Data configurations for the original and extended few-shot conditions in ML-SUPERB.", "description": "This table shows the data configuration for the original and extended few-shot conditions used in the ML-SUPERB benchmark. It specifies the amount of data per language (10 minutes or 1 hour) for normal languages, and for the original few-shot languages,  the LID training data was not presented in the result while ASR only used 5 utterances. For the extended few-shot languages, LID training utilized 10 minutes or 1 hour of data (with language labels only) and ASR training still used 5 utterances.", "section": "4.1 Datasets"}, {"figure_path": "aXApeuAYkg/tables/tables_14_1.jpg", "caption": "Table 8: Comparison of trainable parameters and CERs on ML-SUPERB 10min dataset, including fine-tuning top layers, LoRA, and dual-inference condition aware mHuBERT model.", "description": "This table compares different model adaptation techniques on the ML-SUPERB 10-minute dataset, focusing on the trade-off between the number of trainable parameters and the resulting Character Error Rate (CER) for both normal and few-shot language scenarios.  The approaches compared include fine-tuning various numbers of layers (FT (2L) to FT (12L)), the Low-Rank Adaptation (LoRA) technique, and two versions of the proposed Condition-Aware mHuBERT model (CA-mHubertdual and CA-mHubertdualLS). The table demonstrates the effectiveness of the proposed method in achieving lower CER with fewer parameters, especially in the few-shot setting.", "section": "5.1 Generalization Ability on Unseen Tasks"}, {"figure_path": "aXApeuAYkg/tables/tables_14_2.jpg", "caption": "Table 9: Comparison of adaptation methods in terms of bottleneck dimensions, training speed, and peak memory usage.", "description": "This table compares three different model adaptation methods: Houlsby Adapter, CA-SSLR, and Fine-tuning.  For each method, it shows the bottleneck dimensions used, the training time required (in minutes), and the peak GPU memory usage (in GB). The results highlight the relative efficiency and resource requirements of each method.", "section": "C Training Efficiency and Resource Usage"}, {"figure_path": "aXApeuAYkg/tables/tables_15_1.jpg", "caption": "Table 10: RTF for different components.", "description": "This table shows the real-time factor (RTF) for individual components of the proposed CA-SSLR model, as well as for the XLSR and mHubert SSL models, with and without conditioning.  It helps in understanding the computational overhead introduced by each component of the model, comparing different conditioning strategies and different base models.", "section": "5.1 Generalization Ability on Unseen Tasks"}, {"figure_path": "aXApeuAYkg/tables/tables_15_2.jpg", "caption": "Table 1: Evaluation of adapted XLSR models on the 10-min ML-SUPERB and VoxCeleb dataset for LID, ASR, and SV tasks. These evaluations test the encoder's generalizability across different tasks, demonstrating effectiveness without further task-specific tuning.", "description": "This table presents the results of experiments evaluating the generalization ability of adapted XLSR models on three different speech processing tasks: Language Identification (LID), Automatic Speech Recognition (ASR), and Speaker Verification (SV).  The models were adapted for either LID or ASR and then evaluated on both the adapted task and an unseen task.  The table shows the performance (accuracy or error rate) on both the normal and few-shot datasets for each task and adaptation method. This demonstrates the model's ability to generalize to unseen tasks with minimal additional training.", "section": "5.1 Generalization Ability on Unseen Tasks"}, {"figure_path": "aXApeuAYkg/tables/tables_15_3.jpg", "caption": "Table 1: Evaluation of adapted XLSR models on the 10-min ML-SUPERB and VoxCeleb dataset for LID, ASR, and SV tasks. These evaluations test the encoder's generalizability across different tasks, demonstrating effectiveness without further task-specific tuning.", "description": "This table presents the results of evaluating adapted XLSR models on three different speech processing tasks: Language Identification (LID), Automatic Speech Recognition (ASR), and Speaker Verification (SV).  The models were adapted for a single task (either LID or ASR) and then evaluated on both the adapted task and an unseen task to assess their generalization abilities.  The table shows the performance (accuracy or error rate) for each task and model variant, highlighting the effectiveness of the adaptation methods without needing further task-specific fine-tuning.", "section": "5.1 Generalization Ability on Unseen Tasks"}, {"figure_path": "aXApeuAYkg/tables/tables_17_1.jpg", "caption": "Table 1: Evaluation of adapted XLSR models on the 10-min ML-SUPERB and VoxCeleb dataset for LID, ASR, and SV tasks. These evaluations test the encoder's generalizability across different tasks, demonstrating effectiveness without further task-specific tuning.", "description": "This table presents the results of experiments evaluating the generalization ability of adapted XLSR models across three different speech processing tasks: Language Identification (LID), Automatic Speech Recognition (ASR), and Speaker Verification (SV).  The models were adapted for a single task (either LID or ASR) and then evaluated on both the adapted task and an unseen task. The table shows the performance (LID accuracy, ASR Character Error Rate (CER), and SV Equal Error Rate (EER)) for different adaptation methods, including full fine-tuning, Houlsby adapters, and the proposed CA-SSLR approach. The results demonstrate the effectiveness of the CA-SSLR approach in improving generalization performance across different tasks compared to traditional adaptation methods.", "section": "5 Experiments and Results"}]