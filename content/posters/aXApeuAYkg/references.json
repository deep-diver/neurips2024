{"references": [{"fullname_first_author": "Alexei Baevski", "paper_title": "wav2vec 2.0: A framework for self-supervised learning of speech representations", "publication_date": "2020-12-01", "reason": "This paper introduces wav2vec 2.0, a foundational self-supervised learning model for speech processing that significantly impacts the field and is directly referenced in the study's introduction as a pioneering model."}, {"fullname_first_author": "Arun Babu", "paper_title": "Xls-r: Self-supervised cross-lingual speech representation learning at scale", "publication_date": "2021-11-01", "reason": "This work introduces the XLSR model, a multilingual self-supervised speech representation model which serves as a major component of the research and is explicitly mentioned as a key model in the paper's methodology."}, {"fullname_first_author": "Wei-Ning Hsu", "paper_title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units", "publication_date": "2021-01-01", "reason": "The HuBERT model, introduced in this paper, is another key self-supervised learning model for speech, forming a core component of the research and is specifically mentioned in the paper's related work section."}, {"fullname_first_author": "Sanyuan Chen", "paper_title": "Wavlm: Large-scale self-supervised pre-training for full stack speech processing", "publication_date": "2022-01-01", "reason": "This paper presents WavLM, yet another influential self-supervised model used in the study, forming part of the baseline and is highlighted in the related work section."}, {"fullname_first_author": "Jiatong Shi", "paper_title": "Ml-superb: Multilingual speech universal performance benchmark", "publication_date": "2023-01-01", "reason": "The ML-SUPERB benchmark, detailed in this paper, is the primary dataset used in the research, and its significance is highlighted in the introduction and experimental setup sections, making it critical to the study's methodology."}]}