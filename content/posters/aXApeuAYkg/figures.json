[{"figure_path": "aXApeuAYkg/figures/figures_2_1.jpg", "caption": "Figure 1: CA-SSLR scheme and its time-channel attention conditioner. Only the conditioner and linear projections for the decoders are trainable, and all other parameters are frozen during adaptation.", "description": "This figure shows the architecture of the CA-SSLR model and a detailed view of its time-channel attention conditioner.  The main model consists of a generalist encoder (a pre-trained SSL model whose weights remain frozen during adaptation) that receives speech input. The encoder's output is fed into multiple conditional adapters, one each for language identification and speaker verification.  These adapters use intermediate embeddings from earlier layers to dynamically adjust internal representations of the encoder.  The figure also highlights that only the conditioner and decoder projections are trainable during adaptation; other encoder weights remain frozen, preventing catastrophic forgetting.", "section": "3 Methodology"}, {"figure_path": "aXApeuAYkg/figures/figures_3_1.jpg", "caption": "Figure 2: Architecture of the CA-SSLR model employing hierarchical self-conditioning with Time-Channel Attention Conditioners (TCACs).", "description": "This figure illustrates the hierarchical self-conditioning mechanism in the CA-SSLR model. The SSL encoder is divided into layer groups, and TCACs are inserted after the attention module in each layer to modulate hidden representations based on updated conditioning features from LID and SV decoders. The model aggregates SSL features through a weighted sum, combining outputs from all preceding layer groups.  These features are fed into the LID and SV decoders, which extract and process the information to create conditioning features for the TCACs. The process is repeated at intervals, refining representations and dynamically adapting to language and speaker characteristics. The figure highlights that only the TCACs and the linear projections for decoders are trainable, while pre-trained encoder weights remain fixed.", "section": "3.2 Hierarchical Self-Conditioning in CA-SSLR"}, {"figure_path": "aXApeuAYkg/figures/figures_9_1.jpg", "caption": "Figure 3: CER versus trainable parameters on XLSR model for Normal and Few-shots languages, demonstrating the adaptation ability for the TCA conditioner.", "description": "This figure compares the character error rate (CER) achieved by different model adaptation methods against the number of trainable parameters used.  The methods compared include full fine-tuning (FT), Houlsby adapters, LoRA, and the proposed method (CC-TCAC).  The results are shown separately for datasets with normal and few-shot languages, demonstrating the impact of the adaptation techniques on both well-resourced and low-resource scenarios. The figure highlights that the CC-TCAC method achieves low CERs with relatively few trainable parameters.", "section": "5.3 Generalist Condition-Aware SSLR Model"}]