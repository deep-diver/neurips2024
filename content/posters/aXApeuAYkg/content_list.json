[{"type": "text", "text": "CA-SSLR: Condition-Aware Self-Supervised Learning Representation for Generalized Speech Processing ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yen-Ju Lu\u2020, Jing Liu, Thomas Thebaud\u2020, Laureano Moro-Velazquez\u2020, Ariya Rastrow, Najim Dehak\u2020, Jesus Villalba\u2020 ", "page_idx": 0}, {"type": "text", "text": "\u2020Center for Language and Speech Processing, Johns Hopkins University {ylu125, tthebau1, laureano, ndehak3, jvillal7}@jhu.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We introduce Condition-Aware Self-Supervised Learning Representation (CASSLR), a generalist conditioning model broadly applicable to various speechprocessing tasks. Compared to standard fine-tuning methods that optimize for downstream models, CA-SSLR integrates language and speaker embeddings from earlier layers, making the SSL model aware of the current language and speaker context. This approach reduces the reliance on the input audio features while preserving the integrity of the base SSLR. CA-SSLR improves the model\u2019s capabilities and demonstrates its generality on unseen tasks with minimal task-specific tuning. Our method employs linear modulation to dynamically adjust internal representations, enabling fine-grained adaptability without significantly altering the original model behavior. Experiments show that CA-SSLR reduces the number of trainable parameters, mitigates overfitting, and excels in under-resourced and unseen tasks. Specifically, CA-SSLR achieves a $10\\%$ relative reduction in LID errors, a $37\\%$ improvement in ASR CER on the ML-SUPERB benchmark, and a $27\\%$ decrease in SV EER on VoxCeleb-1, demonstrating its effectiveness. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The emergence of Self-Supervised Learning Representations (SSLRs) models has revolutionized speech processing, setting new standards in the field. Pioneering models like Wav2vec 2.0 Baevski et al. [2020], HuBERT [Hsu et al., 2021], and WavLM [Chen et al., 2022a] leverage unlabeled audio data to learn rich representations of spoken language. These models are pivotal in a wide range of applications, including Speech Recognition (ASR) [Chang et al., 2021], Speaker Verification (SV) [Chen et al., 2022b, Tak et al., 2022], Language Identification (LID) [Bartley et al., 2023], and Speech Translation (ST) [Tang et al., 2022]. Benchmarks such as SUPERB [Yang et al., 2021] and ML-SUPERB [Shi et al., 2023a] have been crucial in evaluating SSL model performance, providing standardized tasks. ", "page_idx": 0}, {"type": "text", "text": "Although SSLR training approaches combine speech from various sources, these models learn representations solely from unpaired audio-only data. When extending SSLR features to multilingual scenarios and low-resource languages, unsupervised training limits the model\u2019s ability to distinguish between different languages, resulting in unified features for all languages. Additionally, labeling all SSL training data with language and speaker information requires significant human effort and is impractical. Thus, a post-training conditioning approach is more favorable. In other fields, methods like [Zhang et al., 2023] and IP-Adaptor [Ye et al., 2023] in image processing, and CTRL [Keskar et al., 2019] in NLP, have successfully integrated conditioning into pretrained models, demonstrating potential applications for speech processing. ", "page_idx": 0}, {"type": "text", "text": "In response to these challenges, we propose Condition-Aware Self-Supervised Learning Representation (CA-SSLR), a generalist conditioning model applicable to various speech-processing tasks such as language identification, multilingual speech recognition, and speaker verification. Unlike standard adaptation methods that optimize the SSLR parameters for downstream models, CA-SSLR integrates language and speaker embeddings from earlier layers, making the SSLR aware of the current language and speaker context. This technique enables the creation of models that perform multiple tasks with a single adapted SSL encoder by strategically injecting conditional adapters into each encoder block while freezing the pretrained encoder weights. CA-SSLR follows a hierarchical self-adaptation structure, where adapters at each layer are conditioned on intermediate task-specific embeddings estimated from lower layers. Attention mechanisms and linear modulation dynamically adjust scaling and biasing, tailoring the model\u2019s response at each time step. The initialization techniques allow the conditioning module to perform identity transformations, ensuring the existing model behavior is maintained when incorporating new conditions. This approach reduces the number of trainable parameters, mitigates overfitting, and avoids catastrophic forgetting. We conduct experiments on three popular types of multilingual speech processing tasks\u2014ASR, LID, and SV to demonstrate the versatility and efficiency of CA-SSLR. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "This work\u2019s main contribution is introducing a novel method for conditioning the SSLRs with limited supervised labels. This leads to generalized speech representation with improved performance using minimal trainable parameters and maintains the model\u2019s behavior. This includes: ", "page_idx": 1}, {"type": "text", "text": "\u2022 Hierarchical Dynamic Conditioning: We design attention-based conditional adapters and integrate them into the SSL model. Our approach dynamically tailors the model\u2019s behavior to the input language and speaker characteristics at each time step, which are periodically estimated from previous layers.   \n\u2022 Preservation of Pre-trained Weights with Efficient Parameter Utilization: The model capitalizes on the knowledge of the foundational model pre-trained weights and introduces lightweight adapters that modulate the encoder hidden representation by a scalar $\\gamma$ and bias $\\beta$ , significantly reducing the trainable parameters. This strategy ensures more stable and parameter-efficient training.   \n\u2022 Harmonized Task Compatibility with Notable Performance Improvements: Our experiments show that CA-SSLR reduces the number of trainable parameters, mitigates overftiting, and excels in under-resourced and unseen tasks. Specifically, CA-SSLR achieves an $27\\%$ relative reduction in LID errors, a $37\\%$ improvement in ASR CER on the ML-SUPERB benchmark, and a $27\\%$ decrease in SV EER on VoxCeleb-1. These results highlight CASSLR\u2019s effectiveness in enhancing multilingual SSLRs while also lowering computational costs for multitask fine-tuning. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Self-supervised learning representation. Self-Supervised Learning (SSL) models, epitomized by Wav2Vec 2.0 [Baevski et al., 2020], HuBERT [Hsu et al., 2021], and WavLM [Chen et al., 2022a], have significantly advanced speech processing by leveraging vast amounts of unlabeled audio data. These models excel in extracting rich speech representations, capturing its intricate acoustic, phonetic, and semantic nuances. These models are fine-tuned on smaller, labeled datasets to adapt the generic representations for specific tasks, achieving impressive results. ", "page_idx": 1}, {"type": "text", "text": "In the realm of cross-lingual speech representation, Wav2Vec 2.0-XLSR (Cross-Lingual Speech Representation) [Babu et al., 2021] takes a significant leap forward. It builds on the robust architecture of Wav2Vec 2.0 but is pre-trained on a diverse, multilingual dataset, learning universal representations transferable across languages. Similarly, mHuBERT (Multilingual HuBERT) [Lee et al., 2021] extends the foundational HuBERT model to process multiple languages effectively. This makes them immensely powerful for multilingual speech recognition and understanding tasks. The benchmark for the SSL models also extends from monolingual SUPERB [Yang et al., 2021] to multilingual ML-SUPERB [Shi et al., 2023a], building new standards for the SSLR models. ", "page_idx": 1}, {"type": "text", "text": "Adaptation Methods. In many studies Yang et al. [2021], Shi et al. [2023a], Chen et al. [2023a], the SSLR remains frozen while decoders are trained for a specific task. Since the encoder is shared across all tasks, this approach offers the advantage that it allows us to evaluate multiple tasks on a given speech signal with just one encoder run. However, systems of this kind often exhibit poorer performance when compared to those incorporating some degree of adaptation of the SSLR to the target task. The latter can involve fine-tuning the entire SSL encoder Chen et al. [2022a], a subset of ", "page_idx": 1}, {"type": "image", "img_path": "aXApeuAYkg/tmp/390b87a69296866e0ae90dc4027c38aa03f8b1f489b0d6e68632609c19d741a9.jpg", "img_caption": ["(a) CA-SSLR improves SSL features by integrat- (b) The trainable time-channel attention conditioner for ing intermediate LID/SV conditions, keeping pre- integrating language and speaker prediction in CA-SSLR. trained parameters frozen. It predicts bias $\\tilde{\\beta}$ and scale $\\tilde{\\gamma}$ using condition feature $\\mathbf{z}$ . "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 1: CA-SSLR scheme and its time-channel attention conditioner. Only the conditioner and linear projections for the decoders are trainable, and all other parameters are frozen during adaptation. ", "page_idx": 2}, {"type": "text", "text": "its layers, or introducing lightweight adapters [Chen et al., 2023b] within its layers. Unfortunately, this results in employing a distinct encoder per task, leading to a large increase in computational load that scales linearly with the number of tasks to be assessed. ", "page_idx": 2}, {"type": "text", "text": "Conditioning Pre-trained Models. Image processing has successfully integrated conditioning into pretrained models using methods like ControlNet [Zhang et al., 2023] and IP-Adaptor [Ye et al., 2023]. ControlNet allows for precise control over generated images by incorporating additional conditions such as edge maps or sketches, while IP-Adaptor uses small-scale adapter modules to adjust the model\u2019s behavior based on specific conditions without altering the pre-trained model\u2019s parameters. These techniques have achieved significant success and offer insights for potential applications in speech processing. Similarly, in Natural Language Processing (NLP), models like the Conditional Transformer Language Model (CTRL) [Keskar et al., 2019] have introduced conditioning to improve model performance. CTRL uses control codes to guide text generation based on specified attributes like style or domain, allowing for efficient adaptation without extensive retraining. The successes in image processing and NLP highlight the potential for conditioning pre-trained SSLRs in speech processing. ", "page_idx": 2}, {"type": "text", "text": "Hierarchical Conditioning. Hierarchical models have been used in previous speech models. [Sanabria and Metze, 2018] proposes a multi-task ASR model that improves intermediate representations by performing Connectionist Temporal Classification at different levels of the network with targets of different granularity. Essentially, representations in lower layers are used to predict character tokens, while higher layers predict subword units with growing vocabulary sizes\u2013from 300 to 10k subword units in the last layer. [Chen et al., 2023a] further explored this by integrating hierarchical conditional layers within the ASR decoder, using ASR tokens predicted from preceding layers to inform subsequent layers. ", "page_idx": 2}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We propose Condition-Aware SSLR (CA-SSLR), designed to serve as a universal encoder for multiple downstream speech tasks. CA-SSLR enhances pre-trained SSL models by integrating intermediate LID and SV predictions to condition and adapt subsequent layers dynamically. This approach allows the model to capture essential language and speaker characteristics, refining its outputs progressively and making it particularly effective in multilingual and multispeaker scenarios. ", "page_idx": 2}, {"type": "text", "text": "Figure 1a illustrates the overall architecture of CA-SSLR. The model consists of a frozen SSL encoder augmented with trainable conditioners and lightweight task-specific decoders. The conditioner ", "page_idx": 2}, {"type": "image", "img_path": "aXApeuAYkg/tmp/40541b1daf69364a6bfde013a0a8bb4a258927299443ed890fff3e4eb9a815c5.jpg", "img_caption": ["(a) Hierarchical conditioning with TCACs to generate feature $\\mathbf{z}$ and modulate layers(b) SSLR layer with with scale $\\tilde{\\gamma}$ and bias $\\tilde{\\beta}$ . conditioning integration, transforming S into $\\tilde{\\mathbf{S}}$ . "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 2: Architecture of the CA-SSLR model employing hierarchical self-conditioning with TimeChannel Attention Conditioners (TCACs). ", "page_idx": 3}, {"type": "text", "text": "modulates the hidden representations of the SSL encoder layers based on conditioning features derived from intermediate LID and SV embeddings. This hierarchical conditioning mechanism enables the model to adapt dynamically to different input conditions while keeping the majority of the pre-trained parameters fixed. In the following sections, we detail the components of CA-SSLR, starting with the conditioner module and then explaining how it integrates into the overall architecture. We also describe the incremental training strategy employed to incorporate conditioning information without catastrophic forgetting. ", "page_idx": 3}, {"type": "text", "text": "3.1 Channel-wise and Time-wise Attention Conditioner ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "A central component of CA-SSLR is the channel-wise conditioner (CC) or the time-channel attention conditioners (TCAC), which modulates the SSL encoder\u2019s hidden representations based on conditioning features. As depicted in Fig. 1b, the TCAC tasks the latent representations $\\mathbf{S}^{(l)}\\in\\mathbb{R}^{C\\times T}$ from layer $l$ of the SSL encoder and a conditioning feature vector $\\mathbf{z}\\in\\mathbb{R}^{\\dot{R}}$ , derived from intermediate LID or SV embeddings. The TCAC outputs modulated latent representations $\\tilde{\\mathbf{S}}^{(l)}$ by applying time-channel-dependent scaling and bias: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{\\mathbf{S}}_{t,c}^{(l)}=\\mathrm{TCAC}(\\mathbf{S}_{t,c}^{(l)},\\mathbf{z})=\\tilde{\\gamma}_{t,c}^{(l)}(\\mathbf{z},\\mathbf{S}^{(l)})\\mathbf{S}_{t,c}^{(l)}+\\tilde{\\beta}_{t,c}^{(l)}(\\mathbf{z},\\mathbf{S}^{(l)})}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $t$ and $c$ index time and channel dimensions, respectively. The scales $\\tilde{\\gamma}_{t,c}^{(l)}$ and biases $\\tilde{\\beta}_{t,c}^{(l)}$ are products of time-dependent and channel-dependent components: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{\\gamma}_{t,c}^{(l)}(\\mathbf{z},\\mathbf{S}^{(l)})=\\alpha_{t}^{(l)}(\\mathbf{z},\\mathbf{S}^{(l)})\\times\\gamma_{c}^{(l)}(\\mathbf{z})\\quad\\quad\\tilde{\\beta}_{t,c}^{(l)}(\\mathbf{z},\\mathbf{S}^{(l)})=\\alpha_{t}^{(l)}(\\mathbf{z},\\mathbf{S}^{(l)})\\times\\beta_{c}^{(l)}(\\mathbf{z})}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The channel-dependent scales $\\gamma^{(l)}\\in\\mathbb{R}^{C}$ and biases $\\beta^{(l)}\\in\\mathbb{R}^{C}$ are computed via linear transformations of the conditioning feature, similar to feature-wise linear modulation [Perez et al., 2018]: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\gamma^{(l)}({\\bf z})={\\bf W}_{\\gamma}^{(l)}{\\bf z}+{\\bf b}_{\\gamma}^{(l)}{\\mathrm{\\boldmath~\\gamma~}}\\beta^{(l)}({\\bf z})={\\bf W}_{\\beta}^{(l)}{\\bf z}+{\\bf b}_{\\beta}^{(l)}{\\mathrm{\\boldmath~\\gamma~}}{\\mathrm{\\boldmath~with~}}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The time-dependent scales $\\alpha^{(l)}\\in\\mathbb{R}^{T}$ are obtained with an additive attention mechanism as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\alpha_{t}^{(l)}(\\mathbf{z},\\mathbf{S}^{(l)})=\\mathbf{v}_{\\alpha}^{\\mathrm{T}}f(\\mathbf{W}_{\\alpha}^{(l)}\\left[\\mathbf{S}_{t}^{(l)}\\right]+\\mathbf{b}_{\\alpha}^{(l)})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $f(.)$ is a ReLU non-linearity, $\\mathbf{W}_{\\alpha}^{(l)}\\ \\in\\ \\mathbb{R}^{C^{\\prime}\\times(C+R)}$ , $\\mathbf{b}_{\\alpha}^{(l)}\\ \\in\\ \\mathbb{R}^{C^{\\prime}}$ , and $\\mathbf{v}_{\\alpha}\\ \\in\\ \\mathbb{R}^{C^{\\prime}}$ . The conditioning feature $\\mathbf{z}$ is obtained by processing the intermediate embeddings $\\mathbf{e}\\in\\mathbb{R}^{E}$ from the LID or SV decoders, as $\\mathbf{z}=\\mathrm{LayerNorm}(\\mathbf{We}+\\mathbf{b})$ , where $\\mathbf{W}\\in\\mathbb{R}^{R\\times E}$ and $\\mathbf{b}\\in\\mathbb{R}^{R}$ are shared linear transformation parameters, and LayerNorm $(\\cdot)$ denotes layer normalization. In scenarios where time-based modulation is unnecessary, the model can switch to the simpler Channel-wise Conditioner (CC) by using only the channel-dependent components $\\gamma$ and $\\beta$ . This flexibility in conditioning design enables the model to be tailored to various speech tasks with differing complexity requirements. By integrating these conditioning methods, CA-SSLR dynamically adapts its internal representations based on language and speaker characteristics. This mechanism enables the integration of conditioning into the model\u2019s latent representations without altering the pre-trained encoder\u2019s original parameters. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "3.2 Hierarchical Self-Conditioning in CA-SSLR ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Building upon the TCAC module, CA-SSLR employs a hierarchical self-conditioning mechanism within the SSL encoder layers. As shown in Figure 2, the SSL encoder is partitioned into layer groups, with TCACs inserted after the attention module in each layer to modulate hidden representations based on updated conditioning features. The model aggregates SSL features through a weighted sum, combining outputs from all preceding layer groups. These aggregated features are then provided to the LID and SV decoders, where LID and SV embeddings are extracted and processed through a linear layer followed by layer normalization to create the conditioning feature ${\\bf z}$ for the TCACs. ", "page_idx": 4}, {"type": "text", "text": "The conditioning feature $\\mathbf{z}$ is re-estimated at intervals\u2014every three layers for LID and every six layers for SV\u2014using the aggregated SSL features from previous groups. This hierarchical design progressively refines the model\u2019s representations, adapting to the input\u2019s language and speaker characteristics at different depths of the network. For example, the initial SSL layer group captures basic language and speaker characteristics, generating embeddings that condition the next group of layers via TCACs. This ongoing refinement allows the model to dynamically adapt based on intermediate predictions, resulting in a context-aware and dynamic representation. ", "page_idx": 4}, {"type": "text", "text": "Each layer group uses distinct TCAC parameters, enabling tailored scaling and bias adjustments at different stages of the model. Notably, only the TCACs and the linear projections for the decoders are trainable, while all other SSL encoder parameters remain fixed during the conditioning insertion. This design minimizes overfitting and accelerates training due to the smaller number of trainable parameters. This hierarchical self-conditioning mechanism enables the model to dynamically capture diverse aspects of input audio, making it a robust tool for comprehensive speech analysis. ", "page_idx": 4}, {"type": "text", "text": "3.3 Incremental Training Strategy ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Incorporating new components into a pre-trained SSL encoder poses the risk of catastrophic forgetting, where the model loses previously acquired knowledge. To mitigate this, we adopt an incremental training strategy that gradually integrates the conditioning information. We initialize the TCAC parameters to ensure that the initial modulated features are identical to the original SSL features. Specifically, we set the initial values such that $\\alpha_{t}~=~1$ for all $t$ , $\\gamma_{c}\\,=\\,1\\$ , and $\\beta_{c}\\:=\\:0$ for all $c$ . According to Eq. (1), this initialization means that $\\tilde{\\mathbf{S}}_{t,c}^{(l)}=\\mathbf{S}_{t,c}^{(l)}$ St(,lc) at the start of training, allowing a smooth transition from the pre-trained model to the conditioned model. ", "page_idx": 4}, {"type": "text", "text": "When multiple conditioning features are involved, such as both LID and SV, we compute separate scaling and bias parameters for each: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\alpha_{\\mathrm{total}}=\\alpha_{\\mathrm{LID}}\\times\\alpha_{\\mathrm{SV}},\\quad\\gamma_{\\mathrm{total}}=\\gamma_{\\mathrm{LID}}\\times\\gamma_{\\mathrm{SV}},\\quad\\beta_{\\mathrm{total}}=\\beta_{\\mathrm{LID}}+\\beta_{\\mathrm{SV}}\\;.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This approach allows the model to incrementally incorporate additional conditioning tasks without disrupting the knowledge acquired from previous tasks. ", "page_idx": 4}, {"type": "text", "text": "4 Experimental Setup ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "4.1 Datasets ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "For the LID and ASR tasks, we utilized the ML-SUPERB benchmark [Shi et al., 2023a]. This corpus comprises two distinct data configurations: 10-minute/language and 1-hour/language for each of the 123 well-represented languages (Normal). Additionally, both configurations include five training utterances for each of 20 selected low-resource languages (Few-shot)1. For the Few-shot languages, we also considered an Extended Few-shot condition in which we augmented the amount of data to match that of the Normal languages. However, the Extended Few-shots only included language labels but not ASR transcripts. This aims to analyze the behavior of few-shot languages with improved language ID accuracy, as achieving satisfactory language accuracy with just five utterances is challenging. This is also a reasonable assumption since obtaining data with language labels is easier and cheaper than obtaining transcribed data. The moderate size of this dataset was ideal for testing our approach, as it allowed us to conduct multiple ablation experiments with limited computing resources. As ML-SUPERB lacks speaker labels, we combined it with VoxCeleb2 [Nagrani et al., 2017] for training models incorporating the SV task. VoxCeleb2 contains 1,092 speech hours from 5,994 speakers, although it lacks LID labels and ASR transcripts. The SV task was tested on the VoxCeleb1 original set. The speech was augmented with Musan noise Snyder et al. [2015] and reverberation Ko et al. [2017] during SV training. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "4.2 Model Architecture ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "SSLR Models. In our system, we employed the best multilingual SSL back-bones in the MLSUPERB benchmark: Wav2Vec2-XLSR with 300M parameters, trained on 128 languages2, and the 100M parameter multilingual Hubert (mHuBERT) model [Lee et al., 2021], trained on English, Spanish, French data from VoxPopuli [Wang et al., 2021] unlabeled speech as our foundational acoustic encoders. These models have demonstrated their efficacy in processing a wide range of linguistic inputs and form the backbone of our system. We experimented with the S3PRL [Yang et al., 2021] and ESPnet [Watanabe et al., 2018] toolkits. Our training dataset combined data labeled for $\\mathrm{ASR+LID}$ labels, LID only, or SV only. Hence, we computed the losses only for the available tasks for each sample. Detailed information on the remaining training hyperparameters is provided in the appendix, and the code will be made available for reproducibility. A model\u2019s training takes about one day using 2 A100 GPUs. ", "page_idx": 5}, {"type": "text", "text": "Speaker and Language Decoders. The speaker and language decoders are based on the ECAPATDNN architecture [Desplanques et al., 2020]. Initially, a convolutional layer projects SSL representation to the decoder dimension (512 for LID and 1024 for SV). This is followed by a sequence of 1-dimensional SE-Res2Net [Gao et al., 2021] layers (one for LID and three for SV). Next, channelwise attentive statistic pooling aggregates the frame-level features into a single utterance-level vector, which is projected into lower-dimensional speaker embedding. The training loss was Additive Angular margin-softmax [Deng et al., 2019] with margin $_{=0.3}$ for SV and margin $_{=0.0}$ for LID. Large margin helps to create highly compact speaker representations [Villalba et al., 2022], while being detrimental in LID [Villalba et al., 2023]. The SV and LID decoders producing the final result consume a weighted average of all SSL layers. Meanwhile, the ones estimating the conditioning embeddings use a weighted average of the SSL layers evaluated up to that point in the chain. Note that all SV and LID decoders share parameters, so the number of trainable parameters remains independent of the frequency with which we re-compute the conditioning embeddings. ", "page_idx": 5}, {"type": "text", "text": "ASR decoder. The ASR decoder conforms to the framework set by the ML-SUPERB benchmark [Shi et al., 2023a], facilitating comparable evaluations. A convolutional downsampling layer halves the SSL feature sequence duration. These features are channeled into a two-layer Transformer with 256-dim self-attention, eight attention heads, and 1024-dim feed-forward layers. A linear output layer with connectionist temporal classification (CTC) loss predicts multi-lingual character-level tokens. ", "page_idx": 5}, {"type": "text", "text": "5 Experiments and Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "5.1 Generalization Ability on Unseen Tasks ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Experiment Setting. We conducted experiments to evaluate the generalization capabilities of the adapted SSLR models on LID, ASR, and SV tasks. The SSLR models were adapted for one task (either LID or ASR) and then evaluated on both the adapted task and an unseen task. For LID adaptation, the SSLR was trained exclusively with LID labels. We compared three setups: full fine-tuning (LID-FT), Houlsby adaptors Houlsby et al. [2019] (LID-Houlsby), and our proposed condition-aware approach $\\mathrm{(LID-CA-\\dot{X}L S R}_{\\mathrm{dual}}^{L})$ . In this setup, we employed an additional LID decoder using the pre-trained SSLR to pre-generate language embeddings, which were then used to condition the SSLR model for a second inference pass. For ASR adaptation, the models were trained with ASR loss using three setups: full fine-tuning (ASR-FT), Houlsby adaptors (ASR-Houlsby), and our ", "page_idx": 5}, {"type": "text", "text": "Table 1: Evaluation of adapted XLSR models on the 10-min ML-SUPERB and VoxCeleb dataset for LID, ASR, and SV tasks. These evaluations test the encoder\u2019s generalizability across different tasks, demonstrating effectiveness without further task-specific tuning. ", "page_idx": 6}, {"type": "table", "img_path": "aXApeuAYkg/tmp/c40206e48279c4cd6362342478ee49090b97b57b57fa7ccf89cd8155c7c73d0e.jpg", "table_caption": ["(a) LID-adapted XLSR models evaluated on LID and ASR tasks. "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "aXApeuAYkg/tmp/82ad3d416c00474fc33337703cd1f8cc8a9590ad7dbff837edd0c99d3abae850.jpg", "table_caption": ["(b) ASR-adapted XLSR models evaluated on ASR and SV tasks. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "proposed hierarchical conditioning method with TCAC layers integrated into the SSLR model with single inference (ASR-CA-XLSRL). During ASR adaptation, the LID decoder is integrated into the SSLR model to provide conditioning features, but SV information was not included during training. ", "page_idx": 6}, {"type": "text", "text": "Results. In LID adaptation (Table 1a), both LID-FT and LID-Houlsby improved LID performance compared to the pre-trained SSL baseline. However, on the unseen ASR task, the fully fine-tuned SSLR encoder improved ASR CER by only $2\\%$ , while LID-Houlsby showed limited generalization, with CER improvements of $5.4\\%$ and $3.9\\%$ for normal and few-shot languages, respectively. Our $\\mathrm{LID-CA-XLSR}_{\\mathrm{dual}}^{L}$ method achieved significantly better generalization, improving ASR CER by $7.3\\%$ and $6.6\\%$ for normal and few-shot languages. In ASR adaptation (Table 1b), all models enhanced ASR performance, but ASR-Houlsby and full fine-tuning degraded SV performance relative to the baseline, highlighting their limited generalization. Our ASR-CA- $X L\\dot{S}\\mathbf{R}^{L}$ approach not only preserved but improved SV performance, reducing EER by relative $10.9\\%$ and DCF by $5.4\\%$ , showcasing strong generalization to the unseen SV task. These results demonstrate that CA-SSLR significantly outperforms full fine-tuning and standard adaptation methods in terms of generalization. By effectively leveraging conditioning information, CA-SSLR adapts across tasks while maintaining performance on unseen ones. Our proposed conditioner offers both robust adaptations on training tasks and superior generalization, making CA-SSLR a versatile and effective solution for multilingual and multispeaker speech processing. ", "page_idx": 6}, {"type": "text", "text": "5.2 Condition-Aware SSLR Model ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Experiment Setting. Table 2 investigates the CA-SSLR approach with hierarchical language conditioning. The first block of the table refers to the baseline where the foundational models are frozen, while the second block $(\\mathbf{CA}\\!\\!-\\!\\!\\mathbf{XLSR}_{\\mathrm{dual}}^{L})$ ) utilizes a separate task-specific LID model to pre-generate the language embedding. The third block presents our proposed approach, where we re-estimate the language embedding every fourth or third layer $(\\mathbf{CA}{-}\\mathbf{X}\\bar{\\mathbf{L}}\\mathbf{S}\\mathbf{R}^{L}$ (4L, 3L)) within the XLSR model, not required a separate LID system. The experiments utilized two types of conditioners: TCAC, which incorporates attention, and a variant without attention\u2014referred to as Channel-wise Conditioners (CC)\u2014where the same scale and bias are applied uniformly across all time frames. The real-time factors (RTF) as proc-time/signal-length are provided for assessing efficiency3. ", "page_idx": 6}, {"type": "table", "img_path": "aXApeuAYkg/tmp/f218adfa70f50f3d5fb4355aae23b4df395103cf32f97e90c5543a00d5fc3238.jpg", "table_caption": ["Table 2: ASR ${\\mathrm{CER}}({\\%})$ and LID Acc $(\\%)$ in ML-SUPERB $10\\mathrm{min}$ . and 1h. sets, comparing different layers to generate the language embedding to condition the following layers. We adapt the XLSR model for LID and ASR tasks. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "aXApeuAYkg/tmp/56e75e5bf689156f04aa340b66681bba11b5daeb16daff08fc10875e8ab7b8d8.jpg", "table_caption": ["Table 3: Experiments on LID and $\\mathrm{LID}+\\mathrm{SV}$ Hierarchical Conditioning. We adapt the XLSR and mHuBERT models for LID and ASR tasks using ${\\mathrm{CA}}{-}{\\mathrm{SSLR}}^{L}$ , and for SV tasks using $\\mathrm{CA-SSLR}^{L,S}$ . Results for Normal languages with $10{-}\\mathrm{min}$ and 1-hour datasets alongside VoxCeleb SV results. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Results. First, we observed that both CA-XLSRdLual and CA-XLSRL systems with TCAC (with attention) generally performed better than the CC (w/o attention) counterparts, reaffirming the beneftis of the time-wise attention design. In the second block, ${\\mathrm{CA-XLSR}}_{\\mathrm{dual}}^{L}$ slightly outperformed $\\mathrm{CA-XLSR}^{L}$ in terms of CER for both the 10-minute and 1-hour datasets. However, its real-time factor (RTF) is akin to the combined RTFs of separate LID and ASR models since it runs Wav2Vec2 twice\u2014once for language embedding extraction and again for ASR conditioning\u2014posing challenges for streaming applications. On the other hand, $\\mathrm{CA-XLSR}^{L}(\\mathrm{CC},\\,3\\mathrm{L})$ excelled among the three approaches, achieving a $35.9\\%$ and $19.0\\%$ relative improvement in Normal and few-shot languages, respectively, compared to the baseline in the 10-minute setup, and $33.5\\%$ and $19.8\\%$ in the 1-hour setup. LID accuracy remained comparable among the various CA-XLSR models, with a notable performance improvement from $90.9\\%$ to $93.4\\%$ in 1-hour setup. ", "page_idx": 7}, {"type": "text", "text": "5.3 Generalist Condition-Aware SSLR Model ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Experiment Setting. Table 3 presents results for general CA-SSLR models that combine Multilingual ASR, LID, and SV tasks. The table compares the baselines, with frozen and fine-tuned SSL models, to two different CA-SSLR Hierarchical models $(\\mathbf{CA}{\\mathrm{-}}\\mathbf{S}\\mathbf{S}\\mathbf{LR}^{L}$ and $\\mathrm{CA-SSLR}^{L,S}$ ). We further include another well-known multilingual SSLR model, mHuBERT, for a comprehensive comparison. The LID conditioning systems $({\\mathbf{C}}{\\mathbf{A}}{\\mathbf{-}}{\\mathbf{S}}{\\mathbf{S}}{\\mathbf{L}}{\\mathbf{R}}^{L})$ ) are the same as from the previous section, conditioning the SSL model only on LID embeddings, with the SV decoder added on top of SSL features without further adaptation. The $\\mathrm{LID}+\\mathrm{SV}$ conditioning system $(\\mathbf{CA}{\\mathrm{-}}\\mathbf{S}\\mathbf{S}\\mathbf{LR}^{L,S})$ ) combines both LID and SV embeddings and is jointly trained on ASR, SV, and LID losses. The intermediate LID embeddings were recomputed every three layers as the best configuration in Table 2, and SV embeddings were recomputed every six SSL layers. Apart from ASR CER and LID Acc on ML-SUPERB, we present SV equal error rates (EER) and detection cost function (DCF), measured at target prior probability $p=0.05$ [Sadjadi et al., 2022], on VoxCeleb1. SV performance varied depending on whether we trained the model combining 10min ML-SUPERB $^+$ VoxCeleb2 or 1h ML-SUPERB $^+$ VoxCeleb2. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Fine-tuning Baseline. In the fully fine-tuning experiment, we initialized the model with pretrained ASR, LID, and SV decoders and fine-tuned for a few epochs. However, this approach resulted in suboptimal performance compared to the frozen SSLR baseline. The \"FT\" experiments showed degraded performance, with LID accuracy decreasing by $5.7\\%$ , ASR CER increasing by $3.1\\%$ , and SV EER worsening by 4.2 in absolute values on average across the four settings. This decline is unexpected, as fine-tuning typically improves performance. This suggests that simultaneous adaptation of the SSL layers to multiple tasks causes conflicting adjustments, reducing the model\u2019s robustness. Consequently, catastrophic forgetting led to worse performance compared to the baseline. Conversely, the condition-aware SSLR models exhibited superior performance comparing with the frozen baseline, indicating that training the inserted condition layers does not alter the model\u2019s behavior for downstream tasks but improves its ability to represent the input speech data. ", "page_idx": 8}, {"type": "text", "text": "Language Conditioned SSLR. ${\\mathrm{CA}}{\\mathrm{-}}{\\mathrm{SSLR}}^{L}({\\mathrm{CC}})$ notably enhanced SV performance w.r.t. the baseline, despite its encoder being solely tuned for ASR and LID tasks. For XLSR, the EER improved by $14\\%$ and $20\\%$ relative for the $10{-}\\mathrm{min}$ . and 1-h. sets, respectively, while DCF improved by $16{-}18\\%$ . Similarly, for mHuBERT, we observed comparable enhancements, with the EER improving by $17\\%$ in both sets and the DCF improving by $17{-}18\\%$ . This demonstrates that the CA-SSLR approach offers superior generalization capabilities compared to the original pre-trained SSL encoder, delivering improved performance. CA-SSLRL(TCAC) performance in SV is comparable to its non-attention counterpart with better performance in LID and ASR as discussed in Sec.5.2. ", "page_idx": 8}, {"type": "text", "text": "Language and Speaker Conditioned SSLR. Adding a speaker conditioner to $\\mathrm{CA-SSLR}^{L,S}$ further improved its performance. In ASR tasks, incorporating the speaker conditioner to $\\mathrm{CA-XLSR}^{L,S}$ reduced CER by $3.1\\%$ for the $10{-}\\mathrm{min}$ . set and $6.2\\%$ for the 1-hr set, relative to $\\mathrm{CA-XLSR}^{L}$ . For LID task, CA-SSLRL,S shows similar performance to other models with relative differences below $3\\%$ . For SV, $\\mathrm{CA-XLSR}^{L,S}$ using channel-wise conditioner (CC) reduced EER by $19.4\u201327.1\\%$ , outperforming $\\mathrm{CA-XLSR}^{L}$ . Switching from CC to TCAC yielded additional gains in ASR, adding a relative improvement of $2.7{-}4.0\\%$ . In contrast, its impact on SV was more modest, with improvements in EER by $14.0\u201321.7\\%$ . Overall, TCAC demonstrated better adaptation ability, while CC excelled in generalization. ", "page_idx": 8}, {"type": "text", "text": "ASR and RTF Discussion. Generally, we observed the largest improvement for ASR when including the language conditioner, as it enables the system to adapt to produce output tokens in the correct language. Conversely, adapting the model to the input speaker provided fewer ASR gains. The XLSR model benefitted from our approach better than mHuBERT, possibly because mHuBERT is $3\\times$ smaller than XLSR, but more importantly, because mHuBERT was trained on just four languages compared to 128 in XLSR. Therefore, the pre-trained mHuBERT has not encountered enough diversity in terms of languages and speakers, thereby limiting its performance in multi-lingual ASR and SV. In terms of RTF, while the conditioned models are $13\\mathrm{-}34\\%$ slower compared to sharing the pre-trained SSL encoder for the three tasks, both CA-SSLRL and $\\mathrm{CA-SSLR}^{L,\\bar{S}}$ offer superior performance while being much faster than running task-specific models separately, indicating a more efficient use of computational resources while running the generalist model. ", "page_idx": 8}, {"type": "text", "text": "5.4 Analysis of the TCA Conditioner ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Ablation study of Conditioning Approach. Table 4 conducts an ablation study for different conditioning methods with $\\mathrm{CA-XLSR}_{\\mathrm{dual}}^{\\bar{L}}$ settings within the ML-SUPERB $10\\mathrm{min}$ dataset regarding ASR CER. First, we used conditioners without attention (CC) on the ground truth LID predictions (G.T.), serving as the upper bound for the performance of our proposed approach. This improved the Normal languages from $29.0\\%$ to $17.2\\%$ , and Few-shot languages from $39.0\\%$ to $27.9\\%$ , w.r.t. the pre-trained XLSR model. This showcases the potential of the condition-aware SSLR. Following, we compared conditioning on hard-predicted language labels (Hard), soft-predicted language labels (Soft), and language embeddings from the LID decoder bottleneck layer (Embed) for comparison. Conditioning on Hard LID labels improved the most in Few-shot languages, improving by $26\\%$ relative to the baseline. On the other hand, the Embed case outperformed the Soft case and provided balanced performance for both Normals and Few-shots languages. Additionally, we compared CC to ", "page_idx": 8}, {"type": "text", "text": "Table 4: Ablation study of condition-aware settings for ASR-adapted XLSR models on 10-min ML-SUPERB dataset, using CC or TCAC. Conditioning is based on predicted language labels or LID embeddings, except in the ground truth (G.T.) experiment. ", "page_idx": 9}, {"type": "table", "img_path": "aXApeuAYkg/tmp/4e436f4b713b4f529a7349e66eb8a026551fedc2bf1f22de236625c5cb0af334.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "aXApeuAYkg/tmp/72114d8e1e8f218e42fa3ca856871b93d446c8a004a0574357ebfdf874a6c7c1.jpg", "img_caption": ["Figure 3: CER versus trainable parameters on XLSR model for Normal and Few-shots languages, demonstrating the adaptation ability for the TCA conditioner. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "TCAC. The TCAC provided the best overall results, improving Normals and Few-shots by $38.6\\%$ and $18.5\\%$ , respectively, w.r.t. baseline. ", "page_idx": 9}, {"type": "text", "text": "Parameter Efficiency in CER Reduction. Figure 3 compares CER versus the number of trainable parameters for different adaptation methods, including our proposed Channel-wise Conditioner and Time-Channel Attention Conditioner (CC-TCAC), the Houlsby adapter, LoRA [Hu et al., 2021], full fine-tuning (FT), and the baseline XLSR model. The Houlsby adapters, with hidden dimensions of 256 and 512, have 18.4M and 30.9M trainable parameters. In comparison, the CC-TCAC approach, conditioned on precomputed LID embeddings with 256 dimensions (18.7M for CC and 22.6M for TCAC), achieves lower CERs with similar parameter counts. LoRA provided only marginal gains over the baseline, aligning with findings from Chen et al. [2023b]. In contrast, FT required fine-tuning 16\u201324 layers (200\u2013300M parameters) to achieve comparable CER reductions, making CC-TCAC about ten times more efficient. As discussed in Sec 5.1, CC-TCAC\u2019s key contribution is its superior generalization ability. While the Houlsby adapter enhances task-specific adaptation, it falls short in generalizing to unseen tasks. In contrast, CC-TCAC achieves both effective adaptation and robust generalization, making it a versatile solution for diverse applications. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper introduces the CA-SSLR framework, an innovative approach that integrates conditioning into pre-trained Self-Supervised Learning (SSL) models by adapting only the trainable conditioner. Through a hierarchical self-conditioning mechanism, where intermediate language and speaker features condition the upper layers of the SSL model, CA-SSLR achieves a $33\\%$ reduction in Character Error Rate compared to the pre-trained baseline, matching the performance of single-task fully fine-tuned models. Additionally, it improves Speaker Verification EER by $27\\%$ and reduce Language Identification errors by relative $10\\%$ in average. The results indicate that condition-aware SSLR models enhance the model\u2019s interpretation of input speech data, providing superior performance compared to traditional fine-tuning methods. This improvement is achieved by dynamically tailoring the model\u2019s response to the input language and speaker characteristics, ensuring robust generalization across various tasks. In summary, CA-SSLR offers a versatile and efficient approach to integrating conditioning information into pre-trained models. This method not only enhances performance across multiple tasks but also ensures efficient parameter utilization, supported by an improved RTF that facilitates its application in real-world scenarios. ", "page_idx": 9}, {"type": "text", "text": "Broader Impact and Limitations The CA-SSLR methodology improves the conditioning of pretrained Self-Supervised Learning (SSL) models for speech processing, improving performance with minimal fine-tuning and reducing computational resource requirements. This advancement facilitates the deployment of robust models in resource-constrained environments, promoting broader access to advanced speech technology. However, there are potential risks. The conditioning mechanisms might amplify biases in the training data, leading to unfair outcomes, particularly for underrepresented languages and speaker groups. Ensuring diverse and balanced datasets, along with continuous monitoring, is crucial to mitigate these risks and prevent perpetuating existing inequities. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Arun Babu, Changhan Wang, Andros Tjandra, Kushal Lakhotia, Qiantong Xu, Naman Goyal, Kritika Singh, Patrick von Platen, Yatharth Saraf, Juan Pino, et al. Xls-r: Self-supervised cross-lingual speech representation learning at scale. arXiv preprint arXiv:2111.09296, 2021.   \nAlexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: A framework for self-supervised learning of speech representations. Advances in neural information processing systems, 33:12449\u201312460, 2020.   \nTravis M Bartley, Fei Jia, Krishna C Puvvada, Samuel Kriman, and Boris Ginsburg. Accidental learners: Spoken language identification in multilingual self-supervised models. In ICASSP 2023- 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1\u20135. IEEE, 2023.   \nXuankai Chang, Takashi Maekaku, Pengcheng Guo, Jing Shi, Yen-Ju Lu, Aswin Shanmugam Subramanian, Tianzi Wang, Shu-wen Yang, Yu Tsao, Hung-yi Lee, et al. An exploration of selfsupervised pretrained representations for end-to-end speech recognition. In 2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pages 228\u2013235. IEEE, 2021.   \nSanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, et al. Wavlm: Large-scale self-supervised pre-training for full stack speech processing. IEEE Journal of Selected Topics in Signal Processing, 16(6): 1505\u20131518, 2022a.   \nSanyuan Chen, Yu Wu, Chengyi Wang, Shujie Liu, Zhuo Chen, Peidong Wang, Gang Liu, Jinyu Li, Jian Wu, Xiangzhan Yu, et al. Why does self-supervised learning for speech recognition benefit speaker recognition? arXiv preprint arXiv:2204.12765, 2022b.   \nWilliam Chen, Brian Yan, Jiatong Shi, Yifan Peng, Soumi Maiti, and Shinji Watanabe. Improving massively multilingual asr with auxiliary ctc objectives. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1\u20135. IEEE, 2023a.   \nZih-Ching Chen, Chin-Lun Fu, Chih-Ying Liu, Shang-Wen Daniel Li, and Hung-yi Lee. Exploring efficient-tuning methods in self-supervised speech models. In 2022 IEEE Spoken Language Technology Workshop (SLT), pages 1120\u20131127. IEEE, 2023b.   \nJiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 4685\u20134694, 2019. doi: 10.1109/CVPR.2019.00482.   \nBrecht Desplanques, Jenthe Thienpondt, and Kris Demuynck. Ecapa-tdnn: Emphasized channel attention, propagation and aggregation in tdnn based speaker verification. arXiv preprint arXiv:2005.07143, 2020.   \nShang-Hua Gao, Ming-Ming Cheng, Kai Zhao, Xin-Yu Zhang, Ming-Hsuan Yang, and Philip Torr. Res2Net: A New Multi-Scale Backbone Architecture. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43(2):652\u2013662, Feb 2021. ISSN 1939-3539.   \nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In International Conference on Machine Learning, pages 2790\u20132799. PMLR, 2019.   \nWei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed. Hubert: Self-supervised speech representation learning by masked prediction of hidden units. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 29:3451\u20133460, 2021.   \nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.   \nNitish Shirish Keskar, Bryan McCann, Lav R Varshney, Caiming Xiong, and Richard Socher. Ctrl: A conditional transformer language model for controllable generation. arXiv preprint arXiv:1909.05858, 2019.   \nTom Ko, Vijayaditya Peddinti, Daniel Povey, Michael L Seltzer, and Sanjeev Khudanpur. A study on data augmentation of reverberant speech for robust speech recognition. In 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5220\u20135224. IEEE, 2017.   \nAnn Lee, Hongyu Gong, Paul-Ambroise Duquenne, Holger Schwenk, Peng-Jen Chen, Changhan Wang, Sravya Popuri, Yossi Adi, Juan Pino, Jiatao Gu, et al. Textless speech-to-speech translation on real data. arXiv preprint arXiv:2112.08352, 2021.   \nArsha Nagrani, Joon Son Chung, and Andrew Zisserman. Voxceleb: a large-scale speaker identification dataset. arXiv preprint arXiv:1706.08612, 2017.   \nEthan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual reasoning with a general conditioning layer. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018.   \nSeyed Omid Sadjadi, Craig Greenberg, Elliot Singer, Lisa Mason, and Douglas Reynolds. The 2021 NIST Speaker Recognition Evaluation. In Proc. The Speaker and Language Recognition Workshop (Odyssey 2022), pages 322\u2013329, 2022. doi: 10.21437/Odyssey.2022-45.   \nRamon Sanabria and Florian Metze. Hierarchical multitask learning with ctc. In 2018 IEEE Spoken Language Technology Workshop (SLT), pages 485\u2013490. IEEE, 2018.   \nJiatong Shi, Dan Berrebbi, William Chen, Ho-Lam Chung, En-Pei Hu, Wei Ping Huang, Xuankai Chang, Shang-Wen Li, Abdelrahman Mohamed, Hung-yi Lee, et al. Ml-superb: Multilingual speech universal performance benchmark. arXiv preprint arXiv:2305.10615, 2023a.   \nJiatong Shi, William Chen, Dan Berrebbi, Hsiu-Hsuan Wang, Wei-Ping Huang, En-Pei Hu, Ho-Lam Chuang, Xuankai Chang, Yuxun Tang, Shang-Wen Li, et al. Findings of the $2023~\\mathrm{ml}.$ -superb challenge: Pre-training and evaluation over more languages and beyond. In 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pages 1\u20138. IEEE, 2023b.   \nDavid Snyder, Guoguo Chen, and Daniel Povey. Musan: A music, speech, and noise corpus. arXiv preprint arXiv:1510.08484, 2015.   \nHemlata Tak, Massimiliano Todisco, Xin Wang, Jee-weon Jung, Junichi Yamagishi, and Nicholas Evans. Automatic speaker verification spoofing and deepfake detection using wav2vec 2.0 and data augmentation. arXiv preprint arXiv:2202.12233, 2022.   \nYun Tang, Hongyu Gong, Ning Dong, Changhan Wang, Wei-Ning Hsu, Jiatao Gu, Alexei Baevski, Xian Li, Abdelrahman Mohamed, Michael Auli, et al. Unified speech-text pre-training for speech translation and recognition. arXiv preprint arXiv:2204.05409, 2022.   \nJes\u00fas Villalba, Bengt J Borgstrom, Saurabh Kataria, Magdalena Rybicka, Carlos D Castillo, Jaejin Cho, L. Paola Garc\u00eda-Perera, Pedro A. Torres-Carrasquillo, and Najim Dehak. Advances in crosslingual and cross-source audio-visual speaker recognition: The jhu-mit system for nist sre21. pages 213\u2013220. ISCA, 6 2022. doi: 10.21437/Odyssey.2022-30. URL https://www.isca-speech. org/archive/odyssey_2022/villalba22b_odyssey.html.   \nJes\u00fas Villalba, Jonas Borgstrom, Maliha Jahan, Saurabh Kataria, Leibny Paola Garcia, Pedro TorresCarrasquillo, and Najim Dehak. Advances in Language Recognition in Low Resource African Languages: The JHU-MIT Submission for NIST LRE22. In Proc. INTERSPEECH 2023, pages 521\u2013525, 2023. doi: 10.21437/Interspeech.2023-1094.   \nChanghan Wang, Morgane Riviere, Ann Lee, Anne Wu, Chaitanya Talnikar, Daniel Haziza, Mary Williamson, Juan Pino, and Emmanuel Dupoux. Voxpopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation. arXiv preprint arXiv:2101.00390, 2021.   \nShinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki Hayashi, Jiro Nishitoba, Yuya Unno, Nelson Enrique Yalta Soplin, Jahn Heymann, Matthew Wiesner, Nanxin Chen, et al. Espnet: End-to-end speech processing toolkit. arXiv preprint arXiv:1804.00015, 2018.   \nShu-wen Yang, Po-Han Chi, Yung-Sung Chuang, Cheng-I Jeff Lai, Kushal Lakhotia, Yist Y Lin, Andy T Liu, Jiatong Shi, Xuankai Chang, Guan-Ting Lin, et al. Superb: Speech processing universal performance benchmark. arXiv preprint arXiv:2105.01051, 2021.   \nHu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721, 2023.   \nLvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3836\u20133847, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Model/Dataset Details and Training Hyper-parameters ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "This appendix provides detailed configurations and hyper-parameters for the decoder models used in our experiments, including ASR, LID, SV decoders, and the CA-SSLR models. The rationale behind specific hyper-parameter choices and architectural details are also discussed to offer insights into the experimental setup and model optimization strategies. ", "page_idx": 12}, {"type": "text", "text": "A.1 Decoder Models for ASR, LID, and SV ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "The ASR, LID, and SV decoder models were optimized for their respective tasks through careful selection of hyper-parameters and architectural configurations. The ASR model directly follows the setting in ML-SUPERB benchmark [Shi et al., 2023a] for comparison. Table 5 summarizes these configurations. The \u201cfull\u201d means one epoch is trained by passing all the training data. ", "page_idx": 12}, {"type": "table", "img_path": "aXApeuAYkg/tmp/85858b67ee222c4fa132981490428b4dc84489ef70ea350638557272595d6fa5.jpg", "table_caption": ["Table 5: Hyper-parameters used for training ASR, LID, and SV decoder models "], "table_footnote": [], "page_idx": 12}, {"type": "table", "img_path": "aXApeuAYkg/tmp/343c5c59fa5d4d135710b57123d6ef7eba8624e1bc0e472f12b5b33e88b38669.jpg", "table_caption": ["Table 6: Training hyper-parameters for CA-SSLR models. The superscripts \u201cDec\u201d and \u201cFeat\u201d represent the decoder and the feature projection layer, respectively. "], "table_footnote": [], "page_idx": 12}, {"type": "text", "text": "A.2 CA-SSLR Hierarchical Models ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Table 6 provides detailed configurations for the CA-SSLR model. In the ${\\mathrm{CA-SSLR}}^{L,S}$ setup, two 256- dimensional embeddings are used to encapsulate language (L) and speaker (S) information, which then determine the parameters $(\\alpha_{\\mathrm{{L}}},\\gamma_{\\mathrm{{L}}},\\beta_{\\mathrm{{L}}})$ and $(\\alpha_{\\mathrm{{S}}},\\gamma_{\\mathrm{{S}}},\\beta_{\\mathrm{{S}}})$ following the procedure outlined in Eq. 5. The training adopts a stepwise approach, using initial parameters from an earlier phase to set up the next. The pretrained ASR, LID, and SV decoders serve as the foundation for initializing ${\\mathrm{CA}}{-}{\\mathrm{SSLR}}^{L}$ ; the SV decoder is fine-tuned further on top of ${\\mathrm{CA}}{\\mathrm{-}}{\\mathrm{SSLR}}^{L}$ ; and both ${\\mathrm{CA}}{-}{\\mathrm{SSLR}}^{L}$ and fine-tuned SV decoder initialize ${\\mathrm{CA-SSLR}}^{L,S}$ . In the table\u2019s \u201cTrainable modules\u201d row, the notations $\\mathrm{LID}^{\\mathrm{Feat}}$ and $\\mathrm{SV}^{\\mathrm{Feat}}$ indicate that the feature projection layers of the LID and SV decoders are adjustable during the training process. We conduct the in Table 6 and Figure 3 multiple times, and the variation are all within $0.2\\%$ CERs range. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A.3 Dataset License and Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.3.1 ML-SUPERB Dataset ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The ML-SUPERB dataset is assembled from a wide collection of multilingual speech corpora, with each contributing corpus being governed by one of a variety of open-source licenses, such as Creative Commons, MIT, GNU, or Free-BSD. These licensing agreements guarantee that the dataset is openly available and can be used freely for both commercial and scholarly research purposes. The 10-minute training set encompasses 37.4 hours of data, and the 1-hour dataset increases the total to 222.4 hours of data. Additionally, the dataset includes development and testing sets, containing 41.8 hours and 45.0 hours of data, respectively. This dataset is designed for multilingual speech recognition and language identification, as used in our work. ", "page_idx": 13}, {"type": "text", "text": "In the original ML-SUPERB settings, there are two types of languages: ", "page_idx": 13}, {"type": "text", "text": "\u2022 Normal Languages: Each has 10 minutes or 1 hour of data per language, used for both LID and ASR training with transcriptions.   \n\u2022 Few-Shot Languages: Each has only 5 utterances. In the original settings, these languages are not presented in the results for LID training and are used for ASR training with available transcriptions. ", "page_idx": 13}, {"type": "text", "text": "For the extended few-shot condition, we incorporate the language labels from these few-shot data for LID training but continue using only 5 utterances with transcriptions for ASR training. Since language labels are more accessible than transcriptions, especially in low-resource scenarios. Table 7 summarizes the data configurations for the original and extended few-shot conditions. ", "page_idx": 13}, {"type": "table", "img_path": "aXApeuAYkg/tmp/494445e436601bcb0a572641e82ac88d39b8ff0c58dec86c022e4c1c4008e108.jpg", "table_caption": ["Table 7: Data configurations for the original and extended few-shot conditions in ML-SUPERB. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "A.3.2 VoxCeleb Dataset ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The VoxCeleb dataset is available under the Creative Commons Attribution 4.0 International license and encompasses comprehensive training, development, and testing data collection. Specifically, it contains 1092 hours of audio from 5,994 speakers for training, 110 hours from 4,933 speakers for development, and 20 hours from 40 speakers designated for testing. Designed to facilitate speaker verification and identification tasks, aligns with our usage in the speaker verification task. To ensure privacy, speaker names within the dataset are anonymized and represented through unique speaker IDs. ", "page_idx": 13}, {"type": "text", "text": "B CER vs. Trainable Parameters ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Table 8 compares the mHuBERT model\u2019s ASR performance against the number of trainable parameters, where the XLSR counterpart is shown in Fig. 3. Both CA-mHuber $\\mathrm{{\\frac{}{d u a l}}}_{}$ and CA-mHuber $\\underset{\\mathrm{-dual}}{L,S}$ are with 256 condition feature dimensions. Notably, the CA-mHuber $\\mathbf{\\Pi}_{\\mathrm{{dual}}}^{L}$ model excels in few-shots scenarios, while the CA-mHuber $\\mathbf{\\Pi}_{\\mathrm{dual}}^{L,S}$ yields CERs for normal languages comparable to a fully fine-tuned 12-layer mHuBERT model using only 15.9M parameters. This efficiency demonstrates the TCA conditioner\u2019s capability in the CA-SSLR framework to deliver fine-tuned levels of ASR accuracy with a significantly reduced parameter count, providing an optimal balance for practical ASR applications. ", "page_idx": 13}, {"type": "text", "text": "Table 8: Comparison of trainable parameters and CERs on ML-SUPERB 10min dataset, including fine-tuning top layers, LoRA, and dual-inference condition aware mHuBERT model. ", "page_idx": 14}, {"type": "table", "img_path": "aXApeuAYkg/tmp/8d0bcb2ac1aaa7b893deafc14501b9d1306367eb84954de381055a2673f77335.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "C Training Efficiency and Resource Usage ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We compare the training speed and resource consumption of different adaptation methods, including Houlsby Adapters, CA-SSLR, and full fine-tuning (FT). Table 9 summarizes the bottleneck dimensions, training times, and peak memory usage for each method. We evaluate the training speech for 10k iterations with batch size 8. We found that the CA-SSLR approach ranks second compared to the Houlsby Adapter and a fully fine-tuning approach in speed and memory usage. However, it is important to note that CA-SSLR surpasses the Houlsby Adapter in adaptation effectiveness and generalization ability, as demonstrated in Table 1. These results indicate that although CA-SSLR incurs a moderate increase in training resources, it provides benefits in performance and generalization. We acknowledge that the current implementation of CA-SSLR is not yet optimized for speed and memory efficiency. Future work will focus on optimizing the model to reduce training time and memory consumption without compromising performance. ", "page_idx": 14}, {"type": "table", "img_path": "aXApeuAYkg/tmp/169208b188bd3c4e55ac13f7fb3b54fcfa7a50794e4cba761e1e4080d2cabd09.jpg", "table_caption": ["Table 9: Comparison of adaptation methods in terms of bottleneck dimensions, training speed, and peak memory usage. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "D RTF Analysis ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Tables 10 and 11 present the real-time factor (RTF) for each individual component, as well as for the combined systems. In Table 11, the term \"separated tasks\" refers to duplicating and fine-tuning the SSLR for each task individually, along with the corresponding total RTF. ", "page_idx": 14}, {"type": "text", "text": "E Few-shots Results ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Within the ML-SUPERB dataset\u2019s 20 few-shots languages, we examined the performance of CASSLR against the established SSL baselines, XLSR and mHuBERT, on models trained in the 10-minute ML-SUPERB set. The LID results indicate a close match between CA-SSLR and the baseline, with approximately half of the few-shot languages exhibiting improvements or matching their baseline performance. Section 5.4 reveals that SSL-based LID models are inherently effective, and extending full fine-tuning does not necessarily enhance results. This observation aligns with the outcomes of other classification tasks adeptly handled by SSL models, as documented in [Chen et al., 2023b]. Furthermore, the CA-SSLR framework demonstrates subtle enhancements for the Normal languages in the 10-minute set in Table 3, indicating that the LID performance remains robust despite the encoder\u2019s additional modifications. ", "page_idx": 14}, {"type": "table", "img_path": "aXApeuAYkg/tmp/e803c6fd651ae40246859bf6fd4c555dda2729c3e87ff9ca816182201af5fb28.jpg", "table_caption": [], "table_footnote": ["Table 10: RTF for different components. "], "page_idx": 15}, {"type": "table", "img_path": "aXApeuAYkg/tmp/43313c2c43af4245a513862f34d4cb0f062d3d81209bac618eaa88a3721ceea6.jpg", "table_caption": [], "table_footnote": ["Table 11: Total RTFs for combined systems. "], "page_idx": 15}, {"type": "text", "text": "Table 12: Evaluation of LID and ASR performance in terms of Accuracy (Acc) and Character Error Rates (CERs) for few-shot learning in low-resource languages using the ML-SUPERB 10-minute set, comparing on XLSR and mHuBERT models. ", "page_idx": 15}, {"type": "table", "img_path": "aXApeuAYkg/tmp/55229b14ba748b0716863ec69d5649afe14981a4c749e200724220714d3ad8b1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Regarding the ASR results, most languages achieved significant CER reductions, ranging from a modest few percent to over $30\\%$ , when compared with SSL baselines. Notably, the Bosnian (bos) language experienced an impressive $45.1\\%$ relative improvement in CER, while Cebuano (ceb) improved by $39.5\\%$ with the XLSR model. With the mHuBERT model, the most substantial gains were observed in Sundanese (sun) and Toki Pona (took), with $19.9\\%$ and $17.2\\%$ CER relative improvements, respectively. These results underscore the CA-SSLR framework\u2019s profound effect in bolstering ASR performance for few-shot languages, especially demonstrating more pronounced improvements with the XLSR model. ", "page_idx": 16}, {"type": "text", "text": "When examining the correlation between LID accuracy and ASR performance, it is apparent that a lower CER does not necessarily align with high LID accuracy. For instance, Serbian (srp) on the XLSR model, despite having a modest LID accuracy of $50.9\\%$ , shows a CER improvement from $57.4\\%$ to $48.1\\%$ . Conversely, Fulah (ful), the sole language to exhibit a CER increase in the XLSR model, presents a higher LID accuracy of $67.5\\%$ . This indicates that the CA-SSLR framework\u2019s efficacy is not solely contingent on high LID prediction accuracy. CA-SSLR\u2019s reliance on embeddings instead of one-hot hard labels for predictions enables the model to maintain or improve ASR performance despite suboptimal LID scores. This approach allows the model to utilize embeddings to distinguish between easily confused languages, enabling the ASR model to predict the correct language accurately. ", "page_idx": 16}, {"type": "text", "text": "F Decode Examples ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Table 13 visualizes ASR outcomes for the XLSR and CA-SSLRL,S models on the ML-SUPERB 10-minute dataset, covering both few-shot and standard language scenarios. It highlights CA-SSLR\u2019s superior language recognition capabilities and success in rectifying the misclassifications encountered with XLSR, often resulting in completely incorrect transcriptions. This is evident in languages such as Lithuanian and Turkish, categorized as few-shot, and Bulgarian, which is better resourced (normal). These findings demonstrate the TCA conditioner\u2019s effectiveness in accurately managing LID embedding features and distinguishing between languages for downstream tasks. ", "page_idx": 16}, {"type": "text", "text": "Moreover, the results from other samples suggest that CA-SSLR can achieve better outcomes during training due to its incorporation of language information, even when the XLSR model correctly predicts the language. This underscores the efficacy of the TCA conditioner in exploiting languagespecific data, thereby enabling CA-SSLR to achieve heightened accuracy across a diverse range of languages. ", "page_idx": 16}, {"type": "text", "text": "G Ethical Statement ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We affirm our commitment to ethical research practices, including respect for privacy and the responsible use of data. The proposed CA-SSLR model improves multi-lingual ASR in 143 languages, including 20 low-resource ones with just five training utterances. In this manner, CA-SSLR contributes to the democratization of speech technology, fostering inclusivity for previously underserved communities. Furthermore, CA-SSLR prioritizes the reduction of computational costs at evaluation time, thereby aiming to mitigate the environmental impact associated with speech applications. We utilized publicly available datasets, namely ML-SUPERB and VoxCeleb, chosen for their moderate size to minimize computing requirements. Our utilization of pre-trained models, specifically XLSR and mHuBERT, aligns with their intended research purposes, as they are widely used within the speech research community. ", "page_idx": 16}, {"type": "text", "text": "However, the capacity for conducting speech and speaker recognition in human conversations poses a notable ethical concern linked to covert eavesdropping by nefarious entities. This capability could be exploited by authoritarian government bodies seeking to suppress free speech and identify dissidents. Therefore, it is imperative to promote public awareness and comprehension regarding the automated analysis of spontaneous speech and its ramifications. ", "page_idx": 16}, {"type": "table", "img_path": "aXApeuAYkg/tmp/510565a9b253e46f61100d92b99ec332f5cfaa7002efeadb220faa99a938ad30.jpg", "table_caption": [], "table_footnote": ["Table 13: The ground truth, predictions from XLSR and CA-SSLR models. Deletions are shown with red strikethrough text, insertions are underlined in blue, and substitutions are marked with yellow highlighting. "], "page_idx": 17}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 18}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 18}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 18}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 18}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 18}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 18}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 18}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: We discuss the generalization ability for the condition-aware SSLR in Sec. 3, and it has been thoroughly evaluating in Sec. 5. We compare the generalization ability across different tasks, which is unseen for the SSLR model. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 18}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: We discussion the limitation in Sec.6. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 19}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 19}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We detail the dataset settings in Sec. 4.1, and Sec. 4.2 describes the model architecture. The detail parameters settings is described in Sec. A. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 20}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 20}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: Since the experiments are conducted with ESPnet and $S3\\mathrm{Pr}1$ , it is harder to submit the whole package for the review, but we promise to release our branch after the review process. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. ", "page_idx": 20}, {"type": "text", "text": "\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). \u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 21}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We detail the dataset settings in Sec. 4.1, and Sec. 4.2 describes the model architecture. The detail parameters settings is described in Sec. A. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 21}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: In Appendix A.2, we report the error bars results while running the experiments different times. Also, we conduct different experiments with slightly different settings, and provide consistant ASR and SV improvements, where the variation among these experiments is much smaller than the improvement comparing with the baseline. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 21}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We discuss the computation resource for training in 4.2, and RTF in Sec.5.2 and Sec.5.3. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 22}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We include the code of ethics in Appendix G. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 22}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: We discussion the broader impacts in Sec.6. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 22}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: Our model has been designed for ASR, SV, and LID, which has been welly studied and have lower risk. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 23}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We mention the datasets and its license in Appendix A.3. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 23}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We detail the dataset settings in Sec. 4.1, and Sec. 4.2 describes the model architecture. The detail parameters settings is described in Sec. A. The model will be released to public after the review process. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets. \u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. ", "page_idx": 23}, {"type": "text", "text": "\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 24}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: We do not research with human subject in our experiments. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 24}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: We do not research with human subject in our experiments. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}]