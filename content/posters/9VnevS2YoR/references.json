{"references": [{"fullname_first_author": "Alex Krizhevsky", "paper_title": "Imagenet classification with deep convolutional neural networks", "publication_date": "2012", "reason": "This paper is foundational for deep learning in computer vision, introducing the successful AlexNet architecture and demonstrating the power of deep convolutional neural networks for image classification."}, {"fullname_first_author": "Kaiming He", "paper_title": "Deep residual learning for image recognition", "publication_date": "2016", "reason": "This paper introduced the ResNet architecture, which addressed the vanishing gradient problem in training very deep networks, enabling significant advancements in image recognition accuracy."}, {"fullname_first_author": "Alexey Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2020", "reason": "This paper introduced the Vision Transformer (ViT), adapting the Transformer architecture from natural language processing to computer vision, demonstrating strong performance and a new approach to image processing."}, {"fullname_first_author": "Ze Liu", "paper_title": "Swin transformer: Hierarchical vision transformer using shifted windows", "publication_date": "2021", "reason": "This paper introduced Swin Transformer, a hierarchical Transformer architecture that improved upon ViT, achieving state-of-the-art results on various computer vision tasks."}, {"fullname_first_author": "Albert Gu", "paper_title": "Efficiently modeling long sequences with structured state spaces", "publication_date": "2021", "reason": "This paper introduced the foundation for the Mamba model, presenting a novel approach to efficiently model long sequences using structured state spaces, which directly inspired the work presented in the main paper."}]}