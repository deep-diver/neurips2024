[{"figure_path": "NU3tE3lIqf/figures/figures_0_1.jpg", "caption": "Figure 1: WildGaussians extends 3DGS [14] to scenes with appearance and illumination changes (left). It jointly optimizes a DINO-based [27] uncertainty predictor to handle occlusions (right).", "description": "This figure showcases the capabilities of WildGaussians in handling complex scenes. The left side demonstrates its ability to extend 3D Gaussian Splatting (3DGS) to manage scenes with variations in appearance and lighting conditions.  The right side highlights the joint optimization of a DINO-based uncertainty predictor to effectively address occlusions within the scene, thus producing a more accurate and robust 3D reconstruction.", "section": "Introduction"}, {"figure_path": "NU3tE3lIqf/figures/figures_2_1.jpg", "caption": "Figure 2: Overview over the core components of WildGaussians. Left: appearance modeling (Sec. 3.2). Per-Gaussian and per-image embeddings are passed as input to the appearance MLP which outputs the parameters of an affine transformation applied to the Gaussian's view-dependent color. Right: uncertainty modeling (Sec. 3.3). An uncertainty estimate is obtained by a learned transformation of the GT image's DINO features. To train the uncertainty, we use the DINO cosine similarity (dashed lines).", "description": "This figure illustrates the core components of the WildGaussians approach. The left side shows the appearance modeling process, where per-Gaussian and per-image embeddings are fed into an MLP to generate an affine transformation for the Gaussian's color. The right side depicts the uncertainty modeling, utilizing DINO features from the ground truth and rendered images to estimate uncertainty via cosine similarity, aiding in occlusion handling during training.", "section": "3 Method"}, {"figure_path": "NU3tE3lIqf/figures/figures_4_1.jpg", "caption": "Figure 3: Uncertainty Losses Under Appearance Changes. We compare MSE and DSSIM uncertainty losses (used by NeRF-W [24] and NeRF On-the-go [31]) to our DINO cosine similarity loss. Under heavy appearance changes (as in Image 1 and 2), both MSE and DSSIM fail to focus on the occluder (humans) and falsely downweight the background, while partly ignoring the occluders.", "description": "This figure compares three different uncertainty loss functions: MSE, DSSIM, and DINO Cosine.  It uses two example images showcasing significant appearance changes (e.g., lighting, shadows). The heatmaps illustrate how each loss function weights the pixels. MSE and DSSIM incorrectly downplay the importance of the occluded regions (humans), while the DINO Cosine loss effectively focuses on the occluders. The comparison highlights the superior robustness of the DINO Cosine loss to variations in appearance when identifying occluded areas.", "section": "3.3 Uncertainty Modeling for Dynamic Masking"}, {"figure_path": "NU3tE3lIqf/figures/figures_7_1.jpg", "caption": "Figure 4: Comparison on NeRF On-the-go Dataset [31]. For both the Fountain and Patio-High scenes, we can see that the baseline methods exhibit different levels of artifacts in the rendering, while our method removes all occluders and shows the best view synthesis results.", "description": "This figure compares the performance of 3DGS [14], NeRF On-the-go [31], and WildGaussians (the proposed method) on three scenes from the NeRF On-the-go dataset with varying occlusion levels (5%, 17%, and 26%).  The images show that the baseline methods struggle to remove occlusions and produce artifacts, especially in the high-occlusion scene. In contrast, WildGaussians successfully removes occlusions and generates photorealistic renderings, highlighting its improved ability to handle occlusions compared to existing methods.", "section": "4.1 Comparison on the NeRF On-the-go Dataset"}, {"figure_path": "NU3tE3lIqf/figures/figures_8_1.jpg", "caption": "Figure 5: Comparison on the Photo Tourism Dataset [35]. In the first row, note that while none of the methods can represent the reflections and details of the flowing water, 3DGS and WildGaussians can provide at least some details even though there are no multiview constraints on the flowing water. On the second row, notice how 3DGS tries to 'simulate' darkness by placing dark - semi-transparent Gaussians in front of the cameras. For WildGaussians, the text on the building is legible. WildGaussians is able to recover fine details in the last row.", "description": "This figure compares the performance of three different methods (3DGS, K-Planes, and WildGaussians) on the Photo Tourism dataset.  The results show WildGaussians' superior ability to handle challenging scenarios such as reflections, fine details, and occlusions, as seen in the comparison of the Trevi Fountain, Brandenburg Gate, and other scenes. The methods are evaluated based on their visual quality and detail preservation when compared to ground truth images.", "section": "4.2 Comparision on Photo Tourism"}, {"figure_path": "NU3tE3lIqf/figures/figures_9_1.jpg", "caption": "Figure 6: Appearance interpolation. We show how the appearance changes as we interpolate from a (daytime) view to a (nighttime) view's appearance. Notice the light sources gradually appearing.", "description": "This figure demonstrates the smooth transition of appearance changes as the model interpolates between a daytime view and a nighttime view.  The gradual appearance of light sources highlights the model's ability to handle variations in illumination.", "section": "5 Conclusion"}, {"figure_path": "NU3tE3lIqf/figures/figures_9_2.jpg", "caption": "Figure 7: Fixed appearance multi-view consistency. We shows the multiview consistency of a fixed nighttime appearance embedding as the camera moves around the fountain.", "description": "This figure demonstrates the robustness of the proposed appearance modeling.  It shows multiple renderings of the Trevi Fountain at night from different viewpoints, all using a single, fixed nighttime appearance embedding. The consistency across views highlights the effectiveness of the approach in handling changes in viewpoint while maintaining a consistent appearance.", "section": "5 Conclusion"}, {"figure_path": "NU3tE3lIqf/figures/figures_9_3.jpg", "caption": "Figure 8: t-SNE for Appearance Embedding. We visualize the training images\u2019 appearance embeddings using t-SNE. See the day/night separation.", "description": "This figure visualizes the appearance embeddings of training images using t-SNE, a dimensionality reduction technique.  The visualization shows that the embeddings are clustered according to image appearance, with day and night images forming distinct groups, demonstrating the effectiveness of the appearance embedding in capturing variations in lighting conditions.", "section": "5 Conclusion"}, {"figure_path": "NU3tE3lIqf/figures/figures_14_1.jpg", "caption": "Figure 9: Photo Tourism ablation study. We show VastGaussian-style appearance modeling, no appearance modeling, no uncertainty modeling, no Gaussian embeddings (only per-image embeddings), and the full method.", "description": "This figure presents an ablation study on the Photo Tourism dataset, comparing different variations of the WildGaussians model.  It shows the results of removing key components of the model: appearance modeling, uncertainty modeling, and per-Gaussian embeddings.  By comparing these results to the full WildGaussians model and the ground truth, the figure illustrates the contribution of each component to the overall performance. The results demonstrate the impact of each component on the model's ability to accurately reconstruct the scene.", "section": "A.3 Extended Ablation Study"}, {"figure_path": "NU3tE3lIqf/figures/figures_15_1.jpg", "caption": "Figure 10: Occluders present in the Photo Tourism [35] and NeRF On-the-go [31] datasets.", "description": "This figure shows example images from the Photo Tourism and NeRF On-the-go datasets to illustrate the types of occlusions present in each dataset.  The Photo Tourism dataset contains scenes with approximately 3.5% occlusion, primarily featuring people walking in front of the monuments.  The NeRF On-the-go dataset shows examples of low (5%) and high (26%) occlusion scenarios, with the higher occlusion scenes clearly demonstrating people and objects obstructing the main view.", "section": "4 Experiments"}]